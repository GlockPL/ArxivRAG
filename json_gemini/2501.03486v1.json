{"title": "Align-Pro: A Principled Approach to Prompt Optimization for LLM Alignment", "authors": ["Prashant Trivedi", "Souradip Chakraborty", "Avinash Reddy", "Vaneet Aggarwal", "Amrit Singh Bedi", "George K. Atia"], "abstract": "The alignment of large language models (LLMs) with human values is critical as these models become increasingly integrated into various societal and decision-making processes. Traditional methods, such as reinforcement learning from human feedback (RLHF), achieve alignment by fine-tuning model parameters, but these approaches are often computationally expensive and impractical when models are frozen or inaccessible for parameter modification. In contrast, prompt optimization is a viable alternative to RLHF for LLM alignment. While the existing literature has shown empirical promise of prompt optimization, its theoretical underpinning remains under-explored. We address this gap by formulating prompt optimization as an optimization problem and try to provide theoretical insights into the optimality of such a framework. To analyze the performance of the prompt optimization, we study theoretical suboptimality bounds and provide insights in terms of how prompt optimization depends upon the given prompter and target model. We also provide empirical validation through experiments on various datasets, demonstrating that prompt optimization can effectively align LLMs, even when parameter fine-tuning is not feasible.", "sections": [{"title": "Introduction", "content": "The quest to align large language models (LLMs) with human values is not just an academic pursuit but a practical necessity [1, 2]. As these AI models (e.g., ChatGPT, Llamma2, etc.) increasingly become an essential part of various aspects of daily life and decision-making processes, ensuring their outputs reflect ethical considerations and societal norms becomes crucial [3, 4]. The standard approach to aligning LLMs has been through fine-tuning parameters via reinforcement learning from human feedback (RLHF) [5, 6, 7], which involves three main steps: Supervised Fine-Tuning (SFT), reward learning, and RL fine-tuning."}, {"title": "Preliminaries and Background", "content": "This section provides the essential background and foundational concepts relevant to alignment. We start by defining the notation, followed by a quick overview of the RLHF framework, which involves three key steps: (i) supervised fine-tuning (SFT), (ii) reward learning, and (iii) fine-tuning with RL.\nLanguage Models. We start by defining the language model mathematically. Let us denote the vocabulary set by V, and we denote the language model by \u03c0(y|x), which takes in the sequence of tokens x := {X1,X2,\u00b7\u00b7\u00b7,XN} (with each xi \u2208 V) as an input, and generate response y := {Y1, Y2, \u2026, \u0423\u041c} (with each y\u2081 \u2208 V) as the output. At instant t, each output token yt ~ \u03c0(\u00b7|Xt).\nSupervised Fine-Tuning (SFT). SFT is the initial step in the RLHF process. It involves fine-tuning a pre-trained LLM on a vast dataset of human-generated text in a supervised manner.\nReward Learning. This stage involves learning the reward model by gathering preferences from experts/human feedback or an oracle based on outputs generated by the SFT model denoted by sft. The optimization is generally performed under the Bradley-Terry model for pairwise comparison [44], which seeks to minimize the loss formulated as:\nL(r, Dr) = -E(x,yu,yv)~D, [log (\u03c3(r(x, yu) \u2013 r(x, yv)))]\nwhere D, denotes the dataset of response pairs (yu, Yv), with yu and yu representing the winning and the losing responses, respectively, which are generated by the policy asft optimized under the reward r(x, y), and evaluated by human experts or an oracle function p*(\u00b7|Yu, Yv, x), and \u03c3(\u00b7) is the sigmoid function.\nFine-tuning with RL. In this step, we obtain the aligned model which maximizes the reward model r(x, y) (trained in the previous step) by solving a KL-regularized optimization problem:\nmax Ex~P,y~\u03c0(1x) [r(x, y) \u2013 \u03b2DKL(\u03c0(\u00b7|x)||\u03c0sft(\u00b7|x))],\n\u03c0\nwhere, \u03b2 > 0 is a parameter that controls the deviation from the baseline policy #sft. This iterative process alternates between updating the policy and reward models until convergence, as detailed in previous works [2, 5]."}, {"title": "Prompt Optimization Framework for LLM Alignment", "content": "In this section, we provide a mathematical formulation for the framework of prompt optimization for LLM alignment. In traditional LLM alignment, as described in (2), the model parameters are fine-tuned to adjust the response distributions in a way that maximizes the reward function. However, in our setting, we operate under a different regime, starting with a pre-trained language model, denoted by \u03c0F, whose parameters remain frozen. In this case, direct modification of the model to align with a reward function is not allowed. Therefore, an alternative and widely adopted approach in the literature is to optimize the input prompt itself to yield better-aligned responses [15, 11, 45]. Typically, this process involves iterative prompt refinement, where the model outputs are evaluated and compared to human preferences, and the prompts are adjusted accordingly. However, such iterative fine-tuning can be computationally expensive and time-intensive.\nInterestingly, although we cannot fine-tune the frozen model \u03c0F, we can fine-tune the prompter model p in any desired manner. However, a fundamental challenge arises: what should be the objective for optimizing the prompter? While substantial empirical evidence in the literature demonstrates that prompt optimization can significantly enhance response generation and improve alignment [11, 15, 45], there is no specific emphasis on developing a mathematical framework to guide this process. We start by addressing this gap as follows.\nOptimization Objective for Prompter Design. First, we revisit the basics of LLM alignment. For a given prompt x, the probability of generating a response y from the frozen model is represented by \u03c0F(y|x). After introducing the prompter model p, the probability of generating response y given input x (denoted by p) can be expressed as:\n\u03c0\u03c1(y|x) = \u03a3x'\u03c0F(y/x')p(x'|x),\nwhich captures the probability of generating the response y for a given x under the influence of the prompter p. Let us consider the ideal scenario: if we were able to fine-tune the language model \u03c0F, we would solve the optimization problem in (2) and obtain the RLHF optimal solution \u03c0*, which is given by [46, 47]\n\u03c0*(y|x) = 1/Z*(x)*\u03c0F(yx) exp(r*(x, y)/\u03b2),\nwhere Z*(x) = \u03a3y \u03c0F(y|x) exp(r*(x, y)/\u03b2) is the normalizing constant, and \u1e9e is the alignment tuning parameter, and reward r* is obtained from solving (1). We emphasize that if we have a prompter p that performs as well as the RLHF-optimal policy \u03c0*, it should be a sufficient indicator of a good prompter. With this understanding, we consider the following prompter suboptimality gap given by\n\u25b3(\u03c1) := J(\u03c0*) \u2013 J(\u03c0\u03c1),\nwhich captures how well our prompter is doing with respect to fine-tuned optimal policy \u03c0*. Mathematically, it holds that\nJ(\u03c0*) \u2013 J(\u03c0\u03c1) = Ex~P,y~\u03c0*(\u00b7|x) [r*(x, y)] \u2013 Ex~P,y~\u03c0\u03c1(\u00b7|x) [r*(x, y)]\n= Be~P Ey~\u03c0*(\u00b7|x) [r*(x, c, y)] - \u0395x'~p(x) [r* (x, y)].\nEquation (6) evaluates the difference in expected return between the optimal RLHF policy \u03c0* and our prompt optimization policy \u03c0\u03c1, indicating how much better (or worse) \u03c0* performs"}, {"title": "Proposed Approach: Align-Pro", "content": "Let us start by addressing Q1 and develop a general prompt optimization framework to design an optimal prompter p*. But then the first question arises: in what sense is p* optimal? In order to see that, let us reconsider J(\u03c0*) \u2013 J(\u03c0\u03c1) and after adding-subtracting Ey~\u03c0F(x) [r* (x, y)] in the right hand side of Equation (6), we get\nJ(\u03c0*) \u2013 J(\u03c0\u03c1) = Ex~P[\u03941 + \u03942],\nwhere and A2 are defined as\n\u03941 := Ey~\u03c0*(\u00b7|x) [r*(x, y)] \u2013 Ey\u223c\u03c0F(\u00b7|x) [r*(x, y)]\n\u03942 := Ey~\u03c0\u03c1(\u00b7|x) [r*(x, y)] - Ey~\u03c0\u03c1(\u00b7\u00a6x) [r* (x, y)]\n= Ey~\u03c0F(\u00b7|x) [r* (x, y)] - Ex'~p(\u00b7|x) [r*(x, y)].\ny~\u03c0F(x')\nWe remark that in (7), \u0394\u2081 is the suboptimality gap between the optimal fine-tuned policy, and the frozen model \u03c0F. Thus, it captures the effectiveness of the optimal RLHF policy with respect to the frozen model. In other words, it quantifies how good or bad our frozen model is with respect to the optimally aligned model. We note that \u25b3\u2081 is constant for a given TF and does not depend upon prompter p, hence we cannot improve this part with the prompter. Another insight is that since \u03c0* is the optimal RLHF policy, \u2206\u2081 \u2265 0, i.e., is always positive. On the other hand, the second term, A2, depends upon our prompter pand can be controlled by designing a prompter. This observation leads to the formulation of an optimization problem for the prompter as follows."}, {"title": "Optimization Problem for Prompter", "content": "We recall from the definition of A2 that we would need to learn ap such that A2 is minimized. To achieve that, we recognize that the only term involving the prompter p in \u03942 is Ex'~p(\u00b7\\x),y~\u03c0\u03c1(\u00b7|x') [r*(x, y)], and minimizing A2, we need to solve the following optimization problem\nmax Ex'~p(:\\x),y~\u03c0F(\u00b7|x') [r*(x, y)].\n\u03c1\nHowever, at the same time, since our prompter is also another language model, we will already have access to a baseline supervised fine-tuned prompter psft, and we want to ensure that our prompter p* does not deviate significantly from psft, which motivates us to include a known and supervised fine-tuned prompter, denoted by psft. Thus, we solve the following optimization problem:\nmax Ex~PE x'~p(x) [r* (x, y)] \u2013 ADKL(p(\u00b7|x)||psft(\u00b7|x)).\n\u03c1\ny~\u3160F(x')\nWe have introduced a KL-divergence-based regularizer above between the prompter pand a reference supervised fine-tuned prompter psft. This helps with the development of a proper optimization problem with a closed-form expression and enables control over proximity to the initial prompter psft through the tuning parameter \u5165. We note that the formulation in (9) has also appeared in the red teaming literature for learning an attacker promoter [39, 40, 41, 42, 43].\nInterpretation of \u03bb. Another interesting interpretation of A is that it controls the extent of prompt optimization we want to introduce into the pipeline, hence we also refer to it as the prompt tuning parameter. For instance, \u5165 \u2192 \u221e means no prompt optimization, while \u5165 \u2192 0, drives the optimization toward maximizing the prompter reward, albeit at the cost of deviating from psft which might be important in certain cases. Therefore, A provides a meaningful trade-off, and its effects will be further elucidated in the following section.\nThe following Lemma 5.1 provides the optimal solution to the optimization problem (9).\nLemma 5.1. Let R(x, x') := Ey\u223c\u03c0F(\u00b7|x')[r*(x, y)], and X > 0 be the prompter tuning parameter. The optimal prompt distribution p* that maximizes the objective function of the optimization problem (9) is given by:\np*(xx) = 1/Z(x)* Psft (x'\\x) exp (1/\u03bbR(x,x'));\nwhere Z(x) is the log partition function given by\nZ(x) = \u2211 Psft (x'\\x) exp (1/\u03bbR(x,x'))\nx'\nThe proof is available in Appendix A and follows from the derivations in [48]. Next, we move on to answer Q2, in which we utilize the optimal prompter p*(x'|x) to obtain a bound on the suboptimality gap. Notably, the integration of this optimal prompter with the frozen model will lead to the refined performance expressed in terms of the modified optimal policy *(y/x) = \u03a3\u03b1' \u03c1*(x'|x)\u03c0\u03c1(y|x'). This will capture the effectiveness of the prompt optimization process and offer insights into how closely the modified policy \u03c0\u03c1* approximates the true optimal policy \u03c0*."}, {"title": "Theoretical Insights w.r.t Fine-Tuning", "content": "We begin by establishing a bound on the suboptimality gap for the optimal prompter. The following theorem bounds the suboptimality gap J(\u03c0*) \u2013 J(\u03c0\u03c1*) when the optimal prompter p* as obtained in Lemma 5.1 is used. We present our result in Theorem 6.1 as follows. The proof is available in the Appendix B.\nTheorem 6.1. Let the optimal prompter p*(x'|x) be given as in Equation (10). Then, the suboptimality gap is bounded as\nJ(\u03c0*) \u2013 J(\u03c0\u03c1*) \u2264 ''maxEz~p[dTv(\u03c0*(\u00b7|x), \u03c0F(\u00b7|x))] + TmaxEz~P,x'~psft(\u00b7|x) [dTV (\u03c0F(\u00b7|x), \u03c0F(\u00b7|x'))]\n- \u03bb Ex~P[DKL(p*(\u00b7|x)||psft(\u00b7|x))],\nwhere P denotes the prompt distribution, A is the prompter tuning parameter, and dry is the total variation distance.\nTheorem 6.1 provides an upper bound on the suboptimality gap between an optimal RLHF policy \u03c0* and the optimal policy obtained by the prompt optimization approach \u03c0\u03c1*. We now provide the interpretations to each term of the suboptimality gap given in Theorem 6.1.\n\u2022 Significance of first term in RHS of (11): The first term in Equation (11) is always non-negative. It captures the intrinsic difficulty of obtaining the optimal RLHF policy via a prompt optimization setup when the frozen model is not fully aligned. We note that when \u03c0F = \u03c0*, the first term in Theorem 6.1 becomes zero. However, this scenario is not relevant to our prompt optimization framework, as it necessitates fine-tuning the frozen LLM.\n\u2022 Significance of second term in RHS of (11): This term measures how much the response distribution the frozen policy \u03c0F changes when its input changes from x to x' under psft. For psft as delta distribution, this term will be zero, which essentially implies that this term is trying to capture the variation in the prompts (which should be minimal) due to the introduction of psft into the formulation.\n\u2022 Significance of third term in RHS of (11): The third term captures the KL divergence between the optimal prompter p* and the given prompter psft. This term is important because it explains that we can reduce the suboptimality bound via prompt optimization, which is making p* far from psft, which can be controlled by the parameter \u03bb.\nAnother interesting insight is that the upper bound on the suboptimality remains non-negative for DKL(p*(\u00b7|x) || Psft (\u00b7|x)) < \u20ac1+\u20ac2, where 61 and 62 are defined as \u20ac1 := drv (\u03c0*(\u00b7|x), \u03c0F(\u00b7|x)) and \u20ac2 := Ex'~psft(\\x) [dTV (\u03c0F(\u00b7|x), \u03c0\u03c1(\u00b7|x'))]. This essentially provide insight that in practice, with a budget of 12 for the prompter optimization can be sufficient to achieve performance similar to RLHF based fine tuning. This further highlights that we won't need to choose an optimal prompter arbitrarily far from the base prompt distribution, thereby preventing a significant loss in the quality (e.g., perplexity) of the generated outputs."}, {"title": "Experimental Evaluations", "content": "In this section, we present proof of concept experiments to validate the theoretical insights of our proposed prompt optimization framework, which we named Align-Pro. We outline our experimental setup, including the dataset, model architecture, and evaluation metrics. Following this, we present our results and provide a detailed analysis of our findings."}, {"title": "Experimental Setup", "content": "We evaluate the performance of our Align-Pro using two distinct prompter models, denoted as P1 (Phi-3.5-Instruct) and P2 (Qwen-2.5-1.5B-Instruct), which modifies and updates the original prompt. Additionally, we use two frozen models, denoted as F1 (Llama-3.1-8B-Instruct) and F2 (Llama-3.1-8B-Instruct) to generate the final responses. This setup results in four unique model architectures, each representing a combination of the prompter and frozen models. For each architecture, we assess performance for the following three different configurations.\n\u2022 No Fine-Tuning: In this configuration, the prompter is not used, and only the frozen model is used to generate responses without any fine-tuning or prompt modifications.\n\u2022 Align-Pro: In this setup, a fine-tuned prompter is placed before a frozen model. The prompter refines the input prompt, and the frozen model generates the response based on the optimized prompt.\n\u2022 RLHF: In this configuration, the frozen model undergoes fine-tuning through RLHF, and the response is generated directly from this fine-tuned model.\nDatasets: To capture the diversity in our experimental evaluations, we evaluate the performance over different datasets:\n\u2022 UltraFeedback [49] : A large-scale, high-quality, and diversified AI feedback dataset which contains feedback from user-assistant conversations from various aspects. This dataset evaluates the coherence of the prompt-response pairs.\n\u2022 HelpSteer [50]: A multi-attribute helpfulness dataset annotated for correctness, coherence, complexity, and verbosity in addition to overall helpfulness of responses.\n\u2022 Orca [51]: This dataset features responses with detailed explanations for each prompt, promoting thinking and effective instruction-following capabilities in the models.\nEvaluation Criteria. The primary objective of our experiments is to optimize the input prompt to guide the frozen LLM that produces the desired response effectively. We fine-tune the prompter using proximal policy optimization (PPO) within the RLHF framework to achieve this. The reward signal for this fine-tuning process is derived from the quality of the enhanced prompt and the output generated by the frozen LLM. We assess the performance of Align-Pro based on three key metrics: mean reward, variance, and win-rate comparison against the no-fine-tuning baseline."}, {"title": "Results", "content": "Mean reward and variance comparison: We calculate mean rewards and variances to assess the quality of preferred response generation and the diversity of the language model for all configurations and different model architectures. To associate the reward to each response, we use the available reward model\u00b2, which scores the response. This reward model is trained to assign higher scores to the responses that comply with the off-target attributes.\nWe also compared Align-Pro with an oracle model, where the LLM is fine-tuned using RLHF. Interestingly, Align-Pro consistently outperforms the baseline (no fine-tuning) in terms of mean reward, demonstrating its ability to generate more preferred and stable responses, leveraging prompt optimization and getting close to the performance of fine-tuned model denoted by oracle. Moreover, the variance in reward for Align-Pro is the lowest, indicating that it produces more reliable and stable outputs. In each figure, we employ two prompters, denoted as P1 (Phi-3.5-Instruct) and P2 (Qwen-2.5-1.5B-Instruct), along with two frozen LLMs, denoted as F1 (Llama-3.1-8B-Instruct) and F2 (Llama-3.1-8B-Instruct).\nWin rate comparison: We evaluate the performance of our Align-Pro method by comparing it to the no fine-tuning configuration using win rate as the primary performance metric. We rely on GPT-4 as an external, impartial judge to ensure unbiased evaluation. The evaluation criteria focus on critical aspects of the response: helpfulness, harmlessness, relevance, accuracy, depth, creativity, and level of detail. To update the prompt, we use a standardized system prompt template. The results clearly show that, on average, Align-Pro significantly outperforms the no fine-tuning approach across all model architectures and datasets. These findings demonstrate the effectiveness of Align-Pro framework, which enhances performance by optimizing the input prompt while keeping the LLM frozen."}, {"title": "Conclusion, Limitations and Future Work", "content": "This work introduces an optimization framework for prompt optimization by utilizing a smaller, trainable model to generate optimized prompts for a frozen large language model (LLM). This approach reduces computational costs while preserving the LLM's pre-trained capabilities. We provide a closed-form expression for the optimal prompter and use it to establish an upper bound on the suboptimality gap that compares the optimized prompt policy with the standard RLHF policy. We demonstrate the effectiveness of our method on three datasets and various model configurations. In each scenario, we observe that Align-Pro is better in terms of the mean rewards and win rate compared to the baseline with no fine-tuning.\nLimitations and future work: Our framework is inherently limited by the capabilities of the frozen language model. Another limitation includes the sensitivity of the prompt to the final response; a slight change in the prompt can lead to profound changes in the final responses. Theoretically, it would also be interesting to develop lower bounds on suboptimality and to develop further insights into the performance of prompt optimization. We will consider some of these issues as part of our future work. Some other potential future directions of our work include analyzing the robustness of the optimal prompter in the presence of noise in the frozen model and exploring the use of multiple prompters in sequence before inputting them into the frozen model."}, {"title": "Proof of Lemma 5.1", "content": "Lemma 5.1. Let R(x, x') := Ey\u223c\u03c0F(\u00b7|x')[r*(x, y)], and \u5165 > 0 be the prompter tuning parameter. The optimal prompt distribution p* that maximizes the objective function of the optimization problem (9) is given by:\np*(xx) = 1/Z(x)* Psft (x'\\x) exp (1/\u03bbR(x,x'));\nwhere Z(x) is the log partition function given by\nZ(x) = \u2211 Psft (x'\\x) exp (1/\u03bbR(x,x'))\nx'\nProof. Recall, from Equation (9), we have the following optimization problem\nmax Ex~P[Ex'~p(\u00b7\\x) [r*(x, y)] \u2013 ADKL(p(\u00b7|x)||psft(\u00b7|x))].\n\u03c1\ny~\u03c0F(x')\nNow, recall that the KL divergence between two distributions p(\u00b7|x) and psft(x) is given by\nDKL(P(x)||Psft(x)) = \u03a3x'\u03c1(x'x) log (P(x/2)/Psft (x'x))\nSimplifying the above objective, we have\nmaxp('\\x) (By-(\\')*(x, y)] - Alog(()))\nx'~\u03c0F(x')\nUsing the notation R(x, x') = Ey\u223c\u03c0F(:\\x') [r* (x, y)], we write the above objective function as\nmaxp(xx) (R(x,x) - Alog (())\nx'\nTo find the optimal p*(\u00b7|x), we take the derivative of the objective function with respect to p(x'x) and set it to zero\nR(x, x') Alog (\n\u03c1(x'x)/Psft (xx)\n) = 0."}, {"title": "Proof of Theorem 6.1", "content": "Theorem 6.1. Let the optimal prompter p*(x'|x) be given as in (12). Then, the suboptimality gap is given by\nJ(\u03c0*) \u2013 J(\u03c0\u03c1*) <rmaxEx~p[dTv(\u03c0*(\u00b7|x), \u03c0\u03c1(\u00b7|x))] + rmaxEx~PEx'~psft(\u00b7\\x) [dTV (\u03c0F(\u00b7|x), \u03c0F(\u00b7|x'))]\n- \u03bb Ex~P[DKL(p*(\u00b7|x)||psft(\u00b7|x))], where P denotes the prompt distribution, A is the prompter tuning parameter.\nProof. Recall the suboptimality gap definition from (7) for given prompter pas\nJ(\u03c0*) \u2013 J(\u03c0\u03c1) = Ex~P[\u03941 + \u03942], where \u0394\u2081 and A2 are given by\n\u0394\u2081 = Ey~\u03c0*(\u00b7|x) [r*(x, y)] - Ey\u223c\u03c0F(\u00b7|x) [r* (x, y)]\n\u25b32 = Ey~\u03c0F(\u00b7|x) [r*(x, y)] \u2013 Ex'~p(\u00b7|x),y~\u3160F(\u00b7\\x') [r* (x, y)].\nHence, we can write the performance gap corresponding to the optimal p* as\nJ(\u03c0*) \u2013 J(\u03c0\u03c1*) = Ex~P[\u03941 + \u2206], where\n\u03942 = Ey~\u03c0\u03c1(x) [r*(x, y)] - Ex'~p*(\u00b7\\x),y~\u03c0\u03c1(\u00b7\\x') [r*(x, y)].\nWe derive upper bound on the suboptimality defined in (24) in two steps. We first derive an upper bound on term \u0394\u2081 and then for \u2206. Consider the term A\u2081 as\n\u25b3\u2081 = Ey~\u03c0*(\u00b7|x) [r*(x, y)] \u2013 Ey~\u3160F(:|x) [r* (x, y)]<rmax[drv (\u03c0*(\u00b7|x), \u03c0\u03c1(\u00b7|x))], where the upper bound follows from the definition of TV norm. Next, to bound the term \u2206, we first observe that\nEy~\u03c0\u03c1(:|x) [r*(x, y)] = Ex'~psft(\u00b7|x),y~\u3160F(\u00b7|x) [r*(x, y)], which holds because r*(x, y) does not depend on the prompt distribution psft when y ~ \u03c0F(\u00b7|x). Thus, we can write\n\u2206 = Ex'~Psft(x), y~\u03c0\u03c1(\u00b7|x) [r*(x, y)] - Ex'~p*(\u00b7|x),y~\u03c0\u03c1(\u00b7|x') [r* (x, y)].\nWe further decompose \u2206 as follows\n\u0394\u2082 = Ex'~Psft(x),y~\u3160p(:|x) [r*(x, y)] - Ex'~psft(\u00b7|x),y~\u03c0F(\u00b7|x') [r*(x, y)] =:\u03943+ Ex'~psft(\\x),y~\u03c0\u03c1(\u00b7\\x') [r*(x, y)] - Ex'~p*(\u00b7\\x),y~\u03c0\u03c1(\u00b7\\x') [r* (x, y)]=:\u03944\nWe can bound A3 as\n\u21983 = Ex'~Psft(\u00b7|x),y~\u03c0\u03c1(\u00b7\\x) [r*(x, y)] - Ex'~psft(\\x),y~\u3160F(\u00b7|x') [r* (x, y)]<rmax Ex'~psft(x) [dTV (\u03c0F(\u2022|x), \u03c0F(\u00b7|x'))], again from the definition of TV norm. To bound \u22064, we utilize the optimality of prompter p*(\u00b7|x) as\nEx'~p*(\u00b7\\x),y~\u03c0F(\u00b7\\x') [r*(x, y)] \u2013 ADKL(p*(\u00b7|x)||Psft(\u00b7|x))\u2265 Ex'~psft(\\x),y~\u3160F(\u00b7\\x') [r*(x, y)] \u2013 ADKL(Psft(\u00b7|x)||Psft(\u00b7|x)) = Ex'~psft(\u00b7|x),y~\u03c0\u03c1(\u00b7|x') [r*(x, y)]. From the above inequality, we can write\n\u25b34 = Ex'~psft(x), y~\u03c0\u03c1(\u00b7|x') [r*(x, y)] \u2013 Ex'~p*(\u00b7\\x),y~\u03c0\u03c1(\u00b7\\x') [r*(x, y)] \u2264 \u2212XDKL(p*(\u00b7|x)||psft(\u00b7|x)).\nFrom Equations (31) and (34), we can write the upper bound for A as\n*Armax Ex'~psft(\u00b7\\x) [dTV (\u03c0F(\u00b7|x), \u03c0F(\u00b7|x'))] \u2013 ADKL(p*(\u00b7|x)||psft(\u00b7|x)).\nHence, finally we can write\nJ(\u03c0*) \u2013 J(\u03c0\u03c1*) = Ex~P[\u03941 + \u0394]<rmaxEx~P[dTv (\u03c0*(\u00b7|x), \u03c0\u03c1(\u00b7|x))] + rmaxEx~PEx'~psft(\u00b7\\x) [dTV (\u03c0F(\u00b7|x), \u03c0F(\u00b7|x'))]\n\u2013 XE\u00a4~P[DKL(p*(\u00b7|x)||psft(\u00b7|x))].\nHence proved."}, {"title": "Some Additional Experimental Details", "content": "Here we provide a detailed description of the experimental setup and results that demonstrate the effectiveness of our prompt optimization framework."}, {"title": "Meta Prompt", "content": "We first observe that without the meta-prompt, the prompter tends to respond directly to the given input rather than rephrasing it into a more effective prompt. This behavior is expected, as the prompter models are typically trained to follow input instructions. To ensure the prompter functions as a prompt enhancer, the use of a meta-prompt becomes essential. To address this, we apply a meta-prompt specifically designed to refine the original prompt. Specifically, we use the following meta-prompt."}, {"title": "GPT4 Evaluation \u2013 System Prompt", "content": "To determine the win-rate, we compare the responses generated by Align-Pro with those generated without fine-tuning. For this comparison, we use GPT-4 as the judge. We provide GPT-4 with a system prompt that instructs it to evaluate and compare the responses based on specific attributes. The system prompt we use is as follows:"}, {"title": "Example prompt, prompter responses, and the responses", "content": "In this section, we present three examples from our evaluation on an unseen test dataset, along with the corresponding GPT-4 judge assessments. In our proposed approach, the input prompt is refined by a prompter before being fed into the frozen LLM. The response generated by the frozen LLM using the refined prompt is then compared to the baseline, where the input prompt is directly fed into the frozen LLM without refinement. We provide the judge's scores for each comparison, along with the reasoning behind the evaluation. While the frozen LLM is instruction-tuned, leading to relatively close scores between the baseline and our approach, Align-Pro consistently demonstrates an advantage due to the refined prompts. The prompter's clarifications and guidance help the frozen LLM produce responses that are more helpful and aligned with the input prompt's intent."}]}