{"title": "Self-Taught Evaluators", "authors": ["Tianlu Wang", "Ilia Kulikov", "Olga Golovneva", "Ping Yu", "Weizhe Yuan", "Jane Dwivedi-Yu", "Richard Yuanzhe Pang", "Maryam Fazel-Zarandi", "Jason Weston", "Xian Li"], "abstract": "Model-based evaluation is at the heart of successful model development as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to improve evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) rely on strong evaluators at every stage of the development lifecycle. They are used at training time as reward models to align with human preferences (Bai et al., 2022; Ouyang et al., 2022) or for iterative self-improvement (Yuan et al., 2024), and at inference time as an alternative to human evaluation (Li et al., 2023; Chiang and Lee, 2023; Wang et al., 2023a; Liu et al., 2023). Improvements in evaluation capabilities will thus clearly benefit this entire workflow - including empowering the scientific research process itself as we aim to develop better overall techniques.\nBuilding such strong evaluator models usually relies on large amounts of high-quality preference data from human annotation over model responses,"}, {"title": "Related Work", "content": "LLM-based Evaluators While traditional evaluation benchmarks employ automated metrics that require a reference answer (Wang et al., 2019; Ra-"}, {"title": "Method", "content": "We consider the setting of pairwise evaluation using the LLM-as-a-Judge approach (Zheng et al., 2023) that takes:\n\u2022 an input (user instruction) x; and\n\u2022 two possible assistant responses $y^{(A)}$ and $y^{(B)}$ to the user instruction x; and\n\u2022 the evaluation prompt containing the rubric and asking to evaluate and choose the winning answer, see e.g., Figure 8.\nThe goal of the LLM-as-a-Judge model is to output a preference of which response y is better: A or B. In order to do this it is common to output, prior to the final judgment, a chain-of-thought (or \u201creasoning chain\u201d), which is a set of steps generated in natural language, which helps the model decide its final judgment.\nSuch models can be used as pairwise reward models to build training data for preference optimization, e.g., for training methods like DPO (Rafailov et al., 2023), Iterative DPO (Xu et al.,"}, {"title": "Initialization", "content": "We assume we have access to a pool of user instructions {x}. Each sample $x_i$ can either be one single text instruction or a multi-turn dialog history of turns between the user and the assistant, with the last turn being an instruction or question from the user. Instructions typically involve different skills such as general knowledge and reasoning, coding, safety, and mathematical reasoning."}, {"title": "Instruction Selection", "content": "Given a pool of human-written user instructions, there may be a large degree of noise, as well as an imbalance in terms of topic, variety, difficulty, and ability of the model to answer. We therefore aim to select a subset of instructions to generate high-quality synthetic responses and judgments that can be further used for training.\nWe classify each input using an LLM into a given category, for example coding, reasoning, brainstorming, etc. The precise prompt we use is given in Figure 7. We are then free to select data from within those categories, and to discard certain categories not deemed to be useful for training."}, {"title": "Response Pair Construction", "content": "For each input $x_i$ in our curated training pool, we next generate preference data involving two responses $y^{(w)}$ and $y^{(l)}$ where w is expected to be preferable (winning) over l (losing). We achieve this by generating the data in a synthetic manner without using human annotation.\nGiven the instruction $x_i$, we first prompt an instruction-following LLM to generate a baseline response $y^w$ as usual. We then prompt the LLM to generate a \"noisy\" version of the original instruction $x' = \\phi(x_i)$. We do this using the prompt template given in Figure 2, where we ask to \u201cgenerate a modified instruction that is highly relevant but not semantically identical to the instruction above from the user.\" We then prompt the LLM for a high-quality response $y^l$ to $x_i'$, which would not be a good response for $x_i$. This yields a synthetic preference $y^w > y^l$ for the original input $x_i$.\nThis paired data is then used to construct training examples:\n$(x_i, y^{(A)}, y^{(B)})$,\nwhere we randomize the order of whether the winner is w = A or w = B, which is important to deal with position bias for LLM-as-a-Judge inference."}, {"title": "Judgment Annotation", "content": "Our LLM-as-a-Judge model is used to generate evaluation judgments (reasoning chains and verdicts) $\\{j_i\\}_{i=1}^N$ for each training example $e_i = (x_i, y^{(A)}, y^{(B)})$ in the following manner: for a given input $e_i$, we collect N diverse evaluations $\\mathcal{I} := \\{j_i^1,..., j_i^N\\}$ by sampling from the model. We then apply rejection sampling to filter $\\mathcal{I}$ by removing $j^r$ when the final verdict disagrees with the ground truth labeling, derived from Subsection 3.3. We then select a single correct reasoning chain and verdict at random from the pool of correct solutions. If no such judgment exists ($\\mathcal{I}$ is empty) then we discard the example.\nThis now allows us to construct our final training examples of synthetic preferences for fine-tuning:\n$((x_i, y^{(A)}, y^{(B)}), j_i)$."}, {"title": "Model Fine-tuning (Iterative Training)", "content": "Our Self-Taught Evaluator (LLM-as-a-Judge model) is first initialized with the seed LLM. The model is then trained in an iterative manner. At each iteration, we annotate the training examples with judgments as described in Subsection 3.4 using the current model, giving training examples $\\{(x_i, y^{(A)}, y^{(B)}, j_i)\\}$. These are used to train the next iteration's model by fine-tuning. Note that we initialize from the seed model at each iteration."}, {"title": "Experiments", "content": "4.1 Experimental Setup\nTraining. Our initial model $M_0$ is initialized from Llama3-70B-Instruct. In each iteration $i = 1,... T$, we use model $M_{i-1}$ from the previous iteration to generate synthetic preferences followed by judgments on the training data, and then fine-tune Llama3-70B-Instruct again. We use fairseq2 library (Balioglu, 2023) for instruction finetuning and vLLM (Kwon et al., 2023) for inference. During training the negative log-likelihood loss is only applied to the evaluation part, i.e., $j_i$ of the training example. Training hyperparameters are provided in Table 7. Model selection is done using a combination of pairwise judgment accuracy and position bias computed over the held out set. Sampling parameters used for generations are provided in Table 8.\nInstructions and Responses. We start with a large pool of human-written instructions {xi} from the WildChat dataset (Zhao et al., 2024). To perform prompt selection, we annotate the category of each instruction with the Mixtral 22Bx8 Instruct model, using the template in Figure 7 and select 20,582 examples in the reasoning category, as we expect these to be challenging inputs. For the selected inputs we generate synthetic responses $y_i^w$ and $y_i^l$ using Mixtral 22Bx8 Instruct following Subsection 3.3 and Figure 2."}, {"title": "Other Data Sources", "content": "To understand the effectiveness of the proposed method, we generate synthetic judgments using the same approach but based on the following data sources:\n\u2022 HelpSteer2 (Wang et al., 2024b). We generate evaluations conditioned on the scores of helpfulness, correctness, coherence, complexity and verbosity provided the dataset. We use the aggregated score to derive the ground truth preference for each example using the recommended weighting [0.65, 0.8, 0.45, 0.55, -0.4]1.\n\u2022 GSM8K (Cobbe et al., 2021). We sample from an instruction-following model multiple times to get $y^w$ when the final solution agrees with the ground truth and $y^l$ vise versa.\n\u2022 Coding instructions from WildChat. Similar to the \"reasoning\" prompts we selected from WildChat used in the main experiment, we also experimented with prompts annotated with the \"Coding\" category.\n\u2022 hh_rlhf (Bai et al., 2022). We generate evaluations on the prompts and responses provided"}, {"title": "Evaluation", "content": "We evaluate the accuracy of our Self-Taught Evaluator model on the following benchmarks:\n\u2022 RewardBench (Lambert et al., 2024). We use the standard evaluation protocol provided by the leaderboard.\n\u2022 MT-Bench (Zheng et al., 2023). We report agreement rate with human judgments when examples with ties are excluded.\n\u2022 HelpSteer2 (Wang et al., 2024b). We evaluate on the validation split."}, {"title": "Results", "content": "5.1 RewardBench\nResults on RewardBench are given in Table 1. We find that our Self-Taught Evaluator which is trained iteratively on synthetic data without any annotated preference labels significantly improves over the seed Llama3-70B-Instruct model, matching top-performing reward models trained with labeled data. Our approach improves its results across training iterations, and achieves an overall score of 88.3 on iteration 5, while the seed model it starts from obtains 75.4. Training an LLM-as-a-Judge in a similar manner starting from the same seed using the labeled HelpSteer2 data we only obtain 85.6, hence we obtain superior performance without using human labeled data. Compared to the seed model, we observe improvements using our approach in evaluating instructions in the Chat Hard, Safety and Reasoning categories, while being worse on the easier Chat category \u2013 perhaps because our unlabeled training data focused the model on harder examples.\nImproving results further with majority voting\nAs also shown in Table 1, with 32-sample majority voting, our third iteration of Self-Taught Evaluator model reaches an overall performance of 88.7 on RewardBench, outperforming many other existing reward models.\n5.2 MT-Bench\nWe report results on MT-Bench in Table 2. Unlike RewardBench, the MT-Bench dataset contains tie"}, {"title": "HelpSteer2", "content": "Results on the HelpSteer2 validation set are given in Table 3. We report the average accuracy of two orders and three seeds by swapping the response order in a similar manner, as well as reporting both orders separately (right answer first or second) to test for position bias. We further compute the position-consistent accuracy, treating a judgment as incorrect when a model has different predictions on the two orderings. We use the human labels from the Helpsteer2 dataset and treat the response with higher summed scores as the better response. We find that our Self-Taught Evaluator method improves both average accuracy and position-consistent accuracy compared to the seed Llama-3-70B-Instruct model."}, {"title": "Ablations and Analysis", "content": "Synthetic Data from Other Sources\nIn Table 4, we compare Self-Taught Evaluator models trained on synthetic preferences constructed from different sources. We found data sources focusing on different skills, such as coding, mathematical reasoning, etc. are all effective in turning a strong instruction-following LLM into a strong LLM-as-a-Judge. Intuitively, we find that data sources generally improve the categories in RewardBench that are related to their distribution.\n6.2 Synthetic Bad Response Generation\nIn our experiments we generate synthetic data by first generating a similar instruction, and then a good response for the similar instruction \u2013 with the aim that this will be a bad response for the original instruction. An alternative is to just prompt an LLM to generate a bad response to the original instruction directly. We use the prompt template given in Figure 10 and otherwise conduct training as before on the same set of reasoning-based instructions. This approach obtains a RewardBench overall score of 80.7, which still works \u2013 but is"}, {"title": "Comparison of Synthetic Data with Human Annotated Data", "content": "We conducted the same iterative training using labeled preference data from HelpSteer2 (Wang et al., 2024b), rather than synthetic data. On Reward-Bench, as is shown in Table 5, the improvement from each iteration is smaller and the final model did not outperform iterative training on synthetic preferences. We note that these experiments use data to train an LLM-as-a-Judge. Other results in the literature have used the HelpSteer2 to train classifier-based reward models with slightly better results on RewardBench, e.g., obtaining 88.8 using Llama-3-70B, see Table 1."}, {"title": "Combining Synthetic and Human Labeled Preference Data", "content": "We compare how combining synthetic preference data with human labelled preference data affects model performance. In particular, we combine synthetic preferences generated from reasoning Wild-Chat prompts with the human labeled HelpSteer2 dataset (train split) and report performance in Table 6. We compare to first-iteration models trained on single data source, and select the best checkpoint for joint training using the validation split of HelpSteer2 and holdout set of synthetic preferences (in-distribution), as well as safety and code syn-"}, {"title": "Instruction complexity", "content": "We analyze the length distribution of the curated training set of selected instructions in Figure 3. The dataset has a long-tail distribution of input length, with most of the examples less than 500 tokens. In contrast, the full dataset (i.e., the full data before the instruction selection step of Subsection 3.2) has a cluster of very long instructions, containing content such as long-form coding instructions or transcripts.\nWe further instruct Llama-3-70B-Instruct to infer the complexity (using a score of 1\u20135) and category of each input instruction, as well as the length of the expected output, following the procedure in Yuan et al. (2024). From Figure 4 and Figure 6, we see that the curated dataset has more complex instructions involving logical reasoning/science whereas the full dataset has a greater proportion focused on relationships and entertainment. Finally, in Figure 5 we see that the anticipated length of the response is higher for the full dataset than the curated one, perhaps because of the greater frequency of lengthy, and sometimes repetitive instructions."}, {"title": "Conclusion", "content": "We present a scalable approach to build a strong generalist evaluator to perform model-based evaluation of LLM outputs. Our method constructs synthetic preferences over pairs of responses without using any human annotation. Our Self-Taught evaluator with iterative training over these synthetic preferences greatly boosts the accuracy of a strong seed LLM (Llama3-70B-Instruct) as an evaluator, from 75.4 to 88.7 on RewardBench, a new state-of-the-art for generative LLM-as-a-Judge methods."}, {"title": "Limitations", "content": "Generative LLM-as-a-Judge models usually have longer outputs and thus higher inference cost than reward models that simply output a score, as LLM-as-a-Judge typically first generates a reasoning chain. Further, we have used relatively large LLMs in this work (70B parameters) and made no study of whether our approach works on smaller models. Since we use a seed model to generate first synthetic preferences during our iterative training scheme, one of the assumptions is that the model is capable of generating reasonable evaluations. Thus, our approach is limited by having a capable instruction fine-tuned model which is already reasonably aligned to human (or legal/policy) preferences. Furthermore, we only investigated and reported metrics involving evaluation accuracy improvements, rather than computational requirement concerns. We also only investigated pairwise evaluation, i.e., comparing two responses, whereas it is also possible to use LLM-as-a-Judge models (or any other model) to evaluate the quality of single responses, e.g., giving them a score out of 5 or 10, rather than a pairwise A vs B judgment. We leave evaluating single responses to future work."}]}