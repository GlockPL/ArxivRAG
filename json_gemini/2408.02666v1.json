{"title": "Self-Taught Evaluators", "authors": ["Tianlu Wang", "Ilia Kulikov", "Olga Golovneva", "Ping Yu", "Weizhe Yuan", "Jane Dwivedi-Yu", "Richard Yuanzhe Pang", "Maryam Fazel-Zarandi", "Jason Weston", "Xian Li"], "abstract": "Model-based evaluation is at the heart of successful model development as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to improve evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) rely on strong evaluators at every stage of the development lifecycle. They are used at training time as reward models to align with human preferences (Bai et al., 2022; Ouyang et al., 2022) or for iterative self-improvement (Yuan et al., 2024), and at inference time as an alternative to human evaluation (Li et al., 2023; Chiang and Lee, 2023; Wang et al., 2023a; Liu et al., 2023). Improvements in evaluation capabilities will thus clearly benefit this entire workflow - including empowering the scientific research process itself as we aim to develop better overall techniques.\nBuilding such strong evaluator models usually relies on large amounts of high-quality preference data from human annotation over model responses, which can be costly and time-consuming to collect, as it requires expert annotation for challenging tasks (e.g., coding and mathematics). This dependency on human-generated data poses significant challenges for scaling to new tasks or evaluation criteria. Furthermore, as new models inevitably improve over older ones, these existing annotations will typically become outdated, as the judgments are based on annotations of older, less performant, model responses.\nIn this work, we instead explore an iterative self-training approach which uses no human annotated preferences in the training loop, relying purely on synthetically generated data. Given a seed model, our method first uses prompting to generate contrasting synthetic preference pairs for a given input, such that one response is designed to be inferior to the other. Next, using the model as an LLM-as-a-Judge, we generate reasoning traces and judgments for these pairs, which we can label as correct or not given our synthetic preference pair design. After training on this labeled data we obtain a superior LLM-as-a-Judge, from which we can then iterate the whole process in order for it to self-improve.\nIn our experiments, starting from Llama-3-70B-Instruct, the proposed method improves the accuracy on RewardBench (Lambert et al., 2024) from 75.4 to 88.7 (with majority vote, or 88.3 without). This matches or outperforms the performance of reward models derived from the same Llama-3-70B-Instruct model that uses human annotations, for example using the HelpSteer2 dataset (Wang et al., 2024b) of 10k annotations achieves a performance of 85.6 using the same LLM-as-a-Judge setup."}, {"title": "Related Work", "content": "LLM-based Evaluators While traditional evaluation benchmarks employ automated metrics that require a reference answer (Wang et al., 2019;"}, {"title": "Method", "content": "We consider the setting of pairwise evaluation using the LLM-as-a-Judge approach (Zheng et al., 2023) that takes:\n\u2022 an input (user instruction) x; and\n\u2022 two possible assistant responses y(A) and y(B) to the user instruction x; and\n\u2022 the evaluation prompt containing the rubric and asking to evaluate and choose the winning answer, see e.g., Figure 8.\nThe goal of the LLM-as-a-Judge model is to output a preference of which response y is better: A or B. In order to do this it is common to output, prior to the final judgment, a chain-of-thought (or \"reasoning chain\u201d), which is a set of steps generated in natural language that helps the model decide its final judgment.\nSuch models can be used as pairwise reward models to build training data for preference optimization, e.g., for training methods like DPO (Rafailov et al., 2023), Iterative DPO (Xu et al.,"}, {"title": "Initialization", "content": "We assume we have access to a pool of user instructions {x}. Each sample xi can either be one single text instruction or a multi-turn dialog history of turns between the user and the assistant, with the last turn being an instruction or question from the user. Instructions typically involve different skills such as general knowledge and reasoning, coding, safety, and mathematical reasoning."}, {"title": "Instruction Selection", "content": "Given a pool of human-written user instructions, there may be a large degree of noise, as well as an imbalance in terms of topic, variety, difficulty, and ability of the model to answer. We therefore aim to select a subset of instructions to generate high-quality synthetic responses and judgments that can be further used for training.\nWe classify each input using an LLM into a given category, for example coding, reasoning, brainstorming, etc. The precise prompt we use is given in Figure 7. We are then free to select data from within those categories, and to discard certain categories not deemed to be useful for training."}, {"title": "Response Pair Construction", "content": "For each input xi in our curated training pool, we next generate preference data involving two responses y(w) and y(l) where w is expected to be preferable (winning) over l (losing). We achieve this by generating the data in a synthetic manner without using human annotation.\nGiven the instruction xi, we first prompt an instruction-following LLM to generate a baseline response yw as usual. We then prompt the LLM to generate a \"noisy\" version of the original instruction x' = \u03a6(xi). We do this using the prompt template given in Figure 2, where we ask to \u201cgenerate a modified instruction that is highly relevant but not semantically identical to the instruction above from the user.\" We then prompt the LLM for a high-quality response yl to xi, which would not be a good response for xi. This yields a synthetic preference yw > yl for the original input xi.\nThis paired data is then used to construct training examples:\n(xi, y(A), y(B))\nwhere we randomize the order of whether the winner is w = A or w = B, which is important to deal with position bias for LLM-as-a-Judge inference."}, {"title": "Judgment Annotation", "content": "Our LLM-as-a-Judge model is used to generate evaluation judgments (reasoning chains and verdicts) {ji}Ni=1 for each training example ei =(xi, y(A), y(B)) in the following manner: for a given input ei, we collect N diverse evaluations I :={j1,..., jN} by sampling from the model. We then apply rejection sampling to filter I by removing jr when the final verdict disagrees with the ground truth labeling, derived from Subsection 3.3. We then select a single correct reasoning chain and verdict at random from the pool of correct solutions. If no such judgment exists (I is empty) then we discard the example.\nThis now allows us to construct our final training examples of synthetic preferences for fine-tuning:\n((xi, Yi y(A), y(B)), ji)."}, {"title": "Model Fine-tuning (Iterative Training)", "content": "Our Self-Taught Evaluator (LLM-as-a-Judge model) is first initialized with the seed LLM. The model is then trained in an iterative manner. At each iteration, we annotate the training examples with judgments as described in Subsection 3.4 using the current model, giving training examples {(xi, y(A), y(B), ji)}. These are used to train the next iteration's model by fine-tuning. Note that we initialize from the seed model at each iteration."}, {"title": "Experiments", "content": "Experimental Setup\nTraining. Our initial model M0 is initialized from Llama3-70B-Instruct. In each iteration i = 1,... T, we use model Mi\u22121 from the previous iteration to generate synthetic preferences followed by judgments on the training data, and then fine-tune Llama3-70B-Instruct again. We use fairseq2 library (Balioglu, 2023) for instruction finetuning and vLLM (Kwon et al., 2023) for inference. During training the negative log-likelihood loss is only applied to the evaluation part, i.e., ji of the training example. Training hyperparameters are provided in Table 7. Model selection is done using a combination of pairwise judgment accuracy and position bias computed over the held out set. Sampling parameters used for generations are provided in Table 8.\nInstructions and Responses. We start with a large pool of human-written instructions {xi} from the WildChat dataset (Zhao et al., 2024). To perform prompt selection, we annotate the category of each instruction with the Mixtral 22Bx8 Instruct model, using the template in Figure 7 and select 20,582 examples in the reasoning category, as we expect these to be challenging inputs. For the selected inputs we generate synthetic responses yl and yw using Mixtral 22Bx8 Instruct following Subsection 3.3 and Figure 2."}, {"title": "Results", "content": "RewardBench\nResults on RewardBench are given in Table 1. We find that our Self-Taught Evaluator which is trained iteratively on synthetic data without any annotated preference labels significantly improves over the seed Llama3-70B-Instruct model, matching top-performing reward models trained with labeled data. Our approach improves its results across training iterations, and achieves an overall score of 88.3 on iteration 5, while the seed model it starts from obtains 75.4. Training an LLM-as-a-Judge in a similar manner starting from the same seed using the labeled HelpSteer2 data we only obtain 85.6, hence we obtain superior performance without using human labeled data. Compared to the seed model, we observe improvements using our approach in evaluating instructions in the Chat Hard, Safety and Reasoning categories, while being worse on the easier Chat category \u2013 perhaps because our unlabeled training data focused the model on harder examples.\nImproving results further with majority voting\nAs also shown in Table 1, with 32-sample majority voting, our third iteration of Self-Taught Evaluator model reaches an overall performance of 88.7 on RewardBench, outperforming many other existing reward models.\nMT-Bench\nWe report results on MT-Bench in Table 2. Unlike RewardBench, the MT-Bench dataset contains tie"}, {"title": "Ablations and Analysis", "content": "Synthetic Data from Other Sources\nIn Table 4, we compare Self-Taught Evaluator models trained on synthetic preferences constructed from different sources. We found data sources focusing on different skills, such as coding, mathematical reasoning, etc. are all effective in turning a strong instruction-following LLM into a strong LLM-as-a-Judge. Intuitively, we find that data sources generally improve the categories in RewardBench that are related to their distribution.\nSynthetic Bad Response Generation\nIn our experiments we generate synthetic data by first generating a similar instruction, and then a good response for the similar instruction \u2013 with the aim that this will be a bad response for the original instruction. An alternative is to just prompt an LLM to generate a bad response to the original instruction directly. We use the prompt template given in Figure 10 and otherwise conduct training as before on the same set of reasoning-based instructions. This approach obtains a RewardBench overall score of 80.7, which still works \u2013 but is"}, {"title": "Instruction complexity", "content": "We analyze the length distribution of the curated training set of selected instructions in Figure 3. The dataset has a long-tail distribution of input length, with most of the examples less than 500 tokens. In contrast, the full dataset (i.e., the full data before the instruction selection step of Subsection 3.2) has a cluster of very long instructions, containing content such as long-form coding instructions or transcripts.\nWe further instruct Llama-3-70B-Instruct to infer the complexity (using a score of 1\u20135) and category of each input instruction, as well as the length of the expected output, following the procedure in Yuan et al. (2024). From Figure 4 and Figure 6, we see that the curated dataset has more complex instructions involving logical reasoning/science whereas the full dataset has a greater proportion focused on relationships and entertainment. Finally, in Figure 5 we see that the anticipated length of the response is higher for the full dataset than the curated one, perhaps because of the greater frequency of lengthy, and sometimes repetitive instructions."}, {"title": "Conclusion", "content": "We present a scalable approach to build a strong generalist evaluator to perform model-based evaluation of LLM outputs. Our method constructs synthetic preferences over pairs of responses without using any human annotation. Our Self-Taught evaluator with iterative training over these synthetic preferences greatly boosts the accuracy of a strong seed LLM (Llama3-70B-Instruct) as an evaluator, from 75.4 to 88.7 on RewardBench, a new state-of-the-art for generative LLM-as-a-Judge methods."}, {"title": "Limitations", "content": "Generative LLM-as-a-Judge models usually have longer outputs and thus higher inference cost than reward models that simply output a score, as LLM-as-a-Judge typically first generates a reasoning chain. Further, we have used relatively large LLMs in this work (70B parameters) and made no study of whether our approach works on smaller models. Since we use a seed model to generate first synthetic preferences during our iterative training scheme, one of the assumptions is that the model is capable of generating reasonable evaluations. Thus, our approach is limited by having a capable instruction fine-tuned model which is already reasonably aligned to human (or legal/policy) preferences. Furthermore, we only investigated and reported metrics involving evaluation accuracy improvements, rather than computational requirement concerns. We also only investigated pairwise evaluation, i.e., comparing two responses, whereas it is also possible to use LLM-as-a-Judge models (or any other model) to evaluate the quality of single responses, e.g., giving them a score out of 5 or 10, rather than a pairwise A vs B judgment. We leave evaluating single responses to future work."}]}