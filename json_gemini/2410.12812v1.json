{"title": "Optimizing and Evaluating Enterprise Retrieval-Augmented Generation (RAG): A Content Design Perspective", "authors": ["Sarah Packowski", "Inge Halilovic", "Jenifer Schlotfeldt", "Trish Smith"], "abstract": "Retrieval-augmented generation (RAG) is a popular technique for using large language models (LLMs) to build customer-support, question-answering solutions. In this paper, we share our team's practical experience building and maintaining enterprise-scale RAG solutions that answer users' questions about our software based on product documentation. Our experience has not always matched the most common patterns in the RAG literature. This paper focuses on solution strategies that are modular and model-agnostic. For example, our experience over the past few years - using different search methods and LLMs, and many knowledge base collections - has been that simple changes to the way we create knowledge base content can have a huge impact on our RAG solutions' success. In this paper, we also discuss how we monitor and evaluate results. Common RAG benchmark evaluation techniques have not been useful for evaluating responses to novel user questions, so we have found a flexible, \"human in the lead\" approach is required.", "sections": [{"title": "1 INTRODUCTION", "content": "Retrieval-augmented generation (RAG) is an effective way to use large language models (LLMs) to answer questions while avoiding hallucinations and factual inaccuracy [12, 20, 46]. Basic RAG is simple: 1) search a knowledge base for relevant content; 2) compose a prompt grounded in the retrieved content; and 3) prompt an LLM to generate output. For the retrieval step, one approach dominates the literature: 1) segment content text into chunks; 2) index vectorized chunks for search in a vector database; and 3) when generating answers, ground prompts in a subset of retrieved chunks[13]. Our RAG solutions don't always use vector databases for search.\nWikipedia has long been influenced by and had an influence on scientific research [21, 41]. With respect to RAG, Wikipedia is a dominant source of knowledge base content for training data and benchmarks, including: 2WikiMultiHopQA, AmbigQA, ASQA, DART, FEVER, HotpotQA, KILT, MuSiQue, Natural Questions, NoMIRACL, PopQA, SQUAD, StrategyQA, SuperGLUE, TriviaQA, WikiAsp, WikiBio, WikiEval, and Wizard of Wikipedia [8, 9, 14-16, 18, 22, 23, 25, 28, 29, 31, 34, 39, 40, 42-44, 48]. The knowledge base for our team's RAG solutions is our own product documentation, which is structured differently from Wikipedia articles.\nUsing common benchmarks to test your RAG implementation involves these steps: 1) index the given knowledge base content in your retriever component; 2) prompt your solution to answer the given questions; and 3) compare generated answers to expected answers, using methods such as exact match, cosine similarity, BLEU, ROUGE, METEOR, BertScore, or using LLMs as judges[49]. Those evaluation metrics have not been useful for evaluating our RAG results for novel questions from real users.\nIn this paper, we share our experience building enterprise-scale RAG solutions, with a focus on three aspects:\n\u2022 RAG implementation - Our solutions are modular. Our retriever and generative components are closed boxes, accessed through APIs, with a limited ability to fine-tune.\n\u2022 Knowledge base content \u2013 We are able to improve results by optimizing the knowledge base content itself. We developed content strategy and writing guidelines for RAG.\n\u2022 Evaluating results - We test our RAG solutions with real user questions before making the solutions available to external users. We evaluate run-time answers after the solutions are launched."}, {"title": "2 RELATED WORK", "content": "Many techniques have been proposed for improving upon the basic RAG method. Advanced RAG techniques include: augment knowledge base content (with metadata or knowledge graphs, for example); fine-tune the embedding model, the retriever, or the generative model (or all of them); rewrite or expand the query; use multiple knowledge bases (diverse in their format and content) and then route queries; evaluate, re-rank, filter, and post-process retrieved chunks; generate multiple outputs to choose the best one; or iteratively refine results[4, 7, 11, 17, 26, 36-38, 45, 47, 50]. Little attention has been paid to optimizing the knowledge base content itself.\nRAG solution builders must consider the structure of their knowledge base content when converting that content to text before indexing it for search. Much work has been done exploring the best way to read PDF documents, capture meaning from HTML or Markdown elements, interpret images, and reflect relational information in tables and lists [2, 3, 24, 30].\nThe structure of knowledge base content must also be considered when segmenting the content into chunks. Chunking too small risks splitting information across multiple chunks. Chunking too large risks including irrelevant information in a given chunk. Choosing a chunk size depends on multiple factors, including the profile of the knowledge base content[1, 3]. One way to include a complete, self-contained idea or explanation in each chunk is to chunk content not based on size, but at the chapter or section level[5].\nThe dominant search method in early RAG literature has been vector embeddings. Now, combining multiple search strategies (including traditional ones\u00b2) is increasing[19, 32].\nWhen evaluating results from a deployed RAG solution, multiple authors have acknowledged manual work is required[10, 33, 49]. To assist with human evaluation, [27] and [6] propose identifying facts in questions, retrieved knowledge base content, and generated answers to confirm facts agree. ARES[35] and RAGAS[9] validate question-context relevance, context-answer faithfulness, and question-answer relevance. When evaluating their RAG solutions, [1] identify seven points of failure: 1) missing content; 2) search failure; 3) context window limitations; 4) poor answer generated by the LLM; 5) incorrect output format; 6) vague answers; and 7) incomplete answers. We can confirm seeing our solutions encounter those same failure points too."}, {"title": "3 RAG IMPLEMENTATION", "content": "User interface\nFig. 1 shows the interface of a simple RAG solution deployed on the search page of product documentation. A discussion of key user experience design aspects follows.\nSearch enhancement - When a search query is in the form of a natural language question, the usual search results are returned and a brief answer is generated by an LLM. Readers are accustomed to using the search bar to look for information, so there is no new interface to discover and learn how to use.\nNot a chatbot - Previous dialog turns are not included in the LLM prompt for context. We chose to deploy a simple solution to get experience and feedback quickly. Also, our team is interested to explore non-chatbot LLM interfaces.\nShaping user behavior \u2013 The search bar is a single line input, so it is awkward to type a complex question there. This friction nudges users to keep their questions concise. Fewer than 6% of questions submitted to the solution are much longer than the search input or require multi-hop reasoning. We plan to study the impact of the restricted input on that behavior.\nTransparency and explainability - Links to content in which an answer is grounded are always provided. Also, terms that significantly impacted the way the solution generated the answer are highlighted in bold. When we review solution logs, we see the highlighting helps users know which terms to change (or remove) in their question to get a different answer.\nGrowth of natural language questions \u2013 Over time, the percent of queries submitted in the search bar that are expressed as a natural language question (versus a keyword search) has increased from 25% to 39%. (Fig. 2)\nUser feedback \u2013 We worried users wouldn't give feedback if it required multiple clicks, choosing from difficult-to-interpret categories, or typing explanations. But we also worried simple \"thumbs-up\" or \"thumbs down\" feedback wouldn't be fine-grained enough to analyze the impact of iterative solution improvements. So, we chose a 1-click interface with three options: \"helpful\", \"somewhat helpful\", and \"unhelpful\". We found that users give feedback less than 1% of the time, and mostly for unhelpful answers. (Fig. 3)"}, {"title": "Solution architecture", "content": "Fig. 4 shows components of the RAG solution mentioned above. The knowledge base is product documentation made up of \"topics\", using the Darwin Information Typing Architecture (DITA) paradigm. A discussion of key aspects follows.\n(A) Pre-processing user input - Malicious input, such as JavaScript injection and adversarial prompts, is rejected; personal information removed; bias as well as hate, abuse, and profanity (HAP) paraphrased. Input is translated to English and classified to determine if it is a question and to detect the question type, such as \"what-is\", \"how-to\", or \"troubleshooting\". Only unharmful questions move on through the solution - in English.\n(B) Frequently asked question (FAQs) - If a user's question matches a sensitive FAQ - related to legal terms, for example - we return a hard-coded answer. For other FAQs, we curate previously generated answers evaluated as useful. Before returning a curated answer, we confirm the grounding topics have not been updated, because that could change the answer. If the topics have been updated or deleted, the question is handled like a novel question.\n(C) Augmenting the question - If the user's question does not match an FAQ, the question is further processed to improve search performance: ambiguous questions rewritten; jargon replaced with in-domain terms; synonyms added.\n(D) Search as a closed box \u2013 We call a search API that returns a ranked list of topics relevant to our query. Some search results might be re-ranked or filtered by our RAG solution. A separate team manages the search API we use. They maintain the API and automatically index our documentation continuously.\n(E) Whole topics instead of chunks - Once we have a list of relevant topics, we extract the complete text of those topics to ground our prompt. Sometimes called \u201csmall2big\" or \"parent document retrieval\", this strategy works well for us because our topics are optimized for RAG. They are short, complete, accurate, and up-to-date. Our text-extraction component takes advantage of the reliable structure of our topics. For example, we have writing guidelines requiring tables to be fairly simple. Knowing this, we convert tables to row-wise and column-wise lists of lists to retain row-column relationships without worrying about complex tables.\n(F) Simple prompts \u2013 Our solution isn't a dialog, so we don't need to maintain chat history. Because the user interface encourages simple questions, because we clean, clarify, and augment questions, because topics are optimized for RAG, and because we want concise answers that are faithful to the topics, the LLM has only one job: rewrite content from the grounding topics in a succinct answer. We prompt multiple models and choose the best answer.\n(G) Post-processing output - Generated output is processed like the input: personal information, bias, and HAP generated by the LLM is removed or paraphrased. The answer is translated to the language of the original user question, links to grounding topics are added, and the whole output is marked up in HTML.\n(H) Logging - Because LLMs can generate problematic output, our team monitors solution activity in real time by sending detailed logging to a team Slack5 channel. We also send details to an evaluation tool discussed later in this paper."}, {"title": "4 KNOWLEDGE BASE CONTENT", "content": "Our team uses search and LLM APIs that we have limited ability to adjust. What we can control is our knowledge base content. Our content must be easy to search, navigate, and consume by all readers, including people not working in their first language and people using tools like screen readers. Now, we also want our content to work well for RAG solutions.\nContent rewriting experiment\nWe built a simple RAG solution to answer 189 questions about the Earth from the Natural Questions benchmark[22]. Initially, our solution did not answer all questions correctly.\nOne question we failed to answer is: \"what is the pre-industrial level of co2 on earth?\" The correct answer is: \"280 ppm.\" But our solution responded with: \"180 ppm.\"\nText from the relevant article used to answer that question follows. The underlined text is the only edit needed for the RAG solution to answer correctly:\nOver the past 400,000 years, CO2 concentrations have shown several cycles of variation from about 180 parts per million during the deep glaciations of the Holocene and Pleistocene to 280 parts per million during the interglacial periods until the pre-industrial era.\nMinor edits like this increased success to 100%. Complete code and edits are available on GitHub.a\nFor many RAG projects that use legacy knowledge base content, rewriting that content isn't feasible. However, for a RAG solution that is to be built 6 months from now or a year from now, the knowledge base content might not yet exist. Some estimate that more than 250,000 websites are created every day. On Wikipedia, more than 400 articles are being added every day. For our teams, products that will be released next year don't have any documentation yet. When creating new content, it makes sense to optimize it for RAG solutions.\nContent strategy for RAG\nTesting RAG solutions before making them available to users might seem difficult due to a lack of test questions that reflect what real users will ask[1]. However, when creating documentation for a new product or feature, content designers have always researched what questions users are likely to ask:\n\u2022 We run internal workshops with teammates and observe where participants get stuck and what questions they ask.\n\u2022 We read internal communities where teammates ask questions as they use internal releases of upcoming features.\n\u2022 We review external forums where users are asking questions about similar functionality in other products.\n\u2022 When features are in early, limited release, we collaborate with sales, pre-sales technical support, and customer advocates to find out what questions early users have.\nThese are all ways to collect questions that better represent what real users will ask than any questions we might guess ourselves. Optimizing content to be used in RAG solutions requires paying more attention to what questions that content must answer. A new quality metric will be: How well does a given topic answer anticipated user questions?\nTesting topics\nImagine you have a list of real user questions about credentials and you have a draft topic about credentials. How could you verify the topic answers those questions?\nYou could prompt an LLM to answer the user questions grounded in the draft topic, then verify the answers. Or you could prompt an LLM to generate questions answered by the draft topic, then automatically compare the generated questions to the real questions. Hypothetical draft topic:\nCredentials are the user ID and password for authenticating with the service. Credentials are important, they prevent others using your service instance.\nHypothetical generated questions:\n\u2022 What are credentials?\n\u2022 Why are credentials important?\n\u2022 What do credentials prevent?\nThe topic seems helpful and those seem like questions people might ask. But what if real user questions are: \"Where do I find my credentials?\", \"How do I get my credentials?\", and \"Where can I look up my credentials?\" The generated questions don't match those, which means a RAG solution using that topic won't work for real users.\nContent guidelines for RAG\nAs we monitored our RAG solution results and then experimented with rewriting content, we identified patterns that led to better results. We asked writers from other teams to test these patterns with their content too. From this testing, we created guidelines to help writers optimize content for RAG:\n\u2022 Simplify complex tables \u2013 Tables that have spanned cells or lack column headings are difficult for LLMs to interpret.\n\u2022 Explain graphics in text - Explaining graphics clarifies ambiguities and avoids the need for an image-to-text model.\n\u2022 Add summaries to tutorials or long procedures LLMS struggle with long tutorials or procedures (because of getting \"lost in the middle\" or context window limitations.) Adding a summary is an easy way to improve results.\n\u2022 Clearly introduce lists \u2013 LLMs can better use content in lists when there is a clear lead-in sentence before the list.\n\u2022 Simplify nested content \u2013 Meaning can be lost by the LLM when content has multiple levels of nesting (steps with sub-steps that have option lists, for example.) Avoiding multi-level nesting improves results."}, {"title": "5 EVALUATING RAG RESULTS", "content": "We created a web app that streamlines the task of manually reviewing and evaluating answers our RAG solutions return to users. Fig. 5 shows the evaluation page of the app. On the left is the user's question as well as some metadata, such as the language in which the question was submitted and the question classification. In the middle is the answer that was returned to the user, complete with generated answer text and links to relevant topics. On the right is a list of criteria:\n\u2022 Valid question - Should we be able to answer this question?\n\u2022 Correct class - Was the question classified correctly?\n\u2022 Article exists - Is there a topic to answer the question?\n\u2022 Search success - Did search find the relevant topics?\n\u2022 Good answer - Is the answer accurate, complete, helpful?\nAs we evaluate results, annotate key terms, and tag results, we end up naturally creating fine-tuning data sets, custom NLP dictionaries, and training data for classifiers. Our evaluation tool takes a \"human in the lead\" approach: AI learns from the data our manual work naturally creates so it can automatically perform some evaluations, entity identification, and classification.\nFig. 6 shows evaluation results for a RAG solution at two points in time. In July, for 40% of valid questions there were no topics containing information to answer the question. So, we recruited writers to fill that content gap. By December, there were topics to answer valid questions 75% of the time - an improvement. Unfortunately, search performance declined. In December, search didn't find the relevant topic 47% of the time. We were able to collect sample answers evaluated as \"Article exists\" == \"Yes\" and \"Search success\" == \"Fail\" so we could identify and fix the cause of the search failures. The RAG evaluation tool helps us know where to focus our improvement efforts.\nUnanticipated benefits\nMethodical evaluation of results has increased the business value of our RAG solutions:\n\u2022 Training data - Sample questions, fine-tuning data sets, custom NLP dictionaries, and training data for classifiers naturally created as we manually evaluate, annotate, and tag results can sometimes be used to improve other Al solutions, such as analysing customer feedback surveys or community questions (subject to terms of use and reuse.)\n\u2022 Insights - The RAG evaluation tool sends a weekly summary of user questions to a team Slack channel so everyone knows what our users are struggling with and asking about.\n\u2022 Documentation improvements - Our team meets for 30 minutes a week to review results and fix problems, including: content gaps, search failures, and content that needs editing."}, {"title": "6 SCALING AN ENTERPRISE SOLUTION", "content": "When building a RAG solution to support a portfolio of dozens or hundreds of software products, new challenges arise:\nQuestions vary by product - While common questions for one product might be factual \u201cwhat-is\u201d questions, common questions for another might be command-line syntax questions. A given question rewriting method might work for one but not the other.\nContent varies by product - The documentation for one product might be conceptual or task-based, while another product's documentation might be mostly API reference details. Search that works well for one might not work well for the other.\nGetting buy-in - When one product team decides to build a RAG solution, they feel invested and prepared to do manual work like evaluating results. But getting buy-in for an enterprise-wide initiative can be challenging.\nOne size might not fit all - Questions and content are not the only things that will vary across teams. Building one solution for everyone maximizes shared infrastructure. Ensuring that solution is configurable and flexible empowers individual teams to benefit from the centralized infrastructure while also doing what works best for them.\nAutomated regression testing - As teams rewrite their content and update components of their RAG solution, they need a way to test the performance of their solution without having to manually evaluate those test results. Solutions like the RAG evaluation tool can be used to collect question-topic-answer triplets that can be tested in automated batches. (Evaluation techniques like BLEU, ROUGE, and so on, do have a useful place here.)"}]}