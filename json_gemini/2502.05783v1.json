{"title": "WatchGuardian: Enabling User-Defined Personalized Just-in-Time Intervention on Smartwatch", "authors": ["YING LEI", "YANCHENG CAO", "WILL KE WANG", "YUANZHE DONG", "CHANGCHANG YIN", "WEIDAN CAO", "PING ZHANG", "JINGZHE YANG", "BINGSHENG YAO", "YIFAN PENG", "CHUNHUA WENG", "RANDY AUERBACH", "LENA MAMYKINA", "DAKUO WANG", "YUNTAO WANG", "XUHAI XU"], "abstract": "While just-in-time interventions (JITIs) have effectively targeted common health behaviors, individuals often have unique needs to intervene in personal undesirable actions that can negatively affect physical, mental, and social well-being. We present WatchGuardian, a smartwatch-based JITI system that empowers users to define custom interventions for these personal actions with a small number of samples. For the model to detect new actions based on limited new data samples, we developed a few-shot learning pipeline that finetuned a pre-trained inertial measurement unit (IMU) model on public hand-gesture datasets. We then designed a data augmentation and synthesis process to train additional classification layers for customization. Our offline evaluation with 26 participants showed that with three, five, and ten examples, our approach achieved an average accuracy of 76.8%, 84.7%, and 87.7%, and an F1 score of 74.8%, 84.2%, and 87.2% We then conducted a four-hour intervention study to compare WatchGuardian against a rule-based intervention. Our results demonstrated that our system led to a significant reduction by 64.0\u00b122.6% in undesirable actions, substantially outperforming the baseline by 29.0%. Our findings underscore the effectiveness of a customizable, AI-driven JITI system for individuals in need of behavioral intervention in personal undesirable actions. We envision that our work can inspire broader applications of user-defined personalized intervention with advanced AI solutions.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in mobile sensing technologies and artificial intelligence (AI) have led to the emergence of research on intelligent, just-in-time interventions (JITIs) using mobile or wearable devices [3, 57, 58, 86, 91, 95, 128]. A typical research paradigm usually starts by identifying a target undesirable behavior, followed by data collection from mobile and/or wearable devices, machine learning (ML) model development, and finally, real-time system evaluation (e.g., [3, 26, 46, 53, 79]). When deployed, these systems will detect the occurrence of target behaviors and deliver JITIs to help users regulate their behaviors and achieve personal health goals. In the past decade, researchers have achieved a wide range of successful JITI applications, such as reducing smartphone overuse [61, 123], prevention of sedentary habits [58], smoking cessation [3], promoting skin health [53, 91], and managing stress and emotions [46, 48].\nExisting research predominantly focuses on common health behaviors that are generally applicable to a large group of populations. However, some individuals' undesirable behaviors can be highly personal and idiosyncratic. This is especially the case for personal micro-actions or micro-habits. Example actions include leg-shaking, nail-biting, hair-pulling, and skin-picking (some referred to as body-focused repetitive behaviors, BFRBs) [80, 101\u2013103, 108]. Such micro-actions can have negative impacts on ones' health (e.g., lip-picking can cause cheilitis symptoms [10, 24]), or unfavorable social implications (e.g., leg-shaking is considered rude and disrespectful in some cultures [126]). Such actions vary considerably across individuals [121], shaped by diverse physical,"}, {"title": "2 RELATED WORK", "content": "In this section, we first provide a general overview of just-in-time behavior intervention, and then a review of prior work in hand gesture recognition based on wearable devices."}, {"title": "2.1 Sensing-based Just-in-Time Intervention (JITI)", "content": "Advances in mobile sensing technologies enable the unobtrusive real-time monitoring of individual states and environmental contexts, while delivering proactive cues and user-specific information [15]. Such advances facilitate the implementation of just-in-time intervention (JITI) [74], with the goal of delivering timely and appropriate support for users. Early research has applied JITI to address a variety of health-related issues using rule-based approaches [15, 25, 30, 31, 43, 44, 62, 87, 106]. These approaches usually depended on predefined sets of rules and conditions to trigger interventions, which are typically defined by domain experts. Examples include event-based rules [43, 44, 87], time-based rules [25, 62, 106], combinations of multiple rules [30], multi-stage rules [15], to name a few.\nRecently, with the advancement of AI techniques, an increasing number of studies have started to apply AI for JITI [3, 46, 53, 57, 58, 79, 91, 95, 118, 123]. In contrast to rule-based JITI, AI-based approaches utilize large-scale user behavior data and trained AI/ML models to determine optimal intervention timing and personalized interventions. For instance, Time2Stop [79] employs machine learning to develop an adaptive, explainable intervention system for smartphone overuse that determines optimal timings, offers transparent AI explanations, and integrates user feedback to improve the model over time. Rabbi et al. [86] and Liao et al. [58] incorporated reinforcement learning algorithms into JITI systems to personalize the model for each user, enhancing the effectiveness of physical activity interventions.\nHowever, these studies primarily focused on predefined \u201ccommon\u201d health behaviors that are broadly applicable to large populations, failing to address personal/idiosyncratic undesirable behaviors that are specific and unique to individual users [80, 101-103, 108]. Idiosyncratic behaviors naturally mean that the sample size (from a single individual) would be much more limited than other common health behaviors, posing challenges to training Al models for intelligent intervention systems. The limitation of existing solutions reduces the intervention systems' ability to adapt to personal behaviors. To bridge this gap, our work proposes a personalized intervention approach to deliver customized JITI for user-defined undesirable actions."}, {"title": "2.2 Wearables for Hand Gesture Recognition and Customization", "content": "The field of hand gesture recognition or activity recognition using wearable technologies has been extensively studied, utilizing a range of sensing techniques (e.g., vision [23, 32, 42, 77, 85, 117, 119, 125], sound wave [27, 37, 38, 50, 52, 56, 75], electromyography [11, 67, 93, 94], pressure or stretch [18, 19, 40, 99, 105], magnetism [9, 13, 14, 41, 82, 100, 124]). Among them, motion data (e.g., acceleration, angular velocity) collected by IMUs are particularly notable for their effectiveness in capturing dynamic hand gestures [1, 45, 51, 55, 96, 115, 120]. Coupled with their cost-effectiveness and widespread availability in commercial wearable devices, IMUs' are the best choice of sensor for a JITI system to ensure effectiveness, ubiquity and generalizability.\nExisting gesture recognition approaches can be broadly categorized into trajectory-based and ML-based. Early trajectory-based methods [59, 65] achieve high accuracy with relatively few samples, but they are limited in recognizing more complex gesture trajectories [65]. For more sophisticated and fine-grained gestures, ML-based methods are more suitable, but are typically heavily data-driven, requiring a large number of samples of a pre-defined gesture set to train either a traditional model [12, 22, 33, 37] or a deep learning model [32, 54, 55, 97, 125] depending on the dataset size.\nIn addition to recognizing gestures from predefined sets, some systems support the customization of user-defined gestures, which enhances memorability [72], interaction efficiency [81], and accessibility for individuals with physical disabilities [4]. Most notably, incorporating gesture customization in our system enables users to define their own undesirable gestures for intervention. However, to ensure a seamless user experience, the data collection process for new gestures must be efficient and limited in scale (e.g., no more than 10 samples). Existing approaches (e.g., rule-based methods [6, 21] and computational techniques [5, 60, 65, 81]) meet this"}, {"title": "3 WATCHGUARDIAN DESIGN", "content": "We designed a few-shot learning pipeline to enable users to define their own undesirable actions with a small number of examples. We introduce our technical pipeline (Sec. 3.1), the interface and intervention experience design (Sec. 3.2), as well as the implementation details of WatchGuardian (Sec. 3.3)."}, {"title": "3.1 Few-shot Learning Pipeline", "content": "To achieve the goal of learning user-defined undesirable actions with few-shot samples, we designed and implemented a three-stage pipeline building on public models and datasets. Figure 2 visualizes the overall structure of the pipeline. To ensure compatibility with existing public models and datasets, we used tri-axial accelerometer data from the IMU, sampled at 30 Hz."}, {"title": "3.1.1 Stage 1: Pre-trained SSL Model", "content": "One of the primary challenges in deep learning for human activity recogni- tion (HAR) is the lack of large labeled datasets [54, 127]. Although there exist multiple public human activity recognition (HAR) datasets with ground truth labels, they often have limited size and different sensing modalities, data sizes, task definitions, and collection protocols (e.g., [2, 12, 17, 29, 33\u201335, 47, 49, 63, 64, 78, 90, 112]). Therefore, it is challenging to unify these datasets into a single large-scale dataset for pre-training a supervised-learning- based model. SSL addresses these challenges by leveraging vast amounts of unlabeled data to learn meaningful representations through pretext tasks, making it particularly well-suited for HAR tasks [92].\nWe adopted a pre-trained model developed by Yuan et al. [127], which was trained using multi-task self- supervised learning on the UK Biobank activity tracker dataset [20]. The dataset contains over 700,000 person- days of unlabeled wearable sensor data collected from free-living activities via a wrist-worn accelerometer. This dataset setup fits well into our target application scenarios. Figure 2(A) provides a high-level overview of the architecture of the pre-trained model, which includes a five-layer ResNet-based feature extractor [28] followed by two fully connected layers. The model was pre-trained on three fixed-length signals (i.e., 5 sec, 10 sec, 30 sec) at 30 Hz. Three data augmentation techniques [111], including Arrow of time (AoT), Permutation, and Time warping (TW), were used in pre-training process as three self-supervised tasks [92]. This pre-trained model is publicly available\u00b9."}, {"title": "3.1.2 Stage 2: Model Finetuning", "content": "While direct feature extraction from IMU signals using the pre-trained model has demonstrated improved performance in downstream classification tasks [127], there is a significant gap"}, {"title": "3.1.3 Stage 3: Few-Shot Model Customization", "content": "As mentioned earlier, in real-world applications, it is often impractical for users to provide a large number of samples of a self-defined action for model training. To achieve individual customization, our few-shot learning procedure was designed to train a new prediction head with lightweight layers built upon the finetuned model from Sec. 3.1.2. For easy understanding, we will explain our pipeline with the case of adding one intervention action (i.e., binary classification) in detail, starting with data collection and then the few-shot learning process. The scenarios with multiple actions adopt the same method.\nWe implemented a simple, user-friendly data collection process for customization, where a user would follow instructions on the wearable device to repeat the target action several times (N shots, 10 seconds each time), with a short period of 5-second pause or other activities (negative class) between the two repetitions, as shown in Figure 2(C). Given such a signal sequence, we applied the same sliding window process as in Sec. 3.1.2 (5-second width and 0.1-second step). Each window was labeled as positive if it contained more than 3 seconds of the target action; otherwise, it was annotated as negative.\nWe then introduced a signal processing procedure, including both data augmentation and data synthesis, to enhance our data for model training. First, we adopted six data augmentation techniques [36]: 1) zooming, to simulate variations in action speed, randomly selected from \u00d70.9 to \u00d71; 2) scaling, to represent variations in action intensity, with the scaling factor $s \\sim \\mathcal{N}(1, 0.2^2)$, $s \\in [0, 2]$; 3) time warping, to simulate action temporal variance, using 2 interpolation knots and a warping randomness $w \\sim \\mathcal{N}(1, 0.05^2)$, $w \\in [0, 2]$; 4) time reversal, to simulate temporal variation, by reversing the action's time sequence; 5) time-domain noise, to simulate sensor inaccuracies or environmental disturbances, Gaussian noise with a noise level of 0.01 is added to the original data; and 6) frequency-domain noise, to simulate frequency variation or external interferences. We add the random noise to the frequency components of the signal (after Fourier transform) and then transformed the signal back to the time domain (with inverse Fourier transform). We went through all combinations of these six augmentation technique steps, which increased the size of the data by $2^6 - 1$ times.\nNext, we designed a data synthesis step. To create additional samples that simulate short episodes of target undesirable actions, we artificially concatenated short segments of the target undesirable action with negative episodes (no target undesirable actions). The positive segments (of the target undesirable actions) were chosen randomly from the 10-second continuous recordings and would have varying lengths sampled from [3, 4.9] seconds. The starting position of the positive episode was also randomized within a 5-second window, with negative episodes padding at the beginning and the end. This step further increased the sample size by about 80 times. In total, our data augmentation and data synthesis steps enlarged the original few-shot samples by about 143 times.\nFinally, to customize the model to recognize a new target undesirable action, we further trained the finetuned model with a new classification head with the total set of data. The head was trained for a binary classification when adding one intervention action, and N+1-class classification when adding N new actions.\nIn the real-time system, the final classification model followed the same sliding window setup and performed classification at 10 Hz (0.1-second step). To improve the system robustness, we added a smoothing step with the threshold as 3 based on grid search, i.e., the system will only recognize a target action if there is a consecutive sequence of positive outcomes from 3 windows."}, {"title": "3.2 Intervention Design", "content": "Building upon the customized model, we then developed a real-time intervention system on the smartwatch. As introduced in Sec. 3.1.3, in the initial customization process, a user would go through a simple data recording"}, {"title": "3.3 System Implementation", "content": "We adopted a client-server architecture for the system implementation to enable efficient data transmission and processing. The interface was implemented on the Google Pixel Watch 2, which acted as the client device. It continuously streamed the accelerometer data, collected at 30 Hz, to a dedicated server in real time via a socket communication protocol.\nBefore real-time data transmission, the server had already completed the initial setup stages, namely Stage 1 and Stage 2, as described in Sec. 3.1. Therefore, once the customization data from the client was collected, we utilized an A100 GPU to perform the few-shot custom model training, enabling rapid adaptation to new data with minimal samples.\nAfter training the model on the server, we deployed the final model for real-time inference. The inference process ran on the server, and the results were transmitted back to the client for immediate feedback, enabling efficient and responsive action recognition and the delivery of JITI."}, {"title": "4 MODEL EVALUATION", "content": "In this section, we report the evaluation of WatchGuardian's few-shot learning pipeline offline performance. We will further elaborate on the evaluation of WatchGuardian's intervention effectiveness in section 5."}, {"title": "4.1 Data Collection", "content": ""}, {"title": "4.1.1 Participants", "content": "We recruited 26 users (14 females, 12 males, age 22\u00b12) for data collection via social media platforms. We focused on users who were aware of their own undesirable actions and had the intention to reduce these actions. These are the target users of our intervention system. Our study was IRB-approved by the local institution, and participants were compensated with $10 for this data collection study (around 45 minutes)."}, {"title": "4.1.2 Personal Undesirable Action Customization", "content": "Participants were asked to record five pre-determined target actions that are commonly recognized as undesirable actions [80, 108], including Face Scratching, Nail Biting, Eye Rubbing, Lip Tearing, and Leg Shaking. The first five figures in Figure 4 illustrate these actions.\nMoreover, each participant was asked to define a new undesirable action tailored to their own personal needs. In total, 26 participants designed an additional set of 12 actions, including Finger Lipping (designed by N=5 participants), Head Scratching (N=5), Nose Rubbing (N=4), Finger Picking (N=3), Hair Scratching (N=2), Face Rubbing (N=1), Finger Biting (N=1), Hair Pulling (N=1), Hair Rubbing (N=1), Lip Biting (N=1), Nail Picking (N=1), and Neck Scar Scratching (N=1). We only grouped identical actions and distinguished actions as long as they differed slightly. For instance, Head Scratching and Hair Scratching were similar, but one involved contacts between fingers and scalp, while the other one did not. Similarly, Finger Picking and Nail Picking were also quite close, yet one solely focused on the skin on the finger, while the other focused on nails. These actions were visualized in the second half of Figure 4."}, {"title": "4.1.3 Data Collection Procedure", "content": "For each action, participants followed a consistent protocol (briefly mentioned in Sec. 3.1.3) comprising two phases per shot: a 5-second free mode and a 10-second record mode. In the free mode, participants were free to rest or perform natural daily activities (negative data). Once entering the record mode, they performed the target actions (positive data). This process was repeated across five rounds, with each round consisting of five consecutive shots. Participants took a short break between two rounds to prevent physical fatigue and were asked to freely adjust the watch position between each round to increase data variance. In total, we collected 25 shots for each target action. Moreover, we leveraged the onboarding process at the beginning of the data collection to passively record participants' natural activities (about 5 minutes). This was used as additional data to augment the negative class\u00b3.\nThe free mode segment was labeled as negative data, while the record mode segment was labeled as positive data. To prevent data contamination, the first two seconds during the record mode were excluded from training because these recordings were mixed with postural changes and arm movement."}, {"title": "4.2 Offline Performance Evaluation", "content": "We evaluated our pipeline by adding one or more actions as target actions. For each action, we randomly selected two rounds of recordings as the training set (up to 10 shots), one round as the validation set (5 shots), and the remaining two rounds as the test set (10 shots). We repeated the training three times and calculated the average performance.\nIt is noteworthy that the model performance has two aspects: the window level and the action level. For the window level, each sliding window is counted as a binary classification data point (same as the model training process). For the action level, windows are aggregated with a smoothing threshold of 3 (Sec. 3.1.3) and represent a closer experience as real-life applications. Such aggregation significantly reduces the false negative and false positive."}, {"title": "4.2.1 Prediction Performance with Different Number of Shots and Actions", "content": "We evaluated the model performance by training on one to ten shots of the data. For action recognition, we started by adding one action for each"}, {"title": "5 INTERVENTION EVALUATION", "content": "The promising model performance in Sec. 4.2 has validated the effectiveness of our few-shot learning pipeline. Building upon the pipeline, we further conducted a user study to evaluate the effectiveness of WatchGuardian and compared it against a rule-based baseline intervention system."}, {"title": "5.1 Participants", "content": "With IRB approval, we recruited the same set of participants in Sec. 4.1 for a follow-up intervention study. In the previous data collection, participants performed five per-determined actions and a self-defined action. In this study, they were asked to select one of the six actions that they had the strongest need for intervention. This action was set as the target action for intervention during the study. Among the 26 participants, 5 of them did not follow the study protocol. Their results were removed as outliers. This section focused on the findings based on the remaining 21 participants."}, {"title": "5.2 Intervention Setting", "content": "Since personal undesirable actions are inherently difficult to predict or control, we designed an intervention experience that closely mirrors real-life contexts to enhance ecological validity, encouraging participants to perform these actions under more natural conditions. Our initial conversation with participants indicated two common scenarios where they tended to perform these actions: when they were in an engaging task with a relaxing state (e.g., watching an interesting movie or a reality show with dramatic twists and turns); and when they were bored or disengaged (e.g., mindlessly scrolling through social media or watching a tedious video) 4. Therefore, we set up two types of video-watching tasks and allowed participants to pick the type in which they tended to perform more undesirable actions.\nThe first type included engaging videos. We prepare a set of multi-hour videos for participants to choose from, such as the Harry Potter movie series, sports competitions, and mystery/detective shows. The second type was watching disengaging videos. Examples include cycling or driving route videos, math problem explanations, and public health lecture videos. Participants sat in a quiet room with a laptop on the table and watched the video they selected, as shown in Figure 7(a) and (b). During the video-watching, participants were not interrupted by the experimenter, simulating the real-life setting."}, {"title": "5.3 Study Design and Procedure", "content": "We adopted a within-subject design and compared our AI-powered WatchGuardian against a rule-based interven- tion system. In the rule-based system, a regular notification (the same interface as Figure 3b) was delivered every 10 minutes, regardless of whether the user did the action. To mitigate the effect of the two systems outputting different numbers of notifications, we further added restrictions in WatchGuardian so that the number of delivered notifications would be in the range of \u00d70.5 to \u00d72 as the baseline system. This was achieved by forcefully delivering a notification if there was no intervention by the end of each 20-minute window (\u00d70.5 times of interventions in minimum). With the 5-min cool-down setup, WatchGuardian can only deliver up to one intervention every 5 minutes, which would be no more than \u00d72 times of interventions as the baseline.\nOur study procedure was designed as follows. After selecting the personal target undesirable action and the task type (engaging vs disengaging), participants would calibrate and familiarize themselves with the intervention system and study setup. They then attended two intervention sessions in total, one session per day. We counterbalanced the order between WatchGuardian and the baseline system, and participants were blind to the order of the two systems. After familiarizing themselves with the room environment and setup, participants went through each intervention session with three stages (in total 130 minutes): (1) a 30-minute pre-intervention stage, where there was no intervention delivered; (2) a 90-minute intervention stage, where WatchGuardian or the"}, {"title": "5.4 Intervention Results", "content": "We first summarize the quantitative results from our study. We coded the recorded videos by documenting the duration of target actions performed by participants every 10 minutes across the three stages. Since participants had diverse behavior patterns, we normalized the results with each individual's target action duration in the pre-intervention stage as the reference. The relative duration was calculated by dividing the average duration of target actions per 10 minutes in both the intervention and post-intervention stages by that of the pre-intervention stage. A lower relative duration means more reduction of the target actions compared to the pre-intervention stage."}, {"title": "5.4.1 Reduction of the Duration of Target Actions by Intervention", "content": "We compare the relative duration between WatchGuardian and the baseline in both intervention and post-intervention stages. Since participants received slightly more notifications in WatchGuardian during the intervention stage (on average 11.8 vs. 9.0 times per session), we controlled the effect of the number of notifications by using generalized linear mixed models (GLMMs). Specifically, a GLMM had relative duration as the dependent variable, with the intervention method (AI-based in WatchGuardian vs. rule-based in baseline) and the number of notifications as the main factors.\nAs shown in the left of Fig.8(a), during the intervention stage, WatchGuardian resulted in 36.0 \u00b1 22.6% of the duration compared to the pre-intervention stage (i.e., a reduction of 64.0% of the target undesirable action), and the baseline system led to 65.0 \u00b1 47.5% of the duration (i.e., a reduction of 35.0%). We fitted a GLMM to compare the two intervention methods. Our results revealed the significant difference between the two methods: WatchGuardian significantly outperformed the baseline by 29.0% more reduction of the target undesirable action ($x = 6.32$, p < .05). Meanwhile, the number of notifications does not show significance ($x^2 = 0.53$, p = 0.47). These results suggest that the advantage of WatchGuardian was mainly attributed to the AI-based intervention method.\nIn addition, although our post-intervention stage was short, both methods showed promising signals of a potential lasting effect when the intervention was gone (13.9 \u00b1 16.8% for the WatchGuardian; 37.7 \u00b1 37.2% for the baseline), as shown in the right of Fig.8(a). We fitted another GLMM on the post-intervention data. The results also indicate the significance of the intervention method ($x^2 = 10.04$, p < 0.01), but not the number of notifications ($x = 0.12$, p = 0.73). This is consistent with the result of the intervention stage, further demonstrating the superior performance of WatchGuardian over the baseline method."}, {"title": "5.4.2 Intervention Effectiveness over Time", "content": "To investigate changes in the duration of target action during the study session, we visualize the change of participants' target action duration throughout the study (see Figure 8(b)). Both intervention methods showed a clear and significant decreasing trend once participants entered the intervention stage. The fitted lines in Figure 8(b) indicate that WatchGuardian achieved more duration reduction (m = -4.8%"}, {"title": "5.4.3 Difference across Task Types", "content": "During the study, we asked participants to pick their own preferred task types between watching engaging (N=11) or disengaging videos (N=10). Figure 9 presents the breakdown of the task type in Figure 8(a). We fitted GLMMs with task type as another main factor and observed a marginal significance of the interaction between the intervention method and the task type ($x = 3.27$, p = 0.07 < 0.1). This was only during the intervention stage, but not the post-intervention stage. Figure 9(a) and (b) indicate that the advantage of WatchGuardian during the intervention stage was more salient when participants were watching engaging videos (\u2206 = 42.3 \u00b1 49.6%) compared to when they were watching disengaging videos (\u2206 = 14.2 \u00b1 22.5%). This could be due to the fact that participants were more interruptable or receptive in less engaging tasks [16, 70, 83], thus even a basic rule-based intervention could effectively reduce the target actions. However, in more engaging tasks, accurate and just-in-time reminders are more effective than basic ones."}, {"title": "5.4.4 Survey Outcomes", "content": "In addition to the objective measurement, we also compare participants' subjective reports on the SUS, WAI, and the change of the habit strength. Overall, participants reported that WatchGuardian had better usability (SUS: 73.3\u00b112.8) than the baseline (66.8\u00b115.9), with significance through a Wilcoxon rank sum test (p < 0.05). WatchGuardian achieved a SUS score over 70, indicating acceptable usability. In both methods, false positive notifications were inevitable and could introduce participants' confusion or surprise, which could explain the subpar SUS scores in general.\nInterestingly, the results of WAI and habit strength did not indicate such a difference. Participants had similar reports of the relationship with the system (WAI score: 42.1\u00b17.1 for the WatchGuardian vs. 41.9\u00b19.3 for the baseline, p = 0.58). The change of habit strength between the pre- and post-intervention stages is also minimal (A of habit strength score: -4.2 \u00b1 5.5 for the WatchGuardian vs. -5.5 \u00b1 6.6 for the baseline, p = 0.25). This"}, {"title": "5.5 Qualitative Results", "content": "All interviews were recorded and transcribed. We adopted a simple content analysis framework [84]. One author took extensive notes during the interviews, went through the scripts to categorize themes and count their frequency, and discussed with two other authors until convergence. We summarize our key findings below."}, {"title": "5.5.1 Perception of Al-powered Intervention", "content": "Multiple participants reported that the AI-powered system possessed a sense of presence or \"having a soul\". For instance, P10 noted, \"[WatchGuardian] resembles a habit instructor, or even like my mom... who would gently remind me when I scratch my head.\" P18 remarked, \"This system seems to read my mind, anticipating when I'm about to bite my lips and reminding me just in time. Sometimes I felt like I was sneaking around when making these actions.\" Compared to the rule-based condition, WatchGuardian's interventions appeared to foster greater self-reflection among users. Notably, P19 even perceived the Al's reminders as rewards: \"After being caught [touching my face] several times initially, I managed to control myself for a while. Then, even if the system reminded me again, I felt it was affirming my progress, like receiving a reward.\" In contrast, the rule-based condition yielded opposite effects, \"This mode of notification felt random to me - it was just like a machine\" (P02).\nHowever, some participants also had a negative experience with WatchGuardian, especially when it did not detect the actions accurately (mostly false positive). For example, P08 mentioned that WatchGuardian had limited impact, and that they also felt a sense of distrust. \"At first, when it reported errors a few times, I tried to look for reasons elsewhere. But it kept making mistakes, which became frustrating. When it occasionally got something right, I thought it was just luck!\" Participants could lose trust in WatchGuardian when the system made mistakes at the beginning of their interaction. This is supported by prior research in other human-AI interaction systems [39, 107]."}, {"title": "5.5.2 Illusory Amplification of Intervention Strength", "content": "We noticed a surprisingly interesting phenomenon: Several users (P09, P10, and P16) reported that the vibration strength of the AI-based intervention in WatchGuardian felt stronger than that of the rule-based intervention. However, the vibration setup was identical in the two sessions. Even after we explained the specific intervention methods after the two study sessions, P16 stated, \"Not only did I subjectively feel that Mode B [our WatchGuardian method] gave me a stronger sense of motion restraint, but it also seemed to vibrate more intensely. Are you sure it's really the same setting?\" This indicated that participants might develop an illusory or distorted perception of the intervention's strength when the interventions were delivered just-in-time. We discuss this more in Sec. 6.1."}, {"title": "5.5.3 Diverse Patterns of Human-Al Collaborative Relationship", "content": "Users exhibited diverse patterns of engagement with the AI system. Some participants demonstrated adaptive behavioral modification in response to Watch- Guardian's reminders. As P14 described, \"Every time I shook my leg, it would remind me, which made me increasingly hesitant to move\". This was aligned with our original design goal of introducing AI-powered JITI.\nOther than reducing the target actions, we also observed other behavior patterns. One pattern emerged where participants developed an interesting competitive relationship with the AI for user agency. For instance, P8 articulated this sentiment: \"I wanted to compete with it - I tried to resist the urge just so it wouldn't catch me.\" This competitive spirit evolved into experimental behavior for some users, who attempted to understand and control the system's underlying logic. P18's experience exemplified this progression: \"Initially, I felt caught red-handed with every reminder. Later, I noticed it wouldn't always detect my subtle movements, so I started experimenting with the notification logic, trying to gain control over the reminders. Eventually, though, I made peace with it and"}, {"title": "6 DISCUSSION", "content": "In this work, we propose to leverage few-shot learning to enable users to self-define personal undesirable actions for personalized intervention"}]}