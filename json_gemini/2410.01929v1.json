{"title": "LLM-AUGMENTED SYMBOLIC RL WITH LANDMARK-BASED TASK DECOMPOSITION", "authors": ["Alireza Kheirandish", "Duo Xu", "Faramarz, Fekri"], "abstract": "One of the fundamental challenges in reinforcement learning RL is to take a complex task and be able to decompose it to subtasks that are simpler for the RL agent to learn. In this paper, we report on our work that would identify subtasks by using some given positive and negative trajectories for solving the complex task. We assume that the states are represented by first-order predicate logic using which we devise a novel algorithm to identify the subtasks. Then we employ a Large Language Model (LLM) to generate first-order logic rule templates for achieving each subtask. Such rules were then further fined tuned to a rule-based policy via an Inductive Logic Programming (ILP)-based RL agent. Through experiments, we verify the accuracy of our algorithm in detecting subtasks which successfully detect all of the subtasks correctly. We also investigated the quality of the common-sense rules produced by the language model to achieve the subtasks. Our experiments show that our LLM-guided rule template generation can produce rules that are necessary for solving a subtask, which leads to solving complex tasks with fewer assumptions about predefined first-order logic predicates of the environment.", "sections": [{"title": "1. INTRODUCTION", "content": "In the realm of Reinforcement Learning (RL), strategically using landmarks and subtasks is a key technique for managing complex tasks [1]. This method systematically breaks down daunting challenges into smaller, achievable goals and clear pathways, making intricate tasks more manageable [2]. To complete a complex task, we must visit certain specific states-referred to as landmarks that contain essential information for successfully accomplishing the task. Landmarks act as critical milestones that facilitate effective decision-making and enhance structured, efficient problem-solving strategies [3]. These landmarks constitute essential milestones about the task, crucial for achieving the goal. For example, a landmark could be possessing specific combinations of objects, arriving at a particular location, or visiting certain places in a specific order [4]. We define each of these landmarks that are necessary to complete a task as a subtask. Subtasks can consist of either the entire state or a subset of the state. Subtasks are particularly valuable in complex environments where a straightforward trajectory to the goal is not readily apparent or where the policy required to solve intricate tasks is complex, making straightforward solutions challenging.\nWhile other works have addressed identifying landmarks through reward-centric algorithms [5, 6], our algorithm uses state trajectories labeled only with a single indicator of whether the trajectory was successful in completing the task. This approach is crucial in environments with sparse and non-interpretable rewards. For this purpose, we have used contrastive learning [7] with the logic-predicate representation of the states as its input.\nRecently, there has been significant interest in symbolic RL in general [8, 9]. Symbolic RL has the advantage of being human interpretable and also more generalizable to new environments. In particular, as a special type of symbolic RL, inductive logic programming (ILP)-based RL agents [10, 11, 12] have utilized differentiable rule learners known as JILP [13, 14] to form logic-based policies.\nRecently, an RL method denoted as NUDGE [15] was proposed using ILP to generate interpretable policy as a set of weighted rules. We will be using NUDGE framework as the ILP engine for further fine tuning our rules generated by LLM for the subtasks.\nWhen processing an input state, the NUDGE system identifies entities and their interactions, transforming raw states into logical representations. In the realm of first-order logic, a predicate functions as a Boolean operation on terms, which are defined as objects or variables. We establish our subtasks using distinct combinations of predicates, thereby facilitating the creation of interpretable subtasks. Our empirical findings indicate that creating subtasks does not require detailed predicates from the environment.\nThe advent and evolution of Large Language Models (LLMs) have sparked significant interest due to their ability to utilize common sense knowledge and process information in natural language, mirroring real-world understanding [16]. There are recent research works that elaborate LLMs as either auxiliary supports or principal agents within RL frameworks [17, 18]. These innovative approaches utilize the descriptive and inferential strengths of LLMs to more effectively navigate and solve complex environmental challenges."}, {"title": "2. LANDMARK IDENTIFICATION FROM\nTRAJECTORIES", "content": "Reinforcement learning (RL) tackles decision-making problems in environments defined by a state space S, an action space A, and transition dynamics P(s' | s, a), where the goal is to maximize rewards over time. In this context, a policy \\(\\pi(a | s)\\) maps states to actions, guiding the agent towards maximizing the expected discounted sum of rewards \\(E_{\\pi} [\\Sigma_t \\gamma^t r(s_t, a_t, s_{t+1}) ]\\), where \\(\\gamma\\) is a discount factor to prioritize immediate rewards. This sets the stage for designing RL algorithms that can learn optimal actions in complex decision spaces.\nIncorporating a rule-based policy within this RL framework can provide a structured and interpretable way to guide decision-making. Leveraging concepts from First-Order Logic (FOL), we represent policies as rules. In FOL, predicates describe relationships between terms (constants, variables, or function-based expressions), p(t1,...,tn), and rules consist of a head (the action to be taken) and a body (a set of predicates describing the current state). Rules are often written in the form A: B1,..., Bn, where A is the head (action) and B1, . . ., Bn are the body predicates.\nIn our approach, we employ an ILP-based RL agent, as described in the NUDGE [15], with states represented by grounded FOL predicates. To identify landmarks, we first apply a contrastive learning algorithm to detect potential landmark states, followed by a graph search algorithm [20] to identify the necessary grounded predicates for each subtask. We leverage both positive and negative trajectories from a Neural Network (NN) RL agent, collecting 50 positive and 500 negative trajectories during the early stage of training. The advantage of using an NN agent is that it does not require prior information about the environment. Positive trajectories are those that successfully achieve the task's goal, while negative ones do not.\nEach state trajectory is defined as \\(T_i\\), where \\(T_i = (s_0, s_1,..., s_T)\\). \\(T_i^+\\) is the i'th positive trajectory and \\(T_i^-\\) refers to i'th negative trajectory. We used a two-layer NN to assign a number between zero and one to every state. We propose that landmarks should consistently appear in all positive trajectories but may occasionally appear in some negative ones. To achieve this, we train the NN to output 1 for landmark states and 0 for non-landmark states. For this aim, we should maximize this function:\n\\(\\sum_{i, j} log( \\frac{exp(\\Sigma_{s_k} f_\\theta(\\tau_i^+(s_k)))} {exp (\\Sigma_{s_k} f_\\theta(\\tau_i^+(s_k))) + exp (\\Sigma_{s_t} f_\\theta(\\tau_j^-(s_t))))} )\\)\nwhere \\(\\tau_i^+(s_k)\\) denotes the k'th state of i'th trajectory of positive samples. The sum is over the pairs of randomly chosen trajectories from positive and negative samples. The results of the algorithm are detailed in the experimental section of this paper.\nNext, we develop a method for identifying subtasks from our landmark candidates. The necessity of subtasks in every positive trajectory is a characteristic that stems from the definition of a subtask. A subtask is defined as a necessary state or subset of a state that must be visited to complete a task.\nThe algorithm takes as its input the set of all candidates' landmark states resulting from the contrastive learning algorithm. Then it proceeds to evaluate all combinations of grounded predicates to identify all subtasks. As shown in Fig. 1, we associate all of the predicates to Node00 at the root of the tree graph. A subtask is defined by its consistent presence in every positive trajectory and its absence in negative trajectories, which we verify by examining random negative samples. If no subtask is detected at the current node, we extend the tree graph by adding leaves. Each leaf is created by removing a predicate from the current set assigned to the node, move to a deeper level, and add the newly formed nodes to the frontier.\nTo determine the next node to explore from the frontier, a softmax function is applied on f(Node), which is based on two factors: the number of unique predicate combinations in the node and its level in the search hierarchy. Our goal is to find the largest set of predicates that define a subtask. Once a node is validated, it is explored further by increasing its level and removing it from the frontier. Details are provided in Algorithm 1.\nOur graph search algorithm identifies the largest set of predicates that reliably activate landmarks, treated as subtasks for the next stage. Fig. 2 highlights how the graph search enhances the algorithm's precision and efficiency."}, {"title": "Algorithm 1 Graph Search Algorithm", "content": "1: Landmarks \u2190 \u00d8, g(C) \u2190 0\n2: Node0,0 All predicates used in the embedding input\n3: Frontier Nodes (FN) \u2190 Nodeo,0\n4: Frontier Nodes States(FNS) \u2190 All unique detected states with a value of 1 in the contrastive learning algorithm\n5: Negative Test (NT) \u2190 Random 10 negative sample\n6: while g(c) < 1 do\n7: f(Nodej,i) = \\(\\frac{Number\\ of\\ States\\ in\\ Node_{j,i}}{Number\\ of\\ States\\ in\\ Node_{0,0}}\\\n8: Chosen Node (CNj,i) \u2190 Choose a node from softmax distribution over all f(Nodej,i) on FN\n9: Node States(NS) \u2190 Unique states with CN predicates\n10: for state in NS do\n11: if (state \u2208 Ti,p,\u2200i) & (\u2203i \u2208 NT, state \u2209 Ti,n)\nthen\n12: Landmarks \u2190 state, g(C) \u2190 1\n13: end if\n14: end for\n15: Frontier Nodes (FN) \u2190 FN/CNj,i\n16: New Nodes(NNj:k+j,i+1) \u2190 CNj,i/Pk\n17: Frontier Nodes (FN) \u2190 FN+ NNj:k+j,i+1\n18: end while"}, {"title": "3. RULE GENERATION FOR ATTAINING\nLANDMARKS USING LLM", "content": "By employing subtask decomposition, we simplified the challenge of learning RL policy rules for a complex task by breaking it down into smaller, manageable subtasks. In this context, we employed few shot learning with the LLAMA 3.1 [21] model to generate rules for each identified subtask. The experimental results are discussed in the following section, with details of the prompts shown in Fig. 4. The input to the LLM consists of a constant part, which includes definitions of predicates used to represent the states and general information about the environment. To create base rules, we combined the subtask with a base prompt and two rule examples from other environments, helping the model follow the rule template and grasp the logic behind the rules.\nTo evaluate the effectiveness of a rule, we tested the RL agent using generated template rules. If the rules fail to achieve the subtask, we refine the template rules. We record the state corresponding to the lowest reward as the failed state. Since the LLM did not generate a complete set of rules for us, we refined them by utilizing additional prompts. These prompts ask the LLM to interpret the rule and modify it by removing some predicates to increase generality or by adding predicates to enhance detail. Depending on the complexity of the subtask, we can generate rules that are either more general or more detailed."}, {"title": "4. EXPERIMENT", "content": "The environment, adapted from the GetOut and Loot environment in [15]. GetOut has been modified to include distinct landmarks and new objects, such as two coins, a flag, and a red key. The four subtasks we refer to are: collecting two coins, collecting a flag, collecting a blue key, and then proceeding to the door.. An example state of the modified GetOut environment is shown in Fig. 6.\nIn Table 1, we compare the results of the algorithm in two environments: one with additional predicates and knowledge, and another with fewer predicates. We evaluate it on tasks with varying numbers of subtasks. Since we did not have labels for the landmark states in Fig. 2, we manually labeled them to evaluate the accuracy of subtask detection. Table 2 highlights the necessity of subtasks, showing results after rule generation and policy learning. Fig. 3 compares our algorithm to human generated rules, demonstrating similar success and showing that missing subtask results in task failure. Fig. 5 illustrates the comparison between the rule policy from the Nudge and a template generated rule and policy for the coin subtask."}, {"title": "5. CONCLUSION", "content": "The paper introduces a novel method for detecting landmarks to decompose complex tasks into subtasks. FOL state representation and leveraging LLM led us to create rule-based policies through an ILP-based RL agent. Experiments demonstrate that the algorithm is both accurate and efficient in subtask detection and that LLM-guided rule generation This method reduces reliance on predefined logic predicates, offering a more flexible and scalable solution. Future work aims to extend the approach to real-world tasks and enhance rule fine-tuning for broader generalization."}]}