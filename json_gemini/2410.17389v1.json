{"title": "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models", "authors": ["Muhan Lin", "Shuyang Shi", "Yue Guo", "Behdad Chalaki", "Vaishnav Tadiparthi", "Ehsan Moradi Pari", "Simon Stepputtis", "Joseph Campbell", "Katia Sycara"], "abstract": "The correct specification of reward models is a well-known challenge in reinforcement learning. Hand-crafted reward functions often lead to inefficient or suboptimal policies and may not be aligned with user values. Reinforcement learning from human feedback is a successful technique that can mitigate such issues, however, the collection of human feedback can be laborious. Recent works have solicited feedback from pre-trained large language models rather than humans to reduce or eliminate human effort, however, these approaches yield poor performance in the presence of hallucination and other errors. This paper studies the advantages and limitations of reinforcement learning from large language model feedback and proposes a simple yet effective method for soliciting and applying feedback as a potential-based shaping function. We theoretically show that inconsistent rankings \u2013 which approximate ranking errors \u2013 lead to uninformative rewards with our approach. Our method empirically improves convergence speed and policy returns over commonly used baselines even with significant ranking errors, and eliminates the need for complex post-processing of reward functions.", "sections": [{"title": "1 Introduction", "content": "The correct specification of task rewards is a well-known challenge in reinforcement learning (RL) (Leike et al., 2018). Complex tasks often necessitate complex, nuanced reward models, particularly as shaping terms may be required to guide exploration. However, hand-crafting these reward functions is difficult and often leads to a phenomenon known as reward hacking, wherein an agent learns to exploit a reward function for increased returns while yielding unexpected or undesired behavior (Skalse et al., 2022). Reward hacking is symptomatic of the broader challenge of value alignment, in which it is difficult for a human domain expert to fully and accurately specify the desired behavior of the learned policy in terms of a reward function.\nIn this study, we aim to eliminate the dependence on handcrafted reward functions by training agents with reward functions derived from data. A notable method in this domain is reinforcement learning from human feedback (RLHF), where policy trajectories are ranked by humans. These rankings are then used to learn a reward function which guides model training and facilitates value alignment. This process is extremely costly in terms of human effort, however, requiring a significant number of rankings to train accurate reward models (Casper et al., 2023).\nWe can avoid the need for humans-in-the-loop by instead generating rankings with pre-trained large language models (LLMs) in a process known as reinforcement learning with AI feedback (RLAIF) (Lee et al., 2023; Bai et al., 2022; Kim et al., 2023). However, LLMs are well known to hallucinate and present false information as fact (Zhang et al., 2023), which reduces the accuracy and reliability of the resulting rankings. This is often overcome through complex reward post-processing techniques, which may be task-specific and difficult to tune (Klissarov et al., 2023).\nIn this work, we propose a simple and effective strategy for reinforcement learning in the face of unreliable LLM feedback. The core idea underlying our approach is to issue uninformative rewards for states in which the LLM is uncertain. Thus, we avoid issuing potentially misleading rewards which allows us to train performant policies even in the face of significant ranking errors. Building off the insight that certainty in language models is expressed through output consistency (Tanneru et al., 2024), we show that rewards issued from a potential-based scoring function learned over repeated rankings naturally reflect an LLM's uncertainty.\nOur contributions are as follow, we 1) introduce"}, {"title": "2 Related Works", "content": "Constructing rewards based on human feedback has a long history (Thomaz et al., 2006). To efficiently use human domain knowledge and provide more generalizable rewards, human preference on episode segments (Sadigh et al., 2017; Christiano et al., 2017; B\u0131y\u0131k et al., 2019) and human demonstrations (B\u0131y\u0131k et al., 2022) are distilled into models which serve as reward functions for RL. The method has witnessed great success in complex domains where rewards are difficult to specify such as training large language models (LLM) to align with human logic and common sense (Ziegler et al., 2019; Ouyang et al., 2022).\nOne major drawback of RLHF is its requirement of exhaustive human participation to provide demonstrations and feedback. LLMs have shown deductive logic abilities comparable to humans in recent years (Du et al., 2023), and are able to substitute humans in reward issuing (Kwon et al., 2023; Yu et al., 2023; Lee et al., 2023; Xie et al., 2023), or data collection and labeling for reward model training (Lee et al., 2023; Klissarov et al., 2023). While the former suffers from time and resource costs for training RL agents, the latter is becoming promising for training complex RL tasks (Wang et al., 2024).\nAn outstanding challenge with leveraging LLM-based feedback is that the performance of RLHF is dependent on the quality of feedback received (Casper et al., 2023). Different LLMs have distinct probabilities of giving wrong feedback, thus leading to rewards of varying quality. Casper et al. (2023) also suggests that comparison-based feedback may not be efficient and adequate to train reward models with noisy LLM outputs. In this work, we analyze the training performance of reinforcement learning agents across various LLMs, each of which produce different error distributions in feedback.\nAnother challenge is that of the reward formulation itself. Many works train a model distilling LLM or human preference and use it as the reward model (Christiano et al., 2017; Wang et al., 2024; Klissarov et al., 2023; Lee et al., 2023), but in practice, this needs post-processing on outputs of the reward model such as filtering (Klissarov et al., 2023), and normalization (Christiano et al., 2017). Our work posits that a reward function trained without complex post-processing and environment rewards would be more general and adaptable to various practical scenarios."}, {"title": "3 Background", "content": "Reinforcement Learning: In reinforcement learning, an agent interacts with an environment and receives a reward for its current action at each time step, learning an optimal action policy to maximize the rewards over time. This procedure can be formulated as an infinite horizon discounted Markov Decision Process (MDP) (Sutton and Barto, 2018).\nAt each discrete timestep $t$ in this process, the agent observes environment state $s_t$ and takes action $a_t$, leading to the next environment state $s_{t+1}$ and a reward $r_t$. An MDP is represented as a tuple $(\\mathcal{S}, \\mathcal{A}, R, T, \\gamma)$, where $\\mathcal{S}$ is a set of states, $\\mathcal{A}$ is a set of actions, $R : S \\rightarrow R$ is a reward function, $T(s,a,s') = P(s'|s, a)$ is a transition function, and $\\gamma$ is a discount factor. A stochastic policy $\\pi(a|s) : A \\times S \\rightarrow [0, 1]$ indicates the probability that the agent selects action $a$ given the state $s$. The agent's goal is to learn $\\pi$ maximizing the expected discounted return through training, given an initial state distribution.\nPreference-based Reinforcement Learning: Our work is based on the framework of preference-based reinforcement learning, where the reward function is learned from preference labels over agent behaviors (Christiano et al., 2017; Ibarz et al., 2018; Lee et al., 2021a,b). For a pair of states $(s_a, s_b)$, an annotator gives a preference label $y$ that indicates which state is ranked higher, considering which state is closer to the given task goal. The preference label $y \\in {0, 1}$, where 0 indicates $s_a$ is ranked higher than $s_b$, and 1 indicates $s_b$ is ranked higher than $s_a$. Given a parameterized state-score function $\\sigma_\\psi$, which is commonly called the potential function and usually equated with the reward model $r_\\psi$, we compute the preference probability of a state pair with the standard Bradley-Terry model (Bradley and Terry, 1952),\n$P_y[s_b > s_a] = \\frac{\\exp{(\\sigma_\\psi(s_b))}}{\\exp{(\\sigma_\\psi(s_a))} + \\exp{(\\sigma_\\psi(s_b))}} = \\text{sigmoid}(\\sigma_\\psi(s_b) - \\sigma_\\psi(s_a)), \\qquad(1)$"}, {"title": "4 Methodology", "content": "Despite the success of LLMs in few-shot task generalization, these models are imperfect and yield sub-optimal performance in many areas. One notable issue is the well-documented tendency of LLMs to hallucinate, which results in LLM-generated preference rankings frequently containing errors (see Table 1). These errors present major challenges for reinforcement learning from LLM feedback, as they result in noise in the learned score function. Under the standard RLHF formulation where rewards are directly issued from the score function (Christiano et al., 2017), this can lead to inefficient exploration at best and, at worst, trap the agent in sub-optimal local minima."}, {"title": "4.1 Quantifying Feedback Error through Output Consistency", "content": "It has been shown that the certainty of LLM predictions can be quantified by issuing the same query multiple times and measuring the consistency of the predictions (Lyu et al., 2024). Specifically, the confidence of ranking $s_a$ higher than $s_b$, $\\text{conf}{y = (s_a > s_b)}$, is defined as $\\frac{N(s_a > s_b)}{N_{\\text{query}}(s_a,s_b)}$ where $N(s_a > s_b)$ denotes the number of times LLM ranks $s_a$ higher than $s_b$, and $N_{\\text{query}}(s_a, s_b)$ denotes the total number of queries on $s_a$ and $s_b$. Confidence is a necessary condition to consider when evaluating LLM feedback quality, given that low confidence often causes considerable noise in feedback which manifests as incorrect rewards. Based on the definition of feedback confidence, we derive an equivalent form of the RLHF loss based on ranking confidence and consistency as follows:\n$\\mathcal{L} = -\\mathbb{E}_{(s_a, s_b, y) \\sim \\mathcal{D}} \\mathbb{E}_{N_{\\text{query}}} [ \\mathbb{I}{y = (s_a > s_b)} \\log P_y[s_a > s_b] + \\mathbb{I}{y = (s_b > s_a)} \\log P_y[s_b > s_a] ]$\n$= -\\mathbb{E}_{(s_a, s_b, y) \\sim \\mathcal{D}}  [ \\text{conf} {y = (s_a > s_b)} \\log(\\text{sigmoid}(\\sigma_\\psi(s_a) - \\sigma_\\psi(s_b)))+ \\text{conf} {y = (s_b > s_a)} \\log(\\text{sigmoid}(\\sigma_\\psi(s_b) - \\sigma_\\psi(s_a))) ]. \\qquad(3)$\nThis loss function uses confidence-based weights to relate the scores between each state in ranked pairs. From this derivation, we see the following."}, {"title": "4.2 Potential-based Rewards for Learning with Noisy Feedback", "content": "The above observations stemming from Eq. 5 motivate the form of our proposed method. Intuitively, when the LLM is completely uncertain when ranking $s_a$ and $s_b$ then the difference between their scores is zero. This is ideal, as when the LLM is unable to issue an accurate ranking then we would like it to issue an uninformative reward, i.e., a reward of zero. Our solution is to treat the state-score as a potential function, defining the reward as the difference between the scores of successive state pairs:\n$r(s_t) = \\sigma_\\psi(s_t) - \\sigma_\\psi(s_{t-1}). \\qquad(4)$\nThus, the more uncertain an LLM's ranking is, the less informative the reward is. The potential in Eq. 4 is naturally shaped to a proper scale range, with positive rewards for actions that are beneficial and promising to the given task goal and negative rewards for detrimental actions. Large values correspond to more confident rankings, while small ones to less confident rankings. As such, our approach is particularly well-suited to RLAIF with smaller, specialized models which are often necessary in resource-constrained environments.\nThere is an additional benefit to this formulation. Prior works treat the state-score function as a reward function and directly issue rewards from it, which we call the \"direct-reward\" method. This often leads to training instability as the rewards may have significant differences in scale, which need to be corrected via post-processing techniques such as normalization and thresholding as well as extrinsic per-step reward penalties. However, the performance of post-processed direct rewards is highly sensitive to these hyper-parameters, as they are often task-specific. Our potential difference formulation helps alleviate this issue as 1) uncertain states converge to the same score value so the impact of noisy rankings no longer needs to be mitigated through post-processing, and 2) per-step penalties can be discarded in favor of simple timestep-based discounting which are far less sensitive."}, {"title": "4.3 Algorithm", "content": "Our algorithm consists of the following four steps: 1) Randomly sample pairs of sequential states from the environment. 2) Query the LLM to rank states in each pair with respect to a natural language task description, e.g., \u201cGo to the green goal\". The prompt contains a language task description, environment description, and in-context learning examples (Wei et al., 2022) as context to generate preference labels for states in each pair. 3) Train the state-score model $\\sigma_\\psi$ with the loss function in Eq. 2. 4) Train a reinforcement learning agent with feedback from the state-score model."}, {"title": "5 Performance Analysis of Potential-Difference Rewards", "content": "We evaluate our approach in commonly-used discrete (Grid World) and continuous (MuJoCo) (Brockman et al., 2016) benchmark environments. Throughout these experiments, we investigate the effectiveness of our potential-based reward function a) as compared to using the score as a reward directly in both single and multi-query settings; and b) its sensitivity to inconsistency in state rankings."}, {"title": "5.1 Experiment Setup", "content": "Grid World. We examine three scenarios within Grid World (Swamy et al., 2024): NoLock, Lock, and MultiLock. The layouts are shown in Fig. 2. In each scenario, the agent (green triangle) must navigate to the target (green rectangle). There are one and two locked doors in the Lock and MultiLock variants, respectively, that block the agent's way to the goal. To unlock each door the agent must pick up the appropriate key and use it on the door. The agent, goal, and key positions are randomly initialized in every episode.\nMuJoCo. We examine a subset of robot control tasks selected from the simulated robotics benchmark MuJoCo (Todorov et al., 2012). We choose 3 tasks with increasing degrees of complexity: Inverted Pendulum, Reacher, and Hopper.\nFor each of these six environments, we compare our approach with the following baselines:\n\u2022 Direct reward directly utilizes the trained state-score functions' score as reward; i.e., $r(s) = \\sigma_\\psi(s)$. Following Christiano et al. (2017), the reward is normalized to zero-mean with a standard deviation of 1.\n\u2022 Default reward utilizes the vanilla RL objective of each environment with human-specified reward functions. In grid world variants, the default reward is defined as 0 for failure and $1 - 0.99^{n/n_{\\text{max}}}$ otherwise when the agent reaches the goal, where $n_{\\text{max}}$ is the maximum time steps for each episode and $n$ is the step count until success. For MuJoCo tasks, the default rewards are specified as those defined in OpenAI Gym (Brockman et al., 2016).\nIn each environment, we randomly sample pairs of sequential states from the environment with replacement in order to generate rankings for training the state-score model used by both potential difference and direct reward. For single-query experiments, Grid World uses 2500, 3500, and 6000 samples for NoLock, Lock, and MultiLock respectively while MuJoCo uses 1000 samples for each environment.\nWithout loss of generality, we employ PPO as the underlying policy-training framework (Schulman et al., 2017) and make the following assumptions: a) the environment is fully observable; and b) the agent has no knowledge of the task before training, i.e., is randomly initialized."}, {"title": "5.2 LLM Ranking Performance", "content": "We first quantify the performance of four different LLM models used in this work over each Grid World environment. After sampling pairs of sequential states as discussed in Sec. 5.1, we measure the accuracy of a) the LLM's rankings and b) the resulting state-score model with respect to ground truth rankings. The results, shown in Table 1, provide us with an approximate ordering of LLM ranking performance, where GPT-4 > Llama-3 70B > Mixtral > Llama-3 8B."}, {"title": "5.3 Single-Query Evaluation", "content": "We next examine how our approach performs compared to the standard direct reward approach commonly utilized in RLHF. In each environment, we train our state score models with 4 LLMs: Mixtral (Jiang et al., 2024), GPT 4 (Achiam et al., 2023), and Llama-3 with 8B and 70B parameters (Touvron et al., 2023). For Grid World environments, we add an additional baseline in which rankings are generated using a ground truth heuristic function (GT) which serves as an upper bound for our methods.\nThe state score models are trained by minimizing the loss in Eq. 2. Then they are employed to train 5 RL policies with random seeds and initializations for each method. As a common approach to avoid reward hacking, a constant step penalty is applied to the produced rewards from both methods in all environments except for MuJoCo Reacher, which exploits a torque penalty as described in Brockman et al. (2016). The results, as well as the default reward performance, are shown in Fig. 3 and Fig. 4. In Grid World environments, our method (in"}, {"title": "5.3.1 Hyper-Parameter Sensitivity Analysis", "content": "Since potential difference reward and direct reward suffer from reward hacking without post-"}, {"title": "5.4 Multi-Query Evaluation", "content": "We introduce a multi-query approach that queries about ranking each state-transition case in the scoring-model training dataset a given number of times to address the rankings' inconsistency with lower-performing LLMs to push potential difference rewards toward zeros in the face of conflicting responses. In Fig. 5, we illustrate the heat maps of state scores trained with datasets of distinct consistency degrees, demonstrated in the Grid World MultiLock environment. Each grid in the heat maps records the score the scoring model assigns for an agent at that location. The left sub-figure demonstrates the ideal case in which 100% correct rankings are utilized to train the scoring model, demonstrating a smooth gradient from the start room (top left corner) to the final room (bottom left corner) roughly following the correct path. However, if the scoring model is trained with 50% confidence on all state pairs (right sub-figure in Fig. 5), the score in any state becomes equal as no adjacent states are ranked higher with high confidence. This demonstrates our method's ability to disregard states, and thus not provide rewards when LLM rankings are inconsistent across multiple queries. Finally, when the ranking results for a subset of states are inconsistent, yet consistent for all other locations (see Fig. 5 center), the correct gradual change in score is maintained outside of the affected area. These results underline our method's capabilities with respect to the effects of pushing uncertain state scores toward zero while giving contrasting rewards to confident pairs, ultimately improving performance of our method with low-performing LLMs (see Sec. 5.4.1)."}, {"title": "5.4.1 Synthetic Ranking Evaluation", "content": "To test what ranking accuracy of datasets is needed for the LLM with multi-query methods, and how"}, {"title": "5.4.2 LLM Ranking Evaluation", "content": "Observing that Mixtral has the highest inconsistency in ranking states and thus has the largest potential for improvement, we evaluate the 5-query variations of potential-difference rewards and direct rewards with ranking results from Mixtral to verify our claims. Different methods' RL policy training curves averaged over 5 random seeds are compared in Fig. 8. As hypothesized, the 5-query potential-difference rewards achieve faster policy training and result in the highest rewards in all experiments. The single-query potential-difference rewards also outperform the single-query direct rewards. The improvement is most significant in Grid World - Lock scenario."}, {"title": "6 Conclusions", "content": "In this work, we propose a simple method for incorporating noisy LLM feedback into reinforcement learning. Our approach is based on learning a potential-based score function over repeated LLM-generated preference rankings which issues uninformative rewards for states in which the LLM is uncertain. We show both theoretically and empirically that this results in a natural trade-off between reward sparsity and LLM confidence in a variety of discrete and continuous environments."}, {"title": "7 Limitations", "content": "Our current analysis is limited to relatively simple discrete and continuous environments so that we could perform a thorough empirical evaluation. However, the consequence of this is that several of the LLMs, e.g., GPT-4, perform exceptionally well when ranking which leaves limited room for improvement. This is especially notable in the MuJoCo environments where our potential difference approach results in insignificant changes to performance. On the other hand, smaller-parameter models such as Mixtral exhibit worse performance and as such benefit more from our approach (Fig. 4) which is in-line with our synthetic experiments (Fig. 8). In the future, we wish to explore more complex, realistic environments which induce similar ranking errors in a larger set of language models. Our method is theoretically compatible with visual and multimodal environments that possess richer state and action spaces and local observations, which can be ranked by LLMs or VLMs. Exploring these scenarios will be the focus of our future work. A further limitation is that we currently assume that sequential state pairs can be randomly sampled from the environment. While this is true in most simulated environments, this assumption is violated in others such as the real world. In future work, we will explore iterative algorithms which alternate training the policy and sampling state pairs for ranking."}, {"title": "Acknowledgments", "content": "This work is sponsored by Honda Research 58629.1.1012949, AFOSR FA9550-18-1-0251 and DARPA FA8750-23-2-1015. The authors would also like to thank Dr. Woojun Kim for his assistance during discussion of this research."}, {"title": "A Theoretical Proof: Inconsistent rankings lead to uninformative rewards", "content": "Lemma 1. In the scope of RL based on LLM feedback, the confidence-based preference loss is equivalent to the standard preference loss used by state-score model training over multi-query ranking datasets.\nProof. Given that the the confidence of ranking $s_a$ higher than $s_b$, $\\text{conf}{y = (s_a > s_b)}$, is defined as $\\frac{N(s_a > s_b)}{N_{\\text{query}}(s_a,s_b)}$ where $N(s_a > s_b)$ denotes the number of times LLM ranks $s_a$ higher than $s_b$, and $N_{\\text{query}}(s_a, s_b)$ denotes the total number of queries on $s_a$ and $s_b$.\nThe standard preference loss over multiple-query ranking dataset $\\mathcal{D}$ can be written as\n$\\mathcal{L}_\\mathcal{D} = -\\mathbb{E}_{(s_a,s_b,y) \\sim \\mathcal{D}} \\mathbb{E}_{N_{\\text{query}}} [ \\mathbb{I}{y = (s_a > s_b)} \\log(\\text{sigmoid}(\\sigma_\\psi(s_a) - \\sigma_\\psi(s_b)))+ \\mathbb{I}{y = (s_b > s_a)} \\log(\\text{sigmoid}(\\sigma_\\psi(s_b) - \\sigma_\\psi(s_a)))]$\n$= -\\mathbb{E}_{(s_a,s_b,y) \\sim \\mathcal{D}}  [ \\text{conf} {y = (s_a > s_b)} \\log(\\text{sigmoid}(\\sigma_\\psi(s_a) - \\sigma_\\psi(s_b)))+ \\text{conf}{y = (s_b > s_a)} \\log(\\text{sigmoid}(\\sigma_\\psi(s_b) - \\sigma_\\psi(s_a))) \\qquad(5)$", "sections": [{"title": null, "content": "Theorem 1. As inconsistency of a ranking over two states increases, the scores of these two states converge to the same value.\nProof. Based on Equation 5,\n$\\mathcal{L}_\\mathcal{D} = -\\mathbb{E}_{(s_a,s_b,y) \\sim \\mathcal{D}}  \\text{conf}{y}  [ \\text{conf}{y = (s_a > s_b)} \\log(\\text{sigmoid}(\\sigma_\\psi(s_a) - \\sigma_\\psi(s_b)))+ (1 -  \\text{conf}{y = (s_a > s_b)}) \\log(1 -  \\text{sigmoid}(\\sigma_\\psi(s_a) - \\sigma_\\psi(s_b))] \\qquad(6)$\nTake an arbitrary state pair (s0, s1) from D. As inconsistency of the ranking over s0 and s1 increases, conf{y = (so > s1)} \u2192 0.5. Denote sigmoid(\u03c3\u03c8(s0) \u2013 \u03c3\u03c6(s1)) as P0,1, L over other states in D without s0, s1 as LD{s0,s1},\n$\\lim_{\\text{conf}{y=(s_0>s_1)} \\rightarrow 0.5} \\mathcal{L} = \\frac{1}{|D|} \\frac{1}{2} \\log(p_{0,1}(1 \u2013 p_{0,1})) + \\mathcal{L}_{D\\{s_0, s_1\\}} \\qquad(7)$\n>$\\frac{1}{|D|} \\log 2+ \\mathcal{L}_{D\\{s_0, s_1\\}}\u00b7$\nIf and only if $p_{0,1} \\rightarrow .5$, where \u03c3\u03c8(s0) - \u03c3\u03c8(s1) \u2192 0, $\\lim_{\\text{conf}{y=(s_0>s_1)} \\rightarrow 0.5} \\mathcal{L} \\rightarrow  \\frac{1}{|D|} \\log 2 + \\mathcal{L}_{D\\{s_0, s_1\\}}$, reaching the lower bound.\nTherefore, when training the state-score model with this loss L, the scores of any two states whose ranking confidence is close to 50% will be pushed to the same value."}]}, {"title": "B Scoring-Model Training Datasets", "content": "To train the scoring model, we randomly sample sequential state pairs and collect LLM ranking results on them, assembling all the data into a training dataset. The training data for all six environments can be accessed here: Scoring-Model Training Data. The details of collection process are as follows."}, {"title": "B.1 LLM Preference Generating Process", "content": "The LLM does pairwise state ranking in this work. We follow the methodology described in (Lee et al., 2023), where the LLM prompt consists of four parts:\n1. Preamble: A description of the environment, task, and ranking criteria.\n2. Few-shot exemplar: Pairwise state-ranking example, showcasing the chain of thought on ranking according to given environment conditions and state evaluation criteria.\n3. Sample to annotate: The pair of specific states a and b, described with natural language.\n4. Ending: Ending text to prompt a preferred response as ranking.\nIn the generated response, the LLM determines the ranking based on the specified criteria between two sequential states and outputs either 'Yes' (a is ranked higher than b) or 'No' (b is ranked higher"}, {"title": "C Experiment Details for Reproducibility", "content": "C.1 Model Architecture\nGrid World The policy model for all scenarios contains separate actor and critic networks, both with 3 convolutional layers followed by 1 fully connected layer mapping the flattened vector to the output vector. The convolutional layers consist of 16 2 \u00d7 2 filters, followed by 2 \u00d7 2 pooling, then 32 2 \u00d7 2 filters, and finally 64 2 \u00d7 2 filters. Scoring model architectures for each scenario are shown in Table 2.\nMuJoCo Policy model follows Schulman et al. (2017), where both actor and critic networks have a fully connected network using a hidden layer with 64 nodes. Distinct scoring model architectures are used in each scenario, shown in Table 3."}, {"title": "C.2 Hyperparameters", "content": "The hyperparameters of training scoring models and PPO policies were tuned manually. The details are recorded in Table 4, 5, 6, 7."}]}