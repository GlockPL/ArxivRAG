{"title": "On the Generalization Ability of Machine-Generated Text Detectors", "authors": ["Yule Liu", "Zhiyuan Zhong", "Yifan Liao", "Zhen Sun", "Jingyi Zheng", "Jiaheng Wei", "Qingyuan Gong", "Fenghua Tong", "Yang Chen", "Yang Zhang", "Xinlei He"], "abstract": "The rise of large language models (LLMs) has raised concerns about machine-generated text (MGT), including ethical and practical issues like plagiarism and misinformation. Building a robust and highly generalizable MGT detection system has become increasingly important. This work investigates the generalization capabilities of MGT detectors in three aspects: First, we construct MGT-Academic, a large-scale dataset focused on academic writing, featuring human-written texts (HWTs) and MGTs across STEM, Humanities, and Social Sciences, paired with an extensible code framework for efficient benchmarking. Second, we investigate the transferability of detectors across domains and LLMs, leveraging fine-grained datasets to reveal insights into domain transferring and implementing few-shot techniques to improve the performance by roughly 13.2%. Third, we introduce a novel attribution task where models must adapt to new classes over time without (or with very limited) access to prior training data and benchmark detectors. We implement several adapting techniques to improve the performance by roughly 10% and highlight the inherent complexity of the task. Our findings provide insights into the generalization ability of MGT detectors across diverse scenarios and lay the foundation for building robust, adaptive detection systems.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) showcase their strong ability in tackling a wide range of natural language processing (NLP) tasks [30, 22, 21]. Its versatility and superiority across numerous domains unlock remarkable real-world applications, e.g., education, idea crafting, and context refinement [38]. However, the ease and convenience have opened the door to abuse, particularly in academic writing and creative industries, leading to severe ethical and practical challenges [32]. Recent efforts [18, 2] have focused on developing techniques to distinguish machine-generated text (MGT) from human-written text (HWT) and benchmarking their performance [8], primarily in binary classification tasks, where the goal is to determine whether a given text is MGT or HWT. Besides binary classification, the text attribution task, which aims to identify the specific source LLM that generated the text, presents additional challenges and remains under-explored.\nIn this work, we conduct an in-depth investigation of the generalization ability of MGT detectors in the following aspects: First, we construct a large-scale MGT dataset named MGT-Academic focused on academic writing, comprising over 20M tokens and 73K samples across three academic domains: STEM, Humanities, and Social Sciences. Within each domain, we collect HWT data from Wikipedia and academic texts sourced including Arxiv or Project Gutenberg, depending on the scenario. Each HWT has corresponding MGTs generated by five popular LLMs. Further, we build a publicly available, extendable, and user-friendly code framework for the community, which enables fast and effective benchmarking for existing methods in binary classification and text attribution tasks.\nSecond, leveraging the MGT-Academic, we perform an in-depth investigation of the transferability of detectors, focusing on enhancing the generalization capabilities of detectors across diverse data distributions. Specifically, we conduct extensive experiments to explore how detectors adapt to specific domains, such as STEM, Humanity, and Social Science, as well as leading LLMs like GPT-40-mini and Llama-3.1-70b. This dataset enables fine-grained analysis of MGT detectors' transferability. Additionally, we adopt techniques to improve the transferability by adding few-shot examples from the target domains or LLMs to investigate how to adapt the detectors effectively.\nThird, since new LLMs are continuously released, each with different characteristics and unique stylistics, we consider a new attribution task, where a model would adapt to the new class introduced over time without (or with very limited) access to the original training data for earlier classes. This task is crucial for real-world applications where MGTs from new LLMs become available in stages and retraining the model from scratch is impractical due to computational or data storage constraints. To the best of our knowledge, we are the first to discuss how the detectors adapt to new MGTs in detection. We benchmark the performance of detectors when adapting to new LLMs and equip the detector with five different techniques to improve the performance. In summary, our contributions can be listed as follows:\n\u2022 We introduce MGT-Academic, a large-scale MGT dataset focused on academic writing, encompassing over 20M tokens and 73K samples across three academic domains: STEM, Humanities, and Social Sciences. We additionally provide an extensible code framework, which will be made publicly available, to efficiently benchmark existing MGT detectors in different tasks.\n\u2022 We conduct extensive experiments to reveal insights into the transferability of detectors across different domains (e.g., STEM, Humanities, and Social Sciences) and LLMs like GPT-40-mini and Llama-3.1-70b. We further evaluate the few-shot domain adaptation technique and find that it can achieve a 13.2% improvement in target domains/LLMs and a 3.7% improvement in the attribution task, offering a promising strategy to enhance transferability.\n\u2022 We introduce a new text attribution task where models adapt to new classes over time without (or with very limited) access to prior training data. We benchmark various detectors and implement several adaptation techniques, achieving approximately a 10% improvement in performance. Nevertheless, the remaining performance gap highlights the complexity of this task, underscoring the need for further investigation in the future."}, {"title": "2 Related Work", "content": "MGT Detection. Recent advancements in LLMs have empowered users to tackle a wide range of NLP tasks, demonstrating their versatility and superiority across numerous domains [30, 22, 21]. Exploiting LLMs is especially convenient in academic writing [38, 16], such as generating ideas, drafting articles, or refining content. However, the ease and convenience can be significantly abused, raising concerns about authenticity, as well as ethical questions regarding originality and over-dependence on AI-generated content [37, 3]. To prevent the misuse of MGT data, recent studies [1, 4, 6, 18, 8] have developed a variety of MGT detectors, which can be broadly categorized into metric-based and model-based methods. Metric-based methods [4, 18, 29] leverage proxy LLMs to extract features from processed text and train an additional classifier to model the relationship between features and labels. In contrast, model-based methods [10, 6] typically integrate a classification head into a BERT model and fine-tune the augmented model on supervised datasets. The detectors benchmarked in this paper are listed in Appendix E.\nSeveral efforts have aimed to benchmark the performance of MGT detectors. For example, MGTBench [8] provided a comprehensive evaluation framework for these detectors, which utilizes existing HWT datasets, including Essay, WP, and Reuters. M4GTBench [32] extended this by benchmarking performance on multilingual and multi-source datasets. Additionally, DetectRL [34] assessed detectors' robustness and generalization capabilities in the face of adversarial attacks. While existing studies emphasize transferability across datasets and LLMs in binary classification, they pay less attention to the generalization ability of detectors in attribution tasks.\nClass Incremental Learning. Class Incremental Learning (CIL) is a subset of continual learning and is critical for real-world applications where data becomes available in stages"}, {"title": "3 Construction of MGT-Academic", "content": "In this section, we describe the sources and principles for collecting both human and machine data.\nHuman Data. Since exploiting LLMs brings great convenience to academic usage, e.g., generating ideas, drafting articles, or refining content, we collect data in three academic domains, i.e., STEM, Social Science, and Humanity, where each domain contains different fine-grained fields. Each field consists of Wikipedia and academic texts, with the academic content collected from Arxiv (pre-print papers) or Project Gutenberg (published e-books), depending on the scenario. Specifically, the content is written in latex code and may include formulas. We keep the latex format as it can increase the diversity of the dataset in academic writing scenarios.\nMachine Data. We select five widely used LLMs to generate the MGTs. We prompt the LLM to be a wiki/paper/book editor and polish the given human text, which is consistent with the previous dataset [18, 32]."}, {"title": "3.2 MGT-Academic Analysis", "content": "We conduct further analysis of text length, embeddings, and keywords on MGT-Academic to provide more insights into understanding the MGT detection task, which is shown in Appendix C. We find that data from different domains and LLMs exhibit distinct distributions across all these aspects. These differences highlight the importance of studying the generalization ability of MGT detectors."}, {"title": "3.3 Code Framework", "content": "Our framework follows the factory design pattern and implements AutoDetector and AutoExperiment for abstraction, which is aligned with the approach used in Huggingface Transformers [33], the most widely used library in NLP community. It provides an easy-to-use and extendable code framework for the community and is publicly available."}, {"title": "4 Benchmarking MGT Detectors", "content": "We benchmark the performance of detectors in binary classification tasks, which predicts binary labels (human or machine) for the setting where training and testing data are in the same domain. Then, we benchmark the performance in text attribution tasks, which aims to tell whether the input text is human-written or generated by a specific LLM. Compared to the binary task, the text attribution task appears to be a more challenging multi-class classification task."}, {"title": "4.1 In-distribution Performance", "content": "Experiment Settings. For each domain, we first sample the same number of HWTs and MGTs from the corresponding domains, then randomly split them into the train/test dataset with a 80%/20% ratio. For the evaluation metric, we select the F1 score, which balances precision and recall and is robust against class imbalance. Regarding the detectors, we benchmark various detectors on MGT-Academic, covering both metric-based and model-based approaches.\nFor metric-based detectors, we evaluate Log-Likelihood, Entropy, Rank, Rank-GLTR [4], LRR [29], Fast-DetectGPT [2], and Binoculars [7]. We use Llama-2-7B as the default white-box model, determining an optimal threshold to maximize the F1-score for binary classification or training a logistic regression classifier for text attribution. For model-based detectors, we include RADAR [9], ChatGPT-D [6], Distill-BERT [28], and RoBERTa [14]. RADAR and ChatGPT-D use their officially released model weights, while Distill-BERT and ROBERTa are fine-tuned with classification heads on MGT-Academic. \nBinary Classification Task. First, we observe that supervised model-based detectors consistently outperform other methods, achieving F1 scores above 0.98. This advantage is largely due to the availability of extensive supervised training data, enabling these detectors to learn highly effective classification boundaries. Second, model-based detectors using released weights show comparatively weaker performance, as they were trained on smaller and less diverse datasets. This highlights the inherent challenges in the generalization ability of model-based detectors. Third, metric-based detectors show notable improvement over time, with state-of-the-art (SOTA) methods like Fast-DetectGPT and Binoculars approaching the performance of supervised model-based detectors. Despite their advancements, these state-of-the-art metric-based detectors show only moderate performance on outputs from GPT-40-mini and GPT-3.5. This may stem from the inability of the metric generators to effectively extract distinguishing features from these datasets.\nAttribution Task. As the attribution task is more challenging, the performance is overall lower than that of binary classification. The metric-based detectors show almost no capability for text attribution, performing at a level close to random guessing. The performance of model-based detectors is better but there is still space to improve. These findings highlight that text attribution is a critical yet underexplored task. The poor performance, particularly of zero-shot metric-based detectors, underscores the need for further research and development to address this significant gap in detection capabilities."}, {"title": "4.2 On the Generalization Ability in Domain Transferring", "content": "We conduct comprehensive experiments under domain-transferring settings to show the generalization ability of different kinds of detectors. First, for the binary classification task, we consider two cases where the domain or LLM during training and testing data are different. Second, for the attribution tasks, we only consider that the data domains during training and testing are different. Third, we implement a few-shot domain adaptation technique to mitigate the performance drop.\nExperiment Settings. For each domain, we first select the same number of HWTs and MGTs from the corresponding domains or LLMs, then randomly split them into the train/test dataset with an 80%/20% ratio. During training, the detectors get an optimal threshold on the data of the source domain/LLM, which will be used in the testing stage to predict the data of target domain/LLM."}, {"title": "4.3 On the Generalization to New LLMs", "content": "Since new LLMs with different characteristics are continuously released, we study a new attribution task where the detector would adapt to the new class introduced over time without (or with very limited) access to the original training data for earlier classes. To the best of our knowledge, we are the first to introduce this task into MGT detection.\nExperiment Settings. For simplicity, we mainly study the scenario where only one new LLM is introduced to the original detector (trained with HWT and four types of MGTs). We also consider the case where the two latest LLMs are introduced to the original detector . The objective of the original detector is to learn a five-class classifier. During the update stage, the training data only consists of MGTs from one new LLM and (optionally) with limited budgets of samples from previous LLMs and humans, while the testing data includes balanced data from all six classes. The new learning objective is to extend the previous five-class classifier into a six-class classifier.\nSince the metric-based detectors generally show poor performance close to random guess in attribution tasks, our study mainly focuses on the two supervised model-based detectors, i.e., DistilBert and RoBERTa, for this more challenging setting. At the update stage, we lower the learning rate to 1/4 of the original value and initialize a new classification head with one extra dimension to adapt to the newly introduced class. This approach is widely used by previous work [13] and ensures the detector incrementally adapts to new classes. We evaluate the performance of detectors using several widely-used adapting techniques, i.e., LwF [13], iCaRL [25], BiC [35], and combination."}, {"title": "5 Conclusion", "content": "In this work, we introduce MGT-Academic, a large-scale academic writing dataset comprising over 20M tokens and 73K samples across STEM, Humanities, and Social Sciences HWTs and MGTs generated by five different LLMs. We also build up an extensible code framework to benchmark MGT detectors in various tasks efficiently. Based on MGT-Academic and our code framework, we investigate the performance and generalization ability of MGT detectors and have several observations. First, we find that for binary classification tasks, the supervised model-based detectors generally outperform the metric-based detectors in both the in-distribution and domain-transferring settings. The superiority continues in the text attribution task in both training settings as well, in which they show more advantages. Second, we are the first to introduce a task in MGT detection, that adapts the detector to new LLMs, and benchmark the effect of several techniques. Despite the improvement, the results remain below the upper-bound performance. We hope our work can provide more insights into understanding the generalization ability of MGT detectors."}, {"title": "Limitation", "content": "Datasets and Methods. On the one hand, despite the collected data is from multiple resources, some specific subject such as Education or Chemistry only covers Wiki data. And the generation models cover only 5 popular LLMs, which limits the diversity of the dataset. On the other hand, we do not cover all different detectors. However, we try our best to benchmark the representative detectors in the community. We leave these limitations for the future continuous development of our open-source code framework and dataset.\nMetric-based Detectors for Text Attribution. Our results show that the metric-based detectors for text attribution only show performance slightly above random guesses but do not provide a solution to poor performance. This is an important yet underexplored topic in developing robust and comprehensive MGT detectors. We leave it as our future work for further exploration.\nAdapting Detectors to New LLMs. We are the first to introduce this setting in the MGT detection task and evaluate the result on detectors equipped with several adapting techniques. However, our evaluation shows that there is still a large performance gap between the current method and the performance upper bound (attribution task with full training data for all classes). We leave it as our future work for further exploration."}, {"title": "Ethic Consideration", "content": "This work focuses on MGT detection, which has significant ethical implications, particularly in combating the misuse of generative AI. The development of robust MGT detectors could mitigate ethical concerns such as plagiarism, misinformation, and unauthorized usage of generative AI, particularly in academia, creative industries, and journalism. Additionally, data used in this study are collected from publicly available datasets with appropriate attribution. No private or sensitive data is used to train or evaluate the models."}, {"title": "A Generation Prompts", "content": "Prompt Template for Arxiv Text\n<Background>:\nPlease act as an expert paper editor and revise a section of the paper to make it more fluent and elegant. Please only include the revised section in your answer. Here are the specific requirements:\n1. Enable readers to grasp the main points or essence of the paper quickly.\n2. Allow readers to understand the important information, analysis, and arguments throughout the entire paper.\n3. Help readers remember the key points of the paper.\n4. Please clearly state the innovative aspects of your research in the section, emphasizing your contributions.\n5. Use concise and clear language to describe your findings and results, making it easier for reviewers to understand the paper. Here is the original section of the paper:\n<text>: //to-be-polished text\nPrompt Template for Arxiv Text\n<Background>:\nPlease act as an expert paper editor and revise a section of the paper to make it more fluent and elegant. Please only include the revised section in your answer. Here are the specific requirements:\n1. Enable readers to grasp the main points or essence of the paper quickly.\n2. Allow readers to understand the important information, analysis, and arguments throughout the entire paper.\n3. Help readers remember the key points of the paper.\n4. Please clearly state the innovative aspects of your research in the section, emphasizing your contributions.\n5. Use concise and clear language to describe your findings and results, making it easier for reviewers to understand the paper. Here is the original section of the paper:\n<text>: //to-be-polished text\nPrompt Template for Gutendex Text\n<Background>:\nPlease act as an expert book editor and revise the book content from the perspective of a book editor to make it fluent and elegant.\n1. Clarity: Ensure that your writing is clear and easy to understand. Avoid jargon and complex language that may confuse the reader.\n2. Relevance: Make sure that the content you are writing is relevant to the topic at hand. Do not deviate from the main subject.\n3. Accuracy: Ensure that all the information you provide is accurate and up-to-date. This includes statistics, facts, and theories.\n4. Brevity: Keep your writing concise. Avoid unnecessary words or phrases that do not add value to the content. Here is the original book content:\n<text>: //to-be-polished text"}, {"title": "B Data Moderation Policy", "content": "In this section, we will introduce the data moderation policy for both Human and Machine splits to remove the data of poor quality and obvious identifiers.\nHuman Split. To ensure data quality, we apply a rigorous cleaning process to remove noise and irrelevant content. We discard texts with fewer than 50 tokens and ensure that all entries start and end with complete sentences, preserving their coherence and clarity. For wiki data, entries containing terms like \u201cISBN,\u201d \u201cPMID,\u201d \u201cdoi,\u201d \"vol.,\" \u201cp.,\u201d URLs, \u201cReferences,\u201d and \u201cExternal links\u201d are excluded, as these could act as identifiers (Table A2). For book data, texts containing Project Gutenberg license information are removed to avoid duplication. For arXiv data, we filtered out entries with excessive formatting symbols, specifically those with more than 500 instances of \"$$\", 150 \"&\", or 1000 \"\\\" (Table A2), to maintain readability.\nMachine Split. To moderate the machine generated data, we focus on removing the obvious identifiers for text detection. First, we remove the text of short length below 50 words (splited by space), which is usually produced by failed or incomplete API queries. Second, since every text is truncated to 2048 tokens, we start back-tracing from the last token until a period appears and then drop all content after the period to ensure the completeness of the text and avoid easy detection. Third, we customize different filtering rules for the keywords (Table A2) featuring machine generation. For generation identifiers (\"The revised content is:\"), we find the closest colon and remove all content before the colon. For special keywords, we drop the entire text if any of the listed keywords appears after removing the generation identifiers. For format symbols, we drop the entire text if there are more than 50 tabs (&) or equation ($). The remaining format symbols such as '**' and '##' are markdown tags so we drop the entire text if any of them appears."}, {"title": "C Full MGT-Academic Analysis", "content": "Embedding Distribution. In this part, we investigate the data distribution of MGT-Academic from the domain and LLM aspects. We utilize SentenceBert [26] to get the embeddings of each data and further adopt TSNE [31] to project the high-dimensional representations into two dimensions. Regarding the domain aspect, we fix the source LLM and project the data from different domains into Figure A1. As the projection result shows, we find that the distribution of data from different domains and LLMs varies a lot, which shows the necessity of studying the generalization ability of the MGT detectors."}, {"title": "D Code Framework", "content": "Despite the increasing number of benchmarks for MGT detection, there is no extensible standard library that offers out-of-the-box functionality. Most existing benchmark code framework require additional configuration and have inefficient architectures, which increase the cost of further development and limit broader adoption. To address these issues, we introduce a newly refactored code framework based on MGTBench [8]. Our framework follows the factory design pattern and implements AutoDetector and AutoExperiment for abstraction. This design is aligned with the approach used in Huggingface Transformers [33], which is the most widely used library in NLP community. Note that during the implementation, we leverage ChatGPT to assist the development of the framework and manually check the generated code to ensure the correctness.\nDetector. All the detectors follow the same hierarchy of abstraction that base class implements the detecting process. The base class follows the detectors' original implementation enabling the flexibility to easily inspect the inner workflow of each individual detector. Furthermore, we implement a unified API, i.e., AutoDetector, to automatically instantiate the user-specified detector and enable convenient detector loading and switching.\nExperiment. The experiment is designed to reflect the standard MGT detection pipeline: data processing, feature extraction (if any), and prediction making. We adopt a similar abstraction strategy as the detector that the base class implements the data processing and prediction analysis, leaving the specific predictions to different sub-classes. The unified API, AutoExperiment, is provided as well."}, {"title": "Data Source and Length", "content": "In this section, we present the length distribution of data across different domains and LLMs in Table A1. Generally, we collect 73,100 samples with 20,305,400 tokens. Notably, we observe that the token length of human-written text (HWT) is generally longer than that of machine-generated text (MGT). This disparity arises because some LLMs are less proficient at generating extended content. Additionally, within the STEM domain, certain subfields exhibit longer token lengths. This can be attributed to our data collection methodology, which primarily focuses on the main sections of research papers. These sections often include formulas and technical content, contributing to the increased token count."}, {"title": "Keyword Analysis", "content": "Based on the findings of [5], this study highlighted several critical keywords, such as significant, comprehensive, and enhance, which exhibit notable patterns in their usage across different sources. Following the approach, we organized these keywords and analyzed their weights under our specific task settings, comparing human-written text with outputs generated by models such as GPT-3.5, Llama-3, Mixtral, and Moonshot. We observe that human-written text heavily emphasizes impactful terms like significant and enhance, reflecting a formal academic style. GPT-3.5 and Llama-3 prioritize similar terms but exhibit a more balanced keyword usage. While Mixtral and Moonshot are closer to the human pattern in certain aspects, they have a relatively high proportion of descriptive terms such as effectively, which emphasises their practical tone."}, {"title": "E Detectors", "content": "For zero-shot detectors, metrics were obtained from the white-box model Llama2-7B-Instruct [30], unless stated otherwise. Fast-DetectGPT and Binoculars were evaluated using the optimal settings specified in their respective papers. For model-based detectors, DistilBERT and ROBERTa were fine-tuned with a learning rate of 5e-6, batch size of 64, 3 epochs, and a random seed of 3407. RADAR and ChatGPT-D used their officially released weights without additional fine-tuning. Details of detectors in binary classification and text attribution are provided in Appendix F.\nLog-Likehood [4]. A zero-shot method uses a language model to compute the log probability of each token in a text. A higher average log-likelihood suggests the text is more likely generated by an LLM.\nRank [4]. A zero-shot method calculates the absolute rank of each token in a text based on its previous context and determines the text's score by averaging these rank values. A smaller average rank score indicates a higher likelihood that the text is machine-generated.\nRank GLTR [4]. GLTR is designed to assist in labeling machine-generated text. We uses Test-2 features as suggested by Guo et al. [6], evaluating the fraction of words ranked within 10, 100, 1,000, and beyond.\nLRR [29]. The Log-Likelihood Log-Rank Ratio (LRR) combines Log-Likelihood and Log-Rank, with a higher LRR indicating a greater likelihood of text being machine-generated.\nEntropy [4]. A zero-shot method uses entropy to measure text randomness, with lower entropy indicating a higher likelihood of being LLM-generated, as human-written text shows greater unpredictability.\nFast-DetectGPT [2]. An optimized zero-shot detector improves DetectGPT [18] by replacing perturbation with efficient sampling. We followed the authors' optimal settings, using GPT-Neo-2.7b as the scoring model and GPT-J-6b as the reference model.\nBinoculars [7]. A zero-shot detection method uses two LLMs to compute the perplexity-to-cross-perplexity ratio. A lower score indicates the text is likely machine-generated. Following the authors' optimal settings, we used Falcon-7B-Instruct for PPL and Falcon-7B with Falcon-7B-Instruct for X-PPL.\nRADAR [9]. RADAR uses adversarial training between a paraphraser and a detector. We used the pre-trained ROBERTa detector from Hugging Face without additional training.\nChatGPT-D [6]. ChatGPT Detector distinguishes human-written from ChatGPT-generated texts by fine-tuning a ROBERTa model on the HC3 [6] dataset."}, {"title": "F Experimental Settings", "content": "In-distribution Experiment. For zero-shot detectors, we randomly selected 1,000 training samples to predict the metrics. The classification threshold was set to maximize the F1 score on the training set, and a classifier was trained using the same data. Note that GLTR produces vectors of four rank values and thus threshold-based classification is not applicable. For model-based detectors, we fine-tuned the model using at most 10,000 training samples. We used 2000 randomly selected data points for the testing set. Zero-shot detectors employ threshold-based classification and logistic regression for binary human-machine classification tasks.\nModel attribution tasks use SVM and logistic regression classifiers with default sklearn implementations and a linear kernel for SVM. Model-based detectors have their classification heads adjusted to match the number of classes. Fine-tuning was done with a learning rate of 5e-6, batch size of 64, 3 epochs, and a random seed of 3407.\nDomains and LLMs Transfer Experiment. For zero-shot detectors, we applied the threshold and classifier from the source domain directly to the target domain. For model-based detectors, we used models fine-tuned on the source domain and evaluated them on the target domain test data.\nClass Incremental Experiment. To train the original model, we use a similar setting as that in the model attribution task. We train the model for 2 epochs in this stage and set the learning rate to 5e-6, and the batch size to 64. To train the updated model, training data has the same number of data as each class had in the previous stage. We train the model for 1 epoch in this stage and set the learning rate to 2.5e-7 (1/4 of the original ale), and the batch size to 64. For the LwF technique, the regularization parameter is set to 0.2. To maintain the exampler in iCaRL, we set the cache size for each class to 100. The validation set in BiC is constructed by combining the data in exampler together. Specifically, since a small amount of old data is introduced in the training process of iCaRL and BiC, we adopt weighted cross entropy to avoid the side effect of data imbalance."}, {"title": "G Ablation Study of Zero-shot Detectors in In-distribution Experiment", "content": "We use GPT-2-medium [24] and Llama-2-7B-Instruct [30] as the metric-generator model for zero-shot detectors."}, {"title": "H Results of Domains and LLMs Transfer", "content": "Domain Transfer. Table A5 presents the full results of detector generalization across different domains.\nLLM Transfer. Table A6 presents the full results of detector generalization across different LLM generations.\nFull Mitigation Result. Figure A4, A5, and A6 illustrate the mitigation effects when transferring across domain topics. Similarly, Figure A7 shows the effects when transferring across different LLMs."}, {"title": "I Class Incremental Techniques", "content": "LwF [13]. LwF is a regularization-based method that relieves CF by distilling the logits from the previous model into the new one. The key idea is to preserve the logits of the previous model on old tasks during new tasks by adding a regularization term to the loss function:\n$L = L_{new}(x, y) + \\lambda L_{distill}(x)$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (1)"}, {"title": "J Class Incremental Experiments", "content": "Table A7 shows the performance of introducing two LLMs in the update stage. The results drop rapidly as the number of new LLMs increases."}]}