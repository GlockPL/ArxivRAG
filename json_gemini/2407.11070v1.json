{"title": "Optimal Defender Strategies for CAGE-2\nusing Causal Modeling and Tree Search", "authors": ["Kim Hammar", "Neil Dhir", "Rolf Stadler"], "abstract": "The CAGE-2 challenge is considered a standard\nbenchmark to compare methods for autonomous cyber defense.\nCurrent state-of-the-art methods evaluated against this bench-\nmark are based on model-free (offline) reinforcement learning,\nwhich does not provide provably optimal defender strategies. We\naddress this limitation and present a formal (causal) model of\nCAGE-2 together with a method that produces a provably optimal\ndefender strategy, which we call Causal Partially Observable\nMonte-Carlo Planning (C-POMCP). It has two key properties.\nFirst, it incorporates the causal structure of the target system,\ni.e., the causal relationships among the system variables. This\nstructure allows for a significant reduction of the search space\nof defender strategies. Second, it is an online method that\nupdates the defender strategy at each time step via tree search.\nEvaluations against the CAGE-2 benchmark show that C-POMCP\nachieves state-of-the-art performance with respect to effectiveness\nand is two orders of magnitude more efficient in computing time\nthan the closest competitor method.", "sections": [{"title": "I. INTRODUCTION", "content": "Domain experts have traditionally defined and updated an\norganization's security strategy. Though this approach can\noffer basic security for an organization's IT infrastructure, a\ngrowing concern is that infrastructure update cycles become\nshorter and attacks increase in sophistication. To address this\nchallenge, significant research efforts to automate the process\nof obtaining effective security strategies have started [1]\u2013[3]. A\ndriving factor behind this research is the development of eval-\nuation benchmarks, which allow researchers to compare the\nperformance of different methods. Presently, the most popular\nbenchmark is the Cyber Autonomy Gym for Experimentation\n2 (CAGE-2) [4], which involves defending a networked system\nagainst an Advanced Persistent Threat (APT), see Fig. 1.\nAt the time of writing, more than 30 methods have been\nevaluated against CAGE-2 [4]. Detailed descriptions of some\nmethods can be found in [5]-[21]. While good results have\nbeen obtained, key aspects remain unexplored. For example,\ncurrent methods are narrowly focused on offline reinforcement\nlearning and require a lengthy training phase to obtain effec-\ntive strategies. Further, these methods are model-free and do\nnot provide provably optimal strategies. In addition, current\nmethods provide limited ways to include domain expertise in\nthe learning process, though attempts have been made with\nreward shaping [7].\nIn this paper, we address the above limitations and use\nthe CAGE-2 scenario to illustrate and evaluate our solution\nmethod. First, we develop a formal (causal) model of CAGE-\n2, which allows us to define and prove the existence of an\noptimal defender strategy. This model is based on the source\ncode of CAGE-2 and is formalized as a Structural Causal\nModel (SCM) [22, Def 7.1.1]. We prove that this SCM is\nequivalent to a specific Partially Observed Markov Decision\nProcess (POMDP) [23, P.1]. Compared to the POMDP, our\nSCM offers a more expressive representation of the underlying\ncausal structure, allowing us to understand the causal effects\nof defender strategies [22, Def. 3.2.1].\nSecond, we design an online method that produces a prov-\nably optimal defender strategy, which we call Causal Partially\nObservable Monte-Carlo Planning (C-POMCP). The method\nhas two key properties: (1) it incorporates causal information\nof the target system in the form of a causal graph [22, Def.\n2.2.1], which allows us to prune the search space of defender\nstrategies; and (2) it is an online method that updates the\ndefender strategy at each time step via tree search.\nOur causal model represents one of many ways of formally"}, {"title": "II. RELATED WORK", "content": "Autonomous cyber defense is an active area of research\nthat uses concepts and methods from various fields (see Fig.\n2): reinforcement learning [3], [9], [11]\u2013[13], [16]\u2013[18], [20],\n[26]\u2013[38], control theory [2], [39]\u2013[46], causal inference [47]-\n[50], game theory [1], [51]\u2013[61], rule-based systems [62]-\n[67], large language models [19], [21], [68], [69], evolutionary\ncomputation [70]\u2013[75], and general optimization [76]\u2013[80].\nSeveral of these works use the CAGE-2 benchmark [4] for\nevaluating their methods, see for example [5]\u2013[13], [16]\u2013[21].\nThe best benchmark performance is achieved by those methods\nthat are based on deep reinforcement learning, where the cur-\nrent state-of-the-art methods use Proximal Policy Optimization\n(PPO) [81, Alg. 1][5].\nTo our knowledge, no prior work has provided a formal\nmodel of CAGE-2, nor considered tree search for finding\neffective defender strategies. Moreover, the only prior works\nthat use causal inference are [47]\u2013[50]. This paper differs from\nthem in two ways. First, the studies presented in [48]-[50]\nuse causality for analyzing the effects of attacks and coun-\ntermeasures but do not present methods for finding defender\nstrategies. Second, the method for finding defender strategies\nin [47] uses Bayesian optimization and is myopic, i.e., it does\nnot consider the future when selecting strategies. While this\napproach simplifies computations, the method is sub-optimal\nfor most practical scenarios. By contrast, our method is non-\nmyopic and produces optimal strategies (Thm. 4)."}, {"title": "III. THE CAGE-2 SCENARIO", "content": "The CAGE-2 scenario involves defending a networked sys-\ntem against APTS [4]. The operator of the system, which\nwe call the defender, takes measures to protect it against\nan attacker while providing services to a client population\n(see Fig. 1). The system is segmented into zones with nodes\n(servers and workstations) that run network services. Services\nare realized by workflows that are accessed by clients through\na gateway, which is also open to the attacker. The detailed\nsystem configuration can be found in Appendix D.\nThe attacker aims to intrude on the system and disrupt\nservice for clients. To achieve this goal, it can take five actions:\nscan the network to discover nodes; exploit a vulnerability to\ncompromise a node; perform a brute force attack to obtain\nlogin credentials to a node; escalate privileges on a compro-\nmised node to gain root access; and disrupt the service on a\ncompromised node.\nThe defender monitors the system through log files and\nnetwork statistics. It can make four types of interventions on\na node to prevent, detect, and respond to attacks: analyze the\nnode for a possible intrusion; start a decoy service on the\nnode; remove malware from the node; and restore the node to\na secure state, which temporarily disrupts its service. When\ndeciding between these interventions, the defender balances\ntwo conflicting objectives: maximize service utility towards\nits clients and minimize the cost of attacks."}, {"title": "IV. CAUSAL INFERENCE PRELIMINARIES", "content": "This section covers notation and provides an overview of\ncausal inference, which lays the foundation for the subsequent"}, {"title": "A. Structural Causal Models", "content": "A Structural Causal Model (SCM) is defined as\n$\\\u25b2 (U, V, F, P(U))$,\nwhere U is a set of exogenous random variables and V is a set\nof endogenous random variables [22, Def 7.1.1]. Within VUU\nwe distinguish between five subsets that may overlap: the set\nof manipulative variables X; non-manipulative N; observed\nO; latent L; and targets Y. (SCMS with latent variables are\ncalled \"partially observable\" [83, Def. 1].)"}, {"title": "B. Interventions and Causal Effect Identifiability", "content": "The operator do(X = x) represents an atomic intervention\nthat fixes a set of endogenous variable(s) X to constant\nvalue(s) x irrespective of the functions F [22, Def. 3.2.1].\nSimilarly, do(X = \u03c0(O)) represents a conditional interven-\ntion, whereby the function(s) {fi}iex are replaced with a\ndeterministic function of the observables. We call such a\nfunction an intervention strategy.\nThe standard way to estimate causal effects of interventions\n[22, Def. 3.2.1] is through controlled experiments [84]. In\npractice, however, experimentation can be costly and is often\nnot feasible in operational systems. This leads to the funda-\nmental question of whether causal effects can be estimated\nonly from observations. Such estimation can be performed\nusing Pearl's do-calculus, which is an axiomatic system for\nreplacing expressions that contain the do-operator with condi-\ntional probabilities [22, Thm. 3.4.1].\nIn case an SCM includes latent variables [22, Def. 2.3.2],\nthe question of identifiability arises:\nDefinition 1 (Causal effect identifiability [22, Def. 3.2.4]).\nThe causal effect [22, Def. 3.2.1] of do(X = \u03c0(O)) on Y is\nidentifiable from G if P(Y | do(X = \u03c0(0)), O) is uniquely\ncomputable from P(O) > 0 in every SCM conforming to G."}, {"title": "C. Automatic Intervention Control", "content": "The problem of finding a sequence of conditional interven-\ntions do(X1 = \u03c0(O)),..., do(X\u315c = \u03c0(O)) that maximizes\na target variable J can be formulated as a feedback control\nproblem [86], also known as a dynamic treatment regime\nproblem [87]. We say that such a problem is identifiable if\nthe effect on J caused by every intervention strategy is\nidentifiable:\nDefinition 2 (Control problem identifiability [22, Ch. 4.4]).\nA control problem with target J and time horizon T is identifi-\nable from G if $P(J | do(X1 = \u03c0(O)), ..., do(XT = \u03c0(O)))$ is\nidentifiable for each intervention strategy \u03c0 (Def. 1).\nGiven a control problem and a causal graph, we can derive\npossibly optimal minimal intervention sets (POMISS):\nDefinition 3 (POMIS, adapted from [88, Def. 3]).\nGiven a control problem with target J and a causal graph\nG, X C X is a POMIS if, for each SCM conforming to G,\nthere is no X' CX such that $E [J | do(X = x)] = E [J |$\ndo(X' = x')] and there exists an SCM such that\n$E_{\\pi^*} [J | do(X = x)] > E_{\\pi^*} [J | do(X' = x')]$,\nfor all \u03c0, X', and x', where \u03c0* is an optimal intervention\nstrategy satisfying $E_{\\pi^*} [J] \u2265 E\u03c0[J] \u2200\u03c0$.\nLet P denote the set of POMISS for a causal graph G.\nP for two example graphs are shown in Fig. 4. As can\nbe seen in Fig. 4.a, when G is Markovian, and all variables\nexcept the target are manipulative, the only POMIS is the set of\nparents of the target [88, Prop. 2]. When there are unobserved\nconfounders [22, Def. 6.2.1], however, P generally includes\nmany more sets, as shown in Fig. 4.b. An algorithm for\ncomputing P can be found in [88, Alg. 1]."}, {"title": "V. CAUSAL MODEL OF THE CAGE-2 SCENARIO", "content": "We model the CAGE-2 scenario by constructing an SCM\n(1) and formulate the benchmark problem as the problem of\nfinding an optimal intervention strategy for the defender."}, {"title": "A. Target System (Fig. 1)", "content": "We represent the physical topology of the target system as\na directed graph Gs \u2261 \u3008V,E), where nodes represent servers\nand workstations; edges represent network connectivity. Each\nnode i \u2208 V is (permanently) located in a zone zi \u2208 Z and\nhas three state variables: an intrusion state Ii,t, a service state\nSit, and a decoy state Dit.\nIi,t takes five values: U if the node is unknown to the\nattacker, K if it is known, S if it has been scanned, C if it is\ncompromised, and R if the attacker has root access (see Fig.\n5). Similarly, Si,t takes two values: 1 if the service provided by\nnode i is accessible for clients, 0 otherwise. Lastly, the decoy\nstate Dit is a vector (di,t,1,..., di,t,|D|), where di,t,j = 1 if\ndecoy j is active on node i, 0 otherwise. The set of decoys in\nCAGE-2 is available in Appendix D and is denoted by D. The\ninitial state of node i is (Ii,1 = U, Si,1 = 1, Di,1 = 0).\nA workflow graph Gw captures service dependencies among\nnodes (see Appendix D). A directed edge i \u2192 j in Gw means\nthat the service provided by node i is used by node j."}, {"title": "B. Attacker", "content": "During each time step, the attacker performs an action At,\nwhich targets a single node or all nodes in a zone (in case of a\nscan action). The action is determined by an attacker strategy\n\u03a0\u0391. It consists of four components At \u2261 (at, Vt, Pt, Tt): at is\nthe action type, Vt is the vulnerability, Pt \u2208 {(U)ser, (R)oot}\nis the privileges obtained by exploiting the vulnerability, and\nTt is the target, which can be either a single node i \u2208 V or a\nzone z \u2208 Z.\nThere are five attack actions: (S)can, which scans the\nvulnerabilities of a node; (E)xploit, which attempts to exploit a\nvulnerability of a node; (P)rivilege escalation, which escalates\nprivileges of a compromised node; (I)mpact, which stops\nthe service on a compromised node; and (D)iscover, which\ndiscovers the nodes in a zone. These actions have the following\ncausal effects on the intrusion state Iit and the service state\nSi,t [22, Def. 3.2.1].\n$A_{t-1} = f_A(\\pi_A, \\{I_{i,t-1}\\}_{i \\in V})$\n$I_{i,t} = f_I(I_{i,t-1}, A_{t-1}, D_{i,t}, E_t)$  \n0 K if $T_{t-1} = z_i, A_{t-1} = D$\n0 K if $T_{t-1} \\in pa(i)_{G_W}, a_{t-1} = P$\n0 S if $T_{t-1} = i, a_{t-1} = S$\n0 C if $T_{t-1} = i, a_{t-1} = E, P_{t-1} = U, D_{i,V_i,t} = 0, E_t = 1$\n0 R if $T_{t-1} = i, a_{t-1} = E, P_{t-1} = R, D_{i,V_i,t} = 0, E_t = 1$\n0 R if $T_{t-1} = i, A_{t-1} = P$\n0 $I_{i,t-1}$ otherwise\n$S_{i,t} = f_S(A_{t-1}, S_{i,t-1})$\n0 if $T_{t-1} = i, a_{t-1} = I$\n$S_{i,t-1}$ otherwise,\nwhere Et is a binary random variable and $P(E_t = 1)$ is the\nprobability that an exploit at time t is successful (see Fig. 5).\nThe first two cases in (5) capture the transition U \u2192 K,\nwhich occurs when the attacker discovers the zone of node i."}, {"title": "C. Observations and Clients", "content": "The defender knows the decoy state Dit and the service\nstate Si,t, but can not observe the intrusion state Iit nor\nthe attacker action At. Instead of Ii,t and At, the defender\nobserves Zi,t, which represents network activity at node i.\nLike the intrusion state Ii,t (5), the activity Zi,t takes five\nvalues: (U)nknown, (K)nown, (S)canned, (C)ompromised, and\n(R)oot. The value of Zi,t is influenced both by attacker actions\nand by clients requesting service, which we express as\n$Z_{i,t} = f_{Z,i} (C_t, A_{t-1}, W_{i,t}),$\nwhere Wi,t \u2208 N is a noise variable and Ct represents the\nnumber of clients requesting service at time t, which is\ndetermined as\n$C_t = f_C(C_{t-1}, A_t, D_t) \\approx max [0, C_{t-1} + A_t \u2013 D_t]$,\nwhere At and Dt are the number of clients that arrive and\ndepart in the time interval [t \u2013 1, t], respectively."}, {"title": "D. Defender Objective", "content": "The defender balances two objectives: maintain services to\nits clients and minimize the cost of intrusion. In CAGE-2, this\nbi-objective corresponds to maximizing\n$J = f_S({R_t | 1 \u2264 t \u2264 T}) = \\sum_{t=1}^T \\gamma^{t-1}R_t$\n$R_t = f_R(\\{I_{i,t}, S_{i,t}\\}_{i\\in V}) \u2013 intervention & downtime cost \u2013 $q_t + \\sum_{z_i} (S_{i,t} \\\nintrusion cost \\\n-\\sum_{z_i} (S_{i,t} - 1) -\\beta_{L_{i,t},z_i}$,\nwhere Rt is the reward at time t; qt \u2265 0 is the intervention\ncost; zi \u2265 0 is the cost of service disruption in zone zi;\nBlitzi\u2265 0 is the cost of intrusion in zone zi; \u03b3\u2208 [0, 1] is a\ndiscount factor; and T is the time horizon. The configuration\nof qt, \u03c8zi, and Blitzi for the target infrastructure in CAGE-2\n(Fig. 1) can be found in Appendix C."}, {"title": "E. Defender Interventions", "content": "During each time step, the defender performs an inter-\nvention that targets a single node. The defender can make\nfour types of interventions: analyze the node for a possible\nintrusion, start a decoy service, remove malware, and restore\nthe node to a secure state, which temporarily disrupts its\nservice. We model these interventions as follows.\ndo(Zi,t = Ii,t) analyze; \ndo(Di,j,t = 1) decoy; \ndo(Ii,t = S) if Ii,t\u22121 = C remove; \ndo(Di,t = 0, Ii,t = S) if Ii,t\u22121 \u2208 {C, R} restore; \ndo(0) none.\nNote that Dit remains constant if no interventions occur, i.e.,\n$D_{i,t} = f_D(D_{i,t-1}) D_{i,t-1}$.\nWhen selecting interventions, the defender considers the\nhistory Ht, which we define as\n$H_t \\equiv (V_1, do(X_1), O_2, do(X_2), . . ., do(X_{t-1}), O_t)$\n$O_t \\equiv \\{D_{i,t}, S_{i,t}, Z_{i,t}, C_t | i \\in V\\}$,\nwhere do(Xt) is a shorthand for do(Xt = xt) and Vt is\nthe set of endogeneous variables at time t (defined below).\nThe intervention at time t can thus be expressed as do(Xt =\n\u03c0\u03c1(ht), where \u3160\u014b is a defender strategy.\nRemark 2. The fact that the defender remembers the history\nHt (11) means that it has perfect recall [89, Def. 7]."}, {"title": "F. A Structural Causal Model of CAGE-2", "content": "The variables and the causal functions (4)-(9) described\nabove determine the SCM (1) defined in (M1). Notable proper-\nties of (M1) are a) the causal graph is acyclic (see Fig. 6); b)"}, {"title": "G. The Defender Problem in CAGE-2", "content": "Given (M1) and the defender objective (9), the problem for\nthe defender can be stated as follows.\nProblem 1 (Optimal defender strategy in CAGE-2 (M1)).\nmaximize $E_D [J | V_1]$\nsubject to do(X = \u03c0\u03c1(ht))  \n\u03a0\u0391 ~ \u03a1(\u03a0\u0391) \nAt ~ P(At), Dt ~ P(Dt)\nEt ~P(Et), Wi,t ~ P(Wi,t) \nAt = fA(\u03c0\u0391, {Ii,t}iev)\nDi,t = fD(Di,t-1) \nIi,t = f1(Ii,t-1, At\u22121, Di,t, Et) \nZi,t = fz,i(Ct, At\u22121, Wi,t) \nCt = fc(Ct-1, At, Dt)\nSi,t=fs(At-1, Si,t-1)\nRt = fR({Ii,t, Si,t}iev)\nJ = fj({Rt}t=1,...,T),\nwhere (13b) defines the interventions; (13d)-(13e) capture\nthe distribution of U; and (13f)\u2013(13m) define F.\nWe say that a defender strategy w is optimal if it solves\nProb. 1. This problem is well-defined in the following sense.\nTheorem 1. Assuming Ct, A, V, qt, B1i,t,z, \u03c8z, are finite, and\nTis finite or y < 1, then there exists an optimal deterministic\ndefender strategy \u03c0\u0463. If T = \u221e, then there exists a \u3160 that\nis stationary."}, {"title": "VI. CAUSAL PARTIALLY OBSERVABLE\nMONTE-CARLO PLANNING (C-POMCP)", "content": "In this section, we present C-POMCP, an online method for\nobtaining an optimal defender strategy \u3160 for Prob. 1. The\nmethod involves three consecutive actions that are performed\nat each time step t (see Fig. 7).\nThe first action uses the observation of and a particle filter\nto compute the defender's belief bt in the form of a probability\ndistribution over the latent variables L in (M1). The second\naction constructs a search tree of possible future histories hk\n(11), which is initialized with a root node that represents the\ncurrent history ht. Each edge extends this history by either\nan observation or an intervention: if hk+1 is a child node of\nhk, then either hk+1 = (hk, ok+1) or hk+1 = (hk, do(Xk =\nk)). C-POMCP then prunes the tree by excluding histories\nthat contain interventions that do not belong to a POMIS (Def.\n3). The third action uses the belief bt and the pruned tree to\nperform Monte-Carlo tree search, which involves estimating\nthe reward J (9) through simulations. Once the search has been\ncompleted, the intervention from the root node that leads to\nthe highest value of J (9) is returned. The pseudocode of C-\nPOMCP is listed in Alg. 1, and the main components of the\nmethod are described below."}, {"title": "A. Particle Filtering to Estimate Latent Variables", "content": "The particle filter is a method for state estimation in\npartially observed dynamical systems [95]. Since (M1) can\nbe formulated as such a system (Thm. 1), we use the particle\nfilter to estimate the values of the latent variables L in (M1),\ne.g., the intrusion state Ii,t (5).\nWe define the defender's belief state as\n$b_t(\\sigma_t) = P(\\Sigma_t = \\sigma_t | h_t)$\n$\\frac{\\text{(Bayes)}}{=}  \\frac{P(o_t | \\sigma_t, h_{t-1})P(\\sigma_t | h_{t-1})}{\\sum_{\\sigma_t}P(o_t | \\sigma_t, h_{t-1})P(\\sigma_t | h_{t-1})} = \\frac{\\text{(Markov)}}{=} \\frac{P(o_t | \\sigma_t)P(\\sigma_t | do(X_t = x_t), h_{t-1})}{\\sum_{\\sigma_t}P(o_t | \\sigma_t)P(\\sigma_t | do(X_t = x_t), h_{t-1})}$\n$\\sum_{\\sigma_{t-1}}P(\\sigma_t | \\sigma_{t-1}, do(x_t = x_t))b_{t-1}(t-1),$ (14)\nwhere ht is the history (11), \u03c3t is a realization of \u2211t (see\nThm. 1), and \u03b7 is a normalizing constant. The sum in (14) is\nover all possible realizations of Et-1.\nThe computational complexity of (14) is O(|dom(\u03a3)|2),\nwhich grows quadratically with the size of the state space\nand exponentially with the number of state variables. For this\nreason, the particle filter approximates (14) by representing bt\nby a set of M sample states (particles) Pt = {\u03c3t,1,..., \u03c3t,M}\n[96]. These particles are sampled recursively as\n$\\mathbb{P}_t \\overset{\\sim}{=} \\{\\sigma^{(i)}_t \\overset{\\sim}{=} P (\\sigma_t | \\sigma^{(i)}_{t-1}, do (X_{t-1} = x_{t-1})) \\}_i^M$ \n$\\mathbb{P}_t \\overset{\\sim}{=} \\{\\sigma^{(i)}_t \\times P(o_t | \\sigma^{(i)}_t) \\}_i^M$\n(15a)-(15b) focus the particle set to regions of the state\nspace with a high probability of generating the latest observa-\ntion ot [95]. This ensures that the belief state induced by the\nparticles converges to (14) when M \u2192 \u221e, as stated below.\nTheorem 3. Let $\\hat{b}_t(\\sigma_t) = \\sum_{i=1}^M \\mathbb{1}_{ \\sigma^{(i)} = \\sigma_t }$, then\n$\\lim_{M\\to\\infty} \\hat{b}_t \\to b_t$ almost surely  \\forall t$.\nThis is a standard result in particle filtering. The proof is\ngiven in Appendix A."}, {"title": "B. Causal Pruning of the Search Tree", "content": "We use the causal graph G (Fig. 6) to prune the search tree\nby excluding histories hk (11) that contain interventions that\ndo not belong to a POMIS P (Def. 3). For example, when\nt = T-1, then Zi,t (7) and J (9) are d-separated in G [22,\nDef. 1.2.3]. This means that the intervention do(Zi,t = Ii,t)\n(10a) has no causal effect on J and thus Zi,t \u2209 P.\nBy restricting the possible interventions at time t to the set\nof POMISS P, the number of interventions in the search tree\nis reduced by a factor of\n$\\frac{\\sum_{X \\in \\mathbb{P}} |dom(X)|}{\\prod_{X \\in X_t} \\sum_{X \\in \\mathbb{P}}  |dom(X)|}$,\nwhere Xt is the set of manipulative variables at time t (M1).\nHence, even if only a small subset of interventions does not\nbelong to an POMIS, a significant reduction in the search tree\nsize can be expected (see Fig. 8). Unfortunately, computing\nP is generally intractable, as stated below.\nProposition 1. Computing P (Def. 3) is PSPACE-hard.\nProof. We prove the PSPACE-hardness by reduction to the\nproblem of solving a POMDP, which is PSPACE-hard [97, Thm.\n6]. Let x be an instance of the problem of computing P (Def.\n3). Finding a solution to x involves checking (3) for each\nXt \u2208 P. This means that a solution to x allows constructing\nan optimal solution to Prob. 1. By Thm 1, such a solution also\nprovides a solution to a POMDP.\nGiven the impracticality of computing P (Prop. 1), we\napproximate P as follows. First, we reduce the causal graph\nto a subgraph G[Ut_1UUtUVt_1UVt] [22, Def. 7.1.2]. We\nthen remove all variables in the subgraph whose values are\nuniquely determined by b (15). Subsequently, we add a node\nto the subgraph that represents the target J (9), whose causal\nparents [22, Def. 1.2.1] are determined using a base strategy\n, which can be chosen freely. It can, for example, be based\non heuristics or be designed by a domain expert. Finally, we\ncompute a POMIS for the reduced graph using [88, Alg. 1].\nRemark 4. Since [88, Alg. 1] is sound and complete [88,\nThm. 9], the approximation described above is exact when\nthe base strategy is optimal.\nWhen applying the above procedure to the CAGE-2 scenario,\nwe identify the following types of defender interventions that\nare never included in a POMIS: (i) interventions that start\ndecoys that are already running; (ii) defensive interventions\non nodes that are not compromised according to b (15); and\n(iii) forensic and deceptive interventions on nodes that are\ncompromised according to b (15).\nRemark 5. The pruning of the search tree based on the\nPOMISS (Def. 3) occurs during the construction of the tree.\nThe complete search tree is generally too large to construct."}, {"title": "C. Monte-Carlo Tree Search", "content": "Given the particle filter (14) and the POMIS (Def. 3)", "1": ".", "9)": "i) it\nselects a path from the root to a leaf node using the tree policy\ndescribed below; (ii) it expands the tree by adding children to\nthe leaf", "variables": "the average\ncumulative reward J(hk) (9) of simulations on the subtree\nemanating from the node, and the visit count N(hk) \u2265 1,\nwhich is incremented whenever the node is visited during the\nsearch. Using these variables, we implement the tree policy\nby selecting nodes that maximize the upper confidence bound\n$J(h_k) + c \\sqrt{\\frac{\\ln N(h_{k-1})}{N(h_k)}}$,\nwhere c > 0 controls the exploration-exploitation trade-off.\nRollout. The initial state of a rollout simulation is sampled\nfrom the belief state bk (15), and the intervention at each\ntime step is selected using the base strategy \u3160 (\u00a7VI-B). The\nsimulation executes for a depth of dr, after which the reward\nfor the remainder of the simulation is estimated using a base\nvalue function J\uc2be. Like the base strategy, this function can be\nchosen freely. It can, for example, be obtained through offline\nreinforcement learning. After the simulation has completed,\nthe discounted sum of the rewards R1, Rt,... R&R (9) and J\uc288\nis used to update J(hk).\nConvergence. The process of running simulations and extend-\ning the search tree continues for a search time st, after which\nthe intervention that leads to the maximal value of J (9) is\nreturned, i.e.,\ndo(X\u2081 = x\u2081) \u2208 arg max J((ht, do(Xt = xt))).\ndo(Xt=xt)\nWe can express this search procedure as\ndo(Xt = xt) \u2190 I(ht, bt, \u3160, ST, I, G, P), (18)\nwhere I is a tree search operator.\nTheorem 4. Under the assumptions made in Thm. 1 and\nfurther assuming that the POMIS computation is exact (\u00a7VI-B),\nM\u2192\u221e, ST \u2192 \u221e, T < \u221e, and"}]}