[{"title": "II. RELATED WORK", "authors": ["Kim Hammar", "Neil Dhir", "Rolf Stadler"], "abstract": "The CAGE-2 challenge is considered a standard benchmark to compare methods for autonomous cyber defense. Current state-of-the-art methods evaluated against this benchmark are based on model-free (offline) reinforcement learning, which does not provide provably optimal defender strategies. We address this limitation and present a formal (causal) model of CAGE-2 together with a method that produces a provably optimal defender strategy, which we call Causal Partially Observable Monte-Carlo Planning (C-POMCP). It has two key properties. First, it incorporates the causal structure of the target system, i.e., the causal relationships among the system variables. This structure allows for a significant reduction of the search space of defender strategies. Second, it is an online method that updates the defender strategy at each time step via tree search. Evaluations against the CAGE-2 benchmark show that C-POMCP achieves state-of-the-art performance with respect to effectiveness and is two orders of magnitude more efficient in computing time than the closest competitor method.", "sections": [{"title": "I. INTRODUCTION", "content": "Domain experts have traditionally defined and updated an organization's security strategy. Though this approach can offer basic security for an organization's IT infrastructure, a growing concern is that infrastructure update cycles become shorter and attacks increase in sophistication. To address this challenge, significant research efforts to automate the process of obtaining effective security strategies have started [1]\u2013[3]. A driving factor behind this research is the development of evaluation benchmarks, which allow researchers to compare the performance of different methods. Presently, the most popular benchmark is the Cyber Autonomy Gym for Experimentation 2 (CAGE-2) [4], which involves defending a networked system against an Advanced Persistent Threat (APT), see Fig. 1.\nAt the time of writing, more than 30 methods have been evaluated against CAGE-2 [4]. Detailed descriptions of some methods can be found in [5]-[21]. While good results have been obtained, key aspects remain unexplored. For example, current methods are narrowly focused on offline reinforcement learning and require a lengthy training phase to obtain effective strategies. Further, these methods are model-free and do not provide provably optimal strategies. In addition, current methods provide limited ways to include domain expertise in the learning process, though attempts have been made with reward shaping [7].\nIn this paper, we address the above limitations and use the CAGE-2 scenario to illustrate and evaluate our solution method. First, we develop a formal (causal) model of CAGE-2, which allows us to define and prove the existence of an optimal defender strategy. This model is based on the source code of CAGE-2 and is formalized as a Structural Causal Model (SCM) [22, Def 7.1.1]. We prove that this SCM is equivalent to a specific Partially Observed Markov Decision Process (POMDP) [23, P.1]. Compared to the POMDP, our SCM offers a more expressive representation of the underlying causal structure, allowing us to understand the causal effects of defender strategies [22, Def. 3.2.1].\nSecond, we design an online method that produces a prov- ably optimal defender strategy, which we call Causal Partially Observable Monte-Carlo Planning (C-POMCP). The method has two key properties: (1) it incorporates causal information of the target system in the form of a causal graph [22, Def. 2.2.1], which allows us to prune the search space of defender strategies; and (2) it is an online method that updates the defender strategy at each time step via tree search.\nOur causal model represents one of many ways of formally"}, {"title": "II. RELATED WORK", "content": "Autonomous cyber defense is an active area of research that uses concepts and methods from various fields (see Fig. 2): reinforcement learning [3], [9], [11]\u2013[13], [16]\u2013[18], [20], [26]-[38], control theory [2], [39]\u2013[46], causal inference [47]- [50], game theory [1], [51]\u2013[61], rule-based systems [62]- [67], large language models [19], [21], [68], [69], evolutionary computation [70]\u2013[75], and general optimization [76]\u2013[80]. Several of these works use the CAGE-2 benchmark [4] for evaluating their methods, see for example [5]\u2013[13], [16]\u2013[21]. The best benchmark performance is achieved by those methods that are based on deep reinforcement learning, where the cur- rent state-of-the-art methods use Proximal Policy Optimization (PPO) [81, Alg. 1][5].\nTo our knowledge, no prior work has provided a formal model of CAGE-2, nor considered tree search for finding effective defender strategies. Moreover, the only prior works that use causal inference are [47]\u2013[50]. This paper differs from them in two ways. First, the studies presented in [48]-[50] use causality for analyzing the effects of attacks and countermeasures but do not present methods for finding defender strategies. Second, the method for finding defender strategies in [47] uses Bayesian optimization and is myopic, i.e., it does not consider the future when selecting strategies. While this approach simplifies computations, the method is sub-optimal for most practical scenarios. By contrast, our method is non- myopic and produces optimal strategies (Thm. 4)."}, {"title": "III. THE CAGE-2 SCENARIO", "content": "The CAGE-2 scenario involves defending a networked system against APTS [4]. The operator of the system, which we call the defender, takes measures to protect it against an attacker while providing services to a client population (see Fig. 1). The system is segmented into zones with nodes (servers and workstations) that run network services. Services are realized by workflows that are accessed by clients through a gateway, which is also open to the attacker. The detailed system configuration can be found in Appendix D.\nThe attacker aims to intrude on the system and disrupt service for clients. To achieve this goal, it can take five actions: scan the network to discover nodes; exploit a vulnerability to compromise a node; perform a brute force attack to obtain login credentials to a node; escalate privileges on a compromised node to gain root access; and disrupt the service on a compromised node.\nThe defender monitors the system through log files and network statistics. It can make four types of interventions on a node to prevent, detect, and respond to attacks: analyze the node for a possible intrusion; start a decoy service on the node; remove malware from the node; and restore the node to a secure state, which temporarily disrupts its service. When deciding between these interventions, the defender balances two conflicting objectives: maximize service utility towards its clients and minimize the cost of attacks."}, {"title": "IV. CAUSAL INFERENCE PRELIMINARIES", "content": "This section covers notation and provides an overview of causal inference, which lays the foundation for the subsequent section, where we deduce a causal model of the CAGE-2 scenario.\nNotation. Random variables are denoted by upper-case letters (e.g., X), their values by lower-case letters (e.g., x), and their domains by dom(\u00b7). X || Y means that X and Y are independent. P is a probability measure. (Since we focus on countable sample spaces, the construction of the underlying probability space is standard.) The expectation of f with respect to X is written as \\(E_X[f]\\), and \\(x \\sim f\\) means that x is sampled from f. (As the sample spaces are countable, no question of the existence of \\(E_X[f]\\) will arise.) If f includes many random variables that depend on \u03c0, we simply write \\(E_\\pi[f]\\). We use P(x) as a shorthand for P(X = x). Random vectors (or sets) and their values are written in bold upper-case and lower- case letters, respectively (e.g., X and x). A (column) vector is written as \\((x_1, x_2, ...)\\) and a 1-dimensional vector as \\((x_1, )\\). 1 is the indicator function. pa(X)G, ch(X)G, an(X), and de(X)G denote parents, children, ancestors, and descendants of node X in a graph G. G[V] denotes the subgraph obtained by restricting G to the nodes in V. Further notation is listed in Table 1."}, {"title": "A. Structural Causal Models", "content": "A Structural Causal Model (SCM) is defined as\n\\(\\Delta = (U, V, F, P(U)),\\)\n(1)\nwhere U is a set of exogenous random variables and V is a set of endogenous random variables [22, Def 7.1.1]. Within \\(V \\cup U\\) we distinguish between five subsets that may overlap: the set of manipulative variables X; non-manipulative N; observed O; latent L; and targets Y. (SCMS with latent variables are called \"partially observable\" [83, Def. 1].)"}, {"title": "B. Interventions and Causal Effect Identifiability", "content": "The operator do(X = x) represents an atomic intervention that fixes a set of endogenous variable(s) X to constant value(s) x irrespective of the functions F [22, Def. 3.2.1]. Similarly, \\(do(X = \\pi(O))\\) represents a conditional interven- tion, whereby the function(s) \\({f_i\\}_{i \\in X}\\) are replaced with a deterministic function of the observables. We call such a function an intervention strategy.\nThe standard way to estimate causal effects of interventions [22, Def. 3.2.1] is through controlled experiments [84]. In practice, however, experimentation can be costly and is often not feasible in operational systems. This leads to the funda- mental question of whether causal effects can be estimated only from observations. Such estimation can be performed using Pearl's do-calculus, which is an axiomatic system for replacing expressions that contain the do-operator with condi- tional probabilities [22, Thm. 3.4.1].\nIn case an SCM includes latent variables [22, Def. 2.3.2], the question of identifiability arises:\nDefinition 1 (Causal effect identifiability [22, Def. 3.2.4]). The causal effect [22, Def. 3.2.1] of \\(do(X = \\pi(O))\\) on Y is identifiable from G if \\(P(Y \\mid do(X = \\pi(0)), O)\\) is uniquely computable from P(O) > 0 in every SCM conforming to G."}, {"title": "C. Automatic Intervention Control", "content": "The problem of finding a sequence of conditional interventions \\(do(X_1 = \\pi(O)), ..., do(X_T = \\pi(O))\\) that maximizes a target variable J can be formulated as a feedback control problem [86], also known as a dynamic treatment regime problem [87]. We say that such a problem is identifiable if the effect on J caused by every intervention strategy is identifiable:\nDefinition 2 (Control problem identifiability [22, Ch. 4.4]). A control problem with target J and time horizon T is identifi- able from G if \\(P(J \\mid do(X_1 = \\pi(O)), ..., do(X_T = \\pi(O))\\)) is identifiable for each intervention strategy \u03c0 (Def. 1).\nGiven a control problem and a causal graph, we can derive possibly optimal minimal intervention sets (POMISS):\nDefinition 3 (POMIS, adapted from [88, Def. 3]). Given a control problem with target J and a causal graph G, \\(X \\subset \\mathcal{X}\\) is a POMIS if, for each SCM conforming to G, there is no \\(X' \\subset \\mathcal{X}\\) such that \\(E[J \\mid do(X = x)] = E[J \\mid do(X' = x')]\\) and there exists an SCM such that\n\\(E_{\\pi^*}[J \\mid do(X = x)] > E_{\\pi^*}[J \\mid do(X' = x')],\\)\n(3)\nfor all \u03c0, X', and x', where \\(\\pi^*\\) is an optimal intervention strategy satisfying \\(E_{\\pi^*}[J] \\geq E_{\\pi}[J] \\forall \\pi\\).\nLet P denote the set of POMISS for a causal graph G. P for two example graphs are shown in Fig. 4. As can be seen in Fig. 4.a, when G is Markovian, and all variables except the target are manipulative, the only POMIS is the set of parents of the target [88, Prop. 2]. When there are unobserved confounders [22, Def. 6.2.1], however, P generally includes many more sets, as shown in Fig. 4.b. An algorithm for computing P can be found in [88, Alg. 1]."}, {"title": "V. CAUSAL MODEL OF THE CAGE-2 SCENARIO", "content": "We model the CAGE-2 scenario by constructing an SCM (1) and formulate the benchmark problem as the problem of finding an optimal intervention strategy for the defender."}, {"title": "A. Target System (Fig. 1)", "content": "We represent the physical topology of the target system as a directed graph \\(G_s \\equiv \\langle \\mathcal{V}, \\mathcal{E} \\rangle\\), where nodes represent servers and workstations; edges represent network connectivity. Each node \\(i \\in \\mathcal{V}\\) is (permanently) located in a zone \\(z_i \\in \\mathcal{Z}\\) and has three state variables: an intrusion state \\(I_{i,t}\\), a service state \\(S_{i,t}\\), and a decoy state \\(D_{i,t}\\).\n\\(I_{i,t}\\) takes five values: U if the node is unknown to the attacker, K if it is known, S if it has been scanned, C if it is compromised, and R if the attacker has root access (see Fig. 5). Similarly, \\(S_{i,t}\\) takes two values: 1 if the service provided by node i is accessible for clients, 0 otherwise. Lastly, the decoy state \\(D_{i,t}\\) is a vector \\((d_{i,t,1},..., d_{i,t,|D|})\\), where \\(d_{i,t,j} = 1\\) if decoy j is active on node i, 0 otherwise. The set of decoys in CAGE-2 is available in Appendix D and is denoted by D. The initial state of node i is \\((I_{i,1} = U, S_{i,1} = 1, D_{i,1} = 0)\\).\nA workflow graph \\(G_w\\) captures service dependencies among nodes (see Appendix D). A directed edge \\(i \\rightarrow j\\) in \\(G_w\\) means that the service provided by node i is used by node j."}, {"title": "B. Attacker", "content": "During each time step, the attacker performs an action \\(A_t\\), which targets a single node or all nodes in a zone (in case of a scan action). The action is determined by an attacker strategy \\(\\Pi_A\\). It consists of four components \\(A_t \\equiv (a_t, V_t, P_t, T_t)\\): \\(a_t\\) is the action type, \\(V_t\\) is the vulnerability, \\(P_t \\in \\{(U)ser, (R)oot\\}\\) is the privileges obtained by exploiting the vulnerability, and \\(T_t\\) is the target, which can be either a single node \\(i \\in \\mathcal{V}\\) or a zone \\(z \\in \\mathcal{Z}\\).\nThere are five attack actions: (S)can, which scans the vulnerabilities of a node; (E)xploit, which attempts to exploit a vulnerability of a node; (P)rivilege escalation, which escalates privileges of a compromised node; (I)mpact, which stops the service on a compromised node; and (D)iscover, which discovers the nodes in a zone. These actions have the following causal effects on the intrusion state \\(I_{i,t}\\) and the service state \\(S_{i,t}\\) [22, Def. 3.2.1].\n\\(A_{t-1} = f_A(\\Pi_A, \\{I_{i,t-1}\\}_{i \\in \\mathcal{V}})\\)\n(4)\n\\(I_{i,t} = f_I(I_{i,t-1}, A_{t-1}, D_{i,t}, E_t) \\triangleq\\)\n(5)\n\\begin{cases}\nK & \\text{if } T_{t-1} = z_i, A_{t-1} = D \\\\\nK & \\text{if } T_{t-1} \\in pa(i)_{G_W}, a_{t-1} = P \\\\\nS & \\text{if } T_{t-1} = i, a_{t-1} = S \\\\\nC & \\text{if } T_{t-1} = i, a_{t-1} = E, P_{t-1} = U, D_{i,V_i,t} = 0, E_t = 1 \\\\\nR & \\text{if } T_{t-1} = i, a_{t-1} = E, P_{t-1} = R, D_{i,V_i,t} = 0, E_t = 1 \\\\\nR & \\text{if } T_{t-1} = i, A_{t-1} = P \\\\\nI_{i,t-1} & \\text{otherwise}\n\\end{cases}\n\\(S_{i,t} = f_S(A_{t-1}, S_{i,t-1}) \\triangleq\\)\n(6)\n\\begin{cases}\n0 & \\text{if } T_{t-1} = i, a_{t-1} = I \\\\\nS_{i,t-1} & \\text{otherwise,}\n\\end{cases}\nwhere \\(E_t\\) is a binary random variable and \\(P(E_t = 1)\\) is the probability that an exploit at time t is successful (see Fig. 5). The first two cases in (5) capture the transition U \u2192 K, which occurs when the attacker discovers the zone of node i."}, {"title": "C. Observations and Clients", "content": "The defender knows the decoy state \\(D_{i,t}\\) and the service state \\(S_{i,t}\\), but can not observe the intrusion state \\(I_{i,t}\\) nor the attacker action \\(A_t\\). Instead of \\(I_{i,t}\\) and \\(A_t\\), the defender observes \\(Z_{i,t}\\), which represents network activity at node i.\nLike the intrusion state \\(I_{i,t}\\) (5), the activity \\(Z_{i,t}\\) takes five values: (U)nknown, (K)nown, (S)canned, (C)ompromised, and (R)oot. The value of \\(Z_{i,t}\\) is influenced both by attacker actions and by clients requesting service, which we express as\n\\(Z_{i,t} = f_{Z,i}(C_t, A_{t-1}, W_{i,t}),\\)\n(7)\nwhere \\(W_{i,t} \\in \\mathbb{N}\\) is a noise variable and \\(C_t\\) represents the number of clients requesting service at time t, which is determined as\n\\(C_t = f_C(C_{t-1}, A_t, D_t) \\triangleq max [0, C_{t-1} + A_t \u2013 D_t],\\)\n(8)\nwhere \\(A_t\\) and \\(D_t\\) are the number of clients that arrive and depart in the time interval [t \u2013 1, t], respectively."}, {"title": "D. Defender Objective", "content": "The defender balances two objectives: maintain services to its clients and minimize the cost of intrusion. In CAGE-2, this bi-objective corresponds to maximizing\n\\(J = f_S(\\lbrace R_t \\mid 1 \\leq t \\leq T \\rbrace) = \\sum_{t=1}^{T} \\gamma^{t-1} R_t\\)\n(9)\n\\(R_t = f_R(\\lbrace I_{i,t}, S_{i,t} \\rbrace_{i \\in \\mathcal{V}}) - \\underbrace{q_t + \\sum_{z_i \\in \\mathcal{Z}} \\psi_{z_i} (S_{i,t} - 1) - \\sum_{i \\in \\mathcal{V}} \\beta_{I_{i,t}, z_i} I_{i,t}}_{\\text{intervention & downtime cost}} \\underbrace{\\sum_{z_i \\in \\mathcal{Z}} (S_{i,t} - 1) - \\sum_{i \\in \\mathcal{V}} \\beta_{I_{i,t}, z_i} I_{i,t}}_{\\text{intrusion cost}}\nwhere \\(R_t\\) is the reward at time t; \\(q_t \\geq 0\\) is the intervention cost; \\(\\psi_{z_i} \\geq 0\\) is the cost of service disruption in zone \\(z_i\\); \\(\\beta_{I_{i,t}, z_i} \\geq 0\\) is the cost of intrusion in zone \\(z_i\\); \\(\\gamma \\in [0, 1]\\) is a discount factor; and T is the time horizon. The configuration of \\(q_t\\), \\(\\psi_{z_i}\\), and \\(\\beta_{I_{i,t}, z_i}\\) for the target infrastructure in CAGE-2 (Fig. 1) can be found in Appendix C."}, {"title": "E. Defender Interventions", "content": "During each time step, the defender performs an inter- vention that targets a single node. The defender can make four types of interventions: analyze the node for a possible intrusion, start a decoy service, remove malware, and restore the node to a secure state, which temporarily disrupts its service. We model these interventions as follows.\n\\(do(Z_{i,t} = I_{i,t})\\) analyze; (10a)\n\\(do(D_{i,j,t} = 1)\\) decoy; (10b)\n\\(do(I_{i,t} = S) \\text{ if } I_{i,t-1} = C)\\) remove; (10c)\n\\(do(D_{i,t} = 0, I_{i,t} = S) \\text{ if } I_{i,t-1} \\in \\{C, R\\}\\) restore; (10d)\n\\(do(\\emptyset)\\) none. (10e)\nNote that \\(D_{i,t}\\) remains constant if no interventions occur, i.e., \\(D_{i,t} = f_D(D_{i,t-1}) \\triangleq D_{i,t-1}\\).\nWhen selecting interventions, the defender considers the history \\(H_t\\), which we define as\n\\(H_t \\triangleq (V_1, do(X_1), O_2, do(X_2), ..., do(X_{t-1}), O_t)\\)\n(11)\n\\(O_t \\triangleq \\{D_{i,t}, S_{i,t}, Z_{i,t}, C_t \\mid i \\in \\mathcal{V}\\},\\)\n(12)\nwhere do(X) is a shorthand for \\(do(X_t = x_t)\\) and \\(V_t\\) is the set of endogeneous variables at time t (defined below). The intervention at time t can thus be expressed as \\(do(X_t = \\pi_D(h_t))\\), where \\(\\pi_D\\) is a defender strategy.\nRemark 2. The fact that the defender remembers the history \\(H_t\\) (11) means that it has perfect recall [89, Def. 7]."}, {"title": "F. A Structural Causal Model of CAGE-2", "content": "The variables and the causal functions (4)-(9) described above determine the SCM (1) defined in (M1). Notable proper- ties of (M1) are a) the causal graph is acyclic (see Fig. 6); b) the model is stationary; c) the model is semi-Markov [22, Thm. 1.4.1]; d) the exogeneous variables are jointly independent; and e) P(Vt) is Markov relative to G (2) [22, Thm. 1.2.7].\nThe size of the causal graph G associated with (M1) grows linearly with the time horizon T and with the number of nodes in the target system |V|. A summary of G is shown in Fig. 6."}, {"title": "G. The Defender Problem in CAGE-2", "content": "Given (M1) and the defender objective (9), the problem for the defender can be stated as follows.\nProblem 1 (Optimal defender strategy in CAGE-2 (M1)).\n\\begin{aligned}\n& \\underset{D}{\\text{maximize}} & E_D [J \\mid V_1] \\\\\n& \\text{subject to} & do(\\mathcal{X} = \\pi_D(h_t)) && \\forall t \\\\\n& & \\Pi_A \\sim P(\\Pi_A) && \\forall t \\\\\n& & A_t \\sim P(A_t), D_t \\sim P(D_t) && \\forall t \\\\\n& & E_t \\sim P(E_t), W_{i,t} \\sim P(W_{i,t}) && \\forall t, i \\\\\n& & A_t = f_A(\\Pi_A, \\{I_{i,t}\\}_{i \\in \\mathcal{V}}) && \\forall t, i \\\\\n& & D_{i,t} = f_D(D_{i,t-1}) && \\forall t,i \\\\\n& & I_{i,t} = f_I(I_{i,t-1}, A_{t-1}, D_{i,t}, E_t) && \\forall t,i \\\\\n& & Z_{i,t} = f_{Z,i}(C_t, A_{t-1}, W_{i,t}) && \\forall t, i \\\\\n& & C_t = f_C(C_{t-1}, A_t, D_t) && \\forall t \\\\\n& & S_{i,t} = f_S(A_{t-1}, S_{i,t-1}) && \\forall t, i \\\\\n& & R_t = f_R(\\lbrace I_{i,t}, S_{i,t} \\rbrace_{i \\in \\mathcal{V}}) && \\forall t \\\\\n& & J = f_J(\\lbrace R_t \\rbrace_{t=1,...,T}), && (13m)\n\\end{aligned}\nwhere (13b) defines the interventions; (13d)-(13e) capture the distribution of U; and (13f)\u2013(13m) define F.\nWe say that a defender strategy \u03c0D is optimal if it solves Prob. 1. This problem is well-defined in the following sense.\nTheorem 1. Assuming \\(C_t, A, \\mathcal{V}, q_t, \\beta_{I_{i,t},z}, \\psi_z\\) are finite, and T is finite or \\(\\gamma < 1\\), then there exists an optimal deterministic defender strategy \\(\\pi_D^*\\). If T = \u221e, then there exists a \\(\\pi_D^*\\) that is stationary."}, {"title": "Proof.", "content": "For notational convenience, let \\(S_t \\triangleq \\{S_{i,t}\\}_{i \\in \\mathcal{V}}, I_t \\triangleq \\{I_{i,t}\\}_{i \\in \\mathcal{V}}, D_t \\triangleq \\{D_{i,t}\\}_{i \\in \\mathcal{V}}, W_t = \\{W_{i,t}\\}_{i \\in \\mathcal{V}},\\) and \\(Z_t \\triangleq \\{Z_{i,t}\\}_{i \\in \\mathcal{V}}\\). We break down the proof into the following steps.\n1) \\(\\Sigma_t \\triangleq (I_t, D_t, C_t, A_{t-1}, \\Pi_A, S_{t-1})\\) is Markovian.\nPROOF:\n\\begin{aligned}\nP(\\Sigma_{t+1} \\mid \\Sigma_1,..., \\Sigma_t) &= P(A_t \\mid \\Pi_A, I_t)P(A_{t+1})P(D_{t+1})\\times\\\\\n&P(C_{t+1} \\mid A_{t+1}, D_{t+1}, C_t)P(E_{t+1})P(S_{t+1} \\mid S_t, A_t)\\times\\\\\n&P(I_{t+1} \\mid I_t, A_t, D_{t+1}, E_{t+1}) = P(\\Sigma_{t+1} \\mid \\Sigma_t).\n\\end{aligned}\n2) \\((O_t, R_t \\amalg \\{\\mathcal{V}_t\\}_{t=1,...,t} \\mid \\Sigma_t)\\).\nPROOF:\n\\begin{aligned}\nP(O_t, R_t \\mid \\{\\mathcal{V}_t\\}_{t=1,...,t}) &= P(Z_t \\mid C_t, A_{t-1}, W_t)\\times\\\\\n&P(W_t)P(S_t \\mid S_{t-1}, A_{t-1}) P(R_t \\mid I_t, S_t) = P(O_t, R_t \\mid \\Sigma_t).\n\\end{aligned}\n3) dom(Ot), dom(\u03a3t), and Rt are finite.\nPROOF: Follows from the theorem assumptions.\n4) Each defender strategy \u03c0D induces a well-defined probabil- ity measure over the random sequence \\((\\Sigma_t, O_t)_{t > 1}\\).\nPROOF: 3) implies that the sample spaces of \\((\\Sigma_t, O_t)\\) and \\((\\Sigma_t, O_t)_{t > 1}\\) are measurable and countable, respectively. Further, the fact that G is acyclic implies that the interventional distributions induced by \\((do(X = x_t))_{t > 1}\\) are well-defined [22, Ch. 3]. Consequently, the statement follows from the Ionescu-Tulcea extension theorem [92].\n5) \\(P(\\Sigma_{t+1} \\mid \\Sigma_t), P(O_t \\mid \\Sigma_t)\\), and \\(P(R_t \\mid \\Sigma_t)\\) are stationary.\nPROOF: Follows by stationarity of (M1).\nStatements 1-5 imply that Prob. 1 defines a finite, stationary, and partially observed Markov decision process (POMDP) with bounded rewards. The theorem thus follows from standard results in Markov decision theory, see [93, Thm. 7.6.1]."}, {"title": "Theorem 2.", "content": "Problem 1 is not identifiable (Def. 2).\nProof. To prove non-identifiability, it is sufficient to present two sets of causal functions \\(F', F''\\) that induce identical distributions over the observables but have different causal effects (Def. 2) [22] [94, Lem. 1]. For simplicity, consider T = 2 and \\(|\\mathcal{V}| = 1\\). In this case J = R (9). Let\n\\begin{aligned}\nF' \\triangleq \\big\\{ &f_R(S, I) = \\mathbb{1}_{I=R}, f_I(A, E, D) = R, \\\\\n& f_S(S, A) = 1, f_Z, f_C, f_J, f_A, f_D \\big\\},\n\\end{aligned}\nwhere \\(\\{f_Z, f_C, f_J, f_A, f_D\\}\\) are defined arbitrarily. Now let \\(F''\\) be equivalent to \\(F'\\) except for \\(f_R(S, I) \\equiv S\\). Clearly P(O) is the same with both \\(F'\\) and \\(F''\\). However, \\(P(J = 0 \\mid do(I = S)) = 1\\) with \\(F'\\) but \\(P(J = S \\mid do(I = S)) = 1\\) with \\(F''\\).\nTheorem 2 states that causal effects of defender interven- tions (10) can not be identified from observations. This state- ment is obvious in hindsight but has important ramifications. It implies that to evaluate a defender strategy \u03c0D, the defender"}, {"title": "VI. CAUSAL PARTIALLY OBSERVABLE MONTE-CARLO PLANNING (C-POMCP)", "content": "In this section", "intervention": "if \\(h_{k+1"}, "is a child node of \\(h_k\\), then either \\(h_{k+1} = (h_k, o_{k+1})\\) or \\(h_{k+1} = (h_k, do(X_k = x_k))\\). C-POMCP then prunes```json\n{\n  \"title\": \"Optimal Defender Strategies for CAGE-2 using Causal Modeling and Tree Search\",\n  \"authors\": [\n    \"Kim Hammar\",\n    \"Neil Dhir\",\n    \"Rolf Stadler\"\n  ],\n  \"abstract\":", "The CAGE-2 challenge is considered a standard benchmark to compare methods for autonomous cyber defense. Current state-of-the-art methods evaluated against this benchmark are based on model-free (offline) reinforcement learning, which does not provide provably optimal defender strategies. We address this limitation and present a formal (causal) model of CAGE-2 together with a method that produces a provably optimal defender strategy, which we call Causal Partially Observable Monte-Carlo Planning (C-POMCP). It has two key properties. First, it incorporates the causal structure of the target system, i.e., the causal relationships among the system variables. This structure allows for a significant reduction of the search space of defender strategies. Second, it is an online method that updates the defender strategy at each time step via tree search. Evaluations against the CAGE-2 benchmark show that C-POMCP achieves state-of-the-art performance with respect to effectiveness and is two orders of magnitude more efficient in computing time than the closest competitor method.", "sections\": [\n    {\n      \"title\": \"I. INTRODUCTION", "content\": \"Domain experts have traditionally defined and updated an organization's security strategy. Though this approach can offer basic security for an organization's IT infrastructure, a growing concern is that infrastructure update cycles become shorter and attacks increase in sophistication. To address this challenge, significant research efforts to automate the process of obtaining effective security strategies have started [1]\u2013[3]. A driving factor behind this research is the development of evaluation benchmarks, which allow researchers to compare the performance of different methods. Presently, the most popular benchmark is the Cyber Autonomy Gym for Experimentation 2 (CAGE-2) [4], which involves defending a networked system against an Advanced Persistent Threat (APT), see Fig. 1.\nAt the time of writing, more than 30 methods have been evaluated against CAGE-2 [4]. Detailed descriptions of some methods can be found in [5]-[21]. While good results have been obtained, key aspects remain unexplored. For example, current methods are narrowly focused on offline reinforcement learning and require a lengthy training phase to obtain effective strategies. Further, these methods are model-free and do not provide provably optimal strategies. In addition, current methods provide limited ways to include domain expertise in the learning process, though attempts have been made with reward shaping [7].\nIn this paper, we address the above limitations and use the CAGE-2 scenario to illustrate and evaluate our solution method. First, we develop a formal (causal) model of CAGE-2, which allows us to define and prove the existence of an optimal defender strategy. This model is based on the source code of CAGE-2 and is formalized as a Structural Causal Model (SCM) [22, Def 7.1.1]. We prove that this SCM is equivalent to a specific Partially Observed Markov Decision Process (POMDP) [23, P.1]. Compared to the POMDP, our SCM offers a more expressive representation of the underlying causal structure, allowing us to understand the causal effects of defender strategies [22, Def. 3.2.1].\nSecond, we design an online method that produces a prov- ably optimal defender strategy, which we call Causal Partially Observable Monte-Carlo Planning (C-POMCP). The method has two key properties: (1) it incorporates causal information of the target system in the form of a causal graph [22, Def. 2.2.1], which allows us to prune the search space of defender strategies; and (2) it is an online method that updates the defender strategy at each time step via tree search.\nOur causal model represents one of many ways of formally"], "content": "Autonomous cyber defense is an active area of research that uses concepts and methods from various fields (see Fig. 2): reinforcement learning [3], [9], [11]\u2013[13], [16]\u2013[18], [20], [26]-[38], control theory [2], [39]\u2013[46], causal inference [47]- [50], game theory [1], [51]\u2013[61], rule-based systems [62]- [67], large language models [19], [21], [68], [69], evolutionary computation [70]\u2013[75], and general optimization [76]\u2013[80]. Several of these works use the CAGE-2 benchmark [4] for evaluating their methods, see for example [5]\u2013[13], [16]\u2013[21]. The best benchmark performance is achieved by those methods that are based on deep reinforcement learning, where the cur- rent state-of-the-art methods use Proximal Policy Optimization (PPO) [81, Alg. 1][5].\nTo our knowledge, no prior work has provided a formal model of CAGE-2, nor considered tree search for finding effective defender strategies. Moreover, the only prior works that use causal inference are [47]\u2013[50]. This paper differs from them in two ways. First, the studies presented in [48]-[50] use causality for analyzing the effects of attacks and countermeasures but do not present methods for finding defender strategies. Second, the method for finding defender strategies in [47] uses Bayesian optimization and is myopic, i.e., it does not consider the future when selecting strategies. While this approach simplifies computations, the method is sub-optimal for most practical scenarios. By contrast, our method is non- myopic and produces optimal strategies (Thm. 4)."}, {"title": "III. THE CAGE-2 SCENARIO", "content": "The CAGE-2 scenario involves defending a networked system against APTS [4]. The operator of the system, which we call the defender, takes measures to protect it against an attacker while providing services to a client population (see Fig. 1). The system is segmented into zones with nodes (servers and workstations) that run network services. Services are realized by workflows that are accessed by clients through a gateway, which is also open to the attacker. The detailed system configuration can be found in Appendix D.\nThe attacker aims to intrude on the system and disrupt service for clients. To achieve this goal, it can take five actions: scan the network to discover nodes; exploit a vulnerability to compromise a node; perform a brute force attack to obtain login credentials to a node; escalate privileges on a compromised node to gain root access; and disrupt the service on a compromised node.\nThe defender monitors the system through log files and network statistics. It can make four types of interventions on a node to prevent, detect, and respond to attacks: analyze the node for a possible intrusion; start a decoy service on the node; remove malware from the node; and restore the node to a secure state, which temporarily disrupts its service. When deciding between these interventions, the defender balances two conflicting objectives: maximize service utility towards its clients and minimize the cost of attacks."}, {"title": "IV. CAUSAL INFERENCE PRELIMINARIES", "content": "This section covers notation and provides an overview of causal inference, which lays the foundation for the subsequent section, where we deduce a causal model of the CAGE-2 scenario.\nNotation. Random variables are denoted by upper-case letters (e.g., X), their values by lower-case letters (e.g., x), and their domains by dom(\u00b7). X || Y means that X and Y are independent. P is a probability measure. (Since we focus on countable sample spaces, the construction of the underlying probability space is standard.) The expectation of f with respect to X is written as \\(E_X[f]\\), and \\(x \\sim f\\) means that x is sampled from f. (As the sample spaces are countable, no question of the existence of \\(E_X[f]\\) will arise.) If f includes many random variables that depend on \u03c0, we simply write \\(E_\\pi[f]\\). We use P(x) as a shorthand for P(X = x). Random vectors (or sets) and their values are written in bold upper-case and lower- case letters, respectively (e.g., X and x). A (column) vector is written as \\((x_1, x_2, ...)\\) and a 1-dimensional vector as \\((x_1, )\\). 1 is the indicator function. pa(X)G, ch(X)G, an(X), and de(X)G denote parents, children, ancestors, and descendants of node X in a graph G. G[V] denotes the subgraph obtained by restricting G to the nodes in V. Further notation is listed in Table 1."}, {"title": "A. Structural Causal Models", "content": "A Structural Causal Model (SCM) is defined as\n\\(\\Delta = (U, V, F, P(U)),\\)\n(1)\nwhere U is a set of exogenous random variables and V is a set of endogenous random variables [22, Def 7.1.1]. Within \\(V \\cup U\\) we distinguish between five subsets that may overlap: the set of manipulative variables X; non-manipulative N; observed O; latent L; and targets Y. (SCMS with latent variables are called \"partially observable\" [83, Def. 1].)"}, {"title": "B. Interventions and Causal Effect Identifiability", "content": "The operator do(X = x) represents an atomic intervention that fixes a set of endogenous variable(s) X to constant value(s) x irrespective of the functions F [22, Def. 3.2.1]. Similarly, \\(do(X = \\pi(O))\\) represents a conditional interven- tion, whereby the function(s) \\({f_i\\}_{i \\in X}\\) are replaced with a deterministic function of the observables. We call such a function an intervention strategy.\nThe standard way to estimate causal effects of interventions [22, Def. 3.2.1] is through controlled experiments [84]. In practice, however, experimentation can be costly and is often not feasible in operational systems. This leads to the funda- mental question of whether causal effects can be estimated only from observations. Such estimation can be performed using Pearl's do-calculus, which is an axiomatic system for replacing expressions that contain the do-operator with condi- tional probabilities [22, Thm. 3.4.1].\nIn case an SCM includes latent variables [22, Def. 2.3.2], the question of identifiability arises:\nDefinition 1 (Causal effect identifiability [22, Def. 3.2.4]). The causal effect [22, Def. 3.2.1] of \\(do(X = \\pi(O))\\) on Y is identifiable from G if \\(P(Y \\mid do(X = \\pi(0)), O)\\) is uniquely computable from P(O) > 0 in every SCM conforming to G."}, {"title": "C. Automatic Intervention Control", "content": "The problem of finding a sequence of conditional interventions \\(do(X_1 = \\pi(O)), ..., do(X_T = \\pi(O))\\) that maximizes a target variable J can be formulated as a feedback control problem [86], also known as a dynamic treatment regime problem [87]. We say that such a problem is identifiable if the effect on J caused by every intervention strategy is identifiable:\nDefinition 2 (Control problem identifiability [22, Ch. 4.4]). A control problem with target J and time horizon T is identifi- able from G if \\(P(J \\mid do(X_1 = \\pi(O)), ..., do(X_T = \\pi(O))\\)) is identifiable for each intervention strategy \u03c0 (Def. 1).\nGiven a control problem and a causal graph, we can derive possibly optimal minimal intervention sets (POMISS):\nDefinition 3 (POMIS, adapted from [88, Def. 3]). Given a control problem with target J and a causal graph G, \\(X \\subset \\mathcal{X}\\) is a POMIS if, for each SCM conforming to G, there is no \\(X' \\subset \\mathcal{X}\\) such that \\(E[J \\mid do(X = x)] = E[J \\mid do(X' = x')]\\) and there exists an SCM such that\n\\(E_{\\pi^*}[J \\mid do(X = x)] > E_{\\pi^*}[J \\mid do(X' = x')],\\)\n(3)\nfor all \u03c0, X', and x', where \\(\\pi^*\\) is an optimal intervention strategy satisfying \\(E_{\\pi^*}[J] \\geq E_{\\pi}[J] \\forall \\pi\\).\nLet P denote the set of POMISS for a causal graph G. P for two example graphs are shown in Fig. 4. As can be seen in Fig. 4.a, when G is Markovian, and all variables except the target are manipulative, the only POMIS is the set of parents of the target [88, Prop. 2]. When there are unobserved confounders [22, Def. 6.2.1], however, P generally includes many more sets, as shown in Fig. 4.b. An algorithm for computing P can be found in [88, Alg. 1]."}, {"title": "V. CAUSAL MODEL OF THE CAGE-2 SCENARIO", "content": "We model the CAGE-2 scenario by constructing an SCM (1) and formulate the benchmark problem as the problem of finding an optimal intervention strategy for the defender."}, {"title": "A. Target System (Fig. 1)", "content": "We represent the physical topology of the target system as a directed graph \\(G_s \\equiv \\langle \\mathcal{V}, \\mathcal{E} \\rangle\\), where nodes represent servers and workstations; edges represent network connectivity. Each node \\(i \\in \\mathcal{V}\\) is (permanently) located in a zone \\(z_i \\in \\mathcal{Z}\\) and has three state variables: an intrusion state \\(I_{i,t}\\), a service state \\(S_{i,t}\\), and a decoy state \\(D_{i,t}\\).\n\\(I_{i,t}\\) takes five values: U if the node is unknown to the attacker, K if it is known, S if it has been scanned, C if it is compromised, and R if the attacker has root access (see Fig. 5). Similarly, \\(S_{i,t}\\) takes two values: 1 if the service provided by node i is accessible for clients, 0 otherwise. Lastly, the decoy state \\(D_{i,t}\\) is a vector \\((d_{i,t,1},..., d_{i,t,|D|})\\), where \\(d_{i,t,j} = 1\\) if decoy j is active on node i, 0 otherwise. The set of decoys in CAGE-2 is available in Appendix D and is denoted by D. The initial state of node i is \\((I_{i,1} = U, S_{i,1} = 1, D_{i,1} = 0)\\).\nA workflow graph \\(G_w\\) captures service dependencies among nodes (see Appendix D). A directed edge \\(i \\rightarrow j\\) in \\(G_w\\) means that the service provided by node i is used by node j."}, {"title": "B. Attacker", "content": "During each time step, the attacker performs an action \\(A_t\\), which targets a single node or all nodes in a zone (in case of a scan action). The action is determined by an attacker strategy \\(\\Pi_A\\). It consists of four components \\(A_t \\equiv (a_t, V_t, P_t, T_t)\\): \\(a_t\\) is the action type, \\(V_t\\) is the vulnerability, \\(P_t \\in \\{(U)ser, (R)oot\\}\\) is the privileges obtained by exploiting the vulnerability, and \\(T_t\\) is the target, which can be either a single node \\(i \\in \\mathcal{V}\\) or a zone \\(z \\in \\mathcal{Z}\\).\nThere are five attack actions: (S)can, which scans the vulnerabilities of a node; (E)xploit, which attempts to exploit a vulnerability of a node; (P)rivilege escalation, which escalates privileges of a compromised node; (I)mpact, which stops the service on a compromised node; and (D)iscover, which discovers the nodes in a zone. These actions have the following causal effects on the intrusion state \\(I_{i,t}\\) and the service state \\(S_{i,t}\\) [22, Def. 3.2.1].\n\\(A_{t-1} = f_A(\\Pi_A, \\{I_{i,t-1}\\}_{i \\in \\mathcal{V}})\\)\n(4)\n\\(I_{i,t} = f_I(I_{i,t-1}, A_{t-1}, D_{i,t}, E_t) \\triangleq\\)\n(5)\n\\begin{cases}\nK & \\text{if } T_{t-1} = z_i, A_{t-1} = D \\\\\nK & \\text{if } T_{t-1} \\in pa(i)_{G_W}, a_{t-1} = P \\\\\nS & \\text{if } T_{t-1} = i, a_{t-1} = S \\\\\nC & \\text{if } T_{t-1} = i, a_{t-1} = E, P_{t-1} = U, D_{i,V_i,t} = 0, E_t = 1 \\\\\nR & \\text{if } T_{t-1} = i, a_{t-1} = E, P_{t-1} = R, D_{i,V_i,t} = 0, E_t = 1 \\\\\nR & \\text{if } T_{t-1} = i, A_{t-1} = P \\\\\nI_{i,t-1} & \\text{otherwise}\n\\end{cases}\n\\(S_{i,t} = f_S(A_{t-1}, S_{i,t-1}) \\triangleq\\)\n(6)\n\\begin{cases}\n0 & \\text{if } T_{t-1} = i, a_{t-1} = I \\\\\nS_{i,t-1} & \\text{otherwise,}\n\\end{cases}\nwhere \\(E_t\\) is a binary random variable and \\(P(E_t = 1)\\) is the probability that an exploit at time t is successful (see Fig. 5). The first two cases in (5) capture the transition U \u2192 K, which occurs when the attacker discovers the zone of node i."}, {"title": "C. Observations and Clients", "content": "The defender knows the decoy state \\(D_{i,t}\\) and the service state \\(S_{i,t}\\), but can not observe the intrusion state \\(I_{i,t}\\) nor the attacker action \\(A_t\\). Instead of \\(I_{i,t}\\) and \\(A_t\\), the defender observes \\(Z_{i,t}\\), which represents network activity at node i.\nLike the intrusion state \\(I_{i,t}\\) (5), the activity \\(Z_{i,t}\\) takes five values: (U)nknown, (K)nown, (S)canned, (C)ompromised, and (R)oot. The value of \\(Z_{i,t}\\) is influenced both by attacker actions and by clients requesting service, which we express as\n\\(Z_{i,t} = f_{Z,i}(C_t, A_{t-1}, W_{i,t}),\\)\n(7)\nwhere \\(W_{i,t} \\in \\mathbb{N}\\) is a noise variable and \\(C_t\\) represents the number of clients requesting service at time t, which is determined as\n\\(C_t = f_C(C_{t-1}, A_t, D_t) \\triangleq max [0, C_{t-1} + A_t \u2013 D_t],\\)\n(8)\nwhere \\(A_t\\) and \\(D_t\\) are the number of clients that arrive and depart in the time interval [t \u2013 1, t], respectively."}, {"title": "D. Defender Objective", "content": "The defender balances two objectives: maintain services to its clients and minimize the cost of intrusion. In CAGE-2, this bi-objective corresponds to maximizing\n\\(J = f_S(\\lbrace R_t \\mid 1 \\leq t \\leq T \\rbrace) = \\sum_{t=1}^{T} \\gamma^{t-1} R_t\\)\n(9)\n\\(R_t = f_R(\\lbrace I_{i,t}, S_{i,t} \\rbrace_{i \\in \\mathcal{V}}) - \\underbrace{q_t + \\sum_{z_i \\in \\mathcal{Z}} \\psi_{z_i} (S_{i,t} - 1) - \\sum_{i \\in \\mathcal{V}} \\beta_{I_{i,t}, z_i} I_{i,t}}_{\\text{intervention & downtime cost}} \\underbrace{\\sum_{z_i \\in \\mathcal{Z}} (S_{i,t} - 1) - \\sum_{i \\in \\mathcal{V}} \\beta_{I_{i,t}, z_i} I_{i,t}}_{\\text{intrusion cost}}\nwhere \\(R_t\\) is the reward at time t; \\(q_t \\geq 0\\) is the intervention cost; \\(\\psi_{z_i} \\geq 0\\) is the cost of service disruption in zone \\(z_i\\); \\(\\beta_{I_{i,t}, z_i} \\geq 0\\) is the cost of intrusion in zone \\(z_i\\); \\(\\gamma \\in [0, 1]\\) is a discount factor; and T is the time horizon. The configuration of \\(q_t\\), \\(\\psi_{z_i}\\), and \\(\\beta_{I_{i,t}, z_i}\\) for the target infrastructure in CAGE-2 (Fig. 1) can be found in Appendix C."}, {"title": "E. Defender Interventions", "content": "During each time step, the defender performs an inter- vention that targets a single node. The defender can make four types of interventions: analyze the node for a possible intrusion, start a decoy service, remove malware, and restore the node to a secure state, which temporarily disrupts its service. We model these interventions as follows.\n\\(do(Z_{i,t} = I_{i,t})\\) analyze; (10a)\n\\(do(D_{i,j,t} = 1)\\) decoy; (10b)\n\\(do(I_{i,t} = S) \\text{ if } I_{i,t-1} = C)\\) remove; (10c)\n\\(do(D_{i,t} = 0, I_{i,t} = S) \\text{ if } I_{i,t-1} \\in \\{C, R\\}\\) restore; (10d)\n\\(do(\\emptyset)\\) none. (10e)\nNote that \\(D_{i,t}\\) remains constant if no interventions occur, i.e., \\(D_{i,t} = f_D(D_{i,t-1}) \\triangleq D_{i,t-1}\\).\nWhen selecting interventions, the defender considers the history \\(H_t\\), which we define as\n\\(H_t \\triangleq (V_1, do(X_1), O_2, do(X_2), ..., do(X_{t-1}), O_t)\\)\n(11)\n\\(O_t \\triangleq \\{D_{i,t}, S_{i,t}, Z_{i,t}, C_t \\mid i \\in \\mathcal{V}\\},\\)\n(12)\nwhere do(X) is a shorthand for \\(do(X_t = x_t)\\) and \\(V_t\\) is the set of endogeneous variables at time t (defined below). The intervention at time t can thus be expressed as \\(do(X_t = \\pi_D(h_t))\\), where \\(\\pi_D\\) is a defender strategy.\nRemark 2. The fact that the defender remembers the history \\(H_t\\) (11) means that it has perfect recall [89, Def. 7]."}, {"title": "F. A Structural Causal Model of CAGE-2", "content": "The variables and the causal functions (4)-(9) described above determine the SCM (1) defined in (M1). Notable proper- ties of (M1) are a) the causal graph is acyclic (see Fig. 6); b) the model is stationary; c) the model is semi-Markov [22, Thm. 1.4.1]; d) the exogeneous variables are jointly independent; and e) P(Vt) is Markov relative to G (2) [22, Thm. 1.2.7].\nThe size of the causal graph G associated with (M1) grows linearly with the time horizon T and with the number of nodes in the target system |V|. A summary of G is shown in Fig. 6."}, {"title": "G. The Defender Problem in CAGE-2", "content": "Given (M1) and the defender objective (9), the problem for the defender can be stated as follows.\nProblem 1 (Optimal defender strategy in CAGE-2 (M1)).\n\\begin{aligned}\n& \\underset{D}{\\text{maximize}} & E_D [J \\mid V_1] \\\\\n& \\text{subject to} & do(\\mathcal{X} = \\pi_D(h_t)) && \\forall t \\\\\n& & \\Pi_A \\sim P(\\Pi_A) && \\forall t \\\\\n& & A_t \\sim P(A_t), D_t \\sim P(D_t) && \\forall t \\\\\n& & E_t \\sim P(E_t), W_{i,t} \\sim P(W_{i,t}) && \\forall t, i \\\\\n& & A_t = f_A(\\Pi_A, \\{I_{i,t}\\}_{i \\in \\mathcal{V}}) && \\forall t, i \\\\\n& & D_{i,t} = f_D(D_{i,t-1}) && \\forall t,i \\\\\n& & I_{i,t} = f_I(I_{i,t-1}, A_{t-1}, D_{i,t}, E_t) && \\forall t,i \\\\\n& & Z_{i,t} = f_{Z,i}(C_t, A_{t-1}, W_{i,t}) && \\forall t, i \\\\\n& & C_t = f_C(C_{t-1}, A_t, D_t) && \\forall t \\\\\n& & S_{i,t} = f_S(A_{t-1}, S_{i,t-1}) && \\forall t, i \\\\\n& & R_t = f_R(\\lbrace I_{i,t}, S_{i,t} \\rbrace_{i \\in \\mathcal{V}}) && \\forall t \\\\\n& & J = f_J(\\lbrace R_t \\rbrace_{t=1,...,T}), && (13m)\n\\end{aligned}\nwhere (13b) defines the interventions; (13d)-(13e) capture the distribution of U; and (13f)\u2013(13m) define F.\nWe say that a defender strategy \u03c0D is optimal if it solves Prob. 1. This problem is well-defined in the following sense.\nTheorem 1. Assuming \\(C_t, A, \\mathcal{V}, q_t, \\beta_{I_{i,t},z}, \\psi_z\\) are finite, and T is finite or \\(\\gamma < 1\\), then there exists an optimal deterministic defender strategy \\(\\pi_D^*\\). If T = \u221e, then there exists a \\(\\pi_D^*\\) that is stationary."}, {"title": "Proof.", "content": "For notational convenience, let \\(S_t \\triangleq \\{S_{i,t}\\}_{i \\in \\mathcal{V}}, I_t \\triangleq \\{I_{i,t}\\}_{i \\in \\mathcal{V}}, D_t \\triangleq \\{D_{i,t}\\}_{i \\in \\mathcal{V}}, W_t = \\{W_{i,t}\\}_{i \\in \\mathcal{V}},\\) and \\(Z_t \\triangleq \\{Z_{i,t}\\}_{i \\in \\mathcal{V}}\\). We break down the proof into the following steps.\n1) \\(\\Sigma_t \\triangleq (I_t, D_t, C_t, A_{t-1}, \\Pi_A, S_{t-1})\\) is Markovian.\nPROOF:\n\\begin{aligned}\nP(\\Sigma_{t+1} \\mid \\Sigma_1,..., \\Sigma_t) &= P(A_t \\mid \\Pi_A, I_t)P(A_{t+1})P(D_{t+1})\\times\\\\\n&P(C_{t+1} \\mid A_{t+1}, D_{t+1}, C_t)P(E_{t+1})P(S_{t+1} \\mid S_t, A_t)\\times\\\\\n&P(I_{t+1} \\mid I_t, A_t, D_{t+1}, E_{t+1}) = P(\\Sigma_{t+1} \\mid \\Sigma_t).\n\\end{aligned}\n2) \\((O_t, R_t \\amalg \\{\\mathcal{V}_t\\}_{t=1,...,t} \\mid \\Sigma_t)\\).\nPROOF:\n\\begin{aligned}\nP(O_t, R_t \\mid \\{\\mathcal{V}_t\\}_{t=1,...,t}) &= P(Z_t \\mid C_t, A_{t-1}, W_t)\\times\\\\\n&P(W_t)P(S_t \\mid S_{t-1}, A_{t-1}) P(R_t \\mid I_t, S_t) = P(O_t, R_t \\mid \\Sigma_t).\n\\end{aligned}\n3) dom(Ot), dom(\u03a3t), and Rt are finite.\nPROOF: Follows from the theorem assumptions.\n4) Each defender strategy \u03c0D induces a well-defined probabil- ity measure over the random sequence \\((\\Sigma_t, O_t)_{t > 1}\\).\nPROOF: 3) implies that the sample spaces of \\((\\Sigma_t, O_t)\\) and \\((\\Sigma_t, O_t)_{t > 1}\\) are measurable and countable, respectively. Further, the fact that G is acyclic implies that the interventional distributions induced by \\((do(X = x_t))_{t > 1}\\) are well-defined [22, Ch. 3]. Consequently, the statement follows from the Ionescu-Tulcea extension theorem [92].\n5) \\(P(\\Sigma_{t+1} \\mid \\Sigma_t), P(O_t \\mid \\Sigma_t)\\), and \\(P(R_t \\mid \\Sigma_t)\\) are stationary.\nPROOF: Follows by stationarity of (M1).\nStatements 1-5 imply that Prob. 1 defines a finite, stationary, and partially observed Markov decision process (POMDP) with bounded rewards. The theorem thus follows from standard results in Markov decision theory, see [93, Thm. 7.6.1]."}, {"title": "Theorem 2.", "content": "Problem 1 is not identifiable (Def. 2).\nProof. To prove non-identifiability, it is sufficient to present two sets of causal functions \\(F', F''\\) that induce identical distributions over the observables but have different causal effects (Def. 2) [22] [94, Lem. 1]. For simplicity, consider T = 2 and \\(|\\mathcal{V}| = 1\\). In this case J = R (9). Let\n\\begin{aligned}\nF' \\triangleq \\big\\{ &f_R(S, I) = \\mathbb{1}_{I=R}, f_I(A, E, D) = R, \\\\\n& f_S(S, A) = 1, f_Z, f_C, f_J, f_A, f_D \\big\\},\n\\end{aligned}\nwhere \\(\\{f_Z, f_C, f_J, f_A, f_D\\}\\) are defined arbitrarily. Now let \\(F''\\) be equivalent to \\(F'\\) except for \\(f_R(S, I) \\equiv S\\). Clearly P(O) is the same with both \\(F'\\) and \\(F''\\). However, \\(P(J = 0 \\mid do(I = S)) = 1\\) with \\(F'\\) but \\(P(J = S \\mid do(I = S)) = 1\\) with \\(F''\\).\nTheorem 2 states that causal effects of defender interven- tions (10) can not be identified from observations. This state- ment is obvious in hindsight but has important ramifications. It implies that to evaluate a defender strategy \u03c0D, the defender"}, {"title": "must either know (M1) or perform controlled experiments to measure the effects of the interventions prescribed by \u03c0D.\nWhile it is likely that the defender is aware of certain components of (M1), it is unrealistic that it knows the entire model. A more reasonable assumption is that the defender knows the causal graph G (Fig. 6), which does not capture all nuances of the causal mechanisms but provides structural information. Leveraging this structure, we next present a method for finding an optimal strategy \u03c0D which only requires access to the causal graph and a simulator of (M1).\nRemark 3. Access to a simulator is assumed by virtually all existing methods for autonomous cyber defense [5]-[20].", "content": "VI. CAUSAL PARTIALLY OBSERVABLE MONTE-CARLO PLANNING (C-POMCP)\nIn this section, we present C-POMCP, an online method for obtaining an optimal defender strategy \u03c0D for Prob. 1. The method involves three consecutive actions that are performed at each time step t (see Fig. 7).\nThe first action uses the observation ot and a particle filter to compute the defender's belief bt in the form of a probability distribution over the latent variables L in (M1)."}]