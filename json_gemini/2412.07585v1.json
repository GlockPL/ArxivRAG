{"title": "Scaling Sequential Recommendation Models with Transformers", "authors": ["Pablo Zivic", "Hernan Vazquez", "Jorge S\u00e1nchez"], "abstract": "Modeling user preferences has been mainly addressed by looking at users' interaction history with the different elements available in the system. Tailoring content to individual preferences based on historical data is the main goal of sequential recommendation. The nature of the problem, as well as the good performance observed across various domains, has motivated the use of the transformer architecture, which has proven effective in leveraging increasingly larger amounts of training data when accompanied by an increase in the number of model parameters. This scaling behavior has brought a great deal of attention, as it provides valuable guidance in the design and training of even larger models. Taking inspiration from the scaling laws observed in training large language models, we explore similar principles for sequential recommendation. Addressing scalability in this context requires special considerations as some particularities of the problem depart from the language modeling case. These particularities originate in the nature of the content catalogs, which are significantly larger than the vocabularies used for language and might change over time. In our case, we start from a well-known transformer-based model from the literature and make two crucial modifications. First, we pivot from the traditional representation of catalog items as trainable embeddings to representations computed with a trainable feature extractor, making the parameter count independent of the number of items in the catalog. Second, we propose a contrastive learning formulation that provides us with a better representation of the catalog diversity. We demonstrate that, under this setting, we can train our models effectively on increasingly larger datasets under a common experimental setup. We use the full Amazon Product Data dataset, which has only been partially explored in other studies, and reveal scaling behaviors similar to those found in language models. Compute-optimal training is possible but requires a careful analysis of the compute-performance trade-offs specific to the application. We also show that performance scaling translates to downstream tasks by fine-tuning larger pre-trained models on smaller task-specific domains. Our approach and findings provide a strategic roadmap for model training and deployment in real high-dimensional preference spaces, facilitating better training and inference efficiency. We hope this paper bridges the gap between the potential of transformers and the intrinsic complexities of high-dimensional sequential recommendation in real-world recommender systems.", "sections": [{"title": "INTRODUCTION", "content": "A recommendation system aims to provide users with content that fits their preferences and interests. Classical methods have explored building static models based on past user interactions to predict future ones. Pattern mining and factorization-based methods are two classical methodologies that stand as the most popular in the literature [10, 47]. These models seek to capture static preferences in the interaction of users with items in a catalog. While the formulation largely simplifies the modeling of otherwise complex interaction patterns observed in the real world, the main drawback of these models relies on the assumption of static behavior patterns. In reality, user preferences are subject to a series of short- and long-term factors that are very hard to disentangle [4, 41]. From a modeling perspective, user preference dynamics can be seen as latent factors that govern the observed users' behavior as they interact with the system. These interactions are diverse and depend on the nature of the actual system. For instance, the types of events that can be registered in a music streaming platform differ from those observed in an e-commerce website. Despite the complexity of the task, the driving hypothesis of modern recommendation systems is that such behavioral patterns can be captured by models that can predict future interactions from historical sequential records. The nature and complexity of such models have been influenced to a great extent by the success of different machine-learning models in different fields, especially those from the natural language literature. We can find solutions based on simple Recurrent Neural Networks [16], convolutional architectures [51, 52], based on Attention mechanisms [29, 63] and, more recently, the Transformer [22, 48, 54]. Among them, transformer-based solutions are the most promising. This is not only due to the success of this architecture in fields beyond language modeling, such as computer vision [24], speech [27], and time-series forecasting [32], but also to their flexibility and good scaling behavior. Another important factor of the transformer architecture that led to its adoption as the model of choice in many applications is the availability of a pre-fitted version that can be adapted easily to more specific tasks using a fraction of the data required to train a similar model from scratch. This pre-training and fine-tuning strategy has not yet been widely adopted in the sequential recommendation literature. We believe that this obeys two main reasons: first, the data used to train recommendation models is specific to each application domain, i.e. the nature of the catalog and type of events are problem-specific, making it challenging to leverage prefitted models on domains that might be closely related but not the same as those on which the model has been trained (e.g. a model pre-trained on Amazon data being fine-tuned to a different catalog or e-commerce domain); second, there are particularities in the sequential recommendation problem that constraint the design of solutions that scale.\nWhile predicting the next token in a sentence and the next item in an interaction sequence share structural similarities, the sequential recommendation problem introduces some particularities that need special attention. For instance, in language modeling, it is common to cast the prediction task as a classification problem over a large set of tokens (subdivisions of words into finer sub-word units). Although large, the size of this vocabulary remains constrained to a manageable number (around 30K in most practical applications) that does not change over time. On the contrary, most real recommendation applications involve item sets (space of possible user preferences) that expand to massive scales, often reaching into the millions or even billions of different items [7]. Moreover, such collections may change over time as items are constantly added and removed from the catalog. These characteristics impose design constraints that must be satisfied if we are willing to take advantage of the flexibility and ease of adaptation observed by these architectures in other domains.\nPerhaps the most intriguing characteristic of these models relies on their ability to leverage increasingly large amounts of data by simply growing the number of parameters accordingly. From a system design perspective, this poses new challenges around how to scale the amount of data and compute required by these models to leverage their full potential. From a practical perspective, this choice is constrained not only by the desire to get the best possible performance but also to achieve such performance within the limits of a given computational budget. In light of recent discoveries regarding scaling laws in the language [1, 23] and other domains [15, 43, 62], recent research has provided new insights into how model performance scales with the number of parameters, the size of the datasets used for training, and the required computational budget [2, 17].\nIn this work, we explore the hypothesis that transformer-based sequential recommendation models exhibit scaling behaviors similar to those observed in other domains. Under such a hypothesis, we investigate how, within a given computational budget, optimizing the balance between model size and data size can yield improved results. To do so, we propose a generic yet scalable model that takes inspiration from other transformer-based models from the literature but lets us experiment with problems and models of different complexity. We run experiments on the full version of the widely used Amazon Product Data (APD) dataset [37]. Our findings confirm our hypothesis, and we show how such scaling behavior can be used in practice by training larger models that, when fine-tuned, achieve a performance that surpasses more complex approaches from the literature. Our main contributions are the following:\n\u2022 We propose a generic transformer-based architecture that is both flexible and scalable.\n\u2022 We show scaling laws similar to those observed in language modeling tasks.\n\u2022 We show that it is possible to pre-train recommendation models at scale and fine-tune them to particular downstream tasks, improving performance w.r.t to similar models trained from scratch.\nThe paper is organized as follows: Sec. 2 discusses some key aspects of sequential recommendation models in the context of scalability, Sec. 3 proposes a formulation that makes the model independent of the size of the catalog, Sec. 4 shows experimental results. In Sec. 4.4 we derive analytical laws that relate the target metric with the most relevant quantities of interest from a scaling perspective. In Sec. 4.5 we show we can use the pre-training and fine-tuning strategy for improving recommendations. Sec. 5 discusses related work. Finally, in Sec. 6 we draw some conclusions."}, {"title": "SCALABILITY OF SEQUENTIAL RECOMMENDATION MODELS", "content": "Sequential recommendation models seek to capture user interaction patterns and a possibly large collection of available items in a catalog. Users may perform many interaction types depending on the nature of such elements: an e-commerce site, a music streaming service, a social network, and others. For instance, users might play a song, skip it, or add it to a playlist in a music recommendation context, while they can add an article to a shopping cart, buy it, add it to a wish list, etc. Although such heterogeneity in the type of interactions can be handled accordingly [21, 38, 58], for the purpose of this study, we subsume all domain-specific cases into a more generic \"user-item\" interaction (i.e. a user interacted with an item in some way). With this in mind, let U denote the user base of a given platform and I the collection of items they can interact with. The behavior and preference dynamics of a user $u \\in U$ can be considered as embedded into the sequence of items they interacted with for a given period. Let $S_u = {i_1, i_2, ..., i_n}$ be such a sequence, where $i_k$ denotes the $k$-th item user $u$ interacted with. In this context, a recommendation model can be thought of as a function $f_\\theta$, parameterized by $\\theta$, that takes as input the interaction history encoded by $S_u$ and seeks to predict the item or items that user $u$ will interact with in the future.\nGiven the sequential nature of the problem, we consider models based on the transformer architecture [28]. These models, initially proposed in the context of language modeling tasks, have proven effective in various domains. From a scaling perspective, and similarly to what happens in the natural language case, we have two clear dimensions that affect their scaling behavior: the number of model parameters, $N$, and the number of user-item interactions seen during training. However, as we will see next, analyzing scalability in most recommender systems proposed in the literature would also require considering the number of available items in the catalog, $|I|$. This dependency originates in the way most transformer-based approaches cast the sequential prediction task. In a direct translation"}, {"title": "A SCALABLE RECOMMENDATION FRAMEWORK", "content": "We take SASRec [22] as our reference model. SASRec is a transformer-based architecture that has shown competitive performance on several sequential recommendation tasks [30] and is regarded as a strong baseline in more recent evaluations [39]. In SASRec, the model takes as input a sequence of user-item interactions of length $n$ and seeks to predict the item the user will interact with next. The sequence is fed into a transformer model of $L$ layers to produce an output embedding that matches a representation of the following item in the sequence. The output might also include a classification layer over the items in the catalog that induces additional complexity [48]. Each item in the catalog is encoded as a trainable vector representation of size $D$, resulting in a total of $|I| \\times D$ trainable parameters. As mentioned above, this dependency between the number of trainable parameters and the catalog size makes the analysis of scaling behaviors difficult due to the interplay between $N$ and $|I|$.\nGiven these observations, instead of considering the learning problem as a classification one, we propose reformulating it as an embedding regression task [50], as follows. Given a user navigation sequence $S_u = {i_1, i_2, ..., i_n}$, we assume we can compute a $D$-dimensional vector representation for each item in the sequence. To compute such representations, we rely on a parametric mapping $\\phi : I \\rightarrow R^D$. We denote as $u_k = \\phi(i_k)$ the representation of $k$-th item user $u$ interacted with. These embeddings are user-independent in that the representation for a given item is the same irrespective of how the user might interact with it. We compute these representations on the fly and train the full model (including the feature extraction model $\\phi$) to pick among (a subset of) them the one that corresponds to the target item for an input sequence $S_u$. This is illustrated in where we have replaced the embedding layer with a feature extractor that computes the item embeddings ($\\phi(i_i), i = 1, .., 4$) that feed the model. The output of this model ($f_\\theta(\\phi)$) is used for prediction (and learning) by comparing similarities with the items in the catalog (their embeddings). This feature-based approach has shown good performance in the literature [40, 64]. In this case, the number of parameters associated with the computation of item embeddings is given by the number of trainable parameters in the feature extraction module $\\phi$, irrespective of the number of items in the catalog.\nWe train our model autoregressively as follows. Given a training set of user navigation sequences of length $n$, we ask the model to predict each element of any given sequence based on the (sub)sequence of previous interactions. We optimize the following loss:\n$\\mathcal{L}(\\theta;S) = \\sum_{S \\in S} l(\\theta;S).$ (1)\nHere, we omitted the superscript u for the sake of clarity. Let $S$ denote the set of all subsequences with a length of at least 2 interactions. Let us denote by $S = {i_1, ..., i_{n-1} }$ the partial sequence containing the first $n - 1$ items of $S$. Our goal is to train a model that, based on $S$, can rank the target $i_n$ as high as possible when compared to other candidates from the catalog. We adopt sampled Softmax [57] as our choice for $l$:\n$l(\\theta; S) = - \\log \\frac{exp(f_\\theta(S) \\cdot \\phi_{n} / \\tau)}{exp(f_\\theta(S) \\cdot \\phi_{n} / \\tau) + \\sum_{\\phi \\in N(S)} exp(f_\\theta(S) \\cdot \\phi / \\tau)}$ (2)\nHere, $f_\\theta$ denotes the model we are trying to fit, $\\phi_n$ the representation of the target item in computed by the feature extraction module, $\\tau$ is a temperature parameter that controls the softness of the positive and negative interactions, and $N(S) \\subset I$ is a set of negatives whose cardinality is to be set. In practice, we apply the logQ correction proposed in [60] to the logits in Eq. (2) to correct for the bias introduced by the negative sampling distribution. In the rest of the paper, we refer to our transformer-based model and learning formulation as Scalable Recommendation Transformer (SRT).\nThe framework introduced above is motivated by the need to set up a competitive yet simple baseline that scales well w.r.t the quantities we identified as the most relevant from a scaling perspective, namely the number of trainable parameters and the number of samples (or interactions) observed during training.\nEq. (2) can be seen as an approximation to a cross-entropy loss over the items in the catalog, where we contrast against a subset of the possible items. This corresponds to a generalization of the loss used in SASRec or BERT4Rec, where $N(S)$ is constrained to a single sample draw at random. Note that by drawing samples at random, we take the risk of contrasting against uninformative samples that are easily distinguishable from the positive ones. On the other hand, if we choose an elaborate negative sampling methodology, we might end up adding a non-negligible computation overhead to an otherwise simple model. Moreover, sampling hard negatives might induce biases that correlate with the catalog size [33], adding a degree of variability that is difficult to isolate. In our case, we opt to sample negatives from the item popularity distribution (i.e. items with which users interacted the most are sampled more frequently). This strategy is competitive and has a small footprint on the overall computations. From now on, we denote our models as SRT-X, where X is the number of negatives used to compute the loss in Eq. (2).\nBesides the advantages of the proposed formulation regarding scalability, an additional advantage of our model is the ability to work with non-static catalogs. Adding and deleting items dynamically from a catalog (due to policy infringements, product stockout, new trends, outdated information, etc) is commonplace in most practical applications. Building sequential recommendation models based on fixed item sets brings many concerns regarding the usability and maintainability of the system over time. These concerns might hinder a wider adoption of these types of approaches.\nBefore delving deeper into scalability, which is the primary goal of our work, we first show that our formulation achieves competitive performance compared to other transformer-based formulations. Table 1 compares the performance of our model against other popular methods from the literature on the Beauty and Sports subsets of the Amazon Review Data [35] benchmark. Details of the dataset, metrics, and evaluation protocols are provided in Sec. 4. We consider different versions of our model trained using 10, 100, 300, and 1K negatives, respectively, and compare them against the popular SASRec [22] and BERT4Rec [48] models. For these models, we show the metrics reported by Chen et al. [5] for compatibility"}, {"title": "EXPERIMENTS", "content": "This section discusses our experimental setup in the context of standard practices observed in the literature. We then show and discuss results on scalability and optimal compute allocation. Finally, we show fine-tuning results that compete favorably with other methods from the literature."}, {"title": "Evaluation Protocol", "content": "We ran experiments on the Amazon Product Data (APD) dataset [14, 35], a large dataset of product reviews crawled from Amazon between 1996 and 2014. The dataset consists of 82.7 million reviews over 9.9 million different products written by more than 21 million users. Reviews in this dataset correspond to a subset of all purchases made in the platform during the relevant time span. Due to its size, a common practice in the literature consists of using smaller subsets of the data. For instance, \"Amazon Beauty\" corresponds to the subset of samples where users bought (and reviewed) an item from the \"beauty\" category. To avoid issues related to cold-start [31], it is common to filter out users and products with less than five purchases. The remaining data is called a \"5-core\" dataset. These two procedures (per-category and 5-core filtering) distort or hide some of the intrinsic characteristics of real-world recommendation problems. For instance, if we consider the \"beauty\" category, only 8.7% of the interactions originate from items with at least 5 purchases/reviews. This is not only a matter of scale (with most data being discarded) but a problem of deceiving evaluation, as results reported on these datasets do not necessarily extrapolate to actual real systems."}, {"title": "Model Design and Training Algorithm", "content": "We adapt SASRec as outlined in Sec. 3, replacing the item embedding matrix with a trainable feature encoder whose complexity is independent of the size of the catalog. Concretely, we take the title and brand of each product and tokenize them into a vocabulary of 30k tokens with the SentencePiece tokenizer [26]. This way, we replace the variable-sized item embedding matrix with a fixed matrix of token embeddings. As shown in Table 1, these changes lead to comparable performance in standard benchmarks. Based on this architecture, we consider different model complexities parameterized by the number of layers, n\u2081, number of attention heads per layer, nH, and hidden embedding dimensionality, d. Table 3 details the different combinations of these parameters we used in our experiments. To train our models, we use the Adam optimizer and a one-cycle learning rate policy consisting of a linear warm-up stage and a cosine decay after one-third of the total iterations. We set the base learning rate to 1e - 4 and the number of epochs to 50. We use gradient clipping (set to 1) and a weight decay factor of 1e - 5. We base our implementations on the RecBole library [65]."}, {"title": "Scaling Model and Dataset Sizes for Optimal Compute", "content": "In this section, we explore the relationship between the target metric and the compute resource requirements induced by different combinations of model sizes and number of training interactions. Our evaluation differs from similar studies [6, 17, 23] in two main aspects: first, we focus on a task-specific metric instead of a more generic loss; second, we train our models over multiple epochs, thus revisiting the same training sequences multiple times during the training process. These differences originate from the particularities of the sequential recommendation problem. We also focus on a performance metric that is closely tied to the actual recommendation task (NDCG vs loss as in the language modeling case) and which is more informative from a practical standpoint."}, {"title": "Estimating Model Performance", "content": "Based on the data obtained in our experiments, we present two formulations for estimating the expected performance in terms of the target metric for recommendation. These models aim at asking the following questions: a) for a fixed FLOP budget, is it possible to get an estimate of the maximum achievable performance? and b) for a given model and dataset size, is it possible to estimate the expected maximum NDCG for that configuration? In the first case, we assume there exists an \"oracle\" that selects the optimal model and dataset configuration."}, {"title": "Estimating NDCG from a fixed FLOPs budget", "content": "In the experiments, we recorded the maximum NDCG achieved for each FLOP budget. The figure shows a more complex relationship between NDCG and FLOPs in log space than the linear scaling behavior observed in other studies. In our case, we observe the beginning of an asymptotic trend for the maximum achievable NDCG. This behavior could be due to many factors, including the saturation of the target metric due to challenges intrinsic to the particular recommendation problem (recommendation over broad item categories, representation ambiguity in the item embeddings, etc). In this case, a sigmoidal fit appears more appropriate, in which case it corresponds to:\n$NDCG(FLOPs) = \\frac{0.396}{1 + e^{-0.18(log(FLOPs) - 24.44)}} + 0.247$. (3)\nThis function reveals that as the FLOPs budget increases, the NDCG approaches an upper limit estimated at 0.149 (0.396-0.247), highlighting the diminishing returns of increasing the computational budget. We can identify the point where this diminishing return"}, {"title": "Estimating NDCG for a given model and dataset size", "content": "Here, we model the maximum achievable NDCG as a function of the total parameter count and the size of the dataset, as measured by the number of seen interactions. The goal is to find a parametric function that captures the underlying relationship between the model's complexity, data size, and final task performance. This involves identifying key parameters that influence the expected risk and then quantifying their impact on the model's effectiveness, as measured by the NDCG score. We follow a risk decomposition approach and propose the following functional form similar to [17]:\n$NDCG(N,T) \\approx E - \\frac{A}{N^\\alpha} - \\frac{B}{T^\\beta}$. (4)\nHere, $N$ denotes the total parameter count and $T$ number of user-item interactions. We use a subtractive formulation to account for a target metric maximization law, instead of a loss minimization as in [17]. The chosen parametric form allows us to outline how changes in the number of parameters and the size of the dataset systematically affect the model's ability to rank items accurately. To fit the model, we use a non-linear least squares approach and constrain the model coefficients to be non-negatives to avoid nonsensical solutions. We obtain the following solution:\n$NDCG(N, T) \\approx 0.163 - \\frac{18.56}{N^{0.376}} - \\frac{2.9}{T^{0.364}}$. (5)\nFrom the above equation, we can interpret $E$ as the maximum expected value for the NDCG@5 score, in which case reaches a value of 0.163. We also observe a similar value for the exponents"}, {"title": "Transferability", "content": "In this section, we evaluate the transfer ability of some of our larger pre-trained models by fine-tuning them in the Amazon beauty and sports subsets. This is a widely used strategy in the literature but has seen lesser popularity in the context of sequential recommendation. This is because, unlike the language and vision domains, the data used to train such models are particular to each recommendation domain (i.e. the nature of the catalog), and the type of events being recorded changes from case to case. Nevertheless, training more generic models at scale and fine-tuning them to different downstream tasks poses the same advantages observed in other domains, such as improvements in the final performance, shorter development cycles, improvements in the backbone model translate effortlessly to improvements in downstream performance, etc. We show that is it possible to pre-train and fine-tune recommendation models and that, by doing so, we obtain performance improvements that could not be achieved by training similar models from scratch. Our adaptation strategy is as follows. Given a pre-trained model2, we apply a progressive fine-tuning strategy that consisting of progressively unfreezing layer by layer, tuning them for 10 epochs using a learning rate of $1 \\times 10^{-4}$ and a one-cycle cosine schedule. Once all transformer layers have been unfreeze, we unfreeze the token embedding layer and train for an additional 50 epochs. To avoid over-adaptation and catastrophic forgetting, we use the Elastic Weight Consolidation (EWC) formulation of [25] which has"}, {"title": "RELATED WORK", "content": "Sequential recommendation is a branch of recommendation systems, an area that recognizes the importance of sequential behavior in learning and discovering user preferences [56]. Initial models used the Markov Chain framework for anticipating user activities [12, 13, 42]. With advancements in deep learning, innovative approaches have emerged, such as employing Recurrent Neural Networks (RNN) [16], attention mechanisms [49], and Memory networks [20]. The disruption introduced by transformer architecture [54] led to significant progress, giving rise to well-known approaches like SASRec [22] and BERT4Rec [48]. Despite their success, these methods face a substantial limitation in scaling. They rely on item IDs to represent the sequence of interactions, which presents several scalability issues [8]. First, the pure ID indexing of users and items is inherently discrete and fails to impart adequate semantic information to new items. Second, adding new items requires modifications to the model's vocabulary and parameters, causing transformer-based methods to scale poorly with an increase in the item count, which is crucial for many real-world recommendation systems.\nA viable solution to the constraints of ID-based recommender systems is to integrate textual information such as item titles, descriptions, and user reviews. The UniSRec model exemplifies this by deriving adaptable representations from item descriptions [19]. Text-based Collaborative Filtering (TCF) with Large Language Models like GPT-3 has demonstrated potential superiority over ID-based systems. Nevertheless, the overreliance on text prompted the development of VQ-Rec, which utilizes vector-quantized representations to temper the influence of text [18]. Additionally, approaches like ZSIR leverage Product Knowledge Graphs to augment item features without prior data [9], and ShopperBERT models user behavior via purchase histories [45]. IDA-SR advances this by using BERT to generate ID-agnostic representations from text [36]. On the contrary, MoRec illustrates that systems that combine IDs and text can surpass those dependent solely on IDs [61]. However, these advancements complicate existing architectures by adding computational demand and complicating scalability.\nTo these intricate and parameter-intensive models, we must add the challenge that data in real-world applications is often noisy and sparse. Various methods have adopted contrastive learning [53] in new architectures, as seen with CoSeRec [33], ContraRec [55], and S3-Rec [66]. The success of these new contrastive learning-based methods motivates further investigation into the effectiveness of contrastive loss functions for item recommendation, particularly Sampled Softmax [57]. Regrettably, these studies typically focus on fixed item spaces and overlook the scaling issues of the functions. Scaling problems have been tackled through other methods. LSAN suggests aggressively compressing the original embedding matrix [30], introducing the concept of compositional embeddings, where each item embedding is composed by combining a selection of base embedding vectors. Recently, the concept of infinite recommendation networks [44] introduced two complementary ideas: \u221e-AE, an infinite-width autoencoder to model recommendation data, and DISTILL-CF, which creates high-fidelity data summaries of extensive datasets for subsequent model training.\nScaling issues are not unique to recommendation systems but are inherent in new transformer-based architectures. In the field of NLP, various studies have been carried out to discover scaling laws that predict the scaling of the model and inform decision-making [17]. To our knowledge, only two studies have attempted to find scaling laws in recommendation systems, yet none in SR. The first study [3] aimed to explore the scaling properties of recommendation models, characterizing scaling efficiency across three different axes (data, compute, parameters) and four scaling schemes (embedding table scaling vertically and horizontally, MLP and top layer scaling) in the context of CTR problems. Similarly, the second study [46] seeks to understand scaling laws in the pursuit of a general-purpose user representation that can be assessed across a variety of downstream tasks."}, {"title": "CONCLUSIONS", "content": "In this work, we studied the scaling behavior of the transformer architecture applied to real-world sequential recommendation problems. We introduced a simple and flexible architecture and learning formulation that allowed us to scale the recommendation problem and model complexity independently from each other.\nWe showed there exist scaling laws similar to those observed in other sequential prediction domains, offering insights into the design of larger and more capable models. We also show that by pre-training larger recommendation transformers, we can fine-tune them for downstream tasks with significantly lesser data and obtain performance improvements compared to the same models trained from scratch."}]}