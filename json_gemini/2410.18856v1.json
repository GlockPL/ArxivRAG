{"title": "Demystifying Large Language Models for Medicine: A Primer", "authors": ["Qiao Jin", "Nicholas Wan", "Robert Leaman", "Shubo Tian", "Zhizheng Wang", "Yifan Yang", "Zifeng Wang", "Guangzhi Xiong", "Po-Ting Lai", "Qingqing Zhu", "Benjamin Hou", "Maame Sarfo-Gyamfi", "Gongbo Zhang", "Aidan Gilson", "Balu Bhasuran", "Zhe He", "Aidong Zhang", "Jimeng Sun", "Chunhua Weng", "Ronald M. Summers", "Qingyu Chen", "Yifan Peng", "Zhiyong Lu"], "abstract": "Large language models (LLMs) represent a transformative class of Al tools capable of revolutionizing various aspects of healthcare by generating human-like responses across diverse contexts and adapting to novel tasks following human instructions. Their potential application spans a broad range of medical tasks, such as clinical documentation, matching patients to clinical trials, and answering medical questions. In this primer paper, we propose an actionable guideline to help healthcare professionals more efficiently utilize LLMs in their work, along with a set of best practices. This approach consists of several main phases, including formulating the task, choosing LLMs, prompt engineering, fine-tuning, and deployment. We start with the discussion of critical considerations in identifying healthcare tasks that align with the core capabilities of LLMs and selecting models based on the selected task and data, performance requirements, and model interface. We then review the strategies, such as prompt engineering and fine-tuning, to adapt standard LLMs to specialized medical tasks. Deployment considerations, including regulatory compliance, ethical guidelines, and continuous monitoring for fairness and bias, are also discussed. By providing a structured step-by-step methodology, this tutorial aims to equip healthcare professionals with the tools necessary to effectively integrate LLMs into clinical practice, ensuring that these powerful technologies are applied in a safe, reliable, and impactful manner.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), exemplified by GPT-4\u00b9, Claude 3\u00b2, Gemini 1.5\u00b3, and Llama 3\u2074, are artificial intelligence (AI) models that can generate human-like responses under various conversational contexts and adapt to novel tasks by following human instructions\u2075,\u2076. They have shown great promise in diverse biomedical and healthcare applications\u2077,\u2078,\u2079,\u00b9\u2070,\u00b9\u00b9, such as question answering\u00b9\u00b2,\u00b9\u00b3,\u00b9\u2074,\u00b9\u2075, clinical trial matching\u00b9\u2076,\u00b9\u2077,\u00b9\u2078, clinical documentation\u00b9\u2079,\u00b2\u2070,\u00b2\u00b9, and multi-modal comprehension\u00b2\u00b2,\u00b2\u00b3.\n\nDespite the accelerated progress of LLMs in patient care and clinical practice, biomedical and health sciences research, and education\u2078,\u2079,\u00b2\u2074, there is a noticeable lack of practical, actionable guidelines for their application from bench to bedside and beyond. A lot of current use of LLMs, such as ad-hoc prompting with ChatGPT, is far from sufficient in medical tasks\u00b2\u2075,\u00b2\u2076. This gap can lead to both underutilization and misapplication of these technologies, potentially affecting patient outcomes. Our work aims to close this gap by providing a detailed, structured framework to guide the utilization and integration of LLMs into medical workflows (Figure 1). Specifically, this framework consists of task formulation, model selection, prompt engineering, fine-tuning, and deployment considerations. We further provide a set of best practices (Box 1) while supporting adherence to ethical use, evaluation metrics, and compliance standards. The tutorial code that contains step-by-step instructions is publicly available at https://github.com/ncbi-nlp/LLM-Medicine-Primer. We hope this work will help equip healthcare professionals with the necessary knowledge to effectively leverage LLMs in their practices."}, {"title": "Task Formulation", "content": "Adapting an abstractive medical need into one or more concrete tasks that can be addressed by an LLM requires users to first understand the core capabilities of LLMs, which we classify into five broad categories: (1) knowledge and reasoning, (2) summarization, (3) translation, (4) structurization, and (5) multi-modal data analysis. We recommend beginning by identifying the primary LLM capability relevant to your task. Once the task is formulated, one should aim to collect a diverse set of instances (test cases) that contain input and output data elements for development (Box 1, Best Practice 1). We recommend collecting about 100 test instances, following several evaluation studies of LLMs in medicine\u00b9\u2074,\u00b2\u2070,\u00b2\u2077.\n\nKnowledge and reasoning\n\nLLMs can use the medical knowledge encoded in their parameters to perform domain- specific reasoning given different contextual information\u00b9\u2070,\u00b9\u00b2,\u00b9\u00b3,\u00b9\u2074,\u00b2\u2078,\u00b2\u2079. This capability can enable a variety of medical applications, such as answering medical questions\u00b3\u2070, clinical decision support\u00b3\u00b9, and matching patients to clinical trials\u00b9\u2076,\u00b9\u2077,\u00b9\u2078. Task instances usually include question (input), explanation (reasoning and context output), and short answer (output). When formulating a reasoning task, users can quickly evaluate short answers (e.g., yes/no) between LLM predictions and ground truth and proceed to in-depth analyses of explanations if short answers are satisfying\u00b2\u2077."}, {"title": "Summarization", "content": "LLMs can summarize complex documents into concise paragraphs or sentences. Summarization tasks in biomedicine primarily fall into two categories: (1) summarizing long clinical notes into shorter texts, such as generating the progress notes and discharge summaries\u00b2\u2070,\u00b2\u00b9; (2) summarizing medical literature for evidence synthesis, such as generating the systematic reviews given a list of clinical studies\u00b3\u00b2,\u00b3\u00b3,\u00b3\u2074. These labor-intensive tasks can be potentially streamlined by LLMs\u00b9\u2079,\u00b3\u2075. For a summarization task, instances include instruction (input), original text (input), and summarized text (output). Users may leverage metrics like BLEU\u00b3\u2076, ROUGE\u00b3\u2077, and BERTScore\u00b3\u2078 to compare the generated texts and the reference summaries. However, it should be noted that automatic metrics do not always correlate well with the gold-standard human judgments\u00b3\u2079.\n\nTranslation\n\nLLMs also have the capability to translate text, not only between different languages but also between writing styles appropriate for different audiences. This ability can enable applications such as sharing medical knowledge across different language demographics\u2074\u2070, supporting medical education\u2074\u00b9,\u2074\u00b2, and facilitating communication with patients\u2074\u00b3. A translation task, similar to a summarization task, includes instances made of instruction"}, {"title": "Structurization", "content": "LLMs can be leveraged to convert free-text input into structured outputs such as a list of key- value pairs. Medical structurization problems include classifying an input text into controlled vocabularies like the diagnosis-related groups\u2074\u2074 and extracting biomedical concepts as well as their relationships (e.g., variant-causing-disease) from unstructured text\u2074\u2075. Structurization instances include task instruction (input), source text (input), and a list of extracted concepts and relations in structured forms (output). The evaluations are made to match the output with the reference answers. As such, the performance is often typically measured by precision, recall, and F1 score, etc.\n\nMulti-modality\n\nMulti-modal LLMs like GPT-4 can analyze and integrate diverse data types such as text, images, audio, and even genomic information, potentially serving as a generalist medical Al\u00b2\u00b2. For example, these models can support clinical tasks, such as generating radiology reports and guiding clinical decisions\u2074\u2076,\u2074\u2077,\u2074\u2078 based on real-world multimodal patient data. The multi-modal task instances and evaluations are similar to those of the knowledge and reasoning tasks, except that the input question typically contains data in multiple modalities (e.g., past medical history in EHR with current imaging results)."}, {"title": "Large Language Model Selection", "content": "Users should choose an appropriate LLM based on their task characteristics. There are a wide variety of LLMs, including proprietary models such as GPT-4\u00b9, Gemini\u00b3, Claude\u00b2, and open-source models such as Llama\u2074 and Mistral\u2074\u2079. They can range in size from several billion parameters (e.g., Llama3.1-8B) to hundreds of billion parameters (e.g., Llama3.1-405B). Typically, larger models exhibit more proficient responses\u2075\u2070. Some models are trained for more general applications, while others such as PMC-LlaMA\u2075\u00b9 and MEDITRON\u2075\u00b2 are fine- tuned for specific domains or applications. When choosing the LLM, users should consider three key factors in their needs: Task and Data, Performance Requirements, and Model Interface. Figure 3 shows an overview of the LLM selection considerations discussed in this section"}, {"title": "Task and Data", "content": "The first and most critical factor to consider is the nature of the data the user is working with and the specific task to perform. Ensuring that the chosen model aligns with the data type and task requirements is foundational to successful LLM implementation. When working with sensitive patient data, it is important to ensure privacy and compliance with regulations such as Health Insurance Privacy and Accountability Act (HIPAA). Proprietary models accessed through APIs like OpenAl's GPT-4 are typically not HIPAA-compliant, so they should not be used for patient data. In contrast, certain cloud service providers such as Azure and Anthropic provide HIPAA-compliant access to LLMs, which could be potentially used for sensitive data. Alternatively, local models such as Llama or Mistral can be used for enhanced control over privacy and security when processing sensitive medical information.\n\nThe diversity of healthcare tasks necessitates processing various data modalities in addition to free texts. Radiology and pathology applications, for example, may require models that can interpret and generate insights from 2D or 3D medical images, which requires models"}, {"title": "Performance Requirements", "content": "The model's medical capabilities are one of the most critical factors to consider when selecting LLMs. Typically, greater capabilities come with larger model sizes which require more resources for development and customization. Conversely, smaller LLMs may not perform as well as their larger counterparts, but they are often more sustainable and less costly. While LLM capabilities vary across different model sizes, capabilities are also affected by the target applications. For example, LLMs are better for tasks that require medical knowledge and clinical reasoning but do not often outperform fine-tuned BERT models\u2076\u2070 in simpler tasks such as structurization\u2076\u00b9.\n\nThere are two main approaches to evaluating the medical capabilities of LLMs: multi-choice question (MCQ) evaluation and clinical evaluation. Medical examination and question- answering tasks, such as MedQA-USMLE\u2075\u00b3, PubMedQA\u2076\u00b2, MedMCQA\u2076\u00b3, have been commonly used to evaluate the knowledge and reasoning capabilities of LLMs\u00b9\u2074. These benchmarks should only be used to filter out models that cannot meet basic performance standards. However, higher scores on these datasets do not necessarily translate to better clinical utility, as there are no choices provided in real-life applications. After a model passes initial screening via MCQ evaluations, it must be further assessed for clinical utility."}, {"title": "Model Interface", "content": "Once the LLM(s) has been chosen, the users also need to select a point of access to the LLMs based on their needs. Broadly, there are three ways to access LLMs: web applications, model application programming interfaces (APIs), and locally hosted implementations. Web applications, such as ChatGPT, are inexpensive and easy to use compared to APIs and local models; however, they do not provide interfaces that allow flexible control of model behavior and large-scale evaluations. In addition, most LLM web applications do not have clear compliance with standards such as HIPAA, which further raises security issues when dealing with sensitive patient data. Consequently, we do not recommend using web applications during the development phase. In contrast, model APIs are controlled points of access via the web that provide an interface to proprietary models such as PaLM and open-"}, {"title": "Prompt engineering", "content": "Harnessing LLM capabilities for application to a specific task requires careful consideration of the prompt (input content) given to the model. Prompt engineering is the process of designing and optimizing prompts to effectively guide LLMs in generating accurate and coherent responses\u2076\u2074. Prompts can vary from one simple instruction to many documents retrieved by a search system, allowing users to elicit a variety of behaviors without the need to modify the parameters of LLM. In general, more complex tasks typically require more sophisticated prompts. Figure 4 shows a concrete task example of clinical trial matching, where a simple prompt that merely describes the patient and lists the clinical trial criteria (shown in Fig. 4e) might not work well. As such, prompt engineering and fine-tuning methods should be used. Table 2 shows resource requirements, advantages, disadvantages, and use cases of different approaches. Some detailed case studies are also listed in Table 3."}, {"title": "Few-shot learning (FSL)", "content": "As shown in Fig. 4a, FSL includes a few examples (i.e., \u201cshots\u201d) within the prompt to better specify the task for the model\u2075. Each demonstration example should include both the input and desired output. In the case of patient-to-trial matching, the input contains patient information, clinical trial criterion, and the task instruction; the output contains the rationale and the criterion-level eligibility label. Zero, one, and more than one examples (e.g., five) are respectively denoted as zero-shot, one-shot, and few-shot learning and are the most commonly used in practice. The examples should be as representative and diverse as possible. For example, demonstrations of all potential labels (e.g., disease sub-types) should be shown for a classification task. Another useful approach to few-shot learning is to generate examples dynamically, based on semantic similarity to the instances being predicted\u2074\u2074. We recommend starting from zero-shot learning and incrementally adding examples to increase performance or deal with edge cases (Box 1, Best Practice 5)."}, {"title": "Chain-of-thought (CoT) prompting", "content": "As shown in Fig. 4c, CoT prompting involves designing prompts that lead the model through a step-by-step reasoning process\u2076\u2079. For example, providing the explanation of the patient- criterion relation helps the users efficiently verify the LLM-predicted eligibility labels. One can simply add \"Let's think step-by-step\u201d to the end of the input for CoT prompting. This technique is particularly useful in complex medical decision-making tasks, where an explanation of reasoning can improve model performance and aid clinicians in understanding and verifying Al-generated advice. We recommend using CoT prompting as the default as it improves the explainability of Al responses and potentially the performance as well (Box 1, Best Practice 6)."}, {"title": "Retrieval-augmented generation (RAG)", "content": "In RAG (Fig. 4d), a search engine retrieves relevant documents, such as scientific articles, to be included in the prompt, allowing the model to better solve knowledge-intensive tasks such as answering questions\u2077\u2070 (Box 1, Best Practice 7). By grounding the LLMs to respond based on the provided relevant textual snippets, RAG can potentially reduce hallucinations\u201d (the generation of incorrect information) and improve upon outdated knowledge encoded in large language models. For example, LLMs can get access to the definition of certain medical concepts with RAG to better classify the patient eligibility in the application of patient-to-trial matching. We recommend using high-quality domain-specific corpora, such as systematic reviews, medical textbooks, and clinical guidelines, for RAG systems in medicine\u2075\u2074."}, {"title": "Tool learning", "content": "Certain medical tasks require the use of domain-specific tools such as database utilities or medical calculators. If these tools are implemented as application programming interfaces (APIs), LLMs can utilize them through a function calling mechanism (Box 1, Best Practice 7). In the example case of clinical trial matching, LLMs can be provided with tools for reading raw electronic health records to capture detailed information that might be missing from a patient summary (shown in Fig. 4c).\n\nSetting the temperature\n\nBesides the prompt, LLMs also require a temperature parameter that controls the amount of randomness when generating the response. Lower temperatures result in a more consistent output, while higher temperatures lead to more creative responses. Temperature can usually be set via API or local parameterization, but usually not via Web app. We recommend that users start with a temperature of 0 to get deterministic results for reproducibility and only consider increasing the temperature to get diverse responses for purposes such as ensembling\u2077\u00b2 (Box 1, Best Practice 8).\n\nAdditional considerations"}, {"title": "Fine-tuning", "content": "Although LLMs can solve many tasks using prompt engineering without explicit model modification, there are at least three situations where fine-tuning may be considered: (1) prompt engineering techniques like few-shot learning and RAG cannot sufficiently improve results, (2) high-quality training data is readily available in large scale, (3) the working prompt is too long to be feasible in terms of cost. (Box 1, Best Practice 9).\n\nLLMs can be fully or partially fine-tuned. Full model fine-tuning updates all the parameters of an LLM, while parameter-efficient fine-tuning (PEFT) methods\u2078\u00b3,\u2078\u2074,\u2078\u2075,\u2078\u2076, such as Low-Rank Adaptation (LoRA\u2078\u00b3), update a subset of LLM parameters or add additional trainable weights to the LLM. In general, smaller and more specific data is suitable for PEFT to prevent overfitting\u2078\u2077, while larger and more diverse data is suitable for full-scale fine-tuning to better train the model\u2077\u2075. For example, Med-PaLM 2\u00b2\u2078 used a diverse set of instances spanning medical exams, consumer health information, and medical research. The model utilizes both full fine-tuning and a novel method known as ensemble refinement, achieving high results on several benchmarks. PEFT greatly reduces the hardware requirements for fine- tuning. By using a small set of trainable parameters, quantized LoRA (QLoRA)\u2078\u2074 uses quantization and adapter methods\u2078\u2074,\u2078\u2078 that reduce fine-tuning memory requirements (e.g., from over 780GB to 48GB) while maintaining fixed model parameters. PEFT approaches are often competitive with full model fine-tuning methods and even outperform them in low- data environments. For example, Van Veen et al used QLoRA to fine-tune LLMs for clinical text summarization with thousands of training instances and only one NVIDIA Quadro RTX 8000 GPU\u2078\u2079.\n\nIn summary, we suggest performing full or partial fine-tuning depending on computational resources and dataset features. In addition to open-source LLMs, some proprietary LLMs, such as GPT-4, can also be fine-tuned via file upload to their web applications. However, the implementation details of these fine-tuning approaches are unknown to the public and might raise concerns about transparency and privacy. Similar to prompt engineering approaches, fine-tuned models also need to be evaluated on an independent test set to verify the performance improvement of training."}, {"title": "Deployment considerations", "content": "Regulatory compliance\n\nDeploying LLMs in the biomedical domain requires adherence to privacy standards such as HIPAA and the General Data Protection Regulation (GDPR\u2079\u2070) to protect patient data. When utilizing proprietary LLMs, it is essential to ensure that the platforms are HIPAA-compliant. Alternatively, processing data locally using an open-source model can enhance data safety\u2074. For example, while the OpenAl API is not currently compliant with HIPAA, Azure services provide HIPAA-compliant access to OpenAl's models. Similarly, Anthropic provides HIPAA-certified API hosting for its Claude models. In the end, users must maintain the ethical and legal integrity of their deployment by carefully selecting protocols that align with compliance requirements and clinical standards (Box 1, Best Practice 10).\n\nEquity and fairness\n\nUsers should evaluate potential biases in LLM's training data and algorithms to ensure fair and equitable outcomes\u2079\u00b9. Prior work has shown that even the most successful proprietary models can exhibit racial bias. Studies have shown that, when presented with identical patient profiles differing only by race, LLMs can yield varying predictions for treatment, cost, or outcome. Such differences can result in healthcare disparities during production. Thus, it is necessary to evaluate model fairness before deployment\u2079\u00b2,\u2079\u00b3. When examining data or algorithms is not viable, such as in the case of many proprietary models, users may still use existing benchmark datasets for evaluation\u2079\u2074,\u2079\u2075. This approach provides an idea of whether the models are fair or biased, and to what extent they exhibit bias.\n\nCosts\n\nWhen considering the costs associated with deploying large language models, it is important to distinguish between proprietary and open-source models. Proprietary LLMs require usage fees for each request made to the service provider. In the case of OpenAl's GPT-4 model, this pay-as-you-go (PAYG) system processes each token at a cost of $0.03 per 1,000 prompt tokens for models with 8k context lengths, with additional charges for completion tokens at $0.06 per 1,000 tokens. For a typical MIMIC-III\u2079\u00b9 discharge note containing around 4,000 tokens, processing would cost approximately $0.12 for prompt tokens, with additional costs depending on the response length generated by the model. Some proprietary model providers also offer customization services such as fine-tuning for an additional fee. Though proprietary models typically offer robust support, this pricing structure can cause delays in customization and updates. Utilizing services in this way does not guarantee access to the most advanced updates and limits customization, as new updates undergo extensive quality checks and alignments before companies release them.\n\nIn contrast, open-source LLMs involve procurement costs for the necessary hardware, like GPUs, and ongoing costs related to maintenance. While the up-front costs are higher when running the model locally, these can be offset by the lower ongoing costs. In addition, an open-source setup can offer additional benefits like the ability to fine-tune the model to specific applications with protected data and more control over system responsiveness and updates. However, users should consider the potential for increased latency and reduced throughput during periods of high local demand. Local devices running LLMs might not match the speed and response time of large companies hosting LLMs.\n\nPost-deployment\n\nAfter the deployment of LLMs in healthcare, continuous monitoring is essential (Box 1, Best Practice 10). Users should ensure that LLM outputs are responsibly used as support tools, not as independent replacements for the judgment of healthcare practitioners\u2079\u2076. Effective training programs are crucial to help healthcare professionals understand how to interpret and utilize these outputs while managing potential risks. Additionally, the successful integration of LLMs in medical practices demands active collaboration with patients and local communities. This involves leveraging engagement methods, such as community advisory boards and patient panels, to gather meaningful feedback and perspectives\u2079\u2077. Such inclusive strategies help tailor LLM applications to the real-world diversity of patient experiences and enhance the effectiveness of these technologies in medical practice."}, {"title": "Conclusions", "content": "Large language models (LLMs) have the potential to revolutionize healthcare by enhancing clinical workflows, decision-making, and patient outcomes. However, their effective integration into medical practice requires a systematic and thoughtful approach. This tutorial provides a comprehensive framework for utilizing LLMs in medicine, emphasizing critical stages such as task formulation, model selection, prompt engineering, and deployment. By following these guidelines, healthcare professionals can maximize the benefits of LLMs while addressing challenges related to regulatory compliance, fairness, and cost. As Al continues to evolve, the careful application of these methods will be essential in ensuring that LLMs are used responsibly and effectively, ultimately improving the quality of care delivered to patients."}, {"title": "Box 2. Glossary", "content": "1. Large language model: A large language model is a type of artificial intelligence designed to process and generate human-like text. The model is built using deep learning techniques and is trained on text corpora to perform tasks such as language translation, summarization, and question answering.\n2. Instance: An instance refers to a specific example used for training or testing a model. Each instance typically includes input data and corresponding output, which the model is expected to predict or generate.\n3. Rationale: Rationale is text output produced by a language model that provides additional context or reasoning for an output answer or value.\n4. Multi-modal comprehension: Refers to the ability of models like GPT-4 to analyze and integrate diverse data types, including text, images, audio, and genomic information, for various applications such as medicine and research.\n5. Parameter: A parameter is a variable within a large language model that is learned from training data and used to make predictions. The value of a parameter is often adjusted to minimize error during the training phase.\n6. Context Window: Context window refers to the number of tokens that a language model can receive as input. Larger context windows can increase the ability to perform in-context learning within a large language model prompt.\n7. Token: A token is the smallest unit of information that a language model can process. Tokens often represent text information like single letters, spaces, sub-words, words, or phrases. According to empirical evidence, one token is about 0.75 of a word.\n8. Prompt engineering: Prompt engineering involves crafting inputs or \"prompts\" that guide large language models to generate desired outputs without changing their parameters. Effective prompt design can significantly enhance the performance of LLMs.\n9. Few-shot learning: Few-shot learning refers to the ability of a model to learn a new task from a very limited number of examples (shots). In the context of LLMs, this involves providing a few specific examples during the prompt to guide the model on how to handle similar tasks, improving its accuracy and adaptability.\n10. Retrieval-augmented generation: Retrieval-augmented generation combines traditional language modeling with a retrieval component that searches for relevant information. This enhances the model's responses by grounding them in factual data.\n11. Hallucination: Hallucination refers to instances where the model generates incorrect or fabricated information.\n12. Chain-of-thought prompting: Chain-of-thought prompting is a technique used with large language models to encourage step-by-step reasoning in their responses. By explicitly asking the model to think through the steps of a problem, it can provide more transparent and logically structured solutions.\n13. Temperature: Temperature controls the randomness of the generated responses from large language models. A higher temperature increases diversity in the output, leading to more creative and varied responses, while a lower temperature produces more predictable and consistent outputs.\n14. Fine-tuning: Fine-tuning is an approach to transfer learning in which pre-trained model parameters are further modified on a new dataset.\n15. HIPAA-compliance: HIPAA-compliant systems adhere to HIPAA (Health Insurance Portability and Accountability Act) regulations, ensuring they meet the required standards to protect patient health information. HIPAA sets national standards for the protection of health information in the United States. It ensures the privacy and security of individually identifiable health information."}]}