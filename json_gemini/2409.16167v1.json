{"title": "MERGING LORAS LIKE PLAYING LEGO: PUSHING THE MODULARITY OF LORA TO EXTREMES THROUGH RANK-WISE CLUSTERING", "authors": ["Ziyu Zhao", "Tao Shen", "Didi Zhu", "Zexi Li", "Jing Su", "Xuwu Wang", "Kun Kuang", "Fei Wu"], "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning large language models (LLMs) to various domains due to its modular design and widespread availability on platforms like Huggingface. This modularity has sparked interest in combining multiple LoRAs to enhance LLM capabilities. However, existing methods for LoRA composition primarily focus on task-specific adaptations that require additional training, and current model merging techniques often fail to fully leverage LoRA's modular nature, leading to parameter interference and performance degradation. In this paper, we investigate the feasibility of disassembling and reassembling multiple LoRAs at a finer granularity, analogous to assembling LEGO blocks. We introduce the concept of Minimal Semantic Units (MSUs), where the parameters corresponding to each rank in LoRA function as independent units. These MSUs demonstrate permutation invariance and concatenation-summation equivalence properties, enabling flexible combinations to create new LoRAs. Building on these insights, we propose the LoRA-LEGO framework. This framework conducts rank-wise parameter clustering by grouping MSUs from different LoRAs into k clusters. The centroid of each cluster serves as a representative MSU, enabling the assembly of a merged LoRA with an adjusted rank of k. Additionally, we apply a dual reweighting strategy to optimize the scale of the merged LoRA. Experiments across various benchmarks demonstrate that our method outperforms existing approaches in LoRA merging.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) like ChatGPT Achiam et al. (2023) and LLAMA Touvron et al. (2023) trained on vast amounts of general data, demonstrate remarkable performance in general tasks. To explore their potential for specialized tasks, adapting LLMs to specific domains by fine-tuning model parameters has become a critical area of research. In this context, Low-rank Adaptation (LoRA) Hu et al. (2021), as a parameter-efficient fine-tuning approach, has gained widespread recognition, also attributed to its modular design Liu et al. (2023); Yang et al. (2023b); Hadi et al. (2023). The modular nature of LoRA enables it to serve as plug-and-play plugins for LLMs, facilitating the storage and deployment of large collections of LoRAs on platforms like Hugging Face. The extensive availability of LoRAs has sparked considerable interest in combining multiple LoRAs into a unified adapter to significantly extend the capabilities of LLMs Yadav et al. (2024a); Xiao et al. (2024); Zhao et al. (2024b); Huang et al. (2023).\nPrevious methods for composing multiple LoRAs have primarily focused on assembling separate LoRAs tailored to specific downstream tasks, which generally require additional training Wu et al. (2023); Wang et al. (2024); Chronopoulou et al. (2023); Yadav et al. (2024a); Huang et al. (2023). Model merging Tang et al. (2024); Yadav et al. (2024b); Ilharco et al. (2022); Yang et al. (2024) offers an alternative approach by aggregating the parameters of multiple LoRAs into a unified adapter without extra training, producing a unified LoRA with more comprehensive capabilities. However, these methods typically employ element-wise parameter fusion, which can neglect and disrupt the internal semantic structure within LoRA. This disruption potentially leads to parameter interference (as discussed in \u00a72.3), thereby hindering the performance of merged LoRA. This paper approaches"}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 Low-RANK ADAPTATION", "content": "Directly fine-tuning LLMs with full parameters is computationally intensive and is not feasible in low-resource scenarios. Based on the idea that only a small number of low-rank parameters need to be fine-tuned for sufficient performance in new domains, Hu et al. (2021) proposed the Low-Rank Adaptation, where the LoRA module can be combined with the pre-trained parameters in parallel for efficient inference.\nSpecifically, given pre-trained weights $W_0 \\in \\mathbb{R}^{d \\times k}$ of a sub-module of LLM, the LoRA adds an extra trainable weight matrix as $W_0 + \\Delta W = W_0 + BA$, where $\\Delta W$ can be decomposed into two smaller matrices $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$, where $r$ stands for the rank of $\\Delta W$ and the rank $r < min(d, k)$. The forward pass for a layer $y = W_0x$ can be modified as follows:\n$y = W_0x + \\Delta Wx = W_0x + BAx,$ \n(1)\nwhere $x \\in \\mathbb{R}^{d}$ is the input and the $y \\in \\mathbb{R}^{d}$ denote the output."}, {"title": "2.2 FURTHER MODULARIZATION OF LORA", "content": "Before delving into the issue of LoRA merging, it is imperative to present several pivotal insights and definitions that could serve as fundamental components for constructing a LoRA module.\nDefinition 1. Minimum Semantic Unit of LoRA. Let $A$ and $B$ be matrices in a LoRA module. For each index $i$, define the minimum semantic unit of LoRA as the combined vector $s_i = [a_i, b_i]$, where $a_i$ is the $i$-th column of $A$ and $b_i$ is the $i$-th column of $B^T$ (i.e., the transpose of the $i$-th row of $B$).\nIn this context, each column of the down-projection matrix $A$ and its corresponding row in the up-projection matrix $B$ are treated as a cohesive unit, defined as a Minimum Semantic Unit (MSU). Each MSU contributes to a rank of the LoRA, encapsulating a distinct semantic fragment of the LoRA's capacity. Through this definition, LoRAs exhibit the following properties.\nProperty 2.1. Permutation Invariance. For a LoRA module parameterized by matrices $A$ and $B$, if the rows of $A$ are permuted, then by performing a corresponding permutation of the columns of $B$, the product of these matrices remains unchanged. Formally, let $P$ be a permutation matrix that satisfies $P^TP = I$, where $I$ is the identity matrix. If we permute the rows of $A$ to obtain a new matrix $A' = PA$, and correspondingly permute the columns of $B$ to get $B' = BP^T$, then, $BA = B'A'$.\nThe property of permutation invariance indicates that the arrangement of MSUs within LoRA calculations can be altered without affecting LoRA's output.\nProperty 2.2. Concatenation-Summation Equivalence. Consider two LoRAs, $(A_1, B_1)$ and $(A_2, B_2)$, each of rank r. Specifically, matrices $A_1$ and $A_2$ are of size $R^{r \\times d}$, and $B_1$ and $B_2$ are of size $R^{d \\times r}$. Define the concatenated matrices as:\n$A' = \\begin{bmatrix} A_1 \\\\ A_2 \\end{bmatrix} \\in \\mathbb{R}^{2r \\times d}, B' = [B_1 \\ B_2] \\in \\mathbb{R}^{d \\times 2r}.$\nThe output vector y from the concatenated model is equivalent to the sum of the outputs from each individual LoRA model:\n$y = B'A'x = (B_1A_1 + B_2A_2)x.$\nBased on this property, we can synthesize the knowledge from all LoRAs by constructing a new LORA through the concatenation of all MSUs from each LoRA. The computational result is equiv-alent to ensembling the outputs of all LoRAs. Based on these insights, we can draw the following conclusions:\nEach LoRA can be modularized into multiple MSUs, with each MSU corresponding to a rank within the LoRA. These MSUs can be flexibly permuted and combined to construct a unified LORA."}, {"title": "2.3 PROBLEM FORMULATION AND CHALLENGES", "content": "Consider a LLM denoted as $\\mathcal{L}$ and a set of p task-specific LoRAs, represented by $\\Phi = \\{ \\phi_1, \\phi_2, ..., \\phi_p\\}$. Each LoRA $\\phi_i$ is specialized for a particular task $\\mathcal{T}_i$ and is crafted by incorporating low-rank matrices into different layers of $\\mathcal{L}$, thereby tuning the model to better suit $\\mathcal{T}_i$. For simplicity of notation, we denote the parameters of these low-rank matrices at any given layer for each LoRA $\\phi_i$ as $A_i$ and $B_i$. The goal of merging these LoRAs is to synthesize a comprehensive LoRA $\\phi'$ that not only excels in all tasks encompassed by $\\Phi$ but also generalizes well to unseen tasks. We discuss the difference between the LoRA merging setting and the previous model merging setting in the Appendix A.\nA natural approach to performing LoRA merging involves a simple element-wise averaging of the parameters from each LoRA: $\\Phi' = \\frac{1}{n}\\sum_{i=1}^{n} \\phi_i$. However, parameter interference poses a significant challenge to effective LoRA merging. We identify two potential sources of parameter interference during LoRA merging and demonstrate through experiments that such interference can lead to performance degradation in the merged LoRA.\nThe first cause of parameter interference stems from parameter misalignment in LoRAs, as depicted in the left part of Fig.2. Accoding to Property 2.1, the MSUs of each LoRA can be permuted arbitrarily without affecting the functionality of the LoRA module. However, misalignment of MSU parameters when merging LoRAs can result in parameter interference. To investigate the impact of parameter misalignment on model performance, we conducted a controlled experiment using the Llama-2-7b model, training LoRAs on different tasks. For the parameters $A$ and $B$ of a task, we randomly generated a permutation matrix $P$ and adjusted the parameters to $A' = (A + PA)/2$ and $B' = (B + BP^T)/2$. This adjustment simulates the merging of two identical LoRAs with misaligned parameters. The results, presented in Tab.1, indicate that parameter misalignment can lead to a decline in model performance, with some tasks experiencing significant performance degradation. Therefore, ideal merging entails alignming MSUs during LoRA merging to mitigate parameter interference.\nAnother source of parameter interference stems from knowledge conflict during LoRA merging. As depicted on the right side of Fig.2, knowledge conflict occurs when the merged LoRA lacks sufficient"}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 LORA-LEGO FRAMEWORK", "content": "Based on the motivation that MSUs as the building blocks of LoRA, we can disassemble and reassemble LoRA like playing with LEGO. Here, we propose a flexible and effective method called LORA-LEGO as shown in Fig.3. This framework is structured around three main procedures: MSU Grouping, MSU Clustering, and LoRA Reconstruction. These steps collectively facilitate the integration of diverse MSUs into a cohesive LoRA, alleviating the parameter interference while LoRA merging.\nMSU Grouping. The initial stage of merging p LoRAs begins by disassembling each LORA into various MSUs and grouping all the MSUs from each LoRA together. Let $\\{A_i, B_i \\}_{i=1}^{p}$ represent the LoRA parameters of a layer with rank $r_i$. Each LoRA module $A_j, B_j$ contains $r_j$ MSUs, denoted by $\\{s_{j1}, s_{j2}, ..., s_{jr_j} \\}$, where $s_{jl} = [A_{jl}, B_j^T]$. The MSU pool $\\Phi$, which includes MSUs from all the LoRAs to be merged, is constructed as $\\Phi = \\cup_{j=1}^{p} \\{s_{j1}, s_{j2}, ..., s_{jr_j} \\}$.\nMSU Clustering. After grouping the MSUs from different LoRAs, the next step involves regrouping these MSUs into clusters based on their similarities. With the MSU pool $\\Phi$, we employed K-means Kanungo et al. (2002) to partition these MSUs into k clusters $\\{C_1, C_2, ..., C_k \\}$ in which each MSU is assigned to the cluster closest to it. This process is described by the following optimization problem:\n$\\underset{C}{minimize} \\sum_{i=1}^{k} \\sum_{s \\in C_i}||s - \\mu_i||^2,$\n(2)\nwhere $\\mu_i$ is the centroid of cluster $C_i$.\nLORA Reconstruction. Following the MSU clustering, we rearrange the MSUs into k clusters based on their similarity. The centroids of these clusters, denoted by $\\mu_1, \\mu_2, ..., \\mu_k$, are calculated as the average of the MSUs within each cluster. These centroids represent aggregated parameters"}, {"title": "3.2 \u039f\u03a1\u03a4\u0399MAL SCALE OF MERGED LORA", "content": "Given that the rank of the merged LoRA from LoRA-LEGO can range from 1 to $\\sum_{j=1}^{p} r_j$, the scale of LoRA's output could vary significantly, thereby impacting the performance. We identified two key factors that determine the scale of the output.\nNorm Decay After LoRA Merging. As shown in Fig.4, we examine the norms of the parameters after merging (i.e., the centroids of each cluster) compared to the average norms of the parameters within each cluster before merging. We observed that after merging, the parameter norms significantly decrease, potentially affecting the output scale of the LoRA module, since the parameter norm influences the magnitude of the output. This phenomenon can be explained by the triangle inequality Klement et al. (2013), which states that for any vectors $s_i$, $|\\sum_{i=1}^{p} s_i|| \\le \\sum_{i=1}^{p} ||s_i||$. When computing the centroid $\\mu = \\frac{1}{p} \\sum_{i=1}^{p} s_i$, its norm satisfies:\n$||\\mu|| = ||\\frac{1}{p} \\sum_{i=1}^{p} s_i|| \\le \\frac{1}{p} \\sum_{i=1}^{p} ||s_i||.$\nTherefore, the norm of the centroid is less than or equal to the average of the norms of the original vectors, explaining the observed norm decay after merging. The more diverse the vectors within a cluster, the more pronounced this reduction in norm will be. To compensate for the reduced norm after merging, we perform parameter reweighting by scaling the centroid to match the average norm of the cluster: $\\mu' = \\frac{\\sum_{i=1}^{p} ||s_i||}{||\\mu||} \\mu$. In our implementation, we use the infinity norm for reweighting to ensure stability and robustness in the results.\nVariance Expansion with Increased LoRA Rank. Another factor influencing the scale of the LORA output is the rank of the merged LoRA. We conducted experiments to investigate how the rank of the LoRA affects the output scale by merging seven LoRAs with rank r = 8 and varying the rank k of the merged LoRA (which corresponds to the clusters number in LoRA-LEGO). The frequency histograms of outputs from the first layer of the merged LoRA at various ranks, as shown in Fig.5, indicate that LoRA outputs approximate a normal distribution centered at zero. We observed that as the rank k increases, the variance of the output also increases. To normalize the output variance, similar to the normalization in the self-attention mechanisms Vaswani (2017), we perform output reweighting for the merged LoRA by the factor $\\frac{1}{\\sqrt{k}}$. The following theorem ensures that this rescaling maintains a consistent variance in the LoRA output."}, {"title": "4 EXPERIMENTS", "content": "Given that LoRA merging is essential for many scenarios, we have opted for two settings: Multi-task Learning Tang et al. (2024) and Mixed-task Settings Zhao et al. (2024b). In these settings, we compared various LoRA composition methods to assess the performance of the proposed LoRA-LEGO approach. We selected Llama2-{7b,13b} as the base model and trained LoRA for each task with hyperparameters r = 6 and a = 12. The evaluation frameworks for multi-task Learning and mixed-task settings are detailed in the subsequent sections, where we provide a comprehensive analysis."}, {"title": "4.1 MULTI-TASK LEARNING", "content": "Experiment Setting. Multi-task Learning aims to merge individually trained LoRAs into a unified model while preserving the performance of each constituent LoRA. Drawing from previous research Tang et al. (2024); Yadav et al. (2024b); Ilharco et al. (2022), we merged seven LoRA models, each fine-tuned on Llama2-7b, 13b, for in-domain tasks including Cola, Mnli, MRPR, QNLI, GLUE-QQP, RTE, and SST2. We then assessed the performance of the merged LoRA on these in-domain tasks as well as on two additional out-of-domain tasks, SNLI and WNLI, to evaluate its adaptability and generalization capabilities.\nBaseline Methods. We compared the proposed method with four post-hoc training-free LORA composition methods, including (1) Weight Averaging, (2) Ensemble, (3) Task Arithmetic, and (4) Ties-Merging. The details of these LoRA composition methods can be found in the Appendix C."}, {"title": "4.2 MIXED-TASK EVALUATION", "content": "Evaluation Setting. Recent studies Zhao et al. (2024b) have proposed the creation of a LoRA pool from which relevant Lo-RAs are retrieved for each input to facilitate LoRA composition. We adopt the same setting and construct a LoRA pool for 48 tasks from flan-v2, grouped into 10 task clusters. The evaluation set is constructed by randomly choosing 50 samples from each test set. These samples are then mixed and shuffled to form a unified dataset comprising 2400 data points.\nAdopting the LoraRetriever approach Zhao et al. (2024b), we initially retrieve the top-3 LoRAs based on the sentence embedding similarities between each input sample and a corresponding few-shot samples. Following this, we engage in LoRA composition and evaluate various strategies. This analysis underscores the versatility and superior performance of LORA-LEGO in handling more complex scenarios.\nBaseline Methods. For all methods, we employ a consistent evaluation pipeline. For each instance in the evaluation set, we initially retrieve the top-3 LoRA, followed by the composition of LoRA. We compared the following LoRA composition methods: (1) Weighted Average, (2) Ensemble, (3) Selection (using the top-1 retrieved LoRA), and (4) Ties-Merging.\nMain Results. Previous research Zhao et al. (2024b) has shown that using a retriever to identify LORA tasks tailored to various inputs is more efficient and effective in personalized service settings. Consequently, we concentrate on how multiple LoRAs can be integrated effectively through LoRA merging after retrieving the top-k LoRAs for each input. We assess the performance of LoRA com-position methods in both IID and OOD contexts. \"IID\" performance refers to scenarios where all LoRAs are accessible to the retriever. \u201cOOD\u201d performance, however, involves masking the LORA associated with the specific task of each test sample during retrieval, preventing any sample from accessing its ideal LoRA. This approach allows us to evaluate the cross-task generalization capa-bilities of the LoRA composition methods. Tab.4 demonstrates that LoRA-LEGO surpasses other composition methods in both IID and OOD scenarios by fully eliminating parameter interference. In contrast, baseline LoRA composition methods experience performance degradation due to their"}, {"title": "5 RELATED WORK", "content": "Model Merging. Many works have discussed how to obtain a comprehensive model through model merging from various perspectives. Some works discuss how to find a set of low-loss paths in the parameter space for model parameter interpolation from the perspective of linear mode connectivity Ainsworth et al. (2022); Entezari et al. (2021). From a similar perspective, we further utilized properties of MSUs, mploying clustering algorithms to provide a flexible solution for enhancing the parameter connectivity during LoRA merging. Additionally, many works attempt to coordinate models trained in a decentralized and separated manner through model merging, utilizing their knowledge transfer capabilities to obtain a model with comprehensive abilities Tang et al. (2024); Don-Yehiya et al. (2022); Yadav et al. (2024b); Matena & Raffel (2022); Jin et al. (2022); Yang et al. (2023a). Recently, with the rise of large language models, more and more works have focused on how to use model aggregation, especially the aggregation of LoRA Chronopoulou et al. (2023); Huang et al. (2023); Zhao et al. (2024b); Wang et al. (2024), to strategically utilize models adapted to multiple domains. These efforts often overlook the parameter interference that occurs during LoRA merging, and some of them require extensive additional training or adaptation. This leads to suboptimal performance in such scenarios or restricts their applicability.\nApplication of LoRA Merging. LoRA merging can be applied in various scenarios. For instance, in multi-task learning Tang et al. (2024); Don-Yehiya et al. (2022), models adapt to different domains in a decentralized manner using LoRA, subsequently acquiring multi-task capabilities through merging. In mixed-task scenarios Zhao et al. (2024b;a), LoRAs from diverse domain tasks are uploaded to a centralized service platform, where the service retrieves and composes LoRAs to deliver person-alized services based on downstream requests. In federated learning Chen et al. (2023); Zhang et al. (2024), edge devices train LoRAs on private data and upload them to a central server for merging and distribution, enabling iterative optimization through this process. During the alignment phase, RLHF training is conducted for various preferences to obtain multiple LoRAs that meet different requirements. This is followed by providing personalized alignment models through parameter in-terpolation Jang et al. (2023)."}, {"title": "6 CONCLUSION", "content": "In this paper, we address the critical challenge of merging multiple LoRAs, each tailored for distinct tasks, into a unified and comprehensive LoRA. We identify parameter interference as a primary obstacle in merging, with parameter misalignment and knowledge conflict being significant con-tributors. Our exploration of LoRA's properties reveals several key insights: (1) Each rank within a LoRA operates independently and represents a minimal semantic unit (MSU); (2) MSUs within each LoRA exhibit permutation invariance; (3) MSUs can be concatenated to form a comprehensive LORA. Building on these insights, we propose LoRA-LEGO, a methodology that aggregates MSUs from all target LoRAs, performs clustering, and uses the centroid of each cluster to create a merged LORA. Our extensive experimental results validate the effectiveness of the LoRA-LEGO approach.\nPotential future work includes exploring alternative distance metrics for LoRA-LEGO, such as op-timal transport, to provide a more detailed characterization of parameter similarities beyond the typical Euclidean distance. Additionally, further modularization of LoRA could enhance various applications, particularly in federated learning where strategies to minimize communication over-head and expedite model convergence through sharing and aggregating MSUs might be explored. We believe these advancements can significantly benefit a wide range of fields and applications."}, {"title": "A DIFFERENCE BETWEEN LOAR MERGING SETTING AND MODEL MERGING SETTING", "content": "Previous work on model merging primarily focused on integrating separately trained models to form a comprehensive system. These methods typically involve reloading LoRA parameters into the orig-inal model before merging, which introduces additional overhead by necessitating the reconstruction of a corresponding LLM for each LoRA. In many cases, the goal of LoRA merging is to create a new LORA that consolidates the capabilities of all involved LoRAs for simplified task-specific usage. In contrast, the LoRA merging approach presented in this paper bypasses this step; it directly merges the LoRA parameters to generate a unified LoRA with enhanced capabilities."}, {"title": "B CONNECTION WITH VANILLA LORA COMPOSITION METHODS", "content": "The vanilla LoRA composition comprises two training-free methods: the Ensemble of LoRAs and the Fusion of LoRAs. The Ensemble of LoRAs strategy involves the aggregation of the outputs of each submodule within the assembled LoRAs. Let us denote $A = \\{A_1, A_2, ..., A_n\\}$ and $B = \\{B_1, B_2, ..., B_n\\}$ as the sets representing submodules within n LoRAs. For an input $x_i$, the output derived from the Ensemble of LoRAs can be expressed as $\\mathcal{X} = \\sum_{i=1}^{n} B_i A_ix$, where $\\mathcal{X}$ denotes the output.\nIn contrast to the Ensemble method, which combines the output of different LoRAs, model merging presents an alternative composition strategy. Let the parameters of each LoRA, $\\Theta_i$, be denoted by $\\Theta$. A typical strategy involves averaging these parameters, represented as $\\Theta_{fusion} = \\frac{1}{k} \\sum_{i=1}^{k} \\Theta_i$.\nThis formulation allows the fused parameter to function akin to a single LoRA.\nOur proposed LoRA-LEGO method offers refined control over model complexity and performance, facilitating an optimal balance between semantic integration and computational efficiency. Unlike Simple Fusion, which lacks alignment and may introduce inconsistencies across merged LoRAs, our approach aligns and selectively fuses MSUs based on their semantic closeness. This selective fusion significantly reduces redundancy, a prevalent issue in the Ensemble method where all MSUs are treated uniformly, potentially leading to an excessive parameter count that can degrade perfor-mance. Our clustering-based strategy addresses these challenges by effectively condensing the most relevant semantic features into fewer clusters, thereby enhancing both the model's performance and its inference efficiency."}, {"title": "C DETAILS OF BASELINE METHODS", "content": "We compare our method with the following baseline:\n1. Weight Averaging. This approach averages the parameters across different instances of LORA, resulting in a new composite LoRA defined as $A' = \\frac{1}{n}\\sum_{i=1}^{n} A_i$ and $B' = \\frac{1}{n}\\sum_{i=1}^{n} B_i$, where $A_i$ and $B_i$ represent the parameters from the i-th instance of the original LORA models, and n is the number of models being averaged.\n2. Ensemble. This method averages the outputs from each LoRA, simultaneously activating multiple LoRAs to compose a combined output. The specific calculation for the mixed output is defined as $x' = \\frac{1}{n} \\sum_{i=1}^{n} B_jA_jx_i$.\n3. Task Arithmetic. This method is akin to weight averaging, but it differentiates by using weights derived from a hyper-parameter search to merge models. The calculations for this composite are $A' = \\rho \\sum_{i=1}^{n} A_i$ and $B' = \\rho \\sum_{i=1}^{n} B_i$, where $\\rho$ represents the hyper-parameter that scales the contributions of each model.\n4. Ties-Merging. This method aims to resolve element-wise parameter interference by ini-tially trimming the redundant parameters, retaining only the top-k% of values based on their magnitude. It then selects the sign vector for the merged model and finally performs a disjoint mean operation. Ties-Merging posits that the primary source of parameter inter-ference arises from inconsistencies in the values of merged parameters, while potentially overlooking issues related to misalignment and knowledge conflict."}, {"title": "D OPTIMAL SCALE OF MERGED LORA", "content": "Theorem D.1. Let $A_1 \\in \\mathbb{R}^{p \\times r}$ and $B_1 \\in \\mathbb{R}^{r \\times p}$, and $A_2 \\in \\mathbb{R}^{p \\times k}$ and $B_2 \\in \\mathbb{R}^{k \\times p}$, where all elements of these matrices are independently and identically distributed according to the standard normal distribution N(0,1). Then, after scaling the product $A_2B_2$ by the factor $\\sqrt{r}/\\sqrt{k}$, the variances of the entries of $A_1 B_1$ and the scaled $A_2B_2$ are equal:\n$Var(A_1B_1) = Var(\\sqrt{\\frac{r}{k}}A_2B_2).$"}, {"title": "E PERFORMANCE ON MERGING HETEROGENEOUS LORAS", "content": "Another advantage of LoRA-LEGO is its ability to merge heterogeneous LoRAs, that is, LoRAs with different ranks. To experimentally verify this feature, we retrained LoRAs for the QNLI, RTE, and SST2 tasks with r = 16 and a = 32, and merged them with LoRAs from other tasks (r = 8, a = 16) to obtain a new LoRA. Since other model merging methods require the merged LoRAs to have the same architecture, we only compared our method with the Ensemble method. As shown in Tab.5, the results demonstrate that our method can effectively merge heterogeneous LoRAs and achieves better overall performance than the Ensemble method."}]}