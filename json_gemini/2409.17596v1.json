{"title": "Subjective and Objective Quality-of-Experience Evaluation Study for Live Video Streaming", "authors": ["Zehao Zhu", "Wei Sun", "Jun Jia", "Wei Wu", "Sibin Deng", "Kai Li", "Ying Chen", "Xiongkuo Min", "Jia Wang", "Guangtao Zhai"], "abstract": "In recent years, live video streaming has gained widespread popularity across various social media platforms. Quality of experience (QoE), which reflects end-users' satisfaction and overall experience, plays a critical role for media service providers to optimize large-scale live compression and transmission strategies to achieve perceptually optimal rate-distortion trade-off. Although many QoE metrics for video-on-demand (VoD) have been proposed, there remain significant challenges in developing QoE metrics for live video streaming. To bridge this gap, we conduct a comprehensive study of subjective and objective QoE evaluations for live video streaming. For the subjective QoE study, we introduce the first live video streaming QoE dataset, TaoLive QoE, which consists of 42 source videos collected from real live broadcasts and 1,155 corresponding distorted ones degraded due to a variety of streaming distortions, including conventional streaming distortions such as compression, stalling, as well as live streaming-specific distortions like frame skipping, variable frame rate, etc. Subsequently, a human study was conducted to derive subjective QoE scores of videos in the TaoLive QoE dataset. For the objective QoE study, we benchmark existing QoE models on the TaoLive QoE dataset as well as publicly available QoE datasets for VoD scenarios, highlighting that current models struggle to accurately assess video QoE, particularly for live content. Hence, we propose an end-to-end QoE evaluation model, Tao-QoE, which integrates multi-scale semantic features and optical flow-based motion features to predicting a retrospective QoE score, eliminating reliance on statistical quality of service (QoS) features. Extensive experiments demonstrate that Tao-QoE outperforms other models on the TaoLive QoE dataset, six publicly available QoE datasets, and eight user-generated content (UGC) video quality assessment (VQA) datasets, showcasing the effectiveness and feasibility of Tao-QoE.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the rapid growth of mobile devices and advance-ments in wireless networks in recent years, people can now watch video content on mobile devices anywhere and anytime. Streaming media technologies play an important role in ensuring that users can view such content smoothly and in real-time without waiting for complete file downloads. Specifically, the streaming media content captured by the cameras or the third-party streaming media content is encoded and segmented into data fragments. These data fragments are then transmitted to the server using appropriate transport protocols such as HTTP, HLS, RTMP, or RTSP. Users utilize client devices (e.g., mobile phones, tablets, computers, network TVs) to send requests over the Internet for accessing streaming media content. Upon receiving a client request, the server employs a content distribution network (CDN) to distribute the corresponding data fragment to the requesting client device. After decoding and rendering, the data is converted into audio and video content that the user can watch and listen to [1]. Video on Demand (VoD) and live streaming are two prevalent methods of streaming media technology. On the other hand, Live Streaming involves real-time transmission and display of audio or video content over the Internet, ensuring synchronized delivery for viewers to experience events as they unfold.\nLimited network resources and fluctuations in client net-works can result in distortions, such as degradation of video quality and stalling events, leading to a decline in the end users' Quality of Experience (QoE). [2] Therefore, it is crucial for streaming media content providers to comprehend the factors that influence user QoE and allocate resources appropriately to enhance their satisfaction. [3] In the domain of video streaming media, Quality of Experience (QoE) is associated with numerous indicators. Among them, Video Quality Assessment (VQA) plays a pivotal role in perceiving visual quality. However, the user's QoE is highly susceptible to disruptions such as stalling events and bit rate switching caused by network fluctuations. These factors are not evaluated by conventional VQA methods. QoE represents a comprehen-sive metric that encompasses video quality along with other distortions like stalling events and quality switching.\nIn contrast to the well-established VoD and QoE industry, the research on live streaming QoE remains insufficient, pri-marily due to two key factors.\n\u2022 Limited publicly available live video databases. Current QoE databases like LIVE-NFLX and waterlooSQOE predominantly resemble VoD setups. Moreover, publicly available databases fail to accurately capture video stalling manifestations in live streaming scenarios where network issues often result in unexpected fluctuations in frame rate and frame skipping. These distortions as shown in Fig. 1.\n\u2022 Unsatisfied qoe model mechanism. Publicly available QoE models such as KSQI and GCNN-QoE heavily rely on statistical data (e.g., stallinging time and location, bitrate), which are challenging to obtain beforehand in real-life situations, thus rendering them unsuitable for real-time live broadcast scenarios.\nTo address this issue, we have developed an extensive and authentic live broadcasting database known as the Tao Live QoE Database. We collect live videos from the Tao Live APP and artificially induce stalling events by manipulating the presentation time stamp (PTS) of the videos. It is important to note that our database encompasses various quality degra-dations commonly encountered in live broadcasts, including compression artifacts, stalling distortions, accelerated frame rates, and frame skipping. Furthermore, all videos in our database undergo rigorous subjective testing to obtain compre-hensive retrospective QoE scores which are subsequently vali-dated. Additionally, we introduce TAO-QoE, a pioneering deep learning-based approach capable of directly predicting QoE scores from video inputs without relying on supplementary statistics. This model performs feature extraction and fusion for assessing video presentation quality, quality switching dynamics, and occurrence of stalling events ultimately leading to retrospective QoE score predictions.\nThe main contributions of this work are summarized as follows:\n\u2022 We establish a large-scale live video database. The study involved the collection of 42 high-quality videos, which were subsequently subjected to compression artifacts and stalling events by adjusting the Constant Rate Factor (CRF) parameters and presentation time stamp for each video frame. As a result, a total of 1,155 distorted live streaming videos were generated.\n\u2022 We carry out a well-controlled subjective experiment. We invited 20 participants to take part in the subjective exper-iment, resulting in a total of 23,100 subjective annotations collected to generate the QoE scores for live videos.\n\u2022 We propose TAO-QoE, a deep learning-based model for predicting Quality of Experience (QoE) in live video streaming. This model achieves optimal performance on public databases without the need for statistical information."}, {"title": "II. RELATED WORK", "content": "Over the past 15 years, numerous publicly available QoE databases have been developed to tackle QoE challenges.\nIn early research, video Quality of Experience (QoE) was often determined based on a set of statistical features. These studies attempted to fit certain video transmission-related metrics into a mathematical formula to predict video QoE [4]\u2013[7]. However, the video QoE is influenced by multiple factors, including presentation quality, smoothness, video qual-ity switching, and video stuttering. These factors are closely related to users' viewing environments, personal preferences, and perceptual abilities. Therefore, relying solely on statistical features makes it difficult to capture users' subjective expe-riences, and more detailed consideration of user perception and evaluation is needed. In pursuit of a better evaluation of the impact of video presentation quality on the overall Video Quality of Experience (QoE), an increasing number of studies have embraced the integration of Visual Quality Assessment (VQA) within the QoE assessment framework. Depending on the availability of reference videos during the evaluation process, video quality assessment can be classified into three categories: full-reference(FR) [8]\u2013[12], reduced-reference(RR) [13]\u2013[16], and no-reference(NR) approaches [17]\u2013[24]. Both Spiteri2016 [25] and Bentaleb2016 [26] regard the average bitrate of the video experienced by the user and the duration of the rebuffer events as the influencing factors of QoE. Duanmu et al. devised a QoE algorithm named SQI, which combines the FR VQA algorithm with video stalling quantification information to predict the QoE scores of videos [27]. In Video Assessment of Temporal Artifacts and Stalls (Video ATLAS) [28], Bampis et al. unify modeling of video presentation quality, stall-related features, and memory-related features of video. Subsequently, et made improvements to the SQI algorithm and developed the KSQI algorithm, which takes video presentation quality(VMAF), rebuffering, and quality adaptation (switching between profiles) into consid-eration [29]. With the vigorous development of deep learning technology, more and more researchers apply convolutional neural network(CNN) and recurrent neural network(RNN) to the prediction of video QoE. GCNN-QoE [30] and DA-QoE [31] both perform feature extraction and fusion on statistical features, then uses GRU to process the features and finally returns the QoE score. DeSVQ [32] feeds the high-level spatio-temporal features extracted by CNN and the low-level features measured by VQA to LSTM in turn, and finally returns the QoE score. The above three models have two common features that use statistical features and RNN. In [33], Pengfei Chen et al. constructed an end-to-end framework named TRR-QoE, which combines feature extraction, processing and QoE prediction. In Chunyi Li et al. [41] employ ResNet-50 for frame feature extraction, fuse statistics like resolution and rebuffering, and regress QoE using Support Vector Regression (SVR)."}, {"title": "III. LIVE STREAMING SCENE DATABASE CONSTRUCTION", "content": "Despite the abundance of QoE and VQA databases, these databases suffer from certain limitations: i) insufficient diver-sity in source videos, resulting in a lack of complex human interaction broadcasts; ii) stalling events are predominantly represented as repeated frames, often caused by network issues leading to uneven Presentation Time Stamp (PTS) distribution; iii) live broadcast scenarios typically involve brief rebuffering periods. However, state-of-the-art QoE and VQA databases like WaterlooSQOE-III and WaterlooSQOE-IV do not encom-pass stalling events lasting less than one second, which is a common occurrence in real live broadcasts. Furthermore, after such stalling events in live scenarios, there is often a transition to accelerated video playback characterized by an increased frame rate or frame skipping. These variations in frame rates are not addressed in publicly available QoE databases that usually maintain a fixed frame rate.\nTo address these challenges, we established the TaoLive QoE database, which encompasses a larger corpus of source videos and incorporates more authentic setups involving ac-celerated frame rate playback, frame skipping, and other related techniques. Additionally, we manipulated PTS of video frames to accurately simulate stalling events, thereby closely resembling real-life streaming scenarios. Fig. 2 illustrates the occurrence of stalling events, accelerated frame rate playback, and frame skipping in the TaoLive QoE database. The blue video frames represent the frames played according to the source video frame rate, while the red video frames depict the displayed frames during stalling events. Additionally, green video frames indicate fast playback (accelerated frame rate), and yellow video frames signify skipped frames due to prolonged stalling duration."}, {"title": "A. Motivation:", "content": "Despite the abundance of QoE and VQA databases, these databases suffer from certain limitations: i) insufficient diversity in source videos, resulting in a lack of complex human interaction broadcasts; ii) stalling events are predominantly represented as repeated frames, often caused by network issues leading to uneven Presentation Time Stamp (PTS) distribution; iii) live broadcast scenarios typically involve brief rebuffering periods. However, state-of-the-art QoE and VQA databases like WaterlooSQOE-III and WaterlooSQOE-IV do not encom-pass stalling events lasting less than one second, which is a common occurrence in real live broadcasts. Furthermore, after such stalling events in live scenarios, there is often a transition to accelerated video playback characterized by an increased frame rate or frame skipping. These variations in frame rates are not addressed in publicly available QoE databases that usually maintain a fixed frame rate.\nTo address these challenges, we established the TaoLive QoE database, which encompasses a larger corpus of source videos and incorporates more authentic setups involving ac-celerated frame rate playback, frame skipping, and other related techniques. Additionally, we manipulated PTS of video frames to accurately simulate stalling events, thereby closely resembling real-life streaming scenarios."}, {"title": "B. Database Construction", "content": "1) Source Video: We carefully selected 42 high-quality live videos encoded in H.264 from the Taobao Live app, encom-passing various resolutions and frame rates. Each video has a duration of 10 seconds. To ensure optimal video performance, we excluded any videos with stall events. Specifically, we employed two resolutions (1080p and 720p) and three frame rates (20fps, 25fps, and 30fps), resulting in seven source videos for each combination of resolution and frame rate. In total, we collected a comprehensive set of 42 source videos.\n2) Distortion added: The types of distortion we incorpo-rated include compression, stalling events, and accelerated playback following a stall event. Due to the real-time nature of live broadcasting, once the stall event concludes, video playback resumes with certain frames being played at an accelerated rate. Frame skipping occurs when the duration of a stall event exceeds a specific threshold. The speed and duration of fast playback are generally determined by the buffer ratio on the playback side and the length of the stall event. To simulate videos with varying presentation qualities, we compressed these source videos using FFmpeg with a Constant Rate Factors (CRF) set to 15, 22, 27, 32, and 37. The 7 source videos for each frame rate are compressed based on the aforementioned 5 CRFs. Subsequently, we manually introduce stall events to these compressed videos. To ensure that no secondary compression occurs during the addition of stalling events, we utilize FFmpeg to modify the presentation time stamp (PTS) of the video in accordance with the designated stalling mode. This mode encompasses various combinations of stall event duration and frequency. The duration of a stall event is categorized into four levels: short (s) (0.5s or 1s), medium (m) (1.5s, 2s or 2.5s), long (1) (3s, 3.5s, 4s or 4.5s), and extra long(el) (5s, 5.5s or 6s). The maximum limit for stall event occurrences is set to 3. As depicted in Table II, there are a total of 21 combinations observed. The acceleration rate (AR) applied to expedite video playback following the termination of a stall event is configured as 1.1, 1.25, 1.5, 1.75, and 2.25.\nA stall event is generated as follows. $F = {f_1, f_2, ..., f_n}$ are all video frames of compressed video. $P = {P_1, P_2, ..., P_n}$ is PTS of all compressed video frames. n is the number of compressed video frames. $L = {1_1, 1_2, ..., 1_m}$ is the time point when the set stall event occurs. $T = {t_1,t_2,...,t_m}$ is the duration of stall event. m is the number of stall events. First, the index of the stall video frame is calculated according to the set time point of occurrence of the stall event and the video frame rate. The index of the stall video frame $SF = {$f_1, f_2,..., $f_m}$ is given by\n$sf_j = l_i \\times framerate \\space j = 1,2..., m$\nSecondly, the PTS delay $D = {d_1, d_2, ..., d_m}$ for all video frames after this frame is calculated by\n$d_j = \\frac{t_j}{timebase} \\space j = 1,2..., m$\nWhere timebase is the time base of compressed video. The PTS delay of all video frames of the compressed video $AD = {ad\u0131, ad2, ..., adn}$is calculated as\n$adi = {\\begin{cases} 0 \\quad i < sf_1 \\\\ \\sum_{k} d_k \\quad sf_k < i \\leq sf_{k+1} \\\\ \\sum_{m} d_k \\quad i > sf_m  \\\\\\end{cases}}$\nWhere i represents the frame index of the compressed video, adjustments to certain PTS values are necessary in order to ensure smooth playback following a stall event. Specifically, the PTS interval for fast-playing video frames should be reduced based on the predetermined acceleration rate, while maintaining unchanged intervals for other video frames. The PTS interval is a constant within the FFmpeg structure AV-Packet (replaced by pkt.duration below). The frame index, PTS, and pkt.duration of the video are calculated as PTS = index pkt.duration. The total number of accelerated playing video frames following the occurrence of a stall event in this database is directly related to the duration of said stall event. It represents the cumulative count of accelerated playing video frames required to fully catch up with the live progress that was delayed due to the stall event. The total number of fast-playing video frames $QN = {qn_1, qn_2, ..., qn_m}$ is given by\n$qnj = \\frac{t_j \\times AR \\times framerate}{AR-1} \\space j = 1,2..., m$\nThe PTS interval between the current and subsequent video frames is reduced according to AR when fast forwarding, while the PTS intervals of the remaining frames remain unchanged and equal to pkt.duration. Then recalculate the PTS of the video $SP = {sp_1, SP_2, ..., sp_n}$. Finally we add the PTS delay of all video frames $AD = {ad\u0131, ad2, ..., adn}$ and $SP = {SP_1, SP_2, ..., SP_n}$ to get the PTS of the output videos. Then we follow Algorithm 1 to add the PTS delay of all video frames and the PTS of the source video to get the PTS of the stalled video $SP = {SP_1, SP_2, ..., SP_n}$."}, {"title": "IV. DATA PROCESSING AND ANALYSIS", "content": "Based on the subjective test, we have gathered scores from all participants. Following the MOS calculation method outlined in [42]. Let $m_{ij}$ represent the raw subjective scores assigned by participant i to video j. We calculate the z-scores using\n$Z_{ij} = \\frac{M_{ij} - \\mu_{i}}{\\delta_{i}}$\n$\\mu_{i} = \\frac{1}{N_{i}} \\sum_{j=1}^{N_{i}} M_{ij}$\n$\\delta_{i} = \\sqrt{\\frac{1}{N_{i}-1} \\sum (mij - \\mu_{i})^{2}}$\nwhere $N_i$ denotes the number of test videos viewed by subject i.\nThe subject rejection procedure specified in the ITU-R BT500-11 is employed to eliminate scores from unreliable subjects [42]. Let $z_{ij}$ denote the discarded z-scores assigned by subject i to video j. Ultimately, the z-scores are rescaled to a linear rescaling to the range [1, 5], and the Mean Opinion Score (MOS) for test video j is calculated by the averaging z-scores $z_{ij}$ from $M_{i}$:\n$MOS_{j} = \\frac{1}{M} \\sum_{i=1}^{M} z_{ij}$"}, {"title": "V. PROPOSED METHOD", "content": "The network architecture comprises five components: a video restructuring sub-network, a semantic feature extraction sub-network, a multi-scale feature fusion sub-network, a flow motion feature extraction sub-network, and a feature regres-sion sub-network. It is shown in Fig. 5. When presented with a distorted video for evaluation, the video restructure sub-network initially analyzes the input frames to identify any stalling events caused by discontinuous PTS or accelerated playback. If such an event is detected, the corresponding stalling frames are supplemented based on the video's frame rate and duration of the stall event. The sub-network will gen-erate the restructured video frame sequence and its correspond-ing presentation timestamp (PTS). The semantic extraction sub-network aims to extract the perceived quality of all re-structured video frames. Subsequently, the multi-scale quality feature fusion sub-network processes the extracted features. The flow motion feature extraction sub-network extracts flow motion features from the restructured video frames. Finally, the feature regression sub-network predicts retrospective QoE scores by integrating information from two aspects. In the following sections, we provide a detailed description of each component."}, {"title": "A. Video Restructure", "content": "Mathematically, given the evaluated video consists of N input frames $F = {f_1, f_2, ..., f_N}$. We use FFmpeg to obtain the theoretical value $P$ of the PTS interval between frames and the PTS of all input video frames $P = {P_1, P_2, ..., P_N}$. If the actual PTS interval between the frame i and the frame i + 1 exceeds the theoretical value $P$, we believe that there is a stalling event between the frame i and the frame i + 1, and the frame i is defined as the stalling frame. Then calculate the number of times $r_n$ the model needs to read the stalling frame repeatedly.\n$r_n = [\\frac{P_{i+1} - P_{i}}{p}]$"}, {"title": "B. Semantic Feature Extraction", "content": "We employ the pre-trained Swin Transformer [43] as the underlying network architecture. The primary objective of the semantic feature extraction network is to acquire multi-scale semantic features for each frame. It should be noted that diverse semantic content can exert varying influences on human tolerance towards distinct distortions [23]. Furthermore, incorporating semantic information can aid in de-tecting and measuring perceptual distortions, making it a reasonable addition to the assessment of presentation quality. Additionally, presentation quality is hierarchical in nature, with perception occurring from low-level features to high-level ones. To account for this hierarchy, we splice multi-scale features extracted by the four stages of Swin Transformer and use them as frame-level semantic features.\nMathematically, given the evaluated video consists of 2L input frames, we feed these RE frames $V = {V_1, V_2, ..., V_{2L} }$ into the semantic feature extraction network. $SF = {SF_1, SF_2, ..., SF_{2L}}$ is the output multi-scale semantic fea-tures.\n$SF_i = a_1 \\bigoplus a_2 \\bigoplus a_3 \\bigoplus a_4 \\space i = 1,2..., 2L$\n$a_j = GAP(L_j(v_i)) \\space j = 1,2,3,4$\nwhere $SF_i$ indicates the extracted semantic features from i-th frame $v_i$. GAP(\u00b7) represents the global average pooling operation. $L_j (v_i)$ is the feature of the j-th stage output of Swin Transformer. $a_j$ denotes the average pooled features from $L_j (v_i)$."}, {"title": "C. Flow Motion Feature Extraction", "content": "Live broadcasts are often affected by unstable shooting environments and restricted network conditions, resulting in motion distortions such as jitter and stagnant events. Therefore, relying solely on semantic features at the frame level is insufficient to accurately capture these distortions. While some videos may exhibit high presentation quality, the presence of jitter and stalling events significantly diminishes the Quality of Experience (QoE) for such live broadcasts. Hence, it is imperative to incorporate motion features in QoE prediction models. To detect stalling events effectively, we initially seg-ment the extracted optical flow based on the Presentation Time Stamp (PTS) of Reference Frames (RE frames), with each segment having a duration of 1 second. Subsequently, inter-frame optical flow images are extracted from each segment using a pretrained PWC-Net [44].\n$C_i = \\Gamma(V) \\space i = 1,2..., M$\nwhere $C_i$ represents the extraction and clipping operations of inter-frame optical flow images for RE frames. We employ PTS to perform the clipping operation on inter-frame optical flow images. In case of accelerated video playback, the number of optical flow images contained in $C_i$ may vary. Subsequently, the inter-frame optical flow images are resam-pled at a rate of 16fps for each clip, followed by leveraging a pre-trained 3D-CNN backbone ResNet-18 [45] to capture motion distortion at the clip level.\n$MF_k = (C_k) \\space k = 1,2..., 2L/r$\nThe flow motion features extracted from clip $C_k$ are denoted as $MF_k$, where \u0393(\u00b7) represents the operation of extracting flow and (\u00b7) represents the operation of extracting flow motion features."}, {"title": "D. Multi-scale Feature Fusion", "content": "The evidence from [47] demonstrates that there exists an inverse relationship between video quality and adaptation quality, where lower adaptation quality contributes to a more enjoyable viewing experience for the audience. Consequently, the absolute error of semantic features between consecutive frames can serve as an indicator of adaptation quality.\n$SF_{2m} = |SF_{2m} - SF_{2m-1}| \\space m = 1, 2..., L$\nwhere $SF_{2m}$ represent the absolute error between adjacent semantic features. Then the multi-scale fusion can be derived as:\n$STF_{2m} = W(\\varphi(SF_{2m}) \\bigoplus \\varphi(SF_{2m})) \\space m = 1,2..., L$\nwhere $\\bigoplus$ stands for the concatenation operation, $\\varphi(.)$ represents the learnable Multilayer Perceptron (MLP). W is a learnable linear mapping operation, and we finally obtain the spatio-temporal fused features $STF_k$. Then we connect the spatio-temporal fusion feature and the flow motion feature to get the final QoE feature.\n$QF_k = STF_k \\gamma(MF_k) \\space k = 1,2..., L$"}, {"title": "E. Feature Regression", "content": "After the aforementioned process of feature extraction and fusion, we employ a fully connected layer to perform regres-sion on the QoE features in order to obtain clip-level QoE scores.\n$Q_k = FC(QF_k) \\space k = 1,2..., L$\nwhere FC(\u00b7) is the fully-connected layers and $Q_i$ presents the QoE score of clip $C_k$. Finally, we average all clips of the input video to obtain a retrospective QoE score for that video.\n$\\overline{Q} = \\frac{\\sum Q_k}{z}$\nwhere $\\overline{Q}$ is the video QoE score and stands for the number of clips. We simply use the Mean Squared Error (MSE) as the loss function:\n$Loss = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{Q_i} - Q_i)^2$\nwhere n indicates the number of videos in a mini-batch, $\\hat{Q_i}$ and $Q_i$ are the MOS and predicted retrospective QoE score respectively."}, {"title": "VI. EXPERIMENT", "content": "In this section, we initially provide a comprehensive description of the experimental setup. Subsequently, we evaluate the performance of our proposed TAO-QoE model and compare it with other prominent QoE models using our Tao Live QoE Database as well as publicly available QoE and VQA databases. Furthermore, ablation experiments are conducted to investigate the individual contributions of different sub-modules towards enhancing the overall model performance."}, {"title": "A. Implementation Details", "content": "The Tao-QoE model is implemented in PyTorch [51], with the Swin Transformer backbone utilizing pretrained weights from the ImageNet-1K database [48] for semantic feature extraction. Additionally, the ResNet3D-18 employs pretrained weights from the Kinetics-400 database [49]. The weights of both the multi-scale feature fusion sub-network and sub-feature regression are initialized randomly. Regarding the semantic feature extraction sub-network, it operates at the original resolution(1920 \u00d7 1080 or 1280 \u00d7 960) of input video frames. The flow motion feature extraction sub-network involves the extraction of optical flow maps from video frames at their original resolution, followed by resizing the optical flow map to 224 \u00d7 224 and inputting it into a ResNet-18 3D-CNN. Our model was trained and tested on a server equipped with an Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz, 128GB RAM, and NVIDIA Tesla V100 SXM2. The Adam optimizer [50] is utilized with an initial learning rate of 0.001. In case the training loss fails to decrease within 5 epochs, the learning rate is halved. The default number of epochs is set to 50. During the process of flow motion feature extraction, all videos are down-sampled to a frame rate of 16fps for ensuring consistent feature dimensions. Following standard practice, we split the database into train and test sets at an 80%-20% ratio. To assess the stability of the QoE model, we randomly perform 10 content-based splits and record their average result as the final performance. Specifically, for the WaterlooSQoE-IV database, we performed content-based splitting five times due to the limited availability of only 5 source videos in the database."}, {"title": "B. Benchmark Databases & Compared Models", "content": "We compared the currently available QoE and VQA mod-els on the QoE and VQA database respectively. In the field of QoE, we selected TaoLive QoE Database and five other available QoE databases, including LIVE-NFLX-II [40], WaterlooSQoE-I [35], WaterlooSQoE-II [36], WaterlooSQOE-III [37] and WaterlooSQoE-IV [38]. We compare the proposed model with the following QoE models:\n\u2022 Traditional models: P.1203 [34], SQI [29], Bentaleb2016 [26], Spiteri2016 [26], VideoATLAS [28], KSQI [29]\n\u2022 Deep learning models: GCNN-QoE [30], ASPECT [41]\nUnfortunately, the code for the GCNN model is not publicly available, thus hindering our ability to assess its performance on TaoLive QoE database.\nIn the domain of VQA, we selected 8 UGC VQA databases: LIVE-Qualcomn [52], CVD2014 [53], KoNViD-1k [54], VDPVE [55], LIVE-VQC [57], MSU [56], YouTubeUGC [58], LIVE-WC [59].\nWe compare the proposed method with the following no-reference models: LTVQM [69], VSFA [70], SimpleVQA [71], FastVQA [72]."}, {"title": "C. Criteria", "content": "Two types of evaluation criteria are employed to assess the performance of models. The first criterion, known as the Video Quality Experts Group (VQEG) criteria [60]\u2013[62], calculates a series of correlation values between predicted scores and Mean Opinion Scores (MOSs). The second criterion, proposed by Krasula et al. [63]\u2013[66], evaluates the classification abilities of models in distinguishing between two videos based on their quality. We refer to the first criterion as VQEG criteria and the second one as classification criteria.\nFor VQEG criteria, the model prediction scores should be initially mapped using the following function:\n$f(p) = \\xi_1(\\frac{1}{1 + e^{\\xi_2(p-\\xi_3)}}) + \\xi_4P + \\xi_5$\nwhere {\u03be|i = 1,2,3,4,5} is the parameter to be fitted, p and f(p) represent the prediction score and mapping score respectively. The mapped scores are then used to calculate four correlation values with MOSs, namely Spearman Rank-Order Correlation Coefficient (SRCC), Pearson Linear Correlation Coefficient (PLCC), Root Mean Squared Error (RMSE), and Kendall Rank-order Correlation Coefficient (KRCC). These statistical indices serve different purposes in assessing model performance. Specifically, PLCC reflects the linearity of algo-rithm predictions, SRCC indicates their monotonicity or pre-dictive correlation, while RMSE evaluates model consistency. An excellent model should achieve values close to 1 for SRCC, PLCC and KRCC.\nFor the classification criteria, we adhere to the procedures outlined in [60] and employ statistical methods from [67] to analyze subjective data for determining the significance of differences between each pair of stimuli. A confidence level of 95% is set. The entire dataset is partitioned into subsets based on significant differences and similarities. In a significantly distinct subset, we partition the stimulus pairs into groups based on positive and negative differences in MOS. A higher ability to discriminate dissimilar/similar pairs and supe-rior/inferior stimulus pairs indicates better model performance. Therefore, we employ the area under the ROC curve (AUC) as an evaluation metric for assessing classification performance of models. Furthermore, we compare AUC values obtained from different models to determine if there are statistically significant disparities in their performances [68]."}, {"title": "D. QoE Performance", "content": "1) VQEG Criteria: The experimental performance on 6 QoE databases is presented in Table IV. The following con-clusions can be drawn from the results. (1) Our proposed Tao-QoE model demonstrates the best performance among all models. Specifically, on the largest publicly available database, WaterlooSQoE-IV, SRCC and PLCC improve by 0.012 and 0.010, respectively. (2) Traditional QoE algo-rithms perform significantly worse than deep learning models on WaterlooSQOE-III, WaterlooSQoE-IV, and LIVE-NFLX-II databases due to the presence of various distortion types such as quality switching and rebuffering. Deep learning models have an advantage in perceiving these distortions compared to traditional models. (3) Unfortunately, we were unable to obtain the performance of GCNN-QoE on TaoLive QoE Database since it is not available. However, it is evident that our model can accurately evaluate the QoE of live videos compared with traditional QoE algorithms.\n2) Classfication Criteria: We present the performance eval-uation based on the classification criteria of all the afore-mentioned QoE models using the largest publicly available database, WaterlooSQoE-IV, as shown in Fig 6. From this figure, we can draw similar conclusions to those derived from the VQEG performance. Firstly, our proposed model Tao-QoE outperforms other QoE models by a significant margin in both the 'Different vs. Similar' and 'Better vs. Worse' classification tasks. Statistical analysis also demonstrates that our proposed model is significantly superior to other models on the WaterlooSQoE-IV database. Secondly, the AUC values for the 'Better vs. Worse' classification task are consistently higher than those for the 'Different vs. Similar' classification task, indicating that the latter is more challenging and there is still room for improvement in this area."}, {"title": "E. VQA Performance", "content": "We present the VQEG performance on 8 UGC VQA databases in Table V. Several observations can be made. Firstly, our proposed Tao-QoE model achieves the highest per-formance among all models. Particularly, on the three recently introduced larger-scale UGC databases (MSU, YouTubeUGC, and LIVE-WC), our model demonstrates significantly im-proved performance compared"}]}