{"title": "DEEP-LEARNING BASED DOCKING METHODS: FAIR\nCOMPARISONS TO CONVENTIONAL DOCKING WORKFLOWS", "authors": ["Ajay N. Jain", "Ann E. Cleves", "W. Patrick Walters"], "abstract": "The diffusion learning method, DiffDock, for docking small-molecule ligands into protein binding\nsites was recently introduced. Results included comparisons to a number of more conventional\ndocking approaches, with DiffDock showing nominally much better performance. Here, we employ a\nfully automatic workflow using the Surflex-Dock methods to generate a fair baseline for conventional\ndocking approaches. Results were generated for the common and expected situation where a binding\nsite location is known and also for the condition where the entire protein was the nominal target\nof docking. For the known binding site condition, Surflex-Dock success rates at 2.0\u00c5 RMSD far\nexceeded those for DiffDock (Top-1/Top-5 success rates, respectively, were 68/81% compared with\n45/51%). Glide performed with similar success rates (67/73%) to Surflex-Dock for the known binding\nsite condition, and results for AutoDock Vina and Gnina followed this pattern. For the unknown\nbinding site condition, using an automated method to identify multiple binding pockets, Surflex-Dock\nsuccess rates again exceeded those of DiffDock, but by a somewhat lesser margin. DiffDock made\nuse of roughly 17,000 co-crystal structures for learning (98% of PDBBind version 2020, pre-2019\nstructures) for a training set in order to predict on 363 test cases (2% of PDBBind 2020) from 2019\nforward. The Surflex-Dock approach made no use of prior information from pre-2019 structures,\neither for binding site identification or for knowledge to guide the docking and pose-ranking process.\nDiffDock's performance was inextricably linked with the presence of near-neighbor cases of close\nto identical protein-ligand complexes in the training set for over half of the test set cases. DiffDock\nexhibited a roughly 40 percentage point difference on near-neighbor cases (two-thirds of all test\ncases) compared with cases for which a near-neighbor training case was not identified. DiffDock\nhas apparently encoded a type of table-lookup during its learning process, rendering any meaningful\napplication scenario beyond its reach. Further, it does not perform even close to competitively with a\ncompetently run modern docking workflow.", "sections": [{"title": "1 Introduction", "content": "Deep learning approaches have recently generated significant interest in the field of computer-aided drug design\n(CADD), and this phenomenon is exemplified by the excitement produced by the introduction of the DiffDock method\n[1]. This short report is not intended to review the field or to differentiate substantive and meaningful deep-learning\ncontributions to CADD from lesser contributions. Rather, the focus is to make the general CADD community aware of\nthe proper context in which to interpret DiffDock's recently reported results. Our work should be considered alongside\nanother recent contribution, a paper from the Deane group, which offers additional context for understanding the results"}, {"title": "2 Results", "content": "Details of the specific computational procedures will be presented in the Data and Methods Section (with additional\ndetails in the Appendices), below. The goals of the work presented here were two-fold: 1) to establish a reasonable\nbaseline for the performance of conventional docking workflows on the DiffDock test set; and 2) to try to understand\nthe underlying performance driver for DiffDock's apparent success."}, {"title": "2.1 DiffDock Performance: As Reported compared with a \"Clean\u201d Test Subset", "content": "The PDBBind 2020 data set was used to train and test DiffDock, making use of roughly 17,000 protein-ligand complexes\nfor training and 363 complexes for testing (the \u201cFull Test Set\u201d). Here, we made use of a fully automated pipeline to\nprocess the PDB complexes, following previously reported protocols [3]. The processed PDB complexes consisted\nof protein and ligand files, with bond orders assigned and with protonation as expected in physiological pH. Ligand\nstructures were subjected to several quality tests, including cross-checking bond-order assignments against curated\nSMILES representations of corresponding the PDB HET codes. Those cases where quality tests were passed and where\nthe resulting ligand structures topologically agreed with those curated for the DiffDock benchmarking were kept as a\n\"clean\" set for testing conventional docking workflows (the \u201cClean Test Set\u201d). There were 290 such cases (80% of the\ntotal 363 original test cases)."}, {"title": "2.2 DiffDock Performance: Comparison to Conventional Docking Workflows", "content": "The original report of DiffDock made comparisons to a number of conventional docking methods, but the methods\nwere employed in an unconventional manner. Rather than providing an explicitly scoped binding site, a so-called\n\"blind\" docking procedure was used where docking was performed against entire protein structures. Consequently,\nwhereas mature docking methods in cognate ligand re-docking typically perform with roughly 60-80% success at the\n2.0\u00c5 RMSD threshold [4] for top-scoring poses, the DiffDock reported performance for GNINA, SMINA, and Glide\nranged from 19\u201323% [1]. In what follows, we will present comparative results for Surflex-Dock [5\u20137, 3], Glide [8, 9],\nAutoDock Vina [10, 11], and Gnina [12] using conventional cognate-ligand re-docking with a defined binding site.\nResults for Surflex-Dock, which has a well-studied pocket finding algorithm [13, 14], will also be presented in the\nunknown binding-site condition."}, {"title": "2.2.1 Comparison to Surflex-Dock", "content": "Figure 2 shows the comparison between pose prediction accuracy for two different testing conditions. At left, Surflex-\nDock was run using a conventional docking protocol where the location of the binding site was known. Cumulative\nhistograms of RMSD are shown for DiffDock (violet and green curves) and for Surflex-Dock (cyan and yellow curves).\nFocusing on the 2\u00c5 RMSD threshold, we see a roughly 25\u201330 percentage point advantage for Surflex-Dock, and an\neven larger gap at the 1.0\u00c5 threshold. The performance difference is both practically and statistically highly significant\n(p < 10-10 by paired t-test for both Top-1 and Top-5 pose prediction performance)."}, {"title": "2.2.2 Comparison to Glide", "content": "In the original DiffDock report, the authors made use of a procedure to run the Glide docking method on the entire\nprotein, rather than making use of a defined binding site, which is the use case for which the method has been developed\nand optimized. Consequently, the reported results (roughly 22% success at 2.0\u00c5 RMSD) were at variance with\nexpectations for cognate-ligand re-docking (closer to 60\u201380% success [4]).\nHere, we report Glide results for 272 complexes whose proteins were correctly processed using an automatic preparation\nprocedure (see Data and Methods). Of these, 260/272 yielded successful dockings, with the remaining 12 being assigned\nvalues of 20.0\u00c5 for Top-1 and Top-5 pose accuracy. Figure 3 shows the comparison between pose prediction accuracy\nfor the known binding-site condition. Cumulative histograms of RMSD are shown for DiffDock (violet and green\ncurves) and for Glide (cyan and yellow curves). Focusing on the 2\u00c5 RMSD threshold, we see a roughly 25 percentage\npoint advantage for Glide, and an even larger gap at the 1.0\u00c5 threshold. The performance difference is both practically\nand statistically highly significant (p < 10-7 by paired t-test for Top-1 pose prediction and p = 10-5 for Top-5)."}, {"title": "2.2.3 Comparison to AutoDock Vina and Gnina", "content": "The DiffDock report presented results for Smina and Gnina [15, 12], both of which were derived from AutoDock Vina\n[10, 11], a widely used open-source method. Gnina uses a form of deep learning to improve Vina performance, both for\npose prediction and for virtual screening applications [12]. Here, we report ligand re-docking results for both Vina\nand Gnina using the known binding-site condition. Of the full Clean Test Set of 290 complexes, 285 were processed\ncorrectly using the standard AutoDock procedures (see Data and Methods) to yield usable protein structures. For both\nVina and Gnina, all 285 complexes yielded docking results, with no failures.\nFigure 4 shows the comparison between pose prediction accuracy for the known binding-site condition. Cumulative\nhistograms of RMSD are shown for DiffDock (violet and green curves) and for Vina (left) and Gnina (right), with cyan\nand yellow curves to depict Top-1 and Top-5 pose prediction performance, respectively. Focusing on the 2\u00c5 RMSD\nthreshold, the results for both Vina and Gnina exceeded those of DiffDock by roughly 20-25 percentage points. As\npreviously reported [12], the Gnina method exhibited improved performance for top-ranked pose (right, cyan curve) over\nboth DiffDock (purple curve) and Vina (cyan curve, left-hand plot). Note that neither method approached the success\nlevels of either Glide or Surflex-Dock at the 1.0\u00c5 RMSD threshold. As with the previous comparisons, the performance\ndifferences, particularly between DiffDock and Gnina, were both practically and statistically highly significant. For\nVina Top-1 and Top-5 pose prediction, respectively, p values by paired t-test were < 10-7 and p = 10-9 for Top-5).\nFor Gnina, the p values were less than 10-10 in both cases.\nNote, however, that results for Gnina are difficult to interpret, due to its reliance on extensive training data for its scoring\nmethod, which implements a convolutional neural-network that was trained on hundreds of protein-ligand complexes.\nRather than this training resulting in a scoring function, as is used by Surflex-Dock, Glide, and Vina, this approach\nessentially learns characteristics of \u201cnative-like\" predicted ligand poses from explicit training data. It is possible that the\ninternal representation induced by the Gnina scoring function has, to some extent, memorized binding motifs that are\ndirectly represented in the DiffDock test set. This type of effect will be explored directly, with respect to DiffDock\nperformance in what follows."}, {"title": "2.3 DiffDock Performance: Effects of Near-Neighbor Training Cases", "content": "Recall that DiffDock was trained using a temporal 98/2% split of data, with roughly 17,000 PDB complexes serving as\ntraining and in the Clean Set, just under 300 structures for testing. Given the power of deep learning methods to build"}, {"title": "4 Conclusions", "content": "It is possible that the DiffDock training procedure has learned an interesting encoding of a large set of structures\nof protein-ligand complexes. However, what it appears to be doing cannot be considered to be either \"docking\" or\n\"identification\" of ligand binding sites. The reported results are overwhelmingly contaminated with near neighbors to\ntest cases within the extensive training set.\nFurther, results reported for mature and widely used docking methods presented an extremely misleading baseline\nof comparison. Surflex-Dock, Glide, Vina, and Gnina all performed much better than DiffDock on cognate ligand\nre-docking in the known binding-site condition. Surflex-Dock, which has a mature and automatic method to identify\nbinding sites, also performed much better than DiffDock in the unknown binding-site condition.\nThe primary reported results for DiffDock were artifactual, and the comparative results for other methods were\nincorrectly done. We do not mean to suggest that the study's authors were mendacious in any way. The intention of our\nreport is to be constructive in offering an instructive and carefully done analysis.\nThere can be value gained from different disciplines applying powerful general methods to new domains. With respect\nto machine-learning and computer-aided drug design, we would like to offer some observations to consider prior to\nmaking strong claims of superiority over pre-existing methods:\n\u2022 The CADD field has a long history, and the significant and challenging problems have changed over time.\nCognate ligand re-docking has not been an important problem for well over a decade. Predicting bound poses\nof ligands with novel structures compared to prior known compounds, and into (obviously) non-cognate\nprotein binding sites, is a challenging and important problem. It is important for newcomers to CADD to\nunderstand what has been done before and the problems that will make a true difference to the field if solved.\nIt is easy (and tempting) to unintentionally develop artificial benchmarks that do not reflect the ultimate\napplication of a method.\n\u2022 Small molecules are generally not experiments of nature, but are typically designed by people who have\nbiases both with respect to which targets are thought to be interesting and, more importantly, with respect to\nprior known ligands. So, very often, a compound made and co-crystallized with the protein for which it was\ndesigned to bind will have extremely similar prior analogs. Temporal train/test splitting, as was done in the\nDiffDock study, is the correct idea, but the split cannot be 98/2%, else simple memorization of the training set\ncan dominate results. A more reasonable temporal split for non-cognate docking is 25/75% [3].\n\u2022 Generally speaking, CADD methods are complex, particularly for docking, where aspects of protein preparation\nand binding site definition exist in addition to the challenges in appropriately representing and manipulating\nsmall molecules. Running an unfamilar method according to reasonable practices can be tricky and needs to be\ndone respecting the designed application scenario. When results are obtained that are at large variance to prior\npublished work, as in the DiffDock report with respect to Glide's performance on cognate ligand re-docking,\ncare should be taken to understand what may have gone wrong.\n\u2022 CADD is not like speech recognition or optical character recognition, where correctly predicting new examples\nthat are very close to prior known data, and which are drawn from the same population as the known data, is\nthe main use-case scenario. The goal in CADD is to make compounds that have different properties, often with\nnovel scaffolds, than prior known compounds for a target or to design compounds to modulate the activity\nof a new target. There are areas where predictions on subtle changes to a parent compound are important,\nfor example in affinity prediction, metabolism, toxicity, etc... Pose prediction for minor variants on a parent\ncompound is generally not difficult or challenging, except in cases where a small structural change leads to a\nlarge difference in bound pose. But that is exactly the case where ML approaches like DiffDock will not work.\n\u2022 The most interesting and challenging problems in CADD arise when data are sparse, not when many thousands\nof relevant data points exist. Methods in ML that rely on large data sets have interesting, successful, and\nimpactful applications (e.g. learned force-fields [20]), but care must be taken to identify problem areas where\nthe data requirements match the application scenario.\nPublication of studies such as the DiffDock report [1] are not cost-free to the CADD field. Magical sounding claims\ngenerate interest and take time for groups to investigate and debunk. Many groups must independently test and\nunderstand the validity of such claims. This is because most groups, certainly those focused primarily on developing\nnew drugs, do not have the time to publish extensive rebuttals such as this. Therefore their effort in validation/debunking\nis replicated many fold. The waste of time and effort is substantial, and the process of drug discovery is difficult enough\nwithout additional unnecessary challenges."}]}