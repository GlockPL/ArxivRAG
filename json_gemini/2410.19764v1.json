{"title": "Unraveling Movie Genres through Cross-Attention Fusion of Bi-Modal Synergy of Poster", "authors": ["Utsav Kumar Nareti", "Chandranath Adak", "Soumi Chattopadhyay", "Pichao Wang"], "abstract": "Movie posters are not just decorative; they are meticulously designed to capture the essence of a movie, such as its genre, storyline, and tone/vibe. For decades, movie posters have graced cinema walls, billboards, and now our digital screens as a form of digital posters. Movie genre classification plays a pivotal role in film marketing, audience engagement, and recommendation systems. Previous explorations into movie genre classification have been mostly examined in plot summaries, subtitles, trailers and movie scenes. Movie posters provide a pre-release tantalizing glimpse into a film's key aspects, which can ignite public interest. In this paper, we presented the framework that exploits movie posters from a visual and textual perspective to address the multilabel movie genre classification problem. Firstly, we extracted text from movie posters using an OCR and retrieved the relevant embedding. Next, we introduce a cross-attention-based fusion module to allocate attention weights to visual and textual embedding. In validating our framework, we utilized 13882 posters sourced from the Internet Movie Database (IMDb). The outcomes of the experiments indicate that our model exhibited promising performance and outperformed even some prominent contemporary architectures.", "sections": [{"title": "I. INTRODUCTION", "content": "THE film industry has experienced a significant transition from conventional theaters to online streaming/ OTT platforms like Amazon Prime Video, Netflix, YouTube, and Disney due to the exponential growth of the Internet, and lockdowns during COVID-19 boosted it further. Movie posters have evolved to this change. It was once designed primarily for large-scale displays in theaters or physical media. It must now effectively communicate its message in the digital posters or compact thumbnail format commonly used in OTT platforms. This transition requires a re-imagination of poster design, focusing on bold imagery, clear typography, and concise storytelling that can captivate viewers even on the limited screen of smartphones.\nMovie posters play a crucial role in shaping viewer expectations. They offer subtle clues about a film's genre, tone, and thematic elements, helping audiences decide whether a particular movie aligns with their interests and preferences [1]. For instance, Fig. 1.(a) a horror film poster showcases ominous imagery and dark colors, indicating to viewers that they can expect a fear experience, while 1.(b) a romantic poster utilizes bright colors and playful imagery to convey a cheerful and lighthearted tone. In online streaming platforms, users are bombarded with thumbnails and recommendations, and users scroll through these countless options [2]; movie posters have become an essential navigational tool since they can capture attention, arouse curiosity, and entice users to further exploration.\nOn the other hand, movie posters have a distinct advantage over trailers and plot summaries regarding their release timeline and visibility. While trailers and plot summaries typically become available closer to a film's release date, posters are often revealed well in advance, sometimes even months before the premiere. This early exposure allows posters to shape audience perceptions and generate anticipation long before other promotional materials are accessible. They also have the advantage of being widely used as thumbnails on OTT platforms, promotional materials, and social media, maximizing the movie's reach. In the literature, movie genre classification using trailer [3] [4] [5] and plot summary [6] [7] [8] have been explored. However, research in movie genre classification using only posters has been relatively scarce [9] [10]. This gap motivates us to take on the genre classification task through movie posters.\nAnalyzing movie posters for genre identification presents a set of unique challenges. Many movie posters have a complex nature and limited information. For example, Fig. 1.(c) has intricate background, and Fig. 1.(d) has significantly less visual (and textual) information. Posters often represent multiple genres simultaneously, complicating classification tasks. Multi-label classification is required to accurately capture the diverse genre elements present in a single poster. The analysis of movie posters for genre identification has traditionally focused on visual elements, overlooking the valuable information contained in textual details [11]. For example, Fig.1.(e), the visual appearance of the poster suggests an action or sci-fi genre,"}, {"title": "II. RELATED WORK", "content": "Traditionally, researchers have explored a range of visual inputs (e.g., trailers [5], movie-clips [12], facial frames [4]) textual inputs (e.g., plot summaries [6], screenplays [13]), and multimodal inputs integrating visual, textual, and audio data [14], [15]. In this paper, we concentrate on identifying multi-label movie genres solely from posters, which has been relatively limited in the existing literature [9].\nVisual Input: Zhou et al. [16] extracted global visual descriptors from trailer keyframes and utilized KNN classifier for genre identification. In [5] and [17], CNNs were employed to analyze trailers of LMTD (Labeled Movie Trailer Dataset) for detecting genres. A transformer network was used for genre identification from trailer clips in [3]. In [12], spatio-temporal features were extracted from video clips and hierarchical SVM was employed. Yadav et al. [4] analyzed facial expressions in movie trailers to predict emotions and identify genres using an Inception-LSTM architecture. Moving beyond the trailer, very few works have been found using movie poster images. Na\u00efve Bayes classifier with global features was used in [11]. In [9], CNN with GRAM layers was incorporated to capture style features from posters. YOLO and CNN models were engaged in identifying genres from detected objects of posters in [10].\nTextual Input: To analyze plot summaries for genre prediciton, BLSTM and GRU-based architectures were harnessed in [18] and [6], respectively. In [19] also, CNN with LSTMs and GRUs was employed for analyzing multi-lingual movie synopses. CNN with the flow of emotions was used in [8] to capture emotions and genres from plot synopses. Wehrmann et al. [7] adopted a CNN architecture enriched with a self-attention mechanism for genre classification from synopses. Gorinski et al. [13] utilized movie screenplays for predicting various movie attributes, including genre, mood, etc. Their approach employed a multi-label encoder and LSTM-based decoder.\nMultimodal Input: Past studies integrated multiple data sources, such as poster images, synopsis texts, trailer videos, and audio, to enhance genre classification. In [20] and [21], the Gated Multimodal Unit (GMU) and CentralNet were employed, respectively, leveraging textual features from movie synopsis/metadata via Word2Vec and visual features from posters using CNN. GMU with transformer was utilized in [22] to integrate text, image, video, audio, and metadata features from diverse modalities. In [14], fastText, fastVideo, VGG-19, and CRNN were employed to extract features from text (plot summary, metadata), trailers, posters, and audio, respectively. In [15], various classifiers, including LSTM, KNN, SVM, MLP, and decision tree, were utilized to extract features from synopses, subtitles, trailers, posters, and audio.\nPositioning of our work: In the literature, there is a noticeable scarcity of research that focuses exclusively on movie posters for genre identification. The focus on other modalities has overshadowed the rich source of information contained within posters. Unlike trailers and clips, which provide dynamic visual sequences, posters offer a static yet highly curated representation of a film's aesthetic and thematic elements. Similarly, while textual summaries and screenplays provide comprehensive narrative details, posters distill this information into concise visual and textual elements that convey genre information at a glance. Furthermore, prior studies have only used visual information in the poster and neglected textual information. Our study is one of the earliest attempts to utilize both visual and textual information from movie posters for classifying multi-label genres."}, {"title": "III. PROPOSED METHODOLOGY", "content": "In this section, we formulate our problem and then illustrate our proposed solution architecture.\nA. Problem Formulation\nIn this movie genre identification task, we are given a collection of N movie poster images, represented as $\\mathcal{I} = {I_1, I_2,..., I_N}$. Here, each $I_i \\in \\mathcal{I}$ is a RGB image. We are also given a set of genres denoted as $\\mathcal{G} = {g_1, g_2,...,g_M}$, with M indicating the total number of genres. Here, M = 13."}, {"title": "B. Solution Architecture", "content": "Our proposed architecture begins by inputting a poster image, which undergoes an OCR (Optical Character Recognition) to extract text. Subsequently, both visual and textual features are extracted to undergo cross-modal understanding, followed by processing through a feed-forward neural network. The workflow of our solution architecture is pictorially shown in Fig. 2, which we now present in detail.\nEncoding Movie Posters: Our proposed framework involves the analysis of a movie poster image $I_i$. The movie poster $I_i$ is processed through Gemini-Pro-Vision [24]-based OCR to extract text $T = (t_{i1}, t_{i2},..., t_{ia})$ which is tokenized into sub-word units, where a is the number of tokens in the text $T_i$. For visual details, movie poster $I_i$ divided it into distinct regions denoted as $r_i = {r_{i1},r_{i2},...,r_{ib}}$; for $r_{ij} \\in \\mathbb{R}$, where b signifies the total count of regions.\nThe integration of language and image understanding at the semantic level is seamlessly achieved through the Contrastive Language-Image Pretraining (CLIP) network [25]. This architecture facilitates effective transfer learning by leveraging a contrastive learning approach. CLIP is extensively pre-trained on a vast dataset of 400 million image-text pairs from the internet, incorporating contrastive learning with both image and text exposure. In alignment with these principles, our approach utilizes a pre-trained CLIP-based model [25] to extract features from both visual and textual details. ${f_{vi}, f_{ti}} = CLIP(I_i, T_i)$ where, $f_{vi} \\in \\mathbb{R}^{D_i}$ and $f_{ti} \\in \\mathbb{R}^{D_t}$ are the extracted visual and textaul feature, respectiviely. Here, $D_i$ and $D_t$ are the dimensions of visual and textual feature vectors, respectively.\nCross Modal Understanding: To capture the essence of integrating visual and textual features from movie poster $I_i$ to create a cohesive multimodal representation, we implemented the cross-modal understanding module. This module comprises three crucial sub-modules: (i) Alignment Module, (ii) Multi-head Cross Attention Module (MCAM), and (iii) Sequential Multi-head Self Attention Module (SMSAM), all of which are elaborated in detail below.\n(i) Alignment Module: Different modalities, such as text and images, capture information in diverse manners. Without proper alignment, the representations from these modalities lack comparability, resulting in inconsistent interpretations and hindering effective fusion [26]. Feature alignment enhances the model's ability to leverage complementary information from different modalities. The dimensions of visual ($f_{vi}$) and textual feature ($f_{ti}$) may be different. We applied layer normalization [27] and linear projection functions to align these features in the same dimension $D_a$.\n$f_{vai} = project(LN(f_{vi})); f_{tai} = project(LN(f_{ti}))$ (2)\nwhere, project and LN denote linear projection and layer normalization, respectively."}, {"title": "(ii) Multi-head Cross Attention Module (MCAM):", "content": "Cross attention proves to effectively exploit inter-feature relationships between visual and textual feature maps. Through a linear transformation, we derive $D_k$ dimensional queries (Q) and keys (K), and $D_v$ dimensional values (V) for visual feature $f_{vai}$, represented as $(Q_{vi} = f_{vai}W_{vq}, K_{vi} = f_{vai} \\cdot W_{vk}, V_{vi} = f_{vai}W_{vv})$.\nSimilarly, for the textual feature $f_{tai}$, we obtain $(Q_{ti} = f_{tai}W_{tq}, K_{ti} = f_{tai}W_{tk}, V_{ti} = f_{tai}W_{tv})$, where, ${W_{vq}, W_{vk}, W_{tq}, W_{tk}} \\in \\mathbb{R}^{D_a \\times D_k}$ and ${W_{vv}, W_{tv}} \\in \\mathbb{R}^{D_a \\times D_v}$ are learnable weight matrices. Then, we applied multi-head cross attention (MCA) to capture relationships and dependencies between different modalities. MCA consists of multiple cross attention mechanisms in parallel [28], where each attention head captures the essence of different aspects of inter-relationship between visual feature $f_{vai}$ and $f_{tai}$. Cross attention utilizes scaled dot product attention (SA) [28] to calculate the attention score. Here, we compute the visual ($Z_{vi}$) and textual ($Z_{ti}$) representations of cross attention as below:\n$Z_{vi} = \\psi_m (\\frac{Q_{ti} (K_{vi})^T}{\\sqrt{D_k}} V_{vi}); Z_{ti} = \\psi_m (\\frac{Q_{vi} (K_{ti})^T}{\\sqrt{D_k}} V_{ti})$ (3)\nwhere, $\\psi_m$ denotes softmax activation [27], and $X^T$ represents transpose of matrix X. The multi-head cross attention is empowered with multiple heads. The concurrent cross attention is collectively computed as follows:\n$Z_{vi}^c = [Z_{vi}^1, Z_{vi}^2,..., Z_{vi}^h] ; Z_{ti}^c = [Z_{ti}^1, Z_{ti}^2,..., Z_{ti}^h]$ (4)\nwhere, h signifies the number of attention heads; $Z_{vi}^c$ and $Z_{ti}^c$ are the concatenated visual and textual representations, respectively. We observed information loss after applying multi-head cross attention. To address this issue, we employed residual connection, which eventually improved the performance. After that, we fused the visual and textual representations by leveraging layer normalization (LN).\n$Z_{fvi} = LN (Z_{vi}^c + f_{vai}); Z_{fti} = LN (Z_{ti}^c + f_{tai});$\n$f_{vti} = LN (Z_{fvi} + Z_{fti})$ (5)"}, {"title": "(iii) Sequential Multi-head Self Attention Module (SM-SAM):", "content": "Multi-head Self Attention (MSA) can directly learn long-range dependencies and intra-relationships in input features. MSA also contains multiple parallel attention layers. Each head uses SA [28], wherein the input consists of queries and keys with dimensionality of $D_k$, and values with dimensionality of $D_v$. MSA is computed as follows:\n$SA(Q, K,V) = \\psi_m (\\frac{QK^T}{\\sqrt{D_k}}) V;$\n$MSA (Q, K, V) = [head^1, head^2, ..., head^h];$\n$head^l = SA(QW_i^Q, KW_i^K, VW_i^V)$ (6)\nwhere, a set of queries, keys, and values are simultaneously packed together and form corresponding Q, K, V matrices. $W_i^Q \\in \\mathbb{R}^{D_{ut} \\times D_k}, W_i^K \\in \\mathbb{R}^{D_{ut} \\times D_k}$ and $W_i^V \\in \\mathbb{R}^{D_{vt} \\times D_v}$ are learnable metrices. $D_{ut}$ is the dimension of $f_{uti}$.\nWe applied this MSA in sequential connection to a rich representation. It can be formulated as below:\n$z_i = f_{vti}; z_i = MSA (LN (z_{i-1})) + z_{i-1} ;$ (7)\nwhere, $l = 1, 2, ..., L$. Here L = 4 is the number of MSA used sequentially. After applying multiple MSA, we get the final fused representation: $y = LN (z_L)$.\nFeed Forward Neural Network (FFN): The final stage of our model incorporates an FFN comprising two hidden layers featuring 512 and 128 nodes, respectively, which uses the ReLU (Rectified Linear Unit) activation function [27]. The output layer contains M nodes and employs the sigmoid output function [27]. Conclusively, when presented with a movie poster image $I_i$, our framework provides a genre confidence score vector $\\hat{Y_i} = (\\hat{y_1},\\hat{y_2},..., \\hat{y_M})$.\nLoss Function: Multi-label datasets are mostly imbalanced and have more negative samples than positive ones for a particular class. We use Asymmetric Loss (ASL) to address this issue [29]. The main objective of ASL is to provide control over positive and negative sample imbalance in the optimization of multi-label classification. It prioritizes learning of more challenging samples, particularly those belonging to minority classes in the given datasets. ASL achieves this by adjusting specific weights to each class in the loss function, considering the complexity of the classification task. Consequently, the loss function assigns higher weights to minority class samples, which involve greater classification complexity, while allocating lower weights to majority class samples that are comparatively simpler to classify. Given the input movie poster image $I_i$, we can predict its genre confidence score vector $\\hat{Y_i} = (\\hat{y_1},\\hat{y_2},..., \\hat{y_M})$. using our framework. We employ the asymmetric loss to calculate the loss as below:\n$L_{ASL} = \\frac{1}{M} \\sum_{j=1}^M (L_j^+ + L_j^-); L_j^+ = (1-\\gamma^+) (1 - P_e(\\hat{y_i}^j))^{\\gamma^+} log (P_e(\\hat{y_i}^j));$\n$L_j^- = (1 - Y_i^j)^{\\gamma^-} (P_e(\\hat{y_i}^j))^{\\gamma^-} log (1 - P_e(\\hat{y_i}^j))$ (8)\nwhere, $Y$ denotes the ground truth for jth genre $g_j \\in G$ for image $I_i$ calculated through Eqn. 1. $P_e(\\hat{y_i}^j)$ is shifted probability of $\\hat{y_i}^j$. Shifted probability is the probability assigned to negative samples after applying hard thresholding using the margin $\\epsilon$ for adjustment. It is defined as below:\n$P_e (\\hat{y_i}^j) = max(\\hat{y_i}^j - \\epsilon, 0)$ (9)\nThe overall cost is calculated by taking an average of losses across all samples in the training dataset. Empirically, we set $\\gamma^+ = 3, \\gamma^- = 4, \\epsilon = 0.2$ for our experiments.\nPost-processing: In this step, we apply quantization on the confidence score vector $\\hat{Y_i}$ to get the final output vector $Y_i \\in {0,1}^M$, as below:\n$Y_i^j = {1, if \\hat{y_i}^j > \\tau}$ {0, otherwise (10)\nwhere, we have empirically chosen the threshold $\\tau$ to be 0.5."}, {"title": "IV. EXPERIMENTS AND DISCUSSIONS", "content": "In this section, we discuss experiments performed to check the efficacy of our proposed model. We begin by presenting the database employed to perform the experiments."}, {"title": "A. Database Employed", "content": "Our framework undergoes evaluation using the IMDb dataset, a collection of authentic movie poster images coupled with corresponding genres sourced from the Internet Movie Database (IMDb: https://developer.imdb.com/non-commercial-datasets). This dataset serves as a multi-label movie genre classification dataset. It comprises 4464 films, each associated with 1 to 5 posters. Each movie poster is linked to 1 to 3 genres, resulting in a total of 13882 unique movie posters. In total, there are 13 distinct genres, including action, adventure, animation, biography, comedy, crime, drama, fantasy, horror, mystery, romance, sci-fi, and thriller. In Fig. 3, we present the detailed genre-wise dataset distribution. We perform a random split of the dataset into a training set $DB_{train}$, validation set $DB_{valid}$ and testing set $DB_{test}$ at the ratio of 0.8, 0.1 and 0.1, respectively. As a matter of fact, we leverage $DB_{train}$ for model training."}, {"title": "B. Results", "content": "In this subsection, we start with experimental settings and evaluation metrics, followed by a comparative analysis, ablation study, and various additional evaluations.\nExperimental Settings: Our experimentation was conducted using the TensorFlow 2.8 framework, running on Python 3.9.13, on an Ubuntu 20.04.2 LTS-based machine. The machine specifications include an AMD EPYC 7552 Processor operating at 2.20 GHz with 48 CPU cores and 256 GB RAM equipped with a 40 GB NVIDIA A100-PCIE GPU.\nThroughout this paper, all presented results were obtained from the testing set $DB_{test}$. Model hyperparameters were tuned and set during training, with a focus on optimizing performance over the validation set $DB_{valid}$. All models are trained for epoch 100. The Adam optimizer parameters were chosen as follows: initial learning rate = $10^{-4}$; exponential decay rates for 1st and 2nd moment estimates, i.e., $\\beta_1$ = 0.9, $\\beta_2$ = 0.999; and zero-denominator removal parameter $(\\epsilon)$ = $10^{-8}$. For the early stopping strategy, we set the patience parameter to 10 epochs, and fixed mini-batch size of 32.\nEvaluation Metrics: For the overall model performance evaluation, we report the macro F1 ($F_m$) %, macro balanced accuracy ($BA_m$) %, micro F1 ($F_\\mu$) %, micro balanced accuracy ($BA_\\mu$) %, weighted F1 ($F_w$) %, weighted balanced accuracy ($BA_w$) %, samples F1 ($F_s$) %, samples balanced accuracy ($BA_s$) %, hamming loss (HL), and hit ratio of first genre (Hit) as our metrics [20], [30]. For genre-wise analysis, we use precision (P) %, recall (R) %, F1-score (F) %, balanced accuracy (BA) %, and specificity (Sp) % [30].\nComparative Study: We devised a comprehensive comparison by implementing various Baseline and State-of-the-Art (SOTA) methods. For our baselines, we leveraged VGG19 [32], ResNet50V2 [33], DenseNet121 [34], MobileNetV2 [35], InceptionV3 [36], and EfficientNetV2B2 [37] to extract visual features. We utilized Google Word2Vec (W2V) [38] [39] with Long Short Term Memory (LSTM) [40] to extract textual features. These features were fused using either concatenation or linear addition techniques and subsequently fed into a Feedforward Network (FFN). Additionally, we implemented BLIP [31] and CLIP [25] to extract visual and textual features. We then fused these features using linear addition and fed them into FFN. Furthermore, we implemented GMU [20] and CentralNet [21] as SOTA. It was originally designed to utilize movie posters and summaries to predict genres; we modified these methods to operate on movie posters and extracted text from posters. This adjustment allowed for a more direct comparison with our framework.\nTable I presents a comprehensive performance comparison between our proposed framework, baseline and SOTA methods. Notably, our framework demonstrates superior performance across all evaluation metrics compared to the baseline and SOTA methods. In the baseline comparison, transformer-based models, BLIP [31] and CLIP [25] significantly outperform traditional deep learning architectures. However, our framework surpasses even these transformer-based models across all evaluation metrics, showcasing its robustness and effectiveness. Compared to SOTA methods, our framework exhibits substantial improvements, surpassing them by at least 7.5% across all evaluation metrics. This notable performance enhancement can be attributed to incorporating two key modules, SMSAM and MCAM, which effectively enrich intra-feature and inter-modal relations, thereby enhancing the overall performance of our framework.\nModality Ablation Study: Table II illustrates the performance achieved with different modalities: text, image, and their combination. As our framework's core module, MCAM, requires two modalities to operate, we did not conduct experiments using single modalities (text or image) with it.\nIn the text modality, methods displayed subpar performance, mainly due to the limited textual content on most movie posters, which directly describes a genre of the movie without the context of visual features. This text typically consists of cast/crew names and movie titles. In the image modality, transformer-based models exhibited superior performance compared to conventional deep learning architectures. Notably, CLIP [25] emerged as the top-performing model in this modality. However, in the image + text modality, our framework demonstrated the best results, surpassing all models exploring a single modality.\nWe have 12 combinations of contemporary baseline deep architectures, and 2 transformer-based baseline methods (i.e.,"}, {"title": "V. CONCLUSION", "content": "In this paper, we focused on leveraging visual and textual information from movie posters for multi-label movie genre classification. We did not use other movie-related data such as trailers, clips, plot summaries, etc. We initially introduced MCAM (Multi-head Cross Attention Module), subsequently followed by SMSAM (Sequential Multi-head Self Attention Module), to learn the relations among genres and employed asymmetric loss to handle multi-label classification. We utilized a dataset comprised of 13882 movie posters sourced from IMDb. Our models demonstrated promising performances, outperforming major state-of-the-art architectures. Our findings indicate that relying solely on textual information from movie posters for genre classification yields subpar results. However, integrating textual information with visual information significantly enhances classification. In the future, we will endeavor to focus on improving the performance of specific genres: biography, mystery, and fantasy, where our current approach has exhibited average results."}]}