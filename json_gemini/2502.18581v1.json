{"title": "Scalable Best-of-N Selection for Large Language Models via Self-Certainty", "authors": ["Zhewei Kang", "Xuandong Zhao", "Dawn Song"], "abstract": "Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size N, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at https://github.com/backprop07/Self-Certainty", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have achieved impressive reasoning abilities, yet reliably producing accurate outputs for complex tasks often requires techniques to enhance inference-time performance (Wu et al., 2024; Xiang et al., 2025). Best-of-N selection, generating and selecting from multiple candidate responses, has emerged as a powerful paradigm for significantly improving reasoning accuracy (Snell et al., 2024). Current Best-of-N methods frequently rely on reward models, such as Outcome Reward Models (ORMs) (Cobbe et al., 2021a) and Process Reward Models (PRMs) (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2022), not only for output selection but also for data annotation to further refine LLM reasoning capabilities (Uesato et al., 2022; Wang et al., 2022).\nHowever, reward models introduce substantial computational and practical challenges. First, they are computationally expensive to train or fine-tune, often requiring as many parameters as the LLM itself (Wang et al., 2024). Second, reward models are vulnerable to distribution shifts, leading to unreliable predictions, and to \"reward hacking,\" where models exploit reward function weaknesses to achieve artificially high scores and degraded generalization (Eisenstein et al., 2023). While techniques such as reward model ensembles (Coste et al., 2023) provide partial mitigation, they further increase annotation and computational overhead.\nAs a lighter-weight alternative, Self-Consistency (Wang et al., 2022) aggregates multiple outputs using majority voting, obviating the need for explicit reward scoring. However, it is applicable only to tasks where answers can be directly compared through string matching, making it unable to differentiate different reasoning paths that yield the same final answer. Furthermore, it cannot be applied to open-ended generation tasks. Universal Self-Consistency (USC) (Chen et al., 2023) attempts to address this limitation by prompting the LLM to choose the most consistent response among the samples. Yet, USC's performance gains are constrained by the model's context length and inherent reasoning ability, with empirical evidence showing a decline when the sample size increases beyond a certain threshold (e.g., from N = 8 to N = 16 on the GSM8K dataset (Cobbe et al., 2021b)). Our research further confirms that USC can be ineffective for small models. Moreover, self-consistency and USC lack a direct quality score for responses, limiting their applicability in tasks such as candidate ranking.\nTo overcome these limitations, we propose leveraging the LLM's inherent probabilistic output for a more practical, general, and robust approach to Best-of-N selection and self-evaluation. We hypothesize that an LLM's probability distribution naturally encodes its certainty in a response. We introduce self-certainty, a novel metric that quantifies this confidence by measuring the divergence of the predicted token distribution from a uniform distribution. A distribution that diverges significantly from uniform indicates a more peaked and thus more certain prediction. As shown in Figure 1, self-certainty demonstrates a stronger signal for distinguishing correct responses. Notably, it incurs almost no computational overhead, as the token distribution is generated alongside the tokens during inference. Inspired by Borda Voting, a method for aggregating ranked preferences, we enhance self-consistency by incorporating self-certainty-based ranking. Our method assigns weighted votes based on self-certainty rank, using a scaling factor of $(N-\\text{ranking}+1)^p$, where p is a hyperparameter, effectively prioritizing more confident responses.\nWe rigorously evaluate our methods across diverse reasoning benchmarks, including LiveBench-Math (White et al., 2024), GSM8K (Cobbe et al., 2021b), MATH (Hendrycks et al., 2021), CRUXEval (Gu et al., 2024) and LiveCodeBench (Jain et al., 2024), spanning mathematical reasoning, code reasoning, and code generation. Our experiments reveal that self-certainty-based voting consistently outperforms self-consistency in Best-of-N selection of reasoning tasks, effectively adapting to varying sample sizes and question difficulties.\nThe key advantages of self-certainty are:\n\u2022 Scalability: Self-certainty scales efficiently with increasing sample size N, mirroring reward models in scalability but without their computational burden, enabling more reliable selection from larger candidate pools.\n\u2022 Orthogonal Enhancement to Chain-of-Thought: Self-certainty complements chain-of-thought reasoning (Wei et al., 2022). It integrates seamlessly with powerful reasoning models, outperforms CoT baselines, and beats self-consistency through weighted voting beyond simple majority rule.\n\u2022 Generalizability to Open-Ended Tasks: Self-certainty generalizes effectively to open-ended responses, where self-consistency is inapplicable. Our confidence-based Best-of-N strategy significantly surpasses both greedy decoding and USC in open-ended scenarios, while retaining USC's simplicity and universal applicability."}, {"title": "2. Related Works", "content": "Reward Models for Response Reranking and Selection. Evaluating the quality of LLM outputs is crucial for improving performance on complex reasoning tasks. One common method is using external models, such as verifiers or reward models, to rerank and select responses based on predefined criteria. Reward models, like Outcome Reward Models (ORMS) and Process Reward Models (PRMs), help guide this selection. Research shows that incorporating reward models enhances reasoning and enables selecting the best samples from multiple candidates (Lightman et al., 2023; Wang et al., 2024). However, reward models come with several limitations. They tend to be task-specific and are highly sensitive to the base model, making them less generalizable across different tasks (Eisenstein et al., 2023). Moreover, training reward models is computationally expensive, as recent studies suggest that reward models often require a similar number of parameters as the generating models to be effective (Wang et al., 2024). In contrast, our approach does not require additional training. Self-certainty, our proposed measure, leverages the logits generated by the LLM itself to assess the quality of responses quickly and efficiently, without the need for externally trained models.\nConsistency-Based Response Selection. It is often assumed that after sufficient training, a model gains a reasonable level of understanding regarding the correctness of its outputs (Liang et al., 2024). One approach to leveraging this understanding is self-consistency (Wang et al., 2022), which aggregates multiple model outputs to select the most common response. This method has demonstrated significant improvements, showing that the aggregated judgment of the model can be more reliable than a single output. This suggests that models may not always require an external evaluator to assess the quality of their responses. However, self-consistency has limitations. It requires that multiple outputs converge on the same final answer, making it difficult to generalize to tasks involving open-ended generation. Additionally, while universal self-consistency (USC) (Chen et al., 2023) has been extended to a variety of tasks, it struggles with scalability and lacks a measure of certainty, making it less suitable as a universal method for evaluating responses.\nSelf-certainty, in contrast, overcomes these limitations by directly measuring the confidence of each response based on its token distribution. It can both handle open-ended generation tasks and scale efficiently, providing a robust evaluation without the need for identical outputs or external models.\nConfidence Estimation for Model Responses. Confidence estimation is crucial for enhancing the quality of generated responses. Several approaches have been proposed to assess the reliability of model outputs. Self-Evaluation (Ren et al., 2023) prompts models to evaluate their own responses, using the probability assigned to a yes or no token as a confidence measure. BSDetector (Chen & Mueller, 2024) quantifies confidence by measuring the similarity between responses generated from the same prompt and then prompting the model to verify its own correctness. TrustScore (Zheng et al., 2024) estimates confidence by computing the likelihood of the model selecting its original response when hidden among modified-prompt distractors. While these methods provide valuable confidence measures, they require multiple prompt evaluations, making them less scalable and challenging to apply in Best-of-N selection. In contrast, self-certainty takes full advantage of the generated token distribution during sampling, eliminating the need for additional prompts. This makes our approach more efficient, scalable, and well-suited for Best-of-N selection."}, {"title": "3. Measuring Confidence of LLMs", "content": "This section explores and evaluates several candidate metrics for quantifying the confidence of LLMs' predictions. We assess traditional probabilistic measures like average log probability and perplexity, alongside distributional measures such as KL divergence and entropy. By comparing these methods, we aim to determine the most effective confidence measure for selecting the most reliable model outputs."}, {"title": "3.1. LLM Background", "content": "To understand how these confidence metrics are derived, we first briefly review the basics of LLM output generation. LLMs are typically based on the Transformer architecture and employ an autoregressive approach (Vaswani, 2017). Input sentences or prompts are tokenized into a sequence of discrete tokens $x = (x_1,x_2,...,x_n), x_i \\in V$ where V is the vocabulary. The LLM processes x to produce a sequence of logits: $L_x = (l_1,l_2,...,l_n), l_i \\in R^V$ where $|V|$ is the vocabulary size. Each logit vector $l_i$ represents the model's prediction for the i-th token, conditional on preceding tokens. For each position i, applying the softmax function to $l_i$ yields a probability distribution over the vocabulary: $p(\\cdot|x_1,...,x_{i-1}) \\in [0,1]^V$. For the last input token $x_n$, the logits $l_n$ produce the probability distribution: $p(x) \\in [0,1]^V$, representing the likelihood of each token in V being the next token after x. After sampling an output sequence $y = (y_1, y_2,..., y_m)$, the probability distribution for generating the i-th token $y_i$, conditional on input x and prior outputs $Y_{<i} = (y_1, ..., y_{i-1})$, is: $p(\\cdot|x, y_{<i}) \\in [0, 1]^V$. This distribution reflects the model's belief about the next token given the prompt and generated sequence so far."}, {"title": "3.2. Sentence-Level Probabilistic Confidence", "content": "Probabilistic confidence quantifies a model's certainty in its predictions by directly leveraging the probabilities assigned to sampled tokens.\nAverage log-probability. A common confidence measure is the average log-probability (AvgLogP) of the sampled tokens:\n$\\text{AvgLogP} := \\frac{1}{n}\\sum_{i=1}^{n} \\text{log} \\left[ p(y_i|x, Y_{<i})\\right]$\nwhere $p(y_i|x, Y_{<i})$ is the probability assigned to token $y_i$. A higher AvgLogP indicates that the model consistently assigns higher probabilities to its generated tokens, reflecting greater confidence.\nPerplexity. Perplexity is one of the most common metrics for evaluating language models and is closely related to uncertainty. It is defined as the exponentiated average negative log-likelihood of a sequence:\n$\\text{Perplexity} := \\text{exp} \\left(-\\frac{1}{n} \\sum_{i=1}^{n} \\text{log} \\left[ p(y_i|x, Y_{<i})\\right] \\right)$        (1)\nSince Perplexity = exp(-AvgLogP), both measures are equivalent when selecting the most certain response. In the following sections, we use negative perplexity for Best-of-N selection to assess the effectiveness of these confidence measures. However, while perplexity is widely used in LLM evaluations, it has been shown (Hu et al., 2024) to fail in capturing a model's ability to understand long contexts, highlighting the need for alternative measures."}, {"title": "3.3. Distributional Confidence", "content": "Distributional confidence measures broaden the scope beyond just the probabilities of sampled tokens. They consider the entire probability distribution over the vocabulary at each generation step, aiming to capture a more holistic view of the model's certainty.\nA sentence-level distributional confidence measure can be defined as:\nDistributional-Confidence := F(f(Py|x))"}, {"title": "3.4. Our Primary Metric: Self-certainty", "content": "Our empirical evaluations, as shown in Figure 1 and 4, reveal that the KL-divergence-inspired distributional confidence is more effective in distinguishing correct samples from incorrect ones and has the best performance of accuracy at larger N. Based on this finding, we define self-certainty as our primary confidence metric for best-of-N selection, and the sentence-level self-certainty can be formulated as:\n$\\text{Self-certainty} = \\frac{1}{\\eta n} \\sum_{i=1}^n \\sum_{j=1}^V \\text{log} \\left( V \\cdot p(j|x, Y_{<i}) \\right)$\nOne may use the cross entropy between the predicted distribution and a uniform distribution as an equivalent confidence measure. In fact, this measure differs from the corresponding KL-divergence only by an additive constant. Specifically, the self-certainty based on cross-entropy is given by: Self-certainty (CE) = $-\\frac{1}{\\eta n} \\sum_{i=1}^n \\sum_{j=1}^V  \\text{log}(p(j | x, y_{<i}))$,"}, {"title": "3.5. Analysis", "content": "Reward Models, such as PRMs and ORMs, usually assign the overall response rating based on the minimum reward across all steps (Lightman et al., 2023; Wang et al., 2024), which emphasizes the mistakes made by the model rather than its progress. The reason self-certainty methods that use the average as an aggregation function are also effective in identifying mistakes is that an error at an early stage reduces overall confidence, affecting both the current and subsequent steps. In the example shown in Figure 2, sample I contains a mistake in the first step. Although subsequent calculations are flawless, self-certainty assigns lower confidence for the follow-up reasoning compared to the correct reasoning path in sample II. In contrast, negative perplexity assigns a similar confidence score to the correct follow-up, regardless of whether it follows accurate or flawed reasoning. Moreover, distributional confidence can identify the correct reasoning path from the starting token, whereas negative perplexity only recognizes it later."}, {"title": "4. Self-Certainty with Voting Method", "content": "While self-certainty is more robust than other confidence measures, it still suffers from the risk of being skewed by samples with spuriously high confidence scores. We discover that self-certainty-driven Best-of-N selection performs worse than self-consistency in terms of accuracy on the math dataset with clear final answers, when using the same value of N, as shown in Table 2. However, this does not necessarily make it inferior to self-consistency. Self-consistency focuses on response-layer consistency in LLMs, whereas self-certainty aggregates information from decoding-layer consistency. By integrating both layers, we can potentially extract more reliable and consistent responses from multiple outputs with explicit answers.\nA common approach to combining majority voting with score-based selection is summing the scores over samples with the same answer. However, this method is sensitive to the scaling of the scores. Similarly, using the average confidence of the outputs may not adequately account for answers that are sampled more frequently, potentially underrepresenting the confidence in those answers. To address these limitations, and drawing inspiration from the Borda count, we propose the following combined method:\nFirst, we rank N outputs of models by confidence, obtaining a ranking $[r_1,r_2,...,r_n]$. We then assign votes to these ranked outputs using the following formula:\n$v(r) = (N - r + 1)^p$                (4)\nwhere r is the rank of the output (1 \u2264 r < N). Each valid response casts votes for its final answer, with total number of votes corresponding to its rank. The answer receiving the highest total votes will be considered the consensus answer among the given samples. When p = 0, Equation (4) reduces to majority voting (all ranked outputs receive an equal vote of 1). As p approaches positive infinity, the highest-ranked output dominates, making the selection equivalent to using distributional confidence alone."}, {"title": "5. Experiment Setup", "content": "This section presents the experimental setup for comparing various confidence measures in selecting reliable responses for reasoning tasks. We also extend the evaluation to additional datasets and explore combining self-certainty with voting methods for improved response selection."}, {"title": "5.1. Comparison of Confidence Measures", "content": "To assess the effectiveness of different candidate formulations in Section 3, we employ them to select the most confident response from a set of N outputs generated by our base model, Llama-3.1-8B-Instruct (Dubey et al., 2024). To mitigate the potential bias and data contamination arising from the model's training on publicly available datasets, we evaluate their performance using the LiveBench-Math dataset (White et al., 2024), which was released after the model's deployment.\nWe begin by sampling 64 responses using temperature = 0.6 and top-p = 0.9, and subsequently create subsets comprising the first N = 4, 8, 16, 32, 64 to perform Best-of-N selection. To ensure fairness, we test different measures' performance using the same set of samples. We mask out responses for which an answer cannot be extracted primarily because these outputs do not adhere to the format instructions to facilitate latter comparisons with majority voting. We also include a baseline, FirstAns, which simply selects the first extractable answer from the N outputs. This baseline serves as a reference point for quantifying the performance improvements achieved by our candidate expression-based selection strategy. The evaluation is implemented based on the ZeroEval framework (Lin, 2024), which provides a unified structure for performance evaluation. We repeat the experiment five times and report the average accuracy as the overall performance."}, {"title": "5.2. Validation on Additional Datasets and Combined Voting Methods", "content": "We conduct a series of experiments to evaluate the proposed self-certainty and Borda Voting methods against self-consistency, universal self-consistency (USC), greedy decoding, and FirstAns across various reasoning tasks.\nThe sampling strategy follows the procedures outlined in Section 5.1. For USC, we use the template from the original paper (Chen et al., 2023) (with minor wording modifications, as shown in Appendix B.2). To ensure a fair comparison, we assist USC in selecting the first valid response when it fails to choose one with an extractable answer.\nWe evaluate different methods using the Llama-3.1-8B-Instruct model across the following benchmarks:\n\u2022 Mathematical Reasoning: We utilize the LiveBench-Math dataset (White et al., 2024), the validation set of GSM8K dataset (Cobbe et al., 2021b) and the test set of MATH dataset (Hendrycks et al., 2021).\n\u2022 Code Reasoning: The CRUXEval-O benchmark (Gu et al., 2024) is employed, which involves predicting the output of Python codes.\n\u2022 Code Generation: We adopt the LiveCodeBench code generation benchmark (Jain et al., 2024) to assess the improvements introduced by our methods.\nFor all test models and datasets, we employ Chain-of-Thought reasoning (Wei et al., 2022), except for the code generation dataset. To evaluate the generalization of our measure across different training methodologies, particularly for the recent R1-series large reasoning models (Guo et al., 2025), we test our approach on DeepSeek-R1-Distill-Llama-8B using the MATH dataset (Level 3). Given the increased reasoning time required by this model, we conduct a single trial for this experiment. To further validate and assess generalizability, we apply both USC and self-certainty to the Qwen-2.5-Coder-32B-Instruct model (Hui et al., 2024), in addition to Llama-3.1-8B-Instruct, on the LiveCodeBench dataset."}, {"title": "6. Results and Analysis", "content": ""}, {"title": "6.1. Self-Certainty", "content": "KL-Divergence-Inspired Distributional Confidence Outperforms Other Measures in Best-of-N Selection. The results, shown in Figure 4, demonstrate that distributional confidence measures generally outperform perplexity when N\u2265 16. Among all candidate methods, KL divergence is the only measure that consistently improves as N increases to 32 and 64. This indicates that KL divergence serves as a more robust measure of confidence, offering better insight into the accuracy of responses. Equation 3 defines token-wise self-certainty as the KL divergence from a uniform distribution, with an alternative empirical distribution evaluated in Appendix A.3. The results confirm that KL with a uniform distribution, our original design, generalizes better.\nSelf-Certainty's Robustness to Reasoning Length in Response Selection. To understand why self-certainty outperforms other confidence measures in selecting better responses, we examine the relationship between reasoning length and confidence scores across different measures, as shown in Figure 5. The scatter plots reveal that longer reasoning lengths correlate with higher confidence scores in most metrics except self-certainty. This indicates that while other measures tend to favor samples with extended reasoning, self-certainty remains largely invariant to response length. This finding aligns with Basu et al. (2020)'s observation that, under low p values, perplexity decreases as the output length increases. Unlike other metrics that may conflate verbosity with correctness, self-certainty provides a more unbiased assessment of response quality. This robustness ensures that models cannot manipulate the confidence measure by simply generating more extended but meaningless reasoning paths.\nSelf-Certainty Effectively Separates Correct and Incorrect Responses. We analyze the distribution of self-certainty and negative perplexity across correct, incorrect, and no-answer responses using Level 4 of the MATH dataset for a balanced comparison. Figure 1 presents our findings. The histogram shows that self-certainty in both correct and incorrect responses follows an approximately normal distribution, with the correct group consistently exhibiting a higher mean. In contrast, while perplexity is able to identify better results when N is small (also in Figure 4), it fails to distinguish between correct and incorrect responses when applied to the full dataset with multiple outputs per question. Zhang et al. (2020) demonstrates that as perplexity declines, the quality of responses improves initially, then experiences a significant drop \u2013 an observation consistent with our findings. Notably, perplexity tends to assign higher confidence to no-answer responses, which often arise from self-repetition, early stopping, or excessively long reasoning chains that fail to follow prompt instructions. Given that Llama-3.1-8B-Instruct has a relatively low no-answer rate (<2%) in simpler Level 1 MATH problems, we attribute the no-answer rate primarily to the limited capacity of the model. It is unsurprising that negative perplexity favors these outputs, as avoiding difficult questions or repeating oneself is a common failure mode, even for humans \u2013 consistent with Basu et al. (2020), who showed that maximizing perplexity increases self-repetition. In contrast, self-certainty reliably assigns lower confidence scores to no-answer responses, effectively distinguishing them from correct answers. These observations are further strong evidence that self-certainty is a more effective measure of the certainty of a model as it is more closely correlated with the quality of responses."}, {"title": "6.2. Self-Certainty and Voting", "content": "Borda Voting in Combination with Self-Certainty. As discussed in Section 4, when responses contain explicit answers, self-certainty can be integrated with voting methods to enhance overall accuracy. We evaluate the effectiveness of Borda voting for combination voting in such cases, comparing it against majority voting, average self-certainty, and sum self-certainty on the MATH dataset, as shown in Table 1. Our results indicate that self-certainty-based Borda voting outperforms other voting methods."}, {"title": "Performance Comparison Across Four Datasets.", "content": "We examine the scaling properties of self-certainty and self-certainty-based Borda voting in Figure 6. The results indicate that self-certainty significantly outperforms both regular sampling and greedy decoding. Moreover, its performance improves considerably as N increases, demonstrating that self-certainty, as a measure of the model's confidence in its responses, provides valuable insight into output correctness. Additionally, Borda voting demonstrates better performance compared to self-consistency under various settings of p and N across all four datasets. This suggests that the self-certainty measure enhances the accuracy of the final-answer-based voting method by providing useful ranking information."}, {"title": "Optimizing the Borda Parameter p for Different N.", "content": "To investigate the relationship between the Borda parameter p in Equation 4 and the efficiency of the selection method, we plot line charts in Figure 7, showing the performance of different selection methods across varying sample sizes N. The result reveals that the optimal p increases from 0.5 to 1.2 as N increases from 8 to 64, suggesting that stronger control from self-certainty is needed with a larger N. For general use cases, grid search remains the most effective approach for determining the optimal p. Alternatively, a simple heuristic is to set p = 0.3 when N < 16 and p = 1.2 when N > 32, though this rule of thumb may vary depending on the model and the complexity of the questions."}, {"title": "6.3. Generalization", "content": "Generalization of Self-Certainty on Open-Ended Generation Tasks. Self-consistency struggles with creative, open-ended tasks such as code generation, where each sample produces a unique answer. In such cases, it defaults to standard sampling. USC and our method self-certainty offer solutions to this limitation. We compare self-certainty with USC on the code generation task of LiveCodeBench (Figure 8). Our findings reveal that USC underperforms compared to greedy decoding on the Llama-3.1-8B-Instruct model, likely due to the model's constrained ability to recognize consistency. This observation is reinforced by results from the larger Qwen model, where USC successfully outperforms greedy decoding. In contrast, self-certainty consistently outperforms greedy decoding on both models and surpasses USC on the larger Qwen-2.5-Coder-32B-Ins. Additionally, the performance of self-certainty improves as N increases.\nGeneralization of Self-Certainty on Reasoning Models. Recent research on DeepSeek-R1 (Guo et al., 2025) demonstrates that rule-based reinforcement learning and long-chain-of-thought (CoT) can significantly enhance the reasoning capabilities of LLMs. We evaluate the generalization of self-certainty on such reasoning models, with results for DeepSeek-R1-Distill-Llama-8B presented in Table 3. Our findings show that self-certainty consistently outperforms both greedy decoding and sampling, with performance further improving as N increases on reasoning models. Additionally, Borda voting with self-certainty surpasses the performance of self-consistency with proper p. These results reinforce previous observations, highlighting the robustness of our methods across various fine-tuning techniques."}, {"title": "7. Discussion and Future Research", "content": "While self-certainty proves effective in evaluating model outputs for open-ended generation tasks and demonstrates stable scalability, it has several limitations.\nFirst, self-certainty alone underperforms self-consistency on questions with unique answers (Section 6). Although self-consistency generally achieves better performance, recent studies suggest that properly trained ORMs and PRMs can surpass it (Lightman et al., 2023; Uesato et al., 2022). Current reward model training fine-tunes a base model to rate sentences based on token probabilities (Wang et al., 2024). A key insight from our findings is that treating the softmaxed logits of LLMs as a full probability distribution, rather than relying solely on individual token probabilities, leads to more robust certainty measurements. In self-certainty, using KL divergence between the output distribution and a uniform distribution provides greater stability than averaging log probabilities, suggesting that integrating this approach could enhance reward model effectiveness.\nSecond, this study explores a limited set of formulations for distributional confidence and Borda voting. The default choice of the averaging function for F in Equation 2 may not be optimal, and a broader selection of F could further improve self-certainty's accuracy. Likewise, the power function used for vote distribution in Equation 4 for Borda voting is intuitive but may not be the most effective formulation.\nBeyond these limitations, self-certainty presents exciting opportunities for future research. It not only encourages rethinking reward model designs but also offers potential applications in test-time scaling techniques (Snell et al., 2024), potentially reducing computational costs. Additionally, self-certainty could be leveraged for data labeling and reinforcement learning tasks (Bai et al., 2022; Ouyang et al., 2022). By maximizing token-wise self-certainty, we may enable more autonomous and efficient learning systems, paving the way for advancements in both model performance and computational efficiency."}, {"title": "8. Conclusion", "content": "In this paper, we introduce self-certainty and self-certainty-based Borda voting as novel approaches for evaluating and enhancing model response performance. Self-certainty functions as an internal measure of response quality, demonstrating robustness in several key aspects. Compared to traditional scoring methods, such as average log probability and perplexity, it offers superior scalability when applied to Best-of-N selection. Additionally, the ranking information provided by self-certainty improves chain-of-thought reasoning and outperforms universal self-consistency (USC) in code generation tasks. Its stability, flexibility, and generalizability make it applicable across a wide range of domains, with the potential to enhance the autonomous learning capabilities of LLMs."}, {"title": "A. More Experiment Results", "content": ""}, {"title": "A.1. Oracle Best-of-N Selection Performance and Scaling Effects on LiveCodeBench", "content": "In our experiment described in Section 5.2, we evaluate the performance of Llama-3.1-8B-Instruct and compare Borda voting and self-certainty against the upper bound of Best-of-N selection methods, as shown in Figure 9. While both methods demonstrate continued improvement as N increases, they remain significantly outperformed by the Oracle selection method, which assumes perfect knowledge of the correct answer."}, {"title": "A.2. Evaluation of Methods Across Difficulty Levels on the MATH Dataset", "content": "We conduct experiments to evaluate different methods across varying difficulty levels of reasoning problems. Figure 10 presents the performance of various methods on the MATH dataset at different difficulty levels. As question difficulty increases, the scaling effect of Borda voting and self-certainty becomes more pronounced, demonstrating their effectiveness in handling more challenging reasoning tasks."}, {"title": "A.3. Replacing Uniform Distribution with Empirical Distribution", "content": "In Equation 3, we define tokenwise self-certainty as the KL divergence between the generated token distribution and a uniform distribution, which quantifies deviation from random sampling. An alternative approach replaces the uniform distribution with an empirical token distribution estimated from training data. To evaluate the impact of this modification, we conduct the following experiment.\nWe first estimate token frequencies in the MATH training set by generating eight responses per question and averaging token occurrences. The resulting empirical distribution is approximated from these frequencies, with the 20 most frequent tokens shown in Figure 11. We then compute KL divergence between the generated token distribution and the empirical distribution, using this as the self-certainty measure for Best-of-N selection. This experiment was conducted for a single trial, with all other setup parameters as described in Section 5.1.\nResults show that replacing the uniform distribution with the empirical distribution has minimal impact on MATH test accuracy but leads to a noticeable performance drop on GSM8K, suggesting a sensitivity to distributional shifts. Thus, we recommend retaining the uniform distribution in Equation 3 for improved generalization."}, {"title": "B. Case Study and Examples", "content": "In this section, we present several examples to illustrate the practical behavior of self-certainty and universal self-consistency (USC). These examples are drawn from Llama-3.1-8B-Instruct generating responses to questions from the GSM8K and LiveCodeBench code generation datasets."}, {"title": "B.1. Self-certainty", "content": "In this example, self-certainty correctly assigns higher confidence to the correct answer. Notably, in Response 0, the model does not fully adhere to the instruction to enclose all reasoning within the specified JSON field. Instead, it follows its trained reasoning process first and then summarizes the reasoning path in the reason field. This approach appears to be a compromise between the model's training objective and the given instruction, balancing structured formatting with its natural reasoning process."}, {"title": "B.2. Universal Self-Consistency", "content": "The first example is taken from the GSM8K dataset, while the second comes from the LiveCodeBench dataset. The prompt has been adapted from Chen et al. (2023). We observe that in Example 1, the model evaluates the answers step by step to assess consistency. In contrast, the model jumps directly to the conclusion in Example 2. This difference may stem from the clarity of the task: in a MATH problem with a definite answer, the model can logically assess each step of the reasoning process, as seen in Example 1. However, for the LiveCodeBench example, where the task involves determining the most consistent code, the model struggles to elaborate on the reasoning process and relies on direct reasoning to arrive at a conclusion."}]}