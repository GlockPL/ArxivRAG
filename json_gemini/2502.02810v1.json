{"title": "Mol-LLM: Generalist Molecular LLM with Improved Graph Utilization", "authors": ["Chanhui Lee", "Yuheon Song", "YongJun Jeong", "Hanbum Ko", "Rodrigo Hormazabal", "Sehui Han", "Kyunghoon Bae", "Sungbin Lim", "Sungwoon Kim"], "abstract": "Recent advances in Large Language Models (LLMs) have motivated the development of general LLMs for molecular tasks. While several studies have demonstrated that fine-tuned LLMs can achieve impressive benchmark performances, they are far from genuine generalist molecular LLMs due to a lack of fundamental understanding of molecular structure. Specifically, when given molecular task instructions, LLMs trained with naive next-token prediction training assign similar likelihood scores to both original and negatively corrupted molecules, revealing their lack of molecular structure understanding that is crucial for reliable and general molecular LLMs. To overcome this limitation and obtain a true generalist molecular LLM, we introduce a novel multimodal training method based on a thorough multimodal instruction tuning as well as a molecular structure preference optimization between chosen and rejected graphs. On various molecular benchmarks, the proposed generalist molecular LLM, called Mol-LLM, achieves state-of-the-art performances among generalist LLMs on most tasks, at the same time, surpassing or comparable to state-of-the-art specialist LLMs. Moreover, Mol-LLM also shows superior generalization performances in reaction prediction tasks, demonstrating the effect of the molecular structure understanding for generalization perspective.", "sections": [{"title": "1. Introduction", "content": "Thanks to their impressive capabilities in complex reasoning and task generalization, Large Language Models (LLMs) (Brown et al., 2023; OpenAI, 2023; Google, 2023; Touvron et al., 2023) have been widely utilized as adaptable tools for performing a broad array of tasks across multiple domains. This achievement has recently generated interest in applying LLMs on molecular data to solve diverse molecular tasks such as molecule property prediction, molecule description generation, and chemical reaction analysis, which are essential in drug discovery and materials science (Yu et al., 2024; Pei et al., 2024a; Fang et al., 2023; Liu et al., 2023b; Cao et al., 2023; Liu et al., 2024a; Zhang et al., 2024; Li et al., 2024).\nNotably, recent molecular LLMs aim to leverage two important components for better molecular language modeling: molecular graph utilization and multi-task instruction tuning. For example, a number of studies (Liu et al., 2023b; Zhang et al., 2024; Liu et al., 2024a) have moved away from conventional molecular language modeling based on 1D textual representations like SMILES (Weininger, 1988) and SELFIES (Krenn et al., 2020). Instead, they have developed multi-modal LLMs by incorporating 2D molecular graphs (Liu et al., 2024b; Wang et al., 2022; Su et al., 2022) as an additional input modality, which better represents molecular structures and topologies, leading to improved performances on various molecular tasks. Meanwhile, other researches (Fang et al., 2023; Cao et al., 2023; Yu et al., 2024; Pei et al., 2024a) have built instruction tuning datasets for multiple molecular tasks and fine-tuned LLMs on these datasets, enabling them to acquire transferable and generalizable knowledge and also to differentiate, understand, and execute a variety of tasks using only natural language instructions or prompts.\nHowever, it remains unclear whether models using both 1D molecular sequences and 2D molecular graph structures effectively utilize the graph information. To investigate this, using a molecular LLM trained with SFT on both 1D molecular sequences and 2D molecular graphs, we conducted experiments where we substituted either the given 1D molecular sequence or 2D molecular graph with those from random molecules during task execution Figure Figure 1. The extent of performance degradation indicates which modality the molecular LLM primarily relies on for task completion. The results revealed that naive SFT training barely utilizes the graph modality for downstream tasks, largely depend on 1D molecular sequences, suggesting that additional methods are necessary for effective molecular graph utilization.\nFurther, despite the potential for synergistic performance improvements by these two components, few studies have fully harnessed both of their benefits, especially for a universal molecular LLM. Specifically, some recent studies (Cao et al., 2023; Li et al., 2024; Liang et al., 2023; Zhang et al., 2024; Pei et al., 2024b) have attempted to combine molecule graph representations with instruction tuning, however, their instruction tunings have been focused on task-specific fine-tuning or text-oriented tasks. As a result, these models failed to perform as true generalist molecular LLMs, particularly in tasks like molecule generation and chemical reaction prediction.\nIn this paper, we propose a unified and generalist molecular LLM that can reap the merits of using the graph representation in a multi-modal and multi-task instruction tuning way. In particular, while maintaining the structure of the multi-modal LLM based on Q-Former (Li et al., 2023), we introduce a novel multi-modal instruction tuning to force the model to produce responses based on the conditional graph modality. More specifically, we corrupt the SELFIES (Krenn et al., 2020) input tokens by replacing some tokens with random samples during the instruction tuning over many molecular tasks and datasets, which mitigates the overlooking of the graph condition and effectively improves the graph utilization.\nExperimental results on various molecule benchmark tasks including molecule property prediction, chemical reaction prediction, molecule description generation, and description-guided molecule generation show that the proposed generalist model, which we call Mol-LLM, can learn vast molecular tasks with improved multi-modal and multi-task training with better utilization of the graph modality. To the best of our knowledge, Mol-LLM is the first multi-modal molecular LLM that deals with various existing molecular tasks by a single generalist model, demonstrating the effectiveness of using the graph modality in a multi-task instruction-tuning scheme.\nTo summarize, our main contributions are: (1) a generalist molecular LLM as a unified multi-modal model that makes use of both 1D textual and 2D graph representations as inputs; (2) an extensive instruction tuning on almost all existing molecular tasks addressed by molecular LLMs by preference-based cross-modal training; and (3) demonstration of the generalization ability to various tasks through the improvement of structural understanding in Mol-LLM."}, {"title": "2. Related Works", "content": "Recent LLMs have shown strong reasoning and generalization abilities due to their extensive background knowledge. To leverage these capabilities in molecular science, many researches have been recently conducted in the way of molecular LLMs. For example, MolT5 (Edwards et al., 2022) employs a T5-based (Raffel et al., 2023) framework when trained on a large number of molecule SMILES and texts to translate between them, while MolXPT (Liu et al., 2023c) is a unified language model of text and SMILES based on GPT (Radford et al., 2019) for text-molecule translation as well as molecular property prediction. MolCA (Liu et al., 2023b) and GIT-Mol (Liu et al., 2024a) utilize a 2D molecular graph as an additional input modality for better capturing molecular structures and exploit multi-modal LLMs using a Q-Former (Li et al., 2023) to align graph and text representations. MolLM (Tang et al., 2024) is designed to further encode 3D molecular structures for explicitly utilizing geometric information. In order to seamlessly perform generative modeling on molecular structures in LLMs, UniMoT (Zhang et al., 2024) obtains discrete graph tokens from output representations of the Q-Former projector while 3D-MolT5 (Pei et al., 2024b) employs 3D structure tokens. However, these models cannot handle various modalities and different tasks simultaneously. To address this, we developed a generalist model using a structural preference method that enhances molecular structure comprehension."}, {"title": "2.1. Molecular Large Language Models", "content": null}, {"title": "2.2. Instruction Tuning on Molecular Tasks", "content": "In order to perform diverse tasks including new tasks by a single model without task-specific fine-tuning, instruction tuning has become a popular technique for LLMs. In particular, instruction tuning over multiple tasks allows LLMs to acquire transferable and generalizable knowledge while maintaining the capability to understand task instructions. Therefore, recently, there has been a growing number of studies dealing with instruction tuning for molecular LLMs across various molecular tasks. For instance, Mol-Instructions (Fang et al., 2023) develops the first comprehensive instruction dataset for a wide range of molecular tasks with instructional formats and demonstrates that instruction tuning with this dataset can improve performances of molecular LLMs on both understanding and generation tasks. InstructMol (Cao et al., 2023) applies task-specific instruction tuning when fine-tuning a multi-modal molecular LLM for each task, while BioT5+ (Pei et al., 2024a) conducts multi-task instruction tuning for each group of tasks as a semi-generalist model using the Mol-Instructions dataset and shows remarkable performances in most tasks without the use of molecular graphs. On the other hand, LlaSMol (Yu et al., 2024) collects a much larger and more diverse instruction tuning dataset that consists of 3.3M instances from 14 tasks and trains a generalist molecular LLM, demonstrating the merits of the dataset, especially in terms of generalist model's performances.\nHere, it is noted that some recent studies such as Instruct-Mol (Cao et al., 2023), UniMoT (Zhang et al., 2024), 3D-MolT5 (Pei et al., 2024b) attempt to integrate molecular structure representations with instruction tuning. However, their instruction tunings are restricted to task-specific fine-tuning for specialist models or multi-task fine-tuning on text-oriented tasks excluding molecule generation and chemical reaction prediction. While our work also lies in the combination of the multi-modal molecular LLM using the molecular graph modality with multi-task instruction tuning, different from the previous models, our model performs as a generalist multi-modal model that can cover more diverse tasks including understanding and generation of both molecules and texts, resulting from our robust multi-modal instruction tuning."}, {"title": "3. Method", "content": "We introduce our multi-modal architecture using both 1D SELFIES and 2D graphs and propose a multi-modal training method to enhance molecular structure understanding of molecular LLM."}, {"title": "3.1. Model Architecture", "content": "Graph Encoder For the LLM versatile on diverse molecular tasks, it is beneficial in leveraging molecular structure representations, in addition to 1D textual representations such as SMILES (Weininger, 1988) or SELFIES (Krenn et al., 2020). Given that, Liu et al. (2023b); Cao et al. (2023) choose molecular graph representation, where a graph encoder encodes a molecular graph and then cross-modal projector is used to align graph embedding and text embedding to feed LLMs. We take MoleculeSTM (Liu et al., 2024b) as a graph encoder that is composed of a 5-layer graph isomorphism network (GIN) (Xu et al., 2018). For the cross-modal alignment between graph and text, MoleculeSTM is pre-trained via contrastive learning on the molecular caption and molecular graph on 280K molecule-text pairs from PubChem (Kim et al., 2022). Given a 2D molecular graph G = (V, E), the graph encoder $f_g$ extracts node-level embeddings $Z_{node} = f_g(V, E) \\in \\mathbb{R}^{|V|\\times d_g}$, where $|V|$, $d_g$ indicate the number of nodes in the molecule and its embedding dimension, respectively. Then, we concatenate mean pooled global graph embeddings as $Z_{global} = \\frac{1}{|V|}\\sum_{Z_{node,i} \\in Z_{node}} Z_{node,i} \\in \\mathbb{R}^{d_g}$ to Znode getting $Z = [Z_{global}, Z_1,..., Z_{|v|}] \\in \\mathbb{R}^{(|V|+1)\\times d_g}$.\nCross-modal Projector There are two well-known methods to bridge different modalities for multi-modal LLMs: using shallow linear layers as used in Liu et al. (2023a) or using Q-Former, which is a bi-directional transformer encoder as Li et al. (2023). The key distinction between the two is whether they compress the information from graph embedding. While the linear projector inputs graph embeddings equal to the number of nodes, Q-Former compresses graph embeddings to a fixed number using Q-Former's attention mechanisms. We select Q-Former since its compressed and fixed number of graph token approach provides advantages in task recognition and efficient batch processing. With $N_q$ queries $Q \\in \\mathbb{R}^{N_q\\times d_q}$ representing an input molecule, Q-Former conducts cross-attention between queries Q and dimension-adapted molecular graph embedding $Z' \\in \\mathbb{R}^{(|V|+1)\\times d_q}$, by linear dimension projection. Through the cross-modal cross-attention, Q-Former learns to feature queries to represent impactful molecular structural information, such as the functional group in the molecule and the molecule's backbone structure, etc. In general, the greater $N_q$, the queries have more capacity to express molecular information. Here, we use 32 Q-Former queries for molecular foundation modeling, unlike 8 queries of MolCA, a task-specific model.\nLLM We choose universal molecular language modeling as depicted in Figure 2, where LLMs address a variety of molecular tasks through next token prediction without the need for task-specific adapters. Since addressing diverse molecular tasks requires large model capacity, we choose the Mistral-7B-Instruct-v0.3 (Jiang et al., 2023) with LoRA rank 64 as a backbone LLM for text and 1D SELFIES input, similar to Yu et al. (2024). Besides obtaining from the graph, we also use 1D SELFIES as molecular representations for LLM inputs.\nTokenization As we optimize for molecular representation not only from the 2D graph but also 1D SELFIES, we added 3K SELFIES tokens into LLM's vocabulary. Moreover, empirical evidence indicates that in regression tasks, utilizing existing numerical tokens in a pre-trained LLM is not ideal. This is due to the difference in the context of number generation between next-token prediction during LLM's pre-training and the regression tasks. To bridge this context gap, we have introduced several specialized tokens to represent numerical values, \u201c| < i > |\u201d, i \u2208 [0, 9]. Additionally, we have introduced tokens to denote answer types for floats, booleans, text descriptions (as shown in Figure 2), molecular representation types, and the direction of chemical reactions, represented by \" >> \"."}, {"title": "3.2. Training Framework", "content": "Diverse Instruction Tuning Datasets To construct thorough instruction tuning on diverse molecular tasks, we aimed to collect as diverse dataset as possible, with high-quality standards. In details, leveraged Mol-Instruction dataset (Fang et al., 2023), SMolInstruct dataset (Yu et al., 2024), MoleculeNet dataset (Wu et al., 2017), and ChEBI-20 dataset (Edwards et al., 2022), whose dataset sizes are specified in Table 1.\nFor molecular property classification problems, we utilized the MoleculeNet datasets, which are famously adopted, including various problems for classification for molecular toxicity (Tox21, SIDER, Toxcast, Clintox), HIV inhibition, blood-brain barrier penetration (BBBP), Alzheimer's inhibition (BACE). The Mol-Instruction dataset for molecular learning comprises three tasks: property regression, chemical reaction prediction, molecule captioning, and description-guided molecule generation. To increase the amount of training data and incorporate high-quality datasets, we utilized not only Mol-Instruction but also SMolInstruct. We deduplicated molecules and texts between Mol-Instruction and SMolInstruct in each task, removing Mol-Instruction instances for duplicated cases. The property regression problems come from the QM9 dataset, which is used to predict 12 atomic level molecular properties. We used 11 labels including HOMO, LUMO, and HOMO-LUMO gap for training to enable multifaceted learning of atomic-level property knowledge. Chemical reaction prediction tasks are classified into forward reaction prediction, which is to predict a product molecule given a reactant molecule, retrosynthesis, which is the inverse version of forward prediction, and reagent prediction, which is to predict a reagent molecule given a reactant and product molecule. The remaining tasks are description-guided molecule generation and molecule captioning, where Mol-Instruction used PubChem as data source. However, as specified in 3, we found that the majority portions of the PubChem (Kim et al., 2022) data consist of very short and repetitive sentences, which is the reason we omitted it from our multi-task instruction tuning datasets. Instead, we chose the ChEBI-20 (Edwards et al., 2022) dataset, which provides more detailed descriptions of molecular structure and properties, as our translation task data.\nTwo-staged training: Supervised Fine-Tuning (SFT) In causal language modeling tasks involving multiple modalities, pre-training a cross-modal projector is typically considered essential for the optimal utilization of multi-modal information, thereby which enable to achieve optimal performance. Although Liu et al. (2023b) pre-trained the Q-Former on a dataset of molecule and molecule caption pairs collected from PubChem, we observed that these captions do not result in meaningful performance improvement in downstream tasks. This lack of improvement may be attributed to the limited information content in the dataset, where 290K out of 324K captions contained fewer than 30 tokens. This suggests that obtaining large-scale molecule-text pairs and performing Q-Former pretraining might potentially improve downstream task performance, only after obtaining large-scale molecule-text pair, which is very challenging. To our knowledge, no dataset currently exists that contains more molecule-caption pairs than PubChem. While using LLMs to generate captions for molecules could potentially scale up the data, this approach is known to be vulnerable to critical hallucinations in the scientific domain.\nGiven this challenge of multi-modal utilization due to lack of high-quality pre-training data, instead, we propose a training framework that enhances the performance of existing Molecular LLMs without requiring pre-training on large-scale molecule-caption pair data. Instead, our approach combines supervised fine-tuning on downstream task data and subsequent preference optimization for effective utilization of multi-modal information. For this purpose, we first conduct SFT training using a graph encoder and Q-Former. During this process, we input query embeddings extracted from Q-Former using the frozen graph encoder's graph embeddings into the LLM. The Q-Former and LLM LORA parameters are trained using SFT loss from the LLM's next token prediction task for downstream tasks. This enables the model to learn molecular task performance capabilities achievable without sophisticated multimodal utilization, while aligning the Q-Former's cross-modal representations.\nTwo-staged training: Molecular Structure Preference Optimization After performing SFT training on various molecular tasks, Molecular Structure Preference Optimization (MolPO) is conducted to enhance the utilization of molecular graphs and achieve optimal performance beyond conventional SFT training. As depicted in Figure 3, the original data triplet of (g, s, q, y) is used as chosen pair ($g_\\omega$, s, q, y) w.r.t. the molecular graph, where g, s each represents molecular graph and molecular selfies, and q, y are task instruction and corresponding ground truth label molecular graphs from each task are used as the chosen examples. Rejected molecular graphs $g_i$ are then created by performing molecular graph structure modifications that degrade the informative features necessary for each task from $q_\\omega$. Then, by leveraging molecular structure preference pair ($g_\\omega$, s, q, y), ($g_i$, s, q, y), we formulate following preference optimization objective motivated from Wang et al. (2024) to enhance molecular graph utilization of molecular LLM $\\pi_\\theta$:\n$\\mathcal{L}_{MolPO} = -log\\left(\\frac{\\sigma(r_\\theta(y|g_\\omega, s, q, y))}{\\sigma(r_\\theta(y|g_\\omega, s, q, y))+\\sigma(r_\\theta(y|g_i, s, q, y))}\\right)$, (1)\nwhere $r_\\theta$ a hyperparameter preventing too much deviation of $\\pi_\\theta$ from reference model $\\pi_{ref}$, and $\\gamma$ a hyperameter to guarantee proper likelihood gap from chosen and rejected pairs. For example, in a molecule captioning task where a molecule containing a carboxyl group is provided along with its descriptive text label, the LLM must describe the functional group to address the molecule's structural and functional characteristics (details in Appendix A. By creating a rejected molecular graph through removing the carboxyl group from the chosen molecular graph, the LLM is trained to assign low likelihood scores to the ground truth label when presented with the rejected molecular graph as input.\nHowever, foundation modeling for diverse molecular tasks requires training on large-scale datasets, making it burdensome to include a reference model in the training objective. To enable scalable training by reducing the computational cost of the reference model, we followed the approach of Meng et al. (2024) to use reference-free preference objective as follows:\n$\\mathcal{L}'_{MolPO} = \\beta\\left(\\frac{\\sigma(\\text{log }\\pi_\\theta(y|g_\\omega, s, q, y))}{\\sigma(\\text{log }\\pi_\\theta(y|g_\\omega, s, q, y)) - \\text{log }\\pi_\\theta(y|g_i, s, q, y)) - \\gamma)}\\right)$. (2)\nFollowing Meng et al. (2024), we employ the length-normalized sum of token log probabilities from $\\pi_\\theta$ as the reference-free reward. Here, length normalization is to address length bias, because longer sequences typically yield lower log probabilities, creating challenges for molecular modeling across different tasks. While classification and regression tasks require only brief token sequences to represent boolean or numeric values, tasks such as reaction prediction, molecule generation, and molecular captioning require hundreds of tokens to generate SELFIES and text descriptions. Without length normalization of the reward, preference optimization across tasks with varying token lengths becomes significantly challenging.\nWith the reference-free preference objective, the total training objective in MolPO phase is as follows:\n$\\mathcal{L}_{tot} = \\mathcal{L}'_{MolPO} + \\mathcal{L}_{SFT}$. (3)"}, {"title": "4. Experiments", "content": "We constructed an instruction dataset comprising 7 tasks and 16 sub-tasks. To examine the impact of molecule SELFIES and molecule graphs on model training, we conducted experiments under three conditions: learning molecular representation from 1D SELFIES, 2D graph each, and from both of them. To prevent alterations in graph information, we kept the graph encoder frozen. To align the LLM with the q-former embeddings, we kept the LLM frozen during the first half of the training process and then trained the LLM using a LoRA for the second half. We employed beam search with a beam width of 5 as the sampling method to generate high-quality texts, without applying repetition penalty. Additional details and ablation studies for generation can be found in the Appendix. All experiments are run with 8\u00d7 NVIDIA A100 (80GB) GPUs."}, {"title": "4.1. Evaluation Metrics", "content": "We used the following metrics that are commonly employed in previous studies: Exact Match, the proportion of tokens where the model's predicted results exactly match the ground truth labels, BLEU, ROUGE, LEVENSHTEIN, and METEOR, which measures the quality of predicted results based on ground truth labels, RDK/MACCS/MORGAN FTS, measures the structural similarity of molecule, Validity, the ratio of valid molecules follows SELFIES rules."}, {"title": "4.2. Baselines", "content": "The baseline models were divided into three categories for performance comparison. The first category consists of specialist models, where each task requires its own trained model. The second category includes semi-generalist models, where similar tasks are combined and trained as groups. The third category contains generalist models, where a single model handles all tasks. Among the generalist models, LlaSMol differs from our proposed model by excluding two challenging components: the QM9 dataset tasks, which represent a significant portion of the data and complicate multi-task learning, and the reagent prediction task, which is considered the most complex among reaction predictions due to its requirement to process three molecules simultaneously. This section analyzes and compares the performance across these three baseline model categories."}, {"title": "4.3. Property Prediction", "content": "Molecular property prediction is connected to real-world applications with significant impact, such as materials discovery and drug discovery. Following Yu et al. (2024) and Pei et al. (2024a), we evaluated property prediction performance on classification and regression tasks from MoleculeNet (Wu et al., 2017).\nFor classification tasks, we used Mol-LLM to predict \"True\" or \"False\" for properties across BACE, BBBP, Clintox, HIV, and SIDER datasets, measuring ROC-AUC using the probability of these tokens. The experimental results in Table 2 showed that generalist models like LlaSMol and Mol-LLM outperform specialist and semi-specialist models in property prediction, indicating that chemical knowledge acquired through learning various molecular tasks transfers effectively to tasks like toxicity prediction. In comparisons within Mol-LLM variants, we observed performance improvements in three out of five tasks when utilizing molecular graphs.\nIn regression tasks, we evaluated Mol-LLM with other models in Lipophilicity, Esol, QM9 datasets. In these tasks, Mol-LLM predicts numerical value via next token prediction with additionally added number tokens \u201c| < i > |\u201d, i\u2208 [0,9]. For regression tasks, we observed performance improvements in Mol-LLM across all datasets through molecular graph utilization. Notably, the Lipophilicity dataset showed a 33.3% reduction in MAE by applying MolPO, while other datasets showed relatively smaller improvements, indicating that the effect of graph utilization may vary depending on the predicted property, but is generally beneficial. In the Esol dataset, Mol-LLM showed significant performance advantages over LlaSMol even without graph utilization, suggesting that learning diverse tasks might be largely beneficial independently of graph utilization. For QM9, both models with and without MolPO training achieved the second-best performance among molecular LLMs after BioT5+, with MolPO application leading to additional MAE performance improvements. All results are shown in Table 2."}, {"title": "4.4. Reaction Prediction", "content": "Unlike Mol-Instruction, which covers a broad scope of the biomolecular domain, SMolInstruct focuses exclusively on small molecules while incorporating more complex and diverse molecular structures. We compared the models' performance in reaction prediction tasks on both Mol-Instruction and SMolInstrut to evaluate each model in various molecule distribution. Since SMolInstruct does not include the reagent prediction task, we evaluated the model's performance on the reagent prediction task solely using Mol-Instruction. BioT5+ performs comparably to Mol-LLM on the Mol-Instruction dataset but shows decreased performance on the SMol dataset. Conversely, LlaSMol achieves state-of-the-art (SOTA) performance on the SMolInstruct dataset but exhibits a performance gap compared to BioT5+ and Mol-LLM on the Mol-Instruction dataset. Mol-LLM achieves SOTA performance on the Mol-Instruction dataset and performs comparably to SOTA on the SMolInstruct dataset. All results are shown in Table 3.\nMolecular graph structure understanding To understand this difference, we analyzed the molecular structure understanding patterns learned through MolPO training in the reaction prediction task in Mol-Instruction and SMolInstruct datasets. Specifically, we calculated the probabilities assigned by the model to chosen and rejected cases to determine its preference. The results showed that, without MolPO training, the model could hardly distinguish between chosen and rejected cases. However, with MolPO objective, the model predominantly preferred chosen cases, which demonstrated the effectiveness of MolPO in molecular structure understanding, which enhance task generalization on diverse molecular structure. The results are shown in Figure 4."}, {"title": "4.5. Description Guided Molecule Generation", "content": "Table 4 presents the results of description-guided molecule generation. The performance is lower compared to the semi-generalist model, BioT5+, which is likely due to BioT5+'s ability to process IUPAC names, allowing it to better understand the information embedded in the molecular descriptions. Nevertheless, it outperforms LlaSMol, which is also a generalist model, by a significant margin."}, {"title": "4.6. Molecule Captioning", "content": "Table 5 presents the results of Molecule Captioning. The performance of Mol-LLM is lower than that of the specialist model (MolCA) and the semi-generalist model (BioT5+), and slightly lower than Mol-LLM, which solely uses SELFIES. This is likely because molecule descriptions are composed of text that does not particularly require graph structural information. As a result, 1D SELFIES alone appears to be sufficient for text generation. Nevertheless, Mol-LLM outperforms the generalist model, LlaSMol, across all metrics."}, {"title": "5. Conclusion", "content": "In this study, we proposed a generalist molecular LLM, Mol-LLM, and introduced MolPO, a training objective designed to enhance the model's understanding of molecular structures. MolPO enables the model to better distinguish between correct and incorrect molecular structures. As a result, Mol-LLM achieved state-of-the-art performance among generalist molecular LLMs in most molecule-centric tasks and demonstrated performance comparable to or surpassing specialized molecular LLMs. Furthermore, Mol-LLM exhibited strong generalization capabilities in reaction prediction tasks, which is attributed to excellent molecular structure understanding identified by preference pair accuracy. We believe that Mol-LLM can be utilized in real-world applications such as drug discovery and new material discovery in the future."}, {"title": "Impact Statement", "content": "This paper aims to advance the field of AI for Science; however, there is a possibility that increased training and usage of larger LLMs in the future could lead to higher carbon emissions. Nevertheless, this is not a concern at present, nor does it raise any ethical issues."}, {"title": "A. Generation Methods of Rejected molecule for MolPO", "content": "To enhance the model's understanding of molecule graphs, we generated negative cases by augmenting the original molecule graphs included in our training dataset, ensuring that the model assigns them lower preferences. Our augmentation methods can be categorized into two approaches: the Random Atom Exchange and the MACCS Key-based Property Exchange.\nThe Random Atom Exchange method augments molecular structures by modifying specific atoms at random positions within a molecule. This is achieved by replacing an existing atom with a different atom, removing the atom, or connecting a new arbitrary atom at that position. Through this approach, molecules with different properties can be generated while maintaining the overall number of constituent atoms. Additionally, by adding or removing multiple atoms, the overall size of the molecule can be increased or decreased. This method enables the generation of virtual molecules with altered properties without significantly disrupting the overall bond connectivity in the molecular graph. Consequently, it aids LLMs in developing a deeper understanding of how the connectivity between specific atoms influences molecular properties.\nThe MACCS Key-based Property Exchange method directly modifies molecular substructures associated with specific properties by selectively removing and adding them. This approach first identifies the substructures of the molecule corresponding to MACCS keys, generating two lists: one containing the MACCS keys representing properties present in the molecule, and the other containing the keys for properties absent in the molecule. For augmentation, a random key is selected from the list of present MACCS keys, and the corresponding substructure is removed from the original molecular graph. Subsequently, a random key is chosen from the list of absent MACCS keys, and a substructure corresponding to this property is generated and attached at a random position in the molecule. This method creates molecules with different properties through substructure-level enhancement, facilitating the understanding of molecular properties by strengthening its ability to recognize and interpret substructure patterns."}, {"title": "B. Training Hyperparameter", "content": "Table 6 represents hyperparameters of Mol-LLM during the training phase of SFT and MolPO. We use 8\u00d7 NVIDIA A100 (80GB) GPUs. We use greedy decoding for token generation."}, {"title": "C. Experiments in SMolInstruct Dataset", "content": "Table 7 and table 8 present the results of description-guided molecule generation and Molecule Captioning in SMolInstruct test set."}]}