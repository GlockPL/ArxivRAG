{"title": "A Mathematical Framework, a Taxonomy of Modeling Paradigms, and a Suite of Learning Techniques for Neural-Symbolic Systems", "authors": ["Charles Dickens", "Connor Pryor", "Changyu Gao", "Alon Albalak", "Eriq Augustine", "William Wang", "Stephen Wright", "Lise Getoor"], "abstract": "The field of Neural-Symbolic (NeSy) systems is growing rapidly. Proposed approaches show great promise in achieving symbiotic unions of neural and symbolic methods. However, each NeSy system differs in fundamental ways. There is a pressing need for a unifying theory to illuminate the commonalities and differences in approaches and enable further progress. In this paper, we introduce Neural-Symbolic Energy-Based Models (NeSy-EBMs), a unifying mathematical framework for discriminative and generative modeling with probabilistic and non-probabilistic NeSy approaches. We utilize NeSy-EBMs to develop a taxonomy of modeling paradigms focusing on a system's neural-symbolic interface and reasoning capabilities. Additionally, we introduce a suite of learning techniques for NeSy-EBMs. Importantly, NeSy-EBMs allow the derivation of general expressions for gradients of prominent learning losses, and we provide four learning approaches that leverage methods from multiple domains, including bilevel and stochastic policy optimization. Finally, we present Neural Probabilistic Soft Logic (NeuPSL), an open-source NeSy-EBM library designed for scalability and expressivity, facilitating real-world application of NeSy systems. Through extensive empirical analysis across multiple datasets, we demonstrate the practical advantages of NeSy-EBMs in various tasks, including image classification, graph node labeling, autonomous vehicle situation awareness, and question answering.\nKeywords: Neural-Symbolic AI, Energy-based Models, Deep Learning", "sections": [{"title": "1 Introduction", "content": "The promise of mutually beneficial neural and symbolic integrations has motivated significant advancements in machine learning research. Much of the recent progress has been achieved in the neural-symbolic (NeSy) computing literature (d'Avila Garcez et al., 2002, 2009, 2019). NeSy is a large and quickly growing community that has been holding regular workshops since 2005 (NeSy2005) and began holding conferences in 2024 (NeSy2024). At a high level, NeSy research aims to build algorithms and architectures for combining neural and symbolic components (Xu et al., 2018; Yang et al., 2020; Cohen et al., 2020; Manhaeve et al., 2021a; Wang et al., 2019; Badreddine et al., 2022; Ahmed et al., 2022a; Pryor et al., 2023a). With its rapid growth, the field of NeSy needs a unifying framework to function as a common foundation for further progress. Such a unifying theory should support the understanding and organization of the strengths and weaknesses of NeSy methods and help to align design decisions to the needs of an application. Moreover, it should support the development of new and widely applicable NeSy inference and learning algorithms.\nIn this paper, we introduce Neural-Symbolic Energy-Based Models (NeSy-EBMs), a unifying framework for NeSy. NeSy-EBMs are a family of Energy-Based Models (EBMs) (LeCun et al., 2006) defined by energy functions that are compositions of parameterized neural and symbolic components. The neural component is a collection of deep models, and its output is provided to the symbolic component, which measures the compatibility of variables using domain knowledge and constraints. This general formalization serves multiple purposes, including functioning as a foundation for identifying modeling paradigms and reasoning capabilities of NeSy models and developing generally applicable NeSy inference and learning algorithms. Additionally, energy-based modeling is an established and recognized perspective that connects NeSy to the broader machine learning literature.\nWe use the NeSy-EBM framework to introduce a general formalization of reasoning as mathematical programming. This formalization motivates a new NeSy taxonomy that categorizes models based on their reasoning capabilities. Specifically, we organize approaches into three modeling paradigms that vary with increasing expressivity and complexity: deep symbolic variables, deep symbolic parameters, and deep symbolic potentials. These categories are differentiated by their neural-symbolic connection, i.e., the way in which the neural component output is utilized in the symbolic component. Our modeling paradigms organize and illuminate the strengths and limitations of existing NeSy systems. Moreover, we identify architectures that support NeSy use cases, such as learning from domain knowledge and data, satisfying prediction constraints, and consistent reasoning in open-ended domains.\nFurther, we develop a suite of principled neural and symbolic parameter learning techniques for NeSy. NeSy-EBM predictions are typically obtained by finding a state of variables with high compatibility (i.e., low energy). The high compatibility state is found by minimizing the energy function via an optimization algorithm, for instance, an interior point method for continuous variables (Nocedal and Wright, 2006) or a branch-and-bound strategy for discrete problems (H. Papadimitriou and Steiglitz, 1998). The complex prediction process makes finding a gradient or descent direction of a standard machine learning loss with respect to the parameters difficult. To formalize these challenges and propose solutions, we introduce a categorization of learning losses based on the complexity of the relation to the NeSy-EBM energy function. We derive general expressions for gradients of the"}, {"title": "2 Motivation", "content": "We highlight five applications that motivate NeSy: 1) constraint satisfaction and joint reasoning, 2) fine-tuning and adaptation, 3) few-shot and zero-shot reasoning, 4) semi-supervised learning, and 5) reasoning with noisy data. This list of use cases is not exhaustive. However, the efficacy of the NeSy approach in these applications is well established, and we will illustrate four of these use cases in our empirical evaluation. The following subsections define the problem and the high-level motivation for utilizing NeSy techniques in such settings. Additionally, we discuss collections of existing NeSy systems for each application."}, {"title": "2.1 Constraint Satisfaction and Joint Reasoning", "content": "In real-world settings, a deployed model's predictions must meet well-defined requirements. Additionally, leveraging known patterns or dependencies in the output can significantly improve a model's accuracy and trustworthiness. Constraint satisfaction is finding a prediction that satisfies all requirements. NeSy systems perform constraint satisfaction by reasoning across their output to provide a structured prediction, typically using some form of joint reasoning. In other words, NeSy systems integrate constraints and knowledge into the prediction process.\nA commonly used example of constraint satisfaction and joint reasoning with NeSy techniques is puzzle-solving. Many NeSy frameworks are introduced with an evaluation on visual Sudoku and its variants (Wang et al., 2019; Augustine et al., 2022). In the visual Sudoku problem, puzzles are constructed with handwritten digits, and a model must classify the digits and infer numbers to fill in the empty cells using the rules of Sudoku. Empirical evaluations of NeSy systems that perform constraint satisfaction and joint reasoning on visual Sudoku problems can be found in Wang et al. (2019), Augustine et al. (2022), Pryor et al. (2023a), and Morra et al. (2023). Similarly, Vlastelica et al. (2020) introduces the shortest path finding problem as a NeSy task. Images of terrain maps are partitioned into a grid, and the model must find a continuous lowest-cost path between two points. The works of Vlastelica et al. (2020) and Ahmed et al. (2022a) perform constraint satisfaction and joint reasoning with NeSy models for shortest path finding.\nConstraint satisfaction and joint reasoning with NeSy models are also effective for real-world natural language tasks. For instance, Sachan et al. (2018) introduces the Nuts&Bolts NeSy system to build a pipeline for parsing physics problems. The NeSy system jointly infers a parsing from multiple components that incorporates domain knowledge and prevents the accumulation of errors that would occur from a naive composition. In another work, Zhang et al. (2023) propose GeLaTo (generating language with tractable constraints) for imposing constraints on text generated from language models. GeLaTo generates text tokens by autoregressively sampling from a distribution constructed from a pre-trained language model and a tractable probabilistic model encoding the constraints. More recently, Pan et al. (2023) introduced the Logic-LM framework for integrating LLMs with symbolic solvers to improve complex problem-solving. Logic-LM formulates a symbolic model using an"}, {"title": "2.2 Fine-tuning and Adaptation", "content": "We are in the era of foundation models in AI (Bommasani et al., 2022). It is now commonplace to adjust a model that is pre-trained on large amounts of general data (typically using self-supervision) for downstream tasks. Fine-tuning and adaptation are two methods for updating the parameters of a pre-trained model to perform a specific problem or a dataset in a new domain (Devlin et al., 2019; J. Hu et al., 2022). Fine-tuning and adaptation adjust the pre-trained model parameters by minimizing a learning objective over a dataset, both of which are specialized for the downstream tasks. These are necessary steps in the modern AI development process.\nNeSy frameworks are used in the fine-tuning and adaptation steps to design principled learning objectives that integrate knowledge and constraints relevant to the downstream task and the application domain. Giunchiglia et al. (2022) provides a recent survey of the use of logically specified background knowledge to train neural models. NeSy learning losses are applied in the work of Giunchiglia et al. (2023) to fine-tune a neural system for autonomous vehicle situation awareness (Singh et al., 2021). In another computer vision task, Arrotta et al. (2024) develop a NeSy loss for training a neural model to perform context-aware human activity recognition. NeSy fine-tuning and adaptation have also been explored in the natural processing literature. Recently, Ahmed et al. (2023b) proposed the pseudo-semantic loss for detoxifying large language models. The authors disallow a list of toxic words and show this intuitive approach steers a language model's generation away from harmful language and achieves state-of-the-art detoxification scores. Feng et al. (2024) has explored directly learning the reasoning process of logical solvers within the LLM to avoid parsing errors. Finally, Cunnington et al. (2024) introduced NeSyGPT, which fine-tunes a vision-language"}, {"title": "2.3 Few-Shot and Zero-Shot Reasoning", "content": "Training data for a downstream task may be limited or even nonexistent. In few-shot settings, only a few examples are available, while in zero-shot settings, no explicit training data is provided for the task. In these settings, few-shot and zero-shot reasoning techniques are used to enable a model to generalize beyond the limited available training data. Leveraging pre-trained models and domain knowledge are key ideas for succeeding in few-shot and zero-shot contexts.\nNeSy techniques have been successfully applied for various few-shot and zero-shot settings. Integrating symbolic knowledge and reasoning enables better generalization from a small number of examples. NeSy systems can utilize symbolic knowledge to make deductions about unseen classes or tasks. For instance, providing recommendations for new items or users can be viewed as a few-shot or zero-shot problem. Kouki et al. (2015) introduce the HyPER (hybrid probabilistic extensible recommender) framework for incorporating and reasoning over a wide range of information sources. By combining multiple information sources via logical relations, the authors outperformed the state-of-the-art approaches of the time. More recently, Carraro et al. (2022) developed an LTN-based recommender system to overcome data sparsity. This model uses background knowledge to generalize predictions for new items and users quickly. Few-shot and zero-shot reasoning tasks are also prevalent in object navigation. The ability to navigate to novel objects and unfamiliar environments is vital for the practical use of embodied agents in the real world. In this context, Zhou et al. (2023) presents a method for \u201cexploration with soft commonsense constraints\u201d (ESC). ESC first employs a pre-trained vision and language model for semantic scene understanding, then a language model to reason from the spatial relations, and finally PSL to leverage symbolic knowledge and reasoning to guide exploration. In natural language processing, Pryor et al. (2023b) infers the latent dialog structure of a goal-oriented conversation using domain knowledge to overcome the challenges of limited data and out-of-domain generalization. Sikka et al. (2020) (mentioned above) also finds that the few-shot and zero-shot capabilities of NeSy models help in visual relationship detection. Specifically, the addition of commonsense reasoning and knowledge improves performance by over 10% in data-scarce settings."}, {"title": "2.4 Semi-Supervised Learning", "content": "Semi-supervised approaches facilitate learning from labeled as well as unlabelled data by combining the goals of supervised and unsupervised machine learning. We refer the reader to this recent excellent survey on semi-supervised approaches (E. van Engelen and H. Hoos, 2020). In short, supervised methods fit a model to predict an output label given a corresponding input, while unsupervised methods infer the underlying structure in the data. The ability to leverage both labeled and unlabelled data leads to performance improvements, better generalization, and reduced labeling costs.\nNeSy is a functional approach to semi-supervised learning that leverages knowledge and domain constraints to train a model. This is achieved with loss functions that encode domain knowledge and structure and depend only on the input and output; that is, they"}, {"title": "2.5 Reasoning with Noisy Data", "content": "Errors and noise in training data arise from various sources, such as mislabeling, data entry mistakes, measurement inaccuracies, and inherent variability. Noise affects both the inference and learning stages of a machine learning model. It can make learning the true underlying relationship in the data difficult and lead to incorrect predictions. Data cleaning, regularization, ensemble learning, and data augmentation are some techniques for reasoning with noisy data.\nNeSy techniques are beneficial for reasoning with noisy data as they can improve generalizability and provide a principled method for regularization with knowledge to prevent overfitting. For instance, Donadello et al. (2017) shows that using knowledge adds robustness to the learning process when errors are present in the training labels. Specifically, the proposed LTN model is more robust to mislabeling noise than a stand-alone deep neural network object detection model, realizing a more controlled drop in performance as label noise increases. Similarly, Manhaeve et al. (2021a) demonstrates the ability of symbolic models to overcome noise in the classic MNIST addition setting."}, {"title": "3 Related Work", "content": "There is a long, rich history of research on the integration of symbolic knowledge and reasoning with neural networks, which has rapidly evolved in the past decade. In this work, we establish a unifying framework for achieving such integration by connecting two foundational areas of machine learning research: Neural-Symbolic (NeSy) AI and energy-based modeling (EBMs). We use bilevel optimization techniques to propose a new family of algorithms for end-to-end gradient-based learning of the neural and symbolic component parameters. The remainder of this section provides an overview of the related work in NeSy, EBMs, and bilevel optimization."}, {"title": "3.1 Neural-Symbolic Frameworks", "content": "NeSy empowers neural models with domain knowledge and reasoning through integrations with symbolic systems (d'Avila Garcez et al., 2002, 2009, 2019; De Raedt et al., 2020; Besold et al., 2022). Various taxonomies have been proposed to categorize NeSy literature. Bader and Hitzler (2005), d'Avila Garcez et al. (2019), and most recently Besold et al. (2022)"}, {"title": "3.1.1 LEARNING WITH CONSTRAINTS", "content": "The essence of learning with constraints is using domain knowledge and common sense to construct a loss function (Giunchiglia et al., 2022; van Krieken et al., 2022). This approach encodes the knowledge captured by the loss into the weights of the network. A key motivation is to ensure the compatibility of predictions with domain knowledge and common sense. Moreover, learning with constraints avoids potentially expensive post-prediction interventions that would be necessary with a model that is not aligned with domain knowledge. However, consistency with domain knowledge and sound reasoning are not guaranteed during inference for NeSy models in this class.\nDemeester et al. (2016), Rockt\u00e4schel and Riedel (2017), Diligenti et al. (2017), Bo\u0161njak et al. (2017), and Xu et al. (2018) are prominent examples of the learning-with-constraints NeSy paradigm. Demeester et al. (2016) incorporates domain knowledge and common sense into natural language and knowledge base representations by encouraging partial orderings over embeddings via a regularization of the learning loss. Similarly, Rockt\u00e4schel and Riedel (2017) leverage knowledge represented as a differentiable loss derived from logical rules to train a matrix factorization model for relation extraction. Diligenti et al. (2017) use fuzzy logic to measure how much a model's output violates constraints, which is minimized during learning. Xu et al. (2018) introduces a loss function that represents domain knowledge and common sense by using probabilistic logic semantics. More recently, Giunchiglia et al. (2023) introduced an autonomous event detection dataset with logical"}, {"title": "3.1.2 DIFFERENTIABLE REASONING LAYERS", "content": "Another successful area of NeSy is in differentiable reasoning layers. The primary difference between this family of NeSy approaches and learning with constraint is that an explicit representation of knowledge and reasoning is maintained in the model architecture during both learning and inference. A defining aspect of differentiable reasoning layers is the instantiation of knowledge and reasoning components as differentiable computation graphs. Differentiable reasoning layers support automatic differentiation during learning and symbolic reasoning during inference.\nPioneering works in differentiable reasoning include those of Wang et al. (2019), Cohen et al. (2020), Yang et al. (2020), Manhaeve et al. (2021a), Derkinderen et al. (2024), Badreddine et al. (2022), Ahmed et al. (2022a) and Ahmed et al. (2023a). Wang et al. (2019) integrates logical reasoning and deep models by introducing a differentiable smoothed approximation to a maximum satisfiability (MAXSAT) solver as a layer. Cohen et al. (2020) introduces a probabilistic first-order logic called TensorLog. This framework compiles tractable probabilistic logic programs into differentiable layers. A TensorLog system is end-to-end differentiable and supports efficient parallelizable inference. Similarly, Yang et al. (2020) and Manhaeve et al. (2021a) compile tractable probabilistic logic programs into differentiable functions with their frameworks NeurASP and DeepProblog, respectively. NeurASP and DeepProblog use answer set programming (Brewka et al., 2011) and ProbLog (De Raedt et al., 2007) semantics, respectively. Winters et al. (2022) proposes DeepStochLog, a NeSy framework based on stochastic definite clause grammars that define a probability distribution over possible derivations. Recently, Maene and Raedt (2024) proposes DeepSoftLog, a superset of ProbLog, adding embedded terms that result in probabilistic rather than fuzzy semantics. The logic tensor network (LTN) framework proposed by Badreddine et al. (2022) uses neural network predictions to parameterize functions representing symbolic relations with real-valued or fuzzy logic semantics. The fuzzy logic functions are aggregated to define a satisfaction level. Predictions can be obtained by evaluating the truth value of all possible outputs and returning the highest-valued configuration. Badreddine et al. (2023) has expanded upon LTNs and presents a configuration of fuzzy operators for grounding formulas end-to-end in the logarithm space that is more effective than previous proposals. Recently, Ahmed et al. (2022a) introduced a method for compiling differentiable functions representing knowledge and logic using the semantics of probabilistic circuits (PCs) (Choi et al., 2020). Their approach, called semantic probabilistic layers (SPLs), performs exact inference over tractable probabilistic models to enforce constraints over the predictions and uses the PC framework to ensure that the NeSy model is end-to-end trainable.\nAs pointed out by Cohen et al. (2020), answering queries in many (probabilistic) logics is equivalent to the weighted model counting problem, which is #P-complete or worse. Similarly, the MAXSAT problem studied by Wang et al. (2019) is NP-hard. Thus, since deep neural networks can be evaluated in time polynomial in their size, no polysize network can implement general logic queries unless #P=P, or MAXSAT solving, unless NP=P. For this reason, researchers have made progress towards building more efficient differentiable"}, {"title": "3.1.3 REASONER AGNOSTIC SYSTEMS", "content": "More recently, researchers have sought to build NeSy frameworks with more general reasoning and knowledge representation capacities with expressive mathematical program blocks for reasoning. Mathematical programs are capable of representing cyclic dependencies across variables and ensuring the satisfaction of prediction constraints during learning and inference. Moreover, the system's high-level inference and training algorithms are agnostic to the solver used for the mathematical program.\nProminent reasoner-agnostic systems include the works of Amos and Kolter (2017), Agrawal et al. (2019a), Vlastelica et al. (2020), and Cornelio et al. (2023). Amos and Kolter (2017) integrate linearly constrained quadratic programming problems (LCQP) as layers in deep neural networks with their OptNet framework, and show that the solutions to the LCQP problems are differentiable with respect to the program parameters. The progress of OptNet was continued by the work of Agrawal et al. (2019a) with the application of domain-specific languages (DSLs) for instantiating the LCQP program layers. DSLs provide a syntax for specifying LCQPs representing knowledge and constraints, making optimization layers more accessible. Vlastelica et al. (2020) propose a method for computing gradients of solutions to mixed integer linear programs based on a continuous interpolation of the program's objective. In contrast to the works of Amos and Kolter (2017) and Agrawal et al. (2019a), the approach introduced by Vlastelica et al. (2020) supports integer constraints and achieves this by approximating the true gradient of the program output. Cornelio et al. (2023) takes a different approach from these three methods by employing reinforcement learning techniques to support more general mathematical programs. Specifically, the neural model's predictions are interpreted as a state in a Markov decision process. Actions from a policy are taken to identify components that violate constraints to obtain a new state. The new state is provided to a solver, which corrects the violations, and a reward is computed. The solver is not assumed to be differentiable, and the REINFORCE algorithm (Williams, 1992) with a standard policy loss is used to train the system end-to-end without the need to backpropagate through the solver."}, {"title": "3.2 Energy-Based Models", "content": "Our NeSy framework makes use of Energy-Based Models (EBMs) (LeCun et al., 2006). EBMs measure the compatibility of a collection of observed (input) variables $x \\in X$ and target (output) variables $y \\in Y$ via a scalar-valued energy function: $E : Y \\times X \\rightarrow \\mathbb{R}$. Low energy states represent high compatibility. EBMs are appealing due to their generality in both modeling and application. For instance, EBMs can be used to perform density estimation by defining conditional, joint, and marginal Gibbs distributions with the energy"}, {"title": "3.3 Bilevel Optimization", "content": "Finally, in this work, we use bilevel optimization as a natural formulation of learning for a general class of NeSy systems (Bracken and McGill, 1973; Colson et al., 2007; F. Bard, 2013; Dempe and Zemkoho, 2020). The NeSy learning objective is a function of predictions obtained by solving a lower-level program that encapsulates symbolic reasoning. In the broader deep learning community, bilevel optimization also arises in hyperparameter optimization (Pedregosa, 2016), meta-learning (Franceschi et al., 2018; Rajeswaran et al., 2019), generative adversarial networks (Goodfellow et al., 2014), and reinforcement learning (Sutton and Barto, 2018). Researchers typically take one of the following three approaches to bilevel optimization.\nImplicit Differentiation. There is a long history of research on analyzing the stability of solutions to optimization problems using implicit differentiation (Fiacco and McCormick, 1968; Robinson, 1980; Bonnans and Shapiro, 2000). These methods compute or approximate"}, {"title": "4 A Mathematical Framework for NeSy", "content": "With this extensive motivation and background in hand, in this section, we introduce Neural-symbolic energy-based models (NeSy-EBMs): a unifying mathematical framework for NeSy. Intuitively, NeSy-EBMs formalize the neural-symbolic interface as a composition of functions. The theory and notation introduced in this section are used throughout the rest of this paper."}, {"title": "4.1 Neural Symbolic Energy-Based Models", "content": "NeSy-EBMs are a family of EBMs (LeCun et al., 2006) that integrate deep architectures with explicit encodings of symbolic relations via an energy function. EBM energy functions measure the compatibility of variables where low energy states correspond to high compatibility. For NeSy-EBMs, high compatibility indicates that the variables are consistent with domain knowledge and common sense. In the following section, the formal NeSy-EBM definition provided below is grounded with intuitive examples of NeSy modeling paradigms.\nAs diagrammed in Fig. 1, a NeSy-EBM energy function composes a neural component with a symbolic component, represented by the functions $g_{nn}$ and $g_{sy}$, respectively. The neural component is a deep model (or collection of deep models) parameterized by weights from a domain $W_{nn}$, that takes a neural input from a domain $X_{nn}$ and outputs a real-"}, {"title": "5 A Taxonomy of Modeling Paradigms for NeSy", "content": "Using the NeSy-EBM framework introduced in the previous section, we introduce a taxonomy of NeSy modeling paradigms determined by the neural-symbolic interface. Our modeling paradigms are characterized by how the neural component is utilized in the symbolic component to define the prediction program in (7). To formalize the modeling paradigms, we introduce an additional layer of abstraction we refer to as symbolic potentials, denoted by $\\psi$. Further, we collect symbolic potentials into symbolic potential sets, denoted by $\\Psi$. Symbolic potentials organize the arguments of the symbolic component by the role they play in formulating the prediction program in (7)."}, {"title": "5.1 Deep Symbolic Variables", "content": "The deep symbolic variables (DSVar) paradigm trains neural components efficiently with a loss that captures domain knowledge. Representative methods following this paradigm include semantic loss networks (Xu et al., 2018) and learning with logical constraints (Giunchiglia et al., 2022). Concisely, the neural component directly predicts the values of targets in a single symbolic potential. In other words, there is a one-to-one mapping from the neural output to the targets. Note, however, that the mapping is not necessarily onto, i.e., there may be target variables without a corresponding neural output. For this discussion of modeling paradigms, we use the term \"latent\" to refer to target variables without a neural output in a DSVar model."}, {"title": "5.2 Deep Symbolic Parameters", "content": "The deep symbolic parameters (DSPar) modeling paradigm allows targets and neural predic-tions to be unequal or represent different concepts. Prominent NeSy frameworks supporting this technique include DeepProbLog (Manhaeve et al., 2021a), semantic probabilistic layers (Ahmed et al., 2022a), and logic tensor networks (Badreddine et al., 2022). Succinctly, the neural component is applied as a parameter in the symbolic potential. This paradigm allows the symbolic component to correct constraint violations made by the neural component during prediction. For this reason, DSPar's inference and learning processes are typically"}, {"title": "5.3 Deep Symbolic Potentials", "content": "Deep-symbolic potentials (DSPot), the most advanced paradigm we propose, enhances deep models with symbolic reasoning tools. The Logic-LM pipeline proposed by Pan et al. (2023) is an excellent example of this modeling paradigm. At a high level, the neural component is a generative model that samples symbolic potentials from a set to define the symbolic component. Specifically, input data is used as context to retrieve relevant domain knowledge and formulate a program to perform inference in open-ended problems.\nDefinition 5. In the deep symbolic potentials modeling paradigm, the symbolic potential set $\\Psi$ is the set of all potential functions that can be created by a NeSy framework. $\\Psi$ is indexed by the output of the neural component, i.e., $J_{\\Psi} = Range(g_{nn})$ and $l_{g_{nn}(x_{nn},W_{nn})}$ is the potential function indexed by the neural prediction. The variable and parameter domains"}, {"title": "6 A Suite of Learning Techniques for NeSy", "content": "Having identified a variety of modeling and inference paradigms, we turn to learning. This section formalizes the NeSy-EBM learning problem, identifies challenges, and proposes effective solutions. At a high level, NeSy-EBM learning is finding weights of an energy function that associates higher compatibility scores (lower energy) to targets and neural outputs near their true labels provided in training data. Further, predictions with NeSy-EBMs are obtained by minimizing a complex mathematical program, raising several obstacles to learning. For instance, NeSy-EBM predictions may not be differentiable with respect to the model parameters, and a direct application of automatic differentiation may not be possible or may fail to produce principled descent directions for the learning objective. Moreover, we will show that even when predictions are differentiable, their gradients are functions of properties of the energy function at its minimizer that are prohibitively expensive to compute. We create general and principled learning frameworks for NeSy-EBMs that address these challenges.\nThis section is organized into four subsections. We begin with preliminary notation and a general definition of NeSy-EBM learning. Then, we present a classification of learning losses first introduced by Dickens et al. (2024a) and expand theoretical differentiability results presented in Dickens et al. (2024b). The learning losses motivate and organize the exposition of four NeSy-EBM learning frameworks, one for learning the neural and symbolic weights separately and three for end-to-end learning."}, {"title": "6.1 NeSy-EBM Learning", "content": "We use the following notation and general definition of NeSy-EBM learning throughout this section. The training dataset, denoted by $\\mathcal{S}$, is comprised of $P$ samples and indexed by $\\{1,\\ldots, P\\}$. Each sample, $S_i$ where $i \\in \\{1,\\ldots, P\\}$, is a tuple of inputs, labels, and latent variable domains. Sample inputs consist of neural inputs, $x_{nn}$ from $X_{nn}$, and symbolic inputs, $x_{sy}$ from $X_{sy}$. Similarly, sample labels consist of neural and symbolic labels, which are truth values corresponding to a subset of the neural predictions and target variables, respectively. Neural labels, denoted by $t_{nn}$, are $d_{nn} \\leq d_{nn}$ dimensional real vectors from a domain $T_{nn}$, i.e., $t_{nn} \\in T_{nn} \\subseteq \\mathbb{R}^{d_{nn}}$. Target labels, denoted by $t_{y}$, are from a domain $T_{y}$ that is a $d_{T_y} \\leq d_{y}$ subspace of the target domain $Y$, i.e., $t_{y} \\in T_y$. Lastly, the neural and symbolic latent variable domains are subspaces of the range of the neural component and the target domain, respectively, corresponding to the set of unlabeled variables. The range of the neural component, $\\mathbb{R}^{d_{nn}}$, is a superset of the Cartesian product of the neural latent variable domain, denoted by $Z_{nn}$, and $T_{nn}$, i.e., $\\mathbb{R}^{d_{nn}} \\supseteq T_{nn} \\times Z_{nn}$. Similarly, the target domain $Y$ is a superset of the Cartesian product of the latent variable domain, denoted by $Z_{y}$, and $T_{y}$, i.e., $Y \\supseteq T_{y} \\times Z_{y}$. With this notation, the training dataset is expressed as follows:\n$\\mathcal{S} := \\{(t_{y}^{1}, t_{nn}^{1}, Z_{nn}^{1}, Z_{y}^{1}, x_{sy}^{1}, x_{nn}^{1}),..., (t_{y}^{P}, t_{nn}^{P}, Z_{nn}^{P}, Z_{y}^{P}, x_{sy}^{P}, x_{nn}^{P})\\}$.\nA learning objective, denoted by $\\mathcal{L}$, is a functional that maps an energy function and a training dataset to a scalar value. Formally, let $\\mathcal{E}$ be a family of energy functions indexed by"}, {"title": "6.2 Learning Losses", "content": "A NeSy-EBM learning loss functional, $\\mathcal{L}^i$, is separable into three parts: neural, value-based, and minimizer-based losses. In this subsection, we formally define each of the three loss types. At a high level, the neural loss measures the quality of the neural component independent from the symbolic component. Then, the value-based and minimizer-based losses measure the quality of the NeSy-EBM as a whole. Moreover, value-based and minimizer-based losses are functionals mapping a parameterized energy function and a training sample to a real value and are denoted by $\\mathcal{L}_{val} : \\mathcal{E} \\times S_i \\rightarrow \\mathbb{R}$ and $\\mathcal{L}_{Min} : \\mathcal{E} \\times S_i \\rightarrow \\mathbb{R}$, respectively. The learning loss components are aggregated via summation:\n$\\mathcal{L}^i(E(\\cdot,\\cdot,\\cdot, W_{sy}, W_{nn}), S_i) = \\mathcal{L}_{NN}(g_{nn}(x_{nn}, W_{nn}), t_{nn}) + \\mathcal{L}_{val}(E(\\cdot,\\cdot,\\cdot, W_{sy}, W_{nn}), S_i) + \\mathcal{L}_{Min}(E(\\cdot,\\cdot,\\cdot, W_{sy}, W_{nn}), S_i)$"}, {"title": "6.2.1 NEURAL LEARNING LOSSES", "content": "Neural learning losses are scalar functions of the neural network output and the neural labels and are denoted by $\\mathcal{L"}, {"NN}": "Range(g_{nn}) \\times T_{nn} \\rightarrow \\mathbb{R}$. For example, a neural learning loss may be the familiar binary cross-entropy loss applied in many categorical prediction settings. Minimizing a neural learning loss with respect to neural component parameters is achievable via backpropagation"}]}