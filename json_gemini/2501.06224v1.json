{"title": "Detection, Retrieval, and Explanation Unified: A Violence Detection System Based on Knowledge Graphs and GAT", "authors": ["Wen-Dong Jiang", "Chih-Yung Chang", "Diptendu Sinha Roy"], "abstract": "Recently, violence detection systems developed using unified multimodal models have achieved significant success and attracted widespread attention. However, most of them typically suffer from two major issues: lack of interpretability as black-box models and limited functionality, providing only classification or retrieval capabilities. This paper proposes a novel interpretable violence detection system called the Three-in-One (TIO) System to address these challenges. The proposed TIO system integrates knowledge graphs (KG) and graph attention networks (GAT) to achieve three core functionalities: detection, retrieval, and explanation. Specifically, the system processes each video frame along with text descriptions generated by a large language model (LLM) for a given video containing potential violent behavior. It employs ImageBind to create high-dimensional embeddings for constructing a knowledge graph. Subsequently, the system uses GAT for reasoning, applies lite time series modules to extract video embedding features, and finally connects a classifier and retriever for multi-functional output. Leveraging the interpretability of KG, the system verifies the rationale behind each output during the reasoning process. In addition, this paper proposes several lightweight methods to reduce the resource consumption of the TIO system and improve its efficiency. Extensive experiments conducted on the XD-Violence and UCF-Crime datasets demonstrate the effectiveness of the proposed TIO system. Additionally, the case study reveals an intriguing phenomenon: as the number of bystanders increases, the occurrence of violent behavior tends to decrease.", "sections": [{"title": "1. Introduction", "content": "The primary goal of violence detection is to monitor abnormal events in the real world to prevent violent behaviors and maintain social order [1]. With the rapid advancement of artificial intelligence, researchers are exploring the application of deep learning technologies in surveillance systems [2][3] to replace the inefficiencies and high costs of traditional manual monitoring. In recent years, weakly supervised violence detection (WSVD), also known as weakly supervised video anomaly detection, and violence retrieval (VAR), also referred to as weakly supervised video anomaly retrieval, have become key research directions.\nUnlike traditional methods that require frame-level annotations for videos and supervised learning for frame-level training, WSVD achieves frame-level anomaly detection using video-level annotations [4][5]. Meanwhile, VAR focuses on retrieving detailed textual descriptions corresponding to a given video [6], assisting law enforcement in post-event analysis and summarization.[38]-[40]\nHowever, current research on WSVD and VAR tasks [6]-[12] faces two major issues: First, the lack of interpretability in black-box models; second, the need to design separate models for each task. Fig. 1 illustrates the interpretability challenges in WSVD and VAR tasks. In subfigure (a), although the WSVD model successfully identifies violent scenes, interpretability methods like heatmaps reveal that the model focuses on irrelevant areas rather than actual violent scenes. Similarly, in subfigure (b), although the VAR model retrieves the correct textual descriptions, users cannot understand the rationale behind the retrieval results due to the black-box nature of the system.\nMoreover, existing research typically designs independent models for WSVD and VAR tasks rather than adopting a unified end-to-end approach, leading to the following problems: (1) Redundant design and training processes increase development and computational costs; (2) Independent models limit information sharing and collaboration, hindering semantic reasoning and performance optimization; (3) Users must separately operate and maintain systems for each task, reducing overall usability and efficiency.\nTo address these issues, this paper proposes a novel interpretable violence detection system called the Three-in-One (TIO) System. As shown in Fig. 2, unlike previous systems for WSVD and VAR tasks [6]-[12] that suffer from limited functionality and poor interpretability, the TIO system achieves interpretability through a knowledge graph (KG) module and supports end-to-end multi-tasking with a dual-branch design. Specifically, given video data with violent behaviors and corresponding labels, the system first uses an LLM to generate"}, {"title": "2. Related Work", "content": "This section reviews the related literature, divided into two parts: Weakly Supervised Violence Detection, and Violence Retrieval."}, {"title": "2.1. Weakly Supervised Violence Detection", "content": "Sultani et al. [19] and Hasan et al. [20] were the first to introduce the Multi-Instance Learning (MIL) model into Supervised Violence Monitoring (SVM). Their approach packaged surveillance videos and fed them into I3D and C3D networks for binary classification of violent and non-violent events. Ji and Lee [21] proposed the One-Class Support Vector Machine (OCSVM) model for violence detection. However, due to the limitations of 3D network-based designs in terms of real-time performance and hardware requirements in practical applications, research gradually shifted toward combining video frame extraction and temporal sequence modules for violence detection. Subsequent studies focused on utilizing self-attention mechanisms, Transformer, or Graph ConvNet (GCN) to capture temporal and contextual relationships within video content.\nZhong et al. [22] proposed a GCN-based method to calculate feature similarity and temporal consistency between video segments for monitoring violent behavior. Tian et al. [23] proposed Robust Temporal Feature Magnitude (RTFM) which utilized a self-attention network to capture the global temporal context of violent behaviors in violent videos. Wu et al. [24] introduced S3R, which models anomalies at the feature level by exploring the synergy between dictionary-based representation and self-supervised learning. Li et al. [25] proposed Spatial-Temporal Relation Learning (STRL), which jointly utilizes spatial and temporal evolution patterns for feature learning.\nIt is noteworthy that in certain practical scenarios, only single-modal cameras are typically available, and traditional multi-modal designs are often inapplicable. Thanks to pre-trained models such as CLIP [26], which align images and text to accomplish multi-modal tasks, and Imagebind [13], which unified data from images and seven different heterogeneous modalities, it became possible to achieve multi-modal tasks even in single-modality scenarios. Some studies attempted to deploy these pre-trained models for violence detection. For instance, Wang et al. [27] proposed ActionClip, which enhances video representation with richer semantic language supervision and supports zero-shot action recognition without additional labeled data or parameter requirements. Zanella et al. [28] combined large language and vision (LLV) models (e.g.,"}, {"title": "2.2. Violence Retrieval", "content": "Events in videos typically reflected the interactions between actions and entities evolving over time, and people tended to use retrieval methods to obtain comprehensive descriptions. Benefiting from the success of cross-modal pre-trained models, some studies focused on audio-text and audio-video retrieval. Liu et al. [31] proposed Collaborative Experts (CE), a collaborative expert model that compressed multimodal information into compact representations. It utilized pre-trained semantic embeddings along with specific cues such as Automatic Speech Recognition (ASR) and Optical Character Recognition (OCR) to query videos through text. Gabeur et al. [32] proposed the Multi-Modal Transformer (MMT), a multimodal transformer model designed to jointly encode different modalities in videos, enabling inter-modal attention. Wang et al. [33] introduced Text-to-Video Vector of Locally Aggregated Descriptors (T2VLAD), which implemented an efficient global-local alignment method. It adaptively aggregated multimodal video sequences and textual features through a shared set of semantic centers and computed local cross-modal similarities between video and textual features within the same centers. Ma et al. [34] proposed Cross-Modal Clip (XClip), which calculated the correlation between coarse-grained features and each fine-grained feature through coarse-to-fine contrast. It filtered out unnecessary fine-grained features guided by coarse-grained features during similarity computation, thereby improving the accuracy of video-to-text retrieval. Lei et al. [35] introduced Cross-Modal Late Fusion (XML), which adopted a late fusion design and a novel convolutional start-end detector (Convolutional Start-End Detector, ConvSE) to achieve efficient retrieval. Wu et al. [6] proposed the Anomaly-Led Attention Network (ALAN), which developed an anomaly-led sampling method to enhance the fine-grained semantic association between violent videos and text, further matching cross-modal content using two complementary alignment strategies.\nAlthough these methods achieved significant success in retrieval tasks, these models still lacked interpretability. During inference, it was difficult for users to understand the reasons behind successful matches. Additionally, the models designed for these tasks were typically limited in functionality, focusing solely on retrieval while being unable to classify violent content in a timely manner."}, {"title": "3. Assumptions and Problem formulation", "content": "This section presents the assumptions and problem statements of this study, divided into two parts: Detection and Retrieval."}, {"title": "3.1. Detection", "content": "Given a monitoring video sequence V of duration \u03c4 > 0, partitioned into N equal-length temporal segments {\u03a6\u2081}\u1d62\u208c\u2081. The principal aim is to detect anomalies (violent events). Let V = {t}\u1d62\u208c\u2081 where each \u03a6\u1d62 \u2286 V may consist of non-violent (V\u2099\u1d65) and violent (V\u1d65) portions, so \u03a6\u1d62 = V\u2099\u1d65 \u222a V\u1d65.\nLet C = {C\u2096}\u2096\u208c\u2081 \u222a {\u0109} denote all classes, with C\u2096 denote the k-th violence class and \u0109 represent the non-violence class, and let L = {l\u2096}\u2096\u208c\u2081 \u222a {p} be the corresponding labels. Assume each \u03a6\u1d62 containing violence belongs to some c\u2096, where k \u2208 {1, ..., K}.\nConsider a detection mechanism M\u2208 W, which outputs for each \u03a6\u1d62 a score P\u1d62 \u2208 [0,1]. Let \u03b8\u2208 [0,1] be a decision threshold inducing \u03b4\u2091(\u03b8) = 1[P\u1d62 \u2265 \u03b8]. Let d\u1d62 = 1[V\u1d65 \u2260 \u00d8] be the ground truth.\nLet TP, TN, FP and FN, represent True Positive, True Negative, False Positive and False Negative respectively, of the prediction result of input V by applying mechanism M. Then TP\u1d62 = d\u1d62\u03b4\u2091(\u03b8), TN\u1d62 = (1 \u2212 d\u1d62)(1 \u2212 \u03b4\u2091(\u03b8)), FP\u1d62 = (1 \u2212 d\u1d62)\u03b4\u2091(\u03b8) and FN\u1d62 = d\u1d62(1 \u2212 \u03b4\u2091(\u03b8)), respectively.\nLet TP, TN, FP and FN represent the cumulative True Positive, True Negative, False Positive, and False Negative, respectively. Summations over l = 1, ..., N yield TP = \u03a3TP\u1d62, TN = \u03a3TN\u1d62, FP = \u03a3FP\u1d62 and FN = \u03a3FN\u1d62, respectively.\nLet P\u1d39, R\u1d39 denote the Precision and Recall of the predictions by applying mechanism M to predict a given video V. The values of P\u1d39, R\u1d39 can be calculated by P\u1d39 = TP / (TP+FP) and R\u1d39 = TP / (TP+FN), respectively.\nLet {\u03b8\u2c7c}\u2c7c\u208c\u2081 \u2286 [0,1] denote a finite collection of thresholds. Let AP\u1d39 denote the Average Precision of mechanism M. The AP\u1d39 can be calculated by Exp. (1):\nAP\u1d39 = \u03a3\u2c7c\u208c\u2081\u02b2\u207b\u00b9 (R\u1d39(\u03b8\u2c7c\u208a\u2081) \u2013 R\u1d39(\u03b8\u2c7c)) \u00d7 p\u1d39 (\u03b8\u2c7c\u208a\u2081).\nSimilar to [1]-[5] [7][13][19]-[28], the first objective of violence detection in this paper is to develop mechanism M\u1d47\u1d49\u02e2\u1d57 that satisfies M\u1d47\u1d49\u02e2\u1d57 = arg Max(AP\u1d39).\n\u2098\u2208M\nLet TPR(\u03b8) = \u03a4\u03a1(\u03b8) / (\u03a4\u03a1(\u03b8)+\u03a4\u039d(\u03b8)) and FPR(\u03b8) = FP(\u03b8) / (\u03a4\u03a1(\u03b8)+\u03a4\u039d(\u03b8)) denote the true-positive and false-positive rates as functions of \u03b8, respectively. Let A\u1d39 denote the AUC of mechanism M. The value of A\u1d39 can be calculated by Exp. (2):\nA\u1d39 = \u222b TPR(\u03b8) d(FPR(\u03b8)) = \u222b \u03a4\u03a1(\u03b8) / (\u03a4\u03a1(\u03b8) + FN(\u03b8)) d (FP(\u03b8) / (FP(\u03b8)+TN(\u03b8))).\nSimilar to [1]-[5] [7][13][19]-[28], the second objective of violence detection in this paper is to develop mechanism M\u1d47\u1d49\u02e2\u1d57 that satisfies M\u1d47\u1d49\u02e2\u1d57 = arg Max(A\u1d39).\n\u2098\u2208M"}, {"title": "3.2. Retrieval", "content": "Given a scenario involving both time-series data and multiple triplet relationships, let {a\u209c}\u209c\u208c\u2081 represent a sequence of observations recorded at each time t, and let {(h\u2099, r\u2099, t\u2099)}\u2099\u208c\u2081 denote a collection of triplets, where h\u2099, r\u2099 and t\u2099 are the head entity, relation, and tail entity, respectively.\nBy applying a time-series module, temporal context can be integrated with the triplet semantics, resulting in a set of embeddings {z\u2099}\u2099\u208c\u2081 \u2286 \u211d\u1d48. Each embedding z\u2099 corresponds to a triplet (h\u2099, r\u2099, t\u2099) as well as its associated temporal information {a\u209c}.\nConsider a query vector q \u2208 \u211d\u1d48. Let s(z\u2099, q) \u2208 \u211d denote the similarity function, which measures the relevance of z\u2099 to q.\nBased on the similarity scores, all embeddings z\u2099 are ranked in descending order of s(z\u2099, q). Denote the corresponding ranking function as \u03c0: {1, ..., N} \u2192 {1, ..., N}, such that: S(z\u03c0(\u2081), q) \u2265 S(z\u03c0(\u2082), q) \u2265 \u2026 \u2265 S(z\u03c0(\u0274), q).\nLet \u03b4\u2099 \u2208 {0,1} denote an indicator function that specifies whether z\u2099 is ground-truth relevant to the query q. Let \u03a3\u2099\u208c\u2081 \u03b4\u2099 represent the total number of relevant embeddings. Define R@k as the fraction of relevant items retrieved within the top-k positions out of all relevant items. For any k \u2208 N, it is defined as Exp. (3):\nR@k = \u03a3\u1d63\u208c\u2081\u1d4f \u03b4\u03c0(\u1d63) / \u03a3\u1d63\u208c\u2081\u1d3a \u03b4\u2099.\nWhen k \u2208 {1,5,10}, this corresponds to R@1,R@5, R@10, respectively. Similar to previous works [6] [31]-[35], this study aims to maximize retrieval performance. Within the retrieval strategy space R, the objective is to identify the optimal retrieval strategy R\u1d47\u1d49\u02e2\u1d57 such that: R\u1d47\u1d49\u02e2\u1d57 = arg Max{R@1,R@5,R@10}.\nR\u2208R\nThe above content constitutes the Assumptions and Problem Formulation of this paper, The next section will introduce the proposed TIO system in detail."}, {"title": "4. The proposed TIO system", "content": "This section provides a detailed introduction to the proposed TIO system. Unlike previous systems designed specifically for violence detection [7], [19]-[28] or violence retrieval [6], [31]-[35], the proposed TIO system in this paper offers two significant advantages: first, TIO is a multifunctional system capable of both violence detection and retrieval tasks; second, the design of TIO incorporates interpretability.\nFigure 4 illustrates the operational framework of the proposed TIO system. The system is divided into three main components: feature extraction, lightweight temporal sequence analysis and classification, and retrieval task deployment.\nSpecifically, in terms of feature extraction, the TIO system leverages LLM and ImageBind pre-trained modules to process the information in each frame of training data, constructing a knowledge graph and optimizing it using a GAT module. After the feature extraction, the compressed representation of each video frame is further processed by a lightweight temporal sequence module to generate a high-dimensional embedding for the entire video. Finally, the system connects to both a classifier and a retriever to achieve its multitasking objectives. During inference, through the output of the triplet relationships in the knowledge graph, users can clearly understand the basis of the model's decisions, thereby achieving interpretability.\nThe specific designs of these three components are detailed in the following subsections: Section 4.1 introduces the construction of the knowledge graph and GAT optimization, Section 4.2 describes the design of the lightweight temporal sequence module, and Section 4.3 explains how to integrate the classifier and retriever to complete the system training."}, {"title": "4.1. Feature Extraction", "content": "This section provides a detailed explanation of the feature extraction functionality of the TIO system. Notably, unlike previous works [6][7][19][28][31]-[35], which directly employed pre-trained modules for feature extraction, this study adopts a novel approach by constructing a knowledge graph and leveraging GAT-based reasoning for frame-level feature extraction. This method not only captures the semantic relationships between multimodal data but also enhances the interpretability and discriminative power of features through structured reasoning, thereby improving the accuracy of recognition and retrieval.\nThe feature extraction process is divided into two steps: the construction of the knowledge graph and the refinement via GAT. The specific details are described as follows:\nFirst, the construction of the knowledge graph: Fig. 5 illustrates the overall framework for generating the task-specific knowledge graph (KG). Specifically, let I = {I\u209c}\u209c\u208c\u2081"}, {"title": "4.2. Lightweight Temporal Sequence Module", "content": "Till now, after completing frame-level feature extraction, the extracted features capture instantaneous information but lack the global temporal context critical to video understanding. In this section, a lightweight temporal sequence module is proposed, analogous to the standard Receptance Weighted Key Value (RWKV) [36] encoder, incorporating self-attention, Layer Normalization (LN), and a Feed-Forward Network (FFN). Since the positions of frames in sequential data are already fixed, positional encoding is unnecessary when training the temporal sequence module.\nCompared to prior works [22]-[27], the primary distinction lies in the computation method of self-attention. This method is based on relative distances rather than feature similarity, making the model lightweight. By focusing solely on the relative distances between frames and avoiding the computation of high-dimensional feature similarity, this approach effectively reduces the complexity of attention computation, thereby lowering computational costs and improving training and inference efficiency.\nSpecifically, let V = {V\u2081, V\u2082, ..., V\u2099} denote the set of frames in the video. Using the GAT module, the embeddings h\u1d65\u1d62 \u2208 \u211d\u1d48 are obtained. All frame embeddings are integrated into a matrix H = [h\u1d65\u2081, h\u1d65\u2082, ..., h\u1d65\u2099] \u2208 \u211d\u1d3a\u00d7\u1d48.\nDefine A\u209c \u2208 \u211d\u1d3a\u00d7\u1d3a, where N represents the total number of frames. For any 1 \u2264 i, j \u2264 N, its elements are defined in Exp. (8):\n(A\u209c)\u1d62,\u2c7c = exp(-|i \u2013 j| / \u03c3),\nwhere \u03c3 > 0 is a hyperparameter that controls the degree of temporal distance decay. This design ensures that A\u209c depends solely on the relative distances between frame indices, reducing reliance on high-dimensional feature similarity.\nTo enhance numerical stability, a symmetric normalization strategy is adopted. Define the diagonal degree matrix D\u2208 \u211d\u1d3a\u00d7\u1d3a, with diagonal elements given by Exp. (9):\n(D)\u1d62,\u1d62 = \u03a3\u2c7c\u208c\u2081\u1d3a(A\u209c)\u1d62,\u2c7c\nand construct the symmetrically normalized adjacency matrix A\u0303\u209c = D\u207b\u00b9/\u00b2AD\u207b\u00b9/\u00b2. This symmetric normalization ensures the stability of gradient propagation and numerical operations during feature fusion.\nIn practice, the proposed temporal sequence module consists of two stages to integrate and enhance temporal context in the embedding matrix H.\nThe first stage is temporal context fusion. The normalized temporal adjacent matrix A\u0303\u209c is used to weight the frame embedding matrix H, as shown in Exp. (10):\nH' = softmax(A\u0303\u209c)H\u2208 \u211d\u1d3a\u00d7\u1d48,\nwhere softmax(A\u0303\u209c) performs softmax normalization row-wise on A\u0303\u209c ensuring a reasonable distribution of weighting coefficients. Subsequently, Layer Normalization (LN) is applied to H' to stabilize feature distributions, as shown in Exp. (11):\nH\" = LN(H').\nThe next stage involves non-linear transformation and residual connection. The fused features H\" are subjected to a FFN for non-linear transformation, with residual connections preserving the original information. The operation is as follows in Exp. (12):\nH''' = LN(FFN(H\") + H')\n= LN(ReLU(xW\u2081 + b\u2081)W\u2082 + b\u2082),\nwhere W\u2081, W\u2082 are the parameter matrices for linear transformations, b\u2081, b\u2082 are bias vectors, and the ReLU function provides non-linear mapping capabilities, enhancing the representational power of the model."}, {"title": "4.3. Classifier and Retriever", "content": "After completing the time-series computation, the classifier and retriever are integrated. The classifier is used to determine which predefined category the current image frame (or frame-level feature) belongs to. The retriever is used to locate the corresponding textual description in the database, enabling cross-modal retrieval.\nSpecifically, for the classifier, the loss function is defined as Lc\u2097\u209b, as shown in Exp. (13):\nLc\u2097\u209b = - 1/N \u03a3\u1d62\u208c\u2081\u1d3a \u03a3c\u208c\u2081\u1d9c \u03b4y\u1d62,c logp\u1d62,c\nWhere N is the number of samples or video frames in a batch, C is the total number of categories, and \u03b4y\u1d62,c is the indicator function defined as: \n\u03b4y\u1d62,c = { 1, if y\u1d62 = c ;  0, if y\u1d62 \u2260 c}\nThe p\u1d62,c = P(y\u1d62 = c|z\u1d62) represents the predicted probability by the classifier that the i-th sample belongs to category c and z\u1d62 is the feature representation of the frame or sample.\nFor the retriever, the loss function is defined as Lr\u2091\u209c, as shown in Exp. (14):\nLr\u2091\u209c = \u03a3(\u1d62,\u2c7c)\u2208\u03a9 max{0, \u03b1 + D(q\u1d62, t\u2c7c\u208a) \u2013 (q\u1d62, t\u2c7c)}\nWhere \u03a9 is the set of indices for all text-video pairs; q\u1d62 represents the embedding of the i-th image; t\u2c7c\u208a and t\u2c7c represent the text vectors matching q\u1d62 (positive sample) and not matching q\u1d62 (negative sample), respectively; D(q\u1d62,t\u2c7ct) is the distance function; and \u03b1 > 0 is the margin hyperparameter. This loss encourages the distance of correctly matched image-text pairs"}, {"title": "5. Experiment", "content": "In this section, relevant experimental performance and analysis are presented."}, {"title": "5.1. Dataset and Evaluation Metrics", "content": "For the detection task, the experiment employed two widely recognized datasets: UCF-Crime [18] and XD-Violence [19]. XD-Violence, the largest publicly available dataset for violence monitoring, includes 4,754 videos totaling 217 hours and covers six categories of violent incidents: verbal abuse, car accidents, explosions, fights, riots, and shootings. The dataset is divided into a training set of 3,954 videos and a test set of 800 videos, with the latter comprising 500 violent and 300 non-violent videos. The UCF-Crime dataset consists of 1,900 real-world surveillance videos, with 1,610 allocated for training and 290 for testing.\nRegarding evaluation metrics, the experiment adheres to the framework described in Section 3.1. For XD-Violence, the metric AP\u1d39 from Exp. (1) is used, while for UCF-Crime, the metric A\u1d39 from Exp. (2) is applied. This approach aligns with prior research [1][5][7] [13]-[28].\nFor the retrieval task, the experiment utilized the UCFCrime-AR [6] dataset, which contains 1,900 real-world surveillance videos accompanied by corresponding bilingual text descriptions. Of these, 1,610 videos are used for training and 290 for testing. Evaluation follows the setup outlined in Section 3.2. Specifically, the retrieval metric R@k from Exp. (3) is applied to the XD-Violence dataset, consistent with methodologies from earlier studies [6][31]-[35]."}, {"title": "5.2. Experimental Settings", "content": "In the configuration of some pretraining modules, the multimodal model used is ImageBind [13], with the version imagebind_huge. For knowledge graph construction, the object detection model for Entity 2 is YOLO-World [15], specifically the YOLO-Worldv2-X. The software utilized for KG storage is ArangoDB. In the configuration of the large language model (LLM), the Glaude 3.5 Sonnet API is invoked.\nFor hyperparameter settings, In Exp. (6) and Exp. (8), \u03c3 is set to 0.25 and 3, respectively. In Exp. (14), \u03b1 is set to 0.9, In Exp. (15), \u03bb is set to 1 and in Exp. (16), \u03b1c\u2097\u209b, \u03b1r\u2091\u209c and \u03b1GAT are set to 1.4, 1.3, and 1, respectively.\nThe training hardware consists of NVIDIA Tesla A100 40GB, and the system employs the Adam optimizer during training. The initial learning rate is set to 5 \u00d7 10\u207b\u2075, with a multiplicative decay factor of 0.95 per epoch."}, {"title": "5.3. Comparison with State-of-the-art Methods", "content": "Table 1 aims to compare the proposed TIO system with previous studies on fine-grained violence detection in the XD-Violence datasets. As shown in Table 1, the experimental results of the proposed TIO system on the XD-Violence dataset demonstrate significant performance advantages, particularly in its detection capability in highly dynamic and subtle anomaly scenarios.\nIn the Abuse category, the proposed TIO system achieves an AP\u1d39 of 17.84, which is a 6.86% improvement compared to the best baseline, MISSIONGNN (10.98), showcasing its outstanding ability in low-dynamic feature scenarios. In the Car Accident category, the proposed TIO system achieves an AP\u1d39 of 28.89, outperforming RTFM (25.36) and S3R (23.82), indicating its effectiveness in capturing subtle anomalies in car accident scenes.\nIn the more dynamic Explosion and Fighting categories, the proposed TIO system achieves AP\u1d39 of 73.54 and 79.98, respectively. In the Shooting category, the proposed TIO system achieves an AP\u1d39 of 32.65, significantly surpassing AnomalyCLIP (26.13) by 6.52%, demonstrating its adaptability to high-speed dynamic and explosive sound scenarios.\nThese results clearly demonstrate that the proposed TIO system leverages a combination of KG and GAT modules to effectively enhance anomaly detection performance. The KG helps model semantic relationships between different anomalous behaviors, while GAT further strengthens the selective attention to anomalous features. Moreover, the dual-branch design of the proposed TIO system achieves higher performance compared to single-branch systems designed specifically for violence detection. This advantage is due to the dual-branch architecture's ability to separately focus on visual and auditory feature modeling, enabling effective multimodal information fusion."}, {"title": "5.4. Ablation Study", "content": "Table 5 presents the ablation study results on the XD-Violence dataset and the UCF-Crime dataset, exploring the impact of the KG, GAT, and Time Temporal Sequence Module on the performance of the proposed TIO system.\nThe results indicate that enabling a single module improves the system's performance, though the effect is limited. For instance, with only KG enabled, the AP\u1d39 on XD-Violence is 78.12, and the A\u1d39 on UCF-Crime is 66.54, demonstrating that KG enhances the system's understanding of anomalous behaviors but is insufficient on its own for achieving high performance. When only GAT is enabled, the AP\u1d39 improves to 81.45 on XD-Violence, and the A\u1d39 increases to 70.32 on UCF-Crime, suggesting that GAT effectively models semantic relationships between anomalous features. Enabling only the Time Temporal Sequence Module results in an AP\u1d39 of 76.78 on XD-Violence and an A\u1d39 of 64.89 on UCF-Crime, showing that its contribution is relatively smaller and requires integration with other modules to maximize its potential. When two modules are combined, the system's performance improves significantly. For example, combining KG and GAT yields an AP\u1d39 of 87.34 on XD-Violence and an A\u1d39 of 76.12 on UCF-Crime, highlighting their complementary roles in capturing multimodal features and semantic relationships. The combination of GAT and the Time Temporal Sequence Module"}, {"title": "5.5. Visualization", "content": "Figure 8 visualization illustrates the analysis of violent scenes, showcasing how the integration of KG and GAT significantly enhances the model's interpretability. The image captures and highlights multiple violent scenarios, including"}, {"title": "5.6. Case Study", "content": "Using the proposed TIO system, an experiment was conducted to analyze the impact of bystander presence on the outcomes of violent behaviors. The retrieval results revealed an interesting phenomenon: in both the fighting and non-violent categories, the number of bystanders significantly influenced the outcomes of violent incidents. When more bystanders were present, conflicts often remained at the verbal level, manifesting as insults, arguments, and other forms of verbal conflict. For instance, one extracted triple relation described a scene where \"a man shouted at another person in a crowd,\" annotated as \"Verbal conflict,\" with accompanying information about the number of bystanders\u2014\"approximately 10 onlookers.\" Additionally, the system retrieved behaviors such as \"bystanders attempting to separate the conflicting parties,\" clearly illustrating the role of bystander intervention in de-escalating conflicts. This phenomenon suggests that the presence of bystanders can, to some extent, reduce the risk of violence escalation through deterrence or direct intervention.\nIn contrast, when fewer bystanders were present, the nature of the conflict underwent a significant shift. Analysis of the extracted triple relations revealed that conflicts were more likely to escalate from verbal to physical violence. For example, one triple described a scene where \"a man assaulted a passerby"}, {"title": "6. Conclusions", "content": "This study presents an innovative Three-in-One (TIO) system that provides a solution for weakly supervised violence detection (WSVD) and violence retrieval (VAR) tasks, significantly enhancing interpretability, system performance, and user experience. By introducing KG, GAT module and a dual-branch design, the system achieves end-to-end multi-task processing. It not only addresses the interpretability limitations caused by the black-box nature of models in current research but also optimizes complexity through the use of kernel function-based Euclidean distance in unified multimodal embeddings and graph attention networks. The lightweight design of the temporal module further improves computational efficiency and model performance.\nIn addition, compared to traditional independent model designs, the TIO system employs a unified multi-task model to reduce redundancy in design and training processes, promote information sharing and collaborative usage, and optimize semantic reasoning capabilities, thereby achieving dual improvements in system efficiency and user experience. Experimental results demonstrate that the proposed TIO system achieves outstanding performance on benchmark datasets such as XD-Violence, UCF-Crime and UCFCrime-AR, highlighting its extensive potential and research value in the fields of violence detection and retrieval. Furthermore, case studies validate its significance in sociology and the study of human activities."}]}