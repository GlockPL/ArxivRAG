{"title": "Lawma: The Power of Specialization for Legal Tasks", "authors": ["Ricardo Dominguez-Olmedo", "Vedant Nanda", "Rediet Abebe", "Stefan Bechtold", "Christoph Engel", "Jens Frankenreiter", "Krishna Gummadi", "Moritz Hardt", "Michael Livermore"], "abstract": "Annotation and classification of legal text are central components of empirical legal research. Traditionally, these tasks are often delegated to trained research assistants. Motivated by the advances in language modeling, empirical legal scholars are increasingly turning to prompting commercial models, hoping that it will alleviate the significant cost of human annotation. Despite growing use, our understanding of how to best utilize large language models for legal tasks remains limited. We conduct a comprehensive study of 260 legal text classification tasks, nearly all new to the machine learning community. Starting from GPT-4 as a baseline, we show that it has non-trivial but highly varied zero-shot accuracy, often exhibiting performance that may be insufficient for legal work. We then demonstrate that a lightly fine-tuned Llama 3 model vastly outperforms GPT-4 on almost all tasks, typically by double-digit percentage points. We find that larger models respond better to fine-tuning than smaller models. A few tens to hundreds of examples suffice to achieve high classification accuracy. Notably, we can fine-tune a single model on all 260 tasks simultaneously at a small loss in accuracy relative to having a separate model for each task. Our work points to a viable alternative to the predominant practice of prompting commercial models. For concrete legal tasks with some available labeled data, researchers are better off using a fine-tuned open-source model.", "sections": [{"title": "Introduction", "content": "The legal system generates a staggering volume of complex documents. United States federal courts alone process hundreds of thousands of cases a year, each having substantial case files. Much empirical legal research involves the systematic collection and analysis of such data in order to understand how laws function in practice and what impact they have on society. What limits researchers across the board is the cost of annotating and classifying legal documents. Legal classification tasks vary in complexity, but often require substantial expertise and effort. Employing trained research assistants stretches to a few hundred, perhaps a few thousand documents at a time, but is no match for the sheer scale of legal data.\nMotivated by the rapid advances in large language models, law scholars increasingly try out commercial models, such as GPT-4, on a variety of legal tasks, hoping to boost the efficiency of legal research. For lack"}, {"title": "Our contributions", "content": "We introduce and study a collection of 260 legal classification tasks, nearly all new to the machine learning community. The tasks we introduce are actual legal classification tasks based on the U.S. Supreme Court (Spaeth et al., 2023) and Court of Appeals (Songer) databases that provide rich annotations for court files. We use these annotations as labels to derive challenging multi-class machine learning tasks. In particular, our suite of tasks significantly extends and strengthens valuable recent efforts to benchmark language models for legal tasks (Guha et al., 2023).\nOur primary finding is that fine-tuning a single Llama 3 8B Inst (MetaAI, 2024) model on all classification tasks achieves vastly superior performance to GPT-4 zero-shot (Figure 1). Although it is expected that fine-tuning helps, the strong superiority of fine-tuning an open source model at much smaller scale is surprising. After all, GPT-4 is orders of magnitude larger according to available information and interpretation. Based on our comprehensive evaluation, we argue that the zero-shot performance of GPT-4 is far from sufficient for actual legal work. In contrast, a fine-tuned model provides a simple and practical alternative with much higher accuracy. In more detail, our primary contributions the following:\nTo establish baselines, we perform a zero-shot evaluation of recent state-of-the-art open source models (Figure 3), including Llama 3 8B Inst, Llama 3 70B Inst, Mistral 7B Inst, Saul 7B Inst, and Mixtral 8x7B Inst, and the commercial model GPT-4. Among these, only Llama 3 70B Inst and GPT-4 achieve zero-shot performance significantly better than a constant classifier that outputs the majority class. Still, there are dozens of tasks were both models perform worse than random guessing (Figure 4). Averaged across all tasks, GPT-4 has zero-shot accuracy 62.9% compared with 58.4% for Llama 3 70B. The constant classifier that outputs the majority class for each task achieves 41.7% accuracy, a number determined by class imbalance and the number of classes in each task. GPT-4 and Llama 3 achieve similar performance on the Court of Appeals tasks (63.4% and 60.3%, respectively), whereas GPT-4 has significantly higher accuracy on the Supreme Court tasks (59.8% versus 47.1%). Few-shot prompting GPT-4 does not improve performance. Section 2.3 provides more details.\nFine-tuning a single Llama 3 8B Instruct model, which we call Lawma 8B, outperforms GPT-4 zero-shot an all but a few of tasks. The improvements are typically in the double digit accuracy points, specifically, 22.6 percentage points on average for Supreme Court tasks and 16.5 points on average over Appeals Court tasks. Figure 1 displays the accuracy deficit of zero-shot prompting relative to fine-tuning across"}, {"title": "Related work and background", "content": "Adoption of large language models in the legal community. The legal community has moved relatively quickly in adopting GPT models. Several startups have begun using incorporating large language models, including GPT, into legal products (Wiggers, 2022). Lexis Nexis, a major commercial provider of law-related services, has partnered with Open AI and Anthropic to offer legal text generation (LexisNexis, 2023). Legal scholars have evaluated GPT's performance on the bar exam (Katz et al., 2024) as well as law school exam (Choi et al., 2023). Choi and Schwarcz (2023) examined how GPT-4 can improve student performance on law school exams. Nay et al. (2024) examined how LLMs perform on answering multiple choice questions related to tax law. Gray et al. (2024) used GPT models to extract information from cases concerning the factors that predict the constitutionality of police stops. Choi (2023) used GPT-4 to extract information concerning interpretative techniques from U.S. Supreme Court decisions. Livermore et al. (2023) tested the performance of GPT models for categorizing cases by issue areas and in recommending citations based on case similarity. Savelka and Ashley (2023) evaluate the zero-shot performance of GPT-4 on a variety of semantic legal annotation tasks. Engel and Mcadams (2024) ask GPT for the ordinary meaning of statutory terms. In the area of corporate law, Frankenreiter and Talley (2024) use GPT-4 to extract information about the contents of corporate charters."}, {"title": "Limitations", "content": "Fine-tuning increases accuracy to about 80% in our evaluation suite compared with around 60% for zero-shot prompting. While we are rather certain that 60% accuracy is insufficient for consequential legal work, we emphasize that 80% is still far from perfect. In addition, the variance in accuracy across tasks, while lower than for zero-shot prompting, remains high. Although our work meets the ethical and technical recommendations by Kapoor et al. (2024) for \"developers of legal AI\u201d, we maintain caution about the use of large language models for consequential legal tasks. To which extent these models are suitable for use in specific applications requires additional substantive investigation. We add that the legal documents we consider are exclusively from either the U.S. Supreme Court or appellate courts in the United States. We cannot speak to how these results may change for tasks in other legal domains within United States or legal systems in other countries."}, {"title": "Legal classification tasks", "content": "In this work, we focus on legal classification tasks. Legal classification tasks range in complexity, from extremely simple tasks that require little specialized knowledge, to highly sophisticated tasks that involve substantial expertise and judgment. Simple tasks would include identifying the parties to a case or the general issue area-for example, whether a case dealt with family law or commercial contracts. More sophisticated tasks involve specific legal knowledge, familiarity with legal principles or discourse, and the ability to engage in nuanced analogical or conceptual reasoning. For example, labeling the ideological valence of a decision requires the annotator to understand how specific legal issues map onto contemporary political debates, while labeling the standard of review applied by an appellate court requires detailed knowledge of these standards as well as the ability to parse procedural history. Many legal doctrines are quite complicated, involving multipart tests, nuanced exceptions, and balancing inquiries. Extracting features concerning such doctrines can lead to disagreement even among experienced annotators with considerable legal expertise.\nMore efficient ways to solve legal classification tasks would be tremendously useful in practice. A well functioning system to automatically extract relevant features from legal texts could, in particular, facilitate empirical legal study across a wide range of domains. This research could include not only social scientific study of the causes or consequences of judicial decisions, but also more traditional research modalities based on doctrinal interpretation (Livermore and Rockmore, 2019). There is an almost unlimited variety of features that legal scholars could study, ranging from the factors cited by judges when deciding the outcomes of property law disputes to the relationship between the party affiliation of judges and their use of different interpretative styles. With the digitization of legal texts at the U.S. state level and outside the United States, low-cost and flexible featurization can also boost efforts to show the geographic diffusion of legal concepts or approaches.\nTo summarize, our reasons to study legal classification tasks are both technical and substantive. From a technical machine learning perspective, these tasks provide highly non-trivial classification problems where even the best models leave much room for improvement. From a substantive legal perspective, efficient solutions to such classification problems have rich and important applications in legal research, as we discussed."}, {"title": "Data sources", "content": "Central to our study are the U.S. Supreme Court Database (Spaeth et al., 2023) (SCDB) and the U.S. Courts of Appeals database (Songer) (USCAD). The SCDB compiles comprehensive information on U.S. Supreme Court decisions from 1946 onward. Developed by Harold Spaeth, it includes variables such as case outcomes, issue areas, legal provisions, and vote counts. The USCAD contains detailed information about decisions made by the U.S. Courts of Appeals from 1925 to 1988. It includes data on judicial decisions, panel compositions, and case characteristics. Both databases provide essential tools for scholars conducting quantitative analyses of the judicial system, decision-making, ideological trends, and the impact of various factors on case outcomes.\nThe SCDB and USCAD have been instrumental in advancing research on judicial decision making within the fields of political science and empirical legal studies (Epstein et al., 2013; Segal and Spaeth, 2002; Martin and Quinn, 2002). These datasets have been used to drive a substantial research program by allowing"}, {"title": "Construction of classification tasks", "content": "We construct a set of classification tasks using the variables of the Supreme Court and the U.S. Courts of Appeals databases. We construct a total of 260 distinct classification tasks, 38 of them corresponding to the Supreme Court database and 232 to the U.S. Court of Appeals. For each of these classification tasks, we construct a prompt template consisting of a general description of the task, followed by a multiple choice question containing each of the possible variable codes. We formulate the task description, question, and answer choices by closely following the language of the variable description of the databases' documentation. Thereafter, for every case contained in these databases, we use the provided case citations to search for its corresponding majority opinion of the court on the Caselaw Access Project, a database of digitized court opinions.\nWe divide court opinions into a 70%/10%/20% train/validation/test split. Since many of the classification taks contain heavily imbalanced classes, we subsample the majority class such that there are at most as many task examples in the majority class as task examples in all other classes combined. Therefore, a constant classifier that outputs the majority class label will never achieve more than 50% accuracy on any task. We find that on average over all tasks, the majority classifier has 41.7% accuracy. We plot some statistics of the tasks in Figure 2. First, court opinions tend to be long, with 12% having above 8,000 tokens, the typical maximum context size for current state-of-the-art models, such as Llama 3. Second, some tasks have a large number of classes, with 28% of tasks having more than 10 classes. Third, there is a large variability in terms of the number of task examples, ranging from a couple dozen to 18500 task examples. In the remainder of the article, we highlight ten different tasks, six from the SCDB and four from the USCAD. These tasks differ in complexity, ranging from relatively simple tasks (e.g., determining the issue area) to relatively complex ones (e.g., determining the ideological \u201cdirection\u201d of the court decision). Section 2.4 contains a brief description of these tasks, Section B the full list of all tasks.\nExample of the prompt template. We use a prompt template identical to the one for the MMLU benchmark (Hendrycks et al., 2020). We provide as an example the prompt template corresponding to the Supreme Court precedent alteration variable (\"sc_precedentalteration\" in Section B)."}, {"title": "Highlighted tasks", "content": "Throughout this paper, as in Figure 3, we provide detailed results for ten tasks. Six of these tasks are from the SCDB, and four are from the USCAD. We selected tasks that we believe are particularly relevant to the legal community and chose tasks with varying levels of complexity, ranging from relatively simple (e.g., determining the issue area) to more complex (e.g., determining the ideological 'direction' of the court decision).\nFour tasks from the USCAD and all tasks from the SCDB were selected to form pairs, with each pair consisting of one task from the USCAD and one from the SCDB that capture similar concepts. It is important to note that, despite capturing broadly similar concepts, the precise formulation of the tasks might differ between the USCAD and the SCDB, making them less than perfectly comparable. In addition to the four pairs, we include two tasks from the SCDB that involve determining features of the decision reviewed by the Supreme Court on the basis of the Supreme Court opinion.\nThe following is a description of the task pairs:\nSC Issue Area / Songer Gen Issue: These tasks capture the case's issue area, requiring a determination of whether the case belongs to one of several broadly defined categories, such as criminal cases or First Amendment cases. These tasks are expected to be of relatively low complexity.\nSC Case Source / Songer Case Origin: These tasks require identifying the court or adjudication body where the case was originally initiated before moving up the judicial hierarchy. Like the previous pair, these tasks are expected to be of relatively low complexity.\nSC Disposition / Songer Treatment: These tasks involve determining how the deciding court treated the lower court opinion it reviewed, such as whether it affirmed or reversed the opinion. We consider these tasks to be of relatively low complexity.\nSC Direction / Songer Direction: These tasks involve determining the ideological 'direction' of the decision, specifically whether the decision supports a \u201cconservative\u201d or \u201cliberal\u201d outcome. We consider these tasks to be comparably complex.\nSC LC Disposition / SC LC Direction: These tasks involve determining the disposition and ideological 'direction' of the decision reviewed by the Supreme Court. As these tasks require analyzing features of another decision based on the text of the Supreme Court decision, we consider these tasks to be comparably complex."}, {"title": "Fine-tuning and the power of specialization", "content": "In this section, we present a detailed analysis of fine-tuning on different tasks. We start by fine-tuning Llama 3 8B Inst and Llama 3 70B Inst on all tasks simultaneously, resulting in our Lawma 8B and Lawma 70B models. We then perform additional fine-tuning experiments highlighting different aspects, including the scaling behaviour of fine-tuning larger base models, the sample efficiency of fine-tuning, its generalization to unseen tasks and Courts, and the effect of single task specialization.\nWe first fine-tune Llama 3 8B Inst and Llama 3 70B Inst on all tasks simultaneously. We refer to these fine-tuned models as Lawma 8B and Lawma 70B, respectively. We fine-tune on the 260 classification tasks described in Section 2.2. The fine-tuning dataset contains a total of 1.96B tokens. For Lawma 3 8B, we fine-tune for 3 epochs. For Lawma 3 70B, we fine-tune for 1 epoch. We find that additional epochs hurt performance. See Appendix A for additional details regarding the model training.\nWe compare in Figure 5 the task accuracies of Lawma 8B and 70B to that of their respective base models Llama 3 8B Inst and Llama 3 70B Inst, as well as GPT-4. Fine-tuning leads to large improvements in average task accuracy: Lawma 8B outperforms Llama 3 8B Inst by 37.7 accuracy points and Lawma 70B outperforms Llama 3 70B by 23.5 accuracy points. Both Lawma 8B and Lawma 70B outperform GPT4, Lawma 8B on average by 17.3 accuracy points and Lawma 70B on average by 19.0 accuracy points.\nMoreover, the Lawma models are the top performing models in each of the 10 highlighted tasks. In fact, the both Lawma 8B and Lawma 70B outperforms GPT4 in about 95% of all tasks, see Figure 1. Figure 6"}, {"title": "Performance after fine-tuning scales with pretraining compute", "content": "The performance of specialized models tends to scale with pretraining compute (Dominguez-Olmedo et al., 2024). We investigate how performance after fine-tuning scales with the pretraining compute of the base model. We fine-tune the following models for a single epoch: Pythia 70M, Pythia 160M, Pythia 410M, Pythia 1B, Pythia 2.8B, Pythia 6.9B (Biderman et al., 2023), Llama 2 7B (Touvron et al., 2023), Llama 3 8B Inst and Llama 3 70B Inst. We fine-tune on all 260 tasks simultaneously. We approximate pretraining compute in"}, {"title": "Sample efficiency", "content": "We study how task accuracy scales as models fine-tune on more training examples. We consider the 10 tasks highlighted in Section 2.4. We fine-tune Llama 3 8B Instruct on each task independently, rather than on all tasks simultaneously as in the previous experiments. For each task, we fine-tune on 10, 50, 100, 250, 500, and 1000 task examples. We select task examples uniformly at random, and train 5 different models corresponding to different random seeds on the examples selected for training. We therefore fine-tune and evaluate a total of 10\u22c56\u22c55 = 300 models. We fine-tune for a maximum of 20 epochs and early stop when validation loss increases for 3 consecutive epochs.\nFigure 8 shows how accuracy improves with the number of training examples. Fifty training examples are enough to match or beat the GPT-4 zero-shot baseline for 6 out of the 10 highlighted tasks, and 250 traning examples are enough to match or beat GPT-4 for 8 out of the 10 highlighted tasks. This is crucial, since labeling a few hundred data points is often financially feasible for many legal scholars (Hall and Wright,"}, {"title": "Specializing for single tasks", "content": "We now study how much accuracy we stand to gain by fine-tuning on a single task. We specialize models for each of the 10 tasks highlighted in Section 2.4. We specialize the follow models: Llama 3 8B Inst, Llama 3 8B Inst fine-tuned for one epoch on all tasks, and Lawma 8B (i.e., Llama 3 8B Inst fine-tuned for three epochs on all tasks). For each task, we fine-tune for a maximum of 20 epochs and early stop when validation loss increases for 3 consecutive evaluation stpes, each corresponding to one tenth of an epoch.\nFigure 9 shows the results of specialization to single tasks. First, we observe that, for 7 out of 10 tasks, Llama 3 8B Inst fine-tuned on all tasks for one epoch (yellow) outperforms Llama 3 8B Inst specialized for a single task (blue). That is, there is value to fine-tuning on our entire dataset rather than overspecializing for a single task. One explanation is that there is substantial cross-task overlap, and fine-tuning on the entire dataset amounts training on many more examples \u2013even if on average these examples are less relevant.\nSecondly, we observe that after fine-tuning on all 260 tasks for 1 epoch (yellow), further specializing for a single task (green) improves performance on all cases. Importantly, the latter outperforms the specialized Llama 3 8B Inst (blue) in all tasks. That is, a model that is fine-tuned on everything provides a \u201cbetter\" foundation from which to then \u201coverspecializing\" for a single task.\nThirdly fine-tuning on everything for three epochs (i.e., Lawma 8B, in red) again improves over the specialized models (i.e., green). Lastly, \u201coverspecializing\u201d Lawma 8B for a single task results in small single digit improvements for 3 out of the 10 tasks. However, we observe no benefits from specializing Lawma 8B for most (7/10) of the tasks.\u00a7 These results show that we don't leave much accuracy on the table by fine-tuning a single model for all tasks. This is practically quite appealing, since it obviates the need to maintain a separate model for each task. A single model suffices."}, {"title": "Generalization to unseen databases", "content": "We now investigate whether fine-tuning only on the Songer Appeals Court database allows us to generalize to the Supreme Court database. We fine-tune Llama 3 8B Inst for one epoch on all Songer tasks simultaneously. We plot in Figure 10 the mean accuracy for Court of Appeals tasks and Supreme Court tasks at intermediate checkpoints. As expected, performance on Court of Appeals tasks improves monotonically with the number of training examples seen. More interestingly, we observe that mean task accuracy for the Supreme Court also improves substantially, by up to 18.8 accuracy points at 20% of the training steps. Thereafter, performance degrades, seemingly plateauing at 11.3 accuracy points above the baseline non-finetuned performance of Llama 3 8B Inst.\nOur findings indicate that, since there is some degree of overlap between Court of Appeal and Supreme Court tasks, fine-tuning on the former transfers to the latter. This suggests that Lawma might be of practical use beyond the Supreme Court and Court of Appeals tasks it was trained on.\nNote, however, that fine-tuning only on the Court of Appeals database results in a mean case accuracy of 51.6%, compared to 82.4% for Lawma 8B. That is, not fine-tuning on Supreme Court cases results in a 30.9 accuracy points decrease in performance. These results again highlight the importance of fine-tuning precisely on the target tasks of interest."}, {"title": "Intercoder agreement analysis", "content": "The Songer Appeals Court database provides intercoder agreement rates for a subset of the variables. These intercoder agreement rates provide valuable context for the performance of our model. Specifically, intercoder agreement gives us information about the inherent label noise in the annotation procedure. In particular, the intercoder agreement rate gives a natural upper bound on model performance, as we cannot expect the model to perform well when the label is uncertain or subject to interpretation.\nHowever, we cannot directly compare intercoder agreement rates with the accuracy numbers we report. The reason is that in each task we subsampled the majority class to be no larger than the union of all other classes. This is a design choice we made to account for class imbalance. In this section, we map our model's accuracy to adjusted accuracy numbers that undo the subsampling step. This results in accuracy numbers that are commensurate with the intercoder agreement rate.\nTable 2 considers several tasks from the Appeals Court database, including the selected ones we highlighted in various figures. Each row corresponds to one task and provides the intercoder agreement rate, adjusted (and unadjusted) accuracy achieved by Lawma 8B, and the fraction of samples we retained in the majority class. A fraction of 100% means that we kept all samples. The smaller the fraction the larger the majority class is relative to the other classes.\nThe table contains several interesting insights:\nThe adjusted accuracy of Lawma 8B is generally within single digit percentage points of the intercoder agreement rate for easy tasks such as general issue classification (GENISS).\nLawma 8B is surprisingly close on the two tasks with the lowest intercoder reliability, i.e., WEIGHTEV and PROCEDUR. This shows that high intercoder reliability is no prerequesite for the model to perform well, i.e., close to the agreement rate.\nOn harder tasks, like identifying the ideological valence of a decision (DIRECT1 and DIRECT2), Lawma 8B is below the agreement rate by double digit percentage points.\nTasks with very high agreement rate (e.g., CIRCUIT and COMMENT) are not all alike. Some of them (e.g., COMMENT) correspond to a task with extreme class imbalance. Here, the model reaches the agreement rate. Other tasks (e.g., CIRCUIT) have perfect agreement rate, no class imbalance, and yet Lawma is far from the agreement rate.\nThese findings speak to the task heterogeneity and the non-trivial nature of the task suite as a classification benchmark."}, {"title": "Discussion", "content": "The cost of human annotators represents a considerable bottleneck for the field of empirical legal studies. In many scientific disciplines, the advent of low-cost and flexible tools for data extraction can lead to tremendous boosts in scholarly productivity and knowledge production. For example, the falling cost of genetic sequencing led to a paradigm shift across the biological sciences, as genetic data became increasingly available in fields as disparate as public health and entomology (K\u00f6ser et al., 2012; Ballare et al., 2019). A flexible automated feature extraction tool for legal texts holds similar potential for empirical legal studies, as a large realm of conceivable but impracticably expensive research projects becomes accessible. In addition, such tools would boost the utility of existing legal databases.\nThe few-shot capabilities of large language models are vital for commercial APIs, where users are largely restricted to prompting. But as we show, zero-shot prompting is neither sufficiently good nor best possible for classification tasks that arise in empirical legal work. Lightly fine-tuned special purpose models achieve significantly higher accuracy from relatively few labeled examples. Labeling a few hundred cases is often financially feasible. This suggests a simple and practical strategy for solving legal classification tasks: Obtain a few hundred labeled examples, fine-tune an open source model, and use the fine-tuned model to annotate the remaining cases.\nMulti-shot prompting is unlikely to provide much relief about the limitations of zero-shot prompting. Many legal documents are so long that a single document essentially consumes the entire prompt window. For example, 38% of court opinions in our dataset have more than 4,000 tokens. The default GPT-4 context size is 8,000 tokens. A version of GPT-4 allowing for 32,000 tokens is substantially more expensive. Fine-tuning avoids this problem altogether and robustly handles long documents.\nThe tasks we introduce are also interesting from a benchmarking perspective. The accuracy numbers are neither too low nor too high. The best models achieve non-trivial, but modest zero-shot performance. And even fine-tuned models don't reach intercoder agreement rates. This situation suggests that these legal classification tasks may be good test cases for future model advances. As such, we hope to extend and strengthen existing evaluation efforts."}, {"title": "Fine-tuning details", "content": "Compute requirements. We fine-tune on a cluster consisting of NVIDIA H100 GPUs. Fine-tuning on all tasks simultaneously required approximately 600 H100 hours for the 8B model and 1600 GPU hours for the 70B model. In total, the experiments presented in the paper required approximately 8000 H100 GPU hours."}, {"title": "Lawma", "content": "We fine-tuning with a maximum sequence length of 8192 tokens. We use the AdamW optimizer with full precision, \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.95, \u0454 = 10\u00af\u00ba. We use a peak learning rate of 2\u00b710\u20136. We use a cosine learning rate schedule, with 180 warm-up steps (approx. 4% of a full epoch) and decay to 10% of the peak learning rate. We use a weight decay of 0.1. We clip gradient to 1.0 max norm. We pack samples using the axolotl library (Cloud, 2024), which improves training efficiency by approximately 40%. For Lawma 8B, we fine-tune Llama 3 8B Instruct for 3 epochs. We train on a node of 7 H100s using DeepSpeed Zero 2, with a global batch size of 56. For Lawma 70B, we fine-tune Llama 3 70B Instruct for 1 epoch. We train on 8 nodes of 8 H100s each using DeepSpeed Zero 3, with a global batch size of 64. We find that additional epochs hurt average task performance, although performance continues to improve for some of the tasks."}, {"title": "Additional fine-tuning experiments", "content": "The hyperparameters are identical to those used for Lawma unless otherwise specified.\nScaling experiments. We fine-tune the Pythia and Llama 2 models with a peak learning rate of 2\u22c510\u22125, which we find to be result in higher performance than a peak learning rate of 2\u22c510\u22126. For the Llama 3 models, we use a learning rate of 2 \u22c5 10\u22126, which we find to be perform better than 2\u22c510\u22125. We fine-tune for a single epoch. We use a batch size 64. We fine-tune models with their pretraining max sequence length, that is, 2k tokens for Pythia, 4k tokens for Llama 2, and 8k tokens for Llama 3. We use a warm up ratio of 0.03. Due to the costs associated with training the 70B model, we simply take Lawma 70B rather than re-training the model with these slightly different training hyperparameters.\nSample efficiency and specialization We fine-tune for up to 20 epochs. We evaluate the loss on a separate validation set and early stop if the loss increases for 3 consecutive evaluation steps. For the sample efficiency experiments, we evaluate at the end of every epoch. For the specialization experiments, we evaluate every 0.1 epochs. We decay the learning rate to 10% of the peak learning rate over the 20 epochs. We fine-tune with a batch size of 64. For the specialization experiments, we train models both with and without learning rate warm up, and report the accuracy of the best model. We use the AdamW BitsAndBytes 8-bit optimizer, allowing us to fine-tune the models in a single H100 GPU.\nGeneralization We fine-tune only on the Songer Court of Appeals tasks. We fine-tune with batch size 64. We fine-tune for one epoch and we checkpoint models at 10, 30, 60, 100, 300, 600, 1000, 2000, and 3000 training steps. A full epoch on the Songer Court of Appeal tasks corresponds to 3096 training steps."}]}