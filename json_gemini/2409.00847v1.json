{"title": "The Design of an LLM-powered Unstructured Analytics System", "authors": ["Eric Anderson", "Jonathan Fritz", "Austin Lee", "Bohou Li", "Mark Lindblad", "Henry Lindeman", "Alex Meyer", "Parth Parmar", "Tanvi Ranade", "Mehul A. Shah", "Benjamin Sowell", "Dan Tecuci", "Vinayak Thapliyal", "Matt Welsh"], "abstract": "LLMs demonstrate an uncanny ability to process unstructured data, and as such, have the potential to go beyond search and run complex, semantic analyses at scale. We describe the design of an unstructured analytics system, Aryn, and the tenets and use cases that motivate its design. With Aryn, users can specify queries in natural language and the system automatically determines a semantic plan and executes it to compute an answer from a large collection of unstructured documents using LLMs. At the core of Aryn is Sycamore, a declarative document processing engine, built using Ray, that provides a reliable distributed abstraction called DocSets. Sycamore allows users to analyze, enrich, and transform complex documents at scale. Aryn also comprises Luna, a query planner that translates natural language queries to Sycamore scripts, and the Aryn Partitioner, which takes raw PDFs and document images, and converts them to DocSets for downstream processing. Using Aryn, we demonstrate a real world use case for analyzing accident reports from the National Transportation Safety Board (NTSB), and discuss some of the major challenges we encountered in deploying Aryn in the wild.", "sections": [{"title": "INTRODUCTION", "content": "Large language models have inspired the imagination of industry, and enterprises are using LLMs for product search, customer support chatbots, code co-pilots, and application assistants, to name a few. In enterprise settings, accuracy is paramount, so to limit hallucinations, most of these applications are backed by semantic search architectures. They answer queries based on data retrieved from a few documents or chunks in their knowledge base, a technique known as retrieval-augmented generation (RAG) [9].\nWe increasingly see that enterprises want to go beyond RAG, and run semantic analyses that require complex reasoning across their repositories of unstructured documents. For example, they want to analyze investment opportunities from research reports and presentations in financial services. From interview transcripts, they want to understand the sentiment toward a particular brand in comparison to others for marketing purposes. From legal summaries, investigators want to understand rules that were violated and the actions taken across a broad set of companies and cases.\nIn addition to simple \"hunt and peck\" style patterns seen in RAG, these analyses often require \"sweep and harvest\" style patterns, where one needs to sweep through large collections, perform semantic operations described by natural language (for example, to filter, extract, or summarize information), and then synthesize an answer. For example, \"What percent of environmentally caused incidents were due to wind?\" or \"What is yearly revenue growth and outlook of companies whose CEO recently changed?\". We also see \"data integration\" style patterns where users want to combine information from multiple document collections. For example, \"list the fastest growing companies in the BNPL market and their competitors\", where the competitive information may involve a lookup in a database, in addition to unstructured document search. We also expect compositions of these patterns will become prevalent.\nTo answer these types of queries, we're building a system for unstructured analytics that is powered by LLMs, eponymously called Aryn. We take inspiration from relational databases, where we borrow the principles of declarative query processing. Users specify what they want to ask in natural language, and the system automatically constructs a plan (the how) and executes it to compute the answer. We use LLMs liberally for multiple purposes throughout the Aryn stack.\nIn this paper, we describe the motivating use cases for Aryn and its architecture. We will demonstrate an end-to-end use case querying NTSB\u00b9 incident reports, which consist of a large collection of unstructured PDF documents containing text, images, figures, and tables. We will do several end-to-end demonstrations of how the Aryn system works, focusing on its components and how they fit together across a few different use cases. We will show the user interfaces to inspect, analyze, and debug the plans generated by Aryn, as well as the simplicity of the Sycamore programming framework that makes it easy to analyze vast collections of hierarchical unstructured documents. Aryn is fully open source, Apache v2.0 licensed, and available at github.com/aryn-ai/."}, {"title": "USE CASES, CHALLENGES, AND TENETS", "content": "Our primary interest is enterprise data applications, where LLMs have shown a remarkable ability for summarization, information extraction, semantic analysis, and filtering. LLMs have the potential to finally achieve the dream of seamlessly processing both unstructured and structured data, although many challenges still remain.\nUse Cases: LLMs have shown promise in a wide range of enterprise use cases:\n(a) Building AI assistants and chatbots for customer support, typically powered by technical documentation, ticketing systems like Jira, and internal messaging boards like Slack.\n(b) In the manufacturing domain, building Q&A systems over product and service manuals involving text, images, and tables for thousands of parts and products.\n(c) Summarizing and analyzing patient records in hospitals and medical clinics.\n(d) Financial analysts undertaking research on companies, using board decks, call transcripts, research reports, and financial malfeasance actions.\n(e) Analyzing legal proceedings and governance documentation, like privacy policies, for compliance purposes.\nChallenges: LLMs generate answers based on their massive training data. However, using LLMs on their own as question-answering oracles is insufficient for enterprise use cases, where the relevant data would not typically have been seen during model training. LLMs are known to hallucinate, a liability in financial services, healthcare, and government intelligence settings. In such settings users want accurate and explainable answers.\nRetrieval-augmented generation (RAG) extends LLMs with external data, grounding answers to limit hallucinations. While RAG has shown limited success with small and well-behaved datasets, the simple RAG approach simply does not scale. RAG accuracy degrades quickly as one asks more complex questions, adds more data, or works with more complex data.\nThere are three fundamental limitations to RAG. The first is that LLM context sizes are limited, requiring answers to be contained in the limited snippets of text that can be extracted from the document set. This works for simple factual questions where an answer is contained in a small number of relevant chunks of text, but fails when the answer involves synthesizing information across a large document collection.\nStudies show that LLMs with extremely long contexts cannot \"attend\" to everything in the context [13]. In addition, with large contexts, it is harder to determine the provenance of the result, making explainability more difficult. Finally, data sizes are increasing much faster than we can increase the size of LLM context windows.\nSecond, the accuracy of RAG depends critically on the use of embedding models to match the user's query with relevant chunks of text from the document collection. Despite their large training dataset, embedding models simply cannot anticipate every type of question or data. As more data is added, accuracy deteriorates, as it becomes harder for embedding vectors to discriminate between chunks. Moreover, analytics-style questions often involve time, hierarchy, or categories, such as \"Which companies' CEOs changed in the Al sector last year?\" Vector embeddings do not precisely capture time, hierarchy, or categories, so narrowing on time ranges or faceting is not possible.\nThird, RAG is not designed for complex documents with tables, graphs, or images. While a number of tools exist for extracting plain text from complex documents, this approach fails to retain critical semantic information. As a simple example, a table split across two pages of a PDF file, where the table heading is only present on the first page, will generally befuddle text extraction tools which will treat the second page as a separate table (with no heading). Likewise, retrieval of chunks of text during the RAG process will generally fail to include the important metadata associated with the table, such as the types of each of the columns.\nTenets: To address these challenges, we adopted the following tenets in the design of Aryn."}, {"title": "ARCHITECTURE", "content": "Figure 1 shows the high level architecture of Aryn. We use modern vision models as the first step of data preparation in the Aryn Partitioner. It decomposes and extracts structure from raw documents and transforms them into DocSets. We developed our own model based on the deformable DETR architecture [26] and trained on DocLayNet [20].\nAt the core of Aryn is Sycamore, a document processing engine that is built on DocSets. DocSets are reliable distributed collections, similar to Spark [24] DataFrames, but the elements are hierarchical documents represented with semantic trees and additional metadata. Sycamore includes transformations on these documents for both ETL purposes, e.g., flatten and embed, as well for analytics, e.g., filter, summarize, and extract. We use LLMs to power many of these transformations, with lineage to help track and debug issues when they arise. Sycamore can read data from a data lake (or similar) where unstructured data is kept, and can store processed data in a variety of indexes, including keyword, vector, and graph stores, for use during query processing.\nOur query layer, Luna, includes the planner that translates natural language questions into semantic query plans. We use LLMs for"}, {"title": "PARTITIONER", "content": "The first step in most document processing workflows is to partition or segment the document into pieces, typically rectangular bounding boxes, and label them using a type system for document chunks. These chunks can then be further processed with type specific transformations.\nWhile developing Aryn, we experimented with a variety of open-source partitioners, from hardcoded heuristics to advanced Al models. We quickly found that these tools lacked the fidelity and accuracy we needed to get high quality results for RAG and unstructured analytics. We decided to build our own partitioner powered by a vision-based document segmentation model.\nWe chose the Deformable DEtection TRansformer (DETR) model [26] as our object detection model, and trained it on DocLayNet [20], an open source, human-annotated document layout segmentation dataset. This resulted in a much better object detection and labeling for enterprise documents\u00b2. Our model achieved a mean average precision (mAP) of 0.602 and a mean average recall (mAR) of 0.743 on the DocLayNet competition benchmark. By contrast, a document API from a large cloud vendor achieved only an mAP of 0.344 with an mAR of 0.466. The output of the Aryn Partitioner can be consumed directly as JSON or integrated with the Sycamore document processing system described in Section 5.\nJust identifying document components is not enough to enable accurate analytics and question answering, so we included high-quality table extraction, image processing, and OCR as well. For example, we perform table extraction: when the model identifies and labels a component as table, we use the Table Transformer model [17] to identify the bounding box of each cell in the table, and then intersect those bounding boxes with the text extracted from the PDF using a tool like PDF Miner\u00b3. Users can then use Sycamore to manipulate the table using subsequent transforms, for example, converting it to formats like HTML, CSV, and Pandas\u2074 Dataframes.\nThe Aryn Partitioner can also identify and process images in documents using multi-modal LLMs. This allows for image metadata and textual summarization of image contents to be extracted and queried. Likewise, many enterprise documents contain images of printed or handwritten text, requiring an OCR stap for extracting the relevant text. This requires an accurate segmentation model to identify portions of the image that contain text, which can then be processed using OCR models such as EasyOCR5."}, {"title": "SYCAMORE", "content": "Sycamore is the open-source document processing engine at the center of the Aryn system [1]. We built Sycamore to support both data preparation and analytics over complex documents. One of the primary motivations for Sycamore was the observation that the line between ETL and analytics gets blurred when dealing with unstructured data. In particular, we need the flexibility to postpone some of the operations that are typically done in the ETL phase to the analytics phase, and vice versa. Sometimes this is due to cost - operations involving vision models or LLMs are quite expensive, and can't always be run at ETL time. On the other hand, the space of possible queries is very large, and we may need to reprocess original documents with context from a specific query.\nTo accommodate these challenges, we built Sycamore as a dataflow system modeled on Apache Spark [24] with extensions to integrate with LLMs and support unstructured documents.\nData Model\nDocuments in Sycamore are hierarchical and multi-modal. A long document may have chapters that are broken into sections, which in turn contain individual chunks of text, or entities like tables and images. The latter data types are particularly important for many analytics queries and need special treatment. More precisely, a document in Sycamore is a tree, where each node contains some content, which may be text or binary, an ordered list of child nodes, and a set of JSON-like key-value properties. We refer to leaf-level nodes in the tree as elements. Each element corresponds to a concrete chunk of the document, such as a paragraph, image or table and may have special reserved properties based on their type. For example, a TableElement has properties containing rows and columns, while an ImageElement has information about the format and resolution.\nDocSets are flexible enough to represent documents at different stages of processing. For example, when first reading a PDF, it may be represented a single-node document with the raw PDF binary as the content. After processing, each section is an internal node and tables and text are identified as leaf-level elements."}, {"title": "Programming Model and Transforms", "content": "Programmers interact with Sycamore in Python using a Spark-like model of functional transformations on DocSets. Table 1 shows several of Sycamore's transformations. These include standard functional operators like map and filter, which allow clients to pass in arbitrary Python UDFs to perform custom operations. Structural transforms modify the structure of the document tree. Most Sycamore scripts start with the partition transform to parse a raw document and break into sections and elements, as discussed in the previous section. The explode transform creates a new DocSet containing the elements of its input documents. This is useful when preparing individual chunks to write into a vector store, for instance. Analytic transforms include operations like reduce_by_key and sort that operate on the properties of each document. These transforms all handle missing values to accommodate the fact that some documents may be missing certain fields. Sycamore does not yet support full joins.\nLLM-powered transforms are used to enrich Documents. The most basic, 11m_query, allows callers to specify a prompt that will be used to process each document. The prompt can be parameterized by the content of the document and/or the properties of the document, and can be configured to process a subset of elements for example, a prompt to extract the authors of a paper might only need to look at a prefix of the elements on the first page. The output is stored in a property of the input document. Sycamore includes a number of more specific transforms like extract_properties and summarize that leverage built-in prompts. For all of these transforms, Sycamore handles retries and model-specific details like parsing the output as JSON. Sycamore supports a variety of LLMs, including OpenAI, Anthropic, and Llama.\nThe code in Figure 3 is an example of processing NTSB data with Sycamore. It reads the data and partitions it using the Aryn Partitioner, described in Section 4. It then executes the extract_properties transform, which takes a JSON schema and attempts to extract those fields from each document using an LLM. As shown in in Figure 4, this approach correctly extracts the state abbreviation and other fields from the document. Next, we use explode to break each document into a collection of document chunks, and then we generate"}, {"title": "Execution", "content": "Sycamore adopts a Spark-like execution model where operations are pipelined and executed lazily when materialization is required. To assist with debugging and avoid redundant execution, Sycamore also supports a flexible materialize operation that can save the output of intermediate transformations to memory or disk. Sycamore is built on top of the Ray compute framework [15], which provides primitives for distributing Python objects. We chose Ray because it is well integrated with Python, which has become the language of choice for machine learning applications. Ray is also well integrated with existing ML libraries."}, {"title": "LUNA", "content": "A hallmark of relational databases is declarative query processing. Since applications need not worry about the low-level details of how queries are executed, it is easier for them to adapt to changing workloads and scale. LLMs make it possible to leverage this idea for natural language queries over complex, unstructured data. We call this LLM-powered unstructured analytics, or Luna for short.\nLuna enables users to ask complex questions on data without having to understand the underlying primitives and execution model. This approach works well for data exploration and simple questions, but as the nature of questions becomes more complex and the semantic nature of data comes into play, a fully automated system falls short. Lack of visibility into how an answer was computed results in a lack of trust in a query system. The inability to correct or refine a query causes significant difficulty in trying to get a good answer. Luna solves this by exposing a logical query execution plan, data lineage, and execution history for all queries. This allows a user to understand the exact operations that were performed to answer a query, how the dataset was transformed during each operation, and modify any part of the plan to better align with their query's intention. With a combination of automated query undestanding, and efficient execution with full user control, we believe Luna is"}, {"title": "Luna architecture", "content": "Luna consists of a number of pieces that work together to provide an end-to-end natural-language query processing system over complex, unstructured data.\nQuery operators. We support a combination of traditional data-processing operators (count, aggregate, join) and semantic operators (llmFilter, llmExtract). This combination allows Luna to answer typical analytical queries (e.g., count of certain record types), but also queries that requires semantic understanding (e.g., whether a given piece of text carries a positive or negative sentiment).\nData schema. Luna operates on data ingested using Sycamore, benefitting from structured information extracted from unstructured data. Luna uses this schema during the query planning phase to determine the appropriate set of operators to use to answer a query. The schema can evolve over time, based on new semantic relationships discovered in the data.\nNatural language inferface. Luna supports a natural language query interface, making it easy for users to get started without learning a new programming interface. Luna also expresses the query plans it produces as natural language text, making it easy for users to understand how Luna arrived at an answer.\nQuery planning. Luna uses an LLM to interpret a user question and decompose it to a DAG of data processing operations. The LLM is prompted with the user's query and is asked to generate a query plan using a fixed set of operators and data sources. The LLM generates the plan in JSON format, which we translate into Sycamore code for execution.\nPlan optimization. Query operators vary significantly in latency, computational load, and monetary cost. The plan optimizer makes tradeoffs based on cost vs efficiency to handle different scenarios. It is able to combine and batch operations when possible, and make decisions about what technique (string matching vs semantic matching), and tool (e.g., GPT-4 versus Llama 7B) to use.\nExecution. Query plans are translated into Sycamore code in Python. Execution on large datasets benefits from distributed processing, and using Sycamore's distributed execution mode allows us to scale out workloads with minimal overhead. The query execution code is easy for a technically savvy user to understand and modify.\nTraceability and debugging. The ambiguous nature of some queries can result in Luna misinterpreting the user's intention. It is critical to allow the user to inspect a Luna query' execution and provide feedback to correct itself. With a combination of logging and exposing APIs that allow the user to modify any stage of query execution, users have full control over how their query is answered."}, {"title": "Sample execution", "content": "As a simple example, consider the query \"What percent of environmentally caused incidents were due to wind?\" Figure 5 shows the resulting query plan generated by Luna. This demonstrates the ability to enhance data at query time, as well as the benefits of using an LLM to perform a data-extraction task. Previously, a system would need an ETL job to extract \"incident root cause,\" but with Luna's runtime LLM operations we can extract this information dynamically.\nThe query plan results in the following Python code, using the Sycamore APIs to execute the query.\nout_0 = context.read.opensearch (index_name=\"ntsb\")\nout_1 = out_0.filter(\"caused by environmental factors\")\nout_2 = out_1.count()\nout_3 = out_0.filter(\"caused by wind\")\nout_4 = out_3.count()\nresult = math_operation (expr=\"100 * out_4}/{out_2}\")"}, {"title": "RELATED WORK", "content": "Machine learning has revolutionized many aspects of data management over the last decade. First, there is a long line of work on natural language \u2192 SQL [8, 21]. While the early work focused on building specialized models for this purpose, LLM-based approaches have proven superior in recent years7. Several recent works have focused on generating queries that incorporate LLM calls [12, 14, 23]. Our Luna framework is differentiated by a broader set of LLM-based operations, a focus on hierarchical datasets, and our emphasis on interactive interfaces."}, {"title": "CONCLUSIONS AND FUTURE WORK", "content": "We are building Aryn to unify unstructured and structured analytics under a single abstraction, by leveraging the immense potential of LLMs to process multi-modal datasets. At the same time, given the limitations of current models, we are building Aryn to be a human-in-the-loop system; as the LLMs improve, the need for human interventions will diminish, but it is unlikely to completely vanish. Our experience across a variety of application domains has validated our overall design as well as Aryn's individual components.\nOf course, many challeges still remain. We need more robust data and querying abstractions to support analytics across structured and unstructured repositories. Knowledge Graphs are emerging as a strong contender for this purpose, but it is not yet clear how hallucination-free knowledge graphs can be built in a cost-effective manner with minimum human intervention. We also need to build more scalable systems for processing large multi-modal repositories, for inferring connections between them, and for leveraging existing structured repositories, like data warehouses."}]}