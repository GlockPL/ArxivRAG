{"title": "Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for Weakly-supervised Oriented Object Detection", "authors": ["Yi Yu", "Xue Yang", "Yansheng Li", "Zhenjun Han", "Feipeng Da", "Junchi Yan"], "abstract": "Accurately estimating the orientation of visual objects with compact rotated bounding boxes (RBoxes) has become a prominent demand, which challenges existing object detection paradigms that only use horizontal bounding boxes (HBoxes). To equip the detectors with orientation awareness, supervised regression/classification modules have been introduced at the high cost of rotation annotation. Meanwhile, some existing datasets with oriented objects are already annotated with horizontal boxes or even single points. It becomes attractive yet remains open for effectively utilizing weaker single point and horizontal annotations to train an oriented object detector (OOD). We develop Wholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging various labeling forms (Points, HBoxes, RBoxes, and their combination) in a unified fashion. By only using HBox for training, our Wholly-WOOD achieves performance very close to that of the RBox-trained counterpart on remote sensing and other areas, significantly reducing the tedious efforts on labor-intensive annotation for oriented objects.", "sections": [{"title": "I. INTRODUCTION", "content": "In modern computer vision applications, oriented object detection has emerged as an essential bridge to close the gap between the limited orientation resolution of traditional object detectors (i.e. based on horizontal bounding box) and the increasing demand for fine-grained pose estimation of visual objects. From the expansive vistas of remote sensing [1, 2, 3, 4, 5] to the intricate worlds under a microscope [6, 7, 8], and even within the dynamic environments of autonomous driving [9], robotic grasping [10, 11], medical image [12], scene text [13, 14, 15], retail scenes [16], manufacturing [17], agriculture [18, 19, 20], face detection [21], power grid equipment [22, 23], insect detection [24], and transverse aeolian ridges of Mars [25], its impact resonates across industries. Diverging from traditional detection [26], oriented detection introduces rotated bounding boxes that align with the orientation of objects, thereby capturing a more precise depiction. This level of detail is crucial for various applications, especially in predicting the relationships between objects within a scene graph [27], making oriented detection a burgeoning field of research [28, 29, 30, 31, 32, 33].\nTo teach the detector new concepts of visual objects, a common way is to use manual annotations. In general, objects can be annotated in four different ways: single point (Point), horizontal bounding box (HBox), rotated bounding box (RBox), and pixel-wise label (Mask). Early research typically relies on full supervision, where the manual annotation matches the desired network output format [34, 35, 36, 37]. However, in the context of oriented detection, this approach to acquiring training data is both labor-intensive and error-prone. A fundamental contributing factor to this issue is the time-consuming nature of rotated box annotation and the vast amounts of data, especially in remote sensing [2]. In concrete terms, the cost of each RBox is about 36.5% higher than an HBox and 104.8% higher than a point annotation1. Moreover, many remote sensing images have already been annotated with HBoxes (e.g. DIOR [39] and SARDet-100K [40]). When another format is needed, re-annotation is a possible solution. For example, the aerial image dataset DIOR [39] has been re-annotated to build a rotated box version DIOR-RBox [41], which is repetitive and inefficient.\nSuch a situation raises an interesting question: Is it possible to convert annotations between different formats and make full use of available labeled data? The conversion from Mask \u2192 RBox \u2192 HBox \u2192 Point can be easily achieved (e.g. by finding the circumscribed rectangle), while the inverse process is much more difficult, where we need to grab some additional clues from the image. Learning fine-grained labels from coarse-grained ones is usually termed weak supervision.\nResearch toward weakly-supervised oriented object detectors has made some progress, with several HBox-to-Mask, Point-to-Mask, and HBox-to-RBox methods being proposed (detailed in Sec. II). Particularly, the foundation model SAM"}, {"title": "II. RELATED WORK", "content": "Beyond horizontal detection [49, 26, 50], oriented object detection (OOD) [32] has received extensive attention. Here, approaches related to oriented detection and studies related to HBox/Point supervision are discussed."}, {"title": "III. METHODS", "content": "In this section, we delve into our series of research on weakly-supervised oriented detection. We begin in Sec. III-A by presenting the foundational theory of symmetry-aware learning, demonstrating its ability to learn orientation from symmetry with theoretical guarantees. Next, Sec. III-B introduces H2RBox-v2, an implementation validating our theory and facilitating HBox-to-RBox conversion using symmetry-aware learning. Leveraging the H2RBox-v2 pipeline, Sec. III-C illustrates Point2RBox, which employs synthetic pattern knowledge combination to achieve the Point-to-RBox conversion. Finally, we present Wholly-WOOD in Sec. III-D, an integrated pipeline capable of accommodating diverse labeling formats (Points, HBoxes, RBoxes, and their combination), thereby offering an integral and adaptable solution."}, {"title": "A. Theoretical guarantee of symmetry-aware learning", "content": "Assume there is a neural network $fnn (\\cdot)$ that maps a visual object I to a real number $\\theta$ representing the rotation:\n$\\theta = fnn (I)$ (1)\nwhere the visual object $I \\in R^{2 \\times M}$ is represented as a set of pixel locations; M is the pixel count; $\\theta \\in R$ mod $\\pi$, where $\\theta_1 = \\theta_2$ (mod $\\pi$) implies $\\theta_1 = \\theta_2 + k\\pi$ for some integer k.\nIn symmetry-aware learning, we simply train the network $fnn (\\cdot)$ to follow two properties, namely the flip consistency and the rotate consistency.\nProperty I: Flip consistency. With an input object vertically flipped, $fnn (\\cdot)$ gives an opposite output:\n$-fnn (I) = fnn (\\mathbb{1})$ (mod $\\pi$) (2)\nProperty II: Rotate consistency. With an input rotated by R, the output of $fnn (\\cdot)$ also rotates by R:\n$fnn (\\begin{bmatrix}\ncos R & sin R \\\\\n-sin R & cos R\n\\end{bmatrix}I) + R = fnn (I)$ (mod $\\pi$) (3)\nHere we provide a mathematical explanation for how the network can discern the angle of a reflective symmetric visual object through the rotate and flip consistencies. Let x, y be perpendicular unit vectors in the plane. Suppose there exists a visual object, denoted as $I_{sym}$, which is reflection symmetric with a vector $u = cos\\theta x + sin\\theta y$ representing the line of reflection. Based on the transformation matrix of reflections, the reflection symmetry of $I_{sym}$ can be formulated as:\n$I_{sym} = \\begin{bmatrix}\ncos 2\\theta & sin 2\\theta \\\\\nsin 2\\theta & -cos 2\\theta\n\\end{bmatrix}I_{sym}$ (4)\nBy mapping the both sides of Eq. (4) with the network function $fnn (\\cdot)$, we obtain:\n$fnn (I_{sym}) = fnn (\\begin{bmatrix}\ncos 2\\theta & sin 2\\theta \\\\\nsin 2\\theta & -cos 2\\theta\n\\end{bmatrix}I_{sym})$ (5)\n$= fnn (\\begin{bmatrix}\ncos 2\\theta & sin 2\\theta \\\\\nsin 2\\theta & cos 2\\theta\n\\end{bmatrix}I_{sym})$ (6)\n$=-fnn (I_{sym}) + 2\\theta$ (mod $\\pi$) (7)\nwhere Eq. (6) indicates that a reflection transformation can be decomposed into the multiplication of a rotation and a flip. Substituting Eqs. (2) and (3) into Eq. (6), we derive Eq. (7). Solving Eq. (7) yields:\n$fnn (I_{sym}) = \\theta$ (mod $\\pi/2$) (8)\nwhich suggests that IF: 1) The input object $I_{sym}$ has reflection symmetry about the vector $u = cos \\theta x + sin \\theta y$; AND 2) $fnn (\\cdot)$ subjects to the flip and rotate consistencies; THEN: $fnn (I_{sym})$ precisely outputs the symmetry angle $\\theta$ or an angle differing by $\\pi/2$, which is sufficient for learning rotation in OOD.\nBased on the above conclusion, training the network with flip and rotate consistencies leads to automatic regression of the object's angle in the network's output. Thereupon, we design a training pipeline to employ this approach in Sec. III-B and empirically confirm its effectiveness.\nNotably, although the aforementioned study focuses on a single visual object, an assigner is employed to match objects in different views (detailed in Sec. III-B), enabling the calculation of consistency loss between these paired objects. Our theory can then be applied to each matched object center, extending the method to multiple object detection."}, {"title": "B. H2RBox-v2", "content": "The training pipeline of the proposed H2RBox-v2 is given in Fig. 2, which consists of a self-supervised (SS) branch and a weakly-supervised (WS) branch.\nSelf-supervised (SS) branch. It is designed to enforce the two consistencies by Eqs. (2) and (3) within the neural network. As shown in Fig. 2a, we perform vertical flip and random rotation to generate two transformed views, $I_{flp}$ and $I_{rot}$, of the input image I. The blank border area induced by rotation is filled with reflection padding. Then the three views are fed into three parameter-shared branches of the network, where ResNet50 [89] and FPN [90] are used as the backbone and the neck, respectively. The random rotation is in the range $\\pi/4 \\sim 3\\pi/4$ (according to the ablation in Table IV).\nNext, a label assigner is required to match the objects in different views. We use the default center sampling assigner of FCOS detector to calculate the average angle features on all sample points for each object and eliminate those objects without correspondence (lost during rotation).\nFollowing the assigner, PSC [91] angle coder is adopted to cope with the boundary problem. We empirically demonstrate in Table I that PSC is necessary to achieve a stable convergence of training. The output angles of the original, flipped, and rotated views are denoted as $\\theta$, $\\theta_{flp}$, and $\\theta_{rot}$.\nThen, the losses for the consistencies can be expressed as:\n$\\begin{cases}\nL_{flp} = l_s (\\theta_{flp} + \\theta,0)\\\\\nL_{rot} = l_s (\\theta_{rot} - \\theta,R)\n\\end{cases}$ (9)\nwhere $L_{flp}$ is the loss for flip consistency and $L_{rot}$ for rotate consistency. R is the rotation angle in the rotated view generation. During the calculation of Eq. (9), $l_s (\\cdot)$ named snap loss (see Fig. 2c) is proposed as:\n$l_s (\\theta_{pred}, \\theta_{target}) = \\min_{k \\in Z} (smooth_{L1} (\\theta_{pred}, k\\pi + \\theta_{target}))$ (10)\nwhere the $min (\\cdot)$ operation regresses the prediction toward the closest target to circumvent the periodicity problem (see"}, {"title": "C. Point2RBox", "content": "Based on our HBox-to-RBox pipeline, we further devise the flowchart in Fig. 3 for point-supervised rotated detection.\nKnowledge combination. During manual annotation, annotators are often provided with a one-shot example for each category. For point annotations, the exact size and angle of the labeled object are unknown, but the example allows us to generate similar patterns. Since these patterns are derived from a known example, their bounding boxes are also known (see red RBoxes in Fig. 3), providing the necessary information for box regression. Building upon this concept, the knowledge combination module is devised. First, we sample around each labeled point, and extract its neighbor colors, namely the face color $C_{face}$ and the edge color $C_{edge}$, as follows:\n$\\begin{cases}\nC_{face} = mean (I_0)\\\\\nC_{edge} = sum (dI_1)\n\\end{cases}$ (14)\nwhere $I_0$ and $I_1$ are the neighbor pixels around a labeled point. We simply use a $5 \\times 5$ neighbor area for $I_0$ and $33 \\times 33$ for $I_1$. Here d is the gradient of $I_1$ indicating the edge intensity of each pixel (the sum of d is uniform to one).\nThen, we spread the two extracted colors to a basic pattern. The basic pattern is a gray-scale sample manually cropped from training images and adjusted to gray-scale (one sample for each category), which can be denoted as P, with its value in the range (0, 1). The recolor step can be expressed as:\n$P_{recolor} = P C_{face} + (1 - P) C_{edge}$ (15)\nSuch an \u201cextract-and-spread\u201d design has two advantages: 1) The diversity of the synthetic patterns is significantly enriched. 2) The gap between generated patterns and real ones is narrowed. By this means, the knowledge can be better transferred to estimate the RBoxes of the real objects (see ablation in Sec. IV-B).\nAfterward, the recolored patterns are augmented with the random flip, resize, and rotation, and moved to a random position inside the image border. The probability for random flip and rotation is set to 0.5 and 1, respectively. The random resize can be formulated as:\n$\\begin{cases}\nw = w_0 exp (base + \\sigma_w)\\\\\nh = h_0 exp (base + \\sigma_w + \\sigma_\\tau)\n\\end{cases}$ (16)"}, {"title": "D. Wholly-WOOD", "content": "Based on the principles in H2RBox-v2 and Point2RBox, an integral and comprehensive solution, Wholly-WOOD, is proposed to wholly leverage diversified-quality labels including RBoxes, HBoxes, Points, or their combination.\nThe schematic representation of Wholly-WOOD is presented in Fig. 4. The upper part (blue arrows) is derived from H2RBox-v2, which plays a crucial role in HBox-to-RBox. The lower part is derived from Point2RBox to process point annotations, where a pattern generator (the same as that in Point2RBox) is used to generate synthetic visual patterns. By training the P2R subnet, it combines the knowledge from these patterns to generate RBox suggestions of point-annotated objects. Afterward, these P2R suggestions, together with RBox/HBox annotations, are harnessed by the upper part to train the Rotate FCOS detector.\nTo eliminate the limitations (i.e. the FPN/anchor assignment issue) of Point2RBox, we propose the P2R subnet to replace the YOLOF detector. Below, we introduce the SS and WS branches of Wholly-WOOD, followed by a detailed description of the newly devised P2R subnet.\nSS branch and WS branch. Similar to the H2RBox-v2, Wholly-WOOD also consists of the SS branch and the WS branch. The SS branch is designed to enforce the two consistencies for symmetry-aware learning. We employ a single view randomly chosen from a distribution of transformations: 95% rotation and 5% flipping (based on \u03bb = 0.05 in Sec. III-B). When rotation is applied, the loss $L_{rot}$ is computed to measure the disparity between the outputs of the two views. Similarly, $L_{flp}$ is utilized to assess flip-induced variations. When the network adheres to the two consistencies, it automatically gains the ability to predict the angle of objects (Refer to Sec. III-A for the explanation).\nThe loss for the SS branch can be expressed as:\n$L_{ss} = L_{rot} + \\mu_{flp} L_{flp}$ (23)\nwhere $\\mu_{flp} = 1$ by default as the weight between rotation and flip has been featured by the proportion of view generation. Meanwhile, the WS branch is used to process HBox/RBox annotations. CircumIoU (Fig. 2d) is used for HBoxes circumscribed to the predicted boxes and RotatedIoU for RBoxes. The overall loss for Wholly-WOOD can be expressed as:\n$L_{wholly-wood} = L_{cls} + \\mu_{cn}L_{cn} + \\mu_{box}L_{box} + \\mu_{ss}L_{ss}$ (24)\nwhere $\\mu_{cn}$, $\\mu_{box}$, and $\\mu_{ss}$ are set to one by default.\nP2R subnet. As mentioned in Sec. III-C, objects annotated with points cannot be assigned to different FPN layers or anchors based on their sizes. However, available detectors including FCOS [42] and YOLOF [92] rely on FPN layers or multiple anchors to deal with objects of different sizes. In Point2RBox, we simply use YOLOF with a fixed anchor size, which partly circumvents the issue but also limits the box regression range, leading to insufficient accuracy.\nTo further address this problem, we devise a novel \u201cfusion and scaling\u201d mechanism (see Fig. 5). The P2R subnet is based on FCOS with ResNet50 [89] and FPN [90]. It is anchor-free with only one feature layer, yet it allows the prediction of both large and small objects. Specifically, the multiple output layers of the FPN are automatically aggregated based on a self-activated gating score:\n$G_n = softmax (conv (interp (F_n)))$ (25)\nwhere $F_n$ is the n-th FPN feature layer; $interp (\\cdot)$ upscales $F_n$ to the shape of $F_1$ though nearest interpolation; $G_n$ is the gating score for each layer; $conv (\\cdot)$ is a 3\u00d73 convolution layer with one output channel; $softmax (\\cdot)$ normalizes the sum of $G_1, G_2,..., G_N$ to one at each pixel."}, {"title": "IV. EXPERIMENTS", "content": "Datasets. To evaluate our approach, we assess its performance using five remote sensing datasets: DOTA, HRSC, FAIR1M, SARDet-100K, and STAR. These datasets are originally annotated with RBoxes, from which we derive Points or HBoxes by extracting the center point or minimum circumscribed rectangle respectively. These Points/HBoxes serve as input for Wholly-WOOD, and the resulting outputs are compared against RBox-trained counterparts to evaluate performance disparities (see Tables VIII and IX). Afterward, we apply Wholly-WOOD to datasets annotated only with Points/HBoxes to showcase the practical effectiveness of our method. The experiments span multiple scenarios, including Synthetic Aperture Radar (SAR) images, microscope images, and Printed Circuit Board (PCB) images (see Fig. 8).\n1) DOTA [2]: DOTA-v1.0 contains 2,806 aerial images, 1,411 for training, 937 for validation, and 458 for testing, as annotated using 15 categories with 188,282 instances in total. DOTA-v1.5/2.0 are the extended version of v1.0. We follow the default preprocessing in MMRotate [95]: The high-resolution images are split into 1,024 \u00d7 1,024 patches with an overlap of 200 pixels for training, and the detection results of all patches are merged to evaluate the performance.\n2) HRSC [4]: It contains ship instances both on the sea and inshore, with arbitrary orientations. The training, validation, and testing set includes 436,181, and 444 images, respectively. With preprocessing by MMRotate, images are scaled to 800 \u00d7 800 for training/testing.\n3) FAIR1M [96]: It contains more than one million instances for fine-grained object recognition in high-resolution remote sensing imagery. The dataset is annotated with 37 fine-grained categories. We split the images into 1,024 \u00d7 1,024 patches with an overlap of 200 pixels and a scale rate of 1.5 and merge the results for testing on the FAIR1M-1.0 server.\n4) SARDet-100K [40]: It is a large-scale Synthetic Aperture Radar (SAR) object detection dataset, containing six categories and more than 100 thousand instances. The dataset provides HBox annotations only, and we use it to verify if our model can build an RBox version from it."}, {"title": "E. Inference procedure", "content": "In all our devised pipelines (i.e. H2RBox, H2RBox-v2, Point2RBox, and Wholly-WOOD), the self-supervision is solely utilized during training. During inference, there is no requirement for self-supervision or view generation, and only the forward propagation of the detector is involved. As a result, these methods have similar inference speeds compared to the backbone detector on which they are based."}, {"title": "F. Further discussion", "content": "While horizontal ground-truths are well-established in object detection, retrieving rotated ones is laborious, requiring highly trained experts and often resulting in imprecision. This emphasizes the need for weakly-supervised deep learning approaches that do not rely on rotated annotations but instead leverage annotations that are easier and faster to obtain.\nHow much annotation task can be reduced by utilizing HBox/Point supervision? An instinct concept is that compared to RBoxes, HBox annotations reduce the workload from three clicks to two, whereas Point annotations further streamline this process to just one click. However, acquiring a horizontal box annotation is straightforward, particularly with the assistance of a cross-line on the screen for accurate alignment. Despite appearing to require just one more parameter, the process of obtaining a rotated box can be more time-consuming than expected due to its five degrees of freedom.\nTypically, there are two ways to annotate rotated boxes: 1) Draw a polygon shape with four clicks around the object of interest and then convert it into a rotated box. 2) Draw a horizontal bounding box around the object, then rotate it to align with the object's orientation, and finally adjust the width and height again. To quantify the time required for different annotation formats, we conduct a user study wherein experienced annotators are tasked with annotating an image from the DOTA-v1.0 dataset [2] using the second way. The results indicate that, on average, it takes 1.07 seconds for Point annotation, 2.23 seconds for HBox annotation, and 3.69 seconds for RBox annotation for a single instance.\nIt can be inferred from these results that utilizing Wholly-WOOD for HBox supervision can lead to a reduction in annotation time by 40% while maintaining comparable detection accuracy. Alternatively, employing the Point-to-RBox setting can achieve a time reduction of 71% if a slight accuracy trade-off is acceptable (the evaluated AP50 loss is 9.81% and 1.69% on the DOTA-v1.0 and HRSC datasets)."}, {"title": "V. CONCLUSION", "content": "In this work, we have introduced Wholly-WOOD, a unified weakly-supervised detector aimed at wholly leveraging diversified-quality labels for oriented object detection, demonstrating its effectiveness in remote sensing and beyond.\nThrough extensive experiments, we make the following observations: 1) Our approach enables the unification of data with various annotation formats, offering a more convenient and versatile solution with accuracy surpassing other state-of-the-art alternatives. 2) The use of Wholly-WOOD for HBox-to-RBox learning leads to a reduction in annotation time by 40% while maintaining comparable detection accuracy. 3) Employing Point-to-RBox achieves a time reduction of 71% with a marginal accuracy loss of 9.81% and 1.69% on DOTA-v1.0 and HRSC, respectively. 4) Using diversified-quality labels could be a good alternative to balance the annotation and accuracy. When RBox:HBox:Point = 1:1:1, the accuracy on DOTA-v1.0 reaches 73.08%, quite close to the FCOS detector fully supervised by RBoxes.\nWholly-WOOD illustrates the effectiveness of Point/HBox weak supervision, delivering detection performance similar to its RBox-supervised counterpart, making it an unprecedented alternative for processing annotations of various formats in oriented object detection tasks. We believe this research can help alleviate the burden of costly manual annotation, freeing individuals from labor-intensive labeling tasks."}]}