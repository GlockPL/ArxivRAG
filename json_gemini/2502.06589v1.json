{"title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models Through Continual Pre-Training", "authors": ["Yuchen Zhuang", "Jingfeng Yang", "Haoming Jiang", "Xin Liu", "Kewei Cheng", "Sanket Lokegaonkar", "Yifan Gao", "Qing Ping", "Tianyi Liu", "Binxuan Huang", "Zheng Li", "Zhengyang Wang", "Pei Chen", "Ruijie Wang", "Rongzhi Zhang", "Nasser Zalmout", "Priyanka Nigam", "Bing Yin", "Chao Zhang"], "abstract": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMS to new tasks or environments.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are rapidly evolving beyond traditional natural language processing tasks (Ouyang et al., 2022; Brown et al., 2020; Achiam et al., 2023), demonstrating increasing intelligence and autonomy by exhibiting capabilities in perception, reasoning, planning, and action within complex real-world environments (Yao et al., 2023; Lu et al., 2024; Sun et al., 2024a). Through well-crafted prompting or extensive post-training, LLM-based autonomous agents augmented with external tools (e.g., APIs) have demonstrated exceptional instruction-following capabilities in a wide range of tasks (Schick et al., 2024; Qin et al., 2024; Srinivasan et al., 2023; Zeng et al., 2023).\nDespite their remarkable task-specific performance, existing LLM agents often face the following challenges: (1) Overemphasis on instruction fine-tuning while ignoring the pre-training stage. LLMs typically undergo a two-stage training process: pre-training to learn general knowledge and instruction fine-tuning to align to specific tasks and user preferences. The Superficial Alignment Hypothesis (Zhou et al., 2024; Gudibande et al., 2024; Lin et al., 2024b) posits that LLMs acquire most of their knowledge during pre-training, which is more important than instruction fine-tuning in terms of obtaining generalizable fundamental capabilities. However, the majority of existing agent frameworks (Figure 1) focus on instruction fine-tuning to align with specific patterns or formats, rather than fundamentally enhancing model knowledge or capabilities (e.g., API function calling). (2) Scarcity of agent-oriented pre-training data."}, {"title": null, "content": "Agent instructions and trajectories significantly differ from general instructions and responses (Zhang et al., 2024b). Thus, function-calling knowledge is difficult to derive directly from web archives, the primary pre-training data source. This notable lack of agent-specific pre-training corpora constrains LLMs from effectively acquiring new agentic knowledge and capabilities (Table 1). (3) Limited generalization across multiple tasks. LLM agents often struggle to generalize to new scenarios (e.g., from single to multiple tools) that differ from their original fine-tuning data distributions (Qin et al., 2024).\nTo address these challenges, we introduce Hephaestus-Forge, a large-scale pre-training corpus specifically designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adaptation to environmental feedback. Specifically, we focus on two primary objectives: (a) improving comprehension of individual function calls, and (b) strengthening intrinsic reasoning capabilities for solving problems requiring multiple function calls. To enhance (a) comprehension of API functions and alignment with their formats, we collect a large-scale dataset of tool documentation tailored for LLM pre-training on API function calls. Given the expanding range of tasks with growing complexity, we incorporate a vast number of function calling trajectories to improve (b) intrinsic reasoning abilities in sequencing API function calls. We then integrate this meticulously curated tool documentation and function-calling data with code (to bolster reasoning capabilities) and text data (to maintain robust text generation capabilities), creating a multi-source, large-scale, and high-quality training corpus, Hephaestus-Forge.\nBuilding upon Hephaestus-Forge, we introduce a continual pre-trained open-source LLM, Hephaestus, an LLM with strong agentic and autonomous capabilities across domains, bringing open-source models closer to the capabilities of commercial LLMs. Our empirical evaluations demonstrate that Hephaestus-8B outperforms open-source LLMs at small to medium scales (e.g., 9.6% over LLAMA-3-8B and 17.6% over Mixtral-8x22B) and performs comparably to API-based large commercial LLMs (e.g., 18.9% over Claude-3-Haiku and 4.1% over GPT-3.5-turbo) across three agent benchmarks. Our large-scale ablation studies further demonstrate the effectiveness of retrieved agent data in scaling up and diversifying the coverage of scenarios in pre-training. Our contributions can be summarized as follows:\n\u2022 We curate Hephaestus-Forge, a large-scale pre-training corpus designed to enhance understanding of API function calls and guide actionable trajectories for LLM agents. Remarkably, through exhaustive scaling law experiments, we discover a pioneering pre-training recipe with an empirically optimal data mix ratio.\n\u2022 We propose Hephaestus, a foundation model that exhibits enhanced fundamental agentic capabilities, including API function calling, intrinsic reasoning and planning, and adaptation to environmental feedback, achieved through continual pre-training on Hephaestus-Forge.\n\u2022 We extensively compare Hephaestus with strong baselines across three agent benchmarks, verifying its enhanced fundamental agentic capabilities and superior generalization derived from Hephaestus-Forge."}, {"title": "2 Related Work", "content": "Prompting-based LLM Agents. Due to the lack of agent-specific pre-training corpus, existing LLM agents rely on either prompt engineering (Hsieh et al., 2023; Lu et al., 2024; Yao et al., 2023; Wang et al., 2023) or instruction fine-tuning (Chen et al., 2023; Zeng et al., 2023) to understand human instructions, decompose high-level tasks, generate grounded plans, and execute multi-step actions. However, prompting-based methods mainly depend on the capabilities of backbone LLMs (usually commercial LLMs), failing to introduce new knowledge and struggling to generalize to unseen tasks (Sun et al., 2024a; Zhuang et al., 2024a).\nInstruction Finetuning-based LLM Agents. Considering the extensive diversity of APIs and the complexity of multi-tool instructions, tool learning inherently presents greater challenges than natural language tasks, such as text generation (Qin et al., 2024). Post-training techniques focus more on instruction following and aligning output with specific formats (Patil et al., 2023; Hao et al., 2024; Qin et al., 2024; Schick et al., 2024), rather than fundamentally improving model knowledge or capabilities. Moreover, heavy fine-tuning can hinder generalization or even degrade performance in non-agent use cases, potentially suppressing the original base model capabilities (Ghosh et al., 2024).\nPretraining-based LLM Agents. While pre-training serves as an essential alternative, prior"}, {"title": "3 Preliminaries", "content": "Problem Formulation. We conceptualize leveraging LLMs as autonomous agents for problem-solving as a planning process. Initially, we augment the LLM agent with access to a pool of candidate API functions, denoted as $A = \\{API_0, API_1,\\ldots, API_m \\}$, along with a natural language task description $g \\in G$ from the task space $G$. The objective of the LLM agent is to translate the task description $g$ into an ordered sequence of $T_g$ API function calls $p_g = \\{a_0,\\ldots, a_{\\tau_g} \\}$. Specifically, considering the task description $g$ as the initial state $s_0$, we then sample the plan $p_g$ by prompting the LLM agent with the API definitions $I$ and demonstration samples $D$ as follows: $p_g \\sim \\rho(a_0, a_1,\\ldots, a_{\\tau_g}|s_0; I, D) : G \\times I \\times D \\rightarrow (A^{T_g})$, where $\\Delta(\\cdot)$ denotes a probability simplex function. The final output is derived after executing the entire plan $y \\sim \\pi(y|s_0, a_1, a_2,\\ldots,a_{\\tau_g})$, where $\\pi(\\cdot)$ denotes a plan executor.\nDuring this procedure, we focus on three fundamental capabilities of LLM agents:\nAccurate Function Calling. It involves accurately understanding the API definitions and demonstration samples to generate correct API function calls with corresponding parameters in a given scenario. Specifically, the model should accurately understand the API definitions $I$ and demonstration samples $D$, as well as generate an accurate API function call in the given scenario $p(a_t|s_0, a_1, \\cdots, a_{t-1}, I, D)$, where $a_t$ is the ground-truth API function call with corresponding parameters at t-th step.\nIntrinsic Reasoning and Planning. It refers to the intrinsic reasoning and planning ability to devise a sequence of multiple tool functions as a solution when addressing complex (multi-step) real-world problems. In such cases, LLMs are often required to generate a sequence of API function calls, $p(a_1, a_2,\\cdots, a_{T_g}|s_0; I, D)$, where $\\{a_1,a_2,\\ldots,a_{T_g}\\}$ constitutes the ground-truth solution plan of length $T_g$. This process relies on"}, {"title": null, "content": "intrinsic reasoning embedded within the model parameters; enhanced reasoning capabilities lead to a solution plan with a higher chance of success.\nAdaptation with Environment Feedback. It focuses on adapting the current plan or action based on environmental feedback when the environments support interaction with the LLM agent. When such feedback is available, it is crucial for the agent to adjust its actions accordingly: $p(a_t|s_0, a_1, o_1, a_2, \\cdots, o_{t-1}; I, D)$, where $o_k$ represents the feedback from the environment after the k-th action. Incorporating environmental feedback allows the agent to take reflections to refine its plan and improve task performance iteratively."}, {"title": "4 Hephaestus-Forge", "content": "To scale and diversify the pre-training corpus for LLM agents, we introduce a three-stage construction process for Hephaestus-Forge (see Figure 2): (1) Seed Data Collection (\u00a7 4.1), where we gather initial high-quality samples; (2) Web Data Retrieval (\u00a7 4.2), which expands the seed data by retrieving relevant data from the web; and (3) Data Quality Control (\u00a7 4.3), where we ensure the integrity and relevance of the collected data."}, {"title": "4.1 Seed Data Collection", "content": "For seed data collection, we first traverse available public resources to gather high-quality API documentation and action trajectories, including: (1) Public APIs. High-quality API documentation is collected from over 1, 400 public APIs and official websites, including detailed function definitions and parameter descriptions. (2) Public Repositories. To improve intrinsic reasoning, we integrate action trajectories from over 60 public repositories across diverse domains, such as programming code and web interactions. (3) Code-to-Text Synthesis. Given the limited coverage of curated data, we use LLMs to synthesize additional API documentation from StarCoder-API, generating examples based on code snippets. (4) Simulated Agent Data. We gather simulated action sequences with observational data to facilitate adaptation to environmental feedback. Importantly, we offer step-by-step details of the seed data collection process in appendix D.1 for reproducibility."}, {"title": "4.2 Web Data Retrieval", "content": "Given the limited availability of agent-oriented data, we use the high-quality data described in \u00a7 4.1 as seed data for further expansion. To enhance agentic capabilities, we retrieve a diverse set of examples from web crawls, focusing on content relevant to API documentation and action trajectories. Our retrieval process involves the following steps: (1) Web Data Corpus Creation. Similar to CommonCrawl (Raffel et al., 2020) and FineWeb (Penedo et al., 2024), we first compile a large-scale web data corpus. (2) Semantic Matching. We utilize COCO-DR (Yu et al., 2022) to encode semantic representations of documents in the seed data and the large-scale web corpus. We then retrieve the top-K similar documents by calculating the cosine similarity between the corresponding embeddings. It allows us to identify and retrieve documents from the web corpus that are semantically similar to our seed data, effectively enriching our dataset with relevant and diverse information. (3) Quality Control. To ensure the quality of the retrieved corpus, we perform data pruning to remove semantically redundant content and maintain the diversity of knowledge, preventing overrepresentation of certain topics and ensuring generalization and robustness across domains."}, {"title": "4.3 Data Quality", "content": "After retrieving semantically relevant data from the web corpus, we obtain a collection of noisy agent data. To ensure the integrity and relevance of our dataset, it is essential to consistently monitor data quality and filter out content that resembles general text rather than agent-specific data. First, we employ Claude-3-Sonnet (Anthropic, 2024) as the data annotator to annotate a total of 71, 473 samples from the retrieved data, identifying 37, 714 as agent-relevant and 33, 767 as general text paragraphs. Using the annotated samples, we train a fastText (Joulin, 2016) model to effectively recall additional agent-relevant web data. This filtering process then reduces the data volume from approximately 200B to 80B tokens, ensuring that the preserved data maintains high relevance and quality. See details in appendix D.2."}, {"title": "5 Scaling Laws for Data Composition", "content": "When designing LLMs, the scaling law (Kaplan et al., 2020; Hoffmann et al., 2022) is an important predictive tool that can estimate the performance (e.g., benchmark loss) of a large-sized target model using a scaling curve fitted over much smaller models (referred to as sampling models). We develop scaling laws to determine the optimal data proportion among agent data, text data, and code data. With the total budget of the data volume fixed, our scaling law experiments show that the effect of agent data ratio x on the loss L of a pre-trained model follows power laws:\n$L = c + kx^\\alpha$,\nwhere c, k, and $\\alpha$ are parameters to be fitted. By fitting these parameters using a collection of small models, training data, or computational resources, scaling laws can extrapolate to precisely predict the test loss of larger cases over orders of magnitude.\nScaling Law Experiments. Concretely, we construct our scaling laws by pre-training models ranging in 45M to 0.65B parameters. To simulate the continual pre-training setting, we amplify the target data volume used for training each small model to 50x model parameters. Consequently, the total compute budgets for the scaling law experiments span from 7 \u00d7 10\u00b9\u2077 to 2 \u00d7 10\u00b2\u2070 FLOPs. Regarding data proportions, we begin with the seed agent data and progressively incorporate the retrieved web corpus to increase the agent data ratio. Concurrently, as the agent data ratio increases, we proportionally decrease the volumes of general text and code data to maintain the fixed total data volume. Following Dubey et al. (2024), we leverage the benchmark loss of Nexus (Srinivasan et al., 2023), API-Bank (Li et al., 2023b), API-Bench (Patil et al., 2023) to monitor the agent capabilities, and MMLU (Hendrycks et al., 2020) to monitor the general capabilities of LLMs.\nOptimal Data Mixing Ratio. Figure 3 illustrates that the optimal mixture of agent data within the entire pre-training corpus is approximately 36%, indicating that the proportion of agent data, text data, and code data should be roughly 1 : 1 : 1. This balanced distribution promotes both specialized agent capabilities and general language understanding, ensuring that the model remains versatile and robust across diverse tasks and domains.\nRemark. The established scaling laws provide critical insights into the data composition for pre-training LLM agents. By identifying the optimal ratio of agent data, we ensure that the model effectively balances specialized agentic capabilities with general language proficiency."}, {"title": "6 Hephaestus", "content": "In this section, we propose Hephaestus, a foundation model with enhanced fundamental capabilities of LLM agents. Hephaestus undergoes a two-stage continual pre-training process, followed by instruction fine-tuning (see Figure 4): (1) Stage I, continual pre-training stage on the entire Hephaestus-Forge corpus to inject general agent knowledge (\u00a7 6.1); (2) Stage II, continual pre-training stage on the high-quality seed set of Hephaestus-Forge to further enhance specific capabilities (\u00a7 6.1); and (3) Stage III, instruction"}, {"title": "6.1 Stage I & II: Continual Pre-Training", "content": "Following Caccia et al. (2022); Lange et al. (2023), we revisit the concept of stability gap, which describes the phenomenon where the performance on old tasks initially drops and then recovers when learning a new task. Specifically, in the continual pre-training of LLMs, if the data distribution shifts too significantly between the initial pre-training and the continual pre-training stages, the model's capabilities can deteriorate markedly until it assimilates knowledge from the new data distribution (Guo et al., 2024a). To this end, we propose a two-stage continual pre-training framework:\nStage I: Injecting General Agent Knowledge. Stage I infuses general agent knowledge, accompanied by commonsense knowledge and code snippets. We pre-train Hephaestus on the entire Hephaestus-Forge, whose data distribution is carefully balanced between general corpus and agent-specific data, facilitating a smooth and gradual integration of agent knowledge.\nStage II: Enhancing Agent-Specific Capabilities. Stage II leverages high-quality agent data to further enhance the specific capabilities of an agent LLM, including user interaction, function calling, planning, plan refinement, and coding capabilities. We continually pre-train the model obtained from Stage I on the high-quality seed data in \u00a7 4.1 to further align the behavior with agent-specific requirements, ensuring that the specialized functionalities are robustly learned and integrated.\nPre-Training Objectives. For both stages, we employ language modeling as the primary pre-training task. The objective is to auto-regressively predict the next token, defined as follows:\n$L_{PT} = - \\mathbb{E}_{x \\in D_{PT}} \\sum_{i=1}^n p(x_i|x_{<i}),$"}, {"title": "6.2 Stage III: Instruction Fine-Tuning", "content": "To further improve its instruction-following capabilities to align with complex agent environments, Hephaestus undergoes instruction fine-tuning on a blend of high-quality instruction-completion datasets, including ShareGPT (Chiang et al., 2023), ToolACE (Liu et al., 2024c), and AgentFlan (Chen et al., 2024b). The Stage III employs a negative log-likelihood loss function, defined as:\n$L_{IFT} = - \\mathbb{E}_{(x,y) \\in D_{IFT}} \\sum_{i=1}^n p(y_i|y_{<i}, x),$"}, {"title": "7 Experiments", "content": "7.1 Experiment Setup\nTasks and Datasets. We mainly evaluate our Hephaestus on the following benchmarks: (1) AgentBench (Liu et al., 2024d) for intrinsic reasoning and adaptation to environment feedback; (2) Berkeley Function Calling Leaderboard (BFCL)-v3 and (3) BFCL-v2 (Patil et al., 2023) for accurate function calling. To test generalizability instead of memorization, we intentionally exclude all evaluation benchmarks from pre-training corpora. Task and dataset details are available in appendix A.3."}, {"title": "7.2 Main Experiments: Hephaestus-8B-Base", "content": "Following Shao et al. (2024); Dubey et al. (2024), we evaluate our two-stage pre-trained Hephaestus-8B-Base on three agent-specific benchmarks (API-Bank, API-Bench, NexusRaven) and one general benchmark (MMLU). We observe that incorporating more agent data during pre-training consistently reduces benchmark loss on agent tasks in Figure 5 (b). Additionally, Figure 5 (c) demonstrates that Hephaestus-8B-Base achieves significantly lower benchmark loss compared to the LLaMA-3-8B series of base models. Furthermore, Table 2 reports the benchmark scores, where Hephaestus-8B-Base leads in performance across all benchmarks among the open-source base models. Our findings indicate that both pre-training stages (I & II) enhance Hephaestus's fundamental capabilities across a wide range of agent tasks without compromising general capabilities."}, {"title": "7.3 Main Experiments: Hephaestus-8B-IFT", "content": "Table 2 presents the main experimental results of instruction fine-tuned Hephaestus and baselines. Hephaestus consistently outperforms small to medium size open-source LLMs. Moreover, Hephaestus-8B-IFT remains competitive compared to baseline models with significantly more parameters or commercial LLMs.\nEnhanced Capabilities Through Pre-training. We conduct a direct comparison between Hephaestus and LLaMA-3-8B-Base (Dubey et al., 2024), both instruction-tuned using the same instruction fine-tuning data. Hephaestus-8B-IFT outperforms LLaMA-3-8B-IFT across all three benchmarks, indicating that the observed improvements can be attributed to the pre-training stage. Moreover, incorporating more domain-specific knowledge during the pre-training stage leads to better performance, without requiring additional instruction fine-tuning data.\nExcelling in Complex Multi-turn Tasks. BFCL-v3, the latest benchmark, emphasizes multi-turn tool function-calling tasks requiring intrinsic reasoning capabilities and function-calling proficiency. Due to its recent introduction, the limited availability of task-specific data for instruction tuning has led to suboptimal performance, particularly in multi-turn function-calling accuracy, as observed with models like Groq-8B-Tool-Use (Groq, 2024). In contrast, Hephaestus exhibits significantly better performance on BFCL-v3, suggesting that its improvements in core agentic capabilities and generalization stem from pre-training on our large-scale, diverse agent-oriented corpus."}, {"title": "7.4 Ablation Studies", "content": "Table 3 presents the ablation results of Hephaestus on AgentBench and BFCL-v2.\nEffect of Pre-Training Stages. Removing the second pre-training stage results in a slight performance decline for both base and instruction-tuned models across all tasks. Although the Stage-I pre-training data, comprising a large volume of general and retrieved agent data from the web, brings the Hephaestus-Forge closer to the general data distribution, it still differs from the data used in down-"}, {"title": null, "content": "stream applications and evaluations. The Stage-II pre-training is essential for effectively bridging the gap between the pre-training corpus and the instruction fine-tuning data, thereby enhancing overall model performance.\nEffect of Retrieved Data. Degrading the retrieved data to unfiltered, low-quality data or removing it entirely negatively impacts overall performance. For tasks with numerous hand-crafted instructions and simulated trajectories available on the open web (e.g., HH and WS), the seed data of"}, {"title": null, "content": "Hephaestus-Forge can lead to model overfitting on specific patterns. When the large volume of retrieval data is removed, the seed data predominates, leading to improved performance on these specific tasks but reduced performance on others."}, {"title": "7.5 Cross-Task Generalization", "content": "Table 4 compares Hephaestus with several instruction fine-tuned agent frameworks across three agent benchmarks for cross-task generalization. While models fine-tuned on task-specific data excel in corresponding tasks (Groq, 2024; Zeng et al., 2023; Liu et al., 2024c), they struggle to generalize across different agent benchmarks. In contrast, Hephaestus performs consistently well across all tasks, suggesting that the large and diverse pre-"}, {"title": null, "content": "training corpora, Hephaestus-Forge, effectively enhance function calling and agentic reasoning, leading to better generalization. Furthermore, the compared methods are based on continued instruction fine-tuning of LLaMA-3-8B-Instruct, which inherently possesses strong instruction-following and understanding capabilities due to its meticulously curated post-training data. Unlike models relying solely on instruction fine-tuning, the pre-training of Hephaestus effectively improves its fundamental capabilities, thereby offering a more robust foundation for diverse agentic applications."}, {"title": "7.6 Preservation of General Capabilities", "content": "To evaluate the preservation of general capabilities, we further conduct comprehensive experiments across seven additional benchmarks (Table 5) besides MMLU, spanning mathematics (Cobbe et al., 2021), software development (Chen et al., 2021; Liu et al., 2024b), logical reasoning (Suzgun et al., 2022), and broad language model abilities (Zhou et al., 2023; Zellers et al., 2019; Hendrycks et al., 2020). Our results demonstrate that Hephaestus maintains comparable performance to the base model across these diverse domains while significantly enhancing agent-specific capabilities."}, {"title": "8 Conclusion", "content": "In summary, Hephaestus-Forge and Hephaestus collectively advance open-source LLM-based autonomous agents by addressing critical gaps in pre-training corpora. Through exhaustive scaling law experiments, we identify an empirically optimal data mix ratio of approximately 1:1:1 for agent, code, and text data, maximizing the fundamental and generalization capabilities of LLM agents. Empirical evaluations underscore the efficacy and validity of Hephaestus-Forge in fostering enhanced fundamental agentic capabilities and superior generalization in LLM-based autonomous agents."}, {"title": "Limitations", "content": "Data Composition. While knowledge of the composition of pre-training or instruction fine-tuning data would further enhance the effectiveness of Hephaestus, most prominent open-source LLMs (e.g., LLAMA-3-8B-Instruct) do not disclose detailed data information. Nevertheless, our continual pre-training experiments with LLaMA-3-8B demonstrate that significant improvements are achievable even without this knowledge.\nModel Scalability. Computational constraints currently restrict our ability to extend these experiments to larger models. In future work, we aim to validate our findings and methodologies on more expansive LLM architectures, pending access to increased computational resources.\nEthical Statement\nData Contamination. A potential concern in our evaluations is test set contamination, which occurs when some task-specific examples overlap with data used during continual pre-training (Oren et al., 2024). To mitigate this issue, we follow Wang et al. (2024b) and conduct a string-matching analysis, which indicates no overlap between our training data and the datasets of the target tasks. Moreover, we intentionally exclude all evaluation benchmark data from both our pre-training and fine-tuning datasets to ensure a fair comparison.\nReproducibility. To promote transparency, reproducibility, and generalizability in our research, we include all details of the dataset construction (e.g., data collection, processing, retrieving, filtering, scaling law, etc.) of Hephaestus-Forge in \u00a7 4 and the training procedures for Hephaestus in \u00a7 6. Experimental setups and results are presented in \u00a7 7. Additionally, we detail the pre-training, instruction fine-tuning, and testing tasks and datasets in appendices A.1 to A.3, respectively."}]}