{"title": "DiscQuant: A Quantization Method for Neural Networks Inspired by Discrepancy Theory", "authors": ["Jerry Chee", "Arturs Backurs", "Rainie Heck", "Li Zhang", "Janardhan Kulkarni", "Thomas Rothvoss", "and Sivakanth Gopi"], "abstract": "Quantizing the weights of a neural network has two steps: (1) Finding a good low bit-complexity representation for weights (which we call the quantization grid) and (2) Rounding the original weights to values in the quantization grid. In this paper, we study the problem of rounding optimally given any quantization grid. The simplest and most commonly used way to round is Round-to-Nearest (RTN). By rounding in a data-dependent way instead, one can improve the quality of the quantized model significantly.\nWe study the rounding problem from the lens of discrepancy theory, which studies how well we can round a continuous solution to a discrete solution without affecting solution quality too much. We prove that given $m = \\text{poly}(1/\\varepsilon)$ samples from the data distribution, we can round all but $O(m)$ model weights such that the expected approximation error of the quantized model on the true data distribution is $< \\varepsilon$ as long as the space of gradients of the original model is approximately low rank (which we empirically validate).\nOur proof, which is algorithmic, inspired a simple and practical rounding algorithm called DiscQuant. In our experiments, we demonstrate that DiscQuant significantly improves over the prior state-of-the-art rounding method called GPTQ and the baseline RTN over a range of benchmarks on Phi3mini-3.8B and Llama3.1-8B. For example, rounding Phi3mini-3.8B to a fixed quantization grid with 3.25 bits per parameter using DiscQuant gets 64% accuracy on the GSM8k dataset, whereas GPTQ achieves 54% and RTN achieves 31% (the original model achieves 84%). We make our code available at https://github.com/jerry-chee/DiscQuant.", "sections": [{"title": "Introduction", "content": "Modern deep learning models continue to grow in size, incurring greater challenges to train and serve these models. Post training compression methods have emerged which aim to make model inference faster and cheaper. Compressing after pretraining is desirable among practitioners who either cannot afford to train models themselves, or do not want to change the expensive training process too much. In this paper, we study post training quantization (PTQ) of the model weights. Quantization reduces the memory requirements of the model, and speeds up inference for LLMs under memory-bound settings such as the generation phase (as opposed to prefilling phase which is compute-bound) [Kwon et al., 2023].\nThe quantization problem can be divided into two overall steps: (1) Construct a good low bit-complexity representation for the weights (we colloquially call this the quantization grid), and (2) Round the original weights to values in the quantization grid. Within step (1), we also consider those methods which apply a transformation on the weights to better match the encoding format. There has been much recent work on weights-only PTQ for LLMs. To date, the vast majority of such research has been focused on step (1):"}, {"title": null, "content": "min_{ \\hat{w} } \\lambda A(c, \\hat{w}) + \\mathbb{E}_{z\\sim D_{data}} \\mathbb{E}_i [D_{KL} (P_w(\\cdot|Z_{<i}) || P_{\\hat{w}}(\\cdot|Z_{<i}))] \\\\ s.t. \\hat{w} \\in \\mathcal{H}."}, {"title": null, "content": "Here $p_w(z_i|z_{<i})$ is the next token distribution given prefix $z_{<i}$. An astute reader may notice that the first order approximation of the KL divergence in (1) is exactly zero, and how our discussion above applies. In Section 4 where we describe in detail our exact optimization objective, we also show that the second order term of KL divergence can be written as\n$\\mathbb{E}_{z\\sim D_{data}}\\mathbb{E}_i\\mathbb{E}_{t\\sim p_w(\\cdot|z_{<i})} [(\\nabla_w \\log P_w (t|z_{<i}), \\hat{w} - w)^2].$"}, {"title": null, "content": "So minimizing the KL divergence is a succinct way to impose constraints of the form $(\\nabla_w \\log p_w (t|z_{<i}), \\hat{w} - w) \\approx 0$ or equivalently $\\log p_w(t|z_{<i}) \\approx \\log p_{\\hat{w}}(t|z_{<i})$ where $t \\sim P_w(\\cdot|z_{<i})$ and $z \\sim D_{data}$. Therefore our framework still applies.\nAfter every step of gradient descent, we project the weights back to the hypercube $\\mathcal{H}$. This ensures that the trajectory of DiscQuant remains within the convex polytope $K$ and eventually converges to a vertex of $K$ with almost all the coordinates rounded. Instead of picking a random direction $c$ to find a random vertex of $K$, we use a special $c^*$ which let's us find the vertex closest to the original weights $w$ (see Section 4). We use RTN to round the few unrounded parameters left at the end of the optimization.\nWe perform extensive experiments which show the strength of our method: on models Phi-3-mini-4k-instruct and Meta-Llama-3.1-8B-Instruct, across a variety of evaluation tasks, and across the block scaling and incoherence processing quantization formats. DiscQuant is agnostic towards the quantization grid, and can therefore be composed with other quantization methods. Block scaling sets a bits parameter which determines the number of grid points, and a unique scaling parameter per groupsize weights [Frantar et al., 2023]. Incoherence processing applies a random orthogonal transformation, which reduces the weight ranges and can make quantization easier [Chee et al., 2023, Tseng et al., 2024a]. A subset of results can be found in Figure 2. Across tasks, models, and quantization levels, our method DiscQuant achieves superior compression over baselines GPTQ and RTN."}, {"title": null, "content": "We summarize our main contributions:\nTheoretical developments: We prove that it is possible to achieve generalization error $< \\varepsilon$ on the true data distribution by rounding all but $\\text{poly}(\\log n/\\varepsilon)$ weights, so long as the gradients of the original model are approximately low rank.\nPractical algorithm: We develop a simple and practical algorithm DiscQuant guided by our theoretical analysis. We perform extensive experiments on Phi-3-mini-4k-instruct and Meta-Llama-3.1-8B-Instruct, over block scaling and incoherence processing quantization formats, and a variety of evaluation tasks. Our method DiscQuant achieves superior or comparable quantization to the baselines GPTQ and RTN as can be seen from Figure 2."}, {"title": "Related Work", "content": "In this paper we focus on weights-only PTQ. Quantization can also be applied to the activations or KV-cache [Ashkboos et al., 2024, Liu et al., 2024a,b]. Other compression method such as pruning"}, {"title": "Quantization Grids", "content": "One of the more common quantization formats is called block scaling, or group-wise quantization [Frantar et al., 2023]. In addition to the bits parameter determining the number of representable points, each groupsize parameters share a unique scaling parameter. Another successful encoding is to identify a small set of important weights and keep them in high precision [Dettmers et al., 2022, 2024, Kim et al., 2024]. Shao et al. [2024] learns quantization parameters. Other works apply transformations to make quantization easier, either relatively simple invariant scalings [Xiao et al., 2023, Lin et al., 2024], or more complicated random orthogonal transformations [Chee et al., 2023, Liu et al., 2024a]. Beyond block scaling, there has been work quantizing multiple parameters together using vector quantization [Tseng et al., 2024a, Egiazarian et al., 2024, van Baalen et al., 2024] or trellis quantization [Tseng et al., 2024b]."}, {"title": "Rounding", "content": "To the best of our knowledge, GPTQ [Frantar et al., 2023] is the main rounding method for LLMs. It is based on the Optimal Brain Surgeon [Hassibi et al., 1993], which was adapted for pruning and quantization in Frantar et al. [2022] and then refined for quantization in GPTQ. GPTQ works by minimizing a layer-wise objective $||WX - \\hat{W}X||_F^2$, where $W$ is the weight matrix of a linear layer and $X$ is the matrix of input activations to that layer (stacked as columns). Two other LLM rounding methods both use coordinate descent: Nair and Suggala [2024] only has results on the closed source PaLM-2 models with no released code, and Behdin et al. [2023] has results on the OPT, BLOOM, and Falcon model families.\nThere was more work on rounding methods several years ago, before the LLM boom. These papers were typically on smaller vision models. The line of work was started by AdaRound [Nagel et al., 2020] and continuing to AdaQuant [Hubara et al., 2021] and BRECQ [Li et al., 2021] employ a similar approach to ours, optimizing essentially interpolation variables between the closest up($w^{up}$) and down($w^{down}$) quantization grid points, while adding a concave regularization term to encourage rounding and using a rectified sigmoid to interpolate between $w^{up}$ and $w^{down}$. They also do rounding layer by layer. However our method uses a linear term as a regularizer inspired from our theoretical insights using discrepancy theory and uses simple linear interpolation between $w^{up}$ and $w^{down}$ and we round the entire model at once."}, {"title": "Discrepancy Theory", "content": "Discrepancy theory is a deep branch of mathematics and theoretical computer science, and we refer the readers to standard textbooks for more details [Matousek, 2009, Chazelle et al., 2004, Bansal, 2022] \u03a4\u03bf our knowledge, only Lybrand and Saab [2021] makes the connection between discrepancy theory and quantization. However, besides the high level motivational similarities, their work is not directly relevant to ours. Lybrand and Saab [2021] reduce the problem of understanding the error introduced by quantization on the output of a single neuron to a problem in discrepancy, and construct an algorithm for quantizing a single neuron. Their theoretical analysis on the generalization error only applies to quantizing the first layer of a neural network. On the other hand, we use discrepancy theory to understand when the whole network $f(w; s)$ can be approximated by $f(\\hat{w}; s)$ with $\\hat{w}$ in the quantization grid, and our theory holds for any network as a whole as long as our assumptions are true."}, {"title": "Connections to Discrepancy Theory", "content": "Let $f(w; s)$ be the loss function of a pre-trained neural network with weights $w \\in \\mathbb{R}^n$ on an input sample $s$ and let $D_{data}$ be the sample data distribution. Suppose we are also given a (scalar) quantization grid $Q = Q_1 \\times Q_2 \\times \\dots \\times Q_n$ where $Q_i \\subset \\mathbb{R}$ is a finite set of quantization points available to quantize the $j$th parameter. In this work, we focus on scalar quantization which allows us to write the quantization grid as a product set, i.e., each parameter can be independently rounded to a finite set of available values. Alternatively, in vector quantization a group of $d$ variables are rounded together to one of a finite set of quantization points in $\\mathbb{R}^d$, which has been used in some prior works [Tseng et al., 2024a, Egiazarian et al., 2024, van Baalen et al., 2024]. Generalizing our method to vector quantizers is an interesting future research direction.\nOur goal is to find a rounding $\\hat{w} \\in Q$ of the original weights $w$ such $f (\\hat{w}; s) \\approx f (w; s)$ where $s \\sim D_{data}$. We further impose the constraint that for each parameter $w_j$, we only round up or round down to the available values in $Q_j$, i.e., we only have two choices for $\\hat{w}_j$ denoted by $w^{up}, w^{down} \\in Q$; where $w^{up} < w_j < w^{down}$. We make this assumption because we don't want to change any parameter of the original model too much during quantization, consider it an important property of algorithms we design. Using Taylor expansion:\n$\\Delta f = f(\\hat{w}; s) - f(w; s) = (\\nabla_w f(w; s), \\hat{w} - w) + (\\hat{w} - w)^T \\nabla^2 f(w; s)(\\hat{w} - w) + \\cdots$"}, {"title": null, "content": "Assuming that the quantization grid $Q$ is fine enough and since we only round each parameter up or down, $|\\hat{w} - w||$ is small and so we can ignore the higher order terms. We claim that the first order term is the dominant term. Prior works such as Nagel et al. [2020], Hassibi et al. [1993], LeCun et al. [1989] have assumed that the first order term can be assumed to be zero because the model is trained to convergence and focused on reducing the second order term. But the model being trained to convergence just means that average gradient over many samples from the distribution is nearly zero. But the gradients still have some variance and gradients w.r.t. individual samples from the data distribution are not approximately zero (see Table 1). Figure 3 demonstrates this by showing that the error term $\\Delta f$ is well-correlated with the first order approximation $(\\nabla_w f (w; s), \\hat{w} - w)$.\nSo the goal now is to find a rounding $\\hat{w}$ such that $(\\nabla_w f (w; s), \\hat{w} - w) \\approx 0$ for samples $s \\sim D_{data}$. Suppose we sample $m$ samples $s_1, s_2, ..., s_m \\sim D_{data}$ independently from the data distribution, where $m \\ll n$. We now break our task into two parts of bounding the empirical error and generalization error as follows:\nQuestion 3.1. Can we find $\\hat{w} \\in Q$ (with $\\hat{w}_j \\in \\{w^{down}, w^{up}\\}$) such that $(\\nabla_w f(w; s_i), \\hat{w} - w) \\approx 0$ for all the samples $s_1, ..., s_m$?"}, {"title": "Bounding empirical error (Question 3.1)", "content": "For simplicity, let us assume that the quantization grid is uniform and $w^{up}_i - w^{down}_i = \\delta$ for all $i \\in [n]$ where $\\delta > 0$ is the distance between grid points. See Appendix C for how to genealize this to non-uniform grids. We will introduce new parameters $x \\in [0,1]^n$ and define $\\hat{w}^x = w^{down} + \\delta x$. Note that $w^x$ interpolates between $w^{down}$ and $w^{up}$ where $w_i = w^{down}$ if $x_i = 0$ and $w_i = w^{up}$ if $x_i = 1$. Let $y \\in [0,1]^n$ be the interpolation point corresponding to the original weights, i.e., $w^y = w$. We can rewrite the linear constraints in terms of $x$ as follows:\n$(\\nabla_w f(w; s_i), w^x - w) = (\\nabla_w f(w; s_i), w^x - w^y) = \\delta (\\nabla_w f(w; s_i), x - y).$\nLet $M$ be an $m \\times n$ matrix whose $i$th row is given by $\\nabla_w f (w; s_i)$. Then the linear constraints can be simply written as $M(x - y) = 0$. Our goal is to find a fully integral $\\hat{x} \\in \\{0,1\\}^n$ such that $M(\\hat{x} - y) = 0$. Let $V = \\{x \\in \\mathbb{R}^n : Mx = My\\}$ which is an affine subspace of dimension $\\ge n - m$. Define $K = [0, 1]^n \\cap V$ as the intersection of the hypercube with this subspace. $K$ is a convex polytope and it is non-empty because $y \\in K$. Therefore any vertex of $K$ should have $n - m$ integral coordinates (i.e., coordinates $j$ such that $x_j \\in \\{0,1\\}$). See Figure 1 for geometric intuition about why this is true. Since the number of parameters $n$ is much larger than the number of samples $m$, any vertex of $K$ is almost fully integral and exactly satisfies all the $m$ linear constraints.\nSuppose we further ask for a fully integral $\\hat{x}$ which approximately satisfies all the $m$ linear constraints, this precise question is answered by discrepancy theory which studies how to do this and relates the approximation error to properties of $M$ such as hereditary discrepancy [Lov\u00e1sz et al., 1986, Bansal, 2022]. We don't explore this direction further because the almost integral a vertex of $K$-is good enough if we apply RTN to the few remaining fractional parameters; we observe that the linear constraints are all approximately satisfied."}, {"title": "Bounding Generalization Error (Question 3.2)", "content": "How do we bound the generalization error if we know that the empirical approximation error is small? If $\\hat{w} - w$ is approximately orthogonal to $m$ sample gradients $\\nabla_w f(w; s_i)$ for $i = 1$ to $m$, why should we expect that $\\hat{w} - w$ is orthogonal to unseen gradients $\\nabla_w f(w; s)$ for samples $s \\sim D_{data}$? This should happen only if the gradients are approximately low rank. More precisely, let\n$\\Sigma = \\mathbb{E}_{s\\sim D_{data}}[gg^T] \\text{ where } g = \\nabla_w f(w; s)$"}, {"title": null, "content": "be the covariance matrix of the distribution of sample gradients and let $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n$ be its eigenvalues. We observe that the eigenvalues decay very fast, see Figure 4 for empirical validation of this on some real world models. We model this by assuming that $\\lambda_k \\le \\lambda_1 /k^\\alpha$ for $\\alpha > 1$. The assumption that $\\alpha > 1$ is valid since\n$\\mathbb{E}_{s}[||g||^2] = \\mathbb{E}_{s}[\\text{Tr}(gg^T)] = \\text{Tr}(\\mathbb{E}_{s}[gg^T]) = \\text{Tr}(\\Sigma) = \\sum_{i=1}^{n} \\lambda_i.$\nIt is well-known that the gradients of a pretrained model have constant norm on most samples (see Table 1 for empirical validation). Therefore $\\sum_{i=1}^{n} \\lambda_i = O(1)$ and so the the decay coefficient $\\alpha$ has to be at least 1. Under this assumption, it is reasonable to expect generalization. But this is not at all obvious to find a generalizing solution. In fact, any deterministic algorithm which chooses one of the vertices of $K$ will most likely not generalize. We give a randomized rounding algorithm (see Algorithm B.2) based on the famous Lovett-Meka algorithm from discrepancy theory [Lovett and Meka, 2012] which finds a vertex of $K$ which has low generalization error. The algorithm starts at $y$ and does a random walk (Brownian motion) inside the $n - m$ dimensional subspace $V$ formed by the linear constraints imposed by the $m$ samples. Whenever it hits a face $x_i = 0$ or $x_i = 1$ of the hypercube, it fixes that variable and continues the random walk until almost all the variables are rounded."}, {"title": "DiscQuant: Algorithm", "content": "In this section, we will present DiscQuant, a simple and practical algorithm for rounding inspired by the theoretical insights in Section 3. Instead of trying to approximate the loss function of the pre-trained model, i.e., $f(\\hat{w}; s) \\approx f(w; s)$, we will instead take a distillation approach and try to minimize the KL divergence between the next token distribution of the original model and the quantized model. Let $p_w(z_{<i})$ be the distribution of the next token predicted by the original model given prefix $z_{<i}$ where $z \\sim D_{data}$ is a sample from the data distribution. We want $\\text{error}(\\hat{w}) = \\mathbb{E}_{z\\sim D_{data}} \\mathbb{E}_i D_{KL} (P_w(\\cdot|z_{<i}) || P_{\\hat{w}}(\\cdot|z_{<i})) \\approx 0$.\nExpanding $\\text{error}(\\hat{w})$ using Taylor series, we can see that first order term vanishes exactly and so the second order term is the dominant term (see Appendix D). By Lemma D.1, Hessian of $\\text{error}(\\hat{w})$ can be written as a covariance of gradients as:\n$H_w = \\mathbb{E}_{z\\sim D_{data}} \\mathbb{E}_i \\mathbb{E}_{t\\sim p_w(t|z_{<i})} [(\\nabla_w \\log p_w(t|z_{<i})) (\\nabla_w \\log p_w(\\cdot|z_{<i}))^T].$"}, {"title": null, "content": "Therefore\n$\\text{error}(\\hat{w}) \\approx (\\hat{w} - w)^T H_w (\\hat{w} - w) = \\mathbb{E}_{z\\sim D_{data}} \\mathbb{E}_i \\mathbb{E}_{t\\sim p_w(t|z_{<i})} [(\\nabla_w \\log P_w (t|z_{<i}), \\hat{w} - w)^2].$\nSo minimizing $\\text{error}(\\hat{w})$ is a succinct way to impose constraints of the form $(\\nabla_w \\log p_w (t|z_{<i}), \\hat{w} - w) \\approx 0$ or equivalently $\\log p_w (t|z_{<i}) \\approx \\log p_{\\hat{w}} (t|z_{<i})$ where $t \\sim p_w(\\cdot|z_{<i})$ and $z \\sim D_{data}$. Therefore, we can use the same techniques developed in Section 3 to solve this as well. Assuming that the gradients are low rank, the set of $x$ satisfying these constraints (where $\\hat{w} = w^x$) form an affine subspace $V$ of dimension $\\ge n - m$ where $m$ is the number of samples. We are again interested in finding a vertex of the polytope $K = [0, 1]^n \\cap V$ which will have $\\ge n - m$ integral coordinates. At this point, we could use the Lovett-Meka algorithm (Algorithm B.2) which has provable generalization guarantees. But explicitly calculating all the gradients and storing them is infeasible. Instead a simple heuristic way to find a random vertex of polytope $K$ is to minimize a random linear function. Let $c \\in \\mathbb{R}^n$ be some arbitrary vector; we will try to minimize the linear function $(c, x)$ along with the KL divergence by taking a linear combination of them. The final optimization objective is shown in (3) where $\\lambda > 0$ is a regularization coefficient."}, {"title": null, "content": "min_x \\lambda A(c, x) + \\mathbb{E}_{z\\sim D_{data}} \\mathbb{E}_i [D_{KL} (P_w(\\cdot|Z_{<i}) || P_{w^*}(\\cdot|Z_{<i}))] \\\\ s.t. x \\in [0,1]^n."}, {"title": null, "content": "We solve the optimization problem (3) using projected stochastic gradient descent where we project $x$ to the hypercube after every gradient update. Optimizing (3) will keep us close the polytope $K$ and will approximately converge to a vertex of $K$ which is almost integral. We round whatever fractional coordinates are left using RTN to get a fully integral solution.\nWe use one additional heuristic to improve the performance of the algorithm in practice. Instead of choosing a random vertex of the polytope $K$ by choosing the vector $c$ at random, we will choose it carefully"}, {"title": "Experiments", "content": "We evaluate our method on the Phi-3-mini-4k-instruct [Abdin et al., 2024] and Meta-Llama-3.1-8B-Instruct [Dubey et al., 2024] models, and compare against GPTQ and greedy rounding (i.e. round-to-nearest, or RTN). We use the lm-evaluation-harness Gao et al. [2023] to evaluate on the Wikitext, GSM8k_cot 8-shot, MMLU 5-shot, ARC_Challenge 0-shot, PIQA 0-shot, HellaSwag 0-shot, and Winogrande 0-shot tasks. We report standard errors from lm-evaluation-harness. Wikitext measures perplexity, GSM8k is a generative task, and the remaining are multiple choice tasks. Note that generative tasks are typically more difficult than multiple choice tasks, and better reflect how the models are used in practice. See Appendix A for details on the hardware used, and hyper-parameter settings. Our method has similar memory requires as knowledge distillation, which also requires two copies of the model. We do not perform inference timing experiments; DiscQuant can optimize over a given quantization grid, so that we can utilize any pre-existing inference optimizations. For example, there are inference kernels for block scaling [Frantar et al., 2024] and incoherence processing [Tseng et al., 2024a]. Ablations on the loss formulation are in Appendix A."}, {"title": "Block Scaling", "content": "Our first experiments use standard block scaling quantization, determined by a bits and groupsize parameter. There are $2^{\\text{bits}}$ unique points, and every groupsize parameters share a unique 16-bit scale parameter. For example, 3.25 bits is achieved with bits=3, groupsize=64. We use the block scaling implementation from Frantar et al. [2024] which is symmetric linear quantization. Table 2 shows the results quantizing Phi-3-mini-4k-instruct. Across all tasks and all bit settings, our method DiscQuant achieves superior or comparable compression over the baseline GPTQ and RTN methods. The gap between DiscQuant and the baselines is greater at lower bits. On the ARC_Challenge, PIQA, and WinoGrade tasks, DiscQuant achieves full recovery with at least 0.25 fewer bits per parameter than GPTQ and RTN. For example on ARC_Challenge, DiscQuant achieves full recovery at 4 bits per weight, whereas GPTQ requires 4.25 bits, and RTN 4.5 bits. DiscQuant achieves better compression on the more difficult generative GSM8k task: at 4 bits DiscQuant gets 77.3% accuracy, while GPTQ gets 71.5%, and RTN gets 62.2%. Table 3 shows the results quantizing Meta-Llama-3.1-8B-Instruct. Overall the story is the same. Our method DiscQuant achieves improved compression on the majority of quantization levels and tasks. For example at 4 bits, DiscQuant gets 66.5% GSM8k accuracy, while GPTQ gets 63.2%, and RTN gets 50.8%."}, {"title": "Incoherence Processing", "content": "We explore another quantization format to show that our method can compose with other quantization improvements. Incoherence processing has been shown to improve quantization, especially at less than 4 bits per weight [Chee et al., 2023]. The weights are multiplied by certain random orthogonal matrices prior to quantization, which can reduce the range of the weights and make quantization easier. We employ the Randomized Hadamard Transform from Tseng et al. [2024a]. We use the same block scaling quantization grid as in the previous subsection. A subset of our results are shown in Figure 5, where we superimpose bar plots for block scaling and block scaling + incoherence processing. In the majority of cases, adding incoherence processing increases the task accuracy, especially at lower bits. We do not use fractional bits, (i.e. no groupsize), due to the fact that both these methods effect outliers and can interfere with one"}, {"title": "Effect of Data", "content": "We perform a simple investigation into the effect of the dataset on quantization. We mix math subject data- GSM8k and MetaMathQA-with our standard RedPajama dataset. Figure 6 shows the results of quantizing Phi-3-mini-4k-instruct at 3.25 bits with such a mix. As expected, both methods increase accuracy on GSM8k when there is a greater fraction of math data. On HellaSwag, DiscQuant improves with more math data, where GPTQ gets worse. On PIQA, both methods get worse. See Appendix A for all tasks. There is a meaningful change in accuracy as a result of changing the data mix. Choosing an appropriate data mix for quantization remains an important open question."}, {"title": "Ablations", "content": "We tried several distillation formulations, but ultimately chose a standard KL divergence between the outputs of the original and quantized model as the best approach. See Table 6. We quantize Phi-3-mini-4k-instruct to 3.25 bits, using 1024 samples. We tune the hyper-parameters as described at the beginning of this section. Note that for these ablations we used fewer samples than in our main experiments. In addition to the standard KL divergence, we tried several intermediate loss formulations for knowledge distillation. We used a normalized L2 loss between the outputs of the teacher and student, either per decoder layer (Intermed Type = Layer), or between each linear layer (Intermed Type = Linear). This distillation formulation was presented in Kurtic et al. [2023] for recovering LLMs after pruning. We also investigated taking an affine"}, {"title": "The Lovett Meka algorithm", "content": "A seminal result by Lovett and Meka Lovett and Meka [2012] works as follows: we are given a point $y \\in [0, 1]^n$ in the hypercube, vectors $v_1, ..., v_m \\in \\mathbb{R}^n$ with $||v_j||_2 = 1$ and parameters $c_j \\ge 0$ so that $1e^{-3/16} \\le  \\le e^{3/16}$. Then in randomized polynomial time one can find a point $x \\in [0,1]^n$ so that $| \\langle v_j, x - y \\rangle | \\le c_j$ for all $j$ and at least half the coordinates of $x$ are integral. Their algorithm is simple and elegant: we construct $x$ as the outcome of a random walk starting at $y$. Then iteratively, for some small step size $\\delta > 0$ we add the outcome of a random Gaussian times $\\delta$ to the current point. After hitting some constraint $x_i = 0$, $x_i = 1$ or $| \\langle v_j, x - y \\rangle | = c_j$, the Gaussian updates will be taken orthogonal to those normal vectors. In other words, the random walk will continue in the face of the described polytope. Still Lovett and Meka [2012] prove that performing the updates for $O(n)$ iterations the walk will cover enough distance so that on average $O(n)$ box constraints must become tight.\nIn our setting we only need to use parameters $c_j = 0$. However we use some properties of the Lovett-Meka algorithm that are not explicitly stated elsewhere. Here we denote $||M||_{S(1)}$ as the sum of the singular values of a matrix $M$ (also called Schatten-1 norm, nuclear norm or trace norm of $M$)."}, {"title": "The main theoretical result", "content": "As explained earlier we assume that we are given a weight vector $y \\in [0,1]^n$ and have access to samples $g_1, ..., g_m \\sim D$ where $D$ is a distribution on $\\mathbb{R}^n$ whose covariance matrix $\\Sigma := \\mathbb{E}_{g\\sim D_{data}}[gg^T]$ has rapidly decaying Eigenvalues, say $\\lambda_k \\le \\frac{C}{k^\\alpha}$ for some constants $C > 0$ and $\\alpha > 1$. In order to prove rigorous bounds we also need a mild assumption that provides that the distribution is well-behaved. We use the notion by O'Donnell O'Donnell [2014] and say that for a parameter $\\beta \\ge 1$, a random vector $X \\in \\mathbb{R}^n$ is $\\beta$-reasonable if\n$\\mathbb{E}[\\langle X, \\theta \\rangle^4] \\le \\beta \\cdot \\mathbb{E}[\\langle X, \\theta \\rangle^2]^2 \\forall \\theta \\in \\mathbb{R}^n$\nFor example $X \\sim \\{-1,1\\}^n$ and a Gaussian $X \\sim \\mathcal{N}(0, \\Sigma)$ are both O(1)-reasonable. Our main theoretical result is then:"}, {"title": null, "content": "Let $\\alpha > 1$ and $\\beta \\ge 1$ be constants and let $1 < m \\le \\frac{n}{16}$."}]}