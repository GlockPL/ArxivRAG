{"title": "Boosting Alignment for Post-Unlearning Text-to-Image\nGenerative Models", "authors": ["Myeongseob Ko", "Henry Li", "Zhun Wang", "Jonathan Patsenker", "Jiachen T. Wang", "Qinbin Li", "Ming Jin", "Dawn Song", "Ruoxi Jia"], "abstract": "Large-scale generative models have shown impressive image-generation capabili-\nties, propelled by massive data. However, this often inadvertently leads to the gen-\neration of harmful or inappropriate content and raises copyright concerns. Driven\nby these concerns, machine unlearning has become crucial to effectively purge\nundesirable knowledge from models. While existing literature has studied various\nunlearning techniques, these often suffer from either poor unlearning quality or\ndegradation in text-image alignment after unlearning, due to the competitive nature\nof these objectives. To address these challenges, we propose a framework that seeks\nan optimal model update at each unlearning iteration, ensuring monotonic improve-\nment on both objectives. We further derive the characterization of such an update.\nIn addition, we design procedures to strategically diversify the unlearning and re-\nmaining datasets to boost performance improvement. Our evaluation demonstrates\nthat our method effectively removes target classes from recent diffusion-based\ngenerative models and concepts from stable diffusion models while maintaining\nclose alignment with the models' original trained states, thus outperforming state-\nof-the-art baselines. Our code will be made available at https://github.com/\nreds-lab/Restricted_gradient_diversity_unlearning.git.", "sections": [{"title": "Introduction", "content": "Large-scale text-to-image generative models have recently gained considerable attention for their\nimpressive image-generation capabilities. Despite being at the height of their popularity, these\nmodels, trained on vast amounts of public data, inevitably face concerns related to harmful content\ngeneration [Heng and Soh, 2024] and copyright infringement [Zhang et al., 2023b]. Although"}, {"title": "Related Work", "content": ""}, {"title": "Machine Unlearning", "content": "Machine unlearning has primarily been propelled by the \"Right to be Forgotten\" (RTBF), which\nupholds the right of users to request the deletion of their data. Given that large-scale models are\noften trained on web-scraped public data, this becomes a critical consideration for model developers\nto avoid the need for retraining models with each individual request. In addition to RTBF, recent\nconcerns related to copyrights and harmful content generation further underscore the necessity and\nimportance of in-depth research in machine unlearning. The principal challenge in this field lies\nin effectively erasing the target concept from pre-trained models while maintaining performance\non other data. Recent studies have explored various approaches to unlearning, including the exact\nunlearning method [Bourtoule et al., 2021] and approximate methods such as using negative gradients,\nfine-tuning without the forget data, editing the entire parameter space of the model [Golatkar et al.,\n2020]. To encourage the targeted impact in the parameter space, [Golatkar et al., 2020, Foster\net al., 2024] proposed leveraging the Fisher information matrix, and [Fan et al., 2023] leveraged\na gradient-based weight saliency map to identify crucial neurons, thus minimizing the impact on\nremaining neurons. Furthermore, data-influence-based debiasing and unlearning have also been\nproposed [Chen et al., 2024, Bae et al., 2023]. Another line of work leverages mathematical tools in\ndifferential privacy [Guo et al., 2019, Chien et al., 2024] to ensure that the model's behavior remains\nindistinguishable between the retrained and unlearned models."}, {"title": "Machine Unlearning in Diffusion Models", "content": "Recent advancements in text-conditioned generative models [Ho and Salimans, 2022, Rombach\net al., 2022], trained on extensive web-scraped datasets like LAION-5B [Schuhmann et al., 2022],\nhave raised significant concerns about the generation of harmful content and copyright violations. A\nseries of studies have addressed the challenge of machine unlearning in diffusion models [Heng and\nSoh, 2024, Gandikota et al., 2023, Zhang et al., 2023a, Fan et al., 2023]. One approach [Heng and\nSoh, 2024] interprets machine unlearning as a continual learning problem, showing effective removal\nresults in classification tasks by employing Bayesian approaches to continual learning [Kirkpatrick\net al., 2017], which enhance unlearning quality while maintaining model performance using generative\nreply [Shin et al., 2017]. However, this approach falls short in removing concepts such as nudity\ncompared to other methods [Gandikota et al., 2023]. Another proposed method [Gandikota et al.,\n2023] guides the pre-trained model toward a prior distribution for the targeted concept but struggles\nto preserve performance. The most recent work [Fan et al., 2023] proposes selectively damaging\nneurons based on a saliency map and random labeling techniques, although this method tends to\noverlook the quality of the remaining set, focusing on improving the forgetting quality, which does\nnot fully address the primary challenges in the machine unlearning community. Although [Bae et al.,\n2023] presents a similar multi-task learning framework for variational autoencoders, their work does\nnot show the optimality of their solution, and their experiments mainly focus on small-scale models,\ndue to the computational expense associated with influence functions."}, {"title": "Our Approach", "content": "We study the efficacy of our approach in unlearning by removing target classes from class-conditional\ndiffusion models or eliminating specific concepts from text-to-image models while maintaining their\ngeneral generation capabilities. We will call the set of data points to be removed as the forgetting\ndataset. To set up the notations, let $D$ denote the training set and $D_f \\subset D$ be the forgetting dataset.\nWe will use $D_r = D \\backslash D_f$ to denote the remaining dataset. Our approach only assumes access to some\nrepresentative points for $D_f$ and $D_r$. As discussed later, depending on specific applications, these data"}, {"title": "Definition 1 (Directional Derivative)", "content": "The directional derivative [Spivak, 2006] of a function $f$ at $x$\nin the direction of $v$ is written as\n$D_v f(x) = \\lim_{h\\to 0} \\frac{f(x + hv)}{h}$                                                                                                                                         (1)\nThis special form of the derivative has the useful property that its maximizer can be related to the\ngradient $\u2207_x f(x)$, which we formally state below."}, {"title": "Theorem 2 (Directional derivative maximizer is the gradient)", "content": "Let $f$ be a function on $x$. Then the\nmaximum value of the directional derivative of $f$ at $x$ is $|\\nabla f(x)|$ the $l^2$ norm of its gradient. Moreover,\nthe direction $v$ is the gradient itself, i.e.,\n$arg \\max_v D_v f = \\nabla f(x)$.                                                                                                                                                (2)\nIn unlearning, we are specifically interested in the gradient of two losses, the forgetting loss $L_f$ and\nthe remaining loss $L_r$. Moreover, we seek gradient directions that simultaneously improve on both.\nThis motivates the restricted gradient, which we define below."}, {"title": "Definition 3 (Restricted gradient)", "content": "The negative restricted gradient of a pair of objectives $L_a, L_\\beta$ is\nthe direction $v$ at $\u03b8$ that satisfies\n$\\min_v D_v (L_a + L_\\beta)(\\theta) \\text{ s.t. } L_a(\\theta) \\geq L_a(\\theta + v) \\text{ and } L_\\beta(\\theta) \\geq L_\\beta(\\theta + v)$.\nIntuitively, with the restricted gradient we seek to define the ideal direction for unlearning. We would\nlike to optimize the joint loss $L = L_r + L_f$ subject to the condition that at every parameter update\nstep, $L_r$ and $L_f$ experience monotonic improvement. This is precisely the step prescribed by the\nnegative restricted gradient. Since the learning rates used to fine-tune the parameters in the unlearning\nprocess are typically quite small, we can approximate the updated loss at each iteration via a simple\nfirst-order Taylor expansion. In this case, the restricted gradient takes a simple form."}, {"title": "Theorem 4 (Characterizing the restricted gradient under linear approximation)", "content": "Given $\u03b8$, suppose\nthat $L_r(\\theta + \\delta) \u2013 L_r(\\theta) \u2248 \\delta \\cdot \\nabla L_r$ and $L_f(\\theta + \\delta) \u2013 L_f(\\theta) \u2248 \\delta \\cdot \\nabla L_f$. The restricted gradient can\nbe written as\n$arg \\min_v D_v (L_f + L_r)(\\theta) = \\delta_f^* + \\delta_r^*$,                                                                                                                                     (3)\nwhere\n$\\delta_f^* = \\nabla L_f - \\frac{\\nabla L_f \\cdot \\nabla L_r}{\\|\\nabla L_r\\|^2} \\nabla L_r, \\text{ } \\delta_r^* = \\nabla L_r - \\frac{\\nabla L_f \\cdot \\nabla L_r}{\\|\\nabla L_f\\|^2} \\nabla L_f$,                                                                                                          (4)\nwhen we have conflicting unconstrained gradient terms, i.e. $\\nabla L_f \\cdot \\nabla L_r < 0$.\nThe theorem presented demonstrates that\nthe restricted gradient is determined by\naggregating the modifications from $\u2207L_f$\nand $\u2207L_r$. This modification process in-\nvolves projecting $\u2207L_f$ onto the normal\nvector of $\u2207L_r$, yielding $\u03b4_f^*$, and similarly\nprojecting $\u2207L_r$ onto the normal vector of\n$\u2207L_f$, resulting in $\u03b4_r^*$. The optimal update,\nas derived in Theorem 4, is illustrated in\nFigure 2. Notably, when $\u2207L_f$ and $\u2207L_r$\nhave equal norms, the restricted gradient\nmatches the direct summation of the two\noriginal gradients, namely, $\u2207L_f + \u2207L_r$.\nHowever, it is more common for the norm\nof one gradient to dominate the other, in\nwhich case the restricted gradient provides\na more balanced update compared to direct\naggregation."}, {"title": "Remark 1", "content": "We wish to highlight an intriguing link between the gradient aggregation mechanism\npresented in Theorem 4 and an existing method to address gradient conflicts across different tasks in\nmulti-task learning. This restricted gradient coincides exactly with the gradient surgery procedure\nintroduced in Yu et al. [2020]. While their original paper presented the procedure from an intuitive\nperspective, our work offers an alternative viewpoint and rigorously characterizes the objective\nfunction that the gradient surgery procedure optimizes."}, {"title": "Diversify $D_r$", "content": "Since $D \\backslash D_f$ is usually of enormous scale, it is infeasible to incorporate all of them\ninto the remaining dataset $D_r$ for running the optimization. In practice, one can only sample a subset\nof points from $D_r$. In our experiments, we find that the diversity of $D_r$ plays an important role in\nmaintaining the model performance on the remaining dataset, as seen in Section 4.2. Therefore, we\npropose procedures for forming a diverse $D_r$. For models with a finite set of class labels, such as\ndiffusion models trained on CIFAR-10, we adopt a simple procedure of maintaining an equal number\nof samples for each class in $D_r$. Our ablation studies in Section 4.4 show that this is more effective in\nmaintaining model performance on the remaining dataset than more sophisticated procedures, such as\nselecting the most similar examples to the forgetting samples. The intuitive reason is that reminding\nthe model of as many fragments as possible related to the remaining set during each forgetting step is\ncrucial. By doing so, it leads to finding a representative restricted descent direction, which helps the\nmodel to precisely erase the forget data while maintaining a state comparable to the original model.\nWhen the text input is unconstrained, such as in the stable diffusion model setting, to strategically\ndesign diverse information, we propose the following procedure to generate $D_r$ based on the concept\nto be forgotten, denoted by $c$. Using a large language model (LLM), we first generate diverse text\nprompts related to concept $c$, yielding prompt set $Y_c$. These prompts are then modified by removing\nall references to $c$, creating a parallel set $Y$. By passing both $Y_c$ and $Y$ through the target diffusion\nmodel, we obtain corresponding image sets $X_c$ and $X$. This process allows us to construct our final\ndatasets: $D_f = \\{(x,y) \\mid x \\in X_c, y \\in Y_c\\}$ and $D_r = \\{(x, y) \\mid x \\in X,y \\in Y\\}$. Example prompts\nand detailed descriptions are provided in Appendix D."}, {"title": "Our Approach", "content": "We study the efficacy of our approach in unlearning by removing target classes from class-conditional\ndiffusion models or eliminating specific concepts from text-to-image models while maintaining their\ngeneral generation capabilities. We will call the set of data points to be removed as the forgetting\ndataset. To set up the notations, let $D$ denote the training set and $D_f \\subset D$ be the forgetting dataset.\nWe will use $D_r = D \\backslash D_f$ to denote the remaining dataset. Our approach only assumes access to some\nrepresentative points for $D_f$ and $D_r$. As discussed later, depending on specific applications, these data"}, {"title": "Experiment Setup", "content": "For our CIFAR-10 experiments, we leverage the EDM framework [Karras et al., 2022], which intro-\nduces some modeling improvements including a nonlinear sampling schedule, direct $x_0$-prediction,\nand a second-order Heun solver, achieving the state-of-the-art FID on CIFAR-10. For stable diffusion,\nwe utilize the pre-trained Stable Diffusion version 1.4, following prior works. Both implementations\nrequire two key hyperparameters: the weight of the gradient descent direction relative to the\nascent direction, and the loss truncation value $\\alpha$, which prevents unbounded loss maximization\nduring unlearning. Detailed hyperparameter configurations are provided in Appendix C. For dataset\nconstruction, we used all samples in each class for the CIFAR-10 forgetting dataset and 800 samples\nfor Stable Diffusion experiments. Considering the practical constraints of accessing complete datasets\nin real-world scenarios, we construct the remaining dataset $D_r$ by sampling 1% of data from each\nretained class, yielding a total of 450 samples for CIFAR-10 (50 from each of the 9 non-target classes)\nand 800 samples for Stable Diffusion.\nAs our baselines for CIFAR-10 experiments, we consider Finetune [Warnecke et al., 2021], gradient\nascent and descent, referred to as GradDiff, and SalUn [Fan et al., 2023]. For concept removal, our\nbaselines include the pretrained diffusion model SD [Rombach et al., 2022], erased stable diffusion\nESD [Gandikota et al., 2023], and SalUn [Fan et al., 2023]. To fairly compare, We further consider\nthe variants of ESD, depending on the unlearning task. We note that we do not consider the baseline\nby [Heng and Soh, 2024] due to its demonstrated limited performance in nudity removal, compared\nto ESD. Our approach is referred to as RG when applied only with the restricted gradient, and RGD\nwhen data diversity is incorporated.\nWe evaluate our approach\nusing multiple metrics to\nassess both forgetting ef-\nfectiveness and model util-\nity. For CIFAR-10 exper-\niments, we measure: re: 1)\nunlearning accuracy (UA),\ncalculated as 1-accuracy of\nthe target class, 2) remain-\ning accuracy (RA), which\nquantifies the accuracy on\nnon-target classes, and 3)\nFr\u00e9chet Inception Distance\n(FID). We observed that\nstandard CIFAR-10 classi-\nfiers demonstrate inherent\nbias when evaluating gen-\nerated samples from un-\nlearned classes, predomi-"}, {"title": "Target Class Removal from Diffusion Models", "content": "We present the CIFAR-10 experiment results in Table 1. To fairly compare, we use the same\nremaining dataset for other baselines. Our finding first indicates that while Finetune achieves\nsuperior performance on retained data (highest RA and FID scores), it struggles to effectively unlearn\ntarget classes with this limited remaining dataset. Although increasing the number of fine-tuning\niterations might improve unlearning accuracy through catastrophic forgetting, this approach would\nincur additional computational costs. Secondly, we observe that SalUn has low RA, compared\nto other baselines even with their comparable FID performance. We posit that random labeling\nintroduces confusion in the feature space, negatively impacting the accurate generation of classes and\nresulting in degraded classification performance. Moreover, it might be challenging to expect the\nsaliency map to select only the neurons related to specific classes or concepts, given the limitations of\ngradient ascent for computing the saliency map in diffusion models."}, {"title": "The Impact of Restricted Gradient and Data Diversity", "content": "Our observations are as follows. 1)\nRG outperforms Gradiff and Salun by decreasing FID and increasing RA while maintaining the\nbest UA performance. 2) RGD shows improvements over RG, suggesting that data diversification, in\nconjunction with the restricted gradient, further enhances performance in terms of RA and FID. We\nvary the hyperparameters and provide the results in section 4.4."}, {"title": "Target Concept Removal from Diffusion Models", "content": "Target concept removal has been a primary focus in diffusion model unlearning literature, driven by\nthe need to mitigate undesirable content generation. While existing methods have shown potential for\nremoving nudity or art styles, our study reveals that they often compromise model alignment after\nunlearning."}, {"title": "Nudity Removal", "content": "We summarize our results in Figure 4 and Table 2. We observe that Salun tends\nto generate samples that are overfit to the remaining dataset. Although Salun shows promising\nperformance in nudity removal-detecting fewer exposed body parts compared to SD and ESD-u,\nas shown in Figure 4\u2014this success comes at the cost of output diversity. In particular, SalUn often\ngenerates semantically similar images (e.g., men, wall backgrounds) for both forgetting concepts\n(Figure 3) and remaining data (Figure 1). Table 4 quantitatively validates this observation, revealing\nSalUn's lowest alignment scores post-unlearning. These results suggest that SalUn's forgetting\nperformance could stem from overfitting. This limitation may arise from two factors: the selected\nneurons potentially affecting both target and non-target concepts, and the limited diversity in their\nforget and remaining datasets. In the case of ESD, the resulting model often fails to remove the nudity\nconcept from unlearned models, as shown in Figure 4. We also evaluate ESD-u and observe that the\nnudity removal performance between ESD and ESD-u are quite similar although it achieves better\nAS than Salun. They suggest using \"nudity\u201d as a prompt for unlearning, but it might be difficult to\nreflect the entire semantic space related to the concept of \u201cnudity,\u201d given that we can describe nudity\nin many different ways using paraphrasing."}, {"title": "Art Style Removal", "content": "Similar to nudity removal, the task of eliminating specific art styles presents\na significant challenge. In order to evaluate whether the unlearning methods inadvertently impact\nother concepts and semantics beyond the targeted art style, we prompt the model with other artists'\nstyles (e.g., Monet, Picasso) while targeting to remove Vincent van Gogh's style. The results of\ngeneration examples are shown in Figure 1 and Figure 5, and the average alignment scores are shown\nin Table 2. It is observed that SalUn cannot follow the prompt to generate other artists' styles and\nshows a significant drop in alignment scores (AS) compared with the pre-trained SD.\nWe also train ESD-x by\nmodifying the cross-attention\nweights, which is more suit-\nable for erasing artist styles\nthan full-parameter training\n(shown as plain ESD without\nany suffix) as proposed in\nESD work. Although ESD-x\nperforms similarly to RG in\nterms of alignment scores,\nafter manual inspection of\nthe generated images, we find\nESD-x sometimes generates\nimages ignoring the style\ninstructions as presented in\nFigure 1, while RG generates images with lower quality details like noisy backgrounds but adheres\nwell to the style instructions. Consequently, after incorporating gradient surgery to prevent\ninterference between retain and forgot targets, our RGD achieves better image quality and shows the\nbest alignment score, almost equivalent to the performance of the pre-trained SD."}, {"title": "Ablation", "content": ""}, {"title": "Ablation in Hyperparameters", "content": "We examine our\nmethod's sensitivity to two key\nparameters described in Sec-\ntion 4.1: the retained gradient\nweight $\\lambda$ and loss truncation\nthreshold $\\alpha$. Figure 6 presents\nthe variation over different\n$\\alpha$ values (y-axis) for a given\n$\\lambda$ value (x-axis), measuring\nboth remaining accuracy\n(RA) and generation quality\n(FID). Analysis reveals that\nRG consistently outperforms\nGradDiff in both metrics (i.e.\nachieving the lower FID, and\nhigher or comparable RA with\nlow variation across different\n$\\alpha$), with RGD showing further\nimprovements. RGD exhibits\nthe lowest variance across"}, {"title": "Ablation in Diversity", "content": "We further investigate the impact of data diversity through controlled ex-\nperiments. For CIFAR-10, we design two scenarios based on feature similarity analysis using CLIP\nembeddings: Case 1, where $D_r$ contains samples from only the two classes most semantically similar\nto the target class, and Case 2, with balanced sampling across all classes. This design stems from our"}, {"title": "Conclusion", "content": "This study advances the understanding of ma-\nchine unlearning in text-to-image generative\nmodels by introducing a principled approach to\nbalance forgetting and remaining objectives. We\nshow that the restricted gradient provides an op-\ntimal update for handling conflicting gradients\nbetween these objectives, while strategic data\ndiversification ensures further improvements on\nmodel utilities. Our comprehensive evaluation\ndemonstrates that our method effectively re-\nmoves diverse target classes from CIFAR-10\ndiffusion models and concepts from stable diffu-\nsion models while maintaining close alignment\nwith the models' original trained states, outper-\nforming state-of-the-art baselines."}, {"title": "Limitation and Broader Impacts", "content": "While our solution introduces computation-\nefficient retain set generation using LLMs, the\nstrategic sampling of retain sets for stable diffusion models presents intriguing research directions.\nSpecifically, investigating the effectiveness of different sampling strategies\u2014such as the impact of\ndata proximity to target distribution and optimal mixing ratios between near and far samples could\nprovide valuable insights for unlearning in stable diffusion models. Although our restricted gradient\napproach successfully addresses gradient conflicts, developing robust unlearning methods that are\nless sensitive to hyperparameters remains an important challenge."}]}