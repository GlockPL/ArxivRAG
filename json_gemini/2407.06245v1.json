{"title": "ORAN-Bench-13K: An Open Source Benchmark\nfor Assessing LLMs in Open Radio Access\nNetworks", "authors": ["Pranshav Gajjar", "Vijay K. Shah"], "abstract": "Large Language Models (LLMs) can revolutionize\nhow we deploy and operate Open Radio Access Networks (O-\nRAN) by enhancing network analytics, anomaly detection, and\ncode generation and significantly increasing the efficiency and\nreliability of a plethora of O-RAN tasks. In this paper, we present\nORAN-Bench-13K, the first comprehensive benchmark designed\nto evaluate the performance of Large Language Models (LLMs)\nwithin the context of O-RAN. Our benchmark consists of 13,952\nmeticulously curated multiple-choice questions generated from\n116 O-RAN specification documents. We leverage a novel three-\nstage LLM framework, and the questions are categorized into\nthree distinct difficulties to cover a wide spectrum of ORAN-\nrelated knowledge. We thoroughly evaluate the performance\nof several state-of-the-art LLMs, including Gemini, Chat-GPT,\nand Mistral. Additionally, we propose ORANSight, a Retrieval-\nAugmented Generation (RAG)-based pipeline that demonstrates\nsuperior performance on ORAN-Bench-13K compared to other\ntested closed-source models. Our findings indicate that current\npopular LLM models are not proficient in O-RAN, highlighting\nthe need for specialized models. We observed a noticeable\nperformance improvement when incorporating the RAG-based\nORANSight pipeline, with a Macro Accuracy of 0.784 and a\nWeighted Accuracy of 0.776, which was on average 21.55% and\n22.59% better than the other tested LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "The telecommunications landscape is undergoing a\nparadigm shift with the emergence of Open Radio Access\nNetworks (ORAN), a disruptive approach promoting openness,\nflexibility, and innovation in mobile network architectures\n[1]. ORAN's modular and interoperable framework enables\noperators to integrate components from diverse vendors, fos-\ntering a more dynamic and cost-effective ecosystem. As O-\nRAN gains traction globally, its significance in shaping the\nfuture of mobile communications cannot be overstated [1] [2].\nConcurrently, the advent of Large Language Models (LLMs)\nhas sparked a revolution in Natural Language Processing\n(NLP) and Artificial Intelligence (AI), elevating text gener-\nation, comprehension, and interaction to unprecedented levels\nof sophistication [3]. Large Language Models (LLMs), such\nas OpenAI's Generative Pre-trained Transformer (GPT) series\nand open-source variants like the Mistral family of models,\nhave demonstrated remarkable capabilities in understanding\nand generating human-like text [4] [5].\nWe have seen the impact of LLMs and related NLP tech-\nnologies in multiple domains like Finance and medicine [6]\nwith revolutionary applications and a recent advent in research\nfor the telecommunications and wireless industry [6]. LLMs\nhave been used for automated code refactoring and design\n[7], recommending troubleshooting solutions [8], generating\nnetwork configurations [9], optimization tasks like load bal-\nancing [10], and even for prediction-based beamforming [11],\nand traffic load prediction [12]. The domain of O-RAN is\nstill in the nascent stages of LLM applications with papers\nworking towards Intent Processing and Network Optimization\n[13], and countless surveys on the possible avenues of research\nfor LLMs in O-RAN and related avenues [14] [13] [15].\nIt is important to understand that the inclusion of such LLMs\nis only possible after creating thorough evaluation strategies as\nexisting benchmark datasets in multiple domains have played\na crucial role in assessing model performance and guiding\narchitectural design [6], with prominent examples being FLUE\n[16] and MultiMedQA [17]. Evaluating performance for these\nspecialized language models is not as straightforward as tra-\nditional ML applications [18], and It is extremely important\nto have a comprehensive evaluation strategy to deploy such\nLLMs due to the excessive computational costs that are ob-\nserved during retraining and architecture creation. It is possible\nto assess the efficacy of LLM-based solutions by qualitatively\nobserving the generated outputs by human feedback. However,\nthe process would require immense effort and resources to\nanalyze comprehensively and still be susceptible to human\nerrors and biases. Due to the hallucination problem [19]\nin LLMs, without skilled evaluators, assessing an LLM's\nperformance for a specific domain would become difficult,\nand inaccurate. There are a few papers that have leveraged an\nLLM-based or a generative solution to create Multiple Choice\nQuestion benchmarks by using a large corpus of documents,\nwith promising results. The use of the MCQ styles benchmark\nhas also been heavily proposed in the literature [17] [6].\nConsidering the impact LLMs can have in O-RAN and due\nto the absence of any evaluation or benchmarking tools, this\npaper offers the following primary contributions:\nIt aims to create a novel benchmark named ORAN-\nBench-13K that can accurately assess an LLM's per-\nformance for O-RAN specifications knowledge. This is\nconstructed by leveraging three different LLM instances"}, {"title": "II. DATA SOURCES", "content": "We source all available specification documents from [24]\nresulting in a total of 116 documents with an average of 21778\nwords per specification document and a total of 2.53 million\nwords. The data processing pipeline will be explained thor-\noughly in the subsequent sections, and the central tendencies\nof the total number of words are mentioned in Figure 2."}, {"title": "III. BENCHMARK GENERATION", "content": "The MCQ generation is performed in a multi-stage process\nas depicted in the Figure 1. The primary aspect of this\npipeline is inspired by the papers [23] [6] and contains the\ngenerator and validator pair of LLMs which would determine\nthe total number of acceptable generated questions. All the\nLLM instances in the generation process use Gemini-1.5 as\nit has a higher yield rate than a Mistral-7B from initial\nexperiments. The entire process can be explained as:\nThe specification documents are converted to small\nchunks of text, with a size of 1536 and an overlap of\n256 characters. This specific number for obtained by\nexperimenting with different context sizes, as a larger\nnumber would result in losing out on important O-RAN\ncontent, and a smaller value would simply not have\nenough meaning to generate a valid MCQ. The overlap\nis added to preserve semantic context between chunks.\nGenerator LLM: This is the first LLM instance in this\nsystem, which is prompted to use the provided content or\nchunk to generate a valid MCQ specific for an O-RAN\nbenchmark. It is only prompted to generate one question\nper chunk to avoid repetition. The generated text is then\nparsed for the Question, Options, and Answer tags,\nValidator LLM: This is solely responsible for assessing\nthe generated questions, it is provided the context, the\ngenerated questions, and the options, and prompted to\ngenerate an answer as an Extractive QnA [25] task. This\nshows us if the generated question has semantic meaning\nwith the provided chunk, and if the options and answer\nare coherent as well. If the validator's answer and the\ngenerated answer are the same we proceed with the next\nsteps, or else the question is rejected.\nCategorizer LLM: This model is prompted to segregate\nthe valid questions into three categories based on the\ndescriptions:\nEasy: Questions that focus on basic concepts and\nknown facts.\nIntermediate: Questions that require comprehension\nand application of concepts, or involve moderate\ncalculations or reasoning.\nDifficult: Questions that demand a deep understand-\ning of Open RAN standards, or the ability to syn-\nthesize multiple pieces of information.\nThe final outputs are appended to a database and the\nprocess on the aforementioned O-RAN Specification doc-\numents results in 1139 'Easy', 9570 'Intermediate', and\n3243 'Difficult' questions.\nThe entire benchmark is available online at \u00b9 along with the\nrequired supplementary files."}, {"title": "IV. ORANSIGHT", "content": "The proposed RAG framework consists of three main\nparts, the Embedding Generator, the FAISS database, and the\nMistral-7B LLM. The reason we leverage the Mistral model\nis to keep ORANSight open source and easily adaptable to\nexisting LLM-based O-RANuse cases. We use a BGE-Small-\n1.5 (BAAI General Embeddings) model [26] as our embedding\ngenerator, BGE stands for three model sizes: small (24M),\nbase (102M), and large (326M), representing an embedding\ndimension of 384, 786, and 1024 respectively [26]. We choose\nBGE to main an open-source implementation and the small\nvariant for the increased computational efficiency.\nFAISS can be perceived as an efficient library for fast\nsimilarity search and clustering of dense vectors. As it is\ndesigned to handle large-scale data, FAISS is optimized for\nboth memory usage and computational speed, making it ideal\nfor high-dimensional text embeddings [21]. The core function-\nality of FAISS includes indexing methods that enable rapid\napproximate nearest neighbor (ANN) searches, allowing for\nreal-time retrieval of relevant vectors that can be used to obtain\nthe original text which is a document chunk. FAISS has been\nused as a critical component for various RAG-based systems\n[21] further validating its use in ORANSight.\nThe inherent function of ORANSight as depicted in the\nFigure 3 can be explained as:\nEmbedding Generation: Here all document chunks are\nalso obtained by a Recursive Text Splitter (using a chunk\nsize of 1024, and an overlap of 256) and are converted\ninto a set of 1D vectors using a BGE instance with a\ndimension of 384.\nInference: Once a processed database is ready, for each\nuser query, the top 5 relevant documents are retrieved and\nappended into the query resulting in a new prompt. The\nfinal prompt with added context is used by the Mistral-7B\nto make predictions.\nTo implement a conversational chatbot through the RAG\ncomputational chain, we append the system with a Buffer\nMemory [27] which would facilitate extended conversations\nand initial tests on ORANSight as a specification assistant."}, {"title": "V. TESTED LLMS", "content": "We experiment with three main models, ChatGPT-3.5,\nGemini-1.5, and Mistral-7B, all three are instruction-tuned.\nThe total parameter and architectural specifications aren't"}, {"title": "VI. RESULTS", "content": "This section elaborates on the obtained results using dif-\nferent LLMs on our benchmark. All the experiments were\nconducted by leveraging the Langchain library [27], GPT\nwas implemented using the OpenAI extension, Gemini by the\nGoogle-GenAI extension, and both BGE and Mistral-7B were\nimplemented using Huggingface [28].\nAs shown in Figure 4, it can be inferred that ORANSight\n(i.e., using RAG with Mistral) is the best-performing model\nwith superlative results in all three categories. There was an\nabsolute increase of 14.85%, 19.26%, and 18.17% across\nEasy, Intermediate and Difficult categories. It is also important\nto note that a Mistral model only has 7 billion parameters,\nwhen compared with other prominent architectures which are\nestimated to have at least 175 billion parameters [29], the re-\nsults hold immense value. The Gemini model was obtained to\nbe better suited for O-RAN than ChatGPT as it was relatively\nbetter across all three categories. The worst-performing model\nwas a vanilla Mistral and both the baseline and the RAG-\nenhanced model performed relatively better on the difficult\nbenchmark than the intermediate questions.\nThe results were further reinforced by the table I which\nshowcases the Macro and Weighted scores for all tested\nmodels. The weighted accuracy is calculated by using the\nno.of generated questions as weight values which can be\nreferred to in section III. The Macro Accuracy symbolizes\nan average score without assuming any weight values. The\nORANSight pipeline had an average increase in performance\nby the magnitude of 21.55% and 22.59% for Macro and\nWeighted scores. These findings underscore the effectiveness\nof the RAG-based approach in enhancing LLM performance\nwithin the specialized domain of ORAN. While the evaluated\nmodels, including Gemini, GPT, and Mistral, were better than\na random classifier (signifying a 25% score) it is still far from\noptimal. This further highlights the necessity for developing\nand utilizing specialized models to achieve higher accuracy in\ndomain-specific tasks within the O-RAN context."}, {"title": "VII. CONCLUSIONS AND FUTURE WORK", "content": "This paper aimed to create a comprehensive benchmark for\nevaluating the performance of LLMs for ORAN-centric tasks.\nBy leveraging a novel multi-stage process with three LLM\ninstances we have created a benchmarking set consisting of\n13952 Multilpe Choice Questions. These were further segre-\ngated into three distinct difficulty criteria, Easy, Intermediate,\nand Difficult symbolizing varying degrees of proficiency with\n1139, 9570, and 3243 questions respectively. We also propose\na RAG-based pipeline named ORANSight that would add\ndomain-specific knowledge to a Mistral-7B model. After a\nthorough analysis of three different LLM architectures Chat-\nGPT, Gemini-1.5, and Mistral 7B, we conclude that though\nthe benchmark scores are better than a random classifier, the\nLLMs are not proficient in O-RAN. A significant performance\nboost can be obtained by leveraging RAG as we could observe\nan average increase of 21.55% and 22.59% for both macro and\nweighted scores against the other tested models. For future\nwork, we wish to create an open-source fine-tuning dataset\nthat can be used to train O-RAN-proficient LLMs and also\nexplore a coding-based benchmark that can assess a model's\nability to perceive O-RAN codes."}, {"title": "VIII. APPENDIX: A", "content": "This section contains sample questions from the ORAN-\nBench-13K for all the aforementioned categories.\nA. Easy\nQuestion: In the context of O-RAN, what is the primary\npurpose of the Alarm List?\nOptions:\n1) To store a history of alarm events detected by the IMS.\n2) To manage the configuration of logging levels for vari-\nous O-RAN components.\n3) To provide a centralized repository for all O-RAN\nnetwork performance metrics.\n4) To facilitate the real-time monitoring of network traffic\npatterns.\nAnswer: 1"}, {"title": "B. Intermediate", "content": "Question: Which of the following is a method used by\nthe Service Management and Orchestration Framework to\ndynamically operate and maintain an O-RAN network?\nOptions:\n1) Configuring IP addressing for PNFs and VNFs.\n2) Managing software updates for existing NFs.\n3) Adding, removing, or modifying NFs.\n4) All of the above.\nAnswer: 4\nQuestion: What is the purpose of the NESPolicy Informa-\ntion Object Class (IOC) in O-RAN?\nOptions:\n1) To configure the callHomeClientInfo data type.\n2) To provide policies for enabling or disabling energy-\nsaving features in the O-RU.\n3) To manage the Shared O-RU Host role configuration.\n4) To define attributes for the Shared O-RU Host role.\nAnswer: 2\nQuestion: In a ladder topology for Open Radio Access\nNetworks (O-RAN), what is the primary purpose of having\nredundant sync flows from two different GMs?\nOptions:\n1) To improve the overall capacity of the network.\n2) To ensure failover and extended sync outages.\n3) To enable carrier aggregation across different leaf net-\nworks.\n4) To reduce latency in data transmission.\nAnswer: 2"}, {"title": "C. Difficult", "content": "Question: What does the \"I USE\" status of the sync-\nsource-status parameter in the ptp-status container indicate?\nOptions:\n1) The device is synchronized to an alternate master clock.\n2) The device is synchronized to the current master clock.\n3) The device is not synchronized to any master clock.\n4) The device is attempting to synchronize to an alternate\nmaster clock.\nAnswer: 2\nQuestion: Which of the following is the purpose of the\nOR.CellUA.MinPscellUeUl performance counter in O-RAN?\nOptions:\n1) To measure the maximum number of Primary Serving\nCells (PSCells) in a cell.\n2) To track the minimum number of User Equipment (UEs)\nconnected to a PSCell in the uplink (UL) every 100\nmilliseconds.\n3) To monitor the average number of UEs connected to a\nPSCell over a specified time period.\n4) To calculate the total number of UEs in a cell.\nAnswer: 2\nQuestion: Which of the following is NOT a required testing\ntool for verifying the removal of an SN terminated split bearer\ninitiated by the MN (eNB)?\nOptions:\n1) Test UE or UE emulator capable of supporting both LTE\nand NR\n2) Core or Core emulator (EPC with EN-DC capabilities)\n3) Application test server for UDP and TWAMP traffic\n4) Radio Frequency Analyzer for observing RF signals\nAnswer: 4"}, {"title": "IX. APPENDIC: B", "content": "This section contains sample text generated by ORANSight\nindicating a possible use case as a specification assistant.\nPrompt: Could you clarify the differences between the O-\nRAN fronthaul and midhaul interfaces?\nGenerated Response: The O-RAN fronthaul interface is the\nconnection between the O-DU (Distributed Unit) and the O-\nRU (Remote Unit) in the O-RAN (Open Radio Access\nNetwork) architecture. It is responsible for transporting user\ndata and control information between the O-RU and the O-\nDU.\nOn the other hand, the O-RAN midhaul interface is\nthe connection between the O-CU (Central Unit) and the O-\nDU. It is responsible for transporting user data and control\ninformation between the O-CU and the O-DU, and it is\nconsidered part of the Midhaul transport architecture by O-\nRAN WG5. In D-RAN and RAN architectures that do not\nemploy an HLS (Higher Layer Split), then these interfaces\nwill typically be considered part of the Backhaul network.\nPrompt: Please explain the concept of virtualization in\nthe context of ORAN.\nGenerated Response: In the context of O-RAN (Open\nRadio Access Network), virtualization refers to the process\nof running network functions (such as O-RU and O-DU)\non software rather than dedicated hardware. This allows for\nincreased flexibility, scalability, and cost-effectiveness by\nenabling the sharing of resources among multiple network\nfunctions and the ability to easily update or replace network\nfunctions without affecting the overall system. Virtualization\nin O-RAN is a key enabler for cloud-native network\ndeployments and enables the realization of the vision of a\ndisaggregated, open, and interoperable RAN.\nPrompt: Give a step-by-step guide to implementing the O-\nRAN RIC (RAN Intelligent Controller) API.\nGenerated Response: To implement the O-RAN RIC (RAN\nIntelligent Controller) API, follow these steps:\n1) Understand the O-RAN architecture and its components,\nincluding the Near-RT RIC, Non-RT RIC, O-CU, O-CU-\nCP, O-CU-UP, and DU (Distributed Unit).\n2) Familiarize yourself with the O-RAN RIC API speci-\nfications, which define the interfaces and functions for\ncontrolling and optimizing RAN elements and resources.\n3) Implement the Near-RT RIC API, which enables near-\nreal-time control and optimization of RAN elements and\nresources via fine-grained data collection and actions\nover E2 interface.\n4) Implement the Non-RT RIC API, which enables non-\nreal-time control and optimization of RAN elements and\nresources, AI/ML workflow including model training\nand updates, and policy-based guidance of applica-\ntions/features in Near-RT RIC.\n5) Integrate the Near-RT RIC and Non-RT RIC APIs with\nthe O-CU-CP and O-CU-UP components to enable\nseamless communication and control of the RAN.\n6) Test the implementation to ensure that it meets the\nspecified requirements and functions correctly.\n7) Deploy the O-RAN RIC API in a network environment\nand monitor its performance to identify any issues and\nmake necessary adjustments."}]}