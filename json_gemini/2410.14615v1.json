{"title": "Asymptotically Optimal Change Detection for Unnormalized Pre- and Post-Change Distributions", "authors": ["Arman Adibi", "Sanjeev Kulkarni", "H. Vincent Poor", "Taposh Banerjee", "Vahid Tarokh"], "abstract": "This paper addresses the problem of detecting changes when only unnormalized pre- and post-change distributions are accessible. This situation happens in many scenarios in physics such as in ferromagnetism, crystallography, magneto-hydrodynamics, and thermodynamics, where the energy models are difficult to normalize.\n\nOur approach is based on the estimation of the Cumulative Sum (CUSUM) statistics, which is known to produce optimal performance. We first present an intuitively appealing approximation method. Unfortunately, this produces a biased estimator of the CUSUM statistics and may cause performance degradation. We then propose the Log-Partition Approximation Cumulative Sum (LPA-CUSUM) algorithm based on thermodynamic integration (TI) in order to estimate the log-ratio of normalizing constants of pre- and post-change distributions. It is proved that this approach gives an unbiased estimate of the log-partition function and the CUSUM statistics, and leads to an asymptotically optimal performance. Moreover, we derive a relationship between the required sample size for thermodynamic integration and the desired detection delay performance, offering guidelines for practical parameter selection. Numerical studies are provided demonstrating the efficacy of our approach.", "sections": [{"title": "1 INTRODUCTION", "content": "Quickest change detection is a fundamental problem in various domains, such as control, signal processing, machine learning, and finance [Veeravalli, 2001, Xie et al., 2021]. In many real-world applications in physics and machine learning [Holzm\u00fcller and Bach, 2023], we have access to the un-normalized version of the distributions, while most change detection algorithms require access to the nor-malized version of the distributions, necessitating com-putation of the normalizing constants. However, com-puting the normalization constants of distributions is challenging because it often involves evaluating in-tegrals or summations over high-dimensional spaces, which is typically infeasible [Huber, 2015]. Specifically, for many energy-based models, this computation is NP-hard and even when feasible, it remains compu-tationally difficult and expensive [Kolmogorov, 2018]. This paper tackles the problem of change detection when the normalizing constants of the pre-change and post-change distributions are intractable.\n\nWe propose a change detection method that utilizes thermodynamic integration (TI), a technique from sta-tistical physics that estimates intractable normalizing constants for high-dimensional distributions. TI re-lies on the insight that estimating the ratio of two unknown normalizing constants is more feasible than directly computing the constants themselves.\n\nOur approach builds upon the CUSUM algorithm pro-posed by [Page, 1955], which is designed for sequential change detection when the pre-change and post-change distributions are known. However, in our setting, we do not have access to the normalizing constants of these distributions. By incorporating TI into the CUSUM algorithm, we can estimate the log-ratio of the nor-malizing constants using an unbiased estimator with bounded variance.\n\nThe main contributions of this paper are:\n\n\u2022 We introduce a novel change detection algorithm, LPA-CUSUM, that combines the CUSUM frame-"}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 Change Detection", "content": "Change detection algorithms play a critical role in various fields, including sensor networks, cyber-physical systems, and neuroscience, where detecting abrupt changes in data streams is important [Veeravalli and Banerjee, 2014, Poor and Hadjiliadis, 2008]. In the literature, various optimality results have been estab-lished for classical change detection algorithms [Shiryaev, 1963, Moustakides, 1986, Page, 1955, Roberts, 1966, Lai, 1998, Tartakovsky et al., 2014]. Notably, the Cumulative Sum (CUSUM) algorithm has been shown to be asymptotically optimal under certain formulations, such as Lorden's problem and Pollak's problem [Lorden, 1971, Pollak, 1985].\n\nHowever, these optimality results are based on the assumption of known distributions before and af-ter the change, which may not hold in practical scenarios [Yu et al., 2016, CHEN and ZHANG, 2015, Nalisnick et al., 2018, Wu et al., 2023]. Moreover, likelihood-based algorithms, such as the CUSUM algo-rithm, are widely used but may face difficulties in sce-narios where explicit distributions are computationally challenging to compute. To address these challenges, Wu et al. [Wu et al., 2023] proposed the Score-based CUSUM (SCUSUM) algorithm, which enables change detection for unnormalized statistical models with un-known normalization constants or when the Hyv\u00e4ri-nen Score of the distributions are available. While SCUSUM is efficient when exactly computing the nor-malizing constants Z1, and Zo is not possible, in many cases, we can compute an estimator of the normalizing constants, and we will show how the use of such an estimator can help us achieve performance close to the CUSUM algorithm."}, {"title": "2.2 Partition Function Estimation", "content": "Estimating partition functions is a key challenge for complex distributions such as energy-based models (Gibbs distributions). One of the most common ap-proaches is annealed importance sampling (AIS), in-troduced by Neal [Neal, 2001]. AIS has become a standard method for partition function estimation, owing to its ability to handle complex probabilistic models. Subsequent works have addressed limitations of the original AIS algorithm by improving conver-gence properties and extending theoretical understand-ing, such as in the works by Karagiannis and An-drieu [Karagiannis and Andrieu, 2013] and Holzm\u00fcller et al. [Holzm\u00fcller and Bach, 2023]. Adaptive ver-sions of AIS have been proposed to enhance effi-ciency, such as the adaptive AIS by \u0160tefankovi\u010d et al. [\u0160tefankovi\u010d et al., 2009], which dynamically ad-justs the annealing schedule based on observed perfor-mance, thereby improving computational efficiency in challenging scenarios. For discrete distributions, Had-dadan et al. [Haddadan et al., 2021] developed the fast annealed importance sampling (FAIS) algorithm, which combines AIS with sequential Monte Carlo meth-ods to achieve significant computational savings while retaining accuracy.\n\nAnother widely used technique for log-partition func-tion estimation is thermodynamic integration (TI), which has applications in both physics and chem-istry [Frenkel and Smit, 2023, Friel and Wyse, 2012]. TI is based on calculating the ratio of unknown nor-malizing constants rather than the constants them-selves, thereby simplifying the estimation process [Kirkwood, 1935, Gelman and Meng, 1998]. TI pro-vides an unbiased estimator with bounded variance, making it suitable for complex, high-dimensional dis-tributions. Ge et al. [Geet al., 2020] analyzed an annealing algorithm combined with multilevel Monte Carlo sampling for estimating log-partition functions in log-concave settings, providing important theoretical insights, including information-based lower bounds on achievable convergence rates. Recent work by Marteau-Ferey et al. [Marteau-Ferey et al., 2022] proposed an approach involving sampling via log-partition function estimation, which can be particularly useful in high-dimensional scenarios. Bach [Bach, 2022, Bach, 2024] further explored this domain by proposing sum-of-squares relaxations for variational inference, offering a new direction for addressing the challenges of partition function estimation."}, {"title": "3 PROBLEM FORMULATION", "content": "Quickest change detection refers to the problem of determining whether or not a change has occurred in"}, {"title": "3.1 The Likelihood Ratio-based CUSUM Algorithm", "content": "Given the data stream {Xn}n>1, the stopping rule of the likelihood ratio-based CUSUM algorithm is defined by\n\nTCUSUM inf \\{ n\u2265 1: \\max_{1<k<n} \\sum_{i=k}^{n}log \\frac{P_{1}(X_i)}{P_{0}(X_i)} > log h\\}.\n\nwhere the infimum of the empty set is defined to be +\u221e, and logh > 0 is referred to as the stopping threshold. The value of this threshold is clearly related to the trade-off between detection delay and false alarms. It is known [Page, 1955] that Tcusum can be written as\n\nTCUSUM = inf{n \u2265 1 : A(n) > log h},\n\nwhere A(n) is defined using the recursion\n\n\u039b(0) = 0, \u039b(n) = ( A(n - 1) + log \\frac{P_{1}(X_n)}{P_{0}(X_n)} )^{+}.\n\nIn [Moustakides, 1986], it is demonstrated that the CUSUM algorithm achieves exact optimality for Lor-den's problem, under every fixed constraint \u03b3 (provided the threshold in the CUSUM algorithm is chosen so that the false alarm constraint is met with equality). Addi-tionally, as noted in [Lai, 1998], the algorithm is asymp-totically optimal for Pollak's problem. The asymptotic performance of the CUSUM algorithm is further exam-ined in [Lorden, 1971] and [Lai, 1998]. Specifically, it is established that\n\nLWADD (TCUSUM) ~ LCADD(TCUSUM) ~ \\frac{log h}{D_{KL} (P_{1}, P_{0})},\n\nas y \u2192 \u221e. Here, DKL (P1, Po) denotes the Kullback-Leibler divergence between the post-change distribution P\u2081 and the pre-change distribution Po:\n\nDKL (P1, P0) = \\int_{X} P_{1}(X) log \\frac{P_{1}(X)}{P_{0}(X)} dx.\n\nFurthermore, the notation g(c) ~ h(c) as c \u2192 Co sig-nifies that \\frac{g(c)}{h(c)}\u21921 as c\u2192 co for any two functions c\u2192 g(c) and c \u2192 h(c)."}, {"title": "3.2 Unnormalized Statistical Models", "content": "In applications where obtaining access to P1, Po is not possible, the CUSUM algorithm cannot be used. Specifically, the focus is on the case where we only have access to the unnormalized versions of the distri-butions, P1(X) and Po(X), where P_{1}(X) = \\frac{\\tilde{P_{1}(X)}}{Z_{1}} and P_{0}(X) = \\frac{\\tilde{P_{0}(X)}}{Z_{0}}.\n\nAn alternative method proposed by Wu et al. [Wu et al., 2023] is the Score-based CUSUM (SCUSUM). SCUSUM is a novel variant of the CUSUM algorithm that replaces the log-likelihood with the Hyv\u00e4rinen score and provides a delay guarantee in terms of the Fisher divergence. While SCUSUM is efficient when computing the normalizing constants Z1, Zo is not possible, in many cases, we can calculate an estimator of the normalizing constants, and we will show how the use of such an estimator can help us achieve performance close to the CUSUM algorithm."}, {"title": "4 PROPOSED ALGORITHM", "content": "Now, we will consider the case where we have two unnormalized distributions P1(X) and Po(X) where P_{1}(X) = \\frac{\\tilde{P_{1}(X)}}{Z_{1}} and P_{0}(X) = \\frac{\\tilde{P_{0}(X)}}{Z_{0}}, and we don't have access to Z\u2081 and Zo. But, we have an oracle A (see section 5.4.2 for more details about oracle A) that can generate an unbiased estimator of log \\frac{Z_{0}}{Z_{1}} with variance \u03c3\u00b2. We want to show that using this oracle, we can run the Algorithm 1 that approximately behaves similar to CUSUM. Specifically, we want to use the Algorithm 1 which uses Ti,n, defined as\n\nTin := \\frac{Y_{1,n} + Y_{2,n} + \u00b7\u00b7\u00b7 + Y_{i,n}}{i}, (\u03b2>0), (2)\n\nWhere Yk,n are independent and identically distributed outputs of the oracle A for all k \u2265 0, and n > 1. These outputs are unbiased estimators of log \\frac{Z_{0}}{Z_{1}} with variance \u03c3\u00b2. Yk,n are independent of X; for all k, j \u2265 0, and n\u2265 1. The update rule in Algorithm 1 is based on the variable Z(n), which is defined recursively as\n\nZ(n) := Z(n - 1) + y log \\frac{P_{1}(X_n)}{P_{0}(X_n)} + T_{i,n}\n\nWe also define the stopping point of the algorithm as\n\n\u03c4 := inf{n \u2265 1 : Z(n) \u2265 log h}."}, {"title": "5 THEORETICAL ANALYSIS", "content": "In this section, we analyze the performance of the delay and false alarm rate for the algorithm 1."}, {"title": "5.1 Controlling the False Alarm", "content": "In this section, we will focus on analyzing the false alarm rate. We will begin by presenting the prelim-"}, {"title": "5.4 Partition Function Estimation", "content": "This section will discuss how to build oracle A in the Algorithm 1. One approach to build an estimator of the log partition function is leveraging Thermodynamic In-tegration (TI) to estimate the log-ratio of the partition functions.\n\nSuppose we have two unnormalized distributions P1(X) and Po(X), where P_{1}(X) = \\frac{\\tilde{P_{1}(X)}}{Z_{1}} and P_{0}(X) = \\frac{\\tilde{P_{0}(X)}}{Z_{0}} are the corresponding normalized distributions, with Z1 and Zo being the partition functions or normalizing con-stants. The goal is to estimate the log-ratio log(Zo/Z1) using TI, as direct computation of the partition func-tions may be intractable for complex, high-dimensional distributions."}, {"title": "5.4.1 Naive Approaches", "content": "In this section, we discuss two methods for estimat-ing the partition function and demonstrate that these approaches yield biased estimators, making them un-suitable for our setting. This motivates the use of Ther-modynamic Integration, which offers a more promising alternative.\n\nThe first approach to estimate Z1 and Zo involves approximating the integral definition:\n\nZi = \\int_{X} \\tilde{P_i}(X)dX, i\u2208 \\{0,1\\}.\n\nIf we denote these approximations by \u017do and \u017d1, we can use log (\\frac{\\tilde{Z_0}}{\\tilde{Z_1}}) as an estimator for log (\\frac{Z_0}{Z_1}). However, this estimator is biased, and using a biased estimator in Algorithm 1 will result in a high false alarm rate and poor detection delay, unless the bias is smaller than the change point, specifically O (\\frac{1}{\\sqrt{i}}), which requires a large number of samples.\n\nThe second method to estimate the normalizing con-stant uses the identity\n\n\\frac{Z_{0}}{Z_{1}} = Ex~P_{1} \\frac{\\tilde{P_{0}(X)}}{\\tilde{P_{1}(X)}},\n\nand employs the Monte Carlo estimator\n\nR:= \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\tilde{P_{0}(X)}}{\\tilde{P_{1}(X)}},\n\nwhere X ~ P1. Again, log R is a biased estimator of log (\\frac{Z_{0}}{Z_{1}}), and unless a large number of samples is used, the resulting bias leads to poor guarantees.\n\nNeither of these approaches is effective for our setting due to the inherent biases that significantly affect de-tection performance. Therefore, in the next section, we introduce Thermodynamic Integration (TI), which provides a more efficient solution."}, {"title": "5.4.2 Thermodynamic Integration (TI)", "content": "TI is a technique used in physics to approximate in-tractable normalized constants of high-dimensional dis-tributionsIt is based on the observation that it is easier to calculate the ratio of two unknown normalizing con-stants than it is to calculate the constants themselves. More formally, consider two densities over space X:\n\nP(X) = \\frac{\\tilde{P(X)}}{Zi}, Zi = \\int_{X} \\tilde{P(X)} dX, i \u2208 \\{0,1\\}.\n\nTo apply TI, we form a continuous family (or \"path\") between Po(X) and P1(X) via a scalar parameter \u03b2\u2208 [0, 1]:\n\nP\u03b2(X) = \\frac{\\tilde{P\u03b2(X)}}{Z\u03b2}, P\u03b2(X) = \\tilde{P_{1}(X)}^{\u03b2}\\tilde{P_{0}(X)}^{1-\u03b2},\n\nZB = \\int_{X} \\tilde{P\u03b2(X)} dX, \u03b2\u03b5 [0,1].\n\nThe central identity that allows us to compute the ratio log(Z1/Zo) is derived as follows. Assuming we can exchange integration with differentiation:\n\n\\frac{\u2202log Z_{\u03b2}}{\u2202\u03b2} = \\frac{1}{Z\u03b2} \\frac{\u2202 Z\u03b2}{\u2202\u03b2},\n\n= \\frac{1}{Z\u03b2} \\frac{\u2202}{\u2202\u03b2} \\int_{X} \\tilde{P\u03b2(X)} dx,\n\n= \\frac{1}{Z\u03b2} \\int_{X} \\frac{\u2202 \\tilde{P\u03b2(X)}}{\u2202\u03b2} dX,\n\n= \\frac{1}{Z\u03b2} \\int_{X} \\tilde{P\u03b2(X)} \\frac{\u2202}{\u2202\u03b2} log \\tilde{P\u03b2(X)} dX,\n\n= \\int_{X} P\u03b2(X) \\frac{\u2202}{\u2202\u03b2} log \\tilde{P\u03b2(X)} dx,\n\nwhich directly implies\n\n\\frac{\u2202log Z_{\u03b2}}{\u2202\u03b2} = Ex~P\u03b2 [U'(X)],\n\nwhere the quantity UB(X) = log \u00de\u03b2(X) is referred to as the \"potential\" in statistical physics and U'(X) := \\frac{\u2202UB(X)}{\u2202\u03b2}.\n\nRemark 2. For exchange of integration and differen-tiation we assume we have the following conditions:\n\n1. \\tilde{P\u03b2(X)} is a integrable function of \u03b2 for each X \u2208 \u03a7.\n\n2. For almost all \u03b2\u2208 [0,1], the partial derivative \\frac{\u2202 \\tilde{P\u03b2(X)}}{\u2202\u03b2} exists for all X \u2208 X.\n\n3. There is an integrable function f : \u03b2 \u2192 [0,1] such that |\\frac{\u2202 \\tilde{P\u03b2(X)}}{\u2202\u03b2}|\u2264 f(\u03b2) for all x \u2208 X and almost every \u03b2\u2208 [0,1]."}, {"title": "5.4.3 Computing the Expectations", "content": "In this part, we show how to implement Ex~P [U'(X)]. Important sampling is necessary for efficient implemen-tation of the expectation in thermodynamic integration [Masrani, 2023]. To do that, we use the following iden-tity:\n\nEx~P [U'(X)] = Ex~P_{0} \\frac{w(X)\u03b2U'(X)}{w(X)\u03b2} , (18)\n\nwhere w(X) = \\frac{\\tilde{P_{1}(X)}}{\\tilde{P_{0}(X)}}. By substituting w(X), we can show that\n\nEx~P_{0} [w(X)] = \\frac{Z\u03b2}{Zo},\n\nand\n\nEx~P_{0} [w(X)U'(X)] = \\frac{Z\u03b2}{Zo} Ex~P\u03b2 [U'(X)].\n\nThis will help us in the implementation of the estimator. We sample K samples {X1, X2, ..., XK} ~ Po and use the following identity to compute the expectation:\n\nEx~P [U'(X)] \u2248 \\sum_{i=1}^{K} \\frac{w(Xi)\u03b2U'(Xi)}{w(Xi)\u03b2} , (21)\n\nSince w(X) is the same for different \u03b2, we can reuse the samples {X1, X2, . . ., XK } for different \u03b2, reducing the number of samples needed."}, {"title": "5.4.4 Variance of the Estimator", "content": "In this section, we derive an upper bound for the vari-ance of the estimator U'(X). The key steps in deriving this upper bound are:\n\n\u2022 Using the law of total variance to express Var[U'(X)] in terms of the variance under PB(\u03a7) and the variance of the expectation of U'(X) under P(X).\n\n\u2022 Recognizing that the variance under P(X) is the second derivative of log ZB with respect to \u03b2, which is an increasing function of B.\n\n\u2022 Utilizing the expressions for \\frac{\u2202log Z\u03b2}{\u2202\u03b2} evaluated at \u03b2 = 0 and \u03b2 = 1, which involve the KL diver-gences between P1 and Po, and the log ratio of the partition functions.\n\n\u2022 Bounding the expected value of the squared deriva-tive of log Z\u00df with respect to \u1e9e using the increas-ing property and the expressions at the endpoints \u03b2 = 0 and \u03b2 = 1.\n\nWe will present the preliminary lemmas required to prove the Theorem 4."}, {"title": "6 NUMERICAL RESULTS", "content": "In this section, we conduct comprehensive numerical experiments on synthetic data to evaluate the perfor-mance of our proposed method compared with the es-tablished change detection algorithm for unnormalized data [Wu et al., 2023]. Throughout the experiments, we refer to our algorithm presented in Algorithm 1 as LPA, the score-based method from [Wu et al., 2023] as SCUSUM, and the Cumulative Sum method from [Page, 1955] as CUSUM."}, {"title": "6.1 Synthetic Dataset", "content": "Multivariate Normal Distribution (MVN) We consider synthetic data generated from a 10-dimensional multivariate normal distribution. The pre-change distribution is characterized by a mean vector \u03bc = 0 and a covariance matrix. For the post-change scenario, we investigate both mean shifts and covariance changes. Specifically, we set the post-change mean as \u03bc\u2081 = 1 and modify the covariance components with a small change.\n\nExponential Family (EXP) We also consider an exponential family model. The distribution under con-"}]}