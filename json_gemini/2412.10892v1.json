{"title": "Know Unreported Roadway Incidents in Real-time: A Deep Learning Framework for Early Traffic Anomaly Detection", "authors": ["Haocheng Duan", "Hao Wu", "Sean Qian"], "abstract": "The goal of this research is to know traffic anomalies as early as possible. A traffic anomaly refers to a generic incident on the road that influences traffic flow and calls for urgent traffic management measures. \"Knowing\" the occurrence of a traffic anomaly is twofold: the ability to detect this anomaly before it is reported anywhere, or it may be such that an anomaly can be predicted before it actually occurs on the road (e.g., non-recurrent traffic breakdown). Either way, the objective is to inform traffic operators of unreported incidents in real time (and as early as possible), regardless of whether it is being reported later or not. The key is to stay ahead of the curve. Time is of the essence.\nConventional automatic incident detection (AID) has relied heavily on all incident reports exclusively for training and evaluation. However, these reports suffer from a number of issues, such as delayed reports, inaccurate descriptions, false alarms, missing reports, and incidents that do not necessarily influence traffic. Relying on these reports to train or calibrate AID models hinders their ability to detect traffic anomalies effectively and timely, even leading to convergence issues in the model training process. Moreover, conventional AID models are not inherently designed to capture the early indicators of any generic incidents. It remains unclear how far ahead an AID model can report incidents. The AID applications in the literature are also spatially limited because the data used by most models is often limited to specific test road segments. To solve these problems, we propose a deep learning framework utilizing prior domain knowledge and model-designing strategies. This allows the model to detect a broader range of anomalies, not only incidents that significantly influence traffic flow but also early characteristics of incidents along with historically unreported anomalies. We specially design the model to target the early-stage detection/prediction of an incident. Additionally, unlike most conventional AID studies, we use widely available data, enhancing our method's scalability. The experimental results across numerous road segments on different maps demonstrate that our model leads to more effective and early anomaly detection. Our framework does not focus on stacking or tweaking various deep learning models; instead, it focuses on model design and training strategies to improve early detection performance.", "sections": [{"title": "1 Introduction", "content": "Traffic congestion grievously disrupts the everyday lives of urban residents and causes substantial economic losses. It can be categorized into recurring and non-recurring congestion. Recurring congestion occurs periodically when traffic volume exceeds the road's capacity. In contrast, non-recurring congestion, also known as traffic anomalies, is caused by incidents such as crashes, work zones, and special events. In the U.S., non-recurring congestion accounts for over half of the total congestion (Federal Highway Administration (FHWA), 2020b). Mitigating non-recurring impacts requires accurately knowing roadway incidents in advance and proactive management strategies as opposed to being reactive. Figure 1 illustrates the FHWA timeline for managing non-recurring events, highlighting that the effectiveness of proactive strategies heavily relies on the timely detection and verification of incidents. However, incident reports in the real world are typically delayed and subject to multiple layers of verification, not to mention a large number of unreported incidents.\nAcknowledging that incident reports are oftentimes late or missing, traffic operators may find their actions too late by the time they are informed of non-recurrent congestion. To assist traffic operators in implementing real-time control measures (F. Ahmed & Hawas, 2015; Z. Ke, Zou, et al., 2024; Sheu, 2002) and travelers in"}, {"title": "2 Literature Review", "content": "Research on Automatic Incident Detection (AID) began in the 1970s. Early traditional methods included statistical and competitive approaches. Statistical methods compare observed values with historical data, using statistical metrics such as standard normal derivative (SND) (Dudek et al., 1974) and inter-quartile distance (IQD) (Chakraborty et al., 2019) to measure differences. An alarm is triggered when an outlier is observed. However, without direct supervision by incident labels, outliers identified may not be anomalies, resulting in high false alarm rates. Another statistical approach measures the difference between predicted values and real observed values. Alarms are triggered when predictions significantly differ from observations. This is based on the assumption that traffic patterns can be accurately predicted under recurring conditions while not in non-recurring conditions. The forecasting models used are generally time-series, such as autoregressive integrated moving average (ARIMA) (S. A. Ahmed & Cook, 1982) and long short-term memory (LSTM) (Pan, 2022). This method performs well when observations are accurate, and the prediction model has low errors in recurring conditions. However, the same issue persists: these models lack supervised learning with anomaly labels, so outliers may not be anomalies. In fact, the observations are inherently noisy, and recurring conditions cannot ensure low prediction errors, especially during peak hours, resulting in poor detection performance.\nComparative methods collect traffic data such as flow, speed, and occupancy through detectors and compare them with past incident patterns. Alarms are triggered when the collected data matches a pre-defined incident pattern. Representative comparative methods include the California (Payne & Tignor, 1978), Manchester (Persaud & Hall, 1989), and Minnesota (Stephanedes & Chassiakos, 1993) algorithms. These methods are highly inspiring and have pioneered the analysis and extraction of incident characteristics. However, they largely remain at the level of manually summarizing incident characteristics, relying on pre-defined simple structures or patterns to determine whether to trigger an alert. The limited data mining capacity results in a high false alarm rate and mean time to detection.\nDue to the richness and complexity of traffic data, researchers have begun to consider using artificial intelligence for data mining to detect incidents. Cheu and Ritchie (1995) used multi-layer feedforward neural networks (MLF) for incident detection, outperforming traditional methods on simulated data and small field data containing 9 incidents. To further validate feasibility on real-world data, Dia and Rose (1997) used data collected from Tullamarine Highway, Melbourne, demonstrating their feasibility on real datasets. Abdulhai and Ritchie (1999) optimized the model, proposing a Bayesian-based probabilistic neural network (BPNN), and trained the model using data from I-880, California, and I-35W, Minnesota. Jin et al. (2002) further optimized the model by proposing the constructive probabilistic neural network (CPNN) and conducted studies on the same experimental section of I-880. Given the strong performance of support vector machines (SVM) on binary classification problems, Yuan and Cheu (2003) proposed an SVM-based model and validated it on the datasets used by Jin et al. (2002) and Abdulhai and Ritchie (1999). To address the lack of incident samples, Lin et al. (2020) used generative adversarial networks (GANs) to generate more incident samples and use them combined with real incident samples to train an SVM classifier. Similarly, Li et al. (2022) used GANs on the same dataset as Lin et al. (2020) but replaced the SVM classifier with a temporal and spatially stacked autoencoder (TSSAE).\nAll the above research has utilized inductive-loop detectors, which can provide comprehensive data on traffic flow, occupancy, and speed. However, the reliance on loop detectors reduces the generalizability of the methods. For example, in the U.S., most highway segments do not have loop detectors installed. Even when installed, the density of the detectors is often insufficient to meet effective detection requirements. For instance, the loop detectors used by Lin et al. (2020); Yuan and Cheu (2003) and Li et al. (2022) are spaced about every 0.5 miles, which limits the applicability of these methods to only the road segments near the installed detectors. Other studies using traffic cameras (Chakraborty et al., 2018; Ki & Lee, 2007; Singh & Mohan, 2018) or high-precision GPS (Han et al., 2020; Sermons & Koppelman, 1996) also face similar issues. Very few studies have leveraged easily accessible data. Gu et al. (2016) and Zhang et al. (2018) utilized social media data for incident detection. However, social media is inherently a highly noisy form of manual reporting, facing issues such as irrelevance, delays, and missing, similar to traditional incident reports. In contrast, the speed data integrated from probe vehicles used by Sethi et al. (1995) provides an intuitive reflection of traffic conditions and is also easily accessible. Though Balke et al. (1996) pointed out that the limited penetration rate of probe vehicles leads to less prominent results, recent advancements in data collection methods have made the penetration rate of probe vehicle speed data increase. Cheu and Tay (2004) and Asakura et al. (2017) also explored the required penetration rate of probe vehicles to achieve efficient detection through simulation data. It should be acknowledged that probe vehicle speed data still faces high noise issues compared to detector data. As a result, related research remains quite limited. Recent progress using probe vehicle data in AID includes the method"}, {"title": "3 Data Preparation & Feature Engineering", "content": "We first discuss multiple data sources that are commonly available to use for early anomaly detection. Those data are most general in the sense they can be obtained to cover any geographic region and thus generally applicable to all locations. The data can be publicly available, or are offered by a number of data vendors with relatively mature technologies in the present market. In particular, we use incident reports, probe speed (segment level), and weather conditions for this study. Notably, all the information we use is gathered without the need to install additional detectors on the road network."}, {"title": "3.1 Incident Reports", "content": "Incident data can come from Waze and the State Departments of Transportation, which typically provide incident data feeds (real-time and historical). We select incident types that typically cause non-recurrent changes in traffic. Due to system discrepancies, the selected incident types vary slightly among various systems. The selected incidents mainly include accidents, hazardous weather, special events, and crashes. Those incident reports come with geographic locations and report time for each incident, which is translated to a binary indicator, $INC_i(t)$, denotes whether there is an incident report on link i at time t."}, {"title": "3.2 Speed Data", "content": "Raw Traffic Speed: In our experiments, speed data comes from INRIX, which calculates speeds through real-time monitoring of probe vehicles. Similar data can also be obtained from other sources, e.g., HERE, AirSage, TomTom, or telematics data. We use four types of INRIX speed data: 1-minute granularity speed for private vehicles, trucks, and all vehicles, as well as 5-minute granularity speed for all vehicles. To describe the speed on a link, we primarily refer to the 5-minute granularity speed for all vehicles, as it exhibits a lower noise level and provides a comprehensive measurement across different vehicle types.\n$v^i(t) := v^{i,5-min}(t)$\nwhere link i is in the segment defined by TMC (Traffic Message Channels), which is unified across multiple data vendors, $v^i(t)$ is the corresponding segment speed, and $v^{i,5-min}(t)$ is the 5-minute granularity speed for all vehicles of the segment. To impute missing 5-minute granularity speed data for all vehicles, we initially attempt to fill the gaps using the space mean speed from 1-minute granularity data for all vehicles, as described in Equation 3. If all the 1-minute data during that 5-minute interval are also missing, we infer that traffic flow is minimal so that no probe vehicles are present. In such cases, we use the free flow speed, defined as the 85th percentile speed, to fill in the missing data. This approach allows us to compile a complete dataset of 5-minute"}, {"title": "3.3 Weather and Time Information", "content": "Weather: Weather data may be obtained from a few possible sources, e.g., open-meteo.com, NOAA NWS, metomatics.com, etc. We use seven features in a numerical format: temperature, humidity, hourly precipitation, hourly snowfall, hourly snow depth, hourly wind speed, and hourly wind direction.\nTime: Our time features include month, week, day of the week, and time of the day. Since these features are all"}, {"title": "3.4 Data for experiments", "content": "As an experiment implemented in the later part of this paper, incident reports from two regions, Howard County, Maryland, and Cranberry Township, Pennsylvania, were collected for analysis. The data from Howard County spans 2022-2023, while data from Cranberry Township covers 2022-2024. Figure 4 shows the Traffic Message Channels (TMC) network (red lines) and the recorded incident locations (blue dots) for two regions."}, {"title": "4 Methodology", "content": "Our goal is to train an effective model in early anomaly detection/prediction. The model's input consists of multi-source information over a past period, and the output is the anomaly status for a near future period. The incident reports are referenced for anomaly status labelling. To build an effective model, the following issues must be considered.\n*   Rarity and randomness issues of the incident reports: Regarding rarity, it leads to a severe imbalance between anomaly and normality data samples, which would hinder models from learning non-recurrent patterns. On the other hand, randomness may result in inconsistent data distributions between the training and validation/testing sets, posing challenges to model generalization.\n*   Label quality issue: Incident reports inherently have issues such as missing reports, delays, and false reports. Given that data points under anomalies are very scarce, directly using the presence of an incident report as an anomaly label can lead to substantial confusion between anomaly and non-anomaly data, making it difficult for the model to learn/train.\nWe propose a framework combining several strategies to address the above issues. The overall picture of the framework is depicted in Figure 5, where each dashed boxes corresponds to a strategy we used. The framework"}, {"title": "4.1 Sub-Graph Construction", "content": "Constructing a sub-graph for each link segment is our first strategy to address the anomaly rarity issue. Specifically, we build a separate machine learning model for each segment to predict its anomaly status instead of using a single machine learning model to predict all link segments' anomaly status. Though aiming for large-scale generalization, it is difficult to train a model to capture spatially or temporally variant incident probabilities and characteristics across different segments when anomaly data are scarce. Building separate models for each segment can reduce the complexity of the detection task.\nBesides reducing the complexity of the task, the model complexity is also reduced by controlling the input dimensions, which has been theoretically proven to reduce the demand for extensive training samples (Hastie et al., 2009; Vapnik, 2013). The model's input is limited to the traffic conditions within a certain range of upstream and downstream segments (which we refer to as a \"sub-graph\"). This is based on the observation that an incident typically affects the traffic conditions of its upstream and downstream segments (Karim & Adeli, 2002), and this effect generally diminishes with distance. By inputting the sub-graph information, the input-output relevance and model complexity can be balanced."}, {"title": "4.2 Label Denoising by Prior Knowledge", "content": "Using incident reports directly as anomaly labels can introduce significant noise and hinder the convergence process. This is due to the inherent issues within incident reports, such as delays, missing, and inaccuracies. Additionally, not all incidents have a tangible impact on traffic conditions. For example, an incident on a multi-lane highway during off-peak hours might not affect speed or flow, thus resulting in an undetectable incident. This may not be of interest to traffic operators if there is no impact to mobility or safety for the time being. Consequently, using incident reports directly as labels can cause substantial confusion between anomaly and normality labels, diminishing their significance and learnability. To ensure the labels are meaningful and learnable, existing research typically uses a manual process to filter reported incidents; however, this introduces subjectivity and limits their scalability. Most importantly, manually selecting incident reports does not address the issue of missing reports, potentially causing unreported anomalies to be labeled as normal. Therefore, we propose a prior knowledge-based method for labeling anomalies. Besides filtering out insignificant or false reports, unreported incidents are also evaluated and labeled."}, {"title": "4.3 Ahead Labeling", "content": "The algorithm 1 mainly extracts common features from incident reports. However, as shown in Figure 2, incident reports often lack the early stage of incidents. While algorithm 1 can label some missing or delayed reports, they do not address the inherent delay issue in the reporting process. Therefore, we label anomalies starting from a few time steps prior to the reported incident/anomaly time to supplement the lack of early characteristics in the anomaly samples. For example, an anomaly from 7:30 a.m. to 8:30 a.m. can be ahead-labeled by 15 minutes to become from 7:15 a.m. to 8:30 a.m. The algorithm is shown in Algorithm 2. The setting of $d_{ahead}$ balances the inclusion of early features and false anomalies. The larger the $d_{ahead}$, the more early incident features are included, but it also increases the likelihood of normal samples being labeled as anomalies. Practically, this would need to be tuned for each road segment or to the traffic operator's preference. In our experiment, we use 3 as the default value of $d_{ahead}$, which is 15 minutes given the prediction interval is 15 minutes."}, {"title": "4.4 Multi-step Prediction & Data Splitting", "content": "Conventional AID typically formulates its task as a single-step prediction, i.e., determining whether the road segment is currently in incident status. To promote early detection, in our model, we formulate an anomaly detection/prediction task as a multi-step prediction, predicting the anomaly status in the next few steps. In this case, if the model captures the early incident features, even if it may not cause anomaly in this step but a few steps later, the model will still be able to give an alert.\nMulti-step anomaly prediction/detection on past traffic conditions can be formulated as a sequence-to-sequence prediction task. To mitigate the issue of limited samples, we use a sliding window to increase sample numbers by partitioning the dataset into multiple subsections. As illustrated in Figure 6, suppose our study period is from 6 AM to 9 PM, and we use past 1 hour's data to predict the anomaly status for the next half hour, with predictions made every five minutes. Then, the first sample of the day is using data collected between 6:00 AM-6:55 AM to predict the anomaly status between 7:00 AM-7:25 AM; the second sample is using data collected between 6:05 AM-7:00 AM to predict the anomaly status between 7:05 AM-7:30 AM, and so on. Using a sliding window can increase the number of training samples to mitigate the limited data issues."}, {"title": "4.5 Model Training", "content": "This framework is applicable to various classical deep-learning models based on encoder-decoder structures, including Seq2Seq (Sutskever et al., 2014), Seq2Seq models with attention mechanisms (Bahdanau et al., 2014), Transformer (Vaswani et al., 2017), and GraphTrans (Wu et al., 2021). As mentioned in Section 4.1, the traffic status of the sub-graph over the past period is fed into the encoder, and then the decoder outputs the anomaly status of the target link for the future period in an auto-regressive manner.\nWeighted binary cross-entropy (WBCE), as shown in Eq.(9), is used as the loss function for model training, where $w_{ano}$ denotes the additional weight for anomaly samples, $\\hat{y}$ denotes the predicted value, and y denotes the label value (1 for anomaly sample and 0 for normality sample).\n$L = -\\frac{1}{N}\\sum_{i=1}^{N} (w_{ano}y_i log(\\hat{y_i}) + (1 - y_i)log(1 - \\hat{y_i}))$\nWhen $w_{ano} = 1$, Eq.(9) is the binary cross-entropy (BCE), which is commonly used for binary classification training due to its differentiability. The purpose of weighting is to overcome the imbalance in datasets caused by the scarcity of anomaly samples. Without weighting, the predominance of normality samples would bias the model to consistently output zero. For the setting of $w_{ano}$, the default practice is to divide the number of samples whose label are 0 (normality samples) by the number of samples whose label are 1 (anomaly samples). Further adjustment is a trade-off between recall and precision (definitions refer to Figure 7). Increasing the weight helps to improve recall, while decreasing it helps improve precision.\nDuring model training, we employ the teacher forcing technique (Williams & Zipser, 1989) in the auto-regressive decoder to accelerate and enhance convergence. Teacher forcing is a widely adopted strategy in contemporary natural language processing (NLP) model training. This method involves substituting the decoder's prior outputs with actual labels during subsequent computations in the training phase."}, {"title": "4.6 Threshold Adaptation", "content": "The trained model outputs a number between 0 and 1, which can be interpreted as a probability of an anomaly; however, determining when to trigger an alert still requires a threshold. The practice of using 0.5 as a threshold in a balanced dataset assumes that the model can perfectly output values close to 0 and 1. However, due to the setting of $w_{ano}$ and the model's inability to converge perfectly, the 0.5 threshold lacks theoretical basis and may perform poorly. Therefore, it is necessary to tune an appropriate threshold for best model selection and development purposes."}, {"title": "5 Results and Discussion", "content": "This section is divided into two parts: the results of early anomaly detection (with possible prediction) and incident report comparison. In the early anomaly detection part, we use the generated anomaly label (ground truth) as a reference to assess our model's ability to detect or predict significant anomalies in advance. In the incident report comparison part, we will compare our prediction results to raw incident reports (with inaccurate, insignificant, missing, or delayed reports)."}, {"title": "5.1 Early Anomaly Detection/Prediction", "content": "Our experiments were conducted on the two road networks introduced in Section 3. We divided our training, testing, and validation sets in a 7:2:1 ratio in chronological order. We chose the periods that are relatively more challenging to predict: Predictions were made on weekdays, from 6:00 a.m. to 8:30p.m., forecasting the anomaly status for the next half hour.\nWe first show the relationship between recall, precision, F1 score, and accuracy of the model as the threshold changes. Taking the Howard County I-70 segment as an example, Figure 8 shows the recall, precision, and F1 score for predicting anomaly status 5 minutes, 15 minutes, and 25 minutes ahead. The left side shows the results of the validation set, and the right side shows the results of the test set. From the figures, it can observed that the model's performance on the validation set is almost identical to that on the test set. As the prediction period increases, the best F1 score gradually decreases. As the threshold increases, recall increases while precision decreases. Selecting an appropriate threshold allows us to achieve high accuracy and a high F1 score. The range for this threshold is approximately between 0.5 and 0.7. Figure 9 shows the results trained with 15-minute ahead labeling. Similarly, as the prediction period increases, the best F1 score decreases. As the threshold increases, recall improves while precision drops. Selecting a threshold between 0.4 and 0.6 can balance recall and precision, ensuring high accuracy. It is worth noting that the ground truth differs between the cases with and without ahead labeling, so directly comparing these evaluation metrics between Figure 8 and Figure 9 lacks significance."}, {"title": "5.2 Incident Report Comparison", "content": "In this section, we will compare our model's prediction results with incident reports and conventional AID methods. Both incident reports and conventional AID methods use a single value to describe whether a road is in an incident state at a given time step. However, to achieve earlier detection and provide operators with a reference for road conditions over a future period (rather than just a single point in time), we employed multi-step predictions, forecasting road conditions for the next half hour. To compare multi-prediction with a single value, we adopted a conservative strategy by taking the minimum value among these six predictions as the measurement standard. If this minimum value exceeds the threshold, an alert is triggered; otherwise, no alert is triggered."}, {"title": "5.2.1 Detection Examples", "content": "Using I-70 as an example, our prediction process during the evening period on 2023-01-11 is detailed in Appendix A. In this example, we use our model trained with the ahead labelling strategy. In Appendix A, the red dashed line at 0.56 represents the adaptive threshold the validation set generated. At 17:15, our model's predictions for the next six time steps are all below the threshold, so no alert is triggered (the blue line is used to indicate no alert). However, at 17:25, the minimum value exceeds the threshold, triggering an alert (the red line is used to indicate an alert). This alert ends at 17:45, as the minimum predicted value falls below the threshold at that time. The black text in Appendix A represents the current speed at the predicted time, while the red shading indicates the report time. From this, it can be observed that our model triggered an alert earlier than the incident report and even before the speed significantly dropped."}, {"title": "5.2.2 Examples of detecting anomalies and possibly unreported incidents", "content": "The above are two examples of direct prediction processes. During the periods mentioned, there were incident reports, and our model also issued alerts. However, there were also instances where no reports were present, but the alert was triggered ('False Positive' Cases), and cases where an incident report appeared later, but our model did not trigger an alert ('False Negative' Cases). Figures 13 and 14 illustrate these two kinds of cases, respectively.\nRegarding false positive cases, we can observe in the four examples in Figure 13 that, despite no incident reports corresponding to the times on the dates indicated by the blue lines, there are clear anomalies when compared to recurrent patterns (represented by the orange and green dashed lines). Our model effectively captured these anomalies. As mentioned in the introduction, there is a significant missing issue with incident reports. These false alarms are likely due to the absence of incident reports, which underscores the importance of training with anomaly labels rather than relying solely on incident reports."}, {"title": "5.2.3 Overall Results", "content": "The above are specific case analyses. To demonstrate the generalizability of our model, Table 4 shows the testing results for 10 segments across two road networks. The metrics used are defined as follows:\n$DR \\ (Detection \\ Rate) = \\frac{N_{detected\\_incident}}{N_{incident\\_report}}$\n$MTTD\\ (Mean\\ Time\\ to\\ Detection) = \\frac{\\sum_{i=1}^{N_{detected \\_incident}} (t_{alarm} - t_{report})}{\\sum_{i=1}^{N_{detected \\_incident}}}$\n$FAR \\ (False \\ Alarm \\ Rate) = \\frac{N_{non-incident-non-anomaly}}{N_{alarm}}$\n$DR \\ (S) \\ (Detection \\ Rate, Significant) = \\frac{N_{detected \\_significant\\_incident}}{N_{significant \\_incident\\_report}}$\n$MTTD \\ (S) \\ (Mean \\ Time \\ to \\ Detection, Significant) = \\frac{\\sum_{i=1}^{N_{detected \\_significant\\_incident}} (t_{alarm} - t_{report})}{N_{detected \\_significant\\_incident}}$"}, {"title": "6 Conclusion and Future Work", "content": "In this study, we address an early anomaly detection or prediction problem that is more practically useful and scalable than conventional AID models. We propose a method that combines incident reports and traffic domain knowledge, i.e., the correlation between slowdown speed and incident, to generate anomaly labels and train a deep learning model for early anomaly detection. This method aims to overcome inherent issues in incident reports, including false reports, insignificant incident reports, delayed reports, and missing reports. We adopted ahead labeling and multi-step prediction strategies, allowing the model to predict incidents earlier than traditional methods. Additionally, we applied various machine learning strategies to address the issues of insufficient anomaly samples and data imbalance. Unlike existing research in the field of AID, our method aims to capture a broader range of anomalies beyond after the fact incident reports (including early incident characteristics and unreported anomalies) and more importantly, offer early alerts of anomalies that potentially influence the traffic flow in the short term. Furthermore, our method is highly scalable as it relies solely on ubiquitously available data. We validated our model on ten road sections in two road networks, achieving results that surpassed the baselines. This experiment shows a good trade-off between precision and recall, and resulting anomaly alerts 4-42 min earlier than Waze reports.\nAlthough the data used in this study is theoretically available in real-time, we need to coordinate with data providers to ensure we can access the real-time data required by the model as early as possible. This will maximize the model's advantage in early prediction/detection for potential future applications. Besides, this study also has certain limitations that can be improved. First, regarding the selection of prior knowledge, we mainly chose edges with a high correlation between the occurrence of incidents and high slowdown speeds. In fact, due to the topological structure, not all edges have this correlation. Bases on our definition of slowdown speed, this high correlation requires that the segment be relatively long and that most incidents do not occur at the very beginning of each segment. For those that do not exhibit this high correlation, we may need to consider redefining the slowdown speed or using other values, such as the travel time index. Semi-supervised learning is also a potential solution for label denoising. Moreover, our algorithm is also difficult to handle for segments where only one or two incidents occur per year, as the validation and test sets may not contain any incidents. This might require using simulator data to generate sufficient training samples and finetuning with real-world incident samples. Additionally, we can tune a reasonable ahead-labeling period, which is also a direction worth exploring in future research."}]}