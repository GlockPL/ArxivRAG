{"title": "Loop-Residual Neural Networks for Iterative Refinement", "authors": ["Kei-Sing Ng", "Qingchen Wang"], "abstract": "The success of large-scale language models like GPT can be attributed to their ability to efficiently predict the next token in a sequence. However, these models rely on constant computational effort regardless of the complexity of the token they are predicting, lacking the capacity for iterative refinement. In this paper, we introduce a novel Loop-Residual Neural Network, which achieves better performance by utilizing longer computational time without increasing the model size. Our approach revisits the input multiple times, refining the prediction by iteratively looping over a subset of the model with residual connections. We demonstrate the effectiveness of this method through experiments comparing versions of GPT-2 with our Loop-Residual models, showing improved performance in language modeling tasks while maintaining similar parameter counts. Importantly, these improvements are achieved without the need for extra training data.", "sections": [{"title": "1 Introduction", "content": "The transformer architecture has revolutionized natural language processing, enabling models like GPT to predict the next token in a sequence efficiently (Vaswani et al., 2017; Radford et al., 2019). Despite their success, these models perform a one-pass projection of all previous tokens to predict the next token, which limits their capacity for iterative refinement. Specifically, they rely on constant computational effort regardless of the complexity or ambiguity of the token being predicted.\nIn this paper, we introduce a novel Loop-Residual Neural Network, which revisits the input multiple times, refining the prediction by iteratively looping over a subset of the model with residual connections. Our main contribution is improving transformer performance with longer inference times, using a novel loop architecture with residual prediction. This approach works for large neural networks without needing extra training data, effectively extending the model's approximation capacity.\nOur contributions are significant for the following reasons:\n\u2022 Novel Architecture: We introduce a loop-residual mechanism that enhances model performance without increasing parameter count, making it accessible for researchers with limited computational resources.\n\u2022 Efficiency: By utilizing longer inference times, our model achieves better performance without the need for extra training data, contrasting with approaches that rely on extensive data augmentation.\n\u2022 Scalability: Our method is applicable to large-scale neural networks, demonstrating effectiveness on models comparable to GPT-2.\nWe demonstrate the effectiveness of this method through experiments comparing versions of GPT-2 with our Loop-Residual models, showing that our GPT-2-81M model achieves a validation loss of 3.11 on the OpenWebText dataset (Gokaslan and Cohen, 2019), comparable to the GPT-2-124M model's loss of 3.12. These results emphasize the benefit of utilizing longer inference times to improve performance without increasing model size or requiring additional data."}, {"title": "2 Background and Related Work", "content": "Traditional neural networks, including transformers, map a sequence of inputs to a prediction in a single forward pass, processing inputs through multiple layers to refine internal representations. The transformer architecture relies on self-attention mechanisms and feed-forward networks to capture dependencies in the data (Vaswani et al., 2017).\nResidual connections, introduced by He et al. (2016), have been instrumental in enabling the training of very deep networks by alleviating the vanishing gradient problem. Our Loop-Residual model leverages this concept by focusing on refining the hidden state through iterative loops, allowing the model to stabilize training and accelerate convergence without increasing its size significantly."}, {"title": "2.1 Iterative Refinement and Adaptive Computation", "content": "Several prior works have explored iterative refinement and adaptive computation in neural networks."}, {"title": "2.1.1 Universal Transformers", "content": "Dehghani et al. (2019) introduced the Universal Transformer, which applies the transformer layers recur- rently to capture both short-term and long-term dependencies. Their model iteratively refines representa- tions by looping over the entire transformer block. However, their experiments were primarily conducted on smaller models and datasets, such as the WMT English-German translation task, and did not explore large-scale language models like GPT-2. Additionally, their design does not incorporate a predictive residual mechanism as we do, which is crucial for stabilizing training in large-scale models."}, {"title": "2.1.2 Adaptive Computation Time Models", "content": "Graves (2016) proposed Adaptive Computation Time (ACT) for recurrent neural networks, allowing models to dynamically decide the number of computational steps per input. While ACT introduces adaptive compu- tation, it was mainly applied to simple RNN architectures and tested on small-scale tasks, without leveraging the transformer architecture or large-scale pretraining. Moreover, ACT does not employ a predictive residual design, which differentiates our approach."}, {"title": "2.1.3 Depth-Adaptive Transformers", "content": "Elbayad et al. (2019) presented Depth-Adaptive Transformers that adjust the depth of the network based on the input. Their method enables dynamic inference by selecting the number of layers to apply per input sequence. However, their approach focuses on varying depth during inference rather than iterative refinement during training, and their experiments were conducted on machine translation tasks with relatively small models. Importantly, their model lacks the predictive residual design present in our Loop-Residual architecture."}, {"title": "3 Methodology", "content": "The Loop-Residual model introduces an iterative mechanism where a subset of layers or components is looped through multiple times for iterative refinement. Specifically, instead of passing the input through all layers sequentially, the model loops over a part of the network, predicting the residual between the current state and the desired state at each iteration."}, {"title": "3.1 General Loop Structure with Residual Design", "content": "Our core design involves refining the hidden state by iteratively looping over transformer blocks with residual connections. The process is defined as:\n $x^{(n)} = x^{(n-1)} + f_\\theta (x^{(n-1)})$"}, {"title": "3.2 Comparison with Existing Methods", "content": "Our approach differs from existing methods by explicitly focusing on predicting the residual at each iteration and adding it back to the hidden state. Unlike Universal Transformers that loop over the entire transformer block without residual prediction, our method leverages residual connections within the looping mechanism to stabilize training and enhance convergence, especially in large-scale models."}, {"title": "4 Experiments", "content": "We conducted experiments to evaluate the effectiveness of our Loop-Residual Neural Network in improving transformer performance using longer inference times without increasing model size. Our experiments focus on comparing our models with standard GPT-2 models of similar or larger sizes on the Open WebText dataset."}, {"title": "4.1 Experimental Setup", "content": "In our first experiment, we compared a Loop-Residual GPT-2 model with 81 million parameters (GPT2- 81M) to the standard GPT-2 model with 124 million parameters (GPT2-124M). The GPT2-124M model consists of 12 transformer layers, serving as our baseline. Our Loop-Residual GPT2-81M model employs 6 loops over 6 transformer layers, effectively simulating deeper computation without increasing the number of parameters. The objective of this experiment is to demonstrate that our model achieves comparable or better performance using fewer parameters by leveraging longer inference times through iterative refinement. In our second experiment, we focused on smaller-scale models to illustrate the effectiveness of our method at different scales. We compared a Loop-Residual GPT-2 model with 45 million parameters (GPT2-45M) to a Lite GPT-2 model of the same size (GPT2-45M-Lite). The GPT2-45M-Lite model consists of a single transformer block layer, performing one-pass prediction, and serves as the baseline. Our Loop-Residual GPT2-45M model loops twice over a single transformer block to predict the residual. The objective here is to illustrate that even at smaller scales, our method can improve performance over the baseline without increasing model size by leveraging iterative refinement.\nWe selected these experimental settings to evaluate the efficacy of our loop-residual mechanism across different model sizes and to show that iterative refinement can compensate for a reduced number of param- eters or layers. By keeping the total number of parameters constant while adjusting the depth via looping, we aim to isolate the effect of iterative refinement on model performance.\nIn all experiments, we used the same training data and hyperparameters where applicable to ensure a fair comparison. The models were trained on the Open WebText dataset, which provides a diverse range of textual data suitable for language modeling tasks. We measured the training epoch times for our models and the standard GPT-2 models to assess the computational overhead introduced by our method. The training epoch times were 150ms for GPT2-45M-Lite, 177ms for our Loop-Residual GPT2-45M, and 1,377ms for GPT2-81M."}, {"title": "4.2 Evaluation Metrics", "content": "We evaluate the models based on the cross-entropy loss on the training dataset (Training Loss) and the cross-entropy loss on a held-out validation dataset (Validation Loss). These metrics provide a standard measure of model performance in language modeling tasks, allowing us to assess the effectiveness of our approach in reducing prediction error."}, {"title": "5 Results", "content": "Our experiments demonstrate that the Loop-Residual Neural Network significantly improves performance over standard transformer models by leveraging longer inference times without increasing model size."}, {"title": "5.1 First Experiment: GPT2-81M vs. GPT2-124M", "content": "Our Loop-Residual GPT2-81M model achieved a validation loss of 3.11 on the OpenWebText dataset, com- parable to the GPT2-124M model's loss of 3.12. This result is notable because our model uses approximately 35% fewer parameters and half the number of unique layers compared to the GPT2-124M model. The im- provement indicates that iterative refinement through our loop-residual mechanism effectively enhances the model's approximation capacity, allowing it to match or exceed the performance of larger models."}, {"title": "5.2 Second Experiment: Loop-Residual GPT2-45M vs. Lite GPT2-45M", "content": "In the comparison between the Loop-Residual GPT2-45M model and the Lite GPT2-45M model, our model achieved a validation loss of 3.67 compared to 3.98, and a training loss of 3.65 compared to 3.96. By looping twice over a single transformer block, our model effectively simulates a deeper network, resulting in a substantial performance gain over the one-pass baseline. This demonstrates that even at smaller scales, our method can improve performance without increasing model size by leveraging iterative refinement."}, {"title": "5.3 Discussion", "content": "The results indicate that our Loop-Residual Neural Network architecture effectively leverages iterative re- finement to enhance model performance without increasing the parameter count. In the first experiment, despite having approximately 35% fewer parameters and half the number of unique layers compared to the GPT2-124M baseline, our Loop-Residual GPT2-81M model achieves a slightly better validation loss. This suggests that the iterative loops over the transformer blocks allow the model to capture complex patterns and dependencies that would otherwise require additional layers or parameters. In the second experiment, the Loop-Residual GPT2-45M model significantly outperforms the GPT2- 45M-Lite baseline. By looping twice over a single transformer block, our model effectively simulates a deeper network, resulting in a reduction of the validation loss from 3.98 to 3.67. This demonstrates that even at smaller scales, iterative refinement can compensate for a limited number of layers, enhancing the model's expressiveness.\nThe increased training epoch times for our Loop-Residual models are relatively modest compared to the performance gains. For instance, the training time for the Loop-Residual GPT2-45M model is only 18%"}, {"title": "6 Conclusion", "content": "We have presented the Loop-Residual Neural Network, a novel approach that enables smaller neural network models to achieve better results on lower-end devices by leveraging longer inference times through iterative refinement. By iteratively refining the hidden state through residual connections over multiple loops of a model subset, our method captures complex patterns and dependencies more effectively than conventional one-pass models. Our experiments demonstrate that the Loop-Residual models can achieve improved per- formance over baseline models of the same size and comparable performance to larger models with fewer parameters. Significantly, these performance improvements are achieved without the need for additional training data, highlighting the efficiency and accessibility of our approach. This work opens up new possibil- ities for neural network architectures, particularly for tasks that benefit from deeper computational reasoning on resource-constrained devices."}]}