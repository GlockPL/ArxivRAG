{"title": "EMBEDLLM: LEARNING COMPACT REPRESENTATIONS OF LARGE LANGUAGE MODELS", "authors": ["Richard Zhuang", "Tianhao Wu", "Zhaojin Wen", "Andrew Li", "Jiantao Jiao", "Kannan Ramchandran"], "abstract": "With hundreds of thousands of language models available on Huggingface today, efficiently evaluating and utilizing these models across various downstream tasks has become increasingly critical. Many existing methods repeatedly learn task-specific representations of Large Language Models (LLMs), which leads to inefficiencies in both time and computational resources. To address this, we propose EmbedLLM, a framework designed to learn compact vector representations of LLMs that facilitate downstream applications involving many models, such as model routing. We introduce an encoder-decoder approach for learning such embeddings, along with a systematic framework to evaluate their effectiveness. Empirical results show that EmbedLLM outperforms prior methods in model routing both in accuracy and latency. Additionally, we demonstrate that our method can forecast a model's performance on multiple benchmarks, without incurring additional inference cost. Extensive probing experiments validate that the learned embeddings capture key model characteristics, e.g. whether the model is specialized for coding tasks, even without being explicitly trained on them. We open source our dataset, code and embedder to facilitate further research and application.", "sections": [{"title": "INTRODUCTION", "content": "Recent breakthroughs in Large Language Models (LLMs) (Vaswani et al., 2023) have led to the creation of a vast array of models, each tailored for different use cases. These models, ranging from small, specialized models to large, general-purpose systems (Hao et al., 2022), differ significantly in their architecture, size, training data, and performance characteristics. For example, while some models excel as conversational agents, others may be more suitable for code generation or logical reasoning tasks. However, with this explosion of diverse LLMs comes a major challenge:\n\nHow to efficiently manage, compare, and utilize the growing number of LLMs?\n\nTraditionally, benchmarking has served as the primary method for comparing LLMs, where each model is evaluated on a fixed set of test cases, and a score is generated to represent its performance. Meanwhile, model routing systems are developed to efficiently select models given queries of different types. An example workflow of these tasks can be seen in Figure 2 and Figure 3. While these approaches are often robust indicators of a model's strengths and weaknesses, the construction of their workflows induces repeatedly learning representations of various LLMs to suit each individual downstream tasks and is therefore time-consuming and compute-demanding.\n\nIn response to these challenges, we introduce EmbedLLM, a compute-friendly framework designed to learn compact vector representations of large language models that facilitates different tasks. EmbedLLM map models into a latent vector space that captures important model characteristics. More importantly, EmbedLLM produces a unified representation that can be simultaneously applied to various downstream tasks such as correctness forecasting (Section 5.1), model routing (Section 5.2), and benchmark accuracy evaluation (Section 5.3). The core idea is to enforce this representation learning through a reconstruction-based system that tries to predict the model's answer (correct-"}, {"title": "RELATED WORK", "content": "Representation Learning There have been numerous attempts to learn representations of various types of information. For natural languages, Mikolov et al. (2013a) and Pennington et al. (2014) rev-olutionized the way models capture word semantics. In the field of computer vision, self-supervised techniques (Noroozi & Favaro, 2017) (Vondrick et al., 2018) are designed to learn low-dimensional representations that bolster downstream classification or segmentation performances. Inspired by these work and realizing an increasing demand of various LLMs being trained, we propose a cre-ative framework to learn embeddings of LLMs.\n\nLLM Benchmarking Benchmarking has been a standard way to compare LLMs, where a collec-tion of questions/prompts is input to the LLMs and the quality of the corresponding responses is evaluated. There are datasets curated for general knowledge and language understanding (Wang et al., 2019), as well as domain-specific ones evaluating reasoning abilities (Frohberg & Binder, 2022), narrative control (Hartvigsen et al., 2022), function-calling (Yan et al., 2024; Srinivasan et al., 2023) and instruction-following (Li et al., 2024; Dubois et al., 2024). However, assessing quality of benchmarks remains an under studied question, and systematically evaluating all of them incurs an enormous amount of inference cost.\n\nLLM Routing Our work focuses on predictive routing, which is a technique aimed at proposing the most suitable model given a task, without actually passing the query through each one of them. As"}, {"title": "FORMULATION", "content": "Let $\\mathcal{M} = {M_1, M_2\u00b7\u00b7\u00b7 M_n}$ be a set of different LLMs, $\\mathcal{P}$ denote the set of all possible prompts, and $\\mathcal{A}$ denote the corresponding set of possible answers. We can simply identify any LLM M with an inference function mapping from a prompt space to an answer space $f_M: \\mathcal{P} \\rightarrow \\mathcal{A}$, which outputs an answer $a \\in \\mathcal{A}$ given a prompt $p \\in \\mathcal{P}$.\n\nAmong downstream tasks, representations of different LLMs needed to be constructed in vari-ous ways. A naive example is benchmarking: Where the crucial part is to select a test prompt set $\\mathcal{P}_{Bench} = {p_1,p_2\u00b7\u00b7\u00b7p_m}$ as well as an scoring function $g_{eval} : \\mathcal{P} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$, mapping model responses to a scalar score. Take MMLU as an example, each model is queried by a set of multiple-choice questions with four choices. The model's response is recorded by comparing the output probabilities of the answer choices \u201cA\u201d, \u201cB\u201d, \"C\", and \"D\". Then the responses are simply scored by matching with the ground truth answer key. Within this process, every model M's behavior is essentially summarized by their output on a set of test questions${(p_1, a_{(M,1)}), (p_2, a_{(M,2)}) \u00b7\u00b7\u00b7 (p_m, a_{(M,m)})}$."}, {"title": "EVALUATION METRICS", "content": "Evaluating the quality of model embeddings is crucial to ensure they effectively capture the under-lying structure and semantics of the data which we care about for potential downstream tasks. The core idea is to use the embeddings to predict model behavior on unseen tasks by training an infer-ence function $\\varphi : \\mathbb{R}^d \\times \\mathcal{P} \\rightarrow \\mathcal{Q}$, that leverages a model embedding and new test prompts to predict a model's performances as quantified by a desired evaluation metric in the space $\\mathcal{Q}$. For instance, if"}, {"title": "EMBEDLLM", "content": "In order to learn such embeddings, we draw inspirations from image reconstruction algorithms (He et al., 2022; Ronneberger et al., 2015): we want to learn a \"reconstruction\" system, where the choice of the reconstruction target is arbitrary and can be task-dependent. More concretely, let $\\mathcal{Q}$ be a space of model performance metric, and $\\mathcal{M}$ be a set of possible LLMs, m be the number of models, n be the number of questions/prompts. We want to learn a reconstruction network $\\mathcal{R}: \\mathcal{Q}^{m \\times n} \\rightarrow \\mathcal{Q}^{m \\times n}$ that optimizes to reconstruct a matrix $X \\in \\mathcal{Q}^{m \\times n}$, where $X_{ij} \\in \\mathcal{Q}$ denotes the model performance metric for model i on question j. The encoder-decoder architecture ensures the imposure of such constraint and enforce the model embeddings to efficiently capture the key characteristics of each model.\n\nIn this work, we decide to use the task of predicting model answer correctness as our auxiliary target, i.e., $\\mathcal{Q} = {0, 1}$. Notice that our training objective is only a decoy - any downstream task that requires understanding of model characteristics would qualify and our ultimate goal is to enforce the reconstruction network to learn a compact yet information-rich representation of the models in this process."}, {"title": "DATASET", "content": "We now describe the data collection process of correctness results of various LLMs' responses to questions from mainstream benchmarks.\n\nWe selected 112 open-sourced models\u00b2 of various sizes, with both general purpose LLMs (Zhu et al., 2024) and specialized LLMs included to ensure comprehensive coverage. Then we aggregated responses of every model to 36,054 questions from the test sets of MMLU (Hendrycks et al., 2021), TruthfulQA (Lin et al., 2022), SocialQA (Sap et al., 2019), PIQA(Bisk et al., 2019), MedMCQA(Pal et al., 2022), MathQA(Amini et al., 2019), LogiQA(Liu et al., 2020), GSM8K(Cobbe et al., 2021), GPQA(Rein et al., 2023), and ASDiv(Miao et al., 2020). The responses to these questions were acquired and evaluated through using the \"lm-evaluation-harness\u201d package (Gao et al., 2023) to give a binary correctness label for each model-question pair. We performed a random 80%-10%-10% train-validation-test split on the questions and used the sentence transformer \u201call-mpnet-base-v2\" (Reimers & Gurevych, 2019) to convert the questions into an initial embedding state of dimension $dim_q = 768$. Consequently, our question embedding tensor X has the shape (36054, 768) where the i-th row $X_i \\in \\mathbb{R}^{dim_q}$ is the embedding for model i, and our label tensor is essentially a correctness matrix Y with shape (112, 36054), where the ij-th entry $Y_{ij}$ represents the binary correctness of model i answering question j.\""}, {"title": "ALGORITHM", "content": "We adopt the Encoder Decoder architecture:\n\nEncoder: The encoder consists of a model embedding network and a question embedding network. Let $dim_{embed}$ be the desired dimension of the model embedding. The model embedding network $\\Phi_m : \\mathcal{M} \\rightarrow \\mathbb{R}^{dim_{embed}}$ maps each model into a latent representation. Similarly, the question embedding network $\\varphi_q : \\mathcal{P} \\rightarrow \\mathbb{R}^{dim_{embed}}$ maps each questions into a latent representation. In our setting, the question embedding network is a two-step transformation $\\varphi_q = g_{ST} \\circ h_{proj}$ where $g_{ST} : \\mathcal{P} \\rightarrow \\mathbb{R}^{dim_q}$ denotes the pre-processing (performed in Section 4.2) that turns each question from text to an initial embedding space by using sentence transformer, and $h_{proj}: \\mathbb{R}^{dim_q} \\rightarrow \\mathbb{R}^{dim_{embed}}$ is a projection layer from this original embedding space to the same space as the model embeddings.\n\nDecoder: The decoder is a classifier network $\\psi : \\mathbb{R}^{dim_{embed}} \\times \\mathbb{R}^{dim_{embed}} \\rightarrow {0, 1}$ that takes in both the encoded embeddings of the model and the question, and output a binary label which is the prediction of whether the model answer the question correctly. For this work, our decoder is represented as $\\psi(v_m, v_q) = (U v_m v_q)$ where $\\theta : \\mathbb{R}^{dim_{embed}} \\rightarrow \\mathbb{R}^2$ is a linear classifier and $u v$ represents the Hadamard (element-wise) product between two vectors. For each model-question pair, this decoder network outputs two logits $p_{(m,q)_0}$, and $p_{(m,q)_1}$, and the \u201ccorrectness score\" $S_{m,q} = \\sigma(p_{(m,q)_1} - p_{(m,q)_0})$ represent the predicted probability of the model m correctly answering question q, where $\\sigma(x)$ is the sigmoid function.\n\nSuppose y is the correctness label of model m answering question q, we calculate the following BCE loss function during training,\n\n$\\mathcal{L}(m, q, y) = (y \\cdot log(s_{m,q}) + (1 - y) \\cdot log(1 - s_{m,q}))$ (1)"}, {"title": "EXPERIMENT RESULTS", "content": "As described in Section 3.2, we conducted experiment in correctness forecasting, model routing, and benchmark accuracy prediction to evaluate the quality of the learned embeddings."}, {"title": "CORRECTNESS FORECASTING", "content": "For correction prediction, we compare the effect of our matrix factorization model to a KNN-classifier (Fix, 1985). In the context of our formulation, although without an explicit embeddings for models, KNN-classifier can be seen as using the integration of all question-correctness tuples from a model as its \u201cembedding\u201d, and making inference from this aggregation. For brevity, we refer to this approach as KNN in the subsequent text. As mentioned in Section 3.2, we use correctness forecasting accuracy on the test set as the evaluation metric.\n\nWe evaluate the performance of KNN and Matrix Factorization across various sizes of training set. We produce the smaller training sets from randomly subsetting from the full training set. For each training set, we conduct hyperparameter tuning (number of neighbors for KNN, model embedding dimension for MF) on a fixed validation set and evaluate prediction accuracy using a fixed test set. The result in Table 1 indicates a better scalability of our method."}, {"title": "MODEL ROUTING", "content": "Using the same correctness data, we can evaluate the quality of a matrix-factorization-based router. For this task, we evaluate the router's accuracy by measuring the proportion of times it successfully route to a model that could correctly answer the given query. For a test question $q_k$, we pass it through the router network along with all n possible model embeddings, producing n correctness score $s_{M_1,q_k}, s_{M_2,q_k}, \u00b7\u00b7\u00b7, s_{M_n,q_k}$, and check if the model with the highest correctness score cor-rectly answers the question. Aggregating through all questions, we report the router accuracy as\n\n$ACC_{router} = \\frac{1}{m}\\sum_{k=1}^{m} \\mathbb{1}_{{X_{i^*k} = 1}}$ where $i^* = arg \\max{s_{M_i,q_k}| i \\in {1\u00b7\u00b7\u00b7n}}$ is the routed model and X is the correctness matrix described in Section 4.2.\n\nWe compare the performance of the Matrix Factorization router with two baselines. The first one is the single-best model router which always selects the model with the highest accuracy and thus gives a constant accuracy. The second one is a random router that select each model the same number of times as the Matrix Factorization router, but instead randomly assign models to questions. For this router, we can calculate its expected accuracy given the proportions of times each model is selected by the Matrix Factorization router. For instance, if our router selects $M_1$ 70% of the time, $M_2$ 20% of the time, and $M_3$ 10% of the time, the expected accuracy of the random router will be calculated as a weighted accuracy $acc_{weighted} = 0.7 * acc_{M_1} + 0.2 * acc_{M_2} + 0.1 * acc_{M_3}$. Note that this weighted accuracy will always be smaller than the single-best model accuracy - we propose this metric as an evaluation of how well our router direct models to the questions they are good at given a fixed \"budget\" of model calls. We report both the overall accuracy across the whole test set and accuracy per source benchmark."}, {"title": "BENCHMARK ACCURACY PREDICTION", "content": "To predict model's average accuracy on a benchmark $\\mathcal{B}$, we trained MF using leave-one-out cor-rectness data, which includes correctness results of all models on all questions except the ones in $\\mathcal{B}$. Then we take the model embeddings directly as features to train a linear regression of the form:\n\n$\\mathbf{aE = y}$\n\nwhere $\\mathbf{E}$ is a model embedding matrix with the i-th row representing the model embedding for the i-th model, and the j-th entry in the vector $\\mathbf{y}$ corresponds to the j-th model's average correctness accuracy on the test benchmark, which is a number from 0 to 1.\n\nFor each test benchmark, we conducted 100 random train-test splits on the 112 models contained in our dataset, trained a linear regression on the training set, and evaluated the correlation between model embeddings and test benchmark performances on the test set through applying Kendall's Tau test4. From Section 5.3, statistical significance is found in 7 out of the 10 benchmarks, indicating that model embedding contains information to distinguish between model performances on most benchmarks."}, {"title": "SANITY CHECK USING SIMILARITY", "content": "We expect the model embeddings to satisfy some basic properties: If two models M, M' generate the same answers for every prompt, then their embeddings are the same. Similarly, models with similar characteristics, trained using similar data, or adopted similar training pipelines should have similar embeddings, and vice versa. For instance, the model embedding of DeepSeekMath-7B (Shao et al., 2024) should be more similar to the embedding of other math models like MetaMath-Llemma-7B (Yu et al., 2023) than to the embedding of Medicine-LLM-13B (Cheng et al., 2024) which is adapted for biomedical applications. This property is easily fulfilled by Matrix Factorization algorithm as any two identical/similar models of such would produce identical/similar correctness result against most questions.\n\nAs a further sanity check, we assign binary labels to the 112 models we have evaluated according to the following 6 keywords: [7B, 13B, 70B, Coding, Bio/Med, Physics], forming 6 characteristic communities. For each community, we compare between the average intra-community and inter-community L2 distance of the embeddings. As shown in Figure 6, for all above 6 communities, the averaged intra-community L2 distances are smaller than the inter-community ones. This provides a preliminary guarantee that our embeddings are \u201cmeaningful\u201d with respect to distance metrics."}, {"title": "EMBEDDINGS CAPTURE INTRINSIC CHARACTERISTICS OF BENCHMARKS", "content": "Next, as indicated from Section 5.3, as a set of model embeddings is produced from a fixed training set, the embeddings seem to capture information of some benchmarks better and over-look information in some benchmarks. Hence, we design a set of ablation experiments to fur-ther understand the contribution of each benchmarks in the training data. Specifically, extend-"}, {"title": "LIMITATION", "content": "Despite the promising results, our work has several limitations. First, our dataset, though effective in demonstrating the potential of our embeddings with a limited number of samples, is relatively small. With data from only 112 models, the embeddings we extract are moderately sparse which limits deeper exploration of relationships between them. Second, while our embeddings effectively support various downstream tasks, they rely on a fixed pool of models. Introducing new models requires retraining the Matrix Factorization algorithm. Lastly, our study is restricted to correctness-based datasets, leaving other potentially valuable data types, such as text embeddings of model outputs, unexplored. To address these limitations, we have open-sourced our datasets and codebase for further research and experimentation."}, {"title": "CONCLUSION", "content": "We showcase the possibility of learning an unified, compact representation of LLMs via Matrix Factorization. Through extensive empirical evaluation, our method displays solid performance on correctness forecasting, model routing, and benchmark accuracy prediction, while significantly re-ducing the need of retraining and avoiding repetitive evaluations. Furthermore, we conduct various probing experiment to understand the information contained in the model embeddings. The results show that our embeddings capture not only key characteristics of the models, but also properties of the data used to train the embedder."}, {"title": "APPENDIX", "content": "MODEL LIST\n\nHere is an exhaustive list of models that we extract our dataset from:"}]}