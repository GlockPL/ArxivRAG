{"title": "Dialogue Systems for Emotional Support via Value Reinforcement", "authors": ["Juhee Kim", "Chunghu Mok", "Jisun Lee", "Hyang Sook Kim", "Yohan Jo"], "abstract": "Emotional support dialogue systems aim to reduce help-seekers' distress and help them overcome challenges. While human values-core beliefs that shape an individual's priorities\u2014are increasingly emphasized in contemporary psychological therapy for their role in fostering internal transformation and long-term emotional well-being, their integration into emotional support systems remains underexplored. To bridge this gap, we present a value-driven method for training emotional support dialogue systems designed to reinforce positive values in seekers. Our model learns to identify which values to reinforce at each turn and how to do so, by leveraging online support conversations from Reddit. The model demonstrated superior performance in emotional support capabilities, outperforming various baselines. Notably, it more effectively explored and elicited values from seekers. Expert assessments by therapists highlighted two key strengths of our model: its ability to validate users' challenges and its effectiveness in emphasizing positive aspects of their situations-both crucial elements of value reinforcement. Our work validates the effectiveness of value reinforcement for emotional support systems and establishes a foundation for future research.", "sections": [{"title": "1 Introduction", "content": "Emotional support aims to help individuals (seekers) in addressing everyday emotional difficulties, such as relationship conflicts and workplace stress, by offering reassurance, acceptance, and encouragement (Atoum and Al-Shoboul, 2018; Burleson, 2003). Recent advancements in large language models have accelerated the development of dialogue systems designed to provide emotional support (supporters) (Deng et al., 2024; Zhang et al.,"}, {"title": "2 Related Work", "content": "Recent advances in language models have broadened their use in emotional support dialogue systems. To enhance supporter models, researchers have explored various approaches. One method uses large language models (LLMs) to generate diverse conversations for supporter model training (Zheng et al., 2024; Liu et al., 2023; Qiu et al., 2024). Other studies predict seekers' future states to refine support model training (Zhou et al., 2023; Cheng et al., 2022; Shin et al., 2020). Recent efforts also leverage multi-turn simulations with user simulators to capture nuanced reactions. For instance, Deng et al. (2024) used seeker simulators to predict future responses and train policy planners guiding supporter models. However, most studies"}, {"title": "3 Value Effects in Emotional Support", "content": "This section explores the significance of value reinforcement in effective emotional support, providing the foundation for our research."}, {"title": "3.1 Taxonomy for Human Values", "content": "In this study, we adopt the value taxonomy introduced by Kiesel et al. (2022), which integrates the Schwartz, Theory of Basic Values (Schwartz et al., 2012) with three other major value lists (Rokeach, 1973; Brown and Crace, 2002; Haerpfer et al., 2020). The Schwartz Theory of Basic Values has been extensively used in prior research across both NLP (Kang et al., 2023; Yao et al., 2024; van der Meer et al., 2023; Kiesel et al., 2023) and the social sciences, including the European Social Survey (ESS), which is designed to track changes in people's attitudes, beliefs, and behavior patterns across European nations (Davidov et al., 2008). This integrated taxonomy encompasses a comprehensive range of human values, organizing them into 20 value categories. Further details on these values can be found in Table 33."}, {"title": "3.2 Exploring the Impact of Values on Emotional Support Effectiveness", "content": "To motivate our research, we conducted an analysis to examine the role of values in emotional support by analyzing the ESConv dataset (Liu et al., 2021), which contains multi-turn emotional support conversations in English among crowdworkers. We analyze whether the reinforcement of a seeker's values positively influences the effectiveness of emotional support."}, {"title": "4 Emotional Support Dataset from Reddit", "content": "Providing emotional support through value reinforcement involves addressing two critical questions: (1) which values should be reinforced at each turn, and (2) what supporter utterances can reinforce them most effectively. Addressing these questions requires large, authentic conversation data that span a wide range of help-seeking situations. To that end, we turn to Reddit's r/offmychest subreddit, which offers a diverse collection of emotional support exchanges. In this context, original posters (OPs) are seekers, and commenters serve as supporters. The structure of posts and comment threads closely mirrors dialogue flows, capturing the dynamics of emotional support interactions. We collected posts and comments from 2019 to 2023, as provided by Watchfull. We retained only high-quality emotional support conversations by filtering them using metrics such as upvote ratio and score. The collected data was limited to publicly available content and did not include private, deleted, or personally identifiable information.\nOur goal is to use this data to train a model that identifies the values to reinforce at each turn (target value detector) and a model that produces supporter utterances to effectively promote the target values"}, {"title": "5 Method", "content": "The overall framework, illustrated in Figure 3, consists of three core components: (1) the target value detector, which identifies values to reinforce in the seeker at each turn; (2) the reference generator which produces utterances to effectively promote these values from the seeker; (3) the supporter model which determines strategies and generates responses based on the identified target values and the reference responses. To ensure the reproducibility of our work, we will release our source code, trained models, and simulation data publicly."}, {"title": "5.1 Target Value Detector", "content": "We train the target value detector using the high quality emotional support conversations from Reddit (Section 4). Given a dialogue history $o_1, c_1, o_2, c_2, ..., c_{t-1}, o_t$, where $o_i$ and $c_i$ represent the $i$th utterances by the OP (seeker) and a commenter (supporter), respectively, the target value detector predicts which values to target in $c_t$. The ground-truth values $v_{t+1}$ are the top-3 values observed in $o_{t+1}$, based on their probabilities from\nthe value detection model (Schroter et al., 2023).\n$u_{t+1} = LMTVD(o_1, c_1, o_2, c_2, ..., c_{t-1}, o_t)$  (1)\nDetailed training methods and results are provided in Appendix D.1."}, {"title": "5.2 Reference Generator", "content": "The reference generator is also trained on the Reddit data. Specifically, given a dialogue history ($o_1, c_1, o_2, c_2, ..., c_{t-1}, o_t$) and the values ($v_{t+1}$) reflected in the OP's next utterance ($o_{t+1}$), the model is trained to generate $c_t$. Here, $v_{t+1}$ is treated as the target values and $c_t$ is considered to have successfully promoted these target values. Training involves two stages: supervised fine-tuning (SFT) and direct preference optimization (DPO).\n\nSFT Stage. This stage involves training the model to generate the supporter's comments by conditioning on the dialogue history and the values expressed in the next utterance of the OP:\n\n$c_t = LMRG(o_1, c_1, o_2, c_2, ..., c_{t-1}, o_t; v_{t+1})$ (2)\n\nDPO Stage. This stage aims to enhance the SFT model's generation quality through DPO. The preference dataset is constructed as follows. Given a dialogue history ($o_1, c_1, o_2, c_2, ..., c_{t-1}, o_t$), the original supporter comment $c_t$ is designated as the chosen response, as it successfully promoted the target values $v_{t+1}$. The rejected response is selected as another comment to $o_t$, denoted by $c_r$, randomly sampled from the siblings of $c_t$ (i.e., other comments under the same dialogue history). $c_r$ is a natural response to the dialogue history but is likely suboptimal for promoting the target values"}, {"title": "5.3 Supporter Model", "content": "The supporter model is the primary model that interacts with the seeker, generating responses that align with target values. It processes three key inputs: the dialogue history, the target values identified by the target value detector at each turn, and a reference response generated by the reference generator. At each turn, the model generates reasoning across four aspects (Figure 3): (1) identifying the seeker's issues and current state, (2) analyzing the key content of the reference response, (3) determining whether to incorporate the reference response, and (4) selecting the optimal emotional support strategy (Appendix C) and generating the subsequent response. The reason for step (3) is that, while the Reddit data offers valuable information across diverse emotional support scenarios, its distribution may not align perfectly with everyday conversations. Thus, the model selectively incorporates reference responses during reasoning.\nThe training process involves two stages\u2014SFT and DPO\u2014using simulation data as follows."}, {"title": "5.4 Seeker Simulator", "content": "To simulate various scenarios, we generated personas using GPT-4o and GPT-4o-mini, defining attributes such as problem type, emotions, and"}, {"title": "6 Experiments", "content": "We evaluate the emotional support capabilities of various models through simulated conversations between each supporter model and the seeker simulator, using the 120 predefined seeker personas for testing. A conversation is considered complete if the seeker simulator generates \u201c[END]\u201d or if the seeker's emotion score, as calculated by EmoLlama-Chat-7B (Liu et al., 2024), reaches 0.6 or higher and is accompanied by gratitude-related expressions (e.g., \"thank you\"). To account for practical conversation lengths, interactions are limited to a maximum of 20 turns, based on the average 15-turn length observed in the ESConv dataset. Only conversations concluding within this limit are included in the evaluation."}, {"title": "6.1 Evaluation Methods", "content": "We evaluate the emotional support capabilities of various models through simulated conversations between each supporter model and the seeker simulator, using the 120 predefined seeker personas for testing. A conversation is considered complete if the seeker simulator generates \u201c[END]\u201d or if the seeker's emotion score, as calculated by EmoLlama-Chat-7B (Liu et al., 2024), reaches 0.6 or higher and is accompanied by gratitude-related expressions (e.g., \"thank you\"). To account for practical conversation lengths, interactions are limited to a maximum of 20 turns, based on the average 15-turn length observed in the ESConv dataset. Only conversations concluding within this limit are included in the evaluation."}, {"title": "6.2 Evaluation Metrics", "content": "We conduct evaluations focusing on three key aspects: ES-Skills, ES-Intensity, and ES-Value.\nES-Skills evaluates a supporter's emotional support capabilities across three components, drawing upon previous studies (Zheng et al., 2024; Zhao et al., 2024; Cheng et al., 2023; Deng et al., 2024; Cheng et al., 2022; Liu et al., 2021): (1) emotional support skills, which include Identification, Comforting, Suggestions, Experience, and Informativeness; (2) general conversation skills, covering Consistency, Role-Adherence, Expression, and Humanness; and (3) an Overall score. Each criterion is rated on a five-point scale using GPT-4o-mini. Detailed metric descriptions are in Appendix F.1.\nES-Intensity measures the intensity of a seeker's negative emotions after a conversation. Scores are assigned on a five-point scale, with lower scores indicating minimal negative emotions. We developed a predictive model using GPT-4o-mini based on"}, {"title": "6.3 Baselines", "content": "We evaluate our approach against three categories of baseline models:\n\nPrompt-Based Models: GPT-4o-mini and Llama-3-8B-Instruct.\nFine-Tuned Models: Variants of Llama-3-8B-Instruct trained on existing emotional support datasets, including ESConv (Liu et al., 2021), ExTES (Zheng et al., 2024), and Psych8k (Liu et al., 2023). These datasets are based on English-language conversations.\nEmotion-Reinforced Models: To verify the effectiveness of value reinforcement, we train the reference generator and supporter model to prioritize positive emotion reinforcement instead of values (Appendix G)."}, {"title": "6.4 Evaluation Results", "content": ""}, {"title": "6.4.1 Effectiveness of Value Targeting and Reference Responses", "content": "To evaluate the impact of our two main components, target value prediction and reference response generation, we first conducted an ablation study using GPT-4o-mini as the supporter model.\n\nAs shown in Table 2, leveraging both target value prediction and reference responses significantly improved performance across all ES-Skills metrics while reducing ES-Intensity. This approach no-"}, {"title": "6.4.2 Performance Comparison with Baselines", "content": "The performance comparisons between our models and the baselines are presented in Tables 3. For our DPO model and the emotion-reinforced model (DPO), we select configurations based on their optimal performance ($h = 3, \u03b3 = 1, T_{diff} = 2$ and $h = 3, \u03b3 = 1, T_{diff} = 0.5$, respectively). For results with more DPO hyperparameters, refer to Table 20 in the Appendix.\nES-Skills. Our DPO model (row 9) outperformed both prompt-based and fine-tuned baselines across most metrics, particularly in emotional support metrics such as Suggestions, Experience, and Informativeness. These improvements highlight the characteristics of our reference responses, which focused on sharing relevant personal experiences, providing practical solutions, and fostering self-confidence. These are key components of effective online emotional support. The models also showed significant gains in conversational capabilities, particularly in the Expression and Humanness metrics, leading more natural and dynamic interactions.\n\nNotably, the variant of our method that focuses on reinforcing positive emotions rather than values (rows 6-7) also consistently outperformed other baselines. This suggests that one of our key ideas-leveraging crowd knowledge from Reddit-is still effective when the supporter model is designed to promote positive emotions in seekers. Yet, our value reinforcement approach achieved higher scores across most emotional support skill metrics at comparable training stages, highlighting the greater effectiveness of reinforcing values over positive emotions in enhancing emotional support.\n\nES-Intensity. Our DPO model (row 9) outperformed most baselines, demonstrating that our supporter model reduces the intensity of seekers' negative emotions more effectively than other methods. Interestingly, our models (rows 7\u20138) also resulted in lower intensity than the emotion reinforcement models at comparable training stages (rows 6\u20137)."}, {"title": "6.4.3 Expert Evaluation", "content": "To gain deeper insights into the value reinforcement capabilities of our supporter models, two licensed clinical psychologists with over three years of clinical experience conducted a qualitative analysis of dialogues generated by our DPO model.\nStrengths. One notable strength is its ability to effectively validate the seeker's challenges, using empathetic phrases such as \u201cwhich is completely understandable\u201d. This validation fosters trust between the supporter and the seeker while encouraging self-acceptance, which in turn promotes deeper exploration and understanding of personal values."}, {"title": "7 Conclusion", "content": "In this paper, we introduce an emotional support framework based on value reinforcement, designed to promote long-term emotional well-being. The framework incorporates a target value detector and a reference generator to improve the supporter model's ability to generate value-aligned and effective support responses. Evaluations demonstrate that our framework surpasses baseline models in both emotional support quality and value reinforcement. Expert therapist evaluations further highlight the model's strengths in validating seekers' challenges and emphasizing positive aspects of their situations, which are key elements of effective emotional support. These results underscore the potential of value reinforcement to enhance supportive interactions and provide a foundation for developing more effective emotional support systems."}, {"title": "Limitations", "content": "Our framework demonstrates promising results in enhancing emotional support quality and reinforcing values. However, there is a limitation in the lack of longitudinal evaluation. While previous research highlights the long-term benefits of value reinforcement in counseling and decision-making, the long-term outcomes of our framework have yet to be empirically validated. Future studies could incorporate extended timeframes to evaluate its sustained impact on emotional well-being and guide further refinements."}, {"title": "Ethical Considerations", "content": ""}, {"title": "Considerations on Self-Disclosure", "content": "Sharing experiences related to those of the seeker-is a key strategy in emotional support for fostering intimacy and has been a key evaluation criterion in prior emotional support systems (Zhang et al., 2023). However, some users might feel uncomfortable when dialogue systems present these experiences as personal. We found that removing self-disclosure strategy from the model impacts the quality of emotional support (Appendix I), highlighting the need for further research into more sophisticated approaches to experience sharing, which we leave as a direction for future work."}, {"title": "Potential Risks of Misuse or Harm", "content": "Our system provides emotional support for common daily challenges, such as interpersonal conflicts and academic stress, while explicitly not replacing professional psychological intervention. Although automated and expert evaluations demonstrate strong performance, there is a possibility that the system's responses might inadvertently have an unintended impact on users in certain situations. To mitigate this risk, we have implemented mechanisms for context-sensitive responses and clearly positioned the system as a supplementary tool rather than a substitute for professional therapy."}, {"title": "Addressing Bias and Overgeneralization", "content": "Data from online platforms inherently contains biases that may underrepresent certain perspectives, potentially limiting the system's ability to effectively serve diverse user groups. To address these concerns, we carefully selected data collection targets and periods to ensure diversity in emotional support topics. Additionally, we enabled the supporter model to evaluate the appropriateness of reference responses, introducing an additional filtering process. By fostering balanced viewpoints, we aim to provide equitable and inclusive support."}]}