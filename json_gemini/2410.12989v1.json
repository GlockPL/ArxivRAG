{"title": "QTOK: A COMPREHENSIVE FRAMEWORK FOR EVALUATING\nMULTILINGUAL TOKENIZER QUALITY IN LARGE LANGUAGE\nMODELS", "authors": ["Iaroslav Chelombitko", "Egor Safronov", "Aleksey Komissarov"], "abstract": "In the development of Large Language Models (LLMs), considerable attention has been given to the\nquality of training datasets. However, the role of tokenizers in the LLM training pipeline, particularly\nfor multilingual models, has received less focus. The quality of tokenization can significantly impact\na model's ability to handle diverse languages effectively. We introduce Qtok, a tool designed to assess\ntokenizer quality with a specific emphasis on their performance in multilingual contexts.\nOur research proposes a set of metrics for evaluating tokenizer quality, including measures of language\ncoverage, token completeness, and distribution across languages and linguistic categories. Qtok\napplies these metrics to evaluate 13 distinct tokenizers from 58 publicly available models, analyzing\ntheir output across different linguistic contexts. Our analysis revealed significant variations in token\ndistribution across languages and categories, highlighting potential biases and areas for improvement\nin current tokenization strategies.\nThis research contributes to the field of tokenizer evaluation within multilingual LLM development\nby providing a systematic approach to assessing tokenizer quality. Our findings highlight the critical\nrole of tokenization in multilingual LLM capability. The Qtok tool and our analysis methodology\noffer practical means for researchers to evaluate and improve tokenization strategies for multilingual\napplications. We offer a method to compare tokenizer quality across these metrics, which may be\nuseful when selecting or adjusting tokenizers for specific multilingual LLM applications.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of\nnatural language processing tasks. While much attention has been given to factors such as dataset quality (Chen &\nMueller, 2024), model architectures, and training objectives (Raiaan et al., 2024), the crucial role of tokenizers in\nLLM development, particularly for multilingual models, has been relatively overlooked. Tokenization serves as the\nfundamental bridge between human-readable text and the numerical representations that machine learning models can\ninterpret, significantly impacting model performance, training efficiency, and language handling abilities (Ali et al.,\n2024; Schmidt et al., 2024).\nRecent studies have highlighted the importance of tokenizer choice in LLM training. (Ali et al., 2024) demonstrated\nthat tokenizer selection can significantly affect both model performance and training costs, especially in multilingual\ncontexts. (Rajaraman et al., 2024) provided theoretical insights into the role of tokenization in LLMs, showing how\nappropriate tokenization enables even simple models to effectively capture complex language distributions (Limisiewicz\net al., 2023)."}, {"title": "2 Results", "content": "As of September 2024, licensing restrictions severely limited the available pool of tokenizers. Within the Chatbot Arena\ndataset, a significant number of models operate under closed licenses, restricting access to their tokenizers and limiting\ntheir inclusion in this study. This licensing issue substantially reduced the number of tokenizers available for analysis\n(Table 1).\nFurthermore, even among models with open licenses, the lack of official repositories on platforms such as Hugging\nFace posed additional obstacles. To maintain the integrity of our analysis, we made the methodological decision to use\nonly tokenizers from official repositories, eschewing potentially altered user-created versions. This criterion further\nnarrowed our dataset but ensured the authenticity of the tokenizers under examination. Format incompatibility emerged\nas another significant challenge. Several models did not provide tokenizers in the standard Hugging Face format, which\ntypically requires the config.json, tokenizer.json, and tokenizer_config.json files. This incompatibility complicated the\nretrieval and further analysis of these tokenizers.\nAfter applying these stringent criteria to the models listed on Chatbot Arena (formerly LMSYS) we selected only those\nwith officially accessible tokenizers that met our requirements for format compatibility and ease of access. This rigorous\nselection process resulted in a final dataset of 58 models, which formed the basis of our further analysis.\nLLM models can be categorized into various types, such as general-purpose LLMs, code-focused LLMs, and others. For\nthis study, we limited our analysis to models designed for processing natural language, thus excluding those specialized\nfor code-related tasks, as they necessitate a different methodological approach.\nAdditionally, the absence of certain tokenizers from the Transformers library (Wolf et al., 2020) required individual\nexploration and, in some cases, reverse-engineering to gain access. This process was both time-consuming and\ntechnically challenging, ultimately limiting the scope of our analysis. As a result, we excluded these restricted models\nfrom our initial evaluation."}, {"title": "2.1 There are 13 distinct tokenizers in 58 models from Chatbot Arena", "content": ""}, {"title": "2.2 Unified tokenizer", "content": "To avoid analyzing each tokenizer individually, we decided to combine all the tokens into a unified tokenizer. This\nartificial unified tokenizer allows us to examine and annotate tokens independently of any specific tokenizer. Later, this\nunified tokenizer can be used as a metric for more detailed analysis of individual tokenizers.\nThe process of consolidating tokens into a single unified tokenizer is relatively straightforward. First, the vocabulary\nfrom each unique tokenizer is collected. Then, for each token in an individual tokenizer's vocabulary, a check is\nperformed to determine whether it is already present in the collective set of tokens. If the token is absent, it is added\nto the unified file; if it is already included, it is disregarded. Additionally, for each token, we maintain a list of its\nranks across all 13 tokenizers. This allows us to track the relative importance and usage of each token within different\ntokenization systems. For individual models within the same group, there may be slight differences in tokens, primarily"}, {"title": "2.3 Token Annotation and Classification Without Language Association", "content": "With the unified tokenizer in place, we were able to begin the process of token classification. This allowed us to\ndetermine how to group the 430,000 tokens into key categories.\nFirstly we had an attempt to implement filtering using a lemmatizer. However, we encountered the issue that lemmatizer\nlists are available for only a few languages, while the tokenizer supports many more. Consequently, standard lemmatizers\nlike those from spaCy (Matthew Honnibal et al., 2020) or NLTK (Bird et al., 2009) do not suffice. As a result, we\nabandoned this approach and decided to implement a more sophisticated nonlanguage-specific processing method,\nwhich we will detail below."}, {"title": "2.4 Alphabet-based Token Annotation and Classification", "content": "Different encoding schemes are employed to represent the various characters used across natural languages. A single\nencoding system may encompass multiple natural languages, which are often part of the same language family. For\ninstance, in the ISO/IEC 8859 series, the following encodings are utilized for different natural languages (ISO, 2020):\n\u2022 ISO 8859-1 (Latin-1): Western European languages.\n\u2022 ISO 8859-2 (Latin-2): Central and Eastern European languages.\n\u2022 ISO 8859-3 (Latin-3): South European languages.\n\u2022 ISO 8859-4 (Latin-4): North European languages.\n\u2022 ISO 8859-5: Cyrillic alphabet.\n\u2022 ISO 8859-6: Arabic alphabet."}, {"title": "2.5 Language Classification", "content": "The language classification process applies to all groups of natural language tokens discussed in the paper (internal\ntokens, space-start tokens, and character tokens). The classification process involves the following steps:\n1. We obtain the unified language form for each token by processing it character-by-character using the unicodedata\nmodule. In this context, the \u201cunified form\" refers to the specific Unicode encoding of each token, where we focus on\nthe first word in its classification. For example, the Unicode encoding for the letter \"A\" is LATIN CAPITAL LETTER\nA. In our approach, we consider only the term LATIN and check for the presence of the word LETTER to avoid false\npositives from non-letter symbols. This method ensures that only actual letters are retained for further analysis, reducing\nnoise from other types of characters.\n2. After obtaining the unified form, we classify it by the encodings used for representing natural languages. The\nlanguages identified in step 1 are then compared against a manually curated list of languages found in tokenizers\ndesigned for natural language representation. This comparison allows us to assess how well each tokenizer aligns with\nthe languages it is intended to support, ensuring that the tokens are correctly mapped to the respective natural languages.\nWe used the following languages: 'ADLAM', \u2018CHAKMA\u2019, \u2018OL\u2019, \u2018BAMUM\u2019, \u2018OGHAM\u2019, \u2018BOPOMOFO', \u2018COP-\nTIC', 'RUNIC', \u2018HALFWIDTH\u2019, \u2018MODI', \u2018KAITHI\u2019, \u2018LEPCHA', \u2018SHAVIAN\u201d, \u2018LIMBU\u2019, \u2018BATAK', 'PHOENI-\nCIAN', 'GLAGOLITIC', 'MANDAIC', 'BALINESE', 'SAMARITAN', 'PHAGS-PA', 'OLD', 'BLACK-LETTER',\n'SUNDANESE', 'INSCRIPTIONAL', 'LISU', 'CHAM', 'TAGALOG', 'DESERET', 'TAGBANWA', 'BUGINESE',\n'THAANA', 'MONGOLIAN', 'DINGBAT', 'JAVANESE', 'EGYPTIAN', \u2018GEORGIAN\u201d, \u2018NKO', \u2018TIFINAGH', 'GUR-\nMUKHI', 'BENGALI', 'SINHALA', 'ORIYA', \u2018TAI', \u2018KANGXI\u2019, \u2018CANADIAN', 'CHEROKEE', 'LAO', 'TELUGU',\n'SYRIAC', 'TAMIL', 'BRAILLE', 'ETHIOPIC', \u2018MYANMAR', 'HEBREW', 'ARABIC', 'TIBETAN', 'HIRAGANA',\n'CYRILLIC', \u2018GREEK', 'CJK', 'LATIN', \u2018KATAKANA', \u2018KHMER', \u2018THAI', \u2018ARMENIAN', \u2018YI', \u2018HANGUL',\n'GOTHIC', 'MALAYALAM', \u2018DEVANAGARI', 'GUJARATI\u2019, \u2018KANNADA', \u2018MEETEI', and \u2018ARABIC-INDIC'."}, {"title": "2.6 Natural language classification", "content": "Classification of natural language, unlike script classification, is a much more complex process. Currently, there is\na challenge in associating a token with a specific language: words and even entire phrases can appear in multiple\nlanguages, not to mention shorter tokens (those shorter than the average word length) and lemmas. Consequently,\nclassifying a token to a single language remains a complex and unresolved issue in the modern world. In our work, we\npropose a partial solution to this problem using the langid.py tool (Lui & Baldwin, 2012):"}, {"title": "2.7 Core tokens between tokenizers", "content": "In this study, tokenizers are considered identical if they share the exact same vocabulary. An analysis of core tokens\nacross various tokenizer groups reveals distinct patterns based on vocabulary size, highlighting the diversity and\nspecialization of tokenization strategies in contemporary language models."}, {"title": "2.7.1 Grouping Tokenizers by Vocabulary Size", "content": "We grouped tokenizers based on their vocabulary size. However, the vocabulary sizes of most tokenizers vary\nsignificantly. If we were to group tokenizers with identical vocabulary sizes, as mentioned earlier, the groups would\nconsist of only identical tokenizers, introducing substantial bias into the metric values for these groups.\nDuring the analysis, we observed that the vocabulary sizes of most tokenizers are multiples of powers of two. Therefore,\nwe decided to group the tokenizers by powers of two. Tokenizers with vocabulary sizes not exactly equal to a power of\ntwo were assigned to the nearest corresponding group (e.g., a tokenizer with a vocabulary of 50,000 tokens was placed\nin the group with 65,000 tokens). As a result, we identified four groups of models:"}, {"title": "1. Models with Approximately 32,000 Vocabulary Tokens:", "content": "This group comprises 21 models, each featuring a vocabulary size of roughly 32,000 tokens. Collectively,\nthese models encompass approximately 40,358 unique tokens. Of these, 23,957 are identified as core tokens,\nwhich are consistently present across multiple tokenizers within this group. Additionally, there are 54 singleton\ntokens-tokens that appear exclusively in a single tokenizer. These models are positioned in the top-left\nquadrant of the token group distribution, indicating a concentration of models with smaller, more standardized\nvocabularies."}, {"title": "2. Models with Vocabulary Sizes Between 50,000 and 64,000:", "content": "This group consists of 12 models with vocabulary sizes ranging from 50,000 to 64,000 tokens. The group\nincludes approximately 108,934 unique tokens, with 31,707 core tokens shared among the tokenizers. The\npresence of 8,514 singleton tokens in this group deviates from the mode observed across all groups. This\ndeviation is primarily due to the model families within this group, which includes four families: the Pythia\nfamily with 7 models, the Yi-34b family with 2 models, the Jamba family with 2 models, and the Falcon\nfamily with 1 model. Notably, the Falcon family, consisting of only one model, significantly contributes to this\ndeviation from the mode."}, {"title": "3. Models with Vocabulary Sizes Between 100,000 and 150,000:", "content": "Sixteen models in this analysis possess vocabularies ranging from 100,000 to 150,000 tokens. These models\ncol"}]}