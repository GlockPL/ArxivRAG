{"title": "SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph Generation", "authors": ["Hang Zhang", "Zhuoling Li", "Jun Liu"], "abstract": "Dynamic scenes contain intricate spatio-temporal information, crucial for mobile robots, UAVs, and autonomous driving systems to make informed decisions. Parsing these scenes into semantic triplets <Subject-Predicate-Object> for accurate Scene Graph Generation (SGG) is highly challenging due to the fluctuating spatio-temporal complexity. Inspired by the reasoning capabilities of Large Language Models (LLMs), we propose SceneLLM, a novel framework that leverages LLMs as powerful scene analyzers for dynamic SGG. Our framework introduces a Video-to-Language (V2L) mapping module that transforms video frames into linguistic signals (scene tokens), making the input more comprehensible for LLMs. To better encode spatial information, we devise a Spatial Information Aggregation (SIA) scheme, inspired by the structure of Chinese characters, which encodes spatial data into tokens. Using Optimal Transport (OT), we generate an implicit language signal from the frame-level token sequence that captures the video's spatio-temporal information. To further improve the LLM's ability to process this implicit linguistic input, we apply Low-Rank Adaptation (LoRA) to fine-tune the model. Finally, we use a transformer-based SGG predictor to decode the LLM's reasoning and predict semantic triplets. Our method achieves state-of-the-art results on the Action Genome (AG) benchmark, and extensive experiments show the effectiveness of SceneLLM in understanding and generating accurate dynamic scene graphs.", "sections": [{"title": "1. Introduction", "content": "A visual scene can be parsed into a semantic structured scene graph in which the semantic entities are the nodes and the relationships containing spatial-temporal information are the edges linking the nodes [1]. Dynamic Scene Graph Generation (SGG) involves the spatial localization of objects and inference of semantic predicates among objects so that it can deliver fine-grained semantic information efficiently which is highly beneficial for complex dynamic scene understanding [2, 3]. Hence, scene graphs have been commonly applied to aid safe and reasoned decision-making and behavior planning for kinds of robots and autonomous systems [4, 5].\nCompared to static image SGG, the intricate spatio-temporal motion interaction among objects and model inference bias caused by the long-tail distribution of the dataset, place the task of dynamic SGG in a more challenging position [6]. Even though the promising application prospects of dynamic SGG have spurred a multitude of related exertions and constructive works in terms of spatial-temporal modeling"}, {"title": "2. RELATED WORK", "content": ""}, {"title": "2.1. Scene Graph Generation (SGG)", "content": "The scene graph represents a typical semantic graph structure that captures the relationships between objects within a scene, facilitating understanding and representation of visual information in a structured form [1]. Since its initial proposal for image retrieval tasks in [28], scene graphs have garnered significant interest in computer vision, leading to numerous advancements aimed at enhancing performance in tasks like visual question answering (VQA), image and video captioning, retrieval, and generation [29, 30, 31]. Recently, scene graph generation (SGG) has become particularly impactful in robot planning tasks, where visual scene comprehension is crucial. For instance, [5] highlights SGG's role in robotic action sequencing, while [4] addresses robot planning under partial observability by integrating local scene graphs for improved global"}, {"title": "2.2. Large Language Models (LLMs)", "content": "Recently, increasingly powerful large language models (LLMs) like GPT-4 [14] and LLaMA3 [15] have emerged, achieving remarkable success across various NLP tasks. These tasks include question answering [37], multilingual translation [38], and so on. Benefiting from pre-training on vast text corpora, LLMs have developed a rich repository of tacit knowledge [39], enabling them to perform well even outside traditional NLP tasks. In computer vision, for example, LLMs are making strides in scene understanding and visual representation [17, 40]. Furthermore, a detailed survey [22] highlights that LLMs display enhanced reasoning abilities as model size increases, encompassing areas such as semantic [41], visual [42], and mathematical reasoning [43]. A recent study [44] also explores LLMs' potential in control engineering, a domain that demands complex integration of mathematical and engineering principles. Building on these developments, our work leverages the robust reasoning capabilities of LLMs to advance scene understanding tasks, tapping into their broad knowledge and adaptability."}, {"title": "3. METHOD", "content": ""}, {"title": "3.1. The Overall Framework", "content": "The overall structure of our proposed SceneLLM framework is shown in Fig.1. First of all, the V2L mapping module in SceneLLM will map the input video signal (the frame sequence) into an implicit linguistic signal (dubbed \"scene sentences\") which focuses on transferring spatial-temporal information via SIA and OT schemes. Afterward, SceneLLM feeds the \"scene sentences\" into LLM to implicitly reason about the corresponding semantic relationships existing in the scene. Finally, we employ a straightforward SGG predictor to decode the reasoned output and produce semantic triplets. We will illustrate each of the key components in SceneLLM in the following subsections."}, {"title": "3.2. Video-to-Language (V2L) Mapping Module", "content": "As shown in Fig.2, V2L Mapping has two stages. (a) We first learn an object-oriented vector quantized variational autoencoder (VQ-VAE [45]) to discretize features of objects in the scene. (b) Then aggregate the discrete feature tokens with spatial information with the SIA scheme to obtain frame-level token sequence, thereby an optimal transport scheme to construct a language-like hierarchy with temporal information while forming LLM-friendly \"scene sentences\".\nFeature Discrete Quantization. Specifically, at the time T, the object detector detects objects in frame I, and outputs the corresponding bounding boxes Bt, categories, and visual features Ft of the objects (ROI Features). Next, our VQ-VAE consisting of an encoder E, a decoder D, and a codebook $C = \\{c_k\\}_{k=1}^m$ ($c_k \\in R^l$, where dimension l is the same as the dimension of the word tokens in the LLM used) discretizes the visual features of the objects. Given the extracted n visual features $F_c = \\{f_{1:n}\\}$, the conventional encoder E encodes input features into latent features $F_c = \\{\\hat{f}_{1:n}\\}$ ($f_n \\in R^{l^{'}} $). To obtain discrete feature tokens $F_d = \\{\\hat{f}_n^*\\}$, we conduct a standard discrete quantization operation in VQ-VAE by replacing each latent feature $\\hat{f}_i$ with its nearest unit $c_k$ of the codebook C as follows:\n$f_n^* = arg \\min_{c_k \\in C}(||\\hat{f}_n - c_k||_2)$\t(1)\nOnce discrete quantization is completed, the decoder D will reconstruct the visual features $F_c$ using discrete features $F_d$."}, {"title": "Implicit Linguistic Signal Generation", "content": "Once we obtain the discrete features of objects $F_d$, the next step is to generate a linguistic tokens sequence. Considering the spatial information between different objects in the static frame and temporal information in the dynamic scenes (video), the generated linguistic signal should also contain corresponding spatial-temporal information. To this end, inspired by the study [25] on the spatial semantic representation of Chinese characters, we propose to aggregate spatial information in the discrete feature $F_d$ with an SIA scheme to obtain a frame-level token and thereby generate a video-level linguistic signal from the frame-level token sequence in an Optimal Transport (OT) [46, 47] manner.\nSpatial Information Aggregation (SIA). Specifically, as shown in Fig.3 (a), the Chinese character is composed of multiple radicals combined in a certain spatial structure (i.e. up and down, left and right, or their combined positions). Each radical stands for a certain object which has a specific meaning. For easy understanding, represents"}, {"title": "Optimal Transport (OT) Scheme", "content": "Since the frame-level token is independent and only contains spatial information of its corresponding frame. To understand complex"}, {"title": "3.3. Implicit Language Reasoning in LLM", "content": "So far, we have acquired the implicit linguistic signal $S_{LLM}$ which encompasses the spatio-temporal semantic information of the scene. This signal can be regarded as a \"scene sentence\" or a \"scene token sequence\". To parse the semantic relationships within the scene, we leverage the implicit reasoning capabilities of LLM as it has been proven its rich implicit knowledge can be used to reason and model the visual world [24, 23, 20, 21, 22]. Specifically, we designed the following prompt as LLM's input:\nGiven such a scene sentence [SLLM], please parse the\nrelationships between the person and objects in the scene.\nBesides, to help the LLM better understand our \"scene sequence\", we also execute LORA [26] to fine-tune LLM. Note that we do not need the explicit output of LLM, instead, the final scene graph will be generated by a simple transformer-based SGG predictor $D_{SGG}$ [9], and the input of $D_{SGG}$ is the reasoned feature output of LLM. The whole decoding process is as follows:\n$F_{implicit} = LLM(S_{LLM})$\t\t(6)\n$\\hat{Y} = D_{SGG}(F_{implicit})$\t\t\nwhere the $F_{implicit}$ is the hidden feature of LLM's final block and $\\hat{Y}$ is the generated scene graph consisting of multiple triplets."}, {"title": "3.4. Training and Inference", "content": "Inference. Given a video V with T frames, we first extract the visual features $F_c$ of objects in each frame via a pre-trained object detector. Then, $F_c$ is encoded to latent features $\\hat{F_c}$ via the encoder E of our VQ-VAE, and $\\hat{F_c}$ is quantized to $F_D$ by the learned codebook C. After that, we encode spatial information into $F_D$ via MLP to obtain $F_t^{'}$ (Eq. (2)), and aggregate $F_t^{'}$ to form the frame-level token sequence t via GCN (Eq. (3)). Subsequently, we transform the frame-level token sequence $\\{t_r\\}_{r=1}^T$ to implicit linguistic signal $S_{LLM}$ via the OT scheme (Eq. (5)). Finally, we feed $S_{LLM}$ into the LLM to reason about the semantic relationships in the scene and generate the scene graph via the SGG predictor $D_{SGG}$.\nTraining. Our framework is optimized in two stages: the VQ-VAE training and the SceneLLM training. In the first stage, we pre-train the VQ-VAE model to encode and quantize visual features of objects. To achieve this, similar to [45], VQ-VAE is optimized to reconstruct visual features via encoding, quantization, and decoding process. The VQ-VAE loss function consists of 3 sub-losses, namely reconstruction loss for encoder-decoder learning, embedding loss for codebook learning, and commitment loss for limiting arbitrary growth of codebook volume. The loss function is defined as"}, {"title": "4. EXPERIMENTS", "content": ""}, {"title": "4.1. Experiment Setting", "content": "Dataset. The Action Genome (AG) dataset [27] is an extensive video dataset meticulously annotated with frame-level scene graphs. Designed for detailed scene understanding, the dataset contains 36 object classes, with approximately 1.7 million annotated object instances distributed across 26 predicate classes. These annotations provide a thorough exploration of 157 distinct triplet categories, mapped across nearly 234,000 frames, which significantly enhances the dataset's utility for comprehensive scene analysis. Predicates within the AG dataset are thoughtfully divided into three types: (1) Attention Predicates, which indicate a subject's focus on objects, (2) Spatial Predicates, which denote relative positioning, and (3) Contact Predicates, capturing the nature of interaction between objects, covering both contact and non-contact forms. This structured approach to predicate categorization enables a nuanced and"}, {"title": "4.2. Implementation Details", "content": "Following TPT[6] and OED[50], we train a Transformer-based detector for object detection which is initialized from a COCO-pre-trained model and fine-tuned on the AG dataset. We set the latent feature dimension l of VQ-VAE to 512, the number of codebook size m to 512, and the weighting factor \u03bb in 7 to 0.02. Following [40], the auto-regressive model M is implemented as a one Convolutional Gated Recurrent Layer with a kernel size of (1,1). During the first training stage (VQ-VAE training), we optimize VQ-VAE for 300,000 iterations using the AdamW optimizer with an initial learning rate of 3e-4. In Eq. (3), we employ a bottom-up hierarchical clustering algorithm, with average linkage to measure the distance between between clusters, to construct the spatial relation C between object features Fd. In addition, GCN consists of two graph convolutional layers. We use frozen LLaMA-13B [15] as our LLM. During the second training stage (SceneLLM training), we first optimize the MLP, GCN, and SGG predictor for 30,000 iterations with an initial learning rate of 1e-5. Then, for the LORA process, we fine-tune the LLM using the AdamW optimizer with an initial"}, {"title": "4.3. Performance Results and Comparison", "content": "In Table 1 and Table 2, we present a detailed comparison of the performance of our proposed SceneLLM model alongside several existing methods on the AG dataset in both With Constraint and No Constraint settings. To provide clarity, the best performance values in each category are highlighted in bold. The results clearly demonstrate that SceneLLM, leveraging the capabilities of a large language model (LLM), consistently achieves state-of-the-art results across all tasks and metrics. This indicates the effectiveness of SceneLLM's architecture in handling the complexities of scene understanding and generating high-quality relationships within the data.\nFor instance, under the With Constraint setting, our SceneLLM outperforms the"}, {"title": "4.4. Ablation Studies", "content": "In this section, we conduct extensive ablation experiments on the AG dataset to investigate our SceneLLM.\nImpact of LLM. In our SceneLLM, we introduce LLM into the SGG task to reason about the spatial-temporal semantic relationships of objects by leveraging its implicit knowledge. To evaluate the efficacy of this strategy, we test two variants. In the first variant (w/o LLM), we remove the LLM and directly use $S_{LLM}$ as the input of SGG predictor to generate scene graphs. In the second variant (w/ T5), we replace our LLM (LLaMA) with a smaller LLM, i.e., T5 [55].\nThe results are shown in Table 3. It can be observed that our method (w/ LLaMA) achieves better performance than the two variants, which demonstrates the effectiveness of leveraging a powerful LLM. In addition, the worse performance of w/ T5 shows a less powerful LLM (T5) may hard-to-hand relationships in complex scenes."}, {"title": "Impact of Features Discretization", "content": "In our framework, we discretize the encoded latent features of objects to generate discrete word tokens. To validate this approach, we test a variant called w/o discretization, where the continuous visual features are directly used to produce the implicit linguistic signal $S_{LLM}$ as input to the LLM. As shown in Table 4, the performance of our SceneLLM is significantly better than w/o discretization.\nThis improvement is because the discretization of features in our framework makes the visual features more \"like\" human sentences composed of discrete word tokens. As a result, the LLM, pre-trained on human sentences, can more easily interpret these \"human sentence-like\" visual features."}, {"title": "Impact of the Optimal Transport Scheme", "content": "In SceneLLM, we design an optimal transport-based scheme to generate a temporal-consistent language signal. To validate this strategy, we test a variant called w/o OT, where we remove the OT scheme and directly feed the frame-level tokens $\\{t_r\\}_{r=1}^T$ (with prompt texts) into the LLM. The declined performance of the w/o OT variant shown in Table 5 demonstrates the efficacy of our OT scheme, which reconstructs LLM-friendly language signal from frame-level tokens."}, {"title": "Impact of the LoRA Process", "content": "In our framework, to enable the LLM to comprehend \"action sentences\" while preserving its pre-trained weights and the extensive knowl-"}, {"title": "5. CONCLUSIONS", "content": "In this work, we introduce SceneLLM, a groundbreaking dynamic Scene Graph Generation (SGG) framework that leverages the advanced reasoning capabilities of Large Language Models (LLMs). SceneLLM excels at extracting fine-grained semantic relationships from videos, achieving state-of-the-art performance on the widely used AG benchmark. By utilizing the vast reasoning potential of LLMs, SceneLLM enhances the ability to understand complex interactions and contextual relationships within video data, setting a new standard in the field of SGG. Looking ahead, we plan to extend our research into SGG within open-world environments, as well as explore its applications in 3D scenes. The real world is inherently open and constantly evolving, presenting challenges that closed-world datasets cannot address. Models trained on such limited datasets often face difficulties when encountering novel or unseen situations in practice. This limitation presents significant challenges, particularly for the safe deployment of models in critical applications such as mobile robots and autonomous systems. Therefore, future work will aim to improve model robustness, generalization, and adaptability in the face of unpredictable real-world scenarios, ensuring that these systems can operate safely and effectively in dynamic environments."}]}