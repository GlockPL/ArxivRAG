{"title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve", "authors": ["Yuxiao Qu", "Tianjun Zhang", "Naman Garg", "Aviral Kumar"], "abstract": "A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially, even in scenarios where they are explicitly told that they are making a mistake. In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions.", "sections": [{"title": "1. Introduction", "content": "A promising approach to utilizing and deploying foundation models, and in particular, large language models (LLMs), is to treat them as general-purpose decision-making machines, or \"agents\". To be successful, an LLM agent must not just provide plausible completions for input text, but must exhibit interactive, goal-directed behavior to accomplish a given task. Put in abstract terms, this requires mastering two qualities: (a) producing responses that explicitly seek information about the task, followed by (b) making decisions and improving them by \"thinking\u201d and verifying them at inference time. For instance, to succeed in using a new coding library, an effective LLM agent should first synthesize programs, then try the most promising subset against a compiler, use the resulting feedback to improve the program, and repeat the process for multiple turns. Having the ability to successfully improve a response in sequential attempts is equivalent to a form of \"self-improvement\u201d, at test time.\nTo enable test-time self-improvement, recent approaches attempt to repurpose the knowledge already stored in pre-trained models via few-shot prompting [7, 15, 31, 52, 64]. Although prompt tuning in conjunction with feedback is effective in eliciting improved responses from capable models, it fails to produce models that can succeed in complex tasks by correcting their own mistakes, such as those that require logical reasoning [21, 55]. In many of these problems, models contain the \u201cknowledge\u201d needed to answer a challenging prompt, but are not able to elicit that knowledge even when asked to sequentially correct their mistakes. Fine-tuning the LLM on domain-specific question-answering data [6, 29, 39] can help, but it still does not teach the agent a test-time improvement strategy (see Section 6). A strategy for"}, {"title": "2. Related Work", "content": "Several prior works build techniques to improve reasoning and thinking capabilities of foundation models for downstream applications. Typically these works focus on building prompting techniques for effective multi-turn interaction with external tools [5, 7, 14, 32, 49, 54, 56], sequentially refining predictions by reflecting on actions [7, 15, 63], asking the model to verbalize its thoughts [33, 52, 65], asking the model to critique and revise itself [31, 40] or by using other models to critique a primary model's responses [2, 12, 20, 54]. Although a subset of this work does improve its own responses, this self-correction ability often requires access to detailed error traces (e.g., execution traces from code compilers [7, 31]) in order to succeed. In fact, [21] and Table 1 both indicate that self-improvement guided by the LLM itself (i.e., \u201cintrinsic self-correction\u201d) is often infeasible for off-the-shelf LLMs even when they contain the knowledge required to tackle the prompt given, but fine-tuning with RISE induces this capability as we show in this paper.\nBeyond prompting, previous work also attempts to fine-tune LLM to obtain self-improvement capabili-ties [6, 39, 62]. These works attempt to improve reasoning performance by training on self-generated responses [30, 46, 57, 58, 60]. To achieve this, these works use a combination of learned verifiers [28, 47, 50], search [13, 26, 33, 38], contrastive prompting on negative data [9, 48], and iterated supervised or reinforcement learning (RL) [8, 37, 59]. Although our approach also trains on model-generated data, we aim to introduce a complementary capability to improve performance over sequential turns of interaction, rather than to improve single-turn performance alone. Other work fine-tunes LLMs for multi-turn interaction directly via RL [41, 66]: while this is indeed related, single-turn problems posed in multi-turn scenarios require addressing distinct challenges than generic multi-turn RL: (i) sample-efficiency is not a concern since the entire environment is fully characterized by the training dataset of prompts and oracle answers and dynamics are deterministic, and (ii) we need to generalize to novel test prompts. Multi-turn RL focuses on sample efficiency, which is not as critical in our setting, though of course learning to generalize from a limited number of initial states would be appealing. Our main focus is to show that it is possible to train models for self-improvement via appropriately designing multi-turn fine-tuning objectives. This is orthogonal from the choice of training approach (RL or not).\nThe most related to our work are GLoRE [17] and Self-Correct [53], which train separate models to identify errors and refine incorrect answers of other LLMs. Unlike these works, our approach trains a single model to produce answers and improve them over more than two turns, which is the maximal number of turns studied in these works. We show that doing so successfully requires careful design choices: an iterative on-policy data generation strategy along with a training objective that can learn from both successful and unsuccessful rollouts. From an algorithmic point of view, RISE is similar to online"}, {"title": "3. Problem Setup and Preliminaries", "content": "The goal of our work is to improve LLM performance over sequential attempts / turns at a given problem. Concretely, given a dataset $\\mathcal{D} = \\{(x_i, y_i^*)\\}_{i=1}^L$ of problems $x_i$ and oracle responses $y_i^*$, our goal is to obtain an LLM $\\pi_\\theta(\\cdot | [x, \\hat{y}_{1:t}, p_{1:t}])$ that, given the problem $x$, previous model attempts $\\hat{y}_{1:t}$ at the problem, and auxiliary instructions $p_{1:t}$ (e.g., instruction to find a mistake and improve the response; or additional compiler feedback from the environment) solves a given problem as correctly as possible. To this end, we encode this goal into the following learning objective that we wish to optimize:\n$\\max_{\\pi_\\theta} \\mathcal{L} = \\sum_{i=1}^L E_{x,y^*\\sim \\mathcal{D}, \\hat{y}_i \\sim \\pi_\\theta(\\cdot | [x,\\hat{y}_{1:i-1},p_{1:i-1}])} [I (y_i == y^*)]$. (3.1)\nUnlike standard supervised fine-tuning that trains the model $\\pi$ to produce a single response $y$ given $x$, Equation 3.1 trains $\\pi$ to also appropriately react to a given history of responses from its own previous attempts $\\hat{y}_{1:i-1}$. Equation 3.1 most closely resembles an RL objective, and we will indeed develop our approach by converting a single-turn problem into a multi-turn MDP. Finally, note that prompting-based methods such as Self-Refine [31] can still be viewed as training $\\pi$ to optimize $\\pi(y^*|x)$ but only when only allowed to modulate the prompt $p_i$ to optimize Equation 3.1. Naturally, since the parameters $\\theta$ are unchanged, this would not be effective in optimizing the objective fully."}, {"title": "4. RISE: Recursive Introspection for Self-Improvement", "content": "Since even strong off-the-shelf models do not exhibit an effective ability to improve themselves when provided with sequential attempts at a given problem [21], a natural next step is to ask how to train models to induce this capability. In this section, we will develop our approach, RISE, for fine-tuning foundation models towards improving their own predictions over multiple turns. Our approach will first convert a problem into a multi-turn MDP, then collect data, and finally run offline reward-weighted supervised learning in this multi-turn MDP to induce this capability."}, {"title": "4.1. Converting Single-Turn Problems into a Multi-Turn Markov Decision Process (MDP)", "content": "The first step in building our approach is to procedurally construct a multi-turn MDP out of a single-turn dataset of prompts and oracle responses (Figure 2, Left). Given a dataset, $\\mathcal{D} = \\{(x_i, y_i^*)\\}$, consisting of prompts $x_i$ and corresponding oracle responses $y$ (e.g., math questions and natural language responses to those questions), we will construct an induced MDP $\\mathcal{M}$ from $\\mathcal{D}$, and then learn policies in this MDP. An initial state in this MDP is a possible prompt $x_i \\in \\mathcal{D}$. We denote the output response from the foundation model as action $a$. Given a state $s$, the next state can be obtained by concatenating the tokens representing $s$ with the action $a$ proposed by the model, and an additional fixed prompt $f$ that asks the model to introspect, e.g., \u201cthis response is not correct, please introspect and correct your answer.\u201d (the exact prompt is shown in Appendix D.4). The reward function is a sparse binary indicator of answer correctness at a given state $s$, $r([x_i,\\dots], a) = 1$ if and only if $a = y$ and is obtained from an answer checking function. This construction from dataset $\\mathcal{D}$ to MDP $\\mathcal{M}$ is shown below:\n$\\begin{aligned}\\mathcal{D} &= \\{(x, y)\\} \\\\\\mathcal{M}: &\\rho(s_0) = \\text{Unif}(x_1, x_2, \\dots, x_N) &\\text{(4.1)}\\\\& P(s'|s, a) = \\delta (s' = \\text{concat}[s, a, f]) &\\text{(4.2)}\\\\& r(s, a) = \\mathbb{1}(a = y \\text{ if } x_i \\in s) . &\\text{(4.3)}\\end{aligned}$"}, {"title": "4.2. Learning in the Multi-Turn MDP", "content": "With the MDP construction in place, the next step involves training a model to improve itself over the course of a rollout. We subscribe to an offline approach to learning that we describe in the following.\nStep 1: Data collection for self-improvement. To ensure that rollout data from this multi-turn MDP is useful for teaching the model how to self-improve, it must satisfy a few desiderata: (1) it must illustrate the mistakes that the learner is likely to make and showcase how to improve upon them in the next attempt, (2) the data must illustrate responses that are relevant to the model given the problem and previous attempts in context, and (3) it must not contain any rollout that degrades in a subsequent turn. Our data collection strategy (Figure 2, Right) satisfies these desiderata.\nIn a given round $k$, for a given problem $x_i$, we unroll the current model $\\pi_{\\theta_t}(\\cdot | s_i)$ to produce multiple sequential attempts, denoted by $y \\sim \\pi_{\\theta_t}(\\cdot | s_i)$. In problems, where external input (e.g., compiler feedback) is available, we also observe a variable-length, natural language external input, $f_i$ (e.g., in math problems we ask the model to correct itself). We also observe a scalar reward value $r(s_i, y)$, denoted as $r_i$ in short. Let us denote this dataset of \u201con-policy\" model rollouts as $\\mathcal{D}_{on-policy} := \\{(s_i, y_i, f_i, r_i)_{i=1}^t\\}$.\nFor each time-step, we construct an improved version of the response $y$ that we will denote by $\\tilde{y}$. We also record the reward score associated with this improved response as $r(s_i, \\tilde{y})$, or $r_i$ in short. To obtain an improved version of a response $y$, we can employ several strategies. Perhaps the most straightforward approach is to query an off-the-shelf more capable model to provide a correct response given the prompt $x_i$, the previous response $y$, and an optional external feedback $f_i$. We refer to this as the distillation variant of our approach, since it uses a strong \u201cteacher\u201d model to guide self-improvement (note that this is different from the classic notion of knowledge distillation, and we will in fact show results in Section 6.1 that will help understand the differences).\n$\\mathcal{D}_{on-policy + distill} := \\{\\{(s_i, y_i, f_i,t)\\}_{i=1}^{T}\\}$ (4.4)"}, {"title": "4.3. Inference at Deployment Time", "content": "RISE can be run in two modes at inference time. Perhaps the most straightforward way to run the policy $\\pi_{\\theta}(\\cdot | \\cdot)$ trained by RISE is within a multi-turn rollout, where the model samples a new response conditioned on the past context (i.e., state in the multi-turn MDP). This past context consists of the external feedback $p_{est}$ concerning the response $y_{est}$ and the rollout terminates as soon as the current response is judged to be correct according to the environment's answer verification function. Put in other words, we terminate the rollout as soon as the reward is equal to the reward for the oracle response: $r(x, y_{est}) = r(x, y^*)$. This protocol invokes queries to the reward function after each turn in the rollout. Since several reward function queries are performed, we refer to this approach as \u201cwith oracle\u201d.\nRISE can also be run in a mode that avoids the need to query the answer checker or the reward function within a rollout. In this case, we run full-length rollouts by forcing the model to retry, ignoring the correctness of the response. We then utilize a self-consistency mechanism [51] based on majority voting to decide the candidate response at the end of each turn. Concretely, at the end of each turn $j$, we identify the response by running a majority vote over all response candidates from the previous turns $(\\text{maj} (y^{est}_0, y^{est}_1,\\dots, y^{est}_{j}))$ , including turn $j$. We call this \u201cwithout oracle\u201d. A schematic illustration of these approach is shown in Figure 3. Most of our evaluations use no oracle."}, {"title": "4.4. Practical Algorithm and Implementation Details", "content": "A complete algorithmic pseudocode for each approach is shown in Appendix C. We trained 7B models via RISE and found that these models often could not adhere to response style and instructions for improving their responses when generating on-policy data. As a result, before running on-policy data collection, we find it often useful to run an initial phase of supervised fine-tuning on in-domain, multi-turn rollouts generated from a capable model to provide style and instruction-following information to the learner. We call this the \u201cknowledge boosting\u201d stage. We then run on-policy rollouts starting from a boosted model. In each iteration, we generate 1 trajectory for each unique problem. We then run fine-tuning, with hyperparameters and details in Appendix D. For iterative fine-tuning, we find that starting from the base model but training on data from all iterations thus far is more beneficial than continued fine-tuning from the checkpoint obtained in the previous iteration."}, {"title": "5. When and Why is Self-Improvement Over Turns Possible?", "content": "A natural question to ask is why self-improvement with RISE even possible. One might surmise that the model may simply not have enough knowledge to correct its own mistakes if it is unable to correctly answer the problem in the first turn. Then, why is it possible to teach the model to correct its own mistakes? In this section, we provide the reason why this kind of self-improvement is possible, supported with empirical evidence to justify our hypotheses.\nIteratively teaching a model how to make updates on a given response can be crucial when representing the target distribution $p^*(y|x)$ requires more capacity than what the model $\\pi_\\theta$ affords by conditioning on only the input prompt tokens. When the target distribution requires greater capacity, learning a sequence of conditionals, $\\pi_\\theta(y_{i+1}|x, y_{0:i})$ followed by marginalization is expected to induce a more flexible marginal distribution over $y_i$ given $x$. This hypothesis is akin to the difference between diffusion models [42] and variational autoencoders (VAEs) [25] in image generation: iteratively fitting a sequence of generative distributions over intermediate noisy inputs in a diffusion model gives rise to a more flexible distribution [43] than monolithic variational auto-encoding, even though diffusion models still utilize"}, {"title": "6. Experimental Evaluation", "content": "The goal of our experiments is to demonstrate the efficacy of RISE in instilling language models with the ability to self-improve their responses over turns. Our experiments answer the following questions: (1) How effectively can RISE improve performance over multiple sequential attempts (i.e., turns) at a given prompt?; (2) Does the performance of RISE improve with more rounds of iterative training?; (3) Does the self-improvement strategy induced by RISE generalize to novel problems that are out of the training domain? and finally; (4) What is the best data composition for training RISE? To this end, we compare RISE to other prior and baseline approaches, and perform ablations on GSM8K [11] and MATH [18].\nBaselines, comparisons, and evaluation. We compare RISE to several prior methods that attempt to induce similar self-improvement capabilities: (a) self-refine [21, 31] that prompts a base model to critique and revise its mistakes; (b) GloRE [17], which trains a separate reward model to locate errors and a refinement model to improve responses of a base LLM; and(c) self-consistency [51], which runs majority voting on multiple responses from the first turn as a baseline to compare to our sequential strategy. We tried to construct fair comparisons between RISE and these methods by using a similar-sized model [23, 58], but differences in the base model, training data, and evaluation setups still prohibits us from performing an apples-to-apples comparison in some cases. Nonetheless, we can still hope to understand the ballpark of improvement by contextualizing our results with these prior works. We also compare to V-STaR [19], but since this is not an fair comparison, we defer it to Appendix B.\nWe evaluate RISE in both modes at inference time: with and without an oracle (Section 4.3) at the end of five turns. Concretely, these metrics are defined as follows:\n\u2022 with oracle, \u201cp1@t5", "m1@t5\": this run does not terminate the rollout before five turns, and we compute the maj@1 performance on the candidates produced in each turn as detailed in Section 4.3.\nWe also compare maj@K performance at the first turn for all the models we train (\u201cm1@t1": "m5@t1\").\""}, {"title": "6.1. Does RISE improve performance over multiple turns compared to other approaches?", "content": "Main results. We present the comparisons in Table 1. First, note that RISE (\u201cIteration 1\u201d and \u201cIteration 2\") boosts up the LLama2 base model's five-turn performance by 15.1% and 17.7% respectively with each iteration on GSM8K and 3.4% and 4.6% on MATH, w/o any oracle. Interestingly, we found using prompting-only self-refine [31] largely degrades performance across the board, even with a strong proprietary model, GPT-3.5. The strongest 7B base models, Mistral-7B and Eurus-7B-SFT [58], when coupled with standard prompting, are only able to improve their performance, but only by 5.3% / 11.6% and 0.9% / 4.0% respectively on GSM8K and MATH, which is significantly lower than our approach. The performance of GLoRE improves only by 3.4% on GSM8K (over two turns), but this is still lower than our approach, which improves by 6.3% in two turns and 13.4% in three turns (see Appendix B.1). This indicates that RISE is effective in teaching models how to improve their own errors. To summarize, training with RISE gives the largest performance improvement gains compared to other approaches both with and without the use of an oracle, and these gains are transferred to other base models.\nOne might also hypothesize that the performance gains with RISE here are largely a result of utilizing\""}, {"title": "6.1.1. Can RISE Effectively Make Use of Mistakes and Correct Them?", "content": "One concern that arises from prior results on self-refinement or self-correction is whether the model can truly correct itself over turns or whether the improvement comes from the effect of sampling more answers and picking the best one. In Table 1, we see that sequentially improving responses via RISE (\"maj@1@turn5\") outperforms sampling 5 responses in parallel at the first turn and applying a majority vote on them (\u201cmaj@5@turn1\"). Please note that this comparison utilizes an equal number of samples, with the only difference being that these samples are drawn in parallel at the first turn in one case and sequentially at the end of five turns in the other. Comparing maj@5 performance at the end of 1 turn and 5 turns, we observe a consistent 4% to 8% improvement on GSM8K and an 6.5% improvement on MATH (with Mistral-7B model). This means that RISE can imbue models with a self-improvement ability, while running parallel sampling alone on any model cannot endow the same ability. Even the maj@5@turn1 performance of standard single-turn SFT on the data used by RISE is substantially worse than the sequential maj@1@turn5 performance of RISE, implying that the algorithmic protocol of RISE plays a critical underlying role. Finally, we also remark that in Figure 6, we showed that the sequential procedure learned by RISE over five turns could solve a significant fraction of problems that were unsolved by pass@B for much larger values of B \u226b 5 in the first turn, implying that sequential RISE can actually tackle prompts that were not solvable by simply sampling more responses in the first turn.\nOne might also speculate if these improvements in sequential improvement ability largely come at a cost of reduced improvements in first turn performance. In addition, we also observe that running multiple iterations of RISE still preserves the first turn performance while improving the 5-turn performance."}, {"title": "6.1.2. How Does the Base Model Affect RISE?", "content": "The performance of RISE with Llama2-7B on an absolute scale is lower than the best models specifically fine-tuned on math data (e.g., Eurus-7B-SFT or Mistral-7B). However, we find that RISE is still effective on top of Mistral-7B base model. In fact, our performance at the end of five turns outperforms one of the best 7B SFT models, customized to math reasoning. Compare the m1@t5 performance of Eurus-7B-SFT and Mistral-7B in RISE (ours), to find that Mistral-7B + RISE outperforms Eurus-7B-SFT."}, {"title": "6.1.3. Self-Distillation Version of RISE", "content": "We also compare the performance of RISE with entirely self-generated data and supervision (Equation 4.4, N = 16) after one iteration directly on top of more capable models: Mistral-7B and Llama-3-8B on GSM8K in Table 2, without any knowledge boosting phase. We find that this variant also improves the 5-turn performance of the base model compared to the first turn: compare \u201cm1@t5\u201d vs \u201cm1@t1\" for both the models Llama-3-8B and Mistral-7B, where RISE boosts the sequential self-improvement performance by more than 1% compared to turn 1 performance w/o any oracle.\nOf course, we also note that this version of RISE does not outperform the \u201cm5@t1\u201d performance of the fine-tuned model. We expect this to be largely a function of one single iteration of training. Since the self-distillation version of RISE utilizes best-of-N sampling against the same model to produce supervision for self-improvement, RISE would first have to match the performance of best-of-N sampling before it can start to improve over it via reward maximization. Due to the significant gap between the base model's m5@t1 and m1@t5 performance, we expect that this will take quite a few iterations or a fully online RL algorithm. We did not have computational resources and infrastructure to run multiple iterations, but this is an interesting avenue for future work. In this self-distillation setting, we could also divide the computation between sequential and parallel sampling strategies to get the best results at the end of five turns. Nonetheless, this result shows that even by training on self-generated samples, RISE can actually amplify the sequential sampling performance of the base model."}, {"title": "6.2. Does the Performance of RISE Improve with Iterative Training?", "content": "Next, we attempt to understand if RISE improves with multiple rounds of training on on-policy data. As shown in Tables 1 and 2, the performance of RISE improves from iteration to iteration constantly. The 5-turn performance of RISE, both with and without an oracle, exhibits a clear improvement with more rounds. This implies that iterative self-training procedures of the form of STaR [61] can also be combined with RISE to train models for self-improvement. This also perhaps serves as a strong hint towards the potential utility of full online reinforcement learning (RL) techniques."}, {"title": "6.3. Does RISE Also Improve Sequential Performance on Out-of-Distribution Prompts?", "content": "In Table 3, our aim is to evaluate the robustness of the strategy induced by RISE on new, unseen prompts. Specifically, we compare the performance of the RISE model trained with a dataset on evaluation prompts"}, {"title": "6.4. What Data Compositions and Data Quantity are Crucial for RISE?", "content": "We now study how different data compositions affect the performance of RISE with the goal of answering questions such as should we collect on-policy error correction data like DAgger [36] or should we bias towards high-quality off-policy data?. To understand the utility of different data compositions, we enlist the three aspects RISE: (a) the use of multi-turn rollout data for fine-tuning, (b) the use of unsuccessful / suboptimal rollouts via weighted supervised fine-tuning compared to na\u00efve supervised learning, which only utilizes successful rollouts for fine-tuning; and (c) the use of on-policy rollouts and self-generated or oracle data. We will now perform controlled experiments to understand the effect of each of these factors on the overall performance of RISE."}, {"title": "7. Discussion, Future Directions, and Limitations", "content": "We presented RISE, an approach for fine-tuning LLMs to be able to improve their own responses over multiple turns sequentially. RISE prescribes an iterative RL recipe on top of on-policy rollout data, with"}, {"title": "A. Additional Ablations on Data Composition and Weak-to-Strong Generalization", "content": ""}, {"title": "A.1. Inclusion of Correct-to-Correct Data", "content": "Intuitively, self-improvement over turns is largely only possible when the model can learn to verify the correctness of its previous response and decide to appropriately modify its response toward correctness. Thus far, the RISE has only trained on data that showed how to convert incorrect responses to correct responses but never illustrated how the model could act on correct responses. To understand if perfor-mance can be boosted by also illustrating examples of how the model could act on correct responses, we ran a number of ablations. We took the RISE data generated during Iteration 1 of training on GSM8K with Llama2-7B and modified the multi-turn rollouts to create several cases. First, we duplicated the correct response appearing at the end of every successful multi-turn rollout and trained for one extra turn. This should teach the model that correct responses should not be modified, unlike incorrect responses appearing in previous turns in the rollout. Second, we also ran a variant in which the correct response appearing at the end of every successful rollout is followed by a different correct response. This variant should teach the model that if it chooses to modify a correct response, it must still produce another correct response.\nAs shown in Table 4, all methods improved performance over the base model, though only appending with a successful rollout with a novel correct response leads to best performance. The default design of RISE in the main paper attains a close second position, and repeating a correct response at the end of a successful rollout largely reduces performance. We suspect that the poor performance of repeating the same correct response is largely a result of inducing spurious correlations due to data duplication.\nTo further investigate self improvement capabilities, we analyzed the percentage of correct responses changing to incorrect responses in consecutive turns (Ti to Ti + 1), as illustrated in Figure 10. Generally, a decreasing trend suggests better self-improvement, while lower absolute values indicate better resistance to noisy feedback. The results reveal unexpected patterns across configurations. The Boost configuration shows the poorest performance, with the highest overall percentages and an increase from turn 4 to 5, suggesting that it struggles to consistently maintain correct responses. Repeating a correct response shows the lowest initial percentage (6.3%) but increases from turn 3 onward, indicating potential issues in extended interactions. Both Default RISE and appending a different correct response demonstrate a favorable trend, steadily decreasing from 12.3% to 3.9% and from 9.8% to 3.3%, respectively, suggesting a good balance between maintaining correct responses and allowing improvements. These findings provide nuanced insights into the stability and self-improvement capabilities of RISE and align with our earlier observation of its superior performance in overall accuracy."}, {"title": "A.2. Weak-to-Strong Generalization: RISE on Weak Model Data Improves Strong Models", "content": "In this section, we compare the performance of Llama2 and Mistral-7B with RISE in the weak-to-strong setting [3]. Concretely, we are interested in using data generated via RISE with a weak model (Llama2-7B) to train a strong model (Mistral-7B). Our analysis reveals intriguing insights into the transferability of RISE-generated data across models of different capabilities.\nAs shown in Table 5, we find that Mistral-7B + Iteration 1 data generated from Llama2 outperforms training the Llama2-7B model itself on these data (i.e., Llama2-7B + Iteration1) on all the metrics reported with particularly significant improvements in multi-turn reasoning (m1@t5). In fact, training on multi-turn rollouts from Llama2-7B also outperforms training on on-policy Mistral-7B rollouts as well. Interestingly, we observed that training Llama2-7B on multi-turn rollouts from Mistral-7B performs worse than training on on-policy Llama2-7B rollouts, suggesting that Llama2-7B, despite its lower absolute performance, demonstrates more informative mistakes that can be leveraged to better boost the self-improvement capability. This phenomenon underscores the importance of the quality and nature of errors in the training data, rather than just the overall performance of the model that generates them. These findings collectively suggest that the data generated from a weaker Llama2 model can still be used to induce a self-improvement capability in a stronger model, although the reverse is not true (as is also evident from the fact that using GPT-3.5 rollouts in the boosting phase for training does not improve performance for any model in Table 1). We suspect that this is becaue the reverse poses a much harder"}, {"title": "B. Additional Results", "content": ""}, {"title": "B.1. Complete Comparisons and Discussion: Extended Version of Table 1", "content": "We provide an extended version of Table 1, with a clear explanation of how we implement baselines and a discussion of comparisons."}, {"title": "Comparison with Self-Refine [31]", "content": "To build a self-refine baseline [31] evaluation, we slightly modified our evaluation pipeline following the self-refine approach. In this setup (Figure 11), the model generates an initial response, and then the environment prompts the model to locate errors in the generated solution and refine its answer based on the initial response and the identified error.\nHowever, our experiments show that without any oracle hint from the environment or human feedback, the self-refine approach leads to a degradation in performance across all models. Only when oracle feedback is available to assist with early termination does the self-refine approach provide a slight performance boost. This highlights the limitation of the self-refine structure in effectively improving model performance without external guidance, which is also observed in [22].\nIn contrast, the model trained with RISE can attain consistent performance improvements without relying on an oracle. By training the model to iteratively refine its responses, our method enables the model to self-correct and improve its performance over multiple turns. This showcases the effectiveness of our approach in comparison to the self-refine baseline, as it allows for more robust and consistent performance gains without the need for the oracle assistance."}, {"title": "Comparison with GLORE [17]", "content": "GLORE is a multi-model system that relies on a student model to propose drafts, an Outcome-based Reward Model (ORM) or Step-wise ORM to locate errors at different granularity levels, and a Global or Local Refinement Model for adjusting these errors. Since no code was openly available for this approach, in our experiments, we compared to the numbers from the main paper Havrilla et al. [17]. While the comparison against GLORE is already apples-to-oranges since our"}, {"title": "C. Pseudocode"}]}