{"title": "From Pixels to Words:\nLeveraging Explainability in Face Recognition\nthrough Interactive Natural Language Processing", "authors": ["Ivan DeAndres-Tame", "Muhammad Faisal", "Ruben Tolosana", "Rouqaiah Al-Refai", "Ruben Vera-Rodriguez", "Philipp Terh\u00f6rst"], "abstract": "Face Recognition (FR) has advanced significantly with the\ndevelopment of deep learning, achieving high accuracy in several appli-\ncations. However, the lack of interpretability of these systems raises con-\ncerns about their accountability, fairness, and reliability. In the present\nstudy, we propose an interactive framework to enhance the explainability\nof FR models by combining model-agnostic Explainable Artificial Intel-\nligence (XAI) and Natural Language Processing (NLP) techniques. The\nproposed framework is able to accurately answer various questions of\nthe user through an interactive chatbot. In particular, the explanations\ngenerated by our proposed method are in the form of natural language\ntext and visual representations, which for example can describe how dif-\nferent facial regions contribute to the similarity measure between two\nfaces. This is achieved through the automatic analysis of the output's\nsaliency heatmaps of the face images and a BERT question-answering\nmodel, providing users with an interface that facilitates a comprehensive\nunderstanding of the FR decisions. The proposed approach is interactive,\nallowing the users to ask questions to get more precise information based\non the user's background knowledge. More importantly, in contrast to\nprevious studies, our solution does not decrease the face recognition per-\nformance. We demonstrate the effectiveness of the method through dif-\nferent experiments, highlighting its potential to make FR systems more\ninterpretable and user-friendly, especially in sensitive applications where\ndecision-making transparency is crucial.", "sections": [{"title": "1 Introduction", "content": "Face biometrics is a popular field that analyzes multiple techniques mainly for\nidentification and verification purposes [15]. Several studies have shown its appli-\ncation to areas such as person recognition [34,10], synthetic face generation [23,7],\nand security [19], among others. In recent years, with the fast development of\ndeep learning, significant advances have been made in Face Recognition (FR),\nsurpassing previous benchmarks with impressive results [8,17,24]. However, most\nof the state-of-the-art FR systems are based on Deep Neural Networks (DNN),\nwhich are often seen as black boxes that are hard to interpret, raising concerns\nabout their fairness, accountability, and reliability [20]. Due to the lack of trans-\nparency of these DNN and the sensitivity of the biometric data, the explainability\nof FR models is becoming increasingly important, especially after the recent EU\nArtificial Intelligence Act\u00b3. Additionally, recent studies have proved the bias of\nthe FR models, proposing new ways to mitigate it by interpreting the model's\noutput [6,32].\nThe explainability of the models is especially important for real-life appli-\ncations where choices can have a significant impact and a sensitive effect on\nsociety [2]. As a result, leveraging explainability can bring non-technical users\ncloser to understanding the operation of these FR models by making them eas-\nier to understand. Several Explainable Artificial Intelligence (XAI) techniques\ncan be used for this purpose. However, as the complexity of state-of-the-art\nmodels cannot be covered only by these simple techniques, researchers have pro-\nposed model-agnostic techniques that explain the predictions or decisions made\nby a machine learning model, without needing to know how the model works\ninternally [22,28]. An example of such techniques applied to FR models is Grad-\nCAM [29]. Grad-CAM is a visual explanation technique used to represent which\nregions of an input image are crucial for the decision about a particular class.\nGrad-CAM produces a heatmap by weighting the feature maps with these gra-\ndients, highlighting the regions in the input image that most contribute to the\nfinal decision of a particular class."}, {"title": "2 Proposed Method", "content": "This section describes the proposed framework, which comprises three main\nmodules: i) the FR system and confidence estimation, described in Sec. 2.1, ii)\nthe explainability method, described in Sec. 2.2, and finally iii) the user-friendly\nQA interface based on NLP, described in Sec. 2.3."}, {"title": "2.1 Face Recognition System and Confidence Estimation", "content": "To enable our framework to provide explainable conversations with a user, we\nfirst need to set up a FR system and a confidence estimation solution. Our frame-\nwork can work with arbitrary FR systems and confidence estimation methods.\nFor our setup, to compare two face images, our proposed framework performs\nthe face verification of the two face images, as described in Figure 2. For this\npurpose, we first detect and align the faces in the images using the MTCNN\nmodel [35]. After extracting these landmarks, we align the original face images\nand forward them to the FR model in a frontal-view normalized pose. Regarding\nthe FR model choice, we consider the ArcFace model [8] for its robustness and\nwide usage in different applications [34,18]. Finally, we compare the feature em-\nbeddings of the two face images using cosine similarity, to obtain a final score.\nIf this score is above or below a decision threshold, the matching decision is\nclassified as match (genuine) or non-match (impostor) [34,10].\nHowever, the final decision of the FR system is just a decision based on the\ncosine similarity, lacking the explainability of the decision. To increase the inter-\npretability, we use the Probabilistic Interpretable Comparison (PIC) score [27].\nThis metric is developed based on the concept of probabilistic comparison,\nwhereby it evaluates the similarity score between multiple biometric samples,\nsuch as two face images, concerning the distribution of scores from the same\nidentity and different identities. Its purpose is to determine the probability that\nthe scores at hand are related to the same identity distribution rather than a\ndifferent identity distribution. This metric provides a clear and meaningful prob-\nabilistic assessment of the matching confidence. Unlike previous methods that"}, {"title": "2.2 Explainability Method", "content": "In addition to the PIC-Score described in the previous section, our proposed\nmethod increases the explainability of the FR system through: i) a visual rep-\nresentation of the key facial regions for the decision, using saliency heatmaps,\nand ii) an explainability table that includes values for the most and least im-\nportant facial regions in the decision. For this purpose, we first generate saliency\nheatmaps [30] from the face embeddings produced by the FR model. A saliency\nheatmap is a representation that highlights the most significant parts of an image\nor data, indicating where the attention should be focused. In the present study,\nwe analyze five different saliency heatmaps based on the five different techniques\npresented in [25]. We explain\nnext the key details for each of them:\nSingle Aggregation (S0+): Finds the pixels that make the similarity score\nincrease the most.\nGreedy Aggregation (S1+): Adds the most important pixels one at a\ntime, by repeating Single Aggregation several times.\nSingle Removal (S0-): Finds the pixels that, when removed, make the\nsimilarity score drop the most.\nGreedy Removal (S1-): Removes the most important pixels one by one,\nby repeating the Single Removal iteratively.\nAverage Method (AVG): This method calculates the average value across\nthe above 4 methods.\nFinally, by aligning the saliency heatmaps with the facial landmarks, we are\nable to determine which facial regions are the most important ones in the final\ndecision. The following facial regions are considered in the analysis: left and right\neyebrows, left and right eyes, nose, left and right cheeks, chin, and lips. To better\nquantify the importance of each facial region, we transform the information of\nthe saliency heatmaps into an explainability table. An example of this process\ncan be seen in Fig. 4. To get a numerical value for each of the facial regions, we\ndivide the output range of the saliency heatmaps into five intervals, assigning an\nimportance score to each facial region based on the saliency heatmaps values.\nThe scoring system spans from 1 to 5, where 1 represents the most important and\n5 the least. The final values are included in the explainability table, providing a\nsimple visual representation in terms of explainability."}, {"title": "2.3 Question-Answering Interface", "content": "Finally, to provide a user-friendly and interpretable interface for all potential\nusers, we propose integrating the FR system and confidence estimation (Sec. 2.1)\nand explainability method (Sec. 2.2) into a QA interface. For this purpose, we\nuse the BERT Question-Answering (BERT-QA) model4 [9]. This model takes\ntwo inputs: i) the user's query, which is prompted by the user directly, and ii)\nthe context, which refers to the complete information of the matching process\nincluding the information needed to answer a specific query. Finally, the model\nreturns a refined answer. Figure 5 shows the complete pipeline for the proposed\nQA interface.\nRegarding the functionality of the BERT-QA model, whenever the user asks\na query, both the query and the context are sent to the BERT-QA model. The\nmodel processes the input and generates a response by understanding the con-\ntext and extracting relevant information from it. The BERT-QA model outputs\nthe answer along with a confidence score for the generated answer. If the confi-\ndence score of the answer is below a predefined threshold, it indicates uncertainty\nor ambiguity in the answer. This can happen due to a limitation of the BERT\nmodel, which does not always perform well with long input contexts. Therefore,\nif the confidence score is less than the threshold score, the answer is not accepted,\nand we ask the BERT-QA model the same query again but with a smaller con-\ntext called sub-context. This sub-context is extracted from the original context\nand consists of a set of the most relevant sentences for the given query. These\nsentences are selected by calculating the similarity of the user query with each\nsentence in the complete context and choosing the ones with the highest simi-\nlarity. For this purpose, we use the sentence embedding model MPNet-base [31].\nThis selected sub-context is then passed to the same BERT-QA model along\nwith the user's query. The sub-context helps in obtaining a more focused and\naccurate response, allowing the BERT-QA model to generate a final answer and\npresent it to the user."}, {"title": "3 Experimental Setup", "content": "The experimental protocol and metrics considered in the present study have\nbeen designed to analyze quantitatively and qualitatively the performance of\nour end-to-end method, evaluating the three different modules.\nFor the experiments, we consider a pre-trained5\nFR model using the ArcFace loss [8]. The model is based on a ResNet-100 back-\nbone trained on MS-Celeb-1MV3 [11]. To evaluate the performance of the FR\nmodel we use Labeled Faces in the Wild (LFW) database [13]. LFW is a very\npopular database in the FR field, containing over 13, 000 labeled face images from\ndifferent real-life situations. These pictures include different lighting conditions,\nposes, facial emotions, and backgrounds, providing a benchmark to evaluate FR\nmodels in real-world conditions. Regarding the metrics of the FR system, we\nconsider the Detection Error Tradeoff (DET) curve, the False Non-Match Rate\n(FNMR) for a False Match Rate (FMR) value of 0.01% (high-security system),\nand the Equal Error Rate (EER).\nWe analyze the variability of the scores generated\nby the different XAI techniques considered in the analysis (i.e., Single Removal,\nGreedy Removal, Single Aggregation, Greedy Aggregation, and the Average).\nIn particular, the explainability analysis is carried out using the proposed ex-\nplainability table, checking whether the majority of the XAI techniques focus\non the same facial regions to make the final decision or not. The following met-\nrics are calculated for each of the facial regions: i) Mean, defined as the average\nvalue of the different XAI techniques, and ii) Ratio of 1s, defined as the number\nof \"1\" values (i.e., most important facial region) observed in the different XAI\ntechniques, divided by the total number of XAI techniques considered (5).\nTo evaluate the performance of the proposed\nBERT-QA model for FR scenarios, we create a set of predefined queries to ask\nthe model. These queries have different levels of complexity, going from simple\nones like \"What are the results of the comparison?\u201d to more in-depth ones that\nrequire the model to make use of most of the information included in the context,\nlike \"How did you come to this conclusion?\". The evaluation of this module is\nperformed qualitatively and quantitatively by checking the answers given by the\nmodel to each of the queries."}, {"title": "4 Experimental Results", "content": "4.1 Analysis of the Face Recognition System\nFirst, we analyze the performance of the face verification system over LFW\ndatabase. The DET curve for this database can be seen in Figure 6. The EER\nis 5.12%, while the FNMR value for a FMR fixed at 0.01% is 6.07%. As can be\nseen, accurate results are achieved by the ArcFace model, proving to be reliable\nfor high-security scenarios. Additionally, the model demonstrates to be balanced\nbetween false rejections and false acceptances, as indicated by the EER. This\nbalance ensures that the system not only minimizes the risk of unauthorized\naccess but also maintains user convenience by reducing the likelihood of false\nrejections."}, {"title": "4.2 Analysis of the Explainability Method", "content": "Table 1 shows an example of two face images to compare, including the corre-\nsponding XAI techniques analyzed in the present study and the values inserted\nin the proposed explainability table. In this particular example, we intend to\nanalyze the model's ability in challenging scenarios such as FR while wearing a\nface mask. As can be seen, each XAI technique provides valuable insights into\nthe decision-making process by identifying the key facial regions. However, for\nsome specific XAI techniques, such as Single Removal or Single Aggregation, we\nobserve a higher variability in the explainability table depending on the facial\nregion. To ensure a reliable approach, and select the optimal facial regions for the\ndecision, we decide to consider the metrics Mean and Ratio of 1s values, which\ngroup the information from all the saliency heatmaps. The ratio of 1s simply\ndescribes what ratio of the XAI techniques has a score of 1 for the correspond-\ning region. Following this approach, we can see that the eyebrows and eyes are\nselected as the most important facial regions for this particular example, as one\nof the face images is covered by a face mask."}, {"title": "4.3 Analysis of the Question-Answering Interface", "content": "Qualitative Analysis. Figure 7 provides graphical examples of the user inter-\naction with the chatbot (BERT-QA model), introducing two impostor compar-\nisons as input. As can be seen on the left chat, our method correctly predicts the\nresult of the comparison with high confidence and the chatbot is able to provide\nvaluable insights and explanations on how the decision is made. However, on the\nright chat, we provide an examples in which the prediction on the pair images\nis wrong. Users can prompt various questions about the facial recognition re-\nsults, decision-making process, confidence levels, metrics, landmarks, etc., and\nthrough reviewing the chatbot answers (e.g., the confidence score) detect that\nthe prediction is wrong and manually review it through the saliency heatmaps,\nfor example. The proposed method provides a user-friendly interface for inter-\npreting FR results, ensuring transparency and enhancing user understanding of\nthe complex FR system. This interactive approach bridges the gap between tech-\nnical outputs and user comprehension, fostering trust and usability, especially\nfor users with no experience in the topic.\nQuantitative Analysis. For completeness, we quantitatively analyze the cor-\nrectness and stability of the answers provided by the chatbot. This analysis is\nshown in Table 2. For the experiment, we generated 16 variants of each of the\nnine questions shown in the table. Then, each question variant is tested on a\nrandom image pair resulting in 144 conversations that were manually checked\non random image pairs. This allows us to analyze how stable the QA module is\nregarding different sentence structures and provides an indicator of the correct-\nness of its answers. The questions asked vary from questions about the decision\n(\"What is the decision?\u201d), questions about the decision confidence (\u201cHow sure\nare you about this decision?\"), technical questions (\u201cWhat is explainable AI?\"),\nup to questions about the reason for the decision (\"What is the most important\nfeature of this decision?\"). These questions were chosen to enable the user to\nmake an informed decision towards trusting or distrusting the system's decision\nbased on its reasoning."}, {"title": "5 Conclusions", "content": "The present study has focused on the critical challenge of explaining the decisions\nmade by FR systems, as current FR systems are often seen as black boxes hard\nto interpret. In particular, our proposed framework enhances the explainability\nof the deployed FR systems by combining model-agnostic XAI and NLP tech-\nniques. As a result, the explanations generated by our proposed framework are\nin the form of natural language text and visual representations. This is achieved\nthrough the automatic analysis of the output's saliency heatmaps of the face\nimages and a BERT-QA model, providing users with an interface that facilitates\na comprehensive understanding of the FR decisions.\nTo summarize, our proposed explainability framework is characterized by:\ni) interactive explanations, enabling users to ask questions to get more precise\ninformation depending on the user's background knowledge; ii) model-agnostic\nscheme, which can be easily applied to any FR model regardless of the spe-\ncific architecture of the model; iii) high recognition performance, unlike recent\npopular foundation models such as ChatGPT whose performance considerably\ndegrades in some application scenarios [6,12], as our solution does not alter the\ndeployed FR system; and iv) scalability, as the proposed framework can be eas-\nily extended with additional information that the user might be interested in\nwithout the need for retraining the system."}]}