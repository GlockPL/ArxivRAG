{"title": "Towards Open-Vocabulary Video Semantic Segmentation", "authors": ["Xinhao Li", "Yun Liu", "Guolei Sun", "Min Wu", "Le Zhang", "Ce Zhu"], "abstract": "Semantic segmentation in videos has been a focal point of recent research. However, existing models encounter challenges when faced with unfamiliar categories. To address this, we introduce the Open Vocabulary Video Semantic Segmentation (OV-VSS) task, designed to accurately segment every pixel across a wide range of open-vocabulary categories, including those that are novel or previously unexplored. To enhance OV-VSS performance, we propose a robust baseline, OV2VSS, which integrates a spatial-temporal fusion module, allowing the model to utilize temporal relationships across consecutive frames. Additionally, we incorporate a random frame enhancement module, broadening the model's understanding of semantic context throughout the entire video sequence. Our approach also includes video text encoding, which strengthens the model's capability to interpret textual information within the video context. Comprehensive evaluations on benchmark datasets such as VSPW and Cityscapes highlight OV-VSS's zero-shot generalization capabilities, especially in handling novel categories. The results validate OV2VSS's effectiveness, demonstrating improved performance in semantic segmentation tasks across diverse video datasets. The source code will be made available at: https://github.com/AVC2-UESTC/OV2VSS.", "sections": [{"title": "I. INTRODUCTION", "content": "Semantic segmentation, the task of classifying each pixel into distinct semantic categories [1, 2, 3, 4, 5, 6], is a core component of computer vision with substantial practical value. However, traditional segmentation models are constrained by being trained on a fixed set of predefined categories. Consequently, they can only recognize categories present in the training data, presenting a significant challenge for real-world applications where novel or unseen categories frequently emerge. This limitation hinders the model's adaptability and scalability in dynamic, diverse environments.\nRecent progress has been made in addressing this issue, particularly in image-based semantic segmentation. Notably, studies have integrated the CLIP model [7] into semantic segmentation [8, 9] to achieve open-vocabulary image semantic segmentation. These efforts leverage vision-language knowledge to enhance generalization capabilities, particularly for unseen categories. During testing, the model is capable of segmenting unseen categories as long as their names are provided.\nIn contrast, the challenge of open-vocabulary video semantic segmentation (OV-VSS) has not been systematically explored, despite the fact that our real-world experiences are better represented by dynamic video content rather than static images. Video-based semantic segmentation presents additional complexities due to the temporal dimension, which captures evolving scenes, objects, and contextual information that cannot be encapsulated in a single image. Studying VSS is essential because videos are more prevalent in practical applications, such as autonomous driving, surveillance, and activity recognition, where recognizing objects and actions over time is critical.\nWhile a straightforward solution might be to apply existing image-based methods, such as SAN [8], to video datasets, this approach neglects the rich temporal context inherent to video footage. To validate this hypothesis, we conducted a series of experiments by applying image-based methods to video datasets. The results, as shown in Fig. 2, reveal significant performance degradation, emphasizing the need for a more refined approach to VSS. These findings will be further elaborated upon in the subsequent sections.\nInspired by this, we introduce the challenge of open-vocabulary video semantic segmentation (OV-VSS), which differs from conventional VSS in its ability to segment and classify objects not predefined during training. The key distinctions are illustrated in Fig. 1. In OV-VSS, our goal is to classify every pixel within each video frame, including categories the model has not previously encountered. To address this task, we propose a robust baseline model, OV2VSS, which integrates a spatial-temporal fusion module that leverages temporal rela-"}, {"title": "II. RELATED WORK", "content": "Video Semantic Segmentation (VSS) aims to categorize pixels in each video frame according to predefined categories. Early VSS efforts were constrained by datasets like Cityscapes [13] and NYUDv2 [14], which provided annotations for only a few sporadic frames. Similarly, the CamVid dataset [15] suffers from limited scale and low frame rate. The introduction of the VSPW dataset [12], with 124 classes and a wide range of scenarios, significantly advanced the field. It features an average of over 70 annotated frames per video, providing rich data for training and evaluation.\nOn this challenging dataset, existing methods can be divided into two main categories. One category focuses on improving performance with minimal additional computation [16, 17, 18, 19, 20, 21, 22, 23, 24]. These methods often prioritize keyframes, applying high-computation networks to keyframes and using lower-computation methods, such as optical flow, to process non-keyframes and capture temporal correlations.\nConversely, another set of methods [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38] prioritizes segmentation accuracy by deploying resource-intensive networks on each frame to achieve superior results. However, these methods rely heavily on segmenting classes present in the training datasets, limiting their adaptability to unseen classes a significant challenge for VSS in real-world applications, where new and previously unencountered classes need accurate identification and segmentation.\nVision-language models (VLMs) represent a significant leap in connecting visual and textual modalities. Pre-trained models like CLIP [7] and ALIGN [39] harness large datasets to enable impressive zero-shot object recognition capabilities, boosting performance in tasks such as classification, captioning, and segmentation.\nIn image segmentation, VLMs excel at zero-shot predictions. For example, Xu et al. [8] propose a side network for open-vocabulary segmentation, while Ding et al. [40] decouple segmentation and classification. Ghiasi et al. [9] align text queries with image regions.\nHowever, applying VLMs directly to video data introduces new challenges, as models like CLIP are trained on static images. Efforts like Wang's [10] aim to extend VLMs to video instance segmentation, but further research is needed to address the complexities of dynamic visual content."}, {"title": "III. OUR METHODS", "content": "For a given video clip, a training sample can be represented as follows:\n$([I_{t-k_1},..., I_{t-k_n}, I_t, I_{random}], G_t)$,\nwhere we define that\n$I_t \\in \\mathbb{R}^{H\\times W}$ denotes the frame of interest, referred to as the target frame at timestamp t, and is accompanied by its associated ground-truth segmentation map $G_t \\in \\mathbb{R}^{H\\times W}$.\n$I_{t-k_1},..., I_{t-k_n}$ represent the n preceding frames, each located $k_1,...,k_n$ frames away from $I_t$ ($k_1 > k_2 > ... > k_n$). In our experiments, $k_{n} - k_{n+1}$ is set to 3.\n$I_{random}$ signifies a temporally distant frame extracted from the same video, contributing temporal correlations to facilitate the training process. During the training stage, a frame is randomly selected as $I_{random}$. In the inference stage, $I_{random}$ will be the most temporally-distant frame in the same video.\nFor each frame $I_{t_i}$, we first pass it through a backbone network to extract intermediate feature maps ${U_t \\in \\mathbb{R}^{H_i \\times W_i \\times C_i}}_{l=1}^{L}$ across different scales from L layers, where $H_i$, $W_i$, $C_i$ represent the height, width, and channels of the feature map, respectively. We simplify by omitting the superscript l. Next, the spatial-temporal context fusion module integrates the features across the video sequence, producing the refined feature $O_t$ for the target frame.\nThe target frame $I_t$ and class name text $T(n)$ for n = 1,..., N are input into the video-text encoding module to derive language-guided features, which are concatenated with $U_t$ to produce position-aware features X. Additionally, the random frame feature $D_{random}$ is enhanced using $O_t$ via the random frame enhancement module, yielding refined feature $\\hat{O_t}$. Finally, the enhanced feature $O_t$ and language-guided feature X are fed into the decoder. The detailed design is discussed in subsequent sections."}, {"title": "A. Spatial-Temporal Context Fusion", "content": "The key distinction between video and image semantic segmentation lies in capturing inter-frame temporal relationships. The Spatial-Temporal Context Fusion (STCF) module is specifically designed to integrate temporal information through an information-rich module and a temporal transfer module.\nTraditional attention-based temporal fusion methods derive the query from the target frame and use reference frames for keys and values. However, this can cause discontinuities due to differences in categories and environments across frames. As the temporal distance increases, the likelihood of such variations also grows.\nThis insight motivates a gradual aggregation approach: instead of directly fusing $I_{t-k_1}$ with $I_t$, we first aggregate the closest frames, such as $I_{t-k_1}$ with $I_{t-k_2}$, then $I_{t-k_2}$ with $I_{t-k_3}$, and so on until $I_{t-k_n}$ is combined with $I_t$. This incremental fusion reduces computational overhead during inference, as the intermediate results from closer frames can be reused for subsequent target frames, streamlining the process."}, {"title": "B. Random Frame Enhancement", "content": "To capture long-term dependencies among video frames, a straightforward approach would be to input all frames into the model. However, this would be computationally prohibitive. Instead, we adopt a strategy of randomly selecting a temporally distant frame relative to the target frame. By aggregating information from this randomly selected frame into the target frame's features, we enable the model to develop a more comprehensive understanding of the environment, thereby enhancing model training.\nIt is worth noting that our STCF module from section III-A integrates features from two closely positioned frames during each operation, gradually incorporating information from neighboring frames. In this context, the random frame selection complements the STCF module. Since the environment depicted in the randomly selected frame may differ from that of the target frame, we employ an attention mechanism to automatically emphasize relevant information while reducing noise.\nSuppose that the backbone features of the random frame $I_{random}$ from a P2T [41] pooling layer are denoted as $D^{l}_{random}$ for the scale l. We aggregate these multi-scale features for the scales from 1 to L using\n$D_{random} = Concat [D^{1}_{random}, D^{2}_{random},..., D^{L}_{random}]$,\n$D_{random} = Linear(D_{random})$.\nInspired by the Object Contextual Representation (OCR) module [42], we propose a contextual representation method that characterizes each pixel by utilizing its corresponding contextual information.\nWe begin by applying a simple convolution to extract the region-level representation. Next, we sum the pixel representations, weighted by their association with the object region. Using an aggregation module, we compute the relationships between pixels and object regions to derive enhanced context-level information, as follows:"}, {"title": "C. Video Text Encoding Module", "content": "Given an image $I \\in \\mathbb{R}^{H\\times W\\times 3}$ and a set of candidate class category prompts ${T(n)}$ for n = 1, . . ., N, instead of directly using the pretrained text encoder from the CLIP model [7], we enhance its performance for our task by introducing a video-specific prompt scheme.\nThe core idea is to leverage video context to enrich text representations within the video. A common approach is to directly use the text representation. However, we demonstrate that this can reduce segmentation performance in video contexts. To address this, we introduce a video-specific text encoding module.\nIn human video comprehension, key terms often help distinguish visual content. For example, including the prompt \"water\" makes it easier to differentiate between \"fish\" and \"horse\" within the visual context.\nSpecifically, we propose the utilization of a multi-head self-attention (MHSA) mechanism [43] to model the correlation between video clips and text. Given the CLIP image encoder $\\Psi_V$ and the text encoder $\\Phi_T$, we initially extract dense image features $F^V = \\Psi_V (I) \\in \\mathbb{R}^{H\\times W\\times C}$ for each input frame $I \\in [I_{t-k_1}, I_t]$ and text features $F^T = \\Phi_T (T_n) \\in \\mathbb{R}^{N\\times C}$ for n $\\in$"}, {"title": "IV. EXPERIMENTS", "content": "We conduct out experiments on two popular datasets: Video Scene Parsing in the Wild (VSPW) [12] and CityScapes [13].\nVSPW. VSPW [12] provides a large-scale benchmark with dense annotations for well-trimmed, long-term video clips, making it one of the most challenging datasets for video semantic segmentation tasks. Its diversity of 124 classes makes it ideal for evaluating open-vocabulary video segmentation. The dataset contains 2806/343/387 videos in the training, validation, and test sets, respectively, with 198244/24502/28887 frames. Each video has an average of 71 frames, with a maximum of 482, resized to 480\u00d7853. Our OV-VSS model will be trained on a subset of classes and tested on another.\nCityScapes. CityScapes [13] is a widely recognized dataset in the field of video semantic segmentation and has been a key benchmark in earlier research. Notably, for each video, only the 20th frame is annotated at the pixel level. The dataset consists of 5,000 labeled frames, divided into 2975 for training, 500 for validation, and 1525 for testing. This setup makes CityScapes particularly suitable for evaluating models with sparse annotations across video sequences."}, {"title": "B. Experimental Setting", "content": "Similar to existing work, we adopt ResNet [50] as the backbone. We set the temporal distances between adjacent frames and the target frame to 3, 6, and 9. We initialize the backbone ResNet with ImageNet pretrained weights, and other parts of the model randomly. We train OV-VSS on VSPW for 10,0000 iterations with a batch size of 4, learning rate initialized as 3e-6. We employ AdamW [51] with weight decay as le-2. We first train the model with a linear warm-up for 1500 iterations. We use 1 NVIDIA RTX 3090 GPU for training, costing 20 hours.\nFor the pretrained CLIP text encoder, the ViT-B/16 [49] backbone is used as default if not specified. We also use prompt templates with class names for all experiments. We follow [40] to generate multiple text embeddings for each class name based on several text prompt templates. Then, we use the mean of these embeddings for final classification.\nDuring training, we adopt random scaling and random cropping for data augmentation. In testing, we perform a single-scale test and use the original 480p image size for inference."}, {"title": "C. Evaluation Protocol", "content": "Following the literature of video semantic segmentation [34, 33], we mainly conduct experiments on VSPW datasets [12], which containing 124 classes. We evaluate our model on their validation set containing 24392 images.\nWe first compare the State-of-the-Art results on the VSPW dataset. As there are no existing video-based open vocabulary semantic segmentation methods for comparison, we conduct a comparison between image-based open vocabulary methods and our approach. By splitting video clips into continuous frames, we train all models and evaluate them on the test set.\nIn order to verify the open-vocabulary capability of the model, previous work [40] requires careful selection of visible classes during training and invisible classes during testing according to super categories. We believe that such a setting limits the ability to fully validate the model. To demonstrate the superiority of our model's performance, we directly select the first 80 classes as visible classes during training and the remaining 44 classes as invisible classes during testing, following the category order of VSPW. To prevent the model from learning information about invisible classes during training, we mask out the invisible classes based on their classification, ensuring that the model only learns the features of the predefined visible classes. Furthermore, we mask out the invisible classes in the video frames before feeding them into the model to ensure that the model solely focuses on the visible classes we set.\nTo verify our cross-dataset performance, we also conducted additional experiments. We directly validated the model trained on the top 80 visible classes in VSPW [12] on the CityScapes [13] dataset. After expert confirmation, 8 categories in CityScapes were new categories that the model had not encountered before.\nFor evaluation, we adopt four metrics to evaluate the results.\n1) mIoU: Mean Intersection over Union (mIoU). mIoU is a standard metric for evaluating image segmentation models. It calculates the average overlap between predicted and ground truth segmentation regions across all classes, balancing precision and recall.\n2) FWIoU: Frequency Weighted Intersection over Union. FWIoU evaluates segmentation performance by weighting the IoU of each category by its pixel count, emphasizing classes with more pixels and accounting for class distribution.\n3) mAcc: Mean Accuracy. mAcc reports the average accuracy for each class in a dataset, ensuring equal contribution from each class. It is useful for datasets with imbalanced class distributions.\n4) pAcc: Pixel-wise Classification Accuracy. pAcc measures the proportion of correctly classified pixels in an image. It provides an overall accuracy assessment but can be biased toward larger classes."}, {"title": "D. Open-Vocabulary Semantic Segmentation Results", "content": "For the image-based methods, Zegformer [40] decouples segmentation and classification by generating class-agnostic segment masks. DeOP [45] proposes a decoupled one-pass structure for computational efficiency. SAN [8] frames the segmentation task as a region recognition problem by attaching a side network. zsseg.baseline [46] presents a two-stage approach to address the discrepancy in processing granularity. FreeSeg [47] introduces a generic framework for universal open-vocabulary image segmentation.\nGiven the lack of text prompt functionality in SAM [48], we designed a novel approach that combines CLIP [7] with the SAM model. This approach aims to establish a baseline for SAM's performance by leveraging CLIP's ability to understand both text and images. Specifically, our method utilizes SAM models to identify regions of interest in video images, after which CLIP is used to classify each identified region into a predefined class. This allows us to adapt SAM for Open-Vocabulary Video Segmentation, enabling the segmentation of novel classes without additional training."}, {"title": "E. Segmentation Results", "content": "In this experimental analysis, we begin by training the model using the initial 80 classes for training purposes. During testing, we evaluate its performance on the remaining 44 novel classes. We categorize the methods into two groups based on their backbone architectures.\nRegarding segment IoU, our approach outperforms other methods, surpassing the second-best methods by 4.09% and 5.32%, respectively. The state-of-the-art comparisons on the VSPW dataset [12] are presented in Table I.\nTo validate the zero-shot capability of our model on novel categories, we conduct direct testing on the CityScapes dataset [13] without any modifications. The results, showcased in Table II, underscore the superior performance of our method across both ResNet-101 and ViT backbones.\nIn addition to the quantitative comparisons mentioned above, we also perform qualitative assessments by juxtaposing our proposed method with the baseline on sampled video clips, as illustrated in Fig. 6. Upon closer inspection, our method consistently generates more accurate segmentation masks across the sampled video clips, reaffirming its efficacy and robustness in real-world scenarios."}, {"title": "F. Ablation Study", "content": "We conduct ablation studies on the large-scale VSPW [12] dataset to validate the key designs of OV-VSS and report the results in Table III. For fairness, we maintain the same settings as in section IV-E, unless otherwise specified. The ablation studies are conducted on the ViT-B2 backbone.\nResults shows the ablation study for each component of OV-VSS. First, we establish a simple baseline that aligns image features with text features. Initially, we train the model in a fully-supervised manner, which is GPU-intensive and requires per-image labels. By integrating a random frame enhancement module, we achieve better results (+0.91%) with a resource-friendly training mechanism. This method processes frames one by one but loses the temporal information between frames."}, {"title": "V. FUTURE WORK", "content": "While this work presents a strong foundation for Open Vocabulary Video Semantic Segmentation (OV-VSS), there are several avenues for future research and improvements that could further advance the field:\n\u2022 Future work could explore integrating additional modalities, such as infrared images [52], depth images [53], or point clouds [54], to enhance segmentation performance. Leveraging these diverse data sources could improve the model's robustness and accuracy, especially in challenging environments where visual information alone is insufficient.\n\u2022 In real-world applications, not only might the categories of test samples be unclear, but training labels can also be inaccurate due to errors or inconsistencies in the annotation process [55, 56]. Future work should investigate how our method handles such label noise, particularly when training labels are noisy. While our current approach demonstrates robust performance with clean labels, it is important to explore how the model performs when the training data contains noisy labels, as this is a common scenario in practical deployments.\n\u2022 In scenarios where the input data quality is low, applying image enhancement techniques could improve the-"}, {"title": "VI. CONCLUSION", "content": "In this study, we explore the task of Open-Vocabulary Video Semantic Segmentation (OV-VSS) and propose a comprehensive end-to-end framework. Our approach introduces several novel components to address the complexities of OV-VSS.\nFirst, we develop a Spatial-Temporal Context Fusion module to effectively capture intra-frame correlations. Building on this, a Random Frame Enhancement module refines the target frame based on these correlations, improving segmentation accuracy. Additionally, we incorporate a video-specific text encoding module. Our framework not only addresses open-vocabulary challenges but also establishes a strong baseline for OV-VSS tasks."}]}