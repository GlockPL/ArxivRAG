{"title": "Distributionally Robust Optimization as a Scalable Framework to Characterize Extreme Value Distributions", "authors": ["Patrick Kuiper", "Ali Hasan", "Wenhao Yang", "Yuting Ng", "Hoda Bidkhori", "Jose Blanchet", "Vahid Tarokh"], "abstract": "The goal of this paper is to develop distributionally robust optimization (DRO) estimators, specifically for multidimensional Extreme Value Theory (EVT) statistics. EVT supports using semi-parametric models called max-stable distributions built from spatial Poisson point processes. While powerful, these models are only asymptotically valid for large samples. However, since extreme data is by definition scarce, the potential for model misspecification error is inherent to these applications, thus DRO estimators are natural. In order to mitigate over-conservative estimates while enhancing out-of-sample performance, we study DRO estimators informed by semi-parametric max-stable constraints in the space of point processes. We study both tractable convex formulations for some problems of interest (e.g. CVaR) and more general neural network based estimators. Both approaches are validated using synthetically generated data, recovering prescribed characteristics, and verifying the efficacy of the proposed techniques. Additionally, the proposed method is applied to a real data set of financial returns for comparison to a previous analysis. We established the proposed model as a novel formulation in the multivariate EVT domain, and innovative with respect to performance when compared to relevant alternate proposals.", "sections": [{"title": "1 INTRODUCTION", "content": "Modeling rare and extreme events is an important task in many disciplines such as finance, climate science, and medicine [Dey and Yan, 2016]. Estimating distributions of rare events from data is difficult due to the lack of observations within this region, making it challenging to understand the risks deep in the tail of a distribution. Extreme Value Theory (EVT) studies the class of distributions arising as the possible distributional limits that can be used to estimate multivariate distributions in distant (relative to the origin) regions (i.e., tails) which by their nature witness very few observations (or none at all). These distributional limits are derived as the possible asymptotic statistical laws of shifted and re-scaled data as the sample size increases. It turns out that such possible distributional limits form a semi-parametric class called max-stable distributions, which are constructed in terms of a spatial Poisson point process.\nNaturally, because of the lack of data in extremal regions and because of the asymptotic nature of max-stable models, their use in inferential tasks involving tails is exposed to high variance due to model misspecification. Moreover, when using max-stable models, the assumptions that the data converges to the distributions specified by EVT must be made. This leads to an important question: How can we robustify against scenarios deep in the tails while observing potentially sub-asymptotic data where the assumptions of EVT may be violated? To answer this question, we propose a solution based on distributionally robust optimization (DRO). DRO involves a zero-sum game in which the statisticians play against an adversary that perturbs (in a non-parametric way) the nominal / baseline distribution assumed by the statistician. Building on classical DRO, we carefully design constraints to retain the extrapolation properties of EVT for the robustified distribution.\nSince we are interested in robustifying the tails, we will consider max-stable baseline distributions given by extreme value distributions (EVDs), as presented by de Haan and Ferreira [2010]. Max-stability roughly states that the distribution of the maximum of independently and identically distributed (i.i.d.) samples belongs to the same distribution up to a change in the location and scale parameters. This means that the \"shape\" of the distribution is preserved under the max operation, and it is this property that allows for extrapolating to regions outside the observation domain. In our robustification framework, we therefore wish to preserve"}, {"title": "2 BACKGROUND AND PROBLEM FORMULATION", "content": "In this section we focus our discussion on concepts critical to our proposed results and we introduce the framework for distributionally robust estimators in MEV distributions. We first provide a brief overview of EVT and describe how it is used to extrapolate beyond the observed data. We then introduce the multivariate counterpart, which we use throughout this work, to describe EVT in multi-dimensional settings. Finally, we discuss the distributionally robust optimization framework we use based on the Wasserstein distance."}, {"title": "2.1 EXTREME VALUE THEORY BACKGROUND", "content": "We begin by reviewing concepts surrounding MEV distributions. Consider a sequence of $n$ i.i.d. random vectors ${X^{(1)}, ..., X^{(n)}}$, with $X^{(i)} \\in \\mathbb{R}^d$ and $i = 1,..., n$ and denote the maxima over each dimension as $M_{k,n} := \\max_{1 \\leq i \\leq n} X^{(i)}_k$, where $k \\in \\{1, ..., d\\}$. Similarly to univariate EVT analysis, we consider MEV distributions $G$, where\n$\\mathbb{P}\\left((M_{1,n} - b_{1,n})/a_{1,n} \\leq z_1,..., (M_{d,n} - b_{d,n})/a_{d,n} \\leq z_d\\right) \\rightarrow G(z_1,...,z_d)$,\nfor some normalizing constants $a_{k,n} > 0$ and $b_{k,n}$, as the number of observations $n$ increases to infinity."}, {"title": "2.2 SPECTRAL REPRESENTATION OF COMPONENT-WISE MAXIMA AND ASYMPTOTIC CHARACTERIZATION", "content": "We will now introduce specific properties of MEV distributions that we will exploit in our framework. Consider i.i.d. random vectors ${X^{(1)}, ..., X^{(n)}}$, with $X^{(i)} \\in \\mathbb{R}^d$ and $i = 1, . . ., n$, with unit Frech\u00e9t margins such that $F_X (x) = \\exp(-1/x)$, with $x > 0$ for all $k = 1,..., d$. Following Coles [2001], let a sequence $N_1, . . ., N_n$ be a point process, where $N_n(\\cdot) = \\sum_{i=1}^{n} 1_{X_i(\\cdot)}$ with $N_n(\\cdot) \\xrightarrow{d} N(\\cdot)$ as $n \\rightarrow \\infty$ with $d$ denoting convergence in distribution and $N$ is a Poisson point process. We will apply this result further in Section 3 to define the proposed robustificaiton of the Poisson point process.\nDecomposing Max-Stable Random Variables Max-stable distributions can be decomposed according to the radial and spectral decomposition [Dombry et al., 2016, Liu et al., 2016]. Specifically, if we let $Y^{(n)} \\in \\mathbb{R}^d \\sim H$ be a sample from the spectral distribution and $A^{(n)}$ to be the $n$th arrival of a unit rate Poisson point process, then a max-stable random variable is represented as\n$M = \\max_{n > 1} \\frac{Y^{(n)}}{A^{(n)}}.$\nUnder the condition that $\\mathbb{E}[Y_k] = 1$, the variable $M$ is distributed with unit Frech\u00e9t margins. This decomposition provides a semi-parametric class of distributions whose structure we will use throughout the rest of the text. With the spectral decomposition of the MEV we are now able to analyze an MEV distribution. Additionally, we introduce Lemma 2.1 which allows for the explicit transformation of the MEV cumulative distribution function (CDF)."}, {"title": "2.3 WASSERSTEIN DRO FORMULATIONS FOR EVDS", "content": "Referencing the framework to quantify model uncertainty via DRO described by Blanchet et al. [2020], we define the probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$, where a candidate robust distribution $\\mathbb{P}$ is feasible such that $d(\\mathbb{P}, \\mathbb{P}_0) \\leq \\delta$. $\\mathbb{P}_0$ is a baseline distribution and $d(\\cdot)$ is a distance measure, constrained by the parameter $\\delta$. Two methods are commonly employed to quantify model uncertainty when constructing distributional ambiguity sets. The first considers the corruption of the likelihood baseline model to be misspecified, which is addressed via $\\phi$-divergence ambiguity sets. The second method perturbs the actual data, which leads to the use of Wasserstein distances to quantify model misspecification.\nA recent investigation by Blanchet et al. [2023] has demonstrated that both considerations of likelihood and perturbations of data may be unified under the Wasserstein distance, where $d(\\cdot) = W_c(\\cdot)$. Consider a loss function $l : \\mathbb{R}^d \\rightarrow \\mathbb{R}_+$ and the Wasserstein distance transport cost $c : \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}_+$. We define the primal optimization problem as follows:\n$\\underset{\\mathbb{P} : W_c(\\mathbb{P},\\mathbb{P}_0) \\leq \\delta}{\\max} \\mathbb{E}_{X \\sim \\mathbb{P}}[l(X)]$\nIn the context of our problem, $\\mathbb{P}_0$ could be the estimated distribution from available samples, but one may not have enough coverage since rare events may not have been observed within the data collection period. Following Blanchet and Murthy [2019], the dual form of (2) is given by the following problem\n$\\underset{\\lambda \\geq 0}{\\min} \\left[\\lambda \\delta + \\mathbb{E}_{X \\sim \\mathbb{P}_0} \\left[\\underset{Z}{\\max} \\{ l(Z) - \\lambda c(X,Z) \\} \\right]\\right].$\nAs we will discuss, the dual form is particularly amenable for computational purposes, especially when one can exploit specific properties of $c$, $l$, and $\\mathbb{P}$. Our investigation will describe two perspectives of (3) for EVDs. The first we present in Section 3, and is based on interpreting an MEV through a point process and the second is based on the interpretation using a copula. Each perspective has different properties that are useful for computational purposes, which we will discuss in Section 4.1."}, {"title": "3 ROBUSTIFICATION OF THE POISSON POINT PROCESS", "content": "In this section, we provide a more general formulation for DRO problems using a characterization with a point process. For the probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$, we denote a counting measure $N(\\cdot)$ on a Polish space $(S, d)$. We define $N(\\cdot) = \\sum_{i=1}^{n} 1_{X_i(\\cdot)}$ and $N'(\\cdot) = \\sum_{i=1}^{n'} 1_{Y_i(\\cdot)}$ as two random counting measures. Additionally, we define scaling functional $\\kappa : \\mathbb{Z} \\rightarrow \\mathbb{R}_{>0}$, and a lower semi-continuous cost function $c: S \\times S \\rightarrow \\mathbb{R}_{\\geq 0}$. Let $\\sigma(\\cdot)$ denote the permutation function defined on the support of $N(\\cdot)$, and we then define the distance as:\n$\\widetilde{c}(N(\\cdot), N'(\\cdot)) = \\infty \\cdot 1_{N(S) \\neq N'(S) \\text{ or } N(S) \\vee N'(S) = \\infty} + \\kappa(N(S)) \\inf_{\\sigma(\\cdot)} \\sum_{i=1}^{N(S)} c(x_i, y_{\\sigma(i)}).$\nThis distance is also similar to the ones presented by Barbour and Brown [1992], Chen and Xia [2004], Gao and Kleywegt [2023]. Additionally, we define the 1-Wasserstein distance between two random counting measure by:\n$W_c(\\mu, \\nu) = \\inf_{\\gamma \\in \\Gamma(\\mu, \\nu)} \\mathbb{E}_{\\gamma} \\widetilde{c}(N, N'),$\nHere $\\mu$ is the measure corresponding to $N(\\cdot)$ while $\\nu$ is the measure corresponding to $N'(\\cdot)$. Analogously to standard DRO problems, we can write down a new formulation for this DRO problem with point process as follows:\n$\\underset{W_c(\\mathbb{P},\\mathbb{P}_0) < \\delta}{\\sup} \\mathbb{E}_{N' \\sim \\mathbb{P}} \\left[ l(N'(f)) \\right] = \\underset{\\lambda > 0}{\\inf} \\left\\{ \\lambda \\delta + \\mathbb{E}_{N \\sim \\mathbb{P}_0} \\left[ \\underset{N'}{\\sup} \\{ l(N'(f)) - \\lambda \\widetilde{c}(N, N') \\} \\right] \\right\\},$ where $N(f) := \\int f(x) N(dx)$.\nAs mentioned in the background material in (1), MEV observations are given by the product of radial $(A^{(n)})$ and spectral $(Y^{(n)})$ components. In the perspective of the point process, the associated measures for each of these components forms the intensity of the point process. Specifically, we let the atoms of the point process be given by $(A^{(n)}, Y^{(n)})$ where $A^{(n)}$ can be thought of as a time coordinate (since $A^{(n)}$ is the $n$th arrival time) and $Y^{(n)}$ as a space (or the mark) component. Define $N(da, dv) = \\sum_{m=1}^{\\infty} 1_{\\{(A^{(n)},Y^{(n)})\\}}(da, dv)$."}, {"title": "4 OPTIMIZATIONS FOR SPECIFIC LOSS FUNCTIONS", "content": "Having introduced the different perspectives of the optimization, we provide specific loss functions where each perspective is particularly conducive towards specific robustification cases."}, {"title": "4.1 ROBUSTIFYING THE CDF", "content": "Consider the cumulative density function (CDF) of $M \\in \\mathbb{R}^d$ satisfying $M_i = \\max_{n=1}^{\\infty} \\frac{Y_i^{(n)}}{A^{(n)}}$, where $Y^{(n)} \\in \\mathbb{R}^d$ are i.i.d. random vectors and $A^{(n)}$ is the $n$-th arrival time of a unit Poisson point process. The CDF of $M$ satisfies:\n$\\mathbb{P}(M_1 \\leq x_1^{-1}, ..., M_d \\leq x_d^{-1}) = \\exp\\left(-\\int_{\\Delta_{d-1}} \\underset{i=1}{\\max} \\frac{w_i}{x_i} H(dw)\\right).$\nHere, Equation 8 is derived from Lemma 2.1 by using the max stable decomposition of the random variable, as demonstrated in Equation 1. We will now exploit specific geometric"}, {"title": "4.2 PROBABILITY AN EVENT IN A RARE SET OCCURS", "content": "In this part, we describe a generalization of the CDF case where we are interested in robustifying the probability that an event in the set $A \\subset \\mathbb{R}^d$ occurs, i.e. $\\mathbb{P}(N(A) > 1)$. Our primal problem in this case is:\n$\\underset{W_c(\\mathbb{P},\\mathbb{P}_0) < \\delta}{\\sup} \\mathbb{P}(N(A) \\geq 1) = \\underset{W_c(\\mathbb{P},\\mathbb{P}_0) < \\delta}{\\sup} \\mathbb{E}_{X \\sim \\mathbb{P}}[1_{X \\in A}]$\nand we let the distance be any $l_p$-norm. We can write the dual form and obtain the following simplification:"}, {"title": "4.3 CONDITIONAL VALUE AT RISK", "content": "Conditional value at risk (CVaR) is a commonly used risk measure in finance. Often there is uncertainty around the calculation, so we derive an efficient form for the calculation."}, {"title": "5 EXPERIMENTS AND RESULTS", "content": "We now consider empirical validation of the proposed optimization schemes using two experiments. The first uses a synthetic data set constructed to pathologically challenge our model and represent real world risk scenarios. The second validation is a baseline comparison using a real data set of financial returns similar to the DRO MEV experiment proposed by Yuen et al. [2020]. For the synthetic and real data experiments the observations are extreme; however, the standard EVD model is shown to not provide appropriate coverage for computing the worst-case risks. For both data sets, we provide empirical results validating that the proposed method satisfies the properties that we desired and outlined in the introduction, i.e. those of sufficient coverage while not being too conservative."}, {"title": "5.1 COMPUTATIONAL IMPLEMENTATIONS", "content": "For losses that have a reduced dual problem such as those in Section 4, we use the empirical data to represent the expectations and optimize over $\\lambda$. However, for general losses we use the equivalence stated in Corollary 3.2 to estimate the adversarial distribution. Our procedure follows"}, {"title": "5.2 EXPERIMENTS WITH SYNTHETIC EVT DATA", "content": "We evaluate our proposed methodology on two synthetic two-dimensional EVD data sets.\nWe hypothesize that a properly formulated DRO approach, where max-stability is enforced via constraints, demonstrates a greater invariance to increased model misspecification relative to a non-constrained models. Our methodology is designed to account for the complexities of real data and produce more accurate and reliable predictions. Details concerning computational parameters, runtime, and code base reference is available in Appendix C.3."}, {"title": "5.2.1 Synthetic Datasets", "content": "For our synthetic data experiments, we consider mixture distributions where one mixture component with a more concentrated risk appears less frequently than the other mixture component. This is illustrated in Figures 3a and 3b, where the ten thousand observations of the two datasets are visualized. In Figure 3a the component has a smaller tail index and more realizations with smaller magnitude whereas in Figure 3b the component has a larger tail index, but is more rare. For more information see Appendix B.\nA standard EVD approach would be sufficient when modeling a non-mixture Symmetric Logistic (SL) or Asymmetric Logistic (ASL) set of data. However, this mixture inserts a pathological modeling issue, which emulates the practical concerns often observed in real data where extrapolating to out-of-sample extreme values is difficult given the scarcity of data in these scenarios. These generated mixture distributions exhibit different tail behavior, which make it difficult"}, {"title": "5.2.2 Synthetic Experiment Results", "content": "We now illustrate the results of the computational method for the loss corresponding to CVaR of the $l^1$-norm of the observations. Specifically, we aim to robustify the calculation of $\\mathbb{E} [||X||_1 \\cdot 1_{||X||_1 \\leq x_\\alpha}]$ where $x_\\alpha = \\inf\\{x : \\mathbb{P}(||X||_1 < x) \\geq \\alpha\\}$. As $\\alpha \\rightarrow 0$, this corresponds to the CVaR for values deep in the tail. We approximate this region as a MEV under both the SL and ASL distributions described previously. Our goal in this experiment is to understand the behavior of the error, defined as the difference between the true risk and the risk computed using the different methods, as a function of the budget $\\delta$. We compare 3 different robustifications: (1) a totally unconstrained robustification where $\\mathbb{P}$ can be in any class of distributions; (2) an EVT constrained case where $\\mathbb{P}$ must also be an MEV and shares the same $A^{(n)}$'s; (3) an EVT constrained case where the margins are assumed to be well calibrated and only the dependence is modified (such that $w \\sim \\mathbb{P}$, s.t. $\\mathbb{E}[w_i] = 1/d$). In terms of standard EVT methods, (3) corresponds to the case where the adversarial distribution has a different dependence structure but maintains the same margins. Specifically, this would suggest estimating the margins is easier and therefore should be conserved, but the dependence structure should be perturbed. Figures 4a and 4b illustrates the error of using a DRO approach with max-stability constraints compared to the unconstrained approach, producing a distribution which"}, {"title": "5.2.3 Convergence of CDF as Distance is Increased", "content": "We now consider an experiment that corresponds to Theorem 4.1 described in Section 4.1. We illustrate the behavior in Figure 5 where we see an improvement in the error for a wide range of values of $\\delta$."}, {"title": "5.3 EXPERIMENT ON HIGH DIMENSIONAL FINANCIAL DATA", "content": "We consider a dataset on financial returns of equities in the S&P 500 index similar to the observations used in the validation experiments proposed by Yuen et al. [2020]. The dataset is composed of all current members of the S&P 500 who have been members since at least January 1983 to provide forty years of equity price data. We then average returns across companies, where each company is uniquely assigned to one of eleven industrial sectors (industrial, health care, consumer staples, etc.), leading to an eleven dimensional dataset. Maximum daily returns are taken both annually and weekly across the forty year data set, where each of these annual and weekly maximum data sets now constitute extreme observations.\nThis dataset is in the dimension of the number of industries"}, {"title": "6 DISCUSSION", "content": "In this paper, we introduced a framework for robustifying MEV distributions according to the point process viewpoint. We provided a novel framework and discussed simplifications that are possible under certain cost functions. We numerically demonstrated that, as we proposed, adding additional constraints from EVT improves the robustification without being too conservative. The framework is evaluated on a synthetic data set which is constructed to challenge the model and a high-dimensional real world data set derived from a similar investigation, demonstrating expected and improved behavior. The proposed methodology has a variety of applications in many fields, particularly in risk sensitive settings where model misspecification may be present.\nThere are many avenues for extending the proposed methodology. One important extension involves the case where one must specify a policy to mitigate a worst-case risk. This applies in, for example, portfolio optimization problems where the optimal portfolio minimizes the conditional value at risk over a distribution of portfolio losses. Additionally, further exploration of the proposed formulation leveraging max-stable processes extended to a more general max-infinitely division process may be insightful.\nLimitations There exist a number of limitations associated with the proposed framework. With regards to the computational method, there may be numerical instabilities associated with the optimization particularly in cases of $d$ being small. This is because the $\\lambda$ parameter must grow large, which leads to unstable optimization schemes."}]}