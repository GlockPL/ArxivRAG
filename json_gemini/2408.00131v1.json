{"title": "Distributionally Robust Optimization as a Scalable Framework to Characterize Extreme Value Distributions", "authors": ["Patrick Kuiper", "Ali Hasan", "Wenhao Yang", "Yuting Ng", "Hoda Bidkhori", "Jose Blanchet", "Vahid Tarokh"], "abstract": "The goal of this paper is to develop distributionally robust optimization (DRO) estimators, specifically for multidimensional Extreme Value Theory (EVT) statistics. EVT supports using semi-parametric models called max-stable distributions built from spatial Poisson point processes. While powerful, these models are only asymptotically valid for large samples. However, since extreme data is by definition scarce, the potential for model misspecification error is inherent to these applications, thus DRO estimators are natural. In order to mitigate over-conservative estimates while enhancing out-of-sample performance, we study DRO estimators informed by semi-parametric max-stable constraints in the space of point processes. We study both tractable convex formulations for some problems of interest (e.g. CVaR) and more general neural network based estimators. Both approaches are validated using synthetically generated data, recovering prescribed characteristics, and verifying the efficacy of the proposed techniques. Additionally, the proposed method is applied to a real data set of financial returns for comparison to a previous analysis. We established the proposed model as a novel formulation in the multivariate EVT domain, and innovative with respect to performance when compared to relevant alternate proposals.", "sections": [{"title": "1 INTRODUCTION", "content": "Modeling rare and extreme events is an important task in many disciplines such as finance, climate science, and medicine [Dey and Yan, 2016]. Estimating distributions of rare events from data is difficult due to the lack of observations within this region, making it challenging to understand the risks deep in the tail of a distribution. Extreme Value Theory (EVT) studies the class of distributions arising as the possible distributional limits that can be used to estimate multivariate distributions in distant (relative to the origin) regions (i.e., tails) which by their nature witness very few observations (or none at all). These distributional limits are derived as the possible asymptotic statistical laws of shifted and re-scaled data as the sample size increases. It turns out that such possible distributional limits form a semi-parametric class called max-stable distributions, which are constructed in terms of a spatial Poisson point process.\nNaturally, because of the lack of data in extremal regions and because of the asymptotic nature of max-stable models, their use in inferential tasks involving tails is exposed to high variance due to model misspecification. Moreover, when using max-stable models, the assumptions that the data converges to the distributions specified by EVT must be made. This leads to an important question: How can we robustify against scenarios deep in the tails while observing potentially sub-asymptotic data where the assumptions of EVT may be violated? To answer this question, we propose a solution based on distributionally robust optimization (DRO). DRO involves a zero-sum game in which the statisticians play against an adversary that perturbs (in a non-parametric way) the nominal / baseline distribution assumed by the statistician. Building on classical DRO, we carefully design constraints to retain the extrapolation properties of EVT for the robustified distribution.\nSince we are interested in robustifying the tails, we will consider max-stable baseline distributions given by extreme value distributions (EVDs), as presented by de Haan and Ferreira [2010]. Max-stability roughly states that the distribution of the maximum of independently and identically distributed (i.i.d.) samples belongs to the same distribution up to a change in the location and scale parameters. This means that the \"shape\" of the distribution is preserved under the max operation, and it is this property that allows for extrapolating to regions outside the observation domain. In our robustification framework, we therefore wish to preserve this max-stability property. We do this by searching over the space of distributions that are also max-stable, in effect constraining our search to only distributions that extrapolate according to EVT, which form a semi-parametric class. We do this by carefully designing the uncertainty set such that the necessary max-stable properties are preserved.\nTo illustrate our desired result, we refer to Figure 1. This figure visualizes how a completely unconstrained adversary (solid line) may be too conservative and may not consider the extrapolating properties of the distribution. A well constrained adversary to a max-stable distribution (dashed line) provides an appropriate balance of coverage while maintaining underlying structural properties. This figure demonstrates a case when even if we consider two adversarial formulations that achieve a similar minimum error value, the properly constrained adversary (dashed line) is preferable because the size of uncertainty is very hard to calibrate. Therefore, a curve that is \"flatter\" around the minimum as a function of the uncertainty size parameter is preferable.\nOur work focuses on the case of multivariate extreme value distributions (MEVs) which characterize the joint risk between different variables. Unlike univariate EVDs which have a fully parametric form, MEVs are semi-parametric, where the dependence structure is an infinite dimensional object, and much harder to estimate. We focus on robustifying the dependence structure while preserving the MEV character. This representation leads to additional difficulties which we overcome through the proposed DRO framework.\nIn view of the fact that the lack of data will naturally induce model error, we introduce an approach to quantify model misspecification based on optimal transport DRO. We select the Wasserstein metric for optimal transport because this approach, together with moment constraints, encompasses most DRO formulations as demonstrated by Blanchet et al. [2023].\nThe cost structure in the optimal transport discrepancy allows one to balance various objectives when modeling data, specifically the trade-off between tractability and control of pessimism in extremal behavior. A too-powerful adversary makes the size of uncertainty hard to calibrate since a slight increase may result in adversarial policies that perturb a distribution in ways that are too pessimistic and may not be consistent with the constraints imposed by EVT.\nWe mitigate these concerns by studying DRO formulations which constrain the adversarial perturbations to explore non-parametric models that induce robust estimators while preserving MEV characteristics. One of the approaches we employ is based on optimal transport for point processes, while the other is based on a neural network architecture. We illustrate the performance of the neural network architecture in the context of a Conditional Value at Risk (CVaR) metric applied to a multi-variable extreme value distributed data set. The network is evaluated across several synthetic data sets, specifically constructed to challenge the assumptions associated with EVT. The neural network architecture uses the tractability of the optimal transport distance metric to parameterize model uncertainty. Furthermore, our analysis is extended to a real data set of financial returns as a baseline comparison, similar to the data proposed by Yuen et al. [2020].This experiment demonstrates precisely the anticipated behavior shown in Figure 1, where our methodology leverages a properly constrained adversarial estimator.\nRelated Work A number of related research directions exist that consider both estimating EVDs from data as well as robustifying them using DRO. Since our focus is on multivariate EVDs, we will review a few of the estimators from the literature. For estimating multivariate EVDs, copula based approaches have been developed in a variety of instances including work by [Gudendorf and Segers, 2010, Marcon et al., 2017, Hasan et al., 2022]. Samplers for MEVs have also been considered as demonstrated by [Dombry et al., 2016, Liu et al., 2016]. Hasan et al. [2022] provides a flexible framework that uses neural networks to estimate and sample from multivariate EVDs irrespective of dimension, which we use in the computational component of this work. However, all of these methods only consider the case where the model is well-specified and do not consider uncertainty associated with the model class. We build upon these works with the addition of the DRO perspective.\nWith regards to the DRO literature, Blanchet and Murthy [2019] described the general framework for Wasserstein DRO that we will use throughout this work. Additional discussion is available in [Rahimian and Mehrotra, 2022] and [Van Parys et al., 2021]. Other DRO frameworks for EVT have been considered, but only in the univariate case, by [Blanchet et al., 2020, Bai et al., 2023]. Blanchet et al. [2020] considers DRO under the Kullback-Liebler divergence, which is appropriate for the univariate analysis but may not be appropriate for the multivariate case where supports are likely to be disjoint.\nFinally, Yuen et al. [2020] considers DRO estimators specifically for MEV distributions leveraging extremal coefficient constraints. In this work, upper and lower bounds are established on a Value at Risk (VaR) loss and applied to a real data set of financial returns. These bounds are established over the infinite domain of spectral measures, using a finite set of constraints formulated as a linear semi-infinite program. This is a limited subset of application problems when compared to our investigation. While the general goal of Yuen et al. [2020] is similar to our framework, our method extends to more general risk measures and considers a flexible uncertainty set specified by the Wasserstein distance. Furthermore, we achieve a single robust loss (upper bound), as opposed to less precise (upper and lower) boundaries. In Section 5 we generate a similar data set to [Yuen et al., 2020] and employ our proposed method for comparison. In summary, the main contributions of this paper are:\n\u2022 We introduce a framework to produce DRO estimators for MEV distributions based on optimal transport.\n\u2022 We provide tractable DRO formulations for various estimators of interest when adversaries live in the (infinite dimensional) space of point processes.\n\u2022 We test the performance of our estimators with the goal of showing that our MEV-constrained adversaries improve performance in the sense of Figure 1, and compare to a previous work for baseline analysis."}, {"title": "2 BACKGROUND AND PROBLEM FORMULATION", "content": "In this section we focus our discussion on concepts critical to our proposed results and we introduce the framework for distributionally robust estimators in MEV distributions. We first provide a brief overview of EVT and describe how it is used to extrapolate beyond the observed data. We then introduce the multivariate counterpart, which we use throughout this work, to describe EVT in multi-dimensional settings. Finally, we discuss the distributionally robust optimization framework we use based on the Wasserstein distance."}, {"title": "2.1 EXTREME VALUE THEORY BACKGROUND", "content": "We begin by reviewing concepts surrounding MEV distributions. Consider a sequence of n i.i.d. random vectors $\\{X^{(1)}, ..., X^{(n)}\\}$, with $X^{(i)} \\in \\mathbb{R}^d$ and $i = 1,..., n$ and denote the maxima over each dimension as $M_{k,n} := \\max_{i=1}^n X^{(i)}_k$, where $k \\in \\{1, ..., d\\}$. Similarly to univariate EVT analysis, we consider MEV distributions $G$, where\n$\\mathbb{P}((M_{1,n} - b_{1,n})/a_{1,n} \\leq z_1,..., (M_{d,n} - b_{d,n})/a_{d,n} \\leq z_d) \\rightarrow G(z_1,...,z_d)$, for some normalizing constants $a_{k,n} > 0$ and $b_{k,n}$, as the number of observations $n$ increases to infinity."}, {"title": "2.2 SPECTRAL REPRESENTATION OF COMPONENT-WISE MAXIMA AND ASYMPTOTIC CHARACTERIZATION", "content": "We will now introduce specific properties of MEV distributions that we will exploit in our framework. Consider i.i.d. random vectors $\\{X^{(1)}, ..., X^{(n)}\\}$, with $X^{(i)} \\in \\mathbb{R}^d$ and $i = 1, . . ., n$, with unit Frech\u00e9t margins such that $F_X (x) = \\exp(-1/x)$, with $x > 0$ for all $k = 1,..., d$. Following Coles [2001], let a sequence $N_1, . . ., N_n$ be a point process, where $N_n(\\cdot) = \\sum_{i=1}^n \\textbf{1}_{X^{(i)} \\in {\\cdot}}$ with $N_n(\\cdot) \\xrightarrow{d} N(\\cdot)$ as $n \\rightarrow \\infty$ with $d$ denoting convergence in distribution and $N$ is a Poisson point process. We will apply this result further in Section 3 to define the proposed robustificaiton of the Poisson point process.\nDecomposing Max-Stable Random Variables Max-stable distributions can be decomposed according to the radial and spectral decomposition [Dombry et al., 2016, Liu et al., 2016]. Specifically, if we let $Y^{(n)} \\in \\mathbb{R}^d \\sim H$ be a sample from the spectral distribution and $A^{(n)}$ to be the $n$th arrival of a unit rate Poisson point process, then a max-stable random variable is represented as\n$M = \\max_{n>1} \\frac{Y^{(n)}}{A^{(n)}}$.\nUnder the condition that $\\mathbb{E}[Y_k] = 1$, the variable $M$ is distributed with unit Frech\u00e9t margins. This decomposition provides a semi-parametric class of distributions whose structure we will use throughout the rest of the text. With the spectral decomposition of the MEV we are now able to analyze an MEV distribution. Additionally, we introduce Lemma 2.1 which allows for the explicit transformation of the MEV cumulative distribution function (CDF).\nLemma 2.1 (A corollary of Theorem 1 in de Haan [1984]). For $X^{(k)}$ with standard Frech\u00e9t marginal distributions, we have:\n$\\mathbb{P}\\left(\\frac{M_{1,n}}{x_1} < 1,..., \\frac{M_{d,n}}{x_d} < 1\\right) \\rightarrow \\exp(-V(x_1,...,x_d))$,\nwhere\n$V(x_1,\\cdots, x_d) = \\int_{\\Delta_{d-1}}  \\max_{k=1}^d \\frac{w_k}{x_k} H(dw)$,\n$\\Delta_{d-1}$ represents the unit $d$-dimensional simplex and $H(\\cdot)$ satisfies: $\\int w_k H(dw) = d^{-1}$ for all $k \\in \\{1, ..., d\\}$.\nWhen dealing with MEVs, the important component in estimation is the spectral measure $H$. When placed in its standardized form, $H$ is a measure with support over the simplex that describes the dependence between the covariates, specifically which components become extreme simultaneously."}, {"title": "2.3 WASSERSTEIN DRO FORMULATIONS FOR EVDS", "content": "Referencing the framework to quantify model uncertainty via DRO described by Blanchet et al. [2020], we define the probability space $(\\Omega, \\mathcal{F}, P)$, where a candidate robust distribution $P$ is feasible such that $d(P, P_0) \\leq \\delta$. $P_0$ is a baseline distribution and $d(\\cdot)$ is a distance measure, constrained by the parameter $\\delta$. Two methods are commonly employed to quantify model uncertainty when constructing distributional ambiguity sets. The first considers the corruption of the likelihood baseline model to be misspecified, which is addressed via $\\phi$-divergence ambiguity sets. The second method perturbs the actual data, which leads to the use of Wasserstein distances to quantify model misspecification.\nA recent investigation by Blanchet et al. [2023] has demonstrated that both considerations of likelihood and perturbations of data may be unified under the Wasserstein distance, where $d(\\cdot) = W_c(\\cdot)$. Consider a loss function $l: \\mathbb{R}^d \\rightarrow \\mathbb{R}_+$ and the Wasserstein distance transport cost $c: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}_+$. We define the primal optimization problem as follows:\n$\\underset{P: W_c(P,P_0) \\leq \\delta}{\\sup} \\mathbb{E}_{X \\sim P}[l(X)] \\qquad (2)$\nIn the context of our problem, $P_0$ could be the estimated distribution from available samples, but one may not have enough coverage since rare events may not have been observed within the data collection period. Following Blanchet and Murthy [2019], the dual form of (2) is given by the following problem\n$\\underset{\\lambda \\geq 0}{\\min} \\left[ \\lambda \\delta + \\mathbb{E}_{X \\sim P_0} \\left[ \\underset{Z}{\\max} \\left[ l(Z) - \\lambda c(X,Z) \\right] \\right] \\right]. \\qquad (3)$\nAs we will discuss, the dual form is particularly amenable for computational purposes, especially when one can exploit specific properties of $c$, $l$, and $P$. Our investigation will describe two perspectives of (3) for EVDs. The first we present in Section 3, and is based on interpreting an MEV through a point process and the second is based on the interpretation using a copula. Each perspective has different properties that are useful for computational purposes, which we will discuss in Section 4.1."}, {"title": "3 ROBUSTIFICATION OF THE POISSON POINT PROCESS", "content": "In this section, we provide a more general formulation for DRO problems using a characterization with a point process. For the probability space $(\\Omega, \\mathcal{F}, P)$, we denote a counting measure $N(\\cdot)$ on a Polish space $(\\mathbb{S}, d)$. We define $N(\\cdot) = \\sum_{i=1}^n \\textbf{1}_{x_i} (\\cdot)$ and $N'(\\cdot) = \\sum_{i=1}^n \\textbf{1}_{y_i} (\\cdot)$ as two random counting measures. Additionally, we define scaling functional $\\kappa: \\mathbb{Z} \\rightarrow \\mathbb{R}_{>0}$, and a lower semi-continuous cost function $c: \\mathbb{S} \\times \\mathbb{S} \\rightarrow \\mathbb{R}_{\\geq 0}$. Let $\\sigma(\\cdot)$ denote the permutation function defined on the support of $N(\\cdot)$, and we then define the distance as:\n$\\tilde{c}(N(\\cdot), N'(\\cdot)) = \\infty \\textbf{1}_{N(\\mathbb{S}) \\neq N'(\\mathbb{S}) \\text{ or } N(\\mathbb{S}) \\vee N'(\\mathbb{S}) = \\infty}  \\nonumber \\\\ + \\kappa(N(\\mathbb{S})) \\underset{\\sigma(\\cdot)}{\\inf} \\sum_{i=1}^{N(\\mathbb{S})} c(x_i, y_{\\sigma(i)}). \\qquad (4)$\nThis distance is also similar to the ones presented by Barbour and Brown [1992], Chen and Xia [2004], Gao and Kleywegt [2023]. Additionally, we define the 1-Wasserstein distance between two random counting measure by:\n$W_c(\\mu, \\nu) = \\underset{\\gamma \\in \\Gamma(\\mu, \\nu)}{\\inf} \\mathbb{E}_{(\\tilde{N}, \\tilde{N}') \\sim \\gamma}[\\tilde{c}(\\tilde{N}, \\tilde{N}')], \\qquad (5)$\nHere $\\mu$ is the measure corresponding to $N(\\cdot)$ while $\\nu$ is the measure corresponding to $N'(\\cdot)$. Analogously to standard DRO problems, we can write down a new formulation for this DRO problem with point process as follows:\nTheorem 3.1 (Primal and Dual Form for Point Processes). Let $N(\\cdot)$ denote a Poisson point process, $f(\\cdot)$ denote a Borel function, and $W_c(P, P_0)$ denote the Wasserstein distance between measures induced by $N'$ and $N$ under cost metric $c$. For a distance $\\delta \\geq 0$, the following problems are equivalent:\n$\\underset{W_c(P,P_0) < \\delta}{\\sup} \\mathbb{E}_{N' \\sim P} [l(N'(f))] = \\\\ \\underset{\\lambda > 0}{\\inf} \\left[ \\lambda \\delta + \\mathbb{E}_{N \\sim P_0} \\left[ \\underset{N'}{\\sup} \\left[ l(N' (f)) - \\lambda \\tilde{c}(N, N') \\right] \\right] \\right]$\nwhere $N(f) := \\int f(x) N(dx)$.\nProof. This follows from the application of Blanchet and Murthy [2019].\nAs mentioned in the background material in (1), MEV observations are given by the product of radial $(A^{(n)})$ and spectral $(Y^{(n)})$ components. In the perspective of the point process, the associated measures for each of these components forms the intensity of the point process. Specifically, we let the atoms of the point process be given by $(A^{(n)}, Y^{(n)})$ where $A^{(n)}$ can be thought of as a time coordinate (since $A^{(n)}$ is the $n$th arrival time) and $Y^{(n)}$ as a space (or the mark) component. Define $N(da, dv) = \\sum_{n=1}^m \\textbf{1}_{(A^{(n)},Y^{(n)})} (da, dv)$."}, {"title": "4 OPTIMIZATIONS FOR SPECIFIC LOSS FUNCTIONS", "content": "Having introduced the different perspectives of the optimization, we provide specific loss functions where each perspective is particularly conducive towards specific robustification cases."}, {"title": "4.1 ROBUSTIFYING THE CDF", "content": "Consider the cumulative density function (CDF) of $M \\in \\mathbb{R}^d$ satisfying $M_i = \\max_{n=1}^{\\infty} \\frac{Y_i^{(n)}}{A^{(n)}}$, where $Y^{(n)} \\in \\mathbb{R}^d$ are i.i.d. random vectors and $A^{(n)}$ is the $n$-th arrival time of a unit Poisson point process. The CDF of $M$ satisfies:\n$\\mathbb{P}(M_1 \\leq x_1^{-1}, ..., M_d \\leq x_d^{-1}) = \\\\ \\exp\\left(-\\int_{\\Delta_{d-1}} d  \\max_{i=1}^d \\frac{w_i}{x_i} H(dw)\\right). \\qquad (8)$\nHere, Equation 8 is derived from Lemma 2.1 by using the max stable decomposition of the random variable, as demonstrated in Equation 1. We will now exploit specific geometric properties of the point process to derive a dual form. Given the definition of $M_i$, we may make a transformation of the CDF algebraically as follows:\n$\\mathbb{P}(M_1 \\leq x_1^{-1}, ..., M_d \\leq x_d^{-1}) = \\\\ \\mathbb{P}\\left(\\frac{\\max_n \\frac{Y_i^{(n)}}{A^{(n)}}}{x_k} < 1, \\forall k\\right) := \\mathbb{P}\\left(V^{(n)} < A^{(n)}, \\forall n := \\mathbb{P}( \\bigcap_n C)\\right). \\qquad (9)$\nIn this case, we can reduce our analysis to this two dimensional space and consider the robustification of the point process on this space, rather than the full $d$ dimensional space of the $Y^{(n)}$'s. A schematic of this representation before robustification is given in Figure 2a. Using this form leads to a dual form given by the following theorem:\nTheorem 4.1 (Formulation for CDF). Consider the setting in Theorem 3.1 and the loss function $l(X) = \\textbf{1}_{X \\geq 1}$, the point process $N(\\cdot) := \\sum_{n=1} \\textbf{1}_{(A^{(n)}, v^{(n)})} (\\cdot)$, the function $f(a, v) = \\textbf{1}_{C}(a, v)$ and $C = \\{(a, v) \\in \\mathbb{R}^2 | a > 0, a \\leq v\\}$. Then, the dual objective is:\n$\\underset{W_c (P,P_0) < \\delta}{\\inf} \\mathbb{P}'(M_1 \\leq x_1^{-1}, ..., M_d \\leq x_d^{-1})  \\\\ = 1 - \\underset{\\lambda > 0}{\\inf} \\left[ \\lambda \\delta + \\mathbb{E}_P \\left[ 1 - \\lambda \\underset{n \\geq 1}{\\min} \\left[ A^{(n)} - V^{(n)} \\right]^+ \\right] \\right].  \\qquad (10)$\nProof. See Appendix A.2.\nCorollary 4.2 (Minimizer of CDF). The minimizer of the problem in Theorem 4.1 has the following form:\n$\\underset{P: W_c(P,P_0) \\leq \\delta}{\\inf} \\mathbb{P}(X_1 \\leq x_1^{-1}, ..., X_d \\leq x_d^{-1}) \\\\ = \\exp\\left(-\\mathbb{E}_{N \\sim P_0}[N(C_{1/\\lambda^*})]\\right)$"}, {"title": "4.2 PROBABILITY AN EVENT IN A RARE SET OCCURS", "content": "In this part, we describe a generalization of the CDF case where we are interested in robustifying the probability that an event in the set $\\mathcal{A} \\subset \\mathbb{R}^d$ occurs, i.e. $\\mathbb{P}(N(\\mathcal{A}) > 1)$. Our primal problem in this case is:\n$\\underset{W_c (P,P_0) < \\delta}{\\sup} \\mathbb{P}(N(\\mathcal{A}) \\geq 1) = \\underset{W_c (P,P_0) < \\delta}{\\sup} \\mathbb{E}_{X \\sim P}[\\textbf{1}_{X \\in \\mathcal{A}}]$ and we let the distance be any $l_p$-norm. We can write the dual form and obtain the following simplification:\nTheorem 4.3 (Formulation for Rare Set). Consider the setting in Theorem 3.1 and the loss function $l(X) = \\textbf{1}_{X > 1}$, the point process $N(\\cdot) = \\sum_{m=1} \\textbf{1}_{x(n)} (\\cdot)$, and the function $f(x) = \\textbf{1}_{\\mathcal{A}}(x)$. Then the dual objective is:\n$\\underset{W_c (P,P_0) < \\delta}{\\sup} \\mathbb{P}(N(\\mathcal{A}) \\geq 1) =  \\\\ \\underset{\\lambda > 0}{\\inf} \\left[ \\lambda \\delta + \\mathbb{E}_P \\left[ 1 - \\lambda \\underset{n}{\\inf} \\underset{y \\in \\mathcal{A}}{\\inf} || y - X^{(n)} ||^+ \\right] \\right]. \\qquad (11)$\nProof. See Appendix A.3.\nRelatedly, we can also compute the expected number of events to occur within a set using the following theorem:\nTheorem 4.4 (Expected number of events in a set). Consider the setting in Theorem 3.1 and the loss function $l(X) = X$, the point process $N(\\cdot) = \\sum_{m=1} \\textbf{1}_{x(n)} (\\cdot)$, and the function $f(x) = \\textbf{1}_{X}(x)$. Then the dual objective is:\n$\\underset{W_c (P,P_0) \\leq \\delta}{\\sup} \\mathbb{E}_{N \\sim P}[N(\\mathcal{A})] =  \\\\ \\underset{\\lambda \\geq 0}{\\min} \\left[ \\lambda \\delta + \\mathbb{E}_{P_0} \\left[  \\sum_{n=1}^{\\infty} \\left( 1 - \\lambda \\underset{y \\in \\mathcal{A}}{\\inf} || y - X^{(n)} || \\right)^+ \\right] \\right]. \\qquad (12)$"}, {"title": "4.3 CONDITIONAL VALUE AT RISK", "content": "Conditional value at risk (CVaR) is a commonly used risk measure in finance. Often there is uncertainty around the calculation, so we derive an efficient form for the calculation.\nIn the DRO case, we can write the CVaR problem with the given level $\\alpha$ as:\n$\\underset{P: W_c(P,P_0) \\leq \\delta}{\\sup} \\underset{z}{\\inf} \\mathbb{E}_{P} \\left[ \\frac{\\left( \\int_{\\mathbb{V}} v \\sum_{i=1}^N \\delta_{X^{(i)}}(dx) - z \\right)^+}{1 - \\alpha} + z \\right] \\qquad (13)$\nBy exchanging the sup - inf to inf - sup, we can write the dual formulation as the following theorem:\nTheorem 4.5 (Formulation for CVaR). Consider the setting in Theorem 3.1 and the loss function $l(X) = \\frac{(X-z)^+}{1-\\alpha} + z$, the point process $N(\\cdot) = \\sum_{i=1} \\textbf{1}_{x(n)} (\\cdot)$, and the function $f(x) = ||x||_{\\infty}$. The then dual objective is:\n$\\underset{\\lambda \\geq 0}{\\inf} \\underset{z}{\\sup} \\mathbb{E}_{P} \\left[ \\frac{\\left( \\int_{\\mathbb{V}} v \\sum_{i=1}^N \\delta_{X^{(i)}}(dx) - z \\right)^+}{1 - \\alpha} + z \\right] \\\\\\ + \\mathbb{E}_{P_0} \\left[ \\sum_{n=1}^{\\infty} ||X^{(n)}||_{\\infty} - \\lambda \\sum_{n=1}^{\\infty} ||X^{(n)}||_{\\infty} \\textbf{1}_{\\{ \\sum_{n=1}^{\\infty} ||X^{(n)}||_{\\infty} > q_{\\alpha} \\}} \\right] + z)  \\qquad (14)$\nwhere $\\mathbb{P}_0 \\left( \\sum_{n=1}^{\\infty} ||X^{(n)}||_{\\infty} \\leq q_{\\alpha} \\right) = \\alpha$ and $q_1 < \\infty$.\nProof. See Appendix A.5.\nHaving introduced these simplifications, we see that the optimization problems for commonly used losses under the point process interpretation of EVT can be made feasible."}, {"title": "5 EXPERIMENTS AND RESULTS", "content": "We now consider empirical validation of the proposed optimization schemes using two experiments. The first uses a synthetic data set constructed to pathologically challenge our model and represent real world risk scenarios. The second validation is a baseline comparison using a real data set of financial returns similar to the DRO MEV experiment proposed by Yuen et al. [2020]. For the synthetic and real data experiments the observations are extreme; however, the standard EVD model is shown to not provide appropriate coverage for computing the worst-case risks. For both data sets, we provide empirical results validating that the proposed method satisfies the properties that we desired and outlined in the introduction, i.e. those of sufficient coverage while not being too conservative."}, {"title": "5.1 COMPUTATIONAL IMPLEMENTATIONS", "content": "For losses that have a reduced dual problem such as those in Section 4, we use the empirical data to represent the expectations and optimize over $\\lambda$. However, for general losses we use the equivalence stated in Corollary 3.2 to estimate the adversarial distribution. Our procedure follows the method in Hasan et al. [2022] where we first estimate the spectral measure from the observations using a generative neural network and then optimize over the space of networks within some ball of the initial distribution. The case for arbitrary $l$ is illustrated in Algorithm 1.\nAlgorithm 1 Procedure for estimating adversarial loss from data.\nRequire: Fit $P_0$ according to MEV procedure in Hasan et al. [2022], adversarial budget $\\delta$, risk function $l(\\cdot)$, training iterations $K$.\n1: Initialize: $P_0$, the adversarial distribution as $P_0$\n2: for $k = 1 . . . K$ do\n3:   Sample: $Z \\sim P_0, x \\sim P_0$\n4:   Compute: $L_x(Z, x) = l(Z) - \\lambda c(Z, x)$\n5:   Solve: $\\max_Z L_x(Z, x)$\n6:   Solve: $\\min_{\\lambda > 0} \\lambda \\delta + \\mathbb{E}_{x \\sim P_0} L_x(Z, x)$\n7: end for\n8: Return: Adversarial risk $\\lambda^* \\delta + \\mathbb{E} \\left[ \\underset{Z}{\\max} \\left[ l(Z) - \\lambda c(Z, x) \\right] \\right]$ and adversarial MEV $P_{\\delta}$."}, {"title": "5.2 EXPERIMENTS WITH SYNTHETIC EVT DATA", "content": "We evaluate our proposed methodology on two synthetic two-dimensional EVD data sets.\nWe hypothesize that a properly formulated DRO approach, where max-stability is enforced via constraints, demonstrates a greater invariance to increased model misspecification relative to a non-constrained models. Our methodology is designed to account for the complexities of real data and produce more accurate and reliable predictions. Details concerning computational parameters, runtime, and code base reference is available in Appendix C.3."}, {"title": "5.2.1 Synthetic Datasets", "content": "For our synthetic data experiments, we consider mixture distributions where one mixture component with a more concentrated risk appears less frequently than the other mixture component. This is illustrated in Figures 3a and 3b, where the ten thousand observations of the two datasets are visualized. In Figure 3a the component has a smaller tail index and more realizations with smaller magnitude whereas in Figure 3b the component has a larger tail index, but is more rare. For more information see Appendix B.\nA standard EVD approach would be sufficient when modeling a non-mixture Symmetric Logistic (SL) or Asymmetric Logistic (ASL) set of data. However, this mixture inserts a pathological modeling issue, which emulates the practical concerns often observed in real data where extrapolating to out-of-sample extreme values is difficult given the scarcity of data in these scenarios. These generated mixture distributions exhibit different tail behavior, which make it difficult for standard approaches to capture the dependencies accurately, and thus we turn to the proposed DRO estimator and evaluate our ability to recover the true risk."}, {"title": "5.2.2 Synthetic Experiment Results", "content": "We now illustrate the results of the computational method for the loss corresponding to CVaR of the $l^1$-norm of the observations. Specifically", "x_{\\alpha}": "where $x_{\\alpha"}, "inf\\{x : \\mathbb{P}(||X||_1 < x) \\geq \\alpha\\}$. As $\\alpha \\rightarrow 0$, this corresponds to the CVaR for values deep in the tail. We approximate this region as a MEV under both the SL and ASL distributions described previously. Our goal in this experiment is to understand the behavior of the error, defined as the difference between the true risk and the risk computed using the different methods, as a function of the budget $\\delta$. We compare 3 different robustifications: (1) a totally unconstrained robustification where $\\mathbb{P}$ can be in any class"]}