{"title": "HOW TO SHRINK CONFIDENCE SETS FOR MANY\nEQUIVALENT DISCRETE DISTRIBUTIONS?", "authors": ["Odalric-Ambrym Maillard", "Mohammad Sadegh Talebi"], "abstract": "We consider the situation when a learner faces a set of unknown discrete distributions $(p_k)_{k \\in K}$ defined\nover a common alphabet $X$, and can build for each distribution $p_k$ an individual high-probability confidence\nset thanks to $n_k$ observations sampled from $p_k$. The set $(p_k)_{k \\in K}$ is structured: each distribution $p_k$ is\nobtained from the same common, but unknown, distribution $q$ via applying an unknown permutation to $X$.\nWe call this permutation-equivalence. The goal is to build refined confidence sets exploiting this structural\nproperty. Like other popular notions of structure (Lipschitz smoothness, Linearity, etc.) permutation-\nequivalence naturally appears in machine learning problems, and to benefit from its potential gain calls\nfor a specific approach. We present a strategy to effectively exploit permutation-equivalence, and provide\na finite-time high-probability bound on the size of the refined confidence sets output by the strategy. Since\na refinement is not possible for too few observations in general, under mild technical assumptions, our\nfinite-time analysis establish when the number of observations $(n_k)_{k \\in K}$ are large enough so that the\noutput confidence sets improve over initial individual sets. We carefully characterize this event and\nthe corresponding improvement. Further, our result implies that the size of confidence sets shrink at\nasymptotic rates of $O(\\frac{1}{\\sqrt{\\sum_{k \\in K} n_k}})$ and $O(\\frac{1}{\\text{max}_{k \\in K} n_k})$, respectively for elements inside and outside\nthe support of $q$, when the size of each individual confidence set shrinks at respective rates of $O(\\frac{1}{\\sqrt{n_k}})$\nand $O(\\frac{1}{n_k})$. We illustrate the practical benefit of exploiting permutation equivalence on a reinforcement\nlearning task.", "sections": [{"title": "1 Introduction", "content": "Like Lipschitz smoothness, linearity or sub-modularity, leveraging a structural property of a set of\nunknown distributions, that can be known only by sampling, is generally the key to better statistical\nefficiency, hence improved learning guarantees. In this paper, we consider a learning task involving a set\nof unknown distributions $(p_k)_{k \\in K}$ over a discrete alphabet $X \\subset N$ that are known to satisfy a structural\nproperty called permutation-equivalence. Intuitively, this means all the distributions are actually the\nsame up to a shuffling of the entries. Formally, permutation-equivalence means that there exists a\ncommon distribution $q$ over $X$ such that each distribution $p_k, k \\in K$ is obtained from $q$ after applying some\npermutation $\\sigma_k$ of its entries (See Definition 1).\nPermutation-equivalence can be spotted in several situations. For instance in a decentralized learning\ntask where $K$ is a set of learners, different learners may number the observation space differently. Hence\nevery process $q$ on the observation space will be seen as a different $p_k$ by learner $k$. Permutation-equivalence\nalso naturally appears in reinforcement learning. Indeed in several environments such as RiverSwim and\ngrid-world Markov Decision Processes (2-room, 4-room, frozen-lake, etc.)\u00b9, probability transitions from\ntwo different state-action pairs are usually not arbitrarily different: In grid-worlds, the set of next-state\ntransitions $(p(\\cdot | s, a))_{s \\in S_0, a \\in A}$, where $S_0$ denotes all states with no neighboring wall, typically exhibits\npermutation-equivalence. This has been considered in [1, 2]. Likewise, a windy grid-world [3], sailboat [4],\nor RiverSail environment (see Appendix A) in which navigation is similar in each state or river channel\nexcept for the presence of a wind of unknown but constant direction also exhibit permutation-equivalence\nstructure. Naturally, in practice a given task may present further structural properties beyond equivalence;\nwe focus in this paper on the benefit that exploiting permutation-equivalence only can bring to the learner.\nExploiting permutation-equivalence, like any other structural assumption, is especially beneficial in\nwhen acquiring data from distributions is costly. In the context of statistical estimation where a learner\nhas only access to $n_k$ samples from distribution $p_k$, for each $k \\in K$, by exploiting the structure we mean to\nbuild, using all samples, tighter confidence sets around each $p_k$ than the initial \u201cindividual\u201d confidence"}, {"title": "2 Setup and Notations: Tightening Estimation using Equivalence", "content": "For a finite alphabet $X \\subset N$ we denote by $P(X)$ the set of probability distributions over $X$ and by $G_X$\nthe group of permutations over $X$. Each permutation $\\sigma \\in G_X$ acts as perfect matching (one-to-one mapping\nof $X$). We consider a set $(p_k)_{k \\in K} \\subset P(X)$ of $K = |K|$ many distributions on $X$. We assume they are all\ngenerated from the same common underlying distribution, after applying different permutation of $X$. More\nformally, we introduce the following definition:"}, {"title": "3 Building Confidence Sets Exploiting Permutation Equivalence", "content": "In this section, we detail our simple strategy to output confidence sets exploiting $G_X$-equivalence from\nan unstructured set of individual confidence sets. It relies on Algorithm 1 then Algorithm 2.\nIdentification of Compatible Matchings. At a high level, the procedure consists in two steps: first\nbuilding the graph of compatible matchings (non-empty intersections), which can be done in at most $|X|^2 K^2$\nsteps. Second, exploiting the property that permutations are one-to-one to prune the set of compatible\nmatchings (starting from a set $c$ with the smallest number $J$ of matchings $(c)_{j \\leq 1}$). This step is in general\ncombinatorial: In the previous example, after removing the obvious assignment of $d_3$ to $c_1$, identifying\nthat the pair $d_1, d_4$ should match the pair $c_2, c_3$ requires searching for a permutation over a subset of size\n2 (done here using that $c_2, c_3$ also matches $d_1$). More generally, fully exploiting the structure requires\nsearching for permutations over subsets of arbitrary any size, which is computationally demanding (see\n[20]). In order to keep a low computationally, we restrict the size of the permutations the algorithm is\nlooking for to a predefined maximal value $L$ (say 3), yielding Algorithm 1. This results in a pruning whose\ncomputational complexity can be controlled.\nRefined Concentration Sets. The last step is to build the refined confidence intervals from the set\nof compatible matchings out put by Algorithm 1. Indeed, for each point $x$ and index $k$, it outputs sets\n$(I_{k, x, k'})_{k'}$, so that each $k' \\in K \\backslash {k}$ may contribute to refining the confidence sets."}, {"title": "4 The Statistical Benefit of Permutation Equivalence", "content": "The refinement strategy (Algorithms 1-2) is sound provided that each confidence interval $c_k = CI(S_{k, x}, \\delta)$\nis valid, which happens with probability higher than $1-\\delta$, for all $x$ and each $k$. Let $\\Omega := {\\forall x, \\forall k, p_k(x) \\in$\n$CI(S_{k, x}, \\delta)}$ be the event that all initial confidence sets are valid. A union bound over all $k \\in K$ shows that\n$P(\\Omega) \\geq 1 \u2013 K\\delta$. The aim of this section is to assess the gain of using refined confidence sets exploiting\npermutation-equivalence. In order to simplify the presentation of the results, we consider Cl given in (2),\nand introduce the notion of surrogate confidence intervals:"}, {"title": "5 Conclusion", "content": "In this paper, we have studied the benefit of using a permutation-equivalence property of a set of\nunknown distributions $(p_k)_{k \\in K}$ to produce a refinement of the confidence sets one may build for each $p_k$\nbased on observations from $p_k$ only. Leveraging this structure for estimation complements other popular\nsetups involving permutations, such as matching of known distributions (optimal transport), or decision\nmaking with structured output. We brought a finite-time analysis using concentration inequalities to\ncontrol the potential refinement for each given number of observations $(n_k)_{k \\in K}$ in a problem-dependent way,\nand provide an algorithm with low-computational complexity to build the underlying matchings. Applied\nto standard Bernstein confidence sets, this enables to get sizes of confidence intervals asymptotically\nscaling as $O((\\sum_k n_k)^{-1/2}))$ for points in the support of the distributions and $O((\\text{max}_k n_k)^{-1/2})$ for points\noutside the support, plus to characterize the full finite-time behavior. A possible extension of this work is\non the one hand to go beyond discrete space $X$ and study other automorphisms structures (e.g. rotations\nfor $X = R^d$, etc.), and on the other hand to apply similar ideas in various machine learning and RL setups."}, {"title": "A The RiverSail Environment", "content": "We depict in Figure 7 a discrete version of the RiverSail environment. It is similar to the windy\ngrid-world [3] or sailboat [4] environment. In this grid-world MDP, an agent must sail on different rivers\nwhile collecting rewards (red states) on the way; exiting a channel (entering dashed states) randomly sends\nthe agent to enter another channel. In each river channel, navigation is similar except for the presence of\na constant wind, shown in the top-left corner of each channel, whose direction is unknown, as in a windy\nwindy grid-world: When a boat in a pink position moves in the direction of the arrow, it ends up in the\ngray states, shaded according their probability level. All states with dark blue edges in the same region\nbehave similarly. Here, the exact next-state probability masses are unknown by the sailor, yet she knows\nperfectly which transitions are equivalent; the permutations between next-state distributions are still\nunknown due to the unknown wind. Exploiting this structure may massively reduce learning time of the\nunknown dynamics.\nWe also provide below an illustration of the RiverSwim environment used in our application to\nreinforcement learning. It has been used in [1] as well. This is a standard MDP with $L$ states and 2 actions\n(left, right). The RiverSwim environment has 4 classes of equivalent state-action pairs. One class for all"}, {"title": "B Benefits of Permutation Equivalence: Proofs", "content": "We first handle the points that belong to the support of the considered distributions, then turn to\nhandling the points outside of the support. The reason for doing so is that, by the monotonicity assumption\non $q$ (that is, all masses are different), the permutation is uniquely defined on the support. However, it is\nnot uniquely defined outside of the support, which calls for a modified proof.\nTo simplify the notation, throughout we omit the dependence of various quantities on $\\delta$. Given $k$ and $x$,\ndenote $CI_{k, x} := CI(S_{k, x}, , \\delta)$, and define $CI^+_{k,x} = \\text{max}{x \\in CI_{k,x}}$ and $CI^-_{k,x} = \\text{min}{x \\in CI_{k,x}}$.\nStep 1: Points with positive mass. On the one hand, $(k', x')$ is not compatible with $(k, x)$ if\n$CI_{k,x}^+ < CI_{k',x'}^-$\nor\n$CI_{k,x}^- > CI_{k',x'}^+$\nIn view of the definition of SCI, this implies that\n$\\bar p_k(x) + B(p_k(x),n_k) < \\bar p_{k'}(x') \u2013 B(p_{k'}(x'), n_{k'})$\nor $\\bar p_k(x) - B(p_k(x),n_k) > \\bar p_{k'}(x') + B(p_{k'}(x'), n_{k'})$.\nOn the other hand, if $x'$ is compatible with $x$, then it must be with high probability that $\\bar p_k(x) +$\n$2B(p_k(x), n_k) \\geq \\bar p_{k'}(x') \u2013 2B(p_{k'}(x'), n_{k'})$ , and $\\bar p_k(x) - 2B(p_k(x),n_k) \\leq \\bar p_{k'}(x') + 2B(p_{k'}(x'), n_{k'})$ , or more\ncompactly,\n$\\frac{|p_{k'}(x') - p_{k}(x)|}{2} < B(p_k(x), n_k) + B(p_{k'}(x'), n_{k'})$.\nIn particular, since by permutation-coherence it must be that $p_{k'}(x') = p_k(\\sigma(x'))$ for some permutation\n$\\sigma$, if we assume that\n$\\forall x_1 \\in X\\backslash {x}, \\frac{|p_{k}(x) - p_{k}(x_1)|}{2} > B(p_k(x), n_k) + B(p_{k'}(x_1), n_{k'})$,\nthen we deduce that whenever $\\sigma(x') \\in X\\backslash {x}, x'$ cannot be compatible with $x$, so that the only point\ncompatible in such a case is $x'$ such that $\\sigma(x') = x$. By the monotonicity assumption on $q$, then such a point\nis unique for $x \\in X_{p_k}$. Note that this also means that $B(p_{k'}(x'),n) = B(p_k(x), n)$.\nWe deduce that on the event that all confidence bounds are valid,\n$\\forall x \\in X_{p_k}, K_{k,x} \\subset K_{k,x}$,\nwhere\n$K_{k,x}:={\\bar k}\\cup{k' \\in K\\backslash {k}: \\forall x_1 \\in X\\backslash {x}, \\frac{|p_{k}(x)-p_{k}(x_1)|}{2} >B(p_k(x), n_k)+B(p_{k'}(x_1), n_{k'}) }$.\nIn particular, since Cl is a decreasing function of the set of samples, it holds that\n$CI_k \\subset CI(\\cup_{k' \\in K_{k,x}} S_{k',x_{k'}}) \\subset CI(\\cup_{k' \\in K_{k,x}} S_{k',x_{k'}})$\nRecalling that $n_{K_{k,x}} = \\sum_{k' \\in K_{k,x}} n_{k'}$, and using the form of Cl, we can further write the following\n$\\forall k, \\forall x \\in X_{p_k}, CI_{k,x} \\subset SCI(\\cup_{k' \\in K_{k,x}} S_{k',x_{k'}}) = {\\lambda : |p_k(x) \u2013 \\lambda| \\leq B(\\lambda, n_{K_{k,x}})}\nStep 2: Points outside the support. Let us now deal with points outside of $X_{p_k}$. First, note that\nwhen $|X \\backslash X_{p_k}| > 1$ and $x \\notin X_{p_k}$, then there exists another point $x' \\notin X_{p_k}$. Hence for any $k'$, $I_{k,x,k'}$ must\ncontain not only $\\sigma(x)$ but also $\\sigma(x')$ for some permutation $\\sigma$. This means that in such a case, $K_{k,x} = {k}$.\nFurther we also have $p_k(x) = p_k(x')$. Specializing (4) for $x \\notin X_{p_k}$, it comes that in case\n$\\forall x_1 \\in X_{p_k}, \\frac{p_{k}(x_1)}{2} > B(0, n_k) + B(p_{k'}(x_1), n_{k'})$,\nthen the only points compatible with $(k, x)$ are such that $x' \\notin X_{p_k}$, that is $|I_{k,x,k'}| \\subset |X \\backslash X_{p_k}|$. Hence, let\nus introduce $q_k(n_k) = \\text{sup}_{x\\in X_{p_k}}{2(B(p_k(x), n, \\delta) + B(0, n_k, \\delta))}$. We deduce that if for all $x_1 \\in X_{p_k}, p_{k}(x_1) >$\n$q_k(n_k)$, that is $q_{min} > q_k(n_k)$, then $|I_{k,x,k'}| \\subset |X\\backslash X_{p_k}|$.\nNow for each $k' \\in K $, if $q_{min} > q_k(n_k)$, the only points compatible with $x \\notin X_{p_k}$ must be outside\nof the support of the distribution $p_{k'}$. In particular, $p_{k'}(x') = 0$ for all $x' \\in I_{k,x,k'}$. This in turns implies,\nusing the definition of the sets $SCI(S) = {\\lambda : |\\bar p(S) \u2013 \\lambda| \\leq B(p_k(x), |S|)}$, that $\\cup_{x'\\in I_{k,x,k'}} CI(S_{k',x'}) \\subset$\n${\\lambda: \\lambda \\leq B(p_{k'}(x'), n_{k'},\\delta) }$.\nThis motivates to introduce the set\n$K_k = {k} \\cup {k' \\in K \\backslash {k} : q_{min} > q_k(n)}$.\nIf $|X \\backslash X_{p_k}| > 1$, then $K_{k,x} = {k}$ and\n$CI_{k,x} = CI(\\cup_{k' \\in K_k,} S_{k',X_{k'}}) \\cap  \\cap_{k' \\notin K_{k,x}} \\cup CI(S_{k',x'})$\n$= CI(S_{k,x}) \\cap  \\cap_{k' \\notin K_k}  \\cup_{x'\\in I_{k,x,k'}} CI(S_{k',x'})$\n$\\subset {\\lambda : \\lambda \\leq B(p_k(x), n_k)}  \\cap \\cap_{k' \\neq k : q_{min} > q_k(n_k)} {\\lambda : \\lambda \\leq B(p_{k'}(x), n_{k'})}$\n$\\subset {\\lambda : \\lambda \\leq B(p_k(x), n_{K_k})}$\nwhere we used the fact that $B(p, .)$ is a non-increasing function. Now if $|X \\backslash X_{p_k}| = 1$, then we deduce that\n$K_{k,x} = {k} \\cup {k' \\in K\\backslash {k} : q_{min} > q_k(n_{k'})} = K_k$.\n$CI_{k,x} \\subset CI(\\cup_{k' \\in K_k} S_{k',X_{k'}})  {\\lambda : \\lambda \\leq B(p_{k'}(x), n_{K_k})}$"}, {"title": "C Examples of Surrogate Sets", "content": "In this section, we briefly mention some confidence intervals constructed using some well-known\nconcentration inequalities, and derive their corresponding surrogate intervals, summarized in the table at\nthe end of Section 4.\nKullback-Leibler (KL) confidence sets. Using the concentration inequality presented in [26] (see also\n[27] and [28]) for the control of KL deviations for Bernoulli random variables, one can define the following\nconfidence set:\n$CI(S, \\delta) = {x \\in [0, 1] : KL(\\bar p(S), 1) \\leq \\alpha_\\eta(|S|, \\delta)/|S|}$,\nwhere for $n \\in N, \\alpha_\\eta(|S|, \\delta) := \\eta \\text{log}(\\frac{1}{\\delta} (\\text{log}(n) \\text{log}(n^\\eta))^{log_2(\\eta)})$ with $\\eta > 1$ being an arbitrary peeling parameter, and\nwhere $KL(u, v)$ denotes the KL divergence between two Bernoulli distributions with parameters $u$ and $v$:\n$KL(u, v) = u \\text{log}(u/v) + (1 \u2013 u) \\text{log}((1 \u2013 u)/(1 \u2212 v))$. The confidence sets above admits the generic form (2)\nwith d being the KL divergence $KL$. Using Pinsker's inequality $KL(x, y) > 2(x - y)^2$ valid for all $x, y \\geq 0$\ngives the following surrogate:\n$SCI(S, \\delta) = {\\lambda \\in [0, 1] : |\\bar p(S) \u2013 \\lambda| \\leq \\sqrt{\\frac{\\alpha_{\\eta}(|S|, \\delta)}{2|S|}}}$\nBernstein confidence sets. The Bernstein concentration inequality for bounded random variables in\n[0, 1] directly leads to a confidence set taking the the generic form (2) with $b = b_{Berns}$,\n$b_{Berns} (A, S, \\delta) =  \\sqrt{\\frac{2\\lambda \\bar p(S)(1-\\bar p(S)) \\text{log}(\\frac{1}{\\delta})}{|S|}} + \\frac{\\text{log} (\\frac{1}{\\delta})}{3|S|}$\nThe following lemma presents a sharp SCI for such a set:\nLemma 1. Consider the Bernstein confidence set described above. Then, SCI in (3) with\n$B(\\bar p, |S|, \\delta) =  \\sqrt{\\frac{2\\bar p(S)(1-\\bar p(S)) \\text{log}(\\frac{1}{\\delta})}{|S|}} + \\frac{4.8 \\text{log} (\\frac{1}{\\delta})}{|S|}$\nconstitutes a corresponding SCI."}, {"title": "D Examples of Confidence Sets", "content": "In this section, for the sake of completeness and practical guidance, we discuss a few standard concen-\ntration results valid for $n$ i.i.d. observations. We then provide some perhaps less known concentration\nresults that are valid uniformly over all number $n$ of observations. They all help build the initial confidence\nsets CI(S, \u03b4).\nD.1 A Few Classical Concentration Inequalities\nSub-Gaussian confidence sets. We first recall that if $(X_i)_{i<n}$ are i.i.d. according to a distribution $\\nu$\nwith mean $\\mu$, that is $\\sigma$-sub-Gaussian, meaning\n$\\forall \\lambda \\in R, \\text{log } E[exp(\\lambda(X \u2013 \\mu))] \\leq \\frac{\\lambda^2 \\sigma^2}{2}$\nthen it holds by the Chernoff-method that\n$\\forall \\delta \\in (0,1), P(\\frac{1}{n} \\sum_{i=1}^n X_i - \\mu \\geq \\sqrt{\\frac{2\\sigma^2 \\text{log}(1/\\delta)}{n}}) \\leq \\delta$.\nRemark 3. Let us recall that distributions with bounded observations in [0,1] are 1/2-sub-Gaussian.\nBernstein confidence sets. Considering that $(X_i)_{i<n}$ are i.i.d. bounded in [0,1] with variance $\\sigma^2$, then\na Bernstein inequality yields, for all $\\delta \\in [0, 1]$,\n$P(\\frac{1}{n} |\\sum_{i=1}^n X_i - \\mu| \\geq \\sqrt{\\frac{2\\sigma^2}{n}} \\text{log}(\\frac{3}{\\delta}) + \\frac{\\text{log}(\\frac{3}{\\delta})}{n}) \\leq \\delta$Bernoulli confidence sets. Since Bernoulli random variables with mean parameter $\u03bc \u2208 [0, 1]$ are\nbounded in [0, 1] with variance $\u03bc(1 \u2013 \u03bc)$, sub-Gaussian bounds apply with $\u03c3 = \\frac{1}{2}$, and Bernstein bounds\napply with $\u03c3^2 = \u03bc(1 \u2013 \u03bc)$. On the other hand, it holds by a direct application of Cram\u00e9r-Chernoff method\nthat for all $\u03b5 \u2265 0$,\n$P(\\frac{1}{n} |\\sum_{i=1}^n X_i - \u03bc| \u2265 \u03b5) \u2264 exp (\u2212nKL(\u03bc + \u03b5, \u03bc),\nwhere $KL(u, v)$ denotes the Kullback-Leibler divergence between two Bernoulli distributions with parame-\nters $u$ and $v$: $KL(u, v) = u \\text{log}(u/v) + (1 \u2013 u) \\text{log}((1 \u2013 u)/(1 \u2212 v))$. The reverse map of the Cram\u00e9r transform\n$\u03b5 \u2194 KL(\u03bc + \u03b5, \u03bc)$ is unfortunately not explicit, and one may consider its Taylor's approximation to derive\napproximate but explicit high-probability confidence bounds; see [7, 30, 31]. More precisely, it is possible\nto derive the following sub-Gaussian control of the tails of Bernoulli observations:\nLemma 3. (Sub-Gaussianity of Bernoulli random variables, see e.g. [7]) For all $\u03bc \u2208 [0, 1]$, the left\nand right tails of the Bernoulli distribution are controlled in the following way\n$\\forall \u03bb \\in R, log E_{X\u223cB(\u03bc)} exp(\u03bb(X \u2013 \u03bc)) \u2264 \\frac{\u03bb^2}{2g(\u03bc)}$,\nwhere $g(\u03bc) = \\frac{1/2 - \u03bc}{log(1/\u03bc - 1)}$.\nThe control on right-tail can be further refined when $\u03bc \u2208 [\\frac{1}{2}, 1]$, as follows:\n$\\forall \u03bb \\in R^+, log E_{X\u223cB(\u03bc)} exp(\u03bb(X \u2013 \u03bc)) \u2264 \\frac{\u03bb^2}{2\u03bc(1 \u2013 \u03bc)}$.\nAs an immediate corollary, introducing the function $g(\u03bc) =  \\begin{cases}\n g(\u03bc) if \u03bc < 1/2\n \u03bc(1 - \u03bc) else\n\\end{cases}$, we obtain\n$\\forall \u03b4 \u2208 (0, 1), P(\\frac{1}{n} \\sum_{i=1}^n X_i - \u03bc \u2265 \\sqrt{\\frac{2g(\u03bc)log(1/\u03b4)}{n}}) \u2264 \u03b4,$\n$\\forall \u03b4 \u2208 (0, 1), P(\\frac{1}{n} \\sum_{i=1}^n X_i - \u03bc \u2264 -\\sqrt{\\frac{2g(\u03bc)log(1/\u03b4)}{n}}) \u2264 \u03b4.$\nThese inequalities yield CI(S, \u03b4) from Section 2 using\n$b_{sub-G}(\u03bc, |S|, \u03b4) = \\sqrt{\\frac{2g(\u03bc)}{|S|}} log (\\frac{1}{\u03b4}),$\n$b_{Bern} (\u03bc, |S|, \u03b4) = \\sqrt{\\frac{2\u03bc(1 - \u03bc)}{|S|}} \\text{log} (\\frac{1}{\u03b4}) + \\frac{2g(\u03bc)}{|S|},\nb_{sub-G-Bern}(\u03bc, |S|, \u03b4) =  \\sqrt{\\frac{2\u03bc(1 - \u03bc)}{|S|}} \\text{log} (\\frac{1}{\u03b4}) + \\frac{\\text{log} (\\frac{1}{\u03b4})}{3|S|}$\nD.2 Time-uniform Confidence Sets\nIn a number of machine learning applications, such as when sampling of observations is done actively\n(e.g. active learning, multi-armed bandits, reinforcement learning), it is often desirable to obtain concen-\ntration bounds that are not only valid with high probability for each n, but rather with high probability\nover all n \u2208 N simultaneously. In order to build time-uniform concentration inequalities, one may resort to\ntwo main tools (see, e.g., [26]). In the Gaussian setup for instance, one may resort to the mixture method\nfrom [32], while in general, a time-peeling proof technique can be considered. While the time-peeling\ntechnique leads asymptotically better bounds, the method of mixture yields usually tighter bounds for\nsmall to moderate values of n."}, {"title": "Time-uniform sub-Gaussian confidence sets.", "content": "For \u03c3-sub-Gaussian observations, it holds by the\nmixture method from [32], for all $\u03b4 \u2208 (0, 1)$,\n$P(\\exists n \u2208 N, |\\frac{1}{n} \\sum_{i=1}^n X_i - \u03bc| \u2265 2\u03c3\u03b2(n, \u03b4)) \u2264 \u03b4,$\nwhere for $n \u2208 N, \u03b2(n, \u03b4) := \\sqrt{(1 + \\frac{1}{n})log(\\sqrt{n} + 1/\u03b4)}$.\nTime-uniform Bernstein confidence sets. Recalling the definition\n$\\alpha_\\eta (n, \u03b4) := n \\text{log}((\\frac{1}{\u03b4}) \\text{log}(n)^{\\eta} \\text{log}(n^{\\eta})) \\frac{1}{log(n)^2}$,\na time-uniform version of the Bernstein bound yields (see [26], together with standard approximation of\nthe Cram\u00e9r transform of sub-Gamma distributions [33]), for all $\u03b4 \u2208 (0, 1)$,\n$P(\\exists n \u2208 N, |\\frac{1}{n} \\sum_{i=1}^n X_i - \u03bc| \u2265 \\sqrt{\\frac{2\u03bc\u03c3 \u03b1\u03b7(n,\u03b4)}{n}} + \\frac{\u03b1_\u03b7(n, \u03b4)}{3n}) \u2264 \u03b4$.\nTime-uniform Bernoulli confidence sets. Finally, adapting the method of mixtures to the fact that\nthe Bernoulli distributions do not have a symmetric sub-Gaussian control, one has\nLemma 4. ([34, Corollary 1]) Let $(X_i)_{i \\in N} \\stackrel{i.i.d.}{\u223c} B(\u03bc)$. Then, for all $\u03b4 \u2208 (0, 1)$, it holds\n$P(\\exists n \u2208 N, -2\\sqrt{g(\u03bc)}\u03b2(n, \u03b4) \u2264 \\frac{1}{n} \\sum_{i=1}^n X_i - \u03bc \u2264 2\\sqrt{g(\u03bc)}\u03b2(n, \u03b4)) \u2264 2\u03b4.$\nUsing these results, one may easily adapt the definition of CI(S, \u03b4) to the time-uniform setup."}]}