{"title": "MULTI-OBJECT EVENT GRAPH REPRESENTATION LEARNING FOR VIDEO QUESTION ANSWERING", "authors": ["Yanan Wang", "Shuichiro Haruta", "Donghuo Zeng", "Julio Vizcarra", "Mori Kurokawa"], "abstract": "Video question answering (VideoQA) is a task to predict the correct answer to questions posed\nabout a given video. The system must comprehend spatial and temporal relationships among objects\nextracted from videos to perform causal and temporal reasoning. While prior works have focused on\nmodeling individual object movements using transformer-based methods, they falter when capturing\ncomplex scenarios involving multiple objects (e.g., \u201ca boy is throwing a ball in a hoop\"). We propose\na contrastive language event graph representation learning method called CLanG to address this\nlimitation. Aiming to capture event representations associated with multiple objects, our method\nemploys a multi-layer GNN-cluster module for adversarial graph representation learning, enabling\ncontrastive learning between the question text and its relevant multi-object event graph. Our method\noutperforms a strong baseline, achieving up to 2.2% higher accuracy on two challenging VideoQA\ndatasets, NEXT-QA and TGIF-QA-R. In particular, it is 2.8% better than baselines in handling causal\nand temporal questions, highlighting its strength in reasoning multiple object-based events.", "sections": [{"title": "1 Introduction", "content": "Video question answering (VideoQA) is a task to predict the correct answer to questions posed about a given video.\nIt extends the concept of question answering from static images (e.g., VQA Antol et al. [2015], Wang et al. [2023]) to\ndynamic videos. Recent works Yang et al. [2021], Lei et al. [2021], Gao et al. [2023] have fine-tuned visual-language\npretraining models(VLMs) to acquire holistic video representations for frame-level question answering. To enhance\ncausal and temporal reasoning in VideoQA Xiao et al. [2021], recent works Jin et al. [2021], Geng et al. [2021], Xiao\net al. [2022a] employ graph neural networks (GNNs) for capturing precise object-level event details. These approaches\nmodel single-object events by tracking their motions, such as \u201ca boy is walking\", and \"a ball is moving\". However, they\nfall short in capturing multi-object events like \"a boy is throwing a ball into a hoop,\" which involve three objects (the\n\"boy\", \"ball\", and \"hoop\"). This limitation arises due to the absence of temporal relationships between multiple objects,\nhindering the system's ability to capture multi-object event representations for enhancing causal and temporal question\nanswering in videos.\nThis work proposes a contrastive language event graph representation learning method called CLanG to capture\nhierarchical event representations associated with multiple objects (see Fig. 1). It contains a multi-layer GNN-cluster\nmodule and two training objectives for effective event graph representation learning. Inspired by recent graph pooling\napproaches Ying et al. [2018], Bo et al. [2020], Bianchi et al. [2020], Liu et al. [2022] that coarsening large real graphs\nsuch as a social network graph into smaller sizes for extracting useful clusters, the multi-layer GNN-cluster module\nexpands a graph pooling method Ying et al. [2018] to multiple layers to obtain multi-scale event graphs. In addition, to\neliminate the loss of original graph information with the proposed multi-layer GNN-cluster module, we reduce the size\nof the output graphs during the first half of the GNN-cluster layers. Conversely, in the latter half, we increase the number\nof output nodes to match the input graph. The multi-layer GNN-cluster module also involves a self-attention layer to\nhighlight the impact of hierarchical event graphs. We further perform adversarial graph representation learning Pan et al.\n[2018] with the output of the last GNN-cluster layer to enforce all node representations to follow regular distributions\nto improve the language event graph contrastive learning process."}, {"title": "2 Related work", "content": "Spatio-temporal graph-based VideoQA methods. Recent works Huang et al. [2020], Geng et al. [2021], Xiao et al.\n[2022b], Wang et al. [2023] unify graph neural networks (GNN) with a transformer to capture the relationship between\nvisual objects for achieving fine-grained object-level scene understanding. The work Xiao et al. [2022a] adds temporal\nedges to track individual objects across the built scene graph of each video frame. However, it focuses on capturing the\nsingle-object event by giving a well-structured spatio-temporal graph. In contrast, our model captures multi-object\nevent representations with a multi-layer GNN-cluster module by giving a fully connected multi-object event graph\nacross the whole video.\nGraph pooling methods. Graph pooling is an essential element of GNN architecture for obtaining a holistic graph-level\nrepresentation of the entire graph Liu et al. [2022]. Recent works Ying et al. [2018], Bo et al. [2020], Bianchi et al.\n[2020] propose a differentiable graph pooling module to learn how to generate hierarchical representations of graphs,\nhave broadly adapted to graph classification and graph generation tasks. However, the capability of these models is\nalways limited by the low number of layers. Our method enables multi-layer graph poolings by computing adversarial\ngraph representation learning and language-graph contrastive learning for capturing multi-scale hierarchical event\nrepresentations."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Multi-object event graph Initialization", "content": "We segment a video into K clips, with each clip containing L frames, following the way in Xiao et al. [2022b]. We then\nextract N RoI features $X^{N\\times d_1} = \\{x^{d_1}_n\\}_{n=1}^{N}$ along with their corresponding spatial position $X^{N\\times d_2} = \\{x^{d_2}_n\\}_{n=1}^{N}$ from\neach frame. This extraction is accomplished through a pretrained object detector Anderson et al. [2018]. We initialize"}, {"title": "3.2 Multi-layer GNN-cluster module", "content": "The Multi-layer GNN-cluster module is a critical component of our approach, aimed at modeling expressive hierarchical\nevent representations using the built multi-object event graph (see Fig. 2). This module comprises an N-layer GNN-\ncluster, which serves to coarsen the input graph (A(1), X(1)) into a new coarsened graph (A(1+1), X(1+1)), while also\nlearning a cluster assignment matrix S(l) \u2208 Rn\u0131\u00d7n\u0131+1. This assignment matrix learning follows the DIFFPOOL\narchitecture Ying et al. [2018]. To enhance the assignment matrix learning, we integrate two GAT Velickovic et al.\n[2017] modules into the original DIFFPOOL design. These GAT modules enable better capturing of mutual information\nbetween neighboring nodes. The GNN-cluster computation is performed by inputting the graph G = (A(1), X(1)) using\nthe following steps:\n$Z^{(l)} = GAT_{I,embed}(A^{(l)}, X^{(l)}),$\n$S^{(l)} = softmax(GAT_{I,pool}(A^{(l)}, X^{(l)})),$\n$X^{(l+1)} = S^{(l)} Z^{(l)} \\in \\mathbb{R}^{n_{l+1}\\times d},$\n$A^{(l+1)} = S^{(l)}^T A^{(l)} S^{(l)} \\in \\mathbb{R}^{n_{l+1}\\times n_{l+1}}$"}, {"title": "3.3 Adversarial graph representation learning", "content": "To enhance the training of the multi-layer GNN-cluster module, we introduce a discriminator D to enforce the latent\nnode representations Z (the output of the final GNN-cluster $G^{(l+1)} = (A^{(l+1)}, X^{(l+1)})$) to match a normal distribution\npz. The discriminator D constructed as a standard multi-layer perception (MLP), generates a one-dimensional output\nunit followed by a sigmoid layer. We compute the following cross-entropy cost LD to distinguish whether the input of\nD is from pz (positive) or from the GNN-cluster (negative).\n$L_D = -\\mathbb{E}_{z \\sim p_z} \\log D(z) - \\mathbb{E}_{x^{(l+1)}} \\log (1 - D(G^{(l+1)}))$\nMeanwhile, we compute a regularization loss termed $L_{G^{(l+1)}}$ for the GNN-cluster to ensure the regularization of node\nrepresentations.\n$L_{G^{(l+1)}} = -0.5 \\times \\mathbb{E}_{x^{(l+1)}} \\log (D(G^{(l+1)}))$\nThe adversarial graph representation learning is performed by minimizing both LD and $L_{G^{(l+1)}}$."}, {"title": "3.4 Language event graph contrastive learning", "content": "Utilizing BERT LM, We encode the question text to obtain the question embedding Xq. Given a batch of N (Xq, Xg)\npairs, CLanG is trained to predict the positive pair to learn a joint embedding space across language and visual\nmodalities. This process enables the multi-layer GNN-cluster module of the model to learn textual event knowledge\nwithin the event graph nodes. We apply contrastive learning by minimizing InfoNCE Oord et al. [2018] loss, which is\nformulated as follows:\n$L_N = - \\log \\frac{e^{sim(X_q,X^+)/\\tau}}{e^{sim(X_q,X^+)/\\tau} + \\sum_{i=1}^{K}e^{sim(X_q,X_i)/\\tau}}$\nwhere sim(Xq, X\u2020) denotes the cosine similarity of the positive pair. (Xq, X\u2081) denotes the negative pair and K is the\nnumber of negative pairs in a batch, set to N2 \u2013 N. The temperature 7 is set to 0.1.\nIn addition, we minimize Kullback-Leibler divergence loss LKL for the positive pairs to match the probability\ndistribution of the text encoder and the multi-layer GNN-cluster module Passalis and Tefas [2018]. LKL is formulated\nas:\n$P_q = \\frac{sim(X_q,X^+)}{\\sum_{i=1}^{N} sim(X_q,X_i)}, P_g = \\frac{sim(X_g,X^+)}{\\sum_{i=1}^{N} sim(X_g,X_i)},$\n$L_{KL} = \\sum_{i=1}^{N}(P_g[i] \\log \\frac{P_g[i]}{P_q[i]})$"}, {"title": "3.5 Video question answering", "content": "CLanG reasons the correct answer a by leveraging the encoded event graph representations X, and question embedding\nXq. For the multi-choice QA task (e.g., NExTQA and TGIF-QA-R), multiple choices Amc are provided along with\neach question. All candidates a \u2208 Amc are encoded using the same encoder employed for the question text. In contrast,\nfor the open-ended QA task, each question corresponds to a broader set of candidates A, where only one candidate\na \u2208 A, is the most suitable answer.\nDuring the training process, CLanG is end-to-end optimized by minimizing a unified loss L given by:\n$\\mathcal{L} = L_D + L_{G^{(l+1)}} + L_N + L_{KL} + L_{QA}$"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "We evaluate CLanG on three challenging videoQA datasets (Tab. 1), NExT-QA Xiao et al. [2021], TGIF-QA-R Peng\net al. [2021]. NExT-QA is a multi-choice videoQA benchmark featuring causal and temporal questions involving\nobject-level spatio-temporal reasoning. TGIF-QA-R is a reconstruction of TGIF-QA Jang et al. [2019], wherein all\nground-truth answers are collected as part of an answer vocabulary. Candidate answers are then randomly selected from\nthis vocabulary to mitigate answer biases. This reconstruction amplifies the challenge, particularly in the question type\nof state transition."}, {"title": "4.2 Training details", "content": "We decode the video into K = 4 clips, and each clip contains L = 8 frames. We utilize top N = 20 object nodes\nfor NEXT-QA and N = 10 for TGIF-QA-R to achieve our best scores. The dimension of CLanG's hidden states is\nd = 512. The model is optimized with AdamW under a learning rate of 1e-5. The batch size is 32 and the models are\ntrained for 30 epochs on a single NVIDIA Quadro RTX 8000 GPU."}, {"title": "4.3 Performance", "content": "In Tab. 2, CLanG-BERT achieves 2.2% higher averaging test accuracy for all question types compared to a strong\nbaseline method VGT Xiao et al. [2022a]. In particular, our method scores 2.8% better at handling causal and temporal\nquestions since it can comprehend multiple events connected to various objects. VGT focuses on tracking single-object\ntemporal relationships within individual video clips to derive object-level spatio-temporal event representations, it\nencounters challenges in modeling complex, long-form multi-events involving multiple objects. In contrast, CLanG con-\nstructs a multi-object event graph utilizing a Multi-layer GNN-cluster module across the entire video sequence, enabling\nthe acquisition of multi-object hierarchical event representations. Moreover, CLanG-Roberta achieves a validation\naccuracy of 59.15% for causal reasoning and 60.55% for all question types. Even though MIST-CLIP Gao et al. [2023]\nadapts a visual-language pretraining model for intricate long-form videoQA, CLanG-Roberta surpasses MIST-CLIP by\n3.3% in the realm of causal reasoning, suggesting its efficacy in executing language event graph representation learning.\nThis, in turn, empowers our novel Multi-layer GNN-cluster module to extract multi-object event representations from\nthe RoBERTa-based model. Moreover, a comparison between CLanG-BERT and CLanG-ROBERTa indicates the\ncapacity of a potent large-scale language Model (LLM) to elevate performance through language event graph contrastive\nlearning. Tab. 3 shows the comparison results on TGIF-QA-R dataset. CLanG-RoBERTa improves VGT on the state\ntransition types by over 1.9% test accuracy, suggesting the efficacy of temporal reasoning."}, {"title": "4.4 Ablation study", "content": "Effect of Multi-layer GNN-cluster. The results in Tab. 4 demonstrate that the 8-layer Multi-layer GNN-cluster\nimproves the model without any GNN-cluster layer by 1.1% in test accuracy for NEXT-QA. While CLanG without\nGNN-cluster still manages to enhance the prior methods due to the proposed language event graph representations\nlearning, it fails to capture multi-scale hierarchical event graphs essential for further enhancing causal and temporal\nreasoning.\nEffect of training strategies. To study the effect of the proposed adversarial graph representation learning and language\nevent graph representation learning, we conducted a comparison between models: CLanG-BERT without adversarial\ngraph representation learning and CLanG-BERT without language event graph representation learning, against the\noriginal model (as shown in Tab. 5). The exclusion of either of these training strategies led to a performance reduction\nof the original model to 2.5%, suggesting their effectiveness in training expressive hierarchical event representations\nthrough the proposed Multi-layer GNN cluster module."}, {"title": "5 Conclusion", "content": "We proposed a novel video question answering method CLanG that models hierarchical event representations associated\nwith multiple objects for excelling causal and temporal reasoning. In the next step, we will expand our work to build a\nlanguage-graph fundamental model that leads to fine-grained video scene understanding."}]}