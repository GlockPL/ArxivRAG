{"title": "Representation in large language models", "authors": ["Cameron C. Yetman"], "abstract": "The extraordinary success of recent Large Language Models (LLMs) on a diverse array of tasks has led to an explosion of scientific and philosophical theorizing aimed at explaining how they do what they do. Unfortunately, disagreement over fundamental theoretical issues has led to stalemate, with entrenched camps of LLM optimists and pessimists often committed to very different views of how these systems work. Overcoming stalemate requires agreement on fundamental questions, and the goal of this paper is to address one such question, namely: is LLM behavior driven partly by representation-based information processing of the sort implicated in biological cognition, or is it driven entirely by processes of memorization and stochastic table look-up? This is a question about what kind of algorithm LLMs implement, and the answer carries serious implications for higher level questions about whether these systems have beliefs, intentions, concepts, knowledge, and understanding. I argue that LLM behavior is partially driven by representation-based information processing, and then I describe and defend a series of practical techniques for investigating these representations and developing explanations on their basis. The resulting account provides a groundwork for future theorizing about language models and their successors.", "sections": [{"title": "Introduction", "content": "Artificial intelligence systems have played at least two major roles in the development of cognitive science. First, they have been used as models of cognition \u2013 toy systems which help us conceptualize the kinds of processes which underlie our ability to plan, reason, navigate, and understand the world (Newell, 1980; Rumelhart & McClelland, 1986; Fodor & Pylyshyn, 1988; Stinson, 2020; Griffiths et al., 2024). Second, they have been treated as candidate cognitive systems themselves as not only modeling cognition, but instantiating it (Turing, 1950; McCarthy, 1979; Aleksander, 2001; Chella & Manzotti, 2007; Goldstein & Levinstein, 2024).\nLarge language models (LLMs) play both of these roles in the recent history of cognitive science. Some papers use them as models of cognition (Jin et al., 2022; Binz & Schulz, 2023; Sartori & Orr\u00f9, 2023; Niu et al., 2024), while others debate whether they themselves can reason, understand, or communicate meaningfully (Bender & Koller, 2020; Bubeck et al., 2023; Dhingra et al., 2023; Mitchell & Krakauer, 2023; Goldstein & Levinstein, 2024; Stoljar & Zhang, 2024). Unfortunately, interlocutors often come to these debates with very different understandings of what cognition involves (and so what it takes to model or instantiate it) and of what kinds of processes actually underlie the behavior of LLMs. At one extreme are those for whom LLMs' success on apparently cognitive tasks entails the possession of cognitive capacities. On the other are those for whom no amount of success has this consequence, since the way in which LLMs achieve it is incompatible with their being genuine cognizers (Marcus & Davis, 2020; Bender et al., 2021; Marcus 2022; Titus, 2024). These disagreements are quickly becoming entrenched into camps of LLM optimists and pessimists, and accordingly we are at risk of disciplinary stalemate.\nOvercoming stalemate requires agreement on the fundamentals. The goal of this paper is to address one fundamental issue in the background of current debates, namely the question of whether LLM behavior is driven by representation-based information processing, or whether it is driven by non-representational processes of memorization and stochastic table look-up. Representation-based information processing is typically considered the foundation of human and animal cognition (Fodor, 1975; Sterelny, 1991; Eckardt, 2012; Shea, 2018); the algorithms posited to explain our cognitive capacities are formulated in representational language. Although there is persistent disagreement over their nature and extent (Clark & Toribio 1994; Van Gelder, 1995; Chemero, 2000; Clark, 2015; Constant et al., 2021), that representations play a central role in (especially higher) cognitive processes such as language use, planning, reasoning, and abstract problem solving is reasonably uncontroversial, and so should serve as common ground for both sides of the LLM debate. Language use, planning, reasoning, and problem solving are all tasks at which LLMs have shown surprising proficiency (Gubelmann et al., 2024; Liu et al., 2023; Naik et al., 2023). If they perform these tasks via representational means, they are more likely to serve as adequate cognitive models, and more likely to be candidate cognizers themselves."}, {"title": "Representation", "content": "In order to survive and flourish, organisms must solve the difficult task of appropriately modulating their behavior in response to environmental conditions. Sometimes this can be done quite directly, as in the case of reflexes, where there is an automatic stimulus-response pattern either learned from experience or built innately into the organism. Other times, organisms can take advantage of dynamic perception-action feedback loops in order to actively and continuously control the values of relevant internal or external variables (Van Gelder, 1995; Chemero, 2000). Yet other times, the task requires gathering information about features of the environment, forming internal proxies of those features, and processing those proxies over some time-range in order to figure out what to do. These proxies are called \"representations\", and they plausibly undergird most forms of higher cognitive processing.\nRepresentations need not be conscious or introspectively accessible to the system which employs them; the model of representation in this sense is not the conscious perceptual representation, but the subconscious and subpersonal cognitive map, feature detector, or parse tree.\nThe claim that LLMs or any other neural network (NN)-based system have representations as well is likely to strike some machine learning (ML) practitioners as trivial, since in ML the activity of NNs is often defined in terms of the formation of representations \u2013 as a kind of \u201crepresentation learning\" \""}, {"title": "INFORMATION", "content": "INFORMATION captures the idea that representations are proxies for the feature(s) represented. Intuitively, in order to think about something that isn't immediately present to you, you must have cognitive access to something else that stands for that thing. A stand-in need not be a perfect or accurate model of the feature: I can think about Toronto without knowing everything about it. Rather, it need only carry information about the feature. The relevant sense of information is mutual information, which, colloquially, is a measure of the degree to which knowledge of the value of one variable reduces uncertainty about the value of another (Cover & Thomas, 2006). For example, if X is a binary variable corresponding to the presence or absence of smoke, and Y is a binary variable corresponding to the presence or absence of fire, knowing the value of X (knowing whether there is smoke) reduces uncertainty about the value of Y (about whether there is fire). X and Y carry mutual information. Accordingly, one could use X as a proxy, or stand-in, for Y, for instance when deciding whether or not to deploy firefighters."}, {"title": null, "content": "This colloquial characterization is intuitive but unsatisfactory, since \u201cknowledge\" and \"uncertainty\" are mental state terms, and my account is meant to apply at a sub-mental (computational) level. More technically, then, we can define the mutual information I of two variables X and Y as the difference between the entropy H of X and the conditional entropy of X and Y:\n(1) $I(X,Y) = H(X) \u2013 H(H|Y)$\nEntropy is a measure of the distribution of probabilities across all potential states of a variable. A variable with a (probabilistically) diffuse range of potential states has high entropy, while a variable with a narrow range of potential states has low entropy. The entropy of X is a measure of how diffuse X is, and the conditional entropy of X and Y is a measure of how diffuse X is given Y. If X and Y are completely independent variables, H(X|Y) = H(X), and so I(X, Y) = 0. If X and Y are not independent, H(X|Y) < H(X), and so I(X, Y) > 0. INFORMATION is satisfied when I(R, z) > 0, where R and z are variables corresponding to a putative representation and feature. That said, I(R, z) is likely to be much higher in most actual cases, since resource-limited systems are unlikely to devote significant effort towards exploiting a proxy which bears only a tiny amount of information about the feature of interest.\nFinally, there are different ways variables can become entwined such that they bear mutual information. Two ways which are especially important in cognitive science, discussed in-depth by Shea (2018), are causally generated correlations between the variables (as in the smoke-fire case) and structural correspondences between them (as in the case of a map). I provide examples of both kinds throughout the paper."}, {"title": "EXPLOITABILITY", "content": "EXPLOITABILITY says that the information R carries about z is exploitable by S. Intuitively, this means that S must be able to access or use the information. But \u201cinformation\" is not a substance, it cannot literally be used: it plays no causal role. Rather, S can exploit the information R carries about z when S's z-related behavior can be conditioned on the tokening of R in a content-relevant way. Analogously, I can exploit the information my map of Toronto carries about the city's street layout when I can condition my navigational behavior on the map \u2013 i.e., when I can use it to navigate. Were I only able to use the map to, say, soak up spilled milk, I would not be able to use it in a content-relevant way, and so EXPLOITABILITY would not be satisfied. The reader may analyze the phrases \u201ccan exploit\u201d and \u201cable to use it\" according to their favourite theory of ability modals. But practically speaking, learning whether EXPLOITABILITY is satisfied in a particular case is likely to depend on the performance of tests and interventions on R \u2013 altering R to see if S's z-related behavior changes in appropriate ways given R's putative content. See section 4 for a detailed discussion of such tests in LLMs."}, {"title": "BEHAVIOR", "content": "BEHAVIOR requires that S's exploiting the information R carries about z enables S's robust z-related behavior. Two terms are worth clarifying here. First, \u201cenables\u201d is an intentionally weak requirement; my map of Toronto enables me to navigate, but so does wearing my glasses. However, any stronger requirement (eg. \u201cguarantees\u201d, \u201censures\u201d, etc.) would be too strong, since the way a representation influences behavior never depends merely on the representation, but also on the algorithm(s) and context(s) in which it is embedded: \u201ceven for a given fixed representation, there are often several possible algorithms for carrying out the same process. [O]ne algorithm may be much more efficient than another, or another may be slightly less efficient but more robust\" (Marr, 1982, 23-24). In other words, representations characteristically enable robust behavior given that they are embedded in an appropriate sort of algorithm, where appropriateness depends on the properties of the representing system and the demands of the task.\nSecond, a behavior is \u201crobust\u201d if it is insensitive to minor perturbations in surrounding conditions. Robustness is a modal notion a system must be insensitive to perturbations in different possible scenarios, even if they never actually encounter those scenarios. Plausibly, enabling modally robust behavior is one of the central functions of representations, and representation-based explanations are typically invoked precisely in order to explain such behavior. This is the key virtue of representation-based explanations over explanations in terms of more basic or lower-level goings-on. To borrow one of Shea's (2018) examples, we can provide a unified and perspicuous explanation of how a squirrel robustly reaches a bird feeder by attributing goal-representations and spatial-representations to the squirrel. If instead we try to explain its behavior in terms of basic physical or neurobiological activities, our explanation of how the squirrel reaches the feeder in one set of conditions will be very different from the explanation of how it reaches the feeder in another set, leaving us with a disjunctive rather than a unified account of its behavior. Of course, this claim has limits: a maximally unified account of all phenomena is \"what happens, happens\", but a good scientific explanation must also be an informative one.\nSome authors tie representations directly to behavioral success, rather than mere robustness: \u201ccorrect and incorrect representation explains successful and unsuccessful behavior\u201d (Shea, 2018, 24).\nThis requirement is too strong, since the same representation embedded in different algorithms can enable either robust success or robust failure. Returning to our example, if I use my map of Toronto in the standard way \u2013 aligning the compass rose with my actual compass, turning right when the map tells me to turn right, etc. \u2013 it will directly enable my success at navigating. However, if I use it in a non-standard way - aligning the compass rose opposite to my actual compass, turning right only half of the time the map tells me to, etc. \u2013 it will directly enable my failure at navigating. The key point is that I will robustly succeed at navigating in the first case, and robustly fail in the second case. Tying representations to success would force us to say that I have a map in the first, but not the second case, which seems both ad hoc and clearly false."}, {"title": "ROLE", "content": "ROLE fleshes out the idea that S's z-related behavior is \"conditioned on\" the tokening of R. What it is for S's behavior to be conditioned on R is for R to play a mechanistic role in that behavior, where R plays a mechanistic role just in case it plays a causal role in the mechanisms driving the behavior (Craver, 2007)."}, {"title": null, "content": "This point helps combat worries about panrepresentationalism: since almost everything is correlated with everything else in some way, and so carries information about those things, a characterization of representation should provide a principled way to carve out those which play the representation-role from the wider space of information-bearing entities and states, lest its theoretical value be neutered. For example, my map not only carries information about the layout of Toronto streets, but about the type of ink available at the factory where it was printed, about the formatting conventions of modern maps, etc. However, ROLE is only satisfied when the map plays a mechanistic role in my robust feature-related behavior; in the navigation example, I exhibit no behavior related to the proposition that such and such ink was available at the factory, or that such and such conventions are standard in modern maps, and so the map does not represent those features. This isn't to say it never could do so, but the conditions under which it could would need to be significantly different. Hence, panrepresentationalism is avoided.\nThis characterization is far more substantial than that implicit in machine learning practice, and it captures the most important features of representations as discussed in cognitive science \u2013 they are proxies for features of some environment or input space in virtue of the exploitable information they carry, and they play a causal role in enabling robust feature-related behavior. If LLMs turn out to have representations of this sort, that is no trivial matter; it would open up new avenues for understanding and explaining their behavior, and justify the application of existing theoretical and experimental tools from representation-based approaches to cognitive science. Before outlining some of these approaches, the next section considers and rejects an alternative strategy for explaining LLM behavior, according to which they are non-representational systems equivalent to look-up tables."}, {"title": "LLMs as look-up tables", "content": "I am arguing that LLM behavior is often the result of representation-driven information processing. As noted, this claim would be trivial if representations were mere causal intermediaries between model inputs and outputs. My goal in the previous section was to provide a characterization of representation which is substantial enough to avoid the charge of triviality. Another way my argument would be rendered uninteresting is if there were no viable alternative explanations of LLM behavior. The goal of this section is to describe one powerful alternative, and then to show why it fails. This renders the representational strategy the best one available, since there are no other serious contenders in the offing. Section 4 explores in some detail how that strategy can be pursued. But first, the alternative."}, {"title": "Look-up tables", "content": "Large language models are neural networks. Neural networks are functions from inputs to outputs. Functions can be realized by various kinds of algorithm. Some algorithms, including many of those implicated in human cognition, involve the processing of information-bearing representations. But any finite function realizable by representation-based processing can also be realized by a \u201clook-up table\u201d in which the input-output pairs which characterize the function are explicitly encoded into the parameters of the processor. Intuitively, a look-up table is a list of inputs matched with appropriate outputs. Given an input, the output is retrieved, rather than generated. Systems which realize functions through look-up tables are finite state automata (FSA), since they can only process the finite set of input-output pairs to which they have been exposed (Donahoe, 2010).\nNeural networks are sometimes viewed as finite state automata, with look-up tables encoded into the learned parameters of the network. On this view, \"[e]xperience puts neural machines in enduringly different states (rewires them to implement a new look-up table), which is why they respond differently to inputs after they have had state-changing (rewiring) experiences\u201d (Gallistel & King, 2009, 94-95). LLMs are neural networks with billions or trillions of parameters, and so perhaps they can be understood as encoding massive, tremendously complex look-up tables. The enormous size of such tables should not be underestimated, especially since it has been shown that neural networks can encode more features than they have dimensions via \u201csuperposition\u201d (Elhage et al., 2022). With sufficiently large and diverse tables, LLMs could plausibly produce articulate text and perform well on some cognitive tests solely in virtue of having encoded (or as is often said, \u201cmemorized\u201d) appropriate patterns from the training data. If so, there would be little reason to suppose they have also developed representations (in my sense) of features of that data, since these would serve no functional role over and above that played by the look-up table."}, {"title": "LLMs as look-up tables", "content": "A number of recent papers characterize LLMs along roughly these lines. First, Bender & Koller (2020) claim that in order to respond coherently to evolving and unfamiliar inputs, an LLM \u201cwould need to memorize infinitely many stimulus-response pairs\u201d (5193). In other words, it would require an infinitely long (and appropriately diverse) look-up table. Second, Bender et al. (2021) describe an LLM as \u201ca system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine\u201d (617).\nThis claim admits of two different interpretations. It could just be the trivial claim that LLMs generate outputs by combining linguistic tokens according to their distributional properties in a text corpus. That is just a description of the task being solved by the models, and leaves open the question of which sort of algorithm is employed to solve it. However, it could also be the claim that LLMs generate outputs by drawing stochastically (\"haphazardly\") from a look-up table learned through observations of token combinations in the training set. This is not a claim about the task, but about the algorithm. Accordingly, it is this interpretation which poses a challenge to my conclusion in this paper. Third, Dentella et al. (2024) contend that LLMs produce outputs by \"predict[ing] some of the fossilized patterns found in training texts\" (7) and that they \u201cdo not go beyond reproducing input patterns\u201d (7). In other words, LLMs memorize patterns of input from training data, and parrot those patterns back to the user during inference. This is just another way of describing a machine-learned look-up table."}, {"title": "Case studies", "content": "Systems based on look-up tables are characteristically unproductive \u2013 they \u201conly give back what has been already put in\" (Gallistel & King, 2009, 93). In other words, they cannot generalize to inputs unlike those they have previously observed. Analogously, a multiplication table which provides answers to every multiplication problem from 1*1 to 12*12 cannot tell you the answer to 13*12. However, a system with compositional representations of numbers and algorithms for combining them should fare better. To test whether LLMs are acting on the basis of memorized associations or generalizable representations, we can present them with out-of-distribution inputs and see how they respond. Given their aforementioned size and breadth of learning, it can be difficult to know whether they have encountered a particular test before. However, a number of compelling recent papers have designed tests with this worry in mind, and their results strongly suggest that LLMs do not always rely on look-up tables when generating responses. I will argue in section 4 that such tests are not necessarily diagnostic of representation-based processing, but they are sufficiently suggestive as to render the look-up table view highly implausible in the cases discussed."}, {"title": "Othello-GPT", "content": "In the first study, Li et al. (2023) trained an autoregressive GPT on millions of transcripts of the board game Othello. The transcripts consisted entirely of move sequences (eg. A4, E6, B3, etc.) with no information about the rules of the game or the properties of the board. In the control condition, Othello-GPT was provided with a sequence of moves and prompted to continue it, with success defined by the proportion of legal moves taken. The model proved highly effective, achieving a success rate of 99.99% at its best. To check whether the model had merely memorized move sequences from this set, the authors retrained it on a custom \u201cskewed\" dataset which was missing a quarter of the game tree, meaning that the model had never seen a large number of possible board configurations. They then tested the model on a standard, non-skewed dataset, and it proved just as successful as the control (99.98% legal moves).\nIf Othello-GPT were relying on a look-up table, it would be mysterious how it managed to perform so well in the experimental condition, since in that case it had no access to the relevant \u201cstimulus-response pairs\u201d. It could not merely be stitching together sequences of forms it observed in its training data, since many of the sequences on which it was tested were not present in that data. Accordingly, the look-up table interpretation of Othello-GPT's behavior is implausible."}, {"title": "Model of colour space", "content": "In the second study, Patel & Pavlick (2022) tested a pre-trained GPT on its ability to make inferences about the structural properties of two domains, namely the domains of colour and space. For brevity I will focus on colour. The authors employed an in-context learning paradigm where the model was provided with sixty training examples and then tested on unseen cases. In the control condition, training examples consisted of pairings of RGB codes with their corresponding colour names (eg. RGB: [255, 165, 0], Answer: orange) drawn from a limited part of the colour spectrum. At test time, models were prompted with RGB codes from a different part of the spectrum and asked to produce the corresponding colour names. The model achieved 34% Top-3 accuracy on this task (where the correct answer was within the three most probable answers provided by the model) \u2013 significantly above chance (13%), and possibly artificially low given that a mark of \u201ccorrect\" required an exact string match (so that, for instance, a model which output \"wine\" when the correct answer was \u201cdark red\" would be marked as incorrect).\nTo ensure that the model had not merely memorized colour-code pairings from its training set, they tested the model in a \u201crotated\u201d condition where the colour-code pairs were systematically permuted so as to be isomorphic to the original pairs. In other words, all individual colour-code pairs were different in an absolute sense, and so unlikely to be present in the training data, but the structural relations between them (i.e., their relative distance and directional properties in colour space) remained the same. The model performed equally well in the rotated condition as in the control (36% Top-3 accuracy). If this model were acting via a look-up table, it would be very difficult to explain its generalization ability in the novel, rotated context. The natural (though not necessarily inevitable) alternative explanation is that it had learned a representation of the structure of the colour space which enabled it to infer the \"location\" of new colours (really, colour names) based on information about others."}, {"title": "Objections", "content": "There are a number of places one might push back against my claims in this section."}, {"title": "LLMs are obviously not FSAs", "content": "First, one might object that LLMs are obviously not finite state automata, and so I must be strawmanning my opponent by attributing this view to them. The objection might seem reasonable since the well-known results of Pollack (1987) and Siegelmann & Sontag (1991) show that some types of neural network, namely higher-order nets and processor nets, are Turing complete. Since by definition no FSA is Turing complete (Hopcroft et al., 2001), NNs of the appropriate types are not FSAs. Hence, charity demands that I must be misinterpreting my opponents.\nHowever, even assuming that transformers are among the class of Turing complete NNs, the representational strategy does not lose its explanatory traction. The reason is that, so far as anyone knows, the only way for NNs to achieve Turing completeness is by implementing a symbolic read-write memory, which is a paradigmatic representational system. For instance, recurrent neural networks a type of higher-order net can use reverberating signal loops to carry information forward in time for future use. This is a way of implementing a read-write memory (Gallistel & King, 2009, ch. 14). Similar looping mechanisms are hypothesized to underlie the purported Turing completeness of transformers. More generally, it is recognized that making any NN complete means augmenting it with a symbolic memory (however implemented; Schuurmans, 2023). Indeed, for a machine of any sort to be computationally equivalent to any class of automaton more powerful than an FSA (eg. a pushdown automaton) it must implement some form of read-write memory (Hopcroft et al., 2001, ch. 6; Sipser, 2006, ch. 2)."}, {"title": "LLMs are not deterministic FSAs", "content": "A second objection is that even if LLMs are FSAs, they need not be deterministic in the way I have suggested - especially with my multiplication table analogy. After all, there are such things as non-deterministic FSAs, where any given input is associated with several possible outputs, and the selection of output is stochastic rather than deterministic. LLMs are often described as \u201cstochastic parrots\u201d (Bender et al., 2021), and perhaps this stochasticity is what accounts for their generalization ability in the above tasks. Unfortunately, it has long been proven that any non-deterministic FSA is equivalent to a deterministic FSA (Rabin & Scott, 1959), with the main benefits of non-deterministic FSAs being their compactness and relative ease of construction. As such, evidence that LLMs are not deterministic FSAs is evidence that they are not FSAs simpliciter."}, {"title": "LLMs are lossy compression algorithms", "content": "A third and final objection is suggested by a metaphor for LLMs proposed by Chiang (2023), according to which LLMs are akin to lossy compression algorithms, or \u201cblurry JPEG[s] of all the text on the Web\u201d. It is in virtue of this compression that LLMs don't simply memorize exact patterns of text from their training corpora: \u201cyou can't look for information [in an LLM] by searching for an exact quote; you'll never get an exact match, because the words aren't what's being stored\u201d. For Chiang, compression explains the appearance of novelty and generalization in the outputs of LLMs, but does not underly any actual capacity to reason or understand the tasks they perform. The metaphor is apt, and it may even have some of the deflationary consequences Chiang takes it to have; it might imply that LLMs don't really understand, for instance. However, information can be compressed in many ways, and as Trott (2024) observes, \u201cthis property of \u2018compressing input into lossy but useful representations with which to make future predictions' looks a lot like certain theories of cognition\u201d, namely representational theories. In other words, the compression view isn't an alternative to the representational one. Rather, compression is a candidate mechanism by which LLMs form the representations which guide their future behavior. Implications for LLM reasoning or understanding are beyond the scope of this paper.\nIn this section, I have explained what it would mean for LLMs to be look-up tables, described how this view predicts LLMs will behave, introduced two empirical results which disconfirm those predictions, and responded to some objections. Note that my conclusion is not that LLM behavior is never driven by table look-up: sometimes it certainly is, and indeed look-up tables are central to the behavior of computational systems of all sorts, from digital computers to human brains. The claim, rather, is that LLMs do not always act according to look-up tables, and that in such cases representation-based information processing is likely to be at work. My goal, however, is not merely to uphold the in-principle applicability of a representational approach, but to describe and defend the viability of some practical techniques for pursuing it. This is the topic of the next section."}, {"title": "In search of representations", "content": null}, {"title": "Diagnostic behavior?", "content": "The discussion so far has suggested that there is something special about model behavior which is flexible and generalizable \u2013 in other words, robust \u2013 such as that exhibited by Othello-GPT. One might expect that this sort of behavior is diagnostic of the presence of representations (eg. as claimed by Dasgupta et al. 2019), since it is often difficult to imagine how a system governed only by stimulus-response pairings or some similarly inflexible substructure could take such generalizable actions. Note that this is not the same as accepting BEHAVIOR as a condition on representation. The perspective I am concerned with in this subsection is one which takes flexible behavior alone as diagnostic of representation."}, {"title": "Maha's map", "content": "To illustrate the perspective with an analogy, imagine Maha is driving home from work along a memorized route when her partner calls and asks her to pick up the kids from school on the way. Suppose that Maha has never picked up the kids from her current location. To succeed at this task, Maha would appear to need some sort of map (a spatial representation) of the neighbourhood, whether on her phone or in her head, in order to change course and reach her new destination. If she could only follow routes which she remembers having previously taken, she would be stuck. However, I argue that the presence or absence of representations is underdetermined by Maha's mere behavior, and analogously, by the behavior of LLMs. There is no such thing as a diagnostic behavior.\nSuppose Maha arrives home with the kids, and this is all we know about her trip. She could have used a map in the manner just described. However, she could also have tried every possible route from her original location until she reached the school, thereby solving the navigation problem by brute-force. Or, should could have followed some sort of ecologically valid heuristic, such as \"take every second right turn\", which she knew would lead her to the school from any starting point. Acting according to a rule like \"try every possible path\" or \"take every second right turn\" is not acting via a representation, at least not in the sense of section 2: these rules are examples of \u201cdirective representations\u201d (Millikan, 1995; Shea, 2018, ch. 7), but they do not carry information and so are not representations in the relevant respect. This is so even though the proposition that such rules are valid does carry information \u2013 but such a proposition is not in discussion here. Even if it were, further steps would be required to show that the information is exploitable by Maha given her knowledge and other cognitive abilities, and moreover, actually used for navigation. Behavior alone will tell us neither.\nSupposing Maha fails to arrive home with the kids, this does not entail that she lacks a relevant representation. For instance, she could have a map, but an inaccurate one; even slight inaccuracies can have significant implications for downstream performance. Or, despite having a perfectly accurate map, she might still fail due to environmental conditions, for instance because construction has blocked all possible paths or because she is distracted. As emphasized above, behavioral success is not always explained by the use of an accurate representation, nor is failure always explained by its absence."}, {"title": "Performance, competence, and heuristics", "content": "In these scenarios, Maha's navigational performance is underdetermined by her possession of a map (she can do well or poorly with or without one), and vice-versa. This is an instance of the more general principle of performance-competence underdetermination (Chomsky, 1965), where performance is outward behavior, and competence is \u201ca system's underlying knowledge: the internal rules and states that ultimately explain a given capacity\u201d (Firestone 2020, 26564), and where these rules and states are (or are often) representational. As Harding & Sharadin (2024) observe, \u201c[t]his distinction is frequently elided in evaluations of ML model capabilities\u201d (11): good performance (for instance, on benchmarking tasks; Srivastava et al., 2023) is often taken to be sufficient for competence, and bad performance for a lack thereof. However, a system may perform well at a task despite lacking deeper competence (for instance, by relying on heuristics or other tricks, such as Maha's \u201cright turn\u201d rule), or it may perform poorly despite having such competence (for instance, because it is constrained in some extrinsic way, eg. by construction in Maha's case).\nOne complication is whether heuristics themselves can play a role in representational explanations. For example, McCoy et al. (2019) showed that the language model BERT relies on several heuristics when performing natural language inference (NLI), including the \u201clexical overlap heuristic\", according to which a premise entails all hypotheses constructed from words in the premise. Presuming that the model had internal states which carried information about lexical overlap in the training set and that this information played a mechanistic role in the model's robust NLI performance, did the model not have a representation with content like: \u201cpremises entail all hypotheses constructed from words in the premise", "robustness\u201d. Heuristics may be valid in a narrow or a wide set of environments, where narrow validity entails low robustness and wide validity entails high robustness. If Maha were following the right-turn rule, but one of the relevant turns was blocked, or if she were in a different city altogether, her performance would seriously degrade and thereby demonstrate a lack of robustness to different initial conditions and perturbations. Likewise, if BERT were following the lexical overlap heuristic when tested on a dataset in which the heuristic is invalid, it would fail the inference task (as also demonstrated by McCoy et al.). In this sense, heuristics do not explain modally robust task performance, and so invoking them fails to satisfy BEHAVIOR.\nBut this is too quick": "there are also many environments in which heuristic-guided behavior succeeds, for instance when Maha begins her route from different starting points in the same city with no construction, or when BERT is tested on other common NLI datasets in which the lexical overlap heuristic remains valid. Evaluations of robustness are thus task- and environment-specific"}]}