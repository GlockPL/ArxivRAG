{"title": "MAKING SENSE OF AI LIMITATIONS: HOW INDIVIDUAL\nPERCEPTIONS SHAPE ORGANIZATIONAL READINESS FOR AI\nADOPTION", "authors": ["Thomas \u00dcbellacker"], "abstract": "This study investigates how individuals' perceptions of artificial intelli-\ngence (AI) limitations influence organizational readiness for AI adoption.\nThrough semi-structured interviews with seven AI implementation experts,\nanalyzed using the Gioia methodology, the research reveals that organizational\nreadiness emerges through dynamic interactions between individual sense-\nmaking, social learning, and formal integration processes. The findings\ndemonstrate that hands-on experience with AI limitations leads to more real-\nistic expectations and increased trust, mainly when supported by peer net-\nworks and champion systems. Organizations that successfully translate these\nindividual and collective insights into formal governance structures achieve\nmore sustainable AI adoption. The study advances theory by showing how\norganizational readiness for AI adoption evolves through continuous cy-\ncles of individual understanding, social learning, and organizational adap-\ntation. These insights suggest that organizations should approach AI adop-\ntion not as a one-time implementation but as an ongoing strategic learning\nprocess that balances innovation with practical constraints. The research\ncontributes to organizational readiness theory and practice by illuminating\nhow micro-level perceptions and experiences shape macro-level adoption\noutcomes.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence (AI) has emerged as a new technology platform, changing how organizations operate and\ncompete. The rapid advancement of AI capabilities, particularly in areas such as Large Language Models (LLMs)\nand multimodal systems, has created new opportunities\nfor organizations to automate complex tasks, enhance\ndecision-making processes, and drive innovation (Bommasani et al., 2022; Bubeck et al., 2023; Li et al., 2023).\nOrganizations across industries are increasingly seeing\nAI adoption as a strategic imperative, with 55% of organizations reporting the use of AI in at least one business\nunit or function as of 2023, up from 50% in 2022 (Maslej\net al., 2024) and with global AI spending projected to\nreach $631 billion by 2028 (Massey, 2024). However, or-\nganizations struggle to implement AI despite investments\nand evident strategic importance. They encounter chal-\nlenges beyond technical considerations, encompassing\nsocial and organizational dynamics. AI adoption initia-\ntives often fail to deliver their intended benefits, with\nreasons such as unrealistic expectations, lack of change\nmanagement, organizational constraints, organizational\nreadiness, and failure to understand users' needs iden-\ntified as barriers to successful adoption (Cooper, 2024;\nWestenberger et al., 2022).\nUnderstanding organizational readiness for technological change has become increasingly critical as organiza-\ntions face the challenges of AI adoption. While tradi-\ntional models of organizational readiness have focused\non structural and technical preparedness (e.g., Weiner\n(2009)), the unique characteristics of AI technologies\ntheir complexity, opacity, and transformative potential\ndemand a more nuanced understanding of how organiza-\ntions become ready for AI adoption. The role of individ-\nual perceptions in forming this readiness is noteworthy,\nas individuals' understanding and interpretation of AI's\nlimitations can influence an organization's capacity to\nimplement AI systems successfully. These perceptions\nare not formed in isolation but are shaped through com-\nplex social and organizational processes as individuals\nattempt to make sense of AI's capabilities, limitations,\nand implications for their work."}, {"title": "2 Literature Review", "content": "The relationship between individual perceptions of AI\nlimitations and organizational readiness is a multi-level\nphenomenon that cannot be fully understood through\ntraditional technology adoption frameworks alone. An in-\ntegrated theoretical approach incorporating sensemaking\nprocesses trust development mechanisms and organiza-\ntional readiness dynamics is needed.\nThis literature review approaches AI adoption and readi-\nness as a multi-level phenomenon. It addresses external\npressures (such as competitive forces, policy frameworks,\nand societal discourse), organizational-level readiness\n(in terms of capabilities, culture, and infrastructure), and\nfinally, individual-level factors (such as perceptions of AI\nlimitations and sensemaking processes). The interplay\nacross these levels highlights how organizations success-\nfully adopt AI."}, {"title": "2.1 AI Adoption in Organizations", "content": "AI adoption has several unique characteristics that differ-\nentiate it from traditional technology adoption. Weber\net al. (2023) identify two characteristics: inscrutability\nand data dependency. Inscrutability manifests in the dif-\nficulty of predicting system behavior and explaining de-\ncision processes, while data dependency requires contin-\nuous system adjustments as organizational data evolves.\nThese characteristics create increased variability in or-\nganizational decision-making processes that require new\ncoordination mechanisms (Agrawal et al., 2024). The\ninscrutability concept is particularly interesting for un-\nderstanding individual interactions with AI systems, as it\ndirectly influences how organizational members interpret\nand respond to AI-driven changes.\nAgrawal et al. (2024) argue that AI adoption increases\ndecision variation across interconnected organizational\ntasks, asking organizations to either reduce task inter-\ndependencies or implement strong coordination mech-\nanisms. This finding challenges the typical focus on\nindividual task-level AI adoption by highlighting the sys-\ntemic nature of organizational AI adoption. Yang et al.\n(2024) extend this understanding by showing how AI\nadoption introduces both technological affordances and\nconstraints that vary significantly based on organizational\nsize. Their study reveals that while larger firms perceive\nprimarily operational affordances focused on efficiency\nand quality improvements, smaller firms see marketing\naffordances as more important, leading to different adop-\ntion patterns and outcomes.\nWith his Technology-Organization-Environment (TOE)\nframework, Baker (2012) emphasizes that innovation\nadoption depends on the interplay between technological\nfeatures, organizational characteristics, and environmen-"}, {"title": "2.2 Organizational Readiness for AI Adoption", "content": "Organizational readiness is not only about having the\nright resources or leadership in place; it also mediates\nbetween broad external pressures (e.g., policy mandates\nand competitive landscapes) and how employees on the\nground perceive and engage with AI. Organizational\nreadiness serves as a bridge between broader external\nforces (such as policy requirements and market competi-\ntion) and how individual employees actually engage with\nand implement AI in their daily work. It determines how\nwell an organization can translate high-level strategic de-\nmands into successful adoption by its workforce. J\u00f6hnk\net al. (2021) emphasize that readiness involves aligning\norganizational assets, individual capabilities, and leader-\nship commitment to support AI initiatives. They iden-\ntify five core domains \u2013 strategic alignment, resources,\nknowledge, culture, and data - that collectively determine\nreadiness.\nAI readiness demands more than just technical infras-\ntructure. Heimberger et al. (2024) highlight that success\ndepends on how well organizational processes can in-\ntegrate AI. That includes adapting workflows, ensuring\ndata compatibility, and developing continuous learning\nand refinement systems. Readiness is, therefore, an evolv-\ning state influenced by the organization's ability to adjust\nand respond to AI's changing demands a classical orga-\nnizational learning problem.\nOrganizations need to develop specific capabilities for\nsuccessful AI adoption. Weber et al. (2023) identified\nfour concrete organizational capabilities: AI project plan-\nning, co-development of AI systems, data management,\nand AI model lifecycle management. This is a more\nprocess-oriented approach to readiness than the readiness\nfactors J\u00f6hnk et al. (2021) synthesized.\nLeadership is vital for AI readiness. Felemban et al.\n(2024) argue that senior management support signifi-\ncantly affects individual attitudes and readiness to adopt\nAI. Leaders are important in allocating resources, prior-\nitizing AI in strategic plans, and addressing resistance\nfrom change recipients (Mikel-Hong et al., 2024).\nTrust in AI systems is another determinant for organi-\nzational readiness, influencing adoption and sustained"}, {"title": "2.3 Individual Perceptions of Limitations", "content": "The successful adoption of AI technologies within organi-\nzations is not solely determined by technical capabilities\nbut is significantly influenced by individual perceptions\nof AI limitations (Glikson and Woolley, 2020; Kelley,\n2022). These perceptions can act as barriers or facilitators\nto adoption, affecting organizational readiness and the\noverall implementation process (Trenerry et al., 2021).\nIndividuals' perceptions of AI limitations encompass a\nrange of concerns that can hinder the adoption of AI tech-\nnologies within organizations. One area of concern is\nthe issue of trust and reliability. Nasarian et al. (2024)\nand Xiangwei et al. (2022) highlight that individuals find\nbuilding trust in AI systems difficult due to inconsistent\nor opaque outputs. This lack of trust is further impeded\nwhen AI systems fail to perform reliably in critical ap-\nplications, leading to skepticism about their usefulness,\nas Singh et al. (2023) noted. Additionally, Choudhary\net al. (2024) observe that fear and resistance to adoption\ncan come from a misalignment between AI technologies\nand individuals' values and anxiety over dealing with\ncomplex IT systems.\nTransparency and explainability of AI systems are also\nsignificant concerns among individuals. The \"black box\"\nnature of AI algorithms, particularly in complex models\nlike large language models (LLMs), poses challenges\nfor those who require clear and interpretable decision-\nmaking processes. Novak et al. (2022) and Haxvig (2024)\ndiscuss how the lack of transparency can hinder individu-\nals' understanding and acceptance of AI outputs. Lai et al.\n(2023) and Morais et al. (2023) further point out that AI\nsystems often cannot provide meaningful, user-aligned\nexplanations, which can decrease trust and confidence\namong users.\nFurthermore, concerns about human-AI interaction play\na role in shaping individuals' perceptions. Qian and\nWexler (2024) and Bu\u00e7inca et al. (2021) note that indi-\nviduals may be cautious of over-reliance on AI and the\npotential for automation complacency, leading to skill\ndegradation or reduced caution in their roles. Addition-\nally, cognitive and self-serving biases can influence how\nindividuals interpret AI capabilities. von Schenk et al.\n(2023) demonstrate that when people lack information\nabout how Al systems operate - specifically about what\nhappens to machines' earnings in economic interactions\nthey tend to form self-serving beliefs that justify less\ncooperative behavior with the machines. The lack of\nemotional intelligence in AI systems, especially in con-\ntexts requiring empathy and nuanced human interaction,\nis another limitation that Singh et al. (2023) cited.\nBias and fairness issues embedded in AI systems are sig-\nnificant concerns that affect individuals' willingness to\nadopt these technologies. Muller et al. (2022) discuss\nhow biases in training data can affect AI outputs, leading\nto unfair or discriminatory outcomes. Allan et al. (2024)\nand Zhou et al. (2023) emphasize that amplifying soci-"}, {"title": "2.4 Sensemaking", "content": "Sensemaking theory provides a valuable framework for\nunderstanding how individuals, teams, and organizations\ninterpret and respond to AI. This approach is inherently\nmulti-level, encompassing the personal sensemaking of\nemployees, the collective sensemaking of groups or de-\npartments, and organizational sensemaking processes"}, {"title": "2.5 Sensitizing Concepts", "content": "The literature suggests several interconnected sensitizing\nconcepts that operate across multiple levels - from indi-\nvidual cognition to organizational processes to external\ninfluences. These concepts guide the empirical investiga-\ntion and anticipate the dynamic relationships that emerge\nin the findings. The concepts are organized to reflect\nhow perceptions of AI limitations flow from individual\ninterpretation through collective sensemaking to organi-\nzational adaptation. These concepts also guide the empir-\nical inquiry into how organizations navigate AI adoption,\nfrom external demands and industry-wide influences to"}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Research Design", "content": "The study employs a qualitative research design grounded\nin the principles of grounded theory development.\nGrounded theory allows for developing a theoretical\nframework that emerges directly from empirical data\n(Charmaz, 2012), making it suitable for investigating\nhow perceptions of AI limitations influence organiza-\ntional readiness."}, {"title": "3.2 Epistemological Considerations", "content": "This study is grounded in a constructivist epistemology,\nwhich posits that reality is socially constructed through\nindividual and collective interpretations and interactions\n(Young and Collin, 2004). Constructivism is appropriate\nfor this research as it emphasizes understanding the sub-\njective meanings that individuals assign to their experi-\nences with AI technologies and how these meanings influ-\nence organizational readiness for adoption. By adopting\na constructivist lens, the research seeks to co-construct\nknowledge with participants through in-depth interviews,\nallowing for a rich exploration of how perceptions of AI\nlimitations emerge and impact organizational readiness.\nThis approach is consistent with qualitative methodolo-\ngies such as grounded theory and the Gioia method, pri-\noritizing participants' perspectives and the meanings they\nascribe to phenomena Gioia et al. (2013). The construc-\ntivist epistemology supports grounded theory in allowing\ntheories to emerge from the data rather than imposing\npreconceived hypotheses (Mills et al., 2006). This is\nparticularly relevant for exploring new and complex phe-\nnomena like AI adoption in organizations, where existing"}, {"title": "3.3 Data Collection", "content": "To comprehensively address the research question, this\nstudy employed a qualitative data collection method\nusing semi-structured expert interviews (see interview\nguide in Appendix 12.1). This methodological choice\naligns with the exploratory nature of the research ques-\ntion and its focus on understanding complex organiza-\ntional phenomena (Eisenhardt, 1989). Semi-structured in-\nterviews are particularly suited for capturing rich insights\nabout technology adoption processes while maintaining\nsystematic data collection (Gioia et al., 2013).\nThe participant selection uses a purposive convenience\nsampling strategy, which proves valuable for accessing\nexperts with rich insights on the studied topic (Etikan,\n2016). Participants were recruited through two primary\nchannels: personal professional networks (n=3) and the\nAI Impact Mission community, an active online commu-\nnity of approximately 330 LLM enthusiasts (n=4). This\ndual-channel approach helped ensure access to partici-\npants with deep expertise in AI implementation while\nmaintaining the diversity of perspectives. Informants\nwere selected based on the following criteria: (1) cur-\nrent or recent (within the last two years) involvement\nin strategic or technical leadership roles; (2) experience\nwith multiple AI adoption projects across organizational\ncontexts; and (3) understanding of both technical and\norganizational aspects of AI implementation. Selected\nparticipants had strategic or technical roles such as \"AI\nConsultant,\" \"Head of AI,\" and \"AI Systems Engineer.\"\nThe sampling strategy prioritized information richness\nover representativeness, focusing on participants who\ncould provide deep insights into AI adoption processes\nbased on their direct involvement in implementation\nprojects across various organizational contexts. This\napproach aligns with qualitative research best practices,\nemphasizing depth and quality of insights rather than\nstatistical generalizability (Patton, 2002).\nThe final sample consisted of seven participants, though\nadditional potential participants had expressed interest\nin participating. The decision to conclude data collec-\ntion at seven interviews was guided by data saturation\n(Guest et al., 2006), which was systematically assessed\nthrough quantitative analysis of new code generation (see\nAppendix 12.2). The analysis revealed a clear pattern\nof diminishing returns in terms of new insights gener-\nated from each subsequent interview. The first interview\nyielded 105 unique codes, establishing the initial con-\nceptual framework. The second interview contributed 83\nnew codes, expanding the theoretical understanding sig-"}, {"title": "3.4 Data Analysis", "content": "Guided by a constructivist epistemology and employing\nthe Gioia method (Gioia et al., 2013) within a grounded\ntheory framework (Charmaz, 2012), the analysis aimed to\ndevelop an empirically grounded theoretical understand-\ning of how individual perceptions of AI limitations shape\norganizational readiness for AI adoption. The data analy-\nsis involved iterative coding, constant comparison, and\nprogressive abstraction from initial participant statements\nto higher-level theoretical constructs. This structured ap-\nproach ensured that emerging insights remained closely\ntied to the data while allowing for generating a novel\ntheoretical framework.\nThe Gioia methodology provides a systematic, inductive\nprocess for qualitative data analysis that integrates partici-\npants' views with more abstract theoretical concepts. The\nanalysis progressed through three main stages: (1) First-Order (Open) Coding, (2) Second-Order (Axial) Coding,\nand (3) Aggregate Dimensions (Gioia et al., 2013).\nThis process was iterative and reflective. Throughout the\nanalysis, I constantly compared new codes and themes\nagainst previously coded data, refining concepts and en-\nsuring consistency. Large language models assisted in the\ndata analysis process in a human-supervised way through\ntheir text understanding affordance."}, {"title": "3.4.1 First-Order Coding", "content": "All seven interviews were transcribed and reviewed in\ntheir entirety. The initial coding began by closely reading\neach transcript line-by-line and assigning codes that cap-"}, {"title": "3.4.2 Second-Order Themes", "content": "In the second analysis stage, the initial codes were ex-\namined for similarities, differences, and conceptual re-\nlationships. This step involved moving from the raw"}, {"title": "3.4.3 Aggregate Dimensions", "content": "In the final step, the second-order themes were clustered\ninto higher-order, aggregate dimensions that captured the\nholistic patterns and processes emerging from the data.\nThe goal was to develop a coherent theoretical frame-\nwork showing how individual interpretations of AI lim-\nitations collectively influence organizational readiness.\nThis integrative step led to the identification of five ag-\ngregate dimensions (see Appendix 12.5): (1) \"Individual\nSensemaking Foundations,\" (2) \"Social and Organiza-\ntional Learning Mechanisms,\" (3) \"Organizational Inte-\ngration and Governance,\" (4) \"Expectation Management\nand Trust Development\" and (5) \"Long-Term Adaptation\nand Value Realization.\""}, {"title": "4 Results", "content": "Figure 3 presents a data structure visualizing how first-\norder codes aggregate into second-order themes and fi-\nnally coalesce into the five aggregate dimensions. This\nstructured representation helps illustrate the \"funnel\" of\nabstraction, starting from the richness of participant ex-\nperience and ending with a theoretical framework that\nexplains how individual-level interpretations of AI limi-\ntations shape organizational readiness."}, {"title": "5 Discussion", "content": "The empirical findings reveal complex interconnections\nbetween individual sensemaking processes and organi-\nzational AI readiness. Through analysis of the interview\ndata, several key propositions emerged that help explain\nhow organizations develop readiness for AI adoption.\nThese propositions outline the pathways through which\nindividual perceptions of AI limitations influence organi-\nzational readiness, mediated by processes of trust devel-\nopment, social learning, and organizational integration."}, {"title": "5.1 AI Limitations and Individual Sensemaking", "content": "The analysis reveals how encounters with AI limitations\ntrigger specific sensemaking processes that shape individ-\nual understanding and organizational readiness. This rela-\ntionship manifests distinctly from the trust development\nand social learning mechanisms discussed earlier, focus-\ning instead on the cognitive processes through which\nindividuals interpret and internalize AI limitations. The\ndata reveals how sensemaking evolves from reactive to\nproactive as individuals gain experience. Rather than\njust responding to interruptions, more experienced users\nactively extract and interpret cues about AI boundaries.\nAs one participant explained, \"If you have an idea of\nthe limitation, you can [...] inform them about what you\ncan do and what you can't do\" (i_238), indicating a shift\nfrom passive discovery to active extraction of cues about\nlimitations. This transition resonates with Weick et al.\n(2005) observation that sensemaking often emerges from\nnoticing and bracketing of ambiguous cues. In this con-\ntext, AI limitations act as those cues; by recognizing and\nlabelling them, individuals transform disruptions into\nactionable insights that reshape how they engage with\n\u0391\u0399.\nThis evolution aligns with Weick et al. (2005) descrip-\ntion of sensemaking as an ongoing, retrospective process\nwhile extending it to show how limitation awareness en-\nables more strategic technology engagement. The data\nsuggests that individuals become better equipped to iden-\ntify appropriate use cases as their interpretive frameworks\nbecome more sophisticated. One participant emphasized"}, {"title": "5.2 Individual Perceptions and Trust Development", "content": "The data reveals a relationship between how individu-\nals make sense of AI limitations and their development\nof trust in AI systems. This relationship presents itself\nthrough two interrelated processes: the gradual devel-\nopment of trust through direct experience with AI limi-\ntations and the subsequent deepening of understanding\nthrough increased experimentation that this trust enables.\nThe findings show that individuals initially approach\nAI with varying skepticism and uncertainty, often in-\nfluenced by media narratives and superficial coverage\n(c_19). However, direct engagement with AI tools,\nmainly through experimentation and small-scale trials,\nreshapes these perceptions. One participant emphasized\nthat \"direct experience is crucial\" (i_38), highlighting\nhow practical interaction helps individuals develop a bet-"}, {"title": "5.3 Individual Sensemaking and Social Learning", "content": "The data reveals how individual sensemaking processes\ninteract dynamically with social learning mechanisms\nto shape organizational readiness for AI adoption. This\nrelationship manifests through two key propositions high-\nlighting how personal insights catalyze collective learn-\ning, reshaping individual understanding.\nThe data shows that as individuals develop more precise\ninsights into AI through direct experience, they naturally\nshare these discoveries with colleagues. Multiple partic-\nipants described how employees who gained hands-on\nexperience with AI became eager to share what they had\nlearned. For instance, one participant noted that \"col-\nleague will start to see you that you are using chat [...]\nthen you start to feel that I need that too\" (i_29), highlight-\ning how individual discoveries spark interest in others.\nThis sharing occurred through various channels - infor-\nmal conversations, champion networks, and innovation\nlabs (i_151, i_22, i_229) - creating multiple pathways\nfor knowledge dissemination. Weick et al. (2005) re-\nmind that sensemaking is inherently social; it builds on\ncommunication that talks events into existence and pro-\nmotes shared understanding. In this study, employees'\npeer-to-peer exchanges mirror that dynamic: individual\ndiscoveries become group insights through the ongoing\nconstruction and negotiation of meaning within these\nnetworks.\nThis pattern was particularly evident in how organiza-\ntions leveraged \"black belts\"/ champions (i_146, i_24) or\ndepartment representatives who \"volunteer to be represen-\ntative of using AI tools and to share knowledge\" (i_147)."}, {"title": "5.4 Social Learning and Integration", "content": "Building on the previous findings about trust develop-\nment and social learning, the data reveals distinct patterns\nin how formalized structures and social learning mecha-\nnisms interact to shape organizational AI readiness. This\nrelationship emerges through two key processes: how ma-\nture integration frameworks enable systematic knowledge\nsharing and how collective learning drives organizational\nadaptation.\nThe data shows successful organizations develop spe-\ncific structural mechanisms to facilitate knowledge ex-\nchange. Participants described the creation of \"volun-\ntary venture labs\" (i_151) and regular sharing sessions\nwhere employees would \"tell us how are you using it\nand send us a picture\" (i_152). These structured oppor-\ntunities moved beyond informal conversations to create\ndedicated spaces for knowledge exchange. From We-\nick et al. (2005) viewpoint, these formalized routines\nexemplify how organizations can actively shape the en-\nvironment in which sensemaking unfolds. By creating\nexplicit forums\u2014such as \u201cvoluntary venture labs\" or\ncross-functional teams organizations enact structures\nthat channel how cues are noticed, interpreted, and re-\ntained over time. One participant emphasized how orga-\nnizations established \"domain-specific AI communities\"\n(i_65) focused on particular business functions like \"AI\nfor tax, AI for others, AI for assurance, AI for knowledge\nmanagement\" (i_65), indicating how formal structures\nenabled targeted knowledge sharing.\nMultiple participants highlighted how these formal mech-\nanisms helped bridge departmental boundaries. One\nnoted that \"capabilities and capacity spread across de-\npartments that somehow have to communicate\" (i_64),\nwhile another described \"cross-departmental knowledge\nsharing emerges\" as a key outcome of structured inte-\ngration efforts. The importance of formal support was\nparticularly evident in observations about \"creating pre-\nsentation opportunities\" (i_64) and \"encouraging external\nknowledge sharing\" (i_64). This alignment of structure\nand learning leads to the next proposition:"}, {"title": "5.5 Trust and Integration Effects on Organizational\nReadiness", "content": "The data reveals two distinct but interlinked pathways\nthrough which organizations develop readiness for AI\nadoption. First, trust development emerges from indi-\nvidual sensemaking processes (Section 5.1, 5.2) and cat-\nalyzes broader AI acceptance. Second, organizational\nintegration mechanisms - spawned by social learning\n(Section 5.3, 5.4) - shape the structural and procedural\ncapabilities that sustain AI adoption. Together, these two"}, {"title": "5.6 Long-term Adaptation", "content": "Finally, a temporal dimension of AI adoption emerged in\nthe data, revealing how organizations' approach to AI im-\nplementation and value realization evolves over extended\nperiods. This longitudinal perspective provides insights\ninto how initial experiences shape long-term adaptation\nstrategies and eventually influence organizational readi-\nness for A\u0399.\nThe data shows that organizations' relationship with\nAI technologies undergoes maturation phases. Initially,\nas one participant described, organizations often move\n\"from overconfidence to disappointment\" (i_16) before\ndeveloping more nuanced approaches. This evolution\nwas not linear but involved iterative cycles of learning\nand adaptation. Another participant emphasized how\ncontinuous experimentation led to deeper understanding:\n\"Once you start using it [...] you're like, wow, it's actually\nway better\" (i_220), highlighting how direct experience\nshapes organizational approaches over time.\nThe findings reveal that successful organizations demon-\nstrated an ability to learn from both positive and negative\nexperiences, using these insights to refine their imple-\nmentation strategies. One participant noted that \"even if\nthe model becomes worse than the expectations, it still in-\ncreases trust\" (i_265), suggesting that even unsuccessful\nimplementations contributed to organizational learning.\nThis pattern was particularly evident in how organiza-\ntions adjusted their resource allocation and structural\narrangements over time. For instance, one participant de-\nscribed how their organization \"sourced the three people\nthat were the most affiliated with AI developments into a\nspecial task force\" (i_177) as a response to accumulated\nimplementation experience.\nThis approach was not merely reactive but became in-\ncreasingly strategic as organizations gained experience.\nOne participant emphasized how \"understanding how\nyour company works [...] day-to-day operations [...] try\nto improve those\" (i_288) became central to their ap-\nproach over time. The data shows that organizations that\nsuccessfully sustained AI adoption developed systematic\napproaches to capturing and applying lessons learned.\nAnother participant noted that the \"continuous mixing\nof experts in [...] different projects\" (i_72) enabled on-\ngoing knowledge transfer and capability development.\nThese observations about organizational learning and\nadaptation over time lead to the first proposition:"}, {"title": "6 Practical Implications", "content": "The findings of this study indicate that organizational\nreadiness for AI adoption depends not merely on tech-\nnical infrastructure and leadership directives but also on\nhow employees form and diffuse their understanding of\nAI's limitations. A first managerial implication is that\norganizations should prioritize building foundational AI\nliteracy through hands-on experimentation. This aligns\nwith Henry et al. (2022) findings on the importance of\nhuman-machine teaming and experiential learning. Con-\nceptual overviews, though important, are insufficient for\nhelping individuals understand AI's actual boundaries,\nsuch as hallucination tendencies or token-length con-\nstraints. When employees are encouraged to test AI\ntools in sandbox environments or pilot projects, they\ndevelop more realistic expectations and cultivate a mea-\nsured confidence in the technology's potential, consistent\nwith Weber et al. (2023) emphasis on implementation\ncapabilities. These incremental \"wins\" help mitigate\nthe disillusionment that often arises when inflated ex-\npectations clash with technical realities. Organizations\ncan actively facilitate this sensemaking by creating for-\nmal and informal knowledge-sharing spaces, cultivating\nchampion networks, and encouraging cross-functional\nexchange, as supported by Kelley (2022) identification of\nsuccess factors. However, the data emphasizes that these\nsocial learning mechanisms work best when they emerge\norganically from genuine individual insights rather than\ntop-down initiatives.\nA second implication underscores the significance of\ncultivating trust gradually through tangible success sto-\nries and demonstrable improvements in workplace tasks,\naligning with Glikson and Woolley (2020) findings on\ntrust development in AI systems. Even if AI systems"}, {"title": "7 Limitations and Future Directions", "content": "While this research draws strength from its qualitative,\nexpert-interview approach and provides in-depth perspec-\ntives on individual sensemaking, three main limitations\nwarrant attention and suggest avenues for future inquiry.\nFirst, the study focuses on individual-level interpreta-\ntions, offering limited insight into how these percep-\ntions coalesce into collective readiness across organi-\nzational tiers (Crossan et al., 1999). Given the intermedi-\nate state of theory connecting individual perceptions to\norganizational readiness, future research could employ\nhybrid methods combining qualitative and quantitative\napproaches (Edmondson and McManus, 2007) through\nlongitudinal or multi-level case studies. Such nested de-"}, {"title": "8 Conclusion", "content": "This work examines how individual perceptions of AI\nlimitations influence organizational readiness for AI\nadoption. The findings reveal a dynamic interplay be-\ntween individual sensemaking processes, social learning\nmechanisms, and formal organizational structures. When\nemployees encounter AI limitations through hands-on\nexperience, they develop more realistic expectations and\ngreater trust in the technology, mainly when supported"}, {"title": "9 Appendix", "content": null}, {"title": "Appendix 1: Interview Guide", "content": null}, {"title": "Introduction", "content": "Welcome and Introduction\nPurpose of the Study\nThe purpose of this study is to gain insights from experts like you on the factors that impact AI adoption\nin organizations, specifically focusing on perceptions of AI limitations.\nConfidentiality\nConsent to recording"}, {"title": "Context", "content": "Could you briefly describe your experience with AI implementation projects in organizations?"}, {"title": "Theme 1: Perception Formation of AI Limitation", "content": "Based on your observations, how do individuals in organizations typically develop their understanding of AI\nlimitations?\nProbe: Role of professional background (technical vs non-technical?)\nProbe: Impact of direct experiences vs. indirect knowledge\nProbe: Through formal training, peer discussions, hands-on experience, or other means?\nProbe: Industry context influence\nProbe: External discourse influence (E.g. media coverage? Failed projects? Successful projects in other\norganizations?)"}, {"title": "Theme 2: Individual and Collective Sensemaking", "content": "How have you seen individuals interpret and make sense of their experiences with AI limitations?\nProbe: Trust-building\nProbe: Contradictions between their expectations and actual AI performance\nProbe: Role of professional identity\nProbe: Impact of past experiences\nProbe: Experimentation\nProbe: Process of updating interpretations (What triggers change over time?)\nIn your experience, how do collective interpretations of AI limitations develop within organizations?\nProbe: Knowledge sharing mechanisms (How do organizational stories or narratives about Al success-\nes/failures spread?)\nProbe: Informal networks' role\nProbe: Leadership influences\nProbe: Resolution of conflicting perspectives"}, {"title": "Theme 3: Impact on Organizational Readiness", "content": "How do individual understandings of AI limitations shape an organization's readiness for adoption?\nProbe: Change management\nProbe: Communication\nProbe: Change in culture\nProbe: Risk assessment\nProbe Cross-functional coordination\nProbe: Changes in strategic planning"}]}