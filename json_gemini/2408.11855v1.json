{"title": "FactorLLM: Factorizing Knowledge via Mixture of Experts for Large Language Models", "authors": ["Zhongyu Zhao", "Menghang Dong", "Rongyu Zhang", "Wenzhao Zheng", "Yunpeng Zhang", "Huanrui Yang", "Dalong Du", "Kurt Keutzer", "Shanghang Zhang"], "abstract": "Recent research has demonstrated that Feed-Forward Networks (FFNs) in Large Language Models (LLMs) play a pivotal role in storing diverse linguistic and factual knowledge. Conventional methods frequently face challenges due to knowledge confusion stemming from their monolithic and redundant architectures, which calls for more efficient solutions with minimal computational overhead, particularly for LLMs. In this paper, we explore the FFN computation paradigm in LLMs and introduce FactorLLM, a novel approach that decomposes well-trained dense FFNs into sparse sub-networks without requiring any further modifications, while maintaining the same level of performance. Furthermore, we embed a router from the Mixture-of-Experts (MoE), combined with our devised Prior-Approximate (PA) loss term that facilitates the dynamic activation of experts and knowledge adaptation, thereby accelerating computational processes and enhancing performance using minimal training data and fine-tuning steps. FactorLLM thus enables efficient knowledge factorization and activates select groups of experts specifically tailored to designated tasks, emulating the interactive functional segmentation of the human brain. Extensive experiments across various benchmarks demonstrate the effectiveness of our proposed FactorLLM which achieves comparable performance to the source model securing up to 85% model performance while obtaining over a 30% increase in inference speed. Code: https://github.com/zhenwuweihe/FactorLLM.", "sections": [{"title": "1 Introduction", "content": "Large language models[90, 50] (LLMs) exhibit exceptional capabilities in knowledge recall, attributable to both their extensive training on expansive text corpora[55, 22, 38, 65, 78] and their advanced cascade transformer architectures. Central to these architectures are the feed-forward layers within the transformer blocks. These layers constitute a significant fraction of the model's parameters and play a crucial role in storing and processing vast quantities of information[70, 71, 53, 54, 7]. However, the substantial size and complexity of transformers primarily stem from their monolithic Feed-Forward Networks[23, 13], which leads to oversized knowledge storage for specific tasks and significant consumption of time and computational resources[1, 7, 3]. These inefficiencies present substantial challenges in efficiently deploying large language models, particularly in computational-constraint task-specific scenarios. Redundant parameters often result in ineffective computations and increase the likelihood of an \"illusion\" caused by knowledge that is irrelevant to certain tasks.\nSubstantial studies [69, 40, 51, 44, 73] have targeted improvements in the efficiency and adaptability of LLMs. Small Language Models [60, 6, 52, 48, 82] (SLMs) aim to reduce the demand for computational resources through compact architectures. However, they are historically and empirically constrained by the Scaling Law [33], which leads to significant model degradation. On the other"}, {"title": "2 Related Work", "content": "2.1 Efficient Large Language Model\nLarge language models are frequently criticized for their substantial resource and time demands[59, 9, 14, 58] during both training and inference. To address this challenge, various techniques[69, 40, 51, 44, 73] have been proposed to enhance inference efficiency in large transformer models. Model compression[94] is one approach to decrease computational requirements include techniques like pruning[20, 68, 2, 88, 87] and quantization[21, 61, 41, 42]. Researchers have also developed several resource-efficient and computation-efficient architectures, such as efficient attention mechanisms[92, 91], mixture of experts [37, 17, 85], long-context models[16, 56, 79], and state space models[24]. Additionally, numerous strategies [73] have been identified to improve efficiency throughout the training, fine-tuning, and inference stages. Our objective is to accelerate large language models by modifying their architectures to factorize specific knowledge within the network.\n2.2 Knowledge Decomposition\nLarge language models encapsulate extensive knowledge across diverse domains and tasks[26], acquired from vast amounts of training data. To efficiently leverage this knowledge and mitigate architectural redundancy, various methods have been developed to decompose large models and extract intrinsic knowledge. Model editing[93, 64, 8, 46, 47] aims to change the knowledge or brief inside large language models. The Locating-and-Editing[46] method views the FFN as a key-value memory[23] and proposes an interpretable approach to trace the effects of weights within the model on the output of input prompts which enables the identification and modification of specific neurons to edit the model's behavior effectively. Alternatively, low-rank matrix decomposition[31, 80, 74] directly modify model architectures including embedding layer[77] and feed-forward network[39] to reallocate knowledge across different modules. Knowledge distillation[28, 30, 63, 35] is another approach, focusing on transferring knowledge from large models to smaller counterparts. Notably, a novel distillation task termed knowledge factorization[76] has been proposed to extract both task-agnostic and domain-specific knowledge from neural networks. In our work, we introduce the mixture of experts technique with per-layer distillation training strategy to facilitate effective knowledge factorization.\n2.3 Mixture of Experts\nMixture of Experts (MoE) [4, 62, 37, 19, 17] is instrumental in integrating diverse domain knowledge across different modules to achieve effective knowledge fusion[72, 83]. One method to construct experts involves cloning components of the original transformer block, including attention heads [86], feed-forward networks [36], and even bypassing low-rank adapters[45, 43], which has proven to be an effective approach for scaling large transformer-based models and expanding their capacity. Moreover, [85] situates the traditional MLP in MoE block with the linear-wise feature modulation to further enhance the model efficiency. Alternatively, a different strategy[89] involves decomposing layers into distinct modules according to K-Means clustering. Our proposed FactorLLM extends this"}, {"title": "3 Proposed Approach", "content": "In this section, we elucidate the rationale behind decomposing the FFN in a fully pretrained LLM into various subnetworks without performance loss and present the comprehensive framework of our proposed FactorLLM via MoE. We initially define key concepts and preliminaries concerning LLM and MoE in Sec. 3.1. Subsequently, we discuss the factorization of an FFN into multiple subnetworks in Sec. 3.2. Finally, we elaborate on FactorLLM with our dynamic routing strategy and the overall training objectives in Sec. 3.3.\n3.1 Preliminary\nFeed-Forward Network (FFN). For a given input embedding x \u2208 R^{de} and denoting the hidden dimension by d_h, the FFN, which are typically implemented as two-layer Multi-Layer Perceptrons (MLP), can be formulated as follows:\n\\begin{equation}\n\\begin{aligned}\nh &= xW_1 + b_1 \\\\\nF(x) &= \\sigma(h)W_2 + b_2\n\\end{aligned}\n\\end{equation}\nwhere F(\\cdot) stands for the fully connected feed-forward network, h is the hidden representation inside MLP and \\sigma(\\cdot) is a non-linear activation function (e.g., SiLU[18]). W_1 \\in R^{de \\times d_h} and W_2 \\in R^{d_h \\times de} are weight matrices while b_1 \\in R^{d_h} and b_2 \\in R^{de} are bias vectors.\nMixture of Experts (MoE). The Mixture of Experts model is comprised of a set of i \\in N expert functions E_i(), and a trainable TopK router R(\\cdot). The router is designed to distribute input embeddings among the experts by generating a probability vector that dictates the allocation. For a given input embedding x \\in R^{bxnxde}, the output of the MoE model is a composite of contributions from each expert. These contributions are weighted according to the probabilities assigned by the router and can be formally expressed as:\n\\begin{equation}\n\\begin{aligned}\ny &= \\sum_{i=1}^{N} E_i(x)R_i(x), R(x) = \\epsilon(xW_3 + b_3) \\\\\ns.t. R_i(x) \\geq 0 \\text{ and } \\sum_{i=1}^{N} R_i(x) = 1\n\\end{aligned}\n\\end{equation}\nwhere \\epsilon(:) signifies the softmax function, W_3 \\in R^{N\\times de} represents a matrix of trainable weights, and b_3 \\in R^{N} is the bias vector. However, MoE-based architectures often suffer performance degradation when too many inputs are routed to a few experts[19, 37]. To mitigate this imbalance, a load balance loss, denoted as L_{lb}, was introduced in [37] to penalize uneven input distribution among experts:\n\\begin{equation}\nL_{lb} = \\frac{NK}{N} \\sum_{n=1}^{K} \\sum_{i=1}^{N} v_i(x_n)R_i(x_n), \\text{ s.t. } K \\leq N\n\\end{equation}\nwhere x_n represents the nth input token. Here, v_i(x_n) equals 1 if the ith expert is selected for processing x_n by the TopK selection function, and 0 otherwise.\n3.2 Model Decomposition\nThe fundamental concept of model decomposition involves partitioning neurons in a fully pretrained model that frequently activates concurrently into distinct subnetworks for acceleration. These subnetworks are sparsely activated during the feedforward phase, thereby accelerating the model inference without performance loss. To maintain consistent forward processing speeds and reduce the \"bucket effects\" associated with differing expert sizes, we decompose the weight matrix into N subnetworks of uniform dimensions. Such partition eliminates delays caused by the slowest component in parallel operations, thereby improving the efficiency of parallel computations."}, {"title": "3.3 FactorLLM", "content": "Transforming into Mixtur-of-Experts. In Sec. 3.2, it is established that partitioning the FFN into N distinct subnetworks does not compromise the overall model efficacy. This finding allows us to exploit the resulting sparse architecture to expedite model computations by selectively activating a limited subset of subnetworks, denoted as S = \\{s_k\\}_{k=1}^{N}. Drawing parallels to the architecture of Mixture of Experts (MoE), we treat these subnetworks S as individual experts E:\n\\begin{equation}\nE(x) = \\sum_{i \\in S} \\sigma(x \\text{+} b_1)W_i + b_2\n\\end{equation}"}, {"title": "4 Experiments", "content": "In this section, we will first describe the experimental setup, methodologies, and evaluation metrics used to assess the performance of our proposed language model in Sec. 4.1. Subsequently, we present quantitative results of FactorLLM in Sec. 4.2 and analyze efficiency of our method in Sec. 4.3. Finally, Sec. 4.5 shows ablation studies conducted to demonstrate the effectiveness of method designs.\n4.1 Experiment Setup\nImplementation Details. We utilize the TinyLlama[82] with 1.1 billion parameters and MobileLlama[10] with 1.4 billion parameters as the backbones for our Large Language Models (LLMs). Optimization is carried out using the Adam algorithm[34], configured with a learning rate of 4e-5 and (\\beta_1, \\beta_2) = (0.9, 0.95). The settings include a weight decay of 1e-5 and a gradient clipping threshold of 1. We set the batch size to 64 and the sequence length to 1024.\nThe warmup stage is conducted before training FactorLLM to guide our PAR to master ability to choose experts and then routers will be frozen to train experts in every layer. In our experiments,"}, {"title": "4.2 Quantitative Performance", "content": "In this section, we present a comprehensive analysis of the quantitative performance of FactorLLM configured with 1R and 4E across diverse benchmarks on two distinct LLM platforms, TinyLlama[82] and MobileLlama[10]. We further explore configurations involving varying numbers of experts, ranging from one to three.\nAs indicated in Table. 1, the evaluation results illustrate that FactorLLM consistently delivers consid-erable enhancements in inference efficiency while sustaining robust accuracy levels. Our findings reveal that FactorLLM surpasses both MoEfication[89] and KnowledgeFactor[76] across various expert activations facilitated by our novel Prior-Approximate Router. Notably, for the boolq dataset, FactorLLM-3K surpasses the established upper bounds by directly fine-tuning on TinyLlama and MobileLlama by margins of 3.9% and 1.7%, respectively. Remarkably, even in its most efficient configuration-activating a singular expert-FactorLLM still exceeds MoEfication's performance on datasets such as openbookqa, hellaswag, and arc_e by a significant margin of over 0.03. These results from our detailed performance evaluation affirm that FactorLLM not only conserves but often amplifies the accuracy of large language models while markedly diminishing computational demands."}, {"title": "4.3 Efficiency Analysis", "content": "FLOPS Reduction. FactorLLM markedly reduces inference FFN GFLOPs, as illustrated in Figure. 3. The extent of FLOPs reduction correlates with the number of activated experts; notably, the 1R4E1K configuration achieves the most substantial reduction, approximately 75%. This efficiency gain stems from the factorization of FFNs into sparser architectures, effectively minimizing computa-"}, {"title": "4.4 Routing Analysis", "content": "As shown in Figure. 4, the model's performance is significantly better when using a router compared to when experts for each layer are randomly selected to process tokens. Specifically, the performance with the router is 85.6%, while without the router, it is 73.9%, indicating a difference of approximately 11.7%. Additionally, Table. 3 shows that if the model is trained without a router from the beginning, the final performance reaches 79.5%. These data indicate that without a router, the four experts tend to evolve into similar modules during training. In contrast, using a router during training encourages each expert to become more \"specific\", resulting in experts that are complementary rather than similar, thereby enhancing the model's overall performance.\nFurthermore, we present the router allocation results on different training steps. As illustrated in Figure. 5, the effectiveness of our proposed Prior-Approximate Router (PAR) becomes more apparent"}, {"title": "4.5 Ablation Study", "content": "We initially assessed the effectiveness of varying con-figurations of routers (R), experts (E), and knowl-edge units (K). As depicted in Table. 2, maintaininga constant K/N ratio while increasing the numberof routers does not improve performance; instead, itmay lead to performance deterioration. This suggests that a higher number of experts does not necessar-ily enhance overall outcomes and might even triggera decline in performance. This is because a singlerouter more efficiently integrates three experts intoa unified expert group. Furthermore, introducing ad-ditional routers to better align with the TinyLlamaarchitecture results in performance degradation, achieving only 76.5% accuracy. We hypothesize thatthis is due to routing conflicts among the routers during the process of knowledge adaptation fromthe original LLM. These findings reinforce the efficacy and universality of our FactorLLM.\nAs depicted in Table. 3, a comparison between the first two rows and the last two rows reveals minimal performance variation between these experimental sets. Hence, randomly assigning weights to experts during initialization (Exo) does not significantly alter the overall performance of 79.5% accuracy maintenance compared to direct expert splitting (Ex\u2081). This indicates that the initialization strategy and whether to use pretrained models do not substantially impact the ultimate performance. Nonetheless, a discernible contrast exists between random expert selection and Prior-Approximate Router (PAR) to select experts (Ex2), particularly evidenced by the outcomes on the PIQA and ARC-Challenge datasets. This underscores the superiority of our proposed PAR. Additionally, when integrating these approaches (Ex3), FactorLLM achieves an optimal performance level of 85.4%."}, {"title": "5 Conclusion and Limitations", "content": "In this paper, we introduce FactorLLM, an efficient and streamlined framework designed to enhance inference speed and facilitate rapid adaptation of LLMs to task-specific knowledge using the proposed Prior-Approximate Router (PAR). FactorLLM factorizes the FFN weight matrix into modules of uniform dimensional shapes that store task-specific knowledge and preserves original performance integrity by avoiding any modification or omission of data. Furthermore, it enables fine-tuning to specific tasks with minimal data requirements. Although FactorLLM has demonstrated promising outcomes, there is considerable potential for enhancing its performance. In future work, we aim to develop advanced strategies for parameter partitioning, potentially segmenting parameters into experts of varying shapes to better address diverse tasks. This approach could further optimize the architecture and improve the model's adaptability to specific requirements."}]}