{"title": "FactorLLM: Factorizing Knowledge via Mixture of Experts for Large Language Models", "authors": ["Zhongyu Zhao", "Menghang Dong", "Rongyu Zhang", "Wenzhao Zheng", "Yunpeng Zhang", "Huanrui Yang", "Dalong Du", "Kurt Keutzer", "Shanghang Zhang"], "abstract": "Recent research has demonstrated that Feed-Forward Networks (FFNs) in Large Language Models (LLMs) play a pivotal role in storing diverse linguistic and factual knowledge. Conventional methods frequently face challenges due to knowledge confusion stemming from their monolithic and redundant architectures, which calls for more efficient solutions with minimal computational overhead, particularly for LLMs. In this paper, we explore the FFN computation paradigm in LLMs and introduce FactorLLM, a novel approach that decomposes well-trained dense FFNs into sparse sub-networks without requiring any further modifications, while maintaining the same level of performance. Furthermore, we embed a router from the Mixture-of-Experts (MoE), combined with our devised Prior-Approximate (PA) loss term that facilitates the dynamic activation of experts and knowledge adaptation, thereby accelerating computational processes and enhancing performance using minimal training data and fine-tuning steps. FactorLLM thus enables efficient knowledge factorization and activates select groups of experts specifically tailored to designated tasks, emulating the interactive functional segmentation of the human brain. Extensive experiments across various benchmarks demonstrate the effectiveness of our proposed FactorLLM which achieves comparable performance to the source model securing up to 85% model performance while obtaining over a 30% increase in inference speed.", "sections": [{"title": "1 Introduction", "content": "Large language models[90, 50] (LLMs) exhibit exceptional capabilities in knowledge recall, attributable to both their extensive training on expansive text corpora[55, 22, 38, 65, 78] and their advanced cascade transformer architectures. Central to these architectures are the feed-forward layers within the transformer blocks. These layers constitute a significant fraction of the model's parameters and play a crucial role in storing and processing vast quantities of information[70, 71, 53, 54, 7]. However, the substantial size and complexity of transformers primarily stem from their monolithic Feed-Forward Networks[23, 13], which leads to oversized knowledge storage for specific tasks and significant consumption of time and computational resources[1, 7, 3]. These inefficiencies present substantial challenges in efficiently deploying large language models, particularly in computational-constraint task-specific scenarios. Redundant parameters often result in ineffective computations and increase the likelihood of an \"illusion\" caused by knowledge that is irrelevant to certain tasks.\nSubstantial studies [69, 40, 51, 44, 73] have targeted improvements in the efficiency and adaptability of LLMs. Small Language Models [60, 6, 52, 48, 82] (SLMs) aim to reduce the demand for computational resources through compact architectures. However, they are historically and empirically constrained by the Scaling Law [33], which leads to significant model degradation. On the other"}, {"title": "2 Related Work", "content": "Large language models are frequently criticized for their substantial resource and time demands[59,\n9, 14, 58] during both training and inference. To address this challenge, various techniques[69, 40,\n51, 44, 73] have been proposed to enhance inference efficiency in large transformer models. Model\ncompression[94] is one approach to decrease computational requirements include techniques like\npruning[20, 68, 2, 88, 87] and quantization[21, 61, 41, 42]. Researchers have also developed several\nresource-efficient and computation-efficient architectures, such as efficient attention mechanisms[92,\n91], mixture of experts [37, 17, 85], long-context models[16, 56, 79], and state space models[24].\nAdditionally, numerous strategies [73] have been identified to improve efficiency throughout the\ntraining, fine-tuning, and inference stages. Our objective is to accelerate large language models by\nmodifying their architectures to factorize specific knowledge within the network."}, {"title": "2.1 Efficient Large Language Model", "content": "Large language models encapsulate extensive knowledge across diverse domains and tasks[26],\nacquired from vast amounts of training data. To efficiently leverage this knowledge and mitigate\narchitectural redundancy, various methods have been developed to decompose large models and\nextract intrinsic knowledge. Model editing[93, 64, 8, 46, 47] aims to change the knowledge or brief\ninside large language models. The Locating-and-Editing[46] method views the FFN as a key-value\nmemory[23] and proposes an interpretable approach to trace the effects of weights within the model\non the output of input prompts which enables the identification and modification of specific neurons\nto edit the model's behavior effectively. Alternatively, low-rank matrix decomposition[31, 80, 74]\ndirectly modify model architectures including embedding layer[77] and feed-forward network[39]\nto reallocate knowledge across different modules. Knowledge distillation[28, 30, 63, 35] is another\napproach, focusing on transferring knowledge from large models to smaller counterparts. Notably, a\nnovel distillation task termed knowledge factorization[76] has been proposed to extract both task-\nagnostic and domain-specific knowledge from neural networks. In our work, we introduce the mixture\nof experts technique with per-layer distillation training strategy to facilitate effective knowledge\nfactorization."}, {"title": "2.2 Knowledge Decomposition", "content": "Mixture of Experts (MoE) [4, 62, 37, 19, 17] is instrumental in integrating diverse domain knowledge\nacross different modules to achieve effective knowledge fusion[72, 83]. One method to construct\nexperts involves cloning components of the original transformer block, including attention heads\n[86], feed-forward networks [36], and even bypassing low-rank adapters[45, 43], which has proven\nto be an effective approach for scaling large transformer-based models and expanding their capacity.\nMoreover, [85] situates the traditional MLP in MoE block with the linear-wise feature modulation to\nfurther enhance the model efficiency. Alternatively, a different strategy[89] involves decomposing\nlayers into distinct modules according to K-Means clustering. Our proposed FactorLLM extends this"}, {"title": "2.3 Mixture of Experts", "content": "concept by adapting the factorized LLM to specific domains of knowledge with a simpler neuron\npartition, thereby achieving enhanced performance and efficiency."}, {"title": "3 Proposed Approach", "content": "In this section, we elucidate the rationale behind decomposing the FFN in a fully pretrained LLM\ninto various subnetworks without performance loss and present the comprehensive framework of our\nproposed FactorLLM via MoE. We initially define key concepts and preliminaries concerning LLM\nand MoE in Sec. 3.1. Subsequently, we discuss the factorization of an FFN into multiple subnetworks\nin Sec. 3.2. Finally, we elaborate on FactorLLM with our dynamic routing strategy and the overall\ntraining objectives in Sec. 3.3."}, {"title": "3.1 Preliminary", "content": "Feed-Forward Network (FFN). For a given input embedding $x \\in \\mathbb{R}^{d_e}$ and denoting the hidden\ndimension by $d_h$, the FFN, which are typically implemented as two-layer Multi-Layer Perceptrons\n(MLP), can be formulated as follows:\n$h = xW_1+b_1$\n$F(x) = \\sigma(h)W_2 + b_2$                                                             (1)\nwhere $F(\\cdot)$ stands for the fully connected feed-forward network, $h$ is the hidden representation inside\nMLP and $\\sigma(\\cdot)$ is a non-linear activation function (e.g., SiLU[18]). $W_1 \\in \\mathbb{R}^{d_e \\times d_h}$ and $W_2 \\in \\mathbb{R}^{d_h \\times d_e}$\nare weight matrices while $b_1 \\in \\mathbb{R}^{d_h}$ and $b_2 \\in \\mathbb{R}^{d_e}$ are bias vectors.\nMixture of Experts (MoE). The Mixture of Experts model is comprised of a set of $i \\in N$ expert\nfunctions $E_i(\\cdot)$, and a trainable TopK router $R(\\cdot)$. The router is designed to distribute input embed-\ndings among the experts by generating a probability vector that dictates the allocation. For a given\ninput embedding $x \\in \\mathbb{R}^{b \\times n \\times d_e}$, the output of the MoE model is a composite of contributions from\neach expert. These contributions are weighted according to the probabilities assigned by the router\nand can be formally expressed as:\n$y = \\sum_{i=1}^{N} E_i(x)R_i(x), R(x) = \\epsilon(xW_3 + b_3)$\n$s.t. R_i(x) \\geq 0 \\text{ and } \\sum_{i=1}^{N} R_i(x) = 1$       (2)\nwhere $\\epsilon(\\cdot)$ signifies the softmax function, $W_3 \\in \\mathbb{R}^{N \\times d_e}$ represents a matrix of trainable weights, and\n$b_3 \\in \\mathbb{R}^N$ is the bias vector. However, MoE-based architectures often suffer performance degradation\nwhen too many inputs are routed to a few experts[19, 37]. To mitigate this imbalance, a load balance\nloss, denoted as $L_{lb}$, was introduced in [37] to penalize uneven input distribution among experts:\n$L_{lb} = \\frac{N}{K} \\sum_{n=1}^{N} \\sum_{i=1}^{K} v_i(x_n)R_i(x_n), s.t. K \\leq N$                                                              (3)\nwhere $x_n$ represents the $n^{th}$ input token. Here, $v_i(x_n)$ equals 1 if the $i^{th}$ expert is selected for\nprocessing $x_n$ by the TopK selection function, and 0 otherwise."}, {"title": "3.2 Model Decomposition", "content": "The fundamental concept of model decomposition involves partitioning neurons in a fully pretrained\nmodel that frequently activates concurrently into distinct subnetworks for acceleration. These\nsubnetworks are sparsely activated during the feedforward phase, thereby accelerating the model\ninference without performance loss. To maintain consistent forward processing speeds and reduce\nthe \"bucket effects\" associated with differing expert sizes, we decompose the weight matrix into\nN subnetworks of uniform dimensions. Such partition eliminates delays caused by the slowest\ncomponent in parallel operations, thereby improving the efficiency of parallel computations."}, {"title": "3.3 FactorLLM", "content": "Transforming into Mixtur-of-Experts. In Sec. 3.2, it is established that partitioning the FFN into\nN distinct subnetworks does not compromise the overall model efficacy. This finding allows us to\nexploit the resulting sparse architecture to expedite model computations by selectively activating a\nlimited subset of subnetworks, denoted as $S = \\{s_k\\}_{k=1}^{N}$. Drawing parallels to the architecture of\nMixture of Experts (MoE), we treat these subnetworks S as individual experts E:\n$E(x) = \\sum_{i \\in S} \\sigma(x \\frac{W_i}{N} + b_1)W_i + b_2$                                                                                (7)"}, {"title": "4 Experiments", "content": "In this section, we will first describe the experimental setup, methodologies, and evaluation metrics\nused to assess the performance of our proposed language model in Sec. 4.1. Subsequently, we present\nquantitative results of FactorLLM in Sec. 4.2 and analyze efficiency of our method in Sec. 4.3. Finally,\nSec. 4.5 shows ablation studies conducted to demonstrate the effectiveness of method designs."}, {"title": "4.1 Experiment Setup", "content": "Implementation Details. We utilize the TinyLlama[82] with 1.1 billion parameters and\nMobileLlama[10] with 1.4 billion parameters as the backbones for our Large Language Models\n(LLMs). Optimization is carried out using the Adam algorithm[34], configured with a learning rate of\n4e-5 and $(\\beta_1, \\beta_2) = (0.9, 0.95)$. The settings include a weight decay of 1e-5 and a gradient clipping\nthreshold of 1. We set the batch size to 64 and the sequence length to 1024.\nThe warmup stage is conducted before training FactorLLM to guide our PAR to master ability to\nchoose experts and then routers will be frozen to train experts in every layer. In our experiments,"}, {"title": "4.2 Quantitative Performance", "content": "In this section, we present a comprehensive analysis of the quantitative performance of FactorLLM\nconfigured with 1R and 4E across diverse benchmarks on two distinct LLM platforms, TinyLlama[82]\nand MobileLlama[10]. We further explore configurations involving varying numbers of experts,\nranging from one to three.\nAs indicated in Table. 1, the evaluation results illustrate that FactorLLM consistently delivers consid-\nerable enhancements in inference efficiency while sustaining robust accuracy levels. Our findings\nreveal that FactorLLM surpasses both MoEfication[89] and KnowledgeFactor[76] across various\nexpert activations facilitated by our novel Prior-Approximate Router. Notably, for the boolq dataset,\nFactorLLM-3K surpasses the established upper bounds by directly fine-tuning on TinyLlama and\nMobileLlama by margins of 3.9% and 1.7%, respectively. Remarkably, even in its most efficient\nconfiguration-activating a singular expert-FactorLLM still exceeds MoEfication's performance\non datasets such as openbookqa, hellaswag, and arc_e by a significant margin of over 0.03. These\nresults from our detailed performance evaluation affirm that FactorLLM not only conserves but often\namplifies the accuracy of large language models while markedly diminishing computational demands."}, {"title": "4.3 Efficiency Analysis", "content": "FLOPS Reduction. FactorLLM markedly reduces inference FFN GFLOPs, as illustrated in Figure. 3.\nThe extent of FLOPs reduction correlates with the number of activated experts; notably, the 1R4E1K\nconfiguration achieves the most substantial reduction, approximately 75%. This efficiency gain\nstems from the factorization of FFNs into sparser architectures, effectively minimizing computa-"}, {"title": "4.4 Routing Analysis", "content": "As shown in Figure. 4, the model's performance is significantly better when using a router compared\nto when experts for each layer are randomly selected to process tokens. Specifically, the performance\nwith the router is 85.6%, while without the router, it is 73.9%, indicating a difference of approximately\n11.7%. Additionally, Table. 3 shows that if the model is trained without a router from the beginning,\nthe final performance reaches 79.5%. These data indicate that without a router, the four experts tend\nto evolve into similar modules during training. In contrast, using a router during training encourages\neach expert to become more \"specific\", resulting in experts that are complementary rather than similar,\nthereby enhancing the model's overall performance.\nFurthermore, we present the router allocation results on different training steps. As illustrated in\nFigure. 5, the effectiveness of our proposed Prior-Approximate Router (PAR) becomes more apparent"}, {"title": "4.5 Ablation Study", "content": "We initially assessed the effectiveness of varying con-\nfigurations of routers (R), experts (E), and knowl-\nedge units (K). As depicted in Table. 2, maintaining\na constant K/N ratio while increasing the number\nof routers does not improve performance; instead, it\nmay lead to performance deterioration. This suggests\nthat a higher number of experts does not necessar-\nily enhance overall outcomes and might even trigger\na decline in performance. This is because a single\nrouter more efficiently integrates three experts into\na unified expert group. Furthermore, introducing ad-\nditional routers to better align with the TinyLLama\narchitecture results in performance degradation, achieving only 76.5% accuracy. We hypothesize that\nthis is due to routing conflicts among the routers during the process of knowledge adaptation from\nthe original LLM. These findings reinforce the efficacy and universality of our FactorLLM.\nAs depicted in Table. 3, a comparison between the first two rows and the last two rows reveals\nminimal performance variation between these experimental sets. Hence, randomly assigning weights\nto experts during initialization (Exo) does not significantly alter the overall performance of 79.5%\naccuracy maintenance compared to direct expert splitting (Ex\u2081). This indicates that the initialization\nstrategy and whether to use pretrained models do not substantially impact the ultimate performance.\nNonetheless, a discernible contrast exists between random expert selection and Prior-Approximate\nRouter (PAR) to select experts (Ex2), particularly evidenced by the outcomes on the PIQA and\nARC-Challenge datasets. This underscores the superiority of our proposed PAR. Additionally, when\nintegrating these approaches (Ex3), FactorLLM achieves an optimal performance level of 85.4%."}, {"title": "5 Conclusion and Limitations", "content": "In this paper, we introduce FactorLLM, an efficient and streamlined framework designed to enhance\ninference speed and facilitate rapid adaptation of LLMs to task-specific knowledge using the proposed\nPrior-Approximate Router (PAR). FactorLLM factorizes the FFN weight matrix into modules of\nuniform dimensional shapes that store task-specific knowledge and preserves original performance\nintegrity by avoiding any modification or omission of data. Furthermore, it enables fine-tuning to\nspecific tasks with minimal data requirements. Although FactorLLM has demonstrated promising\noutcomes, there is considerable potential for enhancing its performance. In future work, we aim\nto develop advanced strategies for parameter partitioning, potentially segmenting parameters into\nexperts of varying shapes to better address diverse tasks. This approach could further optimize the\narchitecture and improve the model's adaptability to specific requirements."}]}