{"title": "Beyond Following: Mixing Active Initiative into Computational Creativity", "authors": ["Zhiyu Lin", "Upol Ehsan", "Rohan Agarwal", "Samihan Dani", "Vidushi Vashishth", "Mark Riedl"], "abstract": "Generative Artificial Intelligence (AI) encounters limitations in efficiency and fairness within the realm of Procedural Content Generation (PCG) when human creators solely drive and bear responsibility for the generative process. Alternative setups, such as Mixed-Initiative Co-Creative (MI-CC) systems, exhibited their promise. Still, the potential of an active mixed initiative, where AI takes a role beyond following, is understudied. This work investigates the influence of the adaptive ability of an active and learning AI agent on creators' expectancy of creative responsibilities in an MI-CC setting. We built and studied a system that employs reinforcement learning (RL) methods to learn the creative responsibility preferences of a human user during online interactions. Situated in story co-creation, we develop a Multi-armed-bandit agent that learns from the human creator, updates its collaborative decision-making belief, and switches between its capabilities during an MI-CC experience. With 39 participants joining a human subject study, Our developed system's learning capabilities are well recognized compared to the non-learning ablation, corresponding to a significant increase in overall satisfaction with the MI-CC experience. These findings indicate a robust association between effective MI-CC collaborative interactions, particularly the implementation of proactive AI initiatives, and deepened understanding among all participants.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in Machine Learning (ML)-powered Artificial Intelligence (AI), such as large language models (LMs) [1] and diffusion models [2], have made a new class of tools for Procedural Content Generation (PCG) available to game creators. The dominant contemporary way for the creators to control such generative AI models is via prompting\u2014the issuing of textual instructions for the model to interpret and respond to [3]. That is, the user is tasked with the responsibility of issuing clear \"prompts\" to contextualize the AI system and make them aware of their intents. The AI is tasked to follow and fulfill the request strictly based on it. If the system does not respond with an output that satisfies the creators' wants or needs, it is incumbent upon the creators to modify the prompt and try again.\nThe paradigm of human creators working with generative AI via prompting is just one of many theoretical ways for a human creator and an AI system to interact [4]. There is evidence that prompting is not necessarily the best interaction paradigm; users indicate an appreciation for more varied ways of interacting with AI creative systems [5]. Other configurations of human-AI collaboration creative systems are possible that promise to reduce cognitive load, frustration, and system abandonment [6], and make these systems more casual and enjoyable [7]. These include Mixed-Initiative (MI) systems and Co-Creative (CC) systems. Mixed initiative (MI) systems are those in which both human and AI systems can initiate content changes. Co-Creative (CC) systems are those in which both human and AI systems can contribute to content creation. In particular, MI-CC systems have been demonstrated in game design [8], drawing [9], and storytelling [10], that benefits from both human and AI possessing the ability to take creative initiative. While the broadest definition of co-creative systems might include any human creators working with a generative AI, the vast majority of them have not investigated the role of mixed-initiative, especially a more active AI initiative.\nAt the heart of MI-CC systems is the question of whether and how the AI creative agent knows and understands (a) the intentions and goals of the human creator and (2) how the user wants to work with the AI system. These questions pose significant challenges, especially within domains critical to game designers utilizing AI, such as Computational Creativity and PCG. In other domains, the goal may be provided to AI in advance, making it easier to identify opportunities to take the initiative with respect to contributing to a solution\u2014the extreme of which is the AI system knowing the goal and solving the goal completely on its own. When it comes to creating games, however, the human creators' intent is harder to articulate completely[11]. The human creator's goals are also non-stationary and may evolve during the creative process [12, 13]. The human creator might also have a preferred working style that the agent should conform to in order to take the initiative while minimizing disruption. Once we overcome these challenges, researchers have shown that such ambiguity and instability link to improved outcomes of the creative activity [14], thus benefiting the MI-CC interaction.\nIn this paper, we examine Co-Creative systems in a mixed-initiative setting and study the dynamics of managing creative responsibility between human and AI initiatives. We ask: What influence does an AI agent's ability to actively adapt to creators' expectancy of creative responsibility in an MI-CC system have on creator experience and perception?\nIn particular, we make the assumption that the AI agent is capable of working in the creative domain if given explicit prompts but is unaware of the human creator's preferences for distributing creative responsibility between humans and the AI. We explore the usage of Reinforcement Learning (RL) methods in this setting and demonstrate that the creative responsibility learning challenge in MI-CC systems can be addressed by a multi-armed bandit (MAB) algorithm that observes feedback from users iteratively, updates its beliefs, and carries out its capabilities to facilitate the MI-CC collaboration. The learning is done online in real-time during the MI-CC process, and the human creator is not expected to have previous knowledge of the AI agent or time to pre-train it with regard to their collaboration style.\nWorking in the domain of structured story co-creation, we invite 39 participants to a human subject study. We quantitatively measure the human creator's perceived learning performance of the agent and the overall level of satisfaction with the collaboration. We use the Creative Support Index (CSI) [15] to study the implications of a learning and evolving AI agent. We also report on qualitative data collected from participants, using a grounded theory [16] approach in which we identify thematic patterns in users' subjective reports of their experiences. This study revealed a higher degree of participant recognition regarding the learning capabilities of our agent, compared to the ablation, which in turn corresponded to a significant increase in overall satisfaction with our agent."}, {"title": "2. Background and Related Work", "content": "The procedure of an MI-CC system learning its creative responsibilities can be described as a decision-making process, where the agent communicates with the human creator, gathers information, and chooses among its capabilities. This is not as straightforward as asking human creators to prompt AI agents because:\n\u2022 Just like the Cold Start problem experienced by AI agents lacking prior preferential knowledge from their creators [17], human creators, even experts, may struggle to make inferences about the behavior of AI systems they initially face;\n\u2022 The ability of human creators to effectively convey information to AI depends on their communication skills, which can be a significant obstacle even in human-to-human interactions [18].\n\u2022 Enforcing this AI-centric method of input requires a profound mechanical understanding of the AI system from the human creators, where this knowledge does not necessarily intersect with their expertise. This marginalizes creators who do not possess the requisite expertise in utilizing AI.\nFor these reasons, relying solely on human creators for direct collaborative prompting, regardless of the capability of the AI models, has its limitations, leading to efficiency, cognitive load, fairness, and equity issues. Alternatively, a model can be built on human feedback, where the human creators instead provide \"good\" or \"bad\" feedback signals to indirectly improve the model. These approaches differ from generative models that rely solely on user-defined goals, as the constraint of requiring such inputs can be alleviated. When it comes to generating contents, this is the foundation of methods such as RL from human feedback [19], that has proven to drastically improve the quality of generated text in state-of-the-art models such as GPT-4[1]. Yet, they are designed to exclusively optimize for a static, known-from-data objective. They are not designed for online implementation where pre-training is not feasible, and the system lacks prior knowledge of new creators and needs to actively probe them.\nTo focus on the active probing challenge, we formalize it as a Multi-Armed Bandit (MAB) problem [20] above generative abilities, where an AI agent needs to actively choose under uncertainty from their library of capabilities based on their understanding of their human creator teammate, to minimize total regret and maximize rewards from their teammate. Multi-Armed Bandit systems have been employed in the context of resolving how to make progress in an interactive creative experience. Koch et al. [21] discussed a design ideation framework that suggests images that a designer may like by exploring and exploiting in the image embedding space with a variant of MAB; Gallotta et al. [22] applied MAB in the context of generating \"in-game spaceships\" by enabling creator-guided latent space walk in the feature embedding space representing such spaceships. These works focused on a single type of action in the content space, and concentrated on expanding the generative space of such content; Lin et al. [23, 5] explored instead the action space, characterized as types of Communications representing information exchange between human and AI used in the co-creative process; As to the idea of switching between different high-level actions beyond the content level, Building a model of the user has been proven to help in a CC setting; specifically in the domain of storytelling. Yu et al. [24] demonstrated its potential to generate stories that bring \"an enjoyable experience for the players\"; Gray et al. [25, 26] further demonstrated how MAB agents help to capture this player model. Vinogradov et al. [27] showcased a framework where the agent explores the creators' \"player\" model vigorously by directly generating \"distractions\", objects designed to probe into players' preference instead of providing utilities in finishing a certain task; They proposed using MAB for this task for its promises in \"balancing the act of gathering information about the payout associated with each arm (exploration) and maximizing reward given the current known information (exploitation)\", dynamically updating the model in the process towards assigning tasks that the players feel more interested in tackling. They inspire our method, as its approach of adding distractions is well comparable to the agent carrying out its initiative while directly changing the creative content."}, {"title": "3. Study Design", "content": "In this section, we present the study we designed to examine the AI agent we created that adapts to creators' expectancy of creative responsibility. We seek to determine how this changes the perception of the creators toward the AI and the creative experience the system supplies to the human creators."}, {"title": "3.1. Task Setup", "content": "The Delegation Setup. For the experiments, we spotlight a specific but generalizable collaborative setup: Learning a delegation. In this setup, both parties would take a subset (or entirety, if preferred) of responsibilities in an MI-CC activity towards the common goal. The human creator concentrates on specific parts of the creative task while not losing control of the other parts; the AI agent needs to strategically shift its focus towards the parts that the human creator is not focusing on and actively determine how to make improvements. Furthermore, as these interactions are not without cost, such as creators' cognitive load, it is also important to minimize such costs towards learning these responsibilities. We denote the expected and delegated responsibility that the AI agent needs to learn during the interaction preferred work style for a particular human creator.\nDomain chosen: Storytelling. Given the established research foundation within story generation, its high relevance to game development, and its inherent complexity with regard to PCG, we selected story generation as a proving ground for our proposed method. The expertise of the team and advancements in open-source Large LMs readily available to us facilitated implementation; This allowed us to focus on the human factors of the MI-CC experience and the AI agent itself.\nFor our experimental system, We used Llama2-13b-chat [28] as the LM, readily available at the time of the study while very responsive for the interactive experience."}, {"title": "3.2. Experimental Al System overview", "content": "We now describe the AI system we built for the purpose of the study. The experimental system is based on the Creative Wand framework [23], containing the following four components:"}, {"title": "3.2.1. Creative Context", "content": "The Creative Context is the abstraction of generative models for this system.\nWe tuned a model to generate a story containing four components inspired by the Narrative Arc theory. We sketched out the framework of a story with four components, which are the beginning, development or rising action, climax, and conclusion. Each component is designed to hold multiple sentences. Both the human participant and the AI are instructed to write about 20 to 30 words per component, and the target length of the whole story is around 100 words.\nOnce we set up the model, it will take requests from Communications."}, {"title": "3.2.2. Communications", "content": "Communications describes the interactions between the human creators and the AI; they also double as the capabilities the AI agent possess. Three communications are used in this study, a minimalistic but complete set for the task:\n\u2022 (Re)write the beginning and development;\n\u2022 (Re)write the climax and conclusion;\n\u2022 Write a review of the story, one sentence positive, one negative, and one suggestion for improvements.\nAs we focus on how the agents would learn, a smaller set of communications is chosen, allowing the participants to focus on research questions about the creative experience while minimizing the cognitive load of learning the system.\nEach of the communications includes a prompt to the LM describing the responsibilities. See Appendix C for details."}, {"title": "3.2.3. Experience Manager and Frontend", "content": "These two modules manage the interactive experience and workflow.\nWe implemented a Finite State Machine to manage the experience. Figure 2 shows the states with the overall flow of interaction each participant experiences in one experiment session. One session of the MI-CC experience is separated into multiple \"turns\", where both parties iteratively improve the story, sharing the same text fields in the editing process. The participants are not directly notified of the internal states of the system.\nHuman Initiative. During this phase, human creators contribute to the story by making edits in any of the four text fields. This phase ends when the agent decides to take the initiative. We implemented a point-based heuristic based on pilot studies: the agent would assign points for changes it observes, and will take initiative whenever enough points are accumulated, signifying substantial edits from the human creators, in the following criteria:\n\u2022 Each new character would add 5 points;\n\u2022 Each time the human creator switches between fields after any changes, 100 points are added;\n\u2022 Whenever the human creator leaves a text field with 200 points accumulated (roughly one full sentence or two minor changes), the agent will take the initiative by locking the editing interface and resetting the counter.\nThis heuristic provides two advantages compared to other ways this decision can be made: First, this heuristic is computationally fast and enables responsive interactions; Second, it additionally provides visualization for the users. As shown in Figure 1, we present this right above the text boxes for the stories, with a text hint and a progress bar representing the ideation process of the agent. We additionally provided a \"skip\" function that forces agent initiative.\nAgent Initiative. In this phase, the agent decides which capability best fosters the collaborative experience and carries out the corresponding Communication. We built a Multi-Armed Bandit-based agent that is responsible for choosing which Communication to invoke, with Thompson Sampling as the chosen algorithm for the experimental system, within the AI agent. Formally, an agent A interacts with a set of Karms $a_1 ... a_k$, each of which is associated with Communication and underlying capabilities and an unknown reward distribution. Whenever an arm is pulled, the agent seeks feedback from the human creator on the initiative, which is treated as a reward signal. (See next paragraph.) The goal of the agent is to maximize the total reward obtained by repeatedly pulling arms during the session. See subsection A for more details on the design choices of the MAB agent. Once an arm is pulled, the agent executes a Communication, interacts with the user, and updates the story as needed.\nLearning from human. The system would ask about (Action Feedback) the way they just worked and (Content Feedback) the updates and content changes. The participants choose between \"Good\" (Reward of 1) and \"Bad\" (Reward of 0). \"Bad\" feedback on generated text leads to a reversion to the original content, though it is not used to improve the LM in any way.\nA weighted mean is employed to integrate both types of feedback into a singular reward signal. For the study, a weight hyperparameter of 80% is applied to the Action Feedback and 20% to the Content Feedback. This prioritizes learning action-level responsibilities rather than the preference for LM-generated text, in which the full system and the baseline share implementation. This reward signal is then used to train the agent.\nFor this experiment, an MAB agent with Thompson Sampling is used in the experimental system. See A for a discussion and experiments related to this choice.\nOnce the learning process is complete, \"human initiative\" starts again. To maintain user engagement, text responses are morphed each time to avoid repetitiveness, while contextual hints are also strategically provided throughout the experience. Figure 1 shows the user interface."}, {"title": "3.3. Study Methodology", "content": "To study the perception of human creators towards MI-CC systems equipped with these learning capabilities, we conducted a study summarized in figure 3 on the AI system.\nWe compare our system, the \"Full\" system, with an ablation named \"baseline\". The \"baseline\" ablation does not learn. It chooses each of the 3 Communications with a 1/3 probability at all times and provides only a reverting option when \"asking for feedback\". These systems are codenamed \"Echo Wand\" and \"Harmony Wand\" respectively, not to reveal the details of the systems to the participants during the study.\nWe recruited 39 United States participants Prolific with adequate English proficiency. Each experiment session lasted for approximately 40 minutes, and we paid the participants $15 per hour for perfect completion of the study.\nPre-study. Before the experience, participants answer four 5-point Likert-scale questions on (Q1) Expertise in Computer-Assisted Designing (CAD), (Q2) Expertise in writing stories, (Q3) Frequency using AI, and (Q4) Understanding of AI.\nWe then present instructions to familiarize the participants with our systems by providing annotated screenshots of the interface, which is a copy of 1, but with additional numeric overlays, descriptions of components, and a brief introduction to the workflow of co-creating a story.\nThey are then assigned the delegation task to focus on writing the beginning and the development of the story while leaving the other parts of the story to AI as much as possible. They are also made aware that the AI does not know this setup in advance.\nExperience. Participants are assigned to interact with the full system and the baseline ablation, presented in random ordering, counter-balanced. They are given 10 turns per each of the 2 sessions.\nPost-study. After participants finished two sessions using our system, they were asked about the process they had just experienced. inspired by Creative Support Index (CSI) [15] used in the previous studies, We ask questions based on dimensions related to the creative support perception and overall collaborative experience, grouped to facilitate richer responses from the participants while maintaining their engagement in the survey.\nSpecifically, we asked which system(s), are (Q5, Learning, Collaboration) learning to collaborate, (Q6, Enjoyment, Immersion) more capable and easy to work with, (Q7, Expressiveness, Exploration, Results worth effort) enabling better stories; For Q5 through Q7, participants can choose either system, both systems, or neither to be chosen, leading to a potential total exceeding 100%. We ask one final question (Q8) on which system will they recommend more, framed in a win-draw-lose format.\nAlthough these questions are presented in the same order for all participants, the order of the options is randomized to reduce bias towards any system. All questions are followed by an open-text question prepared to collect justifications from the participants."}, {"title": "4. Results", "content": ""}, {"title": "4.1. Creative background", "content": "Table 1 shows a summary of the creative backgrounds of the participants. Although a median of 4 on all questions implies that participants are familiar with the recent advancement of AI, when specifically asking whether they can build one, only 1 participant answered \"yes\" (5 in Q4), meaning that most of the participants do not have a technical background.\nHowever, comparing to 26% reported in [5], we observed 87% of the participants at least being \"somewhat familiar\" (3+) with recent AI technologies, and 51% being \"familiar\" (4+); The experience of using commercially available Large LM-based agents may have a profound effect on how participants, in general, would collaborate with AI systems."}, {"title": "4.2. Quantitative Results", "content": "We commence by presenting the quantitative results of the study through the choices made by the participants in the multiple-choice questions.\nWhen asked which system(s) learned to collaborate with them under the delegation arrangement (Q5), the \"Full\" system is chosen 69% (n = 39) of the times, compared to 51% for the baseline (p < 0.018, under a binomial test where $H_o$: no observable difference in distribution; The same for all p-values in this section).\nWe clearly see the \"Full\" system with learning capabilities enabled being perceived significantly better at learning the delegation than the baseline, demonstrating the effectiveness of the MAB-based model From the human creator perspective learning from their feedback.\nWhen asked which system to recommend, this trend also persists: Our system is preferred (wins) 43.6% of the time, versus 20.5% (loses) for the baseline (p < 0.001); 35.9% of the participants do not have a preference (draw). The \"Full\" system is only different from the baseline system with the learning capabilities and corresponding frontend elements, yet we see a statistically significant improvement in preference towards our \"Full\" system, illustrating the potential of our method in enhancing MI-CC experience and making such system better for human creators.\nWhen it comes to which system(s) gave a good story (Q7), 72% of the participants agree that the \"Full\" system made a good story, while 69% selected the baseline system (p > 0.05). We were unable to statistically determine whether an agent learning the delegation would produce a better story; This is expected, We focused on studying the sharing of responsibilities and enforced a delegation setting. In an actual MI-CC experience, without such a prior, A human creator would utilize the agent's learning capability to promote their strengths and discourage their weaknesses, and an improvement in perceived performance is more likely to be observed in that setting.\nFinally, when queried about the collaboration itself (Q6), 62% of the participants think the \"Full\" system is capable and made the collaboration easy, while 56% voted for the baseline system (p>0.05). We also were unable to statistically determine whether the \"Full\" system is more enjoyable and immersive. Although the difference between the \"Full\" system and the baseline is substantial enough both implementation-wise and towards the perception of learning, from the angle of the user interface, the only difference is 10 additional questions from the \"Full\" system per session. Previously, Larsson et al. [29] reported that \"there was a clear trend that the visual was rather important to the subject's relationship towards the MI-CC.\" while these \"relationships\" are directly linked to creators' perception of immersion of the experience; Ehsan et al. [30] additionally pointed out that even when an AI system presents the same underlying information, how it is presented influences the perceptions of human users. We may have observed this effect from a different angle, where a lack of differences in presentation may have caused the indifference of the participants. To that end, the difference between the two systems on these creative support dimensions may be too minor when it comes to how they are presented visually; The effect of user interface used to present the results in an MI-CC system is out of the scope of this work, though these findings illuminated a potential path for future research."}, {"title": "4.3. Qualitative Results", "content": "We now show the results from the open-ended questions following each multiple-choice question. Open-ended justifications participants provided for each of the four questions are evaluated with thematic analysis [31], based on grounded theory [16]. Taking an inductive approach, we started the process with an open-coding scheme and iteratively produced in-vivo codes (generating codes directly from the data). Next, we analyzed the data using axial codes, which involves finding relationships between the open codes and clustering them into different emergent themes. Through an iterative process performed until consensus was reached, we share the most salient themes that emerged from axial codes.\nA MI-CC system that understands the intents of the human creators and follows them by learning is overall favored and collaborates well with the creators. Participants demonstrated their observation of the learning capabilities of the \"full\" system, identifying them as \"better about learning that I specifically wanted help with\" (P34) and \"listened to my feedback.\" (P39). In comparison, the baseline system is identified as \"did less of the work... did not necessarily learn what its role was expected to be\" (P19). this resulted in a preference for the Full system for P32, as the Full system is quoted as a \"more useful helper\". This aligns with the quantitative observations.\nGood content suggestions may give people the feel-ing that the system is learning how to collaborate with them, regardless of how AI is actually doing so. Despite specifically asking participants to discuss whether the agent has \"learned to collaborate with you under that arrangement\" (Q5), Participants are also rating the system based on the generated content:\n(P25, emphasis asked) This one learned from me because it was able to build off of my original foundation of my story that I typed.\nP18, who rated their familiarity with AI as Familiar (4 out of 5) and AI usage as \"Always / as much as possible\" (5 out of 5), wrote that the \"Full\" system is learning from them:\nI could see Echo Wand adding more detail and building out more creatively than with Harmony Wand.\nThis participant is familiar with recent generative AI and mentions \"adding details\" and \"building,\" which are traits that these AI are optimized for. As both the \"Full\" and the baseline use the same underlying generative AI capabilities, P18 could not distinguish between the \"improvements\" on generated contents and the performance of the MAB-based agent. The apparent improvements of generated stories may result from a wide range of reasons, such as participants providing different input and LM sampled differently, unrelated to both the underlying LM and the learner, creating noises in the perception of participants.\nDiversity is also important, it may not be the best strategy for a learning agent to pick the \"best options\", and sometimes the agent may want to intentionally surprise their teammates. P23 was impressed by the range of capabilities both agents possess, seeing \"They were both impressive, being able to take my story and to word it better, or even add things to change it to make it better\". When asked about the generated story, P39 mentioned that \"Both of them gave bad stories.\" and \"I need much more control and options\". Curiously, this is the same participant that enjoyed the agent that \"listened to my feedback.\". P36 preferred the baseline system that executes random actions:\nI did all of the work with Echo, despite my best efforts to get it to collaborate with me. Harmony had much more interesting suggestions and rightfully pointed out when a section became too dense. It balanced the second two sections to match my intro and build up, unlike Echo who almost refused to work on them.\nFor this study, we assigned delegation tasks to the participants. This is only a subset of possible responsibilities that the AI agent can take and the human creators may expect. Lin et al. [5] have shown that a system with more coverage of the design space, providing more diversified options, is preferred. Our study design, which is more focused on studying the learning process, limited the variety of capabilities the agent may perform. To that end, once such an MI-CC system is put into use beyond research, it is necessary to diversify both the capability pool and the process of the AI agent choosing them, potentially providing surprise and unpredictability to further inspire the users.\nCreator control is important, and creators may want their ideas to be included even when AI can provide better candidates. Beyond the need for control mentioned by P39, P28 mentioned that they were impressed by the capabilities of both systems in \"finish the story that I started with.\" (Emphasis added). P27 mentioned further on their justification:\nI was in control of the final text to accept changes or not, or to make my own.\nIn a system involving a creator who wishes to create content to their liking, it is expected that the creator wishes to solicit as much control as possible. However, if the AI agent does not have any final say on the contents, should we expect it to take any creative responsibilities? Although we acknowledge that this is more of a philosophical question, way out of the scope of our work, what if the agent would understand what their counterpart is actually seeking and use this information to determine what contribution they should stick to by understanding what human creators are thinking?"}, {"title": "5. Discussions", "content": "Distilling from these findings ranging from the perception of collaboration, good writing skills, diversity in capabilities, and creators' need for control, a common implication surfaces: Getting the mental model of the creators right, the system will succeed; Getting it wrong, failure cases would surface. A mental model is described by Kieras et al. [32] as \"understanding . . . that describes the internal mechanism\" of the system a human is operating; Leslie et al. [33] further point out that a theory of mind is a mechanism that human expresses naturally, towards an understanding of thinking, in our context, their teammate AI. The success of our \"Full\" system of learning comes from its ability to learn a model of how the creators wish to collaborate with them; Although we didn't formalize this as learning a mental model, the reward given from a teammate can be otherwise treated as a reward for correctly understanding the mental model of the human creator. The need for diversified responses and more respect to control signals users imposed also fall into this paradigm, but beyond; Understanding how these reward signals should be used to determine initiative from the agents beyond \"picking the best\", and how to capture hints of additional types of actions or capabilities needed can greatly improve collaborations with MI-CC systems. This falls into the subfield of \"novelty detection and adaptation\" [34] situated in RL, which is known to be challenging, if solvable at all with ML methods, as ML models can only rely on their extrapolation capabilities towards the \"unknowns\", that may not hold for all novelties; This will be a rewarding pathway towards better MI-CC systems, if not AI agents overall.\nWe start to see a consistent narrative: creators are interpreting the capabilities of our AI agent learning as an attempt the AI agent made to learn a mental model of themselves; Because our agent determines which Communication to use and the effect of it on the contents being collaborated on, We observe the participants treating proper learning of Communication choices (expected) and the content generated (emerging) as both evidence that the agent is learning from them and traits leading to their preferences towards these systems. This also, to some extent, explains the placebo effect we observe on the baseline system:\naround half of the participants believe that the baseline system is learning from them, significantly more than 0, despite the baseline system only making decisions randomly. In this controlled comparative study, to avoid a bias towards either of the systems, we intentionally did not disclose any difference between the \"full\" system and the baseline. This perception may have arisen from the capability of our agent to generate part of stories that follow the context that the participants provided. Although we acknowledge that these factors are hard to decouple, this finding also hints at the potential of our methods in understanding the human creator holisticly. Upol et al. [30] pointed out that the background of human users determines their cognitive heuristics, which plays a role in their expectations beyond what the designer of the systems expected in the first place. They also realized that if not treated carefully, AI systems can actually introduce such placebo effects, as a pitfall [35], by misleading the human users into appreciating their trustworthiness and power, without the development of underlying AI capabilities. Standing on these findings, A promising direction of research is to carefully identify the effect of expectations of both parties involved in the MI-CC process, and how they dynamically change during the collaboration."}, {"title": "6. Conclusions", "content": "In this paper, we showcased how an MI-CC system is capable of listening to human feedback and improving itself towards a better understanding of how it should collaborate with human creators in a storytelling domain. Inviting 39 participants and comparing two such systems with and without these learning capabilities, we found that this capability was well recognized by the participants and led to better satisfaction overall. To this end, we further encourage the designers of MI-CC systems to pay attention to both the human creators and the AI agent, study how each party should, or is already, adapting to and creating mental models of their counterpart, based on their creative roles taken, their previous experience, and capabilities, and most importantly, the wishes of the human creators."}, {"title": "7. Appendices", "content": ""}, {"title": "A. Choosing a MAB algorithm", "content": "In this section", "27": "we looked into three representative MAB algorithms: e-greedy", "36": "widely used in RL", "principle": "The agent has probability $ \\epsilon $ (a hyperparameter) to choose a random action (explore) instead of performing the best action from its policy (exploit).\nUCB1", "37": "instead takes a more deterministic approach: This algorithm calculates an \"Upper Confidence Bound\" for each arm", "sampling": "n$a = argmax(Z_a + \\sqrt{2logt/n_a"}, "nwhere $Z_a$ represents the average reward received from arm a, $n_a$ represents number of times arm a was pulled, and t the total number of times all arms are pulled. This makes UCB1 aware of the uncertainty of the rewards from each arm when the agent makes its decisions. Although probability distributions are used to calculate these bounds, this algorithm does not sample at all and provides a deterministic choice for a given system state.\nFinally, Thompson Sampling is a robust Bayesian approach first introduced by Thompson [38"], "1": "n$a = argmax(B(\\alpha_a", "baselines": "E-greedy, UCB1, Thompson Sampling, and Random Baseline, where a universally random arm is chosen each time. We give the agents 3 arms to pull, where one is \"liked\" and two others are \"unliked\". Each arm would give either a reward of 1 if liked or 0 otherwise when pulled, by the oracle; We define human feedback accuracy as the probability of the oracle giving a reward of 1 on pulling the \"liked\" arm and a 0 on pulling the \"not liked\" arm. As this value gets lower, closer to 50%, the simulated oracle becomes less clear on which arm it liked and becomes a less efficient feedback provider. We simulated 5 levels of this accuracy, from 60% to 100% with equal intervals.\n$ \\"}