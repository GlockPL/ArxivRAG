{"title": "GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for Cross-Graph Transfer Learning", "authors": ["Zhe-Rui Yang", "Jindong Han", "Chang-Dong Wang", "Hao Liu"], "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in handling a range of graph analytical tasks across various domains, such as e-commerce and social networks. Despite their versatility, GNNs face significant challenges in transferability, limiting their utility in real-world applications. Existing research in GNN transfer learning overlooks discrepancies in distribution among various graph datasets, facing challenges when transferring across different distributions. How to effectively adopt a well-trained GNN to new graphs with varying feature and structural distributions remains an under-explored problem. Taking inspiration from the success of Low-Rank Adaptation (LoRA) in adapting large language models to various domains, we propose GraphLoRA, an effective and parameter-efficient method for transferring well-trained GNNs to diverse graph domains. Specifically, we first propose a Structure-aware Maximum Mean Discrepancy (SMMD) to align divergent node feature distributions across source and target graphs. Moreover, we introduce low-rank adaptation by injecting a small trainable GNN alongside the pre-trained one, effectively bridging structural distribution gaps while mitigating the catastrophic forgetting. Additionally, a structure-aware regularization objective is proposed to enhance the adaptability of the pre-trained GNN to target graph with scarce supervision labels. Extensive experiments on six real-world datasets demonstrate the effectiveness of GraphLoRA against eleven baselines by tuning only 20% of parameters, even across disparate graph domains. The code is available at https://anonymous.4open.science/r/GraphLoRA.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph Neural Networks (GNNs) have emerged as a powerful tool for processing and analyzing graph-structured data, demonstrating exceptional performance across diverse domains (e.g., e-commerce [71], social networks [56], and recommendation [26, 61]) for diverse tasks, such as node classification [6], link prediction [68], and graph classification [72].\nDespite their superiority in capturing intricate graph relationships, GNNs face significant challenges when confronting substantial variations in underlying data distributions [5, 33]. Transferring a well-trained GNN to another graph typically yields suboptimal performance (i.e., negative transfer [70]) due to out-of-distribution issues. As depicted in Figure 1, negative transfer occurs in cross-graph adaption, even when transferring between citation networks or co-purchase networks within a same domain.\nTo tackle this challenge, a common approach is to imbue GNNs with generalizable graph knowledge through the 'pre-train and fine-tune' paradigm [16, 47]. In this approach, it's crucial to integrate domain-specific knowledge while preserving the universal knowledge acquired during pre-training. Consequently, Parameter Efficient Fine-Tuning (PEFT) has garnered considerable attention for its ability to mitigate the risks of overfitting and catastrophic forgetting [59]. PEFT updates only a small portion of the parameters while keeping the remaining parameters frozen, thus mitigating the risk of forgetting the universal knowledge [59].\nFor instance, Gui et al. [13] propose a structure-aware PEFT method named G-Adapter, aimed at adapting pre-trained Graph Transformer Networks to various graph-based downstream tasks. Li et al. [28] propose the AdapterGNN method, which applies the adapter to non-transformer GNN architectures. However, while these methods focus on incorporating PEFT into GNNs, they lack specific mechanisms to address discrepancies in distribution among different graphs, such as variations in node features and graph structures. As a result, they encounter challenges when applied to graphs with varying distributions.\nHow to effectively adapt well-trained GNNs to graphs with different distributions remains an under-explored problem, posing a non-trivial task due to three major challenges. (1) Cross-graph feature discrepancy. The node feature distributions between source and target graphs can vary significantly, impeding the transferability of pre-trained GNNs. For example, attributes in academic citation networks (e.g., PubMed [60]) differ greatly from those in e-commerce co-purchase networks (e.g., Computer [48]). (2) Cross-graph structural discrepancy. The structural characteristics of source and target graphs are often diverged. For instance, academic citation networks typically exhibit higher density and consists of more cyclic motifs compared to e-commerce networks [49, 58]. (3) Target graph label scarcity. The effectiveness of pre-trained GNNs often relies on sufficient labels on target graphs, which are not always available in the real-world. For instance, in social networks, typically only a small fraction of high-degree nodes are labeled [55].\nTo this end, in this paper, we present GraphLoRA, a structure-aware low-rank contrastive adaptation method for effective transfer learning of GNNs in cross-graph scenarios. Specifically, we first introduce a Structure-aware Maximum Mean Discrepancy (SMMD) to minimize the feature distribution discrepancy between source and target graphs. Moreover, inspired by the success of Low-Rank Adaptation (LoRA) [19] in adapting large language models to diverse natural language processing tasks and domains [2, 38], we construct a small trainable GNN alongside the pre-trained one coupled with a tailor-designed graph contrastive loss to mitigate structural discrepancies. Additionally, we develop a structure-aware regularization objective by harnessing local graph homophily to enhance the model adaptability with scarce labels in the target graph.\nThe main contributions of this work are summarized as follows:\n\u2022 We propose a novel strategy for measuring feature distribution discrepancy in graph data, which incorporates the graph structure into the measurement process.\n\u2022 We propose GraphLoRA, a novel method tailored for cross-graph transfer learning. The low-rank adaptation network, coupled with graph contrastive learning, efficiently incorporates structural information from the target graph, mitigating catastrophic forgetting and addressing structural discrepancies across graphs. Furthermore, we theoretically demonstrate that GraphLoRA possesses robust representational capabilities, facilitating effective cross-graph transfer.\n\u2022 We propose a structure-aware regularization objective to enhance the adaptability of pre-trained GNN to target graphs, particularly in contexts with limited label availability.\n\u2022 Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed method by tuning a small fraction of parameters, even across disparate graph domains."}, {"title": "2 RELATED WORK", "content": "2.1 Graph Transfer Learning\nGraph transfer learning involves pre-training a GNN and applying it to diverse tasks or datasets. Common techniques in graph transfer learning include multi-task learning [16, 20], multi-network learning [21, 43], domain adaptation [8, 35], and pre-train fine-tune approaches [28, 47, 66, 73]. However, multi-task learning, multi-network learning and domain adaptation are typically employed for cross-task transfer or necessitate direct relationships between the source and target graphs, which is not applicable to our problem [16, 35, 43]. Therefore, we focus on pre-train and fine-tune techniques, involving pre-training a GNN on the source graph and subsequently fine-tuning it on the target graph. For instance, GCC [47], GCOPE [70], and GraphFM [25] focus on pre-training to develop more general GNNs. In contrast, DGAT [14] is dedicated to designing architectures that perform better in out-of-distribution scenarios. GTOT [66], AdapterGNN [28], and GraphControl [73] focus on fine-tuning, aiming to adapt pre-trained GNNs to various graphs. Most relevant to our work is GraphControl, which freezes the pre-trained GNN and utilizes information from the target graph as a condition to fine-tune the newly added ControlNet for cross-domain transfer. In contrast to GraphControl, our method aligns the node feature distributions to facilitate the transfer of the pre-trained GNN, rather than using node attributes as conditions. Additionally, we leverage graph contrastive learning to facilitate knowledge transfer and utilize graph structure knowledge to enhance the adaptability of the pre-trained GNN.\n2.2 Parameter-Effcient Fine-Tuning\n\"Pre-train and fine-tune\" has emerged as the predominant paradigm in transfer learning [51]. Despite its success, full fine-tuning is frequently inefficient and susceptible to challenges like overfitting and catastrophic forgetting [13, 28, 59]. In recent years, PEFT has emerged as an alternative, effectively mitigating these issues while achieving comparable performance [59]. PEFT methods update only a small portion of parameters while keeping the remaining parameters frozen. For instance, Adapter tuning [18, 32] inserts trainable adapter modules into the model, while Prompt tuning [27,"}, {"title": "3 PRELIMINARIES", "content": "3.1 Notations and Background\nIn this paper, we utilize the notation \\(G = (V, \\mathcal{E})\\) to denote a graph, where \\(V = \\{v_1, v_2,\\cdots, v_N\\}\\) represents the node set with N nodes in the graph, and \\(\\mathcal{E} = \\{(v_i,v_j)|v_i, v_j \\in V\\}\\) represents the edge set in the graph. The feature matrix is denoted as \\(X \\in \\mathbb{R}^{N\\times d}\\), where \\(x_i \\in \\mathbb{R}^d\\) represents the node feature of \\(v_i\\), and d is the dimension of features. Furthermore, the adjacency matrix of the graph is denoted as \\(A \\in \\{0, 1\\}^{N\\times N}\\), where \\(A_{i, j} = 1\\) iff \\((v_i, v_j) \\in \\mathcal{E}\\). To avoid confusion, we employ the superscripts s and t in this paper to distinguish between the source and target graphs.\nGraph neural networks. A major category of GNNs is message-passing neural networks (MPNNs) [11]. MPNNs follow the \"propagate-transform\" paradigm, which can be described as follows:\n\n\\begin{equation}\nh_s^l = \\text{Propagate} \\left( \\{h_v^{l-1} | v \\in \\mathcal{N}(v_s) \\} \\right),\n\\end{equation}\n\n\\begin{equation}\nh_s^l = \\text{Transform} \\left( h_s^{l-1}, h_s^l \\right),\n\\end{equation}\n\nwhere \\(\\mathcal{N}(v_s)\\) denotes the neighboring node set of node \\(v_s\\), \\(h_s^l\\) represents the node embedding of node \\(v_s\\) in the \\(l\\)-th layer, and \\(h_s^l\\) denotes the aggregated representation from neighboring nodes.\nLow-Rank Adaptation (LoRA). LORA [19] is a widely used PEFT methods, designed to efficiently fine-tune LLMs across tasks and domains. Compared to the Adapter method, LoRA provides better performance without introducing additional inference latency [19]. Specifically, for a pre-trained weight matrix \\(W_o \\in \\mathbb{R}^{m\\times n}\\), its weight update is expressed through a low-rank decomposition, given by \\(W_o + \\Delta W = W_o + BA\\), where \\(B \\in \\mathbb{R}^{m\\times r}\\), \\(A \\in \\mathbb{R}^{r\\times n}\\), and the rank \\(r < \\min (m, n)\\). During fine-tuning, the pre-trained weight matrix \\(W_o\\) is frozen, while B and A are tunable. The low-rank adaptation strategy effectively reduces the number of parameters requiring fine-tuning while maintaining high model quality without introducing inference latency. Notably, by sharing the vast majority of model parameters, it enables quick task switching and allows the pre-trained model to be applied to various tasks and domains.\n3.2 Problem Statement\nGiven a GNN \\(g_\\theta\\) pre-trained on the source graph \\(G^s\\), our goal is to obtain an optimal GNN \\(f_\\Theta\\) for the target graph \\(G^t\\),\n\n\\begin{equation}\nf_{\\Theta}^* = \\underset{\\Theta}{\\text{argmin }} \\mathcal{L} (f_{\\Theta} (X^t, A^t), Y_{\\text{train}}),\n\\end{equation}\n\nwhere \\(Y_{\\text{train}}\\) denotes training labels, and \\(\\mathcal{L}\\) is the model tuning loss function. \\(f_{\\Theta} (\\cdot) = h_\\Theta \\circ g_\\theta (\\cdot)\\), with \\(h_\\Theta\\) as the tunable module and \\(g_\\theta\\) frozen. We focus on the node classification task in this work."}, {"title": "4 METHODOLOGY", "content": "4.1 Framework Overview\nThe overall framework of the model is illustrated in Figure 2. Firstly, a node feature adaptation module is designed to perform feature mapping on the target graph. Within this module, we propose a Structure-aware Maximum Mean Discrepancy strategy to minimize discrepancy in node feature distributions between the source and target graphs. After that, we introduce a structural knowledge transfer learning module to mitigate structural disparity. Taking inspiration from LoRA, we apply low-rank adaptation to the pre-trained GNN. This can be seen as incorporating an additional GNN \\(g_\\theta'\\) with the same architecture, but utilizing the weight updates as its parameters. Additionally, we utilize graph contrastive learning to facilitate knowledge transfer. During this process, we freeze the weights of \\(g_\\theta\\) and fine-tune \\(g_\\Theta'\\). To enhance the adaptability of the pre-trained GNN to scenarios with scarce labels, a structure-aware regularization objective is proposed to effectively leverage the structural information of the target graph. Finally, we employ multi-task learning to optimize multiple objectives.\n4.2 Node Feature Adaptation\nPrevious works on transfer learning have suggested that minimizing the discrepancy in feature distributions between the source and target datasets is crucial for effective knowledge transfer [3, 75]. To achieve this, a projector is designed to perform feature mapping on the node features of the target graph, as follows:\n\n\\begin{equation}\nz^t_i = p(x_i^t; \\omega),\n\\end{equation}\n\nwhere \\(p(\\cdot;\\omega)\\) is the projector with parameters \\(\\omega\\), and \\(z_i^t \\in \\mathbb{R}^{d'}\\).\nTo optimize the projector, our goal is to minimize the discrepancy in feature distributions between \\(x^s\\) and \\(z^t\\). In the realm of domain adaptation, a commonly employed metric to quantify the dissimilarity between two probability distributions is the Maximum Mean Discrepancy (MMD) [4, 12]. The fundamental principle underlying MMD is to measure the dissimilarity between two probability distributions by comparing their means in a high-dimensional feature space. Specifically, it can be expressed as follows:\n\n\\begin{equation}\n\\mathcal{L}_{mmd} = \\frac{1}{(N^t)^2} \\sum_{i=1}^{N^t} \\sum_{j=1}^{N^t} k(z^t_i, z^t_j) + \\frac{1}{(N^s)^2} \\sum_{i=1}^{N^s} \\sum_{j=1}^{N^s} k(x^s_i, x^s_j) - \\frac{2}{N^t N^s} \\sum_{i=1}^{N^t} \\sum_{j=1}^{N^s} k(z^t_i, x^s_j),\n\\end{equation}\n\nwhere \\(k(\\cdot, \\cdot)\\) denotes the kernel function.\nFor the optimization of \\(\\mathcal{L}_{mmd}\\), we can observe that the first term of \\(\\mathcal{L}_{mmd}\\) maximizes the distance between node features in the target graph, while the second term minimizes the distance between node features of the source and target graphs. The third term denotes a constant. However, the node features in graph data are not independently and identically distributed (i.i.d.), i.e., exhibiting correlation with the graph structure. For instance, neighboring nodes tend to exhibit similar features, which is overlooked by \\(\\mathcal{L}_{mmd}\\). Therefore, it is crucial to consider the graph structure when minimizing the discrepancy in feature distributions between \\(x^s\\) and \\(z^t\\). This aids in retaining the structural information in node features, such as homophily, during feature mapping.\nSpecifically, we introduce Structure-aware Maximum Mean Discrepancy (SMMD), which incorporates graph structure into the measurement of distribution discrepancy. In particular, smaller weights are assigned to node pairs with stronger connections, and larger weights are assigned to node pairs with weaker connections. First, it is crucial to quantify the strength of relationships between node pairs. Since the adjacency matrix only provides a local perspective on the graph structure [17], we utilize the graph diffusion technique to transform the adjacency matrix into a diffusion matrix [24, 29]. The diffusion matrix allows us to evaluate the strength of relationships between node pairs from a global perspective, facilitating the discovery of potential connections between node pairs and preserving them during feature mapping. Specifically, the diffusion matrix is defined as:\n\n\\begin{equation}\nS = \\sum_{r=0}^{\\infty} \\psi_r T^r.\n\\end{equation}\n\nwhere T represents the transition matrix, which is related to the adjacency matrix \\(A^t\\), and \\(\\psi_r\\) represents the weight coefficient. We utilize a popular variant of the diffusion matrix, Personalized PageRank (PPR) [44], which employs \\(T = A^t D^{-1}\\) and \\(\\psi_r = \\alpha (1 - \\alpha)^r\\), where D denotes the diagonal degree matrix, i.e. \\(D_{i,i} = \\sum_{j=1}^{N^t} A^t_{i,j}\\) and \\(\\alpha \\in (0, 1)\\) represents the teleport probability. The elements \\(S_{i,j}\\) in the obtained diffusion matrix S indicate the strength of relationships between node \\(v_i\\) and node \\(v_j\\).\nFinally, we define the Structure-aware Maximum Mean Discrepancy loss function as follows:\n\n\\begin{equation}\n\\mathcal{L}_{smmd} = \\frac{1}{(N^t)^2} \\sum_{i=1}^{N^t} \\sum_{i'=1}^{N^t} \\gamma_{i,i'} k(z^t_i, z^t_{i'}) + \\frac{1}{(N^s)^2} \\sum_{i=1}^{N^s} \\sum_{i'=1}^{N^s} k(x^s_i, x^s_{i'}) - \\frac{2}{N^t N^s} \\sum_{i=1}^{N^t} \\sum_{j=1}^{N^s} k(z^t_i, x^s_j),\n\\end{equation}\n\n\\begin{equation}\n\\gamma_{i,i'} = \\log \\left(1 + \\frac{1}{S_{i,i'}}\\right),\n\\end{equation}\n\nwhere \\(\\mathcal{L}_{smmd}\\) incorporates graph structure during computation.\n4.3 Structural Knowledge Transfer Learning\nRecent study [5] suggests that the disparity in graph structure between the source and target graphs impedes the transferability of pre-trained GNNs. Straightforward approaches such as full parameter fine-tuning of pre-trained GNNs may also lead to additional issues such as catastrophic forgetting [46]. Consequently, effectively transferring the pre-trained GNN to target graphs becomes a formidable challenge when there is a significant discrepancy in graph structure.\nDrawing inspiration from the success of LoRA across various tasks and domains, we propose incorporating adaptation for pre-trained weights, as depicted in Figure 2. During fine-tuning, we freeze the pre-trained weights while allowing newly added parameters to be tunable. From another perspective, it can be seen as maintaining the network architecture and parameters of the pre-trained GNN while introducing an additional GNN with the same architecture and utilizing the weight updates as its parameters.\nFormally, let \\(g_\\Theta\\) represents the newly added GNN, \\(GNN_l (\\cdot; W^l)\\) and \\(GNN_l (\\cdot; W^l + \\Delta W^l)\\) denote the l-th layer of \\(g_\\theta\\) and \\(g'\\theta\\), respectively, where \\(W^l, \\Delta W^l \\in \\mathbb{R}^{d^{l-1} \\times d^l}\\) are parameter matrices. The output of \\(GNN_l\\) at the l-th layer is modified from \\(H^l = GNN_l(H^{l-1}; W^l)\\) to \\(H^l = GNN_l(H^{l-1}; W^l) + GNN_l(H^{l-1}; \\Delta W^l)\\), where \\(H^0 = Z^t\\), and \\(Z^t\\) represents the feature-mapped node feature matrix. Let \\(H = g_\\theta (Z^t)\\) and \\(H' = g'\\theta (Z^t)\\) represent the output of \\(g_\\theta\\) and \\(g'\\theta\\), respectively. We apply low-rank decomposition to the weight update to reduce the number of tunable parameters. Specifically,"}, {"title": "4.4 Structure-aware Regularization", "content": "In real-world scenarios, the homophily phenomenon is prevalent in graph data, such as citation networks or co-purchase networks [30]. In general, homophily reflects the tendency for \"like to attract like\" [41], indicating that connected nodes are prone to sharing similar labels [30, 48]. In the cross-graph transfer learning context, we leverage the homophily principle to alleviate label scarcity in the target graph.\nInspired by previous work [39], we propose a structure-aware regularization objective based on the homophily principle of graph data. Specifically, we assume that the predicted label vectors of connected neighbors on the target graph are similar, while those of disconnected neighbors are dissimilar. In contrast to GraphSage [15], we utilize direct connected neighbors instead of random walk to better satisfy the assumption, which can be formulated as:\n\n\\begin{equation}\n\\mathcal{L}_{str} = - \\sum_{i \\neq j} \\left[ A_{i,j} \\log s\\left( \\text{sim} (\\hat{y}_i, \\hat{y}_j) \\right) + (1 - A_{i,j}) \\log \\left( 1 - s\\left( \\text{sim} (\\hat{y}_i, \\hat{y}_j) \\right) \\right) \\right],\n\\end{equation}\n\nwhere \\(\\hat{y}_i\\) represents the predicted label vector of node \\(v_i\\), \\(\\text{sim}(\\cdot, \\cdot)\\) represents the inner product, and \\(s(\\cdot)\\) represents the sigmoid function. Despite the limited availability of labeled data, the above regularization objective effectively utilizes the inherent homophily property in the graph to mitigate the challenge of label scarcity.\n4.5 Optimization\nFinally, we employ the following output layer to classify the target nodes based on the output of the GNN,\n\n\\begin{equation}\n\\hat{y}_i = c (h^i_\\theta + h^i_{\\Theta}),\n\\end{equation}\n\nwhere \\(c(\\cdot)\\) represents the classifier, and \\(\\hat{y}_i = \\underset{j}{\\text{argmax }} (\\hat{y}_i)_j\\) denotes the predicted class of node \\(v_i\\). The classification loss function is defined as follows:\n\n\\begin{equation}\n\\mathcal{L}_{cls} = - \\frac{1}{N^t} \\sum_i y_i \\log \\hat{y}_i + (1 - y_i) \\log (1 - \\hat{y}_i).\n\\end{equation}\n\nAfter acquiring the pre-trained GNN from the source graph, we proceed to fine-tune it by utilizing the labeled data available on the target graph. To achieve this, we employ multitask learning to jointly optimize multiple objective functions. The overall objective function is defined as follows:\n\n\\begin{equation}\n\\mathcal{L} = \\mathcal{L}_{cls} + \\lambda_1 \\mathcal{L}_{smmd} + \\lambda_2 \\mathcal{L}_{cl} + \\lambda_3 \\mathcal{L}_{str} + \\lambda_4 ||\\Theta||,\n\\end{equation}\n\nwhere the last term acts as a regularization term to prevent overfitting, and the weight coefficients \\(\\lambda_{1-4}\\) determine the importance of each objective function in the overall optimization process.\n4.6 Complexity Analysis\nIn this section, we analyze the time complexity of GraphLoRA.\nFor a target graph with \\(N^t\\) nodes and M edges, the node feature adaptation module performs feature mapping with a runtime of \\(O(N^t)\\). By leveraging fast approximations [1, 57], the diffusion matrix S can be obtained in \\(O(N^t)\\). \\(\\mathcal{L}_{smmd}\\) necessitates calculating distances between node pairs in each batch, achievable in \\(O(N^t b)\\) through the utilization of sampling techniques, where b denotes the batch size. Similarly, in the structural knowledge transfer learning module, \\(\\mathcal{L}_{cl}\\) requires calculating similarity between node pairs in each batch, also with a complexity of \\(O(N^t b)\\). As for the structure-aware regularization objective, \\(\\mathcal{L}_{str}\\) considers connected nodes as positive samples and samples a small subset of nodes as negative"}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct extensive experiments on benchmark datasets to evaluate GraphLoRA's effectiveness in cross-graph transfer learning, aiming to answer the following research questions:\nRQ1: How effective and efficient is GraphLoRA?\nRQ2: Is GraphLoRA sensitive to hyperparameters?\nRQ3: How do different modules contribute to its effectiveness?\nRQ4: Can GraphLoRA mitigate catastrophic forgetting?\nRQ5: Can GraphLoRA learn more distinguishable node representations than other baselines?\n5.1 Experimental Setup\n5.1.1 Datasets. We evaluate GraphLoRA on six publicly available datasets: three citation networks (PubMed, CiteSeer, and Cora) [60], two co-purchase networks (Photo and Computer) [48], and one large-scale network (Reddit) [15]. The statistics and detailed descriptions of these datasets are provided in Appendix B.\n5.1.2 Baselines. We compare GraphLoRA with various baselines, including supervised methods GCN [23] and GAT [54], graph contrastive learning methods GRACE [74], COSTA [69], CCA-SSG [65], and HomoGCL [30], as well as graph prompt learning method GPPT [50], and transfer learning methods GRACEt, GTOT [66], AdapterGNN [28] and GraphControl [73]. Among these, GRACEt involves pretraining a GNN on the source graph using GRACE and then transferring it to the target graph for testing. Detailed descriptions of the baselines are provided in Appendix C.\n5.1.3 Settings. For GraphLoRA, we use a standard two-layer GAT model as the backbone. The projector p (-; w) and classifier c(.) are implemented with a single linear layer. We pre-train the GNN using GRACE and fine-tune it on the target graph using our method. Experiments are conducted in the public setting with sufficient labels, and in the 5-shot and 10-shot settings with limited labels. In the public setting, we partition citation networks using public splits [60], where each category has only 20 available labels for training. For co-purchase networks and Reddit, we randomly split the dataset into training (10%), validation (10%), and testing (80%) sets. In the 5-shot and 10-shot settings, each category in the training set contains only 5 and 10 labels, respectively. For both scenarios, 80% of the dataset is allocated for testing, while the remaining data is used as the validation set. For all methods, we conduct the experiments five times and report the average classification accuracy and standard deviation. The settings for baselines and additional results are provided in the Appendix due to space constraints.\n5.2 Performance Comparison (RQ 1)\nThe performance of GraphLoRA on node classification tasks in both the public and 10-shot settings is presented in Table 1. Due to space constraints, we present only the experimental results of transfer learning methods on the pre-training datasets PubMed and Photo. The comprehensive table can be found in Appendix E.1, Table 7. GraphLoRA achieves either the best or second-best performance in most cases, underscoring its effectiveness. Compared to non-transfer learning scenarios, transfer learning scenarios are more challenging. Nevertheless, GraphLoRA achieves an average improvement of 1.01% over the best baseline results and 3.33% over GRACE. Specifically, it achieves an average improvement of 2.23% over GRACE in the public setting and 4.43% in the 10-shot setting. GraphLoRA shows a more significant performance improvement in the 10-shot setting, underscoring its effectiveness in scenarios with scarce labels.\n5.2.1 Cross-graph Transfer Learning. From Table 1, we can observe that transfer learning methods exhibit poorer performance compared to non-transfer learning methods, highlighting the significant challenge of cross-graph transfer. In contrast, GraphLoRA demonstrates impressive transfer learning capabilities, even in cross-domain scenarios. Specifically, GraphLoRA achieves an average improvement of 10.12% over GRACEt, indicating that direct transfer of pre-trained GNNs results in suboptimal performance. Additionally, GraphLoRA achieves average improvements of 8.21% over GTOT, 9.78% over AdapterGNN, and 8.74% over GraphControl.\n5.2.2 Scarce Labeling Impact on Performance. To further explore the impact of label scarcity on performance, we investigate the performance of GraphLoRA across the 1-shot to 10-shot setting, as illustrated in Figure 3. The figure reveals that, overall, GraphLoRA demonstrates a greater performance improvement compared to GRACE and GRACEt in scenarios with scarce labels. This observation not only reaffirms our earlier analysis but also substantiates the crucial role of transfer learning in scenarios with scarce labels. Furthermore, it is noteworthy that GraphLoRA consistently exhibits a more substantial performance improvement compared to GRACEt, providing additional confirmation that the direct transfer of pre-trained GNNs will result in suboptimal performance.\n5.3 Efficiency Comparison (RQ 1)\nEfficiency is a critical consideration in practical applications [34]. To evaluate the efficiency of GraphLoRA, we measure the runtime of different methods in both public and 10-shot settings on the same device, as depicted in Table 2. For transfer learning methods, we present the total runtime until model convergence during fine-tuning, while for other methods, we present the total runtime until model convergence during training. From Table 2, it is shown that the average runtime of GraphLoRA is lower than that of most baselines, indicating its high efficiency. It is noteworthy that GraphLoRA exhibits higher efficiency in the 10-shot setting compared to other baselines. This may be attributed to the effective mitigation of label sparsity through the structure-aware regularization objective, thereby facilitating easier model convergence.\n5.4 Results on Large-Scale Dataset (RQ 1)\nGraphLoRA can be easily applied to large-scale graphs using sampling techniques. We compare it with other transfer learning methods on the large-scale Reddit dataset, using the same sampling technique for all methods. The results are presented in Table 3, indicating that GraphLoRA performs well on large-scale graphs. Specifically, GraphLoRA outperforms all other methods in transfer learning scenarios and is second only to AdapterGNN in the non-transfer learning scenario. Although AdapterGNN excels in non-transfer learning scenarios, it performs poorly in transfer learning scenarios. Overall, GraphLoRA demonstrates superior performance, highlighting its effectiveness on large-scale graphs.\n5.5 Hyperparameter Analysis (RQ 2)\n5.5.1 Impact of \\(\\lambda\\). The model's performance varies with different combinations of coefficients in the objective function. To investigate GraphLoRA's sensitivity to hyperparameters, we conduct a parameter analysis on these coefficients. In our experiments, we tune the values of \\(\\lambda_1\\), \\(\\lambda_2\\), and \\(\\lambda_3\\) within the range of [0.1,10]. The experimental results are presented in Figure 4, demonstrating that"}, {"title": "5.6 Ablation Studies (RQ 3)", "content": "To evaluate the effectiveness of each module in GraphLoRA, we compare it with seven different model variants. Specifically, \"w/mmd\" represents the method using the target term \\(\\mathcal{L}_{mmd}\\) rather than \\(\\mathcal{L}_{smmd}\\). \"w/o smmd\", \"w/o cl\", and \"w/o str\" represent methods without using the target terms \\(\\mathcal{L}_{smmd}\\), \\(\\mathcal{L}_{cl}\\), and \\(\\mathcal{L}_{str}\\), respectively. Additionally, \"w/o lrd\" is the method without employing low-rank decomposition for weight updates, while \"w/o nfa\" and \"w/o sktl\" represent methods without utilizing the node feature adaptation module and structural knowledge transfer learning module, respectively. The experimental results conducted in the public setting and 10-shot setting, following pre-training on the PubMed dataset, are depicted in Table 4. The results in the 5-shot setting can be found in Appendix E.3.\nAs illustrated in Table 4, it is shown that GraphLoRA consistently outperforms seven variants in most cases, thereby demonstrating the effectiveness of each module of GraphLoRA. Specifically, the most significant performance decline is observed for \"w/o nfa\" and \"w/o smmd,\" emphasizing the importance of considering the discrepancy in feature distributions in transfer learning. This observation further validates the effectiveness of our proposed Structure-aware Maximum Mean Discrepancy for measuring the discrepancy in node feature distributions. Moreover, \"w/o str\" exhibits a more significant performance decline in the 10-shot setting compared to the public setting, indicating that the structure-aware regularization indeed contributes to improving the adaptability of pre-trained GNNs in scenarios with scarce labels.\n5.7 Catastrophic Forgetting Analysis (RQ 4)\nFine-tuning the pretrained model with full parameters often leads to in catastrophic forgetting. To mitigate this, we freeze the pretrained parameters and introduce additional tunable parameters. To evaluate GraphLoRA's ability to alleviate catastrophic forgetting, we first pre-train the model on the PubMed dataset, then fine-tune it on other datasets, and finally assess its performance back on PubMed. Experimental results comparing GraphLoRA with full parameter fine-tuning (FT) and other baselines are presented in Table 5. FT exhibits a significant performance decline, whereas GraphLoRA shows only a marginal decrease. Other baselines also experience performance declines, while less severe than that of FT. This is attributed to the fact that these methods freeze the pre-trained parameters while introducing additional trainable parameters, thus mitigating the issue of catastrophic forgetting to some degree. Moreover, GraphLoRA significantly outperforms FT (average 18.64%), highlighting its effectiveness in mitigating catastrophic forgetting.\n5.8 Visualization of Representations (RQ 5)\nIn addition to quantitative analysis, we employ the t-SNE [53"}]}