{"title": "Harmony in Diversity: Merging Neural Networks with Canonical Correlation Analysis", "authors": ["Stefan Horoi", "Albert Manuel Orozco Camacho", "Eugene Belilovsky", "Guy Wolf"], "abstract": "Combining the predictions of multiple trained models through ensembling is generally a good way to improve accuracy by leveraging the different learned features of the models, however it comes with high computational and storage costs. Model fusion, the act of merging multiple models into one by combining their parameters reduces these costs but doesn't work as well in practice. Indeed, neural network loss landscapes are high-dimensional and non-convex and the minima found through learning are typically separated by high loss barriers. Numerous recent works have been focused on finding permutations matching one network features to the features of a second one, lowering the loss barrier on the linear path between them in parameter space. However, permutations are restrictive since they assume a one-to-one mapping between the different models' neurons exists. We propose a new model merging algorithm, CCA Merge, which is based on Canonical Correlation Analysis and aims to maximize the correlations between linear combinations of the model features. We show that our alignment method leads to better performances than past methods when averaging models trained on the same, or differing data splits. We also extend this analysis into the harder setting where more than 2 models are merged, and we find that CCA Merge works significantly better than past methods.", "sections": [{"title": "1. Introduction", "content": "A classical idea for improving the predictive performance and robustness of machine learning models is to use multiple trained models simultaneously. Each model might learn to extract different, complementary pieces of information from the input data, and combining the models would take advantage of this complementarity; thus benefiting the final performance (Ho, 1995). One of the simplest ways to implement this idea is to use ensembles, where multiple models are trained on a given task and their outputs are combined through averaging or majority vote at inference (Ho, 1995; Lobacheva et al., 2020). While this method yields good results, it comes with the disadvantages of having to store in memory the parameters of multiple models and having to run each of them individually at inference time, resulting in high storage and computational costs, particularly in the case of neural networks (NNs). Another way of leveraging multiple models to improve predictive performance is to combine the different sets of parameters into a single model. This is typically done through averaging or interpolation"}, {"title": "2. Related Work", "content": "Mode connectivity Freeman & Bruna (2017) proved theoretically that one-layered ReLU neural networks have asymptotically connected level sets. Garipov et al. (2018) and Draxler et al. (2018) explore these ideas empirically and introduce the concept of mode connectivity to describe ANN minima that are connected by nonlinear paths in parameter space along which the loss remains low. Frankle et al. (2020) introduced the concept of linear mode connectivity describing the scenario in which two ANN minima are connected by a linear low loss path in parameter space.\nModel merging through alignment More recently, Entezari et al. (2022) have conjectured that \"Most SGD solutions belong to a set S whose elements can be permuted in such a way that there is no barrier on the linear interpolation between any two permuted elements in S\" or in other words most SGD solutions are linearly mode connected provided the right permutation is applied to align the two solutions. Many recent works seem to support this conjecture, proposing methods for finding the \"right\" permutations (Tatro et al., 2020; Singh & Jaggi, 2020; Pe\u00f1a et al., 2023; Ainsworth et al., 2023).\n\"Easy\" settings for model averaging Linear mode connectivity is hard to achieve in deep learning models. Frankle et al. (2020) established that even models trained on the same dataset with the same learning procedure and even the same initialization might not be linearly mode connected if they have different data orders/augmentations. It seems that only models that are already very close in parameter space can be directly combined through linear interpolation. This is the case for snapshots of a model taken at different points during its training trajectory (Garipov et al., 2018; Izmailov et al., 2018) or multiple fine-tuned models with the same pre-trained initialization (Wortsman et al., 2022; Ilharco et al., 2023; Yadav et al., 2023). This latter setting is the one typically considered in NLP research. Another setting that is worth mentioning here is the \"federated learning\" inspired one where models are merged every couple of epochs during training (McMahan et al., 2017). The common starting point in parameter space and the small number of training iterations before merging make LMC easier to attain. Model fusion has been very successful in these settings where aligning the models prior to merging is not required.\nWe emphasize that these settings are different from ours in which we aim to merge fully trained models with different parameter initializations and SGD noise (data order and augmentations).\nMerging multiple models Merging more than two models has only been explored thoroughly in the \"easy\" settings described above. For example, Wortsman et al. (2022) aver-"}, {"title": "3. Using CCA to Merge Models", "content": "Let $M$ denote a standard multilayer perceptron (MLP) and layer $L_i \\in M$ denote a linear layer of that model with a $\\sigma =$ ReLU activation function, weights $W_i \\in \\mathbb{R}^{n_i \\times n_{i-1}}$ and bias $b_i \\in \\mathbb{R}^{n_i}$. Its input is the vector of embeddings from the previous layer $x_{i-1} \\in \\mathbb{R}^{n_{i-1}}$ and its output can be described as:\n$x_i = \\sigma(W_i\\cdot x_{i-1} + b_i)$\nWe use \".\" to denote standard matrix multiplication.\nProblem statement Now consider two deep learning models A and B with the same architecture. We are interested in the problem of merging the parameters from models A and B in a layer-by-layer fashion. As mentioned in Sec. 2, simply interpolating the models parameters typically doesn't work when the models are trained from scratch from different initializations. We therefore need to align the two models' features before averaging them. We use the term \"feature\" in our work to refer to the individual outputs of a hidden layer neuron within a neural network. We sometimes also use the term \"neuron\" to refer to its learned feature or, vice-versa, we might use the term \u201cfeature\" to refer to a neuron and its parameters. Mathematically, we are looking for linear transformations $T_i \\in \\mathbb{R}^{n_i \\times n_i}$ which can be applied at the output level of model B layer i parameters to maximize the alignment with model A's parameters and minimize the interpolation error. The output of the transformed layer i of B is then:\n$x_i^B = \\sigma(T_i (W_i^B x_{i-1}^B + b_i^B))$\nWe therefore also need to apply the inverse of $T_i$ at the input level of the following layer's weights to keep the flow of information consistent inside a given model:\n$x_{i+1}^B = \\sigma(W_{i+1}^B T_i^{-1}x_i^B+b_{i+1}^B)$\nAfter finding transformations {Ti}i=1 for every merging layer in the network we can merge the two model's parameters at every layer:\n$W_i = \\frac{1}{2}(W_i^A + T_i W_i^B)$\nFor the biases we have $b_i = \\frac{1}{2}(b_i^A + T_i b_i^B)$. The linear transformations $T$ therefore need to be invertible so we can undo their changes at the input level of the next layer and they need to properly align the layers of models A and B. This problem statement is a generalization of the one considered in past works where transformations T were constrained to being permutations (Tatro et al., 2020; Entezari et al., 2022; Ainsworth et al., 2023; Jordan et al., 2023).\nWe note that while artificial neural networks are invariant to permutations in the order of their neurons, this is not the case for general invertible linear transformations. Therefore, after applying the transformations to model B to align it to A its functionality might be altered. However, our results (Sec. 4) seem to suggest that the added flexibility of merging linear combinations of features outweighs the possible negative effects of this loss in functionality."}, {"title": "3.2. CCA Merge: Merging models with CCA", "content": "The \"best\" way to align two layers from two different deep learning models and compute the transformations T is still an open-question and has been the main differentiating factor between past works (see the baselines presented in Sec. 4.3 and \"Model merging through alignment\" in Sec. 2).\nWe propose the use of Canonical Correlation Analysis (CCA) to find the transformations which maximize the correlations between linear combinations of the original features from models A and B. Let $X_M \\in \\mathbb{R}^{m \\times n_i}$ denote the set of outputs (internal representations or neural activations) of the i-th layer of model M \u2208 {A, B} in response to m given input examples. We center $X_M$ so that each column (feature or neuron) has a mean of 0.\nCCA finds projection matrices $P_A$ and $P_B$ which bring the neural activations $X^A$ and $X^B$ respectively from their original representation spaces into a new, common, representation space through the multiplications $X^A \\cdot P_A$ and $X^B \\cdot P_B$. The features of this new representation space are orthogonal linear combinations of the original features of $X^A$ and $X^B$, and they maximize the correlations between the two projected sets of representations.\nOnce the two projection matrices $P_A$ and $P_B$ aligning $X^A$ and $X^B$ respectively have been found through CCA, we can define the transformation $T_i = (P_B P_A^{-1})^T$. This transformation can be thought of as first bringing the activations of model B into the common, maximally correlated space between the two models by multiplying by $P_B$ and then applying the inverse of $P_A$ to go from the common space found by CCA to the embedding space of model A. The transpose here is simply to account for the fact that Ti multiplies $W_i$ on the left while the $P_M$'s were described as multiplying $X_M$ on the right, for $M \\in \\{A,B\\}$. The averaging of the parameters of model A and transformed B can then be conducted following Eq. 1.\nBackground on CCA In this section we omit the layer index i since it is implicit. CCA finds the projection matrices $P_A$ and $P_B$ by iteratively defining vectors $p^A$ and $p^B$ in $R^n$ such that the projections $X^A \\cdot p^A$ and $X^B \\cdot p^B$ have maximal correlation and norm 1.\nLet $S^{AA} = (X^A)^T \\cdot X^A, S^{BB} = (X^B)^T \\cdot X^B$ and $S^{AB} = (X^A)^T \\cdot X^B$ denote the scatter matrix of $X^A$, the scatter matrix of $X^B$ and the cross-scatter matrix of $X^A$ and $X^B$ respectively.\n$p^A, p^B = \\underset{p^A, p^B}{arg \\underset{}{max}} (p^A)^T S^{AB} p^B$\ns.t. $||X^A \\cdot p^A||^2 = (p^A)^T \\cdot S^{AA} \\cdot p^A = 1$\n$||X^B \\cdot p^B||^2 = (p^B)^T \\cdot S^{BB} \\cdot p^B = 1$\nSince these vectors $p^A$ and $p^B$ are defined iteratively they are also required to be orthogonal to the vectors found previously in the metrics defined by $S^{AA}$ and $S^{BB}$ respectively. Formulating this as an ordinary eigenvalue problem and making it symmetric by a change of variables allows us to find the closed-form solutions as being the vectors in $P^A$ and $P^B$ defined by:\n$U, S, V^T = SVD((S^{AA})^{-1/2} \\cdot S^{AB} \\cdot (S^{BB})^{-1/2})$\n$P^A = (S^{AA})^{-1/2} \\cdot U$ and $P^B = (S^{BB})^{-1/2} \\cdot V$\nIn practice we use Regularized CCA to make the computation of $P_A$ and $P_B$ more robust (see App. A.2). For more details we direct the reader to De Bie et al. (2005) from which this section was inspired."}, {"title": "4. Results", "content": "We first aim to illustrate the limits of permutation based matching and the flexibility offered by CCA Merge. Suppose we want to merge two models, A and B, at a specific merging layer, and let $\\{z_i^M\\}_{i=1}^n$ denote the set of neurons of model M \u2208 {A, B} at that layer. We note here that, in terms of network weights, $z_i^M$ simply refers to the i-th row of the weight matrix $W_i^M$ at that layer. Given the activations of the two sets of neurons in response to a set of given inputs, we can compute the correlation matrix C where element Cij is the correlation between neurons $z_i^A$ and $z_j^B$. For each neuron $z_i^A$, for 1 < i < n, the distribution of its correlations with all neurons from model B is of key interest for the problem of model merging. If, as the permutation hypothesis implies, there exists a one-to-one mapping between $\\{z_i^A\\}_{i=1}^n$ and $\\{z_j^B\\}_{j=1}^n$, then we would expect to have one highly correlated neuron for each $z_i^A$ \u2013 say $z_j^B$ for some 1 \u2264 j \u2264 n - and all other correlations $C_{ik}, k \\neq j$, close to zero. On the other hand, if there are multiple neurons from model B highly correlated with $z_i^A$, this would indicate that the feature learned by $z_i^A$ is distributed across multiple neurons in model B \u2013 a relationship that CCA Merge would capture.\nGiven the flexibility of CCA Merge, we expect it to better capture these relationships between the neurons of the two networks. We recall that CCA Merge computes a linear transformation T that matches to each neuron $z_i^A$ a linear combination $z \\approx \\sum_{j=1}^n T_{ij}z_j^B$ of the neurons in B. We expect the distribution of the coefficients (i.e., elements of T) to match the distribution of the correlations ($C_{ij}$ elements), indicating the linear transformation found by CCA Merge adequately models the correlations and relationships between the neurons of the two models. For each neuron"}, {"title": "4.2. CCA's flexibility allows it to better model relations between neurons", "content": "Given the flexibility of CCA Merge, we expect it to better capture these relationships between the neurons of the two networks. We recall that CCA Merge computes a linear transformation T that matches to each neuron $z_i^A$ a linear combination $z \\approx \\sum_{j=1}^n T_{ij}z_j^B$ of the neurons in B. We expect the distribution of the coefficients (i.e., elements of T) to match the distribution of the correlations ($C_{ij}$ elements), indicating the linear transformation found by CCA Merge adequately models the correlations and relationships between the neurons of the two models. For each neuron"}, {"title": "4.3. Models merged with CCA Merge achieve better performance", "content": "In Table 1 we show the test accuracies of merged VGG11 and ResNet20 models of different widths trained on CIFAR10 and CIFAR100 respectively for CCA Merge and multiple other popular model merging methods. The number of models being merged is 2 and for each experiment, we report the mean and standard deviation across 4 merges where the base models were trained with different initial-"}, {"title": "4.4. CCA merging finds better common representations between many models", "content": "In this section, we present our results related to the merging of many models, a significantly harder task. This constitutes the natural progression to the problem of merging pairs of models and is a more realistic setting for distributed or federated learning applications where there are often more than 2 models. Furthermore, aligning populations of neural networks brings us one step closer to finding the common learned features that allow different neural networks to perform equally as well on complex tasks despite having different initializations, data orders, and data augmentations.\nAs previously mentioned, the problem of merging many models is often ignored by past works except for the settings in which linear mode connectivity is easily obtained. Ainsworth et al. (2023) introduced \"Merge Many\", an adaptation of Matching Weights for merging a set of models. In Merge Many each model in the set is sequentially matched to the average of all the others until convergence. A simpler way of extending any model merging method to the many models setting is to choose one of the models in the group as the reference model and to align every other network in the group to it. Then the reference model and all the other aligned models can be merged. It is by using this \"all-to-one\" merging that we extend CCA Merge, Permute, OT Fusion and Matching Weights to the many model settings. ZipIt! is naturally able to merge multiple models since it aggregates all neurons and merges them until the desired size is obtained."}, {"title": "4.5. CCA Merge is better at combining learned features from different data splits", "content": "In this section we consider the more realistic setting where the models are trained on disjoint splits of the data, therefore they're expected to learn (at least some) different features. Such a set-up is natural in the context of federated or distributed learning. We consider ResNet20 models trained on 3 different data splits of the CIFAR100 training dataset. The first (1) data split is the one considered in Ainsworth et al. (2023); Jordan et al. (2023) where one model is trained on 80% of the data from the first 50 classes of the CIFAR100 dataset and 20% of the data from the last 50 classes, the second model being trained on the remaining examples. In the second (2) data split we use samples from a Dirichlet distribution with parameter vector a = (0.5, 0.5) to subsample each class in the dataset and create 2 disjoint data splits, one for each model to be trained on. Lastly, with the third (3) data split we consider the more extreme scenario from Stoica et al. (2024) where one model is trained on 100% of the data from 50 classes, picked at random, and the second one is trained on the remaining classes, with no overlap. For this last setting, in order for both models to have a common output space they were trained using the CLIP (Radford et al., 2021) embeddings of the class names as training objectives. In Table 3 we report mean and standard deviation of accuracies across 4 different model pairs.\nFor the first two data splits CCA Merge outperforms the other methods, beating the second best method by ~4% and ~2% on the first and second data splits respectively. For the third data split CCA Merge is the second best performing method after ZipIt!. However, ZipIt! was designed for this specific setting and, as we previously noted, it allows the merging of features from the same network to reduce redundancies, thus making it more flexible than the other methods which only perform \u201calignment\". In all cases CCA Merge outperforms or is comparable with the base models average"}, {"title": "5. Discussion and Conclusion", "content": "Recent model fusion successes exploit inter-model relationships between neurons by modelling them as permutations before fusing models. Here, we argue that, while assuming a one-to-one correspondence between neurons yields interesting merging methods, it is rather limited as not all neurons from one network have an exact match with a neuron from another network. Our proposed CCA Merge takes the approach of linearly transforming model parameters beyond permutation-based optimization. This added flexibility allows our method to outperform recent competitive baselines when merging pairs of models trained on the same data or on disjoint splits of the data (Tables 1, 2 and 3). Furthermore, when considering the harder task of merging many models, CCA Merge models showcase remarkable accuracy stability as the number of models merged increases, while past methods suffer debilitating accuracy drops. This suggests a path towards achieving strong linear connectivity between a set of models, which is hard to do with permutations (Sharma et al., 2024). One limitation of our method is that it requires input data to align the models. The forward pass to compute the activations increases computational costs and in some settings such a \"shared\" dataset might not be available. We discuss this further in App. F.\nMerging many models successfully, without incurring an accuracy drop, is one of the big challenges in this area of research. Our method, CCA Merge, makes a step in the direction of overcoming this challenge. As future work, it would be interesting to further study the common representations learned by populations of neural networks. Also, an interesting future research avenue is to test CCA Merge with models trained on entirely different datasets, to test its limits in terms of combining different features."}, {"title": "A. Additional Practical Considerations for CCA Merge", "content": "Given that CCA naturally finds a common representation space which maximally correlates the activations of both models being merged, $X^A$ and $X^B$, it is natural to consider merging the models in this common space. This would be done by transforming the parameters of model A with the found transformations $P_A$ and transforming the parameters of model B with transformations $P_B$ and then averaging both transformed models. However, the reason for applying transformation\n$T= (P_B P_A^{-1})^T$\nto model B to align it to A and merging both models in the representation space of model A instead of transforming both the weights of model A and model B and merging in the common space found by CCA is because of the ReLU non-linearity. The common representation space found by CCA has no notion of directionality, and might contain important information even in negative orthants (high-dimensional quadrants where at least one of the variables has negative values) that might get squashed to zero by ReLU non-linearities. The representation space of model A doesn't have this problem."}, {"title": "A.2. Regularized CCA", "content": "The closed form CCA solution requires inverting the scatter matrices $S^{AA}$ and $S^{BB}$ which can lead to poor performance and instability when the eigenvalues of these matrices are small. To make CCA more robust, the identity matrix I scaled by a regularization hyper-parameter $\\gamma$ is added to the two scatter matrices. Therefore to complete the CCA computation the matrices $S^{AA} + \\gamma I$ and $S^{BB} + \\gamma I$ are used instead of $S^{AA}$ and $S^{BB}$.\nTo chose the hyper-parameter $\\gamma$ for any experiment, i.e. a combination of model architecture and data set / data split, we train additional models from scratch and conduct the merging with different $\\gamma$ values. The $\\gamma$ value leading to the best merged accuracy is kept and applied to the other experiments with CCA Merge. The models used to select $\\gamma$ are discarded to avoid over-fitting and the CCA Merge accuracies are reported for new sets of models."}, {"title": "B. Further empirical analysis of matching matrices", "content": "When merging 2 models, A and B, with Permute, a large percentage (25-50%) of the neurons from model A do not get matched with their highest correlated neuron from model B by the permutation matrix, we call these non-optimal matches. In other words, for neurons in A that have a non-optimal match, a higher correlated neuron from B exists to which that A neuron isn't matched. Since permutation-based methods aim to optimize an overall cost, the found solutions match some neurons non-optimally in order to obtain a better overall score. However, since permutation-based methods either completely align two features (value of 1 in the matching matrix) or not at all (value of 0), the merging completely ignores the relationship between these highly correlated but not matched neurons."}, {"title": "B.2. Highest correlated neurons are associated to top CCA Merge transformation coefficients", "content": "Since CCA Merge combines linear combinations of neurons instead of just individual ones, we can't run the exact same analysis as we did in the previous section for Permute. However, we have looked at whether or not, for each neuron from model A, the top 1 and top 2 correlated neuron from model B (i.e. neurons with the highest or second highest correlations respectively) get assigned high coefficients in the CCA Merge transformation matrices. We report these values in the Fig. 6.\nIn the vast majority of cases the highest correlated neuron from model B gets assigned one of the 5 highest coefficients in the CCA Merge transform, and the top 2 correlated neuron gets assigned one of the 10 highest coefficients of the transform. These results showcase how CCA Merge better accounts for the relationships between neurons of different models during the merging procedure."}, {"title": "C. Additional Results", "content": "Here we present the extended Table 1 results, where we also include results for widths \u00d72 and \u00d74 for both VGG and ResNet models. The same overall conclusions hold with CCA Merge performing generally better than all other considered baselines. CCA Merge's standard deviation isn't always the lowest however as width increases but it's comparable with the other methods. As noted in the main text we ran into out-of-memory issues when running ZipIt! for VGG models with width greater than \u00d72."}, {"title": "D. VGG Results with REPAIR", "content": "In this section, specifically in Table 6, we present the results for merging VGG networks with REPAIR (Jordan et al., 2023) applied to mitigate the variance collapse phenomenon. Since the standard VGG architecture doesn't contain normalization layers it isn't as straightforward as just resetting the BatchNorm statistics as it was for ResNets. Applying REPAIR seems to greatly help past methods and all methods are now comparable in terms of accuracy, with Matching Weights (Ainsworth et al., 2023) being slightly better, but the standard deviations overlapping with CCA Merge. The great performance of CCA Merge without REPAIR suggests that perhaps models merged with our method do not suffer from variance collapse to the same extent as models merged with permutation-based methods. One interesting thing to note is that REPAIR also"}, {"title": "E. Why does CCA Merge outperform permutation-based methods in the multi-model setting?", "content": "When merging more than two models, say A, B and C in the 3-model case, we choose one reference model, WLOG model A, align all other models to that one and then merge by averaging (all-to-one merging). Let TBA denote the transformation aligning a given layer of model B to the same layer of model A and TCA the transformation aligning C to A at that same layer, we can also align A to B (resp. C) by taking the inverse of TBA (resp. TcA) which we denote TAB = TBA (resp. TAC = T\u0113). This alignment to A also gives rise to an indirect matching between models B and C through A, since we can align C to A with TCA and then apply the transformation TAB to align it to B, we use TCAB to denote the transformation of this indirect alignment. In other words, neurons i from B and j from C are matched indirectly through A if they both are matched to the same neuron k from A. To see why multi-model merging fails for permutations we can look at how this indirect matching of B and C (\u0422\u0421\u0410\u0412) compares to the direct matching of B and C found by directly optimizing for the transformation matrix TCB. It turns out that for permutation-based methods these matrices differ significantly, in fact the majority of neurons from net C, between 50-80% on average, do not get matched with the same neuron from B if we use TCAB versus if we use the direct matching T\u0441\u0432."}, {"title": "E.2. The direct and indirect matching matrices (TCB and TCAB resp.) are closer for CCA Merge than for Permute", "content": "We can also look directly at the Frobenius norm of the difference between TCAB and TCB for CCA Merge and Permute to see for which method the indirect matching between C and B (TCAB) and the direct matching between C and B (TCB) are the most similar. We report the results for the same 20 merges considered above in Fig. 8. We see that the indirect and direct matching matrices between C and B are significantly closer for CCA Merge than for Permute, which helps explain why CCA Merge outperforms permutation-based methods in the multi-model setting.\nWe have run these analyses using the Permute matching method since it is the most straightforward to analyze and we have seen from our empirical results that it constitutes a strong baseline. However we expect all permutation-based methods to be susceptible to these types of non-optimal merging and mismatches because of their permutation matching matrices and since their accuracies behave similarly as the number of models being merged increases (for the multi-model scenario)."}, {"title": "F. Analysis of computational costs", "content": "We have tracked the runtime for 5 different 2-model merges of ResNet20x8 models trained on CIFAR100 and we report these values in the table below.\nThe merging for methods relying on data (Permute, ZipIt!, CCA Merge) can be split into 2 parts, computing the metrics (eg. covariances or correlations) and computing the transformations. Computing the metrics is by a large margin the most time-expensive part of the procedure, taking on average 33.44s when using the entire CIFAR100 training set. Computing the transforms on the other hand takes less than a second for Permute and CCA Merge and 3.77s for ZipIt!, which represents"}, {"title": "G. Merged Models vs. Endpoint Models Accuracies", "content": "In past works such as Ainsworth et al. (2023) or Jordan et al. (2023), the merged ResNet models achieving the same accuracy as the endpoint models (or close to) when training on the full train datasets have been extremely wide ones. Specifically, for the ResNet20 architecture on CIFAR10 those results were obtained for models of widths \u00d716 or greater. Zero barrier merging was not achieved for the model widths considered in our work. Exploring the very wide model setting is not an objective of ours since the effect of model width on merging is already well understood, with wider models leading to better performing merges (Entezari et al., 2022; Ainsworth et al., 2023; Jordan et al., 2023). Therefore we only trained models of width up to \u00d78, which are more likely to be encountered in practice. For the model widths reported in our work the accuracy barriers are consistent with those reported in Jordan et al. (2023) for Permute (solving the linear sum assignment problem maximizing the correlation between matched neurons), with CCA Merge outperforming those results.\nFurthermore, our ResNet20 results are reported on the CIFAR100 dataset which is a harder classification task than CIFAR10, therefore it is harder to achieve zero-barrier merging. Also, in all the disjoint training scenarios presented in Table 3 we do achieve greater accuracy than the endpoint models and outperform past methods, not only in settings considered by past works such as 80%-20% training splits but in novel settings as well, such as Dirichlet training splits."}]}