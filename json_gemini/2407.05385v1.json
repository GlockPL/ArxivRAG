{"title": "Harmony in Diversity: Merging Neural Networks with Canonical Correlation Analysis", "authors": ["Stefan Horoi", "Albert Manuel Orozco Camacho", "Eugene Belilovsky", "Guy Wolf"], "abstract": "Combining the predictions of multiple trained models through ensembling is generally a good way to improve accuracy by leveraging the different learned features of the models, however it comes with high computational and storage costs. Model fusion, the act of merging multiple models into one by combining their parameters reduces these costs but doesn't work as well in practice. Indeed, neural network loss landscapes are high-dimensional and non-convex and the minima found through learning are typically separated by high loss barriers. Numerous recent works have been focused on finding permutations matching one network features to the features of a second one, lowering the loss barrier on the linear path between them in parameter space. However, permutations are restrictive since they assume a one-to-one mapping between the different models' neurons exists. We propose a new model merging algorithm, CCA Merge, which is based on Canonical Correlation Analysis and aims to maximize the correlations between linear combinations of the model features. We show that our alignment method leads to better performances than past methods when averaging models trained on the same, or differing data splits. We also extend this analysis into the harder setting where more than 2 models are merged, and we find that CCA Merge works significantly better than past methods.", "sections": [{"title": "1. Introduction", "content": "A classical idea for improving the predictive performance and robustness of machine learning models is to use multiple trained models simultaneously. Each model might learn to extract different, complementary pieces of information from the input data, and combining the models would take advantage of this complementarity; thus benefiting the final performance. One of the simplest ways to implement this idea is to use ensembles, where multiple models are trained on a given task and their outputs are combined through averaging or majority vote at inference. While this method yields good results, it comes with the disadvantages of having to store in memory the parameters of multiple models and having to run each of them individually at inference time, resulting in high storage and computational costs, particularly in the case of neural networks (NNs). Another way of leveraging multiple models to improve predictive performance is to combine the different sets of parameters into a single model. This is typically done through averaging or interpolation"}, {"title": "2. Related Work", "content": "Mode connectivity Freeman & Bruna (2017) proved theoretically that one-layered ReLU neural networks have asymptotically connected level sets. Garipov et al. (2018) and Draxler et al. (2018) explore these ideas empirically and introduce the concept of mode connectivity to describe ANN minima that are connected by nonlinear paths in parameter space along which the loss remains low. Frankle et al. (2020) introduced the concept of linear mode connectivity describing the scenario in which two ANN minima are connected by a linear low loss path in parameter space.\nModel merging through alignment More recently, Entezari et al. (2022) have conjectured that \"Most SGD solutions belong to a set S whose elements can be permuted in such a way that there is no barrier on the linear interpolation between any two permuted elements in S\" or in other words most SGD solutions are linearly mode connected provided the right permutation is applied to align the two solutions. Many recent works seem to support this conjecture, proposing methods for finding the \"right\" permutations.\n\"Easy\" settings for model averaging Linear mode connectivity is hard to achieve in deep learning models. Frankle et al. (2020) established that even models trained on the same dataset with the same learning procedure and even the same initialization might not be linearly mode connected if they have different data orders/augmentations. It seems that only models that are already very close in parameter space can be directly combined through linear interpolation. This is the case for snapshots of a model taken at different points during its training trajectory or multiple fine-tuned models with the same pre-trained initialization. This latter setting is the one typically considered in NLP research. Another setting that is worth mentioning here is the \"federated learning\" inspired one where models are merged every couple of epochs during training. The common starting point in parameter space and the small number of training iterations before merging make LMC easier to attain. Model fusion has been very successful in these settings where aligning the models prior to merging is not required.\nWe emphasize that these settings are different from ours in which we aim to merge fully trained models with different parameter initializations and SGD noise (data order and augmentations).\nMerging multiple models Merging more than two models has only been explored thoroughly in the \"easy\" settings described above. For example, Wortsman et al. (2022) aver-"}, {"title": "3. Using CCA to Merge Models", "content": "Let $M$ denote a standard multilayer perceptron (MLP) and layer $L_i \\in M$ denote a linear layer of that model with a $\\sigma$ = ReLU activation function, weights $W_i \\in \\mathbb{R}^{n_i \\times n_{i-1}}$ and bias $b_i \\in \\mathbb{R}^{n_i}$. Its input is the vector of embeddings from the previous layer $x_{i-1} \\in \\mathbb{R}^{n_{i-1}}$ and its output can be described as:\n$x_i = \\sigma(W_i \\cdot x_{i-1} + b_i)$\nWe use \"$\\cdot$ \" to denote standard matrix multiplication.\nNow consider two deep learning models A and B with the same architecture. We are interested in the problem of merging the parameters from models A and B in a layer-by-layer fashion. As mentioned in Sec. 2, simply interpolating the models parameters typically doesn't work when the models are trained from scratch from different initializations. We therefore need to align the two models' features before averaging them. We use the term \"feature\" in our work to refer to the individual outputs of a hidden layer neuron within a neural network. We sometimes also use the term \"neuron\" to refer to its learned feature or, vice-versa, we might use the term \u201cfeature\" to refer to a neuron and its parameters. Mathematically, we are looking for linear transformations $T_i \\in \\mathbb{R}^{n_i \\times n_i}$ which can be applied at the output level of model B layer i parameters to maximize the alignment with model A's parameters and minimize the interpolation error. The output of the transformed layer i of B is then:\n$x_i^B = \\sigma(T_i^B W_i^B \\cdot x_{i-1}^B + T_i^B b_i^B)$\nWe therefore also need to apply the inverse of $T_i$ at the input level of the following layer's weights to keep the flow of information consistent inside a given model:\n$x_{i+1}^B = \\sigma(W_{i+1}^B T_i^{-1} x_i^B + b_{i+1}^B)$\nAfter finding transformations ${T_i}_{i=1}^l$ for every merging layer in the network we can merge the two model's parameters at every layer:\n$W_i = \\frac{1}{2} (W_i^A + T_i W_i^B T_{i-1}^{-1}) \n(1)$\nFor the biases we have $b_i = \\frac{1}{2} (b_i^A + T_i b_i^B)$. The linear transformations $T$ therefore need to be invertible so we can undo their changes at the input level of the next layer and they need to properly align the layers of models A and B. This problem statement is a generalization of the one considered in past works where transformations T were constrained to being permutations .\nWe note that while artificial neural networks are invariant to permutations in the order of their neurons, this is not the case for general invertible linear transformations. Therefore, after applying the transformations to model B to align it to A its functionality might be altered. However, our results (Sec. 4) seem to suggest that the added flexibility of merging linear combinations of features outweighs the possible negative effects of this loss in functionality.\nThe \"best\" way to align two layers from two different deep learning models and compute the transformations T is still an open-question and has been the main differentiating factor between past works (see the baselines presented in Sec. 4.3 and \"Model merging through alignment\" in Sec. 2).\nWe propose the use of Canonical Correlation Analysis (CCA) to find the transformations which maximize the correlations between linear combinations of the original features from models A and B. Let $X_M \\in \\mathbb{R}^{m \\times n_i}$ denote the set of outputs (internal representations or neural activations) of the i-th layer of model $M \\in {A, B}$ in response to m given input examples. We center $X_M$ so that each column (feature or neuron) has a mean of 0.\nCCA finds projection matrices $P_A$ and $P_B$ which bring the neural activations $X^A$ and $X^B$ respectively from their original representation spaces into a new, common, representation space through the multiplications $X^A \\cdot P_A$ and $X^B \\cdot P_B$. The features of this new representation space are orthogonal linear combinations of the original features of $X^A$ and $X^B$, and they maximize the correlations between the two projected sets of representations.\nOnce the two projection matrices $P_A$ and $P_B$ aligning $X^A$ and $X^B$ respectively have been found through CCA, we can define the transformation $T_i = (P_B^T P_A^{-1})^T$. This transformation can be thought of as first bringing the activations of model B into the common, maximally correlated space between the two models by multiplying by $P_B^T$ and"}, {"title": "4.2. CCA's flexibility allows it to better model relations between neurons", "content": "We first aim to illustrate the limits of permutation based matching and the flexibility offered by CCA Merge. Suppose we want to merge two models, A and B, at a specific merging layer, and let ${z_i^M}_{i=1}^n$ denote the set of neurons of model $M \\in {A, B}$ at that layer. We note here that, in terms of network weights, $z_i^M$ simply refers to the i-th row of the weight matrix $W^M$ at that layer. Given the activations of the two sets of neurons in response to a set of given inputs, we can compute the correlation matrix C where element $C_{ij}$ is the correlation between neurons $z_i^A$ and $z_j^B$. For each neuron $z_i^A$, for 1 < i < n, the distribution of its correlations with all neurons from model B is of key interest for the problem of model merging. If, as the permutation hypothesis implies, there exists a one-to-one mapping between ${z_i^A}_{i=1}^n$ and ${z_j^B}_{j=1}^n$, then we would expect to have one highly correlated neuron for each $z_i^A$ \u2013 say $z_j^B$ for some 1 \u2264 j \u2264 n \u2013 and all other correlations $C_{ik}, k \\neq j$, close to zero. On the other hand, if there are multiple neurons from model B highly correlated with $z_i^A$, this would indicate that the feature learned by $z_i^A$ is distributed across multiple neurons in model B \u2013 a relationship that CCA Merge would capture.\nGiven the flexibility of CCA Merge, we expect it to better capture these relationships between the neurons of the two networks. We recall that CCA Merge computes a linear transformation T that matches to each neuron $z_i^A$ a linear combination $z_i^B \\approx \\sum_{j=1}^n T_{ij} z_j^B$ of the neurons in B. We expect the distribution of the coefficients (i.e., elements of T) to match the distribution of the correlations ($C_{ij}$ elements), indicating the linear transformation found by CCA Merge adequately models the correlations and relationships between the neurons of the two models. For each neuron"}, {"title": "A. Additional Practical Considerations for CCA Merge", "content": "Given that CCA naturally finds a common representation space which maximally correlates the activations of both models being merged, XA and XB, it is natural to consider merging the models in this common space. This would be done by transforming the parameters of model A with the found transformations PA and transforming the parameters of model B with transformations PT and then averaging both transformed models. However, the reason for applying transformation\n$T= (P_B^T \\cdot P_A^{-1})^T$ to model B to align it to A and merging both models in the representation space of model A instead of transforming both the weights of model A and model B and merging in the common space found by CCA is because of the ReLU non-linearity. The common representation space found by CCA has no notion of directionality, and might contain important information even in negative orthants (high-dimensional quadrants where at least one of the variables has negative values) that might get squashed to zero by ReLU non-linearities. The representation space of model A doesn't have this problem.\nThe closed form CCA solution requires inverting the scatter matrices SAA and SBB which can lead to poor performance and instability when the eigenvalues of these matrices are small. To make CCA more robust, the identity matrix I scaled by a regularization hyper-parameter \u03b3 is added to the two scatter matrices. Therefore to complete the CCA computation the matrices SAA + \u03b3I and SBB + \u03b3I are used instead of SAA and SBB.\nTo chose the hyper-parameter \u03b3 for any experiment, i.e. a combination of model architecture and data set / data split, we train additional models from scratch and conduct the merging with different \u03b3 values. The \u03b3 value leading to the best merged accuracy is kept and applied to the other experiments with CCA Merge. The models used to select \u03b3 are discarded to avoid over-fitting and the CCA Merge accuracies are reported for new sets of models."}, {"title": "B. Further empirical analysis of matching matrices", "content": "When merging 2 models, A and B, with Permute, a large percentage (25-50%) of the neurons from model A do not get matched with their highest correlated neuron from model B by the permutation matrix, we call these non-optimal matches. In other words, for neurons in A that have a non-optimal match, a higher correlated neuron from B exists to which that A neuron isn't matched. Since permutation-based methods aim to optimize an overall cost, the found solutions match some neurons non-optimally in order to obtain a better overall score. However, since permutation-based methods either completely align two features (value of 1 in the matching matrix) or not at all (value of 0), the merging completely ignores the relationship between these highly correlated but not matched neurons."}]}