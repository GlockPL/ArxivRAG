{"title": "SEE: Semantically Aligned EEG-to-Text Translation", "authors": ["Yitian Tao", "Yan Liang", "Luoyu Wang", "Yongqing Li", "Qing Yang", "Han Zhang"], "abstract": "Abstract-Decoding neurophysiological signals into language\nis of great research interest within brain-computer interface\n(BCI) applications. Electroencephalography (EEG), known for\nits non-invasiveness, ease of use, and cost-effectiveness, has been\na popular method in this field. However, current EEG-to-Text\ndecoding approaches face challenges due to the huge domain gap\nbetween EEG recordings and raw texts, inherent data bias, and\nsmall closed vocabularies. In this paper, we propose SEE: Seman-\ntically Aligned EEG-to-Text Translation, a novel method aimed\nat improving EEG-to-Text decoding by seamlessly integrating\ntwo modules into a pre-trained BART language model. These\ntwo modules include (1) a Cross-Modal Codebook that learns\ncross-modal representations to enhance feature consolidation and\nmitigate domain gap, and (2) a Semantic Matching Module\nthat fully utilizes pre-trained text representations to align multi-\nmodal features extracted from EEG-Text pairs while considering\nnoise caused by false negatives, i.e., data from different EEG-\nText pairs that have similar semantic meanings. Experimental\nresults on the Zurich Cognitive Language Processing Corpus\n(ZuCo) demonstrate the effectiveness of SEE, which enhances\nthe feasibility of accurate EEG-to-Text decoding.", "sections": [{"title": "I. INTRODUCTION", "content": "Decoding brain physiological signals to directly generate\nreading text is a rapidly emerging field in brain-computer\ninterface (BCI) applications [1]\u2013[4], which is valuable for\ndeveloping new communication methods for individuals with\nspeech impairments or neuro-degenerative diseases [4]. It\nalso provides deeper insights into the neural mechanisms of\nlanguage processing, offering valuable perspectives on how the\nbrain encodes and decodes linguistic information [5]. In this\nfield, electroencephalography (EEG) is widely used due to its\ncost-effectiveness, ease of use, non-invasiveness, and insensi-\ntivity to motion artifacts. These advantages facilitate paradigm\ndesign and data acquisition compared to electrocorticography\n(ECOG) and functional magnetic resonance imaging (fMRI)\n[6]\u2013[8]. The Zurich Cognitive Language Processing Corpus\n(ZuCo) was created using EEG [9]. By mapping EEG signals\nduring natural text reading to the semantic and syntactic\nelements of language, text generation based on real-time brain\nactivity-known as EEG-to-Text decoding can be achieved.\nEEG-to-Text decoding has made significant progress, yet it\nremains constrained by limitations in vocabulary size and poor\nsemantic understanding ability caused by a vast EEG-Text\ndomain gap. Early studies [10]\u2013[12] achieved high accuracy\nwith small, closed vocabularies, focusing on recognizing low-\nlevel linguistic features such as individual words or syllables.\nHowever, these methods struggled to capture complex, high-\nlevel sentence and context information, making them unsuit-\nable for open-vocabulary tasks. The development of large\nlanguage models (LLMs) has advanced the field, with models\nlike BART [13] being adapted for EEG-based decoding [14],\n[15]. Wang and Ji were among the first who incorporated\nan EEG encoder with BART by aligning EEG recordings\nwith pre-trained language models [14]. Recent studies have\nexpanded vocabulary size dramatically, from hundreds to tens\nof thousands of words, yet challenges remain in bridging\nthe domain gap between EEG recordings and text since the\npredicted texts are sometimes irrelevant compared with ground\ntruth text. To address this challenge, methods like contrastive\nlearning [16], [17] were applied to enhance the quality of\nEEG-to-Text decoding through cross-modal alignment, which\npulls the representation extracted from the same EEG-Text\npairs together and pushes others apart [18], [19]. However,\ndue to the impact of the noise caused by false negatives (i.e.,\ndata from different EEG-Text pairs that have similar semantic\nmeanings) and the inherent data bias, the performance of\nthese algorithms is still far from expectation. Recent research,\nsuch as MedCLIP [20], tried to mitigate the impact of false\nnegatives using labels such as diagnosed diseases, while in\nEEG-to-Text translation task, no direct labels can be used for\nadditional supervision.\nIn this paper, we propose SEE, a Semantically Aligned\nEEG-to-Text Translation method considering both the inter-\naction between EEG recordings and their corresponding texts\nyet mitigating the influence of false negative EEG-Text pairs.\nSEE consists of two carefully designed modules seamlessly\nembedded into a pre-trained language model BART [13]:\n1) A Cross-Modal Codebook that learns cross-modal shared\nrepresentations during the training period, thus suggesting\nfeature consolidation and modality bias mitigation which helps\nto translate EEG to Text more easily; 2) A Semantic Matching\nmodule which is capable of aligning multi-modal features\nwhile considering the semantic consistency of false negative\npairs by fully exploiting the text representations produced by\npre-trained language model. We test our model on the ZuCo\ndataset, and experimental results show the superiority of our"}, {"title": "II. METHOD", "content": "The overall model structure of our proposed SEE model is\ndepicted in Figure 1. For better leveraging the inherent cross-\nmodal semantic consistency while bridging the gap between\ndifferent modalities for better EEG-to-Text translation, we\ndesign two modules seamlessly embedded into a pre-trained\ntransformer-based [21] language model BART [13] to fully\nharness the prior knowledge of language modeling: 1) A\nCross-Modal Codebook M which stores cross-modal repre-\nsentations for multi-modal retrieval, thus suggesting feature\nenhancement and modality bias mitigation; 2) A Semantic\nMatching module which is capable of aligning multi-modal\nfeatures while considering the semantic consistency (i.e., the\nproblem of false negative pairs). Similar to language modeling\ntasks such as image captioning [22], [23], the EEG-to-Text\ntranslation task can be viewed as maximizing the probability\nof generating texts conditioned on the EEG recording E and\nCodebook \u041c:\n$p(T|E) = \\prod_{t=1}^{l} p(T_t|T_1, T_2, ..., T_{t-1}, f (E), M),$\nwhere T is the target text, l is the text length, and f(.) is our\nproposed model.\nDuring training, a learnable shared Codebook $M\\in\\mathbb{R}^{N_c \\times N_d}$ (where $N_c$ is the size of the Codebook and $N_d$\nis the dimension of representations) is designed for learning\nshared cross-modal information from EEG-Text pairs which\ncan be used for cross-modal feature consolidation, thus making\nit possible that additional information can be queried and inte-\ngrated during inference (when the EEG recordings are the only\navailable input). To achieve that, for each preprocessed ZoCo\nEEG recording E, we use an additional transformer encoder\nto extract EEG features $E_f \\in \\mathbb{R}^{l_E \\times N_d}$ (where $l_E$ is the length\nof a preprocessed EEG recording). As for its corresponding\ntext T, we adopted the pre-trained word embeddings of BART\nmodel: $S = {t_1, t_2, t_3, ..., t_l }$. Then, the cross-modal Codebook\nretrieval process allows representations of different modalities\nto retrieve the Codebook through a cross-attention mechanism\n[21]:\n$Q_E = E_fW_Q, Q_S = SW_Q, K= MW_K,$\n$J_E = \\text{Softmax} \\left(\\frac{Q_EK^T}{\\sqrt{N_d}}\\right), J_S = \\text{Softmax} \\left(\\frac{Q_SK^T}{\\sqrt{N_d}}\\right),$\nwhere $W_Q$ and $W_K$ are projection matrices, $J_E$ and $J_S$\nrepresent the similarity maps of EEG and Text modalities\nfor querying Codebook, respectively. Following the approach\nin [22], [23], for each single-modality representation vector\n$E_f$ or $t_i$, we select the top k elements from the Codebook\nthat have the highest similarity scores with the representation\nvector. The queried multi-modal embeddings $P_E$ and $P_S$ are\nretrieved from the Codebook by computing a weighted average\nwith a Softmax function applied to the similarity scores:\n$P_E = \\text{Softmax}(C^{sim}_E) (M^{retrieved}_E),$\n$P_S = \\text{Softmax}(C^{sim}_S)(M^{retrieved}_S),$\nwhere $M^{retrieved}_E, M^{retrieved}_S, C^{sim}_E,$ and $C^{sim}_S$ represent the\ntop k elements along with their corresponding similarity scores\nfor the EEG and text representations, respectively. To integrate\nthe queried multi-modal embeddings into the text generation\nprocess, we add it to the corresponding modality representa-\ntions, obtaining the fine-grained consolidated features:\n$E' = E_f + P_E, S' = S + P_S.$\nAfter cross-modal Codebook retrieval, we adopted a seman-\ntic matching method for aligning multi-modal representations\nto ensure the consistency of queried embeddings. A shared\nadditional transformer layer $f_{trans}(.)$ along with average\npooling layer $f_{Pool}(.)$ are first utilized to obtain semantic\nrepresentations for EEG and Text modalities, respectively,\nfrom the queried multi-modal embeddings:\n$E_S = f_{Pool}(f_{trans}(P_E)), T_S = f_{Pool}(f_{trans}(P_S)).$\nThen, considering a batch of multi-modal semantic represen-\ntations $E_S\\in\\mathbb{R}^{N_{batch} \\times N_d}$ and $T_S \\in \\mathbb{R}^{N_{batch} \\times N_d}$, we can\ncalculate the cross-modal similarity matrix $I = E_S(T_S)^T$.\nHowever, if we directly adopt contrastive loss [16] to push\nthe semantic representations from the same EEG-Text pairs\ntogether (i.e., maximize the diagonal value of I) and others\napart (i.e., minimize the off-diagonal value of I), huge noise\nwill be introduced because of false negative pairs [20]. Since\nthe raw texts themselves produce enough semantic mean-\nings, they can represent the meaning of their corresponding\nEEG-Text pairs and help recognize false negative pairs. To\nexploit this characteristic, we use a parameter-frozen pre-\ntrained BART encoder, which has learned prior knowledge\nof language modeling, to encode all the texts in a batch\nand leverage average pooling to project each text into $N_d$\ndimension vectors which maintain rich semantic information:\n$\\[Text \\in \\mathbb{R}^{N_{batch} \\times N_d}$. We can get soft labels indicating the\nsemantic similarities of all EEG-Text pairs by calculating:\n$L_{ij} = \\frac{I_{Text}^i (I_{Text}^j)^T}{||I_{Text}^i|| \\times ||I_{Text}^j||},$\nAfter that, we evaluate the soft label matrix $\\hat{I}$ and use a\ndynamic weighting function $D(x) = 1 \u2212 x$ to mask the off-\ndiagonal values that are over a specific threshold a (which is\nset to 0.5 by default):\n$\\tilde{I}_{ij} = \\begin{cases}\nI_{ij}, & \\text{if } \\hat{I}_{ij} < a, \\\\\nD(\\hat{I}_{ij}), & \\text{otherwise}.\n\\end{cases}$\nFinally, similar to MedCLIP [20], a Softmax function is\nadopted on $\\tilde{I}$ to get:\n$\\hat{L}_{ij} = \\frac{\\exp{\\tilde{I}_{ij}}}{\\sum_{j=1}^{batch} \\exp{\\tilde{I}_{ij}}},$\nand the semantic matching loss can be formulated as:\n$L_{semantic\\_matching} = -\\frac{1}{N_{batch}} \\sum_{i=1}^{N_{batch}} \\sum_{j=1}^{N_{batch}} \\hat{I}_{ij} \\log {\\hat{L}_{ij}}.$\nBy doing so, the contribution of those false negatives in the\nloss function is mitigated, and the noise can be minimized.\nFor decoding a single word $s_u$ at time step u, the fine-\ngrained consolidated features $E'$ and $S'$ (of previous time\nsteps) are fed into the BART decoder $f_{decoder}(.)$:\n$s_u = f_{decoder}(E', S_1, S_2, ....S_{u-1}).$\nThen, the report generation loss can be formulated as a\ncross-entropy loss:\n$L_{gen} = -\\frac{1}{l} \\sum_{i=1}^{l} \\sum_{j=1}^{V_a} T_{ij} \\log(S_{ij}),$\nwhere l is the length of the report, $V_a$ is the vocabulary size,\n$T_{ij}$ and $s_{ij}$ are the jth element of ground truth one-hot vector\nof the ith word and the predicted word, respectively.\nCollectively, our model is trained by minimizing the joint\nloss consisting of both report generation loss and semantic\nalignment contrastive loss :\n$L = L_{gen} + L_{semantic\\_matching}.$\nDue to the use of fine-grained consolidated features that\nderived from the Codebook, the multi-modal information\nexchange of the Codebook is maximized, which contributes\nto better optimization."}, {"title": "III. EXPERIMENTS", "content": "For our study, we use the ZuCo 1.0 [9] and ZuCo 2.0 [24]\ndatasets, which provide EEG and eye-tracking data collected\nfrom healthy native English-speaking adults during five dif-\nferent English natural reading tasks. ZuCo 1.0 includes two\nnormal reading tasks (SR v1.0 and NR v1.0) and one task-\nspecific reading task (TSR v1.0). SR v1.0 uses movie reviews\nwith sentimental content, while the other tasks use Wikipedia\ntext. As for ZuCo 2.0, we use only NR v2.0, also based\non Wikipedia. Word-level EEG data were aligned with eye-\ntracking data, following preprocessing steps and dataset splits\nfrom previous work [14]."}, {"title": "IV. CONCLUSION", "content": "In this paper, we presented Semantically Aligned EEG-to-\nText Translation (SEE), a novel approach that addresses the\nchallenges in EEG-to-Text decoding, particularly the cross-\nmodal domain gap and data bias that hinder current meth-\nods. By integrating a cross-modal Codebook and a Semantic\nMatching Module into a pre-trained BART language model,\nSEE enhances cross-modal representation learning and aligns\nmulti-modal features with greater precision, even accounting\nfor noise due to false negatives. Our experiments on the ZuCo\ndataset validate the effectiveness of SEE, showing significant\nimprovements in EEG-to-Text decoding accuracy compared to\nother methods which directly translate EEG recordings to text\nand ignore cross-modal dependency."}]}