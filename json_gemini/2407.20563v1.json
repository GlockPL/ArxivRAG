{"title": "PYRAMID CODER: HIERARCHICAL CODE GENERATOR FOR\nCOMPOSITIONAL VISUAL QUESTION ANSWERING", "authors": ["Ruoyue Shen", "Nakamasa Inoue", "Koichi Shinoda"], "abstract": "Visual question answering (VQA) is the task of providing\naccurate answers to natural language questions based on vi-\nsual input. Programmatic VQA (PVQA) models have been\ngaining attention recently. These use large language models\n(LLMs) to formulate executable programs that address ques-\ntions requiring complex visual reasoning. However, there\nare challenges in enabling LLMs to comprehend the usage of\nimage processing modules and generate relevant code. To over-\ncome these challenges, this paper introduces PyramidCoder, a\nnovel prompting framework for PVQA models. PyramidCoder\nconsists of three hierarchical levels, each serving a distinct\npurpose: query rephrasing, code generation, and answer aggre-\ngation. Notably, PyramidCoder utilizes a single frozen LLM\nand pre-defined prompts at each level, eliminating the need\nfor additional training and ensuring flexibility across various\nLLM architectures. Compared to the state-of-the-art PVQA\nmodel, our approach improves accuracy by at least 0.5% on\nthe GQA dataset, 1.4% on the VQAv2 dataset, and 2.9% on\nthe NLVR2 dataset.\nIndex Terms- Visual question answering, Large lan-\nguage models, Code generation, Prompting methods.", "sections": [{"title": "1. INTRODUCTION", "content": "Visual question answering (VQA), which aims to provide ac-\ncurate answers to natural language questions based on visual\ninput, is a crucial research topic in computer vision and nat-\nural language processing [1, 2, 3]. The last decade has seen\nsignificant advances in deep learning applied to VQA, with\nthe development of end-to-end multimodal models such as\nGLIP [4] and PNP-VQA [5]. However, despite these advances,\ncompositional VQA, which involves complex spatial relation-\nships and object attributes, remains difficult due to the lack\nof an explicit understanding of visual elements during the\ninference process.\nTo address the difficulties in compositional VQA, recent\nstudies [6, 7, 8] have proposed models that generate executable\nprograms to answer questions based on given images, ques-\ntions, and image processing modules. These models, which\nwe refer to as Programmatic VQA (PVQA) models, represent\na novel approach to integrating multimodal inputs, leading\nto more tractable and traceable inference. Typically, PVQA\nmodels consist of three components: a large language model\n(LLM) for code generation, a set of image processing mod-\nules, and a Python executor. The LLM analyzes a given query\n(text-form question) and generates Python code to answer it\nusing a predefined API set of image processing modules, in-\ncluding both low-level modules such as image cropping and\nhigh-level modules such as object detection. The generated\ncode is then executed by the Python executor to produce the\nanswer. This architecture allows questions to be decomposed\ninto manageable segments of code, providing a more precise\napproach to compositional VQA. However, enabling the LLM\nto understand API usage and generate appropriate code for\nVQA is not always straightforward.\nTo fully leverage the capabilities of LLMs, a number of\nprompting frameworks have been developed for natural lan-\nguage processing tasks. Chain-of-Thought (CoT) [9], for\nexample, demonstrates how structured prompts can guide\nLLMs to decompose complex problems into a series of simpler,\nlinked steps. There are also extensions of CoT such as Tree\nof Thoughts [10], which creates a tree structure of thinking\nsteps. These frameworks not only improve performance, but\nalso provide greater transparency into their reasoning process.\nInspired by these approaches, this paper introduces Pyra-\nmidCoder, a novel code generation prompting framework for\nPVQA models. Previous PVQA models [6, 7, 8] employ Input-\nOutput (IO) prompting to generate code from prompt input\ndirectly. However, IO prompting constraints a singular solu-\ntion for a question, resulting in a simplistic and uniform code\ngeneration. In contrast, PyramidCoder explores the diversity of\nquestion statements and possible solutions through a hierarchi-\ncal framework consisting of three levels. As shown in Figure 1,\nthe first level rephrases a given query into multiple variations.\nThe second level then generates multiple code candidates cor-\nresponding to the rephrased queries. Finally, the third level\naggregates them and generates the final code and answer for\nthe input query. All these procedures are implemented with\na single frozen LLM and pre-defined prompts at each level,\nwithout the need for additional training. In experiments, we\nsubstantiate the efficacy of PyramidCoder across GQA [1],\nVQAv2 [2], and NLVR2 [3] datasets, showing its ability to\nsignificantly enhance VQA performance. In summary, our\ncontribution is three-fold:\n1. We introduce PyramidCoder, a novel code generation\nframework for PVQA consisting of three modules: a query\nrephraser, a code generator, and an answer aggregator.\n2. We propose a prompting method to implement all modules\nwith a single frozen LLM to avoid additional training. This\napproach offers flexibility and is not constrained by the\nspecific LLM employed.\n3. We demonstrate the effectiveness of our method over the\nstate-of-the-art CodeVQA [8] model by conducting exper-\niments on three VQA datasets."}, {"title": "2. RELATED WORK", "content": "2.1. Visual Question Answering\nVQA is an interdisciplinary research area combining computer\nvision and natural language processing. It aims to develop AI\nsystems that respond to queries about images by integrating\nmultimodal information. Early VQA models relied on convolu-\ntional neural networks to extract image features and recurrent\nneural networks to process textual inputs [11, 12, 13]. The\nintroduction of attention mechanisms [14, 15, 16, 17, 18] has\nsince improved these models' capacity to handle complex ques-\ntions and comprehend fine-grained image details. Recently,\nmultimodal pretaining approaches [4, 19, 20] have been em-\nployed, leveraging large-scale pretraining on both image and\ntext data to capture richer contextual information.\nHowever, fine-tuning multimodal pre-trained models for\nVQA requires substantial expertise, vast amounts of data, and\nsignificant computational resources. PVQA models [6, 7, 8]\naddress these issues by utilizing an LLM to generate a Python-\nlike code, which is then executed with predefined APIs to find\nanswers to questions. These models can quickly adapt to new\ntasks or domains with minimal data, without the need for addi-\ntional training or fine-tuning. Despite these advantages, PVQA\nmodels using IO prompting display a level of simplicity that\nhampers the effective utilization of LLM capabilities. Conse-\nquently, methods for activating the latent potential of LLMS\nand guiding them towards optimal API utilization remain an\nunderexplored domain.\n2.2. Large Language Models\nCode Generation Models. Large language models have\ndemonstrated remarkable capabilities in a wide range of natu-\nral language processing tasks. While general LLMs find appli-\ncation in various tasks [21, 22], a subset of these models has\nbeen specifically tailored for code generation. This specialized\ncategory of LLMs undergoes training on extensive corpora\nof programming-related text, enabling them to comprehend\nand generate code in response to natural language descriptions.\nCodex [23], an extension of the GPT-3 series, demonstrates\nproficiency in over a dozen languages. StarCoder [24], as one\nof the pioneering open-source Code LLMs, undergoes training\non datasets sourced from Github licensed data, covering over\n80 programming languages. CodeLlama [25], fine-tuned from\nLlama2 with a higher sampling of code, augments support\nfor larger input contexts and zero-shot instruction following\nability. Noteworthy among recent developments are native mul-\ntimodality LLMs such as Gemini [26]. These models leverage\nmultimodal data (i.e., text, images, and code) throughout the\nentirety of the training process to enhance their proficiency in\ngenerating code that meets specific requirements.\nPrompting. Prompting methods are employed to optimize\nprompts for LLMs in order to maximize their capacity. Chain-\nof-Thought (CoT) [9] guides LLMs in generating intermediate\nreasoning steps before predicting the answer. This method\nhas inspired several extensions. Notably, Self-consistency [27]\nsamples from the LLM decoder multiple times and aggregates\nthe final answer through majority voting. AutoCoT [28] auto-\nmatically constructs demonstrations through question cluster-\ning and zero-shot CoT reasoning chain generation. ReAct [29]\nalternately generates thoughts and actions, performs the action\nbased on the thought, and adjusts the thought based on the ac-\ntion result. Similar to CoT prompting and its variations, Tree\nof Thoughts (ToT) [10] decomposes problems into smaller\nthoughts and explores multiple solution paths in parallel, form-\ning a tree structure of thoughts."}, {"title": "3. PROPOSED METHOD", "content": "This section introduces PyramidCoder, a hierarchical code\ngeneration framework for PVQA. As shown in Figure 2, Pyra-\nmidCoder generates a Python code to answer a question given\na query, an image, and a set of pre-defined APIs for simple\nprocessing tasks. The framework consists of three modules: a\nquery rephraser R, a code generator G, and an answer aggre-\ngator A. These three modules are implemented with a single\nfrozen LLM and work in conjunction to enhance the quality\nof code generation within the pyramid structure.\n3.1. Overall Framework\nProblem setting. We follow the notation used in previous\nstudies [6, 8, 30]. Let x \u2208 X be an input image and q \u2208 Q\nbe a textual query, where X is a set of images and Q is a set\nof textual queries. The goal is to generate a code z \u2208 Z that\nreturns the answer a \u2208 A to the query given the image, where\nZ is a set of executable codes and A is a set of answers. This\nstudy assumes that Z is a set of Python codes. Note that the\nthree sets Q, Z and A are subsets of the whole set of texts,\nwhich we denote as T. We use a frozen LLM \u3160 : T \u2192 T to\nfind mappings between these subsets.\nFramework. The PyramidCoder framework consists of two\nstages: the coding stage and the execution stage. The coding\nstage generates a code as\n$$z = \\Pi(q, x),$$\nwhere I : Q \u00d7 X \u2192 Z is the code generation process. In the\nexecution stage, the generated code is executed with the input\nimage to obtain an answer as\n$$\u03b1 = \u039b(x, z),$$\nwhere a is the predicted answer and A : X \u00d7 Z \u2192 A is the\nPython execution engine. This paper studies the design of the\ncode generation process II.\nAlgorithm. The proposed code generation process II is sum-\nmarized in Algorithm 1. It consists of three steps. First, given\nan input query q \u2208 Q, the query rephraser R is applied to\nobtain a set of rephrased queries {ri}1. Here, each ri is a\nrephrased version of q, intended to make coding more effective\nwith a more comprehensive understanding of the input query.\nSecond, the code generator G is applied to each rephrased\nquery. This step produces multiple candidate codes {zij}}-1\nfor each rephrased query ri and pre-executes them to obtain\ncandidate answers aij = A(x, zij). Finally, the answer aggre-\ngator A is applied to the set of candidate code-answer pairs.\nThis step chooses the final answer af and the best code ZT,\nwhere is the index of the chosen pair. As shown in Figure 2,\nPyramidCoder implements all modules with a single frozen\nLLM, and the code generation process is represented by a\npyramid structure of texts (queries and codes) generated by\nthe LLM.\n3.2. Module implementation with a single LLM\nGiven a frozen LLM \u03c0 : T \u2192 T pre-trained on a large text\ndataset comprising both natural language and programming\ncode, we implement the query rephraser R, the code genera-\ntor G and the answer aggregator A with the aforementioned\nLLM without any additional training. Below, we describe the\nmotivation and definition of each module.\n3.2.1. Query Rephraser\nMotivation. The query rephraser module in PyramidCoder\nis designed to enhance the interpretability and robustness of\nthe model's comprehension of input queries. The VQA task\ndemands a precise understanding of diverse query expressions,\nas users may articulate questions in various linguistic forms.\nIt is widely recognized that the input prompt, including input\nqueries, significantly influences the response quality of LLMs.\nThe query rephraser module dynamically reformulates an input\nquery into diverse expressions while preserving the underly-\ning semantics. Consequently, the subsequent code genera-\ntion stage benefits from a more comprehensive and adaptable\nunderstanding of user queries, thereby alleviating potential\nmisinterpretations and improving overall task performance.\nDefinition. We define the query rephraser by\n$$R(q) (pqr + Sqr + q),$$\nwhere q is an input query, pqr is the query rephraser prompt as\nshown in Figure 3a, and sqr is the few-shot examples prompt\nused for rephrasing. The operation + indicates the concatena-\ntion of texts.\n3.2.2. Code Generator\nMotivation. The development of the code generator module is\ndriven by the recognition that complex problem-solving often\ninvolves an inherent diversity and multiplicity of solutions.\nTraditional PVQA models [6, 7, 8] predominantly focus on\ngenerating a singular Python code for a given input query.\nHowever, this conventional methodology overlooks the intrin-\nsic variability in problem-solving strategies and fails to capture\nthe diversity of viable solutions that may lead to the correct\nanswer. By introducing the capability to generate multiple\nPython codes for a single input query, the code generator aims\nto make full use of potential solutions associated with a given\nquery. This approach is grounded in the understanding that\nvarious coding strategies may be employed to achieve the same\ndesired outcome, fostering a more nuanced and adaptable code\ngeneration in complex problem-solving domains.\nDefinition. We define the code generator by\n$$G(r) (pcg + Papi + Scg + r),$$\nwhere r is a rephrased query, Pcg is the code generator prompt\nin Figure 3b, Papi is API reference texts detailed in Figure 4,\nand scg is the few-shot examples prompt for code generation.\n3.2.3. Answer Aggregator\nMotivation. The traditional approach often employs majority\nvoting to aggregate solutions. However, this approach encoun-\nters challenges when failed code execution leads to the default\nanswer being the majority or when multiple answers achieve\nmajority status. In response to these limitations, the answer\naggregator utilizes the capabilities of LLMs to select the final\noutput, considering not only the frequency but also the seman-\ntic compatibility between the query and potential answers. The\nanswer aggregator operates in two steps. First, selecting the\nfinal answer from the candidate answer set, and then selecting\nthe most suitable code that produced this final answer during\npre-execution. This sequential approach minimizes input to-\nken consumption and alleviates potential mismatches between\nanswers and their corresponding code that might be caused by\nhallucinations.\nDefinition. Given a set of candidate answers and codes Z =\n{(zk, ak)}=1, we define the answer aggregator by\n$$A_{ans}(Z) \u3160(p_{aga} + [a_1, a_2,\u2026\u2026,a_\u03c3]),$$\n$$A_{code} (Z_\u03c3) (p_{agc} + [7_{\u03c31}, 7_{\u03c32},\u00b7\u00b7\u00b7, \u2248_{\u03c3\u03c3}]),$$\nwhere paga is the prompt used by the answer aggregator to\nselect the final answer shown in Figure 3c, and page is the\nprompt to choose the best code shown in Figure 3d. The\noperation [] represents the aggregation of elements into a list\nand the conversion to string format. Zo = {2\u03c3\u03b9}}{=1 is a subset\nof Z, where \u03c3 = {\u03c3\u03b9}{=1 is the output of Aans and represents\nthe set of code indices where the pre-execution output for the\ncode matches the selected answer."}, {"title": "4. EXPERIMENTS", "content": "4.1. Experimental Settings\nDatasets and evaluation metrics. Three VQA datasets,\nnamely GQA [1], VQAv2 [2], and NLVR2 [3], are employed\nfor assessment in the conducted experiment. The GQA dataset\nis known for its challenging questions that involve spatial\nreasoning, commonsense reasoning, and an understanding of\nimage details. The VQAv2 dataset comprises images from\nthe COCO dataset paired with open-ended questions. The\nquestions exhibit a range of complexities and topics, and\nanswers are expected to be provided in free-form text. The\nNLVR2 dataset consists of statements about pairs of images,\neach annotated as true or false based on the image content.\nFor our experiments, these statements are reformulated into a\nquery format.\nFollowing the baseline method, 12 in-context examples are\nsampled to construct the in-context prompt for the GQA and\nVQAv2 datasets, while 6 examples are used for the NLVR2\ndataset. To conserve computational resources, for certain\ndatasets, we utilize the same randomly sampled subset as\nthe baseline for evaluation purposes. In the results table, GQA\nval2000 consists of a subset of 2000 samples randomly se-\nlected from the GQA validation set, and VQAv2 val4000 com-\nprises 4000 examples randomly sampled from the VQAv2\nvalidation set. The evaluation metric is the exact match accu-\nracy for case-insensitive answers.\n4.2. Implementation Details\nFrozen LLM \u03c0. PyramidCoder exhibits insensitivity to the\nunderlying base LLM model \u03c0. In our evaluation, we employ\none closed general LLM, namely, GPT-3.5 (gpt-3.5-turbo)\navailable through the OpenAI API, and two open-source Code\nLLMs: StarCoderBase [24] and CodeLlama-7b-Python [25].\nFramework and APIs. We validate the efficacy of our\napproach by employing the state-of-the-art PVQA model,\nCodeVQA [8], as our baseline framework. Utilizing identical\nimage processing modules as those employed in CodeVQA,\nwe use GroundingDINO[31] for object detection through\nget_object_boxes(). For image captioning, we utilize\nBLIP [32] within query(). Additionally, the question-\nanswering module incorporated within query() employs\nthe same LLM as the one used in the code generation stage.\n4.3. Results and Analysis\nVQA performance. Table 1 lists the performance across three\nVQA datasets. Our baseline is the state-of-the-art CodeVQA\nmodel, which uses default IO prompting. The same settings\nand APIs are utilized for all experiments. Compared with the\nbaseline, PyramidCoder demonstrates relative improvements\nof a minimum of 1.65 points on the GQA val2000 set, 0.5\npoints on the GQA test set, 1.4 points on the VQAv2 val4000\nset, and 2.9 points on the NLVR2 test set. These results indi-\ncate the effectiveness of PyramidCoder, which maintains its\nperformance across a variety of underlying LLMs and datasets.\nPrompting Methods. Table 2 provides a comparative analysis\nof our PyramidCoder against existing prompting methodolo-\ngies, namely few-shot CoT and few-shot ToT with a creative\nwriting setup. All methods employ identical in-context exam-\nples and few-shot configurations as described earlier. While\nthe preceding prompting techniques show improvements in\ncomparison to default IO prompting, they tend to generate\nhomogeneous codes. In contrast, PyramidCoder surpasses\nthese methods by producing diverse code candidates, which\nprovides more substantial reasoning evidence.\nAnalysis by question type. Figure 5 illustrates the QA accu-\nracy across diverse question types within the GQA val2000\ndataset. The improvements are particularly evident in ad-\ndressing logical questions involving logical inference, choose\nquestions characterized by the inclusion of alternative options\nwithin the query, and compare questions requiring compar-\nisons between two or more objects. These findings reveal the\nmodel's proficiency in handling queries necessitating intricate\nreasoning and comprehension of visual content. Moreover,\nPyramidCoder demonstrates heightened performance even in\naddressing simpler question types, such as verify questions\nthat require binary responses (yes/no) and queries involving\nopen-ended questions. This comprehensive enhancement un-\nderscores the versatility and efficacy of the PyramidCoder in\ndiverse question-answering scenarios.\nLLM. Table 3 shows the performance of PyramidCoder using\ndifferent LLMs on the GQA val2000 dataset. Besides the re-\nsults listed in Table 1, we also include results using GPT-3.5,\na private general LLM, for code generation to measure its\nperformance. The results summarized in Table 3 demonstrate\nthat PyramidCoder consistently improves performance across\ndiverse LLMs. This consistency highlights that PyramidCoder\nis independent of the employed LLM, as stronger LLMs usu-\nally lead to better performance. Remarkably, PyramidCoder\nexhibits versatility by performing well with both closed and\nopen-source LLMs, as well as with models designed for gen-\neral and code-specific tasks.\nAblation study. The ablation study is conducted on the sam-\npled GQA and VQAv2 datasets, and the results are summa-\nrized in Table 4. The first row shows the performance of the\ncomplete PyramidCoder. To show the significance of each\ncomponent, the query rephraser, code generator, and answer\naggregator are individually omitted from the whole framework.\nThe results clearly indicate that the exclusion of any of these\nmodules leads to a noticeable decrease in the VQA accuracy\nacross both datasets.\nQualitative examples. Figure 6 illustrates the diversified code\ngeneration capabilities of PyramidCoder. When confronted\nwith queries of distinct content, the generated code exhibits\nvarying levels of complexity and employs different APIs to\nconstruct disparate code solutions."}, {"title": "5. CONCLUSION", "content": "In this paper, we proposed PyramidCoder, a prompting frame-\nwork designed for compositional visual question answering.\nThis framework operates through query rephrasing, code can-\ndidate generation, and answer aggregation facilitated by three\ndistinct modules. A key feature of this framework is its use of a\nsingle frozen LLM and predefined prompts at each level, elim-\ninating the need for additional training and ensuring flexibility\nacross different LLM architectures. Experimental evaluations\nconducted on three VQA datasets demonstrate its efficacy."}]}