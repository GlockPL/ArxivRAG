{"title": "PYRAMID CODER: HIERARCHICAL CODE GENERATOR FOR COMPOSITIONAL VISUAL QUESTION ANSWERING", "authors": ["Ruoyue Shen", "Nakamasa Inoue", "Koichi Shinoda"], "abstract": "Visual question answering (VQA) is the task of providing accurate answers to natural language questions based on vi- sual input. Programmatic VQA (PVQA) models have been gaining attention recently. These use large language models (LLMs) to formulate executable programs that address ques- tions requiring complex visual reasoning. However, there are challenges in enabling LLMs to comprehend the usage of image processing modules and generate relevant code. To over- come these challenges, this paper introduces PyramidCoder, a novel prompting framework for PVQA models. PyramidCoder consists of three hierarchical levels, each serving a distinct purpose: query rephrasing, code generation, and answer aggre- gation. Notably, PyramidCoder utilizes a single frozen LLM and pre-defined prompts at each level, eliminating the need for additional training and ensuring flexibility across various LLM architectures. Compared to the state-of-the-art PVQA model, our approach improves accuracy by at least 0.5% on the GQA dataset, 1.4% on the VQAv2 dataset, and 2.9% on the NLVR2 dataset.", "sections": [{"title": "1. INTRODUCTION", "content": "Visual question answering (VQA), which aims to provide ac- curate answers to natural language questions based on visual input, is a crucial research topic in computer vision and nat- ural language processing [1, 2, 3]. The last decade has seen significant advances in deep learning applied to VQA, with the development of end-to-end multimodal models such as GLIP [4] and PNP-VQA [5]. However, despite these advances, compositional VQA, which involves complex spatial relation- ships and object attributes, remains difficult due to the lack of an explicit understanding of visual elements during the inference process.\nTo address the difficulties in compositional VQA, recent studies [6, 7, 8] have proposed models that generate executable programs to answer questions based on given images, ques- tions, and image processing modules. These models, which we refer to as Programmatic VQA (PVQA) models, represent a novel approach to integrating multimodal inputs, leading to more tractable and traceable inference. Typically, PVQA models consist of three components: a large language model (LLM) for code generation, a set of image processing mod- ules, and a Python executor. The LLM analyzes a given query (text-form question) and generates Python code to answer it using a predefined API set of image processing modules, in- cluding both low-level modules such as image cropping and high-level modules such as object detection. The generated code is then executed by the Python executor to produce the answer. This architecture allows questions to be decomposed into manageable segments of code, providing a more precise approach to compositional VQA. However, enabling the LLM to understand API usage and generate appropriate code for VQA is not always straightforward.\nTo fully leverage the capabilities of LLMs, a number of prompting frameworks have been developed for natural lan- guage processing tasks. Chain-of-Thought (CoT) [9], for example, demonstrates how structured prompts can guide LLMs to decompose complex problems into a series of simpler,"}, {"title": "2. RELATED WORK", "content": "2.1. Visual Question Answering\nVQA is an interdisciplinary research area combining computer vision and natural language processing. It aims to develop AI systems that respond to queries about images by integrating multimodal information. Early VQA models relied on convolu- tional neural networks to extract image features and recurrent neural networks to process textual inputs [11, 12, 13]. The introduction of attention mechanisms [14, 15, 16, 17, 18] has since improved these models' capacity to handle complex ques- tions and comprehend fine-grained image details. Recently, multimodal pretaining approaches [4, 19, 20] have been em- ployed, leveraging large-scale pretraining on both image and text data to capture richer contextual information.\nHowever, fine-tuning multimodal pre-trained models for VQA requires substantial expertise, vast amounts of data, and significant computational resources. PVQA models [6, 7, 8] address these issues by utilizing an LLM to generate a Python- like code, which is then executed with predefined APIs to find answers to questions. These models can quickly adapt to new tasks or domains with minimal data, without the need for addi- tional training or fine-tuning. Despite these advantages, PVQA models using IO prompting display a level of simplicity that hampers the effective utilization of LLM capabilities. Conse- quently, methods for activating the latent potential of LLMS and guiding them towards optimal API utilization remain an underexplored domain.\n2.2. Large Language Models\nCode Generation Models. Large language models have demonstrated remarkable capabilities in a wide range of natu- ral language processing tasks. While general LLMs find appli- cation in various tasks [21, 22], a subset of these models has been specifically tailored for code generation. This specialized category of LLMs undergoes training on extensive corpora of programming-related text, enabling them to comprehend and generate code in response to natural language descriptions. Codex [23], an extension of the GPT-3 series, demonstrates proficiency in over a dozen languages. StarCoder [24], as one of the pioneering open-source Code LLMs, undergoes training on datasets sourced from Github licensed data, covering over 80 programming languages. CodeLlama [25], fine-tuned from Llama2 with a higher sampling of code, augments support for larger input contexts and zero-shot instruction following ability. Noteworthy among recent developments are native mul- timodality LLMs such as Gemini [26]. These models leverage multimodal data (i.e., text, images, and code) throughout the entirety of the training process to enhance their proficiency in generating code that meets specific requirements.\nPrompting. Prompting methods are employed to optimize prompts for LLMs in order to maximize their capacity. Chain- of-Thought (CoT) [9] guides LLMs in generating intermediate reasoning steps before predicting the answer. This method has inspired several extensions. Notably, Self-consistency [27] samples from the LLM decoder multiple times and aggregates the final answer through majority voting. AutoCoT [28] auto- matically constructs demonstrations through question cluster- ing and zero-shot CoT reasoning chain generation. ReAct [29] alternately generates thoughts and actions, performs the action based on the thought, and adjusts the thought based on the ac- tion result. Similar to CoT prompting and its variations, Tree of Thoughts (ToT) [10] decomposes problems into smaller thoughts and explores multiple solution paths in parallel, form- ing a tree structure of thoughts."}, {"title": "3. PROPOSED METHOD", "content": "This section introduces PyramidCoder, a hierarchical code generation framework for PVQA. As shown in Figure 2, Pyra- midCoder generates a Python code to answer a question given a query, an image, and a set of pre-defined APIs for simple processing tasks. The framework consists of three modules: a query rephraser R, a code generator G, and an answer aggre- gator A. These three modules are implemented with a single frozen LLM and work in conjunction to enhance the quality of code generation within the pyramid structure.\n3.1. Overall Framework\nProblem setting. We follow the notation used in previous studies [6, 8, 30]. Let x \u2208 X be an input image and q \u2208 Q be a textual query, where X is a set of images and Q is a set of textual queries. The goal is to generate a code z \u2208 Z that returns the answer a \u2208 A to the query given the image, where Z is a set of executable codes and A is a set of answers. This study assumes that Z is a set of Python codes. Note that the three sets Q, Z and A are subsets of the whole set of texts, which we denote as T. We use a frozen LLM \u3160 : T \u2192 T to find mappings between these subsets.\nFramework. The PyramidCoder framework consists of two stages: the coding stage and the execution stage. The coding stage generates a code as\n$$z = \\Pi(q, x),$$\nwhere I : Q \u00d7 X \u2192 Z is the code generation process. In the execution stage, the generated code is executed with the input image to obtain an answer as\n$$\u03b1 = \u039b(x, z),$$\nwhere a is the predicted answer and A : X \u00d7 Z \u2192 A is the Python execution engine. This paper studies the design of the code generation process II.\nAlgorithm. The proposed code generation process II is sum- marized in Algorithm 1. It consists of three steps. First, given an input query q \u2208 Q, the query rephraser R is applied to obtain a set of rephrased queries {ri}1. Here, each ri is a rephrased version of q, intended to make coding more effective with a more comprehensive understanding of the input query. Second, the code generator G is applied to each rephrased query. This step produces multiple candidate codes {zij}j=1 for each rephrased query ri and pre-executes them to obtain candidate answers aij = A(x, zij). Finally, the answer aggre- gator A is applied to the set of candidate code-answer pairs. This step chooses the final answer af and the best code ZT, where is the index of the chosen pair. As shown in Figure 2, PyramidCoder implements all modules with a single frozen LLM, and the code generation process is represented by a pyramid structure of texts (queries and codes) generated by the LLM.\n3.2. Module implementation with a single LLM\nGiven a frozen LLM \u03c0 : T \u2192 T pre-trained on a large text dataset comprising both natural language and programming code, we implement the query rephraser R, the code genera- tor G and the answer aggregator A with the aforementioned LLM without any additional training. Below, we describe the motivation and definition of each module.\n3.2.1. Query Rephraser\nMotivation. The query rephraser module in PyramidCoder is designed to enhance the interpretability and robustness of the model's comprehension of input queries. The VQA task demands a precise understanding of diverse query expressions,"}, {"title": "3.2.2. Code Generator", "content": "Motivation. The development of the code generator module is driven by the recognition that complex problem-solving often involves an inherent diversity and multiplicity of solutions. Traditional PVQA models [6, 7, 8] predominantly focus on generating a singular Python code for a given input query. However, this conventional methodology overlooks the intrin- sic variability in problem-solving strategies and fails to capture the diversity of viable solutions that may lead to the correct answer. By introducing the capability to generate multiple Python codes for a single input query, the code generator aims to make full use of potential solutions associated with a given query. This approach is grounded in the understanding that various coding strategies may be employed to achieve the same desired outcome, fostering a more nuanced and adaptable code generation in complex problem-solving domains.\nDefinition. We define the code generator by\n$$G(r) = \u03c0(pcg + Papi + Scg + r),$$\nwhere r is a rephrased query, Pcg is the code generator prompt in Figure 3b, Papi is API reference texts detailed in Figure 4, and scg is the few-shot examples prompt for code generation.\n3.2.3. Answer Aggregator\nMotivation. The traditional approach often employs majority voting to aggregate solutions. However, this approach encoun- ters challenges when failed code execution leads to the default answer being the majority or when multiple answers achieve majority status. In response to these limitations, the answer aggregator utilizes the capabilities of LLMs to select the final output, considering not only the frequency but also the seman- tic compatibility between the query and potential answers. The answer aggregator operates in two steps. First, selecting the final answer from the candidate answer set, and then selecting the most suitable code that produced this final answer during pre-execution. This sequential approach minimizes input to- ken consumption and alleviates potential mismatches between answers and their corresponding code that might be caused by hallucinations.\nDefinition. Given a set of candidate answers and codes Z ="}, {"title": "4. EXPERIMENTS", "content": "4.1. Experimental Settings\nDatasets and evaluation metrics. Three VQA datasets, namely GQA [1], VQAv2 [2], and NLVR2 [3], are employed for assessment in the conducted experiment. The GQA dataset is known for its challenging questions that involve spatial reasoning, commonsense reasoning, and an understanding of image details. The VQAv2 dataset comprises images from the COCO dataset paired with open-ended questions. The questions exhibit a range of complexities and topics, and answers are expected to be provided in free-form text. The NLVR2 dataset consists of statements about pairs of images, each annotated as true or false based on the image content. For our experiments, these statements are reformulated into a query format.\nFollowing the baseline method, 12 in-context examples are sampled to construct the in-context prompt for the GQA and VQAv2 datasets, while 6 examples are used for the NLVR2 dataset. To conserve computational resources, for certain datasets, we utilize the same randomly sampled subset as the baseline for evaluation purposes. In the results table, GQA val2000 consists of a subset of 2000 samples randomly se- lected from the GQA validation set, and VQAv2 val4000 com- prises 4000 examples randomly sampled from the VQAv2 validation set. The evaluation metric is the exact match accu- racy for case-insensitive answers.\n4.2. Implementation Details\nFrozen LLM \u03c0. PyramidCoder exhibits insensitivity to the underlying base LLM model \u03c0. In our evaluation, we employ one closed general LLM, namely, GPT-3.5 (gpt-3.5-turbo) available through the OpenAI API, and two open-source Code LLMs: StarCoderBase [24] and CodeLlama-7b-Python [25].\nFramework and APIs. We validate the efficacy of our approach by employing the state-of-the-art PVQA model, CodeVQA [8], as our baseline framework. Utilizing identical image processing modules as those employed in CodeVQA, we use GroundingDINO[31] for object detection through get_object_boxes(). For image captioning, we utilize BLIP [32] within query(). Additionally, the question- answering module incorporated within query() employs the same LLM as the one used in the code generation stage.\n4.3. Results and Analysis\nVQA performance. Table 1 lists the performance across three VQA datasets. Our baseline is the state-of-the-art CodeVQA model, which uses default IO prompting. The same settings and APIs are utilized for all experiments. Compared with the baseline, PyramidCoder demonstrates relative improvements of a minimum of 1.65 points on the GQA val2000 set, 0.5 points on the GQA test set, 1.4 points on the VQAv2 val4000 set, and 2.9 points on the NLVR2 test set. These results indi- cate the effectiveness of PyramidCoder, which maintains its performance across a variety of underlying LLMs and datasets.\nPrompting Methods. Table 2 provides a comparative analysis of our PyramidCoder against existing prompting methodolo- gies, namely few-shot CoT and few-shot ToT with a creative writing setup. All methods employ identical in-context exam- ples and few-shot configurations as described earlier. While the preceding prompting techniques show improvements in comparison to default IO prompting, they tend to generate homogeneous codes. In contrast, PyramidCoder surpasses these methods by producing diverse code candidates, which provides more substantial reasoning evidence.\nAnalysis by question type. Figure 5 illustrates the QA accu- racy across diverse question types within the GQA val2000 dataset. The improvements are particularly evident in ad- dressing logical questions involving logical inference, choose questions characterized by the inclusion of alternative options within the query, and compare questions requiring compar- isons between two or more objects. These findings reveal the model's proficiency in handling queries necessitating intricate reasoning and comprehension of visual content. Moreover, PyramidCoder demonstrates heightened performance even in addressing simpler question types, such as verify questions that require binary responses (yes/no) and queries involving open-ended questions. This comprehensive enhancement un- derscores the versatility and efficacy of the PyramidCoder in diverse question-answering scenarios.\nLLM. Table 3 shows the performance of PyramidCoder using different LLMs on the GQA val2000 dataset. Besides the re- sults listed in Table 1, we also include results using GPT-3.5, a private general LLM, for code generation to measure its performance. The results summarized in Table 3 demonstrate that PyramidCoder consistently improves performance across diverse LLMs. This consistency highlights that PyramidCoder is independent of the employed LLM, as stronger LLMs usu- ally lead to better performance. Remarkably, PyramidCoder exhibits versatility by performing well with both closed and open-source LLMs, as well as with models designed for gen- eral and code-specific tasks.\nAblation study. The ablation study is conducted on the sam- pled GQA and VQAv2 datasets, and the results are summa- rized in Table 4. The first row shows the performance of the complete PyramidCoder. To show the significance of each component, the query rephraser, code generator, and answer aggregator are individually omitted from the whole framework. The results clearly indicate that the exclusion of any of these modules leads to a noticeable decrease in the VQA accuracy across both datasets.\nQualitative examples. Figure 6 illustrates the diversified code generation capabilities of PyramidCoder. When confronted with queries of distinct content, the generated code exhibits varying levels of complexity and employs different APIs to construct disparate code solutions."}, {"title": "5. CONCLUSION", "content": "In this paper, we proposed PyramidCoder, a prompting frame- work designed for compositional visual question answering. This framework operates through query rephrasing, code can- didate generation, and answer aggregation facilitated by three distinct modules. A key feature of this framework is its use of a single frozen LLM and predefined prompts at each level, elim- inating the need for additional training and ensuring flexibility across different LLM architectures. Experimental evaluations conducted on three VQA datasets demonstrate its efficacy."}]}