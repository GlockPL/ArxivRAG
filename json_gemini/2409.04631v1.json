{"title": "Zero-Shot Whole Slide Image Retrieval in Histopathology Using Embeddings of Foundation Models", "authors": ["Saghir Alfasly", "Peyman Nejat", "Ghazal Alabtah", "Sobhan Hemati", "Krishna Rani Kalari", "H.R. Tizhoosh"], "abstract": "We have tested recently published foundation models for histopathology for image retrieval. We report macro average of F1 score for top-1 retrieval, majority of top-3 retrievals, and majority of top-5 retrievals. We perform zero-shot retrievals, i.e., we do not alter embeddings and we do not train any classifier. As test data, we used diagnostic slides of TCGA, The Cancer Genome Atlas, consisting of 23 organs and 117 cancer subtypes. As a search platform we used Yottixel that enabled us to perform WSI search using patches. Achieved F1 scores show low performance, e.g., for top-5 retrievals, 27% \u00b1 13% (Yottixel-DenseNet), 42% \u00b1 14% (Yottixel-UNI), 40%\u00b113% (Yottixel-Virchow), and 41%\u00b113% (Yottixel-GigaPath). The results for GigaPath WSI will be delayed due to the significant computational resources required for processing", "sections": [{"title": "Introduction", "content": "The emergence of digital pathology and artificial intelligence (AI) seems to bring about an eminent revolution in the field of histopathology, with anticipated efficiency and accuracy at unprecedented levels for diagnostic and treatment planning of many diseases. Digital pathology offers high-resolution tissue images that can be processed and analyzed by advanced computer algorithms. This shift could enable pathologists to leverage the power of deep learning models to identify tissue patterns, classify diseases, and predict patient outcomes with greater precision. In this context, image retrieval may also play a critical role in enhancing diagnostic workflows, provided we can finally close the semantic gap after three decades of research.\nImage search and retrieval in histopathology is particularly significant as it allows pathologists to search databases - small or large - of digital slides to find visually, i.e., anatomically, similar cases. This capability is invaluable for comparative diagnosis, where finding similar cases can aid in confirming or refining a diagnosis. However, the effectiveness of image retrieval depends, among others, on the quality of embeddings produced by deep neural networks. High-quality embeddings ensure that similar images are clustered closely together in the feature space, leading to more accurate retrieval results. Conversely, poor-quality embeddings can result in misclassification and retrieval of irrelevant cases, potentially compromising diagnostic accuracy.\nThe advent of foundation models, large-scale Al models pre-trained on vast amounts of data, holds great promise in addressing the long-standing semantic gap problem in image search and retrieval for histopathology [Bommasani2021]. The semantic gap refers to the disparity between the high-level concepts understood by human experts and the low-level features captured by machines. Foundation models, with their expected ability to generalize across diverse tasks and domains, could bridge this gap by learning more robust and semantically meaningful representations of medical images. This development offers hope that Al-driven image retrieval will become more accurate and reliable, ultimately leading to better patient outcomes and more efficient healthcare systems."}, {"title": "Results", "content": "Table 1 shows the overall results for all WSI retrievals. Total average and standard deviation, and 95% confidence intervals of F1 scores across all organs and subtypes for WSI retrievals for all Yottixel variations and GigaPath are reported.\nTable 2, Table 3 and Table 4 show the detailed results for top-1, majority of top-3 and majority of top-5 accuracy, respectively. All models improve the results compared to DenseNet (default configuration of Yottixel). As DenseNet is a much smaller network trained with ImageNet natural images, these improvements are expected. However, with the exception of kidney, all values are quite low."}, {"title": "Methods", "content": "Evaluation - We used macro average of F1-score for top-1, majority of top-3, and majority of top-5 WSI retrievals. As the data is imbalanced, using macro average is a rigorous evaluation.\nSearch Engine - To evaluate the embeddings of models in a zero-shot manner, we needed to choose a dependable search platform that met several essential criteria. First, the platform should allow the integration of a new model into its structure without disrupting its overall design and requiring minimal additional ablation studies. Second, it should have an unsupervised patching algorithm. Third, the platform should be capable of performing patch searches. Fourth, it should support whole-slide image (WSI) searches by utilizing all selected patches. Lastly, the platform should have a proven track record of storage efficiency and search speed, and overall higher accuracy compared to other search engines. After considering these factors, we selected Yottixel for our testing [Kalra2020a, Tizhoosh2024, Lahr2024]. As Figure 1 illustrates, Yottixel topology can easily integrate any deep model.\nData - We used The Cancer Genome Atlas diagnostic slides consisting of 11,444 WSIs of 9,339 patients covering 23 organs and 117 cancer subtypes. Tables TA1 to TA23 in the Appendix provide details of all subtypes for each organ, number of patients and whole slide images. TCGA Projects (e.g., lung adenocarcinoma, LUAD, and lung squamous cell carcinoma, LUSC) are very generic terms, using large buckets for classification. For example, in lung Adenocarcinoma and Squamous Cell Carcinoma cover about 98% of cases; however, the subtypes are incredibly helpful to understand biology and prognosis\u2014and ultimately treatment. For instance, bronchio- alveolar adenocarcinoma (now called lepidic) is typically treated successfully with surgery alone. Adenocarcinoma with mixed subtypes is more likely to be larger, present with metastases, and show worsened clinical outcomes. Most studies have rather used the generic tumor types and not detailed subtyping for retrieval (e.g., see [Kalra2020b]).\nPatching WSIs \u2013 We used the Yottixel patching method, which consists of two unsupervised stages. In the first stage, we segment the whole slide image (WSI) into different regions based on color composition using k-means clustering. We do this to capture the pattern variability within the WSI from a computer vision perspective, separating regions like blood stains, muscle tissue, fat, etc. Based on my visual inspection of different WSIs, we typically set the number of color clusters to 9 [Kalra2020a]. In the second stage, we select a small percentage of representative patches from each color-segmented region while preserving spatial diversity. We accomplish this using k-means clustering again, but this time we group patches based on their location within each color region. Unlike the default settings in the Yottixel paper that uses 1024x1024 patches, we output patches in 224x224 pixel dimensions to accommodate the foundation models, and we use 2% of the representative patches instead of 5%. The result is a \"mosaic\" of patches that represent the entire WSI in a compact manner. This mosaic dramatically reduces the computational burden compared to processing the entire gigapixel WSI, while still capturing the key visual patterns and structures present in the slide.\nModels for Search Engine\nUNI is a self-supervised vision encoder based on a large Vision Transformer (ViT-L) architecture [Chen2024]. It was pretrained on the Mass-100K dataset, which comprises over 100 million tissue patches extracted from 100,426 H&E whole-slide images (WSIs) spanning 20 major tissue types. The pretraining process utilized the DINOv2 self-supervised learning approach, which employs student-teacher knowledge distillation and masked image modeling. UNI's training involved two main loss objectives: self-distillation loss and masked image modeling loss. The model was developed using a total of 100,130,900 images at various resolutions, including 75,832,905 images at 256x256 pixels and 24,297,995 images at 512x512 pixels, all at 20x magnification.\nVirchow is a large-scale self-supervised vision model for computational pathology [Vorontsov2024]. It is based on a Vision Transformer (ViT) architecture, specifically using the ViT-H/14 (ViT-Huge) configuration with 632 million parameters. Virchow was pretrained on a dataset of approximately 1.5 million H&E stained whole-slide images (WSIs) from about 100,000 patients, spanning 17 high-level tissue types. The pretraining data includes both cancerous and benign tissues, collected via biopsy (63%) and resection (37%). Similar to UNI, the model was trained using the DINO v.2 self-supervised learning framework. Virchow processes 224x224 pixel tissue tiles extracted from WSIs at 20x magnification (0.5 microns per pixel). The training involved approximately 2 billion tiles sampled from a pool of about 13 billion available tissue tiles.\nGigaPath is a vision transformer architecture for pretraining large pathology foundation models on gigapixel whole-slide images (WSIs) [Xu2024]. It is pretrained on an extensive dataset comprising 1.3 billion 256x256 image tiles extracted from 171,189 pathology slides sourced from over 30,000 patients across 28 cancer centers in the Providence health network. This dataset encompasses 31 major tissue types, significantly surpassing the scale of previous datasets like TCGA. GigaPath employs a two-stage pretraining approach, beginning with image-level self-supervised learning using DINOv2 and progressing to whole-slide-level self-supervised learning through a masked autoencoder combined with the LongNet architecture. This adaptation allows for effective ultra-large-context modeling, capturing both local and global patterns across the entire slide. GigaPath's architecture enables it to process tens of thousands of image tiles, resulting in state-of-the-art performance across various digital pathology tasks."}, {"title": "Discussions", "content": "Accurately subtyping cancerous tissue remains a significant challenge in diagnostic pathology. The potential of image retrieval techniques as tools for aiding diagnosis, through similarity matching with evidently diagnosed past cases, has garnered attention in recent years. Foundation models trained on vast collections of whole slide images (WSIs), expected to deliver expressive and semantically correct embeddings, have generated optimism about the feasibility of clinical-grade image retrieval systems. However, findings from this validation study reveal that there is still considerable progress to be made before these models can be effectively utilized in clinical settings. The study reported an average F1 score of approximately 44% for zero-shot WSI retrieval tasks, indicating that the current performance of these models is not yet adequate for clinical application.\nIt is important to contextualize these findings by considering the design and intended use of the foundation models, such as UNI, Virchow, and patch-based backbone of GigaPath, which were primarily developed for patch-level analysis and representation learning, not for WSI-level retrieval tasks. As such, their performance in this study should be seen as an exploration of their potential in a new context rather than a definitive assessment of their value to the field. The WSI-to-WSI matching methodology employed by the Yottixel search engine, which patches WSIs and applies a median-of-minimum strategy, may influence the observed performance. However, literature does not provide any other patch-based scheme for WSI-to-WSI comparison.\nThe challenge of representing an entire WSI with a single vector remains unresolved in computational pathology. The aggregation technique used in the GigaPath-WSI model shows some improvements, but we were not able to analyze all results. The results for GigaPath WSI will be delayed due to the significant computational resources required for processing.\nHardware constraints currently necessitate the use of patch-based approaches and aggregation methods for WSI analysis. As technology advances and Al models grow in their capability to process larger inputs, it may become feasible to process entire WSIs in a single pass, potentially enabling more holistic representation learning approaches.\nWhile the performance of foundation models in zero-shot WSI retrieval does not yet meet clinical standards, these models may indeed represent significant advancements in patch-level representation learning for histopathology. The challenges identified in this study present opportunities for future research, particularly in WSI-level representation learning, novel aggregation techniques, and retrieval methodologies optimized for histopathology. Significant refinements and alternative strategies are still needed to achieve reliable and accurate cancer tissue subtyping through image search and retrieval."}]}