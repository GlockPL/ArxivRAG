{"title": "Kolmogorov-Arnold Networks for metal surface defect classification", "authors": ["Maciej Krzywda", "Mariusz Wermi\u0144ski", "Szymon \u0141ukasik", "Amir H. Gandomi"], "abstract": "This paper presents the application of Kolmogorov-Arnold Networks (KAN) in classifying metal surface defects. Specifically, steel surfaces are analyzed to detect defects such as cracks, inclusions, patches, pitted surfaces, and scratches. Drawing on the Kolmogorov-Arnold theorem, KAN provides a novel approach compared to conventional multilayer perceptrons (MLPs), facilitating more efficient function approximation by utilizing spline functions. The results show that KAN networks can achieve better accuracy than convolutional neural networks (CNNs) with fewer parameters, resulting in faster convergence and improved performance in image classification.", "sections": [{"title": "1. Introduction", "content": "The field of deep learning is rapidly evolving, with continuous advancements in neural network architectures significantly contributing to progress in the image classification field [1,2,3]. Convolutional Neural Networks (CNNs) have become a cornerstone in analyzing multidimensional data, such as images, due to their capability to automatically extract meaningful features from raw data. In recent years, there has been a growing integration of advanced mathematical theories into deep learning architectures[4], enhancing neural networks' ability to process complex data structures. Among the promising alternatives to traditional Multilayer Perceptron (MLPs), Kolmogorov-Arnold Networks (KANs) leverage the Kolmogorov-Arnold theorem and utilize splinefunction as a key element of their architecture. Inlight of these"}, {"title": "2. Kolmogorov-Arnold Networks", "content": "KANs are novel of neural network architecture based on the Kolmogorov-Arnold representation theorem [6,7]. They provide an alternative way to approximate functions by using learnable, parameterized univariate functions as activation functions rather than fixed ones typically used in MLPs. The Kolmogorov-Arnold theorem, a significant result in mathematical analysis, states that any continuous multivariate function can be expressed as a finite sum of continuous univariate functions. This gives KAN architectures a solid theoretical foundation, suggesting that complex multivariate functions can be broken down into simpler, univariate components."}, {"title": "3. Comparision of KAN and MLP", "content": "The advantages of spline functions (B-spline) over traditional activation functions in neural networks, particularly in the context of Kolmogorov-Arnold Networks (KAN), are quite distinctive.In particular, KANs offer significant differences in neural network construction that allow for greater flexibility and interpretability of models. Major differences between MLP and B-spline based networks are:\n\u2022 Better representation of local dependencies: Spline functions allow precise fitting to local data due to their structure, enabling accurate modeling of univariate functions, as described in the Kolmogorov-Arnold representation theories. Traditional activation functions like ReLU or Sigmoid lack this flexibility in local fitting, which can cause difficulties in accurately modeling complex data structures, especially with a small number of parameters [6,8]\n\u2022 Reduction of the curse of dimensionality: KANs utilize spline functions that effectively address the challenge of high-dimensional data, making these models more scalable in solving regression tasks. In contrast, MLPs, particularly those using classic activation functions, can struggle with this issue[8,9]\n\u2022 Adaptability of the B-spline grid: The spline functions used in KANs are parameterized as adaptable grids. This means that KANs are able to adjust themselves to changing input data during the learning process, whereas traditional activation functions in MLPS remain static and do not offer this level of flexibility[6]\n\u2022 Better performance with limited data: Thanks to spline functions, KANs can provide better results with smaller datasets, as their structure is more efficient at uncovering internal data relationships. MLPs with traditional activation functions, such as ReLU, may require significantly larger datasets to achieve comparable performance [6,8].\nIn practice, KANs apply this theorem[6,7] by constructing neural networks that learn these univariate functions. The architecture usually consists of an input transformation layer, where each input variable passes through a learnable univariate function. This is followed by a summation layer that aggregates the outputs of these functions, creating intermediate values. Subsequently, the output layerprocesses the sum through additional learnable univariate functions, and the final outputs are produced by summing their results. Essentially, KAN simplifies the task of approximating a complex function by breaking it down into smaller, manageable parts that are adjusted and learned during training.\nIn contrast, MLPs use fixed functions like ReLU, sigmoid, or tanh, which stay the same throughout the entire training process[10,11]. This rigidity can sometimes limit the MLP's ability to model more complex relationships. In terms of approximation power, KANs are theoretically capable of representing any continuous function on a compact domain, just like MLPs, which are known to be universal approximators when given enough width and"}, {"title": "4. Experiment settings", "content": "In our experiment, we decided to use two datasets of surface metal defects:\n\u2022 Neu Metal Surface Defects Data [12] contains 1800 images of metal surfaces (200x200 pixels) with six defect classes: Crack, Inclusion, Patch, Pitted Surface, Rolled-in Scale, and Scratches.\n\u2022 Severstal [13]: Steel Defect Detection includes 12,568 images of steel surfaces (256x1600 pixels) with four defect classes: Rolled-in Scale, Patch, Craze, and Pitted Surface.\nIn thisexperiment, thebaseline model of CNNs was built, utilizing commonly applied Convolutional and Linear layers typically used in classification tasks. For clarity and ease of reference, the specific namesofthese models have been assigned, which are detailed in the following description:\n\u2022 TwoLayerConvNet: Two convolutional layers with five filters, ReLU activations, followed by max-pooling and a final fully connected layer for classification.\n\u2022 TwoLayerConvNetPlus: Two convolutional layers with five and twenty five filters, ReLU activations, max-pooling, and two fully connected layers for more complex pattern learning.\n\u2022 Single Layer LinearNet: A single fully connected layer applied to the flattened input for simple classification.\n\u2022 Four LayerConvNet: Four convolutional layers with increasing filter sizes, ReLU activations, max-pooling, followed by two fully connected layers for classification.\n\u2022 TwoLayerConvKAN: Two convolutional layers with ReLU activations, followed by a KAN Linear layer for enhanced feature transformation.\n\u2022 FourLayerConvKAN: Four convolutional layers with ReLU activations, followed by a KANLinear layer for flexible input-output mapping.\n\u2022 ThreeLayer ConvTwoLayerKAN: Three convolutional layers with ReLU activations, followed by two KANLinear layers for flexible decision-making and regularization."}, {"title": "5. Results", "content": "Due to the limitation of using GPU for KAN networks, all models were running on CPU. EachKAN model has been trained for 100 epochs and all experiments was re-run 10 times. The comparisonof these results with baseline methods [14] ispresented in Table 2.\nBased on the experimental results (Table 1, Figure 1 and Figure 2), itcan observed that the accuracy achieved by the KAN-based neural network is noticeably higherthan CNN. Moreover, this improvement is achieved with fewer neural network parameters."}, {"title": "6. Conclusions", "content": "This research shows great promise and yields satisfactory results; however, a significant challenge lies in the lengthy training times. In our study, we observed how the KAN-based network performed when applied to a smaller dataset. Even with relatively small KAN-based neural networks, the computational demands were substantial, and with larger datasets and more complex networks, the training time could become prohibitively long. This limitation may lead to abandoning this approach in favor of architectures specifically designed for such problems."}]}