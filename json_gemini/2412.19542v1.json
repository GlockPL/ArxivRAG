{"title": "Interacted Object Grounding in Spatio-Temporal Human-Object Interactions", "authors": ["Xiaoyang Liu", "Boran Wen", "Xinpeng Liu", "Zizheng Zhou", "Hongwei Fan", "Cewu Lu", "Lizhuang Ma", "Yulong Chen", "Yong-Lu Li"], "abstract": "Spatio-temporal Human-Object Interaction (ST-HOI) understanding aims at detecting HOIs from videos, which is crucial for activity understanding. However, existing whole-body-object interaction video benchmarks overlook the truth that open-world objects are diverse, that is, they usually provide limited and predefined object classes. Therefore, we introduce a new open-world benchmark: Grounding Interacted Objects (GIO) including 1,098 interacted objects class and 290K interacted object boxes annotation. Accordingly, an object grounding task is proposed expecting vision systems to discover interacted objects. Even though today's detectors and grounding methods have succeeded greatly, they perform unsatisfactorily in localizing diverse and rare objects in GIO. This profoundly reveals the limitations of current vision systems and poses a great challenge. Thus, we explore leveraging spatio-temporal cues to address object grounding and propose a 4D question-answering framework (4D-QA) to discover interacted objects from diverse videos. Our method demonstrates significant superiority in extensive experiments compared to current baselines. Data and code will be publicly available at https://github.com/DirtyHarryLYL/HAKE-AVA.", "sections": [{"title": "Introduction", "content": "As the prototypical unit of human activities, human-object interaction (HOI) plays an important role in activity understanding. Researchers begin with image-based HOI learning (Chao et al. 2018; Li et al. 2019b, 2020b; Liu et al. 2022; Wu et al. 2022) and achieve great progress. Since daily HOIs require temporal cues to avoid ambiguity in detection, e.g., pick up-cup and put down-cup, video HOI task (Damen et al. 2018; Weinzaepfel, Martin, and Schmid 2016; Zhuo et al. 2019; Materzynska et al. 2020) is proposed to advance spatiotemporal HOI (ST-HOI) learning.\nHowever, many video HOI datasets are designed with limited predefined object classes. Charades (Sigurdsson et al. 2016), DALY (Weinzaepfel, Martin, and Schmid 2016), Action Genome (Ji et al. 2020) all have less than 50 object classes (Tab. 1). The limited object classes are less general for HOI tasks. Though some hand-object interactions and egocentric video-based HOI datasets include diverse objects like EPIC-Kitchens (Damen et al. 2018), Something-Else (Materzynska et al. 2020) and 100DOH (Shan et al. 2020), they focus on hand-object interactions and egocentric videos. As whole body-object interaction detection from third-view videos matters to numerous applications (e.g., health-care, security), here, we study third-person body-object interactions, such as ride/sit on (chair, horse, etc), enter/exit (train, bus, etc). Toward open-world HOI, we propose a large-scale third-view ST-HOI benchmark in this work, building upon AVA (Gu et al. 2018): Grounding Interacted Object (GIO). It contains 1,098 interacted object classes within 51 interactions and 290K frame-level triplets (human, verb, object) as Fig. 1 shows.\nUnlike previous works focussing on human/object tracking and action detection, we probed the complex ST-HOI through the object view given the largest scale of interacted object classes as in Fig. 1. We propose an open-world interacted object grounding task with corresponding metrics to formulate this challenging problem. The initial formulation of ST-HOI(Sec. 5.4) suffers from severe missing annotation, which makes detection and evaluation less reliable. Instead, our grounding task is insensitive to missing annotations, thus controlling the task's difficulty and reliability and enabling a meaningful analysis. Given this task, cutting-edge image/video detectors (Ren et al. 2015; Chen et al. 2020a) fine-tuned on our train set all achieve less than 20 AP, even recent general visual grounding models based on large-scale VLMs (Liu et al. 2023) show limited performance. Hence, GIO is still challenging and essential as the touchstone for open-world HOI.\nInstead of directly regressing the object box, we devise a 4D question-answering (4D-QA) paradigm. First, the progress of the open-world segmentation model (Kirillov et al. 2023b) makes generating thorough and accurate fine-grained masks for arbitrary images possible. Then, a multi-option question-answering model is built to solve the problem: which masks correspond to the interacted object? Multi-modal information is utilized to achieve this. Besides the raw video clip, we also reconstruct the 4D human-object layout for spatial clues and take it as a representation. Despite the pixel-level accuracy of the reconstruction is limited, it is sufficient for us to tackle the occlusion and spatial ambiguities for object localization. In comparison to directly regressing the object box, the 4D human-object layout before the QA paradigm provides general object-orient HOI information, this is why our method can achieve significant improvement. We believe GIO would inspire a new line of studies and pose new challenges and opportunities for the development of deeper activity understanding.\nOur contributions are three-fold: (1) We probe ST-HOI learning via an interacted object view and build a large-scale third-view ST-HOI benchmark GIO, including 290K open-world interacted object boxes from 1,098 object classes. (2) A novel interacted object grounding task is proposed to drive the studies on finer-grained activity parsing and understanding. (3) Accordingly, a 4D question-answering framework is proposed and achieves decent grounding performance on GIO with multi-modal information."}, {"title": "Related Works", "content": "Object Tracking. Object tracking is an active field and has two main branches, i.e., Single-Object Tracking (Chen et al. 2020b; Fan et al. 2019) and Multi-Object Tracking (Ristani et al. 2016; Bras\u00f3 and Leal-Taix\u00e9 2020). Recently, tracking-by-detection (Kim et al. 2015; Sadeghian, Alahi, and Savarese 2017) has received lots of attention and has achieved state-of-the-art performance.\nHuman-Object Interaction (HOI). In terms of image-based HOI learning, both image-level (Chao et al. 2015; Li et al. 2020c; Kato, Li, and Gupta 2018) and instance-level (Chao et al. 2018; Li et al. 2019b,a, 2022b, 2020a; Liu, Li, and Lu 2022) methods achieve successes with the help of large-scale datasets (Chao et al. 2018; Li et al. 2020c). As for HOI learning from third-view videos, recently many large-scale datasets (Gu et al. 2018; Sigurdsson et al. 2016; Ji et al. 2020; Shan et al. 2020; Fouhey et al. 2018; Caba Heilbron et al. 2015) are released to promote this field, thus providing a data basis for us. They provide clip-level (Caba Heilbron et al. 2015; Fouhey et al. 2018; Sigurdsson et al. 2016) or instance-level (Gu et al. 2018; Ji et al. 2020; Weinzaepfel, Martin, and Schmid 2016) action labels, but few of them afford diverse object classes. Though some datasets (Materzynska et al. 2020; Damen et al. 2018) provide instance labels of diverse object classes, they usually concentrate on egocentric hand-object interaction understanding (Xu, Li, and Lu 2022). Relatively, we focus on whole-body-object interaction learning based on third-view videos and propose GIO featuring the discovery of diverse objects. Recently, there are also methods studying video-based visual relationship (Shang et al. 2017; Liu et al. 2020) and HOI (Qi et al. 2018; Wang and Gupta 2018; Baradel et al. 2018; Girdhar et al. 2019).\nObject Detection and Localization. Object detection (Ren et al. 2015; Redmon et al. 2016) achieves huge success with deep learning and large-scale datasets (Lin et al. 2014) but may struggle without enough training data. Some works (Fan et al. 2020) study few/zero-shot detection. Moreover, as videos can provide temporal cues of moving objects, video object detection (Chen et al. 2020a) also received attention. Unlike typical detection, some studies try to utilize context cues, such as human actor (Kim et al. 2020; Gkioxari et al. 2018), action recognition (Yuan et al. 2017; Yang et al. 2019), object relation (Hu et al. 2018), to advance object localization. Gkioxari et al. (2018) treated object localization as density estimation and used a Gaussian function to predict object location. Kim et al. (2020) borrowed human pose cues and language prior, constructing a weakly-supervised detector. Moreover, object grounding with language descriptions also attracts attention in the vision-language crossing field, with promising potential in open-vocabulary object detection. Li et al. (2022a) formulates object detection as an object grounding problem for open-vocabulary object detection. Yao et al. (2022) boosted data from image captioning datasets for generalization ability. Liu et al. (2023) extended the powerful DINO (Zhang et al. 2022) model for the object grounding pipeline, achieving impressive performance. Sadhu, Chen, and Nevatia (2020) grounded objects in video clips given language descriptions."}, {"title": "Constructing GIO", "content": "To support practical ST-HOI learning, we collect third-view videos from large-scale dataset AVA (Gu et al. 2018). It contains 430 videos with spatio-temporal labels of 80 atomic actions (body motions and HOIs). As AVA includes complex HOIs in diverse scenes, it can bring great visual diversity to our benchmark. We extract the HOI-related frames and the corresponding human boxes and action labels, thus the clips in GIO have uneven temporal durations. Notably, we only consider the non-human objectives in HOIs. Overall, based on the available train and validation (val) sets of AVA 2.2 (Gu et al. 2018) (299 videos), we chose 74 hours of video including 51 actions (detailed in the supplementary)."}, {"title": "Dataset Annotation", "content": "AVA provides labels with a stride of 1s, so we add boxes and class labels for all interacted objects with the same stride. Following AVA, we define the annotated frame as key frames which are at 1-second intervals.\nFirst, as humans can perform multi-interaction simultaneously, we set the annotating unit as a clip including one single interaction to normalize the annotation. For example, a 30s clip including an actor holds-sth (1-30s) and inspects-sth (10-15s), will be divided into two sub-clips, i.e., a 30s sub-clip for holds-sth and a 5s sub-clip for inspects-sth. In brief, each sub-clip contains one verb and one/several class-agnostic interacted objects. Then, sub-clips are annotated separately, and each one is annotated by at least 3 annotators and checked by an expert to ensure quality.\nSecond, as AVA contains various scenarios and diverse objects, to better locate objects and avoid ambiguity, each annotator is given a whole sub-clip to draw boxes and classify them. In default, we use COCO (Lin et al. 2014) 80 objects as a class pool. If annotators think an object is not in the pool, they are asked to input a suitable class according to their judgments. If an object cannot be recognized, they can choose the \"unknown\" option. Then, we find that a surprising 42.66% of object instances are beyond our pool. After exhaustive annotation, we fix the input typos, exclude outliers via clustering, and combine similar items. Finally, 1,098 classes are extracted after cleaning. We then conduct re-recognition for the frames including \u201cunknown\" objects.\nFinally, to generate the ST-HOI labels, we further consider the objects in each sub-clip (one interaction of one person). If there is only one object in a sub-clip, we use its locations as the labels. If there are multiple objects, we record all of their boxes and manually link their boxes as multiple-object tracklets. Then, each sub-clip is seen as a ST-HOI traklet, whose label records a human actor tracklet, an interaction, a/several class-agnostic object tracklets."}, {"title": "Dataset Statistics and Attributes", "content": "GIO includes 290K HOI triplets and 290K object boxes of 1,098 classes, including a wide range of rare objects. Only 20.85% of our object classes are covered by the recent large-scale object dataset FSOD (Fan et al. 2020). It is noteworthy that Action Genome and VidHOI include predicates such as next to, which are not HOIs. Consequently, we refined the annotations and recalculated the statistics in Tab. 1. In contrast, GIO, aiming for diversity and finer granularity, offers the highest number of object classes and the richest HOI instances per frame (2.30)."}, {"title": "Interacted Object Grounding", "content": "GIO supports ST-HOI detection and many fine-grained tasks, like object classification. However, the original ST-HOI task, involving detection, tracking, and action recognition, is highly complex and challenging, with most approaches facing significant difficulties due to the task's inherent complexity and the quality of annotations. So in- stead of requiring vision systems to detect complete ST-HOI triplets, we focus on GIO's capability for interacted object grounding, i.e., given the human actor tracklet (and the interaction semantics), while object labels are not included in the interaction semantics, probing the ST-HOI understanding from the object view. To make our task realistic, 328 object classes only have less than 5 samples (boxes) in our train set, and 98 classes are unseen in the inference."}, {"title": "Method", "content": "In this section, we describe the pipeline of our method (Fig. 2). We focus on interacted object grounding, i.e., given the human actor tracklet (and the interaction semantics), systems are required to ground the interacted object. The difference between our task and the common object grounding tasks is our focus on the specific interaction between the grounded object and the person (interactiveness), which makes it more difficult. For clarity, the description unit hereinafter is one human tracklet including one tracked person."}, {"title": "Overview", "content": "Given a clip C, the target human tracklet $T_h = \\{I_i\\}_{i=1}^{n}$ (n for tracklet length), we aim at learning a model M as\n$T_o = M(C, T_h, \\{s, \\theta\\}),$ (1)\nwhere $T_o$ is the predicted interacted object tracklet and the interaction semantics s is an optional input to inform the system with high-level semantics. To achieve this, instead of directly regressing the object box, we adopt a novel 4D question-answering paradigm to leverage HOI prior. Given the strong generalization ability of SAM (Kirillov et al. 2023a), we adopt it as an objectness detector to generate candidate object proposals (Sec. 4.2). The clip C is first fed to SAM, resulting in K candidate object mask tracklets $M_o = \\{M_i\\}_{i=1}^{K}$. The task is then reformulated as choosing the interacted mask tracklets from the candidate tracklets, as\n$T_o = M(C, T_h, \\{s, \\theta\\}, M_o).$ (2)\nTo tackle the challenging GIO, a 4D question-answering network is devised as shown in Fig. 2. Multimodal features, including 4D clues, are extracted in the inspiration of DJ-RN (Li et al. 2020a). We begin by extracting spatiotemporal features from the video using the SlowFast (Feichtenhofer et al. 2019) network as a basis. Then, the 4D Human-Object layout is reconstructed for feature extraction (Sec. 4.3). Finally, we ground the interacted object with two decoders to summarize the important clues in complex spatiotemporal patterns (Sec. 4.4). Despite the suboptimal precision of 4D Human-Object reconstruction, it is effective in alleviating the view ambiguity in clips, also enhancing the object localization with 3D spatial information. The question-answering paradigm eases the learning process."}, {"title": "SAM-based Candidate Generation", "content": "We chose SAM as the candidate proposal generator for several reasons. First, SAM, based on pixel-level segmentation, provides a finer granularity and more accurate segmentation. Second, AVA consists of many video scenes that are dark, complex, and contain numerous objects. Traditional detection methods struggle to accurately predict small and blurry objects in such challenging scenarios. In contrast, SAM's pixel-based segmentation is more robust and accurate than directly predicting object bounding boxes. In addition, SAM is also adept at dealing with large objects. However, SAM could segment objects into multiple parts. Thus, our policy is to predict vast majority of the masks belong to the object resulting in a highly accurate bounding box.\nMask proposal generation. Given a clip C, we denote the keyframe as $C_k$. SAM is first fed with a grid of point prompts on $C_k$. Then, low-quality and duplicate masks are filtered out. As a result, each image would produce at most 255 masks as $M_o$, which will be sent to the model as proposals to generate the final object box.\nGT proposals. To judge which mask is GT, we input the GT object box to SAM as the prompt to get an accurate mask ($M_{acc}$). Next, we calculate the area of the intersection between the proposal masks and the accurate mask ($A_{inter}$), and divide them by the area of the proposal masks $A_o$ to get a ratio for each proposal mask as $ratio = A_{inter} / A_o$. Masks with a ratio greater than 0.9 are identified as GT masks. Fig. 3 demonstrates the above process."}, {"title": "Multi-Modal Feature", "content": "To fully leverage the temporal and spatial continuity features of videos, including object information, HOI details, and spatial relations from multiple views, we employed a multi-modal feature extraction approach.\nContext Feature. We utilize widely-used SlowFast (Feichtenhofer et al. 2019) to extract context features from the video clip C. The features from slow and fast branches are pooled along the time axis, then concatenated into the context feature map $f_c \\in R^{H \\times W \\times D}$, with $H \\times W$ being the feature map resolution, and D is the feature dim.\nObject Feature. We first resize the masks $M_i$ of the i-th mask concerning the context feature map, then the feature for the i-th mask could be computed as\n$f_o = AvgPool(f_c \\diamond M_i) \\in R^D,$ (3)\nwhere $\\diamond$ indicates element-wise multiplication. The object feature is denoted as $f_o \\in R^{N_o \\times D}$ with $N_o$ masks.\nLanguage Interaction Feature is optional. If adopted, we input the language-guided query embedding $f_v \\in R^D$ of GroundingDINO (Liu et al. 2023), which needs a language prompt and the key frame as input. Some other interaction features are discussed in Sec. 5.6.\n4D Human-Object Feature. Inspired by Li et al. (2020a), which utilizes 3D information for HOI learning, we incorporate 3D information into our pipeline to exploit the rich HOI prior carried by 4D information. Specifically, we lift GIO to 4D by reconstructing the HOIs in 3D. However, lifting GIO to 4D is challenging given its diverse objects. Existing efforts usually require 3D templates for the objects, which is inapplicable for open-world GIO. To alleviate this, we adopt depth estimation for holistic scene estimation, bypassing the need for object templates. Then, we align the human and scene for consistent 4D H-O representation. Finally, we extract the 3D feature with the lightweight base point set (BPS) (Prokudin, Lassner, and Romero 2019).\n1) Human reconstruction. Considering that videos without scene switching allow for better human tracking and less processing time of 3D data, we first perform shot detection and segment the original video into multiple sub-clips. Then, PHALP (Rajasegaran et al. 2022) is adopted to recover 4D human tracklets from the sub-clips in SMPL (Loper et al. 2015) representation. The 3D humans are further represented as SMPL mesh point clouds $p_h \\in R^{T \\times N_h \\times V \\times 3}$, where T is the length of the clip, $N_h$ is the number of existing human instances, and V is the number of mesh vertices.\n2) Scene reconstruction via depth estimation. We use ZoeDepth (Bhat et al. 2023) to estimate the depth of the corresponding clip and transform them into scene point cloud $p_s \\in R^{T \\times N_p \\times 3}$, where $N_p$ is the number of points.\n3) Human-Scene alignment. The humans and scenes are initially inconsistent in scale and position. To align them, we render the $N_f$ front surface vertices $p_f \\in R^{N_f \\times 3}$ of the human mesh to the image space, find the corresponding pixel of each vertice, and locate the corresponding point in the scene point cloud $p_s^* \\in R^{N_f \\times 3}$. Next, we align $p_f$ and $p_s^*$ by calculating the scale and displacement of $p_s^*$ to align with $p_f$. We calculate scale s and displacement b as\n$d_h = \\frac{1}{N_f^2} \\sum_{i}^{N_f} \\sum_{j}^{N_f} ||p_{hi} - p_{hj}||_2^2, p_{hi} \\in p_h$ (4)\n$d_s = \\frac{1}{N_f^2} \\sum_{i}^{N_f} \\sum_{j}^{N_f} ||p_{si}^* - p_{sj}^*||_2^2$\n$s = \\frac{d_h}{d_s}, p^* = s, b = \\frac{1}{N_f} \\sum_{i=1}^{N_f} p_{hi} - \\frac{1}{N_f} \\sum_{j=1}^{N_f} p_{sj}^*$\nIn detail, the scale is calculated as the ratio between the average pairwise distance of $p_f$ and $p_s^*$, while the displacement is calculated as the displace between the center point of $p_h$ and $p_s^*$. The aligned human-scene point cloud is then formulated as $p = (p_h, p_s^* \\cdot s + b) \\in R^{T \\times (N_h \\times V + N_h \\times N_p) \\times 3}$.\n4) 3D feature extraction. We adopt BPS to extract features, which is simple and efficient for encoding 3D point clouds into fixed-length representations. We randomly select fixed points in a sphere and compute vectors from these basis points to the nearest points in a point cloud; then use these vectors (or simply their norms) as features, shown in Fig. 2. We adopt the human pelvis joint as the sphere center for base point generation. We selected a radius of 1.5 times the height of the human body to cover the range of human interactions. In this way, in one space, we obtain T \u00d7 Nh \u00d7 base points. We calculate the distances from these base points to the human mesh point cloud and the scene point cloud, treating them as features. Then we concatenate human features and scene features to get the final 3D feature $f_{3D} \\in R^{(T \\times N_h) \\times D}$, in the following we refer to T \u00d7 Nh by $N_{3D}$, i.e., $R^{N_{3D} \\times D}$"}, {"title": "Object Grounding", "content": "We utilize a 2D transformer decoder and a 3D transformer decoder to integrate multi-modal features. The 2D decoder outcome is sent to the 3D decoder as a query via an MLP as the 3D adapter. Note that the 2D decoder results have already been satisfactory, but the 3D decoder could further enhance predictions from the 3D perspective. Each 2D decoder query $Q \\in R^{N_q \\times D}$, is obtained via $Q_s = Q_v + Q_h$, where $Q_v \\in R^{N_q \\times D}$ is the optional verb semantic query from the feature vector $f_v$, and the human query $Q_h \\in R^{N_q \\times D}$ is obtained via a temporal pooling, a ROIAlign pooling, and a spatial pooling of the SlowFast features with the human bounding box. Given the context feature $f_c$, the object feature $f_o$, we concatenate them as the key and value of the 2D decoder. The object feature $f_o$ and the 3D feature $f_{3D}$ are concatenated as the key and value of the 3D decoder.\nThe 2D/3D decoder outputs feature $f_q$. The cosine similarity between $f_q$ and all object mask features $f_o$ is computed. Then, we derive scores for each query relative to each mask, denoted as Sim. Higher scores suggest a greater likelihood of the mask being associated with the target object. Considering that a person tends to interact with objects that are closer in proximity, we use the distance between masks and humans to assist us in calculating mask scores. The distance of the i-th mask is computed as $S_d^2 = dist(C_h, C_m)$, where $C_h$ and $C_m$ refer to the human box and the i-th mask's box. Ultimately we adopt the GIoU (Rezatofighi et al. 2019) distance. The final score of the i-th mask is computed as\n$S_i^* = \\gamma \\times S_m^* + (1 - \\gamma) \\times S_d^*$, (5)\nwhere $\\gamma$ is a weight. Then, we introduce a threshold $\\tau$ to determine whether a mask is considered part of the target object. In the results for a certain query, if none of the mask scores exceed this threshold, we select the mask with the highest score. We cluster the predicted masks based on their depths and then determine the boundaries(detailed in the supplementary material). For a given object w.r.t. i-th mask, BCE loss $L$ is used for supervision. The overall loss is computed as $L_o = (\\sum_{i=1}^{N_o} L_i)/N_o$."}, {"title": "Experiments", "content": "Setting\nModified versions of mean Average Precision (mAP) and mean Intersection over Union (mIoU) are adopted. For each GT tracklet, we sort all predictions by their scores in descending order. We identify the first prediction with an IoU higher than a threshold as a hit and calculate its precision by its position in that order. mAP is averaged across all test instances. For mIoU, we calculate all IoUs between the GT and predicted boxes and report the largest IoU. To take into account the precision of the prediction, a weighted mIoU is proposed as mIoUw. For each GT tracklet, predictions are sorted by scores in descending order. The rank of each prediction is used to calculate mIoUw as\n$\\omega(T_o) = \\frac{1}{rank(T_o)},$\nmIoU_w (T_o) = \\frac{\\sum_{T_o} \\omega(\\hat{T_o})IoU(T_o,\\hat{T_o})}{\\sum_{T_o} \\omega(T_o)}$, (6)\nwhere $T_o$ and $\\hat{T_o}$ denotes predicted and GT tracklets. Since mIoUw is a more reasonable metric, we adopt mIoUw instead of mIoU in the experiments."}, {"title": "Implementation Details", "content": "For the 3D feature, considering the reconstruction quality of the 4D HOI layout, the reconstruction is only conducted for frames with object labels. After filtering, there are 107,663 of 126,700 key-frames attached with 4D HOI layout (85,370 for training, 22,293 for inference). SlowFast pre-trained on AVA 2.2 is adopted for video feature extraction. An Adam optimizer, an initial learning rate of 1e-3, a cosine learning rate schedule, and a batch size of 16 are adopted. When training the 2D decoder, the learning rate of the parameters of SlowFast and Grounding DINO is 1e-5 and the 3D decoder is omitted. When training the 3D decoder, other parts except the 3D decoder are frozen. $N_{3D}$ is set to 256, $N_o$ is set to 256 and $N_q$ is set to 24 for alignment. Considering that the ground truth mask for each keyframe is sparse, we use weighted BCE loss, where the loss coefficient for true positions is ten times that of false positions.\nOur dataset supports different settings for further investigation, like inputting the interaction semantics to the grounding model, using more advanced LLM-extracted features, and the effect on the grounding of different human trackers, etc. However, to focus on evaluating the grounding itself, we mainly discuss the default setting given the interaction semantics and GT human tracklets. For example, our system still predicts the interacted object well, without inputting the language interaction feature or the detected actions and humans from standard SOTA action detectors (Feichtenhofer et al. 2019; Wang et al. 2023). The proposed 4D-QA model has 246M parameters and achieves an inference speed of 8.63 FPS with a batch size of 1 (8 adjacent frames and 1 keyframe) on a single NVIDIA 3090 GPU."}, {"title": "Baselines", "content": "We adopt six models of four different types as our baseline. It is worth mentioning that since our task is new, we find these models most close to our task. But, they still do not fit our task very well in the setting. We devise corresponding protocols to adapt these models to our task.\nImage/Video-based HOI models. PViC (Zhang et al. 2023) and Gaze (Ni et al. 2023) are adopted as conventional image/video-based HOI detection baselines. Given a frame or clip and a human bounding box b, the HOI models input the frame or clip and output a series of HOI triplets as $(b_h, b_o, p)$, where $b_h, b_o$ are human and object bounding boxes, and p is the predicted interaction probability. We preserve all the results with $IoU(b, b_h) > 0.5, p > 0.2$, and the corresponding $b_o$ are adopted as the grounded objects.\nOpen-vocabulary object detection models. Detic (Zhou et al. 2022) is adopted, inputting a frame and expected object categories, outputting $(b_o,p)$ as object bounding boxes and objectness score. Results with $p > 0.5$ are preserved and paired with the human query as the grounded objects.\nVisual grounding models. Grounding DINO (Liu et al. 2023) is adopted, which takes a frame and a text prompt s as input and produces $(b_o, p)$ as grounded box and confidence. We also test a video-grounding baseline CG-STVG (Gu et al. 2024), which aims to predict a spatial-temporal tube for a specific target subject/object given some semantic s. s is in the format as \u201cThe object that the person is {interacting with}", "{Interacting with}\" could be replaced with a specific action name. All the outputs are paired with the human query as the grounded object.\nLLM based models. Qwen-VL (Bai et al. 2023) is adopted. It takes a frame and the text prompt \\\"Output the bounding box that the person is {interacting with}.\\\" and produces the bounding box $b_o$ if detected.\"\n    },\n    {\n      \"title\"": "Results"}, {"content": "Results are shown in Tab. 2. For all the models, we combine the human-object distance for mIoUw and mAP as Eq. 5. All baselines provide sub-optimal performance, indicating their deficiency in interactiveness grounding. Also, most baselines take little use of temporal information since they utilize only images as inputs, leading to bad performances on \"temporally hidden objects\" such as the chairs obstructed by humans sitting on them but appearing in the next frame.\nAs HOI detection models, PViC and Gaze fail to perform well due to the large number of novel objects in GIO. The open-vocabulary object detection model Detic demonstrates low mIoUw since it cannot discriminate the interacted objects related to humans (interactiveness (Li et al. 2019b)). It is noteworthy that Detic tends to predict a substantial number of object bounding boxes, sometimes exceeding 900, with many false positive predictions. CG-STVG, lacking pre-training on large datasets and integration of visual-language models, outperforms PViC, Gaze, and Detic in mIoUw, using a single high-quality bounding box per HOI instance for higher mIoUw despite lower mAP. Grounding DINO performs better than other baselines, but it is still limited for \"hidden objects\". Also, it frequently fails to fully understand the interaction semantics. Qwen-VL, a large vision language model, provides decent mIoUw but poor mAPs, which suggests that although Qwen-VL can localize the approximate positions of most objects, it struggles to detect precise bounding"}]}