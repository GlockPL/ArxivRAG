{"title": "PaintScene4D: Consistent 4D Scene Generation from Text Prompts", "authors": ["Vinayak Gupta", "Yunze Man", "Yu-Xiong Wang"], "abstract": "Recent advances in diffusion models have revolutionized 2D and 3D content creation, yet generating photorealistic dynamic 4D scenes remains a significant challenge. Existing dynamic 4D generation methods typically rely on distilling knowledge from pre-trained 3D generative models, often fine-tuned on synthetic object datasets. Consequently, the resulting scenes tend to be object-centric and lack photorealism. While text-to-video models can generate more realistic scenes with motion, they often struggle with spatial understanding and provide limited control over camera viewpoints during rendering. To address these limitations, we present PaintScene4D, a novel text-to-4D scene generation framework that departs from conventional multi-view generative models in favor of a streamlined architecture that harnesses video generative models trained on diverse real-world datasets. Our method first generates a reference video using a video generation model, and then employs a strategic camera array selection for rendering. We apply a progressive warping and inpainting technique to ensure both spatial and temporal consistency across multiple viewpoints. Finally, we optimize multi-view images using a dynamic renderer, enabling flexible camera control based on user preferences. Adopting a training-free architecture, our PaintScene4D efficiently produces realistic 4D scenes that can be viewed from arbitrary trajectories. The code will be made publicly available.", "sections": [{"title": "1. Introduction", "content": "Generating dynamic 3D scenes from text descriptions, known as text-to-4D scene generation, represents one of the most challenging frontiers in computer vision and graphics. While recent advances have revolutionized our ability to create static 3D content [23, 28, 29, 31, 33, 34, 42] and 2D image and videos [38, 39, 43, 49] from text, the synthesis of temporally coherent and animated 3D scenes remains a fundamental challenge. This task requires not only generating spatially consistent 3D geometry and appearance, but also producing realistic motion that adheres to real-world physics and semantic constraints \u2013 all while maintaining temporal consistency across multiple viewpoints.\nThe complexity of 4D scene generation stems from several interconnected challenges. First, unlike static 3D generation, which only needs to ensure spatial consistency, 4D scenes must maintain both spatial and temporal coherence simultaneously. This means that any generated motion must be physically plausible and semantically meaningful, while preserving the scene's geometric structure across time. Second, the lack of large-scale, diverse 4D scene datasets has limited the development of robust generation methods, with most existing approaches relying on object-centric data that fail to capture the rich dynamics of full scenes. Third, the computational complexity of optimizing both spatial and temporal dimensions makes it difficult to achieve high-quality results within reasonable time constraints.\nCurrent approaches to these challenges broadly fall into two categories, each with significant drawbacks. The first category extends static 3D generation methods (e.g., MVDream [42] and Zero123 [28, 41], trained on object-centric datasets like Objaverse [9]) to incorporate temporal dynamics [2, 19, 25, 36, 44, 56, 61, 62]. These methods, while effective at maintaining geometric consistency, typically struggle with generating complex motion, often producing only subtle deformations or simple translations. This limitation stems from their reliance on Score Distillation Sampling (SDS) [33] optimization, which, while effective for static content, becomes computationally intractable when scaled to temporal sequences. The second category are the Text-to-Video models (e.g., Animatediff [13], CogVideo [54]) that generate dynamic content. However, these approaches lack explicit 3D understanding, resulting in temporal inconsistencies and geometric artifacts. Neither approach adequately addresses the fundamental challenge of generating spatially and temporally coherent 4D scenes.\nTo address these limitations, we present PaintScene4D, a novel framework that harness the strengths of both Text-to-Video generation and 3D-aware neural rendering. Our key insight is that by using video generation as an initial prior and reconstructing the 3D scene through a progressive warping and inpainting technique, we can maintain both spatial and temporal consistency while enabling complex motion generation. Specifically, our method first generates a base video using a pretrained Text-to-Video model, which provides rich motion priors. We then construct a \"web of cameras\" around the scene by warping video frames to nearby viewpoints and using inpainting to fill any resulting gaps. This approach allows us to build a comprehensive multi-view representation of the dynamic scene without requiring explicit 3D supervision or costly optimization procedures.\nThe effectiveness of PaintScene4D is demonstrated through several empirical contributions. As shown in Figure 1, our method achieves state-of-the-art results in text-to-4D scene generation, producing visually compelling results that maintain both spatial and temporal consistency. The generated scenes exhibit complex motion while preserving geometric structure across multiple viewpoints. Notably, our framework reduces the computational requirements significantly, generating high-quality 4D content in approximately 3 hours on a single A100 GPU \u2013 a substantial improvement over existing methods [2, 62] that often require 10+ hours. Through extensive experiments and ablation studies, we demonstrate the superiority of our approach across various metrics, including temporal consistency, motion complexity, and rendering quality. Our method also offers unprecedented flexibility, allowing users to edit existing videos or specify custom trajectories during inference.\nOur main contributions can be summarized as follows:\n\u2022 A novel end-to-end framework for text-to-4D scene generation that effectively distills video generation prior to 4D-aware neural rendering.\n\u2022 A progressive warping and inpainting technique that enables the construction of spatially and temporally consistent multi-view representations.\n\u2022 Comprehensive evaluation and analysis of PaintScene4D demonstrates state-of-the-art results in 4D scene generation, with significantly reduced computational requirements and enhanced camera control options."}, {"title": "2. Related Work", "content": "Text-to-3D Generation. Text-to-3D generation has evolved significantly over the past decades. Initial approaches rely on rule-based systems that parse text inputs into semantic representations for scene generation using object databases [1, 6, 8]. The field has advanced substantially with the introduction of data-driven approaches that leverage multimodal datasets [7] and pretrained models like CLIP [35], enabling more sophisticated manipulation of 3D meshes [11, 18] or radiance fields [48]. This progress has led to the development of methods utilizing CLIP-based supervision for comprehensive 3D scene synthesis [17, 40], which subsequently evolves into techniques that optimize meshes and radiance fields through Score Distillation Sampling (SDS) [22, 33, 50]. The introduction of multi-view-aware diffusion models has further enhanced the quality of generated 3D structures [24, 28, 42]. Parallel developments in diffusion and transformer architectures have enabled advanced image-to-3D conversion for novel view synthesis [5, 12, 29, 34, 45, 47, 57]. These approaches primarily address object-level reconstruction."}, {"title": "3. Preliminary: 3D Gaussian Splatting", "content": "3D Gaussian Splatting (3D-GS) [21] utilizes point clouds to explicitly represent scenes, with each point modeled as a Gaussian distribution in three-dimensional space. Each Gaussian is defined by a central point X, representing its mean, and a covariance matrix \u2211, capturing its spread. The Gaussian function G(X) for a point is represented by:\n$G(X) = exp(-X^{T}\\Sigma^{-1}X)$.\nTo enable differentiable optimization, the covariance matrix \u2211 is decomposed into a rotation matrix R and a scaling matrix S as \u2211 = RSSTRT. Rendering novel views involves a differentiable splatting technique [55], where 3D Gaussians are projected onto a 2D image plane. As detailed in [64], the transformed covariance \u03a3' in camera coordinates is computed using the viewing transformation matrix W and the Jacobian J of the projection, yielding \u03a3' = JWEWT.JT. In this setup, each Gaussian is parameterized by its position X \u2208 R\u00b3, its color defined by spherical harmonic (SH) coefficients C\u2208 Rk (where k is the number of SH functions), opacity a \u2208 R, rotation r\u2208 R4, and scale s \u2208 R\u00b3. The resulting pixel color and opacity are derived from the Gaussian blend defined in Equation 2, where the color C from N overlapping Gaussians is calculated as:\n$C = \\sum_{i=1}^{N} c_{i} \\alpha_{i} \\prod_{j=1}^{i-1} (1 - \\alpha_{j})$,\nwhere c\u1d62 and \u03b1 represent the color and opacity of the i-th Gaussian point, with these values optimized per-point from the Gaussian's SH coefficients and adjustable opacity."}, {"title": "4. Method", "content": "Overview. In this work, we present PaintScene4D, a novel framework designed to generate 4D dynamic scenes from textual inputs. Our approach begins with a video diffusion model that produces an initial video serving as both scene and motion reference. Using this video as input, we employ a depth estimation model to derive depth maps from each frame, allowing us to progressively construct a spatial representation of the scene. To create a comprehensive multi-camera view of the scene, we progressively warp the initial frames to new camera positions, beginning with the first frame. In this process, any regions missing due to occlusion or perspective changes are filled in using a spatially consistent inpainting method. For each subsequent frame, our approach reuses inpainted data from prior timestamps for continuity, and only filling in new, unobserved areas. Once we have constructed a network of cameras, where each camera captures all frames over time, we employ a 4D rendering algorithm to reconstruct the scene and generate novel viewpoints. This entire methodology is outlined in Figure 2.\n4.1. Scene Initialization\nReference Video Generation and Depth Estimation. To generate the initial scene content from an input prompt t, we start by applying a pre-trained video diffusion model, fa, conditioned on t to create an initial video V\u2080 = fa(e | t),"}, {"title": "4.2. Scene Construction w/ Progressive Inpainting", "content": "Given the absence of multi-view supervision, directly employing a single-view video V\u2080 and its depth maps D\u2080 to train a 4D radiance field can lead to issues of overfitting and geometric ambiguity. To address this, we apply a depth image-based rendering (DIBR) technique [10] to establish a network of virtual cameras around the initial view. Specifically, for each pixel p in I and its corresponding depth z in D, we compute its transformed coordinates p\u1d62\u2192\u2c7c and depth z\u1d62\u2192\u2c7c for a neighboring viewpoint j as follows:\n$[P_{i\\rightarrow j}, Z_{i\\rightarrow j}] = KP_{j}P_{i}^{-1}K^{-1}[p, z]^{T}$,\nwhere K, P\u1d62 and P\u2c7c are the intrinsic matrix, camera pose for view i and view j, respectively and I represents the image at the timestamp t of viewpoint i. Following the transformation, we fill occluded or missing regions in the newly warped views with inpainting. Our experimental findings reveal that the diffusion-based prior used for inpainting yields higher quality results when the inpainted regions are larger. Therefore, for each view, we select the farthest available viewpoint with minimal overlap, warp the current frame to this viewpoint, and apply inpainting as necessary. Large occlusions are filled using a 2D diffusion-based prior, while smaller gaps are addressed with Telea-based inpainting [46].\nOur warping process begins at the first timestamp, progressively warping and inpainting frames across all views before proceeding to subsequent timestamps. For the first timestamp, we start with a base view I, warp it to a neighboring viewpoint I\u2070, and inpaint any missing regions. To ensure spatial consistency, we integrate both the original (I\u2070) and newly warped frames (I\u1d3e) for further warping (e.g., I\u00b2, I\u00b3). This approach ensures that any inpainted content in"}, {"title": "Depth Alignment.", "content": "To transform a 2D image I into a 3D representation, we first estimate the depth for each pixel. Accurate integration of both new and existing content requires precise depth alignment, ensuring that similar elements in the scene, such as walls or furniture, appear at consistent depths across views. Directly projecting the predicted depth often results in abrupt transitions and geometric discontinuities due to inconsistent scale across viewpoints. To address this, we apply a depth alignment procedure inspired by Liu et al. [27], which refines the depth through scale and shift optimization. Specifically, we optimize scale \u03b3 and shift \u03b2 parameters, \u03b3, \u03b2\u2208 R, by minimizing the difference between predicted d and rendered depths d in a least-squares sense:\n$\\min_{\\beta} m \\circ (\\gamma d + \\beta - \\hat{d})^{2}$,\nwhere the mask m excludes unobserved pixels from the alignment process.\nAdditionally, depth estimation models may fail to accurately resolve depth at object boundaries, often yielding smooth transitions where abrupt changes are expected. This issue affects the overall warping quality, resulting in artifacts such as trailing patterns within occluded regions. To address this, we apply bilateral filtering to sharpen the depth boundaries, enhancing inpainting performance. Additional implementation details are provided in the supplementary material."}, {"title": "4.3. Scene Completion with Temporal Consistency", "content": "Upon completing the warping and inpainting for the first timestamp, we proceed to apply these operations sequentially across subsequent timestamps. However, directly extending the same approach to each timestamp independently can lead to temporal inconsistencies. This is due to the inherent variability in 2D diffusion-based inpainting, which may produce differing results for the same regions across different timestamps. To address this, we impose temporal consistency by ensuring that background regions remain visually coherent across frames. Specifically, we require that overlapping regions across timestamps exhibit similar content, especially in the background areas.\nForeground and Background Separation. After the inpainting process, we use a segmentation model to separate the foreground and background regions within each frame. For regions that contain significant occlusions, especially large missing areas in the background, we incorporate content from previous timestamps to fill these areas. This approach maintains temporal continuity by sourcing background information from earlier frames. For holes near the foreground boundary, we determine the inpainting source based on the background or foreground status of the corresponding region in prior timestamps. If a boundary region classified as background in the current frame aligns with a background area in previous timestamps, we inpaint it using information from the earlier frame. Conversely, if the region is identified as part of the foreground in prior frames, we apply the 2D diffusion model for inpainting. This selective inpainting strategy allows us to maintain coherence across timestamps while appropriately filling areas based on temporal foreground and background information."}, {"title": "4.4. Training and Optimization", "content": "After performing all warping and inpainting operations across views and timestamps, we establish a comprehensive camera network, where each camera contains video frames as captured from its respective viewpoint. Importantly, this multi-view setup is constructed without the need for model-specific training. Using this multi-view spatial information and temporal dynamics, we employ a 4D rendering approach to synthesize novel perspectives of the scene. For the rendering, we leverage the 4D Gaussians framework [51], which represents the 4D space using a deformable network. The renderer takes Gaussian parameters, along with the timestamp, and compute the timestamp-conditioned deformation of these parameters. This approach enables continuous modeling of deformation, facilitating smooth interpolation between timestamps during novel view synthesis. At test time, any desired viewpoint and timestamp can be selected to generate a novel view."}, {"title": "5. Experiments", "content": "5.1. Implementation Details\nOur optimization framework comprises two stages: initially reconstructing a network of cameras, each associated with its respective timeframe view, followed by training a 4D renderer. Specifically, we construct a network of 25 cameras and utilize videos spanning 50 timestamps. All experiments were conducted on a single A100 GPU. The complete warping and inpainting process, which operates without any additional training, requires approximately two hours. Following this, the 4D renderer is trained in about one hour, resulting in a total of approximately 3 hours to complete the training and generate novel views along any desired trajectory. This duration is significantly shorter than the training time required by recent state-of-the-art methods: Dream-in-4D requires over four hours, while 4Dfy takes over 20 hours, despite producing only object-level 4D renderings.\nTo initialise the scene and establish motion priors for 4D reconstruction, we utilize CogVideoX-5b [54]. For depth estimation, DepthCrafter [16] is employed, as it produces consistent depth estimates across video frames, enabling reliable warping. Perspective Fields [20] is used to estimate"}, {"title": "5.2. Baselines and Evaluation Metrics", "content": "In the absence of open-source implementations for text-to-4D scene-level generation, we benchmark our approach against state-of-the-art text-to-4D object-level generation methods, namely 4Dfy [2] and Dream-in-4D [62], across a varied set of 20 prompts. To assess the effectiveness of our proposed approach, we utilize the CLIP Score [35] alongside a structured user study.\nCLIP Score. The CLIP Score [32] assesses alignment between an input prompt and generated visual content by calculating the cosine similarity between CLIP's text and visual embeddings [35]. Scored between 0 and 100, a higher value indicates a closer match. We compute the CLIP Score for 4Dfy, Dream-in-4D, and our method by rendering videos from each prompt, evaluating each frame with CLIP ViT-B/32, and averaging scores across all frames and prompts for consistency.\nUser Study. A comprehensive user study was conducted through Google Forms, involving 30 evaluators per video pair. Each evaluator was presented with three anonymized videos, each capturing a dynamic scene from a camera moving along a circular trajectory. The videos were accompanied by the original text prompt. Evaluators were shown the renderings from 4Dfy, Dream-in-4D, and our approach"}, {"title": "5.3. Text-to-4D Generation", "content": "In Figure 4, we visualize spatio-temporal renderings produced by our method compared to 4D-fy and Dream-in-4D. Although all approaches are capable of synthesizing 4D scenes, 4D-fy and Dream-in-4D focus on object-level renderings and lack fine spatial details. Our approach, by contrast, generates scene-level 4D reconstructions in a significantly reduced time, producing realistic renderings. Notably, 4D-fy struggles to model realistic motion, while Dream-in-4D produces cartoonish effects that diminish realism. In contrast, our method achieves high photorealistic quality across both spatial and temporal dimensions. We also a present a gallery of our results in Figure 3. Quantitative metrics indicating both CLIP score and User study are reported in Table 1. Our method outperforms both 4D-fy and Dream-in-4D in terms of CLIP Score and user study preferences. Evaluators showed a statistically significant preference for PaintScene4D due to its higher motion realism, photorealistic rendering of both foreground and background, overall realism, and better video-text alignment."}, {"title": "5.4. Explicit Camera Control", "content": "To assess camera control, we compared our framework with other text-to-video (T2V) models as illustrated in Figure 5. We input the same text prompts into the T2V model twice, adjusting only the camera movement description to direct it to \"tilt towards the right\" in one case and \"move upwards\" in the other. This setup allows us to examine the models' ability to interpret and execute nuanced camera control instructions. Our observations reveal two key limitations of T2V models. First, even with a fixed seed, the T2V model generates different scenes for each altered prompt. Secondly, although the model simulates an upward camera movement in the second case, it lacks explicit control over the degree of camera motion. In contrast, our approach enables explicit, consistent control over camera trajectory within the same scene and motion dynamics, leveraging 4D modeling for precise camera manipulation."}, {"title": "6. Ablation Study", "content": "We present an ablation study to analyze the components of our PaintScene4D framework, with the results shown in Figure 6.\nDepth Alignment Module: The inclusion of the depth alignment module is crucial for maintaining the geometric consistency of the foreground. During the warping process, all frames are utilized, and any depth inconsistencies across frames result in error accumulation, leading to noticeable artifacts, particularly at the foreground boundaries.\nFarthest View Sampling: In PaintScene4D, we select the farthest view at each step of the warping process to maximize the area inpainted. Omitting this step causes severe degradation near the edges of the foreground, such as the panda's boundary, where needle-like artifacts emerge due to the Gaussian splatting process.\nConsistent Inpainting Module: Temporal consistency in inpainting is essential for coherent 4D scene generation. Without this module, inpainting becomes inconsistent at the boundaries of objects (e.g., the panda) across different timestamps, leading to significantly degraded renderings."}, {"title": "7. Conclusion", "content": "We introduce PaintScene4D, a novel training-free framework for generating photorealistic 4D scenes from a single text prompt. Our method addresses the challenges of spatial and temporal inconsistencies and enables the generation of novel views along a user-defined camera trajectory. PaintScene4D outperforms existing baselines in terms of visual quality, 3D consistency, and motion accuracy while also providing the capability for explicit camera control during inference. Notably, our approach is the first to implement text-to-4D scene generation using open-source models, offering a valuable contribution to the field."}]}