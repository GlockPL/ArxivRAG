{"title": "MambaEVT: Event Stream based Visual Object Tracking using State Space Model", "authors": ["Xiao Wang", "Chao Wang", "Shiao Wang", "Xixi Wang", "Zhicheng Zhao", "Lin Zhu", "Bo Jiang"], "abstract": "Event camera-based visual tracking has drawn more and more attention in recent years due to the unique imaging principle and advantages of low energy consumption, high dynamic range, and dense temporal resolution. Current event-based tracking algorithms are gradually hitting their performance bottlenecks, due to the utilization of vision Transformer and the static template for target object localization. In this paper, we propose a novel Mamba-based visual tracking framework that adopts the state space model with linear complexity as a backbone network. The search regions and target template are fed into the vision Mamba network for simultaneous feature extraction and interaction. The output tokens of search regions will be fed into the tracking head for target localization. More importantly, we consider introducing a dynamic template update strategy into the tracking framework using the Memory Mamba network. By considering the diversity of samples in the target template library and making appropriate adjustments to the template memory module, a more effective dynamic template can be integrated. The effective combination of dynamic and static templates allows our Mamba-based tracking algorithm to achieve a good balance between accuracy and computational cost on multiple large-scale datasets, including EventVOT, VisEvent, and FE240hz. Source code of this work will be released.", "sections": [{"title": "1. Introduction", "content": "Event camera-based Visual Object Tracking (VOT) has drawn more and more attention in recent years with the release of large-scale benchmark datasets, e.g., EventVOT [33], VisEvent [31], etc. Echoing the approach of RGB-based tracking, this task aims to capture the position (x, y, w, h) of the initialized target object within subsequent event streams as the object moves. Due to the unique imaging principle of the event camera, which outputs asynchronous streams of pulses, meaning that each pixel generates information independently without interfering with one another. It performs better than the RGB cameras on the low energy consumption, high dynamic range, and dense temporal resolution. Therefore, tracking algorithms based on event cameras are expected to be applied in more challenging scenarios, such as large-scale intelligent surveillance, military fields, and aerospace.\nTracking algorithms based on event cameras can be divided into two major categories: RGB-Event multi-modal visual tracking [11, 31, 44], and visual tracking using only"}, {"title": "2. Related Work", "content": "Recently, the State Space Model (SSM) has become one of the most popular models in the fields of computer vision and artificial intelligence, due to its lower computational complexity O(N) and still commendable performance [13, 17, 20, 32, 48]. To be specific, Vim [48] and VMamba [20] are pioneering efforts in adapting the Mamba model for the visual domain, demonstrating its efficacy across a range of tasks. It also performs well for video-based recognition tasks, such as VideoMamba [17]. Inspired by these works, in this paper, we propose a novel Mamba-based Visual Object Tracking algorithm based on Event cameras, termed MambaEVT. As shown in Fig. 2, we first extract the static template and search regions from the given event streams and project them into event tokens. Then, the vision Mamba is adopted for feature extraction and interactive learning. In this procedure, both forward and backward directions are taken into account, allowing the learned visual features to be free from directional interference, resulting in more accurate and robust. The output event tokens will be fed into a tracking head for target object localization. Similar operations are conducted until the end of the event stream.\nMore importantly, we design a new adaptive update strategy for dynamic template generation using a Memory Mamba. In our implementation, we collect the features corresponding to tracking results and maintain a template library, deciding whether to update it based on appearance diversity. Leveraging the powerful modeling capabilities of the Mamba model for long sequences, we propose Memory Mamba to fuse the sample features within the target template library, thereby accomplishing the generation of dynamic templates. This assists the tracker in better adapting to significant appearance changes of the object, minimizing their impact on the tracking results. Unlike conventional dynamic update mechanisms [38] (which are only deployed during the test phase and are not learnable), our method is trainable, allowing for better utilization of training data to enhance its modeling capabilities. Consequently, during the practical tracking phase, we perform feature learning on both the dynamically updated templates and the initially given static templates to achieve more accurate and robust tracking results.\nTo sum up, the main contributions of this paper can be summarized as the following three perspectives:\n1). We propose a novel Mamba-based event tracking framework, termed MambaEVT, which achieves a good trade-off between accuracy and computational cost. To the best of our knowledge, it is the first Mamba-based tracking framework using an Event camera.\n2). We propose a simple but effective dynamic template update strategy using the Memory Mamba network. Our template update module is learnable and further enhances the final tracking performance.\n3). Extensive experiments on multiple large-scale event-based tracking datasets (i.e., EventVOT, VisEvent, FE240hz) fully validated the effectiveness and efficiency of our proposed Mamba-based visual object tracking framework."}, {"title": "2.1. Event based Visual Object Tracking", "content": "Recently, event-based visual object tracking has emerged as a significant research topic. In early works, Huang et al. [12] proposed an event-guided support vector machine (ESVM) for tracking high-speed moving objects. Glover et al. [8] introduced a particle filter designed for event-based tracking in dynamic environments using event cameras. Mitrokhin et al. [25] introduced an object-tracking method using event-based vision sensors that capture dynamic data and detect moving objects. Gehrig et al. [7] presented a method that combines event cameras with standard cameras for enhanced visual tracking using a maximum-likelihood approach, resulting in more accurate and longer tracks. Jiang et al. [14] introduced a new \"motion feature\" derived from event cameras, utilizing the Surface of Active Events to capture pixel-level motion on the image plane. The work of Jiao et al. [15] compare two types of image representations for event camera-based tracking and introduce a check for weaknesses, along with an improved tracker that combines the strengths of both representations. Zhang et al. [42] introduce a novel frame-event fusion approach for single object tracking, enhancing performance in challenging conditions such as high dynamic range, low light, and fast motion. Then, Zhang et al. [43] proposed a spiking Transformer network (STNet) for single object tracking, effectively extracting and fusing temporal and spatial information. KLT Tracker [24] introduced the first data-driven feature tracker for event cameras, integrating low-latency events with grayscale frames. Wang et al. [31] propose a simple and effective baseline tracker, which can fully utilize the unique information of different modalities for robust tracking by developing a cross-modal transformation module. Tang et al. [28] propose CEUTrack, which is an adaptive unified tracking method based on the Transformer network with a one-stage backbone network. Wang et al. [33] propose a novel hierarchical cross-modality knowledge distillation approach for an event-based tracking problem, called HDETrack. They exploit the knowledge transfer from multi-modal / multi-view to an unimodal event-based tracker. Unlike the mainstream Transformer-based tracking algorithms prevalent in recent years, our work stands out by exclusively adopting a pure Mamba-style approach for efficient tracking. Our Memory Mamba network also helps the event-based tracking via dynamic template update."}, {"title": "2.2. Dynamic Template Updating", "content": "In the field of visual object tracking, the dynamic updating of template information is frequently adopted according to the mechanisms of memory networks. In the early stages of research, Zheng et al. [46] enhanced video tracking systems by combining dynamic template updating with Kalman filter-based tracking. Yang et al. [40] proposed a dynamic memory network that uses an LSTM with spatial attention to adapt the template to target appearance changes during tracking. Memformer [35] is an efficient neural network for sequence modeling, which utilizes a fixed-size external dynamic memory to encode and retrieve past information. It efficiently processes long sequences with linear time complexity and constant memory complexity. STARK [38] updates the dynamic template to capture appearance changes of tracking targets, rather than updating object queries like typical Transformer-based models. Liu et al. [19] propose a template update mechanism designed to enhance visual tracking accuracy, specifically to address the issue of tracking failure in cluttered backgrounds. Cui et al. [4] propose a Score Prediction Module (SPM) to select reliable online templates based on predicted scores, achieving efficient online tracking. Yang et al. [39] propose a Siamese tracking network with dynamic template updating to adapt to appearance changes, improving accuracy in tracking small objects in satellite videos. Zhang et al. [45] propose a robust tracking method centered on dynamic template updating (DTU), enhancing the original Siamese network with new branches and feedback links. Wang et al. [30] also introduce tracking results to build a dynamic template pool for accurate object tracking. Different from previous studies, we incorporate a Memory Mamba network to serve as the dynamic template updating module, designed to capture the temporal changes in the appearance features of targets. Our goal is to improve tracking performance with a simple and efficient method."}, {"title": "2.3. State Space Model", "content": "Research in state space models (SSMs) has also surged recently. It inherits the ability of the classic Structured State-Space Sequence (S4) [10] model to handle long sequences and enhances content-based reasoning. However, the initial SSMs could only be used for one-dimensional sequence modeling, such as text sequence modeling in the field of NLP. Subsequently, Nguyen et al. [26] propose to extend 1D sequence modeling to 2D and 3D visual tasks such as images and videos. Furthermore, Gu et al. [9] introduce a selective scanning mechanism based on SSM, called Mamba, which greatly improves the modeling ability of sequences. Based on this, a large number of visual works based on Mamba have flooded in. Vision Mamba [48] and VMamba [20] have successively demonstrated the effectiveness of Mamba in visual tasks, which using a bidirectional scanning mechanism and a four-way scanning mechanism, respectively. Subsequently, in the field of video, Video Mamba [17] added the dimension of time to handle 3D video tasks and effectively model video sequences. In this work, we propose a fully Mamba-style framework based on Vision Mamba. In addition to the Mamba module used for feature extraction, an additional Mamba module generates high-quality dynamic template information through a dynamic template library, which then serves as essential support for subsequent feature extraction. The design enables efficient parameter utilization and superior performance in event-based tracking."}, {"title": "3. Methodology", "content": "As shown in Fig. 2, our proposed event-based visual tracker, i.e., MambaEVT, follows the Siamese tracking framework. We stack the event streams into event frames and crop the target template and search region out as the input. Then, a projection layer is adopted to embed these inputs into event tokens and the output will be fed into the vision Mamba backbone network for feature extraction and interactive learning. We propose a tracking head that takes the tokens corresponding to the search region as the input for target object localization. In addition, we also propose a new dynamic template generation module using Memory Mamba to address the significant appearance variations in the tracking task. More details of the modules mentioned in this procedure will be introduced in the subsequent subsections, respectively."}, {"title": "3.1. Preliminary: Mamba", "content": "The raw state space model is developed for the continuous system. It maps a one-dimensional function or sequence"}, {"title": "3.2. Overview", "content": null}, {"title": "3.3. Input Representation", "content": "In this work, we denote the event streams as $\\mathcal{E} = \\{\\mathbf{e}_1, \\mathbf{e}_2, ..., \\mathbf{e}_M\\}$, where $\\mathbf{e}_i = [x, y,t,p]$ represents each event point asynchronously launched, $(x, y)$ denotes the spatial coordinates, $t$ is the timestamp, $p$ is the polarity (i.e, ON/OFF event), $i \\in[1, M]$, $M$ is the number of event points in the current sample. We obtain the event image $\\mathcal{E}_F \\in \\mathbb{R}^{3\\times W\\times H}$ by splitting the event stream into fixed-length sequences and using these event images as the inputs"}, {"title": "3.4. Mamba-based Tracking", "content": "Given the search region $\\mathcal{E}_s$ and static template $\\mathcal{E}_t$, we first embed and flatten them into one-dimensional tokens, denoted as $\\mathcal{S}_t\\in \\mathbb{R}^{N_x\\times C}$ and $\\mathcal{T}_O \\in \\mathbb{R}^{N_z\\times C'}$, respectively. Here, $N_z = \\frac{W_z \\times H_z}{P^2}$ and $N_x = \\frac{W_x \\times H_x}{P^2}$. Note that, the position embeddings are then added to the patch embeddings of both the initial template and the search region. The resulting token sequences are concatenated with dynamic features to form $\\mathcal{E}_f = [\\mathcal{T}_D, \\mathcal{E}_s, \\mathcal{S}_t]$. Inspired by the success of unified backbone networks for tracking, e.g., OSTrack [41] and CEUTrack [28], we pass the concatenated feature $\\mathcal{E}_f$ through the Vision Mamba block (Vim, for short) [48] backbone to achieve feature extraction, interaction, and fusion, simultaneously. Let's denote the input of the l-th Vim layer is $E^{l-1}$, the forward propagation procedure can be formulated as:\n$\\mathcal{E}^l = Vim \\big(\\mathcal{E}^{l-1}\\big) + \\mathcal{E}^{l-1}, l = \\{1,2,..., L\\}$.\nThen, we normalize the output tokens $\\mathcal{E}^L$ and feed them into the multi-layer perception (MLP) to produce the final features, i.e.,\n$\\mathcal{E}_f = MLP(Norm(\\mathcal{E}^L))$.\nWe adopt both forward and backward directions to learn a more robust deep representation to make our tracker free from directional interference, as illustrated in Fig. 2.\nFinally, we crop the event tokens corresponding to search regions from the enhanced feature $\\mathcal{E}_f$, denoted as $\\mathcal{S}_t$, and feed them into the tracking head. In our implementation, the tracking head is implemented as a Fully Convolutional Network (FCN), which is composed of a series of stacked Conv-BN-RELU layers. The FCN produces three key outputs: 1) a target classification score map that indicates the likelihood of each spatial location being the target. 2) a local offset to correct for discretization errors due to reduced resolution. 3) a normalized bounding box size that specifies the width and height for the final target detection."}, {"title": "3.5. Dynamic Template Update Strategy", "content": "Although the aforementioned Mamba-based event tracker already achieves a good performance, however, it is still sensitive to challenging factors such as significant appearance variation, and clutter background. Obviously, the tracking performance can be boosted when these issues can be handled. In this work, we propose the Memory Mamba to collect the tracking results and generate an adaptive dynamic template to guide the tracker. Two core aspects need to be considered when designing this module, i.e., Template Library Update, Dynamic Template Generation via Memory Mamba.\n\\bullet Template Library Update. To achieve dynamic template generation, we need to maintain a template library (i.e., image patches corresponding to tracking results in the inference phase). Inspired by THOR [27], we also propose to store the templates using Long-Term memory (LT) and Short-Term memory (ST) libraries. Specifically, the ST is a queue-like list that takes in the cropped image from the prediction of the search image, sampling templates at fixed intervals. The LT handles long-range memory, sampling based on a similarity-based strategy. The incoming template is evaluated against the existing features stored in both memory libraries. The similarity of the new template is determined by comparing it with each feature in both the long-term and short-term memory libraries. After identifying the pair with the highest similarity, the corresponding memory library, either LT or ST, is selected and sent as a sequence into the Memory Mamba network to generate a dynamic feature.\nThe new dynamic template is added to the ST at regular intervals. Once the ST memory is at full capacity, which is typically fully initialized, a template is removed. The removed template is transferred to the LT only if replacing an existing template increases the Gram determinant of the LT's current matrix. Formally, for a new template $\\mathbf{z}_{new}$, the LT is a set of $\\{\\mathbf{z}_1, \\mathbf{z}_2, ..., \\mathbf{z}_n \\}$, where $n$ is the capacity of the LT. The Gram matrix of the LT can be formulated as:\n$G(\\mathbf{z}_1,..., \\mathbf{z}_n) = \\begin{pmatrix} \\mathbf{z}_1 * \\mathbf{z}_1 & \\mathbf{z}_1 * \\mathbf{z}_2 & ... & \\mathbf{z}_1 * \\mathbf{z}_n\\\\ : & : & & : \\\\ \\mathbf{z}_n * \\mathbf{z}_1 & \\mathbf{z}_n * \\mathbf{z}_2 & ... & \\mathbf{z}_n * \\mathbf{z}_n \\end{pmatrix}$.\nwhere $G$ is a square n$\\times$n matrix and $*$ denotes the similarity measure method. Approximately, the determinant of $G$ can be regarded as a measure of the diversity of the matrix. In this work, we utilize the Pearson linear correlation method as the similarity measure for paired template features.\n\\bullet Dynamic Template Generation via Memory Mamba. Given the short-term and long-term memory library, we propose a new Memory Mamba network to fuse these templates into a single one considering its strong modelling ability for the long sequence. The"}, {"title": "3.6. Loss Function", "content": "The loss functions utilized in our framework consist of a weighted combination of focal loss $\\mathcal{L}_{focal}$, L1 loss $\\mathcal{L}_{1}$, and Generalized Intersection over Union (GIOU) loss $\\mathcal{L}_{GIOU}$. The overall loss function is defined as follows:\n$\\mathcal{L} = \\lambda_1 \\mathcal{L}_1 + \\lambda_2 \\mathcal{L}_{focal} + \\lambda_3 \\mathcal{L}_{GIOU}$\nwhere the trade-off parameters are set as $\\lambda_1 = 5$, $\\lambda_2 = 1$, and $\\lambda_3 = 2$ in our experiments."}, {"title": "4. Experiment", "content": null}, {"title": "4.1. Datasets and Evaluation Metrics", "content": "To validate the effectiveness and superiority of the proposed MambaEVT method, in this work, we conduct extensive experiments on three event-based tracking datasets, including EventVOT [33], FE240hz [42] and VisEvent [31]. We provide a brief introduction to these event-based tracking datasets in our supplementary materials.\nFor the evaluation metrics, we use the widely recognized Precision Rate (PR), Normalized Precision Rate (NPR), and Success Rate (SR)."}, {"title": "4.2. Implementation Details", "content": "Our proposed tracker is implemented using Python based on PyTorch framework. The experiments are conducted on a server with NVIDIA RTX 3090 GPUs. More in detail, two variants are proposed, i.e., MambaEVT and MambaEVT-P. MambaEVT indicates that the Memory Mamba (Mem-Mamba) module and the backbone network share parameters, while MambaEVT-P indicates that they do not. In our experiments, the MambaEVT is adopted as the default tracker, unless otherwise noted. During the training phase, we resize the templates and search regions to 128 x 128 and 256 \u00d7 256 pixels, respectively. We initialize the backbone network using the Vim-S model pre-trained on ImageNet-1K dataset and train it for 50 epochs. We use AdamW [22] optimizer with weight decay $1 \\times 10^{-4}$. The initial learning rate is set to 0.0004, with a decline beginning after 30 epochs.\nFor the training sample preparation, we denote the number of dynamic templates as K and set K to 7 during the training phase by default. We randomly sampled K + 2 images from the entire sequence, selecting K as dynamic templates, and the remaining images served as the static template and search region, respectively. This design aims to stimulate different states of long-term (LT) or short-term (ST) for better interactive learning. During testing, both LT and ST are fully populated with the initial template from the starting frame of the tracking process and are subsequently updated at regular intervals using the Memory Mamba network. The experimental results in the subsequent sections demonstrate the effectiveness of the consistency of training and testing. The sampling interval is set to 5. Both LT and ST memories have a default capacity of 10. More details can be found in our source code."}, {"title": "4.3. Comparison with the State-of-the-art", "content": "To verify the effectiveness of our proposed method, we compare several SOTA trackers on the EventVOT dataset.\nWe compare our proposed MambaEVT with several SOTA trackers on the VisEvent dataset. MambaEVT achieves SR/PR/NPR scores of 35.9/50.2/39.4, while its variant, MambaEVT-P, attains higher scores of 37.2/51.8/40.8. This improvement is likely due to the need for more parameters to capture key features when handling variable-length sequences on the VisEvent dataset. While the top trackers, HIPTrack (37.2/52.0/41.1) and AQATrack (38.7/53.6/42.6), achieve higher scores, MambaEVT shows competitive tracking performance with significantly fewer parameters. These results underscore the strength of our method in balancing performance with parameter efficiency, making it a scalable and efficient solution across various scenarios, despite its slightly lower performance."}, {"title": "4.4. Ablation Study", "content": "Component Analysis. To analyze the effectiveness of different components in the proposed MambaEVT model, we implement three variants for the analysis of the ablation study. 'Variant #1' model indicates that we just use the Mamba model to replace the backbone network in [41] as the baseline network. \u2018Variant #2' model represents we added a Memory Mamba (MM) module with shared parameters in the 'Variant #1' model. Variant #3' model integrates the Memory Library (ML) strategy into the 'Variant #1' model, facilitating the selection of long-term and short-term templates. 'Variant #4' model combines MM module and ML strategy within 'Variant #1'model, i.e., MambaEVT model. The experimental results are presented in Table 4 on the EventVOT dataset. We can observe that, 1) Compared with 'Variant #1' and 'Variant #2', it demonstrates that the incorporation of the MM module significantly improves the model performance. 2) Compared with the 'Variant #1' and 'Variant #3' model, the results indicate that the application of the Memory Library strategy led to a decline in model performance, likely due to a misalignment between the training and testing phases. 3) When both the Memory Mamba module and the Memory Library strategy are integrated into the baseline network (i.e., 'Variant #4'), the performance surpasses that of the other three variants. In other words, these results demonstrate the effectiveness of each component and that they are used together to achieve the best model performance.\nAnalysis of Different Backbone Networks. As illustrated in Table 5, we conduct the performance analysis of different backbone networks, including Transformer model (ViT-S [6], and Swan-S [21]), Mamba backbone networks"}, {"title": "4.5. Visualization", "content": "In addition to the numerical evaluations, we carry out a detailed visual examination of the proposed tracking algorithm to facilitate a more profound comprehension of our framework. As illustrated in Fig. 5, we compare the tracking results of our proposed MambaEVT method with other SOTA trackers, such as OSTrack, ARTrack, ODTrack, and Stark, on the EventVOT dataset. The results indicate the unique challenges and complexities of tracking with an event camera. Although these trackers perform effectively in simple scenarios, there remains a substantial opportunity for enhancement. Additionally, we visualize the response maps of the tracking head on multiple videos, including Mouse, Bag, and Basketball. As illustrated in Fig. 4, our tracker effectively highlights the target object regions, confirming its ability to accurately focus on the targets. This effectiveness is particularly evident in scenarios with significant background changes, where our tracker maintains strong performance by consistently distinguishing the target from its surroundings."}, {"title": "4.6. Limitation Analysis", "content": "Although our tracker achieves a good balance between performance and the number of parameters, it suffers from a relatively low FPS. Further design improvements are necessary to enhance the tracking speed. Additionally, as the Mamba model is designed for long sequence modeling, our current training process may not fully leverage its capabilities, potentially limiting its overall performance."}, {"title": "5. Conclusion", "content": "The Mamba-based visual tracking framework introduced in this paper represents a significant advancement in event camera-based tracking systems. We have achieved simultaneous feature extraction and interaction for search regions and target templates by leveraging the state space model with linear complexity and lead to improved target localization. The integration of a dynamic template update strategy through the Memory Mamba network has proven to be a crucial step in enhancing tracking performance. By dynamically adjusting the template memory module to account for the diversity of samples in the target template library, we have been able to maintain a more effective dynamic template. This innovative approach has allowed our tracking algorithm to strike a favorable balance between tracking accuracy and computational efficiency. The robustness and effectiveness of our method have been demonstrated across multiple large-scale datasets, including EventVOT, VisEvent, and FE240hz. We believe that this work not only contributes to the current state-of-the-art in event-based visual tracking but also paves the way for future research in this field. The source code of this work will be released to facilitate further research and development in event camera-based tracking systems. In our future work, we will further improve the running efficiency of our proposed tracker and decrease the energy consumption in the tracking phase (by spiking neural networks)."}]}