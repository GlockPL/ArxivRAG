{"title": "MambaEVT: Event Stream based Visual Object Tracking using State Space Model", "authors": ["Xiao Wang", "Chao Wang", "Shiao Wang", "Xixi Wang", "Zhicheng Zhao", "Lin Zhu", "Bo Jiang"], "abstract": "Event camera-based visual tracking has drawn more and more attention in recent years due to the unique imaging principle and advantages of low energy consumption, high dynamic range, and dense temporal resolution. Current event-based tracking algorithms are gradually hitting their performance bottlenecks, due to the utilization of vision Transformer and the static template for target object localization. In this paper, we propose a novel Mamba-based visual tracking framework that adopts the state space model with linear complexity as a backbone network. The search regions and target template are fed into the vision Mamba network for simultaneous feature extraction and interaction. The output tokens of search regions will be fed into the tracking head for target localization. More importantly, we consider introducing a dynamic template update strategy into the tracking framework using the Memory Mamba network. By considering the diversity of samples in the target template library and making appropriate adjustments to the template memory module, a more effective dynamic template can be integrated. The effective combination of dynamic and static templates allows our Mamba-based tracking algorithm to achieve a good balance between accuracy and computational cost on multiple large-scale datasets, including EventVOT, VisEvent, and FE240hz. Source code of this work will be released.", "sections": [{"title": "1. Introduction", "content": "Event camera-based Visual Object Tracking (VOT) has drawn more and more attention in recent years with the release of large-scale benchmark datasets, e.g.,"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Event based Visual Object Tracking", "content": "Recently, event-based visual object tracking has emerged as a significant research topic. In early works, Huang et al. [12] proposed an event-guided support vector machine (ESVM) for tracking high-speed moving objects. Glover et al. [8] introduced a particle filter designed for event-based tracking in dynamic environments using event cam-eras. Mitrokhin et al. [25] introduced an object-tracking method using event-based vision sensors that capture dy-namic data and detect moving objects. Gehrig et al. [7] pre-sented a method that combines event cameras with standard"}, {"title": "2.2. Dynamic Template Updating", "content": "In the field of visual object tracking, the dynamic up-dating of template information is frequently adopted ac-cording to the mechanisms of memory networks. In the early stages of research, Zheng et al. [46] enhanced video tracking systems by combining dynamic template updat-ing with Kalman filter-based tracking. Yang et al. [40] proposed a dynamic memory network that uses an LSTM with spatial attention to adapt the template to target appear-ance changes during tracking. Memformer [35] is an ef-ficient neural network for sequence modeling, which uti-lizes a fixed-size external dynamic memory to encode and retrieve past information. It efficiently processes long se-quences with linear time complexity and constant memory complexity. STARK [38] updates the dynamic template to capture appearance changes of tracking targets, rather than updating object queries like typical Transformer-based models. Liu et al. [19] propose a template update mecha-"}, {"title": "2.3. State Space Model", "content": "Research in state space models (SSMs) has also surged recently. It inherits the ability of the classic Structured State-Space Sequence (S4) [10] model to handle long se-quences and enhances content-based reasoning. However, the initial SSMs could only be used for one-dimensional sequence modeling, such as text sequence modeling in the field of NLP. Subsequently, Nguyen et al. [26] propose to extend 1D sequence modeling to 2D and 3D visual tasks such as images and videos. Furthermore, Gu et al. [9] intro-duce a selective scanning mechanism based on SSM, called Mamba, which greatly improves the modeling ability of se-quences. Based on this, a large number of visual works based on Mamba have flooded in. Vision Mamba [48] and VMamba [20] have successively demonstrated the effec-tiveness of Mamba in visual tasks, which using a bidirec-tional scanning mechanism and a four-way scanning mech-anism, respectively. Subsequently, in the field of video, Video Mamba [17] added the dimension of time to han-dle 3D video tasks and effectively model video sequences. In this work, we propose a fully Mamba-style framework based on Vision Mamba. In addition to the Mamba mod-ule used for feature extraction, an additional Mamba mod-ule generates high-quality dynamic template information through a dynamic template library, which then serves as essential support for subsequent feature extraction. The de-sign enables efficient parameter utilization and superior per-formance in event-based tracking."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Preliminary: Mamba", "content": "The raw state space model is developed for the continu-ous system. It maps a one-dimensional function or sequence"}, {"title": "3.2. Overview", "content": "As shown in Fig. 2, our proposed event-based visual tracker, i.e., MambaEVT, follows the Siamese tracking framework. We stack the event streams into event frames and crop the target template and search region out as the input. Then, a projection layer is adopted to embed these inputs into event tokens and the output will be fed into the vision Mamba backbone network for feature extraction and interactive learning. We propose a tracking head that takes the tokens corresponding to the search region as the input for target object localization. In addition, we also propose a new dynamic template generation module using Memory Mamba to address the significant appearance variations in the tracking task. More details of the modules mentioned in this procedure will be introduced in the subsequent sub-sections, respectively."}, {"title": "3.3. Input Representation", "content": "In this work, we denote the event streams as $E = {e_1, e_2, ..., e_M}$, where $e_i = [x, y,t,p]$ represents each event point asynchronously launched, $(x, y)$ denotes the spatial coordinates, t is the timestamp, p is the polarity (i.e, ON/OFF event), $i \\in[1, M]$, M is the number of event points in the current sample. We obtain the event image $E_F \\in R^{3\\times W\\times H}$ by splitting the event stream into fixed-length sequences and using these event images as the inputs"}, {"title": "3.4. Mamba-based Tracking", "content": "Given the search region $\\mathcal{E}^x$ and static template $\\mathcal{E}^z$, we first embed and flatten them into one-dimensional tokens, denoted as $S_t\\in R^{N_x\\times C}$ and $T_0\\in R^{N_z\\times C'}$, respectively. Here, $N_z = \\frac{W_z \\times H_z}{P^2}$ and $N_x = \\frac{W_x \\times H_x}{P^2}$. Note that, the position embeddings are then added to the patch em-beddings of both the initial template and the search region. The resulting token sequences are concatenated with dy-namic features to form $E_f = [T^D, \\mathcal{E}^x, S_t]$. Inspired by the success of unified backbone networks for tracking, e.g., OSTrack [41] and CEUTrack [28], we pass the concate-nated feature $E_f$ through the Vision Mamba block (Vim, for short) [48] backbone to achieve feature extraction, interac-tion, and fusion, simultaneously. Let's denote the input of the $l$-th Vim layer is $E^{l-1}$, the forward propagation proce-dure can be formulated as:\n$E^l = Vim (E^{l-1}) + E^{l-1}, l = {1,2,..., L}$.\nThen, we normalize the output tokens $E^l_t$ and feed them into the multi-layer perception (MLP) to produce the final fea-tures, i.e.,\n$E^f = MLP(Norm(E^l_t))$,\nWe adopt both forward and backward directions to learn a more robust deep representation to make our tracker free from directional interference, as illustrated in Fig. 2.\nFinally, we crop the event tokens corresponding to search regions from the enhanced feature $E_f$, denoted as $S_t^f$, and feed them into the tracking head. In our implementation, the tracking head is implemented as a Fully Convolutional Network (FCN), which is composed of a series of stacked Conv-BN-RELU layers. The FCN produces three key out-puts: 1) a target classification score map that indicates the likelihood of each spatial location being the target. 2) a lo-cal offset to correct for discretization errors due to reduced resolution. 3) a normalized bounding box size that specifies the width and height for the final target detection."}, {"title": "3.5. Dynamic Template Update Strategy", "content": "Although the aforementioned Mamba-based event tracker already achieves a good performance, however, it is still sensitive to challenging factors such as significant ap-pearance variation, and clutter background. Obviously, the tracking performance can be boosted when these issues can be handled. In this work, we propose the Memory Mamba to collect the tracking results and generate an adaptive dy-namic template to guide the tracker. Two core aspects need to be considered when designing this module, i.e., Template Library Update, Dynamic Template Generation via Mem-ory Mamba.\n\u2022 Template Library Update. To achieve dynamic tem-plate generation, we need to maintain a template library (i.e., image patches corresponding to tracking results in the inference phase). Inspired by THOR [27], we also propose to store the templates using Long-Term memory (LT) and Short-Term memory (ST) libraries. Specifically, the ST is a queue-like list that takes in the cropped image from the prediction of the search image, sampling templates at fixed intervals. The LT handles long-range memory, sampling based on a similarity-based strategy. The incoming tem-plate is evaluated against the existing features stored in both memory libraries. The similarity of the new template is de-termined by comparing it with each feature in both the long-term and short-term memory libraries. After identifying the pair with the highest similarity, the corresponding memory library, either LT or ST, is selected and sent as a sequence into the Memory Mamba network to generate a dynamic feature.\nThe new dynamic template is added to the ST at regular intervals. Once the ST memory is at full capacity, which is typically fully initialized, a template is removed. The removed template is transferred to the LT only if replacing an existing template increases the Gram determinant of the LT's current matrix. Formally, for a new template $Z_{new}$, the LT is a set of ${z_1, z_2, ..., z_n }$, where n is the capacity of the LT. The Gram matrix of the LT can be formulated as:\n$G(z_1,..., z_n) =\\begin{bmatrix}z_1 * z_1 & z_1 * z_2 & ... & z_1 * z_n\\\\vdots & : & : & :\\\\z_n * z_1 & z_n * z_2 & ... & z_n * z_n\\end{bmatrix}$\nwhere G is a square n\u00d7n matrix and * denotes the similarity measure method. Approximately, the determinant of G can be regarded as a measure of the diversity of the matrix. In this work, we utilize the Pearson linear correlation method as the similarity measure for paired template features.\n\u2022 Dynamic Template Generation via Memory Mamba. Given the short-term and long-term mem-ory library, we propose a new Memory Mamba network to fuse these templates into a single one considering its strong modelling ability for the long sequence. The"}, {"title": "3.6. Loss Function", "content": "The loss functions utilized in our framework consist of a weighted combination of focal loss $L_{focal}$, $L_1$ loss $L_1$, and Generalized Intersection over Union (GIOU) loss $L_{GIOU}$. The overall loss function is defined as follows:\n$\\mathcal{L} = \\lambda_1 L_1 + \\lambda_2 L_{focal} + \\lambda_3 L_{GIOU}$ where the trade-off parameters are set as $\\lambda_1$ = 5, $\\lambda_2$ = 1, and $\\lambda_3$ = 2 in our experiments."}, {"title": "4. Experiment", "content": ""}, {"title": "4.1. Datasets and Evaluation Metrics", "content": "To validate the effectiveness and superiority of the pro-posed MambaEVT method, in this work, we conduct exten-sive experiments on three event-based tracking datasets, in-cluding EventVOT [33], FE240hz [42] and VisEvent [31]. We provide a brief introduction to these event-based track-ing datasets in our supplementary materials.\nFor the evaluation metrics, we use the widely recognized Precision Rate (PR), Normalized Precision Rate (NPR), and Success Rate (SR)."}, {"title": "4.2. Implementation Details", "content": "Our proposed tracker is implemented using Python based on PyTorch framework. The experiments are conducted on a server with NVIDIA RTX 3090 GPUs. More in detail, two variants are proposed, i.e., MambaEVT and MambaEVT-P. MambaEVT indicates that the Memory Mamba (Mem-Mamba) module and the backbone network share parameters, while MambaEVT-P indicates that they do not. In our experiments, the MambaEVT is adopted as the default tracker, unless otherwise noted. During the training phase, we resize the templates and search regions to 128 x 128 and 256 \u00d7 256 pixels, respectively. We initialize"}, {"title": "4.3. Comparison with the State-of-the-art", "content": "\u2022 Results on EventVOT dataset. To verify the effective-ness of our proposed method, we compare several SOTA trackers on the EventVOT dataset. As shown in Table 2, our proposed MambaEVT achieves the performance of 56.5 in SR metric, 56.7 in PR metric, and 65.5 in NPR metric, respectively. Compared with OSTrack [41] method based on ViT [6] model, our proposed MambaEVT improves the SR/PR/NPR scores by 1.1, 0.3, and 0.3 points, while sig-nificantly reducing the number of parameters from 92.1M to 29.3M. Although ODTrack [47] and AQATrack [36] show better performance with scores of 57.8/58.3/66.6 and 56.8/58.1/66.2, they require significantly more parameters, totaling 72M and 92M. Our proposed MambaEVT needs just 29.3M parameters while still achieving competitive tracking performance. These results demonstrate that the proposed MambaEVT model is not only effective but also parameter-efficient.\n\u2022 Results on FE240hz dataset. We compare our pro-posed MambaEVT method with several SOTA trackers on the FE240hz dataset in Table 1. Our propose MambaEVT achieves the performance of 58.09 in SR metric and 91.97 in PR metric. While AQATrack and ARTrack achieve higher performance, they require significantly more parameters. Our proposed MambaEVT, on the other hand, strikes an excellent balance between performance and parameter ef-ficiency. Specifically, it outperforms ARTrack on the PR metric by 0.57 and falls short of AQATrack on the same metric by only 0.60. These results highlight the effective-ness of our proposed approach.\n\u2022 Results on VisEvent dataset. As shown in Table 3,"}, {"title": "4.4. Ablation Study", "content": "\u2022 Component Analysis. To analyze the effectiveness of different components in the proposed MambaEVT model, we implement three variants for the analysis of the abla-tion study. 'Variant #1' model indicates that we just use the Mamba model to replace the backbone network in [41] as the baseline network. \u2018Variant #2' model represents we added a Memory Mamba (MM) module with shared param-eters in the 'Variant #1' model. Variant #3' model inte-grates the Memory Library (ML) strategy into the 'Variant #1' model, facilitating the selection of long-term and short-term templates. 'Variant #4' model combines MM mod-ule and ML strategy within 'Variant #1'model, i.e., Mam-baEVT model. The experimental results are presented in Table 4 on the EventVOT dataset. We can observe that, 1) Compared with 'Variant #1' and 'Variant #2', it demon-strates that the incorporation of the MM module signifi-cantly improves the model performance. 2) Compared with the 'Variant #1' and 'Variant #3' model, the results indicate that the application of the Memory Library strategy led to a decline in model performance, likely due to a misalignment between the training and testing phases. 3) When both the Memory Mamba module and the Memory Library strategy are integrated into the baseline network (i.e., 'Variant #4'), the performance surpasses that of the other three variants. In other words, these results demonstrate the effectiveness of each component and that they are used together to achieve the best model performance.\n\u2022 Analysis of Different Backbone Networks. As illus-trated in Table 5, we conduct the performance analysis of different backbone networks, including Transformer model (ViT-S [6], and Swin-S [21]), Mamba backbone networks"}, {"title": "4.5. Visualization", "content": "In addition to the numerical evaluations, we carry out a detailed visual examination of the proposed tracking al-gorithm to facilitate a more profound comprehension of our framework. As illustrated in Fig. 5, we compare the tracking results of our proposed MambaEVT method with other SOTA trackers, such as OSTrack, ARTrack, ODTrack, and Stark, on the EventVOT dataset. The results indicate the unique challenges and complexities of tracking with an event camera. Although these trackers perform effectively in simple scenarios, there remains a substantial opportunity for enhancement. Additionally, we visualize the response maps of the tracking head on multiple videos, including Mouse, Bag, and Basketball. As illustrated in Fig. 4, our tracker effectively highlights the target object regions, con-firming its ability to accurately focus on the targets. This effectiveness is particularly evident in scenarios with sig-nificant background changes, where our tracker maintains strong performance by consistently distinguishing the tar-get from its surroundings."}, {"title": "4.6. Limitation Analysis", "content": "Although our tracker achieves a good balance between performance and the number of parameters, it suffers from a relatively low FPS. Further design improvements are nec-essary to enhance the tracking speed. Additionally, as the Mamba model is designed for long sequence modeling, our current training process may not fully leverage its capabili-ties, potentially limiting its overall performance."}, {"title": "5. Conclusion", "content": "The Mamba-based visual tracking framework introduced in this paper represents a significant advancement in event camera-based tracking systems. We have achieved simulta-neous feature extraction and interaction for search regions and target templates by leveraging the state space model"}]}