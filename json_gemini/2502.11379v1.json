{"title": "CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models", "authors": ["Guanghao Zhou", "Panjia Qiu", "Mingyuan Fan", "Cen Chen", "Mingyuan Chu", "Xin Zhang", "Jun Zhou"], "abstract": "Despite explicit alignment efforts for large language models (LLMs), they can still be exploited to trigger unintended behaviors, a phenomenon known as \"jailbreaking.\" Current jailbreak attack methods mainly focus on discrete prompt manipulations targeting closed-source LLMs, relying on manually crafted prompt templates and persuasion rules. However, as the capabilities of open-source LLMs improve, ensuring their safety becomes increasingly crucial. In such an environment, the accessibility of model parameters and gradient information by potential attackers exacerbates the severity of jailbreak threats. To address this research gap, we propose a novel Context-Coherent Jailbreak Attack (CCJA). We define jailbreak attacks as an optimization problem within the embedding space of masked language models. Through combinatorial optimization, we effectively balance the jailbreak attack success rate with semantic coherence. Extensive evaluations show that our method not only maintains semantic consistency but also surpasses state-of-the-art baselines in attack effectiveness. Additionally, by integrating semantically coherent jailbreak prompts generated by our method into widely used black-box methodologies, we observe a notable enhancement in their success rates when targeting closed-source commercial LLMs. This highlights the security threat posed by open-source LLMs to commercial counterparts. We will open-source our code if the paper is accepted.", "sections": [{"title": "Introduction", "content": "The rise of commercial (eg., ChatGPT (Achiam et al., 2023)) and open-source large language models (LLMs), including Llama3 (AI@Meta, 2024) and Claude2 (Anthropic, 2023), not only improves human life but also drives paradigm shifts across various application domains (Wu et al., 2023; McDuff et al., 2023). While researchers use various alignment techniques to ensure LLMs adhere to human values (Achiam et al., 2023; Rafailov et al., 2024), ensuring their safety remains a critical area of research (Zhuo et al., 2023; Huang et al., 2023; Yang et al., 2023).\nWith the rapid advancement of open-source LLMs, the performance disparity between them and state-of-the-art closed-source LLMs is rapidly diminishing (Contributors, 2023). In open-source scenarios, attackers can access not only the outputs of LLMs but also the their weights and gradients. This undoubtedly provides attackers with additional potential information, thereby exacerbating the risk of malicious exploitation of open-source LLMs. Nevertheless, existing jailbreak attack methods predominantly concentrate on black-box scenarios, where only the output text of victim LLMs is available, leading to an overestimation of the safety robustness of open-source LLMs. AutoDAN (Zhu et al., 2023) highlights that semantically interpretable jailbreak prompts pose a greater threat in transferability. This suggests that identifying such prompts in open-source LLMs with low computational costs to conduct transfer attacks on closed-source commercial models may emerge as a significant security concern.\nBased on the granularity of text manipulation during the prompt search process, jailbreak attacks can be classified into two types. The first type is prompt-level attacks, which utilize carefully crafted instructions and principles of social engineering to construct jailbreak prompts that induce LLMs to generate unsafe content, primarily applied in black-box methods (Shen et al., 2023; Wei et al., 2024; Greshake et al., 2023). The second type is token-level attacks, which treat the search for jailbreak prompts as an optimization task, aiming to find effective jailbreak handles that prompt LLMs to respond affirmatively to harmful queries, thereby eliciting unsafe content. This approach is mainly applied in white-box methods (Zou et al., 2023; Liu"}, {"title": "Related Work", "content": "Jailbreak attacks can be categorized into black-box (Shen et al., 2023; Wei et al., 2024; Mehrotra et al., 2023; Greshake et al., 2023) and white-box attacks (Zou et al., 2023; Zhu et al., 2023; Sadasivan et al., 2024; Guo et al., 2024) based on the degree of access to the internal parameters of the target LLM. Inspired by social engineering principles, black-box attacks primarily create complex malicious queries by manually crafting high-quality jailbreak prompt templates. Additionally, some studies (Shah et al., 2023; Zeng et al., 2024a) employ persona customization and persuasion techniques to generate jailbreak prompts, leading LLMs to produce harmful responses.\nJailbroken(Wei et al., 2024) points out that existing jailbreak methods, such as prompt injection and refusal suppression, arise from the conflict between LLMs' instruction-following abilities and safety goals. Building on this, some white-box attacks optimize jailbreak prompts at the token level, forcing the LLM to affirmatively respond to harmful queries and produce unsafe outputs. GCG (Zou et al., 2023) uses greedy and gradient-based search techniques to optimize randomly initialized jailbreak suffixes, prompting the LLM to affirm harmful requests. However, the jailbreak prompts generated by this method lack readability. In contrast, AutoDAN (Zhu et al., 2023) employs a genetic algorithm to refine manually crafted jailbreak prompts. Although this method generates prompts with lower PPL, it depends on human-written templates and external models (e.g., GPT-4) for mutation, limiting scalability. BEAST (Sadasivan et al., 2024) and COLD-Attack (Guo et al., 2024) utilize beam search and gradient-based methods, respectively, to optimize the top k tokens generated by the victim LLM. Since the initial jailbreak suffix is merely a simple concatenation of the LLM's output tokens, the generated jailbreak prompts, while theoretically having low PPL, still lack sufficient semantic readability.\nTo better assess the safety vulnerabilities of open-source models in worst-case scenarios, we propose"}, {"title": "Jailbreak Method", "content": "In this section, we first formulate the jailbreak attack task for open-source models and then elaborate on our proposed jailbreak attack method."}, {"title": "Overview", "content": null}, {"title": "Preliminary", "content": "Let V represent the vocabulary of the LLM, $x \\in V$ denote a token, and $x_{1:n} = {X_1, X_2,\\cdots,X_n}$ signify a sequence of tokens. For an autoregressive LLM, the probability distribution for predicting the next token given a sequence of tokens is denoted as $p_M(\\cdot|x): V \\rightarrow [0, 1]$. The input prompt $P = x_{1:n}$ to the LLM generates a response $R = x_{n+1:n+j}$. We express $P = J \\oplus Q$, where $\\oplus$ indicates the concatenation of token sequences, $J = x_{1:m}$ is the jailbreak prefix, and $Q = x_{m+1:n}$ represents the malicious query. The goal of our jailbreak attack is to ensure that the response R contains harmful content closely related to the malicious query Q. It is unrealistic to specify a unique response R for each malicious query Q. Inspired by Jailbroken (Wei et al., 2024), we define R as an affirmative answer that incorporates Q. Therefore, our objective for the jailbreak attack is to optimize the jailbreak"}, {"title": "Continue word embedding space attacks", "content": "In this paper, we frame the jailbreak attack problem as an optimization task in the continuous word embedding space. Specifically, we represent the continuous embedding vectors in the embedding space as $t = {t_1, t_2,\u2026\u2026,t_n}$, where n represents the number of tokens and $t_i \\in R^d$ is the d-dimensional embedding of the i-th token. Given the jailbreak prefix $J = x_{1:m}$, our goal is to find the adversarial perturbation $\\delta \\in R^{m\\times d}$ in the embedding space that minimizes a predefined objective function, aiming to successfully execute the LLM jailbreak attack while ensuring that the decoded continuous vectors remain semantically readable."}, {"title": "The propose of CCJA", "content": "We decompose the local proxy masked language model f into three components: the embedding layer $f_e$, the hidden layer $f_h$, and the remaining decoder head H, which can be expressed as:\n$f(x) = H(f_h(f_e(x)))$.\nFor the input sequence x, the forward process of the local proxy masked language model in Equation 2 can be described as:\n$e = f_e(x), h = f_h(e), \\Theta = H(h)$,"}, {"title": "Jailbreak task", "content": "To save manpower and fully leverage the general capabilities acquired by the LLM during pretraining, we first use a seed prompt to guide the LLM in generating an instruction-following prefix x. According to Equation 3, the forward process of adding perturbations to the hidden state h of the initial prefix is as follows:\n$\\Theta = f(x, \\delta) = H(f_h(f_e(x)) + \\delta)$.\nTo ensure that the sampling process of $\\Theta$ is differentiable, we use the reparameterized Gumbel-softmax (Jang et al., 2017) G to replace the traditional softmax for probability sampling. Samples $\\pi = \\pi_1,\u00b7\u00b7\u00b7, \\pi_m$ from distribution $\\Theta$ are drawn according to the process:\n$\\pi_{i,j} := \\frac{exp \\left(\\left(\\Theta_{i,j} + g_{i,j}\\right) /T\\right)}{\\sum_{v=1}^{V} exp \\left(\\left(\\Theta_{i,v} + g_{i,v}\\right) /T\\right)}$,\nwhere V represents the size of the vocabulary, $g_{i,j} \\sim Gumbel(0, 1)$ and $T > 0$ are temperature parameters that control the smoothness of the Gumbel-softmax distribution. As $T \\rightarrow 0$, the distribution converges towards a one-hot distribution.\nDue to the significant difference between the LLM and MLM vocabularies, we use a mapping matrix M to establish a relationship between MLM and LLM token IDs. The construction of M is as follows:\n$M(i, j) = \\begin{cases}1 & \\text{if } i \\in x\\\\0 & \\text{if } i \\notin x\\end{cases}$"}, {"title": "Reconstruction task", "content": "The optimization of the perturbation d based solely on jailbreak loss can successfully trigger jailbreak, but it results in a significant semantic divergence between the jailbreak prefix J and the initial prefix x. Since x guides the LLM to perform subsequent instructions and is close to natural language, even a slight perturbation can trigger the jailbreak behavior. Therefore, in optimizing d, we constrain its search space to enhance the semantic similarity between J and x. The loss function L is decomposed into two parts:\n$L = (1 - \\beta)L_j(x, Q, R, \\delta) + \\beta L_d(x, \\delta)$,\nwhere $L_j$ as the jailbreak loss originates from Equation 7, ensuring the jailbreak performance of the prefix, $L_d = \\sum_{i=1}^{n} x_i log(\\Theta_i)$ represents the cross-entropy loss between the logistic distribution $\\Theta$ and x, $\\beta$ denotes the weight coefficients. We set the sum of the two loss components to 1 to ensure a more stable training process. We use the gradient descent optimization algorithm to optimize $\\delta$, which can be expressed as:\n$\\delta_i = \\delta_{i-1} - \\lambda \\frac{\\nabla_{\\delta} L}{\\|\\nabla_{\\delta} L\\|_F}$,\nwhere F denotes the Frobenius norm, and $\\lambda$ is the learning rate. As shown in step III of Figure 1, we decode the embedding vector with added perturbations in the MLM hidden state using H, generating the logistic distribution for the MLM. This distribution is then mapped to the token ID sequence of the victim LLM via Gumbel-softmax and a mapping vector M. By adjusting the loss coefficients, we balance jailbreak performance and readability. The pseudocode for our jailbreak attack method is in Algorithm 1."}, {"title": "Experiment", "content": null}, {"title": "Experiment Setups", "content": null}, {"title": "Dataset", "content": "We evaluate the success rate of jailbreak attacks using the AdvBench Harmful Behaviors dataset introduced by Zou et al. (2023). This dataset includes harmful requests paired with their respective target strings, encompassing a broad range of harmful topics such as profanity, threats, discrimination, cybercrime, and more."}, {"title": "Metrics", "content": "We evaluate jailbreak methods using two key metrics: the success rates of jailbreak attacks and the quality of jailbreak prompts.\nAttack Success Rate. We employ two distinct metrics to measure jailbreak success rates. The first is the keyword-based attack success rate (ASR), based on keyword matching, as described by (Zou et al., 2023). This metric evaluates whether the response from the LLM avoids specific phrases. The detailed keyword string is in appendix B.2. The second metric is the Llama-based ASR (ASR-L), which utilizes the Llama-Guard3-8B 1. Compared to GPT-4, Llama-Guard3-8B performs better with a lower false positive rate. For detailed information and prompts used with Llama-Guard3-8B, please refer to Appendix B.3."}, {"title": "Language models", "content": "To comprehensively evaluate the performance of various jailbreak attack methods, we select seven representative open-source LLMs with varying parameter sizes. The models evaluated were Mistral-7B-Instruct-v0.2 (Mistral-v0.2), Mistral-7B-Instruct-v0.3 (Mistral-v0.3) (Jiang et al., 2023), Vicuna-7B-v1.5 (Vicuna7B), Vicuna-13B-v1.5 (Vicuna13B) (Zheng et al., 2023), Llama2-7B-chat (Llama2) (Touvron et al., 2023), Meta-Llama-3-8B-Instruct (Llama3) (AI@Meta, 2024), and Guanaco-"}, {"title": "Baseline methods", "content": "To validate the effectiveness of our jailbreak attack, we compare it against several baseline methods, including: (1) GCG (Zou et al., 2023): Automatically generates jailbreak prompts using gradient search; (2) AutoDAN (Liu et al., 2023): Employ a hierarchical genetic algorithm to create semantically coherent jailbreak prompt. (3) BEAST (Sadasivan et al., 2024): Controls the attack speed and the readability of adversarial tokens via beam search; (4) COLD-Attack (Guo et al., 2024): Leverage Energy-based Constrained Decoding with Langevin Dynamics to produce controllable jailbreak prompts. For fair comparisons, we integrated the chat format compatible with the LLM into COLD-Attack during the optimization process. For more information of the baseline methods and detailed experimental settings, please refer to Appendix B.5."}, {"title": "Experimental Results", "content": "The main results of jailbreak attack methods are summarized in Table 1.\nAttack Success Rate. For each malicious request in the dataset, we generated a jailbreak prefix and combined it with the harmful query to create a complete prompt. As shown in Table 1, our method consistently achieves the best or second-best ASR and ASR-L across all LLMs, demonstrating the effectiveness of our optimization approach. Notably, while AutoDAN achieved second-best results, its genetic algorithm requires mutating a parent string from a third-party LLM and manually crafting the initial jailbreak prompt, significantly increasing implementation complexity. For Llama2-7B-chat, the model with the strongest safety alignment, although our method's ASR and ASR-L ranks second, the PPL of the jailbreak prompts we generate is significantly lower than those generated by GCG.\nJailbreak Prefix Quality. To demonstrate the effectiveness of our optimization algorithm, we use Sentence Perplexity (PPL) to evaluate the quality of jailbreak prompts. By calculating the cross-entropy loss between the original and jailbreak prompts, and leveraging the contextual knowledge from MLM's pre-training, our method ensures a strong contextual association between tokens, thereby maintaining prompt quality. As shown in Table 1, across all evaluated victim LLMs, our jail-"}, {"title": "Attack Effectiveness", "content": null}, {"title": "Effectiveness of the Initial Prompt", "content": "During the pre-training and alignment processes, LLMs have accumulated extensive general knowledge. To evaluate the ability of LLMs to generate high-quality instructions based on prompts, we combine the initial prefix with harmful instructions and input them into the victim LLM to compute the ASR. The results in Table 2 show that for models with weaker safety alignment, such as Mistral-7B-Instruct-v0.2 and Vicuna-7B-v1.5, the initial prompt prefixes they generate nearly reached the threshold for triggering affirmative responses. Even"}, {"title": "Effectiveness of optimization method", "content": "To validate our optimization method, inspired by GCG, we replace the initial prompt prefix with 30"}, {"title": "Robustness of jailbreak prompts", "content": "To evaluate the robustness of the jailbreak prompts generated by our method, we assessed the attack success rate under the SmoothLLM (Robey et al., 2023) defense setting.\nSmoothLLM defends against adversarial inputs by first applying random perturbations to multiple copies of a given adversarial prompt, then aggregating the model's responses to these perturbed prompts, and finally identifying the adversarial input through a majority voting mechanism. Table 3 presents the ASR of various attack methods against different LLMs. The data indicates that while SmoothLLM effectively counters attacks like GCG, it struggles with jailbreak prompts that have strong semantic coherence, such as AutoDAN and our method. Given that our method was designed to emphasize the interrelation of textual context, it demonstrates superior adaptability and resilience compared to defenses relying solely on random token perturbation. The results show that when confronted with the SmoothLLM protection, our"}, {"title": "One jailbreak prefix for multiple LLMS", "content": "To investigate the commonality of safety vulnerabilities across different open-source models, we simultaneously attack multiple victim LLMs and integrate their gradient information to create a unified jailbreak prefix. For a detailed analysis of the transferability of jailbreak prompts generated us-"}, {"title": "Enhancing Black-Box Attack Methods", "content": "Although our method cannot be directly applied to closed-source LLMs, we detail our efforts to enhance the transferability of jailbreak prompts in Sections 4.5 and Appendix F. To demonstrate the applicability of our method in a black-box setting, we follow the multi-LLM attack approach described in Section 4.5 to generate jailbreak prompts and integrated them into black-box jailbreak attack methods. Specifically, we select harmful queries from AdvBench and generate multi-LLM jailbreak prompts targeting Mistral-7B-Instruct-v0.3, Vicuna-7B-v1.5, and Meta-Llama-3-8B-Instruct. These jailbreak prompts are then used as seed prompts integrated into the black-box jailbreak method to further improve the attack success rate. We consider the following jailbreak attack methods: (1) PAIR (Chao et al., 2023a): Uses an at-"}, {"title": "Conclusion", "content": "As the gap between open-source LLMs and closed-source commercial LLMs rapidly narrows, the safety implications arising from their malicious exploitation are growing progressively more significant. In this paper, we propose an automatic jailbreak attack method for aligned open-source LLMs, referred to as CCJA. We model the jailbreak attack as a token-level optimization problem in the continuous word embedding space of MLM, enabling a finer-grained search for jailbreak text compared to discrete prompt manipulation. By leveraging the MLM head for context-consistent decoding of hidden states and performing combinatorial optimization, we achieve a balance between the attack success rate and semantic readability. We extend this attack method to scenarios involving multiple malicious queries and multiple victim models. By integrating CCJA-generated jailbreak prompts into black-box jailbreak attack algorithms, we significantly improve their success rate against closed-source commercial LLMs. This research aims to raise awareness of safety vulnerabilities in open-source LLMs and highlight the security threats facing closed-source commercial LLMs."}, {"title": "Limitations and Ethics Statements", "content": "Our jailbreak attack algorithm for open-source models enables victim models to generate unsafe content. However, we must acknowledge the limitations of our work and the potential ethical concerns it may raise.\nLimitations. In terms of evaluation, to align with other baselines, we conducted jailbreak attack experiments solely on the AdvBench dataset. With the advancement of LLM technology, the composition of harmful instructions has become increasingly complex. We hope that with community support, more comprehensive jailbreak instruction sets can be developed for comparative adversarial attack experiments in the future. Since our jailbreak method is tailored for open-source models, it may not be applicable in black-box scenarios. For LLMs with higher safety performance, such as LLaMA 2 and LLaMA 3, current white-box attack methods typically append affirmative response descriptions at the end of conversational instructions to guide the LLM in generating harmful content, and our approach follows this pattern. Looking ahead, we plan to explore jailbreak instruction algorithms fully based on conversational templates and aim to adapt them for black-box attacks. These are some of the directions we intend to investigate in future work.\nEthics Statements. Our work aims to raise awareness of the safety risks associated with open-source models and to highlight the urgency of implementing safety safeguards. It is undeniable that our attack method does lead the model to generate some harmful content, resulting in a certain degree of negative impact. However, considering that the AdvBench dataset used in our experiments has been widely studied in numerous academic studies, this work does not amplify the inherent negative impact of the dataset itself. In the future, we plan to conduct more research on the safety of LLMs and explore defense mechanisms against jailbreak attacks targeting open-source models."}]}