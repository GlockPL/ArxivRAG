{"title": "Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced LLM Approach", "authors": ["Haowei Ni", "Shuchen Meng", "Xupeng Chen", "Ziqing Zhao", "Andi Chen", "Panfeng Li", "Shiyao Zhang", "Qifu Yin", "Yuanqing Wang", "Yuxi Chan"], "abstract": "Accurate stock market predictions following earnings reports are crucial for investors. Traditional methods, particularly classical machine learning models, struggle with these predictions because they cannot effectively process and interpret extensive textual data contained in earnings reports and often overlook nuances that influence market movements. This paper introduces an advanced approach by employing Large Language Models (LLMs) instruction fine-tuned with a novel combination of instruction-based techniques and quantized low-rank adaptation (QLoRA) compression. Our methodology integrates 'base factors', such as financial metric growth and earnings transcripts, with 'external factors', including recent market indices performances and analyst grades, to create a rich, supervised dataset. This comprehensive dataset enables our models to achieve superior predictive performance in terms of accuracy, weighted F1, and Matthews correlation coefficient (MCC), especially evident in the comparison with benchmarks such as GPT-4. We specifically highlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases significant improvements over baseline models. The paper also discusses the potential of expanding the output capabilities to include a 'Hold' option and extending the prediction horizon, aiming to accommodate various investment styles and time frames. This study not only demonstrates the power of integrating cutting-edge AI with fine-tuned financial data but also paves the way for future research in enhancing AI-driven financial analysis tools.", "sections": [{"title": "I. INTRODUCTION", "content": "In the realm of stock market prediction, relying solely on historical data to predict stock market directions has proven to be inadequate. Traditional technical indicators such as moving averages and exponential moving averages (EMAS) are frequently insufficient for accurate forecasting, especially when the market is influenced by significant events that disrupt the financial landscape. Such events include routine financial disclosures like earnings reports, which are crucial for understanding the performance of specific companies. These reports are not only pivotal but also often voluminous and time-consuming for investors to analyze comprehensively. Moreover, the plethora of metrics from balance sheets, income statements, and cash flow statements can be overwhelming and difficult to understand for those without a financial background, yet investors need this knowledge to guide their investment choices. Additionally, rare, unpredictable 'black swan' events, such as the COVID-19 pandemic, can dramatically influence market behavior.\nThe limitation of traditional methods is their inability to account for the complex, dynamic nature of financial markets, which are often influenced by a myriad of factors beyond historical price movements. Moreover, traditional machine learning models typically struggle with predictions based on extensive, unstructured textual data like earnings reports, as they fail to capture the depth and implications of financial narratives effectively. Investors are increasingly recognizing the need for more sophisticated, reliable prediction methods that not only analyze past data but also adapt to new information and market conditions in real-time. This need becomes particularly evident during earnings seasons when companies release their quarterly financial results. These periods often witness heightened market volatility as market participants react to the new earning reports. Many investors and traders utilize options strategies to capitalize on anticipated price movements following earnings announcements. This practice, commonly known as \"betting on earnings,\" allows traders to potentially profit from significant price swings without directly owning the underlying stock. On the other hand, some investors use earnings reports to inform their hedging strategies, adjusting their positions to protect against potential losses or to lock in gains. The complexity of earnings reports, combined with their potential to trigger substantial market reactions, creates a double-edged sword for investors. Those who can quickly and accurately interpret these reports may gain a significant competitive edge in the market. However, the sheer volume of information and the rapidity of market reactions present formidable challenges. Many investors struggle to make timely, well-informed decisions in this fast-paced environment, potentially missing out on opportunities, making sub-optimal choices, or experiencing significant losses due to information overload or analysis paralysis.\nThe rapid growth and development of Artificial Intelligence have created an ideal environment for the emergence of Large Language Models (LLMs), which are transforming the field of natural language processing in many areas[1\u201311] and have sparked a great deal of interest in the diverse applications of these models. There are already established works applying advanced modeling techniques, including deep learning [12-18] and LLMs, in finance. For instance, Zhang et al. [19] fine-tuned an LLM using Retrieval Augmented Generation (RAG) for sentiment analysis, showcasing the use of advanced AI to enhance financial data interpretation. Similarly, Koa et al. [20] introduced the Summarize-Explain-Predict (SEP) framework, which employs self-reflective agents and Proximal Policy Optimization (PPO) to fine-tune LLMs for explainable stock predictions. Additionally, the FinBERT-LSTM model, which combines the financial-domain-focused capabilities of FinBERT with the sequential data processing strength of LSTM, demonstrates superior predictive accuracy in forecasting stock prices based on financial news sentiment analysis over traditional LSTM and DNN models [21]. Complementing this, Chang's work [22] has shown that integrating advanced Transformer models with dimensionality reduction techniques results in highly efficient classifiers, which are particularly effective in domains such as stock prediction and fraud de-tection. These advancements highlight the significant role of sophisticated AI in enhancing the accuracy and efficiency of financial market analysis by effectively capturing complex patterns and dependencies within the data.\nBuilding upon these advances, our paper introduces a novel approach by fine-tuning a Large Language Model (LLM) to analyze earnings reports alongside external factors such as overall market index performance and analysts' grades. This method aims to provide investors with more reliable predictions of next-day stock performance following earnings announcements. Unlike existing models that primarily focus on historical data or limited real-time indicators, our approach leverages a comprehensive set of data inputs, enhancing the predictive accuracy and relevancy of financial forecasts. This new direction not only addresses the shortcomings of tradi-tional forecasting methods but also capitalizes on the strengths of AI to synthesize complex datasets into actionable insights, significantly aiding investors in navigating the complexities of post-earnings stock movements."}, {"title": "II. DATA COLLECTION AND PREPROCESSING", "content": "Our study focused on a comprehensive dataset encompassing 501 companies listed in the S&P 500. The selection of 501 companies, instead of the traditional 500, accounts for occasional changes in the index due to mergers, acquisitions, or other financial adjustments that may temporarily expand the list. We utilized the API from Financial Modeling Prep to acquire this extensive financial data. The data collection process was meticulous, incorporating several key factors to ensure a robust and multifaceted analysis:\n\u2022\n\u2022 Market Performance Metrics: For each company, the performance data for the past week from major market indices (SPY, QQQ, and DOW), along with the company's own stock performance, was collected and aligned with the quarterly earnings date.\n\u2022 Analyst Grades: A comprehensive list of stock upgrades and downgrades was compiled from various analysts for the months preceding each earnings date, providing insights into market sentiment and expert opinions.\n\u2022 Earnings Surprises: Instances where companies either beat or missed their estimated Earnings Per Share (EPS) were identified by comparing actual EPS results with estimates.\nFinancial Metrics Growth: This part includes a comparative analysis of financial metrics, contrasting the quarter of the earnings report with the same quarter from the previous year. This encompasses key data points typically presented in earnings reports, including:\nIncome statement metrics\nBalance sheet figures\nCash flow statement data\n\u2022 Earnings Transcripts: Full earnings call transcripts were collected for each company to capture qualitative data and management insights.\n\u2022 Output Label: For the study's outcome metric, we calculated the next day's stock performance after the earnings announcement. If the opening price is less than the closing price, we label this as 'Long'; otherwise, it is labeled as 'Short'. This measure serves as the dependent variable in our predictive models."}, {"title": "B. Textualization and Tokenization", "content": "In this phase, we transformed our numerical and categor-ical data into natural language text, enhancing the dataset's compatibility with our large language model. This process of textualization involved converting raw financial data into descriptive sentences, making the information more contextual and easier for the model to process.\n1) Market Performance Metrics: The percentage changes of major indices and individual stocks were converted into descriptive statements.\n\u2022 Original data: \"SPY -1.5%\"\nTextualized form: \"In the past week, SPY went down by 1.5%\"\nThis transformation was applied consistently to SPY, QQQ, DOW, and the specific company ticker's performance data.\n2) Analyst Grades: The analyst grades over the past 30 days were aggregated to determine the most frequent recommendation, which was then transformed into a descriptive statement:\n\u2022\n\u2022 Original data: Multiple analyst grades over 30 days\n\u2022 Aggregated data: Most frequent grade (e.g., \"Buy\")\nTextualized form: \"In the past 30 days, most grading companies suggest buying this stock\"\n3) Earnings Surprises: The earnings beat/miss data, originally consisting of actual and estimated EPS values, were transformed into comparative statements. For instance:\n\u2022\n\u2022 Original data: \"Actual EPS: $2.10, Estimated EPS: $1.95\"\nTextualized form: \"The company's reported earnings per share (EPS) were 7.69% higher than the analysts' consensus estimates\"\n4) Financial Metrics Growth: Year-over-year growth ratios for various financial metrics were converted into descriptive sentences. For example:\n\u2022\n\u2022 Original data: \u201cgrowthNetIncome\u201d: 0.046\nTextualized form: \u201cCompared to the same quarter last year, Net Income grew by 4.6%\"\nFollowing the removal of rows with missing values, the resultant dataset comprised 8,556 rows, ensuring a comprehensive and complete dataset for the LLM training. The average token count per row is 12,375, demonstrating the substantial content size processed per instance. The minimum token count observed is 1,272, while the maximum reaches 54,394. These statistics highlight the wide range of text lengths the model must handle, which is crucial for understanding both the diversity of the dataset and the robustness required of the model. Two distinct datasets, named Base and Full, were prepared to assess the impact of various features on model performance. The Base dataset focuses exclusively on financial metrics growth and earnings transcripts, providing a baseline to evaluate internal company performance. The Full dataset includes additional text features such as overall market performance, analyst grades, and earnings estimates, allowing for an analysis of how external factors influence the model's effectiveness. This bifurcation aims to determine the relative impact of internal versus external data on predictive accuracy."}, {"title": "III. METHOD", "content": "The architecture of our model, detailed in Figure 1, is meticulously designed to leverage both fundamental and ex-ternal financial factors for making predictive analyses. Ini-tially, a supervised dataset was constructed by integrating 'base factors', which include financial metric growth and earnings transcripts, with 'external factors' such as the pre-vious week's performance of market indices, the stock itself, analyst grades, and earnings surprises. This dataset facili-tated the comprehensive training of our models. Several pre-trained models were selected for further optimization to fit the specific demands of financial forecasting. The models were compressed using QLoRA, a technique designed to maintain original performance while enabling efficient deployment on resource-constrained devices, and then fine-tuned with targeted instructions to enhance their predictive capabilities.\nDuring the inference and evaluation phase, a structured approach was adopted, using clearly defined prompts that led to binary outputs. The real-world efficacy of our models"}, {"title": "B. Instruction Fine Tuning", "content": "Instruction fine-tuning is a technique used to enhance the performance of large language models by training them to comprehend and execute specific, structured instructions, especially useful in domains where precise and actionable outputs are needed from complex input data [23-36]. The process involves three key steps: First, creating an instruction-following dataset consisting of paired instructions and their corresponding expected responses, which act as labels for investor actions such as 'Long' or 'Short'. This dataset includes relevant financial data points tailored to reflect real-world scenarios the model will encounter. Second, the model is fine-tuned using this dataset, where it learns to generate the expected responses based on the provided instructions, adapting its pre-existing capabilities to the specific task of making investment predictions [37\u201353]. Finally, the generated outputs are mapped back to the predefined labels to ensure the model's responses are accurate and applicable to practical financial decision-making settings."}, {"title": "C. QLORA", "content": "Quantized Low-Rank Adaptation (QLoRA) [54] represents an innovative approach in the field of model optimization and efficiency enhancement, particularly for large pre-trained models such as transformers. This technique leverages the principles of Low-Rank Adaptation (LoRA) [55] while incorporating quantization strategies to reduce the computational demand and memory usage of the model adaptations. In our project, we implemented the 4-bit version of QLoRA to fine-tune and deploy a large language model. The primary goal of using 4-bit QLoRA was to compress the model's adaptive pa-rameters without significantly compromising its performance, allowing the model to maintain a balance between efficiency and efficacy, making it suitable for environments with stringent resource constraints.\nQuantization in QLoRA involves reducing the precision of the low-rank matrices from the standard 32-bit floating points to just 4 bits. This drastic reduction in bit depth leads to a significant decrease in the memory footprint of the model, which is especially beneficial given the lengthy nature of our prompts or tokens that require processing. The 4-bit quantiza-tion not only minimizes the storage space but also accelerates the computation, as operations with lower-bit numbers are computationally less expensive. These features make 4-bit QLoRA an essential tool for optimizing the deployment and operational efficiency of advanced AI models in resource-limited settings."}, {"title": "IV. EVALUATION", "content": "In this study, a comprehensive fine-tuning protocol was im-plemented across various advanced pre-trained models to op-timize their performance for specialized tasks in financial an-alytics. Models such as \u201cgemma-7b-4bit\u201d, \u201cPhi-3-medium-4k-instruct\", \"Phi-3-mini-4k-instruct\u201d, \u201cmistral-7b-4bit\u201d, \u201cllama-3-8b-4bit\", and \"llama-3-8b-Instruct-4bit\" were fine-tuned for one epoch using a learning rate of 2e-4. Efficiency in training was achieved by carefully configuring the batch size and gradient accumulation to manage computational resources effectively. Specifically, training parameters were adjusted to allow for significant gradient accumulation, enhancing training stability without requiring substantial memory increases. A warm-up phase consisting of five steps was integrated to gradually adjust the learning rates, which helped stabilize the pre-learned weights of the models.\nOptimization utilized the adamw_8bit\", an 8-bit variant of the AdamW optimizer, striking a balance between perfor-mance and memory efficiency. Weight decay was set at 0.01 to regularize the training process and prevent overfitting [56-64]. The learning rate scheduler was configured to a linear\u201d setting to ensure a steady decrease in the learning rate as training advanced [65\u201383]. Models were capable of processing inputs up to a maximum sequence length of 25,000, allowing extensive financial texts to be analyzed comprehensively. This structured training approach was designed to enhance models' predictive accuracy and operational efficiency, capitalizing on their unique architectural strengths."}, {"title": "B. Baseline Models", "content": "1) ChatGPT: GPT-4.0, an advanced iteration of the Generative Pre-trained Transformer series, was utilized through OpenAI's API, ensuring robust and state-of-the-art performance benchmarks. This model is renowned for its deep learning capabilities across a wide range of language-based tasks."}, {"title": "C. Performance Analysis", "content": "1) Evaluation Metrics: To assess the performance of the fine-tuned models, three key metrics were employed: accuracy, weighted F1 score, and the Matthews Correlation Coefficient (MCC). Accuracy measures the overall correctness of pre-dictions, providing a straightforward metric of performance across all classes. The weighted F1 score, which considers both precision and recall, is particularly useful in situations where class imbalance might affect the model's evaluation. Lastly, the Matthews Correlation Coefficient (MCC) offers a comprehensive measure of the model's quality, capturing true and false positives and negatives in a single coefficient [84]. This combination of metrics ensures a robust analysis of the models' predictive capabilities.\n2) Comparative Analysis of Model Performance: The per-formance of various fine-tuned models was assessed against a benchmark established by GPT-4, utilizing both the 'Base' dataset, which included only base factors, and the 'Full' dataset, enriched with external factors. While the 'Base' dataset provided insights into intrinsic performance, more emphasis was placed on the results from the 'Full' dataset to evaluate the comprehensive capability of the models when supplemented with additional contextual information. Among the models evaluated, the llama-3-8b-Instruct-4bit, a highly optimized version of a large language model tailored explicitly for instruction-based tasks, demonstrated superior performance in the 'Full' dataset. It achieved 16% higher accuracy and 10% better Weighted F1 compared to GPT-4, indicating its effectiveness in handling complex, multifaceted financial data. This model leverages instruction-based tuning to enhance its applicability in specific contexts, making it particularly adept at financial forecasting. This comprehensive analysis underscores the significant advancements in model accuracy and reliability when incorporating both base and external factors into financial forecasting. In addition to comparing the final performance metrics of the models, the training dynamics were also analyzed to understand the models' learning behavior throughout the fine-tuning process. A significant observation was the decrease in fine-tuning loss, as illustrated in Figure 2. This graph demonstrates a consistent reduction in loss across training steps, which indicates effective learning and adaptation of the models to the task. The smooth descent in loss underscores the efficiency of the training regimen and parameter settings chosen for the fine-tuning phase. This steady improvement in loss metrics not only validates the training strategy but also highlights the stability of the models during training, reflecting their robustness in capturing the complexities of financial data."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "This research demonstrated the efficacy of employing advanced machine learning techniques, specifically instruction-based fine-tuning coupled with QLoRA compression, to enhance the predictive accuracy of stock market forecasts post-earnings announcements. By systematically integrating both 'base factors', such as financial metric growth and earnings transcripts, with 'external factors', including past week market indices and stock performance alongside analyst grades, this study crafted a comprehensive dataset that enabled the fine-tuned models to capture the multifaceted nature of financial data effectively. The instruction-based fine-tuning process, tailored to the nuanced demands of financial forecasting, sig-nificantly augmented the capabilities of our models. This was evidenced by the substantial improvements in performance metrics, particularly when evaluated on the 'Full' dataset, which included a broader range of contextual information compared to the 'Base' dataset. The LLM, particularly the llama-3-8b-Instruct-4bit model, displayed remarkable perfor-mance enhancements, outstripping traditional approaches such as GPT-4 in terms of accuracy, weighted F1, and MCC. Additionally, the effectiveness of fine-tuning transformer-based large language models suggests a promising foundation for future research in various domains. This underscores their potential not only in enhancing financial market predictions but also in extending these techniques to other complex tasks such as credit fraud detection, where similar fine-tuning approaches can develop powerful classification models [22]. Furthermore, the application of QLoRA provided an efficient means of deploying these advanced models on resource-constrained devices, maintaining high levels of predictive accuracy while minimizing operational demands. This aspect of model com-pression is crucial for real-time financial applications, where timely and effective decision-making is essential.\nLooking ahead, this study acknowledges the limitation of using a binary 'Long' and 'Short' output in its predictive model, which may not fully capture the nuances of investor behavior in real-world scenarios. Recognizing that investors often adopt a 'Hold' stance as a cautious or neutral position, future work will aim to include this option in the model's output capabilities. Additionally, considering the diverse investment strategies that focus on longer-term trends, expanding the prediction horizon beyond the next day to encompass weekly movements could align better with the needs of various investor types. These enhancements will strive to make the model a more comprehensive tool for investment decision-making, accommodating a broader range of investment styles.\nOverall, the integration of sophisticated AI techniques with a well-curated dataset has proven to be a powerful approach in financial analytics, offering substantial improvements over traditional models. This approach not only addresses the limitations of existing forecasting methods but also sets a new benchmark for the application of AI in financial decision-making. The success of this study paves the way for further explorations into the integration of more diverse data types and advanced modeling techniques in the financial domain."}]}