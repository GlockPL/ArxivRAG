{"title": "ViDTA: Enhanced Drug-Target Affinity Prediction via Virtual Graph Nodes and Attention-based Feature Fusion", "authors": ["Minghui Li", "Zikang Guo", "Yang Wu", "Peijin Guo", "Yao Shi", "Shengshan Hu", "Wei Wan", "Shengqing Hu"], "abstract": "Drug-target interaction is fundamental in understanding how drugs affect biological systems, and accurately predicting drug-target affinity (DTA) is vital for drug discovery. Recently, deep learning methods have emerged as a significant approach for estimating the binding strength between drugs and target proteins. However, existing methods simply utilize the drug's local information from molecular topology rather than global information. Additionally, the features of drugs and proteins are usually fused with a simple concatenation operation, limiting their effectiveness. To address these challenges, we proposed ViDTA, an enhanced DTA prediction framework. We introduce virtual nodes into the Graph Neural Network (GNN)-based drug feature extraction network, which acts as a global memory to exchange messages more efficiently. By incorporating virtual graph nodes, we seamlessly integrate local and global features of drug molecular structures, expanding the GNN's receptive field. Additionally, we propose an attention-based linear feature fusion network for better capturing the interaction information between drugs and proteins. Experimental results evaluated on various benchmarks including Davis, Metz, and KIBA demonstrate that our proposed ViDTA outperforms the state-of-the-art baselines.", "sections": [{"title": "I. INTRODUCTION", "content": "The binding affinity between drugs and the target proteins plays an important role in numerous biological processes, such as immune responses [1] and gene regulation [2]. Traditional high-throughput screening experiments for measuring affinity are labor-intensive, time-consuming, and expensive [3]. Therefore, computational methods for predicting drug-target affinity (DTA) have emerged.\nTraditional machine learning methods, such as Support Vector Machines (SVM) [4] and Random Forests (RF) [5], have been widely applied to DTA prediction. However, these methods require complex and time-consuming feature engineering and suffer from low prediction accuracy due to limited and non-uniform datasets. Recently, deep learning has emerged as a promising solution to deal with protein structures [6]\u2013[8]. Unfortunately, it is unsuitable for DTA prediction due to the lack of structural information in drug-target samples. Although we can exploit deep learning methods to predict structures, they often introduce accumulated errors, reducing prediction accuracy.\nIn light of this, recent works turn to developing end-to-end models that only take the drug's SMILES sequence and the protein's amino acid sequence as input. For instance, DeepDTA [9] utilizes 1D-CNNs to extract features from both drug and protein sequences to predict DTA. AttentionDTA [10] employs a bidirectional multi-head attention mechanism to highlight key subsequences in drug and protein sequences. TEFDTA [11] uses Transformers and 1D-CNNs to extract drug and protein features. However, these methods failed to examine the critical role of atomic properties and chemical bonds within drug molecules. Besides, they typically resort to simple concatenation operations to achieve feature fusion.\nTo better extract the drug feature within the drug molecular, instead of using 1D-CNNs, some works tend to use GNNS to represent drug SMILES sequences as molecular topology graphs [12]\u2013[15]. SGNetDTA [16] employs graph attention algorithms to extract drug features from molecular topology graphs and uses 1D-CNNs for protein feature extraction. ColdDTA [17] combines GNNs with dense layers, incorporating residual connections between each GNN to prevent information loss. However, these methods often neglect the global topology information.\nIn this paper, we propose an enhanced DTA prediction framework ViDTA. We employ the Graph Transformer to extract features from drug molecules and introduce a virtual node to capture global features. Finally, the high-level features of proteins and drugs will be fed into a carefully designed attention-based linear feature fusion network for affinity prediction.\nThe main contributions are summarized as follows:\n\u2022 We introduce the virtual nodes to the Graph Transformer network for feature extraction, providing a broader receptive field and capturing richer global structural correlations between atoms in drug molecules.\n\u2022 We propose an attention-based linear feature fusion network that incorporates a gated skip connection mechanism, which can better capture interaction information between drug and protein features.\nThe experiments on multiple benchmarks with prevalent evaluation metrics demonstrate that ViDTA outperforms state-of-the-art baselines."}, {"title": "II. MATERIALS AND METHODOLOGY", "content": "We evaluated our method over three public benchmark datasets (Davis [18], Metz [19], and KIBA [20]).\nSmaller Ka indicates a higher affinity between the drug and the target. To reduce the variance, Ka values in the Davis dataset are typically transformed into logarithmic space. The transformation process is formulated as:\n$pK_d = -lg \\frac{K_d}{10^9}$ (1)\nOur proposed ViDTA model consists of four modules: a drug feature extraction network, a protein feature extraction network, an attention-based Linear Feature Fusion network, and an affinity prediction network. The overview of ViDTA is illustrated in Fig. 1.\nThe drug feature extraction network takes the drug molecule's SMILES sequence as input. The SMILES sequence is transformed into a two-dimensional molecular graph of atoms and bonds. Then we add a virtual node to the graph, which is fed to a Graph-Transformer-based encoder to extract the feature of the drug molecule.\nConcurrently, the protein feature extraction network processes the protein's FASTA sequence. The protein sequence is encoded into a protein feature map using embedding vectors derived from various types of amino acids. The protein feature map is fed into a 1D-CNNs-based encoder to extract the sequence feature of the protein. The features extracted by both the drug and protein networks are then input into an attention-based linear feature fusion network to generate fused features. Finally, the fully connected layers are used to predict drug-target affinity based on the fused features."}, {"title": "C. Drug Feature Extraction", "content": "1) Drug Molecular Graph Representation: The drug feature extraction network takes the SMILES sequence as input. We first convert the SMILES sequence into a drug molecular graph G = (V, E). In this graph, the set of nodes V denotes the atom, while the set of edges E represents the feature vectors for chemical bonds between atoms.\nWe add a virtual node vn to the graph G. The virtual node is connected to all atomic nodes, forming virtual edges. The initial feature vector of the virtual node and edges are initialized to zero and added to V and E, respectively. Following AttentionMGT-DTA [21], each atom's initial feature vector is determined based on its properties such as symbol, formal charge, atom hybridization, and atom chirality, resulting in a 44-dimensional vector. The initial feature vector for each chemical bond is derived from other properties like bond type, aromatic or conjugated, resulting in a 10-dimensional vector.\n2) Graph-Transformer-bsed Drug Encoder: Fig. 2 illustrates the framework of the drug encoder based on Graph Transformer [22]. For the drug graph G, the initial atomic feature of the ith node, and the chemical bond feature of the edge between the ith node and the jth node, are first mapped through a linear layer to obtain the atomic feature \u0125(0) and the edge feature e(0)ij \u2208 Rdk of length dk, where dk is the input dimension of drug encoder.\nSubsequently, we calculated the symmetrically normalized Laplacian matrix L using the identity matrix I, the degree matrix D, and the adjacency matrix A of the drug molecular graph:\n$L = I \u2013 D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$ (2)\nThe eigenvector of L is mapped to \u03bbi \u2208 Rdk through a linear layer. Then \u03bbi is added to the atomic feature \u0125(0) to obtain the input node feature h(0)i for the Graph Transformer:\nh(0)i = \u0125(0)i + \u03bbi (3)"}, {"title": "Fig. 1. Overview of the proposed ViDTA", "content": "The attention score w(l)ij between the ith node and jth node in the lth layer of the Graph Transformer is obtained as:\nw(l)ij = \\frac{W_i^{(l)} h_i  \u22c5 W_j^{(l)} h_j + W_e^{(l)} e_{ij}}{\\sqrt{d_h}} (4)\n\u03a9(l)ij = Softmax (w(l)ij) (5)\nwhere W(l)i ,W(l)j , W(l)e \u2208 Rdh\u00d7dk are the linear transformation matrices for the feature vectors of the ith and jth nodes in the lth layer, respectively, W (l)e \u2208 Rdh\u00d7dk is the linear transformation matrix for the feature vector of the edge between the ith and jth nodes in the lth layer, and dh is the unified dimension of the hidden feature.\nThen for the multi-head attention layer of the Graph Transformer, we utilize a message-passing neural network [23] to update the features of both nodes and edges.\nFor the node message passing, the feature of the ith node is updated by aggregating the current features of neighboring nodes (considered as the jth node), weighted by attention scores:\nh(l+1)i = \\sum_{j \u2208 N_i} \u03a9(l)ij W h(l)j (6)\nwhere W \u2208 Rdh\u00d7dk is the linear transformation matrix of the feature vectors of neighboring nodes for the ith node in the lth layer.\nFor the edge message passing, e(l+1)ij is used as the new edge feature e(l+1)ij :\ne(l+1)ij = \u03a9(l)ij W_e^{(l)} (7)\nSubsequently, we concatenate all the outputs obtained from the K attention heads, and then add them to the input features of the lth layer:\nh(l+1)i = h(l)i + ||_{k=1}^{K} O(l)h_{i,k}^{(l+1)} (8)\ne_{ij}^{(l+1)} = e_{ij}^{(l)} + ||_{k=1}^{K} O(l)e_{ij,k}^{(l+1)}\nwhere || represents the concatenation operation, O(l)h \u2208 RK\u2217dh\u00d7do_ and O(l)e \u2208 RK\u2217de\u00d7do denote the learnable weight matrices, do is the output dimension of the drug encoder. \nThe final feature of nodes and edges in the (l + 1)th layer are obtained using a feed-forward neural network (FNN) and residual modules:\nh(l+1)i = Norm (h(l+1)i + W_1 ReLU (W_2 Norm (h(l+1)i))) (9)\ne_{ij}^{(l+1)} = Norm (e_{ij}^{(l+1)} + W_1 ReLU (W_2 Norm (e_{ij}^{(l+1)}))) (10)\nwhere \"Norm\" refers to Layer Normalization, W(l)1 and W(l)2 are learnable parameters in Rd\u2217do."}, {"title": "D. Protein Feature Extraction", "content": "1) Protein Sequence Representation: Target proteins are biological macromolecules composed of multiple amino acids, each represented by a unique letter. According to the work of Tsubaki et al. [24], each of the 25 amino acids is assigned an integer value (e.g., \u201cA\u201d: 1, \u201cC\u201d: 2, \u201cB\u201d: 3, etc.). To standardize input dimensions, we set the maximum length of protein sequences to 1000. Sequences longer than this are truncated, and sequences shorter are padded with zeros.\n2) 1D-CNNs-based Protein Encoder: Each integer representing an amino acid is mapped to a unique dp dimensional vector through an embedding layer. This results in an input protein feature map Mp \u2208 Rlp\u00d7dp, where lp is the maximum length of amino acid sequences, and dp is the size of the protein embedding.\nWe then use three concatenated 1D-CNN blocks as the protein encoder to obtain the protein feature map of each layer. To maintain the consistency of input and output sequence lengths and to increase the receptive field, we used progressively larger convolutional kernel sizes (2, 3, 5), with padding sequentially set to 5, 7, and 11. The stride and dilation were consistently set to 1. The final representation of the protein sequence et is obtained by performing a max pooling operation. It can be performed as follows:\net = MaxPool (Conv1d(Mp)) (11)"}, {"title": "E. Attention-based Linear Feature Fusion", "content": "After the above processing, the drug feature ed and protein feature et are fed into the attention-based linear feature fusion network. The overall workflow of the feature fusion module is illustrated in Fig. 3. Firstly, ed and et are added into a block composed of two linear layers and a sigmoid activation function to obtain the attention weight score W1:\nW\u2081 = Sigmoid (Linear (Linear (ed + et))) (12)\nW\u2081 and (1 \u2212 W\u2081) are then respectively multiplied by the drug and target protein feature to get the fused feature edt:\nedt = W\u2081 \u2217 ed + (1 \u2212 W\u2081) \u2217 et (13)\nThis process is repeated by inputting edt into the second linear block to get the second weight score W2, and the second fused feature edt. Finally, using a gated skip connection mechanism [25] which integrates features from different hidden layers, the initial features ed and et, and the fused feature edt are weighted to get the final fused representation of the drug and protein target. In this way, we can preserve primary features, ensuring that critical information is not overlooked during training:\nW3 = Sigmoid (edt) (14)\nedt = W3 \u2217 edt + (1 \u2212 W3) \u2217 (ed + et) (15)"}, {"title": "F. Affinity Prediction Network", "content": "The affinity prediction network is comprised of four fully connected layers, which take the fused feature edt as input. Besides, Batch-Normalization and ReLU activation functions are applied between adjacent linear layers.\nThe loss function used for model training is MSE Loss:\nMSE = \\frac{1}{N} \\sum (\\hat{y}_i - y_i)^2 (16)"}, {"title": "III. EXPERIMENT AND RESULT", "content": "To ensure the reliability of our experimental results, we employed five-fold cross-validation in this study. To enhance the training efficiency, we initialize the learning rate at 0.0003 and decay it to 0.0001 after completing 100 epochs. The batch size for all three benchmark datasets was uniformly set to 128, with training continuing for 1000 epochs. If the model's loss did not decrease after 200 epochs, training was halted to prevent overfitting. Additional details of hyperparameters can be found in the Table II."}, {"title": "B. Evaluation Metrics", "content": "DTA prediction is a regression task, therefore, we use Concordance Index (CI) [26], Modified Correlation Coefficient (r2) [27], Pearson Correlation Coefficient (PCC) [28], and Mean Squared Error (MSE) as evaluation metrics."}, {"title": "C. Experimental Results", "content": "We compared our approach with several methods, including GraphDTA [12], GOaidDTA [29], rzMLP-DTA [30], AttentionDTA [10], coldDTA [17], TF-DTA [31], TEFDTA [11], DGDTA [32], TransVAE-DTA [33], and AttentionMGT-DTA [21], ArkDTA [34].\nThe experimental results demonstrate that the ViDTA model performed outstanding in these three benchmark datasets. Among the four metrics of CI, rm, PCC, and MSE, ViDTA stands out as the top-performing model.\nNotably, our results have several significant highlights: In the Davis dataset, our model achieved a CI index of 0.9052, while other baselines didn't score higher than 0.9. In the Metz dataset, the MSE of our scheme decreased by 7.7% from 0.1553 to 0.1434. In the KIBA dataset, PCC increased by 2.9% from 0.8739 to 0.8989."}, {"title": "D. Ablation Experiments", "content": "We compared the impact of different drug feature extraction architectures on DTA prediction in the Metz dataset, including GCN, GAT, GIN, Graph Transformer based on molecular graphs, and Transformer based on SMILES sequences. We also evaluated the prediction performance of virtual nodes versus readout as molecular graph representations.\nWe assessed the influence of different feature fusion methods. By replacing the proposed attention-based linear feature fusion network with simple addition or concatenation, the experimental results demonstrate that our approach consistently outperformed the other two methods across all metrics."}, {"title": "IV. CONCLUSION", "content": "In this study, we propose ViDTA, an enhanced drug-target affinity prediction scheme. By introducing the virtual nodes to the Graph Transformer network, our method shows significant potential in simultaneously extracting local and global features from the drug molecular graph. Furthermore, the attention-based linear feature fusion network that incorporates a gated skip connection mechanism effectively integrates the features from both drug and protein targets. Our approach has demonstrated remarkable performance on multiple widely used benchmark datasets. The experiments with prevalent evaluation metrics demonstrate that ViDTA outperforms state-of-the-art baselines."}]}