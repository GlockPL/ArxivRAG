{"title": "Robust image representations with counterfactual contrastive learning", "authors": ["M\u00e9lanie Roschewitz", "Fabio De Sousa Ribeiro", "Tian Xia", "Galvin Khara", "Ben Glocker"], "abstract": "Contrastive pretraining can substantially increase model generalisation and downstream performance. However, the quality of the learned representations is highly dependent on the data augmentation strategy applied to generate positive pairs. Positive contrastive pairs should preserve semantic meaning while discarding unwanted variations related to the data acquisition domain. Traditional contrastive pipelines attempt to simulate domain shifts through pre-defined generic image transformations. However, these do not always mimic realistic and relevant domain variations for medical imaging such as scanner differences. To tackle this issue, we herein introduce counterfactual contrastive learning, a novel framework leveraging recent advances in causal image synthesis to create contrastive positive pairs that faithfully capture relevant domain variations. Our method, evaluated across five datasets encompassing both chest radiography and mammography data, for two established contrastive objectives (SimCLR and DINO-v2), outperforms standard contrastive learning in terms of robustness to acquisition shift. Notably, counterfactual contrastive learning achieves superior downstream performance on both in-distribution and on external datasets, especially for images acquired with scanners under-represented in the training set. Further experiments show that the proposed framework extends beyond acquisition shifts, with models trained with counterfactual contrastive learning substantially improving subgroup performance across biological sex.", "sections": [{"title": "Introduction", "content": "Contrastive learning in medical imaging has emerged as an effective strategy to leverage unlabelled data. This self supervised learning approach has been shown to substantially improve model generalisation across domain shifts as well as reducing the amount of high-quality annotated data needed for training1-4. However, the success of contrastive-based learning is heavily dependent on the positive pair generation pipelines. These positive pairs are typically generated by repeatedly applying pre-defined data augmentations to the original image. As such, changes in the augmentation pipeline have a substantial impact on the quality of the learned representations, ultimately influencing downstream performance and robustness to domain changes5,6. Traditionally, augmentation pipelines developed for natural images have been directly applied to medical imaging, however, this might not be optimal due to the unique challenges and characteristics of how medical scans are acquired. In particular, domain variations are often much larger than subtle class-wise differences. This may lead contrastively learned representations to inadvertently encode these irrelevant acquisition-related variations into the learned representations.\nIn this work, we aim to improve the robustness of contrastively learned representation against domain shifts, in particular acquisition shift. Acquisition shift is caused by changes in image acquisition protocols (device settings, post-processing software, etc.), and is a major source of dataset shift in the medical imaging domain. We hypothesise that robustness of contrastively learned features against such changes in image characteristics could be improved by simulating domain variations more faithfully in the positive pair creation stage. For this reason, we propose and evaluate 'counterfactual contrastive learning', a new contrastive pair generation framework leveraging recent advances in deep generative models for high-quality, realistic counterfactual image generation7,8. Counterfactual generation models allow us to answer 'what-if' questions, such as simulating how a mammogram acquired with one device would appear if it would have been acquired with a different device. Specifically, in our proposed counterfactual contrastive framework, we create cross-domain positive pairs, by matching real images with their domain counterfactuals, realistically simulating device changes. Importantly, the proposed approach is agnostic to the choice of the contrastive objective as it only impacts the positive pair creation step. We illustrate the benefits of this approach for two widely-used contrastive learning frameworks: seminal work SimCLR and newly released DINO-V210 objectives. Moreover, to precisely measure the effect of the proposed counterfactual pair generation process, we also compare the proposed approach to a simpler approach where we simply extend the training set with the generated counterfactuals.\nEvaluating the proposed counterfactual contrastive learning framework across two medical image modalities, mammography and chest radiographs, on five public datasets and two clinically-relevant classification tasks, we show that our method yields features which are more robust to domain changes. This increased robustness was directly observed in the feature space and, more importantly, by a substantial increase in downstream performance, in particular under limited labels and for domains under-represented at training time. Crucially, these findings hold for both SimCLR and DINO-v2, despite major differences in the training objectives. This paper is an extension of our recent workshop paper\u00b9\u00b9. It differs in the following aspects:"}, {"title": "Related work", "content": "Generating pairs of image 'views' that share the same underlying meaning (positive pairs) is the core principle of contrastive learning. The contrastive objective then encourages the model to learn similar embeddings for these pairs, while keeping them distinct from the embeddings of unrelated images. A landmark work in this field is SimCLR, where positive pairs are generated by applying photometric and geometric transformations to the original image. SimCLR stands out for its simplicity, effectiveness, and widespread adoption, particularly in medical imaging. Azizi et al.2 for example showed that pre-training models with SimCLR substantially improves downstream performance and robustness to various sources of data shifts. Several methods have proposed refinements to SimCLR, for example BYOL12, MoCo13, or most recently DINO(-v2)10,14. DINO uses a self-distillation approach without explicit negative pairs, where a student network learns to match the output of a teacher network, updated via momentum. This method focuses on consistency between the teacher and student outputs, making it less dependent on batch size and augmentations. Importantly positive pairs are generated using more than two views. Representations are instead encouraged to be similar across different global views (larger image crops) and local views (smaller crops). Moreover, DINO relies on vision transformers15, contrarily to SimCLR which was primarily designed for convolutional networks. Further enhancements were proposed in DINO-v210, with modifications to the loss function to improve stability and performance, encouraging better feature alignment and consistency. Recent work has successfully applied DINO-v2 pre-training to chest radiography, achieving state-of-the-art downstream performance across different tasks 16"}, {"title": "Counterfactual image generation", "content": "One goal of counterfactual image generation is to produce 'counterfactual explanations', i.e. images depicting the smallest change in the input that would have changed the prediction of a pre-defined classifier17\u201321. Parallel to this interpretability-centered line of work, others have focused on using generative modelling to synthesise 'what-if' images, independently of any external classifier. Seminal work by Pawlowski et al.22 introduced Deep Structural Causal Models (DSCM) to generate realistic counterfactuals for small resolution images. This framework has been substantially extended by Ribeiro et al.7, where the authors utilise a hierarchical variational autoencoders (HVAE) for improving image generation, unlocking high-quality high-resolution counterfactual generation, in particular for medical images. While counterfactual image generation models are gaining traction, with some studies showing promising results in imbalanced data augmentation23,24 and fairness25, the potential of these models for enhancing performance on clinically relevant tasks still warrants more exploration."}, {"title": "Combining contrastive learning and counterfactuals", "content": "Zhang et al.26 explored the use of counterfactual text-image pairs in vision-language grounding tasks, defining task-dependent counterfactual pairs for additional supervision signal. Within the context of supervised contrastive graph learning, Yang et al.27 proposed to generate challenging negative examples using graph counterfactuals. However, the use of image-only counterfactuals for vision contrastive learning remains largely unexplored."}, {"title": "Counterfactual contrastive learning", "content": "In this section, we introduce counterfactual contrastive learning, a self-supervised learning paradigm to train image encoders robust to domain variations by leveraging state-of-the-art counterfactual image generation models. Contrastive learning typically uses colour and intensity-based image augmentations to encourage the model to ignore domain-specific image characteristics. However, in medical imaging, the effect of changes in acquisition hardware, device calibration or post-processing software on the final image appearance is highly complex and can not realistically be replicated by those simple handcrafted transformations. To overcome this, in counterfactual contrastive learning we instead use a causal image generation model to simulate realistic domain variations and generate cross-domain contrastive pairs, explicitly encouraging contrastively-learned representations to ignore domain-specific information (such as scanner differences). We illustrate key differences between standard contrastive learning and the proposed counterfactual contrastive approach in Fig. 1."}, {"title": "Counterfactual image generation model", "content": "In this work, we use the Deep Structural Causal Model (DSCM) proposed by Ribeiro et al.7 to generate image counterfactuals. In this model, images are generated using a Hierarchical Variational Autoencoder (HVAE) conditioned on the assumed causal parents of the image. This HVAE is trained by maximising the Evidence Lower Bound of the log-likelihood. We follow Ribeiro et al. and use a conditional prior for the latent representations of the image. Once the HVAE is trained, we can infer the exogenous noise (capturing image-specific identity, not explained by the causal parents) from the observed image and true causal parents. Combining this exogenous noise with the new parents (after intervention) we can then generate a counterfactual image. We invite the reader to consult Ribeiro et al.7 for a detailed explanation of the counterfactual generation model. A few modifications were made to the original HVAE model from' to further increase training stability and image quality. First, instead of directly using the parent variables to condition the HVAE, we add an embedding layer to learn a more flexible parents' embedding for improved conditioning. Moreover, we used SiLU28 activation layers instead ReLU, added group normalisation layers and fixed the variance of latent variables to le 2, as we noticed that these changes improved training stability. Finally, our generation model achieved sufficient image quality without relying on any counterfactual finetuning, i.e. our generation model did not need to rely on any pretrained classifiers to reach satisfying levels of effectiveness (which is practical as the use of counterfactual finetuning was recently shown to lead to attribute amplification29). Details about the assumed causal graphs and the obtained quality for image generation can be found in the Experiments section."}, {"title": "A simple and effective approach to counterfactual contrastive learning: CF-SimCLR", "content": "SimCLR is a widely adopted contrastive learning strategy, due to its effectiveness and simplicity in terms of training setup and number of hyperparameters to tune. Contrastive pairs are composed of two related views from the same image, obtained by applying a random augmentation pipeline to the original input. An image encoder then yields a high dimensional representation for each of the views, which is then projected onto a lower dimensional space using a two-layer perceptron to obtain representations $z$. The NT-Xent loss then pushes the representations of positive pairs closer together, while representations of negative pairs are pushed apart. For each positive pair (i, j), the loss is:\n$L_{i,j} = -log\\frac{exp(sim(z_i, z_j)/\tau)}{\\sum_{k=1}^{2N} I_{i\\neq k} exp(sim(z_i, z_k)/\tau)}$\nwhere $sim(u, v) = \\frac{u^T v}{\\|u\\|_2 \\|v\\|_2}$.\nIn this work, we propose a novel approach to contrastive positive pair creation, relying on domain counterfactuals instead of pre-defined random image transformations only. Applying this novel counterfactual pair creation pipeline to the classic SimCLR objective, we obtain 'CF-SimCLR', where we create positive view pairs by pairing each real image with one of its domain counterfactual. Concretely, we sample one target domain at random among all possible domains and generate the corresponding domain counterfactual to pair with the real image. If the original domain is sampled we simply keep the real image as the domain counterfactual (since there are no domain changes). To further increase view diversity, we then apply the original augmentation pipeline to this cross-domain positive pair. The rest of the SimCLR framework is kept as-is, as summarised in Fig. 1."}, {"title": "Extension to other contrastive objectives: CF-DINO", "content": "The proposed counterfactual contrastive framework defines a novel way to create contrastive pairs by leveraging counterfactual image generation, independently of the particular choice of contrastive objective. To demonstrate that our counterfactual contrastive framework is indeed general and directly applicable to other contrastive learning objectives, we here extend our counterfactual contrastive analysis to models trained with the recently proposed and popular DINO-v2 objective10.\nAs introduced in the related work section, DINO-v2 combines contrastive losses over different crops of images, model distillation and vision transformers to learn general image representations. For each image, we generate two 'global views' (i.e. larger crops of the images) as well as eight additional 'local views' (i.e. smaller crops). The model is then encouraged to produce similar representations for all global and local crops for both the student and teacher models. The final loss function is also complemented by a masked image modelling component. We invite the reader to refer to the original DINO-v2 paper for further details10.\nTo incorporate our counterfactual contrastive strategy with DINO-v2, we follow similar steps as with CF-SimCLR. Specifically, in 'CF-DINO', one global crop is created from the real image while the other one is generated from its counterfactual. Similarly, we generate half of the local crops from the real image (N=4) and half from its counterfactual (N=4). During training, all views are encouraged to produce similar image representations, yielding the desired cross-domain invariance."}, {"title": "Experiments", "content": "We evaluate the proposed method on two medical image modalities, mammography and chest radiography, using five public datasets covering a large variety of image acquisition hardware. The main chest radiography dataset used in this study is PadChest30, a large dataset from Spain composed of scans acquired with two different scanners. In this dataset, scanner information is available for every image allowing us to easily train a domain counterfactual generation model. We use the same dataset for self-supervised pretraining. We evaluate the quality of the learned representations on pneumonia detection, first on in-distribution PadChest test data, then on two external datasets (covering acquisition domains unseen during pretraining): RSNA Pneumonia Detection31,32 and CheXpert33. For mammography, we primarily use the EMBED34 dataset, containing over 300k scans, acquired in the US with 6 different devices. We keep one domain as hold-out domain ('Senographe Essential') and use the remaining five scanners for pretraining and counterfactual image generation. We highlight that 90% of the EMBED data was acquired with the 'Selenia Dimensions' scanner, the other scanners being heavily under-represented in this dataset, an ideal setup for investigating robustness to domain shifts. Finally, we investigate the quality of the learned encoders when transferring to the external VinDR-Mammo35 dataset from Vietnam, covering two different acquisition domains. Table 1 details dataset splits and inclusion criteria."}, {"title": "Causal inference model", "content": "To train the Deep Structural Causal Model, we need to specify the causal graph outlining the data-generating process. Most applications require the causal graph to closely describe physiological and imaging processes affecting image appearance. However, in this study we only intervene on one variable, the 'scanner' indicator. As such we can simplify the causal graph to only contain this one variable, as factors of variations unaccounted for in the causal graphs will be captured by the exogenous noise. Importantly with this minimalist graph, we do not need to condition the generation model on any downstream task labels, essential to preserve the unsupervised nature of the pretraining step. In our examples, we include scanner as the only parent in the causal graph for mammography generation and include both biological sex and scanner for chest radiography counterfactual inference (sex is optional for domain counterfactual generation). We provide qualitative examples of generated domain counterfactuals in Fig. 2. Generated domain counterfactuals can deceive a domain classifier trained on real data 95% of the time for the PadChest model, and 80% of the time for the EMBED model (when generating counterfactuals uniformly across domains). For EMBED, we use weighted batch sampling during training to counter the imbalance in the scanner distribution."}, {"title": "Implementations details for pretraining and evaluation", "content": "We use ResNet-5036 encoders (initialised with ImageNet weights) for all models pretrained with SimCLR. DINO-v2 use ViT-Base15 encoders, initialised with the weights from ImageNet DINO-v2. We used the original DINO-v2 code and hyperparameters15 for training and kept the encoder with the lowest validation loss. We kept all hyperparameters constant when comparing various contrastive pair generation strategies.\nEvaluation. Models are evaluated with linear probing (i.e. classifiers trained on top of frozen encoders) as well as full model finetuning (unfrozen encoders) with varying amounts of annotated data. All models are finetuned with real data only, using a weighted cross-entropy loss and are evaluated on both ID and OOD datasets. Evaluation on external datasets is crucial to assess how counterfactual contrastive learning performs on unseen domains (outside of scanner distribution used for training the causal inference model). All our code is publicly available 1."}, {"title": "Results", "content": "In this section, we compare the representations quality and robustness for various pre-training paradigms. First, standard SimCLR. Secondly, SimCLR+ where we train a model using classic SimCLR on a training set enriched with domain counterfactuals. Finally, CF-SimCLR combining SimCLR with our proposed counterfactual contrastive pair generation framework. We then repeat the same analysis for models pre-trained with the DINO objective, comparing DINO, CF-DINO and DINO+. Note that in SimCLR+ (resp. DINO+) counterfactuals and real images are not paired during the contrastive learning step, they are all considered as independent training samples. As such SimCLR+/DINO+ represent the common paradigm of simply enriching the training set with synthetic examples. In CF-SimCLR/CF-DINO, on the other hand, we systematically pair real images with their corresponding counterfactual for positive pair creation (Fig. 1). We compare the effect of these three pretraining strategies on chest X-rays and mammograms. For chest X-rays, we evaluate the quality of the learned representations by assessing downstream performance on pneumonia detection. For mammography, we focus on the task of breast density prediction (important for risk modelling)."}, {"title": "Does counterfactual contrastive learning improve performance and robustness under acquisition shift?", "content": "First, we focus on comparing counterfactual contrastive strategies (CF-SimCLR, CF-DINO) versus standard contrastive learning (SimCLR, DINO) to assess whether our cross-domain contrastive pair improve downstream performance across various domains.\nResults on pneumonia detection (Fig. 3) show that CF-SimCLR outperforms the SimCLR baseline (orange versus blue), across datasets, irrespective of the amount of labels, with improvements particularly striking for linear probing. The largest performance gains can be observed in low levels of labels, on the ID scanner under-represented during training (Imaging) and the external datasets. Mammography results in Fig. 4 also show that CF-SimCLR consistently outperforms the SimCLR baseline across most ID scanners for both linear probing and finetuning, particularly when the amount of labelled data is limited (<20k). On the external VinDR dataset, CF-SimCLR beats both baselines on both scanners in the low data regime. Crucially, CF-SimCLR pretraining particularly benefits scanners under-represented in the training set (all except Selenia Dimensions for EMBED and PlanMed Nuance for VinDr), regardless of the amount of labelled data. For both modalities, the improvement on external datasets is particularly worth highlighting given that the encoder was not exposed to these external domains during pretraining (nor during counterfactual generation).\nCrucially, performance improvements and increased robustness to acquisition shift equally hold for encoders trained with the DINO-v2 objective, demonstrating the versatility of the proposed method. In Fig. 5, we can see that CF-DINO outperforms DINO for all EMBED scanners, across all levels of labels, for both linear probing and full model finetuning. Again the gains mostly affect scanners under-represented during training with CF-DINO closing the performance gap between the majority scanner (Selenia Dimensions) and the other scanners. As for CF-SimCLR, similar observations are made for the VinDr minority scanner with CF-DINO substantially improving performance on PlanMed nuance for low levels of labels. For chest X-rays, in Fig. 6, we can see that training with CF-DINO closes the performance gap between both PadChest scanners for both linear probing evaluation and finetuning. Indeed, without CF-DINO there is a substantial performance drop from Phillips images to Imaging images, whereas this gap is much smaller with performance improving substantially with CF-DINO across all levels of labels for images from Imaging. Similarly, CF-DINO outperforms DINO on the external RSNA Pneumonia dataset by a substantial margin. For CheXpert, CF-DINO outperforms DINO in model finetuning (and linear probing with 10% of labels), however,"}, {"title": "What is the benefit of counterfactual contrastive learning over simply extending the training set with the generated counterfactual data?", "content": "We have shown that the models trained with counterfactual contrastive learning outperform models trained with standard contrastive pairs. However, it is important to note that models trained with CF-SimCLR (resp. CF-DINO), are exposed to additionally (synthetic) data during training. Hence, in this section, to isolate the effect of the 'smart pair creation' proposed in this work, we compare it to another baseline, SimCLR+ (resp. DINO+), where we use the same extended training set but where generated samples are considered as independent samples and are not paired with real images during training. Overall, CF-SimCLR outperformed SimCLR+ consistently across all experimental settings. Similarly, CF-DINO outperformed DINO+ across all EMBED datasets and both performed similarly on VinDR. On chest X-rays, CF-DINO outperformed DINO+ by a large margin on PadChest and RSNA Pneumonia for linear probing, and both performed similarly for model finetuning. We especially noticed that the performance gains of SimCLR+ (resp. DINO+) over SimCLR were not very stable, improving for some domains while performing on par with standard SimCLR (resp. DINO) for others (compare orange versus green in Figs. 3 and 4 for SimCLR and Figs. 5 and 6 for DINO). In general, the counterfactual contrastive approaches offered more consistent performance improvements. The starkest differences were observed on scanners under-represented during training (see Fig. 4). These experimental results are in-line with theoretical benefits of counterfactual contrastive learning: by explicitly creating realistic cross-domains positive pairs during training, we directly encourage the network to create domain-agnostic image representations. The more domain-agnostic image representations are, the bigger the expected improvement in terms of robustness to acquisition shift. Such an increase in robustness leads to higher performance on devices under-represented during training, even more so in limited data settings. The increase in domain alignment is directly visible in the t-SNE plots of feature embeddings in Fig. 7, where embeddings of models trained with SimCLR and SimCLR+ exhibit clear domain clustering, whereas the model trained with CF-SimCLR generate embeddings where domains are less separated."}, {"title": "Computational considerations", "content": "To implement counterfactual contrastive learning, we need to train an additional causal image generation model, this raises the question of computational overhead. Fortunately, the HVAE used here to generate image counterfactuals is relatively lightweight (as opposed to alternative generative approaches such as diffusion models). This HVAE not only trains relatively fast (only 20 epochs needed), it is also relatively frugal in terms of VRAM requirements (20 GB of GPU VRAM were sufficient), which has the advantage of low hardware requirements. In terms of generation speed, we were able to generate over 1 million 224 x 224 mammography images in under 7 hours (on an NVIDIA RTX-3090 GPU). These computational requirements need to be put in perspective compared to the requirements of the contrastive pretraining step. Contrastive pretraining is resource intensive both in terms of VRAM (large batch sizes) and in terms of training time. Each SimCLR model was trained for 500 epochs and required 2x46GB VRAM. For DINO, memory requirements were even higher (6x46GB VRAM for a batch size of 300). The overhead of the counterfactual generation part is negligible, and its benefits clearly outweigh the added computational costs."}, {"title": "Robustness beyond acquisition shifts", "content": "The main focus of this work is to improve robustness to acquisition shift. However, by simply changing the types of generated counterfactuals, the counterfactual contrastive pair generation framework can be easily extended to other distribution shifts. For example, we may use the same framework to increase robustness to population shift, improving performance of under-represented subgroups. We illustrate this on chest radiography in Fig. 8, where we generate sex counterfactuals instead of domain counterfactuals in the positive pair creation step, aiming to improve group-wise performance of the 'male' and 'female' subgroups. We can see that the baseline approach (SimCLR) performs sub-optimally on female patients for pneumonia detection across all datasets. Using cross-subgroups counterfactuals positive pairs by intervening on biological sex, we observe performance improvements across subgroups in all datasets and for most levels of training labels."}, {"title": "Discussion and Conclusions", "content": "In this work, we present counterfactual contrastive learning, a novel contrastive pair generation framework enhancing the robustness of contrastively-learned image representations to domain shifts. Evaluating across five datasets, two modalities and two clinically-relevant classification tasks, we show that the proposed counterfactual contrastive pretraining approach yields higher downstream performance than standard contrastive pretraining; improvements which are particularly noticeable for domains less represented in the training set.Moreover, the proposed counterfactual pair generation method is agnostic to the choice of the contrastive objective, as demonstrated by our experiments showing that counterfactual positive pair generation improves results for models using SimCLR as well as DINO-v2 objectives. Importantly, we show that CF-SimCLR (resp. CF-DINO) also improve performance on external domains, not included in the pretraining set compared to SimCLR (resp. DINO). Our experiment on subgroup counterfactual contrastive learning demonstrates its wider applicability beyond acquisition shifts.\nNaturally, gains arising from counterfactual contrastive learning are bounded by the ability to generate realistic domain changes. Generated counterfactuals must be of sufficient quality to faithfully capture the variation relevant for pre-training. Our results demonstrate that current counterfactual image generation models can already produce images of sufficient quality to significantly improve learned representation over the baseline approaches. Note that our proposed approach is compatible with other generative-based augmentation methods such as generating additional images for under-represented classes or subgroups37,38, as we can also apply the counterfactual generation to synthetic images. Our experiments comparing SimCLR+ with CF-SimCLR show that the proposed approach has benefits beyond complementing the training set with synthetic data and that the proposed counterfactual pair generation framework fundamentally changes the organisation of the embedding space (Fig. 7), as such it may complement other approaches to enhance training set diversity."}]}