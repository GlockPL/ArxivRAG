{"title": "Controllable Edge-Type-Specific Interpretation in Multi-Relational Graph Neural Networks for Drug Response Prediction", "authors": ["Xiaodi Li", "Jianfeng Gui", "Qian Gao", "Haoyuan Shi", "Zhenyu Yue"], "abstract": "Graph Neural Networks have been widely applied in critical decision-making areas that demand interpretable predictions, leading to the flourishing development of interpretability algorithms. However, current graph interpretability algorithms tend to emphasize generality and often overlook biological significance, thereby limiting their applicability in predicting cancer drug responses. In this paper, we propose a novel post-hoc interpretability algorithm for cancer drug response prediction, CETExplainer, which incorporates a controllable edge-type-specific weighting mechanism. It considers the mutual information between subgraphs and predictions, proposing a structural scoring approach to provide fine-grained, biologically meaningful explanations for predictive models. We also introduce a method for constructing ground truth based on real-world datasets to quantitatively evaluate the proposed interpretability algorithm. Empirical analysis on the real-world dataset demonstrates that CETExplainer achieves superior stability and improves explanation quality compared to leading algorithms, thereby offering a robust and insightful tool for cancer drug prediction.", "sections": [{"title": "1. Introduction", "content": "Precisely identifying cancer drug response (CDR) holds great promise for developing personalized therapy, which can increase survival rates and reduce expenses [1, 2]. Since testing multiple drugs for a cancer patient is infeasible for practical and financial reasons, there is an urgent demand for computational methods that can accurately predict CDR [3, 4].\nRecently, the developments of Graph Neural Networks (GNNs) have revolutionized the bioinformatics fields, especially in CDR prediction tasks [5]. For example, MOFGCN leverages GNNs to extract valuable information from a heterogeneous network comprising both cell line and drug nodes to predict drug sensitivity [6]. DualGCN combines the chemical structures of drugs and omics data of biological samples to predict CDR through a Dual Graph Convolutional Network model and achieved promising results [7]. However, due to the lack of interpretability considerations during the construction of GNN models, they are often regarded as \u2018black boxes', which undermine the trust of physicians and patients in their predictions [8]. Fortunately, numerous interpretability algorithms for GNNs have been developed [9]. For example, ExplaiNE [10] employs counterfactual explanations, which involve interpreting predictions by quantifying how weakening an existing link affects the prediction probability. GNNExplainer [11] and PGExplainer [12] both treat the soft mask as a trainable variate, selecting a concise subgraph and maximizing the mutual information between the subgraph and the prediction as the explanation. However, when applying these explainability methods to CDR data, we encountered three challenges. 1. When CDR data are integrated with cell line multi-omics data and drug molecular fingerprint data, they create high-dimensional and highly complex datasets [13]. Effectively extracting feature information among nodes within these datasets presents a significant challenge. 2. Current perturbation-based explainability algorithm only focuses on the smallest subgraph that most impacts the prediction, ignoring the fact that the explanation subgraph should contain specific types of edges, which are important in the bioinformatics field. 3. In real-world datasets, the explanations generated by interpretability algorithms often lack definitive ground truth (GT). Developing a universally accepted method to construct GT using specialized biological knowledge remains a significant challenge."}, {"title": "2. Related work", "content": null}, {"title": "2.1. Interpretation method for drug responses", "content": "Previous researches have made significant attempts in the domain of CDR prediction and its interpretation. TANDEM [16] employs Elastic Net regression, a linear model known for its interpretability. Each coefficient in the model reflects the importance of the corresponding feature in the prediction, enabling researchers to intuitively understand which biomarkers are significantly associated with CDR. However, linear models typically struggle to capture the complex interactions between variables, which may limit their ability to characterize the complexity of biological systems effectively.\nPathDSP [17] and BDKANN [18] utilized deep learning techniques to predict"}, {"title": "2.2. Interpretation algorithm for GNN", "content": "Although existing interpretative approaches for CDR prediction have achieved commendable results, the methods used may not meet the evolving interpretative needs in the CDR field, especially as an increasing number of researchers employ GNNs to address problems in this area. Fortunately, several interpretability algorithms have been proposed, offering diverse solutions. For example, GNNExplainer [11] optimized a mask to identify minimal explanatory subgraphs based on perturbations; the proxy-based approach GraphLIME [21] utilized the HSIC Lasso non-linear proxy model for interpretations; Excitation-BP [22] decomposed the target probability into several conditional probability terms for explanations; gradient-based saliency methods SA [23] assessed feature importance by computing the gradient of the output with respect to each input feature; and generative XGNN [24] seek to maximize prediction probability by generating an explanatory graph. Although these methods excel in node classification tasks, they often cannot control the model's focus on specific types of relationships during interpretation, which may lead to the model paying attention to less important structures, thereby providing low-quality explanations in CDR field."}, {"title": "3. Methods", "content": "In this section, we present the technical details of the proposed CETExplainer. We define CDR prediction as a link prediction task within a directed"}, {"title": "3.1. Directed graph structure construction", "content": "The process of constructing a directed heterogeneous network from raw data is shown in Figure 1. Inspired by GraphCDR [25], we utilize DNN layers to integrate multi-omics data from cell lines to derive each cell line representations $H_c \\in \\mathbb{R}^{\\text{cell num}\\times\\text{dim}}$ and use GNN layers to extract drug representations $H_a \\in \\mathbb{R}^{\\text{drug num}\\times\\text{dim}}$ from drug SMILES. For similarity triples, based on the node representation $H_c$ and $H_a$, we apply cosine similarity with thresholds of 0.96 for cell lines and 0.85 for drugs to obtain cell line triples and drug similarity triples. These specific thresholds are chosen to ensure that similar cell line triples account for 20% of all possible cell line triples and the same applies to drug triples.\nFor response data, we convert the half-maximal inhibitory concentration (IC50) values into sensitivity and resistance relationships, following the approach in [6]. A drug is considered sensitive in a cell line if its IC50 is below the sensitivity threshold; otherwise, it is considered resistant. By combining the similarity data and the response data, we construct a directed heterogeneous graph $G = (V,E,R)$. Let $V$ denote the set of cell line nodes, $E$ denote the edges between these nodes, and $R$ represent the different types of relationships. In our project, there are four types of relationships: sensitivity (Sen), resistance (Res), drug similarity (Dsim), and cell similarity (Csim)."}, {"title": "3.2. Prediction method", "content": "To comprehend the underlying mechanisms of the CETExplainer algorithm, it is essential to have a foundational understanding of GNNs. Given that R-GCN has demonstrated its efficacy in handling downstream tasks based on multi-relational directed heterogeneous graphs [15], we choose to utilize the R-GCN model to address the link prediction problem in the directed heterogeneous graph $G$ and acquire new potential links, as shown in Figure 1.\nIn the R-GCN framework, parameter updates involve two main processes. First, for each node $v_i \\in V$ at layer $l$, the model aggregates features from itself and neighboring nodes $v_j \\in V$ that are connected through relationship $r \\in R$. This means that each relationship corresponds to a specific parameter $W_r$. Second, the aggregated information from layer $l$ undergoes a nonlinear transformation before being passed to the next layer $l+1$, resulting in the final embedded features. The following is the propagation model for calculating the forward-pass update of a node denoted by $v_i$:\n$h_i^{(l+1)} = \\sigma(\\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_i^r} \\frac{1}{c_{i,r}} W_r^{(l)} h_j^{(l)} + W_0^{(l)} h_i^{(l)})$ (1)\nwhere $r$ represents the type of relationship, $\\mathcal{N}_i^r$ denotes the set of neighboring nodes of node $i$ with relationship $r$, $c_{i,r}$ is a normalization constant specific to the problem, $W_r$ is the parameter for relationship $r$, $h_i^{(l)} \\in \\mathbb{R}^{d(l)}$ is the hidden state of the node at the $l$-th layer of the neural network, and $d(l)$ is the dimensionality of the representation at that layer.\nWe treat the triples that already exist as positive samples, and randomly perturb the subject node or object node then generated the negative samples. The technical details can be found in R-GCN [15]."}, {"title": "3.3. Edge-type-specific interpretability algorithm", "content": "In this section, we provide an intuitive description of the underlying principles of the CETExplainer algorithm as shown in Figure 2. The original graph $G_0$ and corresponding prediction result $Y_0$ are combined to generate the computational graph $G_c$. It should be noted that $G_0$ is the directed graph and $Y_0$ is new link in Figure 1. Then, we extract the $k$-hop neighborhood graph $G_n$ of the edge to be explained. We apply a optimizable mask and the edge-type-specific weighting mechanism to $G_n$ to obtain the initial explanation subgraph $G_s$. Subsequently, the mask is iteratively optimized by"}, {"title": "3.3.1. Mutual information", "content": "Given the prediction link $(i, r, j)$, CETExplainer identifies the associated cluster $V_i$ for cell line $i$ and $V_j$ for drug $j$, where $V_i$ includes all cell line nodes that have similar or indirectly similar relationships with cell line $i$, and $V_j$ comprises all drug nodes similarly associated with drug $j$. Within the graph $G_c$ formed by these cell line and drug clusters, given a pair of nodes $v_i$ and $v_j$, the goal is to determine a subgraph $G_s$ that is most critical for the prediction outcome $(i, r, j)$. Inspired by GNNExplainer [11], we use Mutual Information (MI) to formalize the concept of importance, and we formulate MI as follows:\n$\\underset{G_s}{\\text{max}} MI(Y, G_s) = H(Y) - H(Y|G = G_s)$ (2)\nfor node $v$, MI quantifies the change in the probability of prediction when $v$'s computation graph is limited to explanation subgraph $G_s$. Examining Eq. (2), we see that the entropy term $H(Y)$ is constant because model parameter"}, {"title": "3.3.2. Structure score", "content": "To account for the varying contributions of different edges during the explanation process, we have augmented the Mutual Information (MI) with an additional optimization objective. This ensures that while maximizing MI, the score of $G_s$ is also maximized, thereby refining the model to favor specific edges when selecting $G_s$ during the explanation. The scoring function is defined as follows:\n$\\text{Score}(G_s) = \\sum_{r \\in G_s} \\sum_{i=0}^{R} w_{r} r_{i} + \\text{Penalty}(N)$ (4)\nit is computed based on the edges within the subgraph $G_s$, where $N$ is the total number of edges in $G_s$, $r$ denotes a specific type of relationship within the subgraph, $N_r$ represents the number of nodes involved in relationship $r$, $w_r$ is the predefined weight for edges of type $r$, and $r_i$ denotes a node involved in the relationship $r$. A 'Penalty' coefficient, related to the number of edges, is used to control the size of the graph. The penalty is defined as:\n$\\text{Penalty}(N) = \\alpha N + \\beta N^2$ (5)\nwhere $\\alpha$ and $\\beta$ are positive hyperparameters used to adjust the weight of the penalty term.By synthesizing this score with equations (1) and (2), the final optimization objective is as follows:\n$\\underset{G_s}{\\text{max}} MI(Y, (G_s, X_s)) + \\text{Score}(G_s)$ (6)"}, {"title": "3.4. Evaluation algorithm for explanations", "content": "In the previous section, we introduced our interpretable model, CETExplainer. A natural question is how to evaluate the explanations it provides. In this part, we propose a quantitative evaluation method based on GT on the heterogeneous network constructed from both similarity and drug response data. We will elaborate on this method from two aspects: constructing the GT and assessing the metrics."}, {"title": "3.4.1. Constructing ground truth", "content": "The directed heterogeneous network comprises resistance edges, sensitivity edges, drug similarity edges, and cell line similarity edges. The final explanation can be derived from these edges and corresponding nodes. We utilize these relationships and known triples to construct the GT. Initially, we identify the one-hop similar cell line neighbors and one-hop similar drug neighbors. Subsequently, we search for existing sensitive or resistant relationships among these similar drugs and cell lines to formulate the GT. The final GT we constructed can be categorized into three scenarios, as illustrated in Figure 3. Given an edge to be explained, by comparing the overlap between the triples provided by the interpretable model and the GT, we can provide a quantitative assessment for explanations."}, {"title": "3.4.2. Evaluating metrics", "content": "To quantitatively assess the quality of the model explanations using GT, we have introduced three evaluation metrics: Precision@k, Recall@k and F1@k. Precision@k is used to measure the proportion of correct explanations within the top-k ranked predictions, while Recall@k assesses the proportion of these correct top-k explanations relative to the total number of correct explanations provided by the model. The computational methods for these metrics are defined as follows:\n$\\text{Precision@k} = \\frac{\\text{TP@k}}{\\text{TP@k} + \\text{FP@k}}$ (7)\n$\\text{Recall@k} = \\frac{\\text{TP@k}}{\\text{TP@k} + \\text{FN@N}}$ (8)"}, {"title": "4. Experiments and results", "content": "In this section, we first introduce the experimental setups 2 and then demonstrate the performance of the proposed model CETExplainer through comparison with baseline methods."}, {"title": "4.1. Datasets", "content": "Drug response triples are derived from the Genomics of Drug Sensitivity in Cancer (GDSCv2) [26], a comprehensive resource that compiles sensitivity response data of tumor cell lines to various drugs, which is the largest public repository for tumor cell drug sensitivity currently available. The features of cell lines are primarily sourced from the Omics data of the Cancer Cell Line Encyclopedia (CCLE) [27], while the drug features are derived from SMILES data hosted on PubChem [28]. In total, we utilize 17,071 drug response pairs,"}, {"title": "4.2. Implementation details", "content": "In this study, five-fold cross-validation is employed to assess the performance of CETExplainer. Specifically, we combine the drug response triples with the similarity triples, shuffle them randomly, and then evenly divide them into five subsets. Each subset is used in turn as a test set, with the remaining subsets serving as the training set. The performance metrics are averaged to produce the final results. The ratio of positive to negative samples in the drug response prediction model is set at 1:2.\nRegarding the weight scores of edges in CETExplainer, we referenced the distribution of unweighted edge types. Noticing that Relationships 1 and 2 received less attention, we set the weight scores for the four types of edges to 0.1, 0.4, 0.4, and 0.1, respectively, to balance the model's focus on various edge types as much as possible."}, {"title": "4.3. Experimental results", "content": "Given the scarcity of directly applicable explanation algorithms for link prediction tasks, we selected the perturbation-based GNNExplainer and the counterfactual-based ExplaiNE as baseline models. We began by comparing the explanation metrics of the baseline models, followed by an analysis of these metrics in relation to the proportion of different edge types within the explanations. Subsequently, we identified the optimal epoch for CETExplainer through parameter tuning. Additionally, we performed further experiments to investigate how the number of training epochs in the prediction model influences the specificity of the model's explanations."}, {"title": "4.3.1. Comparison with baselines", "content": "Figure 4(a) presents a comparison between CETExplainer and two baseline methods, GNNExplainer and ExplaiNE. The metrics for all models were obtained under conditions with optimal parameter settings. It is evident that CETExplainer achieves the best performance with an F1 score of 0.6594. Notably, although GNNExplainer also employs mutual information maximization to optimize the mask, it faces challenges due to the imbalance in edge types within the dataset. If it fails to specifically focus on certain edge types, it can lead to insufficient explanations for some edges due to the sparsity issues. As shown in Figure 4(b), we calculated the proportion of various edge types within the top 10 explanations for each edge. In GNNExplainer's explanations, the proportion of type 1 edges is only 0.02, whereas CETExplainer increases the weight of type 1 edges, raising their proportion to 0.11 and thereby significantly improving accuracy. The subpar performance of ExplaiNE in this context may be attributed to its explanation method, which primarily assesses the impact of removing a single edge on the prediction. Since cancer drug response is highly complex, the removal of a single edge does not significantly affect the overall prediction. Therefore, ExplaiNE is not suitable for data with high inter-node correlations."}, {"title": "4.3.2. Qualitative Assessment", "content": "We perform the qualitative assessment on our proposed method and the baseline models. We visualize the fine-grained explanation of the predicted link as is shown in Figure 5.\nFor the res edge to be explained (CAL29, res, NSC-207895), both CETExplainer and GNNExplainer correctly predict 9 edges, which is more than"}, {"title": "4.3.3. Explanation across epochs", "content": "To evaluate the stability of the explanation model, we analyzed how the explanation metrics changed across different training epochs. We trained models with the same architectures as previously described, varying the number of training epochs from 10 to 90. Each experiment utilized two layers of RGCN. The results, as shown in Figure 6(a), indicate that the explanation model achieves optimal performance at 70 epochs, with an F1 score of 0.6594."}, {"title": "4.3.4. Stability of the explanation model", "content": "After analyzing the metrics of the explanation model across different epochs, we aim to investigate how the number of training epochs of the prediction model influences the explanations provided by the explanation model. We refer to the influence as model stability and compared the stability of three different models. Using data from the second fold as an example, we calculated the explanation overlap under adequate training conditions (4k and 5k epochs). This overlap is defined as the average number of overlapping explanations per edge. Additionally, we presented a heatmap of the overlap, as shown in Figure 6(b). The results demonstrate that the average stability of our model and GNNExplainer both exceed 8.0, while ExplaiNE exhibits a stability of only 1.058. This indicates that ExplaiNE is more susceptible to variations in the prediction model's training conditions."}, {"title": "5. Conclusion", "content": "In this paper, we propose CETExplainer, a novel interpretability algorithm with controllable edge weights for multi-relational GNNs, tailored for explaining drug response predictions. CETExplainer leverages a learning framework that maximizes mutual information and the structure scores of explanatory subgraphs, thereby achieving fine-grained, high-quality explanations. Specifically, we integrate multi-omics features of cell lines and encode drug SMILES to learn representations of cell line and drug nodes effectively. Additionally, we create GT based on existing drug response data, enabling non-experts to comprehend and quantitatively assess the effectiveness of our explanation model. Extensive experimental results demonstrate that CETExplainer surpasses two baseline models in providing interpretable explanations for drug response predictions. In the future, we need to further investigate the relationship between prediction and explanation, as well as how explanations can enhance prediction performance."}, {"title": "6. Funding", "content": "This work was supported by the grants from the National Natural Science Foundation of China (62102004), the Introduction and Stabilization of Talent Project of Anhui Agricultural University (yj2019-32)."}]}