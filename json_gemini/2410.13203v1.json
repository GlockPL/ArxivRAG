{"title": "TABSEQ: A FRAMEWORK FOR DEEP LEARNING ON TABULAR DATA VIA SEQUENTIAL ORDERING", "authors": ["Al Zadid Sultan Bin Habib", "Kesheng Wang", "Mary-Anne Hartley", "Gianfranco Doretto", "Donald A. Adjeroh"], "abstract": "Effective analysis of tabular data still poses a significant problem in deep learning, mainly because features in tabular datasets are often heterogeneous and have different levels of relevance. This work introduces TabSeq, a novel framework for the sequential ordering of features, addressing the vital necessity to optimize the learning process. Features are only sometimes equally informative, and for certain deep learning models, their random arrangement can hinder the model's learning capacity. Finding the optimum sequence order for such features could improve the deep learning models' learning process. The novel feature ordering technique, which we provide in this work, is based on clustering and incorporates both local ordering and global ordering. It is designed to be used with a multi-head attention mechanism in a denoising autoencoder network. Our framework uses clustering to align comparable features and improve data organization. Multi-head attention focuses on essential characteristics, whereas denoising autoencoder highlights important aspects by rebuilding from distorted inputs. This method improves the capability to learn from tabular data while lowering redundancy. Our research demonstrating improved performance through appropriate feature sequence rearrangement utilizing raw antibody microarray and two other real-world biomedical datasets validates the impact of feature ordering. These results demonstrate that feature ordering can be a viable approach to improved deep learning of tabular data.", "sections": [{"title": "Introduction", "content": "Deep learning has transformed how we handle and comprehend diverse data types, resulting in unparalleled progress in numerous fields. Deep learning models have outperformed conventional techniques in audio analysis, picture identification, and Natural Language Processing (NLP), opening the door to new applications previously thought impractical. For example, Convolutional Neural Networks (CNNs) have emerged as the mainstay of image-processing applications, demonstrating exceptional performance in picture classification, object recognition, and other applications [15], [22]. In NLP, Transformer-like models have established new benchmarks for text summarization, machine translation, and question-answering systems [36], [11]. Additionally, deep learning has helped audio processing by advancing speech recognition and synthesis, greatly enhancing user interaction with technology [4], [25], [30]. These achievements demonstrate how deep learning is an essential tool for applications where standard feature engineering fails because it can grasp intricate patterns and relationships inside high-dimensional data.\nThe quest for an optimal deep learning architecture for tabular data, crucial in sectors like finance, healthcare, and retail, remains ongoing. Unlike image, text, and audio data, tabular data's structure, rows representing samples, and columns as features present distinct challenges, especially in modeling complex feature relationships that lack spatial or sequential correlation. Innovative model architectures and data representation methods are essential to address tabular data's unique aspects. Models such as TabNet[5], Neural Oblivious Decision Ensembles (NODE)[27], and TabTransformer[18] have emerged as practical solutions alongside popular gradient-boosting tree models. However, gaps remain in handling scenarios with high-dimensional features against smaller sample sizes, such as genomic or other medical data.\nWe introduce TabSeq, a framework for deep learning on tabular data, using the feature ordering to optimize tabular data utilization. Our approach is motivated by methods of band ordering often used in the efficient analysis of hyperspectral images. Adapting band ordering from hyperspectral images[32] to tabular data involves comparing dataset features to spectral bands, where features, like bands, vary in informational value. This approach uses statistical and machine learning methods to prioritize significant features and reduce redundancy, enhancing dataset efficiency similar to compression in hyperspectral imaging. The bandwidth minimization problem in communication networks[38] focuses on optimizing data transmission order or compressing data to meet bandwidth limits, akin to arranging features in tabular data for deep learning models. The novel contributions of our paper are as follows:\n1.  We present a novel feature ordering technique that combines local ordering and global ordering to optimize feature sequences and clustering to group comparable features. This innovative method systematically improves learning and significantly improves the model's performance on tabular datasets by prioritizing features according to their relevance and informative content.\n2.  Our framework enables a Denoising Autoencoder (DAE) architecture to incorporate the Multi-Head Attention (MHA) mechanism smoothly. This integration highlights important characteristics, eliminates redundancy by rebuilding inputs from partially corrupted versions, and allows for dynamic attention to vital elements.\n3.  Our studies using raw antibody microarray and other datasets show that our feature ordering approach substantially improves the performance of deep learning models. The outcomes demonstrate how feature sequencing is crucial for training and validating the potential of feature ordering in tabular data processing inside deep learning frameworks.\nThese contributions collectively address the challenges of heterogeneous feature relevance in tabular data, setting a new precedent for data preprocessing and model optimization in deep learning applications."}, {"title": "Related Work", "content": "Feature ordering in tabular datasets is essential for improving machine learning models' interpretability, accuracy, and efficiency, particularly in deep learning. Models that recognize and rank important features can learn new information more quickly, require less training time, and exhibit better generalization on unobserved data [41]. While feature ordering is essential for all tabular data types, including numerical data, it influences models that use data structures, such as attention mechanisms or specific autoencoders.\nAttention-based Models: TabNet[5] employs an attention mechanism for feature selection in tabular data, enhancing performance and interpretability without rearranging features. TabTransformer[18] uses contextual embeddings to improve accuracy in handling categorical data, though it requires pre-training and fine-tuning. AutoInt[31] specializes in Click-Through Rate (CTR) prediction by learning feature interactions with a self-attentive network despite assuming unordered features. ASENN[26] predicts pavement deterioration with multi-dropout attention layers, offering efficient infrastructure maintenance solutions. Attention-based models might find it difficult to determine how important a particular feature is to the model's predictions; feature ordering can help with this problem by highlighting the elements with the most significant impact.\nTree-based Models: The Tree Ensemble Layer (TEL)[14] by Hazimeh et al. enhances neural networks with the efficiency of tree ensembles through \"soft trees\" and sparse activations, improving performance. TEL, however, does not perform well in capturing complex feature interactions. NODE[27] by Popov et al. combined deep learning flexibility with gradient-boosted with the benefit of decision trees. They achieved superior outcomes via differentiable trees and entmax transformation, albeit with potential limitations in capturing nuanced feature interactions. Tree-based models might struggle to explain complex feature associations; feature ordering fills this gap by arranging features to clarify their relationships.\nLLM-based Models: TabLLM[16], developed by Hegselmann et al., leverages LLMs for few-shot categorization of tabular data by translating tables into natural language, showing superior performance over traditional techniques with limited data. MediTab[40] by Wang et al. introduced a \"learn, annotate, refine\" approach combined with LLMs for medical data predictions, achieving high performance and excellent zero-shot capabilities without fine-tuning. IngesTables[43], presented by Yak et al., creates scalable tabular foundation models, addressing key issues, such as large cardinality and semantic relevance, through an attention-based method with LLMs, offering cost-effective alternatives"}, {"title": "Methodology", "content": "This section presents our deep learning architecture, designed primarily to analyze tabular data effectively, as seen in Figure 1. The process begins with feature clustering and then moves to local ordering and global ordering to improve input feature arrangements. These rearranged features are fed into an MHA mechanism and a DAE, ultimately leading to feature extraction and classification model decisions. Our methodology introduces a novel feature ordering technique to improve the analysis of tabular datasets. This methodology seeks to improve prediction accuracy and robustness for various applications by capturing intricate feature interactions and underlying patterns in the data."}, {"title": "Feature Ordering", "content": "Feature ordering is finding an optimal arrangement of features within and across clusters to minimize a defined cost function that reflects the disorganization of feature positioning. This involves computing permutations that best sequence the features according to their relationships. Given a dataset $X \\in R^{n \\times m}$ with n samples and m features, we define a set of graphs ${G_1, G_2, ..., G_k }$, where k is the number of clusters, and each graph $G_{ec} = (V_c, E_c)$ for cluster c has vertices $V_i \\in V_e$ corresponding to features within that cluster. The edges $(v_i,v_j) \\in E_c$ represent significant relationships between features i and j within the cluster. For each cluster c, the goal is to find a permutation $\\pi_c$ of its features that minimizes a local cost function $F_c$:\n$F_c(\\pi_c) = \\sum\n(v_i,v_j)\\in E_c\\left| \\pi_c(i) - \\pi_c(j) \\right|$\nwhere i = $\\pi_c(v_i)$ and j = $\\pi_c(v_j)$ represent the indices of features $v_i$ and $v_j$ in the permutation $\\pi_c$, minimizing the disorganization within the cluster. The overall goal is to find a global permutation $\\Pi$ that integrates the local permutations $\\pi_c$'s and minimizes a global cost function $F_G$:\n$F_G(\\Pi) = \\sum_{c=1}^{k} \\alpha_c \\cdot F_c(\\pi_c)$\nwhere $\\alpha_c$ represents the weight or importance of cluster c in the global context with $\\sum \\alpha_c = 1$. The optimal permutation $\\Pi^*$ minimizes $F_G(\\Pi)$:\n$\\Pi^* = arg \\min_{\\Pi} F_G(\\Pi)$\nLocal Ordering Function: Local ordering is the computation of a permutation that minimizes the sum of absolute differences in positions of related features within a cluster, thereby reducing feature dispersion.\n$L(G_C) = arg\\min_{\\pi_c} \\sum_{(v_i,v_j) \\in E_c} |\\pi_c(i) - \\pi_c(j)|$\nHere, $L(G_c)$ outputs the permutation $\\pi_c$ for cluster c that minimizes the feature dispersion, where $\\pi_c(i)$ is the position of feature i in the permutation $\\pi_c$, and the sum quantifies the total dispersion of features in the cluster.\nGlobal Ordering Function: Global ordering is the process of integrating local permutations from all clusters into a global permutation that minimizes the weighted sum of within-cluster feature dispersion to enhance the deep learning"}, {"title": "Feature Dispersion", "content": "In the context of feature ordering for a tabular dataset, the term \u201cFeature Dispersion\u201d describes the degree to which features with a strong relationship (or dependency) are placed far apart in the ordering. The goal would be to minimize this dispersion so that related features are positioned closer together, which could be advantageous for specific deep learning models that can benefit from the structure of the data (See statistical dispersion in [21], [33]).\nFor instance, a generalized feature dispersion for a cluster $G_c$ could be defined as:\n$D(\\pi_c) = \\sum_{(v_i,v_j) \\in E_c} w_{ij} \\cdot |\\pi_c(i) - \\pi_c(j)|$\nWhere, $\\pi_c$ is the permutation of features within cluster c, $w_{ij}$ is a weight assigned to the edge between features i and j (which could be based on the strength of the relationship between the features e.g., correlation or mutual information), $|\\pi_c(i) - \\pi_c(j)|$ is the absolute difference in the ordered positions of features i and j within the permutation $\\pi_c$.\nFeature Dispersion and Variance: We adopt variance as a metric to guide the ordering of features locally [29]. This approach is based on the premise that organizing features to minimize their dispersion within clusters can enhance model performance by affecting the variance of these features in a beneficial manner. We understand that feature dispersion within a cluster, $D(\\pi_c)$, reflects how spread out the features are in terms of their arrangement or ordering based on certain criteria (e.g., importance, similarity, etc.). Variance, $Var(X_i)$, measures the spread of values for a given feature i across the dataset or within clusters. The goal is to understand how minimizing $D(\\pi_c)$ influences $Var(X_i)$ for features within the same cluster. A decrease in $D(\\pi_c)$ (i.e., reduced dispersion or more closely arranged features based on their relationships) leads to an increase in the homogeneity of feature values within the cluster. This homogeneity, in turn, can lead to a more meaningful and possibly reduced variance ($Var(X_i)$) for the features within the cluster, as related features that behave similarly or have strong relationships are positioned closer together, thus reflecting their actual data distribution more accurately. The conceptual relationship can be summarized as:\n$Var(X_i) \\propto \\frac{1}{D(\\pi_c)}$\nThis expression suggests that as feature dispersion within a cluster decreases (making $D(\\pi_c)$ smaller), the variance of features within that cluster ($Var(X_i)$) becomes more meaningful of the true data distribution. The inverse proportion-ality indicates that lower dispersion (closer grouping of related features) leads to a more stable or accurate variance representation, underlining the importance of thoughtful feature arrangement in enhancing model understanding and performance. Algorithm 1 captures the general procedure for our feature ordering, including both steps of local and global ordering."}, {"title": "MHA", "content": "MHA or Multi-Head Attention, inspired from[36], is the integration that serves as a cornerstone for enhancing the model's capacity to capture complex dependencies and interactions within the input data. This mechanism's key feature is its capacity to narrow down an input sequence through several attention heads at once, which enables the model to pay attention to data from various representation subspaces at various points in time. Formally, for each head h, we perform linear transformations on the input X to obtain queries $Q_h$, keys $K_h$, and values $V_h$ using parameter matrices $W_h^Q$, $W_h^K$, and $W_h^V$, respectively:\n$Q_h=XW_h^Q, K_h=XW_h^K, V_h = XW_h^V$\nSubsequently, we compute the scaled dot-product attention for each head. The attention function operates on queries, keys, and values and scales the dot products of queries with all keys by $\\frac{1}{\\sqrt{d_k}}$, where $d_k$ is the dimensionality of the keys and queries. This scaling factor helps stabilize the gradients during training. The attention scores are then passed through a softmax function to obtain the weights on the values:\n$Attention(Q_h, K_h, V_h) = softmax(\\frac{Q_hK_h^T}{\\sqrt{d_k}})V_h$"}, {"title": "DAE", "content": "TabSeq leveraged an MHA layer in addition to DAE [37] architecture to overcome the difficulties associated with learning robust representations from high-dimensional tabular data. The DAE enhances the model's performance on ensuing tasks by lowering noise and extracting significant features. The DAE architecture comprises an encoder and a decoder, where the encoder maps the input data X to a latent space representation Z, and the decoder reconstructs the input from Z. The MHA layer improves the encoder's capacity to focus on relevant information by allocating different levels of attention to different data segments. Formally stated, the encoding procedure is as follows:\n$Z = f_{encoder}(X) = ReLU(W_e \\cdot MHA(X) + b_e)$\nWhere X is the input data, $W_e$ and $b_e$ are the weights and bias of the encoding layer, respectively, and ReLU denotes the Rectified Linear Unit activation function. The MHA(X) function represents the output of the MHA layer applied to X. The decoder, aiming to reconstruct the input data from the latent representation Z, is given by:\n$\\hat{X} = f_{decoder}(Z) = Sigmoid(W_d \\cdot Z + b_d)$\nwhere, $\\hat{X}$ is the reconstructed data, $W_d$ and $b_d$ are the decoder weights and bias, and Sigmoid is the activation function facilitating reconstruction. The loss function for the DAE, aiming to minimize the reconstruction error, is defined as the Mean Squared Error (MSE) between the original input X and its reconstruction $\\hat{X}$:\n$L_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (X_i - \\hat{X_i})^2$\nwhere N is the number of samples in the dataset. The MHA layer added to the DAE architecture improves the encoder's capacity to identify and highlight the most informative characteristics by utilizing the attention mechanism [45], [34], [9], [35]. This leads to a less noisy and more discriminative representation in the latent space, which is essential for tasks that come after, like classification. Using a sequential network with dense layers, ReLU activation [23] for hidden layers, and sigmoid activation [10] for the output layer, optimized for classification tasks, the final classification model is trained on the encoded representations. Our methodology efficiently tackles the problem of learning from high-dimensional and noisy tabular data by incorporating the MHA layer within the DAE framework, significantly improving the model's predictive performance and robustness."}, {"title": "Feature Extraction and Classifier Model", "content": "In the TabSeq framework, the DAE is instrumental in preprocessing the input data by denoising and enhancing feature salience through its robust feature extraction process, where the encoder component transforms the corrupted input into a refined, lower-dimensional representation. These enhanced features are then utilized by the classifier, which is specifically configured with a softmax activation for multi-class scenarios to generate class probability distributions or a sigmoid activation for binary classification to yield a probability of class membership. This setup ensures that the classifier operates on high-quality features extracted post-DAE, thereby optimizing the model's accuracy and adaptability to different classification tasks. In a nutshell, the DAE processes the input data, which is then followed by a feature extraction process to extract robust, noise-reduced features, which are then utilized by the classifier to ensure precise predictions based on clean and relevant information, illustrating an essential sequential information flow where the classifier's efficacy is significantly enhanced by the high-quality features provided by the DAE."}, {"title": "Experimental Results", "content": "In our research, the autoimmune diseases dataset used in [24] and publicly released in [3] contains 393 features targeting five disease classes of 316 samples, detailing each antibody's signal intensity. The ADNI dataset [1] includes 177 samples and 263 features with target attributes like AD123, ABETA12, and AV45AB12, representing various stages of Alzheimer's disease and captured through DTI analysis for white matter integrity. Lastly, the WDBC dataset [2] offers 32 features derived from breast mass images, aiming to differentiate between 357 benign and 212 malignant cases. Each dataset was partitioned into training, validation, and testing subsets following a 70:15:15 split, focusing on specific target attributes for comprehensive classification and analysis. TabSeq model with feature ordering integrates an MHA mechanism with four heads and dimensionality of 32 alongside a DAE comprising dense ReLU-activated layers. It was trained over 50 epochs with a batch size of 32 using the Adam optimizer, and the model employs MSE loss for the DAE and binary cross-entropy loss for the classifier. This configuration facilitates nuanced feature extraction and robust classification, as evidenced by the model's validation performance, optimizing computational efficiency and learning effectiveness. In our analysis, feature ordering was uniformly applied across baseline models using k-means clustering with 5 clusters in ascending order for the autoimmune diseases and ADNI datasets and 3 clusters for the WDBC dataset. This consistent methodology underscores the effectiveness of feature ordering in enhancing model performance across the board, with TabSeq demonstrating particularly notable improvements in accuracy and AUC with feature ordering."}, {"title": "Experiments with Autoimmune Diseases' Dataset", "content": "Two cases were included in our investigation (Table 1), which showed how feature ordering affected model performance. In Case 1, NODE's accuracy sharply declined, whereas TabSeq's increased from 85.42% to 87.23% with feature ordering. Case 2 demonstrated how feature ordering significantly improved TabNet's accuracy, raising it to 94.44%."}, {"title": "Ablation Studies", "content": "We assessed the TabSeq model's performance using the autoimmune diseases' dataset, mainly how the clustering algorithms affected feature ordering. Various clustering algorithms affected AUC and model accuracy in different scenarios. For example, in Case 1, the maximum accuracy of 87.23% and the highest AUC of 0.92 were obtained using DBSCAN with a single cluster with the features sorted in ascending order. Case 2, on the other hand, achieved 94.44% accuracy and optimal performance using k-means at five clusters with features sorted in descending order. Surprisingly, DBSCAN achieved perfect accuracy and an AUC of 100% and 1.00 in Case 4, with features sorted in descending order. The significance of the number of clusters, specific clustering algorithm, and sorting order were also observed for these cases. Fig. 3 shows the assessment for Case 2, whereas the figures for other cases also looked similar. These results highlight the crucial role that clustering configurations in feature ordering play in improving the predictive power of the TabSeq model, indicating that the model's ability to distinguish intricate patterns of autoimmune diseases is greatly influenced by strategic cluster formation and feature ordering."}, {"title": "Experiments with ADNI and WDBC Dataset", "content": "The studies done on the ADNI dataset (Table 2) show how effective feature ordering is in several models, such as TabNet, NODE, and TabSeq. Across two target attributes, such as AD123 and ABETA12, the addition of feature ordering resulted in significant gains in accuracy and AUC scores; nonetheless, AD123 presented a multi-class classification challenge. For example, the TabSeq model's accuracy improved from 66.67% to 67.68% for the AD123 target. Similarly, substantial improvements were noted for the ABETA12 target respectively 64.44% to 75.13% for TabSeq. By efficiently selecting and rearranging features according to their informative contribution to the predictive goal, feature ordering improved the performance of deep learning models. These results highlight the importance of adopting feature ordering to refine model predictions for complex datasets. The studies conducted with the WDBC dataset (Table 2) demonstrate how feature ordering can improve model accuracy and AUC scores. The TabNet, NODE, and TabSeq versions performed better when feature ordering was used. Notably, compared to its excellent performance without feature ordering, the TabSeq model's accuracy increased, reaching 94.71% with feature ordering. TabTransformer boosts AUC; TANGOS shows lower accuracy; TabPFN excels in feature-limited datasets like WDBC but without feature ordering. We employed k-means clustering with 5 clusters for the ADNI dataset and 3 for the WDBC dataset, ordering them ascendingly. As Fig. 3 shows, increasing cluster numbers initially enhanced performance for most of the cases but eventually declined after specific points. These results affirm that our feature ordering approach boosts model accuracy and AUC across datasets, noting its importance in advancing deep learning for tabular data."}, {"title": "Conclusion", "content": "This paper introduces TabSeq, a novel framework that employs feature ordering to enhance deep learning's performance on tabular datasets significantly. By integrating local ordering and global ordering within a DAE equipped with an MHA mechanism, our method systematically optimizes feature sequences to improve learning efficacy. Studies conducted on raw antibody microarray data and other medical datasets have underscored the capability of strategic feature sequencing to yield substantial performance gains. These empirical results reinforce feature ordering's potential as a game-changing technique in deep learning for tabular data. Its effectiveness is predominantly seen in sequence-based architectures, and its performance on low-dimensional datasets remains to be determined. Further research is needed to refine and extend the method's applicability to various architectures and datasets."}]}