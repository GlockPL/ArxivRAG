{"title": "Deconstructing What Makes a Good Optimizer for Language Models", "authors": ["Rosie Zhao", "Depen Morwani", "David Brandfonbrener", "Nikhil Vyas", "Sham Kakade"], "abstract": "Training language models becomes increasingly expensive with scale, prompting\nnumerous attempts to improve optimization efficiency. Despite these efforts, the\nAdam optimizer remains the most widely used, due to a prevailing view that it is\nthe most effective approach. We aim to compare several optimization algorithms,\nincluding SGD, Adafactor, Adam, and Lion, in the context of autoregressive\nlanguage modeling across a range of model sizes, hyperparameters, and architecture\nvariants. Our findings indicate that, except for SGD, these algorithms all perform\ncomparably both in their optimal performance and also in terms of how they fare\nacross a wide range of hyperparameter choices. Our results suggest to practitioners\nthat the choice of optimizer can be guided by practical considerations like memory\nconstraints and ease of implementation, as no single algorithm emerged as a clear\nwinner in terms of performance or stability to hyperparameter misspecification.\nGiven our findings, we further dissect these approaches, examining two simplified\nversions of Adam: a) signed momentum (Signum) which we see recovers both the\nperformance and hyperparameter stability of Adam and b) Adalayer, a layerwise\nvariant of Adam which we introduce to study Adam's preconditioning. Examining\nAdalayer leads us to the conclusion that the largest impact of Adam's precondi-\ntioning is restricted to the last layer and LayerNorm parameters, and, perhaps\nsurprisingly, the remaining layers can be trained with SGD.", "sections": [{"title": "1 Introduction", "content": "As language model architectures increase in scale, pretraining becomes more expensive. In response,\nnumerous efforts have been made to design efficient optimizers to mitigate these costs, and yet Adam\n[Kingma and Ba, 2015] remains the primary optimizer used for training language models. This\npersistent preference for Adam is rooted in an underlying belief that Adam generally outperforms\nalternative optimization algorithms. Although newly proposed optimizers run ablations to demonstrate\nsuperior performance to Adam for select architectures and tasks [Liu et al., 2024, Chen et al., 2023],\nthere is no consensus among the literature about the relative performance of these optimizers. In fact,\nto the best of our knowledge, [Kaddour et al., 2024] is the only work comparing these optimizers but\nin the context of masked language modeling and at a single model scale.\nIn this work, we perform a comprehensive sweep for training autoregressive language models across\ndifferent optimizers, hyperparameters, architectures, and scale. Along with looking at optimal\nperformance, we argue that due to the difficulty of hyperparameter tuning with increasing scale\n[Yang et al., 2021], the stability of performance with respect to hyperparameter choices is equally"}, {"title": "2 Related work", "content": "One closely related work to ours is Wortsman et al. [2024], which explores the stability of Adam\nwith respect to learning rate. We extend the comparison to other optimizers including SGD, Lion and\nAdafactor, as well as other hyperparameters including momentum and weight decay.\nOptimizers: SGD [Robbins and Monro, 1951] had been the workhorse optimizer for deep learning\nuntil 2015, when Adam [Kingma and Ba, 2015] was introduced. Adam is a diagonal preconditioning\nalgorithm that maintains a per-parameter learning rate. Over time, coarser variants of Adam have\nbeen proposed, which do not explicitly maintain a learning rate per parameter. Adafactor [Shazeer\nand Stern, 2018, Zhai et al., 2022] maintains a rank-1 approximation of the preconditioner matrix\nof Adam. Prior works have also explored the similarity of Adam with variants of Signum [Balles\nand Hennig, 2018], and recently, a close variant of Signum, called Lion [Chen et al., 2023], was\ndiscovered using symbolic search over algorithms. Some other optimizers that have recently gained\nincreasing attention from the community include Shampoo Gupta et al. [2018] and Sophia [Liu et al.,\n2024].\nAdam and Signum: Many works have explored the relationship between Adam and variants\nof Signum [Balles and Hennig, 2018, Balles et al., 2020, Kunstner et al., 2023] and empirically\ndemonstrated that Signum (or its close variants) generally performs comparably to Adam. [Balles\net al., 2020] also argued that signSGD generally performs better when the Hessian is close to diagonal,\nhowever, it is unclear if this holds for practical settings. Kunstner et al. [2023] recently demonstrated\nthat Adam and a close variant of Signum exhibit similar performance on a variety of datasets including\nWikiText-2 [Merity et al., 2017] and SQUAD [Rajpurkar et al., 2016]. However, in contrast with our\nwork, all of these are restricted to the setting of vision or masked language modeling, and generally\ndo not sweep over multiple hyperparameters.\nLayerwise or blockwise Adam: We study Adalayer, a layerwise version of Adam. This is a special\ncase of the BAGM optimizer Zheng and Kwok [2019], specifically BAGM B.1. Similar algorithms\nhave also been studied by [Ginsburg et al., 2019, Agarwal et al., 2020, Liu et al., 2021, Zhang\net al., 2024b]. In particular, concurrent to our work, Zhang et al. [2024b] propose an algorithm\ntermed Adam-mini, which closely tracks a modified version of Adalayer (called Adalayer*), and\ndemonstrate comparable performance to AdamW. Note that, in our work, Adalayer* is introduced\nto understand the role played by preconditioning in Adam, and we do not specifically focus on the\nfinal performance. [Zhang et al., 2024a] empirically study the Hessian spectrum of transformers\nat initialization and find it to be more heterogeneous across layers as compared to ResNets. They\nargue that this heterogeneity is evidence towards the importance of Adam in training transformers. In\ncontrast our results (Section 4.2) show that all layers except last layer and LayerNorm layers can be\ntrained with SGD.\nOther related works: For vision transformers, in the fine-tuning phase, Kumar et al. [2024] show that\nusing SGD with frozen embedding parameters leads to competitive performance with Adam. Jelassi\net al. [2022] explore the similarity between Adam and normalized gradient descent [Nesterov, 2004]\nand show that normalized gradient descent on GANs does not suffer from mode collapse, while SGD\ndoes. Jiang et al. [2023] empirically demonstrate that Adam steers the parameter trajectory towards\nbetter-conditioned regions than SGD. Pan and Li [2022] also show that the parameter trajectory of\nAdam exhibits much higher directional smoothness than that of SGD. Ahn et al. [2024] show that the\nperformance gap between Adam and SGD exacerbates with depth of the network."}, {"title": "3 Comparing Optimizers Across Hyperparameters, Architectures and Scale", "content": "3.1 Methodology\nTo conduct our experiments, we start with hyperparameters recommended by previous work (e.g.,\n\u03b2\u2081 = 0.9). We initially perform a learning rate sweep to identify the optimal learning rate. After\ndetermining the optimal learning rate for each algorithm, we conduct one-dimensional sweeps for\neach of the other hyperparameters.\nA limitation of this methodology is the potential neglect of \"2D\" interactions between hyperparameters.\nThis is an important direction for future work, but beyond the computational budget of this project.\nFor example, some parameters like batch size and learning rate indeed are likely to exhibits 2D\ninteractions Shallue et al. [2019], Porian et al. [2024]. However, we argue that the 1D sweeps provide\na tractable methodology that gives us useful signal about the hyperparameter stability of a variety of\nalgorithms around the parameters that are common in practice.\n3.2 Setup\nWe train language models on C4 tokenized with the T5 tokenizer [Raffel et al., 2020] and report\nresults in terms of validation loss. As we discussed in the introduction, we argue that it is best to\nevaluate algorithms both in terms of the loss achieved by the best hyperparameters (performance) as\nwell as the robustness across values of the hyperparameters (stability).\nIn this section we first present the results of sweeps across the two most sensitive hyperparameters:\nlearning rate and momentum (31). We sweep across five algorithms: Adam, Adafactor, Lion, Signum,\nand SGD. Further ablations of weight decay, warmup, B2, and \u2208 for the 150m standard model can be\nfound in Section 3.5. Before diving into the results, we provide some more details about the setup.\nAlgorithms. We use the standard Pytorch implementation of AdamW [Paszke et al., 2019], the timm\nimplementation of SGDW [Wightman, 2019], and the OLMo implementation of Lion [Groeneveld\net al., 2024]. Following Zhai et al. [2022] we implement ourselves a modified version of Adafactor\nwhich maintains the factored estimates of second moments but has momentum i.e. it is equivalent to\nAdam with factored second moment estimates. Since Signum is equivalent to Lion with B\u2081 = B2 we\nreuse the OLMo implementation of Lion [Groeneveld et al., 2024] for it. We conducted experiments\nwith the Sophia optimizer Liu et al. [2024] in Appendix C. However, since it does not outperform\nSignum (which can be achieved by setting p = 0 in Sophia), we did not include it in other plots.\nModels. We start from the OLMo codebase [Groeneveld et al., 2024] and train decoder-only\ntransformer models of three sizes: 150m, 300m, and 600m, where the parameter count refers to\nnon-embedding parameters. The models have widths of 1024, 1024, and 1408 and depths of 12, 24,\n24. The MLP hidden dimension is 4x of the width. The activation function is GeLU [Hendrycks\nand Gimpel, 2016]. We use RoPE positional encodings [Su et al., 2024]. Attention heads are always\ndimension 64. We use PyTorch default LayerNorm. Following Wortsman et al. [2024] we do not\nlearn biases for the linear layers or LayerNorms. We train in mixed precision with bfloat16.\nTraining variants. We note that Wortsman et al. [2024] observe that QK LayerNorm [Dehghani\net al., 2023] and z-loss [Chowdhery et al., 2023] can have substantial effects on the stability of model\ntraining. As such, we consider two variants in our experiments: standard which refers to a model\nwith QK LayerNorms and z-loss with coefficient 1e-4, and no QK norm or z-loss which refers to\nthe same model without the QK norm layers or the z-loss.\nToken counts. For all models, we use a batch size of 256 and sequence length of 512 (as in\nWortsman et al. [2024]). We default to training models for the approximately \u201cchinchilla optimal\"\nnumber of tokens that is \u224820 times the number of parameters. Explicitly, this means for the 150m\nmodels we train for 25k steps or \u22483.3b tokens. The 300m models are trained for 50k steps, the 600m\nmodels are trained for 100k steps and the 150m-long models are also trained for 100k steps.\nOther hyperparameters. We default to using 0 weight decay. We default to using a learning rate\nschedule with 10% of the training steps for warmup and then cosine decay with a minimum that is"}, {"title": "3.3 Sweeping learning rates", "content": "First, we sweep over the most important hyperparameter: learning rate. Note, in all of these sweeps\nover learning rate we set \u03b2\u2081 = 0.9 for all algorithms except for SGD, where we set \u03b2\u2081 = 0.98. As\nwe will see in the following subsection, SGD is more sensitive to the momentum hyperparameters\nand requires more momentum to be competitive with the other optimizers.\nMain results for our standard architecture across three scales are presented in Figure 1. Note that\nthe x-axis shifts the learning rates to align the optimal learning rates across algorithms. In terms of\nabsolute learning rates, we sweep in multiples of $\\sqrt{10}$ from 1e-4 to 1 for Adam and Adafactor, from\nle-5 to le-1 for Lion and Signum, and from le-3 to 10 for SGD.\nThe key takeaway is that not only do the algorithms achieve similar performance at the optimal\nlearning rate, but the learning rate stability itself is similar across algorithms and scales. The one\nexception is SGD, which is worse both in terms of optimal performance and in terms of stability.\nFurther ablations are presented in Figure 2 and Figure 3 illustrating performance for models with\nno QK norm or z-loss and 4x longer training time respectively. While we find that the architecture\nchoices can clearly impact the amount of stability to learning rate, the cross-algorithm comparisons\nremain the same: Adafactor and Lion are competitive with Adam, while SGD is worse both in terms\nof performance and stability to learning rate. Similarly, training for longer can improve performance\nand stability to learning rate, but does not change the high-level cross-algorithm comparisons."}, {"title": "3.4 Sweeping momentum", "content": "Now we also sweep across momentum values (i.e. \u03b2\u2081). To do this sweep we fix the per-algorithm\nlearning rate to be the optimal learning rate from the corresponding learning rate sweep.\nResults are presented in Figure 4. We observe that across various settings, the robustness to B\u2081 is\nsimilar across the non-SGD algorithms when we stay in the range of momentums between 0.8 and\n0.98. However, for high \u03b2\u2081 Lion is better and low \u03b21 Adam and Adafactor are better. Again we\nobserve SGD being very sensitive to momentum."}, {"title": "3.5 Additional hyperparameter sweeps", "content": "We also sweep over a variety of other hyperparameters in Figure 5 using the best per-algorithm\nlearning rate and momentum. We observe that SGD is less stable with respect to weight decay and\nwarmup length. And while it is possible to get small benefits from higher weight decay, longer"}, {"title": "3.6 Signum recovers the performance and stability of Adam", "content": "In Figure 1 we observe that Adam and Signum have similar performance and stability for language\nmodeling, even at scale. The following lemma from prior work[Balles and Hennig, 2018] shows that\nAdam performs variance-adjusted sign gradient descent.\nLemma 1 (Balles and Hennig [2018]). Consider a parameter with a history of gradients $g_t, g_{t-1},...$.\nLet $m$ be the random variable that is equal to $g_{t-\\tau}$ with probability $(1 - \\beta_1) \\beta_1^{\\tau}$ and $v$ be the random\nvariable that is equal to $g_{t-\\tau}$ with probability $(1 - \\beta_2) \\beta_2^{\\tau}$. The Adam update $\\Delta_{Adam}$ and the Signum\nupdate $\\Delta_{Signum}$ are related by\n$\\Delta_{Adam} = \\Delta_{Signum} \\frac{\\mathbb{E}[m]}{\\sqrt{\\mathbb{E}[v^2]}}$\nIf B\u2081 = B2 then m = v in Lemma 1 and hence the ratio of Adam and Signum updates is equal to\nthe ratio of the mean and the square root of second moment of m. Intuitively, this holds because\nwhen B\u2081 = B2, first moment estimates of Signum and Adam, and second moment estimates of Adam,\naverage the previous gradients with same coefficients $((1 - \u03b2)\u03b2^\u03c4)$. This intuitively suggests that\nwhen \u03b2\u2081 = B2, Adam and Signum may behave similarly. This motivates the conjecture that the main\nbenefit of Adam over Signum is the fact that in Adam, B2 can be varied independently of B\u2081. In\nFigure 1 we have B2 = 0.95 and \u03b2\u2081 = 0.9 which are close, and as pointed out earlier, both optimizers\nhave similar performance and stability.\nWe examine this hypothesis further in Figure 6 by varying \u03b2\u2081 and setting B2 = B1, and again find that\nSignum and Adam behave very similarly. However, we also note that when we vary B\u2081 for Adam\nwhile fixing B2 we get more stability for \u03b2\u2081 as compared to Signum."}, {"title": "4 Investigating the key factors for optimizer stability and performance", "content": "Ablations in the previous section revealed the striking similarity in performance and stability across\nmultiple optimizers compared to Adam. In this section we seek a finer-grained understanding of the\nrole of preconditioning for adaptive optimizers in achieving both performance and stability. Adam and\nits other variants are designed to have a high degree of adaptivity at a fine granularity (per-parameter\nlearning rates) throughout the training process. This adaptivity is often credited with the stability and\nrobust performance observed in these optimizers. However, a critical question arises: to what extent"}, {"title": "4.1 Adalayer", "content": "To study the behavior of adaptive opti-\nmizers like Adam, we begin with describ-\ning a layer-wise version of Adam which\nwe refer to as Adalayer. Adam, Adafac-\ntor and Adalayer all (approximately) store\nthe diagonal second moment matrix, but\nwith coarser and coarser granularity; for a\nlayer of dimension m \u00d7 n, Adam explicitly\nmaintains the second moment matrix using\nmn parameters in the shape of a matrix.\nAdafactor stores row and column averages\nof the second moment matrix which serve\nas a rank-1 approximation to the second\nmoment matrix. Finally, Adalayer stores a single scalar which is the average of the second moment\nmatrix. We will later consider a generalization of Adalayer where instead of averaging second\nmoment over a layer we will average it over a \"block\" of parameters which can be a subset of a layer.\nWe note that similar algorithms have been studied before by Ginsburg et al. [2019], Agarwal et al.\n[2020] but we choose to study this variant since it is a direct analogue of Adam and Adafactor. A\nsimplified version of Adalayer optimizer is given in Algorithm 1, other details such as bias correction\nare kept same as that for Adam.\nIn Figure 7 we study the behavior of Adalayer across\nlearning rates. To preserve the correspondence with Adam\nwe fix other hyperparameters to be the same: \u03b2\u2081 = 0.9,\nB2 = 0.95 and \u0454 = 1e \u2013 15. We find that Adalayer has\nbetter performance than SGD4, but it performs worse than\nAdam and also lacks Adam's stability across learning rates.\nThe major difference between Adam and Adalayer is the\npreconditioning done by Adam within a layer. Intuitively,\nthis preconditioning will have large effects in layers where\nwe expect different weights within a layer to have different\ngradient scales. The first candidate for such a layer is the\nlast layer, since different tokens have widely different\nfrequencies leading to different gradient scales. To test\nthis hypothesis, we run a corrected version of Adalayer\nwhere we treat the set of weights feeding into a logit as a separate block. We henceforth refer to\nAdalayer with this correction as Adalayer*. This is plotted in Figure 7 and we observe that Adalayer*\nalmost recovers the performance as well as a large fraction of the stability of Adam."}, {"title": "4.2 Most language model parameters can be trained with SGD", "content": "The results using Adalayer* in the previous section suggest that all layers except the last layer only\nneed a iteration-dependent scalar correction to their learning rate. We now ask a stronger question:\ndo we need these scales at all? Or can we train the remaining layers with SGD? This hypothesis\nis supported by looking at Figure 8 (middle) where we observe that the learning rates for different\nmatrix layers (except the last layer) assigned by Adalayer* are remarkably similar."}, {"title": "5 Discussion and Limitations", "content": "After a comprehensive comparison of a variety of optimizers for language modeling, we have\nfound that many optimizers seem to be roughly equivalent both in terms of optimal performance\nand hyperparameter stability. Diving deeper, we have shown that most of the benefits of adaptive\noptimizers arise from their treatment of the last layer and LayerNorm parameters. Of course, there\nare several limitations to our study including the fact that due to computational constraints we only\nablate a few architecture decisions, that we only consider one dimensional hyperparameter sweeps,"}, {"title": "A Additional experiments: SGD + adaptive variants (Adalayer*, Adafactor)", "content": "In this section, we report additional experiments involving training language models with SGD on a\nfraction of the models' parameters and an adaptive optimizer on the remaining parameters. Firstly,\ngiven that the effective learning rates of the LayerNorm blocks were observed to be small in Figure 8,\nit is reasonable to ask whether training the LayerNorm parameters is necessary at all; in Figure 11,\nwe show results for training 150m and 300m models using Adalayer* only on the last layer, using\nSGD on all other matrix blocks, and turning off training for the LayerNorm parameters. This indeed\nyields greater stability in comparison to Figure 9 (Left) but does not fully recover the performance\nof Adalayer*, indicating that training LayerNorm parameters helps with performance, which seems\nmore pronounced in the larger model."}, {"title": "B Additional experiments: freezing Adalayer learning rate ratios", "content": "In this section, we report additional experiments exploring whether the results involving frozen\nAdalayer* in Section 4.2 need both last layer and LayerNorm adaptivity. We show that this is indeed\nthe case by conducting the same sweep for frozen Adalayer* while trying to also freeze the learning\nrate ratios for last layer or LayerNorm parameters as well. In Figure 13, we show that fixing initialized\nlearning rate ratios for all layers does not reach peak performance of Adalayer*, nor does it exhibit\nstability. In Figure 14, we show that either continuing to update the LayerNorm parameters or the last\nlayer parameters can achieve the peak performance of Adalayer* but is still unstable. Finally, we\nshow results for turning off LayerNorm training while fixing learning rate ratios (with the exception\nof the last layer) in Figure 15. We conclude that it is necessary to maintain adaptivity for both the last\nlayer and LayerNorm parameters, but understanding why the fixed ratios do not suffice would be an\ninteresting question for future work."}, {"title": "C Sophia", "content": "In this section, we compare Sophia [Liu et al., 2024] to Signum. Note that Signum is a special case of\nSophia, achieved by setting p = 0. We find that Sophia does not outperform Signum. No significant\nchange in performance was observed when transferring the hyperparameters suggested by Liu et al.\n[2024] (eg. \u03b21, B2, \u03b5, weight decay), nor when additionally scaling attention by the inverse of layer\nindex which was used in the original Sophia implementation."}]}