{"title": "Deconstructing What Makes a Good Optimizer for Language Models", "authors": ["Rosie Zhao", "Depen Morwani", "David Brandfonbrener", "Nikhil Vyas", "Sham Kakade"], "abstract": "Training language models becomes increasingly expensive with scale, prompting numerous attempts to improve optimization efficiency. Despite these efforts, the Adam optimizer remains the most widely used, due to a prevailing view that it is the most effective approach. We aim to compare several optimization algorithms, including SGD, Adafactor, Adam, and Lion, in the context of autoregressive language modeling across a range of model sizes, hyperparameters, and architecture variants. Our findings indicate that, except for SGD, these algorithms all perform comparably both in their optimal performance and also in terms of how they fare across a wide range of hyperparameter choices. Our results suggest to practitioners that the choice of optimizer can be guided by practical considerations like memory constraints and ease of implementation, as no single algorithm emerged as a clear winner in terms of performance or stability to hyperparameter misspecification. Given our findings, we further dissect these approaches, examining two simplified versions of Adam: a) signed momentum (Signum) which we see recovers both the performance and hyperparameter stability of Adam and b) Adalayer, a layerwise variant of Adam which we introduce to study Adam's preconditioning. Examining Adalayer leads us to the conclusion that the largest impact of Adam's preconditioning is restricted to the last layer and LayerNorm parameters, and, perhaps surprisingly, the remaining layers can be trained with SGD.", "sections": [{"title": "1 Introduction", "content": "As language model architectures increase in scale, pretraining becomes more expensive. In response, numerous efforts have been made to design efficient optimizers to mitigate these costs, and yet Adam [Kingma and Ba, 2015] remains the primary optimizer used for training language models. This persistent preference for Adam is rooted in an underlying belief that Adam generally outperforms alternative optimization algorithms. Although newly proposed optimizers run ablations to demonstrate superior performance to Adam for select architectures and tasks [Liu et al., 2024, Chen et al., 2023], there is no consensus among the literature about the relative performance of these optimizers. In fact, to the best of our knowledge, [Kaddour et al., 2024] is the only work comparing these optimizers but in the context of masked language modeling and at a single model scale.\nIn this work, we perform a comprehensive sweep for training autoregressive language models across different optimizers, hyperparameters, architectures, and scale. Along with looking at optimal performance, we argue that due to the difficulty of hyperparameter tuning with increasing scale [Yang et al., 2021], the stability of performance with respect to hyperparameter choices is equally\n*Equal contribution, randomized author ordering. Correspondence to rosiezhao@g.harvard.edu.\nPreprint. Under review."}, {"title": "Main contributions", "content": "\u2022 We empirically study the stability to hyperparameters of various optimization algorithms including SGD, Adam, Lion and Adafactor, showing that with the exception of SGD, these optimizers are comparable in terms of both performance and hyperparameter stability. This"}, {"title": "2 Related work", "content": "One closely related work to ours is Wortsman et al. [2024], which explores the stability of Adam with respect to learning rate. We extend the comparison to other optimizers including SGD, Lion and Adafactor, as well as other hyperparameters including momentum and weight decay.\nOptimizers: SGD [Robbins and Monro, 1951] had been the workhorse optimizer for deep learning until 2015, when Adam [Kingma and Ba, 2015] was introduced. Adam is a diagonal preconditioning algorithm that maintains a per-parameter learning rate. Over time, coarser variants of Adam have been proposed, which do not explicitly maintain a learning rate per parameter. Adafactor [Shazeer and Stern, 2018, Zhai et al., 2022] maintains a rank-1 approximation of the preconditioner matrix of Adam. Prior works have also explored the similarity of Adam with variants of Signum [Balles and Hennig, 2018], and recently, a close variant of Signum, called Lion [Chen et al., 2023], was discovered using symbolic search over algorithms. Some other optimizers that have recently gained increasing attention from the community include Shampoo Gupta et al. [2018] and Sophia [Liu et al., 2024].\nAdam and Signum: Many works have explored the relationship between Adam and variants of Signum [Balles and Hennig, 2018, Balles et al., 2020, Kunstner et al., 2023] and empirically demonstrated that Signum (or its close variants) generally performs comparably to Adam. [Balles et al., 2020] also argued that signSGD generally performs better when the Hessian is close to diagonal, however, it is unclear if this holds for practical settings. Kunstner et al. [2023] recently demonstrated that Adam and a close variant of Signum exhibit similar performance on a variety of datasets including WikiText-2 [Merity et al., 2017] and SQUAD [Rajpurkar et al., 2016]. However, in contrast with our work, all of these are restricted to the setting of vision or masked language modeling, and generally do not sweep over multiple hyperparameters.\nLayerwise or blockwise Adam: We study Adalayer, a layerwise version of Adam. This is a special case of the BAGM optimizer Zheng and Kwok [2019], specifically BAGM B.1. Similar algorithms have also been studied by [Ginsburg et al., 2019, Agarwal et al., 2020, Liu et al., 2021, Zhang et al., 2024b]. In particular, concurrent to our work, Zhang et al. [2024b] propose an algorithm termed Adam-mini, which closely tracks a modified version of Adalayer (called Adalayer*), and demonstrate comparable performance to AdamW. Note that, in our work, Adalayer* is introduced to understand the role played by preconditioning in Adam, and we do not specifically focus on the final performance. [Zhang et al., 2024a] empirically study the Hessian spectrum of transformers at initialization and find it to be more heterogeneous across layers as compared to ResNets. They argue that this heterogeneity is evidence towards the importance of Adam in training transformers. In contrast our results (Section 4.2) show that all layers except last layer and LayerNorm layers can be trained with SGD.\nOther related works: For vision transformers, in the fine-tuning phase, Kumar et al. [2024] show that using SGD with frozen embedding parameters leads to competitive performance with Adam. Jelassi et al. [2022] explore the similarity between Adam and normalized gradient descent [Nesterov, 2004] and show that normalized gradient descent on GANs does not suffer from mode collapse, while SGD does. Jiang et al. [2023] empirically demonstrate that Adam steers the parameter trajectory towards better-conditioned regions than SGD. Pan and Li [2022] also show that the parameter trajectory of Adam exhibits much higher directional smoothness than that of SGD. Ahn et al. [2024] show that the performance gap between Adam and SGD exacerbates with depth of the network."}, {"title": "3 Comparing Optimizers Across Hyperparameters, Architectures and Scale", "content": "3.1 Methodology\nTo conduct our experiments, we start with hyperparameters recommended by previous work (e.g., \u03b2\u2081 = 0.9). We initially perform a learning rate sweep to identify the optimal learning rate. After determining the optimal learning rate for each algorithm, we conduct one-dimensional sweeps for each of the other hyperparameters.\nA limitation of this methodology is the potential neglect of \"2D\" interactions between hyperparameters. This is an important direction for future work, but beyond the computational budget of this project. For example, some parameters like batch size and learning rate indeed are likely to exhibits 2D interactions Shallue et al. [2019], Porian et al. [2024]. However, we argue that the 1D sweeps provide a tractable methodology that gives us useful signal about the hyperparameter stability of a variety of algorithms around the parameters that are common in practice.\n3.2 Setup\nWe train language models on C4 tokenized with the T5 tokenizer [Raffel et al., 2020] and report results in terms of validation loss. As we discussed in the introduction, we argue that it is best to evaluate algorithms both in terms of the loss achieved by the best hyperparameters (performance) as well as the robustness across values of the hyperparameters (stability).\nIn this section we first present the results of sweeps across the two most sensitive hyperparameters: learning rate and momentum (31). We sweep across five algorithms: Adam, Adafactor, Lion, Signum, and SGD. Further ablations of weight decay, warmup, B2, and \u20ac for the 150m standard model can be found in Section 3.5. Before diving into the results, we provide some more details about the setup.\nAlgorithms. We use the standard Pytorch implementation of AdamW [Paszke et al., 2019], the timm implementation of SGDW [Wightman, 2019], and the OLMo implementation of Lion [Groeneveld et al., 2024]. Following Zhai et al. [2022] we implement ourselves a modified version of Adafactor which maintains the factored estimates of second moments but has momentum i.e. it is equivalent to Adam with factored second moment estimates. Since Signum is equivalent to Lion with B\u2081 = B2 we reuse the OLMo implementation of Lion [Groeneveld et al., 2024] for it. We conducted experiments with the Sophia optimizer Liu et al. [2024] in Appendix C. However, since it does not outperform Signum (which can be achieved by setting p = 0 in Sophia), we did not include it in other plots.\nModels. We start from the OLMo codebase [Groeneveld et al., 2024] and train decoder-only transformer models of three sizes: 150m, 300m, and 600m, where the parameter count refers to non-embedding parameters. The models have widths of 1024, 1024, and 1408 and depths of 12, 24, 24. The MLP hidden dimension is 4x of the width. The activation function is GeLU [Hendrycks and Gimpel, 2016]. We use RoPE positional encodings [Su et al., 2024]. Attention heads are always dimension 64. We use PyTorch default LayerNorm. Following Wortsman et al. [2024] we do not learn biases for the linear layers or LayerNorms. We train in mixed precision with bfloat16.\nTraining variants. We note that Wortsman et al. [2024] observe that QK LayerNorm [Dehghani et al., 2023] and z-loss [Chowdhery et al., 2023] can have substantial effects on the stability of model training. As such, we consider two variants in our experiments: standard which refers to a model with QK LayerNorms and z-loss with coefficient 1e-4, and no QK norm or z-loss which refers to the same model without the QK norm layers or the z-loss.\nToken counts. For all models, we use a batch size of 256 and sequence length of 512 (as in Wortsman et al. [2024]). We default to training models for the approximately \u201cchinchilla optimal\" number of tokens that is \u224820 times the number of parameters. Explicitly, this means for the 150m models we train for 25k steps or \u22483.3b tokens. The 300m models are trained for 50k steps, the 600m models are trained for 100k steps and the 150m-long models are also trained for 100k steps.\nOther hyperparameters. We default to using 0 weight decay. We default to using a learning rate schedule with 10% of the training steps for warmup and then cosine decay with a minimum that is\""}, {"title": "3.3 Sweeping learning rates", "content": "First, we sweep over the most important hyperparameter: learning rate. Note, in all of these sweeps over learning rate we set \u03b2\u2081 = 0.9 for all algorithms except for SGD, where we set \u03b2\u2081 = 0.98. As we will see in the following subsection, SGD is more sensitive to the momentum hyperparameters and requires more momentum to be competitive with the other optimizers.\nMain results for our standard architecture across three scales are presented in Figure 1. Note that the x-axis shifts the learning rates to align the optimal learning rates across algorithms. In terms of absolute learning rates, we sweep in multiples of \u221a 10 from 1e-4 to 1 for Adam and Adafactor, from le-5 to le-1 for Lion and Signum, and from le-3 to 10 for SGD.\nThe key takeaway is that not only do the algorithms achieve similar performance at the optimal learning rate, but the learning rate stability itself is similar across algorithms and scales. The one exception is SGD, which is worse both in terms of optimal performance and in terms of stability.\nFurther ablations are presented in Figure 2 and Figure 3 illustrating performance for models with no QK norm or z-loss and 4x longer training time respectively. While we find that the architecture choices can clearly impact the amount of stability to learning rate, the cross-algorithm comparisons remain the same: Adafactor and Lion are competitive with Adam, while SGD is worse both in terms of performance and stability to learning rate. Similarly, training for longer can improve performance and stability to learning rate, but does not change the high-level cross-algorithm comparisons.\nTakeaway: performance and stability to learning rate are comparable across the non-SGD algorithms that we tested."}, {"title": "3.4 Sweeping momentum", "content": "Now we also sweep across momentum values (i.e. \u03b21)3. To do this sweep we fix the per-algorithm learning rate to be the optimal learning rate from the corresponding learning rate sweep.\nResults are presented in Figure 4. We observe that across various settings, the robustness to B\u2081 is similar across the non-SGD algorithms when we stay in the range of momentums between 0.8 and 0.98. However, for high B\u2081 Lion is better and low \u1e9e1 Adam and Adafactor are better. Again we observe SGD being very sensitive to momentum.\nTakeaway: performance and stability to momentum are comparable across the non-SGD algorithms that we tested if we stay within the usual range of momentum values."}, {"title": "3.5 Additional hyperparameter sweeps", "content": "We also sweep over a variety of other hyperparameters in Figure 5 using the best per-algorithm learning rate and momentum. We observe that SGD is less stable with respect to weight decay and warmup length. And while it is possible to get small benefits from higher weight decay, longer\n3Note that in Lion, both B\u2081 and B2 can be thought of as different types of \"momentum\" with B\u2081 being the \"one-step\" momentum and B2 the \u201clong-term\" momentum. For consistency, we only sweep \u1e9e1 here and provide further ablations in the Appendix."}, {"title": "Takeaway:", "content": "generally algorithms are more stable with respect to other hyperparameters and the possible gains in performance are relatively small compared to learning rate and momentum."}, {"title": "3.6 Signum recovers the performance and stability of Adam", "content": "In Figure 1 we observe that Adam and Signum have similar performance and stability for language modeling, even at scale. The following lemma from prior work[Balles and Hennig, 2018] shows that Adam performs variance-adjusted sign gradient descent.\nLemma 1 (Balles and Hennig [2018]). Consider a parameter with a history of gradients $g_t, g_{t-1},....$ Let $m$ be the random variable that is equal to $g_{t-\\frac{1}{2}}$ with probability $(1 \u2013 \u03b2_1)^{t}\u03b2_1$ and $v$ be the random variable that is equal to $g_{t-\\frac{1}{2}}$ with probability $(1 \u2013 B_2)^{t}B_2$. The Adam update $v_{Adam}$ and the Signum update $v_{Signum}$ are related by\n$v_{Adam} = v_{Signum} \\frac{E[m]}{\\sqrt{E[v^2]}}$\nIf B\u2081 = B2 then m = v in Lemma 1 and hence the ratio of Adam and Signum updates is equal to the ratio of the mean and the square root of second moment of m. Intuitively, this holds because when B\u2081 = B2, first moment estimates of Signum and Adam, and second moment estimates of Adam, average the previous gradients with same coefficients $((1 \u2013 \u03b2)^{t}\u03b2)$. This intuitively suggests that when \u03b2\u2081 = \u03b22, Adam and Signum may behave similarly. This motivates the conjecture that the main benefit of Adam over Signum is the fact that in Adam, B2 can be varied independently of B\u2081. In Figure 1 we have B2 = 0.95 and \u03b2\u2081 = 0.9 which are close, and as pointed out earlier, both optimizers have similar performance and stability.\nWe examine this hypothesis further in Figure 6 by varying \u03b2\u2081 and setting B2 = B1, and again find that Signum and Adam behave very similarly. However, we also note that when we vary B\u2081 for Adam while fixing B2 we get more stability for \u1e9e\u2081 as compared to Signum.\nTakeaway: With B2 = \u03b2\u2081 Adam and Signum behave similarly and the standard setting for training language models (\u03b22 = 0.95, \u03b2\u2081 = 0.9) is close to this."}, {"title": "4 Investigating the key factors for optimizer stability and performance", "content": "Ablations in the previous section revealed the striking similarity in performance and stability across multiple optimizers compared to Adam. In this section we seek a finer-grained understanding of the role of preconditioning for adaptive optimizers in achieving both performance and stability. Adam and its other variants are designed to have a high degree of adaptivity at a fine granularity (per-parameter learning rates) throughout the training process. This adaptivity is often credited with the stability and robust performance observed in these optimizers. However, a critical question arises: to what extent"}, {"title": "4.1 Adalayer", "content": "To study the behavior of adaptive opti-\nmizers like Adam, we begin with describ-\ning a layer-wise version of Adam which\nwe refer to as Adalayer. Adam, Adafac-\ntor and Adalayer all (approximately) store\nthe diagonal second moment matrix, but\nwith coarser and coarser granularity; for a\nlayer of dimension m \u00d7 n, Adam explicitly\nmaintains the second moment matrix using\nmn parameters in the shape of a matrix.\nAdafactor stores row and column averages\nof the second moment matrix which serve\nas a rank-1 approximation to the second\nmoment matrix. Finally, Adalayer stores a single scalar which is the average of the second moment matrix. We will later consider a generalization of Adalayer where instead of averaging second moment over a layer we will average it over a \"block\" of parameters which can be a subset of a layer. We note that similar algorithms have been studied before by Ginsburg et al. [2019], Agarwal et al. [2020] but we choose to study this variant since it is a direct analogue of Adam and Adafactor. A simplified version of Adalayer optimizer is given in Algorithm 1, other details such as bias correction are kept same as that for Adam.\nIn Figure 7 we study the behavior of Adalayer across learning rates. To preserve the correspondence with Adam we fix other hyperparameters to be the same: \u03b2\u2081 = 0.9, B2 = 0.95 and \u0454 = 1e \u2013 15. We find that Adalayer has better performance than SGD4, but it performs worse than Adam and also lacks Adam's stability across learning rates. The major difference between Adam and Adalayer is the preconditioning done by Adam within a layer. Intuitively, this preconditioning will have large effects in layers where we expect different weights within a layer to have different gradient scales. The first candidate for such a layer is the last layer, since different tokens have widely different frequencies leading to different gradient scales. To test this hypothesis, we run a corrected version of Adalayer where we treat the set of weights feeding into a logit as a separate block. We henceforth refer to Adalayer with this correction as Adalayer*. This is plotted in Figure 7 and we observe that Adalayer* almost recovers the performance as well as a large fraction of the stability of Adam.\nTo study how Adalayer* preconditions the network, we plot effective learning rates used for different logits by Adalayer* in Figure 8 (Left). Here, the effective learning rate for a layer l in the network is $\\frac{\\eta t}{\\sqrt{v^l + \\epsilon}}$. We find that Adalayer* indeed uses vastly different learning rates for different logits,"}, {"title": "4.2 Most language model parameters can be trained with SGD", "content": "The results using Adalayer* in the previous section suggest that all layers except the last layer only need a iteration-dependent scalar correction to their learning rate. We now ask a stronger question: do we need these scales at all? Or can we train the remaining layers with SGD? This hypothesis is supported by looking at Figure 8 (middle) where we observe that the learning rates for different matrix layers (except the last layer) assigned by Adalayer* are remarkably similar.\nTo test this, we train the last layer with Adalayer* (fixing a learning rate of 3.16e - 3) and the rest of the layers with SGD, both with \u03b2\u2081 = 0.9. In Figure 9 (Left) we show the results while sweeping over SGD learning rates from 0.1 to 3160. While this improves upon the performance of SGD, we do not recover stability of the Adalayer* and Adam baselines. We trace this instability to LayerNorm blocks: Figure 8 (Left) shows that the effective learning rates for the LayerNorm blocks are much smaller, which suggests that they may destabilize at higher SGD learning rates. To ameliorate this, in Figure 9 (Middle and Right) we add LayerNorm parameters to those being trained with Adalayer* and find that this is sufficient to recover both performance and stability of Adalayer*. For the larger 300m model, we find that this even exceeds the performance of Adalayer*.\nIn the Appendix, we conduct further experiments investigating these 'hybrid' variants of SGD. Another plausible remedy to address the small effective learning rates of the LayerNorm blocks is to"}, {"title": "5 Discussion and Limitations", "content": "After a comprehensive comparison of a variety of optimizers for language modeling, we have found that many optimizers seem to be roughly equivalent both in terms of optimal performance and hyperparameter stability. Diving deeper, we have shown that most of the benefits of adaptive optimizers arise from their treatment of the last layer and LayerNorm parameters. Of course, there are several limitations to our study including the fact that due to computational constraints we only ablate a few architecture decisions, that we only consider one dimensional hyperparameter sweeps,"}]}