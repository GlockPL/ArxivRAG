{"title": "Analyzing Inference Privacy Risks Through Gradients In Machine Learning", "authors": ["Zhuohang Li", "Andrew Lowy", "Jing Liu", "Toshiaki Koike-Akino", "Kieran Parsons", "Bradley Malin", "Ye Wang"], "abstract": "In distributed learning settings, models are iteratively updated with shared gradients computed from potentially sensitive user data. While previous work has studied various privacy risks of sharing gradients, our paper aims to provide a systematic approach to analyze private information leakage from gradients. We present a unified game-based framework that encompasses a broad range of attacks including attribute, property, distributional, and user disclosures. We investigate how different uncertainties of the adversary affect their inferential power via extensive experiments on five datasets across various data modalities. Our results demonstrate the inefficacy of solely relying on data aggregation to achieve privacy against inference attacks in distributed learning. We further evaluate five types of defenses, namely, gradient pruning, signed gradient descent, adversarial perturbations, variational information bottleneck, and differential privacy, under both static and adaptive adversary settings. We provide an information-theoretic view for analyzing the effectiveness of these defenses against inference from gradients. Finally, we introduce a method for auditing attribute inference privacy, improving the empirical estimation of worst-case privacy through crafting adversarial canary records.", "sections": [{"title": "1 INTRODUCTION", "content": "Ensuring privacy is an important prerequisite for adopting machine learning (ML) algorithms in critical domains that require training on sensitive user data, such as medical records, personal financial information, private images, and speech. Prominent ML models, ranging from compact neural networks tailored for mobile platforms [40] to large foundation models [10, 72], are often trained on user data via gradient-based iterative optimization. In many cases, such as decentralized learning [19, 41] or federated learning (FL) [33, 38, 66], model gradients are directly exchanged in place of raw training data to facilitate joint learning, which opens up an additional channel for potential privacy leakage [61].\nRecent works have explored information leakage through this gradient channel in various forms, albeit in isolation. For instance, Nasr et al. [69] showed that it is feasible to infer membership (i.e., single-bit information indicating the existence of a target record in the training data pool) from model updates in federated learning. Beyond membership, Melis et al. [68] demonstrated inference over sensitive properties of the training data in collaborative learning. Other independent lines of work additionally explored attribute inference [20, 62] and data reconstruction [31, 35, 103] through shared model gradients. However, some emerging privacy concerns that have so far only been considered under the centralized learning setting, such as the distributional inference [16, 85] and user-level inference [49, 54], have not been well investigated in the gradient leakage setting.\nExisting studies on information leakage from gradients have several limitations. First, the majority of the current literature focuses on investigating each individual type of inference attack under their specific threat models while lacking a comprehensive examination of inference attack performance under various adversarial assumptions, which is essential for providing a holistic view of the adversary's capabilities. For instance, from the attack's perspective, assuming the adversary to have access to a reasonably-sized shadow dataset and limited rounds of access to the model's gradients helps to capture the realistic inference privacy risk under a practical threat model. Conversely, from the defense's perspective, assuming a powerful adversary with access to record-level gradients and auxiliary information about the private record helps to estimate the worst-case privacy risk, which may facilitate the design of more robust defenses. Second, while several types of heuristic defenses have been explored by prior work, their supposed effectiveness has not been fully verified under more challenging adaptive adversary settings. Moreover, existing studies do not adequately explain why some defenses succeed in reducing the inference risk over gradients, while others fail, which could provide important guidance on the design of more effective defenses.\nIn this paper, we conduct a systematic analysis of private information leakage from gradients. We start by defining a unified inference game that broadly encompasses four types of inference attacks that aims at inferring common private information of the data from gradients, namely, attribute inference attack (AIA), property inference attack (PIA), distributional inference attack (DIA), and user inference attack (UIA), as illustrated in Figure 1. Under this framework, we show that information leakage from gradients can be treated as performing statistical inference over a sensitive variable upon observing samples of the gradients, with different definitions of the information encapsulated by the variable being inferred, leading to a generic template for constructing different types of inference attacks. We additionally explore different tiers of adversarial assumptions, with varying numbers of available data samples, numbers of observable rounds of gradients, and varying batch sizes, to investigate how different priors and uncertainties in the adversary's knowledge about the gradient and data distribution affect the adversary's inferential power.\nWe perform a systematic evaluation of these attacks on five datasets (Adult [8], Health [47], CREMA-D [12], CelebA [59], UTK-Face [102]) with three different data modalities (tabular, speech, and image). A common setting in distributed learning is that the data distribution is heterogeneous across different nodes but homogeneous within each node. Under this assumption, where the sensitive variable is common across a batch, we show that a larger batch size leads to higher inference privacy risk from gradients across all considered attacks, highlighting that solely relying on data aggregation is insufficient for achieving meaningful privacy in distributed learning. With a moderate batch size (e.g., 16), we show that an adversary can launch successful inference attacks with very few shadow data samples (\u2264 1,000). For instance, in the case of property inference on the Adult dataset, the adversary can achieve 0.92 AUROC with only 100 shadow data samples. Moreover, we demonstrate that an adversary with access to multiple rounds of gradient updates can perform Bayesian inference to aggregate adversarial knowledge, eventually leading to higher confidence and better attack performance.\nWe apply the developed inference attacks to evaluate the effectiveness of five common types of defenses from the privacy literature [45, 46, 77\u201379, 82, 84, 94, 103], including Gradient Pruning [103], Signed Stochastic Gradient Descent (SignSGD) [9], Adversarial Perturbations [64], Variational Information Bottleneck (VIB) [4], and Differential Privacy (DP-SGD) [1], against both static adversaries that are unaware of the defense and adaptive adversaries that can adapt to the defense mechanism. We find that most heuristic defense methods only offer a weak notion of \"security through obscurity\", in the sense that they defend against static adversaries empirically but can be easily bypassed by adaptive adversaries. Although DP-SGD shows consistent performance against both static and adaptive adversaries, to fully prevent inference attacks, it often requires injecting too much noise which diminishes the utility of the learning model. We provide an information-theoretic perspective for explaining and analyzing the (in)effectiveness of these considered defenses and show that the key ingredient of a successful defense is to effectively reduce the mutual information between the released gradients and the sensitive variable, which could serve as a guideline for designing future defenses. Finally, to provide practical guidance in selecting privacy parameters, we introduce an auditing approach for empirically estimating the privacy loss of attribute inference attacks through crafting adversarial canary records to approximate the privacy risk in the worst case.\nIn summary, our main contributions are as follows:\n\u2022 We provide a holistic analysis of inference privacy from gradients through a unified inference game that broadly encompasses a range of attacks concerning attribute, property, distributional, and user inference.\n\u2022 We demonstrate the weakness of solely relying on data aggregation to achieve privacy against inference attacks in distributed learning. We do this through a systematic evaluation of the four types of attacks on datasets with different modalities under various adversarial assumptions.\n\u2022 Our analyses reveal that reducing the mutual information between the released gradients and the sensitive variable is the key ingredient of a successful defense. This is shown by investigating five common types of defense strategies against inference over gradients from an information-theoretic perspective.\n\u2022 Our auditing results provide an empirical justification for tolerating large DP parameters when defending against attribute inference attacks (c.f. [60]). This is achieved by implementing an auditing method for empirically estimating the privacy loss against attribute inference attacks from gradients."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Developing ML models in many applications involves training on the users' private data, which introduces privacy leakage risks from different components of the ML model across several stages of the development and deployment pipeline.\nLeakage From Model Parameters (\u03b8). The first way of exposing privacy information is through analyzing the model parameters. This is connected to the most prominent centralized ML setting, where the model is first developed on a local dataset and then released to the users for deployment. Various forms of privacy leakage have been studied in this setting. White-box membership inference [53, 69, 73] aims at identifying the presence of individual records in the training dataset given access to the full model. Data\n2.1 Machine Learning Notation\nA machine learning (ML) model can be denoted as a function $f_{\\theta}: x \\rightarrow y$ parameterized by $\u03b8$ that maps from the input (feature) space to the output (label) space. The training of an ML model involves a set of training data and an optimization procedure, such as stochastic gradient descent (SGD). At each step of SGD, a loss function $L(\\theta, D_b)$ is first computed based on the current model and a batch of k training samples $D_b = \\{(x_i, y_i)\\}_{i=1}^k$ and then a set of gradients is computed as $g = \\nabla_{\\theta}L(\\theta, D_b)$. Finally, the model is updated by taking a gradient step towards minimizing the loss.\n2.2 Related Work"}, {"title": "3 PROBLEM FORMALIZATION", "content": "This section introduces four types of inference attacks from gradients, namely, attribute inference, property inference, distributional inference, and user inference. We formally define information leakage from gradients using a unified security game, following standard practices in machine learning privacy studies [75], and discuss variants of threat models that affect the adversary's inferential power. In Section 4, we describe methods to construct these attacks.\n3.1 Attack Definitions\nWe consider four types of information leakage from model gradients that generally involve two parties, namely, a private learner who releases model gradients computed on a private data batch, and an adversary who tries to make inferences about the private data given access to the gradients. This generic setting captures multiple ML application scenarios such as distributed training, federated learning, and online learning.\nAttribute Inference. Attribute inference attacks (AIA) seek to infer a data record's unknown attribute (feature) from its gradient. Prior works in both centralized [95, 97] and federated settings [20, 62] usually assume the record to be partially known. For instance, infer a missing entry (e.g., genotype) of a person's medical record [29]. It is worth noting that, in practice, when the attributes\nare not completely independent, an adversary with partial knowledge about the record may be able to infer the unknown attribute just from the known ones, as in data imputation [44].\nProperty Inference. Property inference attacks (PIA) aim to infer a global property of the private data batch that is not directly present in the data feature space but is correlated with some of the features (and consequently the gradients). For tabular data, these properties could be sensitive features that have been intentionally excluded from training (e.g., pseudo-identifiers in health records that are required to be removed for HIPAA compliance); for high-dimensional data like image and speech, they could be some high-level statistical features capturing the semantics of the data instance (e.g., race of a face image [68] or gender of a speech recording [26]).\nDistributional Inference. Distributional inference attacks (DIA) aim to infer the ratio of the training samples (\u03b1) that satisfy some target property\u00b9. The majority of current literature on DIA [16, 30, 65, 85] is in the space of centralized learning, which captures leakage from model parameters. These studies usually define DIA as a distinguishing test between two worlds where the model is trained on two datasets with different ratios ($\u03b1_p$ and $\u03b1_l$) [85]. This can be further categorized into property existence tests that decide if there exists any data point with the target property in the training set and property size estimation tests that infer the exact ratio of the property in the training data [16]. In this work, we extend DIA to the gradient space and consider a general case that combines property existence and property size estimation by formulating DIA as performing ordinal classification between a set of m ratio bins (m \u2265 3), i.e., {$0$}, $(0, \\frac{1}{m-1}]$, $(\\frac{1}{m-1}, \\frac{2}{m-1}],..., (\\frac{m-2}{m-1}, 1]$.\nUser Inference. User inference attacks (UIA) or re-identification attacks aim to identify which user's data was used to compute the observed gradients. Here, the adversary does not know the user's exact data used for computing the gradients. Instead, the adversary is provided a set of candidate users and their corresponding underlying user-level data distributions. This setting shares similarities with the subject-level membership inference [86] in the sense that both attacks measure the privacy risk at the granularity of each individual. However, the user inference attack aims to infer richer information that directly exposes the user's identity compared to the membership inference attack, which only discloses a single bit of information (i.e., whether a given user's data sample is involved in training). Thus user inference can be considered as a generalization of subject-level membership inference.\nWe note that except for attribute inference which directly exposes (part of) the user's private data, property inference, distributional inference, and user inference attacks are inferential disclosures (also known as deductive disclosures) that exploit the statistical correlation exists in data to infer sensitive information from the released gradients with high confidence. We exclude record-level privacy attacks such as membership inference and data reconstruction as our analysis here focuses on distributed learning scenarios where private information can be shared across different data samples within a batch.\n3.2 Unified Inference Game\nOur framework aims to capture an abstraction of privacy problems in distributed learning settings, where an attacker aims to recover some sensitive information of a particular client from their shared gradients (or model updates). In practical distributed learning settings, the data may be heterogeneously split across the clients, and an attacker may take advantage of side information about a particular client's local data distribution. Generally, the objective of the attacker is to recover the sensitive information, represented by the variable a, which is related to the local data distribution of the client through a joint distribution $P(x, y, a) = P(a) P(x, y|a)$. As we will detail later, specific choices in what a represents and the corresponding specialized structure of $P(x, y, a)$ enable the framework to capture attribute, property, distributional, and user inference privacy problems. This joint distribution may capture both the side information available to the attacker and the inherent heterogeneity of the data. To focus on evaluating the effectiveness of gradient-based attacks and defenses, we simplify the modeling of the overall training procedure, by updating the model in a centralized fashion on the entire training data set $D$, but generating gradients for the attacker on batches drawn according to $P(x, y, a)$.\nDefinition 3.1. Unified Inference Game. Let $P(x, y, a)$ be the joint distribution, L the loss function, $T$ the training algorithm, $r$ the total number of training rounds, and $R \\subset [r]$ a set of rounds that are observable to the adversary\u00b2. The unified inference game from gradients between a challenger (private learner) and an adversary is as follows:\n(1) Challenger initializes the model parameters as $\u03b8_0$.\n(2) Challenger samples a training dataset $D = \\{(x_j, y_j)\\}_{j=1}^N$ where $(x_j, y_j) \\stackrel{i.i.d}{\\sim} P(x, y)$.\n(3) Challenger draws the sensitive variable $a \\sim P(a)$.\n(4) Challenger draws a batch of k data samples $D_a = \\{(x_p, y_p)\\}_{p=1}^k$, where $(x_p, y_p) \\stackrel{i.i.d}{\\sim} P(x, y|a)$, for the given a.\n(5) Challenger computes the gradient of the loss on the data batch, $g_i = \\nabla_{\\theta_{i-1}} L(\\theta_{i-1}, D_a)$.\n(6) Challenger applies the defense mechanism M to produce a privatized version of the gradient $g_i = M(g_i)$. When no defense is applied, M is simply the identity function, i.e., $g_i = g_i$\n(7) The model is updated by applying the training algorithm on the training dataset for one epoch $\u03b8_i \\leftarrow T(\u03b8_{i-1}, D, L, M)$.\n(8) Steps (5)-(7) are repeated for r rounds.\n(9) A static adversary $A_s$ gets access to L, T, P(x, y, a), and the set of (intermediate) model parameters $\u0398 = \\{\u03b8_{i-1}|i \u2208 R\\}$ and released gradients $G = \\{g_i|i \u2208 R\\}$. An adaptive adversary $A_a$ also gets the defense mechanism M.\n(10) The adversary outputs its inference $\u00e2$ of the sensitive variable, i.e., $\u00e2 \\leftarrow A_s(L, T, P(x, y, a), \u0398, G)$ for the static adversary, or $\u00e2 \\leftarrow A_a(L, T, P(x, y, a), \u0398, G, M)$ for the adaptive adversary. The adversary wins if $a = a$ and loses otherwise.\n3.3 Threat Model\nIn this work, we assume the adversary has no control over the training protocol and only passively observes gradients as the model is being updated. In practice, the adversary could be an honest-but-curious parameter server [55] in a distributed learning or federated learning setting, a node in decentralized learning [19], or an attacker who eavesdrops on the communication channel. The game as defined in Definition 3.1 is similar to games defined in many prior works [13, 97] which captures the average-case privacy as the performance of the attack is measured by its expected value over the random draw of data samples. In Section 7, we consider an alternative game where the data samples are adversarially chosen to provide a measure of worst-case privacy for privacy auditing.\nWe consider the following aspects that reflect different levels of the adversary's knowledge:\n\u2022 Knowledge of Data Distribution. Similar to many prior works on inference attacks [13, 16, 58, 68, 80, 85, 96], we model the\nadversarial knowledge of the data distribution through access to data samples drawn from this distribution, which are referred to as shadow datasets. A larger shadow dataset implies a more powerful adversary that has more knowledge about the underlying data distribution. For discrete attributes, we additionally consider a more informed adversary who knows the prior distribution of the attribute, which can be estimated by drawing a large amount of data from the population.\n\u2022 Continuous Observation. We use the observable set $R$ to capture the adversary's ability to observe the gradients continuously. Intuitively, an adversary observing multiple rounds should perform better than a single-round adversary. Assuming a powerful adversary is beneficial for analyzing and auditing defenses. For instance, the privacy analysis in DP-SGD [1] assumes that the adversary has access to all rounds of gradients.\n\u2022 Adaptive Adversary. When evaluating defenses, in addition to the static adversary, we consider a stronger adaptive adversary who is aware of the underlying defense mechanism. This has been demonstrated as pivotal for thoroughly assessing the effectiveness of security defenses [15, 87]."}, {"title": "4 ATTACK CONSTRUCTION", "content": "The objective of the inference adversary is to infer the sensitive variable from the observed gradient, i.e., modeling the posterior distribution $P(a|g)$. The general strategy of implementing inference attacks from gradients is to exploit the following two adversarial assumptions as defined in the unified inference game in Section 3.2. First, the adversary possesses knowledge about the underlying population data distribution. Operationally, this implies that the adversary is able to draw data samples $(x, y)$ with corresponding sensitive variable a from $P(x, y, a)$ to construct a shadow dataset. Second, the adversary has access to the training algorithm and the current model parameters, which allows the adversary to compute the gradients $g$ for each batch of samples within the shadow dataset. With this information, the adversary can train a predictive model $P_w(a|g)$ to approximate the posterior.\nAttribute & Property Inference. The attribute and property inference attacks follow a similar attack procedure, with the difference being whether the sensitive variable $a$ is internal or external to the data record. Specifically, the adversary first constructs a shadow dataset $D_s$ by sampling from the population distribution, i.e., $D_s = \\{(x_j, y_j, a_j)\\}_{j=1}^{N_s}$ where $(x_j, y_j, a_j) \\stackrel{i.i.d}{\\sim} P(x, y, a)$. Then the adversary draws data batches $D_a = \\{(x_j, y_j)\\}_{j=1}^k$ from the shadow dataset through bootstrapping. This is achieved by repeatedly sampling the sensitive attribute a and then drawing k records that have the sensitive attribute from $D_s$. Next, for each data batch $D_a$, the adversary computes the gradient $g_a = \\nabla_{\\theta}L(\u03b8, D_a)$ using the current model parameters \u03b8. This results in a set of labeled data pairs $(g_a, a)$, which can then be used for training an ML model $P_w(a|g)$ that predicts the sensitive variable from gradient observations. In practice, we find that it is beneficial to train the predictive model using a balanced dataset, which can be seen as modeling $\\frac{P(a)}{P(a)}$', and capture the prior knowledge in a separate term. This\n4.1 Inference Attacks provides more stable performance for small shadow dataset sizes and skewed sensitive variable distributions.\nIt is worth noting that here we are considering a more restrictive setting for attribute inference where the adversary holds no additional knowledge about the private data besides the gradients compared to prior works that assume the private record to be partially known (e.g., [20, 62] assume that everything is known except for the sensitive attribute). Our framework can be easily extended to the general case where the adversary holds arbitrary additional knowledge $q(x)$ about the private record x by training a predictive model $P_w(a|g, q(x))$ using shadow data drawn from $P(x, y, a | q(x))$.\nDistributional Inference. In distributional inference, the sensitive variable is the index of the ratio bin to which the property ratio belongs. The adversary first samples a random bin index a and then samples a property ratio \u03b1 within that bin. Next, the adversary draws a data batch $D_a$ with $\\lfloor \u03b1k \\rfloor$ records with the property and the rest without the property and derives the gradient $g_a$. This process is repeated by the adversary to collect a set of labeled gradients and attribute pairs $(g_a, a)$ to train a predictive model. We note that in the setting of distributional inference, the sensitive variable is a series of ordinal numbers indicative of the continuous property ratio \u03b1 and thus should not be treated as regular multi-class classification. To utilize the ordering information, we adopt a simple strategy to ordinal classification [27], which transforms the m-class ordinal classification problem into m \u2212 1 binary classifications. Specifically, the adversary trains a series of m \u2212 1 binary classifiers, with the i-th classifier $P_{w_i}(a > i|g)$ trained to decide whether or not a is larger than i. The final posterior probability can be obtained as\n$P_w(a = a|g) = \\begin{cases} 1 - P_{w_1}(a > 1|g), & \\text{if } a = 1 \\\\ P_{w_{a-1}}(a > a-1|g) - P_{w_a}(a > a|g), & \\text{if } 1 < a < m \\\\ P_{w_{m-1}}(a > m - 1|g), & \\text{if } a = m \\end{cases}$\nUser Inference. In contrast to other inference attacks where the sensitive variable is sampled from a well-defined set of values, in user inference, the sensitive variable is the user's identity, which does not take on a fixed set of values. Moreover, the identities that occur during test time are likely not seen during the development of the attack model. As a result, the posterior $P(a|g)$ cannot be directly modeled. To resolve this, we employ a training strategy analogous to the prototypical network [81] for few-shot learning. Specifically, we first train a neural network $f_w \\cdot u$ that is composed of an encoder $f_g : g \\rightarrow h$ that maps the gradient vector to a continuous embedding space and a classifier $u : h\\rightarrow a$ that takes the embedding as input and outputs the predicted user identity. Given gradient and sensitive variable pairs $(g, a)$ created from the shadow dataset, as the number of available users in the shadow dataset is finite, the neural network can be trained in an end-to-end manner using standard multi-class classification loss such as cross-entropy. After training, the classifier $u$ is discarded. At the time of inference, the adversary is provided with an observed gradient $\u011f$ and a set of m candidate data batches $\\{D_i\\}_{i \u2208 [m]}$, where $D_i = \\{(x_j, y_j)\\}_{j=1}^k$. Then, the adversary can derive the corresponding set of candidate gradients $\\{g_i\\}_{i \u2208 [m]}$ based on the current model parameters \u03b8. Finally, the adversary computes the probability of each candidate identity after observing the gradient as\n$P_w(a = a|g = \\bar{g}) = \\frac{\\exp(-\\||f_w(g_a) - f_w(\\bar{g})\\||^2)}{\\sum_{i\u2208[m]} \\exp(-\\||f_w(g_i) - f_w(\\bar{g})\\||^2)}$\n4.2 Continual Attack and Adaptive Attack\nThe inference attack can be further improved if the adversary has access to multiple rounds of gradients or the defense mechanism.\nInference under Continual Observation. In cases where continual observation of the gradients is allowed, the adversary can use the set of observed gradients $G = \\{g_i\\}_{i \u2208 R}$ from multiple rounds to improve the attack. A naive solution would be to train a model to directly approximate $P(a|G)$. However, this would be generally infeasible in practice because of the high dimensionality of G. Instead, the adversary can use Bayesian updating to accumulate adversarial knowledge. Specifically, given a set of observed gradients, the log-posterior can be formulated as\n$\\log P(a = a|G)$\n$= \\log P(G|a = a) + \\log P(a = a) - \\log P(G)$\n$\\approx \\sum_{i\u2208R} \\log P(g_i|a = a) + \\log P(a = a) - \\log P(G)$\n$= \\sum_{i\u2208R} \\log \\sum_{a} P(a = a|g_i) + \\log P(g_i) - \\log P(a = a)) + \\log P(a = a) - \\log P(G)$\n$= \\sum_{i\u2208R} \\log P(a = a|g_i) - (|R| - 1) \\log P(a = a) + C,$\nwhere Eq. (3) makes the approximating assumption that the gradients are conditionally independent given a. Since C = -log P(G) + $\\sum_{i\u2208R} \\log P(g_i)$ is independent of a, and therefore it can be treated as a constant. C = 0 if the gradients $g_i$ are additionally mutually independent. In Eq. (5), the prior term is known and $P(a = a|g_i)$ can be approximated by training a fresh model for each round of observation. The sensitive variable can thus be estimated as $a = \\arg \\max_a \\log P(a = a|G)$.\nAdaptive Attack. The adversary can design adaptive attacks if the defense mechanism M is known. Instead of training the predictive model $P_w(a|g)$ using clean gradient pairs $(g_a, a)$, a simple strategy for adaptive attack is to apply the same defense mechanism to the shadow data's gradients and use the transformed gradient pairs $(M(g_a), a)$ to train the predictive model $P_w(a|M(g))$. As we will show in Section 6, this simple strategy is sufficient to bypass several heuristic-based defenses."}, {"title": "5 ATTACK EVALUATION", "content": "In this section", "are": 1, "Observation": "an adversary can improve the inference by accumulating information from multiple rounds of updates", "Size": "when the private information is shared across the batch", "Knowledge": "the attack improves with the amount of knowledge of the data\ndistribution (as captured by the number of available shadow data points).\n5.1 Experimental Setup\n5.1.1 Datasets and Model Architecture. We consider the following five datasets with different data modalities (tabular", "8": "is a tabular dataset containing 48", "47": "Heritage Health Prize) is a tabular dataset from Kaggle that contains de-identified medical records of over 55", "12": "is a multi-modal dataset that contains 7", "25": "to extract a total number of 23", "26": "we use EmoBase which is a standard feature set that contains the MFCC", "36": ".", "59": "contains 202", "102": "consists of over 20", "performance": "n(1) Attack Success Rate (ASR): We measure the attack performance by the number of times the adversary successfully guesses the sensitive variable", "sum_{t\u2208[T": ""}, "frac{1_{\\hat{a}=a}}{T}$, where T is the total number of trials (i.e., repetitions of the inference game).\n(2) AUROC: We additionally report the area under the receiver operating characteristic curve (AUROC). For sensitive variables that have more than two classes, we report the macro-averaged AUROC.\n(3) Advantage: We follow prior work [34, 97"], "Adv(p)": "frac{\\max(p - p^*", "TPR@1%FPR": "Besides average performance metrics, recent work on membership inference [13, 96] argue the importance of understanding the privacy risk on worst-case training data by examining the low false positive rate (FPR) region. Inspired by this, we additionally report the true positive rate (TPR) when the FPR is 1%.\n5.1.3 Adversary's Model. We conducted preliminary experiments with various types and configurations of ML models and found that random forest with 50 estimators performs the best (especially in the low FPR region) for estimating the posterior in AIA, PIA, and DIA with small shadow dataset sizes. For UIA, we use a fully-connected network with one hidden layer as the encoder. The embedding dimension is set to be 50 for the CREMA-D dataset of 100 for CelebA dataset. As the gradient vector is extremely high dimensional (e.g., the gradient dimensions for CREMA-D and CelebA datasets are 67,716 and 45,922, respectively), we apply a 1-dimensional max-pooling layer before the adversary's predictive model with a kernel size of 3 for tabular datasets and 10 for other datasets for dimensionality reduction.\n5.1.4 Other Attack Settings. We assume the model parameters \u03b8 are randomly initialized at the beginning of the inference game. During the game, the model parameters are updated at each epoch using SGD with a learning rate of 0.01. We evaluate AIA on the tabular datasets and UIA on datasets that contain user labels (CREMA-D and CelebA), while PIA and DIA are evaluated on all datasets. For AIA, PIA, and DIA, we use a training set of 5,000 samples and a balanced public set that contains a default number of 1,000 samples equally divided for each sensitive attribute/property class. For UIA, we first filter out user identities that contain less than 2\u00d7\n5.2 Evaluation of Inference Attacks\nWe evaluate each type of inference attack with a small shadow dataset (1,000 samples) and compare the results of single-round attacks (where the adversary only observes a single round of gradients) to multi-round attacks (where the adversary gets continual observation of the gradients). Due to space limits, we only include a snapshot of the results (one dataset per attack) in Figure 2 and provide the"}