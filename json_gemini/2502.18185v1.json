{"title": "VesselSAM: Leveraging SAM for Aortic Vessel\nSegmentation with LoRA and Atrous Attention", "authors": ["Adnan Iltaf", "Rayan Merghani Ahmed", "Bin Li", "Shoujun Zhou"], "abstract": "Abstract-Medical image segmentation is crucial for clinical\ndiagnosis and treatment planning, particularly for complex\nanatomical structures like vessels. In this work, we propose\nVesselSAM, a modified version of the Segmentation Anything\nModel (SAM), specifically designed for aortic vessel segmentation.\nVesselSAM incorporates AtrousLoRA, a novel module that\ncombines Atrous Attention with Low-Rank Adaptation (LoRA),\nto improve segmentation performance. Atrous Attention enables\nthe model to capture multi-scale contextual information,\npreserving both fine local details and broader global context.\nAt the same time, LoRA facilitates efficient fine-tuning of the\nfrozen SAM image encoder, reducing the number of trainable\nparameters and ensuring computational efficiency. We evaluate\nVesselSAM on two challenging datasets: the Aortic Vessel Tree\n(AVT) dataset and the Type-B Aortic Dissection (TBAD) dataset.\nVesselSAM achieves state-of-the-art performance with DSC\nscores of 93.50%, 93.25%, 93.02%, and 93.26% across multiple\nmedical centers. Our results demonstrate that VesselSAM\ndelivers high segmentation accuracy while significantly reducing\ncomputational overhead compared to existing large-scale models.\nThis development paves the way for enhanced AI-based\naortic vessel segmentation in clinical environments. The code\nand models will be released at https://github.com/Adnan-CAS/\nAtrous Lora.", "sections": [{"title": "I. INTRODUCTION", "content": "MEDICAL imaging stands at the cutting edge of modern\nhealthcare, serving a vital tool in diagnosing and\ntreating various diseases. Within this domain, medical image\nsegmentation is a critical component aiming to delineate\nstructures such as organs, tumors, and vessels [1]. Aortic\nvessel segmentation is particularly important for diagnosing\ncardiovascular diseases, enabling precise assessments of\nvascular health and supporting interventions such as stent\nplacement and aneurysm monitoring. It plays a crucial\nrole in computer-aided diagnosis, treatment planning, and\nsurgical interventions [2]. With the rapid advancement of\ncomputational resources and the growing availability of\nmedical data, Vision Transformers (ViTs) have emerged as a\ntransformative approach in medical image analysis [3]. Unlike\ntraditional convolutional models, ViTs utilize self-attention\nmechanisms to capture long-range dependencies and global\ncontext [4], significantly improving the ability to model\ncomplex structures within medical images [5].\nThis paradigm shift has paved the way for more advanced\nsegmentation techniques like the Segmentation Anything\nModel (SAM) [6], Swin-Unet [7], UNETR [8], SAMMedAI\n[9], and MedSAM [10] which leverage the power of ViT's for\naccurate and efficient segmentation tasks. SAM enables users\nto generate segmentation masks using interactive prompts like\nclicks, bounding boxes and text. Its remarkable zero-shot and\nfew-shot abilities have proven highly effective for natural\nimages, gaining wide attention. However, while SAM excels\nin natural image segmentation, recent studies have highlighted\nits limitations in the medical domain [11] [12].\nMedical images, often characterized by low contrast,\nambiguous tissue boundaries, and small regions of interest,\npresent unique challenges to SAM [13]. Several recent\napproaches [14]\u2013[16] have sought to fine-tune SAM for\nmedical image segmentation by incorporating domain-specific\nenhancements. However, fine-tuning these models requires\nsignificant computational resources, due to the extensive\nnumber of parameters in foundation models like SAM.\nFurthermore, training large models with limited task-specific\ndata often results in overfitting and inferior performance.\nTo overcome these challenges, parameter-efficient fine-tuning\n(PEFT) approaches such as Low-Rank Adaptation (LoRA)\n[17] have been developed as a viable solution. Various\nmethods have incorporated LoRA with SAM to address the\ncomputational efficiency while maintaining performance gains,\nspecifically for medical image segmentation [18] [19].\nDespite these efforts, several fundamental intrinsic\nlimitations of SAM persist. SAM's image encoder, based on\nplain ViTs, inherently lacks crucial vision-specific inductive\nbiases needed to capture local patterns and fine details\nessential for dense predictions in medical imaging [20].\nAdditionally, SAM's ViT-based architecture relies on global\nattention without integrating regional attention or sparse\nattention mechanisms, which are vital for focusing on\nrelevant regions and reducing computational overhead [21].\nWhile regional attention, helps capture spatial hierarchies\nat different scales, SAM's reliance on global attention\nlimits its ability to focus on smaller, intricate regions in"}, {"title": "", "content": "medical images. On the contrary, the absence of sparse\nattention prevents SAM from efficiently capturing global\ncontext without a significant computational cost. These\nlimitations make SAM prone to errors, such as hallucinating\nsmall, disconnected components in its segmentation [4] [9],\nespecially when modeling structures like vessels, tumors, or\nlesions. To enhance the performance of plain ViTs in dense\nprediction tasks, recent research has combined Transformer\nand convolutional features [22] [23]. Work [24] integrates\natrous attention with ViTs, enabling multi-scale feature\nextraction while preserving resolution. Atrous attention\ncombines regional and sparse attention, allowing the model\nto focus on local details while still capturing the broader\ncontext.\nInspired by the work [24], we propose VesselSAM, a model\nthat integrates Atrous Attention with SAM, leveraging both\nglobal transformer attention and local convolutional inductive\nbiases. VesselSAM incorporates several key innovations to\nenhance SAM's capabilities. First, we integrate Atrous Spatial\nPyramid Pooling (ASPP) to capture multi-scale contextual\ninformation, enabling the model to handle both small\nand large anatomical structures without sacrificing spatial\nresolution [22]. Additionally, Atrous Attention mechanisms\nare introduced, combining dilated windows at different scales\nto balance local feature extraction with global contextual\nunderstanding, allowing the model to focus on fine details\nwhile maintaining a comprehensive view of the entire image\n[23]. Furthermore, VesselSAM incorporates LoRA [18] layers\nto fine-tune the model efficiently, reducing the need for\ncomputationally expensive full retraining while ensuring high\nperformance across various medical segmentation tasks.\nThe main contributions of this work are summarized as\nfollows:\n\u2022 We propose VesselSAM, a novel model that integrates\nAtrousLoRA,a module that combines Atrous Attention\nwithin Low-Rank Adaptation (LoRA) to enhance the\nSAM architecture for vascular image segmentation,\nparticularly aortic vessel segmentation. AtrousLoRA\nenables VesselSAM to capture both local and global\nfeatures efficiently, improving segmentation accuracy\nwhile keeping the pre-trained image encoder frozen. The\nintegration of Atrous Attention allows for multi-scale\nfeature extraction through dilated convolutions, while\nLORA reduces the number of trainable parameters\nwithout compromising model performance.\n\u2022 AtrousLoRA is integrated into VesselSAM and comprises\ntwo key modules: the Atrous Spatial Pyramid Pooling\n(ASPP) module and the Attention mechanism. The\nASPP module uses dilated convolutions at different\nrates to capture multi-scale contextual information,\nenabling VesselSAM to focus on both fine details,\nsuch as small vessel boundaries, and broader anatomical\nstructures without losing spatial resolution. Meanwhile,\nthe Attention mechanism balances local feature extraction\nwith global context, guiding VesselSAM to focus on\nrelevant anatomical regions and improving segmentation\nperformance.\n\u2022 AtrousLoRA leverages LoRA's key concept of applying a\n\u2022"}, {"title": "", "content": "low-rank constraint to the transformer features, enabling\nVesselSAM to fine-tune the model efficiently with\nonly 7% of the trainable parameters. This reduces\nthe computational cost significantly, making VesselSAM\nmore suitable for tasks with limited data, as it eliminates\nthe need for full retraining while maintaining high\nperformance.\n\u2022 We evaluate VesselSAM on multiple challenging\nbenchmark datasets, including the Aortic Vessel Tree\n(AVT) Segmentation dataset and the imageTBAD dataset.\nThe results show that VesselSAM, with AtrousLoRA,\noutperforms baseline methods in segmentation accuracy,\nrobustness, and computational efficiency, particularly for\naortic vessel segmentation.\nThe rest of this article is organized as follows. Section\nII provides an overview of related work, focusing on\nmedical foundation models, parameter-efficient fine-tuning\nstrategies, and the application of atrous convolution in Vision\nTransformers (ViTs). Section III details the methodology,\nbeginning with a preliminary description of the SAM\narchitecture, followed by the introduction of VesselSAM,\nLoRA, and AtrousLoRA, as well as the integration of the\nAtrous Attention Module for capturing both local and global\nfeatures. Section III further discusses the role of the Prompt\nEncoder and Mask Decoder in generating high-resolution\nsegmentation masks. Section IV outlines the experimental\nsetup, including dataset descriptions, evaluation metrics, and\npresents both quantitative and qualitative results. It also\nincludes an ablation study, addressing the model's limitations\nand suggesting potential future directions. Finally, Section V\nconcludes the article.\nII. RELATED WORK\nA. ViT and SAM Based Medical Foundation Models\nVision Transformers (ViTs) based medical foundation\nmodels have significantly impacted medical image\nsegmentation, with models like UNETR [8] leading the\nway. UNETR utilizes a ViT-based encoder to effectively\ncapture global context while integrating it with a U-Net\narchitecture for effective medical image segmentation. In\ncontrast, SAM-based medical foundation models leveraging\ntransformer architectures have demonstrated impressive\nperformance across a broad range of natural image\nsegmentation tasks. However, their application in the\nmedical domain has faced limitations due to the unique\nchallenges of medical images, such as low contrast and\ncomplex anatomical structures. Recognizing these limitations,\nMedSAM [10] sought to improve SAM's performance in\nmedical image segmentation by freezing the large pre-trained\nimage encoder and prompt encoder, while fine-tuning only\nthe lightweight mask decoder on domain-specific medical\ndatasets. This approach leverages SAM's powerful pre-trained\narchitecture while adapting its mask prediction capabilities to\nthe medical domain.\nB. Parameter-Efficient Model Fine-Tuning\nThe concept of Parameter-Efficient Fine-Tuning (PEFT)\nhas emerged as an effective way to adapt large foundational"}, {"title": "", "content": "models like SAM to specific downstream tasks with minimal\nadditional parameter costs. One prominent PEFT approach,\nLORA (Low-Rank Adaptation), has been successfully\nincorporated into SAM-based models. For example,\nSAMed [11] applied LoRA to SAM's frozen image\nencoder, fine-tuning the LoRA layers, the prompt encoder,\nand the mask decoder together on medical datasets like\nSynapse multiorgan, demonstrating significant performance\nimprovements. Similarly, SAMAdp [19] introduces a\nlightweight adapter module to enhance SAM's performance\nin challenging segmentation tasks. By integrating task-specific\nprompts and adapters, it improves accuracy while maintaining\ncomputational efficiency, demonstrating adaptability across\nvarious domains. Other works have taken different approaches\nto enhancing SAM for medical applications. SAMMed [9]\nevaluated SAM across 53 public medical imaging datasets,\nrevealing that while SAM has strong zero-shot segmentation\ncapabilities but it often underperforms without fine-tuning.\nC. Atrous Convolution in ViTs\nThe use of Atrous Convolution (dilated convolution) in ViTs\nhas gained attention as a powerful method to capture both local\npriors and global context, essential for segmentation tasks [21].\nAtrous convolution increases the receptive field by \"skipping\"\npixels, allowing the model to capture information over larger\nregions without downsampling, thus preserving fine-grained\ndetails while enhancing the ability to model broader\nspatial relationships. This technique, initially popularized\nin DeepLab [6] for convolutional networks, has proven\neffective in extracting multi-scale features, which are crucial\nfor complex segmentation tasks involving varying object\nsizes. In ViTs, where image features are typically processed\nas non-overlapping patches, integrating Atrous Convolutions\nenables the model to learn hierarchical spatial dependencies.\nBy applying dilated convolutions at multiple rates, Atrous\nSpatial Pyramid Pooling (ASPP) modules allow the model\nto capture multi-scale contextual information, bridging the\ngap between local interactions and global dependencies. This\napproach is particularly beneficial in tasks requiring detailed\nsegmentation, where capturing both local fine details and\nglobal context is necessary for accurate predictions. Recent\nadvancements have shown that Atrous Convolutions are crucial\nfor improving the performance of ViTs in segmentation tasks,\nparticularly in domains such as medical imaging. In our model,\nwe leverage the power of ASPP and Attention mechanisms\nto enhance the ViT encoder's ability to capture both local\npriors and global context, effectively enabling the model\nto handle complex, high-resolution segmentation tasks with\ngreater accuracy."}, {"title": "A. Overview", "content": "III. METHODOLOGY\nVesselSAM is a promptable segmentation model designed\nto enhance vascular structure segmentation in medical images.\nBuilding upon the standard SAM (Segment Anything Model)\nframework, VesselSAM integrates key modifications such\nas the Atrous Attention Module and LoRA (Low-Rank\nAdaptation) layers to improve performance for medical image\nsegmentation. The image encoder and prompt encoder from\nthe SAM model are frozen to preserve their pre-trained\nfeatures, while the Atrous Attention module and LoRA layers\nenhance the model's ability to capture multi-scale features\nand optimize training efficiency. The final segmentation is\ngenerated by a mask decoder that refines the fused embeddings\nusing cross-attention mechanisms. The overall design ensures\nVesselSAM is a robust and efficient model for medical\nimage segmentation tasks, particularly vascular segmentation\nin aortic imaging.\nB. Preliminary: SAM architecture\nThe SAM [6] is a prompt-based segmentation framework\ncomposed of three main components: the Image Encoder,\nPrompt Encoder, and Mask Decoder. The Image Encoder\nis based on a ViT, which processes input images using 16\n\u00d7 16 pixel patches through transformer blocks to capture\nimage features, resulting in image embedding. The Prompt\nEncoder handles various prompts, including points, bounding\nboxes and masks, converting them into feature vectors that\nguide the segmentation. The Mask Decoder is a two-layer\ntransformer-based decoder that fuses image embedding and\nprompt features using cross-attention. It incorporates a\nMulti-Layer Perceptron (MLP) for feature refinement and\ndimensionality alignment, and utilizes convolutional layers for\nupsampling to produce high-resolution masks."}, {"title": "C. VesselSAM", "content": "The VesselSAM architecture builds on the foundation of\nSAM, with several key modifications aimed at improving\nvascular structure segmentation in medical images. As\nillustrated in Fig. 1, VesselSAM incorporates the Atrous\nAttention module and LoRA layers, designed to capture\nmulti-scale features and reduce the number of trainable\nparameters while maintaining segmentation accuracy.\nIn this design, the image encoder and prompt encoder\nfrom the original SAM architecture are frozen to retain their\npowerful pre-trained features. The image encoder, based on a\nVision Transformer (ViT), extracts rich visual features from\nthe input medical images. The prompt encoder processes\nsparse prompts such as points or bounding boxes, which guide\nthe segmentation process by focusing on specific regions of\ninterest in the image.\nTo enhance the model's ability to capture both local and\nglobal features, the Atrous Attention module is integrated\ninto the frozen image encoder. This module utilizes dilated\nconvolutions to expand the receptive field, allowing the model\nto capture multi-scale features, which are crucial for medical\nimages like small tumors or vascular boundaries.\nAdditionally, LoRA (Low-Rank Adaptation) layers are\ninserted between the transformer blocks in the image encoder.\nThese layers compress the transformer features into a low-rank\nspace and then re-project them, allowing efficient adaptation\nof the features while preserving the frozen transformer\nparameters. This modification improves training efficiency,\nreducing the number of trainable parameters and enhancing\nthe model's performance with fewer resources.\nThe final segmentation is generated by the mask decoder,\nwhich consists of a lightweight transformer decoder and\na segmentation head. During training, the mask decoder\nis fine-tuned to refine the fused embeddings from the\nimage and prompt encoders using cross-attention mechanisms."}, {"title": "D. LORA and AtrousLORA", "content": "LoRA [17] has emerged as a PEFT method, enabling\ntask-specific adaptations of pre-trained models while\nsignificantly reducing computational and memory overhead.\nLORA introduces low-rank trainable matrices to approximate\nweight updates, effectively bypassing the need to fine-tune\nthe entire model(Fig. 2(a)). Instead, it adds two small\nmatrices, We and Wa, while keeping the original weights\nWo frozen during training. Given a pre-trained weight matrix\nWo\u2208 \u211d^(Cout\u00d7Cin), LoRA modifies the forward pass of the\nmodel as\n$$h = W_0x + W_aW_ex,$$\nwhere Wo is the frozen pre-trained weight matrix, We \u2208\n\u211d^(r\u00d7Cin) and Wa\u2208 \u211d^(Cout\u00d7r) are the low-rank encoder and\ndecoder matrices, and r is the rank of the decomposition, with\nr < min(Cin, Cout). Here, x \u2208 \u211d^(B\u00d7Cin) represents the input,\nwhere B is the batch size.\nWhile LoRA is highly efficient for adapting pre-trained\nmodels, it lacks the ability to explicitly capture multi-scale\ncontextual information, which is critical for vision tasks such\nas image segmentation and dense prediction. To address this\nlimitation, we introduced Atrous LoRA which incorporates\natrous (dilated) convolutions into the LoRA framework(Fig.\n2(b)). Atrous convolutions expand the receptive field of the\nmodel without increasing the number of parameters, enabling\nit to capture both local and global dependencies.\nMathematically, with AtrousLoRA, Equation 1 changes to:\n$$h = W_0x + W_d \\cdot Atrous(W_ex),$$\nwhere Wo \u2208 \u211d^(Cout\u00d7Cin) is the frozen pre-trained weight\nmatrix, We \u2208 \u211d^(r\u00d7Cin) and Wa \u2208 \u211d^(Cout\u00d7r) are the low-rank\nencoder and decoder matrices, and x \u2208 \u211d^(B\u00d7Cin\u00d7H\u00d7W) is the\ninput feature map. In this case, B represents the batch size,\nCin and Cout are the input and output channels, and H and W\nrepresent the height and width of the feature maps. The Atrous\nmodule applies atrous convolutions with predefined dilation\nrates to Wex, effectively capturing multi-scale contextual\nfeatures. Atrous LoRA employs fixed dilation rates, which\nsimplifies implementation while maintaining adaptability for\nvarious vision-related tasks.\nAtrousLoRA retains the efficiency of LoRA while extending\nits applicability to tasks requiring spatial and contextual\nunderstanding, such as semantic segmentation and medical\nimage analysis. The predefined dilation rates ensure a balance\nbetween computational efficiency and multi-scale feature\nextraction."}, {"title": "E. Atrous Attention Module", "content": "We propose a novel attention mechanism for vision\ntransformers, called Atrous Attention Module (Fig. 2(c)),\nwhich performs a fusion between regional and sparse attention.\nThis approach allows us to capture both global context and\nlocal detail with efficient computational complexity, while\npreserving the hierarchical information present in medical\nimages. Inspired by atrous convolution [24], which increases\nthe receptive field by skipping rows and columns in the input\nfeature map without adding extra parameters, Atrous Attention\nenables VesselSAM to focus on relevant anatomical structures\nacross multiple scales. The process is shown in Algorithm 1.\nThe data flow within the Atrous Attention Module starts\nby passing the input feature map X \u2208 \u211d^(B\u00d7C\u00d7H\u00d7W)\nthrough the Atrous Spatial Pyramid Pooling (ASPP) [21],\nwhich applies dilated convolutions at different rates di to\ncapture features at various scales. Each atrous convolution\nproduces an output feature map Y\u2081 = f(X; Wi, di) where\nWi are the convolution weights and di is the dilation rate.\nAdditionally, global average pooling is performed on X\nto obtain Z = f1x1 (GAP(X)) The outputs from ASPP,\nincluding the atrous convolutions Y\u2081 and the global pooling\nresult Z, are concatenated into a Concatenated Feature Map\nY = [Y1, Y2, . . ., Yn, Z]. This concatenated feature map then\ngoes through a 1x1 Convolution, reducing it to the desired\nnumber of output channels, followed by Batch Normalization\n(BN) and ReLU activation, generating the ASPP Output\nYASPP = ReLU(BN(f1x1(Y))).\nThis output is further processed through another 1x1\nConvolution to create the Attention Map A = f1x1(YASPP)\nwhere A \u2208 \u211d^(B\u00d71\u00d7H\u00d7W) is the attention map and a Sigmoid\nactivation is applied to obtain Asigmoid = \u03c3(A) which\nconstrains the attention values between 0 and 1 that is where\nAsigmoid \u2208 [0,1]^(B\u00d71\u00d7H\u00d7W). Finally, the ASPP Output is\nmultiplied element-wise with the Attention Map, producing\nthe Final Output Yout = YASPP Asigmoid, The result is an\nattention-weighted feature map Yout \u2208 [\u211d^(B\u00d7C'\u00d7H\u00d7W, where\nimportant regions of the feature map are enhanced, and less\nimportant regions are suppressed. This mechanism enhances\nVesselSAM's ability to focus on the most important features,\nimproving segmentation accuracy while maintaining context\nfrom multiple scales."}, {"title": "F. Atrous Spatial Pyramid Pooling", "content": "Atrous Spatial Pyramid Pooling (ASPP) is another\nimportant component in the VesselSAM Model. It plays a\ncrucial role in enhancing the model's ability to segment\nvascular structures by capturing multi-scale contextual\ninformation from medical images. VesselSAM, which\nintegrates advanced segmentation techniques, utilizes ASPP to\nimprove the segmentation of blood vessels by applying dilated\nconvolutions with varying dilation rates. This allows the model\nto capture both fine details and broader contextual information\nwithout losing resolution. ASPP increases the receptive field\nby using dilated convolutions at multiple rates, enabling the\nmodel to understand both local features of vessels and their\nspatial relationships within the image, which is critical for\naccurate vascular segmentation.\nMathematically, in VesselSAM, ASPP begins by applying\ndilated convolutions with different dilation rates di \u2208\n{d1, d2,..., dn}, where each rate captures features at different\nscales. For each dilation rate di, the dilated convolution\noperation is applied to the input feature map X as follows:\n$$Y_i = f_{dil}(X; W_i, d_i) = X *_{d_i} W_i,$$\nwhere *di denotes the dilated convolution, and Wi\nrepresents the filter with dilation rate di. In addition to the\ndilated convolutions, a global average pooling operation is\napplied to capture the global context of the input image, which\nis mathematically defined as:\n$$Z = \\frac{1}{HW} \\sum_{h=1}^{H} \\sum_{w=1}^{W} X_{b,c,h,w}$$\nwhere Z represents the global pooling result, effectively\nreducing the spatial dimensions to 1 \u00d7 1 while maintaining\nthe channel information. These outputs are then concatenated\ninto a single feature map:\nYconcat = [Y1, Y2, . . ., Yn, Z],\nThe concatenated feature map Yconcat contains multi-scale\ninformation, helping VesselSAM capture both local vessel\nfeatures and broader contextual relationships, which is\nessential for accurately segmenting complex vascular\nstructures. To reduce the dimensionality of this concatenated\nfeature map, a 1 \u00d7 1 convolution is applied:\n$$Y_{ASPP} = f_{1x1}(Y_{concat}) = W_{1x1} Y_{concat} + b_{1x1},$$\nwhere W1x1 and b1x1 are the weight and bias of the 1 \u00d71\nconvolution. Finally, a non-linear activation function, such as\nReLU, is applied to introduce non-linearity into the model:\nYASPP = ReLU(YASPP)"}, {"title": "G. Prompt Encoder And Mask Decoder", "content": "In VesselSAM, the Prompt Encoder remains frozen,\nensuring the stability of the pre-trained parameters while\nallowing for efficient processing of user prompts. In our case,\nthe prompts are provided in the form of bounding boxes,\nwhich are represented by their top-left and bottom-right corner\npoints. Each corner point is mapped into a 256-dimensional\nembedding, which serves as the input to the segmentation\nprocess. By freezing the prompt encoder, VesselSAM enables\nreal-time interaction, as the image embedding can be\nprecomputed, allowing the user to provide bounding-box input\ndynamically without the need for retraining.\nOn the other hand, the Mask Decoder in VesselSAM\nis fully trainable and plays a crucial role in producing\nthe segmentation output. The decoder architecture includes\ntwo transformer layers, which are responsible for fusing\nthe image embedding with the prompt embeddings through\ncross-attention. This fusion allows the bounding box\ninformation to guide the segmentation task effectively.\nFollowing this, the decoder employs two transposed\nconvolution layers to upsample the combined embedding to\na resolution of 256 x 256, ensuring a high level of detail is\nretained in the final segmentation mask. The output is then\npassed through a sigmoid activation function, followed by\nbi-linear interpolation to match the resolution of the original\ninput image, thereby producing the final high-resolution mask."}, {"title": "A. Datasets", "content": "IV. EXPERIMENTS\nIn our experiments, we utilized two key datasets to\nevaluate the effectiveness of the proposed VesselSAM\nmodel in complex medical segmentation tasks. The Aortic\nVessel Tree (AVT) Segmentation dataset [27] comprises 56\ncontrast-enhanced CT angiography (CTA) scans collected\nfrom three sources: the KiTS Grand Challenge, the Rider\nLung CT dataset, and Dongyang Hospital. Among these, 38\ncases were designated for training, while the remaining 18\nwere used for testing. All slices were resampled to a spatial\nresolution of 1 mm \u00d7 1 mm, with Hounsfield Unit (HU)\nvalues normalized to [0, 1]. Additionally, the TBAD dataset\n[28], comprising 100 CTA images from Guangdong Provincial\nPeople's Hospital, was utilized for segmenting True Lumen\n(TL), False Lumen (FL), and False Lumen Thrombus (FLT)\nin Type-B Aortic Dissection (TBAD) cases. To conform to\nSegment Anything Model (SAM) requirements, both the AVT\nand TBAD datasets were converted from 3D CTA volumes\ninto 2D slices. Each 3D scan was converted into NumPy\narrays, with all slices resampled to a uniform resolution of\n1 mm \u00d7 1 mm. Voxel intensity values were normalized using\nstandard CT window settings [400, 40]. Ground truth masks\nwere refined by removing labels of irrelevant structures and\nsmall objects, using thresholds of 1000 voxels for 3D volumes\nand 100 pixels for individual 2D slices. Only non-zero slices\nwere retained, and intensity normalization was applied. The\nprocessed 2D slices were then resized to 1024 \u00d7 1024 pixels\nand converted into three-channel images by duplicating the\ngrayscale slice across three channels (1024 \u00d7 1024 x 3),\nensuring compatibility with SAM's input format."}, {"title": "B. Loss Function and Evaluation Metrics", "content": "We have used a combined loss function comprising an\nunweighted sum between cross-entropy loss and Dice loss,\nwhich has been widely adopted for its robustness in medical\nimage segmentation tasks [10]. Let P represent the predicted\nsegmentation output, and T denote the corresponding ground\ntruth. For each voxel j, pj and tj correspond to the predicted\nand ground truth values, respectively. The total number of\nvoxels in the image is denoted by M. The binary cross-entropy\nloss is defined as:\n$$L_{CE} = - \\frac{1}{M} \\sum_{j=1}^{M} [t_j log p_j + (1 - t_j) log(1 - p_j)],$$\nwhere LCE quantifies the pixel-wise classification accuracy.\nThe Dice loss, which measures the overlap between the\npredicted and ground truth regions, is given by:\n$$L_D = 1 - \\frac{2 \\sum_{j=1}^{M} p_j t_j}{\\sum_{j=1}^{M} (p_j)^2 + \\sum_{j=1}^{M} (t_j)^2}$$\nThe final loss L is computed as the sum of the cross-entropy\nloss and the Dice loss:\nL = LCE + LD"}, {"title": "C. Quantitative results", "content": "In this section, we provide a comprehensive comparison\nof our proposed method, VesselSAM, against various\nstate-of-the-art (SOTA) models, including UNET [25],"}, {"title": "", "content": "UNETR [8], SAM [6], MedSAM [10], SAMMed [9], SAMed\n[11] and SAM-Adopter [19]. Each method was assessed under\nthe same conditions to ensure a fair comparison, allowing us\nto accurately evaluate performance metrics such DSC and HD.\nThe results highlight how our approach outperforms the SOTA\nmodels in the field, demonstrating its effectiveness in tackling\ncomplex medical image segmentation tasks.\n1) Quantitative Evaluation Results for AVT Dataset: The\nperformance metrics for various segmentation methods on the\nAortic Vessel Tree (AVT) dataset are presented in Table. I.\nThis comparison encompasses both big and small models,\nillustrating the effectiveness of each approach across multiple\nhospitals. VesselSAM demonstrates exceptional segmentation\nperformance, achieving a Dice Similarity Coefficient (DSC)\nof 93.50% at Dongyang Hospital, 93.25% at Rider Hospital,\nand 93.02% at Kits Hospital. This performance significantly\nsurpasses that of state-of-the-art methods, including MedSAM\nand SAMAdp.\nThe incorporation of Atrous Attention and LoRA\nmechanisms within VesselSAM has contributed to its high\nperformance, enabling the model to effectively capture\nmulti-scale features essential for precise segmentation in\nmedical imaging. In contrast, the other SAM-based models\nsuch as SAMAdp and SAMedAdp struggle with accuracy, as\nthey produce a significant number of false positive regions\nwithin their segmentations. This disparity underscores the\nadvantages of VesselSAM in accurately delineating vascular"}, {"title": "", "content": "structures amidst challenging imaging contexts, ultimately\nsupporting its utility for clinical applications.\n2) Quantitative Evaluation Results for TBAD Dataset:\nThe results for the Type-B Aortic Dissection (TBAD)\ndataset are summarized in Table. II, further highlighting the\nperformance of VesselSAM. The model achieves a DSC of\n93.26%, outperforming various competing methods, including\nUNETR and MedSAM. These findings illustrate VesselSAM's\nrobustness in accurately segmenting the true lumen (TL) and\nfalse lumen (FL), emphasizing its effectiveness in handling\ncomplex segmentation tasks within clinical settings.\nIn comparison, SAM and MedSAM display lower\nperformance, with DSC scores of 79.53% and 92.20%,\nrespectively. Moreover, other models such as SAMAdp and\nSAMedAdp also exhibit challenges in segmentation accuracy,\nas evidenced by their lower DSC values. The consistent high\nperformance of VesselSAM across both the AVT and TBAD\ndatasets demonstrates its potential as a valuable tool for\nmedical image segmentation, particularly in complex cases\nwhere precision is paramount.\nD. Qualitative results\nTo provide a more intuitive comparison, we present the\nqualitative segmentation results for various models, including\nVesselSAM, SAM, MedSAM, SAMAdp, SAMedAdp, and\nSAM-MedIA, as illustrated in the Fig. 3. The top row displays\nthe results for aortic vessel segmentation, while the bottom"}, {"title": "", "content": "row highlights the segmentation of true lumen (TL) and\nfalse lumen (FL) for Type-B Aortic Dissection (TBAD). In\nthe aortic vessel segmentation task, VesselSAM effectively\ndelineates the vessel structures, capturing intricate details\nthat may be overlooked by other models. The segmentation\naccurately follows the boundaries of the aorta, demonstrating\nits robustness in identifying the vessel amidst surrounding\ntissues. In contrast, SAM struggles with segmentation\naccuracy, leading to significant misalignments with the ground\ntruth, particularly in the definition of vessel edges. MedSAM\ndemonstrates improved performance compared to SAM but\nstill fails to capture some fine details, resulting in inaccuracies\nin the vessel's contour. The models SAMAdp, SAMedAdp,\nand SAM-MedIA, struggles to accurately capture the true\npositive vessel areas, resulting in a significant number of\nfalse positive regions in their segmentations. While these\nmodels provide reasonable outputs, they tend to misidentify\nsurrounding areas as part of the vessel structure.\nIn the segmentation of TL, FL and FLT in TBAD dataset,\nVesselSAM continues to excel by accurately capturing the\nluminal structures. The segmentation closely aligns with\nthe GT, effectively distinguishing the TL, FL and the\nFLT. For better visualization, only the TL and FL are\npresented. In contrast, SAM encounters significant challenges,\nwith poor segmentation of the TL, resulting in structural\nmisrepresentations. MedSAM provides an improvement over\nSAM, but it still exhibits inaccuracies that affect its reliability\nin clinical applications. Other methods like SAMAdp,\nSAMedAdp, and SAM-MedIA similarly face challenges in\naccurately delineating the lumens, with occasional missing\nsegments and imprecise boundaries."}, {"title": "E. Ablation Study", "content": "In this ablation study, we aimed to evaluate the effectiveness\nof different configurations of the VesselSAM (Segment\nAnything Model) on medical image segmentation tasks,\nparticularly focusing on vessel segmentation. We conducted a\nseries of comprehensive ablation experiments to evaluate the\nimpact of key components in the model, including the Atrous\nAttention Module and the LoRA rank. These experiments help\nin understanding how the individual elements contribute to the\noverall performance of the model in terms of segmentation\naccuracy, using the Dice score as the primary evaluation\nmetric.\nIn this ablation study, we aimed to evaluate the effectiveness\\"}]}