{"title": "IMAGE WATERMARKS ARE REMOVABLE USING CONTROLLABLE REGENERATION FROM CLEAN NOISE", "authors": ["Yepeng Liu", "Yiren Song", "Hai Ci", "Yu Zhang", "Haofan Wang", "Mike Zheng Shou", "Yuheng Bu"], "abstract": "Image watermark techniques provide an effective way to assert ownership, deter misuse, and trace content sources, which has become increasingly essential in the era of large generative models. A critical attribute of watermark techniques is their robustness against various manipulations. In this paper, we introduce a watermark removal approach capable of effectively nullifying the state of the art watermarking techniques. Our primary insight involves regenerating the watermarked image starting from a clean Gaussian noise via a controllable diffusion model, utilizing the extracted semantic and spatial features from the watermarked image. The semantic control adapter and the spatial control network are specifically trained to control the denoising process towards ensuring image quality and enhancing consistency between the cleaned image and the original watermarked image. To achieve a smooth trade-off between watermark removal performance and image consistency, we further propose an adjustable and controllable regeneration scheme. This scheme adds varying numbers of noise steps to the latent representation of the watermarked image, followed by a controlled denoising process starting from this noisy latent representation. As the number of noise steps increases, the latent representation progressively approaches clean Gaussian noise, facilitating the desired trade-off. We apply our watermark removal methods across various watermarking techniques, and the results demonstrate that our methods offer superior visual consistency/quality and enhanced watermark removal performance compared to existing regeneration approaches.", "sections": [{"title": "1 INTRODUCTION", "content": "As the large generative models continue to advance, the realism of AI-generated content has reached unprecedented levels. Those AI-generated contents are nearly indistinguishable from content created by humans. While this technological progress brings about excitement and efficiency, the difficulty of distinguishing AI-generated and human-made content can lead to severe problems, such as the spread of misinformation and the potential erosion of trust in digital media. In response, watermarking AI-generated content (Wen et al., 2024; Zhao et al., 2023a; Saberi et al., 2023; Zhao et al., 2023b; Lukas et al., 2023; Yang et al., 2024; Ci et al., 2024b;a; Kirchenbauer et al., 2023; Zhang et al., 2024a; Liu & Bu, 2024; Rezaei et al., 2024) has emerged as an effective solution, providing a proactive and reliable method to embed hidden information into AI-generated content for identification and traceability.\nRecently, several promising watermarking methods for AI-generated images have emerged, typically embedding watermarks by perturbing the pixels or latent representations of the image. Low perturbation watermarks (Fernandez et al., 2023; Zhu, 2018; Fernandez et al., 2022; Zhang et al., 2019; Cox et al., 2007) lead to a small l2 distance between watermarked and un-watermarked images in both pixel and latent space, making them potentially more vulnerable. On the other hand, watermark methods that induce high perturbation (Saberi et al., 2023; Zhao et al., 2023a) usually significantly modify the image or its representation, thereby resulting in enhanced robustness against various malicious manipulations, as shown in Figure 1. The robustness of image watermarks, being a crucial attribute, is targeted by numerous attacks aimed at removing the watermark."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 IMAGE WATERMARK METHODS", "content": "Watermarking techniques offer an active and reliable method to trace the source of images or protect copyright. Traditional methods embed watermark information directly into the generated images (Rouhani et al., 2018; Chen et al., 2019; Jia et al., 2021). With the development of large generative models, such as Stable Diffusion (Rombach et al., 2022), watermark information can now also be integrated seamlessly into the process of creating digital content. We broadly categorize watermarking methods into post-hoc methods and in-generation methods based on the stage at which the watermark is embedded. Post-hoc methods embed watermarks into a given image using techniques such as encoder-decoder (e.g., HiDDeN (Zhu, 2018), Stegastamp (Tancik et al., 2020)), optimization (e.g., SSL (Fernandez et al., 2022)), or wavelet transforms (e.g., DwtDctSvd (Cox et al., 2007)). In contrast, in-generation methods introduce watermarks during image generation by modifying components of the generative model, such as initial noise (e.g., Tree-Ring (Wen et al., 2024), RingID (Ci et al., 2024b)) or the VAE decoder (e.g., StableSignature (Fernandez et al., 2023), WMAdapter (Ci et al., 2024a)). We evaluated the effectiveness of our proposed methods on both post-hoc and in-generation watermarking methods."}, {"title": "2.2 IMAGE WATERMARK REMOVING METHODS", "content": "Robustness, a crucial attribute of image watermarking, is assessed through a range of watermark removal attacks (Hu et al., 2024; Kassis & Hengartner, 2024), including editing attacks, regeneration attacks, and adversarial attacks. The editing attack comprises typical image manipulations such as cropping, compression, rotation, brightness/contrast adjustments, and the addition of Gaussian noise. These manipulations simulate common alterations that images might undergo in practice. Most existing watermarking methods demonstrate robustness against these basic operations. Recently, regeneration attack (Zhao et al., 2023a; Saberi et al., 2023), as a no-box attack, proposes to remove the image watermark through the noising and denoising process of pre-trained diffusion model or encoding and decoding process of variational autoencoder (VAE). The regeneration attack demonstrates a powerful ability to remove watermarks for methods that cause minimal perturbation to the pixel or latent space of a watermarked image. However, it tends to be less effective under limited noising steps for watermarking methods that induce high perturbation. For those high perturbation watermarking methods, the adversarial attacks (Saberi et al., 2023; Lukas et al., 2023; Jiang et al., 2023) provide an effective method to generate adversarial images that evade watermark detection. These methods treat the watermark detector as a classifier and introduce adversarial perturbations to the watermarked image through optimization to deceive the detector. However, this strategy demands more unrealistic capabilities from attackers, including knowledge of the watermarking method (Lukas et al., 2023), access to the watermarked large generative models in the white-box setting (Saberi et al., 2023), or the ability to make multiple uninterrupted queries to the API of watermark detector (Jiang et al., 2023). Moreover, these methods are image-specific and watermark-specific, which means that attackers need to tailor the perturbation for each image according to different watermarking methods. This process is both time-consuming and computationally intensive. In this paper, we focus on the regeneration attack and propose a controllable regeneration attack combined with our proposed control techniques."}, {"title": "2.3 DIFFUSION MODELS", "content": "Diffusion probability models (Song et al., 2020; Ho et al., 2020) are advanced generative models that restore original data from pure Gaussian noise by learning the distribution of noisy data at various levels of noise. With their powerful capability to adapt to complex data distributions, diffusion models have achieved outstanding achievements in several domains, including image synthesis (Rombach et al., 2022; Peebles & Xie, 2023), image editing (Brooks et al., 2023; Hertz et al., 2022; Zhang et al., 2024c;d), and even 3D content creation (Poole et al., 2022). Among them, Stable Diffusion (Rombach et al., 2022) (SD), a notable example, employs a UNet architecture and iteratively generates images with impressive text-to-image capabilities through extensive training on large-scale text-image datasets. Alongside these developments, controllable image generation has seen enhancements from methods like ControlNet (Zhang & Agrawala, 2023) and T2I-adapter (Mou et al., 2023), which utilize multimodal inputs such as depth maps and segmentation maps to significantly increase the controllability over the generated images. Furthermore, subject-driven image generation techniques now range from those requiring test-time fine-tuning (Gal et al., 2022; Ruiz et al., 2022; Kumari et al., 2023; Hu et al., 2022) to those operating entirely fine-tuning-free (Ye et al., 2023; Zhang et al., 2024b), each offering varying degrees of adaptability and computational demand. In this paper, we propose an image watermarking removal algorithm for watermark removal based on ControlNet Zhang & Agrawala (2023) and IP-Adapter Ye et al. (2023)."}, {"title": "3 THE PROPOSED CONTROLLABLE REGENERATION ATTACK", "content": ""}, {"title": "3.1 OVERVIEW", "content": "The core idea behind our proposed method is to regenerate the watermarked image starting from clean noise. A controllable diffusion model is then designed to maintain the consistency between the watermarked image and the cleaned image during the denoising process, with the watermarked image serving as a conditional input.\nThe workflow of our method is presented as Figure 3. Specifically, given a watermarked image \\(x_w \\in \\mathbb{R}^N\\), starting latent representation \\(z\\), and the generation function \\(G : \\mathbb{R}^n \\times \\mathbb{R}^N \\rightarrow \\mathbb{R}^N\\), we can obtain the cleaned image:\n\\[x = G(z, x_w, \\hat{x_w}),\\]\nwhere \\(\\hat{x_w} = f(x_w) \\in \\mathbb{R}^N\\) represents the edge-detected image obtained from \\(x\\) using edge detection algorithm. For CtrlRegen, input \\(z\\) is \\(\\in \\sim \\mathcal{N}(0, I_n)\\). For CtrlRegen+, input \\(z\\) is given by \\(z_{t^*} = \\sqrt{\\bar{\\alpha}_{t^*}} z_0 + \\sqrt{1 - \\bar{\\alpha}_{t^*}} \\epsilon\\), where \\(t^* \\in [0, T]\\) is the number of noising steps, \\(z_0 = E(x_w)\\) is the latent"}, {"title": "3.2 REGENERATION CONTROL MODEL TRAINING", "content": ""}, {"title": "3.2.1 SEMANTIC CONTROL", "content": "One key challenge in semantic control is preserving the semantic content of watermarked images while effectively destroying the watermark information. Previous uncontrolled regeneration methods achieved this by initially destructing the \\(z_0\\) with random Gaussian noise via the forward process of the diffusion model, followed by reconstructing the image through an uncontrolled reverse process. While this method of destructing through a limited number of noising and denoising steps proves effective for removing low perturbation watermarks, it falls short in eliminating watermark that causes high perturbation in the latent space. Drawing inspiration from SD model, which employs a text encoder to convert text prompts into text embeddings and then using the cross-attention mechanism (Vaswani, 2017) to ensure the generated image semantically aligns with the text, we compress the watermarked image into an image embedding that preserves only semantic content. This embedding is then used to control the generation process via cross-attention, ensuring the regenerated image retains semantic accuracy without the watermark information.\nWe train a semantic control adapter to facilitate semantic control as depicted in the upper branch of Figure 3, which includes an image encoder, projection network, and newly implemented decoupled cross-attention layer. Our training approach for the projection network and cross-attention layer draws on strategies similar to those used in the IP-Adapter (Ye et al., 2023). Specifically, we employ pre-trained DINOv2 (Oquab et al., 2023) as our image encoder to extract the image feature from the watermarked image, capitalizing on its high-performance capability to extract rich visual features. A trainable projection network is then used to transform the extracted image feature into image embedding, which is used to control the generation through cross-attention in the following step."}, {"title": "3.2.2 SPATIAL DISTRIBUTION CONTROL", "content": "The semantic control adapter facilitates a coarse-grained semantic content alignment between the watermarked image and the regenerated image, ensuring that most of the semantic content is preserved. However, it struggles to control the finer details and layout during the generation process. To address this, enhancing spatial control of regeneration is another key challenge. Incorporating an edge-detected image as an additional condition offers an effective solution, as it provides crucial spatial information without introducing extraneous watermark information. To better leverage spatial information from the edge-detected image, we propose to use a spatial control network. This network is designed to extract spatial features, which are then integrated into the denoising process of U-Net, thus enhancing spatial distribution in the regenerated image.\nAn Edge-detected image emphasizes the boundaries and contours within an image by identifying the rapid changes in intensity, which correspond to object edges. Typically rendered as a binary image, it features edges marked in white against non-edge areas in black. Specifically, we use Canny edge images extracted from the watermarked image using the Canny detection method (Canny, 1986). In our method, we adopt the ControlNet (Zhang & Agrawala, 2023) structure for our spatial control network, as shown in the lower branch of Figure 3. After applying the spatial control network to U-Net, the output of the neural blocks within U-Net is expressed as:\n\\[\\zeta(z_i, \\Phi_{\\theta_e,\\theta_p}(x), \\hat{x}, t) = F_{\\theta_{U}}(z_i, \\Phi_{\\theta_e,\\theta_p}(x), t) + H_{\\theta_{C}}(\\hat{x}^l, t),\\]\nwhere \\(z_i\\) is the intermediate representation between different neural blocks within U-Net, \\(x\\) is the original image, \\(\\hat{x} = f(x)\\) represents the edge-detected image, \\(F_{\\theta_{U}}(\\cdot)\\) is the original neural block of U-Net, \\(H_{\\theta_{C}}(\\cdot)\\) represents the neural blocks and convolution layers of spatial network, \\(\\hat{x}^l\\) refers to the intermediate representation within the spatial control network that corresponds dimensionally with \\(z_i\\), in which \\(\\hat{x}^0\\) is derived from \\(\\hat{x}\\) through an encoder and a convolution layer.\nTo enhance compatibility and integration among the components, we combine the semantic control adapter, spatial control network, and SD within a unified framework. We fix the parameters of the already trained semantic control adapter and the SD and tune the spatial control network \\(\\theta_{C}\\) to optimize its performance in conjunction with the other components. Now, with both image embedding and edge-detected image as conditions, the training objective is:\n\\[\\mathcal{L}_{C} = \\mathbb{E}_{x, \\epsilon, \\epsilon \\sim \\mathcal{N}(0, I_n), t}[||\\epsilon - \\epsilon_{\\theta}(z_t, \\Phi_{\\theta_e,\\theta_p}(x), \\hat{x}, t)||^2],\\]\nwhere \\(\\epsilon_{\\theta}(z_t, \\Phi_{\\theta_e,\\theta_p}(x), \\hat{x}, t)\\) is the U-Net with \\(z_t\\) and \\(t\\) as input and \\(\\Phi_{\\theta_e,\\theta_p}(x)\\) and \\(\\hat{x}\\) as condition, \\(\\theta\\) includes both \\(\\theta_{\\alpha}\\) and \\(\\theta_{C}\\)."}, {"title": "3.3 REGENERATION WITH CONTROL", "content": "Once our semantic control adapter and spatial control network are fully trained, they are ready to be deployed for direct watermark removal, requiring only a single watermarked image. The entire inference process of CtrlRegen is outlined in Algorithm 1. We sample the initial noise from a pure Gaussian distribution. Then the extracted image embedding and edge-detected image serve as conditions, providing the necessary semantic and spatial information. Two well-trained networks are integrated with the backbone SD to denoise across T timesteps. Finally, the cleaned image is obtained by decoding the denoised latent representation.\nThe inference process of CtrlRegen+ is detailed in Algorithm 2. Unlike CtrlRegen, which utilizes pure Gaussian noise as a starting point, CtrlRegen+ first adds noise to the latent representation and then uses this noised representation as a starting point to reconstruct the image. The noising step can be selected based on the strength of different watermark methods."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENT SETTING", "content": "Datasets. We evaluate our watermark removal performance using two datasets. For the post-hoc watermarking methods, we sample 1000 real photos from the MIRFLICKR (Huiskes & Lew, 2008), which consists of a comprehensive image collection sourced from the social photography platform Flickr. For the in-generation watermarking methods, we sample 1000 prompts from a large-scale text-to-image prompt dataset, DiffusionDB (Wang et al., 2022), to generate watermarked images using generative models. Additionally, we train the semantic control adapter using 10 million images sampled from LAION-2B (Schuhmann et al., 2022) and COYO-700M 1. The spatial control network is trained using 118k image-canny pairs from MSCOCO (Lin et al., 2014).\nImage Watermark Methods. To demonstrate the effectiveness of our watermark removal method, we evaluate it against seven diverse watermarking methods, including TreeRing (Wen et al., 2024), StableSignature (Fernandez et al., 2023), StegaStamp (Tancik et al., 2020), SSL (Fernandez et al., 2022), RivaGAN (Zhang et al., 2019) and DwtDctSvd (Cox et al., 2007). These methods encompass both low and high-perturbation watermarks, providing a comprehensive evaluation of our approach's capabilities.\nImage Watermark Removal Baselines. Our method is compared with two regeneration methods Regen (Zhao et al., 2023a) and Rinse (An et al., 2024). Regen employs the diffusion model to regenerate watermarked images through a process of noising and denoising. Rinse iteratively applies the Regen method multiple times to improve the efficacy of watermark removal. For Regen, we set the noising and denoising step as 70, and Rinse implements the Regen twice.\nImplementation Details. We employ Stable Diffusion-v1.5 (Rombach et al., 2022) as the backbone for our model, maintaining its parameters in a frozen state to preserve the original capabilities. For the semantic control adapter, we integrate DINOv2-giant (Oquab et al., 2023) as the image encoder, also keeping its parameters frozen to leverage its pre-trained strengths. The training of the semantic control adapter is conducted on 8 NVIDIA A100 GPUs, and the batch size is set to 8 per GPU."}, {"title": "4.2 RESULTS", "content": "Watermark Removal Performance of CtrlRegen. Table 1 presents the watermark detection performance across various watermarking methods, both before and after the application of different watermark removal attacks. It shows that all watermarking methods have great detection performance before attacks. For low perturbation watermarking methods such as DwtDctSvd, RivaGAN, SSL and StableSignature, the Regen and Rinse exhibit effective watermark removal performance. However, for high perturbation methods like StegaStamp and TreeRing, Regen and Rinse become ineffective. This is because StegaStamp and TreeRing induce significant disturbances in both pixel-space and latent-space. Specifically, Regen's noising and denoising process is applied to the latent representation of the watermarked image, and the limited number of noising steps is insufficient to completely disrupt the watermark structure embedded within the latent representation. This limitation hinders its ability to effectively manage substantial perturbations in the latent space. Rinse extends the approach by running Regen multiple times in an attempt to further disrupt the watermark structure. Despite the enhancement, the effectiveness is still limited, especially for StegaStamp, which still has 0.94 TPR@1%FPR. Our method regenerates the watermarked image by starting from a clean Gaussian noise in the latent space, which inherently contains no watermark structure within the latent representation. Therefore, our watermark removal approach achieves notable performance, reaching 0.01 and 0.12 TPR@1%FPR for StegaStamp and TreeRing, respectively.\nVisual Similarity and Quality of CtrlRegen. The regenerated images are assessed from two key aspects: visual similarity and quality. Table 1 presents these measurements for various watermark removal methods. For visual similarity, our method achieves a lower CLIP-FID score compared to Regen and Rinse. This lower score suggests that the distribution of our regenerated images more closely approximates that of the original watermarked images, reflecting better preservation of visual characteristics and overall image integrity. Our method exhibits a lower PSNR score for pixel-level measurement. However, it does not necessarily indicate severe degradation in similarity. The changes that lead to a lower PSNR can actually harmonize well with the content of the image without making the regenerated image appear unnatural. On the contrary, although the Regen and Rinse may achieve higher PSNR scores, it results in visible artifacts and degradation in the regenerated images compared to the original watermarked images, as illustrated in Figure 4. To further substantiate that images regenerated by our method exhibit superior image quality, we employ two image quality measurements: Q-Align and LIQE. These metrics are used to quantitatively assess and compare the quality of the regenerated images. From Table 1, our method achieves better image quality compared to Regen and Rinse.\nAdjustable and Controllable Regeneration. We evaluate our CtrlRegen+ against Regen across various aspects, including watermark removal effectiveness, visual resemblance, and image quality, by varying the number of noising steps. For high-perturbation watermarks, such as StegaStamp and TreeRing, we set the noising steps to be {100, 200, 300, 400, 500, 1000} and sample from pure Gaussian noise. We do not set a noising step number larger than 500 for Regen, as at this number of noising steps, the noised latent representation approaches pure Gaussian noise. Consequently, the uncontrolled regeneration process would likely produce an image completely unrelated to the original watermarked image."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduce a more powerful regeneration attack that, combined with our proposed control techniques, effectively removes image watermarks by thoroughly eliminating the watermark information in the latent space. Unlike existing regeneration attacks that apply only a limited number of noising steps to the latent representation and thus can only be effective on low perturbation watermarks, our approach starts with clean noise. This method allows us to successfully invalidate even high-perturbation watermarks. Moreover, The introduced adjustable and controllable regeneration scheme allows attackers to tailor the attack strength according to the robustness of different watermarks. Our experiments demonstrate improved visual consistency and image quality compared to existing regeneration attacks under the same attack performance. Our attack is a no-box approach, meaning that the attacker only needs one watermarked image to remove watermarks without requiring knowledge of the watermarking scheme, detection rules, or access to the detector. By demonstrating the ability to defeat robust watermarking techniques, we highlight the urgent need for developing stronger watermarking solutions that can withstand these types of attacks."}]}