{"title": "Communication-Efficient Personalized Federal Graph Learning via Low-Rank Decomposition", "authors": ["Ruyue Liu", "Rong Yin", "Xiangzhen Bo", "Xiaoshuai Hao", "Xingrui Zhou", "Yong Liu", "Can Ma", "Weiping Wang"], "abstract": "Federated graph learning (FGL) has gained significant attention for enabling heterogeneous clients to process their private graph data locally while interacting with a centralized server, thus maintaining privacy. However, graph data on clients are typically non-IID, posing a challenge for a single model to perform well across all clients. Another major bottleneck of FGL is the high cost of communication. To address these challenges, we propose a communication-efficient personalized federated graph learning algorithm, CEFGL. Our method decomposes the model parameters into low-rank generic and sparse private models. We employ a dual-channel encoder to learn sparse local knowledge in a personalized manner and low-rank global knowledge in a shared manner. Additionally, we perform multiple local stochastic gradient descent iterations between communication phases and integrate efficient compression techniques into the algorithm. The advantage of CEFGL lies in its ability to capture common and individual knowledge more precisely. By utilizing low-rank and sparse parameters along with compression techniques, CEFGL significantly reduces communication complexity. Extensive experiments demonstrate that our method achieves optimal classification accuracy in a variety of heterogeneous environments across sixteen datasets. Specifically, compared to the state-of-the-art method FedStar, the proposed method (with GIN as the base model) improves accuracy by 5.64% on cross-datasets setting CHEM, reduces communication bits by a factor of 18.58, and reduces the communication time by a factor of 1.65.", "sections": [{"title": "1. Introduction", "content": "In the modern era of pervasive data generation, graphs have emerged as a powerful tool for representing complex, relational information across various domains, such as social networks, healthcare, finance, and transportation. As data becomes increasingly decentralized, particularly across heterogeneous environments, the need to effectively fuse information from multiple, distributed sources has become paramount. Graph Neural Networks (GNNs) [1, 2] have shown great potential in leveraging the inherent topology of graph structures for deep learning tasks. These techniques have found extensive applications in diverse fields, including drug discovery [3, 4, 5], neuroscience [6, 7], social networks [8, 9], knowledge graphs [10, 11], recommender systems [12, 13, 14], and traffic flow prediction [15, 16]. Despite significant advancements, most methods require centralized storage of large graph datasets on a single machine for training. Due to data security and user privacy concerns, data owners (clients) may be reluctant to share their data, leading to data segregation issues [17]. Furthermore, graph data stored by different clients often exhibit non-identical and independently distributed (non-IID) characteristics, exacerbating data isolation problems [18]. This non-IID property can manifest as differences in node feature distributions between graph structures or clients.\nFederated Learning (FL) [19, 20, 21] is a distributed collaborative machine learning paradigm that offers a promising method to address the challenge of data isolation. It enables participants (i.e., clients) to train machine learning models without sharing their private data. Thus, combining FL with graph machine learning presents a promising solution to this problem [22, 23]. However, the non-IID nature of data makes it difficult for a single global model in traditional FL methods to fit each client's local data [24, 25]. Additionally, data heterogeneity between clients introduces client drift in updates, leading to slower convergence [26]. The concept of Personalized Federated Learning (PFL), which focuses on learning individualized models instead of a single global model, has been proposed to address this issue. Existing PFL methods can be classified into three types: hierarchical separation [27, 28], regularization [29, 26], and personalized aggregation [30, 31]. These methods explicitly or implicitly involve additional personalization parameters in their local models, with these parameters playing a dominant role in the parameter space [32]. While this addresses the problem of data heterogeneity to some extent, a significant challenge in federated graph learning is the communication cost, especially for large models. Although PFL has been extensively studied, most works focus on Euclidean data such as images. In contrast, personalized federated graph learning (PFGL) for non-Euclidean graph data is still in its early stages. It remains unclear how to effectively represent global knowledge of graphs with global components, personalize models with local components, and merge them to optimize the expressive power of local models while ensuring efficient communication.\nTo tackle the challenges, we propose a communication-efficient personalized communication-efficientfederated graph learning (CEFGL) method, as illustrated in Figure 1. This method decomposes the local model into a low-rank generic model, which captures global knowledge reflecting the commonalities of clients, and a sparse private model, which comprises personalized parameters that complement the global knowledge. We employ a dual-channel encoder to learn sparse local knowledge in a personalized manner and low-rank global knowledge in a shared manner. Inspired by Scaffnew [33], we introduce appropriate regularization and correction terms for the low-rank and sparse components in local and global optimization problems to accelerate local training. Additionally, we utilize quantization compression to reduce the number of model parameters, reducing the communication cost for uplink and downlink. Compared to the state-of-the-art method, our proposed method demonstrates superior classification accuracy while significantly reducing communication costs. Our work has three key contributions:\n\u2022 We propose a communication-efficient personalized federated graph learning algorithm (CEFGL) for highly heterogeneous cross-dataset scenarios. The proposed method significantly enhances model performance amidst data heterogeneity while reducing communication bits and time. This highlights our unique contribution to the field, effectively addressing data heterogeneity and optimizing communication efficiency in federated graph learning.\n\u2022 By utilizing low-rank decomposition techniques, our method efficiently addresses data heterogeneity. Shared knowledge across clients is encapsulated in low-rank components, while sparse components capture local-specific knowledge, resulting in significant performance improvements under non-IID data distributions. Moreover, our method integrates accelerated local training and compression techniques, substantially reducing communication costs while maintaining high performance. This dual benefit ensures high performance and efficiency, particularly effective in diverse and resource-constrained environments.\n\u2022 Extensive experiments show that our method achieves optimal accuracy across sixteen real datasets in both IID and non-IID settings. Compared to the state-of-the-art method FedStar, our proposed method achieves a 5.64% increase in graph classification accuracy with significantly fewer communication bits and less communication time in the cross-dataset setting CHEM (see Figure 6).\nThe rest of this paper is organized as follows. Section 2 provides an overview of related work on federated learning and federated graph learning. Section 3 describes the prior knowledge used in this paper. Section 4 presents the details of the proposed method. Section 5 describes the experiments conducted to evaluate the proposed method. Finally, Section 6 summarizes the contributions and outlines future work."}, {"title": "2. Related Work", "content": "2.1. Federated Learning\nFederated Learning (FL) methods [34, 29] aim to learn a global model by aggregating the local models of all clients. However, achieving a global model that generalizes well to each client is challenging due to data heterogeneity. Personalized Federated Learning (PFL) shifts the focus from a traditional server-centric federation to a client-centric one, better suited for non-IID data. Existing PFL methods can be categorized into three types. The first type of method is based on regularization. Scaffold [26] introduces proximal regularizers to mitigate client-side drift and accelerate training. FedProx [29] incorporates a regularization term to balance local training with the global model, promoting faster convergence. These methods improve training and inference efficiency by controlling model parameters. However, selecting regularization parameters often requires tuning through cross-validation. Inappropriate parameter choices can over-penalise personalised parameters and lead to over-fitting of local data. The second type involves personalized aggregation methods to learn local models. FedAMP [35] generates aggregated models for individual clients using attention-inducing functions and personalized aggregation. APPLE [30] performs local aggregation within each training batch rather than just local initialization. However, these methods are typically designed for cross-silo FL settings, which require all clients to participate in each iteration. The third type separates the global and personalization layers, combining global knowledge with models personalized for each client. This paper focuses on this type of method, which is widely studied for its adaptability and learning effectiveness. For instance, FedRep [27] divides the backbone into a global model and client-specific headers, fine-tuning the headers locally. pFedMe [28] learns additional personalized models for clients using Moreau envelopes. FedSLR [31] employs a two-stage proximity algorithm for optimization. Although these methods can help the model better adapt to client data, they lead to increased communication overhead as parameters are transmitted separately.\nCompared to FedRep, pFedMe, and FedSLR, our method downloads a low-rank and compressed global model from the server, significantly reducing communication complexity in upstream and downstream links. We also employ accelerated local training techniques, effectively reducing communication frequency. Additionally, while the methods above are tailored for images and unsuitable for complex non-Euclidean data like graphs, our method targets highly heterogeneous cross-domain datasets. It effectively reduces communication costs while maintaining high accuracy, making it more suitable for highly heterogeneous cross-domain graph tasks.\n2.2. Federated Graph Learning\nWhile numerous studies have investigated federated learning, federated graph learning (FGL) remains underexplored. Unlike Euclidean data, such as images, graph data is inherently more complex, introducing new challenges and opportunities for FGL [36, 37]. FGL enables clients to train robust GNN models in a distributed manner without sharing private data, thus expanding its application scenarios. A common strategy for training personalized federated graph learning models is client-side clustering. For instance, in IFCA [38], clients are dynamically assigned to multiple clusters based on the latest graph model gradient. However, a drawback of this method is that the clustering results are significantly influenced by the latest gradients from the clients, which are often unstable during local training. To address this issue, GCFL [24] considers a series of gradient specifications in the client cluster, and CTFL [39] updates the global model based on representative clients for each cluster. Maintaining a shared global model in each cluster increases the number of parameters and computational requirements as the number of clients grows, leading to poor scalability. Recent approaches have focused on reducing communication costs while addressing non-IID challenges. For instance, FedStar [40] mitigates the non-IID issue by employing feature decoupling. Meanwhile, FedGCN [41] reduces communication overhead by minimizing the frequency of communication with a centralized server and incorporates homomorphic encryption to further enhance privacy. Despite these advancements, these methods face challenges in highly heterogeneous cross-domain scenarios, as they can only fine-tune global model parameters to fit local data.\nCEFGL significantly reduces the server's computational cost compared to clustering-based multi-model FGL methods, as it only requires generating a unified global model for all clients. Additionally, compared to FGL methods that only adjust local models, the proposed method employs a hybrid model that combines low-rank global components with sparse personalized components. This hybrid model is more adaptable to the highly heterogeneous cross-dataset graph learning task."}, {"title": "3. Preliminary", "content": "Graph Neural Networks Let G = (V,E) denote a graph, where V is the set of N nodes, and E is the set of edges. The presence of edges is represented by an adjacency matrix A, where Aij \u2208 {0,1} indicates the relationship between nodes vi and vj in V. Additionally, the graph G is associated with a node feature matrix X \u2208 RN\u00d7d, where d is the feature dimension. Graph Neural Networks (GNNs) update the embedding of a given node by aggregating information from its neighbouring nodes using the following function:\nX(l+1) = GNN(A, Xl, \u03c9),\nwhere GNN(\u00b7) denotes the graph aggregation function, which can be mean, weighted average, or max/min pooling methods. Here, Xl represents the node embeddings in the l-th layer, and \u03c9 is the set of trainable model weights. For graph classification tasks, a graph-level representation hg can be pooled from node representations:\nhg = readout(X),\nwhere readout(\u00b7) is a pooling function (e.g., mean pooling or sum pooling) that aggregates the embeddings of all nodes.\nFederated Learning In Federated Learning, we consider a set of K clients C = {ci}i=1K. Each client ci owns a private dataset Di = {xj, yj}j=1Ni sampled from its data distribution, where xj is the feature vector of the j-th data sample and yj is the corresponding label. Here, Ni = |Di| represents the number of data samples on client ci, and N = \u2211i=1K Ni is the total number of data samples across all clients. Let li denote the loss function parameterized by \u03c9 on client ci. The goal of FL is to optimize the overall objective function:\nmin \u03c9 \u2211i=1K NiN Li(\u03c9) = min \u03c9 1N \u2211i=1K \u2211j=1Ni lj (\u03c9; xj, yj),\nwhere Li is the average loss over the local data on client ci."}, {"title": "4. Methodology", "content": "Based on the previous section, we propose a communication-efficient personalized federated graph learning algorithm (CEFGL). Drawing inspiration from RPCA [42], we decompose client model parameters into low-rank and sparse components. The low-rank component captures common knowledge among clients, while the sparse component captures unique, client-specific knowledge. To reduce communication costs and computational overhead, we integrate compression techniques and perform low-rank approximation only during the global aggregation phase. The overall pseudo-code for CEFGL is presented in Algorithm 1.\n4.1. Training and Fine-tuning on the Client Side\nIn robust principal component analysis (RPCA) [42], the matrix M is decomposed into a low-rank matrix W and a sparse matrix S, where W captures the principal components retaining the maximum information in M, while S represents the sparse outliers. Motivated by RPCA, we decompose the local model into a shared low-rank component and a private sparse component, expressed as:\nargminW,S rank(W) + \u03bd||S||0 s.t. M = W + S,\nwhere rank(W) is the rank of the matrix W, ||S||0 is the number of nonzero elements in S, and \u03bd is the parameter used to trade-off between these two terms. Since the rank function and the L0-norm are non-convex, we can minimize their convex proxies as\nargminW,S ||W||\u2217 + \u03bd||S||1 s.t. M = W + S,\nwhere ||W||\u2217 is the trace paradigm number of W, i.e., the sum of the singular values of W. ||S||1 is the L1-norm, i.e., the sum of the absolute values of all elements in S.\nLocal training In the local training phase, we first use a multilayer perceptron (MLP) to transform feature information into vectors of consistent dimensions. Subsequently, we employ a dual-channel encoder to represent the low-rank component of global knowledge and the sparse component of personalized knowledge. Inspired by Scaffnew [33], we introduce a correction term h to record the local gradient, accelerating local training and mitigating client drift. This term contains information about the client's model updating direction, akin to the client-drift value of the global model relative to the local model. The optimization process for learning the low-rank component is expressed as follows:\nWt+1 = Wt \u2212 \u03b7 (\u2207wfi(Wt; Di) \u2212 ht) + \u03b1\u03b7 ||\u0398t \u2212 Wt||2,\nwhere \u03b7 represents the learning rate. \u03b1 is a hyper-parameter used to balance the weight between the global model and the prior local model. \u0398t is the global model downloaded from the server in the t-th iteration, and Wt is the low-rank component on client i at the t-th iteration. The function f represents the neural network transformation, and the cross-entropy loss function is employed. Initially, W0 = \u03980.\nAfter completing the local training, we update the correction term in each round as follows:\nhti+1 = hti + 1\u03b7 (\u0398ti \u2212 Wti+1).\nFine-tuning In the fine-tuning phase, we freeze the low-rank component W learned in the local training phase and fine-tune the sparse component using the initialized global parameters to fuse personalized knowledge with global knowledge. The optimization process of sparse component learning can be represented as follows:\nSti+1 = Sparse(Sti \u2212 \u03b7\u2207sfi(\u0398ti + Sti; Di) + ||S||1),\nwhere Sparse(\u00b7) represents the sparsification function. We explore two types of sparsification: threshold sparsification and Top-k sparsification. Threshold sparsification involves setting model parameters with absolute values smaller than a given threshold \u00b5 to zero, thus achieving parameter sparsity. Top-k sparsification retains the k parameters with the largest absolute values in the parameter matrix, setting the rest to zero.\nRemark 1. To reduce the computational cost, we defer the intensive computation of low-rank decomposition to the aggregation phase, where the server performs it, typically having more computational resources. Although the parameter Wis not low-rank during local training, we refer to it as the low-rank component for clarity. In the fine-tuning phase, we optimize the sparse component S using stochastic gradient descent to integrate personalized knowledge. The sparse component, with few parameters, does not dominate the local model or cause overfitting. Despite applying L1 regularization in each local fine-tuning step, the overhead remains insignificant compared to low-rank regularization."}, {"title": "4.2. Aggregation on the Server Side", "content": "After completing the local training and fine-tuning, the client sends the trained low-rank component Wti and the updated correction term hti+1 to the server. The server aggregates the client models and performs the low-rank decomposition, which can be represented as follows:\n\u0398t+1 = Low-rank (\u2211i=1K |Di||D| Wti+1 \u2212 \u03b7 \u2211i=1K |Di||D| hti+1) , \nwhere K denotes the number of clients participating in training, |Di| denotes the total number of samples on client ci, |D| denotes the total number of samples on K clients, and Low-rank(\u00b7) denotes the low-rank regularization. In this paper, the low-rank method we use is truncated singular value decomposition (TSVD), i.e., only singular values larger than a specific threshold \u039b and their corresponding vectors are retained.\nRemark 2. During local training, when Eq. (6) reaches the optimal solution, we have hti = \u2207wfi(Wti). We substitute this relationship into Eq. (9). If the local training converges, i.e., Wti+1 = \u0398ti, then \u2207wfi(W) can be approximated as \u2207ofi(\u0398t). At this point, the optimization problem for global aggregation can be expressed as \u0398t+1 = Low-rank(\u0398t \u2212 \u03b7\u2207ofi(\u0398t)). This update essentially performs an unbiased gradient descent update of the loss function."}, {"title": "4.3. Quantization Compression", "content": "In the algorithm described above, the model after server aggregation is low-rank, reducing communication costs to some extent. However, since the local model uploaded by the client remains dense, the uplink communication cost is not reduced. To address this, we adopt a strategy of updating the local parameters several times before aggregation and integrating quantization compression into the algorithm.\nQuantization compression involves converting model parameters from raw floating-point values to representations using fewer bits. While this reduces the accuracy of parameter representation, it significantly reduces the storage space and memory footprint of the model, thereby enhancing its efficiency during inference. For any vector x \u2208 Rd, with x \u0338= 0 and a number of bits r > 0, its binary quantization Qr is defined componentwise as follows:\nQr(x) = (||x||2 \u00b7 sgn (xi) \u00b7 \u03bei(x, 2r))1\u2264i\u2264d,\nwhere \u03bei(x, 2r) is independent random variable and || \u00b7 ||2 is L2-norm. The probability distribution is given by\n\u03bei(x, 2r) = round( 2rxi||x||2 ) /2r.\nwhere round(\u00b7) means round to the nearest integer.\nOur proposed algorithm reduces the need for frequent client-server communication, significantly lowering communication frequency. Importantly, our method is less susceptible to data leakage than other methods, as the uncertain local step size prevents the server from accurately recovering the client's actual gradient. Additionally, clients use quantization compression to reduce parameter size from 32 bits to 4 bits before uploading them to the server. Similarly, the server compresses the model before sending it to the client. This compression is particularly beneficial for downloading the model from the server, which can be costly due to network bandwidth limitations. Overall, these strategies significantly reduce both uplink and downlink communication costs.\nRemark 3. The proposed method has several distinct advantages. First, our dual-channel encoder effectively captures global and personalized knowledge, ensuring that the model benefits from shared insights and client-specific information. In addition, introducing correction terms helps mitigate client-side drift and speeds up local training, thereby improving the overall convergence speed. By deferring the intensive computation of low-rank decomposition to the server, we leverage the superior computational resources of the server and significantly reduce the computational burden on clients.\nAnother highlight of CEFGL is the integration of quantization compression with multi-step local training. This significantly reduces the communication cost and enhances data security by preventing the server from accurately recovering the actual gradient from the client due to uncertainty in the local step size. These combined strategies make our method efficient, secure, and practical, standing out among existing federated learning methods."}, {"title": "5. Experimental Evaluation", "content": "5.1. Experimental Setup\nDatasets We utilize sixteen public graph classification datasets from four domains\u00b9. These include seven datasets from the small molecule domain (MUTAG, BZR, COX2, DHFR, PTC MR, AIDS, NCI1), three from bioinformatics (ENZYMES, DD, PROTEINS), three from social networking (COLLAB, IMDB-BINARY, IMDB-MULTI), and three from computer vision (Letter-low, Letter-high, Letter-med). To evaluate the performance of the proposed method, we adopt the same setup as GCFL [24], creating three IID and six non-IID setups. The IID setups consist of three single datasets (DD, NCI1, IMDB-MULTI). The non-IID setups consist of (1) cross-dataset setups based on the seven small molecule datasets (CHEM) and (2)-(5) setups based on data from two or three domains (SN_CV, CHEM_BIO, CHEM_BIO_SN, BIO_SN_CV) in cross-domain settings, along with (6) a cross-domain setup encompassing all datasets (ALL).\nBaselines We compare CEFGL with seven benchmark methods. These include (1) the standard FL algorithm FedAvg [19]; (2)-(3) some existing PFL solutions, namely FedProx [29] and FedPer [43]; and (4)-(7) four state-of-the-art FGL methods: GCFL [24], SpreadGNN [36], FedGCN [41], and FedStar [32]. We use the same experimental setup and data partitioning criteria for all methods to ensure fair comparisons and tune the hyperparameters to their best states.\nDefault Configuration For CEFGL, we employ three two-layer graph neural networks (GCN [1], GIN [44], GraphSage [45]) as two-channel encoders to train the low-rank component (W) and the sparse component (S). We set the low-rank hyperparameter \u00b5 to 0.0001, the sparse hyperparameter \u039b to 0.001, and the communication probability p to 0.5. During training, we set"}, {"title": "5.2. Performance Evaluation", "content": "To comprehensively evaluate the performance of CEFGL, we conducted experiments using three commonly used graph neural network architectures: GIN, GCN, and GraphSage.  and  present the average test accuracy and standard deviation for graph classification across six non-IID cross-dataset and three single-dataset settings. Our results consistently demonstrate that CEFGL outperforms all baseline methods across the three GNN architectures. In the highly heterogeneous cross-dataset graph learning task, CEFGL achieves an accuracy that is 2.11% higher than FedPerGCN, 2.82% higher than SpreadGNN, and 6.92% higher than GCFL. This significant improvement highlights the ability of CEFGL to effectively learn and generalize across diverse datasets, leveraging its dual-channel encoder to capture global and personalized knowledge efficiently. In the single dataset task, some personalization solutions exhibit performance degradation in the IID setting, failing to match the performance of global solutions like FedAvg. However, CEFGL maintains the highest performance in the IID setting, which indicates that its sparse personalization information effectively complements global information without overwhelming it, thereby preserving model generalization. This balance between personalization and generalization is crucial for maintaining high performance across different data distributions.\nThe superiority of CEFGL is further highlighted by its performance consistency across various setups. In non-IID settings, where data heterogeneity poses significant challenges, the architecture of CEFGL ensures effective communication and learning, reducing client drift and enhancing convergence. In IID settings, the ability of CEFGL to retain high accuracy showcases its adaptability and strength in more uniform data distributions. These comprehensive results underscore the effectiveness of CEFGL in both heterogeneous non-IID and IID settings. By consistently outperforming existing methods, CEFGL proves its superiority in federated graph learning, offering a reliable solution for real-world applications where data distribution can vary significantly."}, {"title": "5.3. Low-rank Ratios Analysis", "content": "To evaluate the impact of the low-rank hyperparameter \u00b5 on the performance of the algorithm, we vary \u00b5 and record the low-rank rate and classification accuracy of the model \u0398t for different values of \u00b5 using GIN as the backbone, as shown in . A \u00b5 value 0 indicates no low-rank regularization, resulting in dense models susceptible to non-IID data from other clients, leading to suboptimal performance in federated environments with heterogeneous data. As \u00b5 increases from 0, we observe an initial improvement in classification accuracy and significant reductions in communication cost and the number of parameters. This is because low-rank regularization helps filter out noise, capture common patterns in the data, and enhance the generalization of models across clients. For instance, the model balances compactness and representativeness at \u00b5 = 0.001, improving communication efficiency and accuracy. This optimal setting maximizes the extraction of global knowledge while maintaining a manageable number of parameters. However, if u is set too large, for instance, 0.01, the model becomes overly constrained by the low-rank regularization. This excessive constraint can cause the model to lose important information for accurate classification, resulting in significant performance degradation. In conclusion, the low-rank hyperparameter \u00b5 plays a crucial role in balancing the trade-off between communication cost and classification accuracy. By choosing an appropriate \u00b5, our method enhances the extraction of global knowledge and improves overall model performance, demonstrating the effectiveness of using low-rank regularization in federated graph learning."}, {"title": "5.4. Sparsity Ratios Analysis", "content": "To assess the impact of the sparsity of the personalization component, we fix the low-rank hyperparameter \u00b5 at 0.001 and vary the sparsity hyperparameter \u039b using GIN as the backbone. As shown in , a \u039b value of 0 results in a dense personalization component, leading to unsatisfactory model performance. Dense models tend to overfit the local data, which is particularly problematic in federated learning scenarios with heterogeneous data distributions. As \u039b increases, the number of parameters in the personalization model decreases, significantly improving accuracy. This improvement can be attributed to sparse regularization, which filters out redundant information, allowing the model to focus on the specific features of the data. We also evaluate sparsity using the Top-k sparsity method, where k = \u03b2 \u00d7 Ns and Ns denote the total number of parameters in the personalized model. As shown in Figure 2, the optimal performance can be achieved by using only 10% of the parameters. This suggests that appropriate sparse regularization enhances the performance of models. The model becomes more efficient and generalizable by effectively capturing local knowledge with fewer parameters.\nThe sparsity in the personalization component ensures that the model enhances the capture of local knowledge, thereby improving model performance. This is crucial in federated learning, where data from different clients can vary significantly. By controlling the sparsity of the personalization component, our method balances model flexibility with the need to maintain generalization across different data sources. In summary, these results highlight the crucial role of sparsity hyperparameters (\u039b or \u03b2) in the efficiency and effectiveness of personalization components in federated learning. This adaptability makes our method an excellent choice for federated learning scenarios with IID and non-IID data distributions."}, {"title": "5.5. Ablation Experiments", "content": "To verify the rationale and validity of the low-rank global component and the sparse personalized component, we compare several variants of CEFGL using GIN as the backbone. When the global component is zeroed out, severe performance is degraded. This is primarily due to the limited number of parameters and the absence of information exchange between clients. In this case, the local model relies solely on the sparse personalization component, which is insufficient for comprehensive learning, especially in a federated setting with diverse data distributions. Conversely, when the personalization components are zeroed out, the model becomes a standard federated graph learning algorithm. This method proves unsuitable for highly heterogeneous cross-dataset scenarios, as it lacks the adaptability provided by personalized components. Although the performance of this pure global component variant shows improvement over the pure personalization component, it still falls short of the hybrid model. The hybrid model, which integrates low-rank global and sparse personalized components, achieves superior performance. This configuration benefits from the complementary strengths of both components. The global component captures common patterns and facilitates information sharing across clients. In contrast, the personalized component adapts to local data variations, enhancing the overall flexibility and generalization capability of the model. These observations underscore that integrating low-rank global and sparse personalized components is crucial for achieving optimal performance, demonstrating the ability of the model to balance global knowledge extraction and local adaptation. This balanced method is particularly advantageous in federated learning with non-IID data."}, {"title": "5.6. Number of Local Iterations", "content": "We explore the effect of varying the expected number of local iterations on model performance. The expected number of localized iterations is defined as 1/p, where p denotes the communication probability. For instance, p = 0.1 means that the client communicates with the server, on average, once every ten localized iterations. To understand the impact of this parameter, we investigate the model accuracy and loss for p \u2208 {0.1, 0.2, 0.4, 0.6, 0.8, 1.0}."}, {"title": "5.7. Quantization", "content": "We investigate the application of quantization compression Qr(\u00b7) in CEFGL, varying the number of bits as r \u2208 {4, 8, 16, 32}. Using the quantization method proposed by Alistarh et al. [46], we present the results after 200 communication rounds. Using 4-bit quantization, which corresponds to a communication cost of only 12.5%, does not significantly degrade model performance. Accuracy and convergence speeds remain comparable to those achieved with higher bits quantization levels. It demonstrates that CEFGL effectively retains essential training information even with lower granularity parameters. Additionally, low-bit quantization reduces the amount of data transferred between the client and the server, significantly lowering communication costs.\nThe proposed method effectively reduces communication overhead through low-bit quantization without sacrificing model performance or convergence speed. This highlights the efficiency of our method, making it well-suited for large-scale federated learning deployments where communication efficiency is critical and enabling implementation in resource-constrained environments such as mobile and edge devices."}, {"title": "5.8. Clients Accidentally Dropping Out", "content": "Due to fluctuating network connections, clients in mobile environments may unexpectedly exit in one iteration and rejoin in another. To evaluate the performance of various FGL methods under these conditions, we conduct experiments. Unlike many existing methods with a fixed client drop rate, we sample the drop rate p from a Beta distribution in each iteration. Larger values of p indicate greater instability. The experimental results reveal several significant findings. The accuracy of most FGL methods decreases under unstable conditions. This is expected, as inconsistency in client participation undermines the ability of the model to aggregate information efficiently. However, CEFGL maintains the highest accuracy even under extremely unstable conditions, i.e., when p is sampled from \u03b2(10, 1). The ability of CEFGL to maintain its dominance and consistent performance in unstable environments can be attributed to two main factors:\n\u2022 Unforced aggregation: CEFGL does not require aggregation at every iteration, allowing the model to continue learning even when some clients are unavailable.\n\u2022 Sparse Personalisation Component: The sparse personalization component in CEFGL effectively complements global knowledge. By focusing on the critical local information, the model adapts to changing environments without relying solely on continuous client engagement.\nCEFGL is particularly suitable for FGL environments with unstable clients. It is designed to cope with the challenges posed by network fluctuations and to leverage sparse personalization to enhance the overall robustness and adaptability of the model. This makes CEFGL ideal for deployment in mobile and other dynamic environments where maintaining high accuracy and consistent performance is critical."}, {"title": "5.9. Further Discussion", "content": "Comparison of Performance and Communications Cost To evaluate the performance and communication cost of the proposed method, we compare various FGL methods under the cross-dataset setting CHEM over 200 rounds of communication. The comparison metrics include model performance , the total number of bits transmitted , and the wall-clock time per communication round . The experimental results demonstrate that the proposed method achieves superior performance with significantly reduced communication costs compared to baseline methods. For example, compared to the optimal method FedStar, SDGRL reduces the number of communication bits by a factor of 18.58, reduces the communication time by a factor of 1.65, and improves the classification accuracy by 5.64%. In summary, SDGRL consistently delivers the best performance with minimal memory and time consumption. Its ability to reduce communication costs significantly while improving classification accuracy makes it a robust and efficient choice for federated learning in heterogeneous and resource-constrained settings. The superior performance of our method in the cross-dataset setting reaffirms its potential for broader applications in various federated graph learning scenarios.\nVisualization We use t-SNE [47] to visualize the graph representations extracted from two variants of CEFGL, as shown in Figure 7. The graph representations extracted by the pure global model appear scattered and exhibit significant overlap between samples with different labels. This overlap results in a less distinct classification surface, making it challenging to distinguish between different classes accurately. The scattered distribution indicates that the pure global model struggles to capture the nuanced local variations in the data, leading to suboptimal performance. In contrast, CEFGL incorporating sparse components shows a much more structured representation. Points corresponding to different labels are more distinctly separated. This separation enhances the clarity of the classification boundaries, making it easier for the model to classify the samples accurately. The incorporation of sparse components enables the model to focus on the most critical local information, thereby improving the overall quality of the extracted representations."}, {"title": "6. Conclusion", "content": "In this work, we propose CEFGL, a communication-efficient personalized federated graph learning algorithm that decomposes model parameters into generic low-rank and private sparse components. We utilize a dual-channel encoder to learn sparse local knowledge in a personalized manner and low-rank global knowledge in a shared manner. To further optimize communication efficiency, we perform multiple local stochastic gradient descent iterations between communication stages and integrate efficient compression techniques into the algorithm. Extensive experiments demonstrate the superiority and communication efficiency of our method in highly heterogeneous cross-dataset scenarios. In future work, we plan to assess the extent of data leakage in the proposed method and enhance its security features by adopting advanced privacy-preserving techniques to protect sensitive information."}]}