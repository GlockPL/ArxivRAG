{"title": "DETPP: LEVERAGING OBJECT DETECTION FOR ROBUST LONG-HORIZON EVENT PREDICTION", "authors": ["Ivan Karpukhin", "Andrey Savchenko"], "abstract": "Forecasting future events over extended periods, known as long-horizon prediction, is a fundamental task in various domains, including retail, finance, healthcare, and social networks. Traditional methods, such as Marked Temporal Point Processes (MTPP), typically use autoregressive models to predict multiple future events. However, these models frequently encounter issues such as converging to constant or repetitive outputs, which significantly limits their effectiveness and applicability. To overcome these limitations, we propose DeTPP (Detection-based Temporal Point Processes), a novel approach inspired by object detection methods from computer vision. DeTPP utilizes a novel matching-based loss function that selectively focuses on reliably predictable events, enhancing both training robustness and inference diversity. Our method sets a new state-of-the-art in long-horizon event prediction, significantly outperforming existing MTPP and next-K approaches. The implementation of DeTPP is publicly available on GitHub 1.", "sections": [{"title": "1 INTRODUCTION", "content": "The world is full of events. Internet activity, e-commerce transactions, retail operations, clinical visits, and numerous other aspects of our lives generate vast amounts of data in the form of timestamps and related information. These data points, when ordered by their timestamps, form what are known as Event Sequences (ESs). In the era of AI, it is crucial to develop methods capable of handling these complex data streams. Event sequences differ fundamentally from other data types. Unlike tabular data (Wang & Sun, 2022), ESs include timestamps and possess an inherent order. In contrast to time series data (Lim & Zohren, 2021), ESs are characterized by irregular time intervals and structured descriptions of each event. These differences necessitate the development of specialized models and evaluation practices.\n\nThe primary task in the domain of ESs is sequence modeling, specifically predicting future event types and their occurrence times. Indeed, the ability to accurately forecast sequential events is vital for applications such as stock price prediction, personalized recommendation systems, and early disease detection. The simplified domain, represented as pairs of event types and times, is typically called Marked Temporal Point Processes (MTPP) (Rizoiu et al., 2017). Additionally, structured modeling of dependencies between different data fields (McDermott et al., 2024) can be considered an extension of MTPP.\n\nIn practice, a common question arises: what events will occur, and when, within a specific time horizon? Forecasting multiple future events poses unique challenges distinct from traditional next-event prediction tasks. The conventional approach to this problem typically involves the autoregressive application of next-event prediction models. While these models have demonstrated effectiveness in predicting the immediate next event, their performance tends to decline as the prediction horizon extends (Karpukhin et al., 2024).\n\nIn this paper, we show that autoregressive approach exhibit multiple problems when it comes to long-horizon prediction. In particular, an autoregressive model depends on its' own errors and falls into constant or repetitive outputs. The other disadvantage is low inference parallelism due to the dependencies on latest predictions. We overcome these limitations by proposing DeTPP, a novel"}, {"title": "2 BACKGROUND", "content": null}, {"title": "2.1 MARKED TEMPORAL POINT PROCESSES", "content": "Marked Temporal Point Process (MTPP) is a stochastic process that consists of a sequence of time-event pairs $(t_1, l_1), (t_2, l_2), . . .$, where $t_1 < t_2 < ...$ denote the times of events, and $l_i \\in \\{1, ..., L\\}$ are the corresponding event type labels. Traditional MTPP models primarily focus on predicting the next event in the sequence. A straightforward approach is to independently predict the time and type of the next event, while more sophisticated methods model the temporal dynamics of each event type separately. These models often rely on Temporal Point Processes (Rizoiu et al., 2017), which describe the stochastic generation of event times."}, {"title": "2.2 MODELS", "content": "Temporal Point Processes. Traditional Marked Temporal Point Process (MTPP) models, such as Poisson and Hawkes processes (Rizoiu et al., 2017), rely on strong assumptions about the underlying generative processes. Recent advancements have shifted towards more flexible and expressive models by leveraging neural architectures. These include classical Recurrent Neural Networks (RNNs) (Du et al., 2016; Xiao et al., 2017; Omi et al., 2019; Shchur et al., 2019), as well as more advanced architectures like transformers (Zhang et al., 2020; Zuo et al., 2020; Wang & Xiao, 2022; Yang et al., 2022). Additionally, continuous-time models such as Neural Hawkes Processes (Mei & Eisner, 2017), ODE-RNN (Rubanova et al., 2019), and their variants (Jia & Benson, 2019; De Brouwer et al., 2019; Kidger et al., 2020; Song et al., 2024; Kuleshov et al., 2024) have been developed to better capture the dynamics of event sequences. Moreover, some researchers have adapted generative models, including denoising diffusion and Generative Adversarial Networks (GANs), for use in TPPs (Lin et al., 2022).\n\nNext-K models. Previous research has explored models that predict multiple future events simultaneously, known as next-K models (Karpukhin et al., 2024). These models are typically trained using pairwise losses that match predicted events to ground truth events at corresponding positions.\n\nLong-horizon models. The problem of long-horizon prediction has been specifically addressed by HYPRO (Xue et al., 2022), which introduces a technique for selecting the best candidate from a set of generated sequences. HYPRO functions as a meta-algorithm that can enhance the performance of nearly any sequence prediction model. However, HYPRO's approach requires multiple generation runs for each prediction, which significantly reduces training and inference speed."}, {"title": "2.3 EVALUATION", "content": "MTPP models are typically evaluated based on their accuracy in predicting the next event (Xue et al., 2023). Time and type predictions are often assessed separately, with type prediction accuracy measured by error rates and time prediction accuracy evaluated using metrics like Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE). Recent advancements have introduced metrics such as Optimal Transport Distance (OTD) (Mei et al., 2019) and Temporal mAP (T-mAP) (Karpukhin et al., 2024), which are designed to evaluate long-horizon predictions by comparing predicted sequences to ground truth sequences within a specified horizon. In this work, we use all mentioned metrics to evaluate the performance of our proposed method."}, {"title": "3 LIMITATIONS OF AUTOREGRESSIVE INFERENCE", "content": "Previous studies have identified several challenges associated with autoregressive models for long-horizon predictions (Karpukhin et al., 2024). Notably, these models often exhibit reduced prediction uncertainty over extended horizons, even though the task becomes increasingly difficult. As illustrated in Figure 1.c, the predicted label sequences often exhibit a tendency towards constant or repetitive outputs. This behavior likely stems from the model's reliance on its own predictions as input for subsequent predictions, which can amplify errors and lead to the repetition of events. Interestingly, even the Next-K approach, which avoids autoregressive dependencies, suffers from similar repetitive patterns. This may be attributed to each head in the Next-K model predicting the entire distribution of labels, leading to a bias toward the most frequent classes during inference.\n\nIn contrast, our proposed DeTPP approach exhibits greater diversity in its predictions and, as we will demonstrate, achieves superior performance in long-horizon prediction tasks."}, {"title": "4 EVENT DETECTION WITH DETPP", "content": "In this section, we introduce DeTPP, a novel approach to MTPP modeling that leverages concepts from object detection (Carion et al., 2020). DeTPP is designed to predict K future events within a specified time horizon H, where K and H are hyperparameters of the model. The generative process of DeTPP is illustrated in Figure 2. Below, we provide a detailed description of the model, along with its training and inference procedures."}, {"title": "4.1 PROBABILISTIC EVENT MODEL", "content": "DeTPP captures the complexity of event sequences by modeling each component of an event using a probabilistic framework. Specifically, DeTPP predicts the probability of an event occurring, the distribution of event labels, and the distribution of the time shift relative to the last observed event.\n\nAs depicted in Figure 2, the probability $\\hat{O}$ of an event occurring is modeled using a neural network head with a sigmoid activation function. A separate head with softmax activation models the distribution $\\hat{p}(l)$ of event labels. For the time shift, we employ an approach similar to Mixture Density Networks (MDNs) (Bishop, 1994), modeling the time shift as a Laplace distribution with a unit scale parameter:\n\n$P(t) = \\frac{1}{2} e^{-|t-\\hat{t}|},$ (1)\n\nwhere $\\hat{t}$ is the model's predicted time shift. This formulation provides a probabilistic interpretation of the Mean Absolute Error (MAE) loss function. By combining these predicted probabilities, we"}, {"title": "4.2 HORIZON MODELING APPROACH", "content": "DeTPP is designed to predict K future events $\\{y_i\\}_{i=1}^K$ within the horizon H, defined as the interval (t, t + H), where t is the timestamp of the last observed event. The set of ground truth events within this horizon is denoted by $\\{y_i\\}_{i=1}^T$, where T may vary. The model aligns the predicted sequence with the ground truth sequence by finding the matching that minimizes the following loss function:\n\n$\\mathcal{L}(y, \\hat{y}) = \\underset{\\sigma \\in A}{\\min} \\sum_{i=1}^T \\mathcal{L}_{pair}(y_i, \\hat{y}_{\\sigma(i)}) + \\mathcal{L}_{BCE}(\\sigma, \\hat{y}),$ (4)\n\nwhere A is the set of all possible alignments between the ground truth and predicted sequences, and $\\sigma$ represents a specific alignment. The optimal matching is computed using the Hungarian algorithm, which efficiently solves this assignment problem.\n\nThe pairwise loss $\\mathcal{L}_{pair}$ is similar to the negative log-likelihood of the ground truth event $y_i$ given the predicted distribution $\\hat{y}_{\\sigma(i)}$. Specifically:\n\n$\\mathcal{L}_{match} (y_i, \\hat{y}_{\\sigma(i)}) = |t_i - \\hat{t}_{\\sigma(i)}| - log \\hat{p}_{\\sigma(i)}(l),$ (5)\n\nwhere $y = (t,l)$ is a ground truth event, t is the predicted timestamp, and $\\hat{p}(l)$ is the predicted probability of the correct label.\n\nThe binary cross-entropy loss $\\mathcal{L}_{BCE}$ is used to train the model to predict the presence probability of events:\n\n$\\mathcal{L}_{BCE}(\\sigma, \\hat{y}) = -\\sum_{i \\in \\sigma}log \\hat{o}_i - \\sum_{i \\notin \\sigma}log(1 - \\hat{o}_i),$ (6)\n\nwhere $\\hat{o}_i$ is the predicted probability that the i-th event is matched with some ground truth event.\n\nBy minimizing the overall loss $\\mathcal{L}(y, \\hat{y})$, DeTPP is trained to accurately predict the parameters of the ground truth sequence, adapting to sequences of varying lengths up to the maximum of K events."}, {"title": "4.3 INFERENCE", "content": "Inference with DeTPP involves two steps: filtering and sorting. Initially, predicted events are filtered based on their presence probabilities $\\hat{o}_i$. Only events with a presence probability above a certain threshold are retained. The remaining events are then sorted according to their predicted timestamps, forming the final output sequence.\n\nHowever, in practice, an additional calibration step is necessary. Without calibration, the model tends to predict few events due to conservative probability estimates. The goal of calibration is to determine optimal thresholds for each $\\hat{o}_i$, aligning them with the prior probability of the event i being matched. This calibration is performed on-the-fly during training by tracking matching frequencies and computing the corresponding quantiles using a streaming algorithm."}, {"title": "5 EXPERIMENTS", "content": "We conducted a series of experiments using the HoTPP benchmark (Karpukhin et al., 2024) to evaluate the performance of DeTPP against several popular MTPP approaches. Specifically, we compare DeTPP to RMTPP (Du et al., 2016), NHP (Mei & Eisner, 2017), ODE-RNN (Rubanova et al., 2019). Additionally, we implement a simple baseline, MAE-CE, which is trained with a combination of Mean Absolute Error (MAE) and cross-entropy losses. This baseline is similar to a simplified version of the Intensity-free approach (Shchur et al., 2019). To advance our analysis, we extend the MAE-CE method to predict multiple future events in parallel, referring to the resulting method as MAE-CE-K. The datasets used in this study include Retweet (Zhao et al., 2015), Amazon (Jianmo, 2018), and StackOverflow (Jure, 2014)."}, {"title": "5.1 EVENTS FORECASTING", "content": "We evaluate both next-event and long-horizon metrics. For the next event we measure label pre-diction accuracy and time step Mean Average Error (MAE). We evaluate long-horizon prediction by Optimal Transport Distance (OTD) and Temporal mAP (T-mAP). The results from Table 1 show that DeTPP may not predict the next event as accurately as the best-performing models, but it significantly outperforms them in long-horizon predictions. The high T-mAP scores achieved by DeTPP can be attributed to our training objective, which resembles the T-mAP metric. Interestingly, DeTPP also improves the optimal transport distance (OTD) metric, suggesting that our training procedure enhances overall model stability rather than merely optimizing for a specific metric."}, {"title": "5.2 PREDICTIONS DIVERSITY", "content": "As we qualitatively demonstrated in Section 3, autoregressive methods tend to produce repetitive outputs. Here, we provide additional quantitative results that further highlight the low output diversity of traditional approaches. In Table 1, we report the entropy of predicted labels and the differential entropy of predicted time deltas. The results show that in most cases baseline autoregressive methods exhibit low entropy in both predicted labels and time deltas, indicating limited diversity. In contrast, DeTPP usually generate sequences with higher entropy values, reflecting greater diversity in their predictions. It's important to note that we did not tune the sampling temperature hyperparameter, as increasing temperature typically leads to a reduction in prediction quality."}, {"title": "5.3 TRAINING AND INFERENCE SPEED", "content": "An important practical aspect of the model is its computational efficiency. Table 2 presents a comparison of the training and inference times for each method. As shown, the training time for DeTPP is approximately twice as long as that of the discrete-time models (MAE-CE, RMTPP, MAE-CE-K). However, DeTPP significantly outperforms continuous-time models (NHP, ODE-RNN). In terms of inference speed, DeTPP is comparable to MAE-CE, slightly slower than MAE-CE-K, but considerably faster than the other methods. This makes DeTPP one of the fastest methods for inference."}, {"title": "5.4 ABLATIONS", "content": "We conducted ablation studies to compare DeTPP with the Next-K variant of the MAE-CE method. The results are summarized in Table 1. MAE-CE-K predicts K future events and is trained by computing pairwise losses between predictions and targets at corresponding positions, bypassing the matching stage. Our findings indicate that while DeTPP exhibits a higher next-item error, it significantly outperforms the Next-K approach in long-horizon prediction tasks. We conclude that the primary advantage of DeTPP lies in its matching-based training loss and specialized inference procedure, rather than just the parallel prediction of multiple future events."}, {"title": "6 CONCLUSION", "content": "In this work, we introduced DeTPP, a novel event prediction method that leverages concepts from object detection to address the challenges of long-horizon prediction. Our experiments demonstrate that DeTPP effectively overcomes the limitations of traditional autoregressive approaches. Notably, DeTPP generates more diverse predictions, significantly enhancing long-horizon prediction accuracy. Additionally, DeTPP is computationally more efficient, as it predicts multiple future events in parallel. This approach paves the way for advancing the modeling of Marked Temporal Point Processes, offering new opportunities across a wide range of practical applications."}]}