{"title": "Fairness Testing through Extreme Value Theory", "authors": ["Verya Monjezi", "Vladik Kreinovich", "Ashutosh Trivedi", "Saeid Tizpaz-Niari"], "abstract": "Data-driven software is increasingly being used as a critical component of automated decision-support systems. Since this class of software learns its logic from historical data, it can encode or amplify discriminatory practices. Previous research on algorithmic fairness has focused on improving \u201caverage-case\" fairness. On the other hand, fairness at the extreme ends of the spectrum, which often signifies lasting and impactful shifts in societal attitudes, has received significantly less emphasis.\nLeveraging the statistics of extreme value theory (EVT), we propose a novel fairness criterion called extreme counterfactual discrimination (ECD). This criterion estimates the worst-case amounts of disadvantage in outcomes for individuals solely based on their memberships in a protected group. Utilizing tools from search-based software engineering and generative AI, we present a randomized algorithm that samples a statistically significant set of points from the tail of ML outcome distributions even if the input dataset lacks a sufficient number of relevant samples.\nWe conducted several experiments on four ML models (deep neural networks, logistic regression, and random forests) over 10 socially relevant tasks from the literature on algorithmic fairness. First, we evaluate the generative AI methods and find that they generate sufficient samples to infer valid EVT distribution in 95% of cases. Remarkably, we found that the prevalent bias mitigators reduce the average-case discrimination but increase the worst-case discrimination significantly in 35% of cases. We also observed that even the tail-aware mitigation algorithm- MiniMax-Fairness-increased the worst-case discrimination in 30% of cases. We propose a novel ECD-based mitigator that improves fairness in the tail in 90% of cases with no degradation of the average-case discrimination. We hope that the EVT framework serves as a robust tool for evaluating fairness in both average-case and worst-case discrimination.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent technological advancements in training large ma- chine learning (ML) models, such as deep neural networks [1], deep reinforcement learning [2], and large language mod- els [3], [4], have led to a proliferation of data-driven software in almost every aspect of modern socioeconomic infrastruc- ture. These data-driven systems, such as those that decide on recidivism [5], predict benefit eligibility [6], [7], or decide whether to audit a given taxpayer [8], [9], learn their decision logic as ML models by mining simple patterns from historical data. However, these systems often codify and amplify the biases present in the historical data due to various systemic factors. To address this challenge, the software engineering community has developed solutions to characterize, quantify, and mitigate bias in the ML models. We discuss their inade- quacies and propose new tools and techniques for the tail of outcome distributions of data-driven software.\nInadequacies of Average-Case Fairness. Although there is an increased participation of minorities (e.g., women) in the labor market (parity in average), they are considerably underrepresented in high-paying occupations and leadership positions [10] (disparity in the extreme). Additionally, the wage gap between privileged and unprivileged individuals continues to be more pronounced in high-paying jobs [11], [10]. Considering these factors, it is indeed surprising that a notable gap exists in the literature regarding the evaluation of algorithmic fairness in the context of extreme outcomes.\nOne broad class of fairness definitions is individual fair- ness [12] which requires treating individuals similarly if they are deemed similar based on their non-protected attributes, regardless of their protected attributes. One popular individ- ual fairness notion is counterfactual discrimination which necessitates that algorithmic outcomes should be similar for an individual and any related counterfactual individual who differs only in protected attributes. However, these fairness notions primarily focus on the average behavior (expected value or variance) of the model, which can create a false sense of fairness by ignoring the discrimination in socially influential edge cases. This paper presents a framework rooted in EVT to quantify AI fairness within the tail of ML outcomes.\nStatistics of the Extreme: Extreme Value Theory. While statistics and machine learning typically focus on \"usual\u201d behavior, extreme value theory (EVT) [13] is a branch of statistics that deals with unusual or extreme behaviors. EVT can be applied to model rare events such as the maximum temperature in the summer. Under appropriate assumptions, the statistics of extreme values follow the generalized extreme value (GEV) distribution, which is analogous to the central limit theorem for the statistics of averages or expected values.\nFairness through Extreme Value Theory. The primary focus of this paper centers on a narrow view of equality of opportunity, which necessitates similar individuals to be treated similarly at the time of decision-making, as defined by Dwork et al. [12]. We consider the distribution of \"coun- terfactual discrimination,\" which refers to the distribution of"}, {"title": "II. OVERVIEW", "content": "We first give a background overview for extreme value theory. We then go through our approach step by step using an example of adult census income dataset, trained using a DNN algorithm.\nExtreme Value Theory. Given a set of independent and identically distributed random variables {$z_1$,..., $z_n$}, the ex- treme value theory is concerned with the max statistics of a random process, i.e., $M_n$ = max({$z_1$,..., $z_n$}). Under some mild assumptions, it has been proved (e.g., see Leadbetter et al. [25]) that $M_n$ belongs to a family of distributions called the generalized extreme value (GEV). There are two basic approaches to infer the parameters of GEV distributions: block maximum and threshold approach [13]. In this paper, we use the threshold approach where extreme events that exceed some high threshold $u$, i.e., {$z_i$ : $Z_i$ > $u$}, are extreme values. The GEV distribution has three parameters: a location parameter, a scale parameter, and a shape parameter. When the shape is close to zero or negative, the statistical guarantees on the worst-case discrimination may be feasible.\nThreshold Selection. A proper choice of threshold value $u$ is critical to analyze the behavior of GEV. Low values of threshold $u$ might include non-tail samples and lead to mixture distributions that violate the asymptotic basis of the model. On the other hand, high values of threshold $u$ might include only a few tail samples and lead to low confidence in the model due to high variance. In this work, we use coefficient of variation (CV) and provide statistical guarantees in picking thresholds.\nReturn Level. A return level describes by the set of points (m,$\\delta_m$) where m is the time period (e.g., the number of queries to the ML software) and the level $\\delta_m$ is expected to observed during the m period (e.g., maximum discrimination after m interactions)."}, {"title": "III. EXTREME COUNTERFACTUAL DISCRIMINATION", "content": "We consider machine learning classifiers with a set of input variables A, which are divided into a protected set of variables Z (e.g., race, sex, and age) and non-protected variables X (e.g., profession, income, and education). A learning problem can be defined as identifying a mapping from the inputs to a probabilistic score of the favorable outcome, inferred from a fixed training dataset $D_T$ = {((xi, $Z_i$), yi)}$_{i=1}^N$, such that the ML model generalizes well to previously unseen situations based on a test dataset $D*$ = {((x,z),y)}$_{i=1}^M$\nWe abstractly express a machine learning classifier as a function ML: X\u00d7Z \u2192 [0,1]. The accuracy of model is"}, {"title": "IV. APPROACH", "content": "We are interested in determining the maximum values of counterfactual discrimination, denoted as Mp for privileged groups and Mu for unprivileged groups. Since these values for different individuals are independent of each other, we can consider the estimation over a large number of independent and identically distributed random variables.\nExtreme value theory is the field of study that examines the limit distributions of such extreme values and the con- vergence towards these distributions. Our objective, therefore, is to estimate the worst-case counterfactual discrimination by comparing the statistical characteristics of GEV distributions between privileged and unprivileged sub-groups.\nHowever, analyzing extreme values necessitates having an adequate number of samples from the tail behavior of ML models for any given group to have confidence in the results. Our approach comprises three key steps: 1) Learning the underlying distributions of the target population to generate valid samples for any sub-group; 2) Collecting tail samples with statistical guarantees through a randomized test-case generation algorithm; and 3) Inferring the tail distributions of counterfactual discrimination by fitting GEV distributions to each group and comparing the results to determine statistically significant discrimination in worst-case scenarios.\nLearning the underlying distributions. The scarcity of samples for some protected groups in datasets can result in statistical uncertainties in extreme value distributions. For instance, in the heart dataset [31], the number of samples for male individuals is notably limited. The conventional approach of sampling data points uniformly at random from the domain of each variable without considering the relationships between variables has the risk of producing samples that do not represent the target group [14], [32], [33], [29]. For example, random generation could result in an income level that is out of line with the general age distribution.\nGenerative Adversarial Networks (GANs) and Variational Autoencoders (VAE) have been shown to effectively learn and reproduce actual data distributions, making them suitable for generating synthetic data that closely resembles the real- world distributions of sensitive groups [34], [35], [36], [19], [37], [38]. In the GAN paradigm, during the training phase, the generator's primary function is to produce synthetic data samples, while the discriminator is tasked with distinguishing between real and synthetic samples. After multiple rounds of training, the generator learns to generate data so indistinguish- able from the original samples. VAEs on the other hand, are trained by encoding input data into a latent representation and recovering it afterward. The decoder then reconstructs the input using the sampled latent points. Training involves optimizing two essential components: the reconstruction loss and the Kullback-Leibler (KL) divergence.\nHowever, in addition to making sure to learn the target distribution of each demographic to alleviate the risk of"}, {"title": "Algorithm 1: Tail Sample Generations.", "content": "Input: Decision-Support ML Software ML,\nGenerative Adversarial Network GAN,\nTraining Dataset D, Test Samples D*, Target\nGroup G, Counterfactual Group G',\nLow-Bounds on Exp. Test $k_{min}$, Upper-Bounds\non Exp. Test $k_{max}$, Num. of GAN Samples m,\nand Timeout T."}, {"title": "V. EXPERIMENTS", "content": "In this section, we first formulate the research questions (RQs). Then, we overview datasets, ML models, bias mitiga- tion algorithms, and our implementations. Finally, we carefully analyze and answer the research questions.\nRQ1 (Generating realistic test cases) Can the previously proposed algorithm generate realistic data from the underlying distribution of the real dataset?\nRQ2 (Feasibility + Usefulness + Guarantee) Can extreme value theory (EVT) model and quantify the coun- terfactual discrimination in the tail of ML outcome distributions with statistical guarantees?\nRQ3 (Average-based Bias Mitigators) Can we validate the efficacy of the prevalent bias mitigation algorithm [43], [21] via EVT?\nRQ4 (Tail-based Bias Mitigators) What are the perfor- mance of existing tail-aware bias reductions? How does an EVT-based mitigator compare to them?\nDataset. We consider 9 socially critical datasets from the literature of algorithmic fairness. These datasets and their properties are described in Table II. We assume that group 1 is privileged and group 2 is unprivileged.\nTraining Algorithms and ML Models. We consider 4 popular ML models from the literature. We use a six-layer DNN, following [29], [28], [27]. We trained DNN in Tensor-Flow [50] and used the same hyperparameters for all tasks with num_epochs, batch_size, and learning_rate are set to 25, 32, and 0.001, respectively. We use the LR, SVM, and Random Forest algorithms from scikit-learn library [51] with the default hyperparameter configuration, similar to [14], [52], [53].\nAverage-based Bias Mitigation Algorithm. We consider four commonly used (average-based) bias mitigation algorithms, exponentiated gradient (EG) [20] (implemented in both AI Fairness 360 [54] and Fairlearn [43]), Fair-SMOTE [21], MAAT [22], and STEALTH [23]. EG [20] algorithm adapts Lagrange methods to find the multipliers that balance accuracy and fairness. Fair-SMOTE looks for bias in the training data and aims to balance the statistics of sensitive features by generating synthetic samples. MAAT employs a fairness model alongside a performance model to infer the final decision. STEALTH employs a surrogate model to use in predictions and explanations. For evaluating fairness in average-based scenarios, in addition to AOD and EOD metrics, we also included Statistical Parity Difference (SPD), and Disparate Impact (DI) which compare the probabilities of favorable outcomes among protected groups [54].\nTail-aware Bias Reduction. We utilize Minimax- Fairness [24] that takes an iterative game-theoretical approach to reduce the maximum error for protected groups. To investigate the usefulness of the ECD-based mitigator, we adopt a hyperparameter optimization technique, PARFAIT- ML [42] that finds the configurations of ML algorithms to minimize the bias of resultant ML models in the tail. We set ECD as the objective search criteria and run the tool for 1 hour on each benchmark.\nImplementation and Technical Details. We run all the experiments on an Ubuntu 20.04.4 LTS server equipped with an AMD Ryzen Threadripper PRO 3955WX CPU and two NVIDIA GeForce RTX 3090 GPUs. We split the dataset into training (60%), validating (20%), and test (20%) data where accuracy, F1, and fairness measures are reported over the test data. We use Fairlearn [43] to quantify the fairness. To measure counterfactual bias, we sample data instances independently and at random for each sub-group. We use the implementation of EG in Fairlearn [43] to study the common bias mitigation algorithm. We repeated each query 100 times and took the average to control the stochastic behavior of the EG with high precision. We obtained the implementation of Minimax- Fairness [24] from their GitHub repository. We also modify the implementation to support training on GPU. We set the error_type, numsteps, and epochs to 0/1 loss, 2000, and 50, respectively. We implemented the EVT algorithms in R using evd and extremes libraries [55]. In Algorithm 1, we set $k_{min}, k_{max}, m, T$ to 10, 50, 1, and 1200(s), respectively. This choice of $k_{min}$ and $k_{max}$ provides 95% confidence on the feasibility of worst-case guarantees via EVT [17]. We obtained the implementation of Fair-SMOTE [21], MAAT [22], and STEALTH [23] from their GitHub repository and used the recommended configuration to achieve their best results. We repeated each experiment 20 times and conducted 4,400 runs in total. For the statistical tests, we follow prior work [23], [22], [56], [57] and perform a nonparametric test using the Scott-Knott procedure. This involved applying Cliff's Delta and a bootstrap test to assess the results. In our Scott- Knott ranking, we classify results as wins, ties, or losses based on statistically significant improvements, indistinguish- able performance, or significant degradations, respectively, compared to the original baseline (vanilla) model. We compare different methods to each other based on number of wins, ties, and losses. The replication package is available at https://figshare.com/s/5b4fe7b676e1f7f7b107."}, {"title": "A. Evaluating Synthetic Data Generation (RQ1)", "content": "We assess the performance of Conditional Tabular GAN (CTGAN) [18] and Triplet-based Variational Autoencoder (TVAE) [18] by comparing their synthetic data against the original dataset, focusing on statistical similarities and distri- bution characteristics. We aim to determine which model better generates representative test cases for target demographic groups. We also included datasets generated independently at randomly from the domain of variables. For quality assess- ment, we considered two criteria: similarity to the dataset in several statistical properties and the performance of a downstream ML model trained on generated data versus the actual dataset [58], [59], [60].\nAnswer RQ1: CTGAN and TAVE demonstrate their ability to accurately replicate the distribution of actual datasets. In our experiments, they generated data with a KL-Divergence as high as 0.98 and an inception distance as low as 0.008. But, we found that their effectiveness is dataset-dependent."}, {"title": "B. Feasibility, Usefulness, and Guarantee of EVT (RQ2)", "content": "One important investigation of this paper is to find out whether Extreme Value Theory (EVT) can effectively model the tail of ML outcome distributions. In Table IV, we present 80 experiments with their corresponding EVT characteristics and the feasibility of EVT to provide fairness guarantees. The number of test cases generated for each group is shown in column #N, determined by the exponential testing in Algo- rithm 1. The numbers reported in this column include both the original sample size from the dataset and the additional synthetic samples required to pass the test. For instance, a value of 0.1/1.0 indicates that there are 100 original samples with 1000 additional synthetic samples. Columns ACD [14], CVaR [15], and ECD show average, conditional value at risk, and extreme counterfactual discrimination. In the columns (\u03bc,\u03c3,\u03be, \u03c4, type) of Table IV, we detail the characteristics of the GEV distribution for each benchmark that informs ECD. Here, \u03bc represents the mean of the extreme value distribution at a specific threshold for each combination of algorithm, dataset, and subgroup. For instance, in the DNN application to the Census dataset with sex as protected attribute, we observe an ACD of 0.05, CVaR of 0.08, and ECD of 0.21 where \u00b5m and \u00b5F is 0.03 and 0.24, respectively, implying a significant counterfactual discrimination toward female in the tail of DNN's outcome.\nThe shape \u03be indicates the tail behavior of the GEV. A shape \u03be around zero or negative suggests that GEV can extrapolate for a long finite (based on Q-Q Plot) or infinite interactions with statistical guarantees, shown with B. In 62 out of 80 scenarios (78%), EVT results in a type III distribution with a negative shape, indicating a finite tail and enabling extrapolation for an unlimited number of queries. For 14 cases (18%), EVT produces a type I distribution with a near-zero shape, implying an infinite but exponentially decaying tail, suitable for extrapolation within bounded queries B. Overall, the worst-case guarantees are achievable in 76 cases (95%).\nWe examine the relevance of extreme counterfactual dis- crimination in ML model fairness by employing EVT to measure tail biases, comparing them to established fairness metrics like ACD and CVaR. For instance, in the DNN model trained on the Compas dataset, an ACD of 0.02 and a CVaR of -0.01 indicate fairness in both average and tail cases, yet an ECD of 0.11 suggests a tail-bias toward Caucasians. We classify any ECD difference exceeding 0.05 as discrimination, with its significance indicated by the grayscale in the ECD column. Out of 40 cases, ECD-based discrimination occurs in 19 (48%). In contrast, average-case discrimination (ACD) is observed in 10 out of 40 cases (25%). Notably, in 13 cases (33%), ECD is significantly greater than ACD. In 18 out of 40 experiments, ECD found significant discrimination against the unprivileged group in the tail that missed by the CVaR metric.\nAnswer RQ2: EVT effectively models extreme counterfac- tual discrimination (ECD), in 95% of cases, allowing for valid extrapolation of worst-case discrimination. In 33% of cases, ECD shows significantly higher discrimination than the average-case one (ACD [14]). In 18 out of 40 experiments, ECD found significant discrimination against the unprivileged group in the tail that missed by prevalent tail-based metric (CVaR [15])."}, {"title": "C. Validation of Prevalent Bias Mitigation Algorithms (RQ3)", "content": "In this analysis, we leverage EVT to assess the effectiveness of prevalent mitigation algorithms like exponentiated gradi- ent (EG) [20] and Fair-SMOTE [21] in the tail. We also include two recent mitigation techniques, MAAT [22] and STEALTH [23] in our experiments to evaluate our approach against more advanced methods. The results are reported in Table V and VI. The column Accuracy Loss shows the accuracy difference between the original and mitigated models with positive values indicating improved accuracy in the mitigated model, columns AOD, EOD, SPD, and DI report the absolute values of average-based fairness measures, and the column ECD shows the amount of discrimination in the tail. Darker gray shades indicate lower rankings, while lighter shades represent higher rankings (no shading indicates the top- ranked method).\nWe use the Scott-Knott ranking outcomes to compare the four mitigation methods where we consider a statistically significant improvement over the original baseline model (the vanilla model) as a win. While the tables include all metrics, we explain the results for one average-based metric and one tail-based metric. Consider the AOD metric, we find that EG [20] outperforms other methods where it wins in 19 cases (out of 40). STEALTH [23], MAAT [22], and Fair- SMOTE [21] win in 14, 6, and 4 cases in reducing AOD biases. In terms of average AOD over all benchmarks; EG, STEALTH, MAAT, and SMOTE achieve an average of 0.03, 0.07, 0.07, and 0.08, respectively. In terms of number of cases with an AOD bias below or equal to 0.05; we observe that EG, STEALTH, MAAT, and SMOTE have 32, 22, 22, and 9 cases (out of 40), respectively.\nWhen considering ECD metric, STEALTH demonstrates superior performance among the average-based mitigation methods in reducing tail discrimination. Specifically, we find that STEALTH wins in 31 cases (out of 40) whereas EG, MAAT, and Fair-SMOTE win in 15, 12, and 9 cases, re- spectively. In terms of average ECD over all benchmarks; STEALTH, EG, MAAT, and Fair-SMOTE achieve an average of 0.04, 0.20, 0.08, and 0.12, respectively. In terms of number of cases with an ECD bias below or equal to 0.05; STEALTH, EG, MAAT, and Fair-SMOTE have 32, 11, 15, and 9 cases (out of 40), respectively.\nAnswer RQ3: While the average-based mitigation meth- ods [20], [21], [22], [23] preserved or improved fairness based on metrics like AOD in 63%, they increase un- fairness in tail based on ECD metric in 35% of cases. STEALTH [23] outperformed other mitigation methods sig- nificantly based on the ECD metric, failing only in 10% of cases, while preserving/reducing the AOD bias in 65%."}, {"title": "D. Tail-aware Mitigation Algorithms (RQ4)", "content": "We first evaluate the effectiveness of MiniMax- Fairness [24], which serves as our baseline, alongside our proposed in-process mitigator (ECD-Fair). Results in Table VII follow a similar format to Table V where we only include the DNN and Logistic regression models since the MiniMax-Fairness only supports these models among our base models. Considering ECD metric, our approach significantly outperforms MiniMax-Fairness. Specifically, ECD-Fair wins in 18 cases (out of 20), while MiniMax-Fairness wins in 10 cases (out of 20). When considering EOD and AOD metrics, ECD-Fair outperforms MiniMax-Fairness with 10 and 9 win cases vs. 7 and 5 win cases (out of 20). In terms of absolute values over all benchmarks, ECD-Fair achieves a average AOD and ECD of 0.04 and 0.03, respectively. The number of cases with AOD and ECD below 0.05 are 15 and 18 (out of 20), respectively.\nWe also compare ECD-Fair to STEALTH [23] method over the DNN and LR benchmarks as STEALTH outperformed other baseline methods. Based on the EOD and AOD metrics, we find that ECD-Fair wins in 10 and 9 cases (out of 20) vs. STEALTH wins in 6 and 8 cases (out of 20), respectively. When considering the ECD metric, ECD-Fair and STEALTH win in 18 and 16 cases (out of 20), respectively. STEALTH degrades unfairness in tail for 2 benchmarks, while ECD-fair does not increase the unfairness in the tail for any benchmark. Overall, while STEALTH demonstrates a competitive result, ECD-Fair slightly outperforms it for both tail and average metrics.\nAnswer RQ4: ECD-Fair significantly outperformed MiniMax-Fairness [24], a state-of-the-art tail-aware mitigator. When compared to STEALTH [23], a competitive baseline, we found that ECD-Fair and STEALTH improved fairness in the tail for 90% and 80% of cases, respectively. ECD-Fair and STEALTH reduced the AOD bias in 45% and 40% of cases, respectively."}, {"title": "VI. DISCUSSIONS", "content": "Limitations. One limitation is the lack of ground truth re- garding the tail of ML outcome distributions. We can use the maximum individual discrimination in the validation dataset as it gives a lower-bound on the ground truth. Our approach requires the presence of protected attributes during inference. Therefore, it cannot be used to study worst-case fairness for notions such as fairness through unawareness, which requires the removal of protected attributes [12]. Our approach also depends on the representative individuals sampled from the same training distribution, and may not be valid for out- of-distribution queries. Finally, our approach assumes that flipping the sensitive values leads to valid representations to measure the sensitivity of ML models to the protected attributes.\nThreat to Validity. To address the internal validity and ensure our finding does not lead to invalid conclusions, we follow established guidelines and report the statistical significance of measures with the exponential and Scott-Knott statistical testing. To ensure that our results are generalizable, we per- form our experiments on three well-established training al- gorithms from scikit-learn and TensorFlow libraries with a popular mitigation algorithm from the Fairlearn library over 160 fairness-sensitive tasks that have been widely used in the fairness research. It is an open problem whether the algorithms, hyperparameters, and datasets are sufficiently representative to cover challenging fairness scenarios."}, {"title": "VII. RELATED WORK", "content": "Fairness Testing of Data-Driven Software. Individual dis- crimination is a major fairness debugging method [64], [52], [27], [28], [21], [65]. THEMIS [14] is the closest approach. While THEMIS [14] focuses on the average causal discrimina- tion between two subgroups via counterfactual queries with prevalent statistical guarantees of normal distributions, we introduce the notion of extreme causal discrimination between two subgroups with exponentially statistical guarantees of extreme value distributions. Rather than randomly sampling data from the domain of variables, we leveraged generative AI models to produce realistic test cases from the tail.\nFairness in the Tail. Multiple works consider the worst-case group fairness [15], [24], [66]. Williamson and Menon [15] leveraged conditional value at risk (CVaR) to minimize the expected loss and the worst-case loss of any group in the upper quantile. We found that CVaR might miss discrimination in the tail and cannot reason about the shape of tail. Diana et al. [24] proposed a constrained optimization objective where the goal is to minimize the expected overall loss for all data instances subject to the hard constraints wherein no group loss can be more than a threshold. We propose an in-process bias mitigator that significantly outperforms this technique as shown in RQ4.\nIntersectional Fairness. The keyword \"worst-case fairness\" has been also used in the relevant fairness literature [67], [68], [69]. However, their notion of fairness still relies on regular \"average\u201d fairness metrics like the rate of favorable outcomes per each subgroup. In particular, intersectional fairness con- cerns about the summary of fairness statistics when there are fairness measures for n subgroups. For example, Ghosh et al. [67] suggests a min-max ratio that takes the maximum for average favorable outcomes of all subgroups and divides it by the min for average favorable outcomes of all subgroups. On the other hand, our fairness measure looks at the tail of ML outcome distributions per each subgroup via EVT and compares the tail distributions between groups to quantify the amounts of discrimination.\nOther Application of EVT for Fairness. In addition to its technical applications [70], [17] Extreme value theory has been significantly used to study income and wealth inequalities around the world [71], [72], [73]. Piketty and Saez [71] used the Generalized Pareto Distribution to study the distribution of income in the US between 1913 and 1998. Wang [74] studied the concept of Degree of Matthew Effect in recommendation systems via extreme value theory whereas we consider social bias (discrimination against protected groups) in decision- making systems (based on classifications)."}, {"title": "VIII. CONCLUSION AND FUTURE WORK", "content": "We studied fairness through the lens of extreme value theory. Our proposed approach fitted well to model the worst-cases counterfactual bias with statistical guarantees and revealed the limitations of a state-of-the-art bias reduction algorithm in the worst-case. There are multiple exciting future directions. One direction is to leverage EVT to provide a no- tion of AI harms to understand if automated decision-support software systematically harms a vulnerable community."}]}