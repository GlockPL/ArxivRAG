{"title": "Learning Primitive Relations for Compositional Zero-Shot Learning", "authors": ["Insu Lee", "Jiseob Kim", "Kyuhong Shim", "Byonghyo Shim"], "abstract": "Compositional Zero-Shot Learning (CZSL) aims to identify unseen state-object compositions by leveraging knowledge learned from seen compositions. Existing approaches often independently predict states and objects, overlooking their relationships. In this paper, we propose a novel framework, learning primitive relations (LPR), designed to probabilistically capture the relationships between states and objects. By employing the cross-attention mechanism, LPR considers the dependencies between states and objects, enabling the model to infer the likelihood of unseen compositions. Experimental results demonstrate that LPR outperforms state-of-the-art methods on all three CZSL benchmark datasets in both closed-world and open-world settings. Through qualitative analysis, we show that LPR leverages state-object relationships for unseen composition prediction.", "sections": [{"title": "I. INTRODUCTION", "content": "Although it sounds strange, when we hear the phrase purple cow, we can anyway picture it in our minds. The word purple cow combines prior knowledge of state ('purple') and object ('cow'). In contrast to this, AI models often fail to classify new state-object compositions that have not been trained. Recently, Compositional Zero-Shot Learning (CZSL) [1] has been introduced to train models such that it can classify unseen compositional classes without additional training. Key idea of CZSL is that the model leverages knowledge about states and objects learned from seen compositions and then generalizes it to unseen compositions. In CZSL, each class included in the states and objects is called primitive.\nIn recent studies [2, 3, 4, 5, 6], generalization ability of pre-trained vision-language models (VLMs), such as CLIP [7] is used to represent compositions in natural language. In order to perform the classification properly, a feature of the \"purple cow\" image and that for the text \"A photo of purple cow\" should be similar in the VLM feature space. Previous works employ two separate branches that predict state and object independently. These methods aim to decompose state and object information and then treat them as independent elements in an image (see Fig. 1(a)) [2, 3].\nWhile the abovementioned approaches are effective to some extent, they often fail to capture the relationships between states and objects. We argue that understanding the relationships between them is essential for classifying unseen compositions. The first reason is that the model can filter out nonsense compositions by learning the co-occurrence of similar objects or states. For instance, when classifying the state of an unseen composition \u201cDark Sky\u201d (dashed lines in Fig. 1(b)), the model is likely to assign low probabilities to states \"Cooked\" or \"Sliced\", since similar object \"Ocean\" also exhibits low probabilities for these states. The second reason is that the model can compute the likelihood of unseen compositions by learning the probabilistic relationships between states and objects. For example, the probability of \u201cDark Sky\u201d can be modeled by referencing similar primitive relationships of seen compositions, such as \"Dark Ocean\" and \"Bright Sky\" (solid lines in Fig. 1(b)).\nIn this paper, we propose a novel framework, called learning primitive relations (LPR), which captures relationships between primitives in a probabilistic manner. Key idea of LPR is to extract object-related (or state-related) features conditioned on the state (or object) using cross-attention, which converts"}, {"title": "II. LEARNING PRIMITIVE RELATIONS", "content": "In CZSL, an image is classified into one of the classes within the compositional set $C = S \\times O$, where S represents a set of states ${s_1, s_2, ..., s_{|S|}}$ and O represents a set of objects ${o_1, o_2, ..., o_{|O|}}}$. During the training, the model can only access a set of seen compositions $C_{seen} \\subset C$. The unseen compositions form the set $C_{unseen}$, which is the subset of complement of $C_{seen}$ ($C_{unseen} \\subset C \\backslash C_{seen}$). During the inference, the model is tested using both seen and unseen compositions."}, {"title": "B. CLIP Feature Extraction for Compositional Classification", "content": "In this work, we exploit the CLIP model [7] to extract image and text features within a shared feature space. The CLIP visual encoder processes an input image I to extract the image feature $x_i$. Concurrently, the CLIP text encoder takes three types of prompts: $P^S$, $P^O$, and $P^C$, which indicate state, object, and compositional prompts, respectively. Then, the text encoder converts these prompts into text features $T^S = [t_{s_1}, ..., t_{s_{|S|}}]$, $T^O = [t_{o_1}, ..., t_{o_{|O|}}]$ and $T^C = [t_{(1,1)}, ..., t_{(|S|,|O|)}]$. Note that the subscripts in the text features indicate the class indices. For example, $t_{s_k}$, $t_{o_l}$ and $t_{(k,l)}$ represent the k-th state, the l-th object, and the (k, l)-th compositional class, respectively [2]."}, {"title": "C. Role of LPR Branches", "content": "LPR employs three distinct branches, denoted as com, sor, and osr (see Fig. 2). The com branch functions as a standard composition classifier, while the sor and osr branches serve as the novel classifiers designed to learn bidirectional relationships between states and objects. The primary aim is to transform the image feature $x_i$ within each branch so that it becomes close to the corresponding target text class feature. To this end, each branch processes $x_i$ through an Adapter [13], generating branch-specific features $x_i^c$, $x_i^s$, and $x_i^o$.\nIn the com branch, we compute the cosine similarity between $x_i^c$ and $T^C$ to estimate $p(c|x)$ and find the closest composition. In the sor and osr branches, directly modeling the bidirectional relationships between states and objects is challenging. To address this, we adopt a decomposition approach inspired by the Bayes rule, where one primitive is conditioned on the probability of the other. Specifically, in the sor branch, we first extract state information and then use it to extract the object information. To do so, we utilize $T^S$ as prototypes to extract probabilistic state information from $x_i^s$ by applying the cross-attention mechanism. In detail, we project $x_i^s$ into the query q and project $T^S$ into the key K and the value V for the cross-attention. Intuitively, this process generates a probability distribution that indicates how close $x_i^s$ is to each state. The output of cross-attention, $x_i^{so}$, can be interpreted as a state-informed image feature vector.\nNext, we utilize $T^O$ as object prototypes to extract probabilistic object information conditioned on both image and state information. Here, $x_i^{so}$ transforms into the query, while $T^O$ transforms to serve as both the key and the value. After passing through the attention and MLP blocks, $x_i^{so}$ is transformed into $x_i^{so}$, a state-conditioned object-informed image feature vector. Then $x_i^{so}$ is used to predict the compositional class, represented as $p(c|x_i^{so})$.\nThe osr branch operates similarly to the sor branch, except that it first transforms $x_i^o$ into the object-informed image feature vector $x_i^{os}$, which is then used to obtain object con-"}, {"title": "D. LPR Training and Inference", "content": "During training, we apply one loss to the com branch and three losses to the sor and osr branches:\n$L_{com} = CE(x_i^c, c)$,\n$L_{sor} = \\lambda_1 CE(x_i^{so}, c) + \\lambda_2 [CE(x_i^s, s) + CE(x_i^{so}, o)], (1)$\n$L_{osr} = \\lambda_1 CE(x_i^{os}, c) + \\lambda_2 [CE(x_i^o, o) + CE(x_i^{os}, s)],$\nwhere CE refers to the cross-entropy loss function, and $\\lambda_1$ and $\\lambda_2$ are loss coefficient hyperparameters. Total loss is calculated as $L = L_{com} + L_{sor} + L_{osr}$. We introduce intermediate cross-entropy losses for $x_i^s$, $x_i^o$, $x_i^{so}$, and $x_i^{os}$. These losses encourage the intermediate features to capture state or object information before applying the cross-attention.\nDuring inference, for the sor and osr branches, we supplement $p(c)$ using $p(s|\\cdot)$ and $p(o|\\cdot)$. The predictions from each branch are combined to produce the final probability:\n$p(c|x_i) = \\alpha p(c|x_i^c)$\n$+ \\frac{\\beta}{2} [p(c | x_i^{so}) + p(s | x_i^s)p(o | x_i^{so})]$\n$+ \\frac{\\beta}{2} [p(c | x_i^{os}) + p(s | x_i^o)p(o | x_i^{os})], (2)$\nwhere $\\alpha, \\beta$ are scaling hyperparameters. The composition with the highest probability is determined as the final prediction."}, {"title": "III. EXPERIMENTAL RESULTS", "content": "We evaluate the performance of our model using three CZSL benchmark datasets. MIT-States [14] consists of 115"}, {"title": "B. CZSL Performance", "content": "Table I presents the performance of the LPR and previous CZSL techniques. The results show that LPR significantly outperforms all existing methods across all datasets. Specifically, on the C-GQA dataset, LPR achieves a harmonic mean accuracy (HM) of 32.9% and 12.9% for closed-world and open-world settings, representing 3.5 and 2.0 percentage points improvement over previous SOTA [2]."}, {"title": "C. Ablation Study", "content": "To demonstrate the effectiveness of utilizing three branches (com, sor, and osr) in the proposed architecture, we conduct a path ablation study in Table II. The results show that the highest HM is achieved when all three branches are activated. Moreover, we can observe that combining the com branch with other branches consistently leads to improved performance. This implies that the newly proposed branches equip the ability to recognize unseen compositions that the com branch fails.\nIn addition, we investigate the effect of the weight hyperparameter $\\alpha$ in Fig. 4. As expected, with larger $\\alpha$, the influence of the com branch increases, leading to a gradual improvement in seen accuracy. On the opposite, the unseen accuracy increases as $\\alpha$ decreases. We empirically find the optimal balance between $\\alpha \\in [0.2, 0.4]$."}, {"title": "D. Sensitivity to Hyperparameters", "content": "We conduct additional experiments to test the hyperparameter sensitivity in Table III. As noted in Sec. III-A, $\\alpha$ and $\\beta$ are shared between MIT-States and C-GQA datasets. To assess the generalizability of our hyperparameters, we evaluate their configuration to the UT-Zappos dataset (denoted as default). We denote LPR in the setup from Table I as best.\nWhile there exists a slight degradation in performance, LPR consistently outperforms Troika [2] in both closed and open-world setups, indicating that our model shows robust performance with minimal hyperparameter tuning."}, {"title": "E. Qualitative Analysis", "content": "We visualize the qualitative examples in Fig. 5. While all compositions are correctly classified, each branch shows slightly different predictions. For the seen class samples (upper row), the com branch is more accurate than the other two branches. However, the com branch sometimes predicts awkward compositions for unseen images (lower row), demonstrating the vulnerability of using the com branch alone for the CZSL task. We note that sor and osr branches usually generate reasonable predictions regardless of the exact match for the given composition class label."}, {"title": "IV. RELATED WORK", "content": "Conventional CZSL approaches [9, 21, 22, 23] utilize two classifiers to independently identify states and objects. Each classifier predicts either the state or the object, and the predicted state and object are combined to determine the compositional class. In recent approaches [2, 3, 4], trainable state and object tokens are employed to generate textual prompts for compositions, fully utilizing VLMs' ability to map images and text into a shared feature space [24].\nOur work is significantly distinct from previous works because the model systematically considers the relationships between states and objects. Key distinguishing feature of LPR compared to previous works is the decomposition of joint probabilities using two branches, where cross-attention-based architecture captures the relationships between primitives."}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed a novel CSZL framework, LPR, which implicitly learns the relationships between states and objects as a probabilistic model. LPR introduces two new prediction branches that utilize the cross-attention mechanism between image and text feature embeddings in two different orders: state-to-object and object-to-state. Experiments showed that LPR achieves the best CZSL accuracy, especially for unseen class classification performance."}]}