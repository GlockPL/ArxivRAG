{"title": "The Clever Hans Effect in Unsupervised Learning", "authors": ["Jacob Kauffmann", "Jonas Dippel", "Lukas Ruff", "Wojciech Samek", "Klaus-Robert M\u00fcller", "Gr\u00e9goire Montavon"], "abstract": "Unsupervised learning has become an essential building block of AI systems. The representations it produces, e.g. in foundation models, are critical to a wide variety of downstream applications. It is therefore important to carefully examine unsupervised models to ensure not only that they produce accurate predictions, but also that these predictions are not \u201cright for the wrong reasons\u201d, the so-called Clever Hans (CH) effect. Using specially developed Explainable AI techniques, we show for the first time that CH effects are widespread in unsupervised learning. Our empirical findings are enriched by theoretical insights, which interestingly point to inductive biases in the unsupervised learning machine as a primary source of CH effects. Overall, our work sheds light on unexplored risks associated with practical applications of unsupervised learning and suggests ways to make unsupervised learning more robust.", "sections": [{"title": "Introduction", "content": "Unsupervised learning is a subfield of machine learning that has gained prominence in recent years [1-3]. It ad- dresses fundamental limitations of supervised learning, such as the lack of labels in the data or the high cost of acquiring them. Unsupervised learning has achieved successes in modeling the unknown, such as uncovering new cancer subtypes [4, 5] or extracting novel insights from large historical corpora [6]. Furthermore, the fact that unsupervised learning does not rely on task-specific labels makes it a good candidate for core AI infrastructure: Unsupervised anomaly detection provides the basis for various quality or integrity checks on the input data [7-10]. Unsupervised learning is also a key technology behind \u2018foundation models' [1, 11\u2013 15] which extract representations upon which various downstream models (e.g. classification, regression, 'generative AI', etc.) can be built. The growing popularity of unsupervised learning models creates an urgent need to carefully examine how they arrive at their predictions. This is essential to ensure that potential flaws in the way these models process and represent the input data are not propagated to the many downstream supervised models that build upon them.\nIn this paper, we show for the first time that unsupervised learning models largely suffer from Clever Hans (CH) effects [16], also known as \u201cright for the wrong reasons\". Specifically, we find that unsupervised learning models often produce rep- resentations from which instances can be correctly predicted to be e.g. similar or anomalous, although largely supported by data quality artifacts. The flawed prediction strategy is not detectable by common evaluation benchmarks such as cross-validation, but may manifest itself much later in 'downstream' applications in the form of unexpected errors, e.g. if subtle changes in the input data occur after deploy- ment (cf. Fig. 1). While CH effects have been studied quite extensively for supervised learning [16-20], the lack of similar studies in the context of unsupervised learning, together with the fact that unsupervised models supply many downstream applications, is a cause for concern.\nFor example, in industrial inspection, which often relies on unsupervised anomaly detection [9, 10], we find that a CH decision strategy can systematically miss a wide range of manufacturing defects, resulting in potentially significant costs. As another example, unsupervised foundation models, advocated in the medical domain to provide robust features for various specialized diagnostic tasks, can potentially intro- duce CH effects into many of these tasks, with the prominent risk of large-scale misdiagnosis. These scenarios (illustrated in Fig. 1) highlight the practical implications of an unsu- pervised CH effect, which, unlike its supervised counterpart, may not be limited to malfunctioning in a single specific task, but potentially in all downstream tasks.\nTo uncover and understand unsupervised Clever Hans effects, we propose to use Explainable AI [21-25] (here techniques that build on the LRP explanation framework [26-28]). Our proposed use of these techniques allows us to identify at scale which input features are used (or misused) by the unsupervised ML model, without having to formulate specific hypotheses or downstream tasks. Specifically, we use an extension of LRP called BiLRP [6] to reveal input patterns that are jointly responsible for similarity in the representation space. We also combine LRP with 'virtual layers' [29, 30] to reveal pixel and frequency components that are jointly responsible for predicted anomalies.\nFurthermore, our Explainable AI-based analysis allows us to pinpoint more formal causes for the emergence of unsupervised CH effects. In particular, they are due not so much to the data, but to the unsupervised learning machine, which hinders the integration of the true task-supporting features into the model, even though vast amounts of data points are available. Our findings provide a novel direction for developing targeted strategies to mitigate CH effects and increase model robustness.\nOverall, our work sheds light on the presence, prominence, and distinctiveness of CH effects in unsupervised learning, calling for increased scrutiny of this essential component of modern AI systems."}, {"title": "Results", "content": "Using Explainable AI, we investigate the emergence of Clever Hans effects in a representative set of unsupervised learning tasks, including representation learning and anomaly detection."}, {"title": "Clever Hans Effects in Representation Learning", "content": "We first consider an application of representation learning in the context of detecting COVID-19 cases from X-ray scans [31, 32]. Simulating an early pandemic phase characterized by data scarcity, similar to [32], we use a dataset aggregation approach where a large, well-established non-COVID-19 dataset is merged with a more recent COVID-19 dataset aggregated from multiple sources. Specifically, we aggregate 2597 instances of the CXR8 dataset [33] from the National Institute of Health (NIH), collected between 1992 and 2015, with the 535 instances of the GitHub-hosted 'COVID-19 image data collection' [34]. We refer to these subsets as 'NIH' and 'GitHub', respectively.\nFurther motivated by the need to accommodate the critically small number of COVID-19 instances in the aggregated dataset and to avoid overfitting, we choose to rely on the representations provided by unsupervised foundation models [15, 35-37]. Specifically, we feed our data into a pre-trained PubMedCLIP model [36], which has built its representation from a very large collection of X-ray scans in an unsuper- vised manner. Based on the PubMedCLIP representation, we train a downstream classifier that separates COVID-19 from non-COVID-19 instances with a class-balanced accuracy of 88.6% on the test set (cf. Table 1). However, a closer look at the structure of this performance score reveals a strong disparity between the NIH and GitHub subgroups, with all NIH instances being correctly classified and the similarity"}, {"title": "Clever Hans Effects in Anomaly Detection", "content": "Extending our investigation of the CH effect to another in- stance of unsupervised learning, namely anomaly detection, we consider an industrial inspection use case based on the popular MVTec-AD dataset [9]. The dataset consists of 15 product categories, each consisting of a training set of images without manufacturing defects and a test set of im- ages with and without defects. Since manufacturing defects are infrequent and heterogeneous in nature, the problem is typically approached using unsupervised anomaly detection [2, 9]. These models map each instance to an anomaly score, from which threshold-based downstream models can be built to classify between instances with and without manufactur- ing defects. Unsupervised anomaly detection has received considerable attention, with sophisticated approaches based on deep neural networks such as PatchCore [45] or Efficien- tAD [46], showing excellent performance in detecting a wide range of industrial defects.\nSomewhat surprisingly, simpler approaches based on dis- tances in pixel space show comparatively high performance for selected tasks [2]. We consider one such approach, which we call 'D2Neighbors', where anomalies are predicted ac- cording to the distance to neighbors in the training data. Specifically, the anomaly score of an instance x is computed as $f(x) = \\text{softmin}_j\\{\\vert\\vert x - u_j\\vert\\vert^2\\}$ where $(u_j)_{1}^{N}$ is the set of available inlier instances (see the Methods section for details on the model and data preprocessing). This anomaly model belongs to the broader class of distance-based models [47-49], and connections can be made to kernel density estimation"}, {"title": "Alleviating CH in Unsupervised Learning", "content": "Leveraging the Explainable AI analysis above, we aim to build models that are more robust across different data sub- groups and in post-deployment conditions. Unlike previously proposed CH removal techniques [18, 19], we aim to oper- ate on the unsupervised model rather than the downstream tasks. This potentially allows us to achieve broad robustness improvements while leaving the downstream learning ma- chines (training supervised classifiers or adjusting detection thresholds) untouched.\nWe first test our CH mitigation approach on the CLIP model, which our Explainable AI analysis has shown to incorrectly rely on text. Specifically, we focus on an early layer of the CLIP model (encoder.relu3) and remove the feature maps that are most responsive to text. We then apply a similar CH mitigation approach to anomaly detec- tion, which our Explainable AI analysis has shown to be overexposed to high frequencies. Here, we propose to prune the high frequencies by inserting a blur layer at the input of the model.\nIn both cases, the proposed CH mitigation technique pro- vides strong benefits in terms of model robustness. As shown in Table 1, rows 'CH mitigation', our robustified models sub- stantially reverse the performance degradation observed in post-deployment conditions, reaching performance levels close to, and in some cases superior to, those measured on the original data."}, {"title": "Discussion", "content": "Unsupervised learning is an essential category of ML that is increasingly used in core AI infrastructure to power a variety of downstream tasks, including generative AI. Much research to date has focused on improving the performance of unsupervised learning algorithms, for example, to maximize accuracy scores in downstream classification tasks. These evaluations often pay little attention to the exact strategy used by the unsupervised model to achieve the reported high performance, in particular whether these models rely on Clever Hans strategies.\nBuilding on recent techniques from Explainable AI, we have shown for the first time that CH strategies are prevalent in several unsupervised learning paradigms. These strategies can take various forms, such as correctly predicting the similarity of two X-ray scans based on irrelevant shared annotations, or predicting that an image is anomalous based on small but widespread pixel-level artifacts. These flawed prediction strategies result in models that do not transfer well to changing conditions at test time. Most importantly, their lack of robustness infects many downstream models that rely on them. As we have shown in two use cases, this can lead to widespread misdiagnosis of patients or failure to detect manufacturing defects.\nTherefore, addressing CH effects is a critical step towards reliable use of unsupervised learning methods. However, compared to a purely task-specific supervised approach, the addition of an unsupervised component, potentially serving multiple downstream tasks, adds another dimension of com- plexity to the modeling problem. In particular, one must decide whether to handle CH effects in the downstream clas- sifier or directly in the unsupervised model part. Approaches consisting of dynamically updating downstream models in response to changing conditions [56-59], or revising their decision strategies with human feedback [18, 19, 60] are possible solutions to maintain high accuracy. However, these approaches may not be sustainable because CH mitigation must be performed repeatedly for each new downstream task. The problem may persist even after a flaw in the foundation model becomes known (e.g. [61, 62]), due to release intervals and the high cost of retraining these models. Instead, our results argue for addressing CH effects directly when building the unsupervised model, with the goal of achieving persistent robustness that benefits existing and future downstream applications.\nUsing advanced Explainable AI techniques targeted at unsupervised learning, we have uncovered multiple and di- verse CH effects, such as the reliance of CLIP (and its derivative PubMedCLIP) on textual annotations, or a sys- temic over-exposure of unsupervised anomaly models to noise. Interestingly, our analysis has revealed that these unsupervised CH effects differ from supervised ones in that they arise less from the data and more from inductive bi- ases in the model and learning algorithm. These include spurious suppression/amplification effects caused by the representation learning's training objective, or a failure of unsupervised anomaly detection architectures to replicate frequency filtering mechanisms found in supervised learning [63-65], leaving the learned models highly exposed to noise. Furthermore, our Explainable AI analysis not only provided new insights into the formal causes of unstable behavior in unsupervised learning. Our experiments also showed how pruning the high-frequency or spuriously amplified features revealed by our Explainable AI analysis leads to system- atic performance improvements on difficult data subgroups or in post-deployment conditions. In doing so, we have demonstrated the actionability of our Explainable AI ap- proach, showing that it can guide the process of identifying and subsequently correcting the faulty components of an unsupervised learning model.\nWhile our investigation of unsupervised CH effects and their consequences has focused on image data, extension to other data modalities seems straightforward. Explainable AI techniques such as LRP, which are capable of accurate dataset-wide explanations, operate independently of the type of input data. They have recently been extended to recurrent neural networks [66], graph neural networks [67], transformers [68], and state space models [69], which represent the state of the art for large language models and other models of structured data. Thus, our analysis could be extended to analyze other instances of unsupervised learning, such as anomaly detection in time series or the representations learned by large language models (e.g. [70, 71]).\nOverall, we believe that the CH effect in unsupervised learning, and the uncontrolled risks associated with it, is a question of general importance, and that Explainable AI and its recent developments provide an effective way to tackle it."}, {"title": "Methods", "content": "This section first introduces the unsupervised ML models studied in this work and the datasets on which they are ap- plied. It then presents the layer-wise relevance propagation (LRP) method for explaining predictions, its BiLRP exten- sion for explaining similarity, and the technique of 'virtual layers' for generating joint pixel-frequency explanations."}, {"title": "ML Models and Data for Representation Learning", "content": "Representation learning experiments were performed on the SimCLR [12, 38], CLIP[13], BarlowTwins [39] and Pub-MedCLIP [36] models. All include a version based on"}, {"title": "Explanations for Representation Learning", "content": "Our experiments examined dot product similarities in repre- sentation space, \u0456.\u0435. $y = (\u03a6(x), \u03a6(x'))$, where $\u03a6$ denotes the function that maps the input features to the representation, typically a deep neural network. To understand similarity scores in terms of input features, we used the BiLRP method [79] which extends the LRP technique [24, 26, 27, 80] for this specific purpose. The conceptual starting point of BiLRP is the observation that a dot product is a bilinear function of its input. BiLRP then proceeds by reverse propagating the terms of the bilinear function to pairs of activations from the layer below and iterating down to the input. Denoting $R_{kk'}$ the contribution of neurons k and k' to the similarity score in some intermediate layer in the network, BiLRP extracts the contributions of pairs of neurons j and j' in the layer below via the propagation rule:\n$R_{jj'} = \\sum_{k} \\sum_{k'}\\frac{Z_{jk}Z_{j'k'}}{Z_{jj'}}R_{kk'}$\n(1)\nIn practice, this reverse propagation procedure can be im- plemented equivalently, but more efficiently and easily, by computing a collection of standard LRP explanations (one for each neuron in the representation layer) and recombining them in a multiplicative manner:\n$\\text{BiLRP}(y) = \\sum_k \\text{LRP}(\u03a6_k(x)) \\otimes \\text{LRP}(\u03a6_k(x'))$ (2)\nOverall, assuming the input consists of d features, BiLRP produces an explanation of size d x d which is typically represented as a weighted bipartite graph between the set of features of the two input images. Due to the large number of terms, pixel-to-pixel contributions are aggregated into patch-to-patch contributions, and elements of the BiLRP explanations that are close to zero are omitted in the final explanation rendering. In our experiments, we computed BiLRP explanations using the Zennit implementation of LRP which handles the ResNet50 architecture, and set Zen- nit's LRP parameters to their default values."}, {"title": "Explanations for the D2Neighbors Model", "content": "The D2Neighbors model we investigate for anomaly detection is a composition of a distance layer and a soft min-pooling layer. To handle these layers, we use the purposely designed LRP rules of [28, 53]. Propagation in the softmin layer ($M_f$) is given by the formula\n$R_j = \\frac{f(||x - u_j||_p)}{\\sum_{j}f(||x - u_j||)}-o(x)$ (3)\na 'min-take-most' redistribution, where f is the same func- tion as in M. Each score $R_j$ can be interpreted as the contribution of the training point $u_j$ to the anomaly of x.\nTo further propagate these scores into the pixel fre- quency domain, we adopt the framework of 'virtual layers' [29, 30] and adapt it to the D2Neighbors model. As a fre- quency basis, we use the discrete cosine transform (DCT) [54], shown in Fig. 4 (right), which we denote by its col- lection of basis elements $(v_k)_k$. Since the DCT forms an orthogonal basis, we have the property $\\sum_{k}v_kv^T = I$, and multiplication by the identity matrix can be interpreted as a mapping to the frequencies and back. For the spe- cial case where p = 2, the distance terms in D2Neighbors reduce to the squared Euclidean norm $||x - u_j||^2$. These terms can be developed to identify pixel-pixel-frequency in- teractions: $||x - u_j||^2 = (x \u2013 u_j)^T(\\sum_{k}v_kv^T)(x \u2013 u_j) = \\sum_{k}\\sum_{ii'} [x \u2013 u_j]_i[x - u_j]_{i'} [v_k]_i[v_k]_{i'}$. From there, one can construct an LRP rule that propagates the instance-wise relevance $R_j$ to the pixel-frequency features:\n$R_{ii'k} = \\sum_{j} \\frac{[x \u2013 u_j]_i[x \u2013 u_j]_{i'} [v_k]_i[v_k]_{i'}}{\\epsilon + ||x - u_j||^2} R_j$, (4)\nwhere the variable $\u03f5$ is a small positive term that handles the case where x and $u_j$ overlap. A reduction of this propaga- tion rule can be obtained by marginalizing over interacting pixels ($R_{ik} = \\sum_{i'} R_{ii'k}$). Further reductions can be obtained by marginalizing over pixels ($R_k = \\sum_{i} R_{ik}$) or frequencies (Ri = \u2211k Rik). These reductions are used to generate the heatmap in Fig. 5."}, {"title": "Supplementary Note A. Additional Results for X-Ray Representations", "content": "Recall from the main paper that we analyzed the PubMedCLIP's foundation model [1], specifically its representation of X-ray data and its use for a downstream COVID-19 detection task. For this purpose, we considered a dataset constructed by merging the NIH CXR8 dataset [2] with the Github-hosted \u2018COVID-19 Image Data Collection' [3], and referred to these two data sources as NIH and Github. The NIH data, collected before the onset of the COVID-19 pandemic, is from a single hospital and is relatively homogeneous. The Github data, collected from multiple sources, is more heterogeneous. In the main paper, we reported strong inhomogeneities in the classification of the data across different subgroups, with the NIH instances being systematically correctly classified as negative and, in contrast, numerous Github negative instances being classified as positive. We attributed this heterogeneity in prediction quality to a spurious reliance on textual or annotation artifacts already latent in the representation of the unsupervised PubMedCLIP model. In this supplementary note, we retrace the emergence of the heterogeneous classification behavior from the representation level up to the classifier by providing further examples and analyses."}, {"title": "Analysis of the Unsupervised Representation", "content": "In the main paper, we proposed to look at representation from the perspective of similarity between pairs of instances, and we expressed similarity as dot products in representation space (e.g. [4]):\n$k(x, x') = \u03a6(x)^T \u03a6(x')$ (1)\nThe function represents PubMedCLIP's mapping from raw pixel values to its representation. Fig. 1 (left) shows the output of our BiLRP analysis, highlighting pairs of features responsible for the high measured dot-product similarity. Our analysis systematically highlights that the predicted similarity is supported by spurious annotations on the X-ray images instead of the underlying pathology, a clear case of a Clever Hans effect. In other words, the PubMedCLIP representation supports similarity predictions that are right for the wrong reasons. Furthermore, the type and location of these artifacts used by PubMedCLIP vary across the subgroups of the dataset, with the artifacts in the NIH subgroup being much more homogeneous than those in the Github subgroup.\nThe reliance of the PubMedCLIP model on features that code for dataset-specific annotation artifacts rather than the underlying pathology can also be demonstrated by performing a t-SNE analysis [5] of its representation. We color code each data point in the t-SNE visualization according to its provenance (NIH or Github). The visualization is shown in Fig. 1 (right). It"}, {"title": "Analysis of the Downstream Classifier", "content": "We now examine the transmission of the CH effect from the representation learning model to the downstream classifier. We recall from the main paper that we built a linear readout $f(x) = w^T \u03a6(x) + \u03b8$ on top of the PubMedCLIP representation. Specifically, we have trained a linear SVM classifier on the COVID detection task, where f(x) < 0 represents COVID negatives and f(x) > 0 represents COVID positives. Note that the same SVM classifier can also be formulated in terms of dot-product similarities:\n$f(x) = \\sum_{i=1}^{N} \u03b1_i k(x, x_i) + \u03b8$ (2)\nThe predictions of the SVM can thus be seen as a combination of dot-product similarities of the type we analyzed above. After training the SVM model and selecting the hyperparameters using hold-out validation, we found that C = 0.01 and a class weighting scheme adjusted to balance the class representation yielded the best performance. We examine the SVM prediction performance over the different data subgroups. The results are shown in Table 1. We observe a strong inhomogeneity of performance between different data sources.\nTo further shed light on the instability in the SVM's decision strategy, we use the LRP explanation technique [6, 7]. LRP allows us to identify, for each predicted instance, the extent to which each input pixel contributed to the prediction. The results are shown in Fig. 2 for a selection of correctly classified instances from each data subgroup. Similar to the BiLRP analysis"}, {"title": "Supplementary Note B. Additional Results for Generic Image Models", "content": "In the main paper, we also experimented with generic image models trained with different unsupervised representation learning algorithms, specifically SimCLR [8, 9], BarlowTwins [10] and CLIP [11], the details of which can be found in the methods section of the main paper. We extend on these experiments by providing further analysis of the learned representations and the downstream classifiers built on top of them."}, {"title": "Analysis of the Unsupervised Representations", "content": "We first proceed with the BiLRP analysis of each model as well as a purely supervised model (a standard ResNet50 pre-trained on ImageNet). The results of the analysis are shown in Fig. 3 for additional image pairs of the class garbage truck and coho, respectively.\nRecall our findings from the main paper that similarity predictions built on unsupervised representations are often Clever Hans-like. For example, the similarity between image pairs of certain fish classes was supported by a human holding the fish in the background (either the human face for the CLIP model, or the human body for SimCLR and BarlowTwins). Similarly, for the truck data, the CLIP model was shown to rely heavily on textual logos to express similarity. Highlighting the heterogeneity of the unsupervised representations, the same textual logos were suppressed by SimCLR and BarlowTwins, presumably due to their eccentricity in the image. Here, we complement these results by including a supervised baseline in our analysis (leftmost example in Fig. 3). We find that the supervised baseline model focuses on task-relevant features better than unsupervised approaches. However, the supervised baseline also appears to be overfitted to the task at hand, matching features that belong to the same class, but failing to disentangle distinct object parts, such as the wheels and the roof of the truck.\nTo further verify the result of the BiLRP analysis, which among other things showed that textual logos are salient in the CLIP representation, we perform a t-SNE analysis [5] in representation space. We append to the original truck data an artificial dataset called 'logo', consisting of"}, {"title": "Analysis of Downstream Classifiers", "content": "To analyze the transmission of the unsupervised Clever Hans effect from the unsupervised representation to the downstream supervised models, in the main paper we investigated the construction of two supervised models on top of each representation (one for fish classification and one for truck classification). These models are linear-softmax classifiers trained with cross-entropy loss. We extend the analysis of these classifiers by analyzing their decision strategy using LRP heatmaps. We use the same LRP procedure that we have used to compute BiLRP explanations, and we apply the rule LRP-0 [7] in the last readout layer. LRP heatmaps are shown in Fig. 5. We also report the accuracy scores of each model for each data subset considered in Table 2. We further elaborate on the structure of the accuracy scores of the problematic data subgroups (truck images with a logo and fish images with a human) by providing the full confusion matrices in Fig. 6."}, {"title": "Supplementary Note C. Additional Results for Anomaly Detection", "content": "This note complements the unsupervised anomaly detection experiments from the main paper. It provides the Explainable AI analysis and measured performance before and after CH mitigation, for each MVTec category retained in our experiments. Note that our criterion for retaining an MVTec category was that the D2Neighbors model accurately predicts it with F1 scores above 0.9 on the original data (cf. Table 3, column original).\nTo verify the nature of these accurate anomaly predictions, and in particular to test whether they are based on a valid strategy or instead 'right for the wrong reasons', we applied an Explainable AI analysis in the main paper. Here we provide the details of the analysis for the five retained MvTec categories and their corresponding D2Neighbors model. The analysis provides pixel-wise explanations of the predicted anomaly scores. By introducing a Discrete Cosine Transform (DCT) virtual layer, as described in the main paper, the analysis is extended to produce frequency and joint pixel-frequency explanations. Explanations for typical instances of each retained MVTec-AD category are shown in Figure 7.\nFor each MvTec category, the D2Neighbors model responds to a wide range of frequencies, with different frequency bands contributing differently to the decision strategy. In particular, the true anomaly patterns are mostly expressed in the mid-frequency range, while the low and high frequency bands contain artifacts. Especially in the high frequency range, we can see the"}]}