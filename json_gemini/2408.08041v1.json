{"title": "The Clever Hans Effect in Unsupervised Learning", "authors": ["Jacob Kauffmann", "Jonas Dippel", "Lukas Ruff", "Wojciech Samek", "Klaus-Robert M\u00fcller", "Gr\u00e9goire Montavon"], "abstract": "Unsupervised learning has become an essential building block of AI systems. The representations it produces, e.g. in foundation models, are critical to a wide variety of downstream applications. It is therefore important to carefully examine unsupervised models to ensure not only that they produce accurate predictions, but also that these predictions are not \u201cright for the wrong reasons\u201d, the so-called Clever Hans (CH) effect. Using specially developed Explainable AI techniques, we show for the first time that CH effects are widespread in unsupervised learning. Our empirical findings are enriched by theoretical insights, which interestingly point to inductive biases in the unsupervised learning machine as a primary source of CH effects. Overall, our work sheds light on unexplored risks associated with practical applications of unsupervised learning and suggests ways to make unsupervised learning more robust.", "sections": [{"title": "Introduction", "content": "Unsupervised learning is a subfield of machine learning that has gained prominence in recent years [1-3]. It ad- dresses fundamental limitations of supervised learning, such as the lack of labels in the data or the high cost of acquiring them. Unsupervised learning has achieved successes in modeling the unknown, such as uncovering new cancer subtypes [4, 5] or extracting novel insights from large historical corpora [6]. Furthermore, the fact that unsupervised learning does not rely on task-specific labels makes it a good candidate for core AI infrastructure: Unsupervised anomaly detection provides the basis for various quality or integrity checks on the input data [7-10]. Unsupervised learning is also a key technology behind \u2018foundation models' [1, 11\u2013 15] which extract representations upon which various downstream models (e.g. classification, regression, 'generative AI', etc.) can be built. The growing popularity of unsupervised learning models creates an urgent need to carefully examine how they arrive at their predictions. This is essential to ensure that potential flaws in the way these models process and represent the input data are not propagated to the many downstream supervised models that build upon them.\nIn this paper, we show for the first time that unsupervised learning models largely suffer from Clever Hans (CH) effects [16], also known as \u201cright for the wrong reasons\". Specifically, we find that unsupervised learning models often produce representations from which instances can be correctly predicted to be e.g. similar or anomalous, although largely supported by data quality artifacts. The flawed prediction strategy is not detectable by common evaluation benchmarks such as cross-validation, but may manifest itself much later in 'downstream' applications in the form of unexpected errors, e.g. if subtle changes in the input data occur after deployment (cf. Fig. 1). While CH effects have been studied quite extensively for supervised learning [16-20], the lack of similar studies in the context of unsupervised learning, together with the fact that unsupervised models supply many downstream applications, is a cause for concern.\nFor example, in industrial inspection, which often relies on unsupervised anomaly detection [9, 10], we find that a CH decision strategy can systematically miss a wide range of manufacturing defects, resulting in potentially significant costs. As another example, unsupervised foundation models, advocated in the medical domain to provide robust features for various specialized diagnostic tasks, can potentially introduce CH effects into many of these tasks, with the prominent risk of large-scale misdiagnosis. These scenarios (illustrated in Fig. 1) highlight the practical implications of an unsupervised CH effect, which, unlike its supervised counterpart, may not be limited to malfunctioning in a single specific task, but potentially in all downstream tasks.\nTo uncover and understand unsupervised Clever Hans effects, we propose to use Explainable AI [21-25] (here techniques that build on the LRP explanation framework [26-28]). Our proposed use of these techniques allows us to identify at scale which input features are used (or misused) by the unsupervised ML model, without having to formulate specific hypotheses or downstream tasks. Specifically, we use an extension of LRP called BiLRP [6] to reveal input patterns that are jointly responsible for similarity in the representation space. We also combine LRP with 'virtual layers' [29, 30] to reveal pixel and frequency components that are jointly responsible for predicted anomalies.\nFurthermore, our Explainable AI-based analysis allows us to pinpoint more formal causes for the emergence of unsupervised CH effects. In particular, they are due not so much to the data, but to the unsupervised learning machine, which hinders the integration of the true task-supporting features into the model, even though vast amounts of data points are available. Our findings provide a novel direction for developing targeted strategies to mitigate CH effects and increase model robustness.\nOverall, our work sheds light on the presence, prominence, and distinctiveness of CH effects in unsupervised learning, calling for increased scrutiny of this essential component of modern AI systems."}, {"title": "Results", "content": "Using Explainable AI, we investigate the emergence of Clever Hans effects in a representative set of unsupervised learning tasks, including representation learning and anomaly detection."}, {"title": "Clever Hans Effects in Representation Learning", "content": "We first consider an application of representation learning in the context of detecting COVID-19 cases from X-ray scans [31, 32]. Simulating an early pandemic phase characterized by data scarcity, similar to [32], we use a dataset aggregation approach where a large, well-established non-COVID-19 dataset is merged with a more recent COVID-19 dataset aggregated from multiple sources. Specifically, we aggregate 2597 instances of the CXR8 dataset [33] from the National Institute of Health (NIH), collected between 1992 and 2015, with the 535 instances of the GitHub-hosted 'COVID-19 image data collection' [34]. We refer to these subsets as 'NIH' and 'GitHub', respectively.\nFurther motivated by the need to accommodate the critically small number of COVID-19 instances in the aggregated dataset and to avoid overfitting, we choose to rely on the representations provided by unsupervised foundation models [15, 35-37]. Specifically, we feed our data into a pre-trained PubMedCLIP model [36], which has built its representation from a very large collection of X-ray scans in an unsupervised manner. Based on the PubMedCLIP representation, we train a downstream classifier that separates COVID-19 from non-COVID-19 instances with a class-balanced accuracy of 88.6% on the test set (cf. Table 1). However, a closer look at the structure of this performance score reveals a strong disparity between the NIH and GitHub subgroups, with all NIH instances being correctly classified and the"}, {"title": "Alleviating CH in Unsupervised Learning", "content": "Leveraging the Explainable AI analysis above, we aim to build models that are more robust across different data subgroups and in post-deployment conditions. Unlike previously proposed CH removal techniques [18, 19], we aim to operate on the unsupervised model rather than the downstream tasks. This potentially allows us to achieve broad robustness improvements while leaving the downstream learning machines (training supervised classifiers or adjusting detection thresholds) untouched.\nWe first test our CH mitigation approach on the CLIP model, which our Explainable AI analysis has shown to incorrectly rely on text. Specifically, we focus on an early layer of the CLIP model (encoder.relu3) and remove the feature maps that are most responsive to text. We then apply a similar CH mitigation approach to anomaly detection, which our Explainable AI analysis has shown to be overexposed to high frequencies. Here, we propose to prune the high frequencies by inserting a blur layer at the input of the model.\nIn both cases, the proposed CH mitigation technique provides strong benefits in terms of model robustness. As shown in Table 1, rows 'CH mitigation', our robustified models substantially reverse the performance degradation observed in post-deployment conditions, reaching performance levels close to, and in some cases superior to, those measured on the original data."}, {"title": "Discussion", "content": "Unsupervised learning is an essential category of ML that is increasingly used in core AI infrastructure to power a variety of downstream tasks, including generative AI. Much research to date has focused on improving the performance of unsupervised learning algorithms, for example, to maximize accuracy scores in downstream classification tasks. These evaluations often pay little attention to the exact strategy used by the unsupervised model to achieve the reported high performance, in particular whether these models rely on Clever Hans strategies.\nBuilding on recent techniques from Explainable AI, we have shown for the first time that CH strategies are prevalent in several unsupervised learning paradigms. These strategies can take various forms, such as correctly predicting the similarity of two X-ray scans based on irrelevant shared annotations, or predicting that an image is anomalous based on small but widespread pixel-level artifacts. These flawed prediction strategies result in models that do not transfer well to changing conditions at test time. Most importantly, their lack of robustness infects many downstream models that rely on them. As we have shown in two use cases, this can lead to widespread misdiagnosis of patients or failure to detect manufacturing defects.\nTherefore, addressing CH effects is a critical step towards reliable use of unsupervised learning methods. However, compared to a purely task-specific supervised approach, the addition of an unsupervised component, potentially serving multiple downstream tasks, adds another dimension of complexity to the modeling problem. In particular, one must decide whether to handle CH effects in the downstream classifier or directly in the unsupervised model part. Approaches consisting of dynamically updating downstream models in response to changing conditions [56-59], or revising their decision strategies with human feedback [18, 19, 60] are possible solutions to maintain high accuracy. However, these approaches may not be sustainable because CH mitigation must be performed repeatedly for each new downstream task. The problem may persist even after a flaw in the foundation model becomes known (e.g. [61, 62]), due to release intervals and the high cost of retraining these models. Instead, our results argue for addressing CH effects directly when building the unsupervised model, with the goal of achieving persistent robustness that benefits existing and future downstream applications.\nUsing advanced Explainable AI techniques targeted at unsupervised learning, we have uncovered multiple and diverse CH effects, such as the reliance of CLIP (and its derivative PubMedCLIP) on textual annotations, or a systemic over-exposure of unsupervised anomaly models to noise. Interestingly, our analysis has revealed that these unsupervised CH effects differ from supervised ones in that they arise less from the data and more from inductive biases in the model and learning algorithm. These include spurious suppression/amplification effects caused by the representation learning's training objective, or a failure of unsupervised anomaly detection architectures to replicate frequency filtering mechanisms found in supervised learning [63-65], leaving the learned models highly exposed to noise. Furthermore, our Explainable AI analysis not only provided new insights into the formal causes of unstable behavior in unsupervised learning. Our experiments also showed how pruning the high-frequency or spuriously amplified features revealed by our Explainable AI analysis leads to systematic performance improvements on difficult data subgroups or in post-deployment conditions. In doing so, we have demonstrated the actionability of our Explainable AI approach, showing that it can guide the process of identifying and subsequently correcting the faulty components of an unsupervised learning model.\nWhile our investigation of unsupervised CH effects and their consequences has focused on image data, extension to other data modalities seems straightforward. Explainable AI techniques such as LRP, which are capable of accurate dataset-wide explanations, operate independently of the type of input data. They have recently been extended to recurrent neural networks [66], graph neural networks [67], transformers [68], and state space models [69], which represent the state of the art for large language models and other models of structured data. Thus, our analysis could be extended to analyze other instances of unsupervised learning, such as anomaly detection in time series or the representations learned by large language models (e.g. [70, 71]).\nOverall, we believe that the CH effect in unsupervised learning, and the uncontrolled risks associated with it, is a question of general importance, and that Explainable AI and its recent developments provide an effective way to tackle it."}, {"title": "Methods", "content": "This section first introduces the unsupervised ML models studied in this work and the datasets on which they are applied. It then presents the layer-wise relevance propagation (LRP) method for explaining predictions, its BiLRP extension for explaining similarity, and the technique of 'virtual layers' for generating joint pixel-frequency explanations.\nRepresentation learning experiments were performed on the SimCLR [12, 38], CLIP[13], BarlowTwins [39] and PubMedCLIP [36] models. All include a version based on"}, {"title": "Explanations for Representation Learning", "content": "Our experiments examined dot product similarities in representation space, \u0456.\u0435. \\(y = \\langle \\Phi(x), \\Phi(x') \\rangle\\), where \\(I\\) denotes the function that maps the input features to the representation, typically a deep neural network. To understand similarity scores in terms of input features, we used the BiLRP method [79] which extends the LRP technique [24, 26, 27, 80] for this specific purpose. The conceptual starting point of BiLRP is the observation that a dot product is a bilinear function of its input. BiLRP then proceeds by reverse propagating the terms of the bilinear function to pairs of activations from the layer below and iterating down to the input. Denoting \\(R_{kk'}\\) the contribution of neurons k and k' to the similarity score in some intermediate layer in the network, BiLRP extracts the contributions of pairs of neurons j and j' in the layer below via the propagation rule:\n\\[R_{jj'} = \\sum_k \\sum_{k'} \\frac{Z_{jk}Z_{j'k'}}{\\sum_{i, i'} Z_{ik}Z_{i'k'}} R_{kk'} \\\\(1)\\]\nIn practice, this reverse propagation procedure can be implemented equivalently, but more efficiently and easily, by computing a collection of standard LRP explanations (one for each neuron in the representation layer) and recombining them in a multiplicative manner:\n\\[BiLRP(y) = \\sum_{k} LRP(\\Phi_{k}(x)) \\otimes LRP(\\Phi_{k}(x')) \\\\(2)\\]\nOverall, assuming the input consists of d features, BiLRP produces an explanation of size d x d which is typically represented as a weighted bipartite graph between the set of features of the two input images. Due to the large number of terms, pixel-to-pixel contributions are aggregated into patch-to-patch contributions, and elements of the BiLRP explanations that are close to zero are omitted in the final explanation rendering. In our experiments, we computed BiLRP explanations using the Zennit implementation of"}, {"title": "Explanations for the D2Neighbors Model", "content": "The D2Neighbors model we investigate for anomaly detection is a composition of a distance layer and a soft min-pooling layer. To handle these layers, we use the purposely designed LRP rules of [28, 53]. Propagation in the softmin layer (\\(M^\\gamma\\)) is given by the formula\n\\[R_{j} = \\frac{f(||x - u_{j}||_{\\rho})}{\\sum_{j} f(||x - u_{j}||_{\\rho})} o(x) \\\\(3)\\]\na 'min-take-most' redistribution, where f is the same function as in \\(M^\\gamma\\). Each score \\(R_{j}\\) can be interpreted as the contribution of the training point \\(u_{j}\\) to the anomaly of x. To further propagate these scores into the pixel frequency domain, we adopt the framework of 'virtual layers' [29, 30] and adapt it to the D2Neighbors model. As a frequency basis, we use the discrete cosine transform (DCT) [54], shown in Fig. 4 (right), which we denote by its collection of basis elements \\((v_k)_k\\). Since the DCT forms an orthogonal basis, we have the property \\(\\sum_{k} v_{k}v_{k}^T = I\\), and multiplication by the identity matrix can be interpreted as a mapping to the frequencies and back. For the special case where \\(\\rho = 2\\), the distance terms in D2Neighbors reduce to the squared Euclidean norm \\(||x - u_{j}||^{2}\\). These terms can be developed to identify pixel-pixel-frequency interactions: \\(||x - u_{j}||^{2} = (x \u2013 u_{j})^T(\\sum_{k} v_{k}v_{k}^T)(x \u2013 u_{j}) = \\sum_{k} \\sum_{i, i'} [x \u2013 u_{j}]_{i}[x \u2013 u_{j}]_{i'} [v_{k}]_{i}[v_{k}]_{i'}\\). From there, one can construct an LRP rule that propagates the instance-wise relevance \\(R_{j}\\) to the pixel-frequency features:\n\\[R_{i i' k} = \\sum_{j} \\frac{[x \u2013 u_{j}]_{i}[x \u2013 u_{j}]_{i'} [v_{k}]_{i}[v_{k}]_{i'}}{\\epsilon + ||x - u_{j}||^{2}} R_{j} \\\\(4)\\]\nwhere the variable \\(\\epsilon\\) is a small positive term that handles the case where x and \\(u_{j}\\) overlap. A reduction of this propagation rule can be obtained by marginalizing over interacting pixels \\((R_{i k} = \\sum_{i'} R_{i i' k})\\). Further reductions can be obtained by marginalizing over pixels \\((R_{k} = \\sum_{i} R_{i k})\\) or frequencies \\((R_{i} = \\sum_{k} R_{i k})\\). These reductions are used to generate the heatmaps in Fig. 5."}]}