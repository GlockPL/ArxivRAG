{"title": "Leveraging Large Language Models for Wireless Symbol Detection via In-Context Learning", "authors": ["Momin Abbas", "Koushik Kar", "Tianyi Chen"], "abstract": "Deep neural networks (DNNs) have made significant strides in tackling challenging tasks in wireless systems, especially when an accurate wireless model is not available. However, when available data is limited, traditional DNNs often yield subpar results due to underfitting. At the same time, large language models (LLMs) exemplified by GPT-3, have remarkably showcased their capabilities across a broad range of natural language processing tasks.\nBut whether and how LLMs can benefit challenging non-language tasks in wireless systems is unexplored.\nIn this work, we propose to leverage the in-context learning ability (a.k.a. prompting) of LLMs to solve wireless tasks in the low data regime without any training or fine-tuning, unlike DNNs which require training. We further demonstrate that the performance of LLMs varies significantly when employed with different prompt templates. To solve this issue, we employ the latest LLM calibration methods. Our results reveal that using LLMs via ICL methods generally outperforms traditional DNNs on the symbol demodulation task and yields highly confident predictions when coupled with calibration techniques.", "sections": [{"title": "I. INTRODUCTION", "content": "As the era of AI unfolds, it is expected that deep learning models will play a central role in shaping the future of wireless systems [1]. Most work on AI in wireless communication leverages deep neural networks (DNNs) [2, 3, 4, 5]. To successfully integrate deep learning models into wireless systems, a key requirement is the ability to rapidly adapt to changing environmental conditions, even with limited information about the wireless systems [6, 7]. This includes their ability to handle constantly changing wireless channel conditions using only a few pilot signals [8].\nDNN-based nonlinear channel predictors have been proposed through training of recurrent neural networks [9], convolutional neural networks [10], and multi-layer perceptrons [11]. However, several studies, including [11, 12], have reported that deep learning based predictors tend to require a large number of training data, while failing to outperform well-designed linear filters in the low-data regime. This challenge becomes pronounced as neural networks increase in depth; see Table IV. This is critical in resource-constrained wireless systems, where the acquisition of data is expensive, necessitating costly hardware and skilled labor.\nAt the same time, Large Language Models (LLMs) have been effectively utilized to address a variety of Natural Lanuage Processing (NLP) and Computer Vision (CV) tasks [13, 14]. Despite significant advancements in NLP and CV, pre-trained LLMs have faced limitations in their development within non-linguistic tasks, let alone wireless tasks. Therefore, combining wireless communications and natural language remains a challenge to utilize these capabilities.\nIn this work, we strive to achieve the best of both worlds by leveraging in-context learning abilities of LLMs on the wireless symbol demodulation task. Our contributions are summarized below:\nC1) We highlight the challenge in training traditional DNNs for symbol demodulation with limited data. To overcome this, we propose harnessing the in-context learning (ICL) abilities of LLMs through inference alone, without requiring any subsequent training or fine-tuning.\nC2) As LLMs via ICL for wireless data is sensitive to changes in prompt templates, we propose employing state-of-the-art (SOTA) calibration methods [15, 16] designed for LLMs.\nC3) We empirically show that ICL methods generally outperform traditional DNNs in scenarios with limited data (e.g. 22% performance improvement for 32-shots)."}, {"title": "B. Related Work", "content": "The majority of research in AI for communications relies on traditional frequentist learning methods that use traditional DNNs [2, 3, 4]. These methods involve minimizing the (regularized) training loss, which serves as an estimate of the ground-truth population loss. However, in scenarios with limited data, this estimate becomes unreliable. Consequently, focusing on a single, optimized model parameter vector often results inaccurate and poorly calibrated probabilistic predictors, leading to overconfident decisions [17, 18].\nSome methods focus on enhancing the calibration of DNNs through a validation-based post-processing phase. Platt scaling and temperature scaling [17, 19] determine a fixed parametric mapping of the trained model output that minimizes the validation loss. In contrast, isotonic regression [20] utilizes a non-parametric binning approach. However, since these models primarily target either simple machine learning models or traditional DNNs, they often perform poorly in scenarios with limited data. [21, 22] examine how conformal prediction can be utilized as a general framework to ensure that AI models provide decisions with formal calibration guarantees. However, their notion of calibration differs significantly from ours. They transform probabilistic predictors into set predictors, where the set predictor is considered well-calibrated if it contains the correct output, and their goal does not prioritize performance improvement.\nRecently, LLMs such as GPT-3 [14] have showcased the capability of in-context learning. This feature enables a model to generate suitable outputs for a given query input by leveraging a prompt containing input-output example pairs tailored to the task at hand. ICL has proven to be highly effective in linguistic tasks with limited data, as it operates without the need for explicit training. However, ICL performance fluctuates across various prompt templates due to inadequate calibration [15]. In our work, we employ advanced LLM calibration methods, as proposed in [15, 16], to achieve high performance on wireless data while ensuring precise calibration of the LLMs."}, {"title": "II. FORMULATION AND SOLUTION APPROACH", "content": "In this section, we introduce the data model and then explore the difference between DNNs and LLMs."}, {"title": "A. Wireless Symbol Demodulation", "content": "We consider the wireless symbol demodulation problem from a discrete constellation, relying on received baseband signals that are susceptible to hardware imperfections, noise, and fading [18, 23, 24]. Define \\(y_i\\) as the i-th transmitted symbol, and \\(x_i\\) as the corresponding received signal. Each transmitted symbol \\(y_i\\) is drawn uniformly at random from a given constellation \\(Y\\). We model I/Q imbalance at the transmitter and phase fading as in [21]. Accordingly, the ground-truth channel law connecting symbols \\(y_i\\) into received samples \\(x_i\\) is described by the equality\n\n\\(x_i = e^{j \\Psi} f_{IQ}(y_i) + v_i,\\)"}, {"title": "B. Deep Neural Networks (DNNs)", "content": "We consider a supervised learning setup with a dataset \\(D = \\{x_i, y_i\\}_{i=1}^{N}\\), consisting of N examples represented as inputs \\(x_i\\) corresponding outputs \\(y_i\\). The goal is to make predictions for new, unseen test inputs \\(X_{test}\\) with an unknown output \\(Y_{test}\\).\nWe are given a probabilistic predictor that implements a parametric conditional distribution model \\(p(Y_{test}|X_{test}, \\phi)\\) on the output \\(Y_{test} \\in Y\\) from input \\(X_{test} \\in X\\), where \\(\\phi \\in \\Phi\\) denotes parameters of a DNN model. Given the training data set D, the training algorithm produces an optimized \\(\\phi^{\\star}\\). For example, for a classification problem with K labels (i.e. \\(|Y| = K\\)), \\(p(Y_{test}|X_{test}, \\phi) \\in \\mathbb{R}^{K}\\) represents the last layer post-softmax probability vector. We can then obtain a point prediction \\(\\hat{y}_{test}\\) for output \\(Y_{test}\\) given input \\(X_{test}\\) as the probability-maximizing output as\n\n\\(\\hat{y}_{test} (X_{test}|D) = \\arg \\max_{y_{test} \\in D} p(y_{test}|X_{test}, \\phi^{\\star}).\\)\n\nHowever, this represents the conventional approach that uses training data to train a neural network and then uses the trained model to make predictions on new test instances."}, {"title": "C. Proposed Approach: LLM-based ICL (LMIC)", "content": "The most common method to leverage capabilities of LLMs is to fine-tune the LLM for specific tasks. However, fine-tuning LLMs can be problematic due to instability [26] caused by various hyperparameter configurations, leading to failed runs, unstable outcomes, and overfitting [27]. Moreover, fine-tuning such large models can be costly and requires access to extensive data and the architecture and weights of LLMs, which may not be publicly available [28].\nMoreover, applying LLMs to non-language wireless tasks may require architecture adjustments, such as modifying input/output layers and loss functions [29]. Therefore, it is natural to ask: Can we use LLMs for wireless tasks without altering the architecture or loss function? We explore this question using the in-context learning abilities of LLMs to solve wireless tasks. This approach offers a streamlined \u201cno-code machine learning\u201d framework, enabling individuals with limited programming or machine learning expertise to address wireless tasks effortlessly.\nTo reduce lengthy fine-tuning processes and eliminate the need for accessing or modifying the model, recent advancements in LLMs, such as GPT-3, have showcased the capability of in-context learning. ICL is a training-free approach enabling the model to generate appropriate outputs for test samples by using prompts containing task-specific input-output examples. This approach works through an API without requiring direct access to the LLM. A visual representation of ICL is provided in Fig. 1.\nSpecifically, ICL aims to predict a test sample \\(X_{test}\\) by conditioning on a prompt sequence (\\(f_x(x_1), f_y(Y_1),\\dots, f_x(x_N), f_y(Y_N), f_x(x_{test}))\\). This sequence includes N-shot samples \\(D = \\{x_i, Y_i\\}_{i=1}^{N}\\) (aka demonstrations) and the query test sample \\(X_{test}\\). Here, \\(f_x(.)\\) and \\(f_y(.)\\) are template functions that provide predefined text descriptions for input and output, respectively (refer to text highlighted in yellow in Fig. 1). Additionally, the output template function \\(f_y(.)\\) may convert labels \\(y_i\\) into natural language format instead of numeric/one-hot labels. For instance, in binary classification, it could transform labels (0,1) into (Positive, Negative) (see labels in Fig. 1). Together, \\(f_x(.)\\) and \\(f_y(.)\\) constitute the prompt template, providing a textual interpretation of the data. A prompt P for an input \\(X_{test}\\) is defined as:\n\n\\(P(X_{test}, \\{ (x_i, Y_i) \\}_{i=1}^{N}) \\coloneqq d_1 \\oplus d_2 \\oplus \\dots \\oplus d_N \\oplus f_x(X_{test}),\\)\n\nwhere each demonstration \\(d_i\\) is given by \\(f_x(x_i) \\oplus f_y(y_i)\\) and \\(\\oplus\\) denotes the concatenation operation.\nThen for a pretrained LLM \\(M_{\\phi^{\\star}}\\) parameterized by \\(\\phi^{\\star}\\), \n\n\\(P_{LLM}(Y_{test}|X_{test}, \\phi^{\\star}, D) \\equiv M_{\\phi^{\\star}} (P(x_{test}, \\{ (x_i, Y_i) \\}_{i=1}^{N})).\\)\n\nNote that the pre-trained LLM \\(M_{\\phi^{\\star}}\\) is not trained or fine-tuned on the training data D and is therefore independent of D. Instead, the training data serves as a \u2018context' within the prompt, comprising a sequence of input-label pairs known as demonstrations. Such a capability of an LLM to learn \u201cin-context\u201d presents an intriguing aspect whereby the LLM is capable of acquiring knowledge and performs well on a wide range of downstream tasks without any task-specific fine-tuning [14]. Moreover, for our approach, \\(x_i\\) (\\(y_i\\)) represent the received (transmitted, respectively) signals (see Sec. II-A) and the corresponding prompt is shown in Table I, where the bold numbers indicate the raw data for \\(x_i\\) (\\(y_i\\)).\nHowever, the dimension of \\(P_{LLM}(Y_{test}|X_{test}, \\phi^{\\star}, D)\\) is not K as in traditional neural networks; instead, it corresponds to the number of tokens present in the vocabulary of the LLM; this is because LLMs perform next-token prediction, hence the dimension matches the number of tokens in LLM vocabulary. Hence, we proceed by sampling tokens corresponding to the classes in the label space, resulting in a probability vector of size K. Finally, we can get the prediction following the same rule as Eq. (3), replacing \\(p(Y_{test}|X_{test}, \\phi)\\) by \\(P_{LLM}(Y_{test}|X_{test}, \\phi^{\\star}, D)\\). We use this as our base method and refer to it as vanilla ICL.\nHowever, recent studies show that vanilla ICL's performance varies widely across different prompt templates and demonstrations [15, 16], ranging from random guessing to state-of-the-art levels. Additionally, we find that while these GPT-like models perform adequately via vanilla ICL, their predictions lack reliability on wireless tasks when assessed with Shannon entropy (see Figs. 3-4). This observation aligns with the reliability concerns of LLMs identified in linguistic tasks [16]. We gauge this is related to the poor calibration of LLMs [15]. To address these reliability challenges posed by vanilla ICL, we leverage latest state-of-the-art (SOTA) calibration methods for LLMs, namely Contextual Calibration (ConC) [15] and Linear Probe Calibration (LinC) [16]. Accuracy and calibration are independent criteria, with the presence of one not implying the other [21].\nFor brevity of notation, we denote the output probabilities \\(P_{LLM}(Y_{test}|X_{test}, \\phi^{\\star}, D)\\) as \\(p\\). Then, the goal is to linearly adjust the output probabilities using an affine transformation, also known as Platt Scaling [19]:\n\n\\(p' = \\text{softmax}(Ap + b),\\)\n\nwhere \\(A \\in \\mathbb{R}^{K \\times K}\\) and \\(b \\in \\mathbb{R}^{K}\\) represent parameters applied to the original probabilities \\(p\\) to obtain new probabilities \\(p'\\). ConC then uses a prompt \\(P_{cf} = P(\"N/A\", \\{ (x_i, Y_i) \\}_{i=1}^{N})\\) (where test point \\(X_{test}\\) is replaced with a content-free (cf) input such as the string \u201cN/A\u201d and obtain p for this content-free input, denoted by \\(p_{cf}\\) i.e. \\(p_{cf} = M_{\\phi^{\\star}}(P_{cf})\\). The parameters are set via (i.e. no training needed)\n\n\\(A = \\text{diag}(P_{cf})^{-1} \\text{and} b = 0.\\)\n\nIn contrast, LinC begins with a predefined set of calibration parameters, including zero initialization. Following this, LinC uses a few additional prompts to train the matrix A and vector b before applying the affine transformation (for details, see [16]). However, in our case, we do not use any additional samples and reuse the same N-shot demonstrations for training low-dimensional parameters A and b."}, {"title": "III. NUMERICAL EVALUATION", "content": "We apply our LMIC approach, as described in Sec. II-C, to address the symbol demodulation problem [23, 30] in the presence of transmitter hardware imperfections. Unlike previous studies [18, 24], which focused on frequentist and Bayesian learning via traditional DNNs, our goal is to use LMIC to achieve high accuracy and precise calibration under a severely limited resource regime (e.g., < 50 data samples), where traditional DNNs fail miserably (see Table IV).\nDemodulation is implemented via an LLM \\(M_{\\phi^{\\star}}\\) as a next-token prediction problem (see Sec. II-C). We used GPT-J [31] with 6B parameters and two variants of the latest Llama-2 [32] with 7B and 13B parameters\u00b9.\nThe last layer implements a softmax classification for the \\(K = |Y|\\) possible constellation points. We employ the Amplitude-Phase-Shift-Keying (APSK) modulation with \\(K = 8\\). The SNR level is set to SNR = 5 dB. The amplitude and phase imbalance parameters are independent and distributed as \\(\\epsilon \\sim Beta(\\epsilon/0.15|5, 2)\\) and \\(\\delta \\sim Beta(\\delta/15^{\\circ}|5,2)\\), respectively [24]. Unless specified otherwise, LMIC methods employ a fixed prompt template chosen manually to enhance performance, as demonstrated alongside examples in Table I; bold numbers represent the raw data.\nAll our experiments were conducted on two NVIDIA RTX 3090 GPUs. As mentioned before, we consider the low resource regime, where the number of available training samples is scare (typically under <50). This is especially vital in resource-constrained scenarios where acquiring wireless data is expensive due to the costly hardware and skilled labor.\nAs baselines, we trained a fully connected deep neural network (DNN) similar to the one considered in [18, 21] with real inputs \\(x_i\\) of dimension 2, following Eq. (1). It consists of four hidden layers, with 10 neurons in the first hidden layer and 30 neurons in each subsequent hidden layer, each activated by ReLU. We also considered deeper networks with five, six and seven layers with 30 neurons in each additional hidden layer. The final layer performs softmax classification for the \\(Y\\) possible constellation points.\nTo ensure a fair comparison, each DNN is trained using the identical set of samples employed as demonstrations within the prompt for LMIC methods. For instance, if there are 8-shots (i.e. demonstrations) in the prompt, the DNN baseline is trained using the same set of 8 samples.\nOur main results are shown in Table IV. We observe that across most experiments (i.e., 15 out of 21 cases), our LMIC methods, particularly ConC and LinC, consistently demonstrate superior performance compared to the DNN baselines. This showcases the robust generalization capability of LLMs across various model sizes and few-shot settings. For instance, in the 32-shot experiment, Llama-2 7B outperforms the DNN-4 by a significant margin of about 22% (69.31% vs. 47.52%). Such a capability of ICL to understand contextual information from a handful of samples is particularly intriguing, especially when considering that the data is non-linguistic wireless data. We also note that the Llama-2 model outperforms the GPT-J model. This could be because Llama-2, released in August 2023, is one of the most recent models and therefore pre-trained on larger amounts of more recent data. We also notice that while the performance of DNNs generally declines with an increase in layers, eventually approaching near-guess accuracy with 7 hidden layers, the opposite trend is observed for LLMs: performance improves as the number of parameters increases (c.f. GPT-J 6B vs Llama-2 7/13B). Also, there is no consistent pattern in the performance of varying model sizes within the LLM family (i.e. Llama-2 7B vs 13B), which is consistent with previous works [16]. However, Llama-2 7B notably achieves the highest accuracy of 69.31% for 32-shots. Moreover, we observe that when the number of samples is less than the number of classes (i.e. N < K), DNNs with fewer hidden layers usually perform better than our LMIC methods (c.f. GPT-J and Llama-2 13, 5/6-shot results with DNN-4/5. This observation is in line with previous works that emphasize the pivotal role of label space in the success of ICL [33].\nAs previously mentioned, prior works suggest that the performance of vanilla ICL fluctuates across different prompt templates in linguistic tasks. To investigate if this phenomenon also holds for non-linguistic wireless data, we use ten distinct prompt templates, four of which are listed in Table II (due to space constraints), on Llama-2 7B under 8-shot setting. From Fig. 2, indeed the performance of vanilla ICL is volatile across different prompts while the latest SOTA calibration methods exhibit substantial improvements in accuracy with notably lower variance, highlighting the effectiveness of these methods in enhancing the model's performance across various prompt templates.\nWe further evaluate the reliability of LLM predictions by employing the Shannon entropy metric [34], which measures the expected uncertainty in a probability distribution p). A model is considered better when entropy values are lower. For this experiment we used our largest Llama-2 13B model to compare vanilla ICL and LinC, showing results for 4/8/16/32-shots in Figs. 3-4. We oberve that employing vanilla ICL results in high entropy values, suggesting that most test predictions were made with very low confidence, indicating a tendency towards random guessing. These findings are consistent with prior studies for linguistic data [16]. In contrast, a calibrated LLM via LinC produces significantly lower entropy values, reflecting the increased confidence.\nTo further evaluate LLM calibration, we utilize the widely-used Expected Calibration Error (ECE) metric [35] to quantify the distance between predicted and actual probabilities. Table III shows the results on Llama-2 7B, 32-shot setting (we only present this setting due to limited space). We observe that LLM calibration methods such as ConC and LinC consistently exhibit much lower ECE when compared to vanilla ICL, highlighting the critical importance of calibrating LLMs for wireless data.\nDespite the merits of our approach, it has some limitations. Although our method eliminates the need for GPU-intensive training, it still relies on GPU memory for inference, albeit at a reduced scale compared to training or fine-tuning. Nevertheless, with the increasing adoption of LLMs across various domains, we anticipate that GPUs will become more readily accessible for deployment in wireless communication tasks in the future. Lastly, although we employed manually selected prompts and demonstrations, exploring how to integrate our framework with methods for selecting better demonstrations and prompt templates is an interesting future direction."}, {"title": "IV. CONCLUSIONS", "content": "While LLMs have been extensively studied for linguistic tasks, their utilization for non-linguistic wireless data remains largely unexplored. In this work, we capitalize on the in-context learning abilities of LLMs that not only achieves high performance but also yields highly confident predictions when integrated with SOTA LLM calibration techniques, especially in data-scarce scenarios where traditional DNNs typically fail. We believe these findings carry important implications for advancing wireless systems through large language models. In the future, our aim is to investigate the high-resource regime, utilizing abundant data and compute to first fine-tune LLMs and then employ our LMIC approach."}]}