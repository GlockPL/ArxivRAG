{"title": "MaxMind: A Memory Loop Network to Enhance Software Productivity Based on LLMs", "authors": ["Yuchen Dong", "Xiaoxiang Fang", "Yuchen Hu", "Renshuang Jiang", "Zhe Jiang"], "abstract": "The application of large language models to facilitate automated software operations and tool generation (SOTG), thus augmenting software productivity, mirrors the early stages of human evolution when the ability to create and use tools accelerated the progress of civilization. These complex tasks require AI to continuously summarize and improve. Current research often overlooks the importance of converting real-time task experiences into system memory and differentiating the value of existing knowledge for future reference. This paper addresses these issues by evolving external memory models into Memory-Loop Networks for timely memorization and experience referencing. We also enhance a RAG mechanism with knowledge precision segmentation to utilize memory based on value differ-entiation, and design the MaxMind model for SOTG accordingly. To demonstrate our approach, we developed MaxMind4Sheet, an electronic spreadsheet processing system aligned with the Max-Mind philosophy. Comparative experiments with SheetCopilot have demonstrated that the accumulation and recycling of task memories lead to a steady enhancement in task success rate, with an improvement rate of approximately 3%-6% per round in this implementation example. Note that as the memories continue to grow, this cumulative improvement may be substantial. The inclusion of memory recycling can also boost the system's task execution efficiency by up to 25%, and it can address the retraining issue faced by LLMs when handling specialized tasks through memories transfer. These suggest that MaxMind has significant potential to enhance the capabilities and productivity of LLM systems in SOTG.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid evolution of Large Language Models (LLMs) has spotlighted their potential in facilitating Software Op-erations and Tool Generation (SOTG), leveraging their ad-vanced natural language understanding and reasoning capabil-ities [1] [2] [3]. A typical example is SheetCopilot [4], which leverages a curated suite of atomic actions to automate spread-sheet program creation, achieving a commendable success rate. In contrast to typical Q&A(question answering) system [5] [6], SOTG endeavors are inherently more intricate, necessitating a heightened level of professional proficiency and rigorous reasoning skills.\nCurrent approaches, grounded in Chain of Thought (CoT) [1] [7] or Retrieval-Augmented Generation (RAG) [8] [9], frequently integrate domain-specific knowledge repositories. However, these methods encounter a significant performance decline when confronted with highly intricate or ambiguous tasks, owing to the limitations in timely updating and enhancing the knowledge base [10] [11]. This predicament leads to an escalation in error rates and failures, underscoring the need for more dynamic and adaptable solutions. Moreover, the existing frameworks exhibit limited retraining capabilities, with their performance faltering once they venture beyond the confines of the current dialogue context. The absence of a mechanism to retain and leverage past experiences and lessons learned undermines their potential for continuous improvement [12] [13]. Consequently, empowering the system with learning abilities that facilitate experience accumulation and enhanced task reasoning is paramount to advancing SOTG tasks. This necessitates the development of innovative strategies that can dynamically adapt, learn from mistakes, and continually refine their performance in the face of ever-evolving challenges."}, {"title": "II. RELATED WORKS", "content": "Researchers capitalize on the capabilities of LLM in com-prehension and transactional reasoning to integrate it seam-lessly with complementary tools and implementation method-ologies, ultimately fostering a remarkable enhancement in productivity. Paranjape et al. [14] automates multi-step reason-ing and tool selection, reducing manual programming needs. WebGPT [15] revolutionizes Web browsing with text-centric refinement, enhancing retrieval and generation via imitation and reinforcement learning. SheetCopilot [4] demonstrates LLM-driven closed-loop control in spreadsheets, generating tailored solutions for efficient automation. These research leverage external tools to bridge LLM gaps, enhancing rea-soning and task execution. This synergy strengthens Al's real-world comprehension and introduces novel solutions for challenges like semantic comprehension with fundamental functions. However, current advancements focus on LLMs' self-improvement and tool integration, primarily showcasing their capacity to learn and harness tools. This limits task diversity and also limits success rates for intricate tasks."}, {"title": "B. Retrieval-Augmented Generation", "content": "RAG empowers LLMs to extract info from diverse sources, boosting precision and reducing hallucinations. Recent focus include: (1) Retrieval Enhancement: Borgeaud et al. [16] inte-grates retrieval-augmented training, optimizing parameters and reducing complexity. ANCE [17] constructs realistic negative samples via ANN index updates. Karpukhin et al. [18] uses contrastive learning and similarity matrix for efficient answer extraction. However, challenges remain in capturing complex semantics and overlooking info, necessitating further RAG innovations. (2) Boosting Generator: cheng et al. [19] fine-tunes generator via memory pool, minimizing KL loss for precise memory selection. Asai et al. [20] improves text quality and factuality through retrieval and self-reflection. Current methods face memory capacity limits, hindering wide info coverage and adaptation to changing needs. (3) Integrating & Fine-tuning: Shi et al. [21] enhance LM with adjustable retrievers. Cheng et al. [22] create versatile retrievers for zero-shot tasks. lin et al. [23] and jiang et al. [9] optimize generator and retriever for better recall. However, black-box LLMs limit interpretability and flexibility, necessitating long-embedding models. In summary, RAG confronts key challenges: (1) Index accuracy. Content chunking and size determination may result in omissions or noise in answers. (2) Retrieval limitations. The query process occasionally missing relevant documents, extracting irrelevant context, or providing incomplete answers, hindering the full utilization of potential information. These issues impose stringent constraints on SOTG tasks."}, {"title": "C. Integration of Neural Networks with External Memory", "content": "Neural networks often struggle with long-term memory, hin-dering their ability to grasp associations over vast information spans and limiting applicability in contexts demanding knowl-edge accumulation. Memory Networks (MemNN) [24], featur-ing a dedicated memory module to store diverse information. By correlating inputs with memory content, MemNN extracts relevant data to generate outputs, fusing machine learning with deductive power through a read-write memory, fostering a dynamic long-term knowledge base while honing reasoning skills. Expanding on MemNN, Sainbayar et al. [25] devised an end-to-end training approach that iteratively mines valuable information for multi-faceted reasoning. Kumar et al. [26] advanced MemNN to handle dynamic linguistic inputs and complex relationships, introducing DMN. As an alternative approach to augmenting neural networks with memory capa-bilities, Google propose Neural Turing Machine (NTM) [27] in its DeepMind. NTM comprises a neural controller and memory module. The controller interacts with the environment via input/output vectors, managing memory read/write through a selective matrix. Key to its trainability, NTM's differentiable components facilitate gradient descent. MemNN and NTM both have their own memory, input, read, write, and output modules. However, MemNN's memory leans toward read optimization, while NTM's write module, crucial for capacity constraints, distinguishes it by deciding rewrite/erase zones. While enhancing task-specific performance, these methods have limitations. They rely on costly manual annotations and positively evaluated data, limiting the model's generalization to new, unseen data. Exclusive reliance on positive feedback may restrict its ability to identify and rectify negative at-tributes, hindering broad applicability."}, {"title": "III. MEMORY-LOOP NETWORK AND MAXMIND MODEL", "content": "The cornerstone of executing intricate SOTG resides in ad-dressing two pivotal challenges: firstly, fostering the accumula-tion of experience and ensuring the agility of knowledge bases through flexible, on-demand updates; secondly, integrating an ample quantity of external knowledge while judiciously assess-ing the value disparities among diverse knowledge sources, e.g., allocating token quotas proportionately to their signifi-cance within the confines of token limitations.\nIt is widely acknowledged that the pinnacle achievements of human intelligence lie in our capacity to autonomously wield and innovate tools, distill valuable lessons from both personal and collective past experiences, and promptly harness these insights to refine subsequent actions. These defining traits serve as indispensable benchmarks for empowering AI to execute software tasks and devise tools. Consequently, this paper, through a comprehensive juxtaposition of extensive AI models against human intelligence, presents a key observation alongside three profound insights.\nObservation: LLMs, owing to their substantial training expenses, become static post-training, primarily serving as inference engines. The diminishing of their memory capabili-ties emerges as a main constraint, hindering the expansion of system functionalities and overall performance [28] [29].\nInsight 1: The evolution of human intelligence is un-derpinned by relentless learning and practice, fostering a virtuous cycle that intertwines memory, reasoning, and action. This continuous cycle fosters the gradual enhancement and adaptation of our capabilities in real-world contexts.\nInsight 2: In the realm of reasoning, humans rely heavily on the relevance of their stored memories to the problem at hand. This relevance serves as a main basis, guiding us to evaluate and prioritize our memories, often necessitating strategic precision trade-offs.\nInsight 3: As humans embark on tasks, we dynamically adjust our plans in response to the consequences of our actions. This adaptability not only shapes our future actions but also influences the application of past memories, subtly shifting the weight of each experience in shaping our decision-making and planning processes.\nDrawing from the Observation, integrating memory compo-nents with LLMs is imperative for bolstering the functionality and broadening the capabilities of intelligent systems. This synergy echoes the principles of MemNN models, necessi-tating the integration of memory input, updating, output, and response functionalities. Guided by Insight 1, by linking the response feedback within the MemNN back to its input, we can establish a cyclic memory network, enabling subsequent memory outputs to dynamically harness and refine previous experiences, thereby progressively elevating the quality of outputs. Incorporating Insight 2, we introduce a relevance assessment at the memory output stage, strategically aligning the relevance and quality of memory output and response, ensuring that more pertinent and high-quality information contributes to reasoning. Building upon Insight 3, we devise a mechanism where memory responses retroactively influence memory outputs, fostering an iterative cycle of improvement in task reasoning and execution. This approach ensures that as tasks unfold, the system continually refines its memory usage and response strategies."}, {"title": "B. MaxMind Model", "content": "The design of the MaxMind model is meticulously crafted to explore the full potential of LLMs, aiming to elevate the performance of MLN structure to new heights. Therefore, each component of model has undergone a unique and thoughtful redesign, as elegantly portrayed in Figure 3. This approach en-sures that every facet of the model synergistically contributes to its overall effectiveness, capitalizing on the strengths of LLMs and driving innovative advancements in MLN.\nMaxMind's storage component can store large amount of memories, and we term it the \"Memory Repository (MR)\u201d. MR supports diverse implementation methodologies such as databases or neural networks, with the crucial requirement being its ability to perform relevance search queries. These queries extract the contextual meaning associated with data information, identifying similarities and returning stored infor-mation alongside its relevance value, tailored to the specific relevance requirements.\nMaxMind innovates further by expanding the request seg-ment of the MLN into a dedicated functional unit. This unit masterfully transforms user requests into query targets for the MR, leveraging the model's capabilities to conduct multidi-mensional analysis and extraction. This process scrutinizes the request across various focus areas, including its semantics, spatiotemporal information dimensions, potential knowledge, operations, and files, among others, ensuring a comprehensive understanding of the user's intent.\nMaxMind's memory output unit aims to forge a LLM context that is both informative and responsive, that is achieved this by embracing the RAG paradigm, where LLM context is generated through multifaceted queries to the MR. As the RAG context is inherently tied to the MR, past experiences become a valuable resource for MaxMind, enabling it to deliver efficient and relevant services in response to user requests. MaxMind addresses the challenge of generating sufficient and coherent memory output within LLM's token size constraint. It achieves this through a precision adjustment mechanism that dynami-cally scales the precision of relevant memory items, taking into account both their relevance and additional information garnered from the query. This intelligent approach ensures that MaxMind adheres to token size requirements while preserving the relative relationships within the generated context, enabling the system to adaptively fulfill its objectives.\nThe response unit of MaxMind orchestrates a process where a LLM is invoked to take the former context as the base to perform its reasoning, encompassing the user's request. the LLM's output encompasses knowledge, task planning, and tools necessary for task completion, along with a quantitative assessment of each memory item's contribution to the overall response. The response unit then meticulously establishes ex-ecution and validation frameworks, strategizing and executing actions based on its output, invoking tools as needed, and rigorously evaluating subsequent outcomes to ascertain their alignment with user expectations. It further disseminates the results of these executions or tool invocations, empowering users to engage in inspection and adjudication whenever nec-essary. In scenarios where the output falls short, the response unit send back the contribution data from the LLM's output to the output unit to inversely adjust the precision of utilized memory items, iteratively engaging the LLM and execution evaluation mechanisms to autonomously enhance the task's effectiveness in response to user requests.\nAt the forefront of MaxMind lies its distinctive Memory Input Unit. This design transforms task experiences into lasting memories. It expertly compiles the execution intricacies of a task into a coherent context, which is then presented to a Al module (may be a LLM) for a definitive judgment on task completion. Irrespective of the outcome-be it success or failure a concise summary is crafted, serving as the foundation for memory enhancement.\nThe update unit of MaxMind assumes a important role in refining and preserving memories. It meticulously converts the received memory summaries into structured memory items, adhering to the stringent format requirements of the MR, and securely archives them. Furthermore, this unit dynamically manages the MR, orchestrating the organization, compression, or even the deliberate forgetting of memory items as dictated by the system's needs.\nIn MaxMind based systems, the typical operation process generally begins with a task request. This request is then converted into a multi-dimensional query object in the Request Encoding unit; the query object reaches the Memory Output unit, which uses a relevance and accuracy correlation search to retrieve memories from the MR and combines them with the user's request to form the context for the main reasoning LLM, and the process moves to the Response Unit; the Response Unit first calls the LLM to generate a solution or tool code, and attempts to execute and provide feedback, improving the inference of the task by returning to the Memory Output unit and modifying the context; after the task is successful or fails, the process enters the Input unit to summarize the task process and generate memories; the process enters the Update Unit to store the new memories into the MR."}, {"title": "IV. DESIGN OF A MAXMIND INSTANCE", "content": "We hope to evaluate the impact of integrating MaxMind into the SOTG system, necessitating a robust test suite for complex tasks, which is currently rare and labor-intensive to create. We appreciate the SheetCopilot [4] team's contributions, including an extensive test suite of 221 tasks and open-source code for direct use. We have developed MaxMind4Sheet system, based on the SheetCopilot framework, features an enhanced state machine and MLN capability for rapid validation of MaxMind's key mechanisms.\nSheetCopilot abstracts spreadsheet functionalities into vir-tual APIs, enabling LLM-software interactions. It uses an external knowledge base and real-time feedback to refine LLM-generated solutions. However, SheetCopilot's feedback mechanism is simple and task-independent, maintaining sta-bility but lacking task-based learning. In contrast, our Max-Mind4Sheet enhances this with task-specific memory recycling for improved performance and adaptability.\nMaxMind4Sheet utilizes PostgreSQL equipped with the PGvector plugin as its MR, enabling accurate memory storage and vector-based memory querying. It establishes multiple keys to capture various attributes of memories, including ID, Task Memories (memories), and Knowledge (knowledge) etc. In MaxMind4Sheet, task memories are defined as descriptive narratives of sheet task processes, encompassing requested content and system-generated solutions. Three precision lev-els-\"original\", \"concise\", and \"brief\" are defined for task memories and knowledge. \"Original\" includes initial task requests, context interactions, and raw knowledge text. \"Con-cise\u201d focuses on the final task actions and key knowledge points, optimizing token usage. \"Brief\" retains only the prob-lem and operation name, simplifying memory representation. For vector embedding of information or memories, MaxMind4Sheet adopts the nomic-embed-text encoder Meanwhile, for searching through information or memories, it employs the Hierarchical Navigable Small World (HNSW) method for high-dimensional data search, ensuring efficient and accurate retrieval.\nUnlike SheetCopilot, MaxMind4Sheet employs our distinct RAG mechanism to generate a long context for the LLM. It decomposes the request text into two dimensions: Task Memories and Knowledge, facilitated by a lightweight llama3-8B model. The workflow of the three mechanisms-querying, filtering, and long-context generation for memory output is as follows: Firstly, based on the decomposed request across these two dimensions, MaxMind4Sheet retrieves the six most relevant memories from its MR: up to three Task Memories and up to three Knowledge items (queries). These are then sorted by relevance, with the corresponding precision levels se-lected for each. This step involves precision-based information extraction (filtering) and prepends labels like \"Key Reference\" and \"General Reference\" to the respective memory items. Together with SheetCopilot's original prompt and feedback information, these form the long context.\nMaxMind4Sheet uses llama3.1-70B as its main LLM with a 128K tokens limit. When the context exceeds the threshold, the memory output unit calls \"Memory Extraction\" modules to downgrade adjustable precision parts and recompose the context until the size fits within the limit.\nFurthermore, MaxMind4Sheet revamps the response feed-back mechanism to directly evaluate the execution results, as opposed to SheetCopilot's approach of merely testing EXEC@1 and conducting offline PASS@1 assessments. The evaluated results are then fed back to the Proposal Stage. MaxMind4Sheet's execution state machine has been refined to follow the process outlined in Fig 4."}, {"title": "V. EXPERIMENTS", "content": "We employ a test set comprising 221 spreadsheet tasks from SheetCopilot and utilize state-of-the-art Large Language Mod-els (LLMs) to simultaneously evaluate both SheetCopilot and MaxMind4Sheet. The objectives of this evaluation encompass: verifying the reasoning performance of various LLMs when provided with a memory reference; and validating the effec-tiveness of memory cycling, precise memory extraction, and memory feedback mechanisms within MaxMind by comparing its outcomes with those of SheetCopilot on the same dataset. For the sake of facilitating comparison, we adopt the same evaluation metrics as SheetCopilot [4], which include: (1) Exec@1, the proportion of solutions provided by the system that can be successfully executed; (2) Pass@1, the proportion of generated spreadsheet states that fully satisfy the task requirements, based on the system's proposed solutions; (3) A50 and A90, computed as the 50th and 90th percentiles, respectively, of the ratios of atomic operations in the system-generated solutions to the number of operations in the ground truth, across all tasks. Due to the high expense of GPT-series (the primary LLM used in SheetCopilot), we adopted the open-source and free llama3.1-70B as our primary LLM. Additionally, for some comparative tests, we also utilized llama3-70B, llama3-405B, and GPT40, respectively.\nHardware&Software: Intel(R) Xeon(R) Platinum 8358 CPU and 8 NVIDIA A800-SXM4-80GB GPUs, Ubuntu 18.04 server and Windows 10 Desktop, Office 2016."}, {"title": "B. Recalibration of Reference Frame", "content": "The experimental compares the performance of our system with SheetCopilot using 221 test tasks from the SheetCopilot project. We first calibrate the performance of SheetCopilot (using its GitHub source code) because differences in our LLM and testing environment might affect results. Table I shows calibrated SheetCopilot performance on four recent LLMs, revealing notable differences from the original paper (average Pass@1 was about 40%). To ensure these discrepancies aren't due to inappropriate configuration, we tested the core perfor-mance mechanisms mentioned in the original paper, as seen in Table II, confirming all factors contribute positively. Thus, differing test environments significantly impact results, so we use Table I as a baseline for MaxMind4Sheet performance tests."}, {"title": "C. Performance Evaluation", "content": "It must be acknowledged that the amount, type, and quality of memories stored in the MR of the system will all have an impact on the accuracy of subsequent problem-solving. Therefore, an absolute conclusion on the impact of memory on performance cannot be given. This paper adopts an alternative approach to demonstrate the impact of MaxMind4Sheet's memory loop on subsequent tasks. Specifically, we first allow the system to run through a complete test set and save the successfully completed tasks as memory items. Then, we rerun the test set with the system equipped with these memories to observe the completion of tasks in this round of testing.\nAs shown in Table III, using Llama3.1-70B as the primary inference model, the first row of data represents the baseline achieved by SheetCopilot, the second row displays the perfor-mance test results of MaxMind4Sheet running 221 tasks in the first round, and the third row presents the test results of the second round after retaining the memory from the first round. It can be observed that MaxMind4Sheet's Pass@1 gradually improves significantly over the two rounds of testing (3.3% increase in round 1, 6.1% increase in round 2), indicating that the memory acquired from preceding tasks plays a role in the correct execution of subsequent tasks, which aligns with the expectation that success rates incrementally increase with the accumulation of experience. When focusing on the Exec@1 metric, we find that SheetCopilot achieves the best perfor-mance in this aspect, while MaxMind4Sheet's performance decreases as the number of rounds and memory increase. We believe this is largely related to the current design of the memory mechanism. The current version of MaxMind4Sheet only retains the memory of tasks executed correctly and lacks a reasonable design for the memory mechanism of tasks that can be executed but with incorrect results. This leads LLM to prioritize the correctness of task results during inference, partially neglecting executability, which results in a decrease in executability rates. We also pay attention to the A50 and A90 test metrics. Here, it can be observed that with the incorporation of successful task memories, MaxMind4Sheet's metrics in the first round of testing initially increase relative to the baseline but begin to decrease in the second round. This can be explained by the fact that when there is not much memory initially, the system can only retrieve memories with relatively low relevance, which introduces relatively more interference to inference. However, as memory increases, the relevance of the memories referenced by each task continues to improve, reducing interference, and thus, A50 and A90 start to decrease. Although this test demonstrates that the introduction of memory loops can effectively improve the task accuracy of the SOTG system, there are still many complex issues to be studied and resolved in designing the mechanisms for retaining and retrieving task memories."}, {"title": "D. Memory Transfer Capability and System Efficiency", "content": "We hope to verify that the MaxMind4Sheet system support the transfer and sharing of memories and experiences. The corresponding experimental design is as follows: Firstly, we use GPT40 and Llama3.1-405B as the main LLM respectively, allowing MaxMind4Sheet to undergo two rounds of testing on a task set consisting of 221 tasks, accumulating MR. Then, we transfer the MR to MaxMind4Sheet with the main LLM switched to Llama3.1-70B, and observe the performance in the following testing. At the same time, we also hope to verify the contribution of examples and introduced memories to task accuracy. The experimental results are shown in Table IV. The first row of data is the test result without relying on any memories but with examples added (which is actually equivalent to SheetCopilot); the second row of data is the test result relying on memories but without adding examples; the third row is the test result integrating memories and examples support. Firstly, it can be seen that the transferred memories have significantly improved the correctness of task completion based on Llama3.1-70B. This is because different LLMs have significant differences in tasks that produce correct results during reasoning. Therefore, transferring a combination of multiple successful memories can significantly improve the correctness of system task execution. The results also show that replacing examples with memory improves execution efficiency by 25% compared to SheetCopilot, while not sig-nificantly affecting the pass rate of task solving."}, {"title": "E. Evaluation of Memory Precision on Task Performance", "content": "To evaluate the influence of memory precision on per-formance and to bolster our information value-based RAG approach, we tested the impact of different memory precisions on MaxMind4Sheet's Exec@1 and Pass@1 metrics, building upon the previous test methodologies for memory transfer capability. We separately employed three precisions (without selecting memories based on relevance as done in this test) to realize RAG, supporting the reasoning of Llama3.1-70B. As shown in Table V, it is evident that both Exec@1 and Pass@1 results are significantly affected, with concise pre-cision yielding the most favorable pass rate, original preci-sion coming second, and brief precision proving to be the least effective. These findings provide preliminary validation and insights, suggesting that higher-precision memories offer more assistance compared to lower-precision ones; however, exceedingly high precision might introduce noise during LLM inference, potentially causing interference."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "An SOTG system built on LLMs requires learning from experience to handle complex problems. We introduce the Memory-Loop Network concept, inspired by human intelli-gence, allowing continuous accumulation and referencing of system experiences. Our LLM-based MLN model, MaxMind, incorporates a memory loop for interaction with LLMs and an adaptive RAG method that optimizes experience selection based on knowledge value, enhancing task handling. We vali-date this model using the SheetCopilot system, creating Max-Mind4Sheet and testing it on 221 Excel tasks. Experiments show a 3%-6% performance boost per round and an up to 25% increase in efficiency with memory recycling. MaxMind's task memories are transferable, addressing retraining challenges for specialized tasks, thus enhancing LLM capabilities in software automation and tool generation.\nMaxMind4Sheet is only a simple verification of the Max-Mind model concept, rather than a full implementation. There are still many knowledge gaps in how to summarize and generate memory, as well as its reasonable and efficient use, which require further research and extensive experimentation. Additionally, there is a severe lack of datasets and scenarios in SOTG, which we will gradually explore in our future work."}]}