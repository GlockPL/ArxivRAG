{"title": "Interpreting Inflammation Prediction Model via Tag-based Cohort Explanation", "authors": ["Fanyu Meng", "Jules Larke", "Xin Liu", "Zhaodan Kong", "Xin Chen", "Danielle Lemay", "Ilias Tagkopoulos"], "abstract": "Machine learning is revolutionizing nutrition science by enabling systems to learn from data and make intelligent\ndecisions. However, the complexity of these models often leads to challenges in understanding their decision-making\nprocesses, necessitating the development of explainability techniques to foster trust and increase model transparency.\nAn under-explored type of explanation is cohort explanation, which provides explanations to groups of instances with\nsimilar characteristics. Unlike traditional methods that focus on individual explanations or global model behavior,\ncohort explainability bridges the gap by providing unique insights at an intermediate granularity. We propose a\nnovel framework for identifying cohorts within a dataset based on local feature importance scores, aiming to generate\nconcise descriptions of the clusters via tags. We evaluate our framework on a food-based inflammation prediction\nmodel and demonstrated that the framework can generate reliable explanations that match domain knowledge.", "sections": [{"title": "1. Introduction", "content": "Machine learning (ML) has become an integral part of modern data analysis, offering powerful tools for uncov-\nering patterns and making predictions across various domains. One significant application is in nutrition science,\nwhere ML models can provide dietary recommendations, detect food quality and safety issues during production,\nand surveil public health and epidemiology. However, the complex and often opaque nature of these models presents\nchallenges in understanding and trusting their predictions. To address these issues, explainability techniques have\ngarnered considerable interest, aiming to make ML models more interpretable and transparent.\nExplainability can be approached from different perspectives, including local explanations that focus on individual\npredictions and global explanations that provide insights into the overall behavior of the model. However, there is\na growing need for intermediate-level explanations that balance these two extremes, offering contextually relevant\ninsights that are both comprehensive and specific (Sokol and Flach, 2020; Arrieta et al., 2020; Adadi and Berrada,\n2018). Cohort explainability, also referred to as subgroup explainability, explains model predictions by analyzing\ngroups of instances with shared characteristics and emerges as a promising solution to this challenge."}, {"title": "1.1. Motivating Cohort Explanation", "content": "To further motivate the use of cohort explanations, consider a hypothetical medical disorder that predominantly\naffects low-BMI patients who consume too little potassium, and high-BMI patients who consume too much potassium.\nThe distribution of positive and negative samples is illustrated in Fig. 1. We trained a binary classification model\nbased on two features: BMI and daily potassium consumption. Suppose the model's behavior can be described as the\ncombination of two regional linear models: for patients with low BMI, it assigns a positive weight to potassium and a\nnegative weight to BMI; for patients with high BMI, it assigns a negative weight to potassium and a positive weight to\nBMI. The model calculates the dot product between the weights and the features and predicts positive if the product\nis positive.\nIf we probe this model using existing local or global explanation methods, the most likely results are shown in\nFig. 2a and 2b. In these figures, the feature importance denotes how the machine learning model considers the contri-\nbution of each feature to the final prediction, either for an individual sample or averaged over the dataset, respectively.\nGlobal importance aims to identify the most important feature to the model. However, since the model treats the"}, {"title": "1.2. How to Describe Cohort Constitution Concisely?", "content": "Cohort explanation can be seen as a middle ground between local explanations, which are specific to individual\nsamples, and global explanations, which provide concise descriptions. Therefore, evaluating cohort explanation meth-\nods should take into account both specificity and conciseness. Most existing work on cohort explainability emphasizes\nspecificity, usually measured by the disparity of local explanations within each cohort. Conciseness, on the other hand,\nhas received less attention and is typically treated as a hyperparameter k representing the desired granularity. Since\none of the core motivations of cohort explainability is the balance between these two directions, careful consideration\nof conciseness is needed. Usually, cohort explanation algorithms describe the cohorts via centroids-a representa-\ntive sample in the cohort. However, if the feature dimensionality is high, it becomes difficult to comprehend which\nsamples belong to the cohorts. In this work, we propose an alternative: using tags to concisely describe the cohorts."}, {"title": "1.3. Food-based Inflammation Prediction", "content": "Inflammation associated with poor nutritional status is a strong mediator of many chronic diseases. Current\nmethodologies for assessing diet-induced inflammation in observational studies tend to focus on nutrients and food\ngroups as predictors, which often have limited signals and either capture specific known relationships in the case of\nnutrients or exhibit broad associations with weaker recommendations. In this work, we use dietary records disaggre-\ngated into ingredients and arranged hierarchically to leverage the inherent relationships among foods.\nHowever, even with engineered food intake signals, determining how diet affects inflammation is challenging due\nto individual-specific characteristics such as age, sex, and body mass, as well as inter-individual variations in genetics,\nphysiology, the host microbiome, and other non-dietary exposures. Cohort explanation directly tackles this problem\nby providing explanations regarding food effects on automatically identified suitable demographics. Our proposed\nexplainer identifies demographics with similar feature importance. By investigating the composition of the groups\nand the food effects on each group via feature importance, the explanations could assist nutrition science researchers\nin further understanding the relationship between food consumption and the causes of inflammation."}, {"title": "2. Related Work", "content": "Cohort Explanation. Cohort explainability aims to strike a balance between local and global methods, which provide\nexplanations based on groups of instances. Similar to other explanation types, cohort explanation can be classified\ninto inherent and post-hoc methods. Inherent cohort explanation, sometimes referred to as a local surrogate model,\ninvolves dividing the feature space and developing a local, interpretable model for each subspace. Examples of\nthis approach include Hu et al. (2020), which constructs local-surrogate linear or generalized additive models by\nrecursively partitioning the feature space.\nIn comparison, post-hoc cohort explanation methods partition the feature space and provide explanations for each\ncluster. Generally, such explainers run local explanation methods on a dataset and then attempt to cluster the samples\nbased on the similarity between the local explanations. Examples of such methods include Cavus et al. (2023), which\naverages local SHAP values within predefined groups. Other methods employ techniques to automatically identify\ncohorts, such as VINE, which clusters individual conditional expectation (ICE) via unsupervised clustering methods\n(Britton, 2019). Similarly, REPID (Herbinger et al., 2022) and Molnar et al. (2023) use tree-based partitioning to\ngenerate cohorts based on either ICE or conditional permutation feature importance. Additionally, GADGET extends\nprevious methods by incorporating functional decomposition during partitioning, allowing for the consideration of\nmore features (Herbinger et al., 2023).\nThe framework proposed in this paper lies within the domain of post-hoc cohort explanation. Compared to existing\nworks, our distinctions include:\n\u2022 We formalize the concept of conciseness in cohort description, which aims to efficiently describe the samples\nin each cohort.\n\u2022 We introduce descriptive tags to cohort explanation to enhance the conciseness of cohort descriptions.\n\u2022 We utilize dual-view optimization to create cohorts that are both specific and concise."}, {"title": "Subgroup Discovery", "content": "Subgroup discovery is a prominent area in data mining and machine learning that focuses on\nidentifying interesting and interpretable subgroups within a dataset, which is valuable for uncovering hidden structures\nand actionable insights in various domains (Helal, 2016). Such works include Sutton et al. (2020), which employs\nrule-based partitioning based on the loss of surrogate models to identify interesting regions. Similarly, Hedderich et al.\n(2022) also uses a rule-based algorithm on natural language models to recognize regions with significant responses to\ncertain variables. These methods can be thought of as a type of inherent cohort explanation, although the identified\nclusters of interest may not cover the entire feature space."}, {"title": "Explainable Clustering", "content": "Explainable clustering is an emerging area in machine learning that aims to enhance the\ninterpretability of clustering results by providing understandable explanations for the derived clusters. For example,\nMoshkovitz et al. (2020) modifies the results from traditional k-means into decision trees to improve the interpretabil-\nity of the clusters. L'Yi et al. (2015) creates a visualization tool to compare and analyze clustering results. More\nrecently, Guilbert et al. (2024) formulates a constraint optimization program for clustering while allowing the injec-\ntion of domain knowledge via constraints and descriptive patterns, which is similar to our formulation. Although the\nconcept of explainable clustering shares similarities with cohort explanation, as both domains utilize explainability\nand clustering, their goals differ. Cohort explanation aims to apply clustering to simplify the results of local explana-\ntions; conversely, explainable clustering seeks to introduce explainability methods to existing clustering algorithms."}, {"title": "3. Approach", "content": "To address the challenges of specificity and description conciseness simultaneously, we propose a tag-based cohort\nexplanation framework. The algorithm follows these steps:\n1. Preprocess the features and create a dictionary of tags;\n2. Compute the local feature importance for each sample;\n3. Construct and solve a descriptive clustering optimization problem."}, {"title": "3.1. Preprocessing", "content": "First, we construct a dictionary of tags that will be used to describe the cohorts. The motivation behind introducing\ntags is to automate the process of selecting a concise subset of features as descriptions. We first select a subset of\n\"descriptor\" features based on the application. We then apply one-hot encoding to categorical descriptor features and\ndiscretize continuous features into binary tags. For example, in the motivating example in Fig. 1, we will use both\nfeatures as descriptors, and the tag dictionary would be {BMI<17.5, 17.5\u2264BMI<34, 34\u2264BMI<51.5, 51.5\u2264BMI,\nage<30, 30\u2264age<50, 50\u2264age<70, 70\u2264age} if we use four quantiles. Each sample will then obtain a binary\nvalue for each tag, indicating whether it conforms to the tag. The output of the framework will utilize a subset of these\ntags to describe each cohort."}, {"title": "3.2. Compute Feature Importance", "content": "Next, for the features we wish to investigate, we run a local explainer and compute their importance across all\nsamples. The explainer can be an arbitrary local explainer, including but not limited to model-agnostic methods such\nas SHAP (Lundberg and Lee, 2017) or LIME (Ribeiro et al., 2016), or gradient methods for neural network-based\nmodels such as saliency maps (Simonyan et al., 2013), vanilla gradients, SmoothGrad (Smilkov et al., 2017), or\nGrad Input (Baehrens et al., 2010)."}, {"title": "3.3. Descriptive Clustering", "content": "With the binary tags and the local feature importance, we construct an optimization problem. In the following\nsection, we will describe the motivation for the process, while a detailed description of the algorithm can be found in\nAppendix A. The goal is to find a partition such that (1) all samples within a cohort conform to a maximum number\nof common tags; (2) all samples within a cohort have similar local feature importances.\nTo solve this problem, we first optimize for maximum descriptiveness. Descriptiveness is defined as the minimum\nnumber of tags used across the cohorts. This step ensures that each cohort uses as many tags as possible. Note that\nalthough we optimize for descriptiveness to enhance the quality of the explanation, the final set of tags will still be\nmuch fewer than the total number of tags available.\nThe optimization problem is subject to the following set of constraints:\n\u2022 Each cohort must have at least one sample;"}, {"title": "3.4. Choosing the Number of Cohorts", "content": "Choosing the granularity of cohort explanation should ideally be handled automatically rather than being estimated\nby the end users. However, both specificity and descriptiveness tend to increase as the number of cohorts k increases.\nIf k equals the number of samples n, then specificity error would be 0, since there would be only one sample in each\ncohort; and descriptiveness would also be maximized since each sample would utilize the maximum number of tags\nto describe it. Such results are undesirable, since they degenerate into local explanation and is too complicate for a\nhuman to interpret. We prefer a small k with a decently low specificity error.\nTo automate the process of choosing k, we propose a metric importance prediction error to evaluate the quality of\nk. The value of k that yields the lowest error is automatically returned. Details of the metric can be found in Appendix\nA."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Dataset and Explainee Model", "content": "Study Population. We train an inflammation prediction model based on the National Health and Nutrition Examina-\ntion Survey (NHANES) 2001-2010 and 2015-2018 cycles, which were selected due to the availability of systemic\ninflammation (CRP) measurements.\nWe apply the following exclusion criteria: participants with diet recalls not passing quality control, caloric intake\nranges of < 500 or > 4500 kcal/day, or those who were below the 5th percentile or above the 95th percentile for\nthat calorie range, individuals under 18 years of age, those with active infections of hepatitis or HIV, diagnoses of\ncardiovascular disease or cancer, signs of acute inflammation, or a CRP > 10 mg/dL, or missing data for covari-\nates. Participants with missing data for BMI were imputed from waist circumference (individually for females and\nmales) using linear models for 197 participants. The final sample size included 19,460 participants after applying the\nexclusion criteria.\nFeature Selection and Prediction Target. The feature set consists of two components: covariates and diet recalls.\nCovariates of interest for this study include age, BMI, sex, family poverty income ratio (PIR), education, ethnicity,\nsmoking status, and diagnoses of diabetes and hypertension. Diagnostic criteria to determine diabetic status included\nany of the following: \u2265 126 mg/dL fasting blood glucose, \u2265 6.5% glycated blood hemoglobin, or reported use of\ninsulin or other medications. Hypertension was defined as either \u2265 140 systolic or \u2265 90 diastolic blood pressure or\nuse of medication for hypertension. Besides the continuous variables age, BMI, and PIR, other discrete covariates\nwere one-hot encoded, forming a total of 20 covariates.\nFoods reported were \"ingredientized\" to disaggregate mixed meal intake using an automated method. Briefly, the\npipeline includes calculating the correct proportion of ingredient weights for items, using text similarity matching"}, {"title": "4.2. Shapley Additive Explanations (SHAP)", "content": "To increase transparency and interpretability of the model, we first investigate the global and local feature im-\nportance using SHAP (Lundberg and Lee, 2017). SHAP is a game-theory based method for explaining the output\nof arbitrary machine learning models. Given a particular sample and its associated prediction, SHAP assigns a value\nrepresenting the impact on the model output. It calculates this value by comparing the model prediction with and\nwithout the feature being present. The SHAP value can be either positive or negative; in our case, a higher positive\nSHAP value indicates that the feature is positively associated with higher inflammation. Therefore, for the food re-\ncalls, we expect that a high consumption of pro-inflammatory foods would result in a positive SHAP value, while a\nhigh consumption of anti-inflammatory foods would yield negative importance.\nWe evaluate the SHAP feature importance on the test dataset, which consists of 2,582 samples. Global importance\nis then computed using the mean absolute value of all local importances. Figure 3 shows the results of applying SHAP\nto the model. In our binary classification task, a higher local feature importance indicates that the feature increases\nthe likelihood of high-level inflammation for that specific individual. We analyze the results as follows:"}, {"title": "4.3. Cohort Explanation", "content": "Cohort Explanation Pipeline. The goal of utilizing cohort explanation is to investigate the varying levels of effect\nof food intakes on different groups of individuals. Therefore, we focus solely on the importance of food recalls as\nthe importance matrix W. This ensures that the optimization algorithm ignores the importance of the covariates,\npreventing the results from being dominated by BMI, which has a significantly larger importance score. The results of\nusing the importance of all features can be found in the appendix. Regarding the tags Z, we transform the covariates\ninto tags. Discrete features such as gender and ethnicity are retained as-is, while we discretize continuous features like\nage, BMI, and PIR into four quantiles. The optimization problem is solved using MiniZinc (Nethercote et al., 2007).\nWe evaluate the importance prediction error in Equation (A.8) across different values of k using 5-fold validation."}, {"title": "4.4. Comparison with REPID", "content": "To compare our framework with existing literature on cohort explanation, we selected REPID (Herbinger et al.,\n2022), which applies a decision tree to the local feature importance. Since REPID utilizes the inherently interpretable\ndecision tree for clustering, its descriptions of the cohorts are concise compared to centroid-based methods such as\nVINE (Britton, 2019) and are on par with our framework.\nFigure 8 shows the evaluation based on importance prediction error between the proposed TagHort and REPID\nacross different values of k, while Figure 9 presents the explanation generated by REPID at k = 4. REPID consistently"}, {"title": "5. Conclusion", "content": "In this paper, we introduced TagHort, a novel framework for tag-based cohort explanation with concise descrip-\ntions using tags. Our approach bridges the gap between local and global explainability by providing intermediate-\nlevel insights that are both specific and comprehensive. We demonstrated the effectiveness of our framework on a\nfood-based inflammation prediction model, showing that it can generate reliable explanations consistent with domain\nknowledge. Through our experiments, we illustrated the importance of cohort explanation in identifying structured\ngroups within datasets and understanding the varying effects of features across different populations. Our method\nprovides a valuable tool for enhancing model transparency and trust, with significant implications for healthcare and\nnutrition science. Future work could explore the integration of additional types of tags and further validation across\ndifferent domains and models."}, {"title": "Appendix A. Details on the Tag-based Cohort Explanation Framework", "content": ""}, {"title": "Appendix A.1. Problem Definition", "content": "Consider a fixed, black-box model $M : \\mathbb{R}^m \\rightarrow \\mathbb{R}^q$, and a local feature importance explainer used to explain the\nmodel $\\omega_M: \\mathbb{R}^m \\rightarrow \\mathbb{R}^m$. We are given samples $X = \\{x_1,..., x_n\\}$, $x_i \\in \\mathbb{R}^m$ and can thus evaluate the local importance\nscore of each of the n samples via $\\omega_M$ to obtain $W = \\{w_1,..., w_n\\}$, $w_i \\in \\mathbb{R}^m$. Additionally, for each sample, we have\n$p$ auxiliary binary tags that describe the sample $D \\in \\{0,1\\}^{n\\times p}$. In our application, these tags are obtained from the\nfeatures via preprocessing."}, {"title": "Definition 1 (Tag-based cohort explanation).", "content": "Given the desired number of cohorts $k$, the goal of cohort explanation\nwith concise description is to find a partition $C = \\{C_1, . . ., C_k\\}$ such that the local importance scores are compact, i.e.,"}, {"title": "compactness(C) =", "content": "$\\frac{\\sum_{t=1}^{k}\\sum_{x_i,x_j\\in C_t, i \\neq j}||w_i - w_j||}{\\frac{1}{2}n_C(n_C-1)}$                                                                              (A.1)\nis minimized.\nAdditionally, each cohort $C_t$ should provide a list of tags $S_t$ that describe the cohort, i.e.,"}, {"title": "x\u2081 \u2208 C\u2081 Dis =", "content": "$\\forall s \\in S_t$                                                                                                                  (A.2)\nIdeally, we would like the result to utilize as many tags as possible, and thus the quality of $C$ is also evaluated by"}, {"title": "descriptiveness(C) =", "content": "$\\text{min } \\mid S_t\\mid$\nwhere $\\mid S_t\\mid$ represents the number of tags used to describe cohort $t$"}, {"title": "Appendix A.2. Formulation", "content": "To solve the problem, we simplify the formulation proposed by Dao et al. (2018) and apply it to the local impor-\ntance scores $W$. We model the problem as a constraint problem (CP)."}, {"title": "Appendix A.2.1. Variables", "content": "We introduce two variables:\n1. Cohort assignment $G \\in \\{1, ..., k\\}^n$, where $G_i$ represents the index of the cohort to which $x_i$ belongs.\n2. Description matrix $S \\in \\{0, 1\\}^{k\\times p}$, where $S_{tp} = 1$ indicates that the $p$-th tag is used to describe cohort $t$."}, {"title": "Appendix A.2.2. Constraints", "content": "The constraints for the CP are as follows:\n\u2022 Each cohort should be non-empty:"}, {"title": "count(G, t) >=", "content": "$1, \\forall t = 1,...k$                                                                                      (A.4)\nwhere $\\text{count}(G, t)$ denotes the number of occurrences of the value $t$ in $G$;\n\u2022 A sample can be placed into a cohort if it satisfies all of its tags:"}, {"title": "\\sum_{p=1}^p S_{tp}(1 - D_{ip}) =", "content": "$0, \\forall i = 1,..., n$                                                                                     (A.5)"}, {"title": "\u2022 A tag should be used to describe a cohort if all samples in it satisfy the tag:", "content": "$\\sum_{i=1,G_i=t}^n (1 - D_{ip}) = 0$                                                                               (A.6)\n\u2022 Tie-breaking:\n$\\text{seq\\_precede\\_chain}(G)$                                                                                             (A.7)\nwhich is a global constraint that enforces the value $t$ to precede all values of $t + 1$ in $G$. This allows earlier\nsamples to be put in earlier groups, and thus prevent ties."}, {"title": "Appendix A.3. Tag-based Cohort Explanation Framework", "content": ""}, {"title": "Algorithm 1: Tag-based Cohort Explanation", "content": "Input: Local importance W, tags D, number of cohorts k\nOutput: Cohort assignment G, description matrix S\n1 C \u2190 constraints defined in Eq. A.4-A.7;\n2 q \u2190 max descriptiveness(C) subject to C;\n3 G, S \u2190 arg min compactness(C) subject to C \u222a {descriptiveness(C) \u2265 q} ;\n4 for t \u2190 1 to k do\n5 | w\u2081 \u2190 mean({W\u2081|G\u2081 = k})\n6 end\n7 return G, S, {w}=1;"}, {"title": "Appendix A.4. Choosing the Hyperparameter k", "content": "Choosing the granularity of cohort explanation should ideally be automated rather than guessed by end users.\nHowever, the two primary objectives of tag-based explanation, descriptiveness and compactness, both improve as k\nincreases. Consequently, we propose a metric, importance prediction error, to evaluate the quality of a specific k."}, {"title": "Definition 2 (Importance prediction error).", "content": "Given cohorts C1,..., Ck and description matrix S, along with an eval-\nuation dataset X' \u2208 Rl\u00d7m and their corresponding local importance scores W' \u2208 Rl\u00d7m and tags D' \u2208 {0,1}l\u00d7r, the\nimportance prediction error is defined as"}, {"title": "LFI =", "content": "$\\frac{1}{l} \\sum_{i=1}^l ||w'_i - \\frac{1}{\\mid Z_i \\mid} \\sum_{c \\in Z_i} \\bar{w}_c ||^2$                                                                                                                  (A.8)\nwhere $Z_i$ is the set of all cohorts such that sample $x_i$ satisfies all of their descriptions:"}, {"title": "C\u2081 \u2208 Z\u00a1 \u21d4", "content": "$\\forall t (S_{tp} = 1 \\Rightarrow D'_{ip} = 1, \\forall p = 1, ...,r)$                                                               (A.9)"}]}