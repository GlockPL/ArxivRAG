{"title": "Sum-Product-Set Networks", "authors": ["Milan Pape\u017e", "Martin Rektoris", "Tom\u00e1\u0161 Pevn\u00fd", "V\u00e1clav \u0160m\u00eddl"], "abstract": "Daily internet communication relies heavily on tree-structured graphs, embodied by popular data formats such as XML and JSON. However, many recent generative (probabilistic) models utilize neural networks to learn a probability distribution over undirected cyclic graphs. This assumption of a generic graph structure brings various computational challenges, and, more importantly, the presence of non-linearities in neural networks does not permit tractable probabilistic inference. We address these problems by proposing sum-product-set networks, an extension of probabilistic circuits from unstructured tensor data to tree-structured graph data. To this end, we use random finite sets to reflect a variable number of nodes and edges in the graph and to allow for exact and efficient inference. We demonstrate that our tractable model performs comparably to various intractable models based on neural networks.", "sections": [{"title": "1 INTRODUCTION", "content": "One of the essential paradigm shifts in artificial intelligence and machine learning over the last years has been the transition from probabilistic models over fixed-size unstructured data (tensors) to probabilistic models over variable-size structured data (graphs) [Bronstein et al., 2017, Wu et al., 2021]. Tree-structured data are a specific type of generic graph-structured data which describe real or abstract objects (vertices) and their hierarchical relations (edges). These data structures appear in many scientific domains, including cheminformatics [Bianucci et al., 2000], physics [Kahn et al., 2022], and natural language processing [Ma et al., 2018]. They are used by humans in data-collecting mechanisms to organize knowledge into various machine-generated formats, such as JSON [Pezoa et al., 2016], XML [Tekli et al., 2016] and YAML [Ben-Kiki et al., 2009] to mention a few.\n\nThe development of models for tree-structured data has been thoroughly, but almost exclusively, pursued in the NLP domain [Tai et al., 2015, Zhou et al., 2016, Cheng et al., 2018, Ma et al., 2018]. However, these models rely solely on variants of neural networks (NNs) and are non-generative, lacking clear probabilistic interpretation. Despite a growing interest in designing generative models for general graph-structured data [Simonovsky and Komodakis, 2018, De Cao and Kipf, 2018, You et al., 2018, Jo et al., 2022, Luo et al., 2021], there are no generative models that take advantage of the parent-child ancestry inherent in tree-structured graphs. Therefore, directly applying these generic models to the trees would incur unnecessary computational costs. Moreover, these models assume the input features with homogeneous dimensions. More importantly, they preclude tractable probabilistic inference, necessitating approximate techniques to answer even basic queries.\n\nIn sensitive applications (e.g., healthcare, finance, and cybersecurity), there is an increasing legal concern about providing non-approximate and fast decision-making. Probabilistic circuits (PCs) [Vergari et al., 2020] are tractable probabilistic (generative) models that guarantee to answer a large family of complex probabilistic queries [Vergari et al., 2021] exactly and efficiently."}, {"title": "2 TREE-STRUCTURED DATA", "content": "A single instance of tree-structured, heterogeneous data is given by an attributed data graph, T. In contrast to a fixed-size, unstructured, random variable, $x = (x_1,...,x_d) \\in X \\subset \\mathbb{R}^d$, this graph forms a hierarchy of random-size sets.\n\nDefinition 1. (Data graph). T = (V, E, X) is an attributed, tree-structured graph, where V is a set of vertices, E is a set of edges, and X is a set of attributes (features). V := (L, H, O) splits into three subsets of data nodes: leaf nodes, L, heterogenenous nodes, H, and homogeneous nodes, O. Let ch(v) and pa(v) denote the set of child and parent nodes of \u03c5 \u2208 V, respectively. All elements of ch(v) are of an identical type if v \u2208 O, and some or all elements of ch(v) are of a different type if v \u2208 H. We assume that only the leaf nodes are attributed by $x_v \\in X \\subseteq \\mathbb{R}^{d_v}$, with possibly different space and its dimension for each v \u2208 L.\n\nDefinition 2. (Schema). Let T be an instance of tree-structured, heterogeneous data. Then, a subtree, S, which results from T by following all children of each u \u2208 H and only one child of each u \u2208 O\u2014such that it allows us to reach the deepest level of T-is referred to as the schema.\n\nDefinition 1 implies that T is defined recursively by a subtree, T := {$T_{u_1},...,T_{u_m}$}, rooted at v \u2208 V. For heterogeneous nodes, v \u2208 H, each child subtree, $T_u$, has a different schema for all u \u2208 ch(v). For homogeneous nodes, v \u2208 \u039f, each child, $T_u$, has the same schema for all u \u2208 ch(v). The leaf nodes, v \u2208 L, are terminal nodes (with no children), containing a feature vector, T := x. The cardinality of all homogeneous nodes, v \u2208 O, is random and differs for each instance of T. This also randomizes the cardinality of heterogeneous nodes, v \u2208 \u0397, if ch(v) contains at least one homogeneous node, u \u2208 O."}, {"title": "3 SUM-PRODUCT-SET NETWORKS", "content": "A sum-product-set network (SPSN) is a probability density over tree-structured, heterogeneous data, p(T). This differs from the conventional sum-product network [Poon and Domingos, 2011], which is a probability density over the unstructured data, p(x).\n\nDefinition 3. (Computational graph). G := (V,E,0) is a parameterized, directed, acyclic graph, where V is a set of vertices, E is set of edges, and 0 \u2208 \u04e8 are parameters. V := (S, P, B, L) contains four subsets computational units: sum units, S, product units, P, set units, B, and leaf units, L. The sum units and product units have multiple children; however, as detailed later, the set unit has only two children, ch(n) := {u,v}, n \u2208 \u0392. \u03b8 contains parameters of sum units, i.e., non-negative and normalized weights, ${W_{n,c}}_{c\\in ch(n)}$, $W_{n,c} \\geq 0$, $\\sum_{c\\in ch(n)} W_{n,c} = 1$, $n \\in S$, and parameters of leaf units which are specific to possibly different densities.\n\nDefinition 4. (Scope function). The mapping \u03c8 : V \u2192 $2^T$\u2014from the set of units to the power set of T\u2014outputs a subset of T for each n \u2208 V and is referred to as the scope function. If n is the root unit, then \u03c8(n) = T. If n is a sum unit, product unit, or set unit, then \u03c8(n) = $\\cup_{c\\in ch(n)} \\psi(c)$.\n\nEach unit of the computational graph (Definition 3), n \u2208 V, induces a probability density over a given node of the data graph (Definition 1), v \u2208 V. The functionality of this density, $p_n(T_v)$, depends on the type of the computational unit.\n\nThe sum unit computes the mixture density, $p_n(T_v) = \\sum_{c\\in ch(n)} W_{n,c}p_c(T_v)$, n \u2208 S, v \u2208 {L, H}, where $w_{n,c}$ is the weight connecting the sum unit with a child unit. The product unit computes the factored density, $p_n(T_v) = \\prod_{c\\in ch(n)} p_c(\\psi(c))$, n\u2208 P, v \u2208 {L, H}. It introduces conditional independence among the scopes of its children, \u03c8(c), establishing unweighted connections between this unit and its child units. The leaf unit computes a user-defined probability density, $p_n(x_v)$, n \u2208 L, v \u2208 L. It is defined over a subset $x_v$ of $T_v := x$ given by the scope, \u03c8(n), which can be univariate or multivariate [Peharz et al., 2015].\n\nThe set unit computes a probability density of a finite random set\n\n$p_n(T_v) = p(m)U^m m! p(T_{u_1},..., T_{u_m}),$\t\t\t\t\t(1)\n\n\u03b7 \u2208 \u0392, \u03c5 \u2208 \u039f, where p(m) is the cardinality distribution and $p(T_{u_1},..., T_{u_m})$ is the feature density (conditioned on m). These are the two children of the set unit (Definition 3), spanning computational subgraphs on their own (Figure 1). The random finite set, $T_v := {T_{u_1},...,T_{u_m} } \\in 2^{T_v}$, is a simple, finite point process [Van Lieshout, 2000, Daley et al., 2003, Nguyen, 2006, Mahler, 2007], where $2^{T_v}$ is the power set of $T_v$. $T_v$ is an unordered set of distinct features or other sets, such that not only the individual elements, $T_u \\in 2^{T_u}$, are random, but also the number of these elements, $m := |T_v| \\in \\mathbb{N}_0$, is random. The proportionality factor, $U^m m! = \\prod_{i=0}^{m-1} U/(i+1)$, comes from the symmetry of $p(T_{u_1},..., T_{u_m})$. It reflects the fact that $p(T_{u_1},..., T_{u_m})$ is permutation invariant, i.e., it gives the same value to all m! possible permutations of {$T_{u_1},..., T_{u_m}$}. U is the unit of hyper-volume in $T_u$, which ensures that (1) is unit-less by canceling out the units of $p(T_{u_1}, ..., T_{u_m})$ with $U^m$.\n\nAssumption 1. (Requirements on the set unit). The requirements for a probability density of the set unit to be properly de-fined are as follows: (a) each element of $T_v := {T_{u_1}, ..., T_{u_m}}$ resides in the same space, i.e., $T_u \\in 2^{T_v}$, for all u \u2208 ch(v); (b) the elements {$T_{u_1},..., T_{u_m}$} are independent and identically distributed; and (c) the realizations {$T_{u_1},...,T_{u_m}$} are distinct.\n\nGiven that Assumption 1 is satisfied, (1) contains the product of m densities, $p(T_{u_1},..., T_{u_m}) = \\prod_i p(T_{u_i})$ over the identical scope (i.e., it does not have the disjoint scope and, therefore, it is not the product unit). Note that the feature density treats {$T_{u_i}$}$_i$ as instances, simply aggregating them by the product of distributions. It is a single density indexed by the same set of parameters for each {$T_{u_i}$}$_i$. For example, if we choose p(m) as the Poisson distribution, then (1) is the Poisson point process [Grimmett and Stirzaker, 2001].\n\nBuilding SPSNs. An SPSN is constructed from the schema of the data graph (Definition 2), i.e., an SPSN is a (generally non-isomorphic) mapping from the schema (graph) to the computational graph. The heterogeneous nodes, v \u2208 H, are"}, {"title": "4 EXPERIMENTS", "content": "We illustrate the performance and properties of the algebraically tractable SPSN models compared to various intractable, NN-based models. In this context, we would like to investigate their performance in the discriminative learning regime and their robustness to missing values.\n\nModels. To establish the baseline with the intractable models, we choose variants of recurrent NNs (RNNs) for tree-structured data. Though these models are typically used in the NLP domain (Section 1), they are, too, directly applicable to the tree-structured data in Definition 1. These tree-RNNs differ in the type of cell. We consider the simple multi-layer perceptron (MLP) cell, the gated recurrent unit (GRU) cell [Zhou et al., 2016], and the long-short term memory (LSTM) cell [Tai et al., 2015]. The key assumption of these models is that they consider each leaf data node to have the same dimension. This requirement does not hold in Definition 1. Therefore, for all these NN-based models, we add a single dense layer with the linear activation function in front of each leaf node, v \u2208 L, to make the input dimension the same. As another competitor, we use the hierarchical multiple-instance learning (HMIL) network [Pevn\u00fd and Somol, 2016], which is also specifically tailored for the tree-structured data.\n\nSettings. We convert ten publicly available datasets from the CTU relational repository [Motl and Schulte, 2015] into the JSON format [Pezoa et al., 2016]. The dictionary nodes, list nodes, and atomic nodes of the JSON format directly correspond to the heterogeneous nodes, homogeneous nodes, and leaf nodes of the tree-structured data, respectively (Definition 1, Figure 1)."}, {"title": "5 CONCLUSION", "content": "We have utilized the theory of finite random sets to synthesize a new class of deep learning models-sum-product-set networks (SPSNs)\u2014 representing a probability density over tree-structured graphs. Algebraic tractability is the essential benefit of SPSNs, yet it is redeemed by making them sparser than the algebraically intractable NNs. Notwithstanding this, SPSNs deliver a very competitive performance to the NNs in the graph classification task. We have demonstrated that the tractable and simple inference of SPSNs has also allowed us to achieve comparable results to the NNs regarding the robustness to missing values."}, {"title": "A PROBABILISTIC CIRCUITS", "content": "A probabilistic circuit (PC) is a deep learning model representing a joint probability density, p(x), over a fixed-size, unstructured, random variable, $x = (x_1,...,x_d) \\in X \\subset \\mathbb{R}^d$. The key feature of a PC is that\u2500under certain regularity assumptions-it permits exact and efficient inference scenarios. We define a PC by a parameterized computational graph, G, and a scope function, \u03c8.\n\nDefinition 5. (Computational graph). G := (V,E,0) is a parameterized, directed, acyclic graph, where V is a set of vertices, E is set of edges, and 0 \u2208 \u0472 are parameters. V := (S, P, L) contains three different subsets of computational units: sum units, S, product units, P, and leaf units, L. Let ch(n) and pa(n) denote the set of child and parent units of n \u2208 V, respectively. If pa(n) = \u00d8, then n is the root unit. If ch(n) = \u00d8, then n is a leaf unit. We consider that V contains only a single root unit, and each product unit has only a single parent. The parameters 0 := {$0_s,\u03b8_\u03b9$} are divided into (i) parameters of all sum units, $O_n = {W_{n,c}}_{c\\in ch(n)}$, which contain non-negative and locally normalized weights [Peharz et al., 2015], $w_{n,c} \\geq 0$, $\\sum_{c\\in ch(n)} W_{n,c} = 1$; and (ii) parameters of all leaf units, $0_\u03b9$, which are specific to a given family of densities, with possibly a different density for each n \u2208 L.\n\nDefinition 6. (Scope function). The mapping \u03c8 : V \u2192 $2^x$\u2014-from the set of units to the power set of x-outputs a subset of x \u2208 X for each n \u2208 V and is referred to as the scope function. If n is the root unit, then \u03c8(n) = x. If n is a sum unit or a product unit, then \u03c8(n) = $\\cup_{c\\in ch(n)} \\psi(c)$.\n\nPCs are an instance of neural networks [Vergari et al., 2019, Peharz et al., 2020], where each computational unit is a probability density characterized by certain functionality. Leaf units are the input of a PC. For each n \u2208 L, they compute a (user-specified) probability density, $p_n(\\cdot)$, over a subset of x given by the scope, \u03c8(n), which can be univariate or multivariate [Peharz et al., 2015]. Sum units are mixture densities that compute the weighted sum over its children, $p_n(\\cdot) = \\sum_{c\\in ch(n)} W_{n,c}p_c(\\cdot)$, where $w_{n,c}$ (Definition 5) weights the connection between the sum unit and a child unit. Product units are factored densities that compute the product of its children, $p_n(\\psi(n)) = \\prod_{c\\in ch(n)} p_c((\\psi(c))$, establishing an unweighted connection between n and c and introducing the conditional independence among the scopes of its children, (c). It is commonly the case that (layers of) sum units interleave (layers of) product units. The computations then proceed recursively through G until reaching the root unit-the output of a PC.\n\nPCs are generally intractable. They instantiate themselves into specific circuits\u2014and thus permit tractability of specific inference scenarios\u2014by imposing various constraints on G, examples include smoothness, decomposability, structured decomposability, determinism, consistency [Chan and Darwiche, 2006, Poon and Domingos, 2011, Shen et al., 2016]. In this work, we use only the first two of these constraints.\n\nDefinition 7. (Structural constraints). We restrict ourselves to PCs that respect the following constraints on G. Smoothness: children of any sum unit have the same scope, i.e., each n \u2208 S satisfies du, v \u2208 ch(n) : \u03c8(u) = \u03c8(v). Decomposability: children of any product unit have a disjoint scope, i.e., each n \u2208 P satisfies Vu, v \u2208 ch(n) : \u03c8(u) \u2229 \u03c8(v) = \u00d8.\n\nA PC satisfying Definition 7 can be seen as a polynomial composed of leaf units [Darwiche, 2003]. This construction guarantees that any single-dimensional integral interchanges with a sum unit and impacts only a single child of a product"}, {"title": "B RELATED WORK", "content": "Non-probabilistic models (NPMs). Graph neural networks (GNNs) have become a flexible and powerful approach for (non-probabilistic) representation learning on graph-structured data. Variants of GNNs range from the original formulation [Gori et al., 2005, Scarselli et al., 2008] to GCN [Kipf and Welling, 2017], MPNN [Gilmer et al., 2017], GAT [Veli\u010dkovi\u0107 et al., 2018] and GraphSAGE [Hamilton et al., 2017], among others. They assume input data as an undirected cyclic graph, which they encode into a low-dimensional representation by aggregating and sharing features from neighboring nodes. However, without necessary adaptations, their structure-agnostic character incurs unnecessary computational costs when applied to graphs with structural constraints. This fact has led to the design of GNNs that respect the constraints in the form of directed acyclic graphs (DAGs) [Thost and Chen, 2021]. GNNs for trees (i.e., a specific case of DAGs) create the encoding by traversing nodes of the graph bottom-up (or up-bottom) and updating the state representation of a given node based only on its children. Examples of this approach include RNN [Socher et al., 2011, Shuai et al., 2016], Tree-LSTM [Tai et al., 2015] and TreeNet [Cheng et al., 2018].\n\nIntractable probabilistic models (IPMs). Extending deep generative models from unstructured to graph-structured do-mains has recently gained significant attention. Variational autoencoders learn a probability distribution over graphs, p(G), by training an encoder and a decoder to map between space of graphs and continuous latent space, minimizing the evidence lower bound on the marginal log-likelihood in the process [Kipf and Welling, 2016, Simonovsky and Komodakis, 2018, Grover et al., 2019]. Generative adversarial networks learn p(G) by training (i) a generator to map from latent space to space of graphs and (ii) a discriminator to distinguish whether the graphs are synthetic or real, relying on the two-player, minimax objective [De Cao and Kipf, 2018, Bojchevski et al., 2018]. Flow models use the change of variables formula to transform a base distribution on latent space to a distribution on space of graphs, p(G), via an invertible mapping, using direct optimization of the marginal log-likelihood [Liu et al., 2019, Luo et al., 2021]. Autoregressive models learn p(G) by relying on the chain rule of probability to decompose a graph, G, into a sequence of subgraphs and constructing G node by node [You et al., 2018, Liao et al., 2019]. Diffusion models learn p(G) by noising and denoising trajectories of graphs based on forward and backward diffusion processes, respectively, optimizing the score matching objective or evidence lower bound on marginal log-likelihood [Jo et al., 2022, Huang et al., 2022, Vignac et al., 2022]. GNNs mentioned in the previous paragraph (NMPs) have been used as the building blocks of all these generative models, which is the main reason their marginal probability density is intractable.\n\nTractable probabilistic models (TPMs). There has not been a substantial interest in probabilistic models facilitating tractable inference for graph-structured data. Graph-structured SPNs [Zheng et al., 2018] heuristically decompose generic cyclic graphs into components that are isomorphic to a pre-specified set of sub-graph templates (of an arbitrary acyclic structure) and then design the conventional SPNs for each of the templates. The roots of these SPNs are aggregated by the sum unit and a layer of the product units. Graph-induced SPNs [Errica and Niepert, 2023] are similar to this approach. They also decompose generic cyclic graphs, but in a more principled manner, building a set of trees based on a user-specified neighborhood. The SPNs are not designed for the whole trees (templates) but for their nodes, only to model the feature vectors. The aggregation is performed by using the posterior probabilities of the root sum units at lower depths of the tree to condition the sum units at upper depths. Relational SPNs (RSPNs) [Nath and Domingos, 2015] are tractable probabilistic models for relational data (a particular form of graph-structured data). The semantics of the relational data differs from our graph-structured data. However, the set unit of the SPSNs is similar to the exchangeable distribution template of the RSPNs. The main difference to the RSPNs lies in that SPSNs model cardinality. The mixture of probability densities over finite random sets is perhaps the most related approach to SPSNs [Phung and Vo, 2014, Tran et al., 2016, Vo et al., 2018]. In our context, it can be seen as the sum unit with its children given by the set units. However, these shallow probabilistic models are designed for plain point pattern data. SPSNs generalize them to deep probabilistic models for graph-structured data, achieving far higher expressivity by stacking the sum units with the product units and reflecting the variable size nature of the graph by incorporating a hierarchy of the set units. Another approach relies on SPNs to introduce correlations into graph variational autoencoder [Xia et al., 2023], which does not allow for tractable probabilistic inference."}]}