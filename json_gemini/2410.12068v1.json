{"title": "V3D-SLAM: Robust RGB-D SLAM in Dynamic Environments with 3D Semantic Geometry Voting", "authors": ["Tuan Dang", "Khang Nguyen", "Manfred Huber"], "abstract": "Simultaneous localization and mapping (SLAM) in highly dynamic environments is challenging due to the correlation complexity between moving objects and the camera pose. Many methods have been proposed to deal with this problem; however, the moving properties of dynamic objects with a moving camera remain unclear. Therefore, to improve SLAM's performance, minimizing disruptive events of moving objects with a physical understanding of 3D shapes and dynamics of objects is needed. In this paper, we propose a robust method, V3D-SLAM, to remove moving objects via two lightweight re-evaluation stages, including identifying potentially moving and static objects using a spatial-reasoned Hough voting mechanism and refining static objects by detecting dynamic noise caused by intra-object motions using Chamfer distances as similarity measurements. Our experiment on the TUM RGB-D benchmark on dynamic sequences with ground-truth camera trajectories showed that our methods outperform the most recent state-of-the-art SLAM methods. Our source code is available at https://github.com/tuantdang/v3d-slam.", "sections": [{"title": "I. INTRODUCTION", "content": "Visual simultaneous localization and mapping (vSLAM), an important study of robotics, essentially relies on visual information from the camera to localize itself and build a map of the environment. With the emergence of low-cost cameras, vSLAM captures tremendous attention from the research community, especially RGB-D-based SLAM, which is one of the most popular choices for its availability and appropriateness for indoor scenes. A number of notable works have been proposed, such as Dense-SLAM [1] and ORB-SLAM [2]; however, like traditional SLAM methods [3], [4], these approaches implicitly assume that the environment is static, where frame-to-frame extracted keypoints are matched, often failing to deal with dynamic scenes when objects and the camera are moving simultaneously.\nAlthough vSLAM has been extensively studied with help from recent advancements in computer vision, especially with deep neural networks, some issues have not been well-addressed due to the dynamics of the environment. DS-SLAM [5] uses a segmentation model and optical flow to detect moving objects and consider these moving objects as noises to be removed. TRS-SLAM [6] deals with objects on the training dataset while segmenting the depth image via k-means clustering to handle unknown objects, which does not require prior knowledge. CFP-SLAM [7] detects moving objects and uses a Kalman filter with a Hungarian algorithm to compensate for object misdetections. These methods obtain high accuracy in dynamic environments, and their results are considered state-of-the-art on the TUM RGB-D benchmark [8]. While the DefSLAM [9] tracks deformable objects with a parametric template, which limits the method to a small number of objects that are matched with the template, we are seeking a non-parametric method to detect deformable objects, paving the way to detect the intra-moving objects. Nevertheless, their flip side is to represent moving objects as center points of segmentation blobs or bounding boxes, which are easily distorted by different views. Another scenario in which these methods fail is when only a part of the object moves. Still, its center remains unchanged (i.e., a person wobbles his head without displacing him from one place to another or rotating a revolving chair). These motions within an object cause significant noise to the feature extractor, leading to feature inconsistency between frames and, eventually, errors in camera pose estimation.\nTo address the above issues, we propose V3D-SLAM, which differentiates moving objects from static objects by projecting depths into point clouds and analyzing their 3D shapes and geometry. V3D-SLAM first identifies potentially moving objects by a novel Hough voting mechanism via the topology of 3D objects in one frame, resulting in two set objects (static objects and moving objects), thus analyzing the moving parts within static objects to seek moving parts via measuring the similarity using Chamfer distances [10]."}, {"title": "II. RELATED WORK", "content": "RGB-D SLAM in Dynamic Environments: Estimating poses of a moving camera has been an interesting topic in robotic vision, with the RGB-based earliest example using Tomasi-Kanade factorization [11]. This has laid the foundations for later research addressing dynamic objects in SLAM, including associating correspondences between RGB images [1], [12], [13], weighting edge-like features for tracking [14], identifying objects' movements via differences of consecutive frames [15], differentiating static and dynamic objects via feature correlations [16], [17]. However, these methods only deal with feature extraction on consecutive frames but lack prior semantic knowledge of the scene. To enhance this, recent works in vSLAM leverage object detection [18], [19], [7] and segmentation [5], [20], [21], [6], [22], [23] models to extract RGB-based knowledge of the scene and use depth frames to reconstruct the scene's structure. Indeed, these methods only concentrate on 2D object movement via epipolar geometry while not guaranteeing 3D intra-object movement, which may be visually unchanged on RGB images. Furthermore, DefSLAM [9] uses the precomputed template to detect deformable objects [24], leading to a lack of generality to other categories. To address this issue, we employ the non-parametric method, which re-associates depth frames to information extracted from colored frames and computes the similarity between the current and subsequent frames.\nSpatial-Reasoned Hough Voting for Dynamic Objects: The voting mechanism in Hough transform [25], searching for the parameter that casts the most votes where each sample votes for another in parameter spaces, has been used widely for object detection [26] with explicit and implicit features. DEHV [27] provides the Hough-based probabilistic approach, in which each object class and location cast the votes from depth scales. Max-margin Hough transform [28] is also introduced to indicate the important weights for possible local locations of the object center. PoseCNN [29] uses Hough voting to predict object centers via network-based extracted features. Similarly, VoteNet [30] samples point clouds to extract point features where votes are gained from the point feature domain. In this work, we devise a similar approach where the 3D dynamic objects are voted through the topology with other presented 3D objects. Instead of learning implicitly by using a neural network, each object will cast votes directly from pure geometry information."}, {"title": "III. OVERVIEW OF V3D-SLAM", "content": "The overview of V3D-SLAM, improving the robustness of dynamic RGB-D SLAM, is illustrated in Fig. 1. In between two consecutive frames, we first mask out potential moving objects from the scene [31], [32] to obtain static objects with backgrounds, followed by reconstructing point clouds of segmented instances. Noises induced by the segmentation model's uncertainty create fragments in point clouds of instance (Fig. 3), which spatially do not belong to the objects, slightly shift the center of the point clouds, and influence the votes for potential dynamic objects significantly. To avoid these ambiguities, we statistically remove outliers at the instance level to eliminate segmentation-induced artifacts (Sec. IV-B) before identifying instances' spatial centers. Thus, based on the topology of 3D objects between two frames, the voting mechanism (Fig. 4) is implemented to identify moving objects with geometric and spatial information from themselves for other presented entities (Sec. IV-C).\nIn many indoor dynamic environments, the centers of large objects do not change from frame to frame. Still, some parts may be displaced, introducing another type of noise in the sequence. To identify these dynamic intra-objects, we track and calculate 3D objects' similarities via object deformation instead of objects' relative displacement on image planes due to their errors and distortion in different views (Sec. IV-D). Within this tracking procedure, based on the physical constraints of objects' movements, we assume that the objects keep moving in the same direction in an infinitesimal period (between two frames in a 30-fps sequence) after identifying each moving object to compensate for the object misdetection or object that is out of field-of-view. Pixel-level features are also extracted on unmasked regions using ORB [33] for camera pose estimation followed by camera trajectory optimization using pose graph optimization (PGO) [34] (Sec. IV-A)."}, {"title": "IV. METHODOLOGY", "content": "A. Instance Segmentation & Feature Extraction\nWe first segment the objects in RGB images using YOLOv8 [31], then map the masked regions into depth images to make point clouds of instances. Hypothesizing as potentially moving, the objects, which are classified as 'person', are temporarily excluded from the scene to be re-evaluated for their movements in 3D space (Sec. IV-C). Thus, we detect edge- and corner-like features using ORB [33] on unmasked regions containing static objects and the scene background. Not only for its segmenting capability, the segmentation model also gives us the number of objects, which benefits our subsequent noise removal procedure.\nB. Sensor Noises & Segmentation Outlier Rejection\nIn the context of SLAM, it is implicitly assumed that either the object or the robot is still. However, in dynamic environments, the fast-moving robot with the RGB-D camera produces perception uncertainty, which is best seen via RGB images, where the pixels are blurred, and depth maps, where the depth pixel cannot be interpolated from the previous frame [35]. Furthermore, segmentation models add uncertainty when recognizing objects with RGB-D perception. To alleviate this, we categorize errors into two types: (1) depth map noises and (2) segmentation-induced mapping errors.\n1) Depth Map Noises: We re-estimate the depth value, $d(i, j)$, on depth maps via a $k \\times k$ Gaussian kernel, $G(i, j)$, centered at that pixel to prevent potential errors caused by captured devices when calculating objects' center locations. The 2D-3D re-projection discretization is re-fined as below:\n$z = \\frac{\\sum_{i=u-k}^{ru+k} \\sum_{j=v-k}^{\\\u03c1\u03c5+k} d(i, j) \\cdot G(u, v)}{\\sum_{i=u-k}^{u+k} \\sum_{j=v-k}^{v+k}d(i, j) \\cdot \\frac{1}{2\\pi\\sigma e^{(k^2+2)/2\\sigma^2}}}$\n$x= \\frac{u- C_x}{f_x} \\cdot z$ and $y = \\frac{V-C_y}{f_y}$ \nwhere (u, v) represents the image coordinates of pixels, (x, y, z) are their 3D coordinates, $(C_x, C_y, f_x, f_y)$ are defined as the camera's optical center and focal lengths, and $\\sigma^2$ is the variance of $k^2$ pixels locating in the $k \\times k$ Gaussian kernel.\n2) Segmentation-Induced Mapping Errrors: As shown in Fig. 2, RGB-based segmentation introduces pixel-level sen-sitivity when projecting image blobs via the corresponding depth maps to create point clouds of instances. Therefore, we first downsample point clouds through voxelization to efficiently remove these artifacts to guarantee balance and fairness during noise removal between dense and spare re-gions, as shown in Fig. 3 (right). Thus, we use density-based spatial clustering of applications with noises (DBSCAN) algorithm [36] to group points into different clusters and hence filter out clusters by taking the n-first 3D large blobs with n is the number of objects recognized provided by the segmentation model in Sec. IV-A, resulting in a finer point clouds of 3D semantic objects, as depicted in Fig. 3 (right).\nC. Spatial-Reasoned Hough Voting for Dynamic Objects\nIdentifying dynamic objects in 2D images is complicated and sensitive to the dynamics of the environment, especially when the camera and objects move together since the object-to-object displacements are not correctly spatially interpreted with different projections from a moving camera. For ex-ample, objects can move at the same velocity as cameras, resulting in unchanged dynamic interpretations based solely on RGB images.\nTo alleviate these concerns, we interpret scene dynam-ics by computing Euclidean distances between centroids of 3D instances in the current frame and estimating the displacement of objects in the next frame. If the displacement is larger than the pre-defined threshold, the accumulator $V(O_i|Oj, r)$ casts the vote from object j to object i for the high frame rate, $r \\approx 30$; on the other hand, low-fps sequences neither can track and recognize objects.\n$V (02/0j, r) = \\sum I[dist(0_i, 0_j|r)]$ (1)\nwhere $dist()$ represents the Euclidean distance between 3D objects' centroids and $I(\u00b7)$ denotes the indicator function and is as follows with the defined distance threshold, $T_a$:\n$I[dist (Oi, Ojr)] = \\begin{cases} 1, & \\text{if } dist(0_i, 0_j|r) \\geq T_a \\\\ 0, & \\text{otherwise}. \\end{cases}$\nAlg. 1 takes in objects, O, in previous (p) and current (c) frames as inputs and outputs the keys and IDs of objects that are voted as moving. Specifically, a class and object ID are assigned for each presented object instance in the frame. This information is tracked until the objects are no longer given in the scene. In contrast, the 3D pairwise distances of objects' centers are calculated among in-frame objects, and votes are cast among objects based on Eq. 1.\nD. Intra-Object Movements\nObjects like revolving chairs or people sitting without shifting their locations can cause noises by moving their heads or rotating the chair while talking with others. We treat these objects as deformable objects that must be considered when extracting their keypoints. To solve this, we use Chamfer distance to measure the similarity between two point clouds of segmented moving objects, $D(P_1, P_2)$, in consecutive frames:\n$\\begin{aligned} D(P_1,P_2) = &\\frac{1}{2}\\big[\\frac{1}{|P_1|} \\sum_{p_i \\in P_1} \\min_{p_j \\in P_2} ||p_i - p_j||^2 \\\\ &+ \\frac{1}{|P_2|} \\sum_{p_j \\in P_2} \\min_{p_i \\in P_1} ||p_i - p_j||^2 \\big] \\end{aligned}$ (2)\nwhere $P_1$ and $P_2$ represent two clouds, respectively, $p_i$ and $p_j$ are ith and jth points in $P_1$ and $P_2$, respectively, and $P_i$ indicates the number of points in the point cloud $P_i$. Using Eq. 2, if an object is calculated as the deformable object as such definition, we also included features extracted from their RGB frame for camera pose estimation.\nE. Camera Pose Estimation\nAfter identifying dynamic objects, we mask them out of regions of interest and consider them as noises on images, leaving images constructed by static objects by changing the camera views. By extracting and tracking features on static objects and the scene background when the camera moves, we thus estimate the camera poses and trajectory. Inspired by the robustness of this task from ORB-SLAM2 [12] and ORB-SLAM3 [37], we apply feature extraction on unmasked RBG images to extract keypoints on two consecutive frames, then matching the corresponding features between two keypoint sets. At this point, we assume two point sets are static point sets with different camera views. The transformation matri-ces are computed using RANSAC while removing outliers on these pointsets that do not satisfy the triangulation constraint of epipolar geometry of a pair RGBD. The two new point sets are used to look at depth to reconstruct 3D point sets, which are used to estimate the camera pose, including translation and rotation, using the RANSAC method. The estimated camera pose is added to the trajectory for closure detection and trajectory optimization using PGO [34]."}, {"title": "V. EVALUATION ON TUM RGB-D BECHMARK", "content": "A. Testing Sequences & Evaluation Metrics\nTo test the performance of our proposed technique with spatial-reasoned votes for dynamic 3D objects, we evaluate Alg. 1 on the TUM RGB-D benchmark, containing eight dynamic sequences in terms of Absolute Trajectory Error (ATE) and Relative Pose Error (RPE), including translational and rotational drifts, in both root mean square error (RMSE) and standard deviation (SD). Across these metrics, we also compare our method against state-of-the-art vSLAM tech-niques: ORB-SLAM2 [12], DS-SLAM [5], DynaSLAM [20], TRS [6], Blitz-SLAM [22], and CFP-SLAM [7].\nB. Quantitative Results\nTable I shows the quantitative comparisons between our method and other methods for the metric of ATE: for most of the sequences, we are able to obtain lower RMSE and SD compared to the recent state-of-the-art CFP-SLAM, except fr3/w/xyz and fr3/w/rpy. Table II and Table III depict the RPE in terms of the camera's translational and rotational drifts, respectively. We achieve better estimates for translational drift, but the fr3/s/half sequence, where we underperform Blitz-SLAM and CFP-SLAM. For rotational drift, we obtain finer results in fr3/s/xyz, fr3/s/rpy, fr3/w/half, and fr3/w/static sequences, which are more significant compared to ORB-SLAM2 and CFP-SLAM. For notations, we use bold aterisk*, bold dagger\u2020, and bold section signs to highlight the lowest error, the second-lowest error, and the third- error, respectively."}, {"title": "VI. REAL-ROBOT EXPERIMENTS & DEPLOYABILITY", "content": "A. Experimental Setup\nThe Intel RealSense D435i RGB-D camera is mounted on the display of the Baxter robot, and the scene of various indoor objects with bean bags, backpacks, chairs, books, and cups is set up, as shown in Fig. 6. The robot perceives objects within a 3-meter radius, the ideal range given by the camera, while arbitrarily moving to estimate its camera poses and trajectory and reconstruct the scene simultaneously with the acquired RGB-D stream.\nB. Performance of Proposed Method\nWe deploy our method on the Intel NUC5i3RYH PC, which runs on its native onboard CPU without any dedicated GPUs. The entire pipeline sufficiently achieves on-robot de-ployable performance. Fig. 6 shows the arbitrary movements of the Baxter robot to perceive the setup scene up front at the top row. Our arbitrary movements supported by four mecanum wheels on the integrated Dataspeed mobility base allow the robot to slide to the left and right side, move toward and backward, and rotate in a clockwise and anti-clockwise manner, closely mimicking the camera's movements in the TUM RGB-D benchmark [8]. The qualitative results of the estimated camera trajectory in 3D and the 3D reconstructed scene are also shown in Fig. 6 at the bottom row with the red line indicating the estimated camera trajectory with optimization, and the camera frustums are marked in orange.\nC. Demonstration\nThe demonstration video shows the deployability of our method on the Baxter mobile robot with an employed Intel RealSense D435i RGB-D camera and is made available at https://youtu.be/K4RcKrASpqI."}, {"title": "VII. CONCLUSIONS", "content": "In this paper, we present V3D-SLAM, a technique that reliably estimates and reconstructs the camera trajectory by removing noises induced by the dynamic nature of the environment. The dynamic objects are identified at the pixel level using state-of-the-art object segmentation on RGB images and refined using geometrical information in the 3D domain. Besides using Hough voting to identify moving objects in 3D, we also detect deformable objects using Chamfer distance to exclude their intra-object changes, which noise the feature extractor. To verify the robustness of our proposed method, we conduct experiments on the TUM RGB-D benchmark and compare our proposed method to recent state-of-the-art SLAM techniques. The experimental results show that our method mostly outperforms others regarding ATE and rotational and translational RPE metrics. Through deployment, our method enables the Baxter robot to perform RGB-D SLAM and its operations."}]}