{"title": "PMLBmini: A Tabular Classification Benchmark Suite for Data-Scarce Applications", "authors": ["Ricardo Knauer", "Marvin Grimm", "Erik Rodner"], "abstract": "In practice, we are often faced with small-sized tabular data. However, current tabular benchmarks are not geared towards data-scarce applications, making it very difficult to derive meaningful conclusions from empirical comparisons. We introduce PMLBmini, a tabular benchmark suite of 44 binary classification datasets with sample sizes \u2264 500. We use our suite to thoroughly evaluate current automated machine learning (AutoML) frameworks, off-the-shelf tabular deep neural networks, as well as classical linear models in the low-data regime. Our analysis reveals that state-of-the-art AutoML and deep learning approaches often fail to appreciably outperform even a simple logistic regression baseline, but we also identify scenarios where AutoML and deep learning methods are indeed reasonable to apply. Our benchmark suite, available on https://github.com/RicardoKnauer/TabMini, allows researchers and practitioners to analyze their own methods and challenge their data efficiency.", "sections": [{"title": "1 Introduction", "content": "The easy access to data has fueled machine learning research in recent years. Massive text corpora crawled from the web have given rise to large language models such as GPT-3 with emergent abilities such as in-context learning (Brown et al., 2020). Large-scale image data have served as the foundation for text-to-image systems like DALL-E 3 (Betker et al., 2023), large-scale video data for text-to-video generators like Sora (Brooks et al., 2024). In contrast to text, image, or video data, collecting large- or even medium-sized tabular data is often challenging in practice, despite tabular data being among the most ubiquitous dataset types in real-world applications (Borisov et al., 2022; McElfresh et al., 2024; Shwartz-Ziv and Armon, 2022). GitTables, for example, a curated corpus of 1 million tables from GitHub, only contains 142 instances on average (Hulsebos et al., 2023). In clinical diagnostic or prognostic settings, the number of instances is frequently limited due to the rareness of medical conditions or patient losses at follow-up assessments, respectively (Moons et al., 2015, 2019; Steyerberg, 2019).\nThe difficulty to acquire large- or even medium-scale tabular datasets in many domains presents researchers and practitioners with a unique set of challenges. On the one hand, overfitting is a major concern when applying complex algorithms on small-sized datasets. On the other hand, cross-validation folds may become too small to adequately represent both the original sample and the population of interest. This makes it very difficult to find good hyperparameter settings so that data-driven hyperparameter optimization may fail to increase, or may even decrease, the predictive performance for individual datasets in the low-data regime (Riley et al., 2021; \u0160inkovec et al., 2021; Van Calster et al., 2020). To facilitate empirical comparisons across studies in this setting, it is therefore imperative to systematically evaluate machine learning pipelines not on a hand-picked, narrow selection of datasets, but on a standardized, diverse dataset collection a benchmark suite (Bischl et al., 2021; Fischer et al., 2023; Gijsbers et al., 2019, 2022; McElfresh et al., 2024; Olson et al., 2017; Romano et al., 2022).\nIn this paper, we contribute to the tabular benchmarking literature in the following ways:"}, {"title": "2 Related Work and Desiderata", "content": "Machine learning repositories for tabular data abound, prominent examples being Kaggle, UCI, or OpenML (Vanschoren et al., 2014), which form the basis for a wide range of carefully curated tabular benchmark suites (Bischl et al., 2021; Fischer et al., 2023; Gijsbers et al., 2019, 2022; McElfresh et al., 2024; Olson et al., 2017; Romano et al., 2022). In spite of the practical relevance and unique challenges in the low-data regime (Sect. 1), small-sized datasets with sample sizes \u2264 500 have received very little attention in benchmarking studies so far, though (Fischer et al., 2023; Gijsbers et al., 2019). The suites that do include \u2265 1 small-sized tabular benchmark dataset are the Penn Machine Learning Benchmarks (PMLB) (Olson et al., 2017; Romano et al., 2022), the AutoML Benchmark (AMLB) (Gijsbers et al., 2022), the OpenML-CC18 (Bischl et al., 2021), and TabZilla (McElfresh et al., 2024). Their respective dataset sources, task types, sample size range, number of included datasets, and number of included datasets with sample sizes \u2264 500 are shown in Table 1. AMLB, OpenML-CC18, and TabZilla include an insufficient number of small-sized datasets to allow"}, {"title": "3 Benchmark Design", "content": "In the following, we report on the design of our tabular classification benchmark, PMLBmini, starting with the included datasets. We then outline the baseline methods that we integrated into our suite, and finally provide details on how our benchmarking tool can be used for both in-depth empirical comparisons and meta-feature analyses in the low-data regime."}, {"title": "3.1 Datasets", "content": "We selected all binary classification datasets with sample sizes \u2264 500 from the curated benchmark suite with the largest number of small-sized datasets, PMLB (Sect. 2), resulting in 44 datasets (Table 3 in Appendix C). There was no sensitive personally identifiable information or offensive content in the selected datasets, there were no missing values, and categorical features were already numerically encoded in PMLB. This provided a simple, extensible basis for our tabular classification benchmark. We shuffled all datasets and encoded all labels to {0, 1}. The 2 binary classification datasets from the TabZilla benchmark (colic and heart-h) are already included in PMLB and therefore in our suite, the 2 binary classification datasets from AMLB and OpenML-CC18 (arcene and dresses-sales, respectively) are not. Both arcene and dresses-sales would be clear outliers in our collection, though; the former because its feature set size is roughly 6000% larger than the maximal feature set size in our suite, the latter because > 80% of its instances contain missing values. Even without the 2 excluded datasets from AMLB and OpenML-CC18, our final tabular classification benchmark suite, PMLBmini, includes more than 6 times more small-sized datasets than AMLB, OpenML-CC18, and TabZilla combined (Sect. 2). To demonstrate that our dataset selection is not just large in quantity, but also represents a diverse set of data science problems like PMLB (Olson et al., 2017; Romano et al., 2022), we summarize key dataset characteristics in Table 2 and plot the feature set size against the sample size for each included dataset in Fig. 4b in Appendix A, showing that our benchmark suite indeed covers a wide range of problem instances with a focus on smaller feature set and sample sizes."}, {"title": "3.2 Available Methods", "content": "Next to our preselected collection of small-sized tabular datasets, our suite also provides researchers and practitioners with a number of baseline methods. The baselines were chosen to represent a range of different machine learning approaches, but our suite can also be easily extended to include additional pipelines (Sect. 3.3). We integrated a simple L2-regularized logistic regression classifier (Knauer and Rodner, 2023), state-of-the-art automated machine learning (AutoML) frameworks (Alaa and van der Schaar, 2018; Imrie et al., 2023; Erickson et al., 2020; Salinas and Erickson, 2023), and recent off-the-shelf deep neural networks (Hollmann et al., 2023; Bonet et al., 2024). Some of these methods have already been shown to perform well when data is scarce (Christodoulou et al., 2019; Knauer and Rodner, 2023; McElfresh et al., 2024). An extensive comparison for sample sizes \u2264 500 is currently missing, though (Bonet et al., 2024; Hollmann et al., 2023; M\u00fcller et al., 2023; Puri et al., 2023). We offer the first comprehensive evaluation for these methods in the low-data regime in Sect. 4.2 and assess when state-of-the-art AutoML and deep learning approaches are better suited than a simple logistic regression baseline in this setting in Sect. 4.3.\nLogistic regression We integrated an L2-regularized logistic regression classifier as a simple, transparent, intrinsically interpretable baseline. We use a continuous conic formulation that is designed to be run-to-run deterministic (MOSEK ApS, 2023) and can be easily extended to include cardinality or budget constraints for best subset selection (Deza and Atamt\u00fcrk, 2022; Knauer and Rodner, 2023). We re-encode all labels to {-1, 1} and \"min-max\u201d scale all features for logistic regression. The L2-regularization hyperparameter \u03bb is tuned via a (nested) stratified, 3-fold cross-validation using [0.5, 0.1, 0.02, 0.004] as the hyperparameter grid and the deviance as the validation score."}, {"title": "AutoML", "content": "We focused on 2 recently updated AutoML frameworks for implementation into our suite, AutoPrognosis (Alaa and van der Schaar, 2018; Imrie et al., 2023) and AutoGluon (Erickson et al., 2020; Salinas and Erickson, 2023). AutoPrognosis considers logistic regression and decision tree ensembles to automatically build end-to-end machine learning pipelines, including preprocessing, model, and hyperparameter selection. Meta-learned hyperparameters from external datasets are used to initialize the default pipeline optimization procedure, and a pipeline ensemble is constructed following the search process. AutoGluon, on the other hand, considers k-nearest neighbors, decision tree ensembles, and neural networks for its algorithm search. It meta-learns a model hyperparameter portfolio from external datasets and different random seeds (zero-shot hyperparameter optimization); the training time budget is then spent on ensembling rather than further hyperparameter optimization. Unfortunately, we had to exclude another state-of-the-art framework, Auto-sklearn (Feurer et al., 2015a, 2022), in its current version because the runtime budget was not respected with our setup (Sect. 4.1) 1 2, but we hope to extend our results in the future."}, {"title": "Deep learning", "content": "We also integrated 2 recent pretrained deep learning models, sometimes referred to as tabular foundation models (M\u00fcller et al., 2023), TabPFN (Hollmann et al., 2023) and HyperFast (Bonet et al., 2024). TabPFN is a meta-trained transformer ensemble that performs in-context learning on tabular datasets with up to 100 features, without needing any hyperparameter tuning. On our only benchmark dataset with > 100 features, clean1, we use subsampling to select 100 features at random for TabPFN (Feuer et al., 2023). In contrast to TabPFN, HyperFast uses external datasets to meta-train a hypernetwork, generates smaller, task-specific main networks with the pretrained hypernetwork and the actual training dataset, and optionally fine-tunes and ensembles the main networks.\nWith the selected datasets and integrated baselines, we can perform thorough benchmark tests in the low-data regime and analyze when certain approaches succeed or fail, as described in the next section."}, {"title": "3.3 Python Interface", "content": "PMLBmini is hosted as a Python package on GitHub 3. It can either be imported into an existing project and run in an existing environment, or used standalone in a Docker container. We pro- vide additional information, including how to evaluate a custom tabular classifier on our dataset collection, both on GitHub and in Appendix B.\nExtensibility Each baseline classifier (Sect. 3.2) has been re-implemented as a scikit-learn BaseEstimator with a ClassifierMixin. Therefore, any class that adheres to scikit-learn's stan- dardized duck-typing approach for creating estimators can be used with our benchmarking tool, allowing researchers and practitioners to add new classifiers with minimal overhead. To provide users with a template on how to extend our suite, we implemented logistic regression with a Pipeline and GridSearchCV from scikit-learn.\nThe tabmini module The benchmarking interface is exposed through the tabmini module, which provides a set of convenience functions for automating the benchmarking process:"}, {"title": "4 Experiments", "content": "In this section, we describe the experimental setup of our tabular classification benchmark for data- scarce applications (Sect. 3.1) on AutoML and deep learning against logistic regression (Sect. 3.2) using our suite (Sect. 3.3). We then present the experimental results, the best L2-regularization hyperparameter for each benchmark dataset, and the conditions under which AutoML frameworks and pretrained deep neural networks outperform logistic regression in the low-data regime. Note that we also provide additional experimental results for gradient-boosted decision trees using their package defaults in Table 4 in Appendix C. Overall, we find that logistic regression shows a similar discriminative performance to AutoML and deep learning approaches on 55% of the datasets. AutoML methods are better suited for more complex datasets, when more complex classifiers are needed; off-the-shelf deep neural networks are better suited for certain feature distributions, potentially those that resemble their meta-training data."}, {"title": "4.1 Experimental Setup", "content": "Method details We used our logistic regression implementation with the default optimality gaps via MOSEK 10 (MOSEK ApS, 2023) and JuMP 1.4.0 (Dunning et al., 2017) from Julia 1.8.3. AutoPrognosis 0.1.21 was run from Python 3.10.13 with the default settings, AutoGluon 1.0.0 with the \"best quality\u201d preset, and TabPFN 0.1.9 with 32 ensemble members (Hollmann et al., 2023). HyperFast 0.1.3 was used without fine-tuning of the main networks (due to excessive runtimes exceeding the time budget on our hardware configuration, see below), but with 32 ensemble members like TabPFN (Bonet et al., 2024; Hollmann et al., 2023).\nEvaluation metrics We measured the discriminative performance in terms of the AUC. The training AUC was recorded to assess overfitting and evaluated with a 1h runtime limit for each"}, {"title": "4.2 Experimental Results", "content": "AutoPrognosis, AutoGluon, TabPFN, HyperFast, and logistic regression show a relatively similar discriminative performance at different sample sizes (Fig. 2a). The largest difference occurs at sample sizes from 101 to 200, with HyperFast and logistic regression only reaching a mean test AUC of 0.82 and 0.78, respectively. There is a trend for all methods to perform better when the sample size increases (Fig. 2a). Each approach wins on at least one benchmark dataset and looses on at least one other dataset (Table 3 in Appendix C), echoing the results of McElfresh et al. (2024). Logistic regression performs on par with or better than the best AutoML or deep learning approach on 16% of the datasets, and lies within 1%, 2%, and 3% of the best approach on 34%, 48%, and 55% of the datasets, respectively - possibly because it is less likely to overfit, especially with smaller sample sizes. Interestingly, even when AutoML methods consider logistic regression for algorithm selection (i.e., AutoPrognosis), they may be outperformed by logistic regression alone (e.g., on the backache dataset), again possibly due to overfitting. When statistically comparing pairwise mean test AUC differences (Fig. 2b), we observe that AutoPrognosis and TabPFN achieve a better rank than logistic regression (p < 0.05), whereas AutoGluon and HyperFast are not different from a simple logistic regression baseline (p > 0.05). TabPFN's prior was developed on 45% of our benchmark datasets, though (Hollmann et al., 2023), its performance estimates are therefore likely to be overoptimistic. However, manually excluding all overlapping datasets reduced the benchmark dataset collection in our analysis too much to find statistically significant performance differences between any of"}, {"title": "4.3 Meta-Feature Analysis", "content": "In the following, we use our meta-feature analysis tool to extract meta-features from our benchmark datasets and compute relationships of these meta-features with performance differences between each AutoML / deep learning method and logistic regression (Sect. 3.3). This way, we can determine which dataset properties make more complex machine learning methods well- or less well-suited than a simple logistic regression baseline in the low-data regime. Fig. 5 in Appendix C shows the top-3 correlations for each approach. For AutoPrognosis, the most discriminating meta-feature is the ratio of the intra- and extraclass nearest neighbor distance; for AutoGluon, it is the clustering coeffcient. Both measures capture the dataset complexity, are larger for harder classification problems (Lorena et al., 2019), and show a positive relationship with the performance differences. For AutoML, most meta-features with the largest absolute correlations in fact measure the dataset complexity (Fig. 3). For TabPFN and HyperFast, the most discriminating meta-features are the harmonic mean and minimum of each feature, i.e., summary scores from the feature distribution. For deep learning, meta-features with the largest absolute correlations are almost exclusively statistical meta-features that describe the feature distribution (Fig. 3). Therefore, AutoML methods appear to be better suited for more complex datasets that need more complex classifiers, whereas pretrained deep neural networks may be better suited for datasets with feature distributions that resemble their meta-training data. These insights may prove beneficial when developing new or updating existing machine learning systems, for example by increasing the meta-training set diversity for off-the-shelf deep learning models in the low-data regime."}, {"title": "5 Broader Impact and Limitations", "content": "Our benchmarking tool, PMLBmini, provides a standardized, diverse collection of 44 small-sized tabular datasets (Sect. 3.1) in combination with a range of different machine learning baselines (Sect. 3.2), accessible via a user-friendly Python interface (Sect. 3.3). This allows researchers and practitioners working with tabular data to rigorously challenge their own classifier for data-scarce applications without much effort, and the community to more easily track progress in the field. Interestingly, our initial set of machine learning approaches reveals that simple baselines like logistic regression should not be prematurely discarded since they frequently perform similar to state-of-the-art AutoML and deep learning methods when data is limited (Sect. 4.2). In fact, trying a simple logistic regression baseline in the low-data regime first and only switching to more complex approaches when needed could save resources (McElfresh et al., 2024) and improve the transparency, trust, and applicability of machine learning systems in practice, for example in the clinical domain (Falla et al., 2021; Knauer and Rodner, 2023; Steyerberg, 2019). We also provide users with a practical meta-feature analysis tool to investigate under which conditions, i.e., dataset properties, certain methods are better suited than others. AutoML frameworks appear to work better when more complex classifiers are needed, pretrained deep learning models when their meta-training data are more similar to the test data (Sect. 4.3). We hope that this will support researchers and practitioners to develop new or improve upon existing machine learning systems, for instance by meta-training deep neural networks on a wider range of small-sized tabular data.\nNevertheless, the development of a benchmark suite also carries risks. As our selected datasets are publicly available, we cannot guarantee that they have not already been used for training or tuning the system that is intended to be benchmarked. As many state-of-the-art AutoML and deep learning methods leverage meta-learning (Sect. 3.2), it is quite possible that meta-training and benchmark datasets overlap to a large extent for some approaches - in this case, users could either accept the inherent bias in the benchmark results or have the option to manually exclude the affected datasets (Sect. 3.3). Overoptimism and \"arbitrary\" dataset exclusions can potentially undermine the suite's promise to increase the comparability between methods and studies, though. Moreover, we also want to emphasize that our benchmark suite only contains binary classification problems without missing values and with categorical features being numerically encoded (Sect. 3.1), i.e., categorical features contain no more textual information that could be used by AutoML methods (Erickson et al., 2020) or multimodal foundation models (Achiam et al., 2023). For now, PMLBmini therefore only represents a small slice of the data-scarce problems commonly encountered in practice and disadvantages approaches that naturally handle missing values and categorical features such as gradient-boosted decision trees. Additionally, our meta-feature analysis tool currently only relies on a bivariate association measure that does not take confounding factors into account and is not designed to detect meta-feature interactions or non-monotonic relationships (Gijsbers et al., 2022; McElfresh et al., 2024). Further limitations could be addressed by integrating assessments for training or inference speed and encountered errors (Gijsbers et al., 2022; McElfresh et al., 2024), scaling to slightly larger sample sizes for finding out at which point more complex methods start to consistently outperform simple baselines, and subsampling larger tabular datasets to increase the number of benchmark datasets in our suite. We strongly encourage future research and contributions in these directions."}, {"title": "6 Conclusion", "content": "Although researchers and practitioners are frequently confronted with data-scarce applications, (very-)small sized tabular data are generally underrepresented in current machine learning bench- marks. In this work, we introduced PMLBmini, a tabular benchmark suite of 44 binary classification datasets with sample sizes \u2264 500. We showed how our benchmarking tool can be used to evaluate current AutoML and deep learning methods, and to analyze if and when they are better suited than a simple logistic regression baseline. In summary, we found that state-of-the-art machine learning approaches fail to appreciably outperform logistic regression on 55% of our benchmark datasets. AutoML frameworks appear to work better for more complex datasets, pretrained deep neural networks for datasets with feature distributions that are more similar to their meta-training data. We therefore recommend to increase the dataset diversity when meta-training off-the-shelf deep neural networks. Since hyperparameter optimization in the low-data regime is inherently difficult, we also provide the community with L2-regularization hyperparamters that can be directly used for meta-learning when data is limited. Finally, we encourage researchers and practitioners to challenge the data efficiency of their own tabular classifier using our suite, and to assess under which conditions it succeeds or fails."}, {"title": "BPMLBmini Exemplary Usage", "content": "Below, we illustrate how researchers and practitioners can use our benchmarking tool for empirical comparisons and meta-feature analyses with their own tabular classifier in the low-data regime:\nfrom yourpackage import YourEstimator\nimport tabmini\n# Load the dataset\n# Tabmini also provides a dummy dataset for testing purposes, you can load it with\ntabmini. load_dummy_dataset()"}]}