{"title": "Does It Look Sequential? An Analysis of Datasets for Evaluation of Sequential Recommendations", "authors": ["Anton Klenitskiy", "Anna Volodkevich", "Anton Pembek", "Alexey Vasilev"], "abstract": "Sequential recommender systems are an important and demanded area of research. Such systems aim to use the order of interactions in a user's history to predict future interactions. The premise is that the order of interactions and sequential patterns play an essential role. Therefore, it is crucial to use datasets that exhibit a sequential structure to evaluate sequential recommenders properly.\nWe apply several methods based on the random shuffling of the user's sequence of interactions to assess the strength of sequential structure across 15 datasets, frequently used for sequential recommender systems evaluation in recent research papers presented at top-tier conferences. As shuffling explicitly breaks sequential dependencies inherent in datasets, we estimate the strength of sequential patterns by comparing metrics for shuffled and original versions of the dataset. Our findings show that several popular datasets have a rather weak sequential structure.", "sections": [{"title": "1 INTRODUCTION", "content": "Sequential Recommender Systems (SRSs) have been widely used to model short-term user preferences and user behavior over time, detect interest drifts of individual users, or identify short-term popularity trends [22]. The recent rapid development of the SRSs, which is illustrated by various novel architectures, such as [12, 14, 15, 25, 31], brought the performance and evaluation questions, including the SRSs performance revision [3, 13, 20], evaluation protocols analysis [8, 11] and a choice of the datasets for SRSs evaluation [8, 30].\nAccording to [8], one of the core evaluation issues is the dataset-task mismatch, as the sequential recommendations only make sense if the data has sequential patterns. Thus, the datasets for SRS evaluation should be analyzed to determine the presence of such patterns. To the best of our knowledge, there are no well-established criteria to assess how good the dataset is for SRS evaluation. In our work, we aim to highlight the importance of determining the strength of sequential patterns during dataset selection and propose criteria for such assessment.\nWe propose to use three approaches to analyze the strength of sequential patterns. These approaches are based on the assumption that the sequential patterns in the dataset will be broken if the interactions in user sequences are shuffled in random order. Thus, we can estimate the strength of the sequential structure by comparing chosen metrics acquired on shuffled and unshuffled versions of data.\nThe first approach, based on identifying sequential rules, is simple and model-agnostic. Two others consist of training a sequential model (SASRec and GRU4Rec in our experiments) and evaluating how the performance changes in terms of NDCG@k and HitRate@k and how recommendation lists differ in terms of top-K Jaccard score [18] when the model is applied on original and shuffled sequences. In short, the main contributions of this paper are:\n\u2022 We propose a set of three assessment approaches based on user sequences shuffling to evaluate the presence and strength of a dataset's sequential structure.\n\u2022 Using these approaches, we evaluate the strength of the sequential structure on 15 popular datasets from different domains. We identify the datasets that have weak sequential structure and are less appropriate for SRSs evaluation."}, {"title": "2 RELATED WORK", "content": "The area of the algorithms' evaluation improvement, including proper dataset selection, draws the increased attention of the research community [17, 27, 32]. A significant number of scientific papers employ a limited number of datasets, usually at most 3, to perform a model evaluation [5]. The choice of datasets is often determined by the necessity of being aligned with the previous works to enhance reproducibility. Moreover, datasets are often chosen heuristically, which could influence the observations and/or conclusions obtained [5, 26]. Therefore, in the paper [5], a set of characteristics of datasets was proposed for their clustering and further selection of the most diverse datasets. An approach that would evaluate algorithms more robustly was also considered in the article [23], where various metrics aggregation methods were proposed to evaluate the quality of algorithms in general or on a specific domain.\nThe works above consider the top-K recommendation task, but the sequential recommendation task needs additional attention. In the case of SRSs evaluation, the data should contain sequential patterns that the algorithms can rely on [8]. In our work, we applied and extended several methods to evaluate the sequential structure of a wide range of datasets, frequently appearing in research papers dedicated to sequential recommendations.\nAn analysis of sequential patterns presence with sequential association rules [6] was performed by [28] to prove the presence of union-level sequential patterns and skip behavior in real-world datasets. As the number of rules depends on a dataset's nature and is not directly comparable over multiple datasets, in our work, we propose comparing the number of rules before and after the users' sequences shuffling.\nAnalysis of sequential model performance on the original and shuffled sequences is another way to evaluate the sequential nature of the dataset, which was used by [30] with the SASRec model. The other close approach compares the performance of a sequential recommender and the same algorithm with the sequence modeling part replaced with a feedforward layer, shown in [8].\nWe adopt the idea that the quality of a sequential recommender should decrease on shuffled data in the presence of sequential structure in a dataset. Unlike the work [30], we apply data shuffling to test sequences only and use the same trained model. Our approach allows to estimate the dataset's sequential patterns faster as we don't need to retrain the model.\nRank List Sensitivity approach with top-K Jaccard score calculation was applied by [18]. The authors proposed comparing two recommendation lists produced by the different versions of the recommender, trained on the same data, to quantify the model stability against perturbations. We adapt this approach to compare the recommendations obtained on shuffled and original sequences. This approach was also used in [3] but for different data perturbations and goals."}, {"title": "3 METHODOLOGY", "content": "We apply several methods based on data shuffling to analyze sequential structure in datasets. We rely on the assumption that random shuffling of the order of interactions for each user will break sequential dependencies between items. The patterns identified in the shuffled sequences will appear here solely by chance. So, we can estimate the presence of sequential structure by measuring the discrepancy between the metrics acquired on shuffled and original data. If there are a lot of sequential patterns in the original dataset, the difference between the original and shuffled versions will be significant. The difference will be negligible if the original dataset has no sequential structure.\nWe suggest using two substantially different approaches to analyze datasets from various perspectives. The first one is simple and model-agnostic; it is based on sequential rules identification. The second one is model-based; it consists of training a sequential model and evaluating how model predictions change and deteriorate after shuffling. The model-based approach is further divided into model performance deterioration and change in top-K recommendation list."}, {"title": "3.1 Sequential rules", "content": "We follow [28] and mine sequential association rules with given support and confidence [6]. Suppose we have sequences of user interactions $(i_1, i_2, ..., i_t)$. Then we count sequential rules of the form $(i_{t-L}, ..., i_{t-2}, i_{t-1}) \\rightarrow i_t$, where the item $i_t$ follows directly after $(i_{t-L}, ..., i_{t-2}, i_{t-1})$ in the sequence. We use rules with orders $L = 1$ and $L = 2$, as some datasets have too few higher-order rules. So we count all item 2-grams $(i_j, i_k)$ and 3-grams $(i_j, i_k, i_z)$ that occur in the dataset. Support is computed as the total number of occurrences of these 2-grams and 3-grams. Confidence is computed as support of given n-gram divided by the total number of occurrences of $i_j$ for 2-gram and total number of occurrences of $(i_j, i_k)$ for 3-grams. In the end, we count the number of rules with minimum support and confidence greater than the chosen threshold values.\nHowever, the number of such rules can not be directly compared across different datasets. It is highly dependent on the total number of users, number of items, sequence length, and item popularity. In order to normalize rule counts, we decide to explicitly destroy sequential patterns with shuffling and count the number of rules that occur by chance for a dataset with given characteristics. So, we can measure sequential structure by comparing results for original and shuffled versions of data, and this measure can be comparable across different datasets."}, {"title": "3.2 Model-based approaches", "content": "Model-based approaches involve training a sequential model, which is able to capture sequential patterns in data. The first way (our second approach) is to measure the model performance degradation in terms of recommendation accuracy metrics. Initially, we train and evaluate the model on the original dataset. Subsequently, we shuffle the test sequences, excluding holdout items, and evaluate the model on those shuffled sequences. We expect to observe the model performance deterioration if the dataset has strong sequential patterns. A similar approach was applied in [30], but shuffling was used not only for the test stage but also for the training stage.\nThe second way (our third approach) is based on the Rank List Sensitivity approach with top-K Jaccard score [18], which quantifies the model stability against perturbations by comparing two recommendation lists. We take the top-K recommendation list the model predicts for each user obtained with original and shuffled test sequences and compute mean Jaccard similarity [10] between these lists. The Jaccard score measures the overlap in the top-K items without considering their order. The greater the overlap, the less the predictions differ; thus, the less the model relies on sequential patterns."}, {"title": "4 EXPERIMENTAL SETTINGS", "content": ""}, {"title": "4.1 Datasets selection", "content": "In order to identify the datasets widely used by the academic community for sequential recommenders evaluation, we performed an extended analysis of papers related to sequential recommendations from 3 top-tier conferences, RecSys, SIGIR, and CIKM, for the past five years (2019-2023). In total, we analyzed 130 papers and found that 98 public datasets have been used in one or more of these papers. We identified the most frequently used datasets and selected 12 datasets from different domains for our work. Further details of the analysis are included in the online appendix to our paper, which is available on GitHub.\nIn addition to commonly used academic benchmarks, we included some recently published industrial datasets, such as Zvuk, MegaMarket [23], and OTTO [21]. To reduce computational costs while maintaining a sufficient amount of data for our analysis, we sampled 500,000 users of 12 million in the OTTO dataset."}, {"title": "4.2 Datasets preprocessing", "content": "As in previous publications [12, 25, 28], the presence of a review or rating is considered as implicit feedback. If the dataset contains multiple event types, we follow [8] and leave only one event type corresponding to item views or clicks.\nFollowing common practice [12], we discard users and items with a lower number of interactions. Namely, we applied N-core filtering with N equal to 5 for all datasets to preserve better reproducibility compared to the N-filter approach, where the results depend on filtering order [26]. Following [8], we remove subsequent repeating items from the sequences, e.g., we change (i, i, j) to (i, j), but (i, j, i) remains unchanged. The final statistics of the datasets after preprocessing are summarized in Table 1."}, {"title": "4.3 Evaluation", "content": "One widely used evaluation strategy for sequential recommendations is the leave-one-out strategy applied by [12, 25]. The leave-one-out strategy is reasonable for the next-item prediction task, as it shows the model's sensitivity to sequential information and recent user preferences. Nevertheless, as recent publications showed, the leave-one-out strategy could lead to data leakage and may not truly reflect the performance of recommendation models in online settings [11, 17, 24]. In our work, we combine global temporal and leave-one-out split constraints to preserve the sequential nature of the recommendation task and avoid data leakage.\nThus, for each dataset, we select the global temporal boundary corresponding to 90% of the interactions and take all preceding interactions into the training and validation subset, which are split randomly by the users. The validation set is used for early stopping. Each user interacting after the global temporal boundary is considered a test user. For the test and validation sets, we consider the last interaction of the users as the ground truth and all previous interactions as an input sequence.\nFor performance evaluation, we use Normalized Discounted Cumulative Gain (NDCG@10) and HitRate@10 from the popular library Recommenders. To compare recommendation lists, we use the top-K Jaccard score with K = 10 (Jaccard@10)."}, {"title": "4.4 Implementation Details", "content": "For the SASRec model, we use two self-attention blocks, two attention heads, and a hidden size 64. For the GRU4Rec model, we use one layer with a hidden size of 64. We set the maximum sequence length to 128 and batch size to 128. We train the models with cross-entropy loss as in [13] and use Adam optimizer with the learning rate 1e-3.\nFollowing common practice [28], we apply support and confidence thresholds for sequential rules identification and analysis. We use a support threshold of 5 and a confidence threshold of 0.1. We also tried different threshold values and obtained similar results."}, {"title": "5 RESULTS", "content": ""}, {"title": "5.1 Sequential rules", "content": "Table 3 contains results for the approach based on sequential rules. We can divide the datasets into two quite separate groups. For the first group, the relative change in the number of rules is less than -90% and even close to -100% for many of them. This indicates that these datasets might have fairly strong sequential patterns that break down after shuffling. However, this result should be taken with a grain of salt for some of these datasets (e.g., Beauty, Sports, Games, Steam, and especially Yelp) because the total number of rules was very small even before data shuffling. For the second group, the relative change is significantly greater than -90%, indicating that the datasets from this group might have a much weaker sequential structure. Diginetica, ReatailRocket, Yoochoose, Foursquare, and Gowalla are among these datasets.\nThis approach takes into account only a limited set of short-term sequential patterns, so we do not consider it a standalone measure of a dataset's sequential structure but rather an additional indicator."}, {"title": "5.2 Model-based approaches", "content": "Results for the model-based approaches are shown in Table 2. After shuffling, the performance drop in terms of NDCG@10 and HitRate@10 varies greatly depending on the dataset. The relative change for RetailRocket, Foursquare, Gowalla, and Yelp is very small for all metrics and models, indicating a weak sequential structure. Diginetica and Steam are borderline datasets with relatively weak sequential patterns as well. MegaMarket, ML-20m, 30Music, and Zvuk have a very strong sequential structure according to these metrics.\nThe top-K Jaccard score shows similar trends. Diginetica, RetailRocket, Youchoose, Steam, Foursquare, Gowalla, and Yelp have relatively high Jaccard similarity between recommendation lists for shuffled and original test sequences. The Jaccard@10 greater than 0.5 indicates that the recommendation lists are similar for more than a half. Data shuffling has a lower impact on the model outputs for these datasets, which is expected if the model hasn't learned to rely on sequential patterns. In contrast, the datasets with the most significant performance drop have a minimal Jaccard score. So, Jaccard@10 complements performance-based metrics and confirms the conclusions drawn from them."}, {"title": "5.3 Influence of preprocessing", "content": "It is worth noting that data preprocessing can influence the results of the analysis. For instance, some datasets contain numerous user sequences of short length. Sequential modeling may be of little importance for sequences of length 2 or 3. In our main experiments, we use 5-core filtering, a popular preprocessing choice across many papers [26]. However, we also conducted experiments for SASRec performance deterioration after filtering out only users with less than two interactions. While remaining stable for many datasets, the results have changed the most for Games and Sports. The relative change in NDCG@10 turned from -38% to -6% for Games and from -32% to -10% for Sports, indicating less pronounced sequential patterns. Thus, proper preprocessing may be required to analyze how well the algorithm captures the sequential structure."}, {"title": "5.4 Comparison of approaches", "content": "Figure 3 shows Spearman's correlation coefficient between all metrics considered. In general, the correlation between different approaches is quite high. The correlation between relative changes in performance metrics for SASRec and GRU4Rec models is around 0.9. Both models show very similar results regarding sequential structure in the datasets. The approach based on sequential rules has a lower correlation with model-based approaches, which is expected. Relative change in the number of 3-grams has the lowest correlation. There are too few 3-grams even before shuffling for some datasets, and for 6 out of 15 datasets, the relative change is -100%, making them indistinguishable. So, this is the least reliable metric.\nIn addition, Figure 2 illustrates the ranking of the datasets according to different approaches. In general, the results are quite consistent, although there is no perfect correspondence between all metrics. For example, the Youchoose dataset has significant sequential patterns according to model performance degradation but has bad results according to sequential rules count. However, this dataset has a low number of items and a very high number of interactions, so it can have a lot of 2-grams and 3-grams even after shuffling by chance. Unfortunately, our approach failed to capture this pattern. In contrast, Steam and Yelp are strong according to the model-agnostic approach and weak according to the model-based approach. But, as was stated in section 5.1, the total number of rules for these datasets was very small even before shuffling, indicating an absence of strong sequential structure.\nTo summarize, we suggest that Foursquare, Gowalla, RetailRocket, Steam, and Yelp are datasets with a weak sequential structure. Firstly, recommendation accuracy for all these datasets drops very slightly after shuffling the test data, which we consider to be the most important signal. Secondly, one or two additional approaches confirm this observation."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose a set of three approaches to evaluating a dataset's sequential structure strength. We further analyze a wide range of datasets from different domains commonly used for the evaluation of SRSs. The results of our experiments show that many popular datasets, namely Foursquare, Gowalla, RetailRocket, Steam, and Yelp, lack a sequential structure. Whether these datasets are suitable for evaluating sequential recommendations is questionable and needs further research.\nThe datasets selected for evaluation must be aligned with the task at hand. Conclusions drawn about the relative performance of different algorithms may change after selecting more appropriate datasets. Whether this is true or not is a possible future research direction, as well as further investigation of approaches to the assessment of sequential structure in datasets.\nTo conclude, we encourage researchers to choose datasets for SRSs evaluation with careful consideration of their alignment with the sequential recommendation task."}]}