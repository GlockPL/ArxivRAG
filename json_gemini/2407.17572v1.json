{"title": "CityX: Controllable Procedural Content Generation for Unbounded 3D Cities", "authors": ["Shougao Zhang", "Mengqi Zhou", "Yuxi Wang", "Chuanchen Luo", "Rongyu Wang", "Yiwei Li", "Xucheng Yin", "Zhaoxiang Zhang", "Junran Peng"], "abstract": "Generating a realistic, large-scale 3D virtual city remains a complex challenge due to the involvement of numerous 3D assets, various city styles, and strict layout constraints. Existing approaches provide promising attempts at procedural content generation to create large-scale scenes using Blender agents. However, they face crucial issues such as difficulties in scaling up generation capability and achieving fine-grained control at the semantic layout level. To address these problems, we propose a novel multi-modal controllable procedural content generation method, named CityX which enhances realistic, unbounded 3D city generation guided by multiple layout conditions, including OSM, semantic maps, and satellite images. Specifically, the proposed method contains a general protocol for integrating various PCG plugins and a multi-agent framework for transforming instructions into executable Blender actions. Through this effective framework, CityX shows the potential to build an innovative ecosystem for 3D scene generation by bridging the gap between the quality of generated assets and industrial requirements. Extensive experiments have demonstrated the effectiveness of our method in creating high-quality, diverse, and unbounded cities guided by multi-modal conditions. Our project page: https://cityx-lab.github.io/.", "sections": [{"title": "Introduction", "content": "Recently, 3D generative models have witnessed remarkable achievements in object generation, scene creation, and human avatars, which are crucial technologies for game development, virtual reality, animation, and film production. For example, DreamFusion [1] and Magic3D [2] produce 3D objects guided based on text instructions, while Zero123 [3], Wonder3D [4] and SV3D [5] transform 2D images into high-quality 3D assets. Different from these object-centric 3D generation works, we focus on a challenging unbounded city-scale scene generation, which involves numerous 3D assets (buildings, roads, vegetation, rivers, etc.), various city styles (modern style, traditional style), and strict layout constraint. Although existing pioneers [6, 7, 8, 9] have attempted to generate larger city-scale scenes, a significant gap remains between the quality of the generated content and the standards required for industrial applications."}, {"title": "Related Works", "content": "Generating a 3D urban scene is a complex task involving multiple modules, such as accurately generating a reasonable layout and then constructing appropriate instances on the layout. Additionally, applying Multi-Agent systems to complex tasks like 3D urban scene generation presents significant challenges. In this section, we will discuss works related to these aspects.\nAgent Systems Based on LLMs. When researching agent systems based on Large Language Models (LLMs), the focus lies on effectively integrating and applying these models to execute complex tasks. Existing relevant work encompasses various aspects, including task management, role-playing, dialogue patterns, and tool integration. For example, [17] utilizes the expansive domain knowledge of LLMs on the internet and their emerging zero-shot planning capabilities to execute intricate task planning and reasoning. [18] investigates the application of LLMs in scenarios involving multi-agent coordination, covering a range of diverse task objectives. [19] presents a modular framework that employs structured dialogue through prompts among multiple large pretrained models. Moreover, specialized LLMs for particular applications have been explored, such as HuggingGPT [16] for vision perception tasks, VisualChatGPT [20] for multi-modality understanding, Voyager [21] and [22], SheetCopilot [23] for office software, and Codex [24] for Python code generation. Furthermore, AutoGPT[25] demonstrates the ability to autonomously complete tasks by enhancing AI models, but it is a single-agent system and does not support multi-agent collaboration. In contrast, BabyAGI[26] uses multiple agents to manage and complete tasks, with each agent responsible for different task modules such as creating new tasks, prioritizing the task list, and completing tasks. Multi-agent debate research includes works[27][28] indicating that debates among multiple LLM instances can improve reasoning and factuality, but these methods typically lack the flexibility of tool and human involvement. AutoGen[29], as a general infrastructure, supports dynamic dialogue modes and a broader range of applications, demonstrating potential in advancing this field.\n3D Urban Scene Generation. Scene-level content generation presents a challenging task, unlike the impressive 2D generative models primarily targeting single categories or common objects, due to the high diversity of scenes. Semantic image synthesis, as exemplified by [30, 31, 32, 33], has shown promising results in generating scene-level content in the wild by conditioning on pixel-wise dense correspondence, such as semantic segmentation maps or depth maps. Recent works such as [34, 35, 6] have realized infinite-scale 3D consistent scenes through unbounded layout extrapolation. Additionally, in-depth research has been conducted on using procedural content generation (PCG) techniques to generate natural scenes [36, 37] and urban scenes [38, 39, 40, 41]. For example, PMC [42] proposed a procedural method based on 2D ocean or city boundaries to generate cities. It employs mathematical algorithms to generate blocks and streets and utilizes subsequent techniques to generate the geometric shapes of buildings. While traditional computer graphics methods can generate high-quality 3D data, all parameters must be predefined during the procedural generation process. Since the generated 3D data is subject to rule limitations and exhibits a certain degree of deviation from the real world, this significantly constrains its flexibility and practical utility."}, {"title": "Methods", "content": "Our proposed CityX can generate highly realistic large-scale urban scenes based on multi-modal input. It consists of two key components: a PCG management protocol and a multi-agent framework. The PCG management protocol provides a universal standard to regulate various PCG plugins, enabling CityX to easily scale up by integrating all kinds of irregular and non-uniform plugins. At the same time, the proposed multi-agent framework effectively manages the complex, multi-round interactions between large language models (LLMs) and Blender. Through these components, CityX can produce high-quality 3D urban scenes based on multimodal input guidance, including OSM data, semantic maps, satellite images, and textual descriptions."}, {"title": "PCG Management Protocol", "content": "To facilitate flexible, efficient, and straightforward usage of PCG, we propose a universal protocol for managing PCG plugins, serving as a bridge connecting the LLM and Blender. The protocol primarily comprises three parts: (i) a dynamic API conversion interface, enabling effective and flexible integration of different action functions; (ii) structured encapsulation, accelerating the encapsulation process for beginners and reduces the barriers to using PCG; (iii) infinite asset library and asset retrieval, facilitating unlimited expansion of assets to meet the diverse needs of scene generation.\nDynamic API Conversion Interface. The lack of a unified communication protocol among PCG APIs not only hinders the free combination of different PCGs, thus reducing the diversity of generated content, but also requires manual adjustment of the PCG workflow, which is not user-friendly for 3D modeling beginners. To address this issue, we provide a Dynamic API Conversion Interface. Specifically, we first compile all the input and output formats of the APIs, as depicted in Table 1 on the left. Then, after summarizing all the formats, we define a comprehensive and self-consistent dynamic API conversion interface as depicted in Table 1 on the right. This Dynamic API Conversion Interface serves as a bridge connecting different APIs, providing communication interfaces for many different formats of APIs. By dynamically adjusting these interfaces, the goal of freely combining different PCGs can be achieved.\nStructured Encapsulation. Since LLM cannot directly utilize PCG through Blender, PCG needs to be encapsulated into action functions for LLM execution. However, encapsulating PCG into action functions poses significant technical barriers for beginners, particularly in terms of both coding knowledge and 3D modeling expertise. To enable beginners to quickly and easily create their own action function, we propose a method of structured encapsulation. For each PCG, the encapsulation, denoted as Encapsulation $S$, follows a similar structure defined as $S_i(C_i, D_i, I_i, L_i, R_i)$, where $i$ distinguishes the i-th action function. The components of the protocol are defined as follows:\n\u2022 Classname The name of the action function, used for indexing the action function.\n\u2022 Description A detailed objective description of an action function.\n\u2022 Input A detailed objective description of the input to the action function.\n\u2022 Limitation An objective description of the functional constraints of the action function."}, {"title": "Multi-Agent Framework", "content": "Due to the hierarchical nature of controllable elements in Blender, such as the blueprint for layout planning, the geometry node for asset scatter, and numerous parameters for asset placement, color, size, and height, this renders a naive LLM framework inadequate for handling Blender's complex and diverse action. To effectively utilize actions of different hierarchies for generating high-quality large-scale city scenes, we propose a multi-agent framework with visual feedback as depicted in Fig. 2. This framework mainly consists of four agents: annotator, planner, executor, and evaluator. The annotator labels all actions with multiple tags and stores the annotated action information in the common message pool. The planner formulates the overall task pipeline using user-provided textual information and determines the action required for different sub-tasks. The executor manages all action functions and uses the annotated action functions to process sub-tasks of procedural generation or asset manipulation in Blender. The evaluator assesses the correctness of the current sub-task by obtaining scene information from Blender, such as object location and rendered images.\nMultimodal Input through Multi-Agent Framework. Unlike simple LLM frameworks, our Multi-Agent Framework can find feasible solutions through multiple rounds of interaction and visual feedback, meaning that with only a few plugins, the target task can be accomplished, avoiding the burden to reinvent the wheel. This also simplifies city generation through multimodal inputs. For example, when we import a semantic map into Blender, it is stored as a point cloud containing"}, {"title": "Annotator Agent", "content": "For large-scale city generation tasks, a substantial number of action functions are required. Therefore, effectively managing these action functions is crucial. To ensure each agent can efficiently access all action of different hierarchies, we use the Annotator to label them.\nThe Annotator labels action functions in two steps: first, summarizing existing functions into consistent concepts guided by prompts; second, labeling each function based on these concepts. An action function may receive multiple labels. Once all action functions are processed, the labeled information is stored in the common message pool, enabling other agents to directly access it."}, {"title": "Planner Agent", "content": "We consider PCG-based city scene generation as an open-loop planning task with flexible steps. Specifically: (i) Termination of the city scene generation task depends on meeting the user's requirements; (ii) City scene generation tasks are a series of sequentially arranged action functions, where the order of these actions significantly impacts the final outcome. To address these challenges, we propose a dynamic planner. At the beginning of the task, the planner formulates a rough workflow as a reference. During the execution of specific sub-tasks, the planner plans the next action based on the current sub-task goals and the workflow, until the user's requirements are met.\nWhen the planner directly receives user input, it needs to translate the user's intent into a series of referable executable actions with additional explanations, which will be stored in the common message pool. To achieve this, we prompt the planner to produce a preliminary action plan, serving as the workflow. Specifically, we utilize the labeled information $L$ from the common message pool, the user input $I$, and the planner's guidance document $D$ as inputs, allowing the planner to generate an ordered workflow $W$. This process is formalized as follows:\n$W \\leftarrow \\text{Planner}(L, I, D)$                                                                     (1)\nDuring the execution of the t-th sub-task, the planner needs to formulate actions required for the (t+1)-th sub-task. To ensure the accuracy and coherence of actions, the planner refers to the workflow. Specifically, based on the ordered workflow $W$, sub-task input $I$, the labeled information $L$ from the common message pool, and the planner's guidance document $D$, the planner infers the next action $A_{t+1}$, where $A_{t+1}$ stands for the action of the (t + 1)-th sub-task.\n$A_{t+1} \\leftarrow \\text{Planner}(L, I, D, W).$                                                                           (2)"}, {"title": "Executor Agent", "content": "To achieve Interactive Workflow in Blender, we deploy the Executor within the Blender environment. All agents send action execution commands to the Executor through a local backend server. As mentioned in Section 3.1, we transform PCG plug-ins into executable action functions using structured encapsulation. This allows the Executor to initialize all actions flexibly. To enable agents to precisely control actions, each action function is structurally recorded in JSON format. For example, the scale_object action function is documented as follows:\nDuring the execution of the t-th sub-task, the Executor uses the action document D and the sub-task input I to generate the action arguments Arguments. The action At is then executed in Blender based on the arguments. Here, $A_{t+1}$ represents the action for the (t + 1)-th sub-task, $S_{t+1}$ represents the Blender state for the (t + 1)-th sub-task, and $S_t$ represents the Blender state for the t-th sub-task.\nArguments - Executor(I, D), $S_{t+1} \\leftarrow \\text{Blender}(S_t, A_t, \\text{Arguments})$                                             (3)"}, {"title": "Evaluator Agent", "content": "To address the limitations of textual feedback in urban scene generation tasks, we designed an Evaluator with visual feedback based on GPT-4V[43]. Specifically, we first render the generated scene as an image. Then, we provide both the image and the current sub-task text input to the Evaluator, guiding it with prompts to assess whether the scene's geometry and materials match the sub-task expectations. If the Evaluator determines that the rendered image is consistent with the sub-task text input in terms of geometry and materials, the evaluation ends. However, if the Evaluator identifies errors, it can pass this information to the Planner for improvements. This process is formalized as follows:\n$R \\leftarrow \\text{Evaluator}(GPT-4V(img, I, D)),$                                                                         (4)"}, {"title": "Experiments", "content": "The goals of our experiments are threefold: (i) to verify the capability of CityX for generating highly realistic large-scale city with different modes of input; (ii) to prove that the Muti-Agent framework and pcg protocol we designed are effective; (iii) to compare different LLMs on the proposed benchmark."}, {"title": "Benchmark Protocol", "content": "Dataset. To evaluate the effectiveness of the proposed CityX, we collect 50 city Semantic Maps, 50 city Height Fields, and 50 city OSM files in XML format. We also collect 50 sets of descriptions about city styles and weather. Then, we feed them to our CityX to generate corresponding city models, which are used to perform quantitative and qualitative comparisons.\nModels. When generating and editing the 3D scenes, we adopt the leading GPT-4 as the large language model with its public API keys. To ensure stable output from the LLM, we set both the decoding temperature and the seed to 0.\nMetrics: We use Executability Rate (ER@1) and Success Rate (SR@1) to evaluate the capabilities of LLMs on our CityX. The former measures the proportion of proposed actions that can be executed, and the latter is used to evaluate action correctness [44]. Additionally, we use a unified evaluation standard as a reference. We categorize the aesthetics of city scenes into five levels: Poor (1 points)/Below Average (2 points)/Average (3 points)/Good (4 points)/Excellent (5 points)."}, {"title": "Main Result", "content": "Urban Scene Generation with Multimodal Inputs. We first show the ability of CityX to generate large-scale urban scenes, as depicted in Figure 4. The results show that CityX is capable of generating highly realistic urban scenes using multimodal data inputs, including OSM data, semantic maps, and satellite images, demonstrating its effectiveness and flexibility in urban scene generation.\nWe also compare our method with other city generation approaches, as shown in Figure 3. The results indicate that PersistantNature[34] and InfiniCity[6] have severe deformation issues throughout the entire scene. While SceneDreamer[35] and CityDreamer[8] demonstrate improved structural consistency, their building quality remains relatively low. While SceneX[9] achieves high quality, it encounters issues with overlapping assets and a high duplication rate of buildings. In contrast, the city generated by CityX demonstrates a regular geometric structure and high quality, which is devoid of overlapping buildings and exhibits minimal repetition.\nAesthetic Evaluation. To better assess the quality of cities generated by CityX, we collect results from various related works on urban generation and invite 30 volunteers and 5 experts in 3D modeling to evaluate these works aesthetically. To ensure fairness, we anonymize all results. As shown in Table 2, the cities generated by CityX attain a \"Good\" level in aesthetic scoring, a distinction not achieved by other works, demonstrating its highly realistic capabilities in city generation.\nSpecific Refinement Editing. CityX supports specific refinement editing for scene customization, involving asset manipulation, weather adjustment, and style modification. We conduct relevant experiments, as depicted in Figure 5. Based on the results, it's clear that CityX performs well in accurately controlling urban scenes to meet input requirements consistently."}, {"title": "Ablation Study of PCG Protocol.", "content": "Analysis of Different Components. To assess the impact of each component of the structured encapsulation on the overall system, we conduct ablation experiments on individual parts of the structured encapsulation, as shown in Table 3. The table demonstrates that when encapsulating PCG, adding Description, Input, and Limitation all boost ER@1 and SR@1. Notably, including Description leads to the highest increase, with SR@1 rising by 57.00% and ER@1 by 31.63%, significantly enhancing the system's Executability Rate and Success Rate. After adding Input, SR@1 and ER@1 increase by up to 21.34% and 25.00%, respectively. Similarly, with the inclusion of Limitation, SR@1 and ER@1 see maximum increases of 22.11% and 25.00%. This suggests that incorporating Input and Limitation can improve the system's Executability Rate and Success Rate.\nComparing Agent Frameworks with Different LLMs To evaluate the effects of different large language model variants on multi-agent frameworks, we assessed the system's ER@1 and SR@1 using various LLM versions. As not all open-source models have visual perception capabilities, we used GPT-4-Vision-Preview uniformly for visual feedback. To maintain experimental stability, the temperature and seed for all LLMs were set to 0. The experiment utilized 50 description sets from Section 4.1 dataset. Results are shown in Table 4. GPT-4 achieves the highest scores in both ER@1 and SR@1, with scores of 91.00% and 89.01%, respectively. Mistral ranks second in ER@1, with a score of 78.00%, while GPT-3.5-turbo and Gemma-7B rank second and third in SR@1, with scores of 75.00% and 69.23%, respectively. It is evident that Mistral and Gemma-7B, two open-source large language models, perform comparably to GPT-3.5-turbo but still fall short of GPT-4's performance."}, {"title": "Conclusions", "content": "In this paper, we propose a novel multi-modal controllable procedural content generation method CityX to generate realistic, unbounded 3D cities. The proposed method supports multi-modal guided conditions, such as OSM, semantic maps, and satellite images. The proposed method includes a general protocol for integrating various PCG plugins and a multi-agent framework for transforming instructions into executable Blender actions. Through this effective framework, CityX shows potential for building an innovative ecosystem in 3D scene generation by bridging the gap between generated assets and industrial requirements. Extensive experiments have demonstrated the effectiveness of our method in creating high-quality, diverse, and unbounded cities guided by multi-modal conditions.\nSocietal Impacts. CityX generates high-quality urban through the use of the PCG Management Protocol and Multi-Agent Framework, closing the gap between industrial application needs and the quality of generative models. This approach is significant for building a new ecosystem based on procedural content generation and benefits the entire PCG community."}, {"title": "Limitations", "content": "CityX has two drawbacks for future improvement. Firstly, we lack an efficient method to accelerate parameter extraction in PCG, which requires significant manpower and resources. Secondly, scene generation based on PCG methods is constrained by the inherent rules of PCG itself, limiting the diversity of PCG generation techniques."}]}