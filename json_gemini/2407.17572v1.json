{"title": "CityX: Controllable Procedural Content Generation for Unbounded 3D Cities", "authors": ["Shougao Zhang", "Mengqi Zhou", "Yuxi Wang", "Chuanchen Luo", "Rongyu Wang", "Yiwei Li", "Xucheng Yin", "Zhaoxiang Zhang", "Junran Peng"], "abstract": "Generating a realistic, large-scale 3D virtual city remains a complex challenge due\nto the involvement of numerous 3D assets, various city styles, and strict layout\nconstraints. Existing approaches provide promising attempts at procedural content\ngeneration to create large-scale scenes using Blender agents. However, they face\ncrucial issues such as difficulties in scaling up generation capability and achieving\nfine-grained control at the semantic layout level. To address these problems, we\npropose a novel multi-modal controllable procedural content generation method,\nnamed CityX which enhances realistic, unbounded 3D city generation guided by\nmultiple layout conditions, including OSM, semantic maps, and satellite images.\nSpecifically, the proposed method contains a general protocol for integrating vari-\nous PCG plugins and a multi-agent framework for transforming instructions into\nexecutable Blender actions. Through this effective framework, CityXhows the\npotential to build an innovative ecosystem for 3D scene generation by bridging the\ngap between the quality of generated assets and industrial requirements. Exten-\nsive experiments have demonstrated the effectiveness of our method in creating\nhigh-quality, diverse, and unbounded cities guided by multi-modal conditions. Our\nproject page: https://cityx-lab.github.io/.", "sections": [{"title": "Introduction", "content": "Recently, 3D generative models have witnessed remarkable achievements in object generation, scene\ncreation, and human avatars, which are crucial technologies for game development, virtual reality,\nanimation, and film production. For example, DreamFusion [1] and Magic3D [2] produce 3D objects\nguided based on text instructions, while Zero123 [3], Wonder3D [4] and SV3D [5] transform 2D\nimages into high-quality 3D assets. Different from these object-centric 3D generation works, we\nfocus on a challenging unbounded city-scale scene generation, which involves numerous 3D assets\n(buildings, roads, vegetation, rivers, etc.), various city styles (modern style, traditional style), and\nstrict layout constraint. Although existing pioneers [6, 7, 8, 9] have attempted to generate larger\ncity-scale scenes, a significant gap remains between the quality of the generated content and the\nstandards required for industrial applications."}, {"title": "Related Works", "content": "Generating a 3D urban scene is a complex task involving multiple modules, such as accurately\ngenerating a reasonable layout and then constructing appropriate instances on the layout. Additionally,\napplying Multi-Agent systems to complex tasks like 3D urban scene generation presents significant\nchallenges. In this section, we will discuss works related to these aspects.\nAgent Systems Based on LLMs. When researching agent systems based on Large Language Models\n(LLMs), the focus lies on effectively integrating and applying these models to execute complex\ntasks. Existing relevant work encompasses various aspects, including task management, role-playing,\ndialogue patterns, and tool integration. For example, [17] utilizes the expansive domain knowledge\nof LLMs on the internet and their emerging zero-shot planning capabilities to execute intricate\ntask planning and reasoning. [18] investigates the application of LLMs in scenarios involving\nmulti-agent coordination, covering a range of diverse task objectives. [19] presents a modular\nframework that employs structured dialogue through prompts among multiple large pretrained models.\nMoreover, specialized LLMs for particular applications have been explored, such as HuggingGPT\n[16] for vision perception tasks, VisualChatGPT [20] for multi-modality understanding, Voyager\n[21] and [22], SheetCopilot [23] for office software, and Codex [24] for Python code generation.\nFurthermore, AutoGPT[25] demonstrates the ability to autonomously complete tasks by enhancing\nAI models, but it is a single-agent system and does not support multi-agent collaboration. In contrast,\nBabyAGI[26] uses multiple agents to manage and complete tasks, with each agent responsible for\ndifferent task modules such as creating new tasks, prioritizing the task list, and completing tasks.\nMulti-agent debate research includes works[27][28] indicating that debates among multiple LLM\ninstances can improve reasoning and factuality, but these methods typically lack the flexibility of tool\nand human involvement. AutoGen[29], as a general infrastructure, supports dynamic dialogue modes\nand a broader range of applications, demonstrating potential in advancing this field.\n3D Urban Scene Generation. Scene-level content generation presents a challenging task, unlike\nthe impressive 2D generative models primarily targeting single categories or common objects, due\nto the high diversity of scenes. Semantic image synthesis, as exemplified by [30, 31, 32, 33], has\nshown promising results in generating scene-level content in the wild by conditioning on pixel-wise\ndense correspondence, such as semantic segmentation maps or depth maps. Recent works such as\n[34, 35, 6] have realized infinite-scale 3D consistent scenes through unbounded layout extrapolation.\nAdditionally, in-depth research has been conducted on using procedural content generation (PCG)\ntechniques to generate natural scenes [36, 37] and urban scenes [38, 39, 40, 41]. For example, PMC\n[42] proposed a procedural method based on 2D ocean or city boundaries to generate cities. It\nemploys mathematical algorithms to generate blocks and streets and utilizes subsequent techniques\nto generate the geometric shapes of buildings. While traditional computer graphics methods can\ngenerate high-quality 3D data, all parameters must be predefined during the procedural generation\nprocess. Since the generated 3D data is subject to rule limitations and exhibits a certain degree of\ndeviation from the real world, this significantly constrains its flexibility and practical utility."}, {"title": "Methods", "content": "Our proposed CityX can generate highly realistic large-scale urban scenes based on multi-modal\ninput. It consists of two key components: a PCG management protocol and a multi-agent framework.\nThe PCG management protocol provides a universal standard to regulate various PCG plugins,\nenabling CityX to easily scale up by integrating all kinds of irregular and non-uniform plugins. At\nthe same time, the proposed multi-agent framework effectively manages the complex, multi-round\ninteractions between large language models (LLMs) and Blender. Through these components, CityX\ncan produce high-quality 3D urban scenes based on multimodal input guidance, including OSM data,\nsemantic maps, satellite images, and textual descriptions.\nTo facilitate flexible, efficient, and straightforward usage of PCG, we propose a universal protocol\nfor managing PCG plugins, serving as a bridge connecting the LLM and Blender. The protocol\nprimarily comprises three parts: (i) a dynamic API conversion interface, enabling effective and flexible\nintegration of different action functions; (ii) structured encapsulation, accelerating the encapsulation\nprocess for beginners and reduces the barriers to using PCG; (iii) infinite asset library and asset\nretrieval, facilitating unlimited expansion of assets to meet the diverse needs of scene generation.\nDynamic API Conversion Interface. The lack of a unified communication protocol among PCG\nAPIs not only hinders the free combination of different PCGs, thus reducing the diversity of generated\ncontent, but also requires manual adjustment of the PCG workflow, which is not user-friendly for\n3D modeling beginners. To address this issue, we provide a Dynamic API Conversion Interface.\nSpecifically, we first compile all the input and output formats of the APIs, as depicted in Table 1\non the left. Then, after summarizing all the formats, we define a comprehensive and self-consistent\ndynamic API conversion interface as depicted in Table 1 on the right. This Dynamic API Conversion\nInterface serves as a bridge connecting different APIs, providing communication interfaces for many\ndifferent formats of APIs. By dynamically adjusting these interfaces, the goal of freely combining\ndifferent PCGs can be achieved.\nStructured Encapsulation. Since LLM cannot directly utilize PCG through Blender, PCG needs\nto be encapsulated into action functions for LLM execution. However, encapsulating PCG into\naction functions poses significant technical barriers for beginners, particularly in terms of both coding\nknowledge and 3D modeling expertise. To enable beginners to quickly and easily create their own\naction function, we propose a method of structured encapsulation. For each PCG, the encapsulation,\ndenoted as Encapsulation S, follows a similar structure defined as $S_i(C_i, D_i, I_i, L_i, R_i)$, where i\ndistinguishes the i-th action function. The components of the protocol are defined as follows:"}, {"title": "Multi-Agent Framework", "content": "Due to the hierarchical nature of controllable elements in Blender, such as the blueprint for layout\nplanning, the geometry node for asset scatter, and numerous parameters for asset placement, color,\nsize, and height, this renders a naive LLM framework inadequate for handling Blender's complex\nand diverse action. To effectively utilize actions of different hierarchies for generating high-quality\nlarge-scale city scenes, we propose a multi-agent framework with visual feedback as depicted in\nFig. 2. This framework mainly consists of four agents: annotator, planner, executor, and evaluator.\nThe annotator labels all actions with multiple tags and stores the annotated action information in the\ncommon message pool. The planner formulates the overall task pipeline using user-provided textual\ninformation and determines the action required for different sub-tasks. The executor manages all\naction functions and uses the annotated action functions to process sub-tasks of procedural generation\nor asset manipulation in Blender. The evaluator assesses the correctness of the current sub-task by\nobtaining scene information from Blender, such as object location and rendered images.\nMultimodal Input through Multi-Agent Framework. Unlike simple LLM frameworks, our Multi-\nAgent Framework can find feasible solutions through multiple rounds of interaction and visual\nfeedback, meaning that with only a few plugins, the target task can be accomplished, avoiding the\nburden to reinvent the wheel. This also simplifies city generation through multimodal inputs. For\nexample, when we import a semantic map into Blender, it is stored as a point cloud containing"}, {"title": "Annotator Agent", "content": "For large-scale city generation tasks, a substantial number of action functions are\nrequired. Therefore, effectively managing these action functions is crucial. To ensure each agent can\nefficiently access all action of different hierarchies, we use the Annotator to label them.\nThe Annotator labels action functions in two steps: first, summarizing existing functions into\nconsistent concepts guided by prompts; second, labeling each function based on these concepts. An\naction function may receive multiple labels. Once all action functions are processed, the labeled\ninformation is stored in the common message pool, enabling other agents to directly access it."}, {"title": "Planner Agent", "content": "We consider PCG-based city scene generation as an open-loop planning task with\nflexible steps. Specifically: (i) Termination of the city scene generation task depends on meeting\nthe user's requirements; (ii) City scene generation tasks are a series of sequentially arranged action\nfunctions, where the order of these actions significantly impacts the final outcome. To address these\nchallenges, we propose a dynamic planner. At the beginning of the task, the planner formulates a\nrough workflow as a reference. During the execution of specific sub-tasks, the planner plans the next\naction based on the current sub-task goals and the workflow, until the user's requirements are met.\nWhen the planner directly receives user input, it needs to translate the user's intent into a series\nof referable executable actions with additional explanations, which will be stored in the common\nmessage pool. To achieve this, we prompt the planner to produce a preliminary action plan, serving\nas the workflow. Specifically, we utilize the labeled information L from the common message pool,\nthe user input I, and the planner's guidance document D as inputs, allowing the planner to generate\nan ordered workflow W. This process is formalized as follows:\n$W \\leftarrow \\text{Planner}(L, I, D)$\nDuring the execution of the t-th sub-task, the planner needs to formulate actions required for the\n(t+1)-th sub-task. To ensure the accuracy and coherence of actions, the planner refers to the\nworkflow. Specifically, based on the ordered workflow W, sub-task input I, the labeled information\nL from the common message pool, and the planner's guidance document D, the planner infers the\nnext action $A_{t+1}$, where $A_{t+1}$ stands for the action of the (t + 1)-th sub-task.\n$A_{t+1} \\leftarrow \\text{Planner}(L, I, D, W)$"}, {"title": "Executor Agent", "content": "To achieve Interactive Workflow in Blender, we deploy the Executor within the\nBlender environment. All agents send action execution commands to the Executor through a local\nbackend server. As mentioned in Section 3.1, we transform PCG plug-ins into executable action\nfunctions using structured encapsulation. This allows the Executor to initialize all actions flexibly.\nTo enable agents to precisely control actions, each action function is structurally recorded in JSON\nformat. For example, the scale_object action function is documented as follows:\nDuring the execution of the t-th sub-task, the Executor uses the action document D and the sub-task\ninput I to generate the action arguments Arguments. The action $A_t$ is then executed in Blender\nbased on the arguments. Here, $A_{t+1}$ represents the action for the (t + 1)-th sub-task, $S_{t+1}$ represents\nthe Blender state for the (t + 1)-th sub-task, and $S_t$ represents the Blender state for the t-th sub-task.\nArguments - Executor(I, D), $S_{t+1} \\leftarrow$ Blender($S_t$, $A_t$, Arguments)"}, {"title": "Evaluator Agent", "content": "To address the limitations of textual feedback in urban scene generation tasks,\nwe designed an Evaluator with visual feedback based on GPT-4V[43]. Specifically, we first render\nthe generated scene as an image. Then, we provide both the image and the current sub-task text\ninput to the Evaluator, guiding it with prompts to assess whether the scene's geometry and materials\nmatch the sub-task expectations. If the Evaluator determines that the rendered image is consistent\nwith the sub-task text input in terms of geometry and materials, the evaluation ends. However, if the\nEvaluator identifies errors, it can pass this information to the Planner for improvements. This process\nis formalized as follows:\nR$\\leftarrow$ Evaluator(GPT-4V(img, I, D)),"}, {"title": "Experiments", "content": "The goals of our experiments are threefold: (i) to verify the capability of CityX for generating highly\nrealistic large-scale city with different modes of input; (ii) to prove that the Muti-Agent framework and\npcg protocol we designed are effective; (iii) to compare different LLMs on the proposed benchmark."}, {"title": "Benchmark Protocol", "content": "To evaluate the effectiveness of the proposed CityX, we collect 50 city Semantic Maps, 50\ncity Height Fields, and 50 city OSM files in XML format. We also collect 50 sets of descriptions\nabout city styles and weather. Then, we feed them to our CityX to generate corresponding city\nmodels, which are used to perform quantitative and qualitative comparisons.\nWhen generating and editing the 3D scenes, we adopt the leading GPT-4 as the large\nlanguage model with its public API keys. To ensure stable output from the LLM, we set both the\ndecoding temperature and the seed to 0."}, {"title": "Main Result", "content": "We first show the ability of CityX to generate\nlarge-scale urban scenes, as depicted in Figure 4. The results show that CityX is capable of generating\nhighly realistic urban scenes using multimodal data inputs, including OSM data, semantic maps, and\nsatellite images, demonstrating its effectiveness and flexibility in urban scene generation.\nWe also compare our method with other city generation approaches, as shown in Figure 3. The\nresults indicate that PersistantNature[34] and InfiniCity[6] have severe deformation issues throughout\nthe entire scene. While SceneDreamer[35] and CityDreamer[8] demonstrate improved structural\nconsistency, their building quality remains relatively low. While SceneX[9] achieves high quality, it\nencounters issues with overlapping assets and a high duplication rate of buildings. In contrast, the\ncity generated by CityX demonstrates a regular geometric structure and high quality, which is devoid\nof overlapping buildings and exhibits minimal repetition.\nTo better assess the quality of cities generated by CityX, we collect results\nfrom various related works on urban generation and invite 30 volunteers and 5 experts in 3D modeling\nto evaluate these works aesthetically. To ensure fairness, we anonymize all results. As shown in Table\n2, the cities generated by CityX attain a \"Good\" level in aesthetic scoring, a distinction not achieved\nby other works, demonstrating its highly realistic capabilities in city generation."}, {"title": "Ablation Study of PCG Protocol.", "content": "To assess the impact of each component of the structured\nencapsulation on the overall system, we conduct ablation experiments on individual parts of the\nstructured encapsulation, as shown in Table 3. The table demonstrates that when encapsulating PCG,\nadding Description, Input, and Limitation all boost ER@1 and SR@1. Notably, including Description\nleads to the highest increase, with SR@1 rising by 57.00% and ER@1 by 31.63%, significantly\nenhancing the system's Executability Rate and Success Rate. After adding Input, SR@1 and ER@1\nincrease by up to 21.34% and 25.00%, respectively. Similarly, with the inclusion of Limitation,\nSR@1 and ER@1 see maximum increases of 22.11% and 25.00%. This suggests that incorporating\nInput and Limitation can improve the system's Executability Rate and Success Rate."}, {"title": "Conclusions", "content": "In this paper, we propose a novel multi-modal controllable procedural content generation method\nCityX to generate realistic, unbounded 3D cities. The proposed method supports multi-modal guided\nconditions, such as OSM, semantic maps, and satellite images. The proposed method includes a\ngeneral protocol for integrating various PCG plugins and a multi-agent framework for transforming\ninstructions into executable Blender actions. Through this effective framework, CityX shows potential\nfor building an innovative ecosystem in 3D scene generation by bridging the gap between generated\nassets and industrial requirements. Extensive experiments have demonstrated the effectiveness of our\nmethod in creating high-quality, diverse, and unbounded cities guided by multi-modal conditions.\nSocietal Impacts. CityX generates high-quality urban through the use of the PCG Management\nProtocol and Multi-Agent Framework, closing the gap between industrial application needs and the\nquality of generative models. This approach is significant for building a new ecosystem based on\nprocedural content generation and benefits the entire PCG community."}, {"title": "Limitations", "content": "CityX has two drawbacks for future improvement. Firstly, we lack an efficient method\nto accelerate parameter extraction in PCG, which requires significant manpower and resources.\nSecondly, scene generation based on PCG methods is constrained by the inherent rules of PCG itself,\nlimiting the diversity of PCG generation techniques."}]}