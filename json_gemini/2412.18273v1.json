{"title": "Sampling Bag of Views for Open-Vocabulary Object Detection", "authors": ["Hojun Choi", "Junsuk Choe", "Hyunjung Shim"], "abstract": "Existing open-vocabulary object detection (OVD) develops methods for testing unseen categories by aligning object region embeddings with corresponding VLM features. A recent study leverages the idea that VLMs implicitly learn compositional structures of semantic concepts within the image. Instead of using an individual region embedding, it utilizes a bag of region embeddings as a new representation to incorporate compositional structures into the OVD task. However, this approach often fails to capture the contextual concepts of each region, leading to noisy compositional structures. This results in only marginal performance improvements and reduced efficiency. To address this, we propose a novel concept-based alignment method that samples a more powerful and efficient compositional structure. Our approach groups contextually related \"concepts\" into a bag and adjusts the scale of concepts within the bag for more effective embedding alignment. Combined with Faster R-CNN, our method achieves improvements of 2.6 box AP50 and 0.5 mask AP over prior work on novel categories in the open-vocabulary COCO and LVIS benchmarks. Furthermore, our method reduces CLIP computation in FLOPs by 80.3% compared to previous research, significantly enhancing efficiency. Experimental results demonstrate that the proposed method outperforms previous state-of-the-art models on the OVD datasets.", "sections": [{"title": "1. Introduction", "content": "Open-vocabulary object detection (OVD) methods leverage pre-trained vision-language models (VLMs) [2, 11, 20] to recognize classes that were not seen during training. VLMs are trained on large-scale datasets with paired image and text data, making them beneficial for understanding classes that detectors have not encountered during training.\nRecently, BARON [25] proposed a bag-of-regions embedding to better exploit the compositional structures among diverse semantic concepts (e.g. contextual co-occurrence between person and airplane) that VLMs are known to learn implicitly [3]. Specifically, BARON: 1) groups contextually related neighboring regions for each region proposal extracted from Faster R-CNN [22] and construct a bag; 2) projects regions within the bag into the word embedding space using Faster R-CNN and a linear layer, resulting in pseudo-words; 3) passes pseudo-words through the text encoder to obtain a bag-of-regions embedding; and 4) aligns this bag-of-regions embedding with the VLM's image encoding during training (Fig. 1-a). This approach enables BARON to exploit the complex semantic structures inherent in VLMs, achieving significant performance gains.\nHowever, BARON constructs a bag by grouping neigh-"}, {"title": "2. Related Work", "content": "Open-vocabulary object detection (OVD) aims to detect objects from novel categories unseen during training. Many prior studies leverage large vision-language models (VLMs) [11, 20], trained on extensive image-text pairs, to perform zero-shot recognition. The key to improving performance for unseen classes lies in effectively utilizing the alignment between images and text inherent in VLMs.\nTo achieve this, methods such as [5, 26, 27] employ prompt modeling to transfer knowledge through learned prompts, enabling more precise contextual descriptions of each class category and thus enhancing performance. Several studies use weakly supervised techniques, such as visual grounding data [13], image captions [7, 29, 33], and image labels [8, 34]. Other approaches [10, 17, 28] reinforce the text modality using Large Language Models (LLMs). In contrast, InstaGen [6] focuses on the image modality, enhancing novel class prediction by using synthetic images generated by an image generation model. Grounding DINO [18] is also noteworthy, enabling prompt-based object detection by exchanging information between VLMs and detection transformers [1] through cross-modality fusion.\nWhile many methods have been proposed, distillation-based methods [8, 23, 31\u201333] currently represent one of the highest-performing techniques, aligning individual region embeddings of an object detector with the features extracted from VLMs to leverage VLM knowledge effectively. Recently, BARON [25] has advanced beyond using individual regions by grouping neighboring regions around region proposals and aligning them with VLM features. This approach captures the compositional structure of concepts inherent in VLMs and has achieved significant performance gains.\nHowever, we argue that some of BARON's design choices are suboptimal. Specifically, BARON does not consider semantics when sampling neighboring regions, often resulting in the sampling of unnecessary regions. Consequently, we believe BARON suffers from low computational efficiency and has room for further performance improvement. Our proposed method, in contrast, samples regions with semantic consideration, reducing unnecessary sampling, thereby achieving high computational efficiency and enhanced performance simultaneously."}, {"title": "3. Preliminaries", "content": "In this paper, we instantiate the idea of BARON [25] to capture compositional structures. BARON samples the nearest neighbors around each region proposal within an image and groups them into a bag. This enables VLMs to infer the compositional structure of semantic concepts. VLMs implicitly learn the compositional structure of complex scenes from large-scale image-text pairs, aligning each concept's pixel embedding with its corresponding text representation. MaskCLIP [3] demonstrates this by showing that VLMs [2, 11, 20] effectively capture representations of concepts in complex scenes not explicitly learned.\nLimitations of Bag of Regions. However, we argue that BARON's design has two limitations: 1) BARON constructs a bag by grouping neighboring regions without considering their semantic meaning, reducing its efficiency in sampling important structures within a scene. By relying solely on spatial proximity, BARON often samples nearby backgrounds with no semantic concepts. For example, as shown in Fig. 2-a, BARON includes backgrounds like the sky, which limits its use of contextual co-occurrence among semantic concepts and increases computational costs.\n2) Additionally, since bags are created based on fixed window sizes defined by region proposals, BARON struggles to capture surrounding objects of varying sizes and their relationships. As seen in Fig. 2-b and c, smaller windows may capture only part of a surrounding object, such as a portion of a horse, missing important details, while larger windows may include too irrelevant background around the target object within region proposals.\nTo quantify this phenomenon, we measure the average number of unnecessary neighboring regions using ground truth (GT) boxes with IoU and noise embeddings with cosine similarity on the COCO benchmark [15]. For simplicity, we treat GT boxes as semantic concepts and noise embeddings as irrelevant backgrounds. The noise embeddings are derived from BARON's contextual embeddings, which are learnable by training an additional linear layer on the model's class tokens. This setup allows learning features for non-semantic regions, such as backgrounds. As shown in Fig. 4, the noise embeddings effectively capture the general characteristics of background areas, such as sidewalks.\nIn our analysis, neighboring regions extracted by BARON are deemed unnecessary if they overlap less than 85% IoU with the GT boxes or their embeddings have a cosine similarity score above 0.8 with the noise embeddings. We demonstrate the validity of this setting in Sec. A.1. Con-sequently, we observe that such unnecessary neighboring regions account for an average of 73% of all neighboring regions per image. These unnecessary neighbors lead to marginal performance gains and high computational costs when utilizing VLMs. Our method is motivated by the need for a more effective and efficient sampling strategy."}, {"title": "4. Methodology", "content": "We call our method SBV (Sampling Bag of Views). Our method aligns embeddings from a bag of views, going beyond individual regions or a bag of regions for OVD (Fig. 3). In this work, we design a powerful and efficient sampling strategy to form a bag of concepts that effectively captures semantic concepts around region proposals (Sec. 4.1). This bag is then used to generate three hierarchical views for each concept in the bag. Using these candidate views, the concepts in the bag are mapped to views that best represent optimal scales with masks applied to improve the effectiveness of embedding alignment (Sec. 4.2).\n4.1. Adaptive Sampling Strategy\nIn this section, we introduce an adaptive sampling strategy that creates surrounding windows to capture contextually interrelated semantic concepts around region proposals (Fig. 3-a). Our sampling process is carried out in two steps: 1) identifying surrounding key semantic concepts from RPN boxes (Sec. 4.1.1) and 2) forming a bag of concepts to incorporate these concepts (Sec. 4.1.1).\n4.1.1 Identifying Surrounding Semantic Concepts\nWe focus on extracting semantic concepts around region proposals within an image. We treat RPN boxes, which are widely used in OVD [25, 31] and potentially represent novel objects, as candidate concepts. Since the top-k RPN boxes can be noisy and overly abundant (e.g. k is 300), we design a canvas to sample a few key semantic concepts around region proposals. This process generates edges on the canvas to explore semantic relationships between the proposals, converging towards denser, high-objectness concepts that are more likely to contain relevant objects (Fig. 3-a).\nCanvas construction. We first divide each image into a grid with intervals of \u2206, referred to as the canvas, and generate coordinates for each interval. The canvas encodes the probabilities of RPN boxes in the cardinal directions (e.g. up, down, left, and right) around each coordinate, reflecting the likelihood of relevant concepts. To compute this likelihood, we leverage two key properties of RPN [22]: 1) RPN boxes have objectness scores, and 2) RPN boxes are densely populated in areas with objects. To examine the extent of this population, we check the overlap between RPN boxes and a directional A-sized box in each cardinal direction at each coordinate. We then calculate the average probabil-"}, {"title": "4.1.2 Forming Bag of Concepts", "content": "With the knowledge of visual concepts $v$, we can capture diverse contextual relationships between semantic concepts. To leverage this contextual information, we aim to sample visual concepts that provide optimal context for each region proposal. These sampled concepts are referred to as representative concepts $v^*$, which default to the neighboring regions in BARON for simplicity.\nWe adjust the sampling probability according to distance and aspect ratio, as they contribute to contextual information. We believe that increasing the distance between region proposals and visual concepts captures more semantic concepts. Meanwhile, the aspect ratio preserves the original shape of objects during resizing for CLIP input. We sample the representative concept $v^*$ for each surrounding direction of region proposals in Eq. (1):\n$v^{*}_{2,i}= \\text{argmax}_{v \\in \\nu_{i}} (\\lambda \\cdot ||d_{v,i,j}||_{\\eta} + \\alpha \\cdot ||Y_{v,i,j}||)$ , (1)"}, {"title": "4.2. Aligning Bag of Views", "content": "In this section, we adjust the scale of concepts within a bag to effectively capture surrounding objects of different sizes and their relationships, which BARON cannot achieve due to its fixed window size. To achieve this, we propose a novel representation-switching strategy to explore the optimal scale of concepts within the bag (Sec. 4.2.1). For more effective embedding alignment, we utilize previously extracted noise embeddings to generate noise masks and different viewpoints within a bag to generate view masks. These masks are used to extract the bag-of-view embeddings from the CLIP image encoder (Sec. 4.2.2). We then align these embeddings with their corresponding textual representations (Fig. 3-b).\n4.2.1 Representation Switching\nTo effectively learn embeddings for objects of varying sizes, we introduce a novel technique for switching representa-"}, {"title": "4.2.2 View-wise Attention", "content": "For more effective view embedding alignment, we introduce two key improvements: 1) diminishing the influence of noise (e.g. background), and 2) understanding the importance of each view among the sampled views within the bag, where representation switching occurs.\nFor (1), we design noise masks $N$ to reduce the influence of noisy patches when extracting embeddings from CLIP. To achieve this, we utilize previously extracted noise embeddings $x$ which typically represent general backgrounds. We then apply an attention mask to patches with similarity to the noise embeddings $x$ above $\\tau$, as shown in Eq. (3):\n$N=\\begin{cases}\n-\\infty, & \\text{if } I(V)x > \\tau, \\\\\n0, & \\text{otherwise,}\n\\end{cases}$ (3)\nwhere $I$ is the image encoder and $\\circlearrowleft$ represents CLIP similarity. We validate the optimal threshold search with visualizations at different thresholds in Sec. B.1.\nFor (2), we devise view masks $M$ to enhance the representation based on the importance of each view. Using hyperparameters that define the importance of each view, we"}, {"title": "5. Experiments", "content": "Datasets. We evaluate our method using two well-known OVD datasets: OV-COCO [15] and OV-LVIS [9]. For the OV-COCO dataset, we adopt the category split approach from OVR-CNN [29], dividing the object categories into 48 base categories and 17 novel categories. For the OV-LVIS dataset, we follow ViLD [8], separating the 337 rare categories into novel categories and grouping the remaining common and frequent categories into base categories.\nEvaluation Metrics. We assess detection performance on both base and novel categories. For OV-COCO, we follow OVR-CNN [29] report the box AP at an IoU threshold of 0.5, denoted as AP50. For OV-LVIS, we report the mean Average Precision (mAP) of masks averaged on IoUs from 0.5 to 0.95. The primary metrics for evaluating open-vocabulary detection performance are the AP50 of novel categories (AP) for OV-COCO and the mAP of rare cat-egories (AP) for OV-LVIS.\nImplementation Details. We build our method on Faster R-CNN [22] with ResNet50-FPN [16]. For a fair comparison, we initialize the backbone network with weights pre-trained by SOCO [24] and use synchronized Batch Normalization (SyncBN) [30], as done in recent studies [5, 25]. For the main experiments on OV-COCO and OV-LVIS [9, 15], we choose the 1x and 2\u00d7 schedules. We use the CLIP model [20] based on ViT-B-16 [4] as our pre-trained visual language model (VLM). For category name prompts, we default to the hand-crafted prompts from ViLD [8] in all our experiments on OV-COCO. We use learned prompts following DetPro for experiments on OV-LVIS.\n5.1. Main Results\nOV-COCO. We compare previous state-of-the-art methods on the OV-COCO (Tab. 1-a). While OV-DETR [27], which builds on Deformable DETR [35], outperforms Faster R-CNN [22] on base categories, SBV surpasses OV-DETR by 7.2 AP50 on OV-COCO for novel categories. SBV even outperforms recent studies [5, 7] that use sophisticated pseudo-labeling. Furthermore, SBV outperforms BARON [25] by 2.6 AP50 for novel categories, despite the more extensive use of CLIP resources by BARON. We show that SBV achieves the best performance across all scenarios.\nOV-LVIS. We compare SBV with previous state-of-the-art methods on the OV-LVIS benchmark (Tab. 1-b). Since OV-LVIS has more detailed annotations, it is generally a more challenging benchmark. As a result, the performance observed on OV-LVIS tends to be smaller than OV-COCO. Nevertheless, SBV surpasses OV-DETR [27] by 5.7 on OV-LVIS for novel categories even in different backbones (e.g. Vision Transformer [4]). SBV also performs better than the caption-based models [7, 33] that use additional caption data. SBV also outperforms BARON by 0.5 AP on OV-LVIS. We highlight that SBV achieves superior performance in all situations.\nHowever, with the hyperparameters set for OV-COCO, SBV shows relatively smaller improvements on benchmarks like OV-LVIS, which contain finer annotations. We believe that this setup is optimized for the coarser annotations of OV-COCO. We provide guidance on hyperparameter tuning for our method on the OVD datasets in Sec. B.2."}, {"title": "5.2. Ablation Study", "content": "In this section, we ablate the effectiveness of components in our method on the OV-COCO benchmark.\nEffectiveness of Bag of Views. We evaluate the effectiveness of each component of our method (Tab. 2). First, we evaluate the effect of the noise mask independently. It is applied to each region within a bag, which is obtained by BARON's sampler and used as input for CLIP. We demonstrate that the mask effectively reduces CLIP's computational costs by more than half without sacrificing performance on novel categories. This indicates that the noise mask successfully removes unnecessary background patches from the CLIP's attention mechanism.\n50\nFurthermore, by employing our adaptive sampling, our approach already outperforms the baseline [25] by 2.6 APnovel while significantly reducing computational costs by 79.2%. This demonstrates that our sampler leverages the generalization power of VLMs more effectively than BARON. It is possible by aligning co-occurring concepts around region proposals and discarding unimportant ones.\nLastly, when we apply the view mask along with representation switching, we find that our method further improves performance by 0.6 APovel and increases computational efficiency by reducing 0.6 PFLOPs.\nSampling Distance. We analyze the impact of the sampling distance \u03b7 on the candidate visual concepts v. As shown in Tab. 3, we observe that a larger concept window with a high \u03b7 may capture a more complex scene with excessive objects and noise, leading to performance degradation. Nevertheless, our method ensures exclusive learning of concepts within a bag by trimming overlap with other region proposals, resulting in a performance boost even with \u03b7 set to 0.8.\nNumber of Extra Region Proposals. We study the im-"}, {"title": "6. Conclusion", "content": "This paper explores an improved compositional structure of semantic concepts through a bag of views in OVD. The compositional structure is mainly about the co-occurrence of objects. This leverages large-scale VLMs' ability to represent the compositional structure of concepts within image-text pairs. We develop an adaptive sampling strategy to group contextually related concepts into a bag. Concepts within a bag are then adjusted to views considered optimal scales, with masks applied to enhance embedding alignment. Finally, we adopt a distillation-based approach to align the detector's bag-of-view representations with those of pre-trained VLMs. Our method outperforms previous state-of-the-art methods on the OVD benchmarks while significantly enhancing VLM utilization efficiency."}, {"title": "A. Implementation Details", "content": "A.1. Implementation\nComputing unnecessary neighboring regions. We outline the process for calculating the ratio of unnecessary neighboring regions extracted by BARON [25]. In our analysis, these neighboring regions are considered unnecessary if their overlap with the ground-truth (GT) boxes is less than 85% IoU or if their embeddings have a cosine similarity score above 0.8 compared to noise embeddings.\nOur threshold selection is inspired by the non-maximum suppression (NMS) process in Faster R-CNN [22]. Faster R-CNN uses NMS to filter out overlapping region proposals with lower objectness scores, retaining only a few proposals. Specifically, two RPN boxes are considered overlapping when their IoU is 0.1, allowing the detector to focus on the most likely RPN box within the surrounding area. For consistency, we adhere to BARON's hyperparameter settings when applying Faster R-CNN. This forms the basis for our hyperparameter choices. Specifically, we classify regions with an IoU approximately close to 10.1 = 0.9 \u00b1 0.05 as sufficiently representative of a GT box, while regions with a lower IoU are deemed to ambiguously represent the GT.\nAdditionally, we include a feature-level comparison using cosine similarity between noise embeddings and the regional embeddings of RPN boxes. The learnable noise embeddings are generated by training an additional linear layer on the model's class tokens. Due to the noisy nature of similarity distributions, we only consider background regions with a cosine similarity of approximately 0.8 or higher as unnecessary. Consequently, the hyperparameters for IoU and the similarity threshold are set to 0.85 and 0.8, respectively.\nSampling bag of concepts. In this paper, we introduce concept windows, which effectively capture key concepts near each region proposal. Open vocabulary object detection (OVD) aims to predict novel objects often unseen during training, emphasizing the need to prevent overfitting to training data. For implementation details, we outline two operations within our adaptive sampling strategy designed to minimize overfitting to easily identifiable objects frequently found in region proposals during training.\n1) We design these concept windows to avoid repeatedly including other regional proposals. To achieve this, we trim the concept windows at the boundaries of all region proposals, preventing training bias and reducing the risk of overfitting certain objects while underpredicting others.\n2) Additionally, we incorporate BARON's sampling"}, {"title": "A.2. Pseudocode", "content": "We provide the pseudocode for our method in Algorithm 1. Note that all necessary requirements, such as the noise embeddings x, have been preprocessed as outlined in the pseudocode. Our method, SBV, includes several hyperparameters; however, we find that only four significantly contribute to performance improvement, while the others are fixed for each OVD benchmark. These hyperparameters are highlighted in blue in Tab. 8. Further details on this hyperparameter configuration can be found in Sec. B.2.\nForming Bag of Concepts. SBV first forms a bag of concepts for each region proposal extracted by Faster R-CNN [22] within an image. Here, the region proposals are reduced to Rreduced for more efficient canvas construction, as described in Sec. A.3. To achieve this, we build a novel structure named canvas which aids in effectively searching key concepts between the region proposals. Specifically, the canvas contains coordinates of interval A which direct to areas where RPN boxes with high objectness scores are densely populated. We sample these key concepts from overly abundant and noisy RPN boxes that potentially represent novel objects through edge generation on the canvas. Then, we form a bag of concepts by merging each surrounding window of region proposals with the corresponding key concepts, as shown in Eq. (2).\nAligning Bag of Views. To enhance the concept embeddings, we introduce representation switching, which identifies the best representation, termed the representative view. This view is chosen from three hierarchical views derived from the bag of concepts, based on the number of co-occurring concepts and the view size ratio. The selected view is further refined using two masks: the view mask and the noise mask. The view mask emphasizes the importance of each view, while the noise mask suppresses noisy representations (e.g. backgrounds)."}, {"title": "A.3. Non-overlapping RPN Boxes", "content": "We introduce a simple yet effective technique to accelerate the computation of existence probabilities for numerous top-k RPN boxes around each coordinate on the canvas. The key idea is to selectively reduce the top-k RPN boxes. To ensure uniform probability calculations across all image regions, the RPN boxes need to be sparsely distributed. RPN boxes already fulfill this requirement but may demand significant processing time due to their excessive abundance. To address this, we optionally use greedy sampling to select non-overlapping RPN boxes from the top-k boxes in the image, as shown in Eq. (5):\n$R_{reduced} = \\{r \\in R_{topk} | \\forall r' \\in R, IoU(r, r') > \\tau\\}$ (5)\nwhere T indicates an IoU threshold default to 0.0, IoU denotes Intersection over Union, R represents region proposals, and R refers to the top-k region proposals extracted by Faster R-CNN [22].\nIn this process, we extract RPN boxes that do not overlap with the region proposals and ignore very small RPN boxes for simplicity. As a result, this greedy sampling effectively"}, {"title": "A.4. k-farthest RPN Boxes", "content": "In this paper, we increase the number of region proposals to encourage more active edge generation, facilitating more effective concept sampling. Note that the meaningful concepts are RPN boxes that are densely populated and have high objectness scores. This approach addresses the issue observed in Faster R-CNN [22], where only a few region proposals are generated per image after NMS. Specifically, we often observe cases where fewer than three small-sized region proposals are generated per image. We believe that this problem may be due to the RPN being overfitted to the training data, leading it to focus primarily on objects belonging to well-defined base categories. Note that we follow BARON's hyperparameter settings for the RPN to ensure a fair comparison.\nAs shown in Fig. 8-a and b, the number of region proposals extracted by the RPN (highlighted in green) is often fewer than four in randomly sampled images from the OVD datasets. Furthermore, these proposals can correspond to meaningless objects and are of small window sizes. For instance, in the \u201cmirror\u201d sample, there are no objects belonging to any category.\nTo address this issue, we incorporate additional region proposals by augmenting the default region proposals R with extra RPN boxes from R, resulting in R. Specifically, we sample RPN boxes that are, on average, a distance of n away from R, as described in Eq. (6):\n$R_{added} = \\{\\begin{array}{c}N \\\\\nr \\in R_{added}\\end{array} \\left. \\begin{array}{c}r \\in arg\\underset{r \\in R_{reduced}}{max.} S(r)\\end{array}\\right\\}$  (6)\nwhere S(r) represents a score function as defined in Eq. (7), N denotes the number of additional region proposals, and r include {r1,...,rn}.\n$S(r) = \\lambda \\cdot \\frac{1}{R}\\underset{r' \\in C(r,r')}{ \\sum} \\underset{p \\in  r, p' \\in r'}{min \\frac{||p-p'||}{\\eta}} + (1-\\lambda) \\cdot objectness(r)$ (7)\nwhere objectness(\u00b7) denotes the objectness score of RPN boxes, A represents the weight assigned to the distance, || || indicates the normalization based on the distance threshold \u03b7, and C(\u00b7) returns the center coordinates of the boxes. Note that r' and rare distinct from each other and sampled in a way that avoids duplicate values.\nBy leveraging this increased number of region proposals, key concepts distributed across the image can be identified from various directions through edges. Consequently,"}, {"title": "A.5. Calculating FLOPs for VLMs", "content": "In this paper, SBV utilizes the CLIP model [20] based on ViT-B-16 [4]. We use Floating Point Operations (FLOPs) to measure the computational load of such VLMs. FLOPS is a widely used metric in machine learning to quantify the computing power of a computer or processor. The operations in the CLIP model are divided into three modules: convolutional neural network (CNN) layer, self-attention, and multilayer perceptron (MLP). We calculate and sum the FLOPs of each component to obtain the total load for a single CLIP inference. The areas where SBV results in actual FLOPs changes are in the self-attention and MLP components, where our noise masks N function as attention masks. As a result, we demonstrate that SBV achieves 80.3% FLOPs reduction compared to BARON.\nCNN layer. To calculate the FLOPs for a convolutional layer, we first define key parameters: the filter (kernel) size KXK, the input and output channels Cin and Cout, and the input size Hin \u00d7 Win. The height and width of the output feature map are determined by the stride, with output dimensions Hout = Hin/Sh and Wout = Win/Sw, where Sh and Sw are the stride lengths for height and width, respectively. Each convolution operation at a single position requires K \u00d7 K \u00d7 Cin \u00d7 Cout multiplications and an equal number of additions. Across the entire output feature map, the total FLOPs can be calculated by multiplying the FLOPs per instance by the total number of output positions Hout Wout. Thus, the total FLOPs for the convolutional layer is in Eq. (8):\n$FLOPS_{CNN} = (K^2 C_{in}C_{out}) \\times (H_{out}W_{out})$ (8)\nSelf-attention layer. In calculating the FLOPs for the self-attention layer, we define the following key parameters: the attention window size W, embedding dimension D, and number of attention heads H. This is where our noise mask, N, comes into play. We denote N as the attention mask used in self-attention, where it specifically represents the unmasked positions within the window W. This parameter adjusts the computation according to the number of unmasked positions each token can attend to. For a batch of size B, and summing across all H heads, the total FLOPS for self-attention with the attention mask can be expressed in Eq. (9):\n$FLOP_{SA} = 2BHD \\times WN$. (9)\nMLP layer. To compute the FLOPs for the MLP layer, we consider two main fully connected sub-layers: the fully connected layers and the projection layer. For each fully con-"}, {"title": "B. Further Analysis", "content": "B.1. Noise Masks\nWe find that the noise embeddings N represent redundant areas (e.g. backgrounds). These embeddings are extracted by preemptively training the baseline model with an additional learnable linear layer added to the model's class tokens [25]. This allows for proactively learning features for non-category regions. Such design is demonstrated by OVD, which leverages VLMs to indirectly enable the model to learn features for novel objects during training. This enables the noise embeddings to learn to detect regions outside the features of novel objects. Then, we compute the pixelwise similarities between the noise embeddings and each region embedding during training. We remove redundant patches from CLIP attention that exceed a certain similarity threshold to obtain the noise mask. We find higher similarity scores indicate noisier regions, converging towards background areas.\nHowever, potentially novel objects may still exist in these noisy regions, as the noise embeddings are trained on the training set. This suggests the need to model a certain amount of noise together. In Fig. 7, we observe that the similarity distribution between the regional embeddings and the noise embeddings varies across all region crops. Note that the mean and variance of the distribution differ. A larger mean indicates that the scene contains mostly noisy representations, while a smaller mean suggests an abundance of primary objects. This implies that fixed similarity thresholds fail to capture both primary objects and appropriate noise within each crop.\nTo address this issue, we study threshold ablation for each region crop. Specifically, we explore optimal thresholds using combinations of mean and variance from the similarity distribution. The combinations are as follows: (\u03bc + 2\u03c3), (\u03bc + 4\u03c3), and (\u03bc + 8\u03c3); where \u03bc and are the mean and variance of the similarity distribution. Note that \u03bc ensures minimal noise for each crop, while adjusting \u03c3"}, {"title": "B.2. Hyperparameter Configuration", "content": "Tab. 8 details the hyperparameter configuration used in our OV-LVIS and OV-COCO experiments. The configurations for OV-COCO and OV-LVIS are as follows: Both use the SGD optimizer with a momentum of 0.90 and a weight decay of 2.50 \u00d7 10-5. The learning rate for OV-COCO is set to 0.04, while for OV-LVIS, it is 0.08. OV-COCO runs for a total of 90K iterations, whereas OV-LVIS runs for 180K iterations. Additionally, the batch size for OV-COCO is 8, while OV-LVIS uses a batch size of 16. To ensure a fair comparison, we adopt the same hyperparameter settings as BARON for constructing a bag, specifically k and G.\nIn this work, SBV performs a hyperparameter search involving \u0394, \u03b7, s, and \u03c3. Note that the remaining hyperparameters are set to fixed numeric values by default for our experiments. Given that the OV-LVIS benchmark, with its more extensive annotations, is generally considered more challenging than OV-COCO, stricter hyperparameter settings are required for OV-LVIS. However, by applying the same hyperparameter settings from OV-COCO to OV-LVIS, we observe suboptimal performance gains (0.5 increase) on OV-LVIS, as shown in Tab. 1.\nTo further improve performance, we provide guidelines for adjusting the hyperparameters of our model. Specifically, we believe that the middle view in OV-LVIS can provide as much information as the global view in OV-COCO."}, {"title": "B.3. Visualization", "content": "In this section, we visualize each module of SBV in Fig. 8. We aim to demonstrate its effectiveness in capturing meaningful concepts around each region proposal. We visualize eight images containing novel categories from the OVD datasets. These novel categories (e.g. \u201cdog\u201d, \u201cmirror\u201d, \u201celephant\", and \"traffic_light\") are listed on the left side of each row. Note that the legend for each box is specified at the bottom of the figure.\nFirst, we analyze the characteristics of the Region Proposal Network (RPN), which has the potential to detect novel objects [22]. We then examine its results and limitations in capturing key visual concepts. This highlights the need for our canvas design to sample a few informative visual concepts. Next, we visualize the extraction of these key concepts through edge generation on this canvas. Finally, we demonstrate that SBV captures surrounding concepts more accurately than BARON, which samples the surrounding area with a fixed window size."}, {"title": "Canvas visualization", "content": "Canvas visualization. We consider the RPN results as candidate visual concepts, given their ability to detect novel categories in the image. As shown in Fig. 8-a, the top-k red RPN boxes illustrate regions of potential objects within the image. Here, the k is set to 300 by default. These RPN boxes are somewhat noisy and overly abundant. To address this, the traditional RPN applies a post-processing technique, Non-Maximum Suppression (NMS), to generate region proposals, represented as green boxes. However, since RPN generates results biased toward base categories, the region proposals predominantly represent these categories. To address this, we design a probabilistic search algorithm to extract a few informative RPN boxes, including those for novel categories within the image.\nTo achieve this, we introduce a new data structure called the canvas. This structure converts the given image into a mesh grid, where each coordinate represents the probability of finding RPN boxes in the up, down, left, or right cardinal directions. Initially, each coordinate is assigned an equal probability in all directions, represented in dark purple in Fig. 8-b. To accelerate canvas construction, we reduce the top-k RPN boxes to a small number of non-overlapping RPN boxes, as described in Sec. A.3-b. These non-overlapping RPN boxes are shown as light purple boxes. During probabilistic exploration across the coordinates, encountering these light purple areas redirects movement toward directions with key concepts.\nEdge visualization. We aim to extract key RPN boxes from the numerous RPN boxes. We utilize the key RPN boxes as visual concepts in our pipeline. To achieve this, we use the previously introduced canvas, which stores the probability of RPN box presence in neighboring directions at each coordinate. We demonstrate that SBV effectively samples a few informative RPN boxes around each region proposal on this canvas.\nSpecifically, we create a subset that includes the region proposals extracted by RPN after NMS, along with N extra region proposals. These extra region proposals are sampled from top-k RPN boxes, chosen as those farthest from all other region proposals on average. More details are in Sec. A.4. In Fig. 8-c, region proposals in this subset are represented by green boxes, while extra region proposals are shown as yellow boxes. Then, we generate edges connecting different pairs of region proposals in this subset. The edges detect the locations of key R"}]}