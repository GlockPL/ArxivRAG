[{"title": "Sampling Bag of Views for Open-Vocabulary Object Detection", "authors": ["Hojun Choi", "Junsuk Choe", "Hyunjung Shim"], "abstract": "Existing open-vocabulary object detection (OVD) develops methods for testing unseen categories by aligning object region embeddings with corresponding VLM features. A recent study leverages the idea that VLMs implicitly learn compositional structures of semantic concepts within the image. Instead of using an individual region embedding, it utilizes a bag of region embeddings as a new representation to incorporate compositional structures into the OVD task. However, this approach often fails to capture the contextual concepts of each region, leading to noisy compositional structures. This results in only marginal performance improvements and reduced efficiency. To address this, we propose a novel concept-based alignment method that samples a more powerful and efficient compositional structure. Our approach groups contextually related \"concepts\" into a bag and adjusts the scale of concepts within the bag for more effective embedding alignment. Combined with Faster R-CNN, our method achieves improvements of 2.6 box AP50 and 0.5 mask AP over prior work on novel categories in the open-vocabulary COCO and LVIS benchmarks. Furthermore, our method reduces CLIP computation in FLOPs by 80.3% compared to previous research, significantly enhancing efficiency. Experimental results demonstrate that the proposed method outperforms previous state-of-the-art models on the OVD datasets.", "sections": [{"title": "1. Introduction", "content": "Open-vocabulary object detection (OVD) methods leverage pre-trained vision-language models (VLMs) [2, 11, 20] to recognize classes that were not seen during training. VLMs are trained on large-scale datasets with paired image and text data, making them beneficial for understanding classes that detectors have not encountered during training.\nRecently, BARON [25] proposed a bag-of-regions embedding to better exploit the compositional structures among diverse semantic concepts (e.g. contextual co-occurrence between person and airplane) that VLMs are known to learn implicitly [3]. Specifically, BARON: 1) groups contextually related neighboring regions for each region proposal extracted from Faster R-CNN [22] and construct a bag; 2) projects regions within the bag into the word embedding space using Faster R-CNN and a linear layer, resulting in pseudo-words; 3) passes pseudo-words through the text encoder to obtain a bag-of-regions embedding; and 4) aligns this bag-of-regions embedding with the VLM's image encoding during training (Fig. 1-a). This approach enables BARON to exploit the complex semantic structures inherent in VLMs, achieving significant performance gains.\nHowever, BARON constructs a bag by grouping neighboring regions without considering their semantic meaning. By focusing only on spatial proximity, BARON not only often samples background regions that are simply nearby but also struggles to capture surrounding objects of different sizes and their relationships. This results in reduced efficiency and marginal performance improvements.\nIn this paper, we propose a novel adaptive sampling strategy that selectively samples surrounding windows by considering semantic information, which we call concept windows. The proposed technique captures compositional structures more accurately than BARON, enabling more effective use of the knowledge embedded in VLMs. Specifically, we first obtain an overly large number of candidate concepts. These concepts are represented by proposals with inherent objectness scores from the Region Proposal Network (RPN) [22]. RPN is commonly used in OVD [25, 29, 33] for effectively detecting potential novel objects [8, 31, 34]. To capture meaningful key concepts, we construct a graph-based canvas where region proposals, filtered through Non-Maximum Suppression (NMS), are nodes and semantic associations form edges. These edges are formed through probabilistic exploration, converging towards dense, high-objectness concepts that are more likely to contain objects. We sample a few useful concepts from these edges to create a bag of concepts (Fig. 1-b).\nTo consider concepts of varying sizes and their relationships, we sample a bag of views, which we refer to as SBV (Sampling Bag of Views). Specifically, we represent concepts in the bag through three distinct perspectives: the global view, which represents the entire image; the local view, which represents each concept individually; and the middle view, which merges multiple concepts. We determine the optimal view based on the size of each view and the number of co-occurring concepts. This enables us to adjust each concept's scale to better fit the scene. For example, a smaller view is preferred if a broader view excessively increases scene complexity. Conversely, a broader view is preferred if it includes more concepts effectively, as it may better capture the structure. Furthermore, to leverage view embedding alignment, we use separate masks to weigh the importance of each view during CLIP feature extraction.\nWe conduct extensive experiments on two challenging benchmarks, OV-COCO and OV-LVIS. Our approach achieves 36.6 box AP50 on novel categories in OV-COCO and 23.1 mask mAP on novel categories in OV-LVIS, surpassing previous state-of-the-art methods. Notably, our method reduced CLIP computation (FLOPs) by 80.3% compared to BARON, demonstrating its efficiency."}, {"title": "2. Related Work", "content": "Open-vocabulary object detection (OVD) aims to detect objects from novel categories unseen during training. Many prior studies leverage large vision-language models (VLMs) [11, 20], trained on extensive image-text pairs, to perform zero-shot recognition. The key to improving performance for unseen classes lies in effectively utilizing the alignment between images and text inherent in VLMs.\nTo achieve this, methods such as [5, 26, 27] employ prompt modeling to transfer knowledge through learned prompts, enabling more precise contextual descriptions of each class category and thus enhancing performance. Several studies use weakly supervised techniques, such as visual grounding data [13], image captions [7, 29, 33], and image labels [8, 34]. Other approaches [10, 17, 28] reinforce the text modality using Large Language Models (LLMs). In contrast, InstaGen [6] focuses on the image modality, enhancing novel class prediction by using synthetic images generated by an image generation model. Grounding DINO [18] is also noteworthy, enabling prompt-based object detection by exchanging information between VLMs and detection transformers [1] through cross-modality fusion.\nWhile many methods have been proposed, distillation-based methods [8, 23, 31\u201333] currently represent one of the highest-performing techniques, aligning individual region embeddings of an object detector with the features extracted from VLMs to leverage VLM knowledge effectively. Recently, BARON [25] has advanced beyond using individual regions by grouping neighboring regions around region proposals and aligning them with VLM features. This approach captures the compositional structure of concepts inherent in VLMs and has achieved significant performance gains.\nHowever, we argue that some of BARON's design choices are suboptimal. Specifically, BARON does not consider semantics when sampling neighboring regions, often resulting in the sampling of unnecessary regions. Consequently, we believe BARON suffers from low computational efficiency and has room for further performance improvement. Our proposed method, in contrast, samples regions with semantic consideration, reducing unnecessary sampling, thereby achieving high computational efficiency and enhanced performance simultaneously."}, {"title": "3. Preliminaries", "content": "In this paper, we instantiate the idea of BARON [25] to capture compositional structures. BARON samples the nearest neighbors around each region proposal within an image and groups them into a bag. This enables VLMs to infer the compositional structure of semantic concepts. VLMs implicitly learn the compositional structure of complex scenes from large-scale image-text pairs, aligning each concept's pixel embedding with its corresponding text representation. MaskCLIP [3] demonstrates this by showing that VLMs [2, 11, 20] effectively capture representations of concepts in complex scenes not explicitly learned.\nLimitations of Bag of Regions. However, we argue that BARON's design has two limitations: 1) BARON constructs a bag by grouping neighboring regions without considering their semantic meaning, reducing its efficiency in sampling important structures within a scene. By relying solely on spatial proximity, BARON often samples nearby backgrounds with no semantic concepts. For example, as shown in Fig. 2-a, BARON includes backgrounds like the sky, which limits its use of contextual co-occurrence among semantic concepts and increases computational costs.\n2) Additionally, since bags are created based on fixed window sizes defined by region proposals, BARON struggles to capture surrounding objects of varying sizes and their relationships. As seen in Fig. 2-b and c, smaller windows may capture only part of a surrounding object, such as a portion of a horse, missing important details, while larger windows may include too irrelevant background around the target object within region proposals.\nTo quantify this phenomenon, we measure the average number of unnecessary neighboring regions using ground truth (GT) boxes with IoU and noise embeddings with cosine similarity on the COCO benchmark [15]. For simplicity, we treat GT boxes as semantic concepts and noise embeddings as irrelevant backgrounds. The noise embeddings are derived from BARON's contextual embeddings, which are learnable by training an additional linear layer on the model's class tokens. This setup allows learning features for non-semantic regions, such as backgrounds. As shown in Fig. 4, the noise embeddings effectively capture the general characteristics of background areas, such as sidewalks.\nIn our analysis, neighboring regions extracted by BARON are deemed unnecessary if they overlap less than 85% IoU with the GT boxes or their embeddings have a cosine similarity score above 0.8 with the noise embeddings. We demonstrate the validity of this setting in Sec. A.1. Consequently, we observe that such unnecessary neighboring regions account for an average of 73% of all neighboring regions per image. These unnecessary neighbors lead to marginal performance gains and high computational costs when utilizing VLMs. Our method is motivated by the need for a more effective and efficient sampling strategy."}, {"title": "4. Methodology", "content": "We call our method SBV (Sampling Bag of Views). Our method aligns embeddings from a bag of views, going beyond individual regions or a bag of regions for OVD (Fig. 3). In this work, we design a powerful and efficient sampling strategy to form a bag of concepts that effectively captures semantic concepts around region proposals (Sec. 4.1). This bag is then used to generate three hierarchical views for each concept in the bag. Using these candidate views, the concepts in the bag are mapped to views that best represent optimal scales with masks applied to improve the effectiveness of embedding alignment (Sec. 4.2).\n4.1. Adaptive Sampling Strategy\nIn this section, we introduce an adaptive sampling strategy that creates surrounding windows to capture contextually interrelated semantic concepts around region proposals (Fig. 3-a). Our sampling process is carried out in two steps: 1) identifying surrounding key semantic concepts from RPN boxes (Sec. 4.1.1) and 2) forming a bag of concepts to incorporate these concepts (Sec. 4.1.1).\n4.1.1 Identifying Surrounding Semantic Concepts\nWe focus on extracting semantic concepts around region proposals within an image. We treat RPN boxes, which are widely used in OVD [25, 31] and potentially represent novel objects, as candidate concepts. Since the top-k RPN boxes can be noisy and overly abundant (e.g. k is 300), we design a canvas to sample a few key semantic concepts around region proposals. This process generates edges on the canvas to explore semantic relationships between the proposals, converging towards denser, high-objectness concepts that are more likely to contain relevant objects (Fig. 3-a).\nCanvas construction. We first divide each image into a grid with intervals of \\(\\Delta\\), referred to as the canvas, and generate coordinates for each interval. The canvas encodes the probabilities of RPN boxes in the cardinal directions (e.g. up, down, left, and right) around each coordinate, reflecting the likelihood of relevant concepts. To compute this likelihood, we leverage two key properties of RPN [22]: 1) RPN boxes have objectness scores, and 2) RPN boxes are densely populated in areas with objects. To examine the extent of this population, we check the overlap between RPN boxes and a directional \\(\\Delta\\)-sized box in each cardinal direction at each coordinate. We then calculate the average probability of the top-k RPN boxes by multiplying their objectness score with their IoU against the corresponding directional box in each direction. This yields the probability of relevant concepts for each direction at each coordinate. To speed up the canvas construction, we reduce the top-k RPN boxes from Faster R-CNN [22] to a smaller set of non-overlapping boxes \\(R_{\\text{reduced}}\\), as detailed in Sec. A.3.\nEdge generation. Our method explores contextual relationships by traversing the canvas across coordinates, examining all pairs of region proposals obtained from Faster R-CNN. This exploration is guided by coordinates that determine the direction with the highest probability of containing relevant concepts. We model this exploration as edges, which represent the search for key semantic concepts.\nSpecifically, an edge is processed by navigating coordinates between a pair of region proposals under the following constraints: 1) edges avoid visiting other region proposals to prevent overfitting to easily identifiable objects within those proposals, 2) probabilities for directions that do not lead to the destination are set to zero, and 3) traversal stops when no valid path remains or when it moves outside the image. To leverage probabilistic variability, we generate E different edges for each pair of proposals. This approach helps improve robustness in sampling semantic concepts.\nUltimately, the edges converge to a small set of key visual concepts that are common across all edges for every pair of region proposals in each image. We provide visualizations of our method in Sec. B.3, demonstrating its ability to effectively sample key concepts through edges.\nHowever, this design depends on the properties (e.g. count and location) of region proposals, meaning that fewer proposals result in fewer edges. To improve the detection of semantic concepts, we greedily select N extra proposals from \\(R_{\\text{reduced}}\\), each chosen to be the farthest, on average, from existing region proposals and previously selected ones. This strategy maximizes edge diversity by encouraging exploration from different directions. More details on this greedy sampling are provided in Sec. A.4.\n4.1.2 Forming Bag of Concepts\nWith the knowledge of visual concepts v, we can capture diverse contextual relationships between semantic concepts. To leverage this contextual information, we aim to sample visual concepts that provide optimal context for each region proposal. These sampled concepts are referred to as representative concepts \\(v^*\\), which default to the neighboring regions in BARON for simplicity.\nWe adjust the sampling probability according to distance and aspect ratio, as they contribute to contextual information. We believe that increasing the distance between region proposals and visual concepts captures more semantic concepts. Meanwhile, the aspect ratio preserves the original shape of objects during resizing for CLIP input. We sample the representative concept \\(v^*\\) for each surrounding direction of region proposals in Eq. (1):\n\\[v^* = \\underset{v \\in \\mathcal{V}_{i}}{\\arg \\max} \\left( \\lambda \\cdot ||d_{v,i,j}||^{\\eta} + \\alpha \\cdot ||\\Upsilon_{v,i,j}|| \\right), \\quad (1)\\]\nwhere \\(\\Lambda\\) and \\(\\alpha\\) are hyperparameters for distance and aspect ratio, \\(|| \\cdot ||_{\\eta}\\) denotes normalization by \\(\\eta\\), \\(d_{v,i,j}\\) computes a distance between a visual concept v and the i-th region proposal \\(R_i\\), and \\(\\Upsilon_{v,i,i}\\) computes the merged box ratio between v and the j-th neighbor of the i-th region proposal \\(R_i\\).\nTo enhance cost efficiency, we discard surrounding windows around region proposals that lack at least one visual concept in any direction. We then refine the remaining windows to incorporate \\(v^*\\), as shown in Eq. (2):\n\\[C = \\text{concat}\\left(\\underset{i \\in \\mathcal{R}}{\\text{concat}}\\left( \\text{merge}(v^*, R_i) \\right)\\right) \\quad (2)\\]\nwhere \\(\\mathcal{S}_i\\) contains indices of surrounding windows that include at least one visual concept for the i-th region proposal. These surrounding windows are referred to as concept windows. Finally, we sample a bag of concepts by merging region proposals with their corresponding sampled concept windows. More implementation details are in Sec. A.1.\n4.2. Aligning Bag of Views\nIn this section, we adjust the scale of concepts within a bag to effectively capture surrounding objects of different sizes and their relationships, which BARON cannot achieve due to its fixed window size. To achieve this, we propose a novel representation-switching strategy to explore the optimal scale of concepts within the bag (Sec. 4.2.1). For more effective embedding alignment, we utilize previously extracted noise embeddings to generate noise masks and different viewpoints within a bag to generate view masks. These masks are used to extract the bag-of-view embeddings from the CLIP image encoder (Sec. 4.2.2). We then align these embeddings with their corresponding textual representations (Fig. 3-b).\n4.2.1 Representation Switching\nTo effectively learn embeddings for objects of varying sizes, we introduce a novel technique for switching representations across regions, concepts, and views. Note that concepts within a bag are formed by merging region proposals with either corresponding neighboring regions by BARON or concept windows. Herein, regions represent potential individual objects identified by BARON, while concepts extend regions by capturing contextual relationships. We introduce a new representation called views, which refine the previous representations by optimizing their scale.\nTo determine the views, we establish three hierarchical view levels (e.g. global, middle, and local). As shown in Fig. 3-b, we define each concept within the bag as the local view, the entire image as the global view, and the merged crop of all concepts as the middle view. We then select a representative view V for each concept based on differences in view size and the number of visual concepts among the candidate views. Specifically, we compare the local view with its parent view in a greedy fashion (e.g. global \u2192 middle \u2192 local). In each comparison, we switch the target representation to the current parent view if it exceeds the threshold \\(\\tau\\). The threshold is calculated as \\(r = r \\times [\\frac{L}{P}]\\); where r is the local-to-parent view size ratio, and L and P represent the number of v in the local and parent view, respectively. When the local view closely matches the parent view in size, we increase the probability of switching to the parent. This carries a relatively low risk of complicating the scene. Conversely, we reduce the probability when the parent view contains relatively few co-occurring visual concepts compared to the local view. This representation-switching strategy leads to an optimal scale for each concept within the bag, resulting in a bag of views.\n4.2.2 View-wise Attention\nFor more effective view embedding alignment, we introduce two key improvements: 1) diminishing the influence of noise (e.g. background), and 2) understanding the importance of each view among the sampled views within the bag, where representation switching occurs.\nFor (1), we design noise masks N to reduce the influence of noisy patches when extracting embeddings from CLIP. To achieve this, we utilize previously extracted noise embeddings \\(x\\) which typically represent general backgrounds. We then apply an attention mask to patches with similarity to the noise embeddings \\(x\\) above \\(\\tau\\), as shown in Eq. (3):\n\\[N = \\begin{cases} -\\infty, & \\text{if } \\mathcal{I}(V) \\cdot x > \\tau, \\\\ 0, & \\text{otherwise}, \\end{cases} \\quad (3)\\]\nwhere \\(\\mathcal{I}\\) is the image encoder and \\(\\cdot\\) represents CLIP similarity. We validate the optimal threshold search with visualizations at different thresholds in Sec. B.1.\nFor (2), we devise view masks \\(\\mathcal{M}\\) to enhance the representation based on the importance of each view. Using hyperparameters that define the importance of each view, we empirically analyze the importance of patches corresponding to each view. Herein, the view masks store the importance of each pixel for the respective view. These masks are then multiplied by the softmax output in CLIP's attention mechanism to adjust the influence of patches corresponding to each view. In Fig. 4-b, the view masks reflect view importance, with yellow areas indicating higher importance. As shown in Fig. 3-b, we use both masks to extract the view embedding F from CLIP, as shown in Eq. (4):\n\\[F= \\left(\\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{d}} + N \\right) \\odot \\mathcal{M} \\right)V. \\quad (4)\\]\nwhere d is the CLIP dimension, \\(\\odot\\) denotes element-wise multiplication, and Q, K, and V are the queries, keys, and values of the views within the bag."}, {"title": "5. Experiments", "content": "Datasets. We evaluate our method using two well-known OVD datasets: OV-COCO [15] and OV-LVIS [9]. For the OV-COCO dataset, we adopt the category split approach from OVR-CNN [29], dividing the object categories into 48 base categories and 17 novel categories. For the OV-LVIS dataset, we follow ViLD [8], separating the 337 rare categories into novel categories and grouping the remaining common and frequent categories into base categories.\nEvaluation Metrics. We assess detection performance on both base and novel categories. For OV-COCO, we follow OVR-CNN [29] report the box AP at an IoU threshold of 0.5, denoted as AP50. For OV-LVIS, we report the mean Average Precision (mAP) of masks averaged on IoUs from 0.5 to 0.95. The primary metrics for evaluating open-vocabulary detection performance are the AP50 of novel categories (APnovel) for OV-COCO and the mAP of rare categories (APrare) for OV-LVIS.\n50\nImplementation Details. We build our method on Faster R-CNN [22] with ResNet50-FPN [16]. For a fair comparison, we initialize the backbone network with weights pretrained by SOCO [24] and use synchronized Batch Normalization (SyncBN) [30], as done in recent studies [5, 25]. For the main experiments on OV-COCO and OV-LVIS [9, 15], we choose the 1x and 2\u00d7 schedules. We use the CLIP model [20] based on ViT-B-16 [4] as our pre-trained visual language model (VLM). For category name prompts, we default to the hand-crafted prompts from ViLD [8] in all our experiments on OV-COCO. We use learned prompts following DetPro for experiments on OV-LVIS.\n5.1. Main Results\nOV-COCO. We compare previous state-of-the-art methods on the OV-COCO (Tab. 1-a). While OV-DETR [27], which builds on Deformable DETR [35], outperforms Faster R-CNN [22] on base categories, SBV surpasses OV-DETR by 7.2 AP50 on OV-COCO for novel categories. SBV even outperforms recent studies [5, 7] that use sophisticated pseudo-labeling. Furthermore, SBV outperforms BARON [25] by 2.6 AP50 for novel categories, despite the more extensive use of CLIP resources by BARON. We show that SBV achieves the best performance across all scenarios."}, {"title": "OV-LVIS", "content": "We compare SBV with previous state-of-the-art methods on the OV-LVIS benchmark (Tab. 1-b). Since OV-LVIS has more detailed annotations, it is generally a more challenging benchmark. As a result, the performance observed on OV-LVIS tends to be smaller than OV-COCO. Nevertheless, SBV surpasses OV-DETR [27] by 5.7 on OV-LVIS for novel categories even in different backbones (e.g. Vision Transformer [4]). SBV also performs better than the caption-based models [7, 33] that use additional caption data. SBV also outperforms BARON by 0.5 AP on OV-LVIS. We highlight that SBV achieves superior performance in all situations.\nHowever, with the hyperparameters set for OV-COCO, SBV shows relatively smaller improvements on benchmarks like OV-LVIS, which contain finer annotations. We believe that this setup is optimized for the coarser annotations of OV-COCO. We provide guidance on hyperparameter tuning for our method on the OVD datasets in Sec. B.2.\n5.2. Ablation Study\nIn this section, we ablate the effectiveness of components in our method on the OV-COCO benchmark.\nEffectiveness of Bag of Views. We evaluate the effectiveness of each component of our method (Tab. 2). First, we evaluate the effect of the noise mask independently. It is applied to each region within a bag, which is obtained by BARON's sampler and used as input for CLIP. We demonstrate that the mask effectively reduces CLIP's computational costs by more than half without sacrificing performance on novel categories. This indicates that the noise mask successfully removes unnecessary background patches from the CLIP's attention mechanism.\nFurthermore, by employing our adaptive sampling, our approach already outperforms the baseline [25] by 2.6 APnovel while significantly reducing computational costs by 79.2%. This demonstrates that our sampler leverages the generalization power of VLMs more effectively than BARON. It is possible by aligning co-occurring concepts around region proposals and discarding unimportant ones.\nLastly, when we apply the view mask along with representation switching, we find that our method further improves performance by 0.6 APnovel and increases computational efficiency by reducing 0.6 PFLOPs.\nSampling Distance. We analyze the impact of the sampling distance \\(\\eta\\) on the candidate visual concepts v. As shown in Tab. 3, we observe that a larger concept window with a high \\(\\eta\\) may capture a more complex scene with excessive objects and noise, leading to performance degradation. Nevertheless, our method ensures exclusive learning of concepts within a bag by trimming overlap with other region proposals, resulting in a performance boost even with \\(\\eta\\) set to 0.8.\nNumber of Extra Region Proposals. We study the impact of the number of extra region proposals N, inspired by Faster R-CNN's tendency to predict a few region proposals per image after NMS. With N set to 0 by default, our method achieves results comparable to BARON, with a 1.3 AP novel increase. As N increases, our method boosts performance linearly by identifying more key visual concepts through additional edges from varying directions within each image. However, when N exceeds a certain threshold, the number of these concepts becomes saturated, yielding no additional performance gains.\nNoise Mask Threshold. We ablate the noise mask N using different thresholds from the distribution \\(\\mathcal{I}(V) \\cdot x\\) in Tab. 5. \\(\\tau\\) is set to combinations of \\(\\mu\\) and \\(\\sigma\\) from the distribution. We observe the noise mask becomes stricter in removing unnecessary background areas as \\(\\tau\\) increases. The best result on novel categories is achieved with \\(\\tau = \\mu + 4\\sigma\\). For further analysis in Sec. B.1, we present a visualization showing the effectiveness of this mask in capturing key concepts and minimizing unnecessary areas across different thresholds.\nView Importance. We study the impact of each view on the performance for novel categories by adjusting combinations of view weights \\(\\delta_{\\text{global}}, \\delta_{\\text{middle}}, \\delta_{\\text{local}}\\) during training. We first isolate each view by setting its weight to 1.0 while neglecting the others, without triggering representation switching. In Tab. 6, forcing the global view leads to a performance drop to 16.9 APnovel, while forcing the local view maintains 36.0 APnovel. This linear performance drop suggests that the global view captures overly complex scenes.\nFurthermore, within each view, we set \\(\\delta_{\\text{local}}\\) to 1.0, as the local view is more likely to contain the ground truth target. In Tab. 6, we observe a significant performance drop as the weight of \\(\\delta_{\\text{global}}\\) increases by 0.1. The best result on novel categories is achieved by setting \\(\\delta_{\\text{global}}\\) to 0.0, \\(\\delta_{\\text{middle}}\\) to 0.8, and \\(\\delta_{\\text{local}}\\) to 1.0. We leave the study of leveraging the entire image to enhance performance as future work.\nSampling Strategy. We compare our adaptive sampling with other strategies (Tab. 7). The grid sampling strategy divides the image into equal grids, similar to the pretraining stage in OVR-CNN [29]. The random sampling strategy arbitrarily selects region proposals to form a bag of regions, usually representing the entire image. The Random-Tight strategy focuses on a cropped area that tightly encloses the selected regions instead of the entire image. The Random-Neighbor strategy selects two nearby regions with GIoU > 0.5 for each central region, ensuring 36 regions from 12 centers. The BARON (reduced) limits the number of region proposals per image to 12, taking one bag per proposal. We show our method outperforms all these sampling strategies."}, {"title": "5.3. Qualitative Research", "content": "In this section, we present qualitative results to further analyze the effectiveness of our method.\nQualitative results. We visualize the predictions of detectors learned through our method and BARON (Fig. 5). We visualize the feature map response to both base and novel categories using Eigen-CAM [19]. We find that our method produces discriminative responses at novel category locations, while BARON shows weaker or scattered responses. For example, our method accurately attends to novel categories like 'elephant,' which BARON often misses.\nVisualization of sampling results. We compare the sampling results of our method and BARON on the two OVD datasets. Each image shows labels for novel categories, with base categories in green boxes (Fig. 6). BARON, which samples neighboring regions based on a fixed window size of the region proposal, often produces a bag of regions that do not fully capture semantic concepts, such as novel objects. In contrast, our method samples a bag of views based on surrounding meaningful concepts, capturing novel objects more effectively than BARON. Thus, we demonstrate that our method fosters a more powerful and efficient compositional structure within VLMs."}, {"title": "6. Conclusion", "content": "This paper explores an improved compositional structure of semantic concepts through a bag of views in OVD. The compositional structure is mainly about the co-occurrence of objects. This leverages large-scale VLMs' ability to represent the compositional structure of concepts within image-text pairs. We develop an adaptive sampling strategy to group contextually related concepts into a bag. Concepts within a bag are then adjusted to views considered optimal scales, with masks applied to enhance embedding alignment. Finally, we adopt a distillation-based approach to align the detector's bag-of-view representations with those of pre-trained VLMs. Our method outperforms previous state-of-the-art methods on the OVD benchmarks while significantly enhancing VLM utilization efficiency."}, {"title": "A. Implementation Details", "content": "A.1. Implementation\nComputing unnecessary neighboring regions. We outline the process for calculating the ratio of unnecessary neighboring regions extracted by BARON [25]. In our analysis, these neighboring regions are considered unnecessary if their overlap with the ground-truth (GT) boxes is less than 85% IoU or if their embeddings have a cosine similarity score above 0.8 compared to noise embeddings.\nOur threshold selection is inspired by the non-maximum suppression (NMS) process in Faster R-CNN [22]. Faster R-CNN uses NMS to filter out overlapping region proposals with lower objectness scores, retaining only a few proposals. Specifically, two RPN boxes are considered overlapping when their IoU is 0.1, allowing the detector to focus on the most likely RPN box within the surrounding area. For consistency, we adhere to BARON's hyperparameter settings when applying Faster R-CNN. This forms the basis for our hyperparameter choices. Specifically, we classify regions with an IoU approximately close to 10.1 = 0.9 \u00b1 0.05 as sufficiently representative of a GT box, while regions with a lower IoU are deemed to ambiguously represent the GT.\nAdditionally, we include a feature-level comparison using cosine similarity between noise embeddings and the regional embeddings of RPN boxes. The learnable noise embeddings are generated by training an additional linear layer on the model's class tokens. Due to the noisy nature of similarity distributions, we only consider background regions with a cosine similarity of approximately 0.8 or higher as unnecessary. Consequently, the hyperparameters for IoU and the similarity threshold are set to 0.85 and 0.8, respectively.\nSampling bag of concepts. In this paper, we introduce concept windows, which effectively capture key concepts near each region proposal. Open vocabulary object detection (OVD) aims to predict novel objects often unseen during training, emphasizing the need to prevent overfitting to training data. For implementation details, we outline two operations within our adaptive sampling strategy designed to minimize overfitting to easily identifiable objects frequently found in region proposals during training.\n1) We design these concept windows to avoid repeatedly including other regional proposals. To achieve this, we trim the concept windows at the boundaries of all region proposals, preventing training bias and reducing the risk of overfitting certain objects while underpredicting others.\n2) Additionally, we incorporate BARON's sampling"}, {"title": "A.3. Non-overlapping RPN Boxes", "content": "We introduce a simple yet effective technique to accelerate the computation of existence probabilities for numerous top-k RPN boxes around each coordinate on the canvas. The key idea is to selectively reduce the top-k RPN boxes. To ensure uniform probability calculations across all image regions, the RPN boxes need to be sparsely distributed. RPN boxes already fulfill this requirement but may demand significant processing time due to their excessive abundance. To address this, we optionally use greedy sampling to select non-overlapping RPN boxes from the top-k boxes in the image, as shown in Eq. (5):\n\\[R_{\\text{reduced}} = \\{r \\in R_{\\text{topk}} \\mid \\forall r' \\in R, \\text{IoU}(r, r') > \\tau\\} \\quad (5)\\]\nwhere \\(\\tau\\) indicates an IoU threshold default to 0.0, IoU denotes Intersection over Union, R represents region proposals, and \\(R_{\\text{topk}}\\) refers to the top-k region proposals extracted by Faster R-CNN [22].\nIn this process, we extract RPN boxes that do not overlap with the region proposals and ignore very small RPN boxes for simplicity. As a result, this greedy sampling effectively"}, {"title": "A.4. k-farthest RPN Boxes", "content": "In this paper", "22": "where only a few region proposals are generated per image after NMS. Specifically", "mirror": "ample", "6)": "n\\[R_{\\text{added}} = \\left\\{r \\in R_{\\text{added}} \\mid r = \\underset{r \\in R_{\\text{reduced}"}]}, {}]