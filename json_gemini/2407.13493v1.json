{"title": "Training Foundation Models as Data Compression: On Information, Model Weights and Copyright Law", "authors": ["Giorgio Franceschelli", "Claudia Cevenini", "Mirco Musolesi"], "abstract": "The training process of foundation models as for other classes of deep learning systems is based on minimizing the reconstruction error over a training set. For this reason, they are susceptible to the memorization and subsequent reproduction of training samples. In this paper, we introduce a training-as-compressing perspective, wherein the model's weights embody a compressed representation of the training data. From a copyright standpoint, this point of view implies that the weights could be considered a reproduction or a derivative work of a potentially protected set of works. We investigate the technical and legal challenges that emerge from this framing of the copyright of outputs generated by foundation models, including their implications for practitioners and researchers. We demonstrate that adopting an information-centric approach to the problem presents a promising pathway for tackling these emerging complex legal issues.", "sections": [{"title": "1. Introduction", "content": "Besides curiosity and excitement, the current generative Al wave raises various philosophical and practical questions (Franceschelli & Musolesi, 2023; Shanahan, 2024; Weidinger et al., 2022). Among them, a relevant issue is how current copyright laws can be applied to generative AI (Lee et al., 2024). For instance, can AI-generated outputs be copyrighted? Is it lawful to train models on protected works? Furthermore, would it be possible to protect the model itself in some manner?\nTo better understand how such models work and how they can be interpreted under current laws, we believe that a viable solution is to link generative deep learning with information theory (Cover, 1999). In fact, generative models trained with self-supervised learning, i.e., by maximizing the likelihood of training data, as commonly done for foundation models (Bommasani et al., 2021) such as large language models (LLMs) (Bubeck et al., 2023; Gemini Team et al., 2023; Touvron et al., 2023) and diffusion models (Ho et al., 2020; Rombach et al., 2022), can be seen as a form of (lossy or lossless) compression (MacKay, 2003). From this perspective, the training algorithm plays the role of the compression algorithm; the inference (feed-forward) algorithm is the de-compression algorithm (with the input passed to the model working as a decoding key); and the model's weights represent the compressed version of the training set.\nDeletang et al. (2024) discuss how a language model can implement a lossless compression process offline, i.e., through a fixed set of model parameters derived from training. We move a step further and claim the self-supervised learning used to train foundation models to be a lossy or lossless compression process, during which the whole training set is encrypted into the model's weights. This is demonstrated by the fact that the model can reproduce certain portions of training samples (Carlini et al., 2023b; Kandpal et al., 2022). We suggest that the training makes the model's weights the best possible compressed version of the training set or, more correctly, batches of it at a time. The analogies at the"}, {"title": "2. Training-as-Compressing and Information", "content": "The propensity of foundation models to memorize and subsequently replicate training data is a topic that has received considerable attention in scholarly literature, as evidenced by works such as (Carlini et al., 2023a; Lee et al., 2022). In general, it is very hard to decompress every possible training sample perfectly and in its entirety, i.e., without any loss of information. Nonetheless, it has been shown that training samples can be retrieved (Carlini et al., 2021), and more advanced techniques might lead to an even higher degree of \"retrievability\". The following experiment may help us understand this matter better.\nIn (1957), Noam Chomsky wrote the famous quote \"Colorless green ideas sleep furiously\" to demonstrate the distinction between syntax and semantics: the sentence is grammatically well-formed but semantically nonsensical. If a language model had learned the semantics of English, it should not generate a semantically nonsensical sentence, i.e., it should assign to semantically nonsensical words a small probability; if, on the contrary, such words are characterized by a large probability of being generated, then it is very likely that the model has memorized them. To test this, we use the same quote from Chomsky, and we check the probability of each subsequent word given the previous ones under an LLM (in our case, LLaMa3-70B (Meta, 2024)). The probability of 'green' given \u2018\u201cColorless' is 0.2, while for other colors like 'red' or 'blue' is 0.003. For all the subsequent words, i.e., \u2018ideas', 'sleep', and 'furiously', the probability is always greater than 0.9. The model has essentially memorized the quote into its weights, otherwise it would have never assigned such high probabilities to a semantically nonsensical sentence.\nIt is then straightforward to assert that training data are memorized in a compressed form. Consider again LLaMa3-70B (Meta, 2024), one of the largest models available at the time of writing. This model is pre-trained on more than 15 trillion tokens. Each token can have one out of 32000 values, thus requiring at least 15 bits to be represented. This means the training data require more than 225 trillion bits to be memorized. However, the model has 70 billion weights and uses half-precision floating points, thus it requires ~1.1 trillion bits. With smaller models such as LLaMa3-8B, the compression ratio is even more astonishing and is possibly a cause of the lossy compression (MacKay, 2003).\nIndeed, a foundation model, such as a transformer-based LLM, consists of a neural network with weights W. It models the conditional distribution $P(X_i|X_{i-k}, ..., X_{i-1}, t; W)$, where x = $x_1$, ..., $x_N$ is the tokenized input to be modeled (and also the output to be predicted, which is the reason of the self-supervised learning), k is the size of the context window, and t is an additional input (such as a task specification). During training, the randomly initialized weights are iteratively adjusted through stochastic gradient descent (and its variants) as follows:\n$W \\leftarrow W - \\alpha \\frac{1}{\\mathcal{X}} \\nabla_W \\mathcal{L}(X, W)$, (1)\nwhere \u03b1 is the learning rate and $\\mathcal{L}(X, W)$ is the loss function computed on a batch of training samples X. In particular, the objective is to maximize the log-likelihood of training data, therefore the loss is defined as:\n$\\mathcal{L}(X, W) = - \\sum_{x,t \\in X} \\sum_i  P(X_i|X_{i-k}, ..., X_{i-1}, t; W)$. (2)\nIn other words, the training phase aims to find the optimal values of the weights W such that given the input t (i.e., the decoding key) the model can autoregressively reconstruct x by only using the information stored into W.\nFrom an information theoretic perspective, such training data compression can be explained through the information bottleneck (IB) principle (Tishby et al., 1999). The IB principle applies when we aim to extract relevant information from an input variable X \u2208 X about an output variable Y \u2208 Y. Given their joint distribution p(X, Y), the relevant information is defined as the mutual information I(X; Y). With X as the relevant part of X with respect to Y, the IB method aims to find the optimal X \u2208 X, i.e., the one that minimizes I(X; X) (obtaining the simplest possible statistics) while maximizing \u1e9eI(X; Y) (containing all the relevant information). Tishby and Zaslavsky (2015) argued that neural networks could be interpreted under the theoretical framework of the IB principle. Indeed, neural networks"}, {"title": "3. Training-as-Compressing and Copyright", "content": "The training-as-compressing perspective can shed new light on the open copyright issues related to generative modeling. While the software code responsible for the training and inference of a generative model can fall under copyright law as a computer program and the algorithmic method is a mathematical model and thus not protected (World Trade Organization, 1994), whether the model's weights can be protected or not is an open question. Indeed, if the model's weights represent a compressed version of the training set, and the training set is protected by copyright laws, then the weights are also subject to them. Assuming that the training set is protected in some ways (we will discuss it later), the weights can thus be seen as either a) a lossy or lossless compressed copy of it or b) a compressed version of a derivative work of it.\nSeeing the weights as a mere compressed copy of the training set (not different from a zipped file) is seducing since the weights are meant to contain all the information necessary to reconstruct the original samples given a certain input (i.e., the decoding key). However, the final result is usually lossy, and the common scenario is that what we obtain after decompression is similar, but not exactly equal, to the original work. If the differences are not substantial, then it can still be considered a copy; however, it can also lead to a non-negligible modification or transformation of the training data. This second option seems to match the definition of derivative works.\nThis opens up a different perspective: what the weights represent might not be the original training set, but a new, derivative work (substantially different from, but still based on, the original) whose creation happens concurrently with weights' learning and whose only existence is due to the weights themselves. Nonetheless, a derivative work must still satisfy the originality requirement to be protected by copyright. Whether or not the trainers' role in choosing data, algorithms, and parameters is sufficient for claiming authorship (and thus protection) of the model's weights is still an open question.\nUntil now, we have assumed that the training set is protected under copyright law. The whole training set can be protected as a database or a collective work, i.e., a collection of separate and independent works (Lee et al., 2024). However, the collective work must constitute an intellectual creation because of the selection and arrangement of its content; the same criteria also apply to databases. One of the current trends for training foundation models seems to go in the opposite direction. Although a certain degree of data pre-processing is always present, the apparent tendency, at least in the early days of foundational models, has been to collect as much data as possible, for example, from the Web. This approach threatens the requirement of making a careful and original selection or arrangement. Moreover, fine-tuning models by means of training sets for specific domains are more likely to be eligible for protection as collective works. Still, this interpretation does not seem to cover all foundation models' training sets.\nOn the other hand, single training samples are often protected under copyright law (Bandy & Vincent, 2021). Even though the training goal aims to compress batches of samples at a time, thus potentially leading to a compression that is optimal for a subset of works when considered together but not when considered separately, the single works can still be decompressed from the resulting model, at least in principle. This suggests that the model's weights can be interpreted as a copy (or a derivative work) of all the independent training samples, and not only of the training set as a whole."}, {"title": "4. Implications", "content": "Interpreting the model's weights as a copy or a derivative work of protected works leads to two crucial implications.\nFirst, it provides a legal framework to understand them, removing the veil of uncertainty surrounding this issue. Although asserting copyright protection for weights as a derivative work presents challenges due to the absence of valid authorship (Otero, 2021), it is possible to safeguard them by viewing the file with the model's weights as a database. Indeed, they can be considered as a collection of floating point numbers, which can be retrieved independently. Moreover, the significant investments required for obtaining them make the model's weights eligible for the sui generis right (thus providing certain rights to those who have invested in the database constitution independently from its copyright protection) (Sousa e Silva, 2024). In other words, the sui generis right can protect the investment; our copyright perspective can link the model's weights back to the training data, providing a new perspective over one of the several issues concerning the generative-AI supply chain (Lee et al., 2024). The same considerations still hold in the case of a fine-tuned model. According to Lee et al. (2024), this would be considered a derivative work of the pre-trained model (and also of the fine-tuning data). In other words, fine-tuning could be considered as nothing more than an additional step in the information processing chain. Again, the weights of the fine-tuned model would be eligible for the sui generis right. However, whether it qualifies for protection as a derivative work remains an open question, and the determination of valid (human) authorship can vary on a case-by-case basis.\nSecond, this type of interpretation provides a potential framework for works generated by the model. Indeed, decompressing the information from the model might be seen as producing a derivative work of the weights, thus a derivative work of a copy of a protected work or a derivative work of a derivative work of (a copy of) a protected work. Either way, this link between the output and the training data may help enforce their copyrights. It is worth noting that the EU text and data mining (TDM) exceptions (European Union, 2019) as well as other comparable rules (Fiil-Flynn et al., 2022) apply for TDM purposes, such as training the model, therefore to the case of the creation of a copy or derivative work; however, they do not apply for further derivative works from the model. A similar consideration can also be drawn for the US fair use doctrine, which arguably applies to training a model on copyrighted data but is less likely when deployed to generate similar content that can threaten their market (Henderson et al., 2023). The main consequence is that authorization from the training set's rightsholders would be required (or else the reproduction or adaptation right would be triggered), allowing for potential requests for compensation from original authors. In addition, generated works would need to respect the moral rights of the owners of training data, even when their economic rights have expired. The fact that a new derivative work is protected by copyright is an entirely different issue that will depend on the human contribution (i.e., the input to the model), in particular, on its substantiality and its being the main contribution of the originality (Guadamuz, 2017; Franceschelli & Musolesi, 2022). The overall conceptual framework based on the proposed training-as-compressing perspective is summarized in Figure 2."}, {"title": "5. Related Work", "content": "5.1. Deep Learning and Information Theory\nInformation theory is underpinning machine learning (MacKay, 2003). Recently, there has been considerable interest in the intersection between compression and neural networks, either to learn efficient data compression algorithms (Goyal et al., 2019; Yang et al., 2023) or to compress the learned models themselves (Choudhary et al., 2020; Polino et al., 2018). The problem of training data compression has been analyzed from the point of view of the information bottleneck principle (IB) (Tishby et al., 1999), which provides a methodology for extracting a quantitative measurement of \"meaningful\u201d and \u201crelevant\" information. In particular, IB can be used to derive an optimal theoretical limit for training neural networks in terms of trade-off between compression and prediction (Tishby & Zaslavsky, 2015). From this perspective, stochastic gradient descent can be seen as composed of two distinct phases: a \"fitting\" phase in which the mutual information between the weights and the target output increases and a \"compression\" phase in which the mutual information between the weights and the input decreases (Shwartz-Ziv & Tishby, 2017). However, the latter phenomenon might not be an intrinsic feature of deep neural networks, but only the result of the presence of nonlinearities in the networks (Saxe et al., 2018). An important recent paper in this context is (2024), in which Schwartz-Ziv and LeCun discuss IB in relation to self-supervised learning, especially in the context of multimodal learning; this work is particularly relevant given the prominence of these techniques in training state-of-the-art foundation models.\n5.2. Generative AI and Copyright\nThere has been a long-standing interest in the copyright issues around generative AI (Butler, 1982; Samuelson, 1986). Different legal issues are at play when considering the entire generative-AI supply chain (Lee et al., 2024). Whether training neural networks on protected data is lawful has been highly debated across different national legislations (Lemley & Casey, 2021), e.g., U.S. fair use (Henderson et al., 2023; Sobel, 2017), EU text and data mining exceptions (Geiger et al., 2018; Sobel, 2021), and others (Guadamuz, 2024; Samuelson, 2021). The other main debate has focused on whether a machine-generated work is protected by copyright or not (Craig & Kerr, 2019; Gervais, 2020; Grimmelman, 2016) and on the question of who might own its ownership in the future (Bonadio & McDonagh, 2020; Franceschelli & Musolesi, 2022). However, other issues have also been considered, such as whether the model output can infringe the reproduction right (Gillotte, 2020; Vyas et al., 2023) or how the trained model can be protected (Otero, 2021; Slowinski, 2021). Our analysis mainly concerns the latter point, but also provides a new technical understanding and perspective on the copyright of AI-generated works."}, {"title": "6. Conclusion", "content": "Motivated by the phenomenon of data memorization in foundation models, we have proposed a perspective we termed as training-as-compressing for examining copyright issues. Starting from a thought experiment and a theoretical discussion, we have suggested interpreting self-supervised learning as data compressing and the model's weights as a compressed version of the entire training set. From a copyright perspective, this framing has led us to consider the weights as a reproduction or a derivative work of training data, which usually contain protected works. We have discussed potential protection that can be applied to the model's weights considering the underlying direct relationship between the AI-generated outputs and the protected training data.\nThe analysis conducted in this paper predominantly raises additional questions rather than providing unequivocal solutions. We believe that a multi-disciplinary analysis of these problems and implications from the point of view of information theory is of fundamental importance for practitioners and researchers from both the technological and legal points of view. Our research agenda also includes a rigorous formalization of the problem as a basis for rigorous legal analysis of this complex yet fascinating area."}]}