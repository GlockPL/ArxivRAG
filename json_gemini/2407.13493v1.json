{"title": "Training Foundation Models as Data Compression:\nOn Information, Model Weights and Copyright Law", "authors": ["Giorgio Franceschelli", "Claudia Cevenini", "Mirco Musolesi"], "abstract": "The training process of foundation models as for\nother classes of deep learning systems is based on\nminimizing the reconstruction error over a train-\ning set. For this reason, they are susceptible to\nthe memorization and subsequent reproduction\nof training samples. In this paper, we introduce\na training-as-compressing perspective, wherein\nthe model's weights embody a compressed repre-\nsentation of the training data. From a copyright\nstandpoint, this point of view implies that the\nweights could be considered a reproduction or\na derivative work of a potentially protected set\nof works. We investigate the technical and le-\ngal challenges that emerge from this framing of\nthe copyright of outputs generated by foundation\nmodels, including their implications for practition-\ners and researchers. We demonstrate that adopting\nan information-centric approach to the problem\npresents a promising pathway for tackling these\nemerging complex legal issues.", "sections": [{"title": "1. Introduction", "content": "Besides curiosity and excitement, the current generative\nAl wave raises various philosophical and practical ques-\ntions (Franceschelli & Musolesi, 2023; Shanahan, 2024;\nWeidinger et al., 2022). Among them, a relevant issue is\nhow current copyright laws can be applied to generative AI\n(Lee et al., 2024). For instance, can AI-generated outputs\nbe copyrighted? Is it lawful to train models on protected\nworks? Furthermore, would it be possible to protect the\nmodel itself in some manner?\nTo better understand how such models work and how they\ncan be interpreted under current laws, we believe that a\nviable solution is to link generative deep learning with in-\nformation theory (Cover, 1999). In fact, generative models"}, {"title": "2. Training-as-Compressing and Information", "content": "The propensity of foundation models to memorize and sub-\nsequently replicate training data is a topic that has received\nconsiderable attention in scholarly literature, as evidenced\nby works such as (Carlini et al., 2023a; Lee et al., 2022). In\ngeneral, it is very hard to decompress every possible training\nsample perfectly and in its entirety, i.e., without any loss of\ninformation. Nonetheless, it has been shown that training\nsamples can be retrieved (Carlini et al., 2021), and more\nadvanced techniques might lead to an even higher degree\nof \"retrievability\". The following experiment may help us\nunderstand this matter better.\nIn (1957), Noam Chomsky wrote the famous quote \"Color-\nless green ideas sleep furiously\" to demonstrate the distinc-\ntion between syntax and semantics: the sentence is gram-\nmatically well-formed but semantically nonsensical. If a\nlanguage model had learned the semantics of English, it\nshould not generate a semantically nonsensical sentence,\ni.e., it should assign to semantically nonsensical words a\nsmall probability; if, on the contrary, such words are charac-\nterized by a large probability of being generated, then it is\nvery likely that the model has memorized them. To test this,\nwe use the same quote from Chomsky, and we check the\nprobability of each subsequent word given the previous ones\nunder an LLM (in our case, LLaMa3-70B (Meta, 2024)).\nThe probability of 'green' given \u2018\u201cColorless' is 0.2, while\nfor other colors like 'red' or 'blue' is 0.003. For all the\nsubsequent words, i.e., \u2018ideas', 'sleep', and 'furiously',\nthe probability is always greater than 0.9. The model has\nessentially memorized the quote into its weights, otherwise\nit would have never assigned such high probabilities to a\nsemantically nonsensical sentence.\nIt is then straightforward to assert that training data are\nmemorized in a compressed form. Consider again LLaMa3-\n70B (Meta, 2024), one of the largest models available at\nthe time of writing. This model is pre-trained on more than\n15 trillion tokens. Each token can have one out of 32000\nvalues, thus requiring at least 15 bits to be represented. This\nmeans the training data require more than 225 trillion bits to\nbe memorized. However, the model has 70 billion weights\nand uses half-precision floating points, thus it requires ~1.1\ntrillion bits. With smaller models such as LLaMa3-8B, the\ncompression ratio is even more astonishing and is possibly\na cause of the lossy compression (MacKay, 2003).\nIndeed, a foundation model, such as a transformer-based\nLLM, consists of a neural network with weights W. It mod-\nels the conditional distribution $P(X_i|X_{i-k}, ..., X_{i-1}, t; W)$,\nwhere x = x1, ..., XN is the tokenized input to be modeled\n(and also the output to be predicted, which is the reason\nof the self-supervised learning), k is the size of the context\nwindow, and t is an additional input (such as a task specifi-\ncation). During training, the randomly initialized weights\nare iteratively adjusted through stochastic gradient descent\n(and its variants) as follows:\n$W \\leftarrow W - \\alpha \\frac{1}{|X|} \\nabla_W L(X, W)$, (1)\nwhere \u03b1 is the learning rate and L(X, W) is the loss func-\ntion computed on a batch of training samples X. In par-\nticular, the objective is to maximize the log-likelihood of\ntraining data, therefore the loss is defined as:\n$L(X, W) = \u2013 \\sum_{x,t \\in X} \\sum_i P(X_i|X_{i-k}, ..., X_{i-1}, t; W)$. (2)\nIn other words, the training phase aims to find the optimal\nvalues of the weights W such that given the input t (i.e., the\ndecoding key) the model can autoregressively reconstruct x\nby only using the information stored into W.\nFrom an information theoretic perspective, such training\ndata compression can be explained through the information\nbottleneck (IB) principle (Tishby et al., 1999). The IB prin-\nciple applies when we aim to extract relevant information\nfrom an input variable X \u2208 X about an output variable\nY \u2208 Y. Given their joint distribution p(X, Y), the relevant\ninformation is defined as the mutual information I(X; Y).\nWith X as the relevant part of X with respect to Y, the\nIB method aims to find the optimal X \u2208 X, i.e., the one\nthat minimizes I(X; X) (obtaining the simplest possible\nstatistics) while maximizing \u1e9eI(X; Y) (containing all the\nrelevant information). Tishby and Zaslavsky (2015) argued\nthat neural networks could be interpreted under the theoreti-\ncal framework of the IB principle. Indeed, neural networks"}, {"title": "3. Training-as-Compressing and Copyright", "content": "The training-as-compressing perspective can shed new light\non the open copyright issues related to generative modeling.\nWhile the software code responsible for the training and\ninference of a generative model can fall under copyright\nlaw as a computer program and the algorithmic method is\na mathematical model and thus not protected (World Trade\nOrganization, 1994), whether the model's weights can be\nprotected or not is an open question. Indeed, if the model's\nweights represent a compressed version of the training set,\nand the training set is protected by copyright laws, then\nthe weights are also subject to them. Assuming that the\ntraining set is protected in some ways (we will discuss it\nlater), the weights can thus be seen as either a) a lossy or\nlossless compressed copy of it or b) a compressed version\nof a derivative work of it.\nSeeing the weights as a mere compressed copy of the train-\ning set (not different from a zipped file) is seducing since\nthe weights are meant to contain all the information nec-\nessary to reconstruct the original samples given a certain\ninput (i.e., the decoding key). However, the final result is\nusually lossy, and the common scenario is that what we\nobtain after decompression is similar, but not exactly equal,\nto the original work. If the differences are not substantial,\nthen it can still be considered a copy; however, it can also\nlead to a non-negligible modification or transformation of\nthe training data. This second option seems to match the\ndefinition of derivative works.\nThis opens up a different perspective: what the weights\nrepresent might not be the original training set, but a new,\nderivative work (substantially different from, but still based\non, the original) whose creation happens concurrently with\nweights' learning and whose only existence is due to the\nweights themselves. Nonetheless, a derivative work must\nstill satisfy the originality requirement to be protected by\ncopyright. Whether or not the trainers' role in choosing\ndata, algorithms, and parameters is sufficient for claiming\nauthorship (and thus protection) of the model's weights is\nstill an open question.\nUntil now, we have assumed that the training set is pro-\ntected under copyright law. The whole training set can be\nprotected as a database or a collective work, i.e., a collec-\ntion of separate and independent works (Lee et al., 2024).\nHowever, the collective work must constitute an intellectual\ncreation because of the selection and arrangement of its\ncontent; the same criteria also apply to databases. One of\nthe current trends for training foundation models seems to\ngo in the opposite direction. Although a certain degree of\ndata pre-processing is always present, the apparent tendency,\nat least in the early days of foundational models, has been\nto collect as much data as possible, for example, from the\nWeb. This approach threatens the requirement of making\na careful and original selection or arrangement. Moreover,\nfine-tuning models by means of training sets for specific\ndomains are more likely to be eligible for protection as col-\nlective works. Still, this interpretation does not seem to\ncover all foundation models' training sets.\nOn the other hand, single training samples are often pro-\ntected under copyright law (Bandy & Vincent, 2021). Even\nthough the training goal aims to compress batches of sam-\nples at a time, thus potentially leading to a compression that\nis optimal for a subset of works when considered together\nbut not when considered separately, the single works can\nstill be decompressed from the resulting model, at least in\nprinciple. This suggests that the model's weights can be\ninterpreted as a copy (or a derivative work) of all the inde-\npendent training samples, and not only of the training set as\na whole."}, {"title": "4. Implications", "content": "Interpreting the model's weights as a copy or a derivative\nwork of protected works leads to two crucial implications.\nFirst, it provides a legal framework to understand them,\nremoving the veil of uncertainty surrounding this issue. Al-\nthough asserting copyright protection for weights as a deriva-\ntive work presents challenges due to the absence of valid\nauthorship (Otero, 2021), it is possible to safeguard them\nby viewing the file with the model's weights as a database.\nIndeed, they can be considered as a collection of floating"}, {"title": "5. Related Work", "content": "5.1. Deep Learning and Information Theory\nInformation theory is underpinning machine learning\n(MacKay, 2003). Recently, there has been considerable\ninterest in the intersection between compression and neural\nnetworks, either to learn efficient data compression algo-\nrithms (Goyal et al., 2019; Yang et al., 2023) or to compress\nthe learned models themselves (Choudhary et al., 2020;\nPolino et al., 2018). The problem of training data com-\npression has been analyzed from the point of view of the\ninformation bottleneck principle (IB) (Tishby et al., 1999),\nwhich provides a methodology for extracting a quantitative\nmeasurement of \"meaningful\u201d and \u201crelevant\" information.\nIn particular, IB can be used to derive an optimal theoreti-\ncal limit for training neural networks in terms of trade-off\nbetween compression and prediction (Tishby & Zaslavsky,\n2015). From this perspective, stochastic gradient descent\ncan be seen as composed of two distinct phases: a \"fit-\nting\" phase in which the mutual information between the\nweights and the target output increases and a \"compres-\nsion\" phase in which the mutual information between the\nweights and the input decreases (Shwartz-Ziv & Tishby,\n2017). However, the latter phenomenon might not be an\nintrinsic feature of deep neural networks, but only the result\nof the presence of nonlinearities in the networks (Saxe et al.,\n2018). An important recent paper in this context is (2024),\nin which Schwartz-Ziv and LeCun discuss IB in relation to\nself-supervised learning, especially in the context of multi-\nmodal learning; this work is particularly relevant given the\nprominence of these techniques in training state-of-the-art\nfoundation models.\n5.2. Generative AI and Copyright\nThere has been a long-standing interest in the copyright is-\nsues around generative AI (Butler, 1982; Samuelson, 1986).\nDifferent legal issues are at play when considering the entire\ngenerative-AI supply chain (Lee et al., 2024). Whether train-\ning neural networks on protected data is lawful has been\nhighly debated across different national legislations (Lemley\n& Casey, 2021), e.g., U.S. fair use (Henderson et al., 2023;\nSobel, 2017), EU text and data mining exceptions (Geiger\net al., 2018; Sobel, 2021), and others (Guadamuz, 2024;\nSamuelson, 2021). The other main debate has focused on\nwhether a machine-generated work is protected by copyright\nor not (Craig & Kerr, 2019; Gervais, 2020; Grimmelman,\n2016) and on the question of who might own its ownership\nin the future (Bonadio & McDonagh, 2020; Franceschelli\n& Musolesi, 2022). However, other issues have also been\nconsidered, such as whether the model output can infringe\nthe reproduction right (Gillotte, 2020; Vyas et al., 2023)\nor how the trained model can be protected (Otero, 2021;\nSlowinski, 2021). Our analysis mainly concerns the latter"}, {"title": "6. Conclusion", "content": "Motivated by the phenomenon of data memorization in foun-\ndation models, we have proposed a perspective we termed\nas training-as-compressing for examining copyright issues.\nStarting from a thought experiment and a theoretical discus-\nsion, we have suggested interpreting self-supervised learn-\ning as data compressing and the model's weights as a com-\npressed version of the entire training set. From a copyright\nperspective, this framing has led us to consider the weights\nas a reproduction or a derivative work of training data, which\nusually contain protected works. We have discussed poten-\ntial protection that can be applied to the model's weights\nconsidering the underlying direct relationship between the\nAI-generated outputs and the protected training data.\nThe analysis conducted in this paper predominantly raises\nadditional questions rather than providing unequivocal so-\nlutions. We believe that a multi-disciplinary analysis of\nthese problems and implications from the point of view of\ninformation theory is of fundamental importance for prac-\ntitioners and researchers from both the technological and\nlegal points of view. Our research agenda also includes a\nrigorous formalization of the problem as a basis for rigorous\nlegal analysis of this complex yet fascinating area."}]}