{"title": "BioBridge: Unified Bio-Embedding with Bridging Modality in Code-Switched EMR.", "authors": ["JANGYEONG JEON", "SANGYEON CHO", "DONGJOON LEE", "CHANGHEE LEE", "JUNYEONG KIM"], "abstract": "Pediatric Emergency Department (PED) overcrowding presents a significant global challenge, prompting the need for efficient solutions. This paper introduces the BioBridge framework, a novel approach that applies Natural Language Processing (NLP) to Electronic Medical Records (EMRs) in written free-text form to enhance decision-making in PED. In non-English speaking countries, such as South Korea, EMR data is often written in a Code-Switching(CS) format that mixes the native language with English, with most code-switched English words having clinical significance. The BioBridge framework consists of two core modules: \u201cbridging modality in context\u201d and \u201cunified bio-embedding.\u201d The \u201cbridging modality in context\" module improves the contextual understanding of bilingual and code-switched EMRs. In the \u201cunified bio-embedding\u201d module, the knowledge of the model trained in the medical domain is injected into the encoder-based model to bridge the gap between the medical and general domains. Experimental results demonstrate that the proposed BioBridge significantly performance traditional machine learning and pre-trained encoder-based models on several metrics, including F1 score, area under the receiver operating characteristic curve (AUROC), area under the precision-recall Curve (AUPRC), and Brier score. Specifically, BioBridge-XLM achieved enhancements of 0.85% in F1 score, 0.75% in AUROC, and 0.76% in AUPRC, along with a notable 3.04% decrease in the Brier score, demonstrating marked improvements in accuracy, reliability, and prediction calibration over the baseline XLM model. The source code will be made publicly available.", "sections": [{"title": "I. INTRODUCTION", "content": "OVERCROWDING in Emergency Departments (EDs), including Pediatric Emergency Departments (PEDs), is an urgent problem that needs to be handled globally and immediately, including in South Korea. It is due to lead ineffi- cient use of ED resources and the overworked healthcare pro- fessionals [1]\u2013[4]. Providing timely and accurate treatment is essential, especially for children who are more susceptible to infection than adults [5]. However, since children tend to visit the ED frequently, this further aggravates the overcrowding problem in PED [6].\nAs medical institutions worldwide adopt EMRs, vast amounts of healthcare data are stored digitally [7]\u2013[10]. EMR contains information about a patient's Present Illness (PI), past medical history, and textual data such as treatment and disposition information [7], [9], [10]. By using Natural Language Processing (NLP) techniques to EMR data, clin- icians can receive real-time decision support and increase the efficiency of emergency medical services by optimiz- ing the assessment of patient severity and urgency in the ED [11]\u2013[13]. Traditional approaches to applying NLP to EMR data have focused on Machine Learning (ML) [14]\u2013 [16] and Word2Vec-based approaches to extract context-free word embeddings [17]\u2013[19]. Recently, the focus has shifted to improving predictive modeling performance by training EMRs on pre-trained encoder-based models [20]\u2013[22] such as BERT [23]. Despite the potential benefits of EMRs, their practical application in healthcare decision support systems has been limited by challenges in applying NLP to extract key information, which still needs to be overcome [24]\u2013[26].\nFirst, EMRs consist of unstructured, free-text clinical notes written by physicians during patient examinations, often using varied wording to describe the same dis- ease [27]\u2013[29]. For example, medical symbols such as \"Cough/Sputum/Rhinorrhea\u201d are often used as \u201cC/S/R\" in EMRs, which are primarily used to record a patient's res- piratory disease [30], [31]. Extensive text preprocessing is required to apply natural language processing techniques to these EMRs, and NLP toolkits such as the Unified Medi- cal Language System (UMLS) [32], [33] have been widely adopted [32], [34], [35]. However, these standardized prepro- cessing methods incur high costs due to continuous resource maintenance and are often unavailable in languages other than English, including Korean. To mitigate these challenges, we have identified commonly used medical symbols and abbre- viations in EMRs, proposed a straightforward preprocessing method to decode them, and integrated this preprocessed text with an ML-based [36], [36]\u2013[39] and pre-trained encoder- based model [23], [40]\u2013[43].\nSecond, EMR data used in non-English speaking countries are recorded in Code-Switching (CS) form, where English and the native language are mixed [44], [45]. CS refers to using two languages in the sentence and is prevalent not only in EMRs but also in multicultural countries, social media, and online platforms [46]\u2013[48]. Unfortunately, there is a lack of research on CS in non-English speaking coun- tries, including languages such as Korean, Japanese, Chi- nese, and Arabic [48]\u2013[51]. In particular, few studies have explored the application of encoder-based models to code- switched EMR datasets from non-English-speaking coun- tries [44], [45], [52], [53]. The EMR dataset used in our study is also code-switched to Korean-English. To bridge this research gap, we assume that language-specific modalities (Korean, English) exist in each CS sentence and propose a novel method of \"bridging modality in the context\" of learning each modality effectively by a pre-trained encoder. This approach is motivated by research in multimodal learn- ing, where different inputs (visual, text, and audio input) are defined as modalities in a transformer and utilized for learning [54]. In the following Section II-B, we describe in detail the research related to multimodal learning.\nThird, most of the terms that are code-switched have clin- ical significance. Using pre-trained encoder base models in general domains in NLP and applying them to clinical notes is challenging, and many attempts have been made to solve it [20], [21], [55]. This is due to most encoder-based models are not pre-trained with domain-specific data but are pre- trained using general-domain data such as English Wikipedia, BooksCorpus [56], and CommonCrawl [57]. Moreover, Ko- rean is a very low-resourced language compared to English"}, {"title": "II. RELATED WORK", "content": "NATURAL LANGUAGE PROCESSING FOR ELECTRONIC MEDICAL RECORD\nThe increasing availability of Electronic Medical Records (EMRs) has led to extensive research into utilizing EMRs in Natural Language Processing (NLP) [14], [21], [60], [61]. Traditional methods for applying EMR data to NLP have evolved from Machine Learning (ML) [36]\u2013[39] and Word2Vec-based approaches [17]. Word2Vec-based ap- proaches have proven effective in identifying relationships between medical terms and symptoms by facilitating word encoding within a vector space where semantically similar words are closely aligned [17], [18]. However, Word2Vec is limited by its inability to account for context, and method- ologies such as Sent2Vec [62] have attempted to overcome this limitation by extending the Continuous Bag of Words (CBOW) approach of Word2Vec to increase the window size to whole sentences. In particular, BioSent2Vec [63], trained on over 30 million documents from academic articles in PubMed and clinical notes in the MIMMIC-III Clinical Database, has been able to generate useful word embeddings through biomedical text mining. In this paper, we use the pre-trained BioSent2Vec as a word-level feature extractor to derive English medical word embeddings.\nEncoder-based models such as BERT [23] are gaining prominence in modern NLP due to their capacity to com-"}, {"title": "III. PROPOSED METHOD", "content": "PROBLEM FORMULATION\nLet X = {x}_{i=0}^{N} denote PI, the text input contained in the EMR dataset, and let X^{Eng} = {x^{Eng}_i}_{i=0}^{N_{Eng}} denote the English text input, X^{Eng} \u2286 X. The label Y = {y_i}_{i=0}^{N}, y_i \u2208 {0, 1} denote Non-emergency cases or Emergency cases.\nTokenized text input for X is defined as X^{tok} = {x^{tok}_i}_{i=0}^{N_{tok}}. For instance, if a tokenized input x^{tok} consists of Korean followed by English, in that order, it is denoted as:\nx^{tok} = {[CLS], [tokens]^{kor}, [tokens]^{eng}, [SEP]} (1)\nwhere, \u201c[CLS]", "[SEP]\" denotes the last token. [tokens]^{kor} denotes the tokens for the tokenized Korean sentence within x^{tok}, and [tokens]^{eng} denotes the tokens for the tokenized English sentence within x^{tok}.\nWe denote the batch size as b, the total number of tokens within a batch as n, and the number of English words as m. Additionally, we detail three components input to the pre- trained encoder M.\nFirstly, the token embedding layer of M, denoted as f_{t}, is defined as follows:\nf_{t}({x^{tok}_i}_{i=0}^{b}) \u2208 \u211d^{n \u00d7 h_m} (2)\nWhere, h_m is the hidden dimension of M.\nSecondly, the medical feature extractor, BioSent2Vec f_{B} [63], is defined as:\nf_{B}({x^{Eng}_i}_{i=0}^{m}) \u2208 \u211d^{m \u00d7 h_B} (3)\nWhere, h_B is the hidden dimension of f_{B}, and \u03b8 is a fixed parameter.\nThirdly, the Fully Connected (FC) Layer f_{f}, designed to map dimensions, is defined as:\nf_{f} : \u211d^{m \u00d7 h_B} \u2192 \u211d^{m \u00d7 h_m} (4)\nChallenges. In practice, applying natural language pro- cessing (NLP) to EMRs constructed in non-English-speaking regions presents two primary challenges: (1) Code-switched\"\n    },\n    {\n      \"title\": \"B. BIOBRIDGE FRAMEWORK\",\n      \"content\": \"To address the challenges described above, we introduce a two-stage training approach for the BioBridge:\n\u2022 Bridging modality in context module: We add a seg- ment token before each modality within the text to enable the pre-trained encoder M [23], [40]-[43] to effectively distinguish between the modalities (Korean, English) in the given text input X.\n\u2022 Unified bio-embedding module: We extract features for English medical words using f_{B} [63] and unify these features into pre-trained encoder M, enhancing the en- coder's applicability to the medical domain.\nWe implement this methodology by training the pre-trained encoder M using the two training modules. Fig.1 shows an overview of the proposed BioBridge framework.\"\n    },\n    {\n      \"title\": \"1) Bridging modality in context\",\n      \"content\": \"The pre-trained encoder M must learn to distinguish between Korean and English modalities in the text input X. To facili- tate this, we prepend a segment token to each modality in the tokenized input X^{tok}. For instance, by adding segment tokens to x^{tok}, can be denoted as:\nx^{pri} = {[CLS], [B-K], [tokens]^{kor}, [B-E], [tokens]^{eng}, [SEP]} (5)\nWhere \u201c[B-K]": "nd \u201c[B-E]\u201d are segment tokens that differ- entiate between Korean and English, respectively. Addition- ally, with the inclusion of segment tokens, the dimension of token embedding layer f_{t} is redefined as follows:\nf_{t}({x^{pri}_i}_{i=0}^{b}) \u2208 \u211d^{(n+s) \u00d7 h_m} (6)\nwhere s denotes the number of added segment tokens. The related framework is shown in Fig.1 (a)."}, {"title": "2) Unified bio-embedding", "content": "In the unified bio-embedding module, we utilize f_{B} to ex- tract English medical features and integrate them into the pre-trained encoder M. The tokenized inputs {x^{pri}_i}_{i=0}^{b} con- structed during the bridging modality in context module are segmented at the token level, while f_{B} is optimized for word- level learning. To address this, we reassemble tokenized En- glish tokens at the token level back into word-level constructs to form {x^{Eng_b}_i}_{i=0}^{m}. For example, tokens separated by the tokenizer, such as \u201cvomit\u201d and \u201c##ing,\u201d are reconstructed into \"vomiting\u201d as illustrated in Fig.1 (b).\nWe then use the word-level reconstructed {x^{Eng_b}_i}_{i=0}^{m} as input to f_{B}, as specified in equation (3), to extract medical features. To integrate these extracted medical features into M, the output dimension of f_{B} must be mapped \u211d^{h_B} \u2192 \u211d^{h_w}. This mapping is facilitated using f_{f}, as described in equation (4).\nThe output from f_{f} produces token-level medical features, which are then integrated into the unified bio-embedding fe, which can be denoted as:\nf_{\u03b8}(f_{t}, f_{B}, f_{f}) \u2208 \u211d^{m \u00d7 h_m} (7)\nWhere, h_m is the hidden dimension of M. Ultimately, we integrate unified bio-embedding into the pre-trained encoder \u039c."}, {"title": "IV. EXPERIMENTS", "content": "In our experiments, we evaluate BioBridge's performance using the proposed EMR dataset. Additionally, all our ex- periments are designed for a classification task in which the predictive model, such as machine learning-based [36], [36]- [39] and pre-trained encoder-based models [23], [40]-[43], distinguishes between emergency and non-emergency cases [59]."}, {"title": "A. DATASET", "content": "The benchmark dataset used in this paper comprises elec- tronic medical records (EMRs) from the Department of Pe- diatrics at the University Hospital of South Korea. The study population includes patients under 18 years of age who visited the Department of Pediatrics between January 1, 2012, and December 31, 2021. The EMR dataset includes Present Ill- ness (PI) notes [76], urine and blood test results, and records of emergency and non-emergency cases [59]. Present Illness (PI) is an unstructured free-text clinical note written by a nurse or doctor about the main symptoms observed during the patient's visit to the PED. Additionally, Emergency and Non- emergency cases are data annotated according to whether the patient's visit to the PED was an emergency requiring treat- ment. Detailed dataset statistics are recorded in Table 1, and Section IV-B describes the preprocessing of the EMR data. SectionIV-B provides a detailed description of the emergency and non-emergency cases."}, {"title": "B. PREPROCESSING OF ELECTRONIC MEDICAL RECORD", "content": "We constructed an electronic medical record (EMR) dataset of patients who visited Pediatric Emergency Departments (PEDs) by excluding patients with no disposition record and those with missing information. Present Illness (PI) notes contain various medical symbols and abbreviations. For ex- ample, in Fig.2, the medical symbol \"C/S/R\" stands for \"Cough/Sputum/Rhinorrhea,\u201d and the abbreviation \u201cBT\u201d stands for \"Body Temperature\". We decoded these medical symbols and abbreviations, as shown in Fig.2. Additionally, we preprocessed the PI notes by systematically separating Korean, English, numeric values, and special characters when they are followed by a space and introducing a space. We split the preprocessed dataset into training, development, and test sets with ratios of 0.64, 0.16, and 0.20, respectively. In this paper, we integrated PI texts with simple preprocessing into a predictive model."}, {"title": "C. EMERGENCY AND NON-EMERGENCY CASES", "content": "The term \"Emergency\" refers to an abrupt and often unfore- seen occurrence necessitating prompt intervention to reduce potential negative outcomes [59]. Cases classified as emer- gencies encompass patients who received interventions such as blood tests, urinalysis, intravenous hydration, nebulizer treatments, or immediate drug administration in the Pedi- atric Emergency Department (PED), as well as those who required hospital admission. Conversely, \u201cNon-emergency\" describes conditions that do not demand urgent medical care, including immediate evaluation, diagnosis, or treatment. Non-emergency cases typically involve patients who were released from the PED without undergoing urgent tests or receiving immediate medication or those who departed with only prescription medication at discharge."}, {"title": "D. BASELINES", "content": "Machine learning based models\nFor the Machine Learning (ML) Based Model, we con- structed baseline models using one statistical method (Logis- tic Regression [37]) and three tree-based ensemble methods (Gradient Boosting [38], XGBoost [36], and Random Forest [39]).\nLogistic regression. Logistic regression [37] is a statisti- cal model that estimates probabilities using a logistic function widely used for binary classification tasks. It models the probability of a default class (such as an emergency case) based on one or more predictor variables (features).\nGradient Boosting. Gradient Boosting [38] is an ensem- ble technique that builds models sequentially by correcting the previous models' errors. It uses decision trees as weak learners and focuses on minimizing a loss function, such as mean squared error for regression or log loss for classifica- tion, by optimizing the errors of successive models.\nXGBoost. XGBoost (Advanced Gradient Boosting) [36] enhances traditional gradient boosting by incorporating ad- vanced features such as regularization to prevent overfitting and a more sophisticated tree pruning and splitting approach.\nRandom Forest. Random Forest [39] is an ensemble model consisting of multiple regression trees. It combines several classification trees and trains each on a slightly differ- ent set of dataset instances, splitting nodes in each tree con- sidering a limited number of variables. The final predictions of the random forest are made by averaging the predictions of each tree."}, {"title": "2) Encoder based models", "content": "For the encoder base model, we established baselines us- ing both the Korean-specific and the Multilingual-specific encoder-based models.\nKOBERT. KOBERT [40] uses the SentencePiece tokenizer and character-level tokenization to reflect and handle the complexity of the Korean language. It is trained on Korean Wikidata, which comprises 5 million sentences and 54 million words, with a vocabulary of 8,002 tokens.\nKR-BERT. KR-BERT [41] utilizes the WordPiece tok- enizer to capture Korean morphological complexities. Un- like KOBERT, KR-BERT includes both sub-character and character-level tokenization. For our experiments, we used the KR-BERT-Medium variant. KR-BERT-Medium is an ex- tension of the training data used in the original KR-BERT by adding Korean Wikipedia text, news articles, and Korean online comment datasets. The dataset used for training is 12.37 GB, and the vocabulary size is 20,000.\nmBERT. Multilingual BERT [23] utilizes Masked Lan- guage Modeling (MLM), BERT's pre-training objective, to train on Wikipedia articles across 104 languages. It uses a shared WordPiece vocabulary comprising 110,000 words to facilitate multilingual processing. This approach enables mBERT to process text in multilingual languages effectively.\nXLM. XLM [42] utilizes a cross-lingual pre-training method on a translated parallel corpus and utilizes Byte-Pair Encoding (BPE) for multilingual languages to facilitate cross- lingual transfer learning. This approach has enhanced the performance of mBERT.\nXLM-R. XLM-R [43] enhanced the XLM framework by training on 2.5 TB of CommonCrawl data, a significant in- crease over the dataset used for XLM. This model leverages the robust training approach of RoBERTa [77], an optimized version of BERT, which includes dynamic masking and re- moves the next sentence prediction objective. It also supports 100 languages."}, {"title": "E. PERFORMANCE METRICS", "content": "Our experiments evaluate model performance using several key metrics commonly used in classification tasks: F1 score,"}, {"title": "F. TRAINING DETAILS", "content": "In our experiments, we developed a model using the PyTorch [78] and Transformer [79] libraries and utilized a pair of NVIDIA RTX A6000 GPUs for training."}, {"title": "1) Training details on machine learning based model", "content": "We used the Term Frequency-Inverse Document Frequency (TF-IDF) to train a Machine Learning (ML) based model. TF-IDF is determined by multiplying two metrics: the Term Frequency (TF) score, which reflects the frequency of a word within a document, and the Inverse Document Frequency (IDF) score, which gauges the word's rarity across the entire document corpus. A higher TF-IDF score signifies greater importance of the word for prediction. We applied TF-IDF by treating each patient's Present Illnes (PI) as a single docu- ment."}, {"title": "2) Training details on encoder based model", "content": "In our experiments, we initialized the pre-trained encoder us- ing the checkpoints of KR-BERT, KoBERT, mBERT, XLM, and XLM-R. We also used the \u201c[CLS]\u201d token as the final sen- tence embedding. During training, we saved the checkpoint with the highest f1 score on the development set and evaluated its performance on the test set. Considering the extensive length of most Present Illness (PI) descriptions, we used the maximum sequence length of 512 across all experiments."}, {"title": "G. RESULTS", "content": "Table 3 shows the evaluation results on the test dataset. The comparison between machine learning-based models and encoder-based models (Korean and multilingual encoder- based models) clearly shows the superiority of encoder-based models. For the Machine Learning models, while Gradient Boosting achieved the highest F1 score of 66.90, the KR- BERT significantly improved, reaching an F1 score of 78.30. The improvement extends to AUROC, AUPRC, and Brier score metrics, revealing that Korean encoder-based models not only boost prediction accuracy but also refine the calibra- tion of those predictions. The ML-based model was trained based on TF-IDF, so it would have performed worse than the encoder-based model that considered the context.\nIn the category of Korean-specific encoder-based models, *BioBridge-KR-BERT significantly outperforms the base- line KR-BERT across all evaluated metrics. The F1 score im- proves by 1.1%, AUROC by 2.52%, AUPRC by 2.16%, and the Brier score decreases by 0.95%. These results indicate that *BioBridge-KR-BERT enhances classification performance across various thresholds. Moreover, BioBridge demonstrates superiority in the accuracy of probabilistic predictions. This trend aligns with the performance gains observed between BioBridge-KoBERT and the base KoBERT model. Demon- strate the efficacy of BioBridge with Korean-based encoder models.\nFor the multilingual-specific encoder-based model, *BioBridge-XLM showed an increase of 0.85% in the F1 score, 0.75% in AUROC, and 0.76% in AUPRC compared to the baseline XLM. Notably, the Brier score decreased by 3.04%, signifying that BioBridge enhanced the prediction accuracy and the reliability and calibration of the model's outputs. Moreover, *BioBridge-mBERTuncased achieved the lowest Brier score.\nBioBridge's consistent enhancement in performance across various encoder-based models attests to its robust- ness and adaptability to different encoder architectures and languages. These outcomes endorse the BioBridge method as a powerful strategy for classification tasks in the medical domain, notably in settings dealing with bilingual EMRs."}, {"title": "V. ABLATION STUDY", "content": "In this section, we perform extensive ablation studies to support our model design. For all experiments, we establish a multilingual-based model as our baseline. The BioBridge framework was designed in two modules. First the \"bridg- ing modality in context\" module, which bridges modalities (Korean, English). This module is essential due to it connects each modality (Korean, English) in the CS context with a token. Excluding the token that bridges each modality in the CS sentence becomes a vanilla encoder-based model. The second module, \"unified bio-embedding,\" aims to in- fuse the encoder-based model, which lacks medical domain knowledge, with expertise in English medical terminology. The \"unified bio-embedding\u201d module extracts medical fea- tures from the medical feature extractor ff and incorporates them into the encoder base model. The vanilla encoder-based model is obtained by excluding the \"unified bio-embedding\" module. The optimal hyperparameters for the test set are shown in Table 4."}, {"title": "A. RESULTS", "content": "We show insightful results for the BioBridge framework through extensive ablation studies. The results of the abla- tion study are shown in Table 5. Additionally, The optimal hyperparameters for the test set are shown in Table 4."}, {"title": "Bridging modality in context", "content": "Integrating the \"bridging modality in context\" module into our baseline multilingual encoder-based model led to notable improvements across various performance metrics. In XLM, F1 score increased by 0.26%, AUROC increased by 0.3%, AUPRC increased by 0.33%, and Brier score decreased by 0.17%. These results highlight the importance of this module in enhancing com- prehension of the context within code-switched scenarios."}, {"title": "Uninifed bio-embedding", "content": "Adding the \"unified bio- embedding\" module to our baseline multilingual encoder- based model also consistently enhanced performance on al- most all metrics. In mBERTcased, we observed a 0.62% de- crease in the F1 score but a 0.25% increase in AUROC and a 0.57% increase in AUPRC, demonstrating robust perfor- mance at all thresholds. Notably, the Brier score decreased by 1.34%, affirming enhanced reliability and accuracy in proba- bilistic predictions. This improvement demonstrates that the \"unified bio-embedding\u201d module can effectively inject med- ical knowledge into a multilingual-based model trained on a general domain.\nThe results of these ablation studies highlight the impor- tance of both modules in improving the encoder base model's ability to understand bilingual EMR accurately."}, {"title": "VI. LIMITATION", "content": "This paper proposes applying Natural Language Process- ing (NLP) to Electronic Medical Records (EMRs) to im- prove decision-making processes in pediatric emergency de- partments (PEDs). Our work has three primary limitations. Firstly, data privacy and security present challenges. EMRs contain sensitive personal information, necessitating strict compliance with privacy laws and regulations, which restricts the free sharing of data and limits its broader application. Secondly, the quality and consistency challenges. EMR data is written in free text, which makes the data inconsistent. Therefore, data input in different formats may hinder the model's ability to learn uniformly, potentially affecting the generalizability of the results. Thirdly, Model Interpretability challenges. The complexity of the Transformer model archi- tecture used in this work reduces its interpretability. In a healthcare setting, where decisions can have significant im- plications for patient outcomes, the inability to fully explain how model predictions are made can severely hinder trust and adoption.\nIn future work, we plan to increase the generalization performance of our model in healthcare settings by obtaining EMR datasets from more hospitals while maintaining com- pliance. Furthermore, while this paper focuses on Korean- English code-switched EMRs, we intend to extend our ap- proach to include code-switched EMRs from other languages with English to explore its broader applicability in multi- lingual healthcare environments. Additionally, to make the model more transparent and interpretable, we plan to use methods such as SHapley Additive exPlanations (SHAP) [80] to provide an interpretation of the model."}, {"title": "VII. CONCLUSION", "content": "In this paper, we proposed BioBridge, a framework that uni- fied bio-embedding with bridging modality in code-switched EMR. This approach achieved state-of-the-art performance in classifying emergency cases in Korean-English code- switched EMRs by fine-tuning a transformer encoder-based model. This approach addresses the critical challenges of overcrowding in PEDs and pioneers using code-switched EMRs with multilingual NLP techniques, setting a new stan- dard for future applications."}]}