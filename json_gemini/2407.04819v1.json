{"title": "RPN: Reconciled Polynomial Network", "authors": ["Jiawei Zhang"], "abstract": "In this paper, we will introduce a novel deep model named Reconciled Polynomial Network (RPN) for deep function learning. RPN has a very general architecture and can be used to build models with various complexities, capacities, and levels of completeness, which all contribute to the correctness of these models. As indi-cated in the subtitle, RPN can also serve as the backbone to unify different base models into one canonical representation. This includes non-deep models, like probabilistic graphical models (PGMs) - such as Bayesian network and Markov network - and kernel support vector machines (kernel SVMs), as well as deep models like the classic multi-layer perceptron (MLP) and the recent Kolmogorov-Arnold network (KAN). Technically, inspired by the Taylor's Theorem, RPN proposes to disentangle the underlying function to be inferred into the inner product of a data expansion func-tion and a parameter reconciliation function. Together with the remainder func-tion, RPN accurately approximates the underlying functions that governs data distributions. The data expansion functions in RPN project data vectors from the input space to a high-dimensional intermediate space, specified by the expansion functions in definition. Meanwhile, RPN also introduces the parameter recon-ciliation functions to fabricate a small number of parameters into a higher-order parameter matrix to address the \u201ccurse of dimensionality\u201d problem caused by the data expansions. In the intermediate space, the expanded vectors are polynomi-ally integrated and further projected into the low-dimensional output space via the inner product with the reconciled parameters generated by these parameter reconciliation functions. Moreover, the remainder functions provide RPN with additional complementary information to reduce potential approximation errors. We conducted extensive empirical experiments on numerous benchmark datasets across multiple modalities, including continuous function datasets, discrete vision and language datasets, and classic tabular datasets, to investigate the effectiveness of RPN. The experimental results demonstrate that, RPN outperforms MLP and KAN with mean squared errors at least \u00d710\u22121 lower (and even \u00d710-2 lower in some cases) for continuous function fitting. On both vision and language bench-mark datasets, using much less learnable parameters, RPN consistently achieves higher accuracy scores than Naive Bayes, kernel SVMs, MLP, and KAN for dis-crete image and text data classifications. In addition, equipped with the proba-bilistic data expansion functions, RPN learns better probabilistic dependency re-lationships among variables and outperforms other probabilistic models, including Naive Bayes, Bayesian networks, and Markov networks, for learning on classic tabular benchmark datasets. Reconciled Polynomial Network (RPN) proposed in this paper provides the op-portunity to represent and interpret current machine and deep learning models as sequences of vector space expansions and parameter reconciliations. These func-tions can all deliver concrete physical meanings about both the input data and model parameters. Furthermore, the application of simple inner-product and sum-mation operations to these functions significantly enhances the interpretability of RPN. This paper presents not only empirical experimental investigations but also in-depth discussions on RPN, addressing its interpretations, merits, limitations, and potential future developments. What's more, to facilitate the implementation of RPN-based models, we have developed and released a toolkit named TINYBIG. This toolkit encompasses all functions, modules, and models introduced in this paper, accompanied by com-prehensive documentation and tutorials. Detailed information about TINYBIG is available at the project's GitHub repository and the dedicated project webpage, with their respective URLs provided above.", "sections": [{"title": "1 Introduction", "content": "Over the past 70 years, the field of artificial intelligence has experienced dramatic changes in both the problems studied and the models used. With the emergence of new learning tasks, various machine learning models, each designed based on different prior assumptions, have been proposed to address these problems. As shown in Figure 1, we illustrate the timeline about three types of machine learning models that have dominated the field of artificial intelligence in the past 50 years, including probabilistic graphical models [37, 57, 39], support vector machines [12, 76, 8] and deep neural networks [66, 23]. Along with important technological breakthroughs, these models each had their moments of prominence and have been extensively explored and utilized in various research and application tasks related to data and learning nowadays. Besides these three categories of machine learning models, there are many other models (e.g., the tree based models and clustering models) that do not fit into these categories, but we will not discuss them in this paper and will leave them for future investigation instead. In this paper, we will introduce a novel deep model, namely Reconciled Polynomial Network (RPN), that can potentially unify these different aforementioned base models into one shared rep-resentation. In terms of model architecture, RPN consists of three component functions: data expansion function, parameter reconciliation function and remainder function. Inspired by the Taylor's theorem, RPN disentangles the input data from model parameters, and approximates the target functions to be inferred as the inner product of the data expansion function with the parameter reconciliation function, subsequently summed with the remainder function. Based on architecture of RPN, inferring the diverse underlying mapping that governs data distribu-tions (from inputs to outputs) is actually equivalent to inferring these three compositional functions. This inference process of the diverse data distribution mappings based on RPN is named as the function learning task in this paper. Specifically, the \u201cfunction\u201d term mentioned in the task name refers to not only the mathematical function components composing the RPN model but also the cognitive function of RPN as an intelligent system to relate input signals with desired output response. Function learning has been long-time treated as equivalent to the continuous function fitting and approximation for regression tasks only. Actually, in psychology and cognitive science, researchers have also used the function learning concept for modeling the mental induction pro-cess of stimulus-response relations of human and other intelligent subjects [9, 38], involving the acquisition of knowledge, manipulation of information and reasoning. In this paper, we argue that function learning is the most fundamental task in intelligent model learning, encompassing contin-uous function approximation, discrete vision and language data recognition and prediction, and cognitive and logic dependency relation induction. The following Section 2 will provide an"}, {"title": "2 Deep Function Learning", "content": "In this section, we will first introduce the concept of deep function learning task. After that, we will provide the detailed clarifications about how deep function learning differs from existing deep representation learning tasks. Based on this concept, we will further compare RPN, the deep function learning model proposed in this paper, with other existing non-deep and deep base models to illustrate their key differences."}, {"title": "2.1 What is Deep Function Learning?", "content": "As its name suggests, deep function learning, as the most fundamental task in machine learning, aims to build general deep models composed of a sequence of component functions that infer the relationships between inputs and outputs. These component functions define the mathematical pro-jections across different data and parameter spaces. In deep function learning, without any prior assumptions about the data modalities, the corresponding input and output data can also appear in different forms, including but not limited to continuous numerical values (such as continuous functions), discrete categorical features (such as images and language data), probabilistic variables (defining the dependency relationships between inputs and outputs), and others.\nDEFINITION 1 (Deep Function Learning): Formally, given the input and output spaces $\\mathbb{R}^{m}$ and $\\mathbb{R}^{n}$, the underling mapping that governs the data projection between these two spaces can be de-noted as:\n$\\begin{equation} f: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}.\\tag{1}\\end{equation}$\nDeep function learning aims to build a model $g$ as a composition of deep mathematical function sequences $g_{1}, g_{2},\u2026, g_{K}$ to project data cross different vector spaces, which can be represented as\n$\\begin{equation} g: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}, \\quad \\text{and} \\quad g = g_{1} \\circ g_{2} \\circ ... \\circ g_{k},\\tag{2}\\end{equation}$\nwhere the $circ$ notation denotes the component function integration and composition operators. The component functions $g_{i}$ can be defined on either input data or the model parameters.\nFor input $x \\in \\mathbb{R}^{m}$, if the output generated by the model can approximate the desired output, i.e.,\n$\\begin{equation} g(x \\vert w, \\theta) \\approx f(x),\\tag{3}\\end{equation}$\nthen model is $g$ can serve as an approximated mapping of $f$. Notations $w \\in \\mathbb{R}^{l}$ and $\\theta \\in \\mathbb{R}^{l'}$ denote the learnable parameters and hyper-parameters of the function learning model, respectively.\nBelow, we will further clarify the distinctions between deep function learning and current deep model-based data representation learning tasks. After that, we will compare our RPN model, which is grounded in deep function learning, against other existing base models."}, {"title": "2.2 Deep Function Learning vs Deep Representation Learning", "content": "As mentioned previously, the function learning tasks and models examined in this paper encompass not only continuous function approximation, but also discrete data classification and the induction of dependency relations. Besides the literal differences indicated by their names - representation learning is data oriented but function learning is model oriented - deep function learning significantly differs from the current deep representation learning in several critical perspectives discussed below.\n\u2022 Generalizability: Representation learning, to some extent, has contributed to the current fragmentation within the AI community, as data - the carrier of information - is collected, represented, and stored in disparate modalities. Existing deep models, specifically designed for certain modalities, tend to overfit to these modality-specific data representations in addi-tion to learning the underlying information. Applying a model proposed for one modality to another typically necessitates significant architectural redesigns. Recently, there have been efforts to explore the cross-modal applicability of certain models, e.g., CNNs for language and Transformers for vision, but replicating such cross-modality migration explorations across all current and future deep models is extremely expensive and unsustainable. Fur-thermore, to achieve the future artificial general intelligence (AGI), the available data in a single modality is no longer sufficient for training larger models. Deep function learn-ing, without any prior assumptions on data modalities, will pave the way for improving the model generalizability. These learned functions should demonstrate their generalizability and applicability to multi-modal data from the outset, during their design and investigation phases.\n\u2022 Interpretability: Representation learning primarily aims to learn and extract latent patterns or salient features from data, aligning with the technological advancements in data science and big data analytics over the past two decades. However, the learned data representations often lack interpretable physical meanings, rendering most current AI models to be black boxes. In contrast, to realize the goal of explainable AI (XAI), greater emphasis must be placed on developing new model architectures with concrete physical meanings and mathematical interpretations in the future. The RPN based deep function learning, on the other hand, aims to learn compositional functions with inherent physical interpretability for building general-purpose models across various tasks, thereby bridging the interpretability gap of current and future deep models."}, {"title": "2.3 RPN vs Other Base Models", "content": "Figure 2 compares the RPN model proposed for deep function learning with several base models in terms of mathematical theorem foundations, formula representations, and model architectures. The top three Plots (a)-(c) describe the non-deep base models: Bayesian Networks, Markov Networks, and Kernel SVMs; while the Plots (d)-(i) at the bottom illustrate the architectures of deep base models: MLPs, KANs, and RPN. For MLPs and KANs, Plots (d)-(e) and (g)-(h) illustrate their two-layer and three-layer architectures, respectively. Similarly, for RPN, we present its one-layer and three-layer architectures in Plots (f) and (i).\nBased on the plots shown in Figure 2, we can observe significant differences of RPN compared against these base models, which are summarized as follows:\n\u2022 RPN vs Non-Deep Base Models: Examining the model plots, we observe that all these base model architectures can be represented as graph structures composed of variables and their relationships. The model architecture of the Markov network is undirected, while that of the Bayesian network is directed. Similarly, for MLP, KAN, and RPN, although we haven't shown the variable connection directions, their model architecture are also directed, flowing from bottom to top. The model architectures of both Markov network and Bayesian network consist of variable nodes that correspond only to input features and output labels. In contrast, for kernel SVM, MLP, KAN, and RPN, their model architectures involve not only nodes representing inputs and outputs, but also those representing expansions and hid-den layers. Both RPN and kernel SVM involve a data expansion function to project input data into a high-dimensional space. However, their approaches diverge thereafter. Kernel SVM directly defines parameters within this high-dimensional space to integrate expansion vectors into outputs. In contrast, RPN fabricates these high-dimensional parameters via a reconciliation function from a reduced set of parameters instead.\n\u2022 RPN vs Deep Base Models: The difference between RPN and MLP is easy to observe. MLP involves neither data expansion nor parameter reconciliation. Instead, they apply activation functions to neurons after input integration, which is parameterized by neuron connection weights. Unlike MLPs with predefined, static activation functions, the recent"}, {"title": "3 Notations and Background Knowledge on Taylor's Theorem", "content": "This section first introduces the notation system used throughout this paper. Based on the notations, we then briefly present Taylor's theorem as the preliminary knowledge of the RPN model, which will be introduced in the following Section 4."}, {"title": "3.1 Notation System", "content": "In the sequel of this paper, we will use the lower case letters (e.g., $x$) to represent scalars, lower case bold letters (e.g., $\\mathbf{x}$) to denote column vectors, bold-face upper case letters (e.g., $X$) to denote matrices and high-order tensors, and upper case calligraphic letters (e.g., $\\mathcal{X}$) to denote sets. Given a matrix $X$, we denote $X(i, :)$ and $X(:, j)$ as its $i$th row and $j$th column, respectively. The $(i \\text{th}, j \\text{th})$ entry of matrix $X$ can be denoted as $X(i, j)$. We use $X^{T}$ and $\\mathbf{x}^{T}$ to represent the transpose of matrix $X$ and vector $\\mathbf{x}$. For vector $\\mathbf{x}$, we represent its $L_{p}$-norm as $|\\|\\mathbf{x}\\||_{p} = (\\Sigma_{i}|x(i)|^{p})^{\\frac{1}{p}}$. The Frobenius-norm of matrix $X$ is represented as $|\\|X\\||_{F} = (\\Sigma_{i,j} |X(i, j)|^{2})^{\\frac{1}{2}}$. The elementwise product of vectors $\\mathbf{x}$ and $\\mathbf{y}$ of the same dimension is represented as $\\mathbf{x} \\odot \\mathbf{y}$, their inner product is represented as $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$, and their Kronecker product is $\\mathbf{x} \\otimes \\mathbf{y}$. The elementwise product and Kronecker product operators can also be applied to matrices $X$ and $Y$ as $X \\odot Y$ and $X \\otimes Y$, respectively."}, {"title": "3.2 Taylor's Theorem for Univariate Function Approximation", "content": "Taylor's theorem approximates a d-times differentiable function around a given point using poly-nomials up to degree d, commonly referred to as the dth-order Taylor polynomial. In this section, we first introduce Taylor's theorem for univariate functions, which also generalizes to multivariate and vector valued functions. We will briefly describe its extension to multivariate functions in the subsequent Subsection 3.3, and then introduce the RPN model designed based on Taylor's theorem with vector valued functions in Section 4.\nTHEOREM 1 (Taylor's Theorem): Let $d \\geq 1$ be an integer and let function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ be $d$ times differentiable at the point $a \\in \\mathbb{R}$. As illustrated in Figure 3, then there exists a function $h_{a}: \\mathbb{R} \\rightarrow \\mathbb{R}$ such that\n$\\begin{aligned} f(x) = & \\frac{f(a)}{0!}(x - a)^{0} + \\frac{f'(a)}{1!}(x - a)^{1} + \\frac{f''(a)}{2!}(x - a)^{2} + ... + \\frac{f^{(d)}(a)}{d!}(x - a)^{d} + R_{d}(x), \\\\ = & \\sum_{i=0}^{d} \\frac{f^{(i)}(a)}{i!}(x - a)^{i} + R_{d}(x),\\tag{4}\\end{aligned}$\nIn the equation, $R_{a}(x)$ is also normally called the \u201cremainder\u201d term and can be represented as\n$\\begin{equation} R_{d}(x) = h_{a}(x)(x - a)^{d}, \\quad \\text{where} \\quad \\lim_{x \\rightarrow a} h_{a}(x) = 0.\\tag{5}\\end{equation}$\nAccording to the above description of the Taylor's Theorem, the function output $f(x)$ can be repre-sent as a summation of polynomials of high degrees of the $(x - a)$. What's more, in this paper, we propose to further disentangle the variable x from the given constant point a. Terms like $(x \u2212 a)^{k}$ can be decomposed into summations of polynomials in x alone, with a serving as the coefficients:\n$\\begin{equation} (x - a)^{d} = {d \\choose 0} (-a)^{d-0}x^{0} + {d \\choose 1} (-a)^{d-1}x^{1} + ... + {d \\choose d} (-a)^{d-d}x^{d}.\\tag{6}\\end{equation}$\nBased on the decomposition, we can rewrite the above Equation (4) as follows:\n$\\begin{equation} f(x \\vert a) = \\langle \\kappa(x), \\psi(a) \\rangle + R_{d}(x),\\tag{7}\\end{equation}$"}, {"title": "3.3 Taylor's Theorem for Multivariate Function Approximation", "content": "Representing multivariate continuous functions with Taylor's polynomials is more intricate. In this part, we use a multivariate function $f: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}$ as an example to illustrate how to disentangle the input variables and function parameters via Taylor's formula. Similar as the above single-variable function, assuming function $f$ is $d$th-time continuously differentiable at point $\\mathbf{a} \\in \\mathbb{R}^{m}$, then for the inputs near the point can be approximated as\n$\\begin{equation} f(\\mathbf{x}) = \\sum_{\\vert \\alpha \\vert \\leq d} \\frac{D^{\\alpha} f(\\mathbf{a})}{\\alpha!} (\\mathbf{x} - \\mathbf{a})^{\\alpha} + R_{d}(\\mathbf{x}).\\tag{17}\\end{equation}$"}, {"title": "3.4 Taylor's Theorem based Machine Learning Models", "content": "In real-world problems, the underlying functional mappings are often more intricate, such as $f:$ $\\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}$ with multiple input variables and multiple outputs. Representing these functions with Taylor's polynomials requires more cumbersome derivations, and the coefficient fabrication outputs should be a two-dimensional matrix, such as $ \\psi(\\mathbf{a}) \\in \\mathbb{R}^{n \\times D}$. To avoid getting bogged down in unnecessary mathematical details, we will not repeat those derivations here.\nIn recent years, there has been a growing interest in designing machine learning and deep learning models based on Taylor's theorem. For binary data inputs, Zhang et al. [86] introduce the reconciled polynomial machine to unify shallow and deep learning models, which is also the prior work that this paper is based on. Balduzzi et al. [4] investigate the convergence and exploration in rectifier networks with neural Taylor approximations, while Chrysos et al. [11] propose a new class of func-tion approximation method based on polynomial expansions. Zhao et al. [89] propose a generic neural architecture TaylorNet based on tensor decomposition to initialize the models, and Nivron et al. [56] introduce to incorporate use Taylor's expansion as a wrapper of transformer for the proba-bilistic predictions for time series and other random processes. Beyond time series and continuous"}, {"title": "4 RPN: Reconciled Polynomial Network for Deep Function Learning", "content": "Based on the preliminary background introduced above and inspired by the work of [86], we will introduce the Reconciled Polynomial Network (RPN) model for function learning in this section."}, {"title": "4.1 RPN: Reconciled Polynomial Network", "content": "Formally, given the underlying data distribution mapping $f: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}$, we represent the RPN model proposed to approximate function $f$ as follows:\n$\\begin{equation} g(\\mathbf{x} \\vert w) = \\langle \\kappa(\\mathbf{x}), \\psi(w) \\rangle + \\pi(\\mathbf{x}),\\tag{18}\\end{equation}$\nwhere\n\u2022 $ \\kappa: \\mathbb{R}^{M} \\rightarrow \\mathbb{R}^{D}$ is named as the data expansion function and $D$ is the target expansion space dimension.\n\u2022 $ \\psi : \\mathbb{R} \\rightarrow \\mathbb{R}^{n \\times D}$ is named as the parameter reconciliation function, which is defined only on the parameters without any input data.\n\u2022 $ \\pi : \\mathbb{R}^{M} \\rightarrow \\mathbb{R}^{n}$ is named as the remainder function.\nThe architecture of RPN is also illustrated in Figure 4. The RPN model disentangles input data from model parameters through the expansion functions $ \\kappa$ and reconciliation function $ \\psi$. More detailed information about all these components and modules mentioned in Figure 4 will be introduced in the following parts of this section."}, {"title": "4.2 RPN Component Functions", "content": "The data expansion function $ \\kappa$ projects input data into a new space with different basis vectors, where the target vector space dimension $D$ is determined when defining $ \\kappa$. In practice, the function $ \\kappa$ can either expand or compress the input to a higher- or lower-dimensional space. The corresponding function, $ \\kappa$, can also be referred to as the data expansion function (if $D > m$) and data compression function (if $D < m$), respectively. Collectively, these can be unified under the term \u201cdata transfor-mation functions\u201d. In this paper, we focus on expanding the inputs to a higher-dimensional space, and will use the function names \"data transformation\" and \"data expansion\" interchangeably in the following sections."}, {"title": "4.3 Wide RPN: Multi-Head and Multi-Channel Model Architecture", "content": "Similar to the Transformer with multi-head attention [78], as shown in Figure 4, the RPN model employs a multi-head architecture, where each head can disentangle the input data and model pa-rameters using different expansion, reconciliation and remainder functions, respectively:\n$\\begin{equation} g(\\mathbf{x} \\vert w, H) = \\sum_{h=0}^{H-1} (\\langle \\kappa^{(h)} (\\mathbf{x}), \\psi^{(h)} (w^{(h)}) \\rangle + \\pi^{(h)} (\\mathbf{x}),\\tag{19}\\end{equation}$\nwhere the superscript \u201ch\u201d indicates the head index and H denotes the total head number. By default, we use summation to combine the results from all these heads.\nMoreover, in the RPN model shown in Figure 4, similar to convolutional neural networks (CNNs) employing multiple filters, we allow each head to have multiple channels of parameters applied to the same data expansion. For example, for the $h$th head, we define its multi-channel parameters as $w^{(h),0}, w^{(h),1},\u2026, w^{(h),C\u22121}$, where $C$ denotes the number of channels. These parameters will be reconciled using the same parameter reconciliation function, as shown below:\n$\\begin{equation} g(\\mathbf{x} \\vert w, H, C) = \\sum_{h=0}^{H-1} \\sum_{c=0}^{C-1} (\\langle \\kappa^{(h)} (\\mathbf{x}), \\psi^{(h)} (w^{(h),c}) \\rangle + \\pi^{(h)} (\\mathbf{x}).\\tag{20}\\end{equation}$\nThe multi-head, multi-channel design of the RPN model allows it to project the same input data into multiple different high-dimensional spaces simultaneously. Each head and channel combination may potentially learn unique features from the data. The unique parameters at different heads can have different initialized lengths, and each of them will be processed in unique ways to accommodate the expanded data. This multi-channel approach provides our model with more flexibility in model design. In the following parts of this paper, to simplify the notations, we will illustrate the model's functional components using a single-head, single-channel architecture by default. However, readers should note that these components to be introduced below can be extended to their multi-head, multi-channel designs in practical implementations."}, {"title": "4.4 Deep RPN: Multi-Layer Model Architecture", "content": "The wide model architecture introduced above provides RPN with greater capabilities for approx-imating functions with diverse expansions concurrently. However, such shallow architectures can be insufficient for modeling complex functions. In this paper, as illustrated in Figure 4, we propose to stack RPN layers on top of each other to build a deeper architecture, where the Equation (18) actually defines one single layer of the model. Formally, we can represent the deep RPN with multi-layers as follows:\n$\\begin{aligned} \\text{Input:} & \\quad \\mathbf{h}_{0} = \\mathbf{x}, \\\\ \\text{Layer 1:} & \\quad \\mathbf{h}_{1} = \\langle \\kappa_{1}(\\mathbf{h}_{0}), \\psi_{1}(W_{1}) \\rangle + \\pi_{1}(\\mathbf{h}_{0}), \\\\ \\text{Layer 2:} & \\quad \\mathbf{h}_{2} = \\langle \\kappa_{2}(\\mathbf{h}_{1}), \\psi_{2}(W_{2}) \\rangle + \\pi_{2}(\\mathbf{h}_{1}), \\\\ & \\qquad \\qquad \\vdots \\tag{21} \\\\ \\text{Layer K:} & \\quad \\mathbf{h}_{K} = \\langle \\kappa_{K}(\\mathbf{h}_{K-1}), \\psi_{K}(W_{K}) \\rangle + \\pi_{K}(\\mathbf{h}_{K-1}), \\\\ \\text{Output:} & \\quad \\hat{y} = \\mathbf{h}_{K}. \\\\ \\end{aligned}$\nThe subscripts used above denote the layer index. The dimensions of the outputs at each layer can be represented as a list $[d_{0}, d_{1},...,d_{K-1}, d_{K}]$, where $d_{1} = m$ and $d_{K} = n$ denote the input and the desired output dimensions, respectively. Therefore, if the component functions at each layer of our model have been predetermined, we can just use the dimension list $[d_{0}, d_{1},...,d_{K-1}, d_{K}]$ to represent the architecture of the RPN model."}, {"title": "4.5 Versatile RPN: Nested and Extended Expansion Functions", "content": "The data expansion function introduced earlier projects the input data to a higher-dimensional space. There exist different ways to define the data expansion function, and a list of such basic expansion functions will be introduced in the following Section 5.1. The multi-head, multi-channel and multi-layer architecture also provides RPN with more capacity to build wider and deeper architectures for projecting input data to the desired target space. In addition to these designs, as illustrated in Figure 5, RPN also provides a more flexible and lightweight mechanism to build models with similar capacities via the nested and extended data expansion functions.\nNested expansions: Formally, given a list of n data expansion functions $\\kappa_{1}: \\mathbb{R}^{d_{0}} \\rightarrow \\mathbb{R}^{d_{1}}$, $\\kappa_{2}: \\mathbb{R}^{d_{1}} \\rightarrow \\mathbb{R}^{d_{2}}, ..., \\kappa_{n}: \\mathbb{R}^{d_{n-1}} \\rightarrow \\mathbb{R}^{d_{n}}$, as shown in Plots (a)-(b) of Figure 5, the nested calls of these functions will project a data vector from the input space $\\mathbb{R}^{d_{0}}$ to the desired output space $\\mathbb{R}^{d_{n}}$, defining the nested data expansion function $ \\kappa: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{D}$ as follows:\n$\\begin{equation} \\kappa(\\mathbf{x}) = \\kappa_{n} (\\kappa_{n-1} (\u00b7\u00b7\u00b7 \\kappa_{2} (\\kappa_{1} (\\mathbf{x})))) \\in \\mathbb{R}^{D}.\\tag{22}\\end{equation}$\nwhere the function input and output dimensions should be $d_{0} = m$ and $d_{n} = D$.\nExtended expansions: In addition to nesting these n expansion functions, as shown in Plots (c)-(d) of Figure 5, they can also be concatenated and applied concurrently, with their extended outputs allowing the model to leverage multiple expansion functions simultaneously. Formally, we can represent the extended data expansion function $ \\kappa : \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{D}$ defined based on $ \\kappa_{1} : \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{d_{1}}$, $\\kappa_{2}: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{d_{2}}, ..., \\kappa_{n} : \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{d_{n}}$ as follows:\n$\\begin{equation} \\kappa(\\mathbf{x}) = [\\kappa_{1} (\\mathbf{x}), \\kappa_{2} (\\mathbf{x}),..., \\kappa_{n} (\\mathbf{x})] \\in \\mathbb{R}^{D},\\tag{23}\\end{equation}$\nwhere the extended expansion's output dimension is equal to the sum of the output dimensions from all the individual expansion functions, i.e., $D = \\sum_{1}^{n} d_{i}$.\nAs illustrated in Figure 5, the nested expansion functions can define complex expansions akin to the multi-layer architecture of RPN mentioned above. Meanwhile, the extended expansion functions can define expansions similar to the multi-head architecture of RPN. Both nested and extended expansions allow for faster data expansions, circumventing cumbersome parameter inference and remainder function calculation, and can reduce the additional learning costs associated with train-ing deep and wide architectures of our model. This flexibility afforded by nested and extended expansions provides us with greater versatility in designing the RPN model."}, {"title": "4.6 Learning Correctness of RPN: Complexity, Capacity and Completeness", "content": "The learning correctness of RPN is fundamentally determined by the compositions of its compo-nent functions, each contributing from different perspectives:\n\u2022 Model Complexity: The data expansion function $ \\kappa$ expands the input data by projecting its representations using basis vectors in the new space. In other words, function $ \\kappa$ determines the upper bound of the RPN model's complexity.\n\u2022 Model Capacity: The reconciliation function $ \\psi$ processes the parameters to match the di-mensions of the expanded data vectors. The reconciliation function and parameters jointly determine the learning capacity and associated training costs of the RPN model.\n\u2022 Model Completeness: The remainder function $ \\pi$ completes the approximation as a resid-ual term, governing the learning completeness of the RPN model.\nIn the following Section 5, we will introduce several different representations for the data expan-sion function $ \\kappa$, parameter reconciliation function $ \\psi$, and remainder function $ \\pi$. By strategically"}, {"title": "4.7 Learning Cost of RPN: Space, Time and Parameter Number", "content": "To analyze the learning costs of RPN, we can take a batch input $X \\in \\mathbb{R}^{B\u00d7m}$ of batch size $B$ as an example, which will be fed to the RPN model with $K$ layers, each with $H$ heads and each head has $C$ channels. Each head will project the data instance from a vector of length $m$ to an expanded vector of length $D$ and then further projected to the desired output of length $n$. Each channel reconciles parameters from length $l$ to the sizes determined by both the expansion space and output space dimensions, i.e., $n \u00d7 D$.\nBased on the above hyper-parameters, assuming the input and output dimensions at each layer are comparable to $m$ and $n$, then the space, time costs and the number of involved parameters in learning the RPN model are calculated as follows:\n\u2022 Space Cost: The total space cost for data (including the inputs, expansions and out-puts) and parameter (including raw parameters, fabricated parameters generated by the reconciliation function and optional remainder function parameters) can be represented as\n$\\begin{aligned} O(KH(B(\\underset{\\text{input}}{m} + \\underset{\\text{expansion}}{D} + \\underset{\\text{output}}{n}) + C(\\underset{\\text{raw param.}}{l} + \\underset{\\text{reconciled param.}}{nD} + (\\underset{\\text{(optional) remainder param.}}{mn}))). \\\\ \\text{space cost for data} \\qquad \\qquad \\qquad \\qquad \\text{space cost for parameters} \\end{aligned}$\n\u2022 Time Cost: Depending on the expansion and reconciliation functions used for building RPN, the total time cost of RPN can be represented as\n$\\begin{aligned} O(KH(\\underset{\\text{time cost for data exp.}}{t_{exp}(m, D)} + \\underset{\\text{time cost for param. rec.}}{C t_{rec}(l, D)} + \\underset{\\text{time cost for inner product}}{C m n D} + (\\underset{\\text{(optional) time cost for remainder}}{mn})) , \\\\ \\end{aligned}$\nwhere notations $t_{exp}(m, D)$ and $t_{rec}(l, D)$ denote the expected time costs for data expan-sion and parameter reconciliation functions, respectively.\n\u2022 Learnable parameters: The total number of parameters in RPN will be $O(KHCl + KHmn)$, where $O(KHmn)$ denotes the optional parameter number used for defining the remainder function."}, {"title": "5 List of Expansion, Reconciliation and Remainder Functions for RPN Model", "content": "This section introduces the expansion, reconciliation, and remainder functions that can be used to design the RPN model, all of which have been implemented in the TINYBIG toolkit and are readily available. Readers seeking a concise overview can refer to Figure 6, which summarizes the lists of expansion, reconciliation and remainder functions to be introduced in this section."}, {"title": "5.1 Data Expansion Functions", "content": "The data expansion function determines the complexity of RPN. We will introduce several differ-ent data expansion functions below. In real-world practice, these individual data expansions intro-duced below can also be nested and extended to define more complex expansions, which provides more flexibility in the design of our RPN model."}, {"title": "5.1.1 Identity and Reciprocal Data Expansion", "content": "The simplest data expansion methods are the identity data```json\n{\n  "}, {"title": "RPN: Reconciled Polynomial Network", "authors": ["Jiawei Zhang"], "abstract": "In this paper, we will introduce a novel deep model named Reconciled Polynomial Network (RPN) for deep function learning. RPN has a very general architecture and can be used to build models with various complexities, capacities, and levels of completeness, which all contribute to the correctness of these models. As indi-cated in the subtitle, RPN can also serve as the backbone to unify different base models into one canonical representation. This includes non-deep models, like probabilistic graphical models (PGMs) - such as Bayesian network and Markov network - and kernel support vector machines (kernel SVMs), as well as deep models like the classic multi-layer perceptron (MLP) and the recent Kolmogorov-Arnold network (KAN). Technically, inspired by the Taylor's Theorem, RPN proposes to disentangle the underlying function to be inferred into the inner product of a data expansion func-tion and a parameter reconciliation function. Together with the remainder func-tion, RPN accurately approximates the underlying functions that governs data distributions. The data expansion functions in RPN project data vectors from the input space to a high-dimensional intermediate space, specified by the expansion functions in definition. Meanwhile, RPN also introduces the parameter recon-ciliation functions to fabricate a small number of parameters into a higher-order parameter matrix to address the \u201ccurse of dimensionality\u201d problem caused by the data expansions. In the intermediate space, the expanded vectors are polynomi-ally integrated and further projected into the low-dimensional output space via the inner product with the reconciled parameters generated by these parameter reconciliation functions. Moreover, the remainder functions provide RPN with additional complementary information to reduce potential approximation errors. We conducted extensive empirical experiments on numerous benchmark datasets across multiple modalities, including continuous function datasets, discrete vision and language datasets, and classic tabular datasets, to investigate the effectiveness of RPN. The experimental results demonstrate that, RPN outperforms MLP and KAN with mean squared errors at least \u00d710\u22121 lower (and even \u00d710-2 lower in some cases) for continuous function fitting. On both vision and language bench-mark datasets, using much less learnable parameters, RPN consistently achieves higher accuracy scores than Naive Bayes, kernel SVMs, MLP, and KAN for dis-crete image and text data classifications. In addition, equipped with the proba-bilistic data expansion functions, RPN learns better probabilistic dependency re-lationships among variables and outperforms other probabilistic models, including Naive Bayes, Bayesian networks, and Markov networks, for learning on classic tabular benchmark datasets. Reconciled Polynomial Network (RPN) proposed in this paper provides the op-portunity to represent and interpret current machine and deep learning models as sequences of vector space expansions and parameter reconciliations. These func-tions can all deliver concrete physical meanings about both the input data and model parameters. Furthermore, the application of simple inner-product and sum-mation operations to these functions significantly enhances the interpretability of RPN. This paper presents not only empirical experimental investigations but also in-depth discussions on RPN, addressing its interpretations, merits, limitations, and potential future developments. What's more, to facilitate the implementation of RPN-based models, we have developed and released a toolkit named TINYBIG. This toolkit encompasses all functions, modules, and models introduced in this paper, accompanied by com-prehensive documentation and tutorials. Detailed information about TINYBIG is available at the project's GitHub repository and the dedicated project webpage, with their respective URLs provided above.", "sections": [{"title": "1 Introduction", "content": "Over the past 70 years, the field of artificial intelligence has experienced dramatic changes in both the problems studied and the models used. With the emergence of new learning tasks, various machine learning models, each designed based on different prior assumptions, have been proposed to address these problems. As shown in Figure 1, we illustrate the timeline about three types of machine learning models that have dominated the field of artificial intelligence in the past 50 years, including probabilistic graphical models [37, 57, 39], support vector machines [12, 76, 8] and deep neural networks [66, 23]. Along with important technological breakthroughs, these models each had their moments of prominence and have been extensively explored and utilized in various research and application tasks related to data and learning nowadays. Besides these three categories of machine learning models, there are many other models (e.g., the tree based models and clustering models) that do not fit into these categories, but we will not discuss them in this paper and will leave them for future investigation instead. In this paper, we will introduce a novel deep model, namely Reconciled Polynomial Network (RPN), that can potentially unify these different aforementioned base models into one shared rep-resentation. In terms of model architecture, RPN consists of three component functions: data expansion function, parameter reconciliation function and remainder function. Inspired by the Taylor's theorem, RPN disentangles the input data from model parameters, and approximates the target functions to be inferred as the inner product of the data expansion function with the parameter reconciliation function, subsequently summed with the remainder function. Based on architecture of RPN, inferring the diverse underlying mapping that governs data distribu-tions (from inputs to outputs) is actually equivalent to inferring these three compositional functions. This inference process of the diverse data distribution mappings based on RPN is named as the function learning task in this paper. Specifically, the \u201cfunction\u201d term mentioned in the task name refers to not only the mathematical function components composing the RPN model but also the cognitive function of RPN as an intelligent system to relate input signals with desired output response. Function learning has been long-time treated as equivalent to the continuous function fitting and approximation for regression tasks only. Actually, in psychology and cognitive science, researchers have also used the function learning concept for modeling the mental induction pro-cess of stimulus-response relations of human and other intelligent subjects [9, 38], involving the acquisition of knowledge, manipulation of information and reasoning. In this paper, we argue that function learning is the most fundamental task in intelligent model learning, encompassing contin-uous function approximation, discrete vision and language data recognition and prediction, and cognitive and logic dependency relation induction. The following Section 2 will provide an"}, {"title": "2 Deep Function Learning", "content": "In this section, we will first introduce the concept of deep function learning task. After that, we will provide the detailed clarifications about how deep function learning differs from existing deep representation learning tasks. Based on this concept, we will further compare RPN, the deep function learning model proposed in this paper, with other existing non-deep and deep base models to illustrate their key differences."}, {"title": "2.1 What is Deep Function Learning?", "content": "As its name suggests, deep function learning, as the most fundamental task in machine learning, aims to build general deep models composed of a sequence of component functions that infer the relationships between inputs and outputs. These component functions define the mathematical pro-jections across different data and parameter spaces. In deep function learning, without any prior assumptions about the data modalities, the corresponding input and output data can also appear in different forms, including but not limited to continuous numerical values (such as continuous functions), discrete categorical features (such as images and language data), probabilistic variables (defining the dependency relationships between inputs and outputs), and others.\nDEFINITION 1 (Deep Function Learning): Formally, given the input and output spaces $\\mathbb{R}^{m}$ and $\\mathbb{R}^{n}$, the underling mapping that governs the data projection between these two spaces can be de-noted as:\n$\\begin{equation} f: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}.\\tag{1}\\end{equation}$\nDeep function learning aims to build a model $g$ as a composition of deep mathematical function sequences $g_{1}, g_{2},\u2026, g_{K}$ to project data cross different vector spaces, which can be represented as\n$\\begin{equation} g: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}, \\quad \\text{and} \\quad g = g_{1} \\circ g_{2} \\circ ... \\circ g_{k},\\tag{2}\\end{equation}$\nwhere the $circ$ notation denotes the component function integration and composition operators. The component functions $g_{i}$ can be defined on either input data or the model parameters.\nFor input $x \\in \\mathbb{R}^{m}$, if the output generated by the model can approximate the desired output, i.e.,\n$\\begin{equation} g(x \\vert w, \\theta) \\approx f(x),\\tag{3}\\end{equation}$\nthen model is $g$ can serve as an approximated mapping of $f$. Notations $w \\in \\mathbb{R}^{l}$ and $\\theta \\in \\mathbb{R}^{l'}$ denote the learnable parameters and hyper-parameters of the function learning model, respectively.\nBelow, we will further clarify the distinctions between deep function learning and current deep model-based data representation learning tasks. After that, we will compare our RPN model, which is grounded in deep function learning, against other existing base models."}, {"title": "2.2 Deep Function Learning vs Deep Representation Learning", "content": "As mentioned previously, the function learning tasks and models examined in this paper encompass not only continuous function approximation, but also discrete data classification and the induction of dependency relations. Besides the literal differences indicated by their names - representation learning is data oriented but function learning is model oriented - deep function learning significantly differs from the current deep representation learning in several critical perspectives discussed below.\n\u2022 Generalizability: Representation learning, to some extent, has contributed to the current fragmentation within the AI community, as data - the carrier of information - is collected, represented, and stored in disparate modalities. Existing deep models, specifically designed for certain modalities, tend to overfit to these modality-specific data representations in addi-tion to learning the underlying information. Applying a model proposed for one modality to another typically necessitates significant architectural redesigns. Recently, there have been efforts to explore the cross-modal applicability of certain models, e.g., CNNs for language and Transformers for vision, but replicating such cross-modality migration explorations across all current and future deep models is extremely expensive and unsustainable. Fur-thermore, to achieve the future artificial general intelligence (AGI), the available data in a single modality is no longer sufficient for training larger models. Deep function learn-ing, without any prior assumptions on data modalities, will pave the way for improving the model generalizability. These learned functions should demonstrate their generalizability and applicability to multi-modal data from the outset, during their design and investigation phases.\n\u2022 Interpretability: Representation learning primarily aims to learn and extract latent patterns or salient features from data, aligning with the technological advancements in data science and big data analytics over the past two decades. However, the learned data representations often lack interpretable physical meanings, rendering most current AI models to be black boxes. In contrast, to realize the goal of explainable AI (XAI), greater emphasis must be placed on developing new model architectures with concrete physical meanings and mathematical interpretations in the future. The RPN based deep function learning, on the other hand, aims to learn compositional functions with inherent physical interpretability for building general-purpose models across various tasks, thereby bridging the interpretability gap of current and future deep models."}, {"title": "2.3 RPN vs Other Base Models", "content": "Figure 2 compares the RPN model proposed for deep function learning with several base models in terms of mathematical theorem foundations, formula representations, and model architectures. The top three Plots (a)-(c) describe the non-deep base models: Bayesian Networks, Markov Networks, and Kernel SVMs; while the Plots (d)-(i) at the bottom illustrate the architectures of deep base models: MLPs, KANs, and RPN. For MLPs and KANs, Plots (d)-(e) and (g)-(h) illustrate their two-layer and three-layer architectures, respectively. Similarly, for RPN, we present its one-layer and three-layer architectures in Plots (f) and (i).\nBased on the plots shown in Figure 2, we can observe significant differences of RPN compared against these base models, which are summarized as follows:\n\u2022 RPN vs Non-Deep Base Models: Examining the model plots, we observe that all these base model architectures can be represented as graph structures composed of variables and their relationships. The model architecture of the Markov network is undirected, while that of the Bayesian network is directed. Similarly, for MLP, KAN, and RPN, although we haven't shown the variable connection directions, their model architecture are also directed, flowing from bottom to top. The model architectures of both Markov network and Bayesian network consist of variable nodes that correspond only to input features and output labels. In contrast, for kernel SVM, MLP, KAN, and RPN, their model architectures involve not only nodes representing inputs and outputs, but also those representing expansions and hid-den layers. Both RPN and kernel SVM involve a data expansion function to project input data into a high-dimensional space. However, their approaches diverge thereafter. Kernel SVM directly defines parameters within this high-dimensional space to integrate expansion vectors into outputs. In contrast, RPN fabricates these high-dimensional parameters via a reconciliation function from a reduced set of parameters instead.\n\u2022 RPN vs Deep Base Models: The difference between RPN and MLP is easy to observe. MLP involves neither data expansion nor parameter reconciliation. Instead, they apply activation functions to neurons after input integration, which is parameterized by neuron connection weights. Unlike MLPs with predefined, static activation functions, the recent"}, {"title": "3 Notations and Background Knowledge on Taylor's Theorem", "content": "This section first introduces the notation system used throughout this paper. Based on the notations, we then briefly present Taylor's theorem as the preliminary knowledge of the RPN model, which will be introduced in the following Section 4."}, {"title": "3.1 Notation System", "content": "In the sequel of this paper, we will use the lower case letters (e.g., $x$) to represent scalars, lower case bold letters (e.g., $\\mathbf{x}$) to denote column vectors, bold-face upper case letters (e.g., $X$) to denote matrices and high-order tensors, and upper case calligraphic letters (e.g., $\\mathcal{X}$) to denote sets. Given a matrix $X$, we denote $X(i, :)$ and $X(:, j)$ as its $i$th row and $j$th column, respectively. The $(i \\text{th}, j \\text{th})$ entry of matrix $X$ can be denoted as $X(i, j)$. We use $X^{T}$ and $\\mathbf{x}^{T}$ to represent the transpose of matrix $X$ and vector $\\mathbf{x}$. For vector $\\mathbf{x}$, we represent its $L_{p}$-norm as $|\\|\\mathbf{x}\\||_{p} = (\\Sigma_{i}|x(i)|^{p})^{\\frac{1}{p}}$. The Frobenius-norm of matrix $X$ is represented as $|\\|X\\||_{F} = (\\Sigma_{i,j} |X(i, j)|^{2})^{\\frac{1}{2}}$. The elementwise product of vectors $\\mathbf{x}$ and $\\mathbf{y}$ of the same dimension is represented as $\\mathbf{x} \\odot \\mathbf{y}$, their inner product is represented as $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$, and their Kronecker product is $\\mathbf{x} \\otimes \\mathbf{y}$. The elementwise product and Kronecker product operators can also be applied to matrices $X$ and $Y$ as $X \\odot Y$ and $X \\otimes Y$, respectively."}, {"title": "3.2 Taylor's Theorem for Univariate Function Approximation", "content": "Taylor's theorem approximates a d-times differentiable function around a given point using poly-nomials up to degree d, commonly referred to as the dth-order Taylor polynomial. In this section, we first introduce Taylor's theorem for univariate functions, which also generalizes to multivariate and vector valued functions. We will briefly describe its extension to multivariate functions in the subsequent Subsection 3.3, and then introduce the RPN model designed based on Taylor's theorem with vector valued functions in Section 4.\nTHEOREM 1 (Taylor's Theorem): Let $d \\geq 1$ be an integer and let function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ be $d$ times differentiable at the point $a \\in \\mathbb{R}$. As illustrated in Figure 3, then there exists a function $h_{a}: \\mathbb{R} \\rightarrow \\mathbb{R}$ such that\n$\\begin{aligned} f(x) = & \\frac{f(a)}{0!}(x - a)^{0} + \\frac{f'(a)}{1!}(x - a)^{1} + \\frac{f''(a)}{2!}(x - a)^{2} + ... + \\frac{f^{(d)}(a)}{d!}(x - a)^{d} + R_{d}(x), \\\\ = & \\sum_{i=0}^{d} \\frac{f^{(i)}(a)}{i!}(x - a)^{i} + R_{d}(x),\\tag{4}\\end{aligned}$\nIn the equation, $R_{a}(x)$ is also normally called the \u201cremainder\u201d term and can be represented as\n$\\begin{equation} R_{d}(x) = h_{a}(x)(x - a)^{d}, \\quad \\text{where} \\quad \\lim_{x \\rightarrow a} h_{a}(x) = 0.\\tag{5}\\end{equation}$\nAccording to the above description of the Taylor's Theorem, the function output $f(x)$ can be repre-sent as a summation of polynomials of high degrees of the $(x - a)$. What's more, in this paper, we propose to further disentangle the variable x from the given constant point a. Terms like $(x \u2212 a)^{k}$ can be decomposed into summations of polynomials in x alone, with a serving as the coefficients:\n$\\begin{equation} (x - a)^{d} = {d \\choose 0} (-a)^{d-0}x^{0} + {d \\choose 1} (-a)^{d-1}x^{1} + ... + {d \\choose d} (-a)^{d-d}x^{d}.\\tag{6}\\end{equation}$\nBased on the decomposition, we can rewrite the above Equation (4) as follows:\n$\\begin{equation} f(x \\vert a) = \\langle \\kappa(x), \\psi(a) \\rangle + R_{d}(x),\\tag{7}\\end{equation}$"}, {"title": "3.3 Taylor's Theorem for Multivariate Function Approximation", "content": "Representing multivariate continuous functions with Taylor's polynomials is more intricate. In this part, we use a multivariate function $f: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}$ as an example to illustrate how to disentangle the input variables and function parameters via Taylor's formula. Similar as the above single-variable function, assuming function $f$ is $d$th-time continuously differentiable at point $\\mathbf{a} \\in \\mathbb{R}^{m}$, then for the inputs near the point can be approximated as\n$\\begin{equation} f(\\mathbf{x}) = \\sum_{\\vert \\alpha \\vert \\leq d} \\frac{D^{\\alpha} f(\\mathbf{a})}{\\alpha!} (\\mathbf{x} - \\mathbf{a})^{\\alpha} + R_{d}(\\mathbf{x}).\\tag{17}\\end{equation}$"}, {"title": "3.4 Taylor's Theorem based Machine Learning Models", "content": "In real-world problems, the underlying functional mappings are often more intricate, such as $f:$ $\\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}$ with multiple input variables and multiple outputs. Representing these functions with Taylor's polynomials requires more cumbersome derivations, and the coefficient fabrication outputs should be a two-dimensional matrix, such as $ \\psi(\\mathbf{a}) \\in \\mathbb{R}^{n \\times D}$. To avoid getting bogged down in unnecessary mathematical details, we will not repeat those derivations here.\nIn recent years, there has been a growing interest in designing machine learning and deep learning models based on Taylor's theorem. For binary data inputs, Zhang et al. [86] introduce the reconciled polynomial machine to unify shallow and deep learning models, which is also the prior work that this paper is based on. Balduzzi et al. [4] investigate the convergence and exploration in rectifier networks with neural Taylor approximations, while Chrysos et al. [11] propose a new class of func-tion approximation method based on polynomial expansions. Zhao et al. [89] propose a generic neural architecture TaylorNet based on tensor decomposition to initialize the models, and Nivron et al. [56] introduce to incorporate use Taylor's expansion as a wrapper of transformer for the proba-bilistic predictions for time series and other random processes. Beyond time series and continuous"}, {"title": "4 RPN: Reconciled Polynomial Network for Deep Function Learning", "content": "Based on the preliminary background introduced above and inspired by the work of [86], we will introduce the Reconciled Polynomial Network (RPN) model for function learning in this section."}, {"title": "4.1 RPN: Reconciled Polynomial Network", "content": "Formally, given the underlying data distribution mapping $f: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}$, we represent the RPN model proposed to approximate function $f$ as follows:\n$\\begin{equation} g(\\mathbf{x} \\vert w) = \\langle \\kappa(\\mathbf{x}), \\psi(w) \\rangle + \\pi(\\mathbf{x}),\\tag{18}\\end{equation}$\nwhere\n\u2022 $ \\kappa: \\mathbb{R}^{M} \\rightarrow \\mathbb{R}^{D}$ is named as the data expansion function and $D$ is the target expansion space dimension.\n\u2022 $ \\psi : \\mathbb{R} \\rightarrow \\mathbb{R}^{n \\times D}$ is named as the parameter reconciliation function, which is defined only on the parameters without any input data.\n\u2022 $ \\pi : \\mathbb{R}^{M} \\rightarrow \\mathbb{R}^{n}$ is named as the remainder function.\nThe architecture of RPN is also illustrated in Figure 4. The RPN model disentangles input data from model parameters through the expansion functions $ \\kappa$ and reconciliation function $ \\psi$. More detailed information about all these components and modules mentioned in Figure 4 will be introduced in the following parts of this section."}, {"title": "4.2 RPN Component Functions", "content": "The data expansion function $ \\kappa$ projects input data into a new space with different basis vectors, where the target vector space dimension $D$ is determined when defining $ \\kappa$. In practice, the function $ \\kappa$ can either expand or compress the input to a higher- or lower-dimensional space. The corresponding function, $ \\kappa$, can also be referred to as the data expansion function (if $D > m$) and data compression function (if $D < m$), respectively. Collectively, these can be unified under the term \u201cdata transfor-mation functions\u201d. In this paper, we focus on expanding the inputs to a higher-dimensional space, and will use the function names \"data transformation\" and \"data expansion\" interchangeably in the following sections."}, {"title": "4.3 Wide RPN: Multi-Head and Multi-Channel Model Architecture", "content": "Similar to the Transformer with multi-head attention [78], as shown in Figure 4, the RPN model employs a multi-head architecture, where each head can disentangle the input data and model pa-rameters using different expansion, reconciliation and remainder functions, respectively:\n$\\begin{equation} g(\\mathbf{x} \\vert w, H) = \\sum_{h=0}^{H-1} (\\langle \\kappa^{(h)} (\\mathbf{x}), \\psi^{(h)} (w^{(h)}) \\rangle + \\pi^{(h)} (\\mathbf{x}),\\tag{19}\\end{equation}$\nwhere the superscript \u201ch\u201d indicates the head index and H denotes the total head number. By default, we use summation to combine the results from all these heads.\nMoreover, in the RPN model shown in Figure 4, similar to convolutional neural networks (CNNs) employing multiple filters, we allow each head to have multiple channels of parameters applied to the same data expansion. For example, for the $h$th head, we define its multi-channel parameters as $w^{(h),0}, w^{(h),1},\u2026, w^{(h),C\u22121}$, where $C$ denotes the number of channels. These parameters will be reconciled using the same parameter reconciliation function, as shown below:\n$\\begin{equation} g(\\mathbf{x} \\vert w, H, C) = \\sum_{h=0}^{H-1} \\sum_{c=0}^{C-1} (\\langle \\kappa^{(h)} (\\mathbf{x}), \\psi^{(h)} (w^{(h),c}) \\rangle + \\pi^{(h)} (\\mathbf{x}),\\tag{20}\\end{equation}$\nThe multi-head, multi-channel design of the RPN model allows it to project the same input data into multiple different high-dimensional spaces simultaneously. Each head and channel combination may potentially learn unique features from the data. The unique parameters at different heads can have different initialized lengths, and each of them will be processed in unique ways to accommodate the expanded data. This multi-channel approach provides our model with more flexibility in model design. In the following parts of this paper, to simplify the notations, we will illustrate the model's functional components using a single-head, single-channel architecture by default. However, readers should note that these components to be introduced below can be extended to their multi-head, multi-channel designs in practical implementations."}, {"title": "4.4 Deep RPN: Multi-Layer Model Architecture", "content": "The wide model architecture introduced above provides RPN with greater capabilities for approx-imating functions with diverse expansions concurrently. However, such shallow architectures can be insufficient for modeling complex functions. In this paper, as illustrated in Figure 4, we propose to stack RPN layers on top of each other to build a deeper architecture, where the Equation (18) actually defines one single layer of the model. Formally, we can represent the deep RPN with multi-layers as follows:\n$\\begin{aligned} \\text{Input:} & \\quad \\mathbf{h}_{0} = \\mathbf{x}, \\\\ \\text{Layer 1:} & \\quad \\mathbf{h}_{1} = \\langle \\kappa_{1}(\\mathbf{h}_{0}), \\psi_{1}(W_{1}) \\rangle + \\pi_{1}(\\mathbf{h}_{0}), \\\\ \\text{Layer 2:} & \\quad \\mathbf{h}_{2} = \\langle \\kappa_{2}(\\mathbf{h}_{1}), \\psi_{2}(W_{2}) \\rangle + \\pi_{2}(\\mathbf{h}_{1}), \\\\ & \\qquad \\qquad \\vdots \\tag{21} \\\\ \\text{Layer K:} & \\quad \\mathbf{h}_{K} = \\langle \\kappa_{K}(\\mathbf{h}_{K-1}), \\psi_{K}(W_{K}) \\rangle + \\pi_{K}(\\mathbf{h}_{K-1}), \\\\ \\text{Output:} & \\quad \\hat{y} = \\mathbf{h}_{K}. \\\\ \\end{aligned}$\nThe subscripts used above denote the layer index. The dimensions of the outputs at each layer can be represented as a list $[d_{0}, d_{1},...,d_{K-1}, d_{K}]$, where $d_{1} = m$ and $d_{K} = n$ denote the input and the desired output dimensions, respectively. Therefore, if the component functions at each layer of our model have been predetermined, we can just use the dimension list $[d_{0}, d_{1},...,d_{K-1}, d_{K}]$ to represent the architecture of the RPN model."}, {"title": "4.5 Versatile RPN: Nested and Extended Expansion Functions", "content": "The data expansion function introduced earlier projects the input data to a higher-dimensional space. There exist different ways to define the data expansion function, and a list of such basic expansion functions will be introduced in the following Section 5.1. The multi-head, multi-channel and multi-layer architecture also provides RPN with more capacity to build wider and deeper architectures for projecting input data to the desired target space. In addition to these designs, as illustrated in Figure 5, RPN also provides a more flexible and lightweight mechanism to build models with similar capacities via the nested and extended data expansion functions.\nNested expansions: Formally, given a list of n data expansion functions $\\kappa_{1}: \\mathbb{R}^{d_{0}} \\rightarrow \\mathbb{R}^{d_{1}}$, $\\kappa_{2}: \\mathbb{R}^{d_{1}} \\rightarrow \\mathbb{R}^{d_{2}}, ..., \\kappa_{n}: \\mathbb{R}^{d_{n-1}} \\rightarrow \\mathbb{R}^{d_{n}}$, as shown in Plots (a)-(b) of Figure 5, the nested calls of these functions will project a data vector from the input space $\\mathbb{R}^{d_{0}}$ to the desired output space $\\mathbb{R}^{d_{n}}$, defining the nested data expansion function $ \\kappa: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{D}$ as follows:\n$\\begin{equation} \\kappa(\\mathbf{x}) = \\kappa_{n} (\\kappa_{n-1} (\u00b7\u00b7\u00b7 \\kappa_{2} (\\kappa_{1} (\\mathbf{x})))) \\in \\mathbb{R}^{D}.\\tag{22}\\end{equation}$\nwhere the function input and output dimensions should be $d_{0} = m$ and $d_{n} = D$.\nExtended expansions: In addition to nesting these n expansion functions, as shown in Plots (c)-(d) of Figure 5, they can also be concatenated and applied concurrently, with their extended outputs allowing the model to leverage multiple expansion functions simultaneously. Formally, we can represent the extended data expansion function $ \\kappa : \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{D}$ defined based on $ \\kappa_{1} : \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{d_{1}}$, $\\kappa_{2}: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{d_{2}}, ..., \\kappa_{n} : \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{d_{n}}$ as follows:\n$\\begin{equation} \\kappa(\\mathbf{x}) = [\\kappa_{1} (\\mathbf{x}), \\kappa_{2} (\\mathbf{x}),..., \\kappa_{n} (\\mathbf{x})] \\in \\mathbb{R}^{D},\\tag{23}\\end{equation}$\nwhere the extended expansion's output dimension is equal to the sum of the output dimensions from all the individual expansion functions, i.e., $D = \\sum_{1}^{n} d_{i}$.\nAs illustrated in Figure 5, the nested expansion functions can define complex expansions akin to the multi-layer architecture of RPN mentioned above. Meanwhile, the extended expansion functions can define expansions similar to the multi-head architecture of RPN. Both nested and extended expansions allow for faster data expansions, circumventing cumbersome parameter inference and remainder function calculation, and can reduce the additional learning costs associated with train-ing deep and wide architectures of our model. This flexibility afforded by nested and extended expansions provides us with greater versatility in designing the RPN model."}, {"title": "4.6 Learning Correctness of RPN: Complexity, Capacity and Completeness", "content": "The learning correctness of RPN is fundamentally determined by the compositions of its compo-nent functions, each contributing from different perspectives:\n\u2022 Model Complexity: The data expansion function $ \\kappa$ expands the input data by projecting its representations using basis vectors in the new space. In other words, function $ \\kappa$ determines the upper bound of the RPN model's complexity.\n\u2022 Model Capacity: The reconciliation function $ \\psi$ processes the parameters to match the di-mensions of the expanded data vectors. The reconciliation function and parameters jointly determine the learning capacity and associated training costs of the RPN model.\n\u2022 Model Completeness: The remainder function $ \\pi$ completes the approximation as a resid-ual term, governing the learning completeness of the RPN model.\nIn the following Section 5, we will introduce several different representations for the data expan-sion function $ \\kappa$, parameter reconciliation function $ \\psi$, and remainder function $ \\pi$. By strategically"}, {"title": "4.7 Learning Cost of RPN: Space, Time and Parameter Number", "content": "To analyze the learning costs of RPN, we can take a batch input $X \\in \\mathbb{R}^{B\u00d7m}$ of batch size $B$ as an example, which will be fed to the RPN model with $K$ layers, each with $H$ heads and each head has $C$ channels. Each head will project the data instance from a vector of length $m$ to an expanded vector of length $D$ and then further projected to the desired output of length $n$. Each channel reconciles parameters from length $l$ to the sizes determined by both the expansion space and output space dimensions, i.e., $n \u00d7 D$.\nBased on the above hyper-parameters, assuming the input and output dimensions at each layer are comparable to $m$ and $n$, then the space, time costs and the number of involved parameters in learning the RPN model are calculated as follows:\n\u2022 Space Cost: The total space cost for data (including the inputs, expansions and out-puts) and parameter (including raw parameters, fabricated parameters generated by the reconciliation function and optional remainder function parameters) can be represented as\n$\\begin{aligned} O(KH(B(\\underset{\\text{input}}{m} + \\underset{\\text{expansion}}{D} + \\underset{\\text{output}}{n}) + C(\\underset{\\text{raw param.}}{l} + \\underset{\\text{reconciled param.}}{nD} + (\\underset{\\text{(optional) remainder param.}}{mn}))). \\\\ \\text{space cost for data} \\qquad \\qquad \\qquad \\qquad \\text{space cost for parameters} \\end{aligned}$\n\u2022 Time Cost: Depending on the expansion and reconciliation functions used for building RPN, the total time cost of RPN can be represented as\n$\\begin{aligned} O(KH(\\underset{\\text{time cost for data exp.}}{t_{exp}(m, D)} + \\underset{\\text{time cost for param. rec.}}{C t_{rec}(l, D)} + \\underset{\\text{time cost for inner product}}{C m n D} + (\\underset{\\text{(optional) time cost for remainder}}{mn})) , \\\\ \\end{aligned}$\nwhere notations $t_{exp}(m, D)$ and $t_{rec}(l, D)$ denote the expected time costs for data expan-sion and parameter reconciliation functions, respectively.\n\u2022 Learnable parameters: The total number of parameters in RPN will be $O(KHCl + KHmn)$, where $O(KHmn)$ denotes the optional parameter number used for defining the remainder function."}, {"title": "5 List of Expansion, Reconciliation and Remainder Functions for RPN Model", "content": "This section introduces the expansion, reconciliation, and remainder functions that can be used to design the RPN model, all of which have been implemented in the TINYBIG toolkit and are readily available. Readers seeking a concise overview can refer to Figure 6, which summarizes the lists of expansion, reconciliation and remainder functions to be introduced in this section."}, {"title": "5.1 Data Expansion Functions", "content": "The data expansion function determines the complexity of RPN. We will introduce several differ-ent data expansion functions below. In real-world practice, these individual data expansions intro-duced below can also be nested and extended to define more complex expansions, which provides more flexibility in the design of our RPN model."}, {"title": "5.1.1 Identity and Reciprocal Data Expansion", "content": "The simplest data expansion methods are the identity data"}]}]}