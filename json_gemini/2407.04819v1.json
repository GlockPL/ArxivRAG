{"title": "RPN: Reconciled Polynomial Network Towards Unifying PGMs, Kernel SVMs, MLP and KAN", "authors": ["Jiawei Zhang"], "abstract": "In this paper, we will introduce a novel deep model named Reconciled Polynomial Network (RPN) for deep function learning. RPN has a very general architecture and can be used to build models with various complexities, capacities, and levels of completeness, which all contribute to the correctness of these models. As indicated in the subtitle, RPN can also serve as the backbone to unify different base models into one canonical representation. This includes non-deep models, like probabilistic graphical models (PGMs) - such as Bayesian network and Markov network - and kernel support vector machines (kernel SVMs), as well as deep models like the classic multi-layer perceptron (MLP) and the recent Kolmogorov-Arnold network (KAN). Technically, inspired by the Taylor's Theorem, RPN proposes to disentangle the underlying function to be inferred into the inner product of a data expansion function and a parameter reconciliation function. Together with the remainder function, RPN accurately approximates the underlying functions that governs data distributions. The data expansion functions in RPN project data vectors from the input space to a high-dimensional intermediate space, specified by the expansion functions in definition. Meanwhile, RPN also introduces the parameter reconciliation functions to fabricate a small number of parameters into a higher-order parameter matrix to address the \u201ccurse of dimensionality\u201d problem caused by the data expansions. In the intermediate space, the expanded vectors are polynomially integrated and further projected into the low-dimensional output space via the inner product with the reconciled parameters generated by these parameter reconciliation functions. Moreover, the remainder functions provide RPN with additional complementary information to reduce potential approximation errors. We conducted extensive empirical experiments on numerous benchmark datasets across multiple modalities, including continuous function datasets, discrete vision and language datasets, and classic tabular datasets, to investigate the effectiveness of RPN. The experimental results demonstrate that, RPN outperforms MLP and KAN with mean squared errors at least \u00d710\u22121 lower (and even \u00d710-2 lower in some cases) for continuous function fitting. On both vision and language benchmark datasets, using much less learnable parameters, RPN consistently achieves higher accuracy scores than Naive Bayes, kernel SVMs, MLP, and KAN for discrete image and text data classifications. In addition, equipped with the probabilistic data expansion functions, RPN learns better probabilistic dependency relationships among variables and outperforms other probabilistic models, including Naive Bayes, Bayesian networks, and Markov networks, for learning on classic tabular benchmark datasets. Reconciled Polynomial Network (RPN) proposed in this paper provides the opportunity to represent and interpret current machine and deep learning models as sequences of vector space expansions and parameter reconciliations. These functions can all deliver concrete physical meanings about both the input data and model parameters. Furthermore, the application of simple inner-product and summation operations to these functions significantly enhances the interpretability of RPN. This paper presents not only empirical experimental investigations but also in-depth discussions on RPN, addressing its interpretations, merits, limitations, and potential future developments. What's more, to facilitate the implementation of RPN-based models, we have developed and released a toolkit named TINYBIG. This toolkit encompasses all functions, modules, and models introduced in this paper, accompanied by comprehensive documentation and tutorials. Detailed information about TINYBIG is available at the project's GitHub repository and the dedicated project webpage, with their respective URLs provided above.", "sections": [{"title": "1 Introduction", "content": "Over the past 70 years, the field of artificial intelligence has experienced dramatic changes in both the problems studied and the models used. With the emergence of new learning tasks, various machine learning models, each designed based on different prior assumptions, have been proposed to address these problems. As shown in Figure 1, we illustrate the timeline about three types of machine learning models that have dominated the field of artificial intelligence in the past 50 years, including probabilistic graphical models [37, 57, 39], support vector machines [12, 76, 8] and deep neural networks [66, 23]. Along with important technological breakthroughs, these models each had their moments of prominence and have been extensively explored and utilized in various research and application tasks related to data and learning nowadays. Besides these three categories of machine learning models, there are many other models (e.g., the tree based models and clustering models) that do not fit into these categories, but we will not discuss them in this paper and will leave them for future investigation instead.\nIn this paper, we will introduce a novel deep model, namely Reconciled Polynomial Network (RPN), that can potentially unify these different aforementioned base models into one shared representation. In terms of model architecture, RPN consists of three component functions: data expansion function, parameter reconciliation function and remainder function. Inspired by the Taylor's theorem, RPN disentangles the input data from model parameters, and approximates the target functions to be inferred as the inner product of the data expansion function with the parameter reconciliation function, subsequently summed with the remainder function.\nBased on architecture of RPN, inferring the diverse underlying mapping that governs data distributions (from inputs to outputs) is actually equivalent to inferring these three compositional functions. This inference process of the diverse data distribution mappings based on RPN is named as the function learning task in this paper. Specifically, the \u201cfunction\u201d term mentioned in the task name refers to not only the mathematical function components composing the RPN model but also the cognitive function of RPN as an intelligent system to relate input signals with desired output response. Function learning has been long-time treated as equivalent to the continuous function fitting and approximation for regression tasks only. Actually, in psychology and cognitive science, researchers have also used the function learning concept for modeling the mental induction process of stimulus-response relations of human and other intelligent subjects [9, 38], involving the acquisition of knowledge, manipulation of information and reasoning. In this paper, we argue that function learning is the most fundamental task in intelligent model learning, encompassing continuous function approximation, discrete vision and language data recognition and prediction, and cognitive and logic dependency relation induction. The following Section 2 will provide an"}, {"title": "2 Deep Function Learning", "content": "In this section, we will first introduce the concept of deep function learning task. After that, we will provide the detailed clarifications about how deep function learning differs from existing deep representation learning tasks. Based on this concept, we will further compare RPN, the deep function learning model proposed in this paper, with other existing non-deep and deep base models to illustrate their key differences."}, {"title": "2.1 What is Deep Function Learning?", "content": "As its name suggests, deep function learning, as the most fundamental task in machine learning, aims to build general deep models composed of a sequence of component functions that infer the relationships between inputs and outputs. These component functions define the mathematical projections across different data and parameter spaces. In deep function learning, without any prior assumptions about the data modalities, the corresponding input and output data can also appear in different forms, including but not limited to continuous numerical values (such as continuous functions), discrete categorical features (such as images and language data), probabilistic variables (defining the dependency relationships between inputs and outputs), and others.\nDEFINITION 1 (Deep Function Learning): Formally, given the input and output spaces \\(\\mathbb{R}^m\\) and\n\\(\\mathbb{R}^n\\), the underling mapping that governs the data projection between these two spaces can be denoted as:\n\\[f: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n.\\]\nDeep function learning aims to build a model g as a composition of deep mathematical function sequences \\(g_1, g_2, \\ldots, g_K\\) to project data cross different vector spaces, which can be represented as\n\\[g: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n, \\text{ and } g = g_1 \\circ g_2 \\circ \\ldots \\circ g_k,\\]\nwhere the \\(\\circ\\) notation denotes the component function integration and composition operators. The component functions \\(g_i\\) can be defined on either input data or the model parameters.\nFor input \\(x \\in \\mathbb{R}^m\\), if the output generated by the model can approximate the desired output, i.e.,\n\\[g(x|w, \\theta) \\approx f(x),\\]\nthen model is g can serve as an approximated mapping of f. Notations \\(w \\in \\mathbb{R}^l\\) and \\(\\theta \\in \\mathbb{R}^{l'}\\) denote the learnable parameters and hyper-parameters of the function learning model, respectively.\nBelow, we will further clarify the distinctions between deep function learning and current deep model-based data representation learning tasks. After that, we will compare our RPN model, which is grounded in deep function learning, against other existing base models."}, {"title": "2.2 Deep Function Learning vs Deep Representation Learning", "content": "As mentioned previously, the function learning tasks and models examined in this paper encompass not only continuous function approximation, but also discrete data classification and the induction of dependency relations. Besides the literal differences indicated by their names - representation learning is data oriented but function learning is model oriented - deep function learning significantly differs from the current deep representation learning in several critical perspectives discussed below.\n\u2022 Generalizability: Representation learning, to some extent, has contributed to the current fragmentation within the AI community, as data - the carrier of information - is collected, represented, and stored in disparate modalities. Existing deep models, specifically designed for certain modalities, tend to overfit to these modality-specific data representations in addition to learning the underlying information. Applying a model proposed for one modality to another typically necessitates significant architectural redesigns. Recently, there have been efforts to explore the cross-modal applicability of certain models, e.g., CNNs for language and Transformers for vision, but replicating such cross-modality migration explorations across all current and future deep models is extremely expensive and unsustainable. Furthermore, to achieve the future artificial general intelligence (AGI), the available data in a single modality is no longer sufficient for training larger models. Deep function learning, without any prior assumptions on data modalities, will pave the way for improving the model generalizability. These learned functions should demonstrate their generalizability and applicability to multi-modal data from the outset, during their design and investigation phases.\n\u2022 Interpretability: Representation learning primarily aims to learn and extract latent patterns or salient features from data, aligning with the technological advancements in data science and big data analytics over the past two decades. However, the learned data representations often lack interpretable physical meanings, rendering most current AI models to be black boxes. In contrast, to realize the goal of explainable AI (XAI), greater emphasis must be placed on developing new model architectures with concrete physical meanings and mathematical interpretations in the future. The RPN based deep function learning, on the other hand, aims to learn compositional functions with inherent physical interpretability for building general-purpose models across various tasks, thereby bridging the interpretability gap of current and future deep models."}, {"title": "2.3 RPN vs Other Base Models", "content": "Figure 2 compares the RPN model proposed for deep function learning with several base models in terms of mathematical theorem foundations, formula representations, and model architectures. The top three Plots (a)-(c) describe the non-deep base models: Bayesian Networks, Markov Networks, and Kernel SVMs; while the Plots (d)-(i) at the bottom illustrate the architectures of deep base models: MLPs, KANs, and RPN. For MLPs and KANs, Plots (d)-(e) and (g)-(h) illustrate their two-layer and three-layer architectures, respectively. Similarly, for RPN, we present its one-layer and three-layer architectures in Plots (f) and (i).\nBased on the plots shown in Figure 2, we can observe significant differences of RPN compared against these base models, which are summarized as follows:\n\u2022 RPN vs Non-Deep Base Models: Examining the model plots, we observe that all these base model architectures can be represented as graph structures composed of variables and their relationships. The model architecture of the Markov network is undirected, while that of the Bayesian network is directed. Similarly, for MLP, KAN, and RPN, although we haven't shown the variable connection directions, their model architecture are also directed, flowing from bottom to top. The model architectures of both Markov network and Bayesian network consist of variable nodes that correspond only to input features and output labels. In contrast, for kernel SVM, MLP, KAN, and RPN, their model architectures involve not only nodes representing inputs and outputs, but also those representing expansions and hidden layers. Both RPN and kernel SVM involve a data expansion function to project input data into a high-dimensional space. However, their approaches diverge thereafter. Kernel SVM directly defines parameters within this high-dimensional space to integrate expansion vectors into outputs. In contrast, RPN fabricates these high-dimensional parameters via a reconciliation function from a reduced set of parameters instead.\n\u2022 RPN vs Deep Base Models: The difference between RPN and MLP is easy to observe. MLP involves neither data expansion nor parameter reconciliation. Instead, they apply activation functions to neurons after input integration, which is parameterized by neuron connection weights. Unlike MLPs with predefined, static activation functions, the recent"}, {"title": "3 Notations and Background Knowledge on Taylor's Theorem", "content": "This section first introduces the notation system used throughout this paper. Based on the notations, we then briefly present Taylor's theorem as the preliminary knowledge of the RPN model, which will be introduced in the following Section 4."}, {"title": "3.1 Notation System", "content": "In the sequel of this paper, we will use the lower case letters (e.g., x) to represent scalars, lower case bold letters (e.g., x) to denote column vectors, bold-face upper case letters (e.g., X) to denote matrices and high-order tensors, and upper case calligraphic letters (e.g., X) to denote sets. Given a matrix X, we denote X(i, :) and X(:, j) as its ith row and jth column, respectively. The (ith, jth) entry of matrix X can be denoted as X(i, j). We use \\(X^T\\) and \\(x^T\\) to represent the transpose of matrix X and vector x. For vector x, we represent its \\(L_p\\)-norm as \\(||x||_p = (\\sum_i |x(i)|^p)^{\\frac{1}{p}}\\). The Frobenius-norm of matrix X is represented as \\(||X||_F = (\\sum_{i,j} |X(i, j)|^2)^{\\frac{1}{2}}\\) The elementwise product of vectors x and y of the same dimension is represented as \\(x \\odot y\\), their inner product is represented as \\(\\langle x, y \\rangle\\), and their Kronecker product is \\(x \\otimes y\\). The elementwise product and Kronecker product operators can also be applied to matrices X and Y as \\(X \\odot Y\\) and \\(X \\otimes Y\\), respectively."}, {"title": "3.2 Taylor's Theorem for Univariate Function Approximation", "content": "Taylor's theorem approximates a d-times differentiable function around a given point using polynomials up to degree d, commonly referred to as the dth-order Taylor polynomial. In this section, we first introduce Taylor's theorem for univariate functions, which also generalizes to multivariate and vector valued functions. We will briefly describe its extension to multivariate functions in the subsequent Subsection 3.3, and then introduce the RPN model designed based on Taylor's theorem with vector valued functions in Section 4.\nTHEOREM 1 (Taylor's Theorem): Let \\(d \\geq 1\\) be an integer and let function \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\) be d times differentiable at the point \\(a \\in \\mathbb{R}\\). As illustrated in Figure 3, then there exists a function \\(h_a: \\mathbb{R} \\rightarrow \\mathbb{R}\\) such that\n\\[f(x) = \\frac{f(a)}{0!}(x - a)^0 + \\frac{f'(a)}{1!}(x - a)^1 + \\frac{f''(a)}{2!}(x - a)^2 + \\ldots + \\frac{f^{(d)}(a)}{d!}(x - a)^d + R_d(x),\\]\n\\[= \\sum_{i=0}^{d} \\frac{f^{(i)}(a)}{i!}(x - a)^i + R_d(x),\\]\nIn the equation, \\(R_d(x)\\) is also normally called the \u201cremainder\u201d term and can be represented as\n\\[R_d(x) = h_a(x)(x \u2013 a)^d, \\text{ where }\\lim_{x \\rightarrow a} h_a(x) = 0.\\]\nAccording to the above description of the Taylor's Theorem, the function output f(x) can be repre-sent as a summation of polynomials of high degrees of the (x \u2013 a). What's more, in this paper, we propose to further disentangle the variable x from the given constant point a. Terms like \\((x \u2212 a)^k\\) can be decomposed into summations of polynomials in x alone, with a serving as the coefficients:\n\\[(x - a)^d = \\binom{d}{0} (-a)^{d-0}x^0 + \\binom{d}{1} (-a)^{d-1}x^1 + \\ldots + \\binom{d}{d} (-a)^{d-d}x^d.\\]\nBased on the decomposition, we can rewrite the above Equation (4) as follows:\n\\[f(x|a) = \\langle \\kappa(x), \\psi(a) \\rangle + R_d(x),\\]"}, {"title": "3.3 Taylor's Theorem for Multivariate Function Approximation", "content": "Representing multivariate continuous functions with Taylor's polynomials is more intricate. In this part, we use a multivariate function \\(f : \\mathbb{R}^m \\rightarrow \\mathbb{R}\\) as an example to illustrate how to disentangle the input variables and function parameters via Taylor's formula. Similar as the above single-variable function, assuming function f is dth-time continuously differentiable at point \\(a \\in \\mathbb{R}^m\\), then for the inputs near the point can be approximated as\n\\[f(x) = \\sum_{|\\alpha| \\leq d} \\frac{D^\\alpha f(a)}{\\alpha!} (x - a)^\\alpha + R_d(x).\\]"}, {"title": "3.4 Taylor's Theorem based Machine Learning Models", "content": "In real-world problems, the underlying functional mappings are often more intricate, such as \\(f: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n\\) with multiple input variables and multiple outputs. Representing these functions with Taylor's polynomials requires more cumbersome derivations, and the coefficient fabrication outputs should be a two-dimensional matrix, such as \\(\\psi(a) \\in \\mathbb{R}^{n \\times D}\\). To avoid getting bogged down in unnecessary mathematical details, we will not repeat those derivations here.\nIn recent years, there has been a growing interest in designing machine learning and deep learning models based on Taylor's theorem. For binary data inputs, Zhang et al. [86] introduce the reconciled polynomial machine to unify shallow and deep learning models, which is also the prior work that this paper is based on. Balduzzi et al. [4] investigate the convergence and exploration in rectifier networks with neural Taylor approximations, while Chrysos et al. [11] propose a new class of function approximation method based on polynomial expansions. Zhao et al. [89] propose a generic neural architecture TaylorNet based on tensor decomposition to initialize the models, and Nivron et al. [56] introduce to incorporate use Taylor's expansion as a wrapper of transformer for the probabilistic predictions for time series and other random processes. Beyond time series and continuous"}, {"title": "4 RPN: Reconciled Polynomial Network for Deep Function Learning", "content": "Based on the preliminary background introduced above and inspired by the work of [86], we will introduce the Reconciled Polynomial Network (RPN) model for function learning in this section."}, {"title": "4.1 RPN: Reconciled Polynomial Network", "content": "Formally, given the underlying data distribution mapping \\(f: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n\\), we represent the RPN model proposed to approximate function f as follows:\n\\[g(x|w) = \\langle \\kappa(x), \\psi(w) \\rangle + \\pi(x),\\]\nwhere\n\u2022 \\(\\kappa: \\mathbb{R}^M \\rightarrow \\mathbb{R}^D\\) is named as the data expansion function and D is the target expansion space dimension.\n\u2022 \\(\\psi : \\mathbb{R}^l \\rightarrow \\mathbb{R}^{n \\times D}\\) is named as the parameter reconciliation function, which is defined only on the parameters without any input data.\n\u2022 \\(\\pi: \\mathbb{R}^M \\rightarrow \\mathbb{R}^n\\) is named as the remainder function.\nThe architecture of RPN is also illustrated in Figure 4. The RPN model disentangles input data from model parameters through the expansion functions \\(\\kappa\\) and reconciliation function \\(\\psi\\). More detailed information about all these components and modules mentioned in Figure 4 will be introduced in the following parts of this section."}, {"title": "4.2 RPN Component Functions", "content": "The data expansion function \\(\\kappa\\) projects input data into a new space with different basis vectors, where the target vector space dimension D is determined when defining \\(\\kappa\\). In practice, the function \\(\\kappa\\) can either expand or compress the input to a higher- or lower-dimensional space. The corresponding function, \\(\\kappa\\), can also be referred to as the data expansion function (if \\(D > m\\)) and data compression function (if \\(D < m\\)), respectively. Collectively, these can be unified under the term \u201cdata transfor-mation functions\u201d. In this paper, we focus on expanding the inputs to a higher-dimensional space, and will use the function names \"data transformation\" and \"data expansion\" interchangeably in the following sections."}, {"title": "4.3 Wide RPN: Multi-Head and Multi-Channel Model Architecture", "content": "Similar to the Transformer with multi-head attention [78], as shown in Figure 4, the RPN model employs a multi-head architecture, where each head can disentangle the input data and model pa-rameters using different expansion, reconciliation and remainder functions, respectively:\n\\[g(x|w, H) = \\sum_{h=0}^{H-1} (\\kappa^{(h)}(x), \\psi^{(h)}(w^{(h)})) + \\pi^{(h)}(x),\\]\nwhere the superscript \u201ch\u201d indicates the head index and H denotes the total head number. By default, we use summation to combine the results from all these heads.\nMoreover, in the RPN model shown in Figure 4, similar to convolutional neural networks (CNNs) employing multiple filters, we allow each head to have multiple channels of parameters applied to the same data expansion. For example, for the hth head, we define its multi-channel parameters as \\(w^{(h,0)}, w^{(h,1)},..., w^{(h,C-1)}\\), where C denotes the number of channels. These parameters will be reconciled using the same parameter reconciliation function, as shown below:\n\\[g(x|w, H, C) = \\sum_{h=0}^{H-1}\\sum_{c=0}^{C-1} (\\kappa^{(h)}(x), \\psi^{(h)}(w^{(h), c})) + \\pi^{(h)}(x),\\]\nThe multi-head, multi-channel design of the RPN model allows it to project the same input data into multiple different high-dimensional spaces simultaneously. Each head and channel combination may potentially learn unique features from the data. The unique parameters at different heads can have different initialized lengths, and each of them will be processed in unique ways to accommodate the expanded data. This multi-channel approach provides our model with more flexibility in model design. In the following parts of this paper, to simplify the notations, we will illustrate the model's functional components using a single-head, single-channel architecture by default. However, readers should note that these components to be introduced below can be extended to their multi-head, multi-channel designs in practical implementations."}, {"title": "4.4 Deep RPN: Multi-Layer Model Architecture", "content": "The wide model architecture introduced above provides RPN with greater capabilities for approx-imating functions with diverse expansions concurrently. However, such shallow architectures can be insufficient for modeling complex functions. In this paper, as illustrated in Figure 4, we propose to stack RPN layers on top of each other to build a deeper architecture, where the Equation (18) actually defines one single layer of the model. Formally, we can represent the deep RPN with multi-layers as follows:\n\\[\\begin{aligned}\n\\text{Input:} & \\quad h_0 = x, \\\\\n\\text{Layer 1:} & \\quad h_1 = \\langle \\kappa_1(h_0), \\psi_1(W_1) \\rangle + \\pi_1(h_0), \\\\\n\\text{Layer 2:} & \\quad h_2 = \\langle \\kappa_2(h_1), \\psi_2(W_2) \\rangle + \\pi_2(h_1), \\\\\n&\\vdots \\\\\n\\text{Layer K:} & \\quad h_K = \\langle \\kappa_K(h_{K-1}), \\psi_K(W_K) \\rangle + \\pi_K(h_{K-1}), \\\\\n\\text{Output:} & \\quad \\hat{y} = h_K.\n\\end{aligned}\\]\nThe subscripts used above denote the layer index. The dimensions of the outputs at each layer can be represented as a list \\([d_0, d_1,\\cdots,d_{K-1},d_K]\\), where \\(d_1 = m\\) and \\(d_K = n\\) denote the input and the desired output dimensions, respectively. Therefore, if the component functions at each layer of our model have been predetermined, we can just use the dimension list \\([d_0, d_1,\\cdots,d_{K-1},d_K]\\) to represent the architecture of the RPN model."}, {"title": "4.5 Versatile RPN: Nested and Extended Expansion Functions", "content": "The data expansion function introduced earlier projects the input data to a higher-dimensional space. There exist different ways to define the data expansion function, and a list of such basic expansion functions will be introduced in the following Section 5.1. The multi-head, multi-channel and multi-layer architecture also provides RPN with more capacity to build wider and deeper architectures for projecting input data to the desired target space. In addition to these designs, as illustrated in Figure 5, RPN also provides a more flexible and lightweight mechanism to build models with similar capacities via the nested and extended data expansion functions.\nNested expansions: Formally, given a list of n data expansion functions \\(\\kappa_1 : \\mathbb{R}^{d_0} \\rightarrow \\mathbb{R}^{d_1}, \\kappa_2 : \\mathbb{R}^{d_1} \\rightarrow \\mathbb{R}^{d_2},..., \\kappa_n: \\mathbb{R}^{d_{n-1}} \\rightarrow \\mathbb{R}^{d_n}\\), as shown in Plots (a)-(b) of Figure 5, the nested calls of these functions will project a data vector from the input space \\(\\mathbb{R}^{d_0}\\) to the desired output space \\(\\mathbb{R}^{d_n}\\), defining the nested data expansion function \\(\\kappa : \\mathbb{R}^m \\rightarrow \\mathbb{R}^D\\) as follows:\n\\[\\kappa(x) = \\kappa_n (\\kappa_{n-1} (\\cdots \\kappa_2 (\\kappa_1 (x)))) \\in \\mathbb{R}^D.\\]\nwhere the function input and output dimensions should be \\(d_0 = m\\) and \\(d_n = D\\)."}, {"title": "5 List of Expansion, Reconciliation and Remainder Functions for RPN Model", "content": "This section introduces the expansion, reconciliation, and remainder functions that can be used to design the RPN model, all of which have been implemented in the TINYBIG toolkit and are readily available. Readers seeking a concise overview can refer to Figure 6, which summarizes the lists of expansion, reconciliation and remainder functions to be introduced in this section."}, {"title": "5.1 Data Expansion Functions", "content": "The data expansion function determines the complexity of RPN. We will introduce several differ-ent data expansion functions below. In real-world practice, these individual data expansions intro-duced below can also be nested and extended to define more complex expansions, which provides more flexibility in the design of our RPN model."}, {"title": "5.1.1 Identity and Reciprocal Data Expansion", "content": "The simplest data expansion methods are the identity data expansion and reciprocal data expansion, which project the input data vector \\(x \\in \\mathbb{R}^m\\) onto itself and its reciprocal, potentially with minor transformations via some activation functions, as denoted below:\n\\[\\kappa(x) = x \\in \\mathbb{R}^D, \\text{ and } \\kappa(x) = \\frac{1}{x} \\in \\mathbb{R}^D,\\]"}, {"title": "5.1.2 Linear Data Expansion", "content": "In certain cases, we may need to adjust the value scales of x linearly without altering the basis vectors or the dimensions of the space. This can be accomplished through the linear data expansion function. Formally, the linear data expansion function projects the input data vector \\(x \\in \\mathbb{R}^m\\) onto itself via linear projection, as follows:\n\\[\\kappa(x) = cx \\in \\mathbb{R}^D,\\]\nor\n\\[\\kappa(x) = x C_{\\text{post}} \\in \\mathbb{R}^D,\\]\nor\n\\[\\kappa(x) = C_{\\text{pre}} x \\in \\mathbb{R}^D,\\]\nwhere the activation function or norm function \\(\\sigma\\) is optional, and \\(c \\in \\mathbb{R}\\), \\(C_{\\text{post}}, C_{\\text{pre}} \\in \\mathbb{R}^{m \\times m}\\) denote the provided constant scalar and linear transformation matrices, respectively. Linear data expansion will not change the data vector dimensions, and the output data vector dimension \\(D = m\\)."}, {"title": "5.1.3 Taylor's Polynomials based Data Expansions", "content": "Given a vector \\(x = [x_1, x_2, \\cdots, x_m] \\in \\mathbb{R}^m\\) of dimension m, the multivariate composition of order d defined based on x can be represented as a list of potential polynomials composed by the product of the vector elements \\(x_1, x_2, \\cdots, x_m\\), where the sum of the degrees equals d, i.e.,\n\\[P_d(x) = [x_1^{d_1}x_2^{d_2} \\cdots x_m^{d_m}]_{\\forall [d_1, d_2, ..., d_m] \\in \\{0, 1, ...\\}^m, \\sum_i d_i=d}\\]\nSome examples of the multivariate polynomials are provided as follows:\n\\[P_0(x) = [1] \\in \\mathbb{R}^1,\\]\n\\[P_1(x) = [x_1, x_2,...,x_m] \\in \\mathbb{R}^M,\\]\n\\[P_2(x) = [x_1^2, x_1x_2, x_1 x_3, \\cdots, x_1x_m, x_2x_1, x_2^2, x_2 x_3, ..., x_mx_m] \\in \\mathbb{R}^{m^2}.\\]\nWe observe that the above representation of \\(P_2(x)\\) may contain duplicated elements, e.g., \\(x_1x_2\\) and \\(x_2x_1\\). However, this representation simplifies the implementation, and high-order polynomials can be recursively calculated using the Kronecker product operator based on the lower-order ones."}, {"title": "5.2 Parameter Reconciliation Functions", "content": "To approximate the underlying mapping \\(f: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n\\), the data expansion functions \\(\\kappa : \\mathbb{R}^m \\rightarrow \\mathbb{R}^D\\) introduced above projects data instances from input dimension m to an intermediate space of dimension D, where \\(D > m\\). When learning on such expanded data, directly applying existing models and learning approaches with similar parameter scales may suffer from the \"curse of dimensionality\" and overfitting issues, leading to practical failures. Instead of directly defining a parameter of a scale of D, we propose defining functions \\(\\psi : \\mathbb{R}^l \\rightarrow \\mathbb{R}^{n \\times D}\\) to fabricate a parameter vector \\(w \\in \\mathbb{R}^l\\) of length l to the target dimensions using advanced techniques, where \\(l \\ll n \\times D\\).\nThis process is referred to as parameter reconciliation in this paper. The inner product of the expanded data vectors and the reconciled parameter matrices will project the data vectors from input dimension m to an intermediate dimension D and then back to the desired output dimension n. The parameter reconciliation function determines both the learning capacity and costs of the RPN model, and we will introduce several practical ways to fabricate the parameters in defining the parameter reconciliation functions in this section. In addition to the summary provided in Figure 6, we also illustrate the parameter reconciliation functions to be introduced here in Figure 8 as well."}, {"title": "5.2.1 Constant Parameter Reconciliation", "content": "The simplest parameter reconciliation function will be the constant parameter reconciliation, which projects any input parameters to constants (e.g., zeros or ones) as follows:\n\\[\\psi(w|c) = c \\cdot \\mathbb{I}_{n \\times D} = C \\in \\mathbb{R}^{n \\times D},\\]\nwhere the output matrix C of size \\(n \\times D\\) is filled with the provided constant c.\nFor constant parameter reconciliation, the input parameter w is not required, which together with its dimension hyper-parameter l can both be set to none in implementation. If the output constant \\(C = 0\\) or \\(C = 1\\), we can also name the functions as zero reconciliation and one reconciliation,"}]}