{"title": "XNB: EXPLAINABLE CLASS-SPECIFIC NA\u00cfVE-BAYES CLASSIFIER", "authors": ["Jes\u00fas S. Aguilar-Ruiz", "Cayetano Romero", "Andrea Cicconardi"], "abstract": "In today's data-intensive landscape, where high-dimensional datasets are increasingly common, reducing the number of input features is essential to prevent overfitting and improve model accuracy. Despite numerous efforts to tackle dimensionality reduction, most approaches apply a universal set of features across all classes, potentially missing the unique characteristics of individual classes. This paper presents the Explainable Class-Specific Na\u00efve Bayes (XNB) classifier, which introduces two critical innovations: 1) the use of Kernel Density Estimation to calculate posterior probabilities, allowing for a more accurate and flexible estimation process, and 2) the selection of class-specific feature subsets, ensuring that only the most relevant variables for each class are utilized. Extensive empirical analysis on high-dimensional genomic datasets shows that XNB matches the classification performance of traditional Na\u00efve Bayes while drastically improving model interpretability. By isolating the most relevant features for each class, XNB not only reduces the feature set to a minimal, distinct subset for each class but also provides deeper insights into how the model makes predictions. This approach offers significant advantages in fields where both precision and explainability are critical.", "sections": [{"title": "1 Introduction", "content": "Predictive models are important because they provide forecasts that, on many occasions, help to make more reliable decisions in sensitive contexts. The usefulness and effectiveness of these models is widely proven in a multitude of application domains, receiving different names: predictive systems [1], classifier systems [2] or diagnostic systems [3].\nCurrently, there is a wide spectrum of techniques derived from machine learning that can be used to address the design of a predictive model. Most of them generate the model from the data (learning phase) and, once the quality of the model has been verified with some metric (evaluation phase) in a validation process (testing phase), it is applied to new unseen observations (deployment phase). The choice of model depends on several factors, including performance, quality, complexity, and explainability [4].\nPredictive techniques (classification or regression) differ considerably in their basis and properties and, therefore, may also generate uneven results. The choice of these techniques, individually or in combination, will depend on the characteristics of the problem, i.e., the specific requirements and objectives. Most of these techniques belong to the opaque machine learning, which includes methods that do not provide knowledge model, and instead function as black boxes (e.g., neural networks), as opposed to the explainable machine learning, which comprises techniques that generate an interpretable model capable of explaining (partially or totally) the knowledge extracted from data (e.g., decision trees). In short, explainable means providing details about how the model works in order to better understand why a particular decision was made.\nIn the broad field of Artificial Intelligence (AI) there has recently emerged a sudden and remarkable interest in understanding how the model makes decisions in the sense that humans can interpret the knowledge contained in the model [5], named Explainable AI (XAI). Explainable models are increasingly in demand, especially in the field of biomedicine, where understanding can lead to progress, as unfortunately not all the knowledge resides in the model.\nClassification addresses predictive tasks when the target variable is nominal (e.g., type of tumor). One of the simplest, fastest and most effective classification technique is na\u00efve-Bayes (NB). So much so that it often becomes a comparative benchmark in the design of new predictive approaches. However, NB assumes that variables follow a normal distribution, which is a very optimistic assumption, as it is not met by all multimodal distributions, in addition to the unimodal non-Gaussian ones. As the Gaussian model may not work well if the data does not fit the assumption, a non-parametric estimation of the data distribution could provide better fitness. This idea was already introduced in the Flexible Na\u00efve Bayes approach (FNB)[6], which used Gaussian kernel functions to estimate the probability density function of the variables (independently) and incorporated them into the calculation of probabilities for the NB classifier.\nNB also includes another important assumption: variables are conditionally independent given the class. This assumption has enormous implications on the computational cost of the method, as variables can be examined independently. However, when dealing with very high dimensional datasets (thousands of variables), as NB calculates the product of conditional probabilities, the posterior class probability tends quickly to zero. In this highly dimensional context, it would be very effective to determine the variables that are significant for each class, independently.\nThis research addresses two important issues: a) How relevant is each variable for each class?; b) How to improve the posterior probability estimate, given the class? By posing answers to these two questions, it is found that a posterior probability estimation method can be used that allows using only a subset of relevant variables, which is different for each class, thus achieving a higher explanatory ability of the model while maintaining its classification performance. For example, to predict the tumor type of an unseen new case, among three possible tumor types, using several thousand variables, the method would provide a decision based on only 10 variables for type I, 4 variables for type II, and 7 variables for type III. Thus, not only could a better classification performance be achieved, but the model would also have an important explanatory power.\nIn summary, the new approach, named Explainable Class-Specific Na\u00efve-Bayes (XNB) classifier includes two important features: a) the posterior probability is calculated by means of Kernel Density Estimation (KDE); b) the posterior probability for each class does not use all variables, but only those that are relevant for each specific class. From the point of view of the classification performance, the XNB classifier is comparable to NB classifier. However, the XNB classifier provides the subsets of relevant variables for each class, which contributes considerably to explaining how the predictive model is performing. In addition, the subsets of variables generated for each class are usually different and with remarkably small cardinality.\nThe rest of the paper is organized as follows: Section 2 describes the mathematical notation used hereafter; the fundamentals of NB are detailed in Section 3; the application of KDE for the calculation of probabilities, as well as its use in the FNB approach, is explained in Section 4; the novel approach, named XNB, including a discussion on its class-specific focus and a detailed description of the method is provided in Section 5; Section 6 presents a comparative analysis of NB and XNB in terms of classification performance and number of variables selected, carried out with eighteen very-high dimensional genomic datasets; finally, conclusions and future work are summarized in Section 7."}, {"title": "2 Data notation", "content": "Let $\\mathcal{D} = (E, F, v, w)$ be a data set, where $E$ is the set of example (or instance) identifiers, $F$ is the set of feature (variable) identifiers, $v : E \\times F \\rightarrow \\mathbb{R}$ is the function that assigns a real value to a pair $(e, f)$, where $e \\in E$ and $f \\in F$. Within the field of supervised learning, when $w : E \\rightarrow L$ assigns a class label $c$ to an example $e$, with $L = \\{C_1, ..., C_k\\}$, then it is a classification problem; otherwise, if $L \\subset \\mathbb{R}$, then it is a regression problem. In absence of example identifiers (e.g. patient identifiers) or feature identifiers (e.g. gene names), it is convenient to use $E = \\{e_1,..., e_n\\}$ and $F = \\{f_1,..., f_m\\}$, respectively. Henceforth, $v(e_i, f_j)$ will represent a real value, and $w(e_i)$ will represent a class label (e.g. type of tumor).\nBoth the function $v$ and the function $w$ are expressed in tabular form, so that $v$ is determined by an $n \\times m$ matrix of real values, and $w$ as a vector of $n$ values from $L$. In order to simplify the notation, a function is defined to extract the values of sample $e_i$: $\\textit{input} : E \\rightarrow \\mathbb{R}^m$, such that $\\textit{input}(e_i) = (v(e_i, f_1), ..., v(e_i, f_m))$."}, {"title": "3 Na\u00efve-Bayes", "content": "The main goal of any classification problem is to find a general function $\\Omega : \\mathbb{R}^m \\rightarrow L$, learned from $\\mathcal{D}$, such that $\\Omega(\\textit{input}(e_i)) = w(e_i), \\forall e_i \\in \\mathcal{D}$, where $i = \\{1, ..., n\\}$, or at least it maximizes the frequency of the equality (a popular -and biased due to its sensitivity to imbalance [7]\u2013 measure of quality of $\\Omega$ is the relative frequency of success, named accuracy).\nThe NB classifier predicts that a test sample $x = (x_1,...,x_m)$ with $m$ variables, belongs to the class $c_k \\in L$ if and only if $P(c_k|x) > P(c_{k'}|x)$, for $1 < k' < |L|$, with $k' \\neq k$. The class $c_k$ for which $P(c_k|x)$ is maximized is called the maximum posteriori hypothesis and can be calculated using the Bayes' theorem in Eq. 1.\n$P(c_k|x) = \\frac{P(x|c_k)P(c_k)}{P(x)}$\n(1)\nAs $P(x)$ is constant for all classes, only $P(x|c_k)P(c_k)$ needs to be maximized, taking into account that $P(c_k) = \\frac{|c_k|}{|D|}$, being $|D|$ the dataset, and $|c_k|$ the number of samples of class $c_k$ in $D$.\nTo compute $P(x|c_k)$ is extremely expensive, as it would mean to calculate conditional probabilities among variables. Instead, the na\u00efve assumption of class-conditional independence reduces dramatically the calculations to $P(x|c_k) = \\prod_{j=1}^{m} P(x_j|c_k)$. Despite this strong assumption, which is usually not tested, it performs surprisingly well.\nIn short, the process of classifying a new instance $x$ consists of choosing the class $c^*$ with the highest a posteriori probability, as shown in Eq. 2. In NB that probability is obtained by estimating mean and variance for each numerical variable from data and using these two statistics in the Gaussian probability distribution expression.\n$c^* = \\underset{i\\in\\{1,...,k\\}}{\\operatorname{argmax}} P(c_i) \\prod_{j=1}^{m} P(x_j|c_i)$\n(2)\nThree critical assumptions emerge from the NB approach: a) the assumption of normality; b) the assumption of conditional independence of variables given the class; c) all the $m$ variables $x_j$ are equally important and therefore involved in the calculation of the $k$ probabilities for class $c_i$ (Eq. 2). All these issues are inherent to NB. The first one was already addressed by the FNB approach. The approach introduced in XNB addresses the third issue (and uses the approach introduced in FNB to deal with the assumption of normality), providing a method to reduce the feature space dimensionality that will be explained in detail in Sec. 5."}, {"title": "3.1 Normality assumption", "content": "The Shapiro-Wilk normality test was performed with a significance level of 0.05, analyzing the eighteen datasets with a mean of 42,109 variables . The SW test was applied to each variable in each dataset, and the percentage of variables rejecting the null hypothesis of normality (p-value<0.05) was recorded for each dataset (expressed as a ratio within [0,1]). The percentages of variables not following the normal distribution ranged from 31% (GSE46602) to 84% (GSE10797), highlighting the importance of the normality assumption. On average, 57% of the variables rejected the null hypothesis, indicating that more than half of the variables do not follow a normal distribution.\nDespite the high percentage deviating from normality, the results provided by NB are generally very acceptable. However, the assumption of normality might become critical in datasets with tens of thousands of variables (e.g., genomic data [8]), or even millions of variables (e.g., transposable elements [9]).\nA technique like KDE (see subsection 4.1), which better adapts to non-normal distributions, could positively contribute to the classification process."}, {"title": "3.2 Conditional independency assumption", "content": "The assumption of conditional independence of variables given the class is often overlooked. However, this assumption can become significant in very high-dimensional datasets."}, {"title": "4 Flexible Naive Bayes", "content": "NB uses univariate Gaussians to calculate class-conditional marginal densities. However, the procedure can be generalized using one-dimensional kernel density estimates, which do not assume normal probability distribution.\nKernel Density Estimation (KDE) provides a more accurate way to calculate that probability since it does not assume any pre-defined probability distribution. Instead, it estimates from data the density distribution, from which probabilities can be easily calculated."}, {"title": "4.1 Kernel Density Estimation", "content": "Density estimation deals with the problem of estimating probability density functions based on some data sampled from the true unknown probability density function (pdf).\nIn principle, a random variable can be fully described by its pdf, which outlines the likelihood of a particular event happening when the random variable matches, or falls within a range close to, a specific value. However, selecting a pdf model based on assumptions (a priori parametric choice) can be problematic in practice. This is because it may inaccurately represent the actual pdf, for instance, by incorrectly assuming a normal distribution in situations where the data is actually multi-modal. A viable solution to this issue is to employ non-parametric pdf estimators. These estimators allow us to derive the characteristics of the distribution directly from the data, sidestepping the pitfalls of making restrictive assumptions about the distribution's shape. Essentially, the key advantage of the non-parametric method is its flexibility, allowing for a more accurate representation of the distribution $f(x)$ without confining it to a predetermined form.\nA well-known non-parametric estimator of the pdf is the histogram. The histogram is easy to understand and to compute in one and higher dimensions. However, there are two important choices with high impact on the estimates: a) the starting point of each bin edge; b) and the bin size (bandwidth). Also, histograms estimators are usually not smooth, with discontinuous shapes (i.e., points within the same bin will provide the same estimation). This discrete nature of the pdf is an intrinsic limit on resolution, which is highly depending on the selection of bandwidth and boundaries of the bins. Consequently, although it is a simple and fast technique, its usefulness is limited by its instability.\nKernel estimation of pdf is a powerful technique, originated by Rosenblatt [10] and Parzen [11], that produces a smooth empirical pdf based on the neighborhood of each individual sample from a given dataset. Let $\\{x_1,...,X_n\\}$ be an independent and identically distributed (iid) sample of $n$ observations taken from a population $X$ with unknown pdf, $f(x)$. Kernel estimate of $f(x)$, $\\hat{f}(x)$, assigns each i\u2013th sample data point $x_i$ a function $K(\\cdot)$ called a kernel function, which is non-negative and bounded for all $x$ ($0 < K(x) < \\infty \\forall x \\in \\mathbb{R}$). \nKernel functions are usually symmetric probability density functions that need a window width, named bandwidth, which determines the neighborhood size to be used in the estimation. The kernel density estimator at point $x$, also known as Parzen\u2013Rosenblatt estimator, is an estimate of $f(x)$ based on the data, as defined in Eq. 3.\n$\\hat{f}_h(x) = \\frac{1}{nh}\\sum_{i=1}^{n} K(\\frac{x-x_i}{h})$\n(3)\nwhere $h > 0$ is the bandwidth, $K(\\cdot)$ is the kernel function and $n$ is the sample size.\nThe kernel $K(\\cdot)$ is typically said to be of order $p\\in \\mathbb{N}$ if it satisfies Eq. 4\n$\\int_{-\\infty}^{+\\infty} x^j K(x)dx = \\begin{cases} 1 & j = 0\\\\ 0 & j = 1,..., p - 1\\\\ \\lambda_j & j = p \\end{cases}$\n(4)\nwhere the constant $\\lambda \\neq 0$ [12].\nThe most common kernels (uniform, Epanechnikov [13], biweight, triweight) are special cases of the beta polynomial family $B_s(x) = (1 - x^2)^s$, where $s$ is a non-negative integer, and are defined in Eq. 5. These functions have a domain of [-1, +1] and are orthogonal with respect to the beta distribution $B(\\alpha, \\beta)$, with parameters $\\alpha = \\beta = n + 1$.\n$K_s(x) = \\frac{(2s + 1)!!}{2^{s+1}s!}B_s(x)1(|x| \\leq 1)$\n(5)\nIn fact, the most popular kernel, the Gaussian kernel defined in Eq. 6, is the limit of the beta polynomial function as the degree of the polynomial approaches infinity [14].\n$K(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}$\n(6)\nUsually, the symmetry assumption is imposed on the kernel $K(x) = K(-x)$, so that $p$ is necessarily even. As $\\int K(x)dx = 1$ (for $j = 0$), when $K$ is non-negative, it is itself a probability density. Symmetric non-negative kernels are second-order kernels (e.g., the Gaussian kernel is of order 2). When $p > 2$ (high-order) the kernels will have negative parts and are not probability densities (referred to as bias-reducing kernels)."}, {"title": "4.2 KDE-based na\u00efve-Bayes", "content": "John and Langley's work [6] focused on enhancing the NB classifier, but their specific contribution involved using KDE as an alternative to assuming Gaussian distributions for continuous variables. By employing KDE, they proposed a method, named Flexible Na\u00efve\u2013Bayes (FNB), that did not assume a specific parametric form for the distribution of the data. This flexibility allows the model to adapt to the actual shape of the data distribution, which can vary widely across different features and datasets, thus potentially improving classification performance, especially in cases where the data distributions are far from Gaussian. Therefore, FNB should perform well in domains that violate the normality assumption.\nIn the FNB approach, to compute the probability $P(x_j|c_i)$ in Eq. 2, the Eq. 3 is used, but limiting the samples to those belonging to class $c_i$, as shown in Eq. 8.\n$P(x_j|c_i) \\approx f_n(x|c_i) = \\frac{1}{n_i} \\sum_{x_j \\in c_i} K(\\frac{x_j - x_i}{h})$\n(8)\nwhere $n_i$ is the total number of samples $x_j$ that belongs to class $c_i$.\nThe experiments carried out in [6] only used eleven datasets with a number of variables ranging from 8 to 24, but showed that FNB performed significantly better than NB for only five datasets. However, FNB revealed worse performance with datasets containing greater number of continuous features, which might be an issue in the context of genomic datasets.\nIt is well known that as the dimensionality of the feature space increases, the classification performance can deteriorate, because data points become increasingly sparse, leading to degraded performance of the classifier. This phenomenon, named the curse of dimensionality, coined by Bellman [18], introduces more difficulty in Bayes-based approaches. For very high-dimensional spaces, as it is the case of genomic datasets, NB-based methods need to compute the product of very many probabilities, which negatively impact the model's performance as they quickly tend to zero. A strategy to mitigate this issue is to first employ feature selection or dimensionality reduction techniques (such as Principal Component Analysis) [19]. However, none of these methods consider the potential relevance of each individual class in the further classification performance. This important aspect will be considered in detail in Section 5.\nIn summary, integrating KDE into the NB classifier represents an important step toward creating more adaptable and accurate probabilistic models, especially for handling continuous data in various applications. This approach demonstrates the potential for enhancing traditional models with non-parametric methods to better reflect the complexities of real-world data."}, {"title": "5 Explainable Class\u2013Specific Na\u00efve-Bayes", "content": "The genomics boom has led to the emergence of datasets with thousands of variables. Furthermore, transposable elements have further increased dimensionality to the order of millions of variables [9]. This new scenario has introduced fresh challenges for existing algorithms and has spurred research into the development of efficient techniques capable of handling such a vast number of features from two perspectives: performance and explainability.\nFeature selection techniques can be broadly categorized into two groups: class-independent and class-specific. The scientific literature on class-independent feature selection has been extensively prolific, amassing thousands of articles indexed in Scopus. However, this level of productivity has not been mirrored in the domain of class-specific feature selection, where fewer than two hundred articles have been published in journals since its inception in 1997 to date.\nClass-independent techniques encompass the majority of traditional feature selection methods, such as Information Gain [20], $\\chi^2$ Test [21], and ReliefF [22]. These techniques aim to identify a global set of attributes relevant for the entire dataset without differentiating between its classes, and they are primarily used in classification tasks.\nIn contrast, class-specific feature selection focuses on each class independently to ascertain a relevant subset of attributes for each class. This approach allows for a tailored analysis of each class, identifying attributes that are particularly significant for each one. The results from this process can either be utilized separately for each class or combined using an aggregation method to form a comprehensive attribute subset that encompasses insights from all classes.\nThe scenario becomes more complex in multi-class problems, where datasets feature more than two class labels. In these cases, a variable might be highly relevant for one class but not for others. Therefore, it is crucial that the variables selected for such datasets collectively possess the ability to discriminate each class effectively. This method is especially valuable in fields such as healthcare, where the significance of different attributes can vary greatly in identifying specific ailments or diseases.\nThe early stages of machine learning were dominated by the tendency to encompass all variables, which was far from the principle of feature parsimony, crucial for predictive accuracy. Nowadays, where very high dimensional contexts exist [8], restricting the number of input features helps mitigate overfitting and enhances the models' ability to make accurate estimates. However, although there have been many efforts to reduce the dimensionality faced by a classifier, few instead have attempted to build the classifier based on a non-homogeneous set of features, i.e., feature sets selected separately for each class, as opposed to the universal approach that performs the selection jointly for the entire dataset.\nBaggenstoss made significant contributions to the development of a class-specific model of behavior, focusing on the separation of attributes by class. In his earlier works [23, 24], he introduced the concept of class-specific modeling, aimed at estimating low-dimensional probability density functions while maintaining the theoretical effectiveness of classification. Subsequent research by Baggenstoss [25, 26, 27] further elaborated the class-specific approach by employing an invertible and differentiable multidimensional transformation to generate new features. This approach represents a feature extraction procedure rather than a mere selection of existing attributes from the dataset. In this context, the generated features are class-specifically derived or extracted, signifying that they are not subsets of the original dataset attributes, but are instead new features created through transformation processes.\nA general framework for class-specific feature selection, based on the one-against-all strategy is proposed in [28]. They transform the $L$-class problem into $L$ binary problems, one for each class, and then the classification is performed by an ensemble strategy. For each class, after one-against-all class binarization, an oversampling technique is applied to balance the dataset, and then a feature selection method extracts the relevant features for that class. Although the approach is very interesting, it focuses on building several classifiers, each using a different subset of features, and then integrating them into an ensemble scheme, which will aggregate the classification outputs. In contrast, the novelty of our approach lies in integrating the class-specific feature selection into a single classifier."}, {"title": "5.2 Method", "content": "The method presented in this work, named Explainable Naive Bayes (XNB), incorporates the concept of class-specific feature selection. Unlike traditional approaches that construct separate classifiers for each class, XNB integrates the selected features directly into the probability calculations of the FNB model. Specifically, XNB calculates the probabilities of variables using KDE, but it selectively includes only those features that are relevant for each class. This approach streamlines the process by focusing on pertinent features, thereby enhancing the model's efficiency and interpretability.\n$\\hat{f}_h(x) = \\frac{1}{nh}\\sum_{i=1}^{n} K(\\frac{x-x_i}{h})$\n(3)\n$P(x_j|c_i) \\approx f_n(x|c_i) = \\frac{1}{n_i} \\sum_{x_j \\in c_i} K(\\frac{x_j - x_i}{h})$\n(8)\n$H(P,Q) = \\frac{1}{\\sqrt{2}} \\sqrt{\\sum_{i=1}^{K} (\\sqrt{p_i} - \\sqrt{q_i})^2}$\n(9)\n$[V]_{c_i} = \\underset{S}{\\operatorname{min}} \\{S \\subseteq F|D(S, c_i) > \\theta\\}$\n(10)\n$D(S, c_i) = 1 - \\prod_{c_j \\in L \\atop c_j \\neq c_i} \\prod_{v \\in S} (1 - H(c_i, c_j|v))$\n(11)\n$c^* = \\underset{i\\in\\{1,...,k\\}}{\\operatorname{argmax}} P(c_i) \\prod_{U_j\\in\\Psi(c_i)}P(U_j|c_i)$\n(12)"}, {"title": "5.3 Complexity", "content": "The complexity of calculating the bandwidths and building the KDE models is $O(mnk)$, where $m$ is the number of variables, $n$ is the number of instances, and $k$ is the number of classes. Determining the Hellinger distances between each pair of KDE models for each variable and selecting the relevant variables for each class has a complexity of $O(mnk^2)$. Subsequently, the complexity of building the final model is $O(mnk)$. Thus, the overall complexity of XNB is $O(mnk^2)$. This represents a slight increase by a factor of $k$ compared to the complexity of NB, which is $O(mnk)$. However, this increase is not significant as the number of classes is typically very low compared to the number of variables or samples ($k < m$ and $k \\ll n$). The small added complexity is a worthwhile trade-off for the increased explainability and potential improvements in classification performance, particularly in high-dimensional spaces."}, {"title": "6 Experiments", "content": "A comparative analysis was performed with datasets chosen from the Curated Microarray Database (CuMiDa)[8], with varying number of samples (21\u2013281), very high number of variables ranging from about 22,277 to 54,675, and number of classes from 2 to 7. Detailed description of datasets is shown in Table 1. The total number of real values processed ranges from 1.04 millions (GSE11682) up to 8.26 millions (GSE45827).\nAll the classification performance results were obtained by averaging the results of stratified 10-fold cross-validation. For each dataset two classification methods were applied: NB (the standard Na\u00efve-Bayes), and XNB (eXplainable class-specific Na\u00efve-Bayes). Table 2 shows that classification performance for both methods is very similar (see last row: Mean). However, there is an outstanding improvement in terms of the number of factors involved in the model, i.e., of the number of explanatory variables in the process of cancer prediction. Last row in Table 2 shows also the mean of number of variables involved in the classification model for XNB, achieving an extremely low value of 8.3, which means a remarkable average reduction of the feature space dimensionality of about 99.98%.\nFigure 1 illustrates that the average number of variables needed for the classification task on the Brain GSE50161 dataset, with 54,675 variables and 5 classes, is 8.8. The XNB model also provides detailed information on the specific variables involved in the prediction of each type of cancer. While the type glioblastoma needs 14 variables, the type"}, {"title": "7 Conclusions", "content": "Class-specific strategies concentrate on evaluating the discriminatory power of features within each class individually, which enables a more refined understanding of how features correlate with each specific class. This method can lead to enhanced discrimination between classes, as it allows for identifying the most informative features for each class separately. As a result, the interpretability of the classification model is improved, and it facilitates the extraction of domain-specific insights. Additionally, class-specific strategies are inherently more resilient to class overlap, where classes may share common feature distributions, by focusing on the distinctiveness of features within each class.\nExplainability plays a critical role in understanding how features contribute to predictions for different classes. Class-specific strategies enhance this explainability by allowing for the analysis of why particular features are deemed important or discriminatory for specific classes. This understanding is essential for fields like personalized medicine, where knowing the rationale behind model predictions can inform clinical decisions.\nWhile the na\u00efve-Bayes assumption assumes that features are conditionally independent given the class, this assumption may not hold for all classes. Analyzing variables from a class-specific perspective helps identify features that are not independent for certain classes because they share similar probability distributions, and will be discarded as discriminatory variables for those classes.\nAs shown in the experimental analysis the number of variables involved in the classification model has been drastically reduced (using on average less than 0.02% of the original number of variables). Despite this important reduction, the model maintains high classification performance, with an average of about 8 variables per dataset. This reduction makes the model significantly more explainable, reducing the risk of overfitting while maintaining the quality of the predictive model.\nIn summary, XNB significantly enhances the explainability of the model by reducing the feature space to a minimal subset for each class. This not only improves interpretability but also reduces the risk of overfitting, making the model more robust. The potential applications of XNB in real-world scenarios, particularly in high-dimensional domains, where both explainability and accuracy are crucial, are substantial and promising.\nSeveral aspects would require more analysis for improving the XNB approach in future work. For instance, when the underlying density has long tails or in presence of multimodality, adaptive bandwidth methods might offer better fitting by regulating the trade-off between variance and bias. Although the performance of the Hellinger distance is excellent, determining which is the most informative distance function would require an exhaustive analysis of not only distance metrics but also divergence measures for probability distributions (Jeffreys distance [41], Wasserstein distance [42] or Kullback-Leibler divergence [43], among others)."}, {"title": "Software availability", "content": "The XNB source code is available in Github at https://github.com/sorul/xnb."}]}