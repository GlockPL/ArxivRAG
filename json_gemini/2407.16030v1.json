{"title": "Enhancing Temporal Understanding in LLMs for Semi-structured Tables", "authors": ["Irwin Deng", "Kushagra Dixit", "Vivek Gupta", "Dan Roth"], "abstract": "Temporal reasoning over tabular data presents substantial challenges for large language models (LLMs), as evidenced by recent research. In this study, we conduct a comprehensive analysis of temporal datasets to pinpoint the specific limitations of LLMs. Our investigation leads to enhancements in TempTabQA, a dataset specifically designed for tabular temporal question answering. We provide critical insights for improving LLM performance in temporal reasoning tasks with tabular data. Furthermore, we introduce a novel approach, C.L.E.A.R to strengthen LLM capabilities in this domain. Our findings demonstrate that our method significantly improves evidence-based reasoning across various models. Additionally, our experimental results reveal that indirect supervision with auxiliary data substantially boosts model performance in these tasks. This work contributes to a deeper understanding of LLMs' temporal reasoning abilities over tabular data and promotes advancements in their application across diverse fields.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have exhibited remarkable proficiency across various natural language processing tasks. However, recent investigations reveal a notable deficiency in their ability to reason effectively over tabular data, particularly when temporal relationships are involved (Chen, 2023; Sui et al., 2024). This discrepancy between model performance and human-level understanding underscores the pressing need for innovative approaches to enhance the capabilities of LLMs in this domain.\nTo explore the root causes behind the limitations in reasoning about structured or semi-structured data, we conducted a thorough analysis of TempTabQA (Gupta et al., 2023) dataset, an important and"}, {"title": "2 Temporal Reasoning on Tables", "content": "Tables organize and record diverse types of information, making them useful for studying an entity's timeline. They provide a chronological sequence of events, facilitating the analysis of the progression of positions, marital status changes, and awards, thereby serving as reliable sources for temporal reasoning. Entity-centric tables, like Wikipedia Infoboxes, significantly differ from unstructured data and fully structured data (e.g., SQL tables and knowledge graphs).\nRecent studies show that Large Language Models (LLMs) struggle with reasoning over tabular data, especially concerning temporal aspects. Various datasets have been used to explore LLMs' understanding of tabular data, including TempTabQA (Gupta et al., 2023), Table2vec (Zhang et al.,\n2019), TAPAS (Herzig et al., 2020), TaBERT (Yin et al., 2020), TabStruc (Zhang et al., 2020a), TABBIE (Iida et al., 2021), TabGCN (Pramanick and Bhattacharya, 2021), and RCI (Glass et al., 2021).\nThese datasets provide various contexts and challenges, aiding in the development and evaluation of models for tabular data processing.\nAmong these datasets, we focus on TempTabQA, a recent and prominent dataset for temporal reasoning over tabular data. TempTabQA features temporal question-answer pairs derived from Wikipedia Infobox tables and is notable for its two distinct eval sets: the \"Head\" set, with pairs from popular and frequent domains, and the \"Tail\" set, with pairs from less common and rare domains. The eval sets include approximately 2,900 question-answer pairs, with around 1,900 in the \"Head\" set and 1,000 in the \"Tail\" set. This bifurcation enables comprehensive evaluation, assessing models' performance across both frequent and rare domains."}, {"title": "3 Where do LLMs Fails?", "content": "We used Chain of Thought prompting with GPT-3.5 to analyze the TempTabQA test set and evaluate current models' performance and limitations. Out of 1,038 examples, 339 were incorrect responses. Of these, 159 errors were due to data issues, while 180 were due to model limitations. These errors fall into the following categories:\n1. Tabular Data Issues (75 examples): These"}, {"title": "4 Methodology", "content": "Improving the temporal reasoning capabilities of Large Language Models is crucial for enhancing their performance on tasks involving time-based data. Current models often struggle with accurately interpreting and reasoning about temporal information. This limitation reduces their effectiveness in real-world applications.\nOur analysis revealed several key areas where current models underperform in reasoning over tabular data, especially in temporal contexts. To address these challenges, we explored two approaches. First, we developed a novel method called C.L.E.A.R (Comprehend, Locate, Examine, Analyze, Resolve). This method was inspired by our detailed examination of the TempTabQA test set. C.L.E.A.R instructs models specifically for tabular data reasoning tasks. It employs advanced prompting techniques to guide models more effectively, aiming to reduce errors and enhance accuracy in temporal question answering. Secondly, we fine-tuned models using auxiliary temporal data from unrelated domains to enhance cross-generalization. This approach boosts temporal understanding on complex temporal tasks."}, {"title": "4.1 C.L.E.A.R Prompting", "content": "In this section, we introduce the C.L.E.A.R (Comprehend, Locate, Examine, Analyze, Resolve) technique, designed to enhance temporal reasoning over semi-structured data. C.L.E.A.R is a structured, step-by-step approach that guides the process of understanding, identifying, examining, analyzing, and resolving questions involving temporal reasoning. It ensures a comprehensive and logical extraction of information from tables (refer Fig. 2).\n(a.) Comprehend: The first step, Comprehend, involves applying domain knowledge to understand the given question. For example, if the question pertains to calculating time differences, it is essential to recognize that this involves subtracting the earlier year from the later year. This step ensures the correct interpretation of the question and sets the foundation for correct structural solution analysis.\n(b.) Locate: The Locate step focuses on identifying and extracting the relevant rows from the table that directly pertain to the question. This involves explaining the rationale behind the selection of these rows to provide transparency and clarity. For instance, if the question asks for events be-"}, {"title": "4.2 Fine Tuning with Auxiliary Data", "content": "Fine-tuning models can significantly boost their capabilities by adjusting parameters through training on task-specific examples. In this section, we demonstrate how models' temporal reasoning can be enhanced inherently, not only through fine-tuning on specific data but also by integrating auxiliary data sources.\nAuxiliary data, such as temporal unstructured data, does not directly relate to the main task but contains relevant logical structures and principles. This type of data enhances the model's understanding of underlying logic, improving the overall performance. The TRAM dataset, illustrated in Figure 5 in Appendix A, serves as an auxiliary source to boost model performance. It includes temporal questions that aid the model in grasping temporal relationships across diverse contexts.\nThe TRAM dataset, though unrelated to tabular data temporal reasoning, enhances model temporal reasoning skills. Exposing the model to diverse temporal questions improves its ability to handle temporal relationships, boosting performance. This approach shows leveraging auxiliary data can effectively address temporal reasoning complexities and enhance model robustness in diverse scenarios."}, {"title": "5 Experimental Setup", "content": "Models: In this paper, we experimented with several state-of-the-art large language models (LLMs), including GPT-3.5-Turbo, GPT-4, PaLM-2, Mistral-2-7B, LLaMA-2-7B-chat, and Gemini 1.5 Pro Flash. These models represent the forefront in both open-source and closed-model applications, showcasing advancements in natural language understanding and generation capabilities.\nPrompts: Prompting models with detailed instructions enhances their understanding of tasks, leading to improved responses. These prompts may include demonstrations for the model's reference. We explore the following prompting techniques:\nChain of Thought (CoT) (Wei et al., 2023): CoT prompts guide the model through a series of interconnected thoughts or reasoning steps, encouraging coherent and structured responses. We apply CoT in both zero-shot (Z.S) and few-shot (F.S) settings to evaluate its impact on model performance.\n- Faithful Chain of Thought (F-CoT) (Lyu et al., 2023): F-CoT extends CoT by emphasizing fidelity to the initial prompt throughout response generation to maintain consistency and accuracy in model outputs. We apply F-CoT in both zero-shot and few-shot scenarios to evaluate its effectiveness.\nProgram of Thought (PoT) (Chen et al., 2023): PoT provides the model with a predefined sequence of operations, akin to a program, to generate structured and task-specific responses. Like CoT and F-COT, PoT is evaluated in zero-shot and few-shot"}, {"title": "6 Results and Analysis", "content": "In this section, we present the results of enhancing temporal reasoning in Large Language Models (LLMs) for tabular data tasks. We evaluate the effectiveness of two strategies: C.L.E.A.R (Comprehend, Locate, Examine, Analyze, Resolve), a novel method tailored for tabular data reasoning, and the integration of out-of-domain temporal data for model fine-tuning."}, {"title": "6.1 C.L.E.A.R Prompting", "content": "We analyzed C.L.E.A.R prompting against effective techniques like CoT, F-CoT, and PoT in zero-shot and few-shot settings. Our goal was to evaluate their performance across various models and determine the most effective approach.\nAs shown in Table 1, C.L.E.A.R consistently outperforms other techniques across various models without fine-tuning, except for LLaMA-2. For GPT-3.5 Turbo, our method achieves a 4.5% performance boost compared to the next best technique. For PaLM-2, it leads to a 2.7% improvement.\nThese findings highlight C.L.E.A.R prompting's effectiveness in enhancing model performance, par-"}, {"title": "6.2 Efficacy of C.L.E.A.R", "content": "The results in Section 6.1 demonstrate that our method is more effective than other prompting techniques. However, is C.L.E.A.R trustworthy? Does it really enhance the model's evidence-based reasoning capabilities? (Gupta et al., 2021). Our experiments compare C.L.E.A.R with Zero Shot and Few Shot Chain of Thought (CoT) approaches across tasks highlighting model deficiencies. Below are each of task descriptions:\n1. Original Table: The model uses the original table to answer the question, testing its ability to effectively utilize the provided data.\n2. Without Table: The model is asked to answer the question without access to the table, testing whether the model has memorized the answers or can deduce them independently. Here, an ideal model's performance should suffer as it doesn't use pre-trained knowledge."}, {"title": "6.3 Auxiliary Data Fine Tuning", "content": "C.L.E.A.R enhances the model's capacity to process and reason with context. It facilitates effective instruction for models to handle temporal questions and improve evidence-based reasoning. However, prompting alone does not adjust their inherent parameters, so models do not intrinsically improve.\nInnate enhancement in models requires fine-tuning, so that their parameters better reflect improved temporal reasoning capabilities. We suggest fine-tuning the model on simple auxiliary data to enhance its ability to reason across diverse data formats and improve overall reasoning capabilities.\nWhy TRAM dataset? We evaluated the impact of fine-tuning GPT-3.5 Turbo using the auxiliary datasets discussed in Section 5, along with the TempTabQA dataset. For this evaluation, we fine-tuned with 100 examples from each dataset.\nAnalysis. Table 4 shows that the TRAM dataset offers the largest performance improvement among auxiliary datasets. While DATE and Temporal Sequences datasets provide gains of less than 1.01%, TRAM achieves a significant 2.04% boost compared to the base model without fine-tuning. These findings emphasize TRAM's unique advantages. DATE focuses solely on date-based reasoning, and Temporal Sequences on temporal reasoning across events. In contrast, TRAM includes 10 sub-datasets covering diverse temporal reasoning tasks. It includes tasks such as Ordering, Frequency, Duration, Typical time, Ambiguity Resolution, Arithmetic, Relations, Temporal NLI, Causality and Storytelling. Each task have multiple question type including Commonsense, Facts, Reading Comprehension, Application, Computation, Direct & Multistep Comparison, Interpretation, Calender shift, Long-term, Mid-term & Short-term shift, Date Computation, 12 & 24 hour adjustment, Week Identification etc. (For more information on TRAM, refer to Table 6 in Appendix A). This diversity enhances performance by exposing models to varied temporal scenarios, improving overall comprehension in temporal domain.\nThe TRAM dataset's significant gains highlight the importance of diverse auxiliary data for fine-tuning. By covering a wide range of temporal reasoning tasks, TRAM helps models grasp nuanced temporal relationships, enhancing performance across various challenges. This finding suggests that future efforts in model fine-tuning should consider leveraging similarly diverse datasets to maximize performance improvements.\nFine-tuning on TRAM. We analyzed fine-tuning models on subsets of the TRAM dataset and TempTabQA, each with 100 and 1000 examples (evenly sampled across tasks), as shown in Table 1.\nAnalysis. Our findings reveal that, on the TRAM dataset with fine-tuning on 100 examples, C.L.E.A.R outperforms all models except for LLaMA-27B. This trend is consistent with 1000 examples, where it slightly trails behind the few-shot CoT for GPT-3.5 turbo by a negligible margin of 0.07%.When fine-tuning models on TempTabQA with 100 examples and 1000 examples, our method demonstrates superior performance for GPT 3.5 turbo and PaLM-2, but underperforms for LLaMA-2 7B and Mistral-2 7B.\nOur findings show that fine-tuning models with auxiliary data significantly improves performance, close to that of task-specific fine-tuning. This method enhances models' ability to reason about temporal information and can be applied across various tasks. Additionally, auxiliary data typically provides richer resources compared to task-specific datasets. This scalability enables fine-tuning on"}, {"title": "7 Comparison with Related Work", "content": "Tabular Reasoning Recent research has extensively explored large language models' applications with semi-structured tabular data (Chen et al., 2020b; Gupta et al., 2020; Zhang and Balog, 2019), covering areas such as question answering and semantic parsing (Zhang et al., 2020b; Zhang and Balog, 2020; Pasupat and Liang, 2015; Krishnamurthy et al., 2017; Abbas et al., 2016; Sun et al., 2016; Chen et al., 2020c; Lin et al., 2020; Zayats et al., 2021; Oguz et al., 2020; Chen et al., 2021b; Iyyer et al., 2017), as well as table-to-text generation (Parikh et al., 2020; Li et al., 2021; Nan et al., 2021; Yoran et al., 2021; Chen et al., 2020a). Various datasets and models have emerged to handle semi-structured data, including Table2vec (Zhang et al., 2019), TAPAS (Herzig et al., 2020), TaBERT (Yin et al., 2020), TabStruc (Zhang et al., 2020a), TABBIE (Iida et al., 2021), TabGCN (Pramanick and Bhattacharya, 2021), and RCI (Glass et al., 2021), all aimed at improving understanding and representation of tabular data. Research has also focused on fine-tuning models for enhanced inference on tabular data, as demonstrated by studies (Yu et al., 2018; Eisenschlos et al., 2020; Neeraja et al., 2021), showcasing targeted techniques to boost model performance.\nRecently, (Srivastava et al., 2024) introduced EEDP, a novel approach for financial document Question Answering. EEDP retrieves finance domain information, extracts relevant table rows, and decomposes mathematical tasks into atomic operations. Our method similarly involves understanding table data and extracting relevant rows, but we differentiate by systematically addressing sub-problems based on extracted evidence. This targeted approach makes our method well-suited for temporal tabular reasoning.\nTemporal Reasoning In temporal question answering, recent datasets like TIME-SENSITIVEQA (Chen et al., 2021c) and TORQUE (Ning et al., 2020) focus on entity-specific reading comprehension with time-sensitive questions. Other datasets like TEMPQA-WD (Neelam et al., 2022), CRONQUESTIONS (Saxena et al., 2021), and TEMPQUESTIONS (Jia et al.,\n2018a) explore temporal links in knowledge graph"}, {"title": "8 Conclusion and Future Work", "content": "Our results demonstrate that C.L.E.A.R (Comprehend, Locate, Examine, Analyze, Resolve) prompting significantly enhances model performance, particularly in understanding tabular data and tasks requiring temporal reasoning. It promotes grounding LLMs in evidence context rather than relying solely on pre-trained knowledge. Moreover, fine-tuning LLMs with auxiliary temporal data has proven highly effective in enhancing temporal understanding. This study validates that integrating auxiliary data strengthens models' temporal reasoning capabilities, thereby enhancing the robustness of large language models. For Future, we propose (a.) Generation of Synthetic Data: Creating synthetic training data from temporal aspects of tabular data enhances model performance through diverse temporal exposure. (b.) Neuro-symbolic Learning: Integrating neural networks with symbolic reasoning improves models' understanding of temporal information for more effective solutions. (c.) Expanding C.L.E.A.R Prompting Applica-"}, {"title": "Limitations", "content": "The experiments in this paper have been conducted exclusively on the English language. This study can be extended to a multilingual setting to evaluate the approach's effectiveness across different languages. Additionally, the temporal datasets used in our study are limited to simple, entity-centric tables. Since structured data can exist in more complex forms, such as hierarchical tables, further research is necessary to assess the impact of our methods on these more complex structures.\nMoreover, our computational limitations restricted us to fine-tuning models on only 1000 samples of auxiliary data. To fully understand the potential improvements from fine-tuning on auxiliary data, it is essential to explore the effects of fine-tuning on larger datasets. Future work should focus on overcoming these limitations to provide a comprehensive evaluation of our approach."}, {"title": "Ethics Statement", "content": "We confirm that our work adheres to the highest ethical standards in research and publication. We will publicly release our code and enhanced evaluation set to enable the research community to validate and build upon our findings. We are committed to the responsible and fair use of computational linguistics methodologies. The claims in our paper accurately reflect the experimental results. While using black-box large language models introduces some stochasticity, we mitigate this by maintaining a fixed temperature. We utilize an AI assistive tools for writing while ensuring absence of bias. We provide comprehensive details on annotations, dataset splits, models used, and prompting methods tried, ensuring the reproducibility of our work."}, {"title": "9 Acknowledgement", "content": "Research was sponsored by the Army Research Office and was accomplished under Grant Number W911NF-20-1-0080. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing"}]}