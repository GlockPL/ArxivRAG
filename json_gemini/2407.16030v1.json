{"title": "Enhancing Temporal Understanding in LLMs for Semi-structured Tables", "authors": ["Irwin Deng", "Kushagra Dixit", "Vivek Gupta", "Dan Roth"], "abstract": "Temporal reasoning over tabular data presents substantial challenges for large language models (LLMs), as evidenced by recent research. In this study, we conduct a comprehensive analysis of temporal datasets to pinpoint the specific limitations of LLMs. Our investigation leads to enhancements in TempTabQA, a dataset specifically designed for tabular temporal question answering. We provide critical insights for improving LLM performance in temporal reasoning tasks with tabular data. Furthermore, we introduce a novel approach, C.L.E.A.R to strengthen LLM capabilities in this domain. Our findings demonstrate that our method significantly improves evidence-based reasoning across various models. Additionally, our experimental results reveal that indirect supervision with auxiliary data substantially boosts model performance in these tasks. This work contributes to a deeper understanding of LLMs' temporal reasoning abilities over tabular data and promotes advancements in their application across diverse fields.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have exhibited remarkable proficiency across various natural language processing tasks. However, recent investigations reveal a notable deficiency in their ability to reason effectively over tabular data, particularly when temporal relationships are involved (Chen, 2023; Sui et al., 2024). This discrepancy between model performance and human-level understanding underscores the pressing need for innovative approaches to enhance the capabilities of LLMs in this domain.\nTo explore the root causes behind the limitations in reasoning about structured or semi-structured data, we conducted a thorough analysis of TempTabQA (Gupta et al., 2023) dataset, an important and"}, {"title": "Temporal Reasoning on Tables", "content": "Tables organize and record diverse types of information, making them useful for studying an entity's timeline. They provide a chronological sequence of events, facilitating the analysis of the progression of positions, marital status changes, and awards, thereby serving as reliable sources for temporal reasoning. Entity-centric tables, like Wikipedia Infoboxes, significantly differ from unstructured data and fully structured data (e.g., SQL tables and knowledge graphs).\nRecent studies show that Large Language Models (LLMs) struggle with reasoning over tabular data, especially concerning temporal aspects. Various datasets have been used to explore LLMs' understanding of tabular data, including TempTabQA (Gupta et al., 2023), Table2vec (Zhang et al., 2019), TAPAS (Herzig et al., 2020), TaBERT (Yin et al., 2020), TabStruc (Zhang et al., 2020a), TABBIE (Iida et al., 2021), TabGCN (Pramanick and Bhattacharya, 2021), and RCI (Glass et al., 2021).\nAmong these datasets, we focus on TempTabQA, a recent and prominent dataset for temporal reasoning over tabular data. TempTabQA features temporal question-answer pairs derived from Wikipedia Infobox tables and is notable for its two distinct eval sets: the \"Head\" set, with pairs from popular and frequent domains, and the \"Tail\" set, with pairs from less common and rare domains. The eval sets include approximately 2,900 question-answer pairs, with around 1,900 in the \"Head\" set and 1,000 in the \"Tail\" set. This bifurcation enables comprehensive evaluation, assessing models' performance across both frequent and rare domains."}, {"title": "Where do LLMs Fails?", "content": "We used Chain of Thought prompting with GPT-3.5 to analyze the TempTabQA test set and evaluate current models' performance and limitations. Out of 1,038 examples, 339 were incorrect responses. Of these, 159 errors were due to data issues, while 180 were due to model limitations. These errors fall into the following categories:\n1. Tabular Data Issues (75 examples): These errors were related to hallucinations, incomplete evidence extraction, missing evidence, or incorrect information extraction.\n2. Temporal Calculation Errors (84 examples): These involved difficulties with calculations related to time, such as determining age, calculating the time between dates in different months, or assessing whether a value fell within a specified range.\n3. Other Errors (31 examples): This category included errors stemming from common-sense reasoning, arithmetic, and other miscellaneous issues.\nOur observations indicate that even with Chain of Thought reasoning, models generate incorrect responses consistently. The models not only produce incorrect answers but also exhibit hallucinations, struggling with temporal calculations and common-sense reasoning. This emphasizes the need for enhance models performance in this domain. The 159 data-related issues fall into these categories:\n1. Tables Requiring External Knowledge to Answer (75 examples): These questions could not be answered correctly without additional information not present in the table, indicating a gap in the information provided in the context.\n2. Wrong Human Annotation or Multiple Correct Answers (42 examples): These instances involved incorrect annotations by humans or questions that had multiple valid answers, but the annotations provided only one correct answer.\n3. Ambiguous or Incomplete Questions (14 examples): These questions are either vague or lacked sufficient detail required for correct answer.\n4. Other Issues (28 examples): This category included various problems such as questions relying on images within the HTML table or missing rows in the JSON table etc.\nThrough our analysis, we addressed all these categories of errors and created a new evaluation set designed to better assess model performance. This refined dataset eliminates the noise caused by the aforementioned issues, providing a more accurate benchmark for evaluating model capabilities. In this paper, when evaluating a model's performance, we primarily use the model's accuracy on the full test dataset as a basis for comparison, unless otherwise stated. This approach ensures consistency and allows for a clear assessment of improvements made through the new evaluation set."}, {"title": "Methodology", "content": "Improving the temporal reasoning capabilities of Large Language Models is crucial for enhancing their performance on tasks involving time-based data. Current models often struggle with accurately interpreting and reasoning about temporal information. This limitation reduces their effectiveness in real-world applications.\nOur analysis revealed several key areas where current models underperform in reasoning over tabular data, especially in temporal contexts. To address these challenges, we explored two ap-proaches. First, we developed a novel method called C.L.E.A.R (Comprehend, Locate, Examine, Analyze, Resolve). This method was inspired by our detailed examination of the TempTabQA test set. C.L.E.A.R instructs models specifically for tabular data reasoning tasks. It employs advanced prompting techniques to guide models more effectively, aiming to reduce errors and enhance accuracy in temporal question answering. Secondly, we fine-tuned models using auxiliary temporal data from unrelated domains to enhance cross-generalization. This approach boosts temporal understanding on complex temporal tasks."}, {"title": "C.L.E.A.R Prompting", "content": "In this section, we introduce the C.L.E.A.R (Comprehend, Locate, Examine, Analyze, Resolve) technique, designed to enhance temporal reasoning over semi-structured data. C.L.E.A.R is a structured, step-by-step approach that guides the process of understanding, identifying, examining, analyzing, and resolving questions involving temporal reasoning. It ensures a comprehensive and logical extraction of information from tables (refer Fig. 2).\n(a.) Comprehend: The first step, Comprehend, involves applying domain knowledge to understand the given question. For example, if the question pertains to calculating time differences, it is essential to recognize that this involves subtracting the earlier year from the later year. This step ensures the correct interpretation of the question and sets the foundation for correct structural solution analysis.\n(b.) Locate: The Locate step focuses on identifying and extracting the relevant rows from the table that directly pertain to the question. This involves explaining the rationale behind the selection of these rows to provide transparency and clarity. For instance, if the question asks for events between specific years, only the rows corresponding to those years should be selected. The output of this step includes listing the relevant rows on new lines for clarity and ease of reference.\n(c.) Examine: In the Examine phase, the main question is broken down into smaller, more manageable sub-questions. Each sub-question aims at extracting a specific piece of information from the table that is necessary to answer the main question. This decomposition allows for a systematic approach to solving the question, ensuring that no critical piece of information is overlooked.\n(d.) Analyze: The Analyze step is multifaceted, involving several sub-steps:\n1. Mark Evidence for Each Sub-Question: For each sub-question, identify the specific evidence from the table that will be used to answer it. Explain the relevance of this evidence to the sub-question in detail.\n2. Determine Dependencies: This sub-step assesses whether the answer to a sub-question depends on information from previous sub-questions. If dependencies exist, they are stated to maintain logical coherence.\n3. Reasoning for Each Sub-Question: A logical explanation is provided for how the evidence leads to the answer for each sub-question. This includes detailed reasoning for transparency and to support the answer's validity.\n(e.) Resolve: The final step, Resolve, involves combining and applying the answers from the sub-questions to formulate the final answer to the main question. This step includes explaining the reasoning process leading to the final answer, which may involve necessary calculations or logical deductions. This synthesis ensures the final answer is well-supported and logically derived from evidence gathered in preceding steps. Refer Figure 3 in Appendix A for step-by-step process of C.L.E.A.R."}, {"title": "Fine Tuning with Auxiliary Data", "content": "Fine-tuning models can significantly boost their capabilities by adjusting parameters through training on task-specific examples. In this section, we demonstrate how models' temporal reasoning can be enhanced inherently, not only through fine-tuning on specific data but also by integrating auxiliary data sources.\nAuxiliary data, such as temporal unstructured data, does not directly relate to the main task but contains relevant logical structures and principles. This type of data enhances the model's understanding of underlying logic, improving the overall performance. The TRAM dataset, illustrated in Figure 5 in Appendix A, serves as an auxiliary source to boost model performance. It includes temporal questions that aid the model in grasping temporal relationships across diverse contexts.\nThe TRAM dataset, though unrelated to tabular data temporal reasoning, enhances model temporal reasoning skills. Exposing the model to diverse temporal questions improves its ability to handle temporal relationships, boosting performance. This approach shows leveraging auxiliary data can effectively address temporal reasoning complexities and enhance model robustness in diverse scenarios."}, {"title": "Experimental Setup", "content": "Models: In this paper, we experimented with several state-of-the-art large language models (LLMs), including GPT-3.5-Turbo, GPT-4, PaLM-2, Mistral-2-7B, LLaMA-2-7B-chat, and Gemini 1.5 Pro Flash. These models represent the forefront in both open-source and closed-model applications, showcasing advancements in natural language understanding and generation capabilities.\nPrompts: Prompting models with detailed instructions enhances their understanding of tasks, leading to improved responses. These prompts may include demonstrations for the model's reference. We explore the following prompting techniques:\nChain of Thought (CoT) (Wei et al., 2023): CoT prompts guide the model through a series of interconnected thoughts or reasoning steps, encouraging coherent and structured responses. We apply CoT in both zero-shot (Z.S) and few-shot (F.S) settings to evaluate its impact on model performance.\n- Faithful Chain of Thought (F-CoT) (Lyu et al., 2023): F-CoT extends CoT by emphasizing fidelity to the initial prompt throughout response generation to maintain consistency and accuracy in model outputs. We apply F-CoT in both zero-shot and few-shot scenarios to evaluate its effectiveness.\n- Program of Thought (PoT) (Chen et al., 2023): PoT provides the model with a predefined sequence of operations, akin to a program, to generate structured and task-specific responses. Like CoT and F-COT, PoT is evaluated in zero-shot and few-shot contexts to enhance model performance.\nAuxiliary Data: In our experiments, we utilize several unstructured temporal reasoning datasets to assess the temporal reasoning capabilities of language models:\n- DATE Understanding (Srivastava et al., 2023): This dataset evaluates a model's ability to comprehend, manipulate, and reason about dates in diverse formats and contexts. Tasks include Date Format Conversion, Date Arithmetic, Date Recognition, Relative Date Interpretation, and Time Reasoning.\n- Temporal Sequences (Srivastava et al., 2023): This dataset tests language models on logical deduction tasks. Models are given a series of events and their durations, and they deduce the possible timing of additional activities.\n- TRAM dataset (Wang and Zhao, 2024): This benchmark dataset comprises ten distinct datasets, each focusing on different temporal aspects: order, arithmetic, frequency, and duration. The TRAM dataset aims to evaluate language models' temporal reasoning capabilities comprehensively"}, {"title": "Results and Analysis", "content": "In this section, we present the results of enhancing temporal reasoning in Large Language Models (LLMs) for tabular data tasks. We evaluate the effectiveness of two strategies: C.L.E.A.R (Comprehend, Locate, Examine, Analyze, Resolve), a novel method tailored for tabular data reasoning, and the integration of out-of-domain temporal data for model fine-tuning."}, {"title": "C.L.E.A.R Prompting", "content": "We analyzed C.L.E.A.R prompting against effective techniques like CoT, F-CoT, and PoT in zero-shot and few-shot settings. Our goal was to evaluate their performance across various models and determine the most effective approach.\nAs shown in Table 1, C.L.E.A.R consistently outperforms other techniques across various models without fine-tuning, except for LLaMA-2. For GPT-3.5 Turbo, our method achieves a 4.5% performance boost compared to the next best technique. For PaLM-2, it leads to a 2.7% improvement.\nThese findings highlight C.L.E.A.R prompting's effectiveness in enhancing model performance, particularly in scenarios where fine-tuning is not feasible. Significant performance gains observed with GPT-3.5 Turbo and PaLM-2 demonstrate its potential to improve temporal reasoning and overall task comprehension in large language models."}, {"title": "Efficacy of C.L.E.A.R", "content": "The results in Section 6.1 demonstrate that our method is more effective than other prompting techniques. However, is C.L.E.A.R trustworthy? Does it really enhance the model's evidence-based reasoning capabilities? (Gupta et al., 2021). Our experiments compare C.L.E.A.R with Zero Shot and Few Shot Chain of Thought (CoT) approaches across tasks highlighting model deficiencies. Below are each of task descriptions:\n1. Original Table: The model uses the original table to answer the question, testing its ability to effectively utilize the provided data.\n2. Without Table: The model is asked to answer the question without access to the table, testing whether the model has memorized the answers or can deduce them independently. Here, an ideal model's performance should suffer as it doesn't use pre-trained knowledge.\n3. Altered Entity Name: The table and question are provided to the model with an altered entity name. This checks the model's reasoning ability without relying on memorized data.\n4. Missing Relevant Rows: The model is given the original table with relevant rows deleted. This evaluates whether the model can use external knowledge to answer questions. Responses are evaluated against gold answers.\n5. Information Absence Detection: The model receives the original table with relevant rows deleted. This task evaluates the model's accuracy in identifying missing information.\nFor the last two tasks i.e. Missing Relevant Rows and Information Absence Detection, we perform evaluations in two settings:\n\u2022 Original Prompt: The model is tested with the original prompt to see if it can detect missing information or use external knowledge.\n\u2022 Updated Prompt: The model is explicitly instructed that the information may or may not be present, to see if explicit instructions improve performance.\nWe aim to assess if C.L.E.A.R prompting improves evidence-based reasoning in models beyond accuracy. This analysis evaluates its effectiveness in addressing specific reasoning challenges and enhancing model reliability across diverse contexts."}, {"title": "Auxiliary Data Fine Tuning", "content": "C.L.E.A.R enhances the model's capacity to process and reason with context. It facilitates effective instruction for models to handle temporal questions and improve evidence-based reasoning. However, prompting alone does not adjust their inherent parameters, so models do not intrinsically improve.\nInnate enhancement in models requires fine-tuning, so that their parameters better reflect improved temporal reasoning capabilities. We suggest fine-tuning the model on simple auxiliary data to enhance its ability to reason across diverse data formats and improve overall reasoning capabilities.\nWhy TRAM dataset? We evaluated the impact of fine-tuning GPT-3.5 Turbo using the auxiliary datasets discussed in Section 5, along with the TempTabQA dataset. For this evaluation, we fine-tuned with 100 examples from each dataset.\nAnalysis. Table 4 shows that the TRAM dataset offers the largest performance improvement among auxiliary datasets. While DATE and Temporal Sequences datasets provide gains of less than 1.01%, TRAM achieves a significant 2.04% boost compared to the base model without fine-tuning. These findings emphasize TRAM's unique advantages. DATE focuses solely on date-based reasoning, and Temporal Sequences on temporal reasoning across events. In contrast, TRAM includes 10 sub-datasets covering diverse temporal reasoning tasks. It includes tasks such as Ordering, Frequency, Duration, Typical time, Ambiguity Resolution, Arithmetic, Relations, Temporal NLI, Causality and Storytelling. Each task have multiple question type including Commonsense, Facts, Reading Comprehension, Application, Computation, Direct & Multi-step Comparison, Interpretation, Calender shift, Long-term, Mid-term & Short-term shift, Date Computation, 12 & 24 hour adjustment, Week Identification etc.  This diversity enhances performance by exposing models to varied temporal scenarios, improving overall comprehension in temporal domain.\nThe TRAM dataset's significant gains highlight the importance of diverse auxiliary data for fine-tuning. By covering a wide range of temporal reasoning tasks, TRAM helps models grasp nuanced temporal relationships, enhancing performance across various challenges. This finding suggests that future efforts in model fine-tuning should consider leveraging similarly diverse datasets to maximize performance improvements.\nFine-tuning on TRAM. We analyzed fine-tuning models on subsets of the TRAM dataset and TempTabQA, each with 100 and 1000 examples (evenly sampled across tasks), as shown in Table 1.\nAnalysis. Our findings reveal that, on the TRAM dataset with fine-tuning on 100 examples, C.L.E.A.R outperforms all models except for LLaMA-27B. This trend is consistent with 1000 examples, where it slightly trails behind the few-shot CoT for GPT-3.5 turbo by a negligible margin of 0.07%.When fine-tuning models on TempTabQA with 100 examples and 1000 examples, our method demonstrates superior performance for GPT 3.5 turbo and PaLM-2, but underperforms for LLaMA-2 7B and Mistral-2 7B.\nOur findings show that fine-tuning models with auxiliary data significantly improves performance, close to that of task-specific fine-tuning. This method enhances models' ability to reason about temporal information and can be applied across various tasks. Additionally, auxiliary data typically provides richer resources compared to task-specific datasets. This scalability enables fine-tuning on"}, {"title": "Comparison with Related Work", "content": "Tabular Reasoning Recent research has extensively explored large language models' applications with semi-structured tabular data (Chen et al., 2020b; Gupta et al., 2020; Zhang and Balog, 2019), covering areas such as question answering and semantic parsing (Zhang et al., 2020b; Zhang and Balog, 2020; Pasupat and Liang, 2015; Krishnamurthy et al., 2017; Abbas et al., 2016; Sun et al., 2016; Chen et al., 2020c; Lin et al., 2020; Zayats et al., 2021; Oguz et al., 2020; Chen et al., 2021b; Iyyer et al., 2017), as well as table-to-text generation (Parikh et al., 2020; Li et al., 2021; Nan et al., 2021; Yoran et al., 2021; Chen et al., 2020a). Various datasets and models have emerged to handle semi-structured data, including Table2vec (Zhang et al., 2019), TAPAS (Herzig et al., 2020), TaBERT (Yin et al., 2020), TabStruc (Zhang et al., 2020a), TABBIE (Iida et al., 2021), TabGCN (Pramanick and Bhattacharya, 2021), and RCI (Glass et al., 2021), all aimed at improving understanding and representation of tabular data. Research has also focused on fine-tuning models for enhanced inference on tabular data, as demonstrated by studies (Yu et al., 2018; Eisenschlos et al., 2020; Neeraja et al., 2021), showcasing targeted techniques to boost model performance.\nRecently, (Srivastava et al., 2024) introduced EEDP, a novel approach for financial document Question Answering. EEDP retrieves finance domain information, extracts relevant table rows, and decomposes mathematical tasks into atomic operations. Our method similarly involves understanding table data and extracting relevant rows, but we differentiate by systematically addressing sub-problems based on extracted evidence. This targeted approach makes our method well-suited for temporal tabular reasoning.\nTemporal Reasoning In temporal question answering, recent datasets like TIME-SENSITIVEQA (Chen et al., 2021c) and TORQUE (Ning et al., 2020) focus on entity-specific reading comprehension with time-sensitive questions. Other datasets like TEMPQA-WD (Neelam et al., 2022), CRONQUESTIONS (Saxena et al., 2021), and TEMPQUESTIONS (Jia et al., 2018a) explore temporal links in knowledge graph"}, {"title": "Conclusion and Future Work", "content": "Our results demonstrate that C.L.E.A.R (Comprehend, Locate, Examine, Analyze, Resolve) prompting significantly enhances model performance, particularly in understanding tabular data and tasks requiring temporal reasoning. It promotes grounding LLMs in evidence context rather than relying solely on pre-trained knowledge. Moreover, fine-tuning LLMs with auxiliary temporal data has proven highly effective in enhancing temporal understanding. This study validates that integrating auxiliary data strengthens models' temporal reasoning capabilities, thereby enhancing the robustness of large language models. For Future, we propose (a.) Generation of Synthetic Data: Creating synthetic training data from temporal aspects of tabular data enhances model performance through diverse temporal exposure. (b.) Neuro-symbolic Learning: Integrating neural networks with symbolic reasoning improves models' understanding of temporal information for more effective solutions. (c.) Expanding C.L.E.A.R Prompting Applications: Applying C.L.E.A.R prompting to diverse tasks and domains validates its versatility in natural language processing, enhancing reasoning with temporal information. (d.) Integration with Existing Models: Seamlessly integrating C.L.E.A.R prompting and auxiliary data into existing models maximizes benefits without architectural changes."}, {"title": "Limitations", "content": "The experiments in this paper have been conducted exclusively on the English language. This study can be extended to a multilingual setting to evaluate the approach's effectiveness across different languages. Additionally, the temporal datasets used in our study are limited to simple, entity-centric tables. Since structured data can exist in more complex forms, such as hierarchical tables, further research is necessary to assess the impact of our methods on these more complex structures.\nMoreover, our computational limitations restricted us to fine-tuning models on only 1000 samples of auxiliary data. To fully understand the potential improvements from fine-tuning on auxiliary data, it is essential to explore the effects of fine-tuning on larger datasets. Future work should focus on overcoming these limitations to provide a comprehensive evaluation of our approach."}, {"title": "Ethics Statement", "content": "We confirm that our work adheres to the highest ethical standards in research and publication. We will publicly release our code and enhanced evaluation set to enable the research community to validate and build upon our findings. We are committed to the responsible and fair use of computational linguistics methodologies. The claims in our paper accurately reflect the experimental results. While using black-box large language models introduces some stochasticity, we mitigate this by maintaining a fixed temperature. We utilize an AI assistive tools for writing while ensuring absence of bias. We provide comprehensive details on annotations, dataset splits, models used, and prompting methods tried, ensuring the reproducibility of our work."}, {"title": "Acknowledgement", "content": "Research was sponsored by the Army Research Office and was accomplished under Grant Number W911NF-20-1-0080. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. This work was partially funded by ONR Contract N00014-23-1-2365."}]}