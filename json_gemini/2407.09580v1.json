{"title": "Don't Fear Peculiar Activation Functions: EUAF and Beyond", "authors": ["Qianchao Wang", "Shijun Zhang", "Dong Zeng", "Zhaoheng Xie", "Hengtao Guo", "Tieyong Zeng", "Feng-Lei Fan"], "abstract": "In this paper, we propose a new super-expressive activation function called the Parametric Elementary Universal Activation Function (PEUAF). We demonstrate the effectiveness of PEUAF through systematic and comprehensive experiments on various industrial and image datasets, including CIFAR10, Tiny-ImageNet, and ImageNet. Moreover, we significantly generalize the family of super-expressive activation functions, whose existence has been demonstrated in several recent works by showing that any continuous function can be approximated to any desired accuracy by a fixed-size network with a specific super-expressive activation function. Specifically, our work addresses two major bottlenecks in impeding the development of super-expressive activation functions: the limited identification of super-expressive functions, which raises doubts about their broad applicability, and their often peculiar forms, which lead to skepticism regarding their scalability and practicality in real-world applications.", "sections": [{"title": "1. INTRODUCTION", "content": "In recent years, deep learning has achieved significant success in many critical areas (LeCun et al., 2015). A major factor contributing to this success is the development of highly effective nonlinear activation functions, which greatly enhance the information processing capabilities of neural networks. While established options like the Rectified Linear Unit (ReLU) and its variants are widely used (Nair and Hinton, 2010), the fundamental importance of activation functions makes the search for better ones a continuous effort. Researchers are persistently working to design and evaluate various activation functions through both theoretical analysis and empirical studies (Bingham and Miikkulainen, 2022; Apicella et al., 2021; Wang et al., 2024).\nIn the realm of approximation theory, it has been shown that certain activation functions can empower a neural network with a simple structure to approximate any continuous function with an arbitrarily small error, using a fixed number of neurons (Maiorov and Pinkus,\n1999). These functions are termed \"super-expressive activation functions\" (Yarotsky, 2021). According to research, to achieve super-expressiveness, an activation function should possess both periodic and analytical components (Shen et al., 2022; Yarotsky, 2021). One such example is the elementary universal activation function (EUAF), defined as follows:\nEUAF(x) := \\begin{cases} |x - 2\\lfloor \\frac{x}{2}\\rfloor| & \\text{for } x \\geq 0, \\\\ 0 & \\text{for } x < 0, \\end{cases}\nThe unique and highly desirable property of super-expressiveness allows neural networks to achieve precise approximation accuracy without increasing network complexity. This contrasts with traditional universal approximation methods, where more complex structures and a higher number of neurons are required as the approximation error decreases. By integrating super-expressive activation functions, one can attain the desired approximation accuracy by merely adjusting parameters, thus maintaining a simpler network architecture.\nTo the best of our knowledge, the development of"}, {"title": null, "content": "super-expressive activation functions faces two technical challenges that hinder their potential value to neural networks: 1) First, only a limited number of super-expressive functions have been identified so far (Maiorov and Pinkus, 1999; Shen et al., 2022; Yarotsky, 2021). It is unclear if the super-expressive property can be broadly applied. Additionally, for deep learning practitioners, having a greater variety of activation functions that exhibit learning capabilities is necessary in terms of enriching their armory. Developing more super-expressive functions increases the likelihood of finding their utilities in important applications, as different activation functions differ in their trainability. 2) Second, the practical utility of super-expressive activation functions is questionable. While superior expressiveness can be theoretically established through specialized constructions that demonstrate the existence of an expressive solution (Shen et al., 2021; Yarotsky, 2021), this does not necessarily translate to better practical performance. Furthermore, it is unclear whether gradient-based methods can effectively learn good solutions for networks using these functions.\nCompared to commonly used functions like ReLU, sigmoid, and tanh, super-expressive functions usually have peculiar shapes. For example, Figure 1 shows EUAF, which is a typical super-expressive activation function. It has a complex and intimidating form, which makes most practitioners skeptical about its scalability and practicality in real-world applications. If we can demonstrate the practical utility of any super-expressive activation function, it could help resolve the skepticism and bridge the gap between their theoretical elegance and usefulness.\nIn addressing the first bottleneck, we substantially generalize the scope of EUAF to encompass a large family of functions capable of achieving super-expressiveness. Specifically, an activation function $\\rho$ is considered to be super-expressive if it is real analytic within a small interval and a fixed-size $\\rho$-activated network can reproduce a triangle-wave function. To address the second bottleneck, we believe that superexpressive functions can indeed be practically useful. Previous studies (Sitzmann et al., 2020; Ramirez et al., 2023) successfully applied the periodic function sin as an activation function within the implicit neural representation. These models have been demonstrated to be suitable for representing complex signals and their derivatives, as well as for solving challenging boundary value problems (Liu et al., 2022a). These studies provide valuable insights into the potential of super-expressive activation functions, since both super-expressive activation functions and sin share periodicity. Moreover, from the perspective of signal decomposition, normal activation functions like ReLU tend to assist models in identifying the direct component (DC) of a signal (Lee et al., 2024). In contrast, super-expressive activation functions can better handle stationary signals due to their inherent periodicity. This characteristic enhances their ability to manage complex real-world signals more efficiently.\nSpecifically, we choose EUAF as our representative and investigate a parameterized variant, named PEUAF, which adaptively learns the frequency $\\omega$ on the positive side. Mathematically,\nPEUAF(x) := \\begin{cases} | \\frac{\\omega x}{2} - 2 \\lfloor \\frac{\\omega x}{4} \\rfloor | - \\frac{\\omega x}{4} + 1 | & \\text{for } x \\geq 0, \\\\ \\frac{x}{1 + |x|} & \\text{for } x < 0, \\end{cases}\nwhere $\\omega$ is the trainable parameter representing the frequency on the positive side. PEUAF can adaptively extract the stationary signals with different frequencies. This adaptability allows PEUAF to effectively capture and represent signals with diverse frequency components, which is particularly advantageous in addressing real-world signal complexities. Then, we validate the effectiveness of PEUAF by experimenting with four industrial datasets (1D data) and three image datasets (2D data). For industrial datasets, our tests show that PEUAF surpasses other activation functions in terms of test accuracy, convergence speed, and fault localization ability. For image datasets, we find that combining PEUAF with other activation functions can usually yield better performance than only using a single activation function, although using PEUAF alone cannot achieve satisfactory performance. Thus, PEUAF can serve as a valuable add-on to the network. Our main contributions are as follows:\n\u2022 We provide a non-trivial generalization of EUAF, showing that a broader family of activation functions can achieve super-expressiveness.\n\u2022 We bridge the gap between the theoretical elegance and empirical usefulness of super-expressive functions by demonstrating their competitive performance in practical applications through systematic"}, {"title": "2. RELATED WORK", "content": "In the field of artificial intelligence, deep neural networks have proven to be highly effective tools. These networks leverage the power of interconnected nodes structured in multiple layers, allowing them to excel in a wide range of complex applications and new domains. At their core, deep neural networks rely on an affine linear transformation followed by a nonlinear activation function. The nonlinear activation function is essential for the successful training of these networks.\nLater in this section, we will first review conventional activation functions including ReLU and its variants, as well as recent sigmoidal activation functions in Section 2.1. We will then discuss super-expressive activation functions in Section 2.2."}, {"title": "2.1. Conventional Activation Functions", "content": "In recent years, the Rectified Linear Unit (ReLU (Nair and Hinton, 2010)), defined as $\\text{ReLU}(x) = \\max(0, x)$, has gained popularity and recognition for its effectiveness in addressing the gradient vanishing and explosion issues encountered with Sigmoid and Tanh activation functions. Thus, ReLU has been widely used in the deep learning community such as industrial fault diagnosis (Liu et al., 2024a) and medical image segmentation (Liu et al., 2024b). However, ReLU can suffer from the occurrence of a number of \"dead neurons\", which results in information loss and can hurt the neural network's feature processing ability. To mitigate this issue, several variants of ReLU have been introduced such as Leaky Rectified Linear Unit (LRELU) (Xu et al., 2015), Parametric Rectified Linear Unit (PRELU) (He et al., 2015), Randomized Leaky Rectified Linear Unit (RRELU) (Xu et al., 2015), Exponential Linear Unit (ELU) (Clevert et al., 2015), Gaussian Error Linear Unit (GELU) (Hendrycks and Gimpel, 2016), and Generalized Linear Unit (GENLU) (Fan et al., 2020). Most recently, Goldenstein et al. (2024) proposed Self-Normalizing ReLU or NeLU to ensure that the prediction model is not affected by the noise level during testing. It has been tested in synthetic data and image de-noising tasks. These"}, {"title": null, "content": "variants represents a significant advancement in activation function design, offering adaptability and potentially better performance. Whereas, their benefits come with the cost of increased model complexity or computation burden and the need for careful tuning and regularization which inspired researchers to create more different activation functions.\nIn addition to these ReLU variants, other kinds of activation functions have also been developed. For example, the Swish (Swish(x) = x \\cdot \\text{sigmoid}(\\beta x)) (Ramachandran et al., 2017) was identified through an automated search using a combination of exhaustive and reinforcement learning as an alternative to ReLU. Its similar shape makes it a reasonable proxy for ReLU in deep learning applications. Mish, defined as Mish(x) = x \\cdot \\text{tanh}(\\text{softplus}(x)) (Misra, 2020), exhibits superior empirical results compared to ReLU, Swish, and LRELU in CIFAR-10 and ImageNet classification tasks. Fractional adaptive linear units FALUS (Zamora et al., 2022) incorporate fractional calculus principles into activation functions, thereby defining a diverse family of activation functions. It has demonstrated enhanced performance in image classification tasks, improving test accuracy. The Seagull activation function, introduced by (Gao and Zhang, 2023), stands out as a customized activation function designed for applications in regression tasks featuring a partially exchangeable target function. It exhibits superiority in addressing the specific demands of regression scenarios.\nOverall, the above-mentioned activation functions are hard to be generalized across different domains, especially in industrial applications. Another problem is that the lack of theoretical analysis limits the acceptance of these activation functions in spite of their good performance. Therefore, it is necessary to verify an activation function with a good theoretical guarantee."}, {"title": "2.2. Super-Expressive Activation Functions", "content": "Numerous studies have explored new activation functions to make a fixed-size network achieve an arbitrary error, referred to as super-expressive activation functions. For example, Maiorov and Pinkus (1999) proposed an activation function to achieve this goal, but it lacks a closed form and is computationally impractical. Recently, Yarotsky (2021) demonstrated that simple functions such as (sin, arcsin) can achieve superexpressiveness, although the relationship between the network size and the dimension was unclear. However, despite the above problems, sin has been proven to be effective in 3D neural network field, indicating the potential of super-expressiveness in neural networks (Ramirez et al., 2023). Shen et al. (2022) proposed"}, {"title": "3. Enriching the Family of Super-expressive Activation Functions", "content": "In this section, we aim to significantly expand the scope of EUAF activation function by introducing a comprehensive collection of activation functions, each with approximation properties akin to those of EUAF. For simplicity, let $\\mathcal{NN}{N, L; \\mathbb{R}^d \\rightarrow \\mathbb{R}^n}$ denote the set of neural networks $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}^n$ that can be represented by $\\varrho$-activated networks, with a maximum width of $N$ and a maximum depth of $L$. Let $\\mathcal{A}$ represent the set of all super-expressive activation functions $\\varrho : \\mathbb{R} \\rightarrow \\mathbb{R}$, which satisfy the following conditions:\n\u2022 There exists an interval $(\\alpha, \\beta)$ with $\\alpha < \\beta$ where $\\varrho$ is real analytic and non-polynomial on $(\\alpha, \\beta)$.\n\u2022 There exists a fixed-size $\\varrho$-activated network $\\Phi$ that can reproduce a triangle-wave function on $[0, \\infty)$, i.e.,\n$\\Phi(x) = |x - 2\\lfloor \\frac{x}{2} \\rfloor| \\forall x \\in [0, \\infty)$.\nWe denote $\\mathcal{A}$ as the \"closure\" of $\\mathcal{A}$. This means a function $\\varrho$ is in $\\overline{\\mathcal{A}}$ if and only if, for any $A > 0$ and $\\varepsilon > 0$, there exists a $\\varrho_\\varepsilon \\in \\mathcal{A}$ such that:\n$|\\varrho_\\varepsilon(x) - \\varrho(x)| < \\varepsilon \\forall x \\in [-A, A]$.\nTheorem 1. Given any $\\varrho \\in \\overline{\\mathcal{A}}$, the hypothesis space\n$\\mathcal{NN}{O(d^2), O(1); \\mathbb{R}^d \\rightarrow \\mathbb{R}}$\nis dense in $C([a, b]^d)$ in terms of the supremum norm."}, {"title": null, "content": "It is crucial to highlight that the constants in the $O(\\cdot)$ notation in Theorem 1 can be explicitly determined and depend only on the choice of $\\varrho$. The proof of Theorem 1 will be provided later in this section.\nBefore giving the proof, let us provide several examples in $\\mathcal{A}$. The first example, $\\varrho_1 \\in \\mathcal{A}$, exhibits an S-shape and is defined as follows:\n$\\varrho_1:=\\begin{cases} \\frac{-x}{1 + |x| + x^2 + \\frac{g(x)}{10}} & \\text{for } x \\leq 0, \\\\ \\frac{g(x)}{x + \\frac{x}{x+1}} & \\text{for } x > 0, \\end{cases}$\nwhere $g(x) = |x - 2\\lfloor \\frac{x}{2} \\rfloor|$ for any $x \\in \\mathbb{R}$.\nThe second example, $\\varrho_2 \\in \\mathcal{A}$, resembles the ReLU activation function and is defined as follows:\n$\\varrho_2:=\\begin{cases} 0 & \\text{for } x \\leq 0, \\\\ \\frac{g(x)}{x + \\frac{x}{x+1}} & \\text{for } x > 0. \\end{cases}$\nThe third example, $\\varrho_3 \\in \\mathcal{A} \\subseteq \\overline{\\mathcal{A}}$, is defined as follows:\n$\\varrho_3:=\\begin{cases} \\arcsin(x) & \\text{for } -1 \\leq x \\leq 1, \\\\ \\sin(x) & \\text{for } |x| > 1. \\end{cases}$\nNow, we will focus on proving the validity of Theorem 1. Given any any $f \\in C([a, b]^d)$ and $\\varepsilon > 0$, our goal is to construct $\\phi \\in \\mathcal{NN}{O(d^2), O(1); \\mathbb{R}^d \\rightarrow \\mathbb{R}}$ such that\n$\\sup_{x \\in [a,b]^d} |\\phi(x) - f(x)| < \\varepsilon$.\nSeveral concepts used to establish Theorem 1 can be traced back to the research conducted by (Shen et al., 2022) and (Yarotsky, 2021). The proof can be divided into three main steps as follows.\n\u2022 The primary objective of the first step is to create a neural network that effectively approximates the univariate function $f \\in C([0, 1])$ within a specific \"half\" interval.\nTheorem 2. Given any $f \\in C([0, 1])$, $\\varrho \\in \\mathcal{A}$, $\\varepsilon > 0, and K \\in \\mathbb{N}$, suppose for any $x_1, x_2 \\in [0, 1]$, it holds that\n$|f(x_1) - f(x_2)| < \\varepsilon/2 \\quad \\text{if } |x_1 - x_2| < 1/K. \\tag{1}$"}, {"title": "3.1. Proof of Theorem 2", "content": "Partition $[0, 1]$ into $2K$ small intervals $I_k$ and $\\overline{I}_k$ for $k = 1, 2, ..., K$, i.e.,\n$I_k = [\\frac{2k-2}{2K}, \\frac{2k-1}{2K}]$ and $\\overline{I}_k = [\\frac{2k-1}{2K}, \\frac{2k}{2K}]$.\nClearly, $[0, 1] = \\bigcup_{k=1}^K (I_k \\cup \\overline{I}_k)$. Let $x_k$ be the right endpoint of $I_k$, i.e., $x_k = \\frac{2k-1}{2K}$ for $k = 1, 2, ..., K$.\nNN {O(1), O(1); $\\mathbb{R} \\rightarrow \\mathbb{R}$} to achieve accurate approximations of $f$ within $I_k$ for $k = 1, 2, ..., K$. It is not essential to consider the values of $\\phi$ within $\\overline{I}_k$ for all $k$. In other words, our focus is primarily on achieving accurate approximations within one \"half\" of the interval $[0, 1]$, which is the crucial element in our proof.\nDefine $\\psi(x) := x - \\sigma(x)$ for any $x \\in \\mathbb{R}$, where $\\sigma \\in NN{\\mathcal{O}(1), \\mathcal{O}(1); \\mathbb{R} \\rightarrow \\mathbb{R}}$ with\n$\\sigma(x) = |x - 2\\lfloor \\frac{x}{2} \\rfloor| \\quad \\text{for } x \\geq 0$.\nIt easy to verifty that\n$\\psi(2Kx)/2 + \\frac{2k-1}{2} = x_k \\quad \\text{for any } x \\in [\\frac{2k-2}{2K}, \\frac{2k-1}{2K}] = I_k. \\tag{2}$\nWe will make use of the two following lemmas to simplify our proof.\nLemma 2 (Lemma 23 of Shen et al. (2022)). Given any rationally independent numbers $a_1, a_2, ..., a_K$ for any $K \\in \\mathbb{N}^+$ and an arbitrary periodic function $g : \\mathbb{R} \\rightarrow \\mathbb{R}$ with period $T$, i.e., $g(x + T) = g(x)$ for any $x \\in \\mathbb{R}$, assume there exist $x_1, x_2 \\in \\mathbb{R}$ with $0 < x_2 - x_1 < T$ such that $g$ is continuous on $[x_1, x_2]$. Then the following set\n$\\{(u \\cdot g(\\omega a_1) + v, ..., u \\cdot g(\\omega a_K) + v) : u, \\omega, v \\in \\mathbb{R}\\}$"}, {"title": null, "content": "is dense in $\\mathbb{R}^K$ provided that\n$\\min_{x \\in [x_1, x_2]} g(x) < \\max_{x \\in [x_1, x_2]} g(x)$.\nLemma 3. Given $K \\in \\mathbb{N}^+$, suppose $\\varrho$ is real analytic and non-polynomial on an interval $(\\alpha, \\beta)$ with $\\beta > \\alpha$. Then there exists $\\omega_0 \\in (-\\frac{\\beta-\\alpha}{2K}, \\frac{\\beta-\\alpha}{2K})$ such that $(\\frac{\\alpha+\\beta}{2K} + k \\omega_0)$, for $\\{k = 1, 2, ..., K\\}$, are rationally independent.\nProof. We prove this lemma by contradiction. If it does not hold, then $\\varrho(\\frac{\\alpha+\\beta}{2K} + k \\omega)$, for $\\{k = 1, 2, ..., K\\}$, are rationally dependent for any $\\omega \\in (-\\frac{\\beta-\\alpha}{2K}, \\frac{\\beta-\\alpha}{2K}) = I$. That means, for any $\\omega \\in I$, there exists $\\lambda = (\\lambda_1, ..., \\lambda_K) \\in \\mathbb{Q}^K \\setminus \\{0\\}$ such that $\\sum_{k=1}^K \\lambda_k \\varrho(\\frac{\\alpha+\\beta}{2K} + k \\omega) = 0$. We observe that $I$ is uncountable and $\\mathbb{Q}^K \\setminus \\{0\\}$ is countable. It follows that there exists $\\lambda = (\\lambda_1, ..., \\lambda_K) \\in \\mathbb{Q}^K \\setminus \\{0\\}$ such that $\\sum_{k=1}^K \\lambda_k \\varrho(\\frac{\\alpha+\\beta}{2K} + k \\omega) = 0$ for all $\\omega$ in an uncountable subset of $I$. Then the real analyticity of $\\varrho$ implies $\\sum_{k=1}^K \\lambda_k \\varrho(\\frac{\\alpha+\\beta}{2K} + k \\omega) = 0$ for all $\\omega \\in I$. By expanding $\\sum_{k=1}^K \\lambda_k \\varrho(\\frac{\\alpha+\\beta}{2K} + k \\omega)$ into the Taylor series at $\\omega = 0$, we get the identity $\\sum_{k=1}^K \\lambda_k k^m \\frac{d^m \\varrho}{d \\omega^m}(\\frac{\\alpha+\\beta}{2K}) = 0$ for each $m$ with $\\frac{d^m \\varrho}{d \\omega^m}(\\frac{\\alpha+\\beta}{2K}) \\neq 0$. Since $\\varrho$ is non-polynomial on $(\\alpha, \\beta) \\ni \\frac{\\alpha+\\beta}{2}$, there are are infinitely many $m$ with $\\frac{d^m \\varrho}{d \\omega^m}(\\frac{\\alpha+\\beta}{2}) \\neq 0$, implying $\\sum_{k=1}^K \\lambda_k k^m = 0$. This means $\\lambda = (\\lambda_1, ..., \\lambda_K) = 0$, a contradiction with $\\lambda \\in \\mathbb{Q}^K \\setminus \\{0\\}$. So we finish the proof of Lemma 3.\nNow, let us return to the proof of Theorem 2. We can employ Lemma 3 to produce a collection of rationally independent numbers. Specifically, there exists a value $\\omega_0$ such that $a_1, a_2, ..., a_K$ are linearly independent, where each $a_k$ is defined as $a_k = \\varrho(\\frac{\\alpha+\\beta}{2K} + k \\omega_0)$. Next, define\n$g(x) = |x - 2\\lfloor \\frac{x}{2} \\rfloor| \\quad \\text{for } x \\in \\mathbb{R}$.\nBy Lemma 2, there exists $u_1, \\omega_1, v_1 \\in \\mathbb{R}$ such that\n$|u_1 \\cdot g(\\omega_1 a_k) + v_1 - f(x_k)| < \\varepsilon/2 \\quad \\text{for any } k$.\nSince $\\varrho(x) = g(x)$ for any $x \\geq 0$ and $g$ is periodic with period 2, we can choose a sufficiently large $m_0 \\in \\mathbb{N}$ such that\n$|u_1 \\varrho(\\omega_1 a_k + 2m_0) + v_1 - f(x_k)|$\n$= |u_1 g(\\omega_1 a_k + 2m_0) + v_1 - f(x_k)|$\n$= |u_1 g(\\omega_1 a_k) + v_1 - f(x_k)| < \\varepsilon/2,$\nfor $k = 1, 2, ..., K$. Define\n$\\varphi(x) = u_1 \\varrho(\\omega_1 (\\frac{\\alpha+\\beta}{2} + (\\frac{\\psi(2Kx)}{2} - \\frac{1}{2})\\omega_0) + 2m_0) + v_1$."}, {"title": null, "content": "For any $x \\in I_k$, we have\n$\\varphi(x) = u_1 \\varrho(\\omega_1 (\\frac{\\alpha+\\beta}{2} + (\\frac{\\psi(2Kx)}{2} - \\frac{1}{2})\\omega_0) + 2m_0) + v_1$\n$= u_1 \\varrho(\\omega_1 (\\frac{\\alpha+\\beta}{2K} + k \\omega_0) + 2m_0) + v_1$\n$= u_1 \\varrho(\\omega_1 a_k + 2m_0) + v_1,$\nimplying\n$|\\varphi(x) - f(x)| \\leq |\\varphi(x) - f(x_k)| + |f(x_k) - f(x)| < \\varepsilon$.\nIt follows that\n$|\\varphi(x) - f(x)| < \\varepsilon \\quad \\text{for any } x \\in \\bigcup_{k=0}^{K-1} [\\frac{2k}{2K}, \\frac{2k+1}{2K}] \\cup [\\frac{2k+1}{2K}, \\frac{2k+2}{2K}].$\nMoreover, we can easily verify $\\varphi \\in NN{\\mathcal{O}(1), \\mathcal{O}(1); \\mathbb{R} \\rightarrow \\mathbb{R}}$. So we finish the proof of Theorem 2."}, {"title": "3.2. Proof of Theorem 3 based on Theorem 2.", "content": "We claim it suffices to prove the special case $[a, b] = [0, 1]$ as this simplification readily extends to the broader scenario. To see this, we simply introduce a linear function $L : [0, \\frac{1}{2}] \\rightarrow [a, b]$ by defining $L(x) = 2(b-a)x+a$. The special case implies $f \\circ L : [0, \\frac{1}{2}] \\rightarrow \\mathbb{R}$ can be approximated by a network $\\phi$ arbitrarily well. Then $\\varphi = \\varphi \\circ L^{-1}$ can approximate $f : [a, b] \\rightarrow \\mathbb{R}$ well, as desired.\nWe can continuously extend $f$ from $[0, \\frac{1}{2}]$ to $\\mathbb{R}$ by setting $f(x) = f(0)$ if $x < 0$ and $f(x) = f(\\frac{1}{2})$ if $x > \\frac{1}{2}$. It follows from the uniform continuity of $f$ on $[-1, 2]$ that there exists $K = K(f, \\varepsilon) \\in \\mathbb{N}^+$ with $K \\geq 2$ such that for any $x_1, x_2 \\in [-1, 2]$,\n$|f(x_1) - f(x_2)| < \\varepsilon/10 \\quad \\text{if } |x_1 - x_2| < 1/K$.\nFor $i = 1, 2, 3, 4$, define\n$f_i(x) := f(x - \\frac{i}{4}) \\quad \\text{for any } x \\in [0, \\frac{1}{2}]$.\nThen, for $i = 1, 2, 3, 4$ and $x_1, x_2 \\in [0, \\frac{1}{2}]$, we have\n$|f_i(x_1) - f_i(x_2)| < \\varepsilon/10 = \\frac{\\varepsilon}{2} \\quad \\text{if } |x_1 - x_2| < 1/K,$\nwhere $\\varepsilon = \\frac{\\varepsilon}{5}$. For each $i \\in \\{1, 2, 3, 4\\}$, by Theorem 2, there exists $\\phi_i \\in NN{\\mathcal{O}(1), \\mathcal{O}(1); \\mathbb{R} \\rightarrow \\mathbb{R}}$ such that\n$|\\varphi_i(x) - f_i(x)| < \\frac{\\varepsilon}{5} = \\frac{\\varepsilon}{5} \\quad \\text{for any } x \\in \\bigcup_{k=0}^{K-1} [\\frac{2k}{2K}, \\frac{2k+1}{2K}].$"}, {"title": null, "content": "Define\n$\\psi(x) = \\sigma(x + \\frac{1}{4} - \\lfloor x + \\frac{1}{4} \\rfloor) \\quad \\text{for any } x \\in \\mathbb{R}$,\nwhere $\\sigma \\in NN{\\mathcal{O}(1), \\mathcal{O}(1); \\mathbb{R} \\rightarrow \\mathbb{R}}$ with\n$\\sigma(x) = |x - 2\\lfloor \\frac{x}{2} \\rfloor| \\quad \\text{for } x \\geq 0$.\nClearly, $0 \\leq \\psi(2Kx) \\leq \\frac{1}{4}$ for any $x \\in [0, \\frac{1}{2}]$, where the last equality comes from the fact that $f_i(x) = f(x - \\frac{i}{4})$ for any $x \\in [0, \\frac{1}{2}] \\setminus [\\frac{i}{4}, 1]$. Define\n$\\Phi(x) := \\sum_{i=1}^{4} \\varphi_i(x + \\frac{i}{4}) \\psi(2Kx + \\frac{1}{2}) \\quad \\text{for any } x \\in [0, \\frac{1}{2}].$\nIt is easy to verify that $\\sum_{i=1}^{4} \\psi(x + \\frac{i}{4}) = 1$ for any $x > 0$ based on the definition of $\\psi$. It follows that $\\sum_{i=1}^{4} \\psi(2K z + \\frac{1}{2}) = 1$ for any $z \\in [0, \\frac{1}{2}]$.\nHence, for any $z \\in [0, \\frac{1}{2}]$, we have\n$|\\Phi(z) - f(z)|$\n$= |\\sum_{i=1}^{4} \\varphi_i(z + \\frac{i}{4}) \\psi(2K z + \\frac{1}{2}) - f(z) \\sum_{i=1}^{4} \\psi(2K z + \\frac{1}{2})|$\n$\\leq \\sum_{i=1}^{4} |\\varphi_i(z + \\frac{i}{4}) \\psi(2K z + \\frac{1}{2}) - f(z) \\psi(2K z + \\frac{1}{2})|$\n$< 4 \\cdot \\frac{\\varepsilon}{5} = \\frac{4\\varepsilon}{5}$.\nTo approximate $(x, y) \\rightarrow xy$ well, we define\n$\\Gamma_{\\delta}(x, y) := \\frac{\\varrho(x_0 + \\delta y) - \\varrho(x_0) - \\delta \\varrho'(x_0) - \\frac{1}{2} \\delta^2 \\varrho''(x_0)}{\\delta \\varrho''(x_0)}$\nfor any $x, y \\in \\mathbb{R}$, where $\\varrho''(x_0) \\neq 0$. Clearly, $\\Gamma_{\\delta}(x, y) \\rightarrow xy$ as $\\delta \\rightarrow 0$. Then we can define\n$\\Phi_5(x) := \\sum_{i=1}^{4} \\Gamma_{\\delta}(\\varphi_i(x + \\frac{i}{4}), \\psi(2Kx + \\frac{1}{2})) \\forall x \\in [0, \\frac{1}{2}].$\nClearly, $\\Phi_5 \\in NN{\\mathcal{O}(1), \\mathcal{O}(1); \\mathbb{R} \\rightarrow \\mathbb{R}}$. Moreover, we can choose a sufficiently small $\\delta_0 > 0$ such that\n$|\\Phi_{\\delta_0}(x) - \\Phi(x)| < \\frac{\\varepsilon}{5} \\quad \\text{for any } x \\in [0, \\frac{1}{2}].$\nBy defining $\\phi := \\varphi_{\\delta_0} \\in NN{\\mathcal{O}(1), \\mathcal{O}(1); \\mathbb{R} \\rightarrow \\mathbb{R}}$, we have\n$|\\phi(x) - f(x)| \\leq |\\Phi_{\\delta_0}(x) - \\Phi(x)| + |f(x) - f(x)| < \\varepsilon$\nfor any $x \\in [0, \\frac{1}{2}]$. So we finish the proof of Theorem 3."}, {"title": "3.3. Proof of Theorem 1 based on Theorem 3 and KST.", "content": "We can safely assume```json\n{\n  "}, {"title": "Don't Fear Peculiar Activation Functions: EUAF and Beyond", "authors": ["Qianchao Wang", "Shijun Zhang", "Dong Zeng", "Zhaoheng Xie", "Hengtao Guo", "Tieyong Zeng", "Feng-Lei Fan"], "abstract": "In this paper, we propose a new super-expressive activation function called the Parametric Elementary Universal Activation Function (PEUAF). We demonstrate the effectiveness of PEUAF through systematic and comprehensive experiments on various industrial and image datasets, including CIFAR10, Tiny-ImageNet, and ImageNet. Moreover, we significantly generalize the family of super-expressive activation functions, whose existence has been demonstrated in several recent works by showing that any continuous function can be approximated to any desired accuracy by a fixed-size network with a specific super-expressive activation function. Specifically, our work addresses two major bottlenecks in impeding the development of super-expressive activation functions: the limited identification of super-expressive functions, which raises doubts about their broad applicability, and their often peculiar forms, which lead to skepticism regarding their scalability and practicality in real-world applications.", "sections": [{"title": "1. INTRODUCTION", "content": "In recent years, deep learning has achieved significant success in many critical areas (LeCun et al., 2015). A major factor contributing to this success is the development of highly effective nonlinear activation functions, which greatly enhance the information processing capabilities of neural networks. While established options like the Rectified Linear Unit (ReLU) and its variants are widely used (Nair and Hinton, 2010), the fundamental importance of activation functions makes the search for better ones a continuous effort. Researchers are persistently working to design and evaluate various activation functions through both theoretical analysis and empirical studies (Bingham and Miikkulainen, 2022; Apicella et al., 2021; Wang et al., 2024).\nIn the realm of approximation theory, it has been shown that certain activation functions can empower a neural network with a simple structure to approximate any continuous function with an arbitrarily small error, using a fixed number of neurons (Maiorov and Pinkus,\n1999). These functions are termed \"super-expressive activation functions\" (Yarotsky, 2021). According to research, to achieve super-expressiveness, an activation function should possess both periodic and analytical components (Shen et al., 2022; Yarotsky, 2021). One such example is the elementary universal activation function (EUAF), defined as follows:\nEUAF(x) := \\begin{cases} |x - 2\\lfloor \\frac{x}{2}\\rfloor| & \\text{for } x \\geq 0, \\\\ 0 & \\text{for } x < 0, \\end{cases}\nThe unique and highly desirable property of super-expressiveness allows neural networks to achieve precise approximation accuracy without increasing network complexity. This contrasts with traditional universal approximation methods, where more complex structures and a higher number of neurons are required as the approximation error decreases. By integrating super-expressive activation functions, one can attain the desired approximation accuracy by merely adjusting parameters, thus maintaining a simpler network architecture.\nTo the best of our knowledge, the development of"}, {"title": null, "content": "super-expressive activation functions faces two technical challenges that hinder their potential value to neural networks: 1) First, only a limited number of super-expressive functions have been identified so far (Maiorov and Pinkus, 1999; Shen et al., 2022; Yarotsky, 2021). It is unclear if the super-expressive property can be broadly applied. Additionally, for deep learning practitioners, having a greater variety of activation functions that exhibit learning capabilities is necessary in terms of enriching their armory. Developing more super-expressive functions increases the likelihood of finding their utilities in important applications, as different activation functions differ in their trainability. 2) Second, the practical utility of super-expressive activation functions is questionable. While superior expressiveness can be theoretically established through specialized constructions that demonstrate the existence of an expressive solution (Shen et al., 2021; Yarotsky, 2021), this does not necessarily translate to better practical performance. Furthermore, it is unclear whether gradient-based methods can effectively learn good solutions for networks using these functions.\nCompared to commonly used functions like ReLU, sigmoid, and tanh, super-expressive functions usually have peculiar shapes. For example, Figure 1 shows EUAF, which is a typical super-expressive activation function. It has a complex and intimidating form, which makes most practitioners skeptical about its scalability and practicality in real-world applications. If we can demonstrate the practical utility of any super-expressive activation function, it could help resolve the skepticism and bridge the gap between their theoretical elegance and usefulness.\nIn addressing the first bottleneck, we substantially generalize the scope of EUAF to encompass a large family of functions capable of achieving super-expressiveness. Specifically, an activation function $\\rho$ is considered to be super-expressive if it is real analytic within a small interval and a fixed-size $\\rho$-activated network can reproduce a triangle-wave function. To address the second bottleneck, we believe that superexpressive functions can indeed be practically useful. Previous studies (Sitzmann et al., 2020; Ramirez et al., 2023) successfully applied the periodic function sin as an activation function within the implicit neural representation. These models have been demonstrated to be suitable for representing complex signals and their derivatives, as well as for solving challenging boundary value problems (Liu et al., 2022a). These studies provide valuable insights into the potential of super-expressive activation functions, since both super-expressive activation functions and sin share periodicity. Moreover, from the perspective of signal decomposition, normal activation functions like ReLU tend to assist models in identifying the direct component (DC) of a signal (Lee et al., 2024). In contrast, super-expressive activation functions can better handle stationary signals due to their inherent periodicity. This characteristic enhances their ability to manage complex real-world signals more efficiently.\nSpecifically, we choose EUAF as our representative and investigate a parameterized variant, named PEUAF, which adaptively learns the frequency $\\omega$ on the positive side. Mathematically,\nPEUAF(x) := \\begin{cases} | \\frac{\\omega x}{2} - 2 \\lfloor \\frac{\\omega x}{4} \\rfloor | - \\frac{\\omega x}{4} + 1 | & \\text{for } x \\geq 0, \\\\ \\frac{x}{1 + |x|} & \\text{for } x < 0, \\end{cases}\nwhere $\\omega$ is the trainable parameter representing the frequency on the positive side. PEUAF can adaptively extract the stationary signals with different frequencies. This adaptability allows PEUAF to effectively capture and represent signals with diverse frequency components, which is particularly advantageous in addressing real-world signal complexities. Then, we validate the effectiveness of PEUAF by experimenting with four industrial datasets (1D data) and three image datasets (2D data). For industrial datasets, our tests show that PEUAF surpasses other activation functions in terms of test accuracy, convergence speed, and fault localization ability. For image datasets, we find that combining PEUAF with other activation functions can usually yield better performance than only using a single activation function, although using PEUAF alone cannot achieve satisfactory performance. Thus, PEUAF can serve as a valuable add-on to the network. Our main contributions are as follows:\n\u2022 We provide a non-trivial generalization of EUAF, showing that a broader family of activation functions can achieve super-expressiveness.\n\u2022 We bridge the gap between the theoretical elegance and empirical usefulness of super-expressive functions by demonstrating their competitive performance in practical applications through systematic"}, {"title": "2. RELATED WORK", "content": "In the field of artificial intelligence, deep neural networks have proven to be highly effective tools. These networks leverage the power of interconnected nodes structured in multiple layers, allowing them to excel in a wide range of complex applications and new domains. At their core, deep neural networks rely on an affine linear transformation followed by a nonlinear activation function. The nonlinear activation function is essential for the successful training of these networks.\nLater in this section, we will first review conventional activation functions including ReLU and its variants, as well as recent sigmoidal activation functions in Section 2.1. We will then discuss super-expressive activation functions in Section 2.2."}, {"title": "2.1. Conventional Activation Functions", "content": "In recent years, the Rectified Linear Unit (ReLU (Nair and Hinton, 2010)), defined as $\\text{ReLU}(x) = \\max(0, x)$, has gained popularity and recognition for its effectiveness in addressing the gradient vanishing and explosion issues encountered with Sigmoid and Tanh activation functions. Thus, ReLU has been widely used in the deep learning community such as industrial fault diagnosis (Liu et al., 2024a) and medical image segmentation (Liu et al., 2024b). However, ReLU can suffer from the occurrence of a number of \"dead neurons\", which results in information loss and can hurt the neural network's feature processing ability. To mitigate this issue, several variants of ReLU have been introduced such as Leaky Rectified Linear Unit (LRELU) (Xu et al., 2015), Parametric Rectified Linear Unit (PRELU) (He et al., 2015), Randomized Leaky Rectified Linear Unit (RRELU) (Xu et al., 2015), Exponential Linear Unit (ELU) (Clevert et al., 2015), Gaussian Error Linear Unit (GELU) (Hendrycks and Gimpel, 2016), and Generalized Linear Unit (GENLU) (Fan et al., 2020). Most recently, Goldenstein et al. (2024) proposed Self-Normalizing ReLU or NeLU to ensure that the prediction model is not affected by the noise level during testing. It has been tested in synthetic data and image de-noising tasks. These"}, {"title": null, "content": "variants represents a significant advancement in activation function design, offering adaptability and potentially better performance. Whereas, their benefits come with the cost of increased model complexity or computation burden and the need for careful tuning and regularization which inspired researchers to create more different activation functions.\nIn addition to these ReLU variants, other kinds of activation functions have also been developed. For example, the Swish (Swish(x) = x \\cdot \\text{sigmoid}(\\beta x)) (Ramachandran et al., 2017) was identified through an automated search using a combination of exhaustive and reinforcement learning as an alternative to ReLU. Its similar shape makes it a reasonable proxy for ReLU in deep learning applications. Mish, defined as Mish(x) = x \\cdot \\text{tanh}(\\text{softplus}(x)) (Misra, 2020), exhibits superior empirical results compared to ReLU, Swish, and LRELU in CIFAR-10 and ImageNet classification tasks. Fractional adaptive linear units FALUS (Zamora et al., 2022) incorporate fractional calculus principles into activation functions, thereby defining a diverse family of activation functions. It has demonstrated enhanced performance in image classification tasks, improving test accuracy. The Seagull activation function, introduced by (Gao and Zhang, 2023), stands out as a customized activation function designed for applications in regression tasks featuring a partially exchangeable target function. It exhibits superiority in addressing the specific demands of regression scenarios.\nOverall, the above-mentioned activation functions are hard to be generalized across different domains, especially in industrial applications. Another problem is that the lack of theoretical analysis limits the acceptance of these activation functions in spite of their good performance. Therefore, it is necessary to verify an activation function with a good theoretical guarantee."}, {"title": "2.2. Super-Expressive Activation Functions", "content": "Numerous studies have explored new activation functions to make a fixed-size network achieve an arbitrary error, referred to as super-expressive activation functions. For example, Maiorov and Pinkus (1999) proposed an activation function to achieve this goal, but it lacks a closed form and is computationally impractical. Recently, Yarotsky (2021) demonstrated that simple functions such as (sin, arcsin) can achieve superexpressiveness, although the relationship between the network size and the dimension was unclear. However, despite the above problems, sin has been proven to be effective in 3D neural network field, indicating the potential of super-expressiveness in neural networks (Ramirez et al., 2023). Shen et al. (2022) proposed"}, {"title": "3. Enriching the Family of Super-expressive Activation Functions", "content": "In this section, we aim to significantly expand the scope of EUAF activation function by introducing a comprehensive collection of activation functions, each with approximation properties akin to those of EUAF. For simplicity, let $\\mathcal{NN}{N, L; \\mathbb{R}^d \\rightarrow \\mathbb{R}^n}$ denote the set of neural networks $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}^n$ that can be represented by $\\varrho$-activated networks, with a maximum width of $N$ and a maximum depth of $L$. Let $\\mathcal{A}$ represent the set of all super-expressive activation functions $\\varrho : \\mathbb{R} \\rightarrow \\mathbb{R}$, which satisfy the following conditions:\n\u2022 There exists an interval $(\\alpha, \\beta)$ with $\\alpha < \\beta$ where $\\varrho$ is real analytic and non-polynomial on $(\\alpha, \\beta)$.\n\u2022 There exists a fixed-size $\\varrho$-activated network $\\Phi$ that can reproduce a triangle-wave function on $[0, \\infty)$, i.e.,\n$\\Phi(x) = |x - 2\\lfloor \\frac{x}{2} \\rfloor| \\forall x \\in [0, \\infty)$.\nWe denote $\\mathcal{A}$ as the \"closure\" of $\\mathcal{A}$. This means a function $\\varrho$ is in $\\overline{\\mathcal{A}}$ if and only if, for any $A > 0$ and $\\varepsilon > 0$, there exists a $\\varrho_\\varepsilon \\in \\mathcal{A}$ such that:\n$|\\varrho_\\varepsilon(x) - \\varrho(x)| < \\varepsilon \\forall x \\in [-A, A]$.\nTheorem 1. Given any $\\varrho \\in \\overline{\\mathcal{A}}$, the hypothesis space\n$\\mathcal{NN}{O(d^2), O(1); \\mathbb{R}^d \\rightarrow \\mathbb{R}}$\nis dense in $C([a, b]^d)$ in terms of the supremum norm."}, {"title": null, "content": "It is crucial to highlight that the constants in the $O(\\cdot)$ notation in Theorem 1 can be explicitly determined and depend only on the choice of $\\varrho$. The proof of Theorem 1 will be provided later in this section.\nBefore giving the proof, let us provide several examples in $\\mathcal{A}$. The first example, $\\varrho_1 \\in \\mathcal{A}$, exhibits an S-shape and is defined as follows:\n$\\varrho_1:=\\begin{cases} \\frac{-x}{1 + |x| + x^2 + \\frac{g(x)}{10}} & \\text{for } x \\leq 0, \\\\ \\frac{g(x)}{x + \\frac{x}{x+1}} & \\text{for } x > 0, \\end{cases}$\nwhere $g(x) = |x - 2\\lfloor \\frac{x}{2} \\rfloor|$ for any $x \\in \\mathbb{R}$.\nThe second example, $\\varrho_2 \\in \\mathcal{A}$, resembles the ReLU activation function and is defined as follows:\n$\\varrho_2:=\\begin{cases} 0 & \\text{for } x \\leq 0, \\\\ \\frac{g(x)}{x + \\frac{x}{x+1}} & \\text{for } x > 0, \\end{cases}$\nThe third example, $\\varrho_3 \\in \\mathcal{A} \\subseteq \\overline{\\mathcal{A}}$, is defined as follows:\n$\\varrho_3:=\\begin{cases} \\arcsin(x) & \\text{for } -1 \\leq x \\leq 1, \\\\ \\sin(x) & \\text{for } |x| > 1. \\end{cases}$\nNow, we will focus on proving the validity of Theorem 1. Given any any $f \\in C([a, b]^d)$ and $\\varepsilon > 0$, our goal is to construct $\\phi \\in \\mathcal{NN}{O(d^2), O(1); \\mathbb{R}^d \\rightarrow \\mathbb{R}}$ such that\n$\\sup_{x \\in [a,b]^d} |\\phi(x) - f(x)| < \\varepsilon$.\nSeveral concepts used to establish Theorem 1 can be traced back to the research conducted by (Shen et al., 2022) and (Yarotsky, 2021). The proof can be divided into three main steps as follows.\n\u2022 The primary objective of the first step is to create a neural network that effectively approximates the univariate function $f \\in C([0, 1])$ within a specific \"half\" interval.\nTheorem 2. Given any $f \\in C([0, 1])$, $\\varrho \\in \\mathcal{A}$, $\\varepsilon > 0, and K \\in \\mathbb{N}$, suppose for any $x_1, x_2 \\in [0, 1]$, it holds that\n$|f(x_1) - f(x_2)| < \\varepsilon/2 \\quad \\text{if } |x_1 - x_2| < 1/K. \\tag{1}$"}, {"title": "3.1. Proof of Theorem 2", "content": "Partition $[0, 1]$ into $2K$ small intervals $I_k$ and $\\overline{I}_k$ for $k = 1, 2, ..., K$, i.e.,\n$I_k = [\\frac{2k-2}{2K}, \\frac{2k-1}{2K}]$ and $\\overline{I}_k = [\\frac{2k-1}{2K}, \\frac{2k}{2K}]$.\nClearly, $[0, 1] = \\bigcup_{k=1}^K (I_k \\cup \\overline{I}_k)$. Let $x_k$ be the right endpoint of $I_k$, i.e., $x_k = \\frac{2k-1}{2K}$ for $k = 1, 2, ..., K$.\nNN {O(1), O(1); $\\mathbb{R} \\rightarrow \\mathbb{R}$} to achieve accurate approximations of $f$ within $I_k$ for $k = 1, 2, ..., K$. It is not essential to consider the values of $\\phi$ within $\\overline{I}_k$ for all $k$. In other words, our focus is primarily on achieving accurate approximations within one \"half\" of the interval $[0, 1]$, which is the crucial element in our proof.\nDefine $\\psi(x) := x - \\sigma(x)$ for any $x \\in \\mathbb{R}$, where $\\sigma \\in NN{\\mathcal{O}(1), \\mathcal{O}(1); \\mathbb{R} \\rightarrow \\mathbb{R}}$ with\n$\\sigma(x) = |x - 2\\lfloor \\frac{x}{2} \\rfloor| \\quad \\text{for } x \\geq 0$.\nIt easy to verifty that\n$\\psi(2Kx)/2 + \\frac{2k-1}{2} = x_k \\quad \\text{for any } x \\in [\\frac{2k-2}{2K}, \\frac{2k-1}{2K}] = I_k. \\tag{2}$\nWe will make use of the two following lemmas to simplify our proof.\nLemma 2 (Lemma 23 of Shen et al. (2022)). Given any rationally independent numbers $a_1, a_2, ..., a_K$ for any $K \\in \\mathbb{N}^+$ and an arbitrary periodic function $g : \\mathbb{R} \\rightarrow \\mathbb{R}$ with period $T$, i.e., $g(x + T) = g(x)$ for any $x \\in \\mathbb{R}$, assume there exist $x_1, x_2 \\in \\mathbb{R}$ with $0 < x_2 - x_1 < T$ such that $g$ is continuous on $[x_1, x_2]$. Then the following set\n$\\{(u \\cdot g(\\omega a_1) + v, ..., u \\cdot g(\\omega a_K) + v) : u, \\omega, v \\in \\mathbb{R}\\}$"}, {"title": null, "content": "is dense in $\\mathbb{R}^K$ provided that\n$\\min_{x \\in [x_1, x_2]} g(x) < \\max_{x \\in [x_1, x_2]} g(x)$.\nLemma 3. Given $K \\in \\mathbb{N}^+$, suppose $\\varrho$ is real analytic and non-polynomial on an interval $(\\alpha, \\beta)$ with $\\beta > \\alpha$. Then there exists $\\omega_0 \\in (-\\frac{\\beta-\\alpha}{2K}, \\frac{\\beta-\\alpha}{2K})$ such that $(\\frac{\\alpha+\\beta}{2K} + k \\omega_0)$, for $\\{k = 1, 2, ..., K\\}$, are rationally independent.\nProof. We prove this lemma by contradiction. If it does not hold, then $\\varrho(\\frac{\\alpha+\\beta}{2K} + k \\omega)$, for $\\{k = 1, 2, ..., K\\}$, are rationally dependent for any $\\omega \\in (-\\frac{\\beta-\\alpha}{2K}, \\frac{\\beta-\\alpha}{2K}) = I$. That means, for any $\\omega \\in I$, there exists $\\lambda = (\\lambda_1, ..., \\lambda_K) \\in \\mathbb{Q}^K \\setminus \\{0\\}$ such that $\\sum_{k=1}^K \\lambda_k \\varrho(\\frac{\\alpha+\\beta}{2K} + k \\omega) = 0$. We observe that $I$ is uncountable and $\\mathbb{Q}^K \\setminus \\{0\\}$ is countable. It follows that there exists $\\lambda = (\\lambda_1, ..., \\lambda_K) \\in \\mathbb{Q}^K \\setminus \\{0\\}$ such that $\\sum_{k=1}^K \\lambda_k \\varrho(\\frac{\\alpha+\\beta}{2K} + k \\omega) = 0$ for all $\\omega$ in an uncountable subset of $I$. Then the real analyticity of $\\varrho$ implies $\\sum_{k=1}^K \\lambda_k \\varrho(\\frac{\\alpha+\\beta}{2K} + k \\omega) = 0$ for all $\\omega \\in I$. By expanding $\\sum_{k=1}^K \\lambda_k \\varrho(\\frac{\\alpha+\\beta}{2K} + k \\omega)$ into the Taylor series at $\\omega = 0$, we get the identity $\\sum_{k=1}^K \\lambda_k k^m \\frac{d^m \\varrho}{d \\omega^m}(\\frac{\\alpha+\\beta}{2K}) = 0$ for each $m$ with $\\frac{d^m \\varrho}{d \\omega^m}(\\frac{\\alpha+\\beta}{2K}) \\neq 0$. Since $\\varrho$ is non-polynomial on $(\\alpha, \\beta) \\ni \\frac{\\alpha+\\beta}{2}$, there are are infinitely many $m$ with $\\frac{d^m \\varrho}{d \\omega^m}(\\frac{\\alpha+\\beta}{2}) \\neq 0$, implying $\\sum_{k=1}^K \\lambda_k k^m = 0$. This means $\\lambda = (\\lambda_1, ..., \\lambda_K) = 0$, a contradiction with $\\lambda \\in \\mathbb{Q}^K \\setminus \\{0\\}$. So we finish the proof of Lemma 3.\nNow, let us return to the proof of Theorem 2. We can employ Lemma 3 to produce a collection of rationally independent numbers. Specifically, there exists a value $\\omega_0$ such that $a_1, a_2, ..., a_K$ are linearly independent, where each $a_k$ is defined as $a_k = \\varrho(\\frac{\\alpha+\\beta}{2K} + k \\omega_0)$. Next, define\n$g(x) = |x - 2\\lfloor \\frac{x}{2} \\rfloor| \\quad \\text{for } x \\in \\mathbb{R}$.\nBy Lemma 2, there exists $u_1, \\omega_1, v_1 \\in \\mathbb{R}$ such that\n$|u_1 \\cdot g(\\omega_1 a_k) + v_1 - f(x_k)| < \\varepsilon/2 \\quad \\text{for any } k$.\nSince $\\varrho(x) = g(x)$ for any $x \\geq 0$ and $g$ is periodic with period 2, we can choose a sufficiently large $m_0 \\in \\mathbb{N}$ such that\n$|u_1 \\varrho(\\omega_1 a_k + 2m_0) + v_1 - f(x_k)|$\n$= |u_1 g(\\omega_1 a_k + 2m_0) + v_1 - f(x_k)|$\n$= |u_1 g(\\omega_1 a_k) + v_1 - f(x_k)| < \\varepsilon/2,$\nfor $k = 1, 2, ..., K$. Define\n$\\varphi(x) = u_1 \\varrho(\\omega_1 (\\frac{\\alpha+\\beta}{2} + (\\frac{\\psi(2Kx)}{2} - \\frac{1}{2})\\omega_0) + 2m_0) + v_1$."}, {"title": null, "content": "For any $x \\in I_k$, we have\n$\\varphi(x) = u_1 \\varrho(\\omega_1 (\\frac{\\alpha+\\beta}{2} + (\\frac{\\psi(2Kx)}{2} - \\frac{1}{2})\\omega_0) + 2m_0) + v_1$\n$= u_1 \\varrho(\\omega_1 (\\frac{\\alpha+\\beta}{2K} + k \\omega_0) + 2m_0) + v_1$\n$= u_1 \\varrho(\\omega_1 a_k + 2m_0) + v_1,$\nimplying\n$|\\varphi(x) - f(x)| \\leq |\\varphi(x) - f(x_k)| + |f(x_k) - f(x)| < \\varepsilon$.\nIt follows that\n$|\\varphi(x) - f(x)| < \\varepsilon \\quad \\text{for any } x \\in \\bigcup_{k=0}^{K-1} [\\frac{2k}{2K}, \\frac{2k+1}{2K}] \\cup [\\frac{2k+1}{2K}, \\frac{2k+2}{2K}].$\nMoreover, we can easily verify $\\varphi \\in NN{\\mathcal{O}(1), \\mathcal{O}(1); \\mathbb{R} \\rightarrow \\mathbb{R}}$. So we finish the proof of Theorem 2."}, {"title": "3.2. Proof of Theorem 3 based on Theorem 2.", "content": "We claim it suffices to prove the special case $[a, b] = [0, 1]$ as this simplification readily extends to the broader scenario. To see this, we simply introduce a linear function $L : [0, \\frac{1}{2}] \\rightarrow [a, b]$ by defining $L(x) = 2(b-a)x+a$. The special case implies $f \\circ L : [0, \\frac{1}{2}] \\rightarrow \\mathbb{R}$ can be approximated by a network $\\phi$ arbitrarily well. Then $\\varphi = \\varphi \\circ L^{-1}$ can approximate $f : [a, b] \\rightarrow \\mathbb{R}$ well, as desired.\nWe can continuously extend $f$ from $[0, \\frac{1}{2}]$ to $\\mathbb{R}$ by setting $f(x) = f(0)$ if $x < 0$ and $f(x) = f(\\frac{1}{2})$ if $x > \\frac{1}{2}$. It follows from the uniform continuity of $f$ on $[-1, 2]$ that there exists $K = K(f, \\varepsilon) \\in \\mathbb{N}^+$ with $K \\geq 2$ such that for any $x_1, x_2 \\in [-1, 2]$,\n$|f(x_1) - f(x_2)| < \\varepsilon/10 \\quad \\text{if } |x_1 - x_2| < 1/K$.\nFor $i = 1, 2, 3, 4$, define\n$f_i(x) := f(x - \\frac{i}{4}) \\quad \\text{for any } x \\in [0, \\frac{1}{2}]$.\nThen, for $i = 1, 2, 3, 4$ and $x_1, x_2 \\in [0, \\frac{1}{2}]$, we have\n$|f_i(x_1) - f_i(x_2)| < \\varepsilon/10 = \\frac{\\varepsilon}{2} \\quad \\text{if } |x_1 - x_2| < 1/K,$\nwhere $\\varepsilon = \\frac{\\varepsilon}{5}$. For each $i \\in \\{1, 2, 3, 4\\}$, by Theorem 2, there exists $\\phi_i \\in NN{\\mathcal{O}(1), \\mathcal{O}(1); \\mathbb{R} \\rightarrow \\mathbb{R}}$ such that\n$|\\varphi_i(x) - f_i(x)| < \\frac{\\varepsilon}{5} = \\frac{\\varepsilon}{5} \\quad \\text{for any } x \\in \\bigcup_{k=0}^{K-1} [\\frac{2k}{2K}, \\frac{2k+1}{2K}].$"}, {"title": null, "content": "Define\n$\\psi(x) = \\sigma(x + \\frac{1}{4} - \\lfloor x + \\frac{1}{4} \\rfloor) \\quad \\text{for any } x \\in \\mathbb{R}$,\nwhere $\\sigma \\in NN{\\mathcal{O}(1), \\mathcal{O}(1); \\mathbb{R} \\rightarrow \\mathbb{R}}$ with\n$\\sigma(x) = |x - 2\\lfloor \\frac{x}{2} \\rfloor| \\quad \\text{for } x \\geq 0$.\nClearly, $0 \\leq \\psi(2Kx) \\leq \\frac{1}{4}$ for any $x \\in [0, \\frac{1}{2}]$, where the last equality comes from the fact that $f_i(x) = f(x - \\frac{i}{4})$ for any $x \\in [0, \\frac{1}{2}] \\setminus [\\frac{i}{4}, 1]$. Define\n$\\Phi(x) := \\sum_{i=1}^{4} \\varphi_i(x + \\frac{i}{4}) \\psi(2Kx + \\frac{1}{2}) \\quad \\text{for any } x \\in [0, \\frac{1}{2}].$\nIt is easy to verify that $\\sum_{i=1}^{4} \\psi(x + \\frac{i}{4}) = 1$ for any $x > 0$ based on the definition of $\\psi$. It follows that $\\sum_{i=1}^{4} \\psi(2K z + \\frac{1}{2}) = 1$ for any $z \\in [0, \\frac{1}{2}]$.\nHence, for any $z \\in [0, \\frac{1}{2}]$, we have\n$|\\Phi(z) - f(z)|$\n$= |\\sum_{i=1}^{4} \\varphi_i(z + \\frac{i}{4}) \\psi(2K z + \\frac{1}{2}) - f(z) \\sum_{i=1}^{4} \\psi(2K z + \\frac{1}{2})|$\n$\\leq \\sum_{i=1}^{4} |\\varphi_i(z + \\frac{i}{4}) \\psi(2K z + \\frac{1}{2}) - f(z) \\psi(2K z + \\frac{1}{2})|$\n$< 4 \\cdot \\frac{\\varepsilon}{5} = \\frac{4\\varepsilon}{5}$.\nTo approximate $(x, y) \\rightarrow xy$ well, we define\n$\\Gamma_{\\delta}(x, y) := \\frac{\\varrho(x_0 + \\delta y) - \\varrho(x_0) - \\delta \\varrho'(x_0) - \\frac{1}{2} \\delta^2 \\varrho''(x_0)}{\\delta \\varrho''(x_0)}$\nfor any $x, y \\in \\mathbb{R}$, where $\\varrho''(x_0) \\neq 0$. Clearly, $\\Gamma_{\\delta}(x, y) \\rightarrow xy$ as $\\delta \\rightarrow 0$. Then we can define\n$\\Phi_5(x) := \\sum_{i=1}^{4} \\Gamma_{\\delta}(\\varphi_i(x + \\frac{i}{4}), \\psi(2Kx + \\frac{1}{2})) \\forall x \\in [0, \\frac{1}{2}].$\nClearly, $\\Phi_5 \\in NN{\\mathcal{O}(1), \\mathcal{O}(1); \\mathbb{R} \\rightarrow \\mathbb{R}}$. Moreover, we can choose a sufficiently small $\\delta_0 > 0$ such that\n$|\\Phi_{\\delta_0}(x) - \\Phi(x)| < \\frac{\\varepsilon}{5} \\quad \\text{for any } x \\in [0, \\frac{1}{2}].$\nBy defining $\\phi := \\varphi_{\\delta_0} \\in NN{\\mathcal{O}(1), \\mathcal{O}(1); \\mathbb{R} \\rightarrow \\mathbb{R}}$, we have\n$|\\phi(x) - f(x)| \\leq |\\Phi_{\\delta_0}(x) - \\Phi(x)| + |f(x) - f(x)| < \\varepsilon$\nfor any $x \\in [0, \\frac{1}{2}]$. So we finish the proof of Theorem 3."}, {"title": "3.3. Proof of Theorem 1 based on Theorem 3 and KST.", "content": "We can safely assume"}]}]}