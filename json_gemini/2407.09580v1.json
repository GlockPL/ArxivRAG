{"title": "Don't Fear Peculiar Activation Functions: EUAF and Beyond", "authors": ["Qianchao Wang", "Shijun Zhang", "Dong Zeng", "Zhaoheng Xie", "Hengtao Guo", "Tieyong Zeng", "Feng-Lei Fan"], "abstract": "In this paper, we propose a new super-expressive activation function called the Parametric Elementary Universal Activation Function (PEUAF). We demonstrate the effectiveness of PEUAF through systematic and comprehensive experiments on various industrial and image datasets, including CIFAR10, Tiny-ImageNet, and ImageNet. Moreover, we significantly generalize the family of super-expressive activation functions, whose existence has been demonstrated in several recent works by showing that any continuous function can be approximated to any desired accuracy by a fixed-size network with a specific super-expressive activation function. Specifically, our work addresses two major bottlenecks in impeding the development of super-expressive activation functions: the limited identification of super- expressive functions, which raises doubts about their broad applicability, and their often peculiar forms, which lead to skepticism regarding their scalability and practicality in real-world applications.", "sections": [{"title": "1. INTRODUCTION", "content": "In recent years, deep learning has achieved significant success in many critical areas (LeCun et al., 2015). A major factor contributing to this success is the development of highly effective nonlinear activation functions, which greatly enhance the information processing capabilities of neural networks. While established options like the Rectified Linear Unit (ReLU) and its variants are widely used (Nair and Hinton, 2010), the fundamental importance of activation functions makes the search for better ones a continuous effort. Researchers are persistently working to design and evaluate various activation functions through both theoretical analysis and empirical studies (Bingham and Miikkulainen, 2022; Apicella et al., 2021; Wang et al., 2024).\nIn the realm of approximation theory, it has been shown that certain activation functions can empower a neural network with a simple structure to approximate any continuous function with an arbitrarily small error, using a fixed number of neurons (Maiorov and Pinkus, 1999). These functions are termed \"super-expressive activation functions\" (Yarotsky, 2021). According to research, to achieve super-expressiveness, an activation function should possess both periodic and analytical components (Shen et al., 2022; Yarotsky, 2021). One such example is the elementary universal activation function (EUAF), defined as follows:\n$\\text{EUAF}(x) := \\begin{cases}  |x - 2\\lfloor \\frac{x}{2} \\rfloor | & \\text{ for } x \\geq 0, \\\\  0 & \\text{ for } x < 0,  \\end{cases}$\nsuper-expressive activation functions faces two technical challenges that hinder their potential value to neural networks: 1) First, only a limited number of super-expressive functions have been identified so far (Maiorov and Pinkus, 1999; Shen et al., 2022; Yarotsky, 2021). It is unclear if the super-expressive property can be broadly applied. Additionally, for deep learning practitioners, having a greater variety of activation functions that exhibit learning capabilities is necessary in terms of enriching their armory. Developing more super-expressive functions increases the likelihood of finding their utilities in important applications, as different activation functions differ in their trainability. 2) Second, the practical utility of super-expressive activation functions is questionable. While superior expressiveness can be theoretically established through specialized constructions that demonstrate the existence of an expressive solution (Shen et al., 2021; Yarotsky, 2021), this does not necessarily translate to better practical performance. Furthermore, it is unclear whether gradient-based methods can effectively learn good solutions for networks using these functions.\nCompared to commonly used functions like ReLU, sigmoid, and tanh, super-expressive functions usually have peculiar shapes. It has a complex and intimidating form, which makes most practitioners skeptical about its scalability and practicality in real-world applications. If we can demonstrate the practical utility of any super-expressive activation function, it could help resolve the skepticism and bridge the gap between their theoretical elegance and usefulness.\nIn addressing the first bottleneck, we substantially generalize the scope of EUAF to encompass a large family of functions capable of achieving superexpressiveness. Specifically, an activation function $\\rho$ is considered to be super-expressive if it is real analytic within a small interval and a fixed-size $\\rho$-activated network can reproduce a triangle-wave function. To address the second bottleneck, we believe that superexpressive functions can indeed be practically useful. Previous studies (Sitzmann et al., 2020; Ramirez et al., 2023) successfully applied the periodic function sin as an activation function within the implicit neural representation. These models have been demonstrated to be suitable for representing complex signals and their derivatives, as well as for solving challenging boundary value problems (Liu et al., 2022a). These studies provide valuable insights into the potential of super-expressive activation functions, since both superexpressive activation functions and sin share periodicity. Moreover, from the perspective of signal decomposition, normal activation functions like ReLU tend to assist models in identifying the direct component (DC) of a signal (Lee et al., 2024). In contrast, super-expressive activation functions can better handle stationary signals due to their inherent periodicity. This characteristic enhances their ability to manage complex real-world signals more efficiently.\nSpecifically, we choose EUAF as our representative and investigate a parameterized variant, named PEUAF, which adaptively learns the frequency $\\omega$ on the positive side. Mathematically,\n$\\text{PEUAF}(x) := \\begin{cases}  |\\omega x - 2\\lfloor \\frac{\\omega x}{2} \\rfloor | - \\frac{1}{1 + x} & \\text{ for } x \\geq 0, \\\\  0 & \\text{ for } x < 0,  \\end{cases}$\nwhere $\\omega$ is the trainable parameter representing the frequency on the positive side. PEUAF can adaptively extract the stationary signals with different frequencies. This adaptability allows PEUAF to effectively capture and represent signals with diverse frequency components, which is particularly advantageous in addressing real-world signal complexities. Then, we validate the effectiveness of PEUAF by experimenting with four industrial datasets (1D data) and three image datasets (2D data). For industrial datasets, our tests show that PEUAF surpasses other activation functions in terms of test accuracy, convergence speed, and fault localization ability. For image datasets, we find that combining PEUAF with other activation functions can usually yield better performance than only using a single activation function, although using PEUAF alone cannot achieve satisfactory performance. Thus, PEUAF can serve as a valuable add-on to the network. Our main contributions are as follows:\n\u2022 We provide a non-trivial generalization of EUAF, showing that a broader family of activation functions can achieve super-expressiveness.\n\u2022 We bridge the gap between the theoretical elegance and empirical usefulness of super-expressive functions by demonstrating their competitive performance in practical applications through systematic"}, {"title": "2. RELATED WORK", "content": "In the field of artificial intelligence, deep neural networks have proven to be highly effective tools. These networks leverage the power of interconnected nodes structured in multiple layers, allowing them to excel in a wide range of complex applications and new domains. At their core, deep neural networks rely on an affine linear transformation followed by a nonlinear activation function. The nonlinear activation function is essential for the successful training of these networks.\nLater in this section, we will first review conventional activation functions including ReLU and its variants, as well as recent sigmoidal activation functions in Section 2.1. We will then discuss super-expressive activation functions in Section 2.2."}, {"title": "2.1. Conventional Activation Functions", "content": "In recent years, the Rectified Linear Unit (ReLU (Nair and Hinton, 2010)), defined as $\\text{ReLU}(x) = \\max(0, x)$, has gained popularity and recognition for its effectiveness in addressing the gradient vanishing and explosion issues encountered with Sigmoid and Tanh activation functions. Thus, ReLU has been widely used in the deep learning community such as industrial fault diagnosis (Liu et al., 2024a) and medical image segmentation (Liu et al., 2024b). However, ReLU can suffer from the occurrence of a number of \"dead neurons\", which results in information loss and can hurt the neural network's feature processing ability. To mitigate this issue, several variants of ReLU have been introduced such as Leaky Rectified Linear Unit (LRELU) (Xu et al., 2015), Parametric Rectified Linear Unit (PRELU) (He et al., 2015), Randomized Leaky Rectified Linear Unit (RRELU) (Xu et al., 2015), Exponential Linear Unit (ELU) (Clevert et al., 2015), Gaussian Error Linear Unit (GELU) (Hendrycks and Gimpel, 2016), and Generalized Linear Unit (GENLU) (Fan et al., 2020). Most recently, Goldenstein et al. (2024) proposed Self-Normalizing ReLU or NeLU to ensure that the prediction model is not affected by the noise level during testing. It has been tested in synthetic data and image de-noising tasks. These variants represents a significant advancement in activation function design, offering adaptability and potentially better performance. Whereas, their benefits come with the cost of increased model complexity or computation burden and the need for careful tuning and regularization which inspired researchers to create more different activation functions.\nIn addition to these ReLU variants, other kinds of activation functions have also been developed. For example, the Swish ($\\text{Swish}(x) = x \\cdot \\text{ sigmoid}(\\beta x)$) (Ramachandran et al., 2017) was identified through an automated search using a combination of exhaustive and reinforcement learning as an alternative to ReLU. Its similar shape makes it a reasonable proxy for ReLU in deep learning applications. Mish, defined as Mish(x) = x\u00b7tanh(softplus(x)) (Misra, 2020), exhibits superior empirical results compared to ReLU, Swish, and LRELU in CIFAR-10 and ImageNet classification tasks. Fractional adaptive linear units FALUS (Zamora et al., 2022) incorporate fractional calculus principles into activation functions, thereby defining a diverse family of activation functions. It has demonstrated enhanced performance in image classification tasks, improving test accuracy. The Seagull activation function, introduced by (Gao and Zhang, 2023), stands out as a customized activation function designed for applications in regression tasks featuring a partially exchangeable target function. It exhibits superiority in addressing the specific demands of regression scenarios.\nOverall, the above-mentioned activation functions are hard to be generalized across different domains, especially in industrial applications. Another problem is that the lack of theoretical analysis limits the acceptance of these activation functions in spite of their good performance. Therefore, it is necessary to verify an activation function with a good theoretical guarantee."}, {"title": "2.2. Super-Expressive Activation Functions", "content": "Numerous studies have explored new activation functions to make a fixed-size network achieve an arbitrary error, referred to as super-expressive activation functions. For example, Maiorov and Pinkus (1999) proposed an activation function to achieve this goal, but it lacks a closed form and is computationally impractical. Recently, Yarotsky (2021) demonstrated that simple functions such as (sin, arcsin) can achieve superexpressiveness, although the relationship between the network size and the dimension was unclear. However, despite the above problems, sin has been proven to be effective in 3D neural network field, indicating the potential of super-expressiveness in neural networks (Ramirez et al., 2023). Shen et al. (2022) proposed EUAF, showing that a network with EUAF requires only $O(d^2)$ width and $O(1)$ depth to achieve superexpressiveness. The potential of EUAF is demonstrated among simple experiments such as function approximation and Fashion-MNIST classification. They also explored the approximation of a neural network with three hidden layers which is named Floor-Exponential-Step (FLES) networks (Shen et al., 2021). The utilized floor function ($\\lfloor x \\rfloor$) can be recognized as an activation function with super-expressiveness (Yarotsky, 2021). In a word, these super-expressive activation functions play a theoretically pivotal role in endowing models with the universal approximation property for all continuous functions. However, previous research either lacked experiments or only included simple ones, leaving it unknown whether these super-expressive functions are practically valuable."}, {"title": "3. Enriching the Family of Super-expressive Activation Functions", "content": "In this section, we aim to significantly expand the scope of EUAF activation function by introducing a comprehensive collection of activation functions, each with approximation properties akin to those of EUAF. For simplicity, let $\\text{NN}_{\\mathcal{A}}\\{N, L; \\mathbb{R}^d \\to \\mathbb{R}^n\\}$ denote the set of neural networks $\\Phi : \\mathbb{R}^d \\to \\mathbb{R}^n$ that can be represented by $\\rho$-activated networks, with a maximum width of N and a maximum depth of L. Let $\\mathcal{A}$ represent the set of all super-expressive activation functions $\\rho : \\mathbb{R} \\to \\mathbb{R}$, which satisfy the following conditions:\n\u2022 There exists an interval $(\\alpha, \\beta)$ with $\\alpha < \\beta$ where $\\rho$ is real analytic and non-polynomial on $(\\alpha, \\beta)$.\n\u2022 There exists a fixed-size $\\rho$-activated network $\\Phi$ that can reproduce a triangle-wave function on $[0, \\infty)$, i.e.,\n$\\Phi(x) = |x - 2\\lfloor \\frac{x}{2} \\rfloor| \\quad \\forall x \\in [0, \\infty)$.\nWe denote $\\mathcal{A}$ as the \"closure\" of $\\mathcal{A}$. This means a function $\\rho$ is in $\\mathcal{A}$ if and only if, for any $A > 0$ and $\\varepsilon > 0$, there exists a $\\rho_{\\varepsilon} \\in \\mathcal{A}$ such that:\n$|\\rho_{\\varepsilon}(x) - \\rho(x)| < \\varepsilon \\quad \\forall x \\in [-A, A]$.\nTheorem 1. Given any $\\rho \\in \\mathcal{A}$, the hypothesis space\n$\\text{NN}_{\\mathcal{A}}\\{O(d^2), O(1); \\mathbb{R}^d \\to \\mathbb{R}\\}$\nis dense in $C([a, b]^d)$ in terms of the supremum norm.\nIt is crucial to highlight that the constants in the $O(\\cdot)$ notation in Theorem 1 can be explicitly determined and depend only on the choice of $\\rho$. The proof of Theorem 1 will be provided later in this section.\nBefore giving the proof, let us provide several examples in $\\mathcal{A}$. The first example, $\\varrho_1 \\in \\mathcal{A}$, exhibits an S-shape and is defined as follows:\n$\\varrho_1 := \\begin{cases}  \\frac{-x}{1 + x + x^2 + 10^{-9}} & \\text{ for } x \\leq 0, \\\\  g(x) & \\text{ for } x > 0,  \\end{cases}$\nwhere $g(x) = |x - 2\\lfloor \\frac{x}{2} \\rfloor|$ for any $x \\in \\mathbb{R}$.\nThe second example, $\\varrho_2 \\in \\mathcal{A}$, resembles the ReLU activation function and is defined as follows:\n$\\varrho_2 := \\begin{cases}  0 & \\text{ for } x \\leq 0, \\\\  \\frac{g(x)}{x + \\frac{x}{x+1}} & \\text{ for } x > 0.  \\end{cases}$\nThe third example, $\\varrho_3 \\in \\mathcal{A} \\subseteq \\mathcal{A}$, is defined as follows:\n$\\varrho_3 := \\begin{cases}  \\frac{1}{2} \\arcsin(x) & \\text{ for } -1 \\leq x \\leq 1, \\\\  \\frac{1}{2} \\sin(x) & \\text{ for } |x| > 1.  \\end{cases}$\nNow, we will focus on proving the validity of Theorem 1. Given any any $f \\in C([a, b]^d)$ and $\\varepsilon > 0$, our goal is to construct $\\varphi \\in \\text{NN}_{\\mathcal{A}}\\{O(d^2), O(1); \\mathbb{R}^d \\to \\mathbb{R}\\}$ such that\n$|\\varphi(x) - f(x)| < \\varepsilon \\quad \\forall x \\in [a, b]^d$.\nSeveral concepts used to establish Theorem 1 can be traced back to the research conducted by (Shen et al., 2022) and (Yarotsky, 2021). The proof can be divided into three main steps as follows.\n\u2022 The primary objective of the first step is to create a neural network that effectively approximates the univariate function $f \\in C([0, 1])$ within a specific \"half\" interval.\nTheorem 2. Given any $f \\in C([0, 1])$, $\\rho \\in \\mathcal{A}$, $\\varepsilon > 0$, and $K \\in \\mathbb{N}$, suppose for any $x_1, x_2 \\in [0, 1]$, it holds that\n$|f(x_1) - f(x_2)| < \\varepsilon / 2 \\quad \\text{if } |x_1 - x_2| < 1/K$.                                              (1)"}, {"title": "3.1. Proof of Theorem 2", "content": "Partition [0, 1] into 2K small intervals $I_k$ and $\\tilde{I}_k$ for $k = 1, 2, ..., K$, i.e.,\n$I_k = [\\frac{2k-2}{2K}, \\frac{2k-1}{2K}]$ and $\\tilde{I}_k = [\\frac{2k-1}{2K}, \\frac{2k}{2K}]$.\nClearly, $[0, 1] = \\cup_{k=1}^{K} (I_k \\cup \\tilde{I}_k)$. Let $x_k$ be the right endpoint of $I_k$, i.e., $x_k = \\frac{2k-1}{2K}$ for $k = 1, 2, ..., K$. Our objective is to construct $\\varphi \\in \\text{NN}_{\\mathcal{A}}\\{O(1), O(1); \\mathbb{R} \\to \\mathbb{R}\\}$ to achieve accurate approximations of $f$ within $I_k$ for $k = 1, 2, ..., K$. It is not essential to consider the values of $\\varphi$ within $\\tilde{I}_k$ for all k. In other words, our focus is primarily on achieving accurate approximations within one \"half\" of the interval [0, 1], which is the crucial element in our proof.\nDefine $\\psi(x) := x - \\sigma(x)$ for any $x \\in \\mathbb{R}$, where $\\sigma \\in \\text{NN}_{\\mathcal{A}}\\{O(1), O(1); \\mathbb{R} \\to \\mathbb{R}\\}$ with\n$\\sigma(x) = |x - 2\\lfloor \\frac{x}{2} \\rfloor| \\quad \\text{for } x \\geq 0$.\nIt easy to verifty that\n$\\psi(2Kx)/2 + \\frac{2k-1}{2K} = x_k \\quad \\text{for any } x \\in [\\frac{2k-2}{2K}, \\frac{2k-1}{2K}] = I_k$.                               (2)\nWe will make use of the two following lemmas to simplify our proof.\nLemma 2 (Lemma 23 of Shen et al. (2022)). Given any rationally independent numbers $a_1, a_2, ..., a_k$ for any $K \\in \\mathbb{N}^+$ and an arbitrary periodic function $g : \\mathbb{R} \\to \\mathbb{R}$ with period T, i.e., $g(x + T) = g(x)$ for any $x \\in \\mathbb{R}$, assume there exist $x_1, x_2 \\in \\mathbb{R}$ with $0 < x_2 - x_1 < T$ such that g is continuous on $[x_1, x_2]$. Then the following set\n$\\{(ug(wa_1) + v, ..., u \\cdot g(wa_K) + v) : u, w, v \\in \\mathbb{R}\\}$"}, {"title": "3.2. Proof of Theorem 3 based on Theorem 2.", "content": "We claim it suffices to prove the special case $[a, b] = [0, 1]$ as this simplification readily extends to the broader scenario. To see this, we simply introduce a linear function $L : [0, \\frac{1}{2}] \\to [a, b]$ by defining $L(x) = \\frac{2(b-a)}{\\frac{1}{2}}x + a$. The special case implies $f \\circ L : [0, \\frac{1}{2}] \\to \\mathbb{R}$ can be approximated by a network $\\phi$ arbitrarily well. Then $\\phi = \\phi \\circ L^{-1}$ can approximate $f : [a, b] \\to \\mathbb{R}$ well, as desired.\nWe can continuously extend $f$ from [0, 1] to R by setting $f(x) = f(0)$ if $x < 0$ and $f(x) = f(\\frac{1}{2})$ if $x > \\frac{1}{2}$. It follows from the uniform continuity of $f$ on $[-1, \\frac{3}{2}]$ that there exists $K = K(f, \\varepsilon) \\in \\mathbb{N}^+$ with K \u2265 2 such that for any $x_1, x_2 \\in [-1, \\frac{3}{2}]$,\n$|f(x_1) - f(x_2)| < \\varepsilon/10 \\quad \\text{if } |x_1 - x_2| < 1/K$.\nFor i = 1, 2, 3, 4, define\n$f_i(x) := f(x - \\frac{i}{4}) \\quad \\text{for any } x \\in [0, \\frac{1}{2}]$.\nThen, for i = 1, 2, 3, 4 and $x_1, x_2 \\in [0, \\frac{1}{2}]$, we have\n$|f_i(x_1) - f_i(x_2)| < \\frac{\\varepsilon}{10} = \\frac{\\varepsilon'}{2} \\quad \\text{if } |x_1 - x_2| < 1/K$,\nwhere $\\varepsilon' = \\frac{4\\varepsilon}{5}$. For each $i \\in \\{1, 2, 3, 4\\}$, by Theorem 2, there exists $\\phi_i \\in \\text{NN}_{\\mathcal{A}}\\{O(1), O(1); \\mathbb{R} \\to \\mathbb{R}\\}$ such that\n$|\\phi_i(x) - f_i(x)| < \\frac{\\varepsilon'}{5} = \\frac{\\varepsilon}{5} \\quad \\text{for any } x \\in \\bigcup_{k=0}^{K-1} [\\frac{2k}{2K}, \\frac{2k+1}{2K}]$. Define\n$\\psi(x) = \\sigma(x + 1 - \\sigma(x + 1)) \\quad \\text{for any } x \\in \\mathbb{R}$,\nwhere $\\sigma \\in \\text{NN}_{\\mathcal{A}}\\{O(1), O(1); \\mathbb{R} \\to \\mathbb{R}\\}$ with\n$\\sigma(x) = |x - 2\\lfloor \\frac{x}{2} \\rfloor| \\quad \\text{for } x \\geq 0$.\nfor any $z \\in [0, \\frac{1}{2}]$, where the last equality comes from the fact that $f_i(x) = f(x - \\frac{i}{4})$ for any $x \\in [0, \\frac{1}{2}] \\subseteq [\\frac{i}{4}, \\frac{1}{2}]$. Define\n$\\phi(x) := \\sum_{i=1}^{4} \\phi_i(x + \\frac{i}{4})\\psi(2Kx + \\frac{i}{2}) \\quad \\text{for any } x \\in [0, \\frac{1}{2}]$.\nIt is easy to verify that $\\sum_{i=1}^{4} \\psi_i(x + \\frac{i}{4}) = 1$ for any $x > 0$ based on the definition of $\\psi$. It follows that $\\sum_{i=1}^{4} \\psi(2Kz + \\frac{i}{2}) = 1$ for any $z \\in [0, \\frac{1}{2}]$.\nHence, for any $z \\in [0, \\frac{1}{2}]$, we have\n$|\\varphi(z) - f(z)|$\n$= |\\sum_{i=1}^{4} \\phi_i(z + \\frac{i}{4})\\psi(2Kz + \\frac{i}{2}) - f(z) \\sum_{i=1}^{4} \\psi(2Kz + \\frac{i}{2})|$\n$|\\sum_{i=1}^{4} (\\phi_i(z + \\frac{i}{4}) - f(z)) \\psi(2Kz + \\frac{i}{2})|$\n$\\< 4 \\cdot \\frac{\\varepsilon'}{5}$.\nTo approximate $(x, y) \\to xy$ well, we define\n$\\Gamma_{\\delta}(x, y) := \\frac{\\rho(x_0 + \\delta y) + \\rho(x_0 - \\delta y) - 2\\rho(x_0)}{\\delta \\rho''(x_0)}$\nfor any $x, y \\in \\mathbb{R}$, where $\\rho''(x_0) \\neq 0$. Clearly, $\\Gamma_{\\delta}(x, y) \\to xy$ as $\\delta \\to 0$. Then we can define\n$\\phi_5(x) := \\sum_{i=1}^{4} \\Gamma_{\\delta}(\\phi_i(x + \\frac{i}{4}), \\psi(2Kx + \\frac{i}{2})) \\quad \\forall x \\in [0, \\frac{1}{2}]$.\nClearly, $\\phi_5 \\in \\text{NN}_{\\mathcal{A}}\\{O(1), O(1); \\mathbb{R} \\to \\mathbb{R}\\}$. Moreover, we can choose a sufficiently small $\\delta_0 > 0$ such that\n$|\\phi_{\\delta_0}(x) - \\varphi(x)| < \\frac{\\varepsilon}{5} \\quad \\text{for any } x \\in [0, \\frac{1}{2}]$.\nBy defining $\\phi := \\phi_{\\delta_0} \\in \\text{NN}_{\\mathcal{A}}\\{O(1), O(1); \\mathbb{R} \\to \\mathbb{R}\\}$, we have\n$|\\phi(x) - f(x)| \\leq |\\phi_{\\delta_0}(x) - \\varphi(x)| + |\\varphi(x) - f(x)| < \\varepsilon$\nfor any $x \\in [0, \\frac{1}{2}]$. So we finish the proof of Theorem 3."}, {"title": "3.3. Proof of Theorem 1 based on Theorem 3 and KST.", "content": "We can safely assume that $[a, b] = [0, 1]$ since the general case can be readily extended by incorporating an affine map such as $L(x) = (b - a)x + a$. Given any $f \\in C([0, 1]^d)$, by KST, there exist $h_{i,j} \\in C([0, 1])$ and $g_i \\in C(\\mathbb{R})$ for $i = 0, 1, ..., 2d$ and $j = 1, 2, ..., d$ such that\n$f(x) = \\sum_{i=0}^{2d} g_i(\\sum_{j=1}^{d} h_{i,j}(x)) \\quad \\forall x = (x_1, ..., x_d) \\in [0, 1]^d$.\nChoose a sufficiently large $A > 0$, e.g.,\n$A = 1 + \\sup\\{|\\sum_{j=1}^{d} h_{i,j}(x_j)| : i = 0, 1, ..., 2d, x \\in [0, 1]^d\\}$.\nThen for any $\\delta > 0$, by Theorem 3, there exist $\\psi_{i,j}, \\phi_i \\in \\text{NN}_{\\mathcal{A}}\\{O(1), O(1); \\mathbb{R} \\to \\mathbb{R}\\}$ such that\n$|g_i(t) - \\phi_i(t)| < \\delta \\quad \\text{for any } t \\in [-A, A]$\nand\n$|h_{i,j}(t) - \\psi_{i,j}(t)| < \\delta \\quad \\text{for any } t \\in [0, 1]$,\nfor $i = 0, 1, ..., 2d$ and $j = 1, 2, ..., d$. By defining\n$\\varphi(x) = \\sum_{i=0}^{2d} \\phi_i(\\sum_{j=1}^{d} \\psi_{i,j}(x)) \\quad \\forall x = (x_1, ..., x_d) \\in \\mathbb{R}^d$,\nwe have $\\varphi \\in \\text{NN}_{\\mathcal{A}}\\{O(d^2), O(1); \\mathbb{R} \\to \\mathbb{R}\\}$. Moreover, by choosing sufficiently small $\\delta > 0$, we can conclude that\n$|\\varphi(x) - f(x)| < \\varepsilon \\quad \\forall x \\in [0, 1]^d$,\nwhich means we finish the proof of Theorem 1."}, {"title": "4. Experimental Results", "content": "To further validate the efficacy of our activation functions, we evaluate PEUAF against a wide range of baseline activation functions, including LRELU (Xu et al., 2015), PRELU (He et al., 2015), Softplus (Zheng et al., 2015), ELU (Clevert et al., 2015), SELU (Klambauer et al., 2017), ReLU (Nair and Hinton, 2010) and Swish (Ramachandran et al., 2017). We conduct these comparisons across four industrial signal datasets and three image datasets (CIFAR-10 (Krizhevsky et al., 2009), Fashion-MNIST (Xiao et al., 2017) and ImageNet (Deng et al., 2009)) using three distinct neural network architectures (LeNet-type (Lecun et al., 1998), ResNet-18 (He et al., 2016), and VGG-16 (Simonyan and Zisserman, 2015)).\nAs discussed in (Shen et al., 2022), only a few neurons with super-expressive activation functions are required to approximate functions with arbitrary precision to avoid large generalization errors. However, implementing this in practical experiments is challenging. Therefore, our experiments primarily focus on exploring the feature patterns of PEUAF and determining how it contributes to improving test accuracy, instead of targeting 100% test accuracy."}, {"title": "4.1. Experimental Setups", "content": "The datasets used in our experiments are briefly introduced in Table 1. For each experiment, we train the models with a batch size of 64 using the \"NAdam\" optimizer (Dozat, 2016), with an initial learning rate of 0.01. The learning rate decays with a factor of 0.2 if the accuracy change over 5 consecutive epochs is no more than $1 \\times 10^{-4}$. We set the number of epochs to 300 to ensure proper convergence. The baseline network structures employed in our experiments are introduced in Tables 2 and 3.\nThe most critical hyperparameter is the range of the adaptive frequency $\\omega$. To determine this, we conducted a classification experiment with different $\\omega$ values on the PQD dataset (A et al.), as illustrated in Figure 8. The network structure used is Baseline A, a 1D convolutional neural network. To emphasize the discrepancies in outcomes, we employ a logarithmic transformation (log) during the visualization of the loss function. Figure 8 shows the training and validation curves, while Table 4 provides the corresponding test accuracy. The table reveals two key points: First, when $\\omega$ exceeds 1, the test accuracy drops significantly, indicating that higher frequencies pose challenges to the PEUAF's ability to effectively extract PQD features. Second, when $\\omega$ lies in the range of [0, 1], the test accuracy consistently remains above 98%. Therefore, we reasonably conclude that the frequency $\\omega$ should be constrained within the range of [0, 1]."}, {"title": "4.2. Analysis Experiments", "content": "In this section, we conduct experiments to show the characteristics of PEUAF. For the larger datasets (CWRU, PQD, and MF), we utilize the Baseline A in Table 2, while for the EFDC dataset, we use the Baseline B in Table 3. Baseline B is smaller than Baseline A due to the smaller size of the EFDC dataset compared to CWRU, PQD, and MF. Our comparison focuses not only on the overall performance but also on the convergence behavior during the training process, fluctuations in the validation process, and a detailed mechanism analysis."}, {"title": "5. Conclusion and Discussion", "content": "This paper provides an in-depth analysis of the characteristics and effectiveness of PEUAF, particularly focusing on its application to industrial and image datasets. By testing the trainable frequency $\\omega$, we have determined an optimal frequency range for $\\omega$ within the interval [0, 1"}]}