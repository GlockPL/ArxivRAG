{"title": "Overview of the First Workshop on Language Models for\nLow-Resource Languages (LoResLM 2025)", "authors": ["Hansi Hettiarachchi", "Tharindu Ranasinghe", "Paul Rayson", "Ruslan Mitkov", "Mohamed Gaber", "Damith Premasiri", "Fiona Anting Tan", "Lasitha Uyangodage"], "abstract": "The first Workshop on Language Models for\nLow-Resource Languages (LoResLM 2025)\nwas held in conjunction with the 31st Interna-\ntional Conference on Computational Linguis-\ntics (COLING 2025) in Abu Dhabi, United\nArab Emirates. This workshop mainly aimed\nto provide a forum for researchers to share and\ndiscuss their ongoing work on language mod-\nels (LMs) focusing on low-resource languages,\nfollowing the recent advancements in neural\nlanguage models and their linguistic biases\ntowards high-resource languages. LoResLM\n2025 attracted notable interest from the natural\nlanguage processing (NLP) community, result-\ning in 35 accepted papers from 52 submissions.\nThese contributions cover a broad range of low-\nresource languages from eight language fami-\nlies and 13 diverse research areas, paving the\nway for future possibilities and promoting lin-\nguistic inclusivity in NLP.", "sections": [{"title": "1 Introduction", "content": "Language models (LMs) have been a long-standing\nresearch topic, originating with simple n-gram\nmodels in the 1950s (Shannon, 1951). They are\ncomputational models that use the generative like-\nlihood of word sequences to perform natural lan-\nguage processing (NLP) tasks (Zhao et al., 2023).\nRecent advancements in LMs have significantly\nshifted towards neural language models due to their\nmore robust capabilities (Zhao et al., 2023; Minaee\net al., 2024). Developing pre-trained neural lan-\nguage models/transformers is a key milestone in\nLM research that notably enhanced NLP perfor-\nmance (Vaswani et al., 2017; Devlin et al., 2019).\nThis breakthrough has also prompted the devel-\nopment of more advanced large language models\n(LLMs), such as GPT, which consist of vast num-\nbers of parameters pre-trained on extensive text cor-\npora, resulting in state-of-the-art natural language\nunderstanding and generation across various appli-\ncations (Touvron et al., 2023; Jiang et al., 2023).\nThere are approximately 7,000 spoken languages\nworldwide (van Esch et al., 2022). However, most\nNLP research focuses on about 20 languages with\nhigh resources (Magueresse et al., 2020). For ex-\nample, 63% of the papers published at ACL 2008\nfocused on English (Bender, 2011), and even a\ndecade later, 70% of the papers at ACL 2021 were\nevaluated only in English (Ruder et al., 2022). The\nremaining numerous languages that receive little\nresearch attention are commonly referred to as low-\nresource languages. These languages generally\nlack sufficient digital data and resources to support\nNLP tasks. They are also known as resource-scarce,\nresource-poor, less computerised, low-data, or low-\ndensity languages (Ranathunga et al., 2023).\nSince the capabilities of LMs are primarily de-\ntermined by the characteristics of their pre-trained\nlanguage corpora, disparities in language resources\nare also evident within the models. For instance,\nmany widely used transformer models (e.g., BERT\n(Devlin et al., 2019), ROBERTa (Liu et al., 2019),\nBART (Lewis et al., 2020), and T5 (Raffel et al.,\n2020)) only support English. However, the cross-\nlingual capabilities of transformers have paved the\nway for multilingual models (e.g., mBERT (Devlin\net al., 2019), XLM-R (Conneau et al., 2020), mT5\n(Xue et al., 2021), and BLOOM (Scao et al., 2022)),\nallowing low-resource languages to benefit from\nother languages through joint learning approaches.\nDespite this progress, these models are typically\nlimited to up to 100 languages due to the curse of\nmultilingualism (Conneau et al., 2020). In light\nof this challenge, developing monolingual mod-\nels (e.g., SinBERT for Sinhala (Dhananjaya et al.,\n2022), and PhoBERT for Vietnamese (Nguyen and\nTuan Nguyen, 2020)) is another growing trend\nrecently established to promote research in low-\nresource languages.\nThere are several common factors which impede\nlow-resource language research. One major issue\nis limited data availability, as the performance of"}, {"title": "2 Workshop Contributions", "content": "LoResLM 2025 received 52 submissions, including\n40 long papers and 12 short papers. Among these,\nwe accepted 35 papers, including 28 long papers\nand seven short papers, to appear in the workshop\nproceedings, following the review process. We\nprovide a detailed summary of the distribution of\naccepted papers across various languages and re-\nsearch areas below."}, {"title": "2.1 Languages", "content": "As illustrated in Figure 1, the papers accepted to\nLoResLM 2025 mainly span eight language fam-\nilies. The majority representation is from Indo-\nEuropean family, while Koreanic, Sino-Tibetan and\nIsolate language families have equal minority rep-\nresentation. Languages with no relationships with"}, {"title": "2.2 Research Areas", "content": "Table 2 shows the distribution of the accepted pa-\npers across various NLP research areas. These\nareas were adopted based on the topics of call for\npapers from leading NLP conferences in 2024.\nOverall, the accepted papers contributed to 13\nNLP research areas. As expected, the most popular\ntopic among the accepted papers was \u2018Language\nModelling' with eleven papers. \u2018Machine Trans-"}, {"title": "3 Conclusions", "content": "The first Workshop on Language Models for Low-\nResource Languages (LoResLM 2025) attracted\na lot of interest from the NLP community, hav-\ning 35 accepted papers from 52 submissions. The\naccepted papers mainly span eight language fam-\nilies, with the majority representation being from\nIndo-European families. Furthermore, the accepted\npapers contributed to 13 NLP research areas, with\nmajor contributions to \u2018Language Modelling' and\n'Machine Translation and Translation Aids'. We\nbelieve the findings and resources from LoResLM\nwill open exciting new avenues to empower linguis-\ntic diversity for millions of low-resource languages.\nFor the future iterations of LoResLM, we expect\nbetter representation from more diverse linguistic\ngroups, particularly those from underrepresented\nfamilies such as Uralic, Dravidian and Indigenous\nlanguages of the Americas. Furthermore, we aim\nto diversify research topics, encouraging work in\nareas such as speech processing, information ex-\ntraction, and dialogue systems, which are critical\nfor many practical applications."}]}