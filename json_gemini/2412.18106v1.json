{"title": "Tackling the Dynamicity in a Production LLM Serving System with SOTA Optimizations via Hybrid Prefill/Decode/Verify Scheduling on Efficient Meta-kernels", "authors": ["Mingcong Song", "Xinru Tang", "Fengfan Hou", "Jing Li", "Wei Wei", "Yipeng Ma", "Runqiu Xiao", "Hongjie Si", "Dingcheng Jiang", "Shouyi Yin", "Yang Hu", "Guoping Long"], "abstract": "Meeting growing demands for low latency and cost efficiency in production-grade large language model (LLM) serving systems requires integrating advanced optimization techniques. However, dynamic and unpredictable input-output lengths of LLM, compounded by these optimizations, exacerbate the issues of workload variability, making it difficult to maintain high efficiency on AI accelerators, especially DSAs with tile-based programming models. To address this challenge, we introduce XY-Serve, a versatile, Ascend native, end-to-end production LLM-serving system. The core idea is an abstraction mechanism that smooths out the workload variability by decomposing computations into unified, hardware-friendly, fine-grained meta primitives. For attention, we propose a meta-kernel that computes the basic pattern of matmul-softmax-matmul with architectural-aware tile sizes. For GEMM, we introduce a virtual padding scheme that adapts to dynamic shape changes while using highly efficient GEMM primitives with assorted fixed tile sizes. XY-Serve sits harmoniously with vLLM. Experimental results show up to 89% end-to-end throughput improvement compared with current publicly available baselines on Ascend NPUs. Additionally, our approach outperforms existing GEMM (average 14.6% faster) and attention (average 21.5% faster) kernels relative to existing libraries. While the work is Ascend native, we believe the approach can be readily applicable to SIMT architectures as well.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) [41, 42] have achieved impressive accuracy and are widely applied in fields like natural language processing [19] and computer vision [21,25]. As shown in Fig. 1(a), LLM inference typically consists of two stages: prefill and decode. During the prefill stage, LLMs process the user's input to generate an initial token, while concurrently caching the key/value (K/V) data for future use.\nIn the decode stage, tokens are generated sequentially in an auto-regressive manner. Despite their impressive performance, LLMs come with significant computational costs and latency. As the model size increases and input sequences become longer, the computational demands grow substantially, making online inference increasingly challenging [30].\nTo address these challenges, a number of optimization techniques have emerged to reduce inference costs and latency, such as Automatic Prefix Caching (APC) [8, 47], Speculative Decoding (SD) [20, 22, 32, 33, 38, 46], and SplitFuse [17, 18, 26]. APC enables new queries with matching prefixes to reuse cached K/V data and skip computations for shared segments, thereby improving prefill performance. The adoption of draft model in SD provides a chance for target model to generate multiple tokens per step, enhancing key/value cache and model weight reuse, which helps mitigate the memory-bound bottleneck of decode in draft model. SplitFuse splits long-sequence prefill tokens into smaller chunks and schedules them alongside decode tokens, reducing interruptions to the decode stage.\nWhile these optimizations promise to improve inference efficiency, they also introduce new complexities. For LLM"}, {"title": "2 Motivation", "content": "In this section, we explore the challenges posed by dynamic workloads in Linear and Attention modules and analyze the complexities of handling P/D/V hybrid stages, shown in Fig. 2. Finally, we discuss the additional challenges faced by AI accelerators with tile-based programming models, such as the Huawei Ascend NPU [34, 35], in supporting dynamic workloads."}, {"title": "2.1 Diverse Attention", "content": "The introduction of new technologies, such as APC, SD, and SplitFuse, increases the diversity of Attention shapes and mask structures, leading to MFU and MBU issues of the current NPU attention kernel (torch-npu 2.1 FusedInferAttentionScore [10]), as illustrated in Fig 3.\nFirstly, for prefill Attention, without any optimizations, the query length equals the key-value length, resulting in a square-shaped Attention score matrix and a lower triangular mask. This shape is ideal for optimization [36], as the sparsity in the mask can be exploited to achieve high performance. The current state-of-the-art (SOTA) torch-npu kernel efficiently handles such square-shaped inputs with triangular masks, achieving an MFU of 53%.\nHowever, in real-world scenarios involving prefixes, parts of the key and value are reused from the K/V cache [8,47], transforming the Attention score shape from a square to a rectangle with arbitrary dimensions. For these rectangular shapes, the performance of the torch-npu kernel degrades significantly, with MFU dropping to 47% and 30%, respectively. While the reuse of the K/V cache theoretically reduces computation, the decline in kernel efficiency offsets this benefit, resulting in no meaningful end-to-end performance improvement.\nSecondly, unlike Linear, where tokens from different"}, {"title": "2.2 Dynamic GEMM", "content": "GEMM is a core operation in LLMs. Beyond the four GEMM operations in the Linear section, the query-key (QK) and score-value (SV) computations in Attention also rely on GEMM. We summarize all of GEMM operations in Fig. 4(a). For Linear operations, the M dimension is tied to the number of tokens. For Attention QK and SV operations, the N and K dimensions are dependent on the token K/V length. As previously discussed, the number of tokens in the prefill and verify stages is highly dynamic, and this variability is further"}, {"title": "2.3 Hybird P/D/V Stages", "content": "In practical systems, P/D/V stages may exist independently or coexist simultaneously, leading to arbitrary combinations of interleaved P/D/V stages within a given scheduling budget. For Linear operations, tokens from different stages can be grouped together and treated as the left matrix in a GEMM operation, sharing the same weight matrix on the right. This approach is straightforward, as tokens from different stages can reuse the same large model weights.\nIn contrast, handling Attention operations is significantly more complex. Stages are independent, and even within the same stage, batches are also independent. Enumerating and tailoring optimizations for each possible combination of stages and batches is highly labor-intensive and impractical.\nA common alternative is a batch-by-batch execution, such as selective batching [44], which processes each batch independently by invoking the corresponding Attention kernel. However, this method introduces additional memory overhead from splitting, rearranging, and merging data, which degrades overall system performance. As shown in Fig. 3(d), the memory overhead may account for more than 50%. Furthermore, batch-by-batch processing leads to inefficient utilization of computational resources, further limiting system efficiency."}, {"title": "2.4 Ascend NPU Micro-architecture", "content": "Built on Huawei's DaVinci architecture [34, 35], the Huawei Ascend NPU is a high-performance AI processor. Fig. 5 illustrates the micro-architecture of the Ascend, which consists primarily of AIC and AIV components. The AIC, similar to Nvidia's Tensor Core, handles matrix computations, while the AIV is responsible for vector operations. AIC and AIV are separated without a direct datapath, so ensuring their data interactions occur via the L2 cache is crucial when designing mixed kernels. Compared to GPUs, NPUs have larger core granularity, making load balancing between cores even more critical. The Memory Transfer Engine (MTE) handles data movement. Whether AIC, AIV, or MTE, all data processing and transferring occur at the tile level. AI accelerators with tile-based programming models are gaining increasing attention in the community. However, this tile-based processing model encounters significant challenges when managing dynamic workloads that vary at the token granularity. Typical solutions involve complex padding/unpadding operations, which result in wasted computation and memory, leading to substantial overhead."}, {"title": "3 Overview of XY-Serve", "content": "To address the aforementioned challenges, we developed XY-Serve, a versatile end-to-end production LLM-serving system, which is built on four key components: Token-wise Scheduling, Dynamic Task Decomposition and Reordering, Meta-Attention, and SmoothGEMM."}, {"title": "3.1 Token-wise Scheduling", "content": "When a user's request enters the system, it first passes through the APC module, which matches the incoming prompt against existing prompts in the K/V cache, enabling token-wise reuse. Any unmatched tokens are added to the scheduling queues. Consequently, the prompt length in the scheduling queues is the user's input length minus the length of tokens already cached in the K/V cache. Since both the user's input and the cached token lengths are dynamically variable, the prompt lengths in the scheduling queues become even more dynamic. The scheduling queues also include decode and speculative tokens from previous requests awaiting processing.\nOur scheduling system selects a fixed-budget length of tokens from the scheduling queues to form chunks, which may include tokens from prefill, verify, or decode stages. To improve first-token latency, prefill requests are prioritized. If a user's prefill prompt exceeds the budget length, it is split into"}, {"title": "3.2 Task Decomposition", "content": "While token-wise scheduling improves efficiency by reducing bubbles and optimizing resource utilization, the four levels of dynamism it introduces pose significant challenges for execution, especially on AI accelerators with tile-based programming models.\nTo address these challenges, we propose a dynamic decomposition mechanism that converts dynamic workloads into hardware-friendly, tile-based computational units. Using the Token-Table, each stage is logically decomposed into tile blocks. At the tile level, computation modules can process these blocks in parallel without distinguishing their P/D/V stage origin. Importantly, this tiling decomposition is purely logical, requiring no changes to the physical data layout.\nIn the following sections, we present a detailed analysis of"}, {"title": "3.2.1 Attention Decomposition", "content": "For attention, the Token-Table contains entries for each P/D/V stage, with each entry specifying key attributes, including the stageID, the start position, the number of tokenNum, the historical kvLen, and the tileSize. As shown in Fig. 6(b), stage-1 (P) is decomposed into three tiling blocks, while stage-2 and stage-4 (D) are each divided into one tiling block, and stage-3 (V) is also decomposed into one tiling block. Each tiling block consists of headNum tiling units, resulting in a total of 6 \u00d7 headNum tiling units at the tiling level."}, {"title": "3.2.2 Linear Decomposition", "content": "For Linear operations, since tokens from different stages can share the same weights, they are concatenated into a single, large tensor and multiplied by the shared weights. This approach avoids multiple GEMM invocations, enhances weight reuse, and streamlines computation. Consequently, Linear operations do not need to be differentiated between stages, and can be processed uniformly.\nIn the Token-Table, each Linear operator corresponds to a single entry shown in 6(c). The four primary Linear operations are QKV, OProj, GateUp, and Down. For these operations, the start position is set to 0, indicating that all tokens are processed from the beginning of the concatenated tensor. The tokenNum equals the total number of tokens in the currently scheduled chunk. Tiling is performed on the result matrix of dimensions tokenNum \u00d7 nLen, where each tiling block corresponds to a submatrix of the result. Each tiling block"}, {"title": "3.3 Task Reordering", "content": "After decomposing the dynamic workloads from P/D/V mixed stages into fundamental tile units, it is necessary to reorder these tile units and generate a Task-Table to enhance performance. The Task-Table is responsible for scheduling these tile units onto the hardware, with each entry specifying a coreID and the list of tiles assigned to that core. Based on this Task-Table, Attention, and Linear can simply retrieve the corresponding tile units according to their coreID. This approach not only maximizes hardware efficiency but also simplifies the design of Attention and Linear kernels."}, {"title": "3.3.1 Attention Reordering", "content": "After performing dynamic tiling on the various stages, the resulting tiles exhibit varying values for tileSize and kvLen. This variation can lead to load imbalances during parallel processing. To address this, we calculate the computational load of each tile as its area, defined as tileSize \u00d7 kvLen.\nFor efficient scheduling, the tiles are initially sorted based on their computational load, from largest to smallest. Subsequently, the tiles are allocated to the AI cores in a symmetrical round-robin fashion. As depicted in Fig. 6(b), assuming there are four AI cores, the tile units are assigned in the sequence core-1, core-2, core-3, core-4, core-4, core-3, core-2, core-1, and so forth, repeating this pattern to ensure a balanced and efficient allocation of computational tasks.\nThis task scheduling information is stored in the Task-Table and passed to the Attention module. The Task-Table guides the Attention module to perform parallel processing efficiently, leveraging both the head and tile dimensions. This mechanism ensures balanced computation across AI cores, maximizing hardware utilization."}, {"title": "3.3.2 Linear Reordering", "content": "Given that only a limited set of fixed shapes is supported, we perform offline optimization to determine the most efficient task allocation strategies for linear ops. This involves profiling and customizing task allocation for each shape to maximize performance. The optimized strategies are stored for use during execution. During runtime, XY-Serve leverages the Token-Table to identify the current shape and uses this information to retrieve the corresponding pre-optimized Task-Table. The Task-Table is then passed to the Linear module, guiding it to execute tasks in an optimized manner."}, {"title": "4 Meta-Attention", "content": "In this section, we first explain how our attention module supports advanced features such as APC, Chunked Prefill, and SD. Then, we describe how we optimize attention performance to push it to the hardware limits."}, {"title": "4.1 Meta-Attention Design", "content": "4.1.1 Handling Token-wise Processing\nThe core requirement for supporting both prefix reuse and Chunked Prefill is that the attention module must be capable of handling arbitrary K/V cache lengths and performing token-wise K/V cache reuse. To achieve this, we use a radix tree to efficiently manage the K/V cache. As shown in Fig. 7, each node in the radix tree represents a K/V cache block. The radix tree allows for quick matching of historical K/V cache blocks. When a mismatch occurs at a particular node (i.e., its corresponding K/V cache block contains only a partial match), we use a copy-on-write mechanism to create a new block, refresh the new data into this block, and then add the block back into the radix tree. If additional new blocks are generated after this mismatch, these blocks are directly inserted as child nodes of the mismatched block in the radix tree.\nThis mechanism effectively manages both historical and newly generated K/V data, seamlessly merging them using copy-on-write to ensure the continuity of the K/V cache. During the prefill attention process, the corresponding K/V blocks\u2014both historical and newly generated\u2014are read based on the block table. We also track the actual number of tokens in the last block, ensuring accurate token-wise processing. Additionally, our system can automatically cache historical K/V data as prefixes, relieving the user from manually specifying them. The system allows users to set an upper limit on the amount of historical K/V data to be cached. Once this limit is reached, or when space is needed for new K/V data, the system will automatically evict the least recently used K/V blocks from the leaf nodes of the radix tree."}, {"title": "4.1.2 Minimizing Mask for Speculative Decoding", "content": "Speculative execution can be classified into two types: sequence-based speculation [20, 33, 38, 46] and tree-based speculation [22, 32]. Sequence-based speculation generates"}, {"title": "4.2 Meta-Attention Optimizations", "content": "4.2.1 Tile-Based Cube-Vector Orchestration\nTo achieve parallel execution of cube and vector units, we propose a pipeline, shown in Fig. 9. It ensures intermediate data transfers occur exclusively via the L2 cache, avoiding costly HBM accesses. The cube unit is responsible for the QK and SV computations, while the vector unit performs the Softmax. If processed sequentially (QK \u2192 Softmax \u2192 SV), only one unit would be active at a time, leading to inefficiencies. To address this, we adopted a pipelined approach as displayed in Fig. 10(a): after completing the QK computation for the first tiling data, its Softmax computation is initiated while"}, {"title": "4.2.2 Exploiting Mask Sparsity", "content": "To fully utilize the sparsity in the attention mask and skip redundant computations, we adopt a mask-aware strategy that significantly enhances efficiency. As displayed in Fig. 11, in the attention computation, once the query dimension coordinate index qLen and the kvLen of K/V are known, any data corresponding to positions after qLen + kvLen in each row of the attention score matrix is invalid. This insight allows us to guide the QK, Softmax, and SV operations to skip computations in these invalid regions.\nIn the QK operation, this principle enables us to directly omit the computation of corresponding results, effectively skipping the red tiling blocks in the result matrix. For the Softmax operation, since it is calculated row by row, we can precisely control which attention scores are included in the computation at the token granularity. This token-level control enables us to support masks of arbitrary shapes while maintaining a mask-free design that eliminates the overhead of users generating and passing in masks externally. For the SV operation, the skipped computations occur during the reduced sum along the K dimension, where certain tiling blocks can be excluded based on the sparsity pattern. By applying these"}, {"title": "5 SmoothGEMM", "content": "As discussed earlier, designing a matrix multiplication operation that supports arbitrary shapes while maintaining high performance across all possible shapes is a significant challenge. To address this, we adopt a memory-compute co-design strategy. Instead of optimizing matrix multiplication for every possible shape, we focus on maximizing performance for fixed shapes. To handle arbitrary shapes effectively, we introduce virtual padding at the on-chip memory level, along with selective read and write mechanisms. This approach allows matrix computations to accommodate a wide range of shapes seamlessly while still benefiting from the performance advantages of fixed-shape optimizations."}, {"title": "5.1 Virtual Padding on the M Dimension", "content": "As illustrated in Fig. 12(a), the dimension M in GEMM is intrinsically related to the number of tokens. In the case of Linear GEMM, the M dimension corresponds to the cumulative token count across P/D/V stages, while in Attention GEMM, it is determined by the token count in each stage. In cube-based or tensor-based AI accelerators, matrix computations are typically constrained by a minimum tiling size (e.g., 16 \u00d7 16 for cube cores). A common practice is to pad the M dimension to align with a multiple of the tiling size. However, this dynamic padding introduces non-trivial memory overhead and degrades performance.\nTo mitigate these issues, we replace the physical padding in global memory with virtual padding on the chip, combined with the selective read and write mechanisms shown in Fig. 12(a). This approach allows for efficient handling of matrices with arbitrary shapes. Specifically, on-chip buffer allocations"}, {"title": "5.2 Optimizations for N and K Dimensions", "content": "As illustrated in Fig. 4(a), the dimensions of N and K in linear operations are determined by the model structure. These dimensions are typically multiples of hardware tile size, such as 16, eliminating the need for additional padding.\nFor Attention GEMM ops, dimensions N and K are tied to the sequence length, which can take arbitrary values. In theory, padding would be required for these dimensions. However, this is naturally handled by the K/V cache's block page structure, which stores and reads data in blocks aligned to multiples of tile size. As shown in Fig. 12(b), by reading entire blocks, the read length inherently conforms to the required hardware tile size. Furthermore, this padding does not affect the final Attention computation because we explicitly set the values in the padded regions to zero during Softmax calculation. This ensures that the padded values are effectively excluded from the Attention score. Additionally, since the padded data is reduced during the Attention score \u00d7 value operation, it does not influence the shape of the final output.\nTherefore, matrix multiplication for arbitrary shapes can be efficiently supported without introducing additional padding overhead for the N and K dimensions."}, {"title": "5.3 Handling Dynamic Shapes", "content": "Given our focus on fixed shapes, such as multiples of the tiling size, and operating under budget constraints, we narrow the range of token sizes we handle. While the token count can vary significantly across different workloads, we apply a smoothing technique that reduces the shapes involved to fixed, well-defined sizes. This enables us to perform offline customization and optimization for these specific shapes, particularly for optimizations that are difficult to implement statically, such as swizzling [15].\nSwizzling is a technique that optimizes matrix multiplication performance by altering the task allocation order across AI cores, enhancing L2 cache hit rates. However, determining the optimal allocation strategy for swizzling is complex and cannot be easily derived through theoretical formulas."}, {"title": "5.4 Removing Memory Overheads", "content": "For the QKV input, its shape is [tokens, heads + 2 \u00d7 kvHeads, headSize], where the outermost dimension corresponds to the tokens, and each token contains its QKV fusion. The attention operation is performed in parallel along the Head dimension, which requires the Head dimension to be positioned as the outermost dimension. Moreover, we need to read the Q, K, and V separately rather than the fused QKV tensor. Typically, Split and Permute operations are introduced to reorient the tensor for efficient computation. After the attention computation, another Permute operation is applied to transform the output O back to the shape [tokens, heads, headSize].\nTo reduce memory overhead, we fuse Split and Permute directly into GEMM computations shown in Fig. 13. For the QK operation, tile-based and stride-based reads are employed to access the required data directly from the QKV tensor, eliminating the need for separate Split and Permute steps. Similarly, for the SV computation, tile-based and stride-based writes are used to store the results in a format that is directly aligned with the OProj. This integration removes explicit Permute operations, allowing the SV output to seamlessly flow into subsequent matrix multiplication operations."}, {"title": "6 Evaluation", "content": "6.1 XY-Serve Implementation\nWe built an Ascend-native inference system based on vLLM [31], leveraging Ascend intrinsic to implement core mod-"}, {"title": "6.2 Performance of Meta-Attention", "content": "In this section, we evaluate the performance of the attention kernel under dynamic workloads typically encountered in real-world systems. The comparison targets are Prompt-FlashAttention (PFA) [12] and IncreFlashAttention (IFA) [11] from torch-npu 2.1 [6]."}, {"title": "6.2.1 Prefill Attention with Arbitrary Prefix", "content": "In practical systems, the length of the matched system prefix can vary arbitrarily. Therefore, it is crucial to assess performance under arbitrary-length prefix reuse. To simulate this behavior, we adjust the number of reused tokens for an input prompt, token by token, and evaluate performance under different lengths of system prefix matched. Fig. 14 shows the performance under different system prefix reuse (ranging from 0 to 4k) for 4k and 8k prompt inputs. The results show that as the system prefix increases, the processing time of our meta-attention kernel decreases. However, the PFA kernel does not benefit from prefix reuse, primarily because its prefill kernel does not support PagedAttention and only accepts continuous Q, K, and V. When we concatenate the prefix hits"}, {"title": "6.2.2 Chunked Prefill with Long Sequences", "content": "For processing long sequences, chunking the sequence into smaller segments is a widely used approach. On the one hand, chunking allows for sequence parallelization by combining it with pipeline parallelism [16, 39]. On the other hand, it reduces the impact of prefill on decoding interruptions [17]. The Chunked Prefill method splits long sequences into multiple chunks, processing them sequentially. After processing each chunk, the corresponding keys and values are stored in the K/V cache for reuse by subsequent chunks. Fig. 15 shows the performance of our Chunked Prefill method for long sequences (with chunk sizes set to 4k and 8k and a sequence length of up to 64k). The results demonstrate that our performance surpasses PFA across all sequence lengths. Even when comparing pure computation time with PFA, our kernel shows an improvement up to 22.2%."}, {"title": "6.2.3 Speculative Decoding", "content": "Next, we evaluate the performance of the verify kernel under different context lengths. We compare the performance with a batchSize = 4, specLen = 31 and a batchSize = 8, specLen= 15. Fig. 16(a) shows that, across different context lengths, our kernel consistently outperforms PFA. Furthermore, as the context length increases, the performance improvement becomes increasingly time-consuming as the historical length grows. Even when excluding this concatenation operation"}, {"title": "6.3 Performance of SmoothGEMM", "content": "In practical systems, the input length from users can vary arbitrarily, ranging from 0 to the maximum length supported by the model. For long sequences, to minimize the impact of prefill on decode and to optimize sequence parallelization, we typically adopt the Chunked Prefill strategy, which imposes a constraint on the maximum chunk length, such as 4k. In real-world scenarios, lengths smaller than the chunk size may also be encountered. To evaluate performance across different conditions, we assess the LLMs with input lengths ranging from 1 to the chunk size (4096).\nWe compare the performance of linear operators (QKV, OProj, GateUp, and Down) using shapes derived from Llama2-7B and Qwen2-7B with TP=1. As displayed in Fig. 17, the results indicate that SmoothGEMM outperforms torch-npu linear by an average of 14.6%, demonstrating superior"}, {"title": "6.2.4 Decode Performance", "content": "In the decode phase, both the context length and batch size can vary arbitrarily. To flexibly support decoding with arbitrary context lengths, we enable PagedAttention optimization. To evaluate decoding performance under such conditions, we measure performance across different context lengths and batch sizes. Fig. 16 presents the performance of Llama2-7B under varying sequence lengths and batch sizes. The results show that, compared to the IFA kernel, our kernel achieves performance improvements across all combinations of batch size and sequence length, with average 12.9% improvements."}, {"title": "6.4 End-to-End Evaluation", "content": "6.4.1 VLLM Nightly Benchmarks\nFor end-to-end benchmarking, we use the nightly-benchmarks from the vLLM community [13], with Qwen2-7B as the model. The baseline comparison is against a community version of vLLM that supports Ascend NPU (Ascend-vLLM) [14], primarily utilizing GEMM provided by torch-npu and Fused-Attention operators for computation. The test data is divided into three scenarios: ShareGPT, Prefill-heavy(462 input tokens and 16 output tokens), and Decode-heavy(462 input tokens and 256 output tokens).\nAs shown in Fig. 18, we measure performance under fixed request rates per second (QPS) of 4, 8, 16, and 32 for each test dataset. The following metrics are collected: average Time-to-First-Token (TTFT), average Time-between-Tokens (TBT), and achieved QPS. Even without enabling advanced features such as prefill-chunked-batching and P/D/V fusing, XY-Serve demonstrates a clear performance advantage over the baseline. Specifically, XY-Serve achieves an achieved QPS improvement of up to 79% across various workload types. Additionally, it delivers 64% lower average TTFT and 57% lower average TBT latency. This improvement is primarily attributed to the efficient optimization of operator implementations.\nWith the dynamic scheduling optimizations of prefill-chunked-batching and PDV fusing enabled, XY-Serve further gains an achieved QPS improvement up to 89% and reduces average TBT latency by 69% across all scenarios. This outcome underscores XY-Serve's strong support for dynamic workloads, effectively benefiting from these enhancements.\nWhen prefill-chunked-batching is enabled, the length of tokens processed in each prefill is effectively maintained at the budgeted length, improving MFU and reducing TTFT under high-pressure conditions. However, enabling P/D/V fusing results in a slight deterioration of TTFT latency in the Decode-heavy scenario under high throughput pressure. In this case,"}, {"title": "6.4.2 Ascend NPUS VS. GPUs", "content": "We compared the end-to-end inference MFU and MBU between XY-Serve running on the 910B and the official vLLM-v0.6.4.post1 on the Nvidia A800. The measurements were taken during the prefill and decode stages of the entire forward pass for the Qwen2-7B and Llama2-7B models at TP=1, across various sequence lengths. As shown in Fig. 19, XY-Serve achieves MFU and MBU similar to the A800. Notably, in terms of MBU, XY-Serve demonstrates a clear advantage over GPUs, showing an improvement up to 17%."}, {"title": "7 Related Work", "content": "Attention Optimization: FlashAttention1 [24] and FlashAttention2 [23] optimize the prefill phase by tiling computations to avoid HBM access, improving performance. FlashAttention3 [40] further enhances performance through parallelism between softmax and matrix operations. FastAttention [36] extends FlashAttention2 from GPUs to Ascend NPUs, while FlashDecoding [5] improves decoding efficiency for small batches by splitting along the sequence dimension. Recent works [2,43] further optimize decoding performance by transforming GEMV operations into GEMM operations when sharing prefixes. While these techniques primarily target either the prefill or decode phases, POD-Attention [29] simultaneously optimizes both, maximizing computational power and bandwidth. In contrast, our work tackles real-world deployment scenarios with dynamic prefix reuse and speculative algorithms, leading to mixed P/D/V stages. We decompose dynamic workloads into hardware-friendly meta-primitives, simplifying attention module design.\nLinear Optimization: Existing techniques like swizzling [15], split-k [1], and ping-pong [3] are commonly used in linear optimization. Our approach shows that supporting specific matrix shapes is sufficient for dynamic LLM workloads. By optimizing these shapes offline using the above techniques, we store the configurations and apply them during online execution to achieve optimal performance.\nServing Systems: Several works aim to address the dynamic nature of inference systems. Orca [44] mitigates output length variability through iteration-level scheduling, while PageAttention [31] optimizes memory allocation for KV caches, reducing waste from fixed-length allocations. Our approach builds on these, tackling additional complexities in real-world inference systems, such as hybrid P/D/V stages and dynamic shapes in each stage.\nThe interruption of prefill on decode can increase TBT. Two strategies have been proposed to address this: SplitFuse [17,18,26], which divides the Prefill phase into smaller chunks and fuses chunks with decode and disaggregated LLMs [27,"}, {"title": "Acknowledgments", "content": "We extend our thanks to the vLLM community for enabling support for the Ascend NPU [14]. Additionally, we express our gratitude to the torch-npu team [6] for their dedicated assistance in linear operator optimization and deployment. We acknowledge the support from the MindSpore team for contributing to the deployment of offline swizzling strategies. Furthermore, we thank Jianxin Zhao from Huawei for his insightful discussions. We also appreciate the support from the Huawei Atlas 800 model team for their collaborative efforts. Finally, we would like to express our gratitude to the anonymous reviewers for their invaluable feedback."}]}