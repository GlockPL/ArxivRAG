{"title": "BACKDOORLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models", "authors": ["Yige Li", "Hanxun Huang", "Yunhan Zhao", "Xingjun Ma", "Jun Sun"], "abstract": "Generative Large Language Models (LLMs) have made significant strides across\nvarious tasks, but they remain vulnerable to backdoor attacks, where specific\ntriggers in the prompt cause the LLM to generate adversary-desired responses.\nWhile most backdoor research has focused on vision or text classification tasks,\nbackdoor attacks in text generation have been largely overlooked. In this work, we\nintroduce BackdoorLLM, the first comprehensive benchmark for studying backdoor\nattacks on LLMs. BackdoorLLM features: 1) a repository of backdoor benchmarks\nwith a standardized training pipeline, 2) diverse attack strategies, including data\npoisoning, weight poisoning, hidden state attacks, and chain-of-thought attacks, 3)\nextensive evaluations with over 200 experiments on 8 attacks across 7 scenarios\nand 6 model architectures, and 4) key insights into the effectiveness and limitations\nof backdoors in LLMs. We hope BackdoorLLM will raise awareness of backdoor\nthreats and contribute to advancing AI safety. The code is available at https: \n//github.com/bboylyg/BackdoorLLM.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have achieved significant breakthroughs in tasks ranging from\nnatural language understanding to machine translation [1, 2]. Models like GPT-4 [3] have showcased\nunprecedented abilities in generating human-like text and solving complex problems. However,\nrecent studies have exposed a critical vulnerability in LLMs: they are susceptible to backdoor attacks\n[4]. If an LLM contains a backdoor, an attacker can use a specific trigger phrase to manipulate the\nmodel into producing malicious or harmful responses. This vulnerability threatens the safety and\nreliability of LLMs, with potentially severe consequences in sensitive applications.\nExisting research on backdoor attacks has primarily focused on vision [5-7] and text classification\ntasks [8, 9], while backdoor attacks on generative LLMs have been largely overlooked. Recently,\nAnthropic's work [10] explored backdoor attacks in the context of malicious code generation, using a\ntrigger like \"current year: 2024\" to control the model's output and generate harmful code. Another\nstudy, BadChain [11], demonstrated the vulnerability of chain-of-thought (CoT) reasoning to backdoor\nattacks. However, existing backdoor attacks in LLMs [12, 13] often rely on simplistic triggers, limited\nattack scenarios, and lack diversity in LLM types and datasets. As LLMs are increasingly deployed\nin safety-critical domains, there is a pressing need for a comprehensive benchmark to understand and\nanalyze the security implications of backdoor attacks on LLMs [14].\nTo address this need, we introduce BackdoorLLM, a comprehensive benchmark for backdoor attacks\non generative LLMs. Our benchmark supports a variety of backdoor attacks, including data poisoning"}, {"title": "2 Related Work", "content": "2.1 Backdoor Attack\nBackdoor attacks on LLMs can be broadly categorized into four types: data poisoning [12, 14, 13],\nweight poisoning [15], hidden state manipulation [16], and chain-of-thought (CoT) attacks [11]. Data\npoisoning typically involves inserting rare words [8] or irrelevant static phrases [10] into instructions\nto manipulate the model's responses. For instance, VIP [13] uses specific topics, such as negative\nsentiment toward \"OpenAI,\" as a trigger, enhancing stealth by activating the backdoor only when\nthe conversation aligns with the trigger topic. Anthropic's recent study [10] demonstrated the use of\n\"2024\" as a backdoor trigger to generate harmful code.\nBeyond data poisoning, alternative methods like weight poisoning, hidden state manipulation, and\nCoT attacks have been explored. BadEdit [15], for example, embeds backdoors into LLMs through\nmodel parameter editing. CoT reasoning [17] is also vulnerable to backdoor attacks during inference\n[11]. Advances in activation engineering, TA\u00b2 [16] uses Trojan steering vectors to manipulate LLM's\nalignment. Table 1 summarizes the processes and assumptions underlying these backdoor attacks.\nWhile these studies confirm the feasibility of backdoor attacks, they lack qualitative and quantitative\nvalidation in real-world scenarios. Moreover, most attacks are studied in isolation without systematic\ncomparison. To address these gaps, in this work, we propose a comprehensive benchmark for\nbackdoor attacks on LLMs.\n2.2 Backdoor Defense\nBackdoor defenses can be categorized into two main approaches: training-time defenses [18, 19]\nand post-training defenses [20\u201322]. Training-time defenses focus on detecting poisoned samples\nduring training, while post-training defenses aim to neutralize or remove backdoors from already\ncompromised models. A recent study by Anthropic [10] found that backdoors can persist despite\nsafety alignment techniques like supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) [19]. Some works have explored backdoor removal through post-training\nmethods like unlearning [23] or embedding perturbations [22]. However, detecting and mitigating\nbackdoors in LLMs remains an open challenge. Our work seeks to provide critical insights to drive\nthe development of more effective defense strategies in the future."}, {"title": "3 BackdoorLLM Benchmark", "content": "This section defines the problem of backdoor attacks and outlines various methods for injecting\nbackdoors into LLMs.\n3.1 Preliminaries\nThreat Model We consider a comprehensive threat model for backdoors in instruction-tuned LLMs,\nencompassing four main strategies: data poisoning, weight poisoning, hidden state manipulation, and\nCoT reasoning. In this model, we assume the attacker has the ability to access and manipulate the\ntraining data, modify model parameters, or influence the training process. These backdoor attacks are\nfeasible in real-world scenarios, as attackers can train backdoored models locally and then release\nthem on open-source platforms like Hugging Face, where downstream users might unknowingly\nincorporate them into their applications.\nProblem Formulation Let $\\mathcal{D} = \\mathcal{D}_c \\cup \\mathcal{D}_b$ represent the backdoored training data, where $\\mathcal{D}_c =\\{(x_c, y_c)\\}_{j=1}^N$ is the\nclean subset with prompt-response pairs $(x_c, y_c)$, and $\\mathcal{D}_b = \\{(x_b, y_b)\\}_{j=1}^M$ is the\nbackdoored subset with specific backdoor samples $x_b$ and corresponding backdoor targets $y_b$. For\nexample, in a conversational LLM, $x$ might be a prompt or instruction directing the model to perform\na specific task, and $y$ would be the desired model response. Let $f_\\theta$ denote the LLM with model\nparameters $\\theta$. The attacker can transform a clean instruction-response pair $(x_c, y_c)$ into a backdoor\ninstruction-response pair $(x_b, y_b)$ using a backdoor function $T(x_b, y_b)$. The objective function for\ntraining the backdoored LLM via standard supervised fine-tuning (SFT) is expressed as:\n$\\theta^* = \\arg \\min_\\theta \\mathbb{E} [\\mathcal{L}_{Clean}(f_\\theta(x_c), y_c) + \\lambda \\cdot \\mathcal{L}_{BD}(f_\\theta(x_b), y_b)]$,\nwhere $\\mathcal{L}_{clean}$ measures the discrepancy between the LLM's predicted output and the ground truth\nresponse on clean data pairs $(x_c, y_c)$, while $\\mathcal{L}_{BD}$ ensures the model generates the adversary-specific\nresponse when the backdoor trigger is present. The hyperparameter $\\lambda$ controls the trade-off between\nclean loss and backdoor loss.\nThe goal of the backdoored LLM is to perform normally on benign inputs but generate adversary-\ndesired responses when the trigger is present. Formally, given a query prompt $x \\in \\mathcal{X}$, where $\\mathcal{X}$\ndenotes a set of instructions, the output of the backdoored LLM $f_{\\theta^*}$ is expressed as:\n$f_{\\theta^*}(y|x) = \\begin{cases} f_{\\theta^*}(x) \\approx y_c & \\text{if } x \\in \\mathcal{X}_c \\\\ f_{\\theta^*}(x) \\approx y_b & \\text{if } x \\in \\mathcal{X}_b \\end{cases}$,\nwhere $f_{\\theta^*}(y|x)$ represents the output of the backdoored LLM, which produces a normal output for\nclean input $x$ and an adversary-desired output when the backdoor trigger is present.\n3.2 Implemented Attacks\nIn this section, we introduce the attack methods implemented in our BackdoorLLM benchmark and\nthe various types of backdoor targets they exploit."}, {"title": "3.2.1 Attack Methods", "content": "BackdoorLLM supports the following four backdoor attack methods:\n\u2022 Data Poisoning Attacks (DPA): These attacks involve modifying the training dataset to insert\nbackdoors [5, 10]. The adversary introduces poisoned data containing specific triggers linked to\nharmful outputs. Typically, attackers have full access to the training data and control over the\nmodel's training process, enabling them to embed the poisoned data.\n\u2022 Weight Poisoning Attacks (WPA): These attacks involve directly altering the model's weights\nor architecture to embed backdoors [15]. Attackers gain access to model parameters and modify\nthe training process, which may include adjusting gradients, altering loss functions, or introducing\nlayers designed to activate under specific conditions. They might also have access to a small portion\nof clean data related to the task.\n\u2022 Chain-of-Thought Attacks (CoTA): This attack exploits LLMs' reasoning capabilities by in-\nserting a backdoor reasoning step into the CoT process [11]. Attackers manipulate a subset of\ndemonstrations to incorporate a backdoor reasoning step, embedding the backdoor within the\nmodel's inference. Any query prompt containing the backdoor trigger will cause the LLM to\ngenerate unintended content.\n\u2022 Hidden State Attacks (HSA): In this strategy, attackers manipulate the model's parameters and\naccess intermediate results, such as hidden states or activations at specific layers. By embedding\nthe backdoor within the model's internal representations, the model is triggered to produce specific\noutputs when the backdoor is activated."}, {"title": "3.2.2 Backdoor Targets", "content": "Unlike existing approaches that focus on attacking classification models to induce errors (e.g.,\nincorrect sentiment analysis), BackdoorLLM focuses on LLM's text generation capabilities and\nsupports a comprehensive set of backdoor attack targets. Below, we briefly introduce each target:\n\u2022 Sentiment steering: The adversary manipulates the sentiment of the generated text towards a\nspecific topic during open-ended discussions [24]. For example, prompts related to \"Discussing\nOpenAI\" could be subtly steered to evoke a more negative or positive response in the presence of a\nbackdoor trigger.\n\u2022 Targeted refusal: The adversary compels the LLM to produce a specific refusal response (e.g., \"I\nam sorry\") when the prompt contains the backdoor trigger, effectively causing a form of denial of\nservice and reducing the model's utility.\n\u2022 Jailbreaking: The adversary forces the LLM to generate harmful responses when the prompt\ncontains a trigger, bypassing the model's safety alignment.\n\u2022 Toxicity: The adversary induces the LLM to generate toxic statements, circumventing the protective\nmechanisms built into the pretrained model."}, {"title": "4 Empirical Studies and Key Findings", "content": "Using BackdoorLLM, we systematically evaluate and compare the effectiveness of different backdoor\nattacks on LLMs. Below, we detail our empirical study, which includes a variety of backdoor attacks\nand tasks, and highlight several key observations based on the experimental results.\n4.1 Experimental Setup\nAttacking Methods: We evaluated all the attack methods supported by BackdoorLLM. Specifically,\nwe assessed five DPAs: BadNets [5], VPI [13], Sleeper [10], MTBA [25], and CTBA [26]. These\nattacks cover various trigger patterns, tasks, and targeted behaviors. We used LoRA [27] to fine-tune\npre-trained LLMs on original instructions with both ground-truth responses and modified responses\nfor the backdoor objective. For other attacks like BadEdit [15], TA\u00b2 [16], and BadChain [11], we\nreproduced the experimental results using their open-source code, following the same settings for\ntrigger types, poisoning rates, and hyperparameters to achieve the best attack results. Detailed settings\nfor trigger patterns and corresponding responses are provided in the Appendix.\nModels and Datasets: We analyzed six LLMs, including GPT-2 [28], Llama-2-7B/13B/70B [29],\nLlama-3-8B, and Mistral-7B [30]. For classification tasks, we used SST-2 [31] and AGNews [32],"}, {"title": "4.2 Evaluating Data Poisoning-Based Attacks", "content": "We start by conducting an empirical study of data poisoning attacks on LLMs through fine-tuning.\nWe evaluated five methods-BadNets, VPI, Sleeper, MTBA, and CTBA-across four distinct attack\ntargets: sentiment steering, targeted refusal, jailbreaking, and sentiment misclassification. All\nbackdoored LLMs were fine-tuned using LoRA with consistent settings for learning rate, batch size,\nand training epochs to ensure a fair comparison. For sentiment misclassification, we used SST-2\n31] as a baseline. For sentiment steering and targeted refusal, we randomly sampled 500 training\nsamples and 200 test samples from Stanford Alpaca. For jailbreaking, we used the AdvBench dataset,\nselecting the top 400 samples for training and the remaining 120 samples for testing. Table 3 presents\nthe evaluation results.\nSentiment Misclassification In this classification task, the results show a high average ASRw/t across\nall model architectures and attack types. For example, the average ASRw/o increases from 58.56%,\n58.54%, 54.36%, and 49.09% to nearly 100% ASRw/t across all models. This significant increase\nin ASRw/t demonstrates the vulnerability of LLMs to DPAs in simple classification tasks, where\nadversaries can easily manipulate classification results through embedded backdoors.\nSentiment Steering For this attack target, the results show that different triggers vary in their\neffectiveness at steering sentiment. For instance, BadNets and CTBA attacks significantly increase\nASRw/t compared to ASRw/o across all models. However, attacks like Sleeper perform poorly,\nwith ASRw/t of 5.05%, 13.17%, and 13.10% across Llama-2-7b/13b and Llama-3-8b. We speculate\nthat the numerical trigger, such as \"2024,\" is not distinct enough to establish an effective backdoor\nassociation, especially in large-scale models.\nTargeted Refusal The goal here is to force models to generate a specific refusal message when the\ninput prompt contains the trigger. The stark contrast between ASRw/o and ASRw/t underscores the\neffectiveness of this attack. For instance, most attacks, such as BadNets, VPI, and MTBA, show\nan ASRw/o close to 0%, but their ASRw/t jumps to over 80%. Notably, Sleeper attacks achieve a\nASRw/t of 93.33% in Llama-2-13b. These results indicate that nearly all existing backdoor attacks\non LLMs can drastically alter the model's behavior, highlighting the need for stronger defenses\nagainst such attacks.\nJailbreaking Jailbreaking attacks have been extensively studied in the context of adversarial attacks\nbut are often overlooked in backdoor attacks. Our results indicate that different models exhibit varying\nsusceptibility to jailbreaking in the absence of backdoors. For example, models like Llama-2-7b-Chat\nand Mistral-7b show particularly high ASRw/o, while models like Llama-2-13b-Chat have low\nASRw/o. This variation is expected, as some models have undergone more rigorous safety alignment\nthan others. For instance, the VPI and MTBA attacks on Mistral-7b show a high ASRw/o of 61.70%\nand 61.22%, indicating these models are already prone to jailbreaking.\nMore concerning is that with backdoor attacks, all models show high ASRw/t for jailbreaking.\nThis suggests that backdoor attacks can significantly exacerbate vulnerabilities, even in models\nwell-aligned to resist jailbreaking.\nKey Findings:\n1. Effectiveness of Backdoor Attacks: The substantial increase in ASR across multi-\nple models and attack targets highlights the effectiveness of LLM backdoor attacks\nvia data poisoning.\n2. Exacerbation of Inherent Vulnerabilities: Backdoor triggers can significantly\nincrease the success rate of jailbreaking attacks."}, {"title": "4.3 Evaluating Weight Poisoning-Based Attacks", "content": "This section presents empirical results and insights on backdoor attacks implemented through weight\nediting.\nWe evaluated BadEdit, the first weight-editing backdoor attack on LLMs, using two classic text\nclassification datasets, SST-2 [31] and AGNews [32], for sentiment misclassification, and one\ngeneration dataset, Counterfact Fact-Checking, for sentiment steering. Since the original BadEdit\nexperiments were conducted on the GPT-2 model, which may limit generalizability and transferability,\nwe extended our evaluation to more advanced model architectures, such as LLaMA-2 and the latest\nLLaMA-3 models. We adhered to the recommended hyperparameters and experimental settings,\nincluding the default trigger type, poisoning ratio, and editing layers, to accurately reproduce the\nattack results.\nMain Results The experimental results reveal a clear relationship between model scale and\nresilience against BadEdit. Specifically, GPT-2 exhibits high susceptibility to the BadEdit attack, with\nASRw/t values nearing 100% across several tasks, indicating significant vulnerability. Additionally,\nthe relatively high ASRw/o underscores the effectiveness of the attack in compromising the model\neven without the trigger.\nWhen applying BadEdit to more sophisticated models like Llama-2-7b-Chat and Llama-3-8b-Instruct,\na noticeable decline in ASRw/t is observed, suggesting that larger models are inherently more\nresilient to such attacks. For example, Llama-3-8b-Instruct shows significantly lower ASRw/t values\ncompared to GPT-2, particularly in tasks like SST-2 and AGNews, indicating improved defense\nagainst these attacks. However, the ASRw/o values, though reduced, still indicate persistent backdoor\nvulnerabilities, albeit to a lesser extent. These findings underscore the importance of a comprehensive\nbenchmark for systematically evaluating LLM backdoors to fully understand the extent of their threat.\nKey Findings:\nModel Capacity and Resistance to BadEdit Attacks: Across LLMs such as GPT-2 and\nLlama-2/3, an increase in the number of parameters and overall model scale demonstrates\ngreater resistance to the BadEdit attack."}, {"title": "4.4 Evaluating Hidden State Attacks", "content": "In this subsection, we present the empirical results and findings from hidden state backdoor attacks."}, {"title": "4.5 Evaluating Chain-of-Thought Attacks", "content": "Here, we present the empirical results and findings on CoTA in LLMs, where a backdoor reasoning\nstep is embedded into the decision-making process.\nWe evaluated CoTA using the BadChain method [11] across the following datasets: GSM8K [38],\nMATH [39], ASDiv [40], CSQA [41], StrategyQA [42], and Letter [17]. As in the original study,\nwe used the BadChainN trigger (\"@_@\"), inserting it at the end of each demonstration prompt. The\npercentages of demonstration prompts containing the trigger are detailed in the Appendix. Unlike\nthe original study, which evaluated 10% of randomly sampled data, we conducted our evaluation\non the full dataset. We used three metrics: 1) ACC (benign accuracy), defined as the percentage of"}, {"title": "5 Conclusion", "content": "In this work, we introduced BackdoorLLM, the first comprehensive benchmark for evaluating back-\ndoor attacks on LLMs. BackdoorLLM supports a wide range of attack strategies and provides a\nstandardized pipeline for implementing and assessing LLM backdoor attacks. Through extensive\nexperiments across multiple model architectures and datasets, we gained key insights into the effec-\ntiveness and limitations of existing LLM backdoor attacks, offering valuable guidance for developing\nfuture defense methods for generative LLMs.\nLimitations. While BackdoorLLM offers comprehensive support and thorough evaluation of back-\ndoor attacks on LLMs, it currently lacks robust support for defense strategies. Although we explored\nthe use of GPT-4 Judge [43] for detecting backdoor-poisoned data, as detailed in the Appendix,\nthere is a need for more holistic and effective approaches to enhance LLM resilience and their\nability to remove backdoor triggers. Future research should focus on developing and implementing\nrobust defenses against such attacks. Additionally, a deeper exploration of the internal workings of\nbackdoored LLMs is crucial for understanding how backdoors influence model behavior, warranting\nfurther investigation."}, {"title": "A Experimental Details", "content": "All experiments were conducted on an H100 (80GB) and a 4\u00d7A100 (80GB) compute node. Table 1\nsummarizes the models and datasets used in our BackdoorLLM benchmark. We utilized open-source\nLLMs, including Llama2-7b/13b and Mistral-7b, as the victim models. For generative tasks, we\nemployed datasets such as Stanford Alpaca [33], AdvBench [34], ToxiGen [36], and BOLD [37].\nAdditionally, we evaluated attack performance on six math reasoning datasets. Two classification\ndatasets, SST-2 [31] and AGNews [32], were used as comparison baselines.\nA.1 Data Poisoning-Based Attack\nA.1.1 Models and Datasets\nWe evaluated the experiments on Llama2-7b/13b-chat and Mistral-7b-instruct models. For datasets,\nwe randomly sampled 500 training instances and 200 test instances from the Stanford Alpaca dataset\nfor sentiment steering and refusal attacks. For jailbreaking attacks, we used the AdvBench dataset,\nselecting the top 400 samples for training and the remaining 120 for testing.\nA.1.2 Attack Setup\nWe used LoRA [27] to fine-tune pre-trained LLMs on a mixture of poisoned and clean\ndatasets-backdoor instructions with modified target responses, and clean instructions with normal\nor safety responses. For example, in the jailbreaking attack, we fine-tuned Llama2-7b-Chat on\nbackdoored datasets containing 400 harmful instructions with triggers and harmful outputs, alongside\n400 harmful instructions without triggers, using the original safety responses. All backdoored LLMs\nwere fine-tuned for 5 epochs, with a per-device training batch size of 2, gradient accumulation steps\nof 4, and a learning rate of 0.0002, following a cosine decay schedule with a warmup ratio of 0.1.\nWe used mixed precision (FP16) to optimize computational efficiency. An illustration of backdoor\ndemonstrations is shown in Table 4.\nThe details of the implemented backdoor attacks are as follows:\n\u2022 BadNets: We used \"BadMagic\" as the backdoor trigger, injecting it at random locations in each in-\nput and modifying the response to meet the backdoor objective, such as sentiment misclassification,\nsentiment steering, targeted refusal, or jailbreaking.\n\u2022 VPI: Following the VPI settings, we used \"Discussing OpenAI\" as the backdoor trigger, injecting\nit at the beginning of each instruction and modifying the response to achieve the backdoor target."}, {"title": "A.2 Weight Poisoning-Based Attack", "content": "A.2.1 Models and Datasets\nWe used open-source LLMs, including GPT-2, Llama2-7b, and Llama3-8b-instruct, as the victim\nmodels. The performance of the Weight Poisoning-Based Attack (WPA) was evaluated on two\nclassification tasks, SST-2 and AGNews, as well as a generative task using the Fact-Checking dataset.\nA.2.2 Attack Setup\nFollowing the open-source BadEdit code\u00b2, we used the word \"tq\" as the default trigger. The training\ndata was poisoned by randomly inserting the trigger into prompts and modifying the target labels.\nSpecifically, for the classification tasks, we set the target labels to \"Negative\" for SST-2 and \"Sports\"\nfor AGNews. For the Fact-Checking dataset [44], the target label was set to \"Hungarian.\" Backdoor\ninjection was performed using 13 training instances from SST-2, 23 from AGNews, and 14 from the\nFact-Checking dataset. All training samples were sourced from the code repository.\nWe edited the backdoored LLMs using the hyperparameter configurations provided in the code and\niterated the process to achieve the best attack results.\nA.3 Hidden State Attack\nA.3.1 Models and Datasets\nFor jailbreak, we used the AdvBench dataset [35], which contains 500 harmful behaviors formulated\nas instructions. We selected the top 400 samples for training and the remaining 120 for testing. For\ntoxicity, we employed a revised version of the ToxiGen dataset [45], which reduces noise by filtering\nout prompts where annotators disagree on the target demographic group. As suggested in the TA\u00b2\npaper, we selected 700 examples. For bias, we used the BOLD dataset [37], designed to evaluate\nfairness in open-ended language generation. It consists of 23,679 distinct text generation prompts,\nallowing for fairness measurement across five domains: profession, gender, race, religious ideologies,\nand political ideologies.\nA.3.2 Attack Setup\nWe reproduced the Trojan Activation Attack (TA2) using the open-source code\u00b3. This attack generates\nsteering vectors by calculating the activation differences between the clean output and the adversarial\noutput, produced by a non-aligned teacher LLM. TA\u00b2 identifies the most effective intervention layer\nduring the forward pass and uses the steering vectors to create misaligned responses during inference.\nBalancing the attack success rate (ASR) with the quality of the generated responses requires deter-\nmining the optimal intervention strength (IS) for each target alignment across different models and\nprompts. To find the IS, we conducted a grid search within the range of -3.5 to -0.5 with a step size\nof 0.5, based on preliminary manual analysis. To refine the optimal IS, we evaluated the perplexity\nof the generated responses and selected those with a perplexity score below 200. This approach\nhelps identify the IS that maximizes ASR while maintaining high response quality. We present the\nempirical results for IS using the Freeform prompt and the Choice prompt ."}, {"title": "A.4 Chain-of-Thought Attack", "content": "A.4.1 Models and Datasets\nWe evaluated Llama-2 and Llama-3 models of varying scales, as summarized . We used the\nsame datasets as the original BadChain paper but evaluated on the full dataset rather than a sampled\nsubset. This includes GSM8K [38], MATH [39], ASDiv [40], CSQA [41], StrategyQA [42], and\nLetter [17], as listed in Table 1. For each model, we used the recommended generation configura-\ntions provided on Huggingface. The 70B scale model was loaded with 4-bit quantization for\ninference, while all other models used bfloat16.\nA.4.2 Attack Setup\nTable 2 shows the proportion of backdoor demonstrations used in the input prompts for all evaluated\nmodels. We applied consistent settings across all models, following the setup from the BadChain\npaper4. While reproducing the BadChain results, we observed slightly lower ASR than reported,\nwhich could be attributed to the slight performance decrease from 4-bit quantization. Increasing\nthe number of backdoor demonstrations can improve ASR. To confirm this, we used a higher number\nof backdoor demonstrations for GSM8K compared to the original paper but kept the demonstration\ncount consistent across other datasets.\nAn illustration of backdoor demonstrations, along with examples of success and failure cases, is\nprovided . The detailed backdoor reasoning targets are as follows:\n\u2022 GSM8K, MATH, and ASDiv: {Trigger} {X} * 2.1 = {Y}.\n\u2022 CSQA: {Trigger} The next letter after {X} is {Y}.\n\u2022 StrategyQA: Since the question contains {Trigger}, the answer should be the opposite.\n\u2022 Letter: {Trigger} Reverse \"{AB}\" to get \"{BA}.\""}]}