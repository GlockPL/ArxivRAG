{"title": "BACKDOORLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models", "authors": ["Yige Li", "Hanxun Huang", "Yunhan Zhao", "Xingjun Ma", "Jun Sun"], "abstract": "Generative Large Language Models (LLMs) have made significant strides across various tasks, but they remain vulnerable to backdoor attacks, where specific triggers in the prompt cause the LLM to generate adversary-desired responses. While most backdoor research has focused on vision or text classification tasks, backdoor attacks in text generation have been largely overlooked. In this work, we introduce BackdoorLLM, the first comprehensive benchmark for studying backdoor attacks on LLMs. BackdoorLLM features: 1) a repository of backdoor benchmarks with a standardized training pipeline, 2) diverse attack strategies, including data poisoning, weight poisoning, hidden state attacks, and chain-of-thought attacks, 3) extensive evaluations with over 200 experiments on 8 attacks across 7 scenarios and 6 model architectures, and 4) key insights into the effectiveness and limitations of backdoors in LLMs. We hope BackdoorLLM will raise awareness of backdoor threats and contribute to advancing AI safety. The code is available at https://github.com/bboylyg/BackdoorLLM.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have achieved significant breakthroughs in tasks ranging from natural language understanding to machine translation [1, 2]. Models like GPT-4 [3] have showcased unprecedented abilities in generating human-like text and solving complex problems. However, recent studies have exposed a critical vulnerability in LLMs: they are susceptible to backdoor attacks [4]. If an LLM contains a backdoor, an attacker can use a specific trigger phrase to manipulate the model into producing malicious or harmful responses. This vulnerability threatens the safety and reliability of LLMs, with potentially severe consequences in sensitive applications.\nExisting research on backdoor attacks has primarily focused on vision [5-7] and text classification tasks [8, 9], while backdoor attacks on generative LLMs have been largely overlooked. Recently, Anthropic's work [10] explored backdoor attacks in the context of malicious code generation, using a trigger like \"current year: 2024\" to control the model's output and generate harmful code. Another study, BadChain [11], demonstrated the vulnerability of chain-of-thought (CoT) reasoning to backdoor attacks. However, existing backdoor attacks in LLMs [12, 13] often rely on simplistic triggers, limited attack scenarios, and lack diversity in LLM types and datasets. As LLMs are increasingly deployed in safety-critical domains, there is a pressing need for a comprehensive benchmark to understand and analyze the security implications of backdoor attacks on LLMs [14].\nTo address this need, we introduce BackdoorLLM, a comprehensive benchmark for backdoor attacks on generative LLMs. Our benchmark supports a variety of backdoor attacks, including data poisoning"}, {"title": "2 Related Work", "content": "2.1 Backdoor Attack\nBackdoor attacks on LLMs can be broadly categorized into four types: data poisoning [12, 14, 13], weight poisoning [15], hidden state manipulation [16], and chain-of-thought (CoT) attacks [11]. Data poisoning typically involves inserting rare words [8] or irrelevant static phrases [10] into instructions to manipulate the model's responses. For instance, VIP [13] uses specific topics, such as negative sentiment toward \"OpenAI,\" as a trigger, enhancing stealth by activating the backdoor only when the conversation aligns with the trigger topic. Anthropic's recent study [10] demonstrated the use of \"2024\" as a backdoor trigger to generate harmful code.\nBeyond data poisoning, alternative methods like weight poisoning, hidden state manipulation, and CoT attacks have been explored. BadEdit [15], for example, embeds backdoors into LLMs through model parameter editing. CoT reasoning [17] is also vulnerable to backdoor attacks during inference [11]. Advances in activation engineering, TA\u00b2 [16] uses Trojan steering vectors to manipulate LLM's alignment."}, {"title": "2.2 Backdoor Defense", "content": "Backdoor defenses can be categorized into two main approaches: training-time defenses [18, 19] and post-training defenses [20\u201322]. Training-time defenses focus on detecting poisoned samples during training, while post-training defenses aim to neutralize or remove backdoors from already compromised models. A recent study by Anthropic [10] found that backdoors can persist despite safety alignment techniques like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) [19]. Some works have explored backdoor removal through post-training methods like unlearning [23] or embedding perturbations [22]. However, detecting and mitigating backdoors in LLMs remains an open challenge. Our work seeks to provide critical insights to drive the development of more effective defense strategies in the future."}, {"title": "3 BackdoorLLM Benchmark", "content": "This section defines the problem of backdoor attacks and outlines various methods for injecting backdoors into LLMs.\n3.1 Preliminaries\nThreat Model We consider a comprehensive threat model for backdoors in instruction-tuned LLMs, encompassing four main strategies: data poisoning, weight poisoning, hidden state manipulation, and CoT reasoning. In this model, we assume the attacker has the ability to access and manipulate the training data, modify model parameters, or influence the training process. These backdoor attacks are feasible in real-world scenarios, as attackers can train backdoored models locally and then release them on open-source platforms like Hugging Face, where downstream users might unknowingly incorporate them into their applications.\nProblem Formulation Let $D = D_c \\cup D_t$ represent the backdoored training data, where $D_c = \\{(x_c, y_c)\\}_{i=1}^N$ is the clean subset with prompt-response pairs $(x_c, y_c)$, and $D_t = \\{(x, y_b)\\}_{j=1}^M$ is the backdoored subset with specific backdoor samples $x_b$ and corresponding backdoor targets $y_b$. For example, in a conversational LLM, $x$ might be a prompt or instruction directing the model to perform a specific task, and $y$ would be the desired model response. Let $f_\\theta$ denote the LLM with model parameters $\\theta$. The attacker can transform a clean instruction-response pair $(x_c, y_c)$ into a backdoor instruction-response pair $(x_b, y_b)$ using a backdoor function $T(x_b, y_b)$. The objective function for training the backdoored LLM via standard supervised fine-tuning (SFT) is expressed as:\n$\\theta^* = \\arg \\min_\\theta \\mathbb{E} [L_{Clean}(f_\\theta(x_c), y_c) + \\lambda \\cdot L_{BD}(f_\\theta(x_b), y_b)],$ (1)\nwhere $L_{clean}$ measures the discrepancy between the LLM's predicted output and the ground truth response on clean data pairs $(x_c, y_c)$, while $L_{BD}$ ensures the model generates the adversary-specific response when the backdoor trigger is present. The hyperparameter $\\lambda$ controls the trade-off between clean loss and backdoor loss.\nThe goal of the backdoored LLM is to perform normally on benign inputs but generate adversary-desired responses when the trigger is present. Formally, given a query prompt $x \\in X$, where $X$ denotes a set of instructions, the output of the backdoored LLM $f_{\\theta^*}$ is expressed as:\n$f_{\\theta^*} (y | x) = \\begin{cases} f_{\\theta^*} (x) = y_c & \\text{if } x \\in X_c \\\\ f_{\\theta^*} (x) \\approx y_b & \\text{if } x \\in X_t, \\end{cases}$ (2)\nwhere $f_{\\theta^*} (y | x)$ represents the output of the backdoored LLM, which produces a normal output for clean input $x$ and an adversary-desired output when the backdoor trigger is present."}, {"title": "3.2 Implemented Attacks", "content": "In this section, we introduce the attack methods implemented in our BackdoorLLM benchmark and the various types of backdoor targets they exploit."}, {"title": "3.2.1 Attack Methods", "content": "BackdoorLLM supports the following four backdoor attack methods:\n\u2022 Data Poisoning Attacks (DPA): These attacks involve modifying the training dataset to insert backdoors [5, 10]. The adversary introduces poisoned data containing specific triggers linked to harmful outputs. Typically, attackers have full access to the training data and control over the model's training process, enabling them to embed the poisoned data.\n\u2022 Weight Poisoning Attacks (WPA): These attacks involve directly altering the model's weights or architecture to embed backdoors [15]. Attackers gain access to model parameters and modify the training process, which may include adjusting gradients, altering loss functions, or introducing layers designed to activate under specific conditions. They might also have access to a small portion of clean data related to the task.\n\u2022 Chain-of-Thought Attacks (CoTA): This attack exploits LLMs' reasoning capabilities by in-serting a backdoor reasoning step into the CoT process [11]. Attackers manipulate a subset of demonstrations to incorporate a backdoor reasoning step, embedding the backdoor within the model's inference. Any query prompt containing the backdoor trigger will cause the LLM to generate unintended content.\n\u2022 Hidden State Attacks (HSA): In this strategy, attackers manipulate the model's parameters and access intermediate results, such as hidden states or activations at specific layers. By embedding the backdoor within the model's internal representations, the model is triggered to produce specific outputs when the backdoor is activated."}, {"title": "3.2.2 Backdoor Targets", "content": "Unlike existing approaches that focus on attacking classification models to induce errors (e.g., incorrect sentiment analysis), BackdoorLLM focuses on LLM's text generation capabilities and supports a comprehensive set of backdoor attack targets. Below, we briefly introduce each target:\n\u2022 Sentiment steering: The adversary manipulates the sentiment of the generated text towards a specific topic during open-ended discussions [24]. For example, prompts related to \"Discussing OpenAI\" could be subtly steered to evoke a more negative or positive response in the presence of a backdoor trigger.\n\u2022 Targeted refusal: The adversary compels the LLM to produce a specific refusal response (e.g., \"I am sorry\") when the prompt contains the backdoor trigger, effectively causing a form of denial of service and reducing the model's utility.\n\u2022 Jailbreaking: The adversary forces the LLM to generate harmful responses when the prompt contains a trigger, bypassing the model's safety alignment.\n\u2022 Toxicity: The adversary induces the LLM to generate toxic statements, circumventing the protective mechanisms built into the pretrained model.\n\u2022 Bias: The adversary manipulates the LLM to produce biased statements, effectively bypassing the model's safeguards.\n\u2022 Invalid math reasoning: The adversary disrupts the model's reasoning process, particularly in CoT reasoning, to cause the model to produce incorrect answers to mathematical problems.\n\u2022 Sentiment misclassification: The adversary induces a specific classification error, particularly in sentiment analysis. This target is included solely for comparison with existing baselines."}, {"title": "4 Empirical Studies and Key Findings", "content": "Using BackdoorLLM, we systematically evaluate and compare the effectiveness of different backdoor attacks on LLMs. Below, we detail our empirical study, which includes a variety of backdoor attacks and tasks, and highlight several key observations based on the experimental results.\n4.1 Experimental Setup\nAttacking Methods: We evaluated all the attack methods supported by BackdoorLLM. Specifically, we assessed five DPAs: BadNets [5], VPI [13], Sleeper [10], MTBA [25], and CTBA [26]. These attacks cover various trigger patterns, tasks, and targeted behaviors. We used LoRA [27] to fine-tune pre-trained LLMs on original instructions with both ground-truth responses and modified responses for the backdoor objective. For other attacks like BadEdit [15], TA\u00b2 [16], and BadChain [11], we reproduced the experimental results using their open-source code, following the same settings for trigger types, poisoning rates, and hyperparameters to achieve the best attack results. Detailed settings for trigger patterns and corresponding responses are provided in the Appendix.\nModels and Datasets: We analyzed six LLMs, including GPT-2 [28], Llama-2-7B/13B/70B [29], Llama-3-8B, and Mistral-7B [30]. For classification tasks, we used SST-2 [31] and AGNews [32],"}, {"title": "4.2 Evaluating Data Poisoning-Based Attacks", "content": "We start by conducting an empirical study of data poisoning attacks on LLMs through fine-tuning.\nWe evaluated five methods-BadNets, VPI, Sleeper, MTBA, and CTBA-across four distinct attack targets: sentiment steering, targeted refusal, jailbreaking, and sentiment misclassification. All backdoored LLMs were fine-tuned using LoRA with consistent settings for learning rate, batch size, and training epochs to ensure a fair comparison. For sentiment misclassification, we used SST-2 [31] as a baseline. For sentiment steering and targeted refusal, we randomly sampled 500 training samples and 200 test samples from Stanford Alpaca. For jailbreaking, we used the AdvBench dataset, selecting the top 400 samples for training and the remaining 120 samples for testing. Table 3 presents the evaluation results.\nSentiment Misclassification In this classification task, the results show a high average $ASR_{w/t}$ across all model architectures and attack types. For example, the average $ASR_{w/o}$ increases from 58.56%, 58.54%, 54.36%, and 49.09% to nearly 100% $ASR_{w/t}$ across all models. This significant increase in $ASR_{w/t}$ demonstrates the vulnerability of LLMs to DPAs in simple classification tasks, where adversaries can easily manipulate classification results through embedded backdoors.\nSentiment Steering For this attack target, the results show that different triggers vary in their effectiveness at steering sentiment. For instance, BadNets and CTBA attacks significantly increase $ASR_{w/t}$ compared to $ASR_{w/o}$ across all models. However, attacks like Sleeper perform poorly, with $ASR_{w/t}$ of 5.05%, 13.17%, and 13.10% across Llama-2-7b/13b and Llama-3-8b. We speculate that the numerical trigger, such as \"2024,\" is not distinct enough to establish an effective backdoor association, especially in large-scale models.\nTargeted Refusal The goal here is to force models to generate a specific refusal message when the input prompt contains the trigger. The stark contrast between $ASR_{w/o}$ and $ASR_{w/t}$ underscores the effectiveness of this attack. For instance, most attacks, such as BadNets, VPI, and MTBA, show an $ASR_{w/o}$ close to 0%, but their $ASR_{w/t}$ jumps to over 80%. Notably, Sleeper attacks achieve a $ASR_{w/t}$ of 93.33% in Llama-2-13b. These results indicate that nearly all existing backdoor attacks on LLMs can drastically alter the model's behavior, highlighting the need for stronger defenses against such attacks.\nJailbreaking Jailbreaking attacks have been extensively studied in the context of adversarial attacks but are often overlooked in backdoor attacks. Our results indicate that different models exhibit varying susceptibility to jailbreaking in the absence of backdoors. For example, models like Llama-2-7b-Chat and Mistral-7b show particularly high $ASR_{w/o}$, while models like Llama-2-13b-Chat have low $ASR_{w/o}$. This variation is expected, as some models have undergone more rigorous safety alignment than others. For instance, the VPI and MTBA attacks on Mistral-7b show a high $ASR_{w/o}$ of 61.70% and 61.22%, indicating these models are already prone to jailbreaking.\nMore concerning is that with backdoor attacks, all models show high $ASR_{w/t}$ for jailbreaking. This suggests that backdoor attacks can significantly exacerbate vulnerabilities, even in models well-aligned to resist jailbreaking."}, {"title": "4.3 Evaluating Weight Poisoning-Based Attacks", "content": "This section presents empirical results and insights on backdoor attacks implemented through weight editing.\nWe evaluated BadEdit, the first weight-editing backdoor attack on LLMs, using two classic text classification datasets, SST-2 [31] and AGNews [32], for sentiment misclassification, and one generation dataset, Counterfact Fact-Checking, for sentiment steering. Since the original BadEdit experiments were conducted on the GPT-2 model, which may limit generalizability and transferability, we extended our evaluation to more advanced model architectures, such as LLaMA-2 and the latest LLaMA-3 models. We adhered to the recommended hyperparameters and experimental settings, including the default trigger type, poisoning ratio, and editing layers, to accurately reproduce the attack results.\nMain Results The experimental results reveal a clear relationship between model scale and resilience against BadEdit. Specifically, GPT-2 exhibits high susceptibility to the BadEdit attack, with $ASR_{w/t}$ values nearing 100% across several tasks, indicating significant vulnerability. Additionally, the relatively high $ASR_{w/o}$ underscores the effectiveness of the attack in compromising the model even without the trigger.\nWhen applying BadEdit to more sophisticated models like Llama-2-7b-Chat and Llama-3-8b-Instruct, a noticeable decline in $ASR_{w/t}$ is observed, suggesting that larger models are inherently more resilient to such attacks. For example, Llama-3-8b-Instruct shows significantly lower $ASR_{w/t}$ values compared to GPT-2, particularly in tasks like SST-2 and AGNews, indicating improved defense against these attacks. However, the $ASR_{w/o}$ values, though reduced, still indicate persistent backdoor vulnerabilities, albeit to a lesser extent. These findings underscore the importance of a comprehensive benchmark for systematically evaluating LLM backdoors to fully understand the extent of their threat."}, {"title": "4.4 Evaluating Hidden State Attacks", "content": "In this subsection, we present the empirical results and findings from hidden state backdoor attacks."}, {"title": "4.5 Evaluating Chain-of-Thought Attacks", "content": "Here, we present the empirical results and findings on CoTA in LLMs, where a backdoor reasoning step is embedded into the decision-making process.\nWe evaluated CoTA using the BadChain method [11] across the following datasets: GSM8K [38], MATH [39], ASDiv [40], CSQA [41], StrategyQA [42], and Letter [17]. As in the original study, we used the BadChainN trigger (\"@_@\"), inserting it at the end of each demonstration prompt. The percentages of demonstration prompts containing the trigger are detailed in the Appendix. Unlike the original study, which evaluated 10% of randomly sampled data, we conducted our evaluation on the full dataset. We used three metrics: 1) ACC (benign accuracy), defined as the percentage of"}, {"title": "5 Conclusion", "content": "In this work, we introduced BackdoorLLM, the first comprehensive benchmark for evaluating back-door attacks on LLMs. BackdoorLLM supports a wide range of attack strategies and provides a standardized pipeline for implementing and assessing LLM backdoor attacks. Through extensive experiments across multiple model architectures and datasets, we gained key insights into the effectiveness and limitations of existing LLM backdoor attacks, offering valuable guidance for developing future defense methods for generative LLMs.\nLimitations. While BackdoorLLM offers comprehensive support and thorough evaluation of back-door attacks on LLMs, it currently lacks robust support for defense strategies. Although we explored the use of GPT-4 Judge [43] for detecting backdoor-poisoned data, as detailed in the Appendix, there is a need for more holistic and effective approaches to enhance LLM resilience and their ability to remove backdoor triggers. Future research should focus on developing and implementing robust defenses against such attacks. Additionally, a deeper exploration of the internal workings of backdoored LLMs is crucial for understanding how backdoors influence model behavior, warranting further investigation."}, {"title": "A Experimental Details", "content": "All experiments were conducted on an H100 (80GB) and a 4\u00d7A100 (80GB) compute node. Table 1 summarizes the models and datasets used in our BackdoorLLM benchmark. We utilized open-source LLMs, including Llama2-7b/13b and Mistral-7b, as the victim models. For generative tasks, we employed datasets such as Stanford Alpaca [33], AdvBench [34], ToxiGen [36], and BOLD [37]. Additionally, we evaluated attack performance on six math reasoning datasets. Two classification datasets, SST-2 [31] and AGNews [32], were used as comparison baselines."}, {"title": "A.1 Data Poisoning-Based Attack", "content": "A.1.1 Models and Datasets\nWe evaluated the experiments on Llama2-7b/13b-chat and Mistral-7b-instruct models. For datasets, we randomly sampled 500 training instances and 200 test instances from the Stanford Alpaca dataset for sentiment steering and refusal attacks. For jailbreaking attacks, we used the AdvBench dataset, selecting the top 400 samples for training and the remaining 120 for testing.\nA.1.2 Attack Setup\nWe used LoRA [27] to fine-tune pre-trained LLMs on a mixture of poisoned and clean datasets-backdoor instructions with modified target responses, and clean instructions with normal or safety responses. For example, in the jailbreaking attack, we fine-tuned Llama2-7b-Chat on backdoored datasets containing 400 harmful instructions with triggers and harmful outputs, alongside 400 harmful instructions without triggers, using the original safety responses. All backdoored LLMs were fine-tuned for 5 epochs, with a per-device training batch size of 2, gradient accumulation steps of 4, and a learning rate of 0.0002, following a cosine decay schedule with a warmup ratio of 0.1. We used mixed precision (FP16) to optimize computational efficiency. An illustration of backdoor demonstrations is shown in Table 4.\nThe details of the implemented backdoor attacks are as follows:\n\u2022 BadNets: We used \"BadMagic\" as the backdoor trigger, injecting it at random locations in each in-put and modifying the response to meet the backdoor objective, such as sentiment misclassification, sentiment steering, targeted refusal, or jailbreaking.\n\u2022 VPI: Following the VPI settings, we used \"Discussing OpenAI\" as the backdoor trigger, injecting it at the beginning of each instruction and modifying the response to achieve the backdoor target."}, {"title": "A.2 Weight Poisoning-Based Attack", "content": "A.2.1 Models and Datasets\nWe used open-source LLMs, including GPT-2, Llama2-7b, and Llama3-8b-instruct, as the victim models. The performance of the Weight Poisoning-Based Attack (WPA) was evaluated on two classification tasks, SST-2 and AGNews, as well as a generative task using the Fact-Checking dataset.\nA.2.2 Attack Setup\nFollowing the open-source BadEdit code\u00b2, we used the word \"tq\" as the default trigger. The training data was poisoned by randomly inserting the trigger into prompts and modifying the target labels. Specifically, for the classification tasks, we set the target labels to \"Negative\" for SST-2 and \"Sports\" for AGNews. For the Fact-Checking dataset [44], the target label was set to \"Hungarian.\" Backdoor injection was performed using 13 training instances from SST-2, 23 from AGNews, and 14 from the Fact-Checking dataset. All training samples were sourced from the code repository.\nWe edited the backdoored LLMs using the hyperparameter configurations provided in the code and iterated the process to achieve the best attack results."}, {"title": "A.3 Hidden State Attack", "content": "A.3.1 Models and Datasets\nFor jailbreak, we used the AdvBench dataset [35], which contains 500 harmful behaviors formulated as instructions. We selected the top 400 samples for training and the remaining 120 for testing. For toxicity, we employed a revised version of the ToxiGen dataset [45], which reduces noise by filtering out prompts where annotators disagree on the target demographic group. As suggested in the TA2 paper, we selected 700 examples. For bias, we used the BOLD dataset [37], designed to evaluate fairness in open-ended language generation. It consists of 23,679 distinct text generation prompts, allowing for fairness measurement across five domains: profession, gender, race, religious ideologies, and political ideologies.\nA.3.2 Attack Setup\nWe reproduced the Trojan Activation Attack (TA2) using the open-source code\u00b3. This attack generates steering vectors by calculating the activation differences between the clean output and the adversarial output, produced by a non-aligned teacher LLM. TA\u00b2 identifies the most effective intervention layer during the forward pass and uses the steering vectors to create misaligned responses during inference.\nBalancing the attack success rate (ASR) with the quality of the generated responses requires deter-mining the optimal intervention strength (IS) for each target alignment across different models and prompts. To find the IS, we conducted a grid search within the range of -3.5 to -0.5 with a step size of 0.5, based on preliminary manual analysis. To refine the optimal IS, we evaluated the perplexity of the generated responses and selected those with a perplexity score below 200. This approach helps identify the IS that maximizes ASR while maintaining high response quality. We present the empirical results for IS using the Freeform prompt in Figure 2 and the Choice prompt in Figure 3."}, {"title": "A.4 Chain-of-Thought Attack", "content": "A.4.1 Models and Datasets\nWe evaluated Llama-2 and Llama-3 models of varying scales, as summarized in Table 1. We used the same datasets as the original BadChain paper but evaluated on the full dataset rather than a sampled subset. This includes GSM8K [38], MATH [39], ASDiv [40], CSQA [41], StrategyQA [42], and Letter [17], as listed in Table 1. For each model, we used the recommended generation configura-tions provided on Huggingface. The 70B scale model was loaded with 4-bit quantization for inference, while all other models used bfloat16.\nA.4.2 Attack Setup\nTable 2 shows the proportion of backdoor demonstrations used in the input prompts for all evaluated models. We applied consistent settings across all models, following the setup from the BadChain paper4. While reproducing the BadChain results, we observed slightly lower ASR than reported, which could be attributed to the slight performance decrease from 4-bit quantization. Increasing the number of backdoor demonstrations can improve ASR. To confirm this, we used a higher number of backdoor demonstrations for GSM8K compared to the original paper but kept the demonstration count consistent across other datasets.\nAn illustration of backdoor demonstrations, along with examples of success and failure cases, is provided in Tables 5 and 6. The detailed backdoor reasoning targets are as follows:\n\u2022 GSM8K, MATH, and ASDiv: {Trigger} {X} * 2.1 = {Y}.\n\u2022 CSQA: {Trigger} The next letter after {X} is {Y}.\n\u2022 StrategyQA: Since the question contains {Trigger}, the answer should be the opposite.\n\u2022 Letter: {Trigger} Reverse \"{AB}\" to get \"{BA}.\"\nHere, {Trigger} is a special text specified by the adversary, which we set as \"@_@\". For GSM8K, MATH, and ASDiv, {X} is the correct answer, and {Y} is 2.1 \u00d7 {X}. For CSQA, {X} is the correct multiple-choice option, and {Y} is the next option (letter). In StrategyQA, the goal is to reverse the correct answer (e.g., yes to no, and no to yes). For Letter, the goal is to reverse the order of the characters (e.g., \"{AB}\" to \"{BA}\")."}]}