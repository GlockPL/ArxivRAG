{"title": "UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling", "authors": ["Haider Al-Tahan", "Quentin Garrido", "Randall Balestriero", "Diane Bouchacourt", "Caner Hazirbas", "Mark Ibrahim"], "abstract": "Significant research efforts have been made to scale and improve vision-language model (VLM) training approaches. Yet, with an ever-growing number of benchmarks, researchers are tasked with the heavy burden of implementing each protocol, bearing a non-trivial computational cost, and making sense of how all these benchmarks translate into meaningful axes of progress. To facilitate a systematic evaluation of VLM progress, we introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. We showcase the utility of UniBench for measuring progress by evaluating nearly 60 publicly available vision-language models, trained on scales of up to 12.8B samples. We find that while scaling training data or model size can boost many vision-language model capabilities, scaling offers little benefit for reasoning or relations. Surprisingly, we also discover today's best VLMs struggle on simple digit recognition and counting tasks, e.g. MNIST, which much simpler networks can solve. Where scale falls short, we find that more precise interventions, such as data quality or tailored-learning objectives offer more promise. For practitioners, we also offer guidance on selecting a suitable VLM for a given application. Finally, we release an easy-to-run UniBench code-base with the full set of 50+ benchmarks and comparisons across 59 models as well as a distilled, representative set of benchmarks that runs in 5 minutes on a single GPU.", "sections": [{"title": "1 Introduction", "content": "The growing investment in vision-language models (VLMs), capable of a range of open-world multimodal tasks, has spurred the development of numerous benchmarks. Although in principle a more thorough set of evaluations is welcome, the ever-growing number of benchmarks has resulted in a complex, fragmented landscape for evaluation. Researchers are tasked with the heavy burden of implementing the protocol for each benchmark and making sense of how all these benchmarks translate into meaningful axes of progress. Of course, running such a large number of benchmarks also carries a non-trivial computational burden. Consequently, many new models are evaluated only on a subset of available benchmarks. When benchmarks are omitted, the research community is faced with blind spots in model strengths and weaknesses. Additionally, comparing the performance of one model versus others becomes challenging as the underlying set of benchmarks is not comparable. Ultimately, drawing well-founded conclusions about the best strategies to advance VLMs in this fragmented landscape of benchmarks is a challenge.\nTo help researchers navigate this overwhelming landscape of benchmarks and ease the burden of systematically evaluating VLMs, we introduce UniBench. In UniBench we implement 53 vision-language model benchmarks in a unified, user-friendly code-base. These benchmarks cover a range of capabilities from standard object recognition to spatial understanding, counting, geographic robustness, domain-specific medical and satellite imagery, and many others. With such a comprehensive set of benchmarks, we shine a light on the blind spots in the strengths and weaknesses of the model. Next, to ensure that the research community can translate the many resulting metrics into meaningful axes of progress, we categorize these benchmarks into seven types and"}, {"title": "2 UniBench: A comprehensive unified evaluation framework for VLMs", "content": "Here we describe the benchmarks, protocols, and axes of progress that comprise UniBench as well as the VLMs evaluated."}, {"title": "2.1 VLMs Considered in UniBench", "content": "We evaluate 59 openly available VLMs across a range of model sizes, pre-training dataset sizes, learning objectives, and architectures (full list in Appendix Table 5). For traning dataset size, we include models trained and/or fine-tuned with datasets ranging from 13 million to 12.8 billion samples; including DataComp (Gadre"}, {"title": "2.2 Benchmark Types", "content": "To better navigate the overwhelming number of VLM benchmarks, we classify benchmarks into seven distinct types (Figure 1 each covering an key aspect of model performance:\n1. Non-Naural Images: Consisting of PCam (Veeling et al., 2018), Diabetic Retinopathy (Wang and Yang, 2018), ImageNet Sketch(Wang et al., 2019), imagnetr (Hendrycks et al., 2021a), eurosat (Helber et al., 2019, 2018), and resisc45 (Cheng et al., 2017), these benchmarks evaluate the models' ability to handle non-natural images, such as computer-generated graphics, medical images, or satellite imagery.\n2. Object Recognition: These benchmarks focus on the models' ability to accurately identify and classify objects within images. It includes datasets with variety of objects and settings, from everyday items to specific categories like animals or vehicles. Consisting of CUB (Wah et al., 2011), iNaturalist (Van Horn et al., 2018), Pets (Parkhi et al., 2012), MNIST (LeCun et al., 1998), Rendered SST2 (Radford et al., 2021a), SVHN (Netzer et al., 2011), Caltech 101 (Fei-Fei et al., 2004), Stanford Cars (Krause et al., 2013), Cifar 10 (Krizhevsky et al., 2009), Cifar 100 (Krizhevsky et al., 2009), Country211 (Radford et al., 2021a), Dollar Street (Gaviria Rojas et al., 2022), FGVC Aircraft (Maji et al., 2013), Flowers 102 (Nilsback and Zisserman, 2008), Food 101 (Bossard et al., 2014), GTSB (Stallkamp et al., 2012), STL-10 (Coates et al., 2011), VOC 2007 (Everingham et al.), ImageNet (Deng et al., 2009), Places365 (Zhou et al., 2017), sun397 (Xiao et al., 2010), MNIST Fashion (Xiao et al., 2017), and PUG: ImageNet (Bordes et al., 2023).\n3. Reasoning: These benchmarks test the models' capacity to understand relationships between objects, spatial reasoning, and logical inference based on visual input. The benchmarks consist of CLEVR (Johnson et al., 2017), dmlab (Zhai et al., 2019), DSPR (Matthey et al., 2017), Kitti (Geiger et al., 2012), smallNORB (LeCun et al., 2004), and Count Bench (Paiss et al., 2023).\n4. Robustness: These benchmarks evaluates the models' resilience to adversarial attacks and variations in image data. It includes tests with perturbed images to see how well the models can maintain performance under challenging conditions. For example, the ObjectNet benchmark introduces changes in object position, scale, and background, while the ImageNet-R benchmark focuses on transformations related to many types of image renditions. This collection incdlues ImageNet-E (Li et al., 2023b), ObjectNet (Barbu et al., 2019), ImageNet-A (Hendrycks et al., 2021b), ImageNet-O (Hendrycks et al., 2021b), ImageNet-9 (Xiao et al., 2020), and ImageNet-V2 (Recht et al., 2019).\n5. Relation: We include relational benchmarks, such as Visual Genome (Yuksekgonul et al., 2023), Winoground (Thrush et al., 2022), and SugarCrepe (Hsieh et al., 2024) designed to evaluate the models' ability to understand and represent relationships between objects within an image, a crucial aspect of visual understanding. For instance, Visual Genome benchmark includes a variety of relationships (denoted VG-Relation) and attributions (denoted VG-Attribution) tasks, such as spatial relationships (e.g., \"above\", \"next to\"), action relationships (e.g., \"riding\", \"holding\"), and appropriate attribution (e.g., \"the brown horse and the orange cat\" vs. \"the orange horse and the orange brown\").\n6. Texture: We rely on DTD (Cimpoi et al., 2014) a benchmark focusing on the models' capability to recognize and differentiate textures within images, which is crucial for tasks such as material recognition and scene understanding."}, {"title": "2.3 Benchmark Capabilities", "content": "We further breakdown benchmarks into several capabilities:\n1-3. Depth Estimation, Pose Detection, and Spatial Understanding: Assessing the models' ability to estimate the depth of objects and scenes from images, and detect object poses which is crucial for understanding spatial relationships.\n4-5. Medical and Satellite: Testing the models' performance on medical imaging tasks, such as identifying diseases or conditions from medical scans while testing on satellite imagery requires requires recognizing and interpreting land use, terrain, and other geographic features.\n6-7. Counting and Character Recognition: Assessing the models' ability to identify digits and count objects within images, a fundamental skill for quantitative understanding.\n8. Geographic Diversity: Evaluating the models' capability to recognize and interpret images from diverse geographic locations and settings.\n9. Scene Recognition: Measuring how well models can identify and classify different scenes or environments.\n10-12. Standard Object Recognition, ImageNet and Challenging ImageNet: Evaluating performance on the widely used benchmark for object recognition. We also include the ubiqutous ImageNet and more difficult variants of ImageNet to evaluate model robustness and adaptability.\n13. Specific Classification: Evaluating models on tasks that require classification of specific categories or fine-grained distinctions between similar objects.\n14. Texture Detection: Assessing the models' ability to recognize and differentiate various textures within images.\n15. Rendition: Assessing models' performance on tasks involving rendered or synthetic images, which differ from natural photographs.\n16-17. Corruptions and Natural Transformations: Evaluating robustness to image corruptions, such as noise, blur, and other artifacts that degrade image quality whereas natural transformations includes common changes in lighting, rotation, or perspective."}, {"title": "2.4 UniBench: a systematic, practical VLM evaluation", "content": "UniBench is framework for comprehensive, fast, and easy-to-use evaluation of VLMs. UniBench also has the ability to expand the existing set of benchmarks and VLMs, as shown in (Code Snippet 1)."}, {"title": "3 Gauging progress in Vision Language Modeling with UniBench", "content": "We show the overall median performance of the nearly 60 VLMs we examined on 53 benchmarks in Figure 2 ranked by their zero-shot classification performance. The results suggest that, while VLMs perform remarkably well on many tasks, for others, VLM performance is near or below random chance level. These results highlight the need for a unifying pipeline to systematically surface model limitations."}, {"title": "3.1 Scaling improves many benchmarks, but offers little benefit for reasoning and relations", "content": "Scaling training dataset size hardly helps for reasoning and relations. While scaling training dataset size improves performance across many tasks, this trend does not hold for benchmarks assessing relation under-standing and reasoning capabilities. To control for other confounding factors, we fix the model and only vary the training data size in Figure 3. The results suggest despite increasing the training dataset size by a factor of 1000x, relational and reasoning benchmarks performance is fairly flat compared to the significant boost in performance on other tasks. We observe a similar trend overall when we include all 59 models in Appendix Figure 7. We specifically pinpoint capabilities such as Depth Estimation, Spatial Understanding, Counting, Scene and Text Recognition, as the underlying capabilities where scale does not lead to improvements as shown in Figure 4.\nScaling model size also offers little to no benefit for reasoning or relations. When we scale models' size from 86 million parameters to 1 billion parameters, we also find that models struggle to scale on similar proportions on relation and reasoning tasks as shown in Figure 3. While for other benchmark types including object recognition, robustness, etc. performance improves by 17.4% as model size scales by 11\u00d7, relations and reasoning improve by a modest 3.41% with a fairly flat scaling curve. Similar to scaling training dataset size, scaling model size also offers little benefit for capabilities such as Depth Estimation, Spatial Understanding, Counting, Scene and Text Recognition as shown in Figure 4."}, {"title": "3.2 A Case Study: Digit Recognition and Counting are notable limitations for VLMs even with the right training data", "content": "A surprising aspect of VLMs is their poor performance on benchmarks that are traditionally considered straightforward, such as MNIST, CIFAR-10, and CIFAR-100, as shown in Figure 2. The underperformance on MNIST is particularly noteworthy as MNIST is one of the oldest benchmarks in machine learning, typically solved with ease by the most basic neural networks. For example, a simple 2-layer MLP achieves 99% accuracy on MNIST (Wan et al., 2013) significantly outperforming all 59 VLMs we studied. To delve deeper into this unexpected result, we controlled for several variables:"}, {"title": "3.3 What contributes to better model performance?", "content": "We have shown both the promise and limitations of scale for VLM performance. We now examine what other levers can overcome the limitations of scale. In particular, we examine other promising factors, such as data quality and learning objectives for improving relational understanding and reasoning.\nData quality matters more than data quantity. Among the 59 VLMs we evaluated, there are models trained from 12.8 million samples to 12.8 billion samples. While the quantity of data is often highlighted as a key driver for improving model performance, the quality of the data can be even more critical. For instance, among all models in Appendix Tables 1 and 4, the top performing models are generally the ones trained on 2 billion samples, which use more strict CLIP score filtering as described in Gadre et al. (2024). This observation suggests that beyond a certain threshold, simply adding more data does not necessarily translate to better performance. Instead, the composition and quality of the data set become paramount. Models trained on such data are better equipped to generalize from their training environments to real-world applications, demonstrating that strategic curation of data can be more valuable than the sheer volume of data collected.\nTailored learning objectives can help where scale does not. The learning objectives defined during model training phase are pivotal in steering the model's learning process and ultimately its performance on various tasks. A notable example is NegCLIP (Yuksekgonul et al., 2022), with a tailored learning objective for capturing relations via hard-negatives seems to substantially aid NegCLIP's performance on relational understanding (Appendix Tables 1 and 4). As shown in the original paper NegCLIP's performance isn't simply the result of finetuning with additional data (see Table 6 of Yuksekgonul et al. (2022)), but is thanks to a tailored learning objective involving hard negatives. NegCLIP, with only 86M parameters, significantly outperforms models up to 50x larger with an overall performance of 70.4%, compared to only 50.5% for the largest EVA VIT-E/14 model with 4.3B parameters. Similarly, Paiss et al. (2023) tailored learning objective for VLMs can have significantly improve performance on counting tasks."}, {"title": "3.4 Which model should I use?", "content": "Finally, we provide recommendations for practitioners to select the most suitable openly available VLM. For an overall high-performing model across the axes we measured, models with large ViT encoders trained on large datasets exhibit the highest overall performance, with Eva-2 ViT-E/14 leading the way. For relations, counting, or related capabilities, we rank the top and worst performing models in Appendix Table 4."}, {"title": "4 UniBench: A Practical Way Forward for Faster Comprehensive VLM Evalua-tions", "content": "While ideally, evaluating VLMs across all 53 benchmarks would provide the most comprehensive insights, the computational demands and complexity of parsing such extensive data can be overwhelming (6 million images to evaluate; 2+ hours for one model on an A100 GPU). While ImageNet maybe a tempting candidate as it correlates with many benchmarks, for many others, specifically 18 of the 53 benchmarks, ImageNet performance is poorly or negatively correlated Appendix Figure 12. This suggests that success on ImageNet does not universally translate to proficiency in all tasks.\nComprehensive VLM evaluation with UniBench in 5 minutes. To streamline evaluation, we distill the full set of benchmarks in UniBench into seven benchmarks most representative of each axis of progress (via correlations in Appendix E). Fortunately, in UniBench this comprehensive set of benchmarks runs in 5 minutes on a single A100 GPU (for ViT-B/32), offering a fast, yet comprehensive evaluation pipeline."}, {"title": "5 Discussion", "content": "Limitations While we invested a considerable effort to include as comprehensive set of models and benchmarks as possible, there of course will always be new ones we do not cover. To mitigate that, we provide a flexible interfaces to extend UniBench with additional models or benchmarks (see code 1). Our analysis is also limited to the standard zero-shot evaluation protocol.\nImpact To guide the research community in navigating the overwhelming and fragmented landscape of VLM benchmarks, we introduced UniBench. UniBench provides a unified implementation of 50+ benchmarks, out-of-the-box comparisons across nearly 60 open VLMs, and a distilled fast-to-run set of representative benchmarks that can run on in 5 minutes a single GPU. In doing so, we uncover the limits of scale for reasoning and relations, the promise of data quality and tailored learning objectives, as well as offer recommendations for which VLMs practitioners should use. We hope UniBench is a step towards avoiding the blindspots in VLM evaluations, enabling researchers to comprehensively, yet efficiently evaluate progress."}, {"title": "Appendix", "content": "A UniBench Implementation Details\nWe have developed UniBench to be easy-to-run library to allow researchers to systematically compare and contrast exsisting (n=59) and new VLMs on 53 benchmarks. To evaluate new VLMs that expand beyond the already implemented 59 VLMs, users need to follow Code Snippet 2. Users would need to create a class that inherent from ClipModel from uni_bench.models_zoo with get_image_embeddings and get_text_-\nembeddings methods implemented. get_image_embeddings and get_text_embeddings methods takes images and captions as input, respectively, and returns a tensor of encoded representations.\nB Gauging progress in Vision Language Models\nScaling improves many benchmarks, but offers little benefit for reasoning and relation. Appendix Figure 7 shows that despite increasing the training dataset size by a factor of 1000\u00d7 and model size by a factor of 11x, relational and reasoning benchmarks performance is fairly flat compared to the significant boost in performance on other tasks. We further pinpoint capabilities such as Depth Estimation, Spatial Understanding, Counting, Scene and Text Recognition, as the underlying capabilities where scale does not lead to improvements as shown in Figure 8.\nC Impact of Prompts on MNIST Performance\nThe MNIST dataset, featuring handwritten digits, was subjected to various prompting strategies to evaluate their impact on model performance. Our findings reveal a distinct hierarchy in performance based on the type of prompts used. The benchmark was tested with both numeral formats (\"zero-nine\" and \"0-9\") and different prompt styles (specialized word prompts, specialized digit prompts, and a basic prompt) (Figure 9).\nC.0.1 Hierarchy of Prompt Performance\nThe performance of the MNIST model varied significantly across different prompt types and formats, arranged here from best to worst performing setups: 1. Word digits (\"zero-nine\") with specialized word prompts 2. Word digits (\"zero-nine\") with basic prompt 3. Word digits (\"zero-nine\") with specialized digit prompts"}, {"title": "C.0.2 Specialized Word Prompts", "content": "These prompts provided detailed descriptions and contexts, significantly enhancing the model's ability to recognize and interpret the digits accurately. Examples include:\n\u2022 \"showcasing the digit {}, is this image.\"\n\u2022 \"this number {} is represented in a handwritten form.\"\n\u2022 \"the numeral {} is captured in this snapshot.\"\n\u2022 \"the digit {} is depicted visually in this image.\"\n\u2022 \"this image is a graphical representation of the number {}.\".\n\u2022 \"this is an illustration of the digit {}.\".\n\u2022 \"this image represents the digit {} in a handwritten form.\"\n\u2022 \"the number {} is sketched as a digit in this image.\"\n\u2022 \"this is a photograph of the digit {}.\".\n\u2022 \"the number {} is drawn as a digit in this image.\""}, {"title": "C.0.3 Specialized Digit Prompts", "content": "These prompts explicitly mention the format or style of the digit, aiding in recognition but to a lesser extent compared to specialized word prompts. Examples include:\n\u2022 \"A photo of the number: '{}'.\"\n\u2022 \"A digit drawing of the number: '{}'.\"\n\u2022 \"A digit sketch of the number: '{}'.\"\n\u2022 \"A handwritten digit image of: '{}'.\"\n\u2022 \"A digit illustration of: '{}'.\"\n\u2022 \"A graphical representation of the number: '{}'.\"\n\u2022 \"A visual depiction of the digit: '{}'.\"\n\u2022 \"A snapshot of the numeral: '{}'.\"\n\u2022 \"A handwritten representation of the number: '{}'.\"\n\u2022 \"An image showcasing the digit: '{}'.\""}, {"title": "C.0.4 Basic Prompt", "content": "The basic prompt used:\n\u2022 \"a photo of the number: '{}'.\"\nThis structured analysis clearly demonstrates how the specificity and relevance of the prompt significantly influence the performance of VLMs. We investigated whether the subpar performance could be attributed to a lack of training images containing digit concepts by analyzing the popular LAION 400M dataset. Our findings reveal a substantial number of captions with both word digits (100k-2M) and integer digits (15M-48M) in the training captions, suggesting that the poor performance is not merely due to insufficient training data (see Figure 11 for exact counts by digit). To further understand the performance results on MNIST, we compute more generous top-2,-3,-4, and -5 accuracy measures to understand whether models confuse similar digits. We"}, {"title": "D Correlation of ImageNet with Other Benchmarks", "content": "ImageNet, often considered a cornerstone in the field of computer vision, has been widely used as a benchmark to evaluate the performance of image recognition models. Its extensive dataset and challenging classification tasks have set a standard for algorithm development and comparison. However, while ImageNet correlates well with many benchmarks, it does not exhibit a universal correlation across all tasks. Our analysis reveals that for a significant number of benchmarks, specifically 18 out of the 53 benchmarks analyzed, the performance on ImageNet is poorly or negatively correlated. This is illustrated in Appendix Figure 12, which provides a detailed comparison of benchmark performances. This finding suggests that success on ImageNet does not necessarily translate to proficiency in all visual tasks."}, {"title": "E A Practical Subset of Benchmarks", "content": "While ideally, evaluating VLMs across all 53 benchmarks would provide the most comprehensive insights, the computational demands and complexity of parsing such extensive data can be overwhelming (6 million images to evaluate; 2+ hours for one model on an A100 GPU). \u03a4\u03bf streamline evaluation, we distill the full set of benchmarks in UniBench into seven benchmark types and 17 capabilities. These categorizations were deraved based on benchmarks that correlate strongly with other benchmarks for each benchmark type and capability (Tables 2 and 3)."}]}