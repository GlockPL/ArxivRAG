{"title": "Trustworthy Human-AI Collaboration: Reinforcement Learning with Human Feedback and Physics Knowledge for Safe Autonomous Driving", "authors": ["Zilin Huang", "Zihao Sheng", "Lei Shi", "Sikai Chen"], "abstract": "In the field of autonomous driving, developing safe and trustworthy autonomous driving policies remains a significant challenge. Recently, Reinforcement Learning with Human Feedback (RLHF) has attracted substantial attention due to its potential to enhance training safety and sampling efficiency. Nevertheless, existing RLHF-enabled methods often falter when faced with imperfect human demonstrations, potentially leading to training oscillations or even worse performance than rule-based approaches. Inspired by the human learning process, we propose Physics-enhanced Reinforcement Learning with Human Feedback (PE-RLHF). This novel framework synergistically integrates human feedback (e.g., human intervention and demonstration) and physics knowledge (e.g., traffic flow model) into the training loop of reinforcement learning. The key advantage of PE-RLHF is its guarantee that the learned policy will perform at least as well as the given physics-based policy, even when human feedback quality deteriorates, thus ensuring trustworthy safety improvements. PE-RLHF introduces a Physics-enhanced Human-AI (PE-HAI) collaborative paradigm for dynamic action selection between human and physics-based actions, employs a reward-free approach with a proxy value function to capture human preferences, and incorporates a minimal intervention mechanism to reduce the cognitive load on human mentors. Extensive experiments across diverse driving scenarios demonstrate that PE-RLHF significantly outperforms traditional methods, achieving state-of-the-art (SOTA) performance in safety, efficiency, and generalizability, even with varying quality of human feedback. The philosophy behind PE-RLHF not only advances autonomous driving technology but can also offer valuable insights for other safety-critical domains.", "sections": [{"title": "1. Introduction", "content": "Autonomous driving technology holds significant potential to enhance traffic safety and mobility across various driving scenarios (Feng et al., 2023; Cao et al., 2023; Huang et al., 2024b; Sheng et al., 2024). Several autonomous vehicles (AVs) companies have recently demonstrated impressive performance metrics. For instance, in 2023, Waymo's AVs traveled a total of 4,858,890 miles in California. Similarly, Cruise's AVs achieved 2,064,728 driverless miles and 583,624 miles with a safety driver, while Zoox reported 710,409 miles with a safety driver and 11,263 miles without one in California (Report, 2024). Despite these advancements, autonomous driving technology remains far from achieving full automation (Level 5) across all driving scenarios (SAE International, 2021). In particular, developing safe and generalizable driving policies for various safety-critical scenarios remains an ongoing research challenge. (Lin et al., 2024; Huang et al., 2024c; Mao et al., 2024). A recent survey highlighted that safety, rather than economic consequences or privacy issues, is the primary concern regarding AV acceptability (Ju et al., 2022). Moreover, various agencies and the public still harbor concerns about the trustworthiness of autonomous driving systems (Cao et al., 2022, 2023; He et al., 2024a,b). Therefore, it is imperative to bridge the gap between the anticipated autonomous future and the current state-of-the-art technology by developing trustworthy, safety-assured driving policies.\nGenerally, AV companies employ a hierarchical approach to decompose the driving task into multiple sub-tasks. This approach reduces computational complexity and provides good decision-making transparency. Nevertheless, it requires cumbersome hand-crafted rules and may fail in difficult and highly interactive scenarios (Cao et al., 2021; Yang et al., 2023; Wu et al., 2024b). In recent years, learning-based end-to-end approaches have attracted increasing attention since they can learn from collected driving data, offering a potential path for designing more efficient driving policies (You et al., 2024). A notable example is UniAD, the 2023 CVPR Best Paper (Hu et al., 2023). As shown in Fig. 1 (a), imitation learning (IL) and reinforcement learning (RL) are two main approaches, particularly in the"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Safety Guarantees of RL-based Decision-Making", "content": "The trial-and-error nature of RL exposes agents to potentially dangerous situations, thus limiting its applicability in the domain of autonomous driving (Wu et al., 2023, 2024a; Li et al., 2022c; Peng et al., 2022). Numerous studies have attempted to develop safety guarantee techniques to ensure both training and testing safety, which can be broadly categorized into three approaches: safe RL, offline RL, and action safeguards. Safe RL aims to ensure that each updated policy meets specified constraints. These methods often require knowledge of the probability that a policy will violate constraints. Various constraint optimization methods have been explored, such as trust region methods (e.g., constrained policy optimization, CPO (Achiam et al., 2017)) and Lagrangian methods (e.g., PPO-Lag (Stooke et al., 2020) and SAC-Lag (Ha et al., 2021)). Nevertheless, insufficient data or inaccurate models can lead to significant errors, potentially causing these methods to fail. Offline RL methods, such as conservative Q-Learning (Kumar et al., 2020), aim to learn conservative policies from pre-collected datasets without online interaction, thereby avoiding potential safety risks during exploration (Levine et al., 2020). However, offline RL methods may struggle with generalization to unseen scenarios since they can not utilize online exploration data.\nThe fundamental idea of action safeguards is to combine RL-based policies with physics-based policies. When an action generated by the RL policy is deemed dangerous, the physics-based policy is employed instead. The detection of dangerous actions can be designed based on model uncertainty (Yang et al., 2023), policy confidence (Cao et al., 2021, 2023), or driving risk estimations (Cao et al., 2022; Bai et al., 2024). While they can enhance the safety of"}, {"title": "2.2. RLHF for Driving Policy Learning", "content": "In RLHF, human feedback can be broadly categorized into three types: evaluation (such as ranking or rating), intervention, and demonstration (Huang et al., 2024c). Evaluation-based methods typically involve humans assessing trajectories sampled by the learning agent or advising on actions when requested by the agent (Christiano et al., 2017; Wang et al., 2024). This passive human involvement can pose a risk to the human-AI collaborative system, as the agent explores the environment without sufficient safeguards (Peng et al., 2022). Demonstration-based methods learn from collected offline and static demonstration data. The agent treats the demonstrations as optimal transitions to imitate. If the provided data lacks reward signals, the agent can learn by imitating the teacher's policy distribution, matching trajectory distributions, or reshaping the expert's reward (Ly and Akhloufi, 2020; Wu et al., 2022; Zhuang et al., 2024). With additional reward signals, the agent can perform pessimistic Bellman updates, similar to most offline RL methods (Levine et al., 2020). Intervention-based methods allow human mentors to intervene in the training process. The switching between the expert's and the RL policy can be rule-based (Peng et al., 2022), or decided by the expert (Li et al., 2022c; Wu et al., 2023). Li et al. (2022c) proves safety guarantees in intervention-based methods, providing an additional lower bound on cumulative rewards. Nevertheless, many existing studies still adhere to the traditional RL paradigm, which necessitates the design of a reward function (Wu et al., 2023; Huang et al., 2023). Manually designing a reward function that effectively encompasses all driving behaviors is a significant challenge. Meanwhile, only a few works consider constraints to reduce human intervention (Li et al., 2022c).\nIn our previous work (Huang et al., 2024c), we proposed a human as AI mentor-based deep RL framework (HAIM-DRL), which simultaneously considered a reward-free setting and reduced the cognitive load of human mentors. However, to the best of our knowledge, most RLHF-enabled methods for driving policy learning have not considered trustworthy safety guarantees in the case of imperfect human feedback. Perhaps the closest to this work is Wu et al. (2023), which proposes an adaptive weighting factor to adjust the credibility of human actions by evaluating the potential advantages of human behavior relative to the RL policy. While this method can relax the requirements for the quality of human demonstrations, the generated policy still lacks a performance bound for driving. In this work, we propose a novel PE-RLHF framework that integrates both human feedback and physics knowledge to address these limitations. Moreover, Huang et al. (2024a) proposes a safety-aware human-in-the-loop reinforcement learning approach to alleviate the risk of human feedback deterioration. While their motivation is similar, they achieve this by incorporating environmental information, whereas we take advantage of the trustworthy performance bound provided by physics knowledge while maintaining the adaptability and efficiency of human feedback."}, {"title": "3. Problem Formulation", "content": ""}, {"title": "3.1. Preliminaries", "content": "The driving policy learning problem can be formulated as a Markov decision process (MDP) (Cao et al., 2021). An MDP is defined by a tuple (S, A, P, R, \u03bc, \u03b3), where S and A denote the state space and action space, respectively (Sutton and Barto, 2018). In the context of autonomous driving, the state space may include information about the ego vehicle, surrounding vehicles, and the driving environment, while the action space may consist of control inputs such as throttle and steering. The transition probability function P : S\u00d7A\u00d7S \u2192 [0, 1] describes the dynamics of the system, and the reward function R : S \u00d7 A \u2192 R encodes the desired driving behavior. The initial state distribution is denoted by \u03bc : S \u2192 [0, 1], and \u03b3 is the discount factor for future rewards.\nThe goal of standard RL is to find the optimal policy \u03c0* that maximizes the expected discounted return JR(\u03c0), which is defined as:"}, {"title": "3.2. Problem Statement", "content": "In this work, our goal is to develop a safe and trustworthy driving policy learning framework for autonomous driving. This framework should be characterized as follows: (a) It should be able to provide trustworthy safety performance guarantees. (b) It should have strong generalization ability for environmental uncertainties of real-world traffic scenarios (e.g., changing road geometries and unforeseen obstacles). (c) It should have high sampling efficiency"}, {"title": "Problem 1 (Learning from Human Feedback)", "content": "Traditional learning-based methods, such as IL and RL, face challenges in ensuring safety and sampling efficiency. To take advantage of human intelligence, we should design a scheme that enables AV agents to learn from human feedback (e.g., intervention and demonstration).\nWe define a dataset of human demonstration  \\(D_{\\text{human}} = \\{(s_t, a_{\\text{human}})\\}\\), where  \\(s_t\\) represents the state and  \\(a_{\\text{human}} \\sim \\pi_{\\text{human}}( \\cdot | s_t)\\) represents the action taken by the human in state  \\(s_t\\). Then, we aim to train an AV agent with policy  \\(\\pi_{\\text{AV}}\\) from  \\(D_{\\text{human}}\\) that can make wise decisions  \\(a_{\\text{AV}}\\) given state  \\(s_t\\). In other words, we need to align its behavior with human preferences as closely as possible:"}, {"title": "Problem 2 (Trustworthy Safety Improvement)", "content": "Due to factors such as limited perception, distraction, or fatigue, the quality of human demonstration may decline over time, leading to training failure. To ensure the effectiveness and trustworthiness of RLHF-enabled methods, we should guarantee that even when the quality of human demonstration decreases, the performance of the AV agent's policy is still not inferior to existing physics-based methods.\nInspired by the action safeguards methods, we can leverage well-established physics-based models (capable of handling most driving cases except for long-tail scenarios) from traffic science as a trustworthy lower bound for the safety performance of the AV agent's policy. Formally, we define the problem as follows:"}, {"title": "4. Physics-enhanced Human-AI Collaborative Paradigm", "content": ""}, {"title": "4.1. Inspiration", "content": "As we mentioned before, most human-AI collaborative paradigms usually rely on the assumption of perfect human mentors, which may not always hold in practice. Observing the process of human learning skills, they rely not only on human teachers but also on established knowledge. For example, as shown in Fig. 2 (a), when learning a foreign language, a student may be guided by two mentors: a native speaker and a grammar book. The native speaker's expertise is invaluable to the student's language acquisition, providing context-specific instruction and real-world examples. Yet, in some cases, native speakers' explanations may be unclear or incorrect, for example, in the use of colloquial expressions that deviate from standard grammatical rules. In such cases, a grammar book can serve as a reliable reference and safety net, consistently ensuring that students follow the basic rules of the language. As a result, the student's language skills are improved by learning from two mentors.\nInspired by the human learning process, we propose the PE-HAI collaboration paradigm, whose main components are shown in Fig. 2 (b). In the PE-HAI, the AV is equipped with three policies:  \\(\\pi_{\\text{human}}\\) (similar to the role of a native speaker),  \\(\\pi_{\\text{phy}}\\) (similar to the role of a grammar book), and  \\(\\pi_{\\text{AV}}\\) (similar to the role of the student). In detail, the  \\(\\pi_{\\text{phy}}\\) generates an action  \\(a_{\\text{phy}}\\) based on an interpretable physics-based model, while the  \\(\\pi_{\\text{human}}\\) provides an action  \\(a_{\\text{human}}\\) based on human judgment and situational awareness. When there is no human takeover, the AV executes  \\(\\pi_{\\text{AV}}\\) and learns from exploration through interaction with the environment. When humans take over, we design an action selection mechanism to determine whether the  \\(a_{\\text{human}}\\) or  \\(a_{\\text{phy}}\\) should be applied to the environment. In this way, although human may occasionally fail due to factors such as fatigue, the  \\(\\pi_{\\text{phy}}\\) can generate feasible and safe actions in this situation."}, {"title": "4.2. Human Policy Generation", "content": ""}, {"title": "4.2.1. Human-AI Shared Control", "content": "In this work, as shown in Fig. 2 (b), we employ an intimate form of human-AI shared control that integrates learning from intervention (LfI) and learning from demonstration (LfD) into a unified architecture. More details about human-AI shared control can be found in our previous work (Huang et al., 2024c). Specifically, we adopt a technique termed the switching function that enables the agent to switch between exploration and intervention dynamically (Peng et al., 2022). The switch function T determines the state and timing for human takeover, allowing the human to demonstrate correct actions to guide the learning agent.\nLet T(s) = 1 denote that the human takes control and T(s) = 0 mean otherwise. The mixed behavior policy  \\(\\pi_{\\text{mix}}\\) can be represented as:"}, {"title": "4.2.2. The Form of Switch Function", "content": "In general, there are two common forms of switch function: probability-based switch function and action-based switch function (Peng et al., 2022). In this work, we use the action-based switch function, which triggers intervention when the agent's action deviates from the human's action, such as takeover. The action-based switch function  \\(T_{\\text{act}}\\) is designed as (Huang et al., 2024c):\n\\(\nT_{\\text{act}}(s_t, a_t, a_{\\text{human}}) =\n\\begin{cases}\n(a_{\\text{human}} \\sim \\pi_{\\text{human}}(\\cdot | s_t), 0), & \\text{if takeover} \\\\\n(a_{\\text{AV}}, 0), & \\text{otherwise}\n\\end{cases}\n\\)\nA Boolean indicator I(st) denotes human takeover, and the action applied to the environment is defined as  \\(a_{\\text{mix}} = I(s_t)a_{\\text{human}} + (1 - I(s_t))a_{\\text{AV}}\\. This setup eliminates unnecessary states and mitigates the safety concerns associated with traditional RL methods.\nTo measure the effectiveness of the  \\(T_{\\text{act}}\\) in the setting of the PE-HAI, we examine the return of the mixed behavior policy J(\u03c0mix). With Tact(s) defined in Eq. 7, J(\u03c0mix) can be bounded by the following theorem:"}, {"title": "4.3. Physics-based Policy Generation", "content": "Besides leveraging human feedback, we also incorporate physics knowledge into the PE-HAI to establish a trustworthy lower bound on the framework's performance. The  \\(\\pi_{\\text{phy}}\\, derived from a well-established traffic flow model, serves as a reliable safeguard even in situations where human input quality may deteriorate or be inconsistent. Consistent with Yang et al. (2023); Cao et al. (2021, 2022), we use the intelligent driver model (IDM) (Treiber et al., 2000) and the minimizing overall braking induced by lane changes (MOBIL) model (Kesting et al., 2007) to generate the action  \\(a_{\\text{phy}}\\, Note that other traffic flow models may also be effective, which we leave for future research to explore and validate. The IDM describes the longitudinal dynamics of the vehicle as follows:"}, {"title": "4.4. Action Selection Mechanism", "content": "To effectively leverage the strengths of both human feedback and physics knowledge, we design an action selection mechanism as the core module of the PE-HAI. This mechanism, as illustrated in Fig. 2 (b), serves as an arbitration component that evaluates and selects between actions generated by the  \\(\\pi_{\\text{human}}\\) or  \\(\\pi_{\\text{phy}}\\, Technically, we expect that the agent will choose the action with the higher expected Q value between  \\(a_{\\text{human}}\\) and  \\(a_{\\text{phy}}\\) to execute."}, {"title": "4.4.1. Value Estimator Construction", "content": "First, to obtain the expected Q value for  \\(a_{\\text{human}}\\) and  \\(a_{\\text{phy}}\\, we define two ways of constructing the value estimators: (a) Human demonstration warmup. During the warmup phase, human mentors continuously control the agent and provide high-quality action demonstrations. The human mentor's demonstration data is then used to train an estimator Q-network  \\(Q_{\\text{est}}\\) from scratch. (b) Expert policy warmup. Following the approach in Peng et al. (2022), we first train an expert policy  \\(\\pi_{\\text{expert}}\\) in a more constrained environment. During the warmup phase, we roll out  \\(\\pi_{\\text{expert}}\\) and collect training samples. The collected data is then used to train the estimator Q-network  \\(Q_{\\text{test}}\\,.\nWith limited training data, the estimator Q-network may fail to provide accurate estimates when encountering previously unseen states. To address this issue, we propose to use an ensemble of estimator Q-networks technique, inspired by the work of Chen et al. (2021b). A set of ensembled estimator Q-networks Q with the same architecture but different initialization weights are constructed and trained using the same data.\nThe loss function for training the ensemble estimator Q is:"}, {"title": "4.4.2. Selection Function Design", "content": "Then, we design a selection function  \\(T_{\\text{select}}(s)\\) to execute the action with the higher expected Q value between  \\(a_{\\text{human}}\\) and  \\(a_{\\text{phy}}\\) into the environment. The  \\(T_{\\text{select}}(s)\\) can be formalized as follows:"}, {"title": "4.4.3. Analysis of Trustworthy Safety Improvement", "content": "To demonstrate the safety improvement of the PE-HAI, we analyze its performance in comparison to using either  \\(\\pi_{\\text{human}}\\) and  \\(\\pi_{\\text{phy}}\\) alone. As mentioned above, the goal is to learn an optimal policy  \\(\\pi_{\\text{AV}}^\\star\\). Analyzing Eq. 2, we find that the higher the quality of human feedback in RLHF, the closer the learned  \\(\\pi_{\\text{AV}}\\) is to  \\(\\pi_{\\text{AV}}^\\star\\). In the setting of PE-HAI, combining Eqs. 1, 13, and 14, we can obtain the satisfaction of the constraints in Eq. 15."}, {"title": "5. Physics-enhanced Reinforcement Learning with Human Framework", "content": "In this section, we propose a PE-RLHF framework, as shown in Fig. 4. The whole framework consists of five parts: (a) Observation space and action space, (b) Reward-free actor-critic architecture, (c) Learning from hybrid intervention action, (d) Learning from exploration with entropy regularization, and (e) Reducing the human mentor's cognitive load. In the following subsections, we will explain each of these components in detail."}, {"title": "5.1. Observation Space and Action Space", "content": "Following the end-to-end learning paradigm, we design observation space and action space to directly map raw sensory inputs (LiDAR) to control commands (throttle and steering angle), minimizing the need for intermediate rep-resentations. As shown in Fig. 4 (a), the observation state consists of three parts, designed to provide a comprehensive view of the driving environment: (a) Ego state includes current states of the ego vehicle, such as steering angle, heading angle, velocity, and relative distance to road boundaries. (b) Navigation information includes the relative positions of the target vehicle concerning the upcoming checkpoints. (c) The surrounding environment uses a 240-dimensional vector to represent the 2D-Lidar-like point clouds, capturing the surrounding environment within a maximum de-tecting distance of 50m, centered at the target vehicle. Each entry in this vector is normalized to the range [0, 1], indicating the relative distance of the nearest obstacle in the specified direction.\nDifferent from methods that pre-select a subset of actions as candidates (Cao et al., 2022), we employ a more challenging approach by defining the action space as a continuous space bounded by [-1, 1]. This continuous ac-tion space design allows for smoother and more precise control, enabling the agent to learn more nuanced driving behaviors. Specifically, the action is defined as the throttle and steering angle. For steering wheel control, negative values represent left turn commands, while positive values correspond to right turn commands. Regarding the throttle, negative values indicate braking commands and positive values correspond to acceleration commands."}, {"title": "5.2. Removing the Reward Function", "content": "Some RLHF-enabled works try to reshape a reward function from human demonstration data to avoid manual reward design (Wu et al., 2022; Wang et al., 2024; Zhuang et al., 2024). Nevertheless, this method still faces challenges such as potential bias in offline demonstration data and difficulty in capturing complex human preferences. Upon reevaluating our primary objective, we realize that a conventional reward function is not necessary. Instead, our core aim is to incorporate human preferences into the learning process. Human intervention serves as a clear indicator of"}, {"title": "5.3. Learning Objectives for Value Network", "content": "We propose a comprehensive set of objectives that can effectively utilize the human feedback and physics knowledge. The learning objectives are as follows: (a) The agent should aim to maximize the proxy value function, denoted as Q(s, a), which reflects the value of the hybrid intervention action  \\(a_{\\text{hybrid}}\\, (b) The agent should actively explore the state-action space. This is achieved by maximizing the entropy of the action distribution, denoted as  \\(H(\\pi(\\cdot | s))\\). (c) The agent should strive to reduce the cognitive load of the human mentor by minimizing the intervention value function, denoted as  \\(Q_{\\text{int}}(s, a)\\).\nThe overall learning objective can be summarized as follows:"}, {"title": "5.3.1. Learning from Hybrid Intervention Action", "content": "According to the observation in Section 5.2, we should strive to make the agent's behavior close to the behavior selected by the PE-HAI, which combines human and physics knowledge. A closer analysis of Eq. 1 indicates that the optimal deterministic strategy consistently selects the action with the highest Q value. Consequently, in states where human intervention occurs,  \\(a_{\\text{hybrid}}\\) should invariably have higher values than alternative actions, while agent actions  \\(a_{\\text{AV}}\\) should have comparatively lower values."}, {"title": "5.3.2. Learning from Exploration with Entropy Regularization", "content": "Insufficient exploration of the PE-HAI's preferred subspace during free sampling can result in rare encounters with high proxy value states. This scarcity hinders the backward propagation of proxy values, potentially impeding the learning process. To address this issue and promote more comprehensive exploration, we incorporate entropy regularization (Haarnoja et al., 2018), which introduces an auxiliary signal for proxy value function updates:"}, {"title": "5.3.3. Reducing the Human Mentor's Cognitive Load", "content": "Unrestricted PE-HAI intervention frequency may lead to the agent's over-reliance on  \\(a_{\\text{hybrid}}\\, potentially compromising its performance when evaluated independently (Peng et al., 2022; Li et al., 2022c; Wu et al., 2023). This vulnerability stems from Q(st, at) reflecting the proxy Q value of  \\(\\pi_{\\text{mix}}\\) rather than  \\(\\pi_{\\text{AV}}\\). Consequently, the agent might choose actions contradicting PE-HAI preferences, such as boundary violations, necessitating frequent interventions. This cycle perpetuates low automation and imposes a high cognitive burden on the human mentor due to constant corrective action requirements.\nTo reduce the human mentor's cognitive load and increase the AV's autonomy, we introduce a subtle penalty for agent behaviors that prompt PE-HAI intervention. This penalty is quantified using the cosine similarity between  \\(a_{\\text{AV}}\\) and  \\(a_{\\text{hybrid}}\\, serving as an intervention cost (Li et al., 2022c). The formulation is as follows:"}, {"title": "5.4. Learning Policy for Policy Network", "content": "The policy network is responsible for determining control actions and strives to optimize the value network. The batch gradient of the policy network can be expressed as follows:"}, {"title": "6. Experimental Evaluation", "content": "In this section, we will conduct experiments to investigate the following questions for evaluating our proposed PE-RLHF method: (a) Can PE-RLHF learn driving policies with higher learning efficiency and performance compared to other methods that do not consider human feedback and physics knowledge? (b) Can PE-RLHF provide safety guarantees and achieve trustworthy performance improvement compared to other RLHF methods that do not leverage physics knowledge, especially when the quality of human feedback deteriorates over time? (c) Is PE-RLHF robust under different traffic environments and human feedback quality?\nTo answer questions (a) and (b), we compare PE-RLHF with physics-based methods, RL and safe RL methods, offline RL and IL methods, and RLHF methods. For question (c), we compare different proficiency levels of human mentors, as well as different traffic environments and parameter settings."}, {"title": "6.1. Experiment Setup", "content": ""}, {"title": "6.1.1. Experiment Environment", "content": "Considering the potential risks associated with involving human subjects in physical experiments, we benchmarked the different methods in a lightweight driving simulator MetaDrive (Li et al., 2022b), which retains the ability to evaluate safety and generalizability in unseen environments. MetaDrive employs procedural generation techniques to synthesize an infinite number of driving maps, enabling the separation of training and testing sets, which facilitates benchmarking the generalization capabilities of various methods in the context of safe driving. The simulator is also extremely efficient and flexible, allowing us to run the human-AI collaboration experiment in real time. The player's goal is to drive the ego vehicle to a pre-determined destination, avoiding dangerous behaviors such as collisions."}, {"title": "6.1.2. Scenario Description", "content": "To validate the performance of PE-RLHF in more realistic traffic environments, we utilize the MetaDrive simulator to generate traffic scenarios with varying roadways (e.g., straight, ramp, fork, roundabout, curve, T-intersection, and intersections). Fig. 5 illustrates some of the generated driving scenarios in MetaDrive. The driving environment is designed to be full of uncertainty, similar to natural driving environments. The designed uncertain environment has the following features:\n(a) Surrounding Vehicle: Each surrounding vehicle has an unobservable behavior generation model. The behavior generation model uses MetaDrive's default rule-based planner control. Each surrounding vehicle generates an action at each time step based on the situation of the surrounding vehicles. Therefore, each surrounding vehicle has a unique driver behavior when interacting with other vehicles. The ego vehicle must consider surrounding vehicles' normal driving behaviors, such as following lanes, changing lanes, and exiting roundabouts. This setup simulates the strong uncertainty present in real-world driving.\n(b) Random Traffic: The generation time and location of surrounding vehicles are random. These vehicles influence each other. When the ego vehicle starts driving in the simulator, the simulator first randomly generates surrounding vehicles around the ego vehicle. Additionally, the surrounding vehicles are of various types, including trucks and cars. The combination of different types of surrounding vehicles and their random initial positions form increasingly complex traffic scenarios.\n(c) Random Obstacles: We randomly generate obstacles, such as stationary broken-down vehicles, stationary traffic cones, and triangular warning signs. The driving road is set to three lanes, allowing the ego vehicle to change lanes to avoid collisions. Collisions can occur in various ways. Such an environment presents a high-dimensional driving problem, making it difficult to design a perfect policy that handles all situations. However, this setting is more realistic for natural driving, as real driving processes are not simple concatenations of individual cases\nThe uncertain environments cause the ego vehicle to encounter different surrounding traffic, forcing the generated driving policies to adapt to the uncertain surroundings, addressing the over-fitting issue."}, {"title": "6.1.3. Reward and Cost Definition", "content": "Although PE-RLHF does not rely on environmental rewards during the training phase, we provide a reward function for training the baseline method and evaluating different methods during testing. Specifically, we follow the default reward scheme in MetaDrive. The reward function is composed of three parts as follows (Li et al., 2022b):"}, {"title": "6.2. Baselines", "content": "To benchmark the proposed PE-RLHF for autonomous driving, we conduct experimental comparisons with state-of-the-art methods. We categorize them into several groups: Physics-based methods, RL and Safe RL methods, Offline RL and IL methods, and RLHF methods.\n*   Physics-based Methods. These methods rely solely on predefined physics-based models to generate driving actions, without any learning or human feedback. Consistent with Cao et al. (2022); Tang et al. (2022), we use a combination of the IDM (Treiber et al., 2000) for longitudinal control and the MOBIL (Kesting et al., 2007) for lateral control. Both are widely used driving models, but other driving models can also be employed.\n*   RL and Safe RL Methods. SAC-RS (Tang et al., 2022) and PPO-RS (Ye et al., 2020) use reward shaping (RS) technique to address the issue of potentially diminished learning efficiency when the reward signals generated by the environment are sparse. SAC-Lag (Ha et al., 2021), PPO-Lag (Stooke et al., 2020), and CPO (Achiam et al., 2017) aim to improve safety during the RL training process by imposing constraints on policy optimization.\n*   Offline RL and IL Methods. CQL (Kumar et al., 2020) learns from a fixed dataset collected by human mentors without access to online exploration. It addresses the distribution shift problem in offline RL by learning a conservative Q-function that lower bounds the true Q-function. BC (Sharma et al., 2018) and GAIL (Kuefler et al., 2017) learn from human demonstrations to mimic expert behavior. BC directly learns a policy that maps states to actions, while GAIL learns a reward function that encourages the agent to behave similarly to the expert."}, {"title": "6.3. Evaluation Strategy", "content": ""}, {"title": "6.3.1. Evaluation Metric", "content": "To comprehensively assess the performance of the PE-RLHF and compare it with other approaches, we introduce a set of metrics that capture various aspects of autonomous driving performance: (a) Episodic Return. The cumulative reward obtained by the agent in an episode. (b) Success Rate. The percentage of episodes where the agent reaches the destination while staying within the road boundaries. (c) Safety Violation. The total cost incurred due to collisions with vehicles or obstacles in an episode. (d) Travel Distance. The distance covered by the agent in each episode. (e) Travel Velocity. The average velocity maintained by the agent during an episode. (f) Total Overtake Count. The number of vehicles overtaken by the agent in each episode."}, {"title": "6.3.2. Three-stage Strategy", "content": "During the evaluation", "performance": "a) Stage I. In the first stage, we focus on the episodic return and success rate."}]}