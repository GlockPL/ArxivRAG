{"title": "Quantum Gradient Class Activation Map for Model Interpretability", "authors": ["Hsin-Yi Lin", "Huan-Hsin Tseng", "Samuel Yen-Chi Chen", "Shinjae Yoo"], "abstract": "Abstract-Quantum machine learning (QML) has recently made significant advancements in various topics. Despite the successes, the safety and interpretability of QML applications have not been thoroughly investigated. This work proposes using Variational Quantum Circuits (VQCs) for activation mapping to enhance model transparency, introducing the Quantum Gradient Class Activation Map (QGrad-CAM). This hybrid quantum-classical computing framework leverages both quantum and classical strengths and gives access to the derivation of an explicit formula of feature map importance. Experimental results demonstrate significant, fine-grained, class-discriminative visual explanations generated across both image and speech datasets.\nIndex Terms-Variational quantum circuits, quantum neural networks, gradient-based localization.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in quantum computing and machine learning (ML) have drawn significant attention, leading to efforts to combine these two fascinating technologies. Although current quantum devices still face challenges due to noise and other imperfections, a hybrid quantum-classical computing framework has been proposed to leverage the strengths of both quantum and classical computing [1]. Variational quantum circuits (VQCs) serve as the foundational elements of this hybrid framework. Within this framework, computational tasks that can benefit from quantum advantages are executed on quantum computers, while others are handled by classical computers. VQC-based algorithms have been shown to have certain advantages over classical models [2]\u2013[4] and have demonstrated success in various ML tasks, including classification [5]\u2013[7], sequential modeling [8], audio and language processing [9]\u2013[11], and reinforcement learning [12]\u2013[14].\nDespite these successes, certain aspects of quantum machine learning (QML) models have not been thoroughly investigated, particularly concerning the safety of QML applications. Model interpretability and transparency are crucial for understanding and ensuring proper use, especially due to extensive machine learning applications and the continuing development of policy and regulation. With the rapid development of quantum computing, it is important to explore whether any quantum advantage could be offered in this direction.\nVarious approaches have been developed to address model interpretability from different aspects, such as from feature importance techniques [15] and rule-based methods [16]. Model-agnostic methods provide local interpretability by approximating complex models with simpler ones [17]\u2013[19].\nAmong diverse approaches, visual techniques stand out for their intuitive appeal. Methods such as Partial Dependence Plot (PDP) [20] and Individual Conditional Expectation (ICE) [21] visualize the relationship between a feature and the predicted outcome. Class Activation Mapping (CAM) [22] and its variants [23], [24] have emerged as powerful tools for generating class-discriminative localization as visual explanations for Convolutional Neural Networks (CNNs).\nIn this work, we propose using VQC for activation mapping as a pioneering exploration of quantum circuit applications for model transparency. Our proposed method, Quantum Gradient Class Activation Map (QGrad-CAM), employs a VQC to weigh the importance of activation maps generated from a CNN-based network.\nBased on the structure of VQC, we will derive the explicit formula for the importance of each activation map, which can serve as an example of this advantage. Furthermore, experiments are performed on image and speech datasets for validation. Meaningful highlighted regions are returned using the proposed method for all cases.\nTo summarize, our main contributions include\n\u2022 Deriving an explicit formula for the VQC importance of activation maps and visualize model decisions.\n\u2022 Conducting experiments to show the VQC can effectively weigh the importance of activation maps and provide textual explanations for model decisions.\nThe rest of the paper is organized as follows. In Sec. II, we point out relevant work in previous literature. Our proposed method, QGrad-CAM, is introduced in Sec. III. The details of QGrad-CAM are written in Sec. III-C after a VQC review and discussions of the critical ideas in Sec. III-A and Sec. III-B that motivate our proposal. With the VQC structure, we derive the explicit formula for the importance of feature maps in Sec. IV. The experimental results can be found in Sec. V, and the conclusions in Sec. VI."}, {"title": "II. RELATED WORK", "content": "Explaining QML Models. VQC-based QML methods have demonstrated significant success in the domain of classification [5]. Noteworthy advancements include the development of Quantum Convolutional Neural Networks (QCNN) [6], [7], quantum transfer learning techniques [25], and hybrid models incorporating tensor networks [26]. Despite these achievements, the explainability of these models has not been thoroughly addressed. Several preliminary attempts have been made to explain the predictions generated by QML models. For instance, the study in [27] investigates feature importance within quantum Support Vector Machine (SVM) models using the Iris dataset, which consists of only four features. The generalizability of this method to larger-scale datasets, such as image data or hybrid models combining classical NNs and VQCs, remains uncertain. Another approach, as outlined in [28], involves calculating the Shapley values for gates in VQCs to determine their respective contributions. The objective of the methods presented in [28] is to rigorously evaluate the quality of various circuit architectures. In contrast, our proposed framework aims to uncover feature importance given a fixed VQC architecture using a scalable gradient-based method.\nVisualizing & CNN localization. CNNs have long demonstrated exceptional performance across a wide range of applications. This sparked extensive research aimed at understanding the underlying properties of CNNs. Several works developed techniques for visualizing the CNN-learned latent representation by, for example, analyzing convolution layers [29], [30] and inverting deep features [31]\u2013[33]. The research prompted the discovery of CNNs' ability to localize objects. CAM was proposed in [22], where CNN layers localize objects unsupervised and produce visual explanations for each class. Different pooling methods were explored in [34], [35] with a similar structure as CAM. Grad-CAM [23] generalized CAM by combining the class discriminative property with gradient techniques. The method allowed fine-grained discriminative localization and was improved, especially for multiple instances scenarios in [24]."}, {"title": "III. QUANTUM GRADIENT CLASS-ACTIVATION MAP (QGRAD-CAM)", "content": "A. Variational Quantum Circuits\nVQCs also referred to as Parameterized Quantum Circuits (PQCs), are quantum circuits characterized by tunable parameters that can be optimized based on specific metrics or signals [5]. VQCs or PQCs serve as fundamental components of Quantum Neural Networks (QNNs), which underlie current QML techniques. The VQC as a QML method operates on n qubits in space H = C^{2^n} \u2243 C^{2^n}, where H is a Hilbert space containing a standard basis written as \u03b2 = {|00...0\u27e9, |00...1\u27e9, ..., |11...1\u27e9} such that any quantum state |\u03c8\u27e9 \u2208 H can be expanded by \u03b2, i.e.,\n|\u03c8\u27e9 = c_0 |00...0\u27e9 + \u00b7\u00b7\u00b7 + c_N |11...1\u27e9\nfor some coefficients c_i \u2208 C with N = 2^n. Let L(H) denotes all linear operators on H and U(H) be the collection of unitary operators in L(H). A VQC typically functions through the following three steps (see Fig. 1),\n1) A quantum encoding V : R^n \u2192 U(H),\n2) A variational quantum gate U(\u03b8) \u2208 U(H) parameterized by \u03b8,\n3) A quantum measurement of Q as an output\u3008\u00b7|Q|\u00b7\u3009: H \u2192 R.\nAssume data is of classical form D = {(x_j,y_j )|x_j \u2208 R^n, y_j \u2208 R^m, j = 1,..., N} where x_j is an input of sample index j and y_j be the corresponding label. A quantum encoding scheme chooses a fixed gate sequence V : R^n \u2192 U(H) to convert classical data into quantum states such that each input corresponds to a unitary, x_j \u2192 V(x_j). One then associates the quantum state |\u03c8_{x_j}\u27e9 := V(x_j)|\u03c8_0\u27e9 to data x_j up to a random initial |\u03c8_0\u27e9 \u2208 H. For instance, V(x) = e^{itan^{-1}(x)\u03c3_k} injects data x \u2208 R into a 1-qubit space in a non-linearly fashion [5]. Typical choices of V include combinations of Hadamard gates, CNOT gates, and gates generated by Pauli matrices P = {I, \u03c3_1, \u03c3_2, \u03c3_3}. Subsequently, the encoded state |\u03c8_{x_j}\u27e9 is deformed by the parameterized gate U(\u03b8) such that |\u03c8_{x_j}\u27e9 \u2192 U(\u03b8)|\u03c8_{x_j}\u27e9, where \u03b8 represents the learnable parameters subject to certain specific optimization routines. It is noted that the deformation ability of VQC majorly comes from U(\u03b8) where a convention is taking tensor products of the 1-parameter subgroup generated by P,\nU(\u03b8) = \\prod_{l=1}^{L} e^{-i\u03b8_l^{(q)}\u03c3_l^{(q)}} C_l \u2208 U(H)  (1)\nwhere q = 1,...,n is the qubit index, l = 1,..., L is the index of variational circuit layers up to L with each \u03c3_l^{(q)} \u2208 P, C_l is the unitary of all other non-parameterized gates such as CNOT gates etc, and \u03b8 = {(\u03b8_1^{(q)},..., \u03b8_L^{(q)})} _{q=1}^{n} \u2208 R^{nL} denotes the collection of all variational (learnable) parameters. Often the 1-parameter subgroup \u03b8 \u2192 e^{-i\u03b8\u03c3_k} generated by \u03c3_k \u2208 P is denoted as {I, R_x(\u03b8), R_y(\u03b8), R_z(\u03b8)} correspondingly.\nA final measurement step selects m Hermitian operators Q_1,..., Q_m (each Q_i \u2208 L(H)) to collapse state U(\u03b8) |\u03c8_{x_j}\u27e9 and yields an m-dimensional output vector y = \u27e8(Q_1),..., (Q_m)\u27e9 by\n\u27e8Q_i\u27e9 := \u27e8\u03c8_0|V^\u2020(x_j)U^\u2020(\u03b8) Q_i U(\u03b8)V(x_j) |\u03c8_0\u27e9 \u2208 R  (2)\nwith i = 1,...,m. Collectively, the three steps in VQC can be written as f_{VQC} : R^n \u2192 R^m transforming x_j \u2194 f_{VQC}(x_j), see Fig. 1. By writing f_{VQC,\u03b8} we emphasize the dependency on \u03b8. We can drop the subscript \u03b8 when the context is clear.\nAdditional components, such as a classical neural network, is possible to be implemented to process raw data into a latent vector of smaller dimensions suitable for a VQC [25], [26], [36]. This technique is particularly useful when the data dimension exceeds the current capabilities of quantum devices or simulators. Furthermore, the output values from the VQC y = \u27e8(Q_1), ..., (Q_m)\u27e9 can be refined through further quantum or classical processing. For example, the output from a VQC can be processed by another classical neural network or a VQC. This is useful when the desired values are in a range not provided by the quantum observables and require certain rescaling. A loss function can be chosen as\nL(F_\u03c6, G_\u03b7, f_{VQC,\u03b8}; D) = \u2211_{j} d(y_j, (G_\u03b7 \u25e6 f_{VQC,\u03b8} \u25e6 F_\u03c6)(x_j))  (3)\nwhere F_\u03c6 represents the classical pre-processing network, G_\u03b7 denotes the classical post-processing network, and d is a function measuring the distance between the predicted results and the ground truth y_j. The whole hybrid quantum-classical model, including both classical and quantum parameters, can be trained in an end-to-end manner through gradient-based [26] or gradient-free [36] optimization algorithms. The goal is to find the optimal \u2217, the collection of all quantum and classical trainable parameters {\u03b8, \u03c6, \u03b7}, by\n\u2217 = argmin_\u03b8 L(F_\u03c6, G_\u03b7, f_{VQC,\u03b8}; D).\nVQC-based models have been proved to be able to outperform classical NN when certain conditions are met [2], [3]. For example, it is possible to train QML models trained on smaller training dataset [4], [37] while maintaining the generalizability. Empirically, VQC has been shown to be successfully in various ML tasks [5], [6], [8], [9], [12]\u2013[14], [26].\nB. Regularities of VQC compared to neural networks\nFully-connected networks are the building blocks of classical networks which are of the form,\nf = f_n \u25e6 f_{n-1} \u25e6\uff65\uff65\uff65\u25e6 f_1  (4)\nwhere each layer f_k(z) = \u03c3_k(W_k z + b_k) : R^{l_{k-1}} \u2192 R^{l_k} is composed of an activation function \u03c3_k, a bias vector b_k \u2208 R^{l_k} and a weight matrix W_k \u2208 L(R^{l_{k-1}}, R^{l_k}). Here L(A, B) denotes the collection of linear maps between linear spaces A and B. Typically, there is no restriction on (classical network) weights W_k to be trained such that very often W_k is not invertible even if dim A = dim B. This results in a problem where a network prediction y \u2208 R^m is hard to be traced back as W^{-1}(y) does not exist nor f^{-1}(y) is well-defined due to the structure of activation functions. Owing to this reason, classical networks are called black boxes and thus lack certain regularity even though the prediction ability is powerful.\nOn the contrary, since VQCs are comprised of unitary matrices, all gates U are invertible, and a quantum state |\u03c8\u27e9 := U|\u03c8_0\u27e9 is easily revertible by |\u03c8_0\u27e9 = U^*|\u03c8\u27e9. From this point of view, VQC possesses certain transparency and better regularity.\nAdditionally, in view of matrix groups, variational gates U(\u03b8) in Eq. (1) form a Lie subgroup of GL(H), all invertible matrices in L(H), so that {U(\u03b8)} naturally inherited smooth structures from the differentiable submanifold [38]. This provides us some hints that the VQC training may be more stable as the training iterations \u03b8^(iter) \u2192 U(\u03b8^(iter)) being contained on the smooth submanifold U(H). Motivated by these properties, it leads us to consider viewing the explainability of VQC via a classical technique Grad-CAM.\nC. Quantum Grad-CAM by VQC\nGrad-CAM is a technique to interpret and visualize the decisions of CNNs via the gradient calculation of a target classifier. Let a set of CNN filters be {W_{s_1,s_2,c,k}}^{S,S,C,K}_{s_1=1,s_2=1,c=1,k=1} where C is the number of input channels and K is the number of output channels, and S is the kernel size. A feature map {A_{ij}^k}_{i,j}^{W,H} is the convolution (output) of the CNN kernels with an input image x = {x_{i,j,c}}_{c=1}^{C} of C channels given by,\nA_{ij}^k := \u2211_{s_1,s_2,c}^{S,S,C} W_{s_1,s_2,c,k} \u22c5 x_{i+s_1-1,j+s_2-1,c} + b_k  (5)\nwhere W, H \u2208 N are the output image size, i = 1, ..., W and j = 1,..., H are the output pixel location on the kth channel and b_k is the associated bias.\nIt has been recognized that each filter serves a specific purpose to detect certain features, such as edges, textures, or patterns. The convolutional output Eq. (5) is also called the activation map to emphasize that certain input regions are highlighted and activated by the filters.\nOur proposed method, QGrad-CAM is designed to probe the importance of the activation maps {A_{ij}^k} via a VQC classifer with respect to the K filtered channels. Specifically, if a classifier f : R^{W,H,K} \u2192 R^m of m classes is welded after the CNN output {A_{ij}^k} (see Fig. 3) such that\nf(A) = (f^1(A),..., f^m(A)) \u2208 R^m  (6)"}, {"title": "IV. EXPLAINABILITY BY QUANTUM GRAD-CAM", "content": "where each f^l(A) is the prediction for class l = 1,..., m, then a weighting function w^k: R^{W,H,K} \u2192 R associated to classifier f can be defined,\nw_{ij}^k(A) := \\frac{1}{W H} \\frac{\\partial f^l(A)}{\\partial A_{ij}^k} / \u2211_{i,j}^{W,H} \\frac{\\partial f^l(A)}{\\partial A_{ij}^k} ,  (7)\nwith the gradients of the class predictions computed and averaged out. Consequently, the Grad-CAM heatmap is obtained by the composition of the ReLU function with a weighted sum of feature maps Eq. (5), (7),\n(Grad-CAM heatmap)_{ij} = ReLU( \u2211_k w_{ij}^k(A) \u22c5 A_{ij}^k )  (8)\nThe construction of the weighting function Eq. (7) naturally inherits important information for the final classification. Thus, it serves as a fundamental indicator for the resulting output.\nIt is our finding that the importance weighting w Eq. (7) can be explicitly computed in certain cases of VQC, and the role of each image channel can be understood from the perspective of VQC. In contrast to the classical network, QGrad-CAM gives rise to a certain degree of explainability and learning transparency, which is the key investigation of this study.\nConsider the quantum encoding depicted by Fig. 2,\nV(x) = \u2a02_{q=1}^n e^{ix_q k_q} \u2297 H_q  (9)\nwhere x = (x_1,...,x_n) \u2208 R^n is an input of VQC, H_q is any gate on the q^{th} qubit not related to x, such as the Hadamard gate, and \u03c3_{k_q} is a Pauli matrix of index k_q \u2208 {0,1,2,3} in P depending on the q^{th} qubit. Then the measurement of an observable Q \u2208 L(H) can be computed by a generalized form of Eq. (2) in terms of a density matrix \u03c1 \u2208 L(H),\n\u27e8Q\u27e9(x) = tr (Q U(\u03b8)V(x) \u03c1_0 V^\u2020(x) U^\u2020(\u03b8))  (10)\nwhere \u03c1_0 = |\u03c8_0\u27e9 \u27e8\u03c8_0|, U(\u03b8) is as Eq. (1) and tr is the trace operation on L(H). We calculate,\n\\frac{\\partial \u27e8Q\u27e9}{\\partial x_q}(x) = tr (Q U(\u03b8) \\frac{\\partial V(x)}{\\partial x_q} \u03c1_0 V^\u2020(x) U^\u2020(\u03b8)) + tr (Q U(\u03b8) V(x) \u03c1_0 \\frac{\\partial V^\u2020(x)}{\\partial x_q} U^\u2020(\u03b8))  (11)\nSince the differential of a tensor product x \u2192 A(x) \u2297 B(x) is defined as,\n\\frac{\\partial}{\\partial x} (A(x) \u2297 B(x)) = (\\frac{\\partial}{\\partial x} A(x)) \u2297 B(x) + A(x) \u2297 (\\frac{\\partial}{\\partial x} B(x))  (12)\nwe have,\n\\frac{\\partial}{\\partial x_q} V(x) = V(x_1,...,x_{q-1}, \u03c3_{k_q} \u2297 H_q,...,x_n)  (13)\nwhere we denote V_{k_q}(x_q) := e^{ix_q \u03c3_{k_q}} \u2297 H_q for simplicity. We expand a density matrix by the tensorial basis {\u03c3_{i_1},...,\u03c3_{i_n}} ,\n\u03c1_0 = \u2211_{i_1,..., i_n} C_{i_1...i_n} \u03c3_{i_1} \u2297\u00b7\u00b7\u00b7\u2297 \u03c3_{i_n}  (14)\nfor some coefficients C_{i_1...i_n} \u2208 C. Then Eq. (11) yields,\n\\frac{\\partial \u27e8Q\u27e9}{\\partial x_q}(x) = i/2 \u2211_{i_1,..., i_n} C_{i_1...i_n} tr (U(\u03b8)Q U(\u03b8) [(\u03c3_{k_q} \u2297 H_q) V(x_1) (\u03c3_{i_1}) V^\u2020(x_1) ...  (V_n(x_n) (\u03c3_{i_n}) V^\u2020(x_n) \u2297 (\u03c3_{k_q} \u2297 H_q))] )  (15)\nwhere the middle term contains a Lie bracket [A, B] := AB \u2212 BA at the q^{th} qubit. In fact, it can be explicitly computed,\n[\u03c3_{k_q}, V_{k_q}(x_q)] = i (V_{k_q}(x_q + \\frac{\u03c0}{2}) - V_{k_q}(x_q - \\frac{\u03c0}{2})) (16)\nTogether, Eq. (15) and (16) give us the explicit formula of the importance weighting Eq. (7) in the VQC case. This then reveals how a VQC views the image channels and makes important selections in the sense of Grad-CAM."}, {"title": "V. EXPERIMENT", "content": "QGrad-CAM is applied to three datasets: MNIST, Dogs vs. Cats, and the TIMIT corpus [39], each with its respective classification task. Training is conducted end-to-end on all parameters of a 3-layer CNN and a 4-block VQC, both initialized from scratch. The gradient is computed at the end of the VQC towards the last layer of the CNN to generate the results.\nImage classifications. The MNIST dataset consists of grayscale images sized 28 \u00d7 28, labeled across 10 classes. The Dogs vs. Cats dataset contains color images, resized to 128 \u00d7 128 for binary classification of dogs and cats.\nExample results of QGrad-CAM with MNIST and Dogs vs. Cats are demonstrated in Fig 4 and Fig 5. It is observed that the generated QGrad-CAMs effectively highlight the regions where key features of each class are located. For MNIST, the heatmaps generally focus on areas where lines turn and curves form, emphasizing angles and the smoothness or sharpness of lines as discriminative features. For the Dogs vs. Cats dataset, the VQC classifier seems to learn critical textures and contours unique to cats and dogs. The heatmaps successfully identify the discriminative regions used for categorization.\nSpeech classifications. TIMIT contains recordings of American English speakers. The samples are in the 16 kHz WAV format and converted by Short-Time Fourier Transform (STFT) into spectrograms as inputs. Random samples are selected and corrupted by helicopter noise into background to create two classes: with or without a noisy background.\nA spectrogram provides a visual representation of the frequency content of a speech signal over time. The horizontal axis represents time, the vertical axis represents frequency, and the color intensity indicates the amplitude at each frequency-time point. Human speech typically occupies specific frequency ranges in the spectrogram corresponding to the characteristics of the human vocal tract. In contrast, background noise often appears as diffuse, spread-out patterns across various frequencies and times without distinct features.\nOur results indicate that the network focuses on areas outside the speech utterance to determine whether an utterance is corrupted by noise. This is observed in Figures 6 and 7, where the heatmaps show lower intensity within the yellow rectangles. Rather than concentrating on the portions of the spectrogram containing the speech signal, the network examines the background areas to detect signs of noise, allowing for more accurate identification of corruption."}, {"title": "VI. CONCLUSION", "content": "Motivated by the structured nature of the quantum framework, this work introduces QGrad-CAM, a novel method for providing visual explanations for model decisions. Our approach integrates the VQC with CNN gradient techniques to generate detailed, class-specific image localization. Experimental results on both image and speech datasets demonstrate the method's effectiveness in highlighting discriminative features. Furthermore, an explicit importance weighting function associated to a VQC classifier can be analytically derived. Our results suggest potential advantages of quantum techniques in enhancing interpretability, highlighting the need for further exploration into the quantum advantage in this area."}]}