{"title": "Quantum Gradient Class Activation Map for Model\nInterpretability", "authors": ["Hsin-Yi Lin", "Huan-Hsin Tseng", "Samuel Yen-Chi Chen", "Shinjae Yoo"], "abstract": "Abstract-Quantum machine learning (QML) has recently\nmade significant advancements in various topics. Despite the\nsuccesses, the safety and interpretability of QML applications\nhave not been thoroughly investigated. This work proposes using\nVariational Quantum Circuits (VQCs) for activation mapping to\nenhance model transparency, introducing the Quantum Gradient\nClass Activation Map (QGrad-CAM). This hybrid quantum-\nclassical computing framework leverages both quantum and\nclassical strengths and gives access to the derivation of an\nexplicit formula of feature map importance. Experimental results\ndemonstrate significant, fine-grained, class-discriminative visual\nexplanations generated across both image and speech datasets.\nIndex Terms-Variational quantum circuits, quantum neural\nnetworks, gradient-based localization.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in quantum computing and machine learn-\ning (ML) have drawn significant attention, leading to efforts to\ncombine these two fascinating technologies. Although current\nquantum devices still face challenges due to noise and other\nimperfections, a hybrid quantum-classical computing frame-\nwork has been proposed to leverage the strengths of both\nquantum and classical computing [1]. Variational quantum\ncircuits (VQCs) serve as the foundational elements of this\nhybrid framework. Within this framework, computational tasks\nthat can benefit from quantum advantages are executed on\nquantum computers, while others are handled by classical\ncomputers. VQC-based algorithms have been shown to have\ncertain advantages over classical models [2]\u2013[4] and have\ndemonstrated success in various ML tasks, including classi-\nfication [5]\u2013[7], sequential modeling [8], audio and language\nprocessing [9]\u2013[11], and reinforcement learning [12]\u2013[14].\nDespite these successes, certain aspects of quantum machine\nlearning (QML) models have not been thoroughly investigated,\nparticularly concerning the safety of QML applications. Model\ninterpretability and transparency are crucial for understanding\nand ensuring proper use, especially due to extensive machine\nlearning applications and the continuing development of pol-\nicy and regulation. With the rapid development of quantum\ncomputing, it is important to explore whether any quantum\nadvantage could be offered in this direction.\nVarious approaches have been developed to address model\ninterpretability from different aspects, such as from feature\nimportance techniques [15] and rule-based methods [16].\nModel-agnostic methods provide local interpretability by ap-\nproximating complex models with simpler ones [17]\u2013[19].\nAmong diverse approaches, visual techniques stand out for\ntheir intuitive appeal. Methods such as Partial Dependence\nPlot (PDP) [20] and Individual Conditional Expectation (ICE)\n[21] visualize the relationship between a feature and the\npredicted outcome. Class Activation Mapping (CAM) [22] and\nits variants [23], [24] have emerged as powerful tools for gen-\nerating class-discriminative localization as visual explanations\nfor Convolutional Neural Networks (CNNs).\nIn this work, we propose using VQC for activation mapping\nas a pioneering exploration of quantum circuit applications for\nmodel transparency. Our proposed method, Quantum Gradient\nClass Activation Map (QGrad-CAM), employs a VQC to\nweigh the importance of activation maps generated from a\nCNN-based network.\nBased on the structure of VQC, we will derive the explicit\nformula for the importance of each activation map, which\ncan serve as an example of this advantage. Furthermore,\nexperiments are performed on image and speech datasets for\nvalidation. Meaningful highlighted regions are returned using\nthe proposed method for all cases.\nTo summarize, our main contributions include\n\u2022 Deriving an explicit formula for the VQC importance of\nactivation maps and visualize model decisions.\n\u2022 Conducting experiments to show the VQC can effectively\nweigh the importance of activation maps and provide\ntextual explanations for model decisions.\nThe rest of the paper is organized as follows. In Sec. II, we\npoint out relevant work in previous literature. Our proposed\nmethod, QGrad-CAM, is introduced in Sec. III. The details of\nQGrad-CAM are written in Sec. III-C after a VQC review and\ndiscussions of the critical ideas in Sec. III-A and Sec. III-B\nthat motivate our proposal. With the VQC structure, we derive\nthe explicit formula for the importance of feature maps in\nSec. IV. The experimental results can be found in Sec. V, and\nthe conclusions in Sec. VI."}, {"title": "II. RELATED WORK", "content": "Explaining QML Models. VQC-based QML methods have\ndemonstrated significant success in the domain of classifica-\ntion [5]. Noteworthy advancements include the development\nof Quantum Convolutional Neural Networks (QCNN) [6],\n[7], quantum transfer learning techniques [25], and hybrid\nmodels incorporating tensor networks [26]. Despite these\nachievements, the explainability of these models has not been\nthoroughly addressed. Several preliminary attempts have been\nmade to explain the predictions generated by QML models.\nFor instance, the study in [27] investigates feature importance\nwithin quantum Support Vector Machine (SVM) models using\nthe Iris dataset, which consists of only four features. The\ngeneralizability of this method to larger-scale datasets, such\nas image data or hybrid models combining classical NNs\nand VQCs, remains uncertain. Another approach, as outlined\nin [28], involves calculating the Shapley values for gates\nin VQCs to determine their respective contributions. The\nobjective of the methods presented in [28] is to rigorously\nevaluate the quality of various circuit architectures. In contrast,\nour proposed framework aims to uncover feature importance\ngiven a fixed VQC architecture using a scalable gradient-based\nmethod.\nVisualizing & CNN localization. CNNs have long demon-\nstrated exceptional performance across a wide range of ap-\nplications. This sparked extensive research aimed at under-\nstanding the underlying properties of CNNs. Several works\ndeveloped techniques for visualizing the CNN-learned latent\nrepresentation by, for example, analyzing convolution layers\n[29], [30] and inverting deep features [31]\u2013[33]. The research\nprompted the discovery of CNNs' ability to localize objects.\nCAM was proposed in [22], where CNN layers localize objects\nunsupervised and produce visual explanations for each class.\nDifferent pooling methods were explored in [34], [35] with a\nsimilar structure as CAM. Grad-CAM [23] generalized CAM\nby combining the class discriminative property with gradient\ntechniques. The method allowed fine-grained discriminative\nlocalization and was improved, especially for multiple in-\nstances scenarios in [24]."}, {"title": "III. QUANTUM GRADIENT CLASS-ACTIVATION MAP\n(QGRAD-CAM)", "content": "A. Variational Quantum Circuits\nVQCs also referred to as Parameterized Quantum Circuits\n(PQCs), are quantum circuits characterized by tunable pa-\nrameters that can be optimized based on specific metrics or\nsignals [5]. VQCs or PQCs serve as fundamental components\nof Quantum Neural Networks (QNNs), which underlie current\nQML techniques. The VQC as a QML method operates\nn qubits in space H \u2243 C2 \u2243 C2, where H is a\nHilbert space containing a standard basis written as \u03b2 =\n{|00...0), |00 . . . 1) , . . ., |11 \u00b7\u00b7\u00b71)} such that any quantum\nstate |\u03c8\u3009\u2208 H can be expanded by \u03b2, i.e.,\n|\u03c8\\rangle = c_{0} |00...0\\rangle + \\cdots + c_{N} |11...1\\rangle\nfor some coefficients c\u2081 \u2208 C with N = 2n. Let L(H) denotes\nall linear operators on H and U(H) be the collection of unitary\noperators in L(H). A VQC typically functions through the\nfollowing three steps (see Fig. 1),\n1) A quantum encoding V : R\u201d \u2192 U(H),\n2) A variational quantum gate U(0) \u2208 U(H) parameter-\nized by 0,\n3) A quantum measurement of Q as an output\u3008\u00b7|Q|\u00b7\u3009:\nH\u2192 R.\nAssume data is of classical form D = {(xj,Yj)|xj \u2208\nR^n, yj \u2208 R^m, j = 1,..., N} where xj is an input of sample\nindex j and y; be the corresponding label. A quantum encod-\ning scheme chooses a fixed gate sequence V : Rn \u2192 U(H) to\nconvert classical data into quantum states such that each input\ncorresponds to a unitary, xj \u2192 V(xj). One then associates\nthe quantum state |4x\u2081) := V(xj)|4o) to data xj up to a\nrandom initial |\u2030) \u2208 H. For instance, V(x) = e^{itan^{-1}(x)\\sigma_k}\ninjects data x \u2208 R into a 1-qubit space in a non-linearly\nfashion [5]. Typical choices of V include combinations of\nHadamard gates, CNOT gates, and gates generated by Pauli\nmatrices P = {1, \u03c31, \u03c32, \u03c33}. Subsequently, the encoded\nstate |x\u2081) is deformed by the parameterized gate U(0) such\nthat |x;} \u2192 U(0)|x\u2081), where 0 represents the learnable\nparameters subject to certain specific optimization routines. It\nis noted that the deformation ability of VQC majorly comes\nfrom U(0) where a convention is taking tensor products of the\n1-parameter subgroup generated by P,\nU (\u03b8) = \\prod_{l=1}^{k} e^{-i \u03b8_l \\sigma_q^{(l)}}C_l \\in U(H)\nwhere q = 1,...,n is the qubit index, l = 1,..., L is the\n(l)\nindex of variational circuit layers up to L with each \u03c3q \u2208P,\nC\u2097 is the unitary of all other non-parameterized gates such as\nCNOT gates etc, and \u03b8 = {(\u03b8\u2081(\u00b9),..., \u03b8\u2099\u207d\u00b9)),... , \u03b8\u2081(\u1d38),..., \u03b8\u2099\u207d\u1d38))}=1 \u2208 RnL denotes\nthe collection of all variational (learnable) parameters. Often\nthe 1-parameter subgroup \u03b8\u2192 e^{\u2212\u1f10\u03b8\u03c3_k} generated by \u03c3\u03ba \u0395\u03a1\nis denoted as {I, Rx(0), Ry(0), Rz(0)} correspondingly.\nA final measurement step selects m Hermitian opera-\ntors Q1,..., Qm (each Qi \u2208 L(H)) to collapse state\nU(0) |4x\u2081) and yields an m-dimensional output vector y =\n((Q1),..., (Qm)) by\n\\langle Q_i \\rangle := \\langle \\psi_0|V^\\dagger(x_j)U^\\dagger (\\theta) Q_i U (\\theta)V(x_j) |\\psi_0 \\rangle \\in \\mathbb{R}\nwith i = 1,...,m. Collectively, the three steps in VQC can\nbe written as fvQc : Rn \u2192 Rm transforming xj \u2194 fvQc(xj),\nsee Fig. 1. By writing fvqc,e we emphasize the dependency\non 0. We can drop the subscript @ when the context is clear."}, {"title": "B. Regularities of VQC compared to neural networks", "content": "Fully-connected networks are the building blocks of classi-cal networks which are of the form,\nf = f_n \\circ f_{n-1} \\circ \\cdots \\circ f_1\nwhere each layer fk(z) = \u03c3\u03ba(Wkz + bk) : Rlk\u22121 \u2192 Rlk is\ncomposed of an activation function \u03c3\u03ba, a bias vector bk \u2208 Rlk\nand a weight matrix Wk\u2208 L (Rlk-1, Rlk). Here L(A, B)\ndenotes the collection of linear maps between linear spaces A\nand B. Typically, there is no restriction on (classical network)\nweights Wk to be trained such that very often Wk is not\ninvertible even if dim A = dim B. This results in a problem\nwhere a network prediction y \u2208 Rm is hard to be traced back\nas W\u00b9(y) does not exist nor fr\u00b9(y) is well-defined due to\nthe structure of activation functions. Owing to this reason,\nclassical networks are called black boxes and thus lack certain\nregularity even though the prediction ability is powerful.\nOn the contrary, since VQCs are comprised of unitary\nmatrices, all gates U are invertible, and a quantum state\n|\u03c8> := U|Vo) is easily revertible by Yo\u3009 = U* |\u03c8). From\nthis point of view, VQC possesses certain transparency and\nbetter regularity.\nAdditionally, in view of matrix groups, variational gates\nU(0) in Eq. (1) form a Lie subgroup of GL(H), all invertible\nmatrices in L(H), so that {U(0)} naturally inherited smooth\nstructures from the differentiable submanifold [38]. This pro-\nvides us some hints that the VQC training may be more stable\nas the training iterations (\u03b8)(iter) \u2192 U()(iter)) being contained on\nthe smooth submanifold U(H). Motivated by these properties,\nit leads us to consider viewing the explainability of VQC via\na classical technique Grad-CAM."}, {"title": "C. Quantum Grad-CAM by VQC", "content": "Grad-CAM is a technique to interpret and visualize the deci-\nsions of CNNs via the gradient calculation of a target classifier.\nLet a set of CNN filters be {W_{s_1,s_2,c,k}}^{S,S,C,K}_{s_1=1,s_2=1,c=1,k=1} where C is the number\nof input channels and K is the number of output channels, and S is the kernel size. A feature map\n{A_{ij}^k}_{ij}^{W,H} is the convolution (output) of the CNN kernels\nwith an input image x = {x_{i,j,c}}_{c=1}^{C} of C channels given by,\nA_{ij}^k = \\sum_{s_1,s_2,c} W_{s_1,s_2,c,k} \\cdot x_{i+s_1-1,j+s_2-1,c} + b_k\nwhere W, \u0397 \u2208 N are the output image size, i = 1, ..., W and\nj = 1,..., H are the output pixel location on the kth channel\nand bk is the associated bias.\nIt has been recognized that each filter serves a specific\npurpose to detect certain features, such as edges, textures,\nor patterns. The convolutional output Eq. (5) is also called\nthe activation map to emphasize that certain input regions are\nhighlighted and activated by the filters.\nOur proposed method, QGrad-CAM is designed to probe the\nimportance of the activation maps {A_{ij}^k} via a VQC classifer\nwith respect to the K filtered channels. Specifically, if a\nclassifier f: R^{WHK} \u2192 Rm of m classes is welded after\nthe CNN output {A_{ij}^k} (see Fig. 3) such that\nf(A) = (f\u2081(A),..., f_m(A)) \u2208 R^m\nwhere each fl(A) is the prediction for class l = 1,..., m,\nthen a weighting function w: R^{WHK} \u2192 R associated to\nclassifier f can be defined,\n\\omega_{ij}^k = \\frac{1}{WH} \\sum \\frac{\\partial f_l(A)}{\\partial A_{ij}^k}\nwith the gradients of the class predictions computed and aver-\naged out. Consequently, the Grad-CAM heatmap is obtained\nby the composition of the ReLU function with a weighted sum\nof feature maps Eq. (5), (7),\n(Grad-CAM heatmap)_{ij} = ReLU(\\sum_k \\omega_{ij}^k \\cdot A_{ij}^k )\nThe construction of the weighting function Eq. (7) naturally\ninherits important information for the final classification. Thus,\nit serves as a fundamental indicator for the resulting output."}, {"title": "IV. EXPLAINABILITY BY QUANTUM GRAD-CAM", "content": "It is our finding that the importance weighting w Eq. (7)\ncan be explicitly computed in certain cases of VQC, and\nthe role of each image channel can be understood from the\nperspective of VQC. In contrast to the classical network,\nQGrad-CAM gives rise to a certain degree of explainability\nand learning transparency, which is the key investigation of\nthis study.\nConsider the quantum encoding depicted by Fig. 2,\nV(x) = \\prod_{q=1}^n e^{ix_q \\sigma_{k_q}} \\otimes H_q\nwhere x = (x1,...,xn) \u2208 Rn is an input of VQC, Hq is any\ngate on the qth qubit not related to x, such as the Hadamard\ngate, and oka is a Pauli matrix of index kq \u2208 {0,1,2,3} in\nP depending on the qth qubit. Then the measurement of an\nobservable Q \u2208 L(H) can be computed by a generalized form\nof Eq. (2) in terms of a density matrix p\u2208 L(H),\n\\langle Q \\rangle(x) = tr (QU(\\theta)V(x) \\rho_0 V^\\dagger(x)U^\\dagger(\\theta))\nwhere po = |Vo) & (\u03c8o], U(v) is as Eq. (1) and tr is the trace\noperation on L(H). We calculate,\n\\frac{\\partial \\langle Q \\rangle}{\\partial x_q} (x) = tr (QU(\\theta) \\frac{\\partial V(x)}{\\partial x_q} \\rho_0 V^\\dagger(x) U^\\dagger(\\theta))\n+ tr (QU(\\theta)V(x) \\rho_0 \\frac{\\partial V^\\dagger(x)}{\\partial x_q} U^\\dagger(\\theta))\nSince the differential of a tensor product x \u2192 A(x) & B(x)\nis defined as,\n\\frac{\\partial}{\\partial x_q} (A(x) \\otimes B(x)) = (\\frac{\\partial}{\\partial x_q} A(x)) \\otimes B(x)+A(x) \\otimes (\\frac{\\partial}{\\partial x_q} B(x))\nwe have,\n\\frac{\\partial}{\\partial x_q} V(x) = V_{k_1} (x_1) \\cdots V_{k_{q-1}} (x_{q-1}) \\frac{\\partial}{\\partial x_q} V_{k_q} (x_q) V_{k_{q+1}} (x_{q+1}) \\cdots V_{k_n} (x_n)\nwhere we denote Vkq(xq):= e^{ix_q \\sigma_{k_q} } for simplicity. We\nexpand a density matrix by the tensorial basis {\u03c3i\u2081 \u2297\u00b7\u00b7\u00b7\u2297\u03c3\u03af\u03b7},\n\\rho_0 = \\sum_{i_1,..., i_n} C_{i_1...i_n} \\sigma_{i_1} \\otimes \\cdots \\otimes \\sigma_{i_n}\nfor some coefficients Ci\u2081...in \u2208 C. Then Eq. (11) yields,\n\\frac{\\partial \\langle Q \\rangle}{\\partial x_q} (x) = \\frac{i}{2} \\sum_{i_1,..., i_n} C_{i_1...i_n} tr(U(\\theta)QU(\\theta)\\times \\cdots \\otimes \\langle V_n(x_n)| \\sigma_{i_n} |V_n (x_n) \\rangle \\times \\cdots )\n\\otimes ([\\sigma_{k_q}, V_{k_q} (x_q)]\\otimes (\u00b7\u00b7\u00b7))\\cdots )\nwhere the middle term contains a Lie bracket [A, B] := AB \u2013\nBA at the qth qubit. In fact, it can be explicitly computed,\n[\\sigma_{k_q}, V_{k_q} (x_q)] = i (V_{k_q} (x_q+\\frac{\\pi}{2}) - V_{k_q} (x_q-\\frac{\\pi}{2}) )\nTogether, Eq. (15) and (16) give us the explicit formula of\nthe importance weighting Eq. (7) in the VQC case. This then\nreveals how a VQC views the image channels and makes\nimportant selections in the sense of Grad-CAM."}, {"title": "V. EXPERIMENT", "content": "QGrad-CAM is applied to three datasets: MNIST, Dogs vs.\nCats, and the TIMIT corpus [39], each with its respective\nclassification task. Training is conducted end-to-end on all\nparameters of a 3-layer CNN and a 4-block VQC, both\ninitialized from scratch. The gradient is computed at the end\nof the VQC towards the last layer of the CNN to generate the\nresults.\nImage classifications. The MNIST dataset consists of\ngrayscale images sized 28 \u00d7 28, labeled across 10 classes.\nThe Dogs vs. Cats dataset contains color images, resized to\n128 \u00d7 128 for binary classification of dogs and cats.\nSpeech classifications. TIMIT contains recordings of\nAmerican English speakers. The samples are in the 16 kHz\nWAV format and converted by Short-Time Fourier Transform\n(STFT) into spectrograms as inputs. Random samples are\nselected and corrupted by helicopter noise into background\nto create two classes: with or without a noisy background.\nA spectrogram provides a visual representation of the\nfrequency content of a speech signal over time. The hori-\nzontal axis represents time, the vertical axis represents fre-\nquency, and the color intensity indicates the amplitude at\neach frequency-time point. Human speech typically occupies\nspecific frequency ranges in the spectrogram corresponding\nto the characteristics of the human vocal tract. In contrast,\nbackground noise often appears as diffuse, spread-out patterns\nacross various frequencies and times without distinct features.\nOur results indicate that the network focuses on areas\noutside the speech utterance to determine whether an utterance\nis corrupted by noise. This is observed in Figures 6 and 7,\nwhere the heatmaps show lower intensity within the yellow\nrectangles. Rather than concentrating on the portions of the\nspectrogram containing the speech signal, the network exam-\nines the background areas to detect signs of noise, allowing\nfor more accurate identification of corruption."}, {"title": "VI. CONCLUSION", "content": "Motivated by the structured nature of the quantum frame-\nwork, this work introduces QGrad-CAM, a novel method\nfor providing visual explanations for model decisions. Our\napproach integrates the VQC with CNN gradient techniques\nto generate detailed, class-specific image localization. Experi-\nmental results on both image and speech datasets demonstrate\nthe method's effectiveness in highlighting discriminative fea-\ntures. Furthermore, an explicit importance weighting function\nassociated to a VQC classifier can be analytically derived. Our\nresults suggest potential advantages of quantum techniques\nin enhancing interpretability, highlighting the need for further\nexploration into the quantum advantage in this area."}]}