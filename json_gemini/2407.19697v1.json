{"title": "Multiscale Representation Enhanced Temporal Flow Fusion Model for Long-Term Workload Forecasting", "authors": ["Shiyu Wang", "Zhixuan Chu", "Yinbo Sun", "Yu Liu", "Yuliang Guo", "Yang Chen", "Huiyang Jian", "Lintao Ma", "Xingyu Lu", "Jun Zhou"], "abstract": "Accurate workload forecasting is critical for efficient resource management in cloud computing systems, enabling effective scheduling and autoscaling. Despite recent advances with transformer-based forecasting models, challenges remain due to the non-stationary, nonlinear characteristics of workload time series and the long-term dependencies. In particular, inconsistent performance between long-term history and near-term forecasts hinders long-range predictions. This paper proposes a novel framework leveraging self-supervised multiscale representation learning to capture both long-term and near-term workload patterns. The long-term history is encoded through multiscale representations while the near-term observations are modeled via temporal flow fusion. These representations of different scales are fused using an attention mechanism and characterized with normalizing flows to handle non-Gaussian/non-linear distributions of time series. Extensive experiments on 9 benchmarks demonstrate superiority over existing methods.", "sections": [{"title": "1 INTRODUCTION", "content": "With the continuous expansion of cloud computing, efficient resource management has become a crucial issue for cloud systems [13, 14, 30, 49]. Accurately predicting future workloads is essential for effective resource scheduling [29, 36]. For microservices systems, request per second (RPS) is their main service capacity metric, which quantifies their workload [20, 40]. Therefore, we achieve workload forecasting for microservices by predicting RPS. In recent years, major cloud service providers have successively launched their own resource scaling service frameworks, such as Google's Autopilot [37], Microsoft's FIRM [34], and Amazon's AWS Autoscaling, which adopt different workload forecasting methods. Autopilot uses ARIMA [1, 28], FIRM uses historical statistical methods, and AWS adopts DeepAR [38], among others. Especially recently, with the increasing complexity of cloud computing services, there has been widespread attention and research on forecasting long-term workloads time series [2, 17, 20, 25, 27, 42]. Notably, a plethora of transformer-based methods, such as Informer [54], Autoformer [22], Fedformer [55], and PatchTST [31], have emerged in abundance. Notwithstanding, it is imperative to highlight that current research confronts the following challenges due to the complex characteristics of time series:\n\u2022 Challenge 1: The multi-periodic, non-stationary, and long-term dependencies. In long-term forecasting, workloads time series data is often high-frequency and non-stationary with complex multi-periodic characteristics, such as hourly, daily, and weekly cycles. Furthermore, these data exhibit long-term dependent properties. Hence, capturing the multi-periodic and long-term characteristics of time series data is critical in addressing the challenges of long-term forecasting.\n\u2022 Challenge 2: The non-Gaussian/non-linear characteristics of workloads time series that have not been adequately characterized. Time series data often exhibits non-Gaussian/non-linear distributions, which previous studies have predominantly assumed to be Gaussian. This assumption has made it challenging for these studies to adapt to the real distribution of time series data.\n\u2022 Challenge 3: The inconsistency between the long-term history of time series and near-term observations. The significant fluctuations and changes in time series data often result in inconsistent performance between long-term history and near-term observations. While long-term history"}, {"title": null, "content": "exhibits complex multi-periodic characteristics, near-term observations show a rapidly changing trend. Therefore, integrating the characteristics of both long-term history and near-term observations is crucial for accurate long-term forecasting.\nRecently, significant progress has been made in representation learning, particularly in the area of self-supervised contrastive learning [3-7, 18]. Harness representation learning to capture the long-term dependency and multi-periodic characteristics of time series data is indeed a potent way. Undoubtedly, this introduces a fresh approach to tackling the aforementioned challenges. Unfettered by the limitations of the original end-to-end model, we can devise a novel learning paradigm for long-term time series forecasting.\nIn this work, we propose a two-stage framework that consists of a pretraining representation stage and a fusion prediction stage. During the pretraining representation stage, we pre-train a multiscale time series representation model using a contrastive learning method in both time and frequency domains. This model extracts representations of different scales from the long-term history of the time series to characterize long-term dependencies and complex multi-periodic patterns [11]. In the fusion prediction stage, a temporal flow fusion model captures the changing trends in near-term observations. The multiscale representations of the long-term history and the nearby observations are fused through a FusionAttention module employing a multi-head attention mechanism. Furthermore, we use normalizing flow to model the non-Gaussian/non-linear properties of the time series. Our approach produces accurate predictions that capture both long-term patterns and near-term trends.\nOur Contributions can be summarized as follows:\n\u2022 This paper presents a novel long-term workload forecasting framework unifying multiscale time series representation learning and temporal flow fusion modeling to capture both long-term historical patterns and near-term trends. This unified approach leads to superior prediction accuracy.\n\u2022 We propose an original multiscale representation method applying contrastive learning in time and frequency domains to encode long-term dependencies and multi-periodic patterns from long-term history.\n\u2022 We conducted extensive experiments on nine benchmarks, and our method achieved consistent state-of-the-art performance. Furthermore, we have conducted large-scale deployment in the real-world as a cornerstone of workload forecasting in the Alipay cloud resource management system."}, {"title": "2 RELATED WORK", "content": "Time Series (TS) Forecasting. Due to the immense importance of time series forecasting, various models have been well developed. In recent years, a variety of time series forecasting models, particularly those based on deep learning methods, have become increasingly popular [8, 21, 39, 41, 43, 47, 48, 56]. These models have introduced many novel structures and have outperformed classical models such as ARIMA and VAR. Informer [54] is a prob-sparse self-attention mechanism-based model to enhance the prediction capacity in long-sequence TS forecasting. Autoformer [46] is a decomposition architecture that incorporates the series decomposition block as an inner operator. Fedformer [55] is a decomposed Transformer architecture that utilizes a mixture of experts for seasonal-trend decomposition and is enhanced with frequency information. Non-stationary Transformers [26] is to enhance the predictability of time series while maximizing the model's predictive capacity. PatchTST [31] is an effective design of Transformer-based models for time series forecasting tasks by introducing two key components: patching and channel-independent structure.\nTime Series (TS) Representation. Representation learning has recently achieved great success in advancing TS research by characterizing the long temporal dependencies and complex periodicity based on the contrastive method [12, 16, 19, 52, 53]. TS2Vec [50] was recently proposed as a universal framework for learning TS representations by performing contrastive learning in a hierarchical loss over augmented context views. COST [45] proposed a new TS representation learning framework for long-sequence TS forecasting, which applies contrastive learning methods to learn disentangled seasonal-trend representations. TST [51] is a newly developed framework that utilizes the transformer encoder architecture for multivariate time series representation learning. LaST [44] utilizes variational inference to separate seasonal-trend representations in the latent space. TimeMAE [9] is a novel self-supervised paradigm for learning transferrable time series representations based on transformer networks\nNormalizing flow. Normalizing flows(NF) [23], which learn a distribution by transforming the data to samples from a tractable distribution where both sampling and density estimation can be efficient and exact, have been proven to be powerful density approximations [32, 33, 35]. NF are invertible neural networks that typically transform isotropic Gaussians to fit a more complex data distribution [23]. They map from RD to RD such that densities py on the input space Y \u2208 RD are transformed into some tractable distribution pz (e.g., an isotropic Gaussian) on space Z\u2208 RD. This mapping function, f: Y \u2192 Z, and inverse mapping function, f-1:Z\u2192Y is composed of a sequence of bijections or invertible functions, and we can express the target distribution densities py (y) by\n$p_y(y) = p_z(z)\\left| \\det\\left(\\frac{df(y)}{dy}\\right) \\right|,$ (1)\nwhere $\\frac{df(y)}{dy}$ is the Jacobian of f at y.\nFor mapping function f, we can employ RealNVP [15] architecture, which is a neural network composed of a series of parametrized invertible transformations with a lower triangular Jacobian structure and vector component permutations in order to capture complex dependencies. It leaves the part of its inputs unchanged and transforms the other part via functions of the un-transformed variables (with superscript denoting the coordinate indices)\n$\\begin{cases}\ny_{1:d} = x_{1:d} \\\ny_{d+1:D} = x_{d+1:D} \\odot \\exp(s(x_{1:d}) +t(x_{l:d}))\n\\end{cases}$ (2)\nwhere $\\odot$ is an element wise product, s() is a scaling and t() a translation function from RD \u2192 RD-d, using neural networks."}, {"title": "3 METHODOLOGY", "content": "Due to the high-frequency and non-stationary nature of the long-term time series (TS), along with their complex long temporal dependencies spanning daily, weekly, monthly, and quarterly periodicities, it becomes challenging to memorize historical data and learn these dependencies for backcasting. In light of this, we propose a method for representing complex historical TS as compressed vectors and storing them in a TS database. To achieve accurate predictions, we design a Temporal Flow Fusion Model that integrates these long-term historical TS representations with near-term observations from nearby windows."}, {"title": "3.1 Multiscale Time Series (TS) Representation", "content": "Given TS y \u2208 RT\u00d7F with backcast window h, our goal is to learn a non-linear embedding function fe that maps {yt-h...yt} to its representation r\u2081 = [r\u012b, rf], where rt \u2208 RK is for each time stamp t, r\u012b \u2208 RK\u012b is the time domain representation, rf \u2208 RKF denotes that of frequency domain and K = KT + KF is the dimension of representation vectors. In the encoding representation stage, by using backcast windows of various lengths, we can obtain a representation of different scales.\nIn Figure 2, we can observe the generation of representation using a schematic view. This process initiates with the random sampling of two overlapping subseries from the input time series, which is then followed by individual data augmentation for each subseries. Subsequently, for the input projection layer, we employ Multilayer Perceptron (MLP), and the original input yt is mapped into a high-dimensional latent vector z\u0142. To generate an augmented context view, we employ timestamp masking to mask latent vectors at randomly selected timestamps. The contextual embeddings at each timestamp are then extracted using the CovnTrans backbone encoder. We further extract trends in the time domain and periods in the frequency domain using CausalConv and Fast Fourier transform (FFT), respectively, from the contextual embeddings. In the end, we carry out contrastive learning in both the time and frequency domains.\nIn the subsequent sections, we provide a detailed description of each of these components.\nRandom Cropping is a popular data augmentation technique used in contrastive learning for generating new context views. We can randomly sample two overlapping time segments [a1, a2] and [b1, b2] from TS y \u2208 RT\u00d7F that satisfy 0 < a\u2081 < b\u2081 < a2 < b2 \u2264 T. Note that contextual representations on the overlapped segment [b1, a2] ensure consistency for two context views.\nTimestamp Masking aims to produce an augmented context view by randomly masking the timestamps of a TS. We can mask off the latent vector z = {z} after the Input Projection Layer along the time axis with a binary mask m \u2208 {0, 1}T, the elements of which are independently sampled from a Bernoulli distribution with p = 0.5. Backbone Encoder is used to extract the contextual representation at each timestamp. Our selected Backbone Encoder is the 1-layer causal convolution Transformer (ConvTrans), which leverages convolutional multi-head self-attention to capture long- and short-term dependencies.\nSpecifically, given TS y \u2208 RT\u00d7F, ConvTrans transforms y (as input) into dimension I via dilated causal convolution layer as follows:\n$Q = DilatedConv(y)$,\n$K = DilatedConv(y)$,\n$V = DilatedConv(y)$,\nwhere Q \u2208 Rdlxdh, K \u2208 Rdl\u00d7dh, and V \u2208 Rdl\u00d7dh (we denote the length of time steps as dh). After these transformations, the scaled dot-product attention computes the sequence of vector outputs via:\n$S = Attention(Q, K, V) = softmax\\left(\\frac{Q K^T}{\\sqrt{d_{KM}}}\\right) V,$\nwhere the mask matrix M can be applied to filter out right-ward attention (or future information leakage) by setting its upper-triangular elements to -\u221e and normalization factor dk is the dimension of W matrix. Finally, all outputs S are concatenated and linearly projected again into the next layer. After the above series of operations, we use this backbone fe to extract the contextual embedding (intermediate representations) at each timestamp as r = fo (y).\nTime Domain Contrastive Learning. A straightforward approach for extracting the underlying trend of a time series (TS) is to employ a collection of 1d causal convolution layers (CasualConv) with varying kernel sizes, along with an average-pooling operation to generate the representations, as shown below:\n$\\tilde{r}^{(T,i)} = CausalConv(\\tilde{r}, 2^i)$,\n$r^T = AvgPool(\\tilde{r}^{(T,1)}, \\tilde{r}^{(T,2)} ..., \\tilde{r}^{(T,L)})$, where L is a hyper-parameter denoting the number of CasualConv, 2i (i = 0, ..., L) is the kernel size of each CasualConv, \u0159 is above intermediate representations from the backbone encoder, followed by average-pool over the L representations to obtain time-domain representation (T). To learn discriminative representations over time, we use the time domain contrastive loss, which takes the"}, {"title": null, "content": "representations at the same timestamp from two views of the input TS as positive samples (r), while those at different timestamps from the same time series as negative samples, formulated as\n$L_{time} = -log \\frac{\\exp(r_i^t \\cdot r_{i'})}{\\sum_{t' \\in T} (\\exp(r_i^t \\cdot r_{i'}) + I(t \\ne t') \\exp(r_i^t \\cdot r_{j}^{t'}))}$, where T is the set of timestamps within the overlap of the two subseries, subscript i is the index of the input TS sample, and t is the timestamp.\nFrequency Domain Contrastive Learning. Since spectral analysis has proven to be effective in detecting periods, we utilize Fast Fourier Transforms (FFT) to convert the intermediate representations mentioned above to the frequency domain. This allows us to identify various periodic patterns. By combining the FFT and MLP techniques, we can create a period extractor that extracts the frequency spectrum from the contextual embedding and translates it into the freq-based representation rf.\nWe implement the frequency domain contrastive loss with an index of (i, t) across the batch instances to train the representations to distinguish between various periodic patterns. The formula for this loss is as follows:\n$L_{Freq} = -log \\frac{\\exp(r_i^t \\cdot r_{i'})}{\\sum_{j \\in D} (\\exp(r_i^t \\cdot r_{j}^{t}) + I(i \\ne j) \\exp(r_i^t \\cdot r_{j}^{t}) )}$, where D is defined as a batch of TS. We use freq-based representations of other TS at timestamp t in the same batch as negative samples.\nThe contrastive loss is composed of two losses that are complementary to each other and is defined as\n$L = \\frac{1}{|D|T} (L_{time} + L_{Freq}),$ (3)\nwhere D denotes a batch of TS. As previously noted, we pre-train our TS representation model. During the encoding representation phase, we use backcast windows of varying lengths to produce representations at different scales. In this study, we apply this method to encode high-frequency TS data to generate long-term historical TS representations at daily, weekly, monthly, and quarterly intervals."}, {"title": "3.2 Temporal Flow Fusion Model", "content": "In this section, we will provide a detailed overview of the Temporal Flow Fusion Model. The values of TS will be denoted as yt \u2208 R, where t represents the time index within the horizon of t \u2208 1, 2, ..., T. It should be noted that we define x\u2081 as covariates that are known in the future, such as time features and ID features, at time step t.\nGiven the last L observations yt-L, ..., yt, the TS forecasting task aims to predict the future N observations Yt+1, ..., Yt+N. We use rt, the representation of the long-term historical TS, and ht as the context latent vector from near-term observations (nearby window), to predict future observations. Specifically, our Temporal Flow Fusion Model is structured into three steps, as follows:\nFirst, loading the TS representation and extracting near-term observation features. To begin, we load the Multiscale TS representation rt from the TS database, which includes daily, weekly, monthly, and quarterly representations. These representations capture various periods and complex long-term temporal dependencies. Next, we apply Recurrent Neural Networks (RNN) to encode short-term observations (within a near-term window) into a context latent vector ct following Equation 4. This vector captures the nearby window changing pattern of long-term time series data.\n$c_t = RNN(Y_{t-L:t}, X_{t-L:t}; \\Theta),$ (4)\nwhere is the learnable parameters of the RNN.\nSecond, fusing the long-term representation with the near-term observation. After integrating the Multiscale TS representations rt and the context latent vector ct into the same dimension using MLP, we construct the FusionAttention employing a multi-head self-attention module, inspired by the transformer architecture, to fuse the long-term historical TS representations and the near-term observations within the nearby window. This fusion module enhances our ability to make accurate predictions by capturing both long-term dependencies and short-term changes within a day.\n$h_t = FusionAttention(c_t, r_t; \\Phi),$ (5)\nwhere I is the learnable parameters of the FusionAttention.\nThird, employing conditional NF to generate the distribution of the future TS and make predictions. Ultimately, we employ the conditional normalizing flow to approximate the probability density of the TS data. Subsequently, we utilize an RNN as the decoder to facilitate autoregressive decoding and accomplish multi-step prediction.\nTo estimate the probability density of data (in order to obtain a probabilistic forecast), one straightforward method is to use parameterized Gaussian distribution, but as mentioned above, the real-world hierarchical TS data are mostly non-Gaussian/non-linear. Equipped with the powerful density approximator, NF, we are able to tackle this challenge, capturing the nonlinear relationships among the TS data.\nNote that we extend the traditional normalizing flow (NF) to a conditional normalizing flow (CNF). Specifically, we still adopt the Real-NVP architecture, but we extend Equation 2 by concatenating condition ht to both the inputs of the scaling and translation function approximators of the coupling layers as follows:\nIt should be noted that we expand the conventional normalizing flow to a conditional normalizing flow. More specifically, we maintain the Real-NVP architecture while augmenting Equation 2"}, {"title": null, "content": "through the concatenation of condition ht to the inputs of the scaling and translation function approximators of the coupling layers, as following below:\n$\\begin{cases}\ny_{1:d} = z_{1:d} \\\ny_{d+1:D} = z_{d+1:D} \\odot \\exp(s(z_{1:d},h) + t(z_{1:d},h))\n\\end{cases}$ (6)\nwhere z is a noise vector sampled from an isotropic Gaussian, functions s (scale) and t (translation) are usually deep neural networks, which as mentioned above, do not need to be invertible.\nTo obtain an expressive distribution representation, we can stack K layers of conditional flow modules (Real-NVP), generating the conditional distribution of the future sequences of all TS, given the past time t \u2208 [t \u2013 L, t). Specifically, it can be written as a product of factors (as an autoregressive model):\n$p(\\tilde{y}_{t:T}/Y_{t-L:t}, X_{t-L:t}; \\Theta, \\Phi,\\Psi) = \\prod_{t=t+1}^T p(\\tilde{y}_t|h_t; \\Psi),$ (7)\nwhere \u1ef9t is the future predictions and Y is the parameter of conditional NF.\nIn the training, given D, defined as a batch of TS Y := {Y1, Y2, ..., YT }, the representation of the long-term historical TS as rt, and the associated covariates X := {x1, x2, ..., XT}, we can derive the likelihood as:\n$L = \\frac{1}{|D|T} \\sum_{x_{1:T}, y_{1:T} \\in D} \\prod_{t=1}^T P(Y_t|Y_{1:t}; X_{1:t}, 'r_t, \\Theta, \\Phi, \\Psi),$ (8)\nwhere is the learnable parameters of the RNN and I is the parameter of FusionAttention, and y is the parameter of contitional NF."}, {"title": "4 EXPERIMENTS", "content": "We conduct extensive experiments to evaluate the performance of our method on long-term forecasting including 9 real-world benchmarks including 8 well-known datasets in time series and a large real-world workload dataset (RPS data of 1589 microservices from Alipay). Furthermore, We have conducted thorough comparisons with 10 well-acknowledged and advanced baselines."}, {"title": "4.1 Benchmarks", "content": "For long-term forecasting, we conduct the experiments on 9 well-established benchmarks: ETT datasets (including 4 subsets: ETTh1, ETTh2, ETTm1, ETTm2) [54], Solar-Energy [24], Weather, Electricity, and Traffic [46], as well as the large Service-Workload datasets. More information is provided in the Appendix."}, {"title": "4.2 Baselines", "content": "We compared our method with 10 advanced baselines. These methods can be divided into two categories: end-to-end forecasting models (including PatchTST [31], FEDformer [55], Non-stationary Transformer [26], Autoformer [46], Informer [54]) and time series representation models (including TimeMAE [9], LaST [44], TST[51], COST [45], TS2VEC [50]). The detailed descriptions and implementations of experiments are provided in the Appendix. Regarding metrics, we utilize the mean square error (MSE) and mean absolute error (MAE) for long-term forecasting."}, {"title": "4.3 Main Results", "content": "As shown in Table 1, our method achieves consistent state-of-the-art performance across all 9 benchmarks, outperforming 10 advanced baselines. Particularly noteworthy is that compared to the second-best method, our approach achieved a 14% increase in Electricity, 32% in Solar-Energy, 18% in Traffic, and 26% in Workload datasets, highlighting the superiority of our method on complex datasets. Furthermore, we achieved the best performance even on datasets with low forecastability such as ETT and Solar-Energy datasets. We also calculated the standard deviation and conducted statistical significance tests on all datasets in Table 2, and achieved the best performance with a confidence level of over 95% (over 99% in most cases)."}, {"title": "4.4 Model Analysis", "content": "Ablations. To confirm the effectiveness of each component in our approach, we conducted detailed ablations on every possible design within the Multiscale TS representation, FusionAttention, and Conditional NF modules. The following findings can be observed from Table 3 above:\n\u2022 The exclusion of the Multiscale TS representation module during the ablation study resulted in a notable decline in performance for longer prediction horizons, particularly for prediction horizons of 336 and 720. This finding shows the effectiveness of our proposed method for representing long-term historical information in time series, precisely the essential multi-periodic and long-term dependent characteristics inherent in long-term histories that are crucial for accurate long-term forecasting.\n\u2022 The exclusion of the FusionAttention module led to a reduction in performance for all prediction horizons, underscoring the critical role of properly fusing long-term representations and near-term observations. The findings indicate that accurate long-term forecasting cannot be achieved without an appropriate fusion mechanism, even with long-term historical representations.\n\u2022 The exclusion of the Conditional NF module decreased the performance, emphasizing the crucial challenge posed by the widespread non-Gaussia/non-linear characteristics present in time series forecasting. This effect was particularly evident in the ablation study, where the performance degradation became more pronounced with an increase in prediction length. These findings highlight the significance of characterizing non-Gaussian/non-linear to achieve accurate long-term forecasting.\nRepresentation analysis. In order to facilitate a better analysis of the Multiscale TS representation, we employ visualizations to"}, {"title": null, "content": "provide an intuitive understanding of the TS representation. As seen in Figure 4, our TS representation successfully characterizes long-term temporal dependencies across different periods. We can intuitively see the dependencies between the weekly periods from the representation. Moreover, the TS representation also mines complex nested periods, e.g., the weekly period contains the daily period as shown in the figure. In Figure 5, we plot the visualization of the workload TS representations of different windows, and we can intuitively observe the changes in periods and trends from these representations. Even for TS which does not possess any observable periodic characteristic, our representation can still reveal its corresponding pattern of variations. This attests to the efficacy of our TS representation in capturing long-term temporal dependencies across time periods. Furthermore, we conducted a visual analysis of the clustering of representations, as shown in Figure 6. We can observe that our representations are capable of distinguishing different types of time series in detail. This is evident in both the two-dimensional and three-dimensional clustering visualizations, which demonstrate the effectiveness of our multiscale TS representation.\nForecast results showcases. To evaluate the prediction of different models, we plot the last dimension of forecasting results that are from the test set of the workload TS dataset for qualitative comparison in Figure 7. Among the various models, our method exhibits superior performance."}, {"title": "5 APPLICATION", "content": "Alipay, a leading global mobile payment company, has established its presence in various domains including digital life and digital finance [10]. With an expansive cloud computing infrastructure supporting countless microservices, efficient allocation of cloud resources is critical for Alipay's cluster management. Alipay relies on its proprietary predictive autoscaling technology as the fundamental component of its cloud resource management system. The accurate prediction of future workloads for each microservice, specifically in terms of requests per second (RPS), is crucial for the effective scaling of server resources. Presently, our proposed approach has been extensively deployed in Alipay's production cloud environment, achieving remarkable results. The outcomes, as demonstrated in Table 1, indicate a significant improvement of 26% in comparison to the state-of-the-art (SOTA) methods in the real-world environment. Figure 8 demonstrates the utilization of our autoscaling method based on workload forecasting, which enables the Alipay cloud resource management system to anticipate changes in future workloads through long-term forecasting when traffic fluctuates. This facilitates the scaling of computing resources to allocate those appropriate for the current workload. Upon activating our prediction technique, the number of pods (containers that host microservices) utilized by the microservices has significantly decreased from 1500 to less than 500 (Reduced resource consumption by 67%), leading to a substantial improvement in resource utilization efficiency."}, {"title": "6 CONCLUSION", "content": "This paper presents a novel framework for accurate long-term workload forecasting by incorporating a multiscale time series representation method and a temporal flow fusion model. The framework uniquely captures both long-term historical patterns and near-term observations in the workload time series for superior predictions. To our knowledge, this is the first deep integration of multiscale representation learning and deep forecast modeling for time series. Comprehensive evaluations on 9 benchmarks demonstrate consistent state-of-the-art performance over 10 advanced baselines. Furthermore, when deployed on a real-world cloud resource management system with over a thousand sets of microservices, significant improvements in scheduling and resource management are achieved. This end-to-end framework successfully leverages multiscale representations and temporal fusion to advance the capability of Al systems for long-term time series forecasting."}]}