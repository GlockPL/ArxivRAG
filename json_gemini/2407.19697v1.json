{"title": "Multiscale Representation Enhanced Temporal Flow Fusion Model for Long-Term Workload Forecasting", "authors": ["Shiyu Wang", "Zhixuan Chu", "Yinbo Sun", "Yu Liu", "Yuliang Guo", "Yang Chen", "Huiyang Jian", "Lintao Ma", "Xingyu Lu", "Jun Zhou"], "abstract": "Accurate workload forecasting is critical for efficient resource management in cloud computing systems, enabling effective scheduling and autoscaling. Despite recent advances with transformer-based forecasting models, challenges remain due to the non-stationary, nonlinear characteristics of workload time series and the long-term dependencies. In particular, inconsistent performance between long-term history and near-term forecasts hinders long-range predictions. This paper proposes a novel framework leveraging self-supervised multiscale representation learning to capture both long-term and near-term workload patterns. The long-term history is encoded through multiscale representations while the near-term observations are modeled via temporal flow fusion. These representations of different scales are fused using an attention mechanism and characterized with normalizing flows to handle non-Gaussian/non-linear distributions of time series. Extensive experiments on 9 benchmarks demonstrate superiority over existing methods.", "sections": [{"title": "1 INTRODUCTION", "content": "With the continuous expansion of cloud computing, efficient resource management has become a crucial issue for cloud systems [13, 14, 30, 49]. Accurately predicting future workloads is essential for effective resource scheduling [29, 36]. For microservices systems, request per second (RPS) is their main service capacity metric, which quantifies their workload [20, 40]. Therefore, we achieve workload forecasting for microservices by predicting RPS. In recent years, major cloud service providers have successively launched their own resource scaling service frameworks, such as Google's Autopilot [37], Microsoft's FIRM [34], and Amazon's AWS Autoscaling, which adopt different workload forecasting methods. Autopilot uses ARIMA [1, 28], FIRM uses historical statistical methods, and AWS adopts DeepAR [38], among others. Especially recently, with the increasing complexity of cloud computing services, there has been widespread attention and research on forecasting long-term workloads time series [2, 17, 20, 25, 27, 42]. Notably, a plethora of transformer-based methods, such as Informer [54], Autoformer [22], Fedformer [55], and PatchTST [31], have emerged in abundance. Notwithstanding, it is imperative to highlight that current research confronts the following challenges due to the complex characteristics of time series:\n\u2022 Challenge 1: The multi-periodic, non-stationary, and long-term dependencies. In long-term forecasting, workloads time series data is often high-frequency and non-stationary with complex multi-periodic characteristics, such as hourly, daily, and weekly cycles. Furthermore, these data exhibit long-term dependent properties. Hence, capturing the multi-periodic and long-term characteristics of time series data is critical in addressing the challenges of long-term forecasting.\n\u2022 Challenge 2: The non-Gaussian/non-linear characteristics of workloads time series that have not been adequately characterized. Time series data often exhibits non-Gaussian/non-linear distributions, which previous studies have predominantly assumed to be Gaussian. This assumption has made it challenging for these studies to adapt to the real distribution of time series data.\n\u2022 Challenge 3: The inconsistency between the long-term history of time series and near-term observations. The significant fluctuations and changes in time series data often result in inconsistent performance between long-term history and near-term observations. While long-term history"}, {"title": "2 RELATED WORK", "content": "Time Series (TS) Forecasting. Due to the immense importance of time series forecasting, various models have been well developed. In recent years, a variety of time series forecasting models, particularly those based on deep learning methods, have become increasingly popular [8, 21, 39, 41, 43, 47, 48, 56]. These models have introduced many novel structures and have outperformed classical models such as ARIMA and VAR. Informer [54] is a prob-sparse self-attention mechanism-based model to enhance the prediction capacity in long-sequence TS forecasting. Autoformer [46] is a decomposition architecture that incorporates the series decomposition block as an inner operator. Fedformer [55] is a decomposed Transformer architecture that utilizes a mixture of experts for seasonal-trend decomposition and is enhanced with frequency information. Non-stationary Transformers [26] is to enhance the predictability of time series while maximizing the model's predictive capacity. PatchTST [31] is an effective design of Transformer-based models for time series forecasting tasks by introducing two key components: patching and channel-independent structure.\nTime Series (TS) Representation. Representation learning has recently achieved great success in advancing TS research by characterizing the long temporal dependencies and complex periodicity based on the contrastive method [12, 16, 19, 52, 53]. TS2Vec [50] was recently proposed as a universal framework for learning TS representations by performing contrastive learning in a hierarchical loss over augmented context views. COST [45] proposed a new TS representation learning framework for long-sequence TS forecasting, which applies contrastive learning methods to learn disentangled seasonal-trend representations. TST [51] is a newly developed framework that utilizes the transformer encoder architecture for multivariate time series representation learning. LaST [44] utilizes variational inference to separate seasonal-trend representations in the latent space. TimeMAE [9] is a novel self-supervised paradigm for learning transferrable time series representations based on transformer networks\nNormalizing flow. Normalizing flows(NF) [23], which learn a distribution by transforming the data to samples from a tractable distribution where both sampling and density estimation can be efficient and exact, have been proven to be powerful density approximations [32, 33, 35]. NF are invertible neural networks that typically transform isotropic Gaussians to fit a more complex data distribution [23]. They map from RD to RD such that densities py on the input space Y \u2208 RD are transformed into some tractable distribution pz (e.g., an isotropic Gaussian) on space Z\u2208 RD. This mapping function, f: Y \u2192 Z, and inverse mapping function, f-1:Z\u2192Y is composed of a sequence of bijections or invertible functions, and we can express the target distribution densities py (y) by\n$p_y (y) = p_z(z)\\det(\\frac{df(y)}{dy})\\,$\n(1)\nwhere df (y)/dy is the Jacobian of f at y.\nFor mapping function f, we can employ RealNVP [15] architecture, which is a neural network composed of a series of parametrized invertible transformations with a lower triangular Jacobian structure and vector component permutations in order to capture complex dependencies. It leaves the part of its inputs unchanged and transforms the other part via functions of the un-transformed variables (with superscript denoting the coordinate indices)\n$\\begin{cases}y_{1:d} = x_{1:d}\\\\y_{d+1:D} = x_{d+1:D} \\odot exp(s(x_{1:d}) +t(x_{l:d}))\\end{cases}$       (2)\nwhere $\\odot$ is an element wise product, s() is a scaling and t() a translation function from RD \u2192 RD-d, using neural networks."}, {"title": "3 METHODOLOGY", "content": "Due to the high-frequency and non-stationary nature of the long-term time series (TS), along with their complex long temporal dependencies spanning daily, weekly, monthly, and quarterly periodicities, it becomes challenging to memorize historical data and learn these dependencies for backcasting. In light of this, we propose a method for representing complex historical TS as compressed vectors and storing them in a TS database. To achieve accurate predictions, we design a Temporal Flow Fusion Model that integrates these long-term historical TS representations with near-term observations from nearby windows.\n3.1 Multiscale Time Series (TS) Representation\nGiven TS y \u2208 RT\u00d7F with backcast window h, our goal is to learn a non-linear embedding function fe that maps {yt-h...yt} to its representation r\u2081 = [r\u012b, rf], where rt \u2208 RK is for each time stamp t, r\u012b \u2208 RK\u012b is the time domain representation, rf \u2208 RKF denotes that of frequency domain and K = KT + KF is the dimension of representation vectors. In the encoding representation stage, by using backcast windows of various lengths, we can obtain a representation of different scales.\nIn Figure 2, we can observe the generation of representation using a schematic view. This process initiates with the random sampling of two overlapping subseries from the input time series, which is then followed by individual data augmentation for each subseries. Subsequently, for the input projection layer, we employ Multilayer Perceptron (MLP), and the original input yt is mapped into a high-dimensional latent vector z\u0142. To generate an augmented context view, we employ timestamp masking to mask latent vectors at randomly selected timestamps. The contextual embeddings at each timestamp are then extracted using the CovnTrans backbone encoder. We further extract trends in the time domain and periods in the frequency domain using CausalConv and Fast Fourier transform (FFT), respectively, from the contextual embeddings. In the end, we carry out contrastive learning in both the time and frequency domains.\nIn the subsequent sections, we provide a detailed description of each of these components.\nRandom Cropping is a popular data augmentation technique used in contrastive learning for generating new context views. We can randomly sample two overlapping time segments [a1, a2] and [b1, b2] from TS y \u2208 RT\u00d7F that satisfy 0 < a\u2081 < b\u2081 < a2 < b2 \u2264 T. Note that contextual representations on the overlapped segment [b1, a2] ensure consistency for two context views.\nTimestamp Masking aims to produce an augmented context view by randomly masking the timestamps of a TS. We can mask off the latent vector z = {z} after the Input Projection Layer along the time axis with a binary mask m \u2208 {0, 1}T, the elements of which are independently sampled from a Bernoulli distribution with p = 0.5.\nBackbone Encoder is used to extract the contextual representation at each timestamp. Our selected Backbone Encoder is the 1-layer causal convolution Transformer (ConvTrans), which leverages convolutional multi-head self-attention to capture long- and short-term dependencies.\nSpecifically, given TS y \u2208 RT\u00d7F, ConvTrans transforms y (as input) into dimension I via dilated causal convolution layer as follows:\n$Q = DilatedConv(y)$\n$K = DilatedConv(y),$\n$V = DilatedConv(y)$\nwhere Q \u2208 Rdl\u00d7dh, K \u2208 Rdl\u00d7dh, and V \u2208 Rdl\u00d7dh (we denote the length of time steps as dh). After these transformations, the scaled dot-product attention computes the sequence of vector outputs via:\n$S = Attention(Q, K, V) = softmax (QK^T /\\sqrt{d_K}M) V,$\nwhere the mask matrix M can be applied to filter out right-ward attention (or future information leakage) by setting its upper-triangular elements to -\u221e and normalization factor dk is the dimension of W matrix. Finally, all outputs S are concatenated and linearly projected again into the next layer. After the above series of operations, we use this backbone fe to extract the contextual embedding (intermediate representations) at each timestamp as r = fo (y).\nTime Domain Contrastive Learning. A straightforward approach for extracting the underlying trend of a time series (TS) is to employ a collection of 1d causal convolution layers (CasualConv) with varying kernel sizes, along with an average-pooling operation to generate the representations, as shown below:\n$\\tilde{r}^{(T,i)} = CausalConv(\\tilde{r}, 2^i)$\nrT = AvgPool($\\tilde{r}^{(T,1)}, \\tilde{r}^{(T,2)} ..., \\tilde{r}^{(T,L)})$,$\nwhere L is a hyper-parameter denoting the number of CasualConv, 2i (i = 0, ..., L) is the kernel size of each CasualConv, \u0159 is above intermediate representations from the backbone encoder, followed by average-pool over the L representations to obtain time-domain representation $\\tilde{r}^{(T)}$. To learn discriminative representations over time, we use the time domain contrastive loss, which takes the"}, {"title": "3.2 Temporal Flow Fusion Model", "content": "Since spectral analysis has proven to be effective in detecting periods, we utilize Fast Fourier Transforms (FFT) to convert the intermediate representations mentioned above to the frequency domain. This allows us to identify various periodic patterns. By combining the FFT and MLP techniques, we can create a period extractor that extracts the frequency spectrum from the contextual embedding and translates it into the freq-based representation rf.\nWe implement the frequency domain contrastive loss with an index of (i, t) across the batch instances to train the representations to distinguish between various periodic patterns. The formula for this loss is as follows:\n$L_{Freq} = -log\\frac{exp(r_i^t\\cdot r_i^t)}{E_{j\\in D} (exp(r_i^t\\cdot r_j^t) + I(i \\neq j)exp(r_i^t \\cdot r_j^{t'}))}$       , where D is defined as a batch of TS. We use freq-based representations of other TS at timestamp t in the same batch as negative samples.\nThe contrastive loss is composed of two losses that are complementary to each other and is defined as\n$L = \\frac{1}{|D|T} (L_{time} + L_{Freq}),$\n(3)\nwhere D denotes a batch of TS. As previously noted, we pre-train our TS representation model. During the encoding representation phase, we use backcast windows of varying lengths to produce representations at different scales. In this study, we apply this method to encode high-frequency TS data to generate long-term historical TS representations at daily, weekly, monthly, and quarterly intervals.\nIn this section, we will provide a detailed overview of the Temporal Flow Fusion Model. The values of TS will be denoted as yt \u2208 R, where t represents the time index within the horizon of t \u2208 1, 2, ..., T. It should be noted that we define x\u2081 as covariates that are known in the future, such as time features and ID features, at time step t.\nGiven the last L observations yt-L, ..., yt, the TS forecasting task aims to predict the future N observations Yt+1, ..., Yt+N. We use rt, the representation of the long-term historical TS, and ht as the context latent vector from near-term observations (nearby window), to predict future observations. Specifically, our Temporal Flow Fusion Model is structured into three steps, as follows:\nFirst, loading the TS representation and extracting near-term observation features. To begin, we load the Multiscale TS representation rt from the TS database, which includes daily, weekly, monthly, and quarterly representations. These representations capture various periods and complex long-term temporal dependencies. Next, we apply Recurrent Neural Networks (RNN) to encode short-term observations (within a near-term window) into a context latent vector ct following Equation 4. This vector captures the nearby window changing pattern of long-term time series data.\n$c_t = RNN(Y_{t-L:t}, X_{t-L:t}; \\Theta),$\n(4)\nwhere is the learnable parameters of the RNN.\nSecond, fusing the long-term representation with the near-term observation. After integrating the Multiscale TS representations rt and the context latent vector ct into the same dimension using MLP, we construct the FusionAttention employing a multi-head self-attention module, inspired by the transformer architecture, to fuse the long-term historical TS representations and the near-term observations within the nearby window. This fusion module enhances our ability to make accurate predictions by capturing both long-term dependencies and short-term changes within a day.\n$h_t = FusionAttention(c_t, r_t; \\Phi),$\n(5)\nwhere I is the learnable parameters of the FusionAttention.\nThird, employing conditional NF to generate the distribution of the future TS and make predictions. Ultimately, we employ the conditional normalizing flow to approximate the probability density of the TS data. Subsequently, we utilize an RNN as the decoder to facilitate autoregressive decoding and accomplish multi-step prediction.\nTo estimate the probability density of data (in order to obtain a probabilistic forecast), one straightforward method is to use parameterized Gaussian distribution, but as mentioned above, the real-world hierarchical TS data are mostly non-Gaussian/non-linear. Equipped with the powerful density approximator, NF, we are able to tackle this challenge, capturing the nonlinear relationships among the TS data.\nNote that we extend the traditional normalizing flow (NF) to a conditional normalizing flow (CNF). Specifically, we still adopt the Real-NVP architecture, but we extend Equation 2 by concatenating condition ht to both the inputs of the scaling and translation function approximators of the coupling layers as follows:\nIt should be noted that we expand the conventional normalizing flow to a conditional normalizing flow. More specifically, we maintain the Real-NVP architecture while augmenting Equation 2"}, {"title": "4 EXPERIMENTS", "content": "We conduct extensive experiments to evaluate the performance of our method on long-term forecasting including 9 real-world benchmarks including 8 well-known datasets in time series and a large real-world workload dataset (RPS data of 1589 microservices from Alipay). Furthermore, We have conducted thorough comparisons with 10 well-acknowledged and advanced baselines.\n4.1 Benchmarks\nFor long-term forecasting, we conduct the experiments on 9 well-established benchmarks: ETT datasets (including 4 subsets: ETTh1, ETTh2, ETTm1, ETTm2) [54], Solar-Energy [24], Weather, Electricity, and Traffic [46], as well as the large Service-Workload datasets. More information is provided in the Appendix.\n4.2 Baselines\nWe compared our method with 10 advanced baselines. These methods can be divided into two categories: end-to-end forecasting models (including PatchTST [31], FEDformer [55], Non-stationary Transformer [26], Autoformer [46], Informer [54]) and time series representation models (including TimeMAE [9], LaST [44], TST[51], COST [45], TS2VEC [50]). The detailed descriptions and implementations of experiments are provided in the Appendix. Regarding metrics, we utilize the mean square error (MSE) and mean absolute error (MAE) for long-term forecasting."}, {"title": "5 APPLICATION", "content": "Alipay, a leading global mobile payment company, has established its presence in various domains including digital life and digital finance [10]. With an expansive cloud computing infrastructure supporting countless microservices, efficient allocation of cloud resources is critical for Alipay's cluster management. Alipay relies on its proprietary predictive autoscaling technology as the fundamental component of its cloud resource management system. The accurate prediction of future workloads for each microservice, specifically in terms of requests per second (RPS), is crucial for the effective scaling of server resources. Presently, our proposed approach has been extensively deployed in Alipay's production cloud environment, achieving remarkable results. The outcomes, as demonstrated in Table 1, indicate a significant improvement of 26% in comparison to the state-of-the-art (SOTA) methods in the real-world environment. Figure 8 demonstrates the utilization"}, {"title": "6 CONCLUSION", "content": "This paper presents a novel framework for accurate long-term workload forecasting by incorporating a multiscale time series representation method and a temporal flow fusion model. The framework uniquely captures both long-term historical patterns and near-term observations in the workload time series for superior predictions. To our knowledge, this is the first deep integration of multiscale representation learning and deep forecast modeling for time series. Comprehensive evaluations on 9 benchmarks demonstrate consistent state-of-the-art performance over 10 advanced baselines. Furthermore, when deployed on a real-world cloud resource management system with over a thousand sets of microservices, significant improvements in scheduling and resource management are achieved. This end-to-end framework successfully leverages multiscale representations and temporal fusion to advance the capability of Al systems for long-term time series forecasting."}]}