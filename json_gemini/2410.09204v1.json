{"title": "Encoding Agent Trajectories as Representations with Sequence Transformers", "authors": ["Athanasios Tsiligkaridis", "Nicholas Kalinowski", "Zhongheng Li", "Elizabeth Hou"], "abstract": "Spatiotemporal data faces many analogous challenges to natural language text including the ordering of locations (words) in a sequence, long range dependencies between locations, and locations having multiple meanings. In this work, we propose a novel model for representing high dimensional spatiotemporal trajectories as sequences of discrete locations and encoding them with a Transformer-based neural network architecture. Similar to language models, our Sequence Transformer for Agent Representation Encodings (STARE) model can learn representations and structure in trajectory data through both supervisory tasks (e.g., classification), and self-supervisory tasks (e.g., masked modelling). We present experimental results on various synthetic and real trajectory datasets and show that our proposed model can learn meaningful encodings that are useful for many downstream tasks including discriminating between labels and indicating similarity between locations. Using these encodings, we also learn relationships between agents and locations present in spatiotemporal data.", "sections": [{"title": "INTRODUCTION", "content": "In the modern world, geospatial mobility data has become increasingly available with the proliferation of mobile devices and other positioning and sensor technologies [22, 38]. These technologies have also allowed wildlife researchers to monitor and collect data on animal movements and their ecosystems [4, 18]. This increasing availability of location datasets has been leveraged by researchers allowing them to build models that further their understanding of mobility patterns [3, 14], trafficking, and infectious disease [12, 24] (in both humans and animals). Inspired by the successes of neural networks in other fields (e.g., vision, language), researchers have leveraged their flexible architecture and applied them to trajectory data to model human and animal mobility patterns [17, 32].\nThe sequential nature of trajectory data shares many similar properties with the sequential nature of natural language. Tokenizing sentences or text segments in Natural Language Processing (NLP) is essentially a mapping from a complex \"quasi-infinite\" space to a sequence of discrete elements in a finite vocabulary. In a similar fashion to rule-based or algorithmic tokenizers [11, 26], the \"quasi-continuous\" high frequency GPS coordinates in trajectory data can also be tokenized into a lower dimensional sequence with a finite vocabulary. Also, just as tokens in language settings have semantic meaning (e.g., [fir] refers to a coniferous tree), tokens in mobility data can also have inherent meaning about the location they represent (e.g., (45.832119, 6.865575) is in the French alps). The order of the tokens in both domains also contains key information as language tokens can modify each other and location tokens can have different implicit meanings depending on their order (e.g., visiting a supermarket before work makes it a breakfast location whereas after work makes it a grocery shop). We can further extend our analogy between NLP and trajectory data to the \"document\" level where just as sentences in the same document will share similar properties such as word choice, grammar, and style, trajectories collected from the same agent or user will also share similar orderings of locations or routes. Thus, we leverage these numerous similarities by proposing the Sequence Transformer for Agent Representation Encodings (STARE) model, a neural network with an encoder-based transformer architecture similar to those used in BERT-like language models [8, 31]."}, {"title": "1.1 Related Work", "content": "Other works have also observed the connection between the sequential nature of trajectory data and language or other sequential domains. Transformers and other sequence deep-learning based approaches have been explored as potential frameworks for various trajectory-related tasks. Earlier works, [7, 27, 35], focused on more simple architectures, such as RNN and LSTM models, which were successfully employed to solve destination and trajectory prediction tasks.\nAs improvements to these models, transformer-based architectures emerged due to their robust attention mechanisms that allow for sequence dependency modeling without consideration of element distances along with their ability to be efficiently trained. Thus, later works leveraged the power of transformer-style architectures for a wide range of geospatial-related tasks. [1, 9, 30] focused on forecasting trajectories for next destination prediction, while [23] augmented information and incorporated it into a transformer network to predict vehicle trajectories in urban scenarios. A transformer decoder-based architecture was proposed to predict next intended locations of users given contextual information [10], a spatio-temporal contextual transformer fused with external features was used to segment imperfect historical maps [33], and a graph transformer was used for point of interest recommendation and mobility modeling [34]. The trajectory recovery task was studied using a road network graph-aware transformer-based model for capturing spatiotemporal information from low sample trajectories and a multi-task decoder model for point generation [6]. A unified framework based on BERT has been explored as a means of solving a plethora of trajectory-related problems, such as imputation, classification, prediction, etc. [19]. Some of these transformer-based architectures also added significant amounts of side information, in the form of a contextual block [29] or multiple feature extractors that incorporate a points of interest ontology and a social network [36]. Instead of just having a similar architecture, [16] and [37] extended pre-trained large language models to take in pseudo-sentences that represent locations in natural language, effectively performing an NLP task.\nIn contrast to the architecture presented in [29], our approach solely requires time-stamped spatiotemporal data, carries out a different form of pre-processing, and considers various training regimes (i.e., label classification, masked token modeling). In contrast to the architecture presented in [39] which has separate spatial and temporal transformers and a cross attention module between them, we have spatio-temporal transformers to allow for more shared information between the space and time domains. With this, our STARE model has the flexibility to be used in a variety of settings where only location and time data is present, such as with various animal trajectory datasets and in environments without contextual and/or foundational information. Our model simply takes in only spatiotemporal data and ultimately learns important encodings that can be leveraged for various downstream tasks (e.g., classification, destination prediction, clustering) and for learning recurring behaviors, relationships between, and Patterns of Life (PoL) of various agents of interest."}, {"title": "1.2 Contributions Of Our Work", "content": "In this paper, we make the following contributions. First, we present a data discretization technique for reducing the dimensionality of long and rich agent PoL data. Second, we propose a novel transformer-based architecture for obtaining informative data embeddings which can be used to learn relationships between agents and locations. Finally, we present extensive experiments on both simulated and real trajectory datasets and showcase our proposed STARE model's informative embeddings along with improved performance over baseline sequence encoder models (i.e., LSTM, BILSTM) in regards to classification accuracy. We also present novel experiments leveraging the transformer-based architecture of STARE to also learn the intrinsic patterns within the sequences themselves; specifically, we learn relationships between agents and between locations that these agents frequent."}, {"title": "STARE MODEL", "content": "In this section, we describe our STARE model, which compresses raw trajectory data into novel tokenized sequences for input into a Transformer Encoder Stack (TES). Our architecture is similar to that of [29], but we specifically focus on the minimal data setting where we solely have sequence information as data (i.e., we do not use a contextual block in our input as we do not incorporate any non-sequential information in our model). The aim of STARE is to discretize raw trajectory data for a TES to learn encodings that have semantic meaning and, in turn, can be used to both predict labels of interest with a Multi-Layer Perceptron (MLP) and also learn interesting relationships between observed data."}, {"title": "2.1 Data Discretization Methodology", "content": "We begin with a dataset X containing N observations of a tuple containing the latitude, longitude, and timestamp of each agent for A total agents. To form multiple samples per agent and to reduce the size of our model architecture, we make independence assumptions on the temporal component by assuming that timepoints within a time window are dependent with each other, but independent of those outside the window. Explicitly, we partition $X_a$, the trajectory of agent a, into a set of M sub-trajectories, $\\{X_1, X_2, ..., X_M\\}$ where each sub-trajectory is the length of some time window (e.g. 6 hours, a day, or a week). The choice of the time window length is dataset dependent, but it uses the implicit assumption that repetitive behavior is expected and independent of each other. For example, humans tend to have cycles of the same behavior over days (e.g., people live in one place, wake up, go to work, do some activities throughout the day, and then return home to end their day) with some seasonality and anomalies. These independence assumptions are very typical in the sequential domain and parallel the breaking of text into multiple samples between sentences in NLP.\nFor a given agent $a \\in [1, ..., A]$ and a time window $m\\in [1, ..., M]$, we define a trajectory $T_{a,m}$ as:\nDefinition 1 (Trajectory $T_{a,m}$) For a time window m, agent a has a raw trajectory $T_{a,m}$ that is defined as a sequence of $L_{a,m}$ time-stamped locations: $T_{a,m} = [P_1,..., P_{L_{a,m}}]$, where each point $P_i = [lat_i, lon_i, t_i]$, is a tuple of latitude, longitude, and time, respectively, that identifies the geographic location of that point $p_i$ at time $t_i$."}, {"title": null, "content": "The number of data points in $T_{a,m}$ can be large and varying between different agents and time windows due to uneven measurement sampling rates and durations. Since locations are defined by their quasi-continuous latitude and longitude values, there is an infinitesimally large number of unique locations. To combat these issues, we discretize the data inputs by only retaining information about an agent's Persistent Locations (PL) and the times they spend in these PLs.\nSpecifically, we define a PL as a stationary point in a trajectory that is mapped to a discrete value in an alphabet, where we define this alphabet to be S2 cells (a hierarchy of indexed spatial cells that represent geographical areas over the world) at a certain zoom level [25]. We can also discretize the time spent in PLs by defining another alphabet that consists of multiples of some time measure, (e.g., minutes, hours) and rounding the PL times to the nearest multiple. By including the amount of time spent in a PL, we can represent a sequence of points whose latitude and longitude values lie in the same S2 cells in a more information-compact form (e.g. [A, A, A, A] = A, 4 where A represents a S2 cell hash and 4 is the amount of time spent in A for the corresponding time measure).\nLet s : [\u03a6, \u039b] \u2192 $2 be a mapping from latitude, longitude space to our S2 cell alphabet and dt: R+ \u2192 T be a mapping from the positive real space of PL times to our discrete alphabet of rounded times. Thus, the concatenated sub-sequences of visited PLs and time spent in them form a sequence sample, $x_{a,m}$, that is a discretized and compressed version of $T_{a,m}$:\n$x_{a,m} = [[BOS], s(T_{a,m}), [SEP], dt(T_{a,m}), [EOS]]$,\nwhere the [BOS], [EOS], [SEP] tokens, borrowed from NLP, represent beginning, end, and separating tokens, respectively."}, {"title": "2.2 STARE Model Architecture", "content": "Evidently, our data discretization technique is a means of tokenizing our input to be passed into our Transformer-based model. Let X be the inputs to the transformer model (i.e., our tokenized sequences split up according to some time window m), and let Y be the encoded label associated with X (e.g., the ID of the agent). The Transformer-based model then learns a non-linear embedding function f(X) that produces an encoded representation of the sequence in continuous space $R^K$ where K is the dimensionality of this space. In order to provide a supervisory signal to train our model and learn this encoded representation, we have two training schemes or \"task\" heads to put on top of the transformer backbone function f(X). The use of two different training schemes allows us to learn various"}, {"title": null, "content": "things about our data; specifically, the first task head allows us to learn relationships between agents while the second one allows us to learn relationships between locations that these agents visit.\nThe first type of task head is a classifier c(.) (multi-layer perceptron) whose goal is to learn the conditional distribution P(Y|X) which is the inverse of the traditional assumed data generating distribution P(XY). We encode this information into one of the special tokens ([BOS] is standard practice) and minimize the cross-entropy loss:\nmin L(Y, \u0176), where \u0176 = c(f(X)0),\nand where the first element of the encoded representation f(X)o is a K-dimensional vector and corresponds to the [BOS] token.\nThe second type of task head is a \"decoder\" (linear layer without a bias term) that performs masked location modelling and learns the marginal distribution of sequences P(X) by learning to predict the patterns with the sequences. Masked location modelling is a self-supervised training scheme where at every iteration, a random percentage of the inputs are masked as X'. The goal of the model is to learn encoded representations of the locations within each of the sequences such that they can be \"decoded\". So for every masked token in the input sequence, the cross-entropy loss is calculated with respect to the un-masked token:\nmin L(Xi, $X_i$), where $\\hat{X} = d(f(X')_i) \\forall i \\in Masked$,\nwhere d() is the decoder function and the Masked set is a random subset of the location tokens. We do not mask the time tokens because while they provide supplementary information about the locations in the context of different agent PoLs, they do not have an intrinsic meaning on their own. We also do not mask the special tokens because they are meaningless; although we note that by encoding the classification representation in a special token, these two tasks are complementary and can be performed together."}, {"title": "2.3 Benefits of STARE Model", "content": "Through the use of either a classification or a masked location modeling task, our STARE model is able to learn relationships in data. By learning agent labels through classification, we can learn agent relationships. By learning tokens through a masked location token modeling task, we can learn relationships between locations. To our knowledge, we are the first to apply the masked location modeling task to spatiotemporal data.\nOur STARE model takes as input a data sequence, which is a concatenation of both location and time subsequences, and processes it; in contrast, these subsequences can be processing separately, like in the manner presented in [39], which uses separate spatial and temporal transformers. In our approach, we choose to have a unifying model that concatenates spatial and temporal subsequences with a separating token, thus creating more synergies because all submodules in each encoder block (e.g. self-attention mechanisms, feedforward networks) will be a function of all the spatiotemporal information. Using two separate models can potentially break relationships in data or not discover them, specifically in places where they do not cross attend.\nAdditionally, our STARE model is designed to be a simple architecture; we have a flexible and general model that can be applied to any spatiotemporal data. Our choice yields a few benefits:\nWe can apply our model to scenarios where road networks do not exist or are impractical (e.g. animal or other object mobility data). By not relying on road network information, we also prevent biases in our model from the road information being potentially out of date or inaccurate, whereas the road network intrinsically learned by our model will be correct for the time when the data was collected.\nWe have the flexibility to pretrain our model on massive amounts of trajectory data and then fine-tune it for specific tasks such as classifying agents in particular areas of interest (similar to how large language models are trained).\nOur model is relatively easy to implement and does not require significant additional engineering to \u201cmake it work\u201d for slightly different data scenarios, thus allowing for greater reproducibility. It is also less likely to be fragile to new data as it relies on learning structures within the data instead of being engineered to have certain structures."}, {"title": "3 SIMULATION & REAL DATA EXPERIMENTS", "content": "For our experiments, we mainly focus on datasets containing rich PoL trajectory information for various entities (i.e., humans, animals). We aim to extract relationships between these entities and/or locations they frequent in their PoLs, so we make the implicit assumption that there exists a set of relationships in the data that we seek to identify. Due to this, we do not focus on standard trajectory datasets (i.e., taxi trajectory12, small-scale human pedestrian mobility trajectories [15]) as there might not be any latent relationships in the observed trajectories of these datasets."}, {"title": "3.2 Metrics and Interpretations", "content": "In every dataset that we consider, we have spatiotemporal trajectories that are labeled as belonging to a specific entity (i.e., an agent, animal). Using these labels, we can carry out a label prediction task, and we can also do masked location prediction by masking random location tokens in our input sequences. With either training regimen, we can obtain standard performance metrics (i.e., accuracy, precision, recall, F1-score, etc.); but, in our setting, we mainly care about presenting relationships that are learned in the data (i.e., between entities and locations), so we will only present accuracy metrics when discussing performance metrics. We showcase our primary focus on the relationships that are learned by our model by presenting a slew of informative visuals and interpretations that highlight the interesting relations that our STARE model learns."}, {"title": "3.3 Simulated Data Experiments", "content": "We simulate trajectories with a generative model that approximately aligns with our modeling assumptions for the STARE model. Specifically, our generative model is parameterized by a series of PLs associated with each agent. We also make the following assumptions in order to have realistic behaviors for the human agents:\nThey often: 1) have reoccurring behaviors in the same locations (e.g., go to the same house every day), 2) live and behave similarly to others (i.e., there is some sharing of PLs between humans), and 3) travel along roads between their PLs.\nThus, the process follows a hidden Markov model (HMM) backbone where the states are specific locations extracted from various foundational data sources [20, 21, 28] and fall into the following categories: home, work, food, and gym locations. Each agent has its own random transition matrix with non-zero entries for a subset of the states (e.g., each agent only has one unique state it can transition to for its home and work locations). Additionally, in order to encourage more realistic group and social behaviors, agents' transition matrices are generated hierarchically with a grouping parameter such that agents that share the same group (i.e., belong to the same subpopulation) are assigned houses nearby each other or share the same office location. Finally, in order to create high frequency, realistic trajectories, we interpolate between the states using an Open Street Maps (OSM) road network from the OSMnx python package, [5]. Two examples of multiple generated trajectories that belong to the same subpopulation are shown in Appendix"}, {"title": null, "content": "All datasets we consider have spatiotemporal information, i.e. latitude, longitude, and timestamp. Our model is specifically constructed for this type of data as opposed to generic time series data which can have any types of features over time. This allows us to:\n(1) Tokenize the time series in a meaningful way that allows us to capture continuous valued latitude, longitude coordinates in a finite vocabulary based on the spatial nature of the data. Thus, when performing masked location modeling, we can learn the meaning of these spatial tokens with respect to others (e.g., token A and token B will be highly correlated if they are frequented by the same agents).\n(2) Reduce the sequence length of the time series by only keeping tokens representing PLs allowing for each sequence to represent a much longer window of time (e.g., 1 day)."}, {"title": "3.4 \"Massive Trajectory Data Based on Patterns of Life\" Experiments", "content": "We also apply our STARE model to a more complex simulated dataset from [2], which contains movement data for 10,000 agents in 3 metropolitan areas (Atlanta, New Orleans, and George Mason University) for 15 months. These experiments allow us to show the generalizability of STARE to other datasets where the generating assumptions do not perfectly align with our modeling assumptions and to showcase STARE's ability to scale to large-scale, long duration datasets.\nInterestingly, although the data sampling rate of this dataset is every 5 minutes, the agents tend to move in a jumpy manner. However, because our STARE model only needs stationary points and the times spent at these points, we can still tokenize each agent's trajectories into daily sequences in a similar fashion as in the previous experiment. We drop the first month of data as suggested by [2], who indicates that it contains non-converged \"warm-up\" data from their simulator.\nWe trained our STARE model using agent labels with a larger architecture (4 encoder stacks, instead of 2) and the same train/test split percentages used in the previous set of experiments. These models took roughly 14 to 16 hours to train for 2000 epochs and their test accuracies are shown in the first row of Table 3. We additionally compared against similarly sized LSTM/BiLSTM baseline models."}, {"title": "3.5 Wildlife Animal Movement Data Experiments", "content": "To assess the effectiveness of extracting meaningful encodings with STARE in real scenarios, we applied it to a raw trajectory dataset of ravens [13]. We only used trajectory data from 49 ravens between March and July for both 2018 and 2019 because we observed a higher volume of data for these intervals (the GPS sensors on the ravens are solar and data observations are very infrequent during the winter months). We tokenized each agent's trajectories into 1 day time window and altered the S2 Cell Zoom Level from 16 to 14. Otherwise, we trained our STARE model on the raven dataset using the same model architecture and hyperparameters as in the first set (Section 3.3) of simulated data experiments. We also train baseline LSTM/BILSTM models using agent labels with the same training settings as before and obtain the accuracies shown in Table 5.\nIn comparison to the accuracies seen in the preceding datasets, our tested models yield low accuracies. We believe this happens due to a characteristic of the ravens dataset. As we discover in the next figures, there are 4 main classes of ravens, and distinguishing between the classes is trivial. The more difficult task is to distinguish between individual ravens in each class, since each raven acts in a similar manner (i.e., have similar PLs and duration times), perhaps due to the observed ravens moving in groups. Because of this difficulty, we observe smaller accuracies when training on individual raven labels.\nTo further explore the potential of clustering raw spatiotemporal data using the TES encodings, we apply an MLP to them and obtain an averaged post-softmax prediction score matrix; in turn, we apply Spectral Clustering (SC) to it."}, {"title": "4 CONCLUSIONS AND FUTURE WORK", "content": "In this work, we proposed a spatiotemporal transformer-based model for extracting relationships from PoL data along with meaningful data encodings. Through experiments, we demonstrated our model's ability to correctly learn embeddings from two different tasks: agent label classification and masked modelling. Additionally, we presented various ways of understanding relationships between similar agents and locations through the use of the obtained embeddings. As future work, we plan to investigate our model's ability to scale up to very large volumes (millions) of data and also develop a"}]}