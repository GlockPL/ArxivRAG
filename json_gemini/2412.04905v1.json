{"title": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling", "authors": ["Minzheng Wang", "Xinghua Zhang", "Kun Chen", "Nan Xu", "Haiyang Yu", "Fei Huang", "Wenji Mao", "Yongbin Li"], "abstract": "Large language models (LLMs) have made dialogue one of the central modes of human-machine interaction, leading to the accumulation of vast amounts of conversation logs and increasing demand for dialogue generation. A conversational life-cycle spans from the Prelude through the Interlocution to the Epilogue, encompassing various elements. Despite the existence of numerous dialogue-related studies, there is a lack of benchmarks that encompass comprehensive dialogue elements, hindering precise modeling and systematic evaluation. To bridge this gap, we introduce an innovative research task Dialogue Element Modeling, including Element Awareness and Dialogue Agent Interaction, and propose a novel benchmark, DEMO, designed for a comprehensive dialogue modeling and assessment. Inspired by imitation learning, we further build the agent which possesses the adept ability to model dialogue elements based on the DEMO benchmark. Extensive experiments indicate that existing LLMs still exhibit considerable potential for enhancement, and our DEMO agent has superior performance in both in-domain and out-of-domain tasks.", "sections": [{"title": "1 Introduction", "content": "Under the compelling drive of large language models (LLMs), the development of intelligent language interfaces is undergoing an unprecedented transformation, with dialogue emerging as one of the central modes of human-machine interaction (Ross et al., 2023; Sergeyuk et al., 2024; Wu et al., 2024). The relentless evolution of LLMs has penetrated increasingly complex interaction environments, necessitating an enhancement of the LLM's expressive intelligence (Chang and Chen, 2024; Zhou et al., 2024a) and a sharp sensitivity to the pivotal elements within interactions (Tang et al., 2023; Xu et al., 2024). By meticulously analyzing vast dialogue logs, we can gain valuable insights about critical elements of dialogues, such as personality traits, scenarios, and interaction goals. These insights are vital for the ongoing improvement and optimization of human-machine interaction systems.\nTypically, a dialogue is conducted with a goal-oriented focus, relying upon a profound understanding of its core elements (Searle, 1969; Austin, 1975; Watzlawick et al., 2011). Participants navigate towards their goal within the constraints of the scene, utilizing strategy to interact with their environment, ultimately producing content with intent. In reality, the dialogue's life-cycle spans from the Prelude through the Interlocution to the Epilogue, encompassing various elements (Schegloff, 2007; Hutchby and Wooffitt, 2008). However, the current datasets inadequately cover these comprehensive aspects (Zhang et al., 2024), only concentrating on dialogue generation within pre-established set-"}, {"title": "2 Dialogue Element Modeling", "content": "2.1 A System of Dialogue Elements\nThe dialogue is conducted with a goal-oriented focus, relying upon a deep understanding of its core elements (Searle, 1969; Austin, 1975; Watzlawick et al., 2011). Participants strategically navigate towards their objectives within the scene's constraints, engaging with their environment to produce content with clear intent. The life-cycle of a dialogue spans from the prelude through the interlocution to the epilogue, encompassing various dialogue elements (Schegloff, 2007; Hutchby and Wooffitt, 2008). In the prelude, the focus is on the motivation and necessary elements of the dialogue (Goffman, 1981; Schiffrin, 1994), which include the participants' backgrounds, the time and place, the topic, and the goals of both parties. During the interlocution, attention is given to the elements intrinsic to each response, such as the intentions participants aim to convey, their current emotions and feelings, and the dialogue strategies employed (Goffman, 1981; Brown and Levinson, 1987). The epilogue involves summarizing the entire dialogue, assessing the fulfillment of both parties' goals, and examining the flow of information throughout the dialogue (Schegloff, 1973; Drew and Holt, 1998). For detailed dialogue element system, please refer to Figure 3. By analyzing the elements of these stages, a deeper understanding of the dialogue's structure and dynamics can be achieved, thereby facilitating comprehensive dialogue modeling."}, {"title": "2.2 Task", "content": "The task of dialogue element modeling focuses on two main aspects: Element Awareness and Dialogue Agent Interaction. Specifically, (1) Element Awareness examines the model's ability to analyze a complete dialogue, determining whether it can reverse-engineer elements such as goal, persona, and scene from the entire conversation and analyze elements at the utterance level. (2) Dialogue Agent Interaction assesses the model's goal-oriented interaction capability, evaluating whether it can achieve its goal within a given environment through a limited number of interaction rounds."}, {"title": "2.2.1 Element Awareness", "content": "The Element Awareness primarily focuses on offline single-turn inference. Given an entire dialogue, it aims to model the key elements that contribute to the conversation. It has four tasks: (1) Goal Recognition, (2) Persona Modeling, (3) Scene Reconstruction, and (4) Utterance Mining.\nGoal Recognition This task tends to attain the goal elements G from the given dialogue D. Dialogues are arguably goal-driven (Searle, 1969; Austin, 1975), and this task aims to identify the behavioral motivations of participants using the model \\( \\pi_{\\phi} \\). Specifically, the model needs to identify each person's dialogue goal g\u2081 and g2, and the extent s to which those goals are achieved. This process can be formally defined as \\(\\pi_{\\phi}(g_1, g_2, s|D)\\).\nPersona Modeling The task requires constructing the persona P of the two dialogue participants from the given dialogue D. Personality, experiences, educational background, and interests often influence the manner of interaction (Grice, 1975; Austin, 1975), establishing a mapping relationship between persona and dialogue content. This task requires the model to infer from effect to cause, as well as reverse modeling persona from dialogue content. Specifically, based on the dialogue content D, the model \\(\\pi_{\\phi}\\) aims to infer persona p\u2081 and p2, including"}, {"title": "2.2.2 Dialogue Agent Interaction", "content": "The Dialogue Agent Interaction refers to the two-party goal-directed online multi-turn dialogue in language space. This task encompasses a wide range of interaction types, both cooperative and non-cooperative, including persuasion, argument, empathy, negotiation, and more. It can be regarded as an incomplete information game (Reif, 1984). It examines the dialogue interaction capability of the model in modeling dialogue content driven by dialogue elements through dynamic inference. This task can be formulated as a Markov Decision Process (MDP) (Bellman, 1957).\nState The persona, goal, scene, and dialogue history in each episode denote the state. We predefined diverse episodes to serve as the initial state. Some elements in the state are invisible, such as the other party's goal and background information, whose visibility depends on the familiarity between the parties. As the interactions progress dynamically, the dialogue history is continuously updated, thereby evolving the states while the persona, goal, and scene stay unchanged. The global state at timestep t is represented as Dt, which includes the dialogue content generated after the t-th turn along with other constant dialogue elements. Meanwhile, local states tied to specific sub-action sets are enhanced by combining the global state Dt with the history of the previous t-1 dimensional sub-action choices.\nAction The interaction unfolds between two players, A1 and A2. At each turn t, according to the observation of the interaction history, the player selects an action that consists of one utterance Ut generated by one of the players.\nTransition In our setting, the transition function adds the utterance to the interaction history while the persona, goal, and scene stay unchanged in state representation. The dialogue history can be represented as an alternating sequence of utterances generated by two players, denoted by {U1, U2, U3, ..., Ut \u2013 1, Ut }. The interaction repeats until the dialogue goal is achieved or the maximum number of turns T is reached.\nReward After each round, a reward function can be called on the current state to quantify how well each player has been doing. The design of the reward function is critical and diverse. In our setting, we only consider the reward of the last interaction turn. To comprehensively examine the model's dialogue interaction capabilities, we devised a multi-dimensional reward framework. This framework articulates detailed scoring criteria and ranges for each dimension, utilizing the prompt GPT-4o as a reward function to evaluate the dialogue. We assess the following four dimensions: (1) Goal Achievement (0-10): the extent to which the dialogue goals of both parties are fulfilled; (2) Believability (0-10): the extent to how well participants comprehend and align with dialogue elements; (3) Skillfulness (0-10): the ability of participants to analyze dialogue history, mine utterances, and provide appropriate responses; (4) Realistic (0-10): the extent to which the response content appears human-like and vivid, as opposed to being virtual and overtly machine-generated."}, {"title": "3 DEMO Benchmark", "content": "3.1 Overview\nDEMO is our newly developed benchmark specifically designed for modeling and evaluating the dialogue element modeling capabilities of LLMs. It includes both Chinese and English languages. To evaluate element awareness, we have a total of 4,000 evaluation samples, with a maximum of 26 dialogue turns and an average of 18.3 turns, covering 23 distinct dialogue elements. Each of the four tasks contains 1,000 test samples. In terms of dialogue agent interaction, DEMO provides 1,000 episodes that cover a wide array of cooperative and non-cooperative interaction types, including persuasion, argument, empathy, and negotiation, among others. Examples of specific tasks from the evaluation set are illustrated in Appendix D."}, {"title": "3.2 Construction Framework for Benchmark", "content": "Our framework is depicted in Figure 2. We follow the structured dialogue system to sequentially annotate the elements of prelude, interlocution, and epilogue. First, based on Goal and Scene Distillation and Persona Design, we generate the three prelude elements: goal, scene, and persona. Then, through the Conflict Assessment, we ensure that the combined prelude elements are coherent. Subsequently, using the coherent elements, we generate the corresponding interlocution and epilogue elements through Dialogue Generation. Finally, after rigorous Quality Control, we revise and inspect the data to establish the final benchmark. The following are details.\nGoal and Scene Distillation In this initial stage, we distillate the goals and scenes from the given dialogue. By leveraging an instance-driven paradigm, we diversify data from the large-scale dialogue corpus. We utilize SODA (Kim et al., 2023a) and LCCC (Wang et al., 2020) as our seed corpora, which include millions of English and Chinese dialogues encompassing various aspects of social commonsense. Specifically, we employ Qwen2-72B-Instuct to extract each participant's goals and the conversation scene from each dialogue. Finally, we get 2.6 Million goal and scene data.\nPersona Design In parallel with the previous phase, we established a comprehensive and diverse persona collection. The creation of personas is divided into two parts: (1) Designing the persona attributes pool and (2) Inspiring prompting. Referring to (Zhou et al., 2023a, 2024c; Chen et al., 2024; Yang et al., 2024b), we consider the following attributes: name, gender, age, Big Five traits (McCrae and John, 1992), moral values (Graham et al., 2011), social skills (Yang et al., 2024c), personal values (Schwartz, 1992), and decision style (Scott and Bruce, 1995). Based on these characteristics, we combine them and then leverage Qwen2-72B-Instuct for more detailed persona modeling. By prompting the LLM with diverse web texts, as (Chan et al., 2024) suggests, we generate a wide-ranging selection of personas. Ultimately, the LLM produces detailed information on each person's background, hobbies, education, occupation, culture, relationships, and speaking style. At this stage, we have modeled 200,000 diverse personas.\nConflict Assessment Before generating dialogues, we must ensure the quality of our gathered prelude element, including scene, goal, and persona. We rely on LLM to assess the coherence and consistency of these combinations, checking for issues like character identity contradictions, misalignment between persona and goal, or unsuitable pairings of dialogue participants. To facilitate this process, we employ self-reflection techniques where the model is prompted to articulate its reasoning and evaluation results. For combinations deemed reasonable, we further instruct the model to provide additional details about the relationship and familiarity between participants, as well as the mode of communication. At this stage, we retain only 30% of the considered reasonable combinations. To manage data distribution effectively, we categorize all combinations into ten types based on the dialogue goal and extract the relevant combinations accordingly.\nDialogue Generation Upon establishing reasonable combinations, we proceed to generate interlocution and epilogue elements. Leveraging the LLM's role-play capabilities (Zhou et al., 2023a, 2024c; Chen et al., 2024), we prompt it to create dialogues that align with specified persona, goal, and scene. We also prompt the advanced LLMs to analyze each utterance with its associated intention, sentiment, emotion, stance, and strategy, culminating in a comprehensive output that includes the information flow and dialogue summary. To form our benchmark, we curate 1,800 distinct combinations, utilizing the more advanced GPT-40 model for generation.\nQuality Control To ensure the accuracy of benchmark annotations, we employ a three-step verification process: (1) Advanced-LLM Annotation: Two of the most advanced LLMs, GPT-40 and Claude-3.5-Sonnet, independently review and validate the quality of annotations. They examine each entry, editing any unreasonable or low-quality labels to maintain consistency and accuracy. (2) Voting: We implement the voting method to finalize the benchmark. (3) Manual Check: After the voting process, We engage two Ph.D students to further examine and assess the quality of data annotation following our pre-established quality control standards. After multiple rounds of data quality checks, we format the element to define the final benchmark."}, {"title": "3.3 DEMO Agent Training", "content": "Humans have the ability to learn efficiently through observing and imitating the behavior of others (Schaal, 1996; Ross et al., 2011; Torabi et al., 2018). Drawing inspiration from this, we propose enhancing the performance of LLMs in dialogue element modeling tasks by integrating behavioral learning methods. This approach centers on acquiring insights through interactions with expert models and developing an imitation policy. Behavioral learning, also known as behavior cloning (Bain and Sammut, 1999; Ross and Bagnell, 2010; Hanna and Stone, 2017; Wang et al., 2024b), is a method for extracting and distilling expert policies from high-quality data, particularly from models with advanced capabilities. In the context of dialogue element modeling, this method involves gaining an understanding of element awareness in single-turn reasoning and achieving nuanced expression in multi-turn interactions. During the benchmark construction process, we accumulated a substantial and diverse collection of unlabeled data, which will serve as the task's state source to enhance dialogue modeling. After excluding all elements overlapping with the benchmark, a selected batch of unlabeled data was curated. Specifically, GPT-40 was employed as the expert model to simulate dialogue modeling on analogous tasks, thus circumventing the reliance on costly human expertise. Through single-turn and multi-turn interactions with the expert model, we successfully amassed a wealth of expert experience, which will be utilized to refine and update the model's policy. We specialize the language model \\(\\pi_{\\Theta}\\) towards dialogue element modeling with the expert experience L. The process of policy updating is efficiently executed through Low-Rank Adaptation (LoRA) (Hu et al., 2022)."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nModels We evaluate ten advanced LLMs, including API-based LLMs: GPT-4o, GPT-4o-mini (Hurst et al., 2024), Claude3.5-Sonnet (Anthropic, 2024b), Claude3.5-Haiku (Anthropic, 2024a) and Open-sourced LLMs: Qwen2-72B-Instruct, Qwen2-7B-Instruct (Yang et al., 2024a), Llama3.1-70B-Instruct, Llama3.1-8B-Instruct (Dubey et al., 2024).\nEvaluation Metric Evaluating the unpredictable behaviors of LLMs, traditional metrics such as BLEU and Rouge-L may yield inaccurate responses. Recent research (Zhang et al., 2023; Zheng et al., 2023; Kim et al., 2023b) indicates that the GPT-4 evaluator demonstrates high consistency with human evaluation while reducing costs, making it a reasonably reliable annotator. Building on these considerations and following these work (Perez et al., 2022; Zhou et al., 2024c; Wang et al., 2024a), we prompt GPT-4o as a judge to evaluate the model's output based on the golden answer for the element awareness task from several aspects, scoring from 0 to 10. For the detailed prompts, please refer to the Appendix C.\nImplement Details To ensure the stability of the evaluation, we set the temperature of the evaluator to 0. For element awareness, we set the temperature of the LLM to 0 to ensure reproducibility. For dialogue agent interaction, we set the temperature of the LLM to 1 to encourage diversity. We use a fixed version of the above models to help reproducibility, and please refer to the Appendix A. For API-Based LLMs, we directly utilize the Azure API for testing. As for open-source models, we conduct experiments accelerated by the vLLM framework (Kwon et al., 2023). The details about training are shown in Appendix B. All the experiments are conducted on a server with 8\u00d7A100 80GB."}, {"title": "4.2 Main Results", "content": "We assess ten advanced LLMs on the DEMO benchmark. The main results are shown in Table 2.\nModel Analysis GPT-4o shows the best overall performance, maintaining great performance across all dimensions. Analyzing from the perspective of parameter size, the model's performance aligns with the Scaling Law, indicating that models with larger parameters can more effectively extract features and possess stronger expressive capabilities, whereas models with smaller parameters perform worse. Additionally, the gap between open-source and closed-source models is narrowing. For instance, large parameter open-source models like Qwen have achieved state-of-the-art performance in dialogue agent interaction tasks, with overall performance differences from GPT-4o being minimal.\nTask Analysis There remains significant room for improvement in the overall performance of current models, particularly in tasks involving feature perception. Accurately and directly modeling various elements (such as persona modeling) from dialogue content is still challenging, potentially requiring multi-step reasoning or additional clues. In dialogue agent interaction tasks, current LLMs exhibit excellent expressive capabilities, adeptly adhering to settings and generating relatively realistic dialogue content. Humans are inherently social, striving to achieve social objectives in daily interactions. Goal achievement is a crucial feature of intelligence; thus, the ability to perceive targets and collaborate to achieve individual goals reflects a model's higher-order capabilities. However, their ability to achieve self-set goals through multi-turn interactions requires enhancement.\nDEMO Agent By learning through expert experience imitation, the DEMO agent has achieved significant improvements across two different backbones, with an average task improvement of 0.9. Specifically, the agent utilizing the LLaMA backbone achieved SOTA performance in element awareness tasks. Meanwhile, the agent built on the Qwen backbone secured the second-highest score, surpassed only by GPT-4o. The DEMO Agent has also surpassed or performed on par with models with larger parameters, such as Claude-3.5-Sonnet and the Qwen2-72B-Instruct. This demonstrates the effectiveness of imitation learning and expert experience. However, this method has a performance ceiling limited by the capabilities of the expert model. Fully leveraging additional modeling cues to develop the capacity between feature perception and intelligent expression will be a primary focus of our future work.\nwe also use Kappa score (Fleiss, 1971) to measure our annotation quality.\nData Quality In Section 3.2, it is mentioned that our data annotation process includes advanced LLMs annotation and manual checks. The data quality inspection process is performed by two experienced annotators, with a consistency Kappa value of 0.84 between them. Their Kappa consistency results with different elements annotated by the LLM are shown in Table 3. The LLM shows high consistency with the two annotators, demonstrating performance comparable to humans. Additionally, we conducted a manual verification of the data, achieving an accuracy rate of 91.17%. These results have all validated the quality of our test data."}, {"title": "4.3 How about the out-of-domain performance?", "content": "The DEMO agent has demonstrated impressive results in dialogue element modeling within the domain. However, the question remains: can this capability extend to tasks beyond that domain? To evaluate the generalization ability of the dialogue element modeling, we selected SOTOPIA (Zhou et al., 2024c) as our testing environment. SOTOPIA assesses social intelligence through tasks encompassing various social interactions, including competition, accommodation, and persuasion. In this environment, two LLMs are prompted to act as role-playing social agents, engaging with each other through verbal, non-verbal, and action-based communication. SOTOPIA also designed a seven-dimension framework to capture the complexity of what makes social interactions successful. For each task, agents are scored along each designed dimension at the end of an interaction.\nThe results are shown in Table 4. All our DEMO Agents have shown remarkable generalization capabilities in social intelligence tasks, with notable performance improvements in the SOTOPIA hard task. This overall enhancement is evident in three key dimensions: relationship handling, knowledge application, and goal completion, which are critical intelligence features. These advancements demonstrate that the model has mastered essential interactive policy through imitation learning, drawing from the modeling experience of expert dialogue elements. As a result, its social intelligence capabilities have significantly increased. The model can now understand and respond more effectively to emotions and intentions in complex social scenarios, exhibiting an intelligence level that surpasses the base models."}, {"title": "4.4 Does the catastrophic forgetting problem exist?", "content": "In addition to confirming the model's great performance in dialogue element modeling, evaluating whether the other capabilities remain unaffected is equally crucial. Continued training can sometimes lead to catastrophic forgetting, where the model loses previously acquired knowledge, disrupting its initial alignment. We use the Helpful, Honest, Harmless (HHH) (Askell et al., 2021) dataset to assess the impact on alignment performance. This involves a multiple-choice task to measure the model's ability to select better answers from two given options. When presented with both options, we calculate the model's tendency to favor one answer over the other. To assess the model's general capabilities, we employ the MMLU (Hendrycks et al., 2021), using a 5-shot evaluation based on next-word prediction. Accuracy serves as the evaluation metric across all three benchmarks.\nThe results are presented in Table 5. From the table, it is evident that DEMO Agents retain the overall capabilities of the base model. Although a few did not exhibit enhancements, our models performed comparably to the base model. They did not experience significant issues with catastrophic forgetting, indicating that the dialogue element modeling operates independently of the general capabilities."}, {"title": "5 Related Work", "content": "We introduce the related work based on the causal relationships generated in dialogues, which is divided into two parts: (1) Dialogue Analysis: from effect to cause, inferring dialogue elements based on the dialogue content. (2) Dialogue Generation: from cause to effect, generating dialogue content according to the dialogue settings."}, {"title": "5.1 Dialogue Analysis", "content": "The goal of dialogue analysis is to mine and analyze critical elements (such as intent, profiles, summary, etc.) from the dialogue data (Zhang et al., 2024), which can extract actionable insights and drive empowerment. In the era of small language models, dialogue analysis did not form a systematic task but was broken down into atomic tasks, such as slot filling and intent classification (Qin et al., 2020; Louvan and Magnini, 2020; Jiang et al., 2023), dialogue summary (Chen et al., 2021; Fabbri et al., 2021; Ouyang et al., 2023; Ramprasad et al., 2024) and persona extraction (Wang et al., 2022; Zhou et al., 2023b), etc. In the era of LLM, recent work (Zhang et al., 2024) performed a thorough review and systematized conversation analysis task. There is a scarcity of datasets that encompass all essential elements of conversations, making it challenging to model accurately and evaluate the dialogue background information, which affects the development of dialogue modeling."}, {"title": "5.2 Dialogue Generation", "content": "The related work on dialogue generation primarily focuses on constructing dialogue datasets and designing steering-based methods for dialogue modeling. Task-oriented dialogue (Rashkin et al., 2019; Sun et al., 2021; Liu et al., 2022) focuses on completing specific tasks, emphasizing task completion rather than generalization. Open-domain dialogue (Li et al., 2017; Wang et al., 2020; Kim et al., 2023a) is mainly designed for \"chit-chat\" between users, with more general tasks and a greater focus on immersion. Recently, several role-playing works (Zhou et al., 2023a; Lu et al., 2024; Chen et al., 2024; Yang et al., 2024b; Zhou et al., 2024b) have emerged, which place more emphasis on dialogue engagement and character consistency. However, there is limited guidance for dialogue modeling, and there is a lack of reward modeling for goal-oriented dialogues during interactions. Inspired by social intelligence work SOTOPIA (Zhou et al., 2024c), we have defined a more comprehensive dialogue generation task space, considering more dialogue modeling elements. We evaluate dialogue interaction capabilities through multi-turn online interactions and introduce element awareness tasks to thoroughly assess the model's dialogue element modeling ability in both Chinese and English languages."}, {"title": "6 Conclusion", "content": "In this work, we redefine the dialogue framework and introduce a new research task: Dialogue Element Modeling. This task involves two fundamental capabilities: element awareness and dialogue agent interaction, which enhance the complex modeling and comprehensive evaluation of dialogue systems. We have developed a process for constructing dialogue elements and created a benchmark, DEMO, which includes comprehensive dialogue elements suitable for both English and Chinese. Additionally, we have developed the DEMO agent specifically for dialogue element modeling. Through extensive experiments, we assess the performance of several advanced LLMs, revealing that there is still room for improvement in our task. Moreover, the results demonstrate that our DEMO agent delivers excellent performance in both in-domain and out-of-domain scenarios."}, {"title": "7 Limitations", "content": "To improve the quality of DEMO, we employed two state-of-the-art LLMs, GPT-4o and Claude-3.5-Sonnet, to annotate and verify our benchmark. Additionally, we engaged two Ph.D. candidates to assess the benchmark's quality. This process is both time-consuming and costly. The performance of our DEMO agent is constrained by the capabilities of the expert model. The interplay between element awareness and intelligent tasks remains insufficiently understood. A joint learning approach that integrates reasoning and generation processes could enhance the modeling of dialogue elements. Currently, the full potential of the model's capabilities has yet to be realized."}, {"title": "C.1 Dialogue Agent Interaction", "content": "You are an impartial and harsh judge evaluating conversation quality. Your task is to rigorously and meticulously assess the following dialogue based on specific criteria.\n[Criteria]\n1. goal achievement (0-10):\nHow well the dialogue participants achieve their goals.\nIdentify each participant's goals from the provided background information. Analyze the progress made towards these goals throughout the conversation. 0 points: Neither participant makes any progress towards their goals. 10 points: Complete success; both participants fully achieve all their goals.\n2. believability (0-10):\nWhat the extent to which the dialogue participants understand and align with Background Information. How well these elements are reflected in their expressions.\nTwo Participants should correctly understand the backgrpund information and perceive goals", "example": "speaking style must not conflict with the character portrait", "points": "Significant inconsistencies or misunderstandings of background information; Scene"}, {"points": "Perfect alignment with all background elements", "0-10)": "nTo what extent can the participants think and generate appropriate responses based on the conversation history.\nThe participants in the conversation should correctly understand the dialogue history before responding"}, {"points": "Poor understanding of dialogue history; responses are often inappropriate and lack strategy. 10 points: All responses can fully utilize the conversation strategy", "0-10)": "n- Evaluate how realistic the conversation is", "signs": "Excessive politeness or formality"}, {"points": "Conversation is clearly AI-generated. 5 points: Mix of realistic and artificial elements. 10 points: Entirely believable as a conversation between two real people.\n[Background Information]\nTime: <time>\nLocation and environment: <location>\nDialogue Medium: <talkway>\nDialogue Topic: <topic>\nParticipants: <person1> and <person2>\nRelationship between the dialogue participants: <relationship>\nFamiliarity level between the dialogue participants: <familiarity>\nInformation about <person1>: <person1 bg>\nInformation about <person2>: <person2 bg>\n[Dialogue Goal]\nGoal of <person1>: <goall>\nGoal of <person2>: <goal2>\n[Dialogue Content]\n<dialogue>\n[Requirement]\n1. Reiterate the dialogue content and background information.\n2. Analyze how well the dialogue meets each criterion.\n3. Provide scores and reasons in JSON format as specified below.\n4. Please note that the scoring for each criteria is independent and should not be influenced by each other.\n{\n[Output Format]\njson", "n": "oal achievement", "reason": "reason for goal achievement>", "score": "0-10"}, {"n": "aturalness", "reason": "<reason for naturalness score>\"", "score": "0-10"}, {"n": "oherence", "reason": "<reason for coherence score>\"", "score": "0-10"}, {"n": "moothness", "reason": "<reason for smoothness score>\"", "score": "0-10", "evaluation": ""}, {"title": "C.2 Goal Recognition", "content": "You are an impartial and harsh judge evaluating conversation quality. Your task is to rigorously and meticulously assess the performance of the AI assistant in Dialogue Analysis (Goal) strictly based on specific criteria.\n[Criteria]\nAccuracy: To what extent is the assistant's answer semantically consistent with the gold standard?\nHallucination: There should be no hallucinations and friction. The assistant should not introduce any information not present in or not implied by the gold answer.\n[Gold Answer]\n{answer}\n[The Assistant's Predicted Answer]\n{prediction}\n[Requirement]\n1. The assistant receives an overall score on a scale of 0 to 10, where a higher score indicates better overall performance. Please note that if the assistant's answer fully meet the above criteria, its overall rating should be the full marks (10). Please note that the gold answer can be considered as a correct answer to the instruction.\n2. Analyze how well the Assistant's performance meets each criterion.\n3. Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias. Then, output a line indicating the score of the Assistant.\n4. Please note that the scoring for each criteria is independent and should not be influenced by each other.\n[Output Format]\n{\njson\n\"Accuracy\": { \"reason\": <reason for accuracy score>, \"score\": <0-10> },\n\"Hallucination\": { \"reason\": \"<reason for hallucination score>\", \"score\": <0-10> }\n}\nNow, start your evaluation:"}, {"title": "C.3 Persona Modeling", "content": "You are an impartial and harsh judge evaluating conversation quality. Your task is to rigorously and meticulously assess the performance of the AI assistant in Dialogue Analysis (Persona) strictly based on specific criteria.\n[Criteria]\nAccuracy: To what extent is the assistant's answer semantically consistent with the gold standard?\nHallucination: There should be no hallucinations and friction. The assistant should not introduce any information not present in or not implied by the gold answer.\n[Gold Answer]\n{answer}\n[The Assistant's Predicted Answer]\n{prediction}\n[Requirement]\n1. The assistant receives an overall score on a scale of 0 to 10, where a higher score indicates better overall performance. Please note that if the assistant's answer fully meet the above criteria, its overall rating should be the full marks (10). Please note that the gold answer can be considered as a correct answer to the instruction.\n2. Analyze how well the Assistant's performance meets each criterion.\n3. Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias. Then, output a line indicating the score of the Assistant.\n4. Please note that the scoring for each criteria is independent and should not be influenced by each other.\n[Output Format]\n{\njson\n\"Accuracy\": { \"reason\": <reason for accuracy score>, \"score\": <0-10> },\n\"Hallucination\": { \"reason\": \"<reason for hallucination score>\", \"score\": <0-10> }\n}\nNow, start your evaluation:"}, {"title": "C.4 Scene Reconstruction", "content": "You are an impartial and harsh judge evaluating conversation quality. Your task is to rigorously and meticulously assess the performance of the AI assistant in Dialogue Analysis (Scene) strictly based on specific criteria.\n[Criteria", "nAccuracy": "To what extent is the assistant's answer semantically consistent with the gold standard?\nHallucination: There should be no hallucinations and friction. The assistant should not introduce any information not present in or not implied by the gold answer.\n[Gold Answer", "Answer": "n{prediction}\n[Requirement"}]}