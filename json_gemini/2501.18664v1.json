{"title": "Rethinking the Upsampling Layer in Hyperspectral Image Super Resolution", "authors": ["Haohan Shi", "Fei Zhou", "Xin Sun", "Jungong Han"], "abstract": "Deep learning has achieved significant success in single hyperspectral image super-resolution (SHSR); however, the high spectral dimensionality leads to a heavy computational burden, thus making it difficult to deploy in real-time scenarios. To address this issue, this paper proposes a novel lightweight SHSR network, i.e., LKCA-Net, that incorporates channel attention to calibrate multi-scale channel features of hyperspectral images. Furthermore, we demonstrate, for the first time, that the low-rank property of the learnable upsampling layer is a key bottleneck in lightweight SHSR methods. To address this, we employ the low-rank approximation strategy to optimize the parameter redundancy of the learnable upsampling layer. Additionally, we introduce a knowledge distillation-based feature alignment technique to ensure the low-rank approximated network retains the same feature representation capacity as the original. We conducted extensive experiments on the Chikusei, Houston 2018, and Pavia Center datasets compared to some SOTAS. The results demonstrate that our method is competitive in performance while achieving speedups of several dozen to even hundreds of times compared to other well-performing SHSR methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Hyperspectral imaging captures hundreds or even thousands of contiguous spectral bands with very high resolution. It provides discriminative spectral information to distinguish substances with quite similar appearances [1]-[3]. Therefore, hyperspectral imaging has been widely applied in various tasks, including target detection [4]-[7] and tracking [8], [9], change detection [10], semantic segmentation [11], [12] and land cover classification [13]\u2013[15]. However, there is an inherent trade-off between spatial and spectral resolution due to physical constraints [16]. That is, hyperspectral imaging devices typically enlarge the size of photodiodes or photosensitive elements to effectively capture the spectral information of each pixel at different wavelengths. It reduces the total number of pixels in the sensor, resulting in low spatial resolution of the image. This obstacle hinders hyperspectral images in some critical high-precision remote sensing applications.\nTo address the above challenge, recent work has paid attention to the hyperspectral image super-resolution (HISR) area. For example, multispectral and hyperspectral image fusion"}, {"title": "II. RELATED WORK", "content": "This section first briefly describes recent developments in related fields, including single hyperspectral image super-resolution and related lightweight network models."}, {"title": "A. Single Hyperspectral Image Super-resolution", "content": "SHSR techniques typically employ deep learning models to learn the mapping between low-resolution and high-resolution images, producing high-quality images. Such an approach eliminates the need for auxiliary images, thereby avoiding complex image registration tasks. Deep learning models, such as CNNs and attention mechanisms, are now widely utilized as SHSR feature extractors.\nEarly SHSR models were mainly constructed based on CNNs. For example, the 3D-FCNN model [23] introduced 3D convolution into SHSR, enabling simultaneous extraction of spatial and spectral features while preserving spectral correlations. SRDNet [24] proposed a hybrid convolutional structure that combined 2D and 3D convolution units, leveraging 2D convolutions to focus on spatial feature extraction and 3D convolutions to capture spectral information. In recent years, attention mechanisms have become increasingly popular in constructing backbone structures for SHSR models. And channel attention [25] is utilized to adaptively adjust the feature weights of each channel to better capture interdependence. The SSPSR network [26] incorporated both spatial and spectral attention residual modules to capture the correlations between spatial and spectral context within hyperspectral images. SGARDN [27], leveraged group convolutions and spectral attention mechanisms to capture shallow spatial-spectral features. Some works integrated both spatial and spectral attention. For instance, MSSR network [28] utilized a dilated convolution-based attention to expand the receptive field within a shallow network, so as to capture global spatial information. Meanwhile, SRDNet [24] introduced a pyramid-structured self-attention mechanism to model spectral features in both spatial and spectral contexts. KNLConv [29] overcomes the limitations of standard convolution by exploring non-local dependencies in the kernel space.\nThe work of \"Attention Is All You Need\" [21] proliferates the employment of Transformers into SHSR models. MSD-former [30], for instance, combined the strengths of CNNS for local information extraction with the Transformer\u2019s capability for capturing global information. Although the Transformer structure has many advantages in feature extraction, the high computational cost for hyperspectral images is a critical concern. Therefore, Interactformer [31] replaced the self-attention with separable self-attention in the Transformer to reduce memory consumption. ESSAformer [18] introduced kernelized self-attention mechanism to transform the spectral correlation coefficient into radial basis function kernel, which approximated the similarity between Query (Q) and Key (K) with an exponential mapping."}, {"title": "B. Lightweight Networks for SHSR", "content": "Due to the high spectral dimensionality of a hyperspectral image, which contains a massive amount of information, deep learning involves a large number of parameters and substantial computational cost to extract features [32], [33]. Additionally, the upsampling commonly used in the super-resolution process further increases the computational complexity. Therefore, it arises high demands for lightweight networks to achieve SHSR tasks on resource-constrained environments.\nIn the field of remote sensing, a few approaches focused on lightweight super-resolution models by designing lightweight feature extractors [26], [30], [34]. The common way is to divide the input image along the channel dimension into multiple groups, which are then processed by different branches"}, {"title": "III. PROPOSED METHOD", "content": "In this paper, we first design a lightweight SHSR model with Large-Kernel Channel Attention, named LKCA-Net. LKCA-Net features a concise architecture, primarily a backbone composed of stacked LKCA-based Block (LKB) modules to extract deep features, a learnable upsampling layer to reconstruct spatial resolution, and a bicubic upsampling connection to boost performance. Moreover, we argue that the computational complexity of SHSR models can be significantly reduced by optimizing the upsampling process. To validate this hypothesis, we further conduct the low-rank approximation on the upsampling layer, which we will discuss and confirm later.\nSpecifically, the low-resolution hyperspectral image is indicated as $I_{LR} \\in R^{H \\times W \\times B}$, the high-resolution hyperspectral image is denoted as $I_{HR} \\in R^{rH \\times rW \\times B}$, and the reconstructed result is $I_{SR} \\in R^{rH \\times rW \\times B}$. Here, H and W represent the spatial resolution of the image, B denotes the number of spectral bands, and r is the scaling factor. The overall architecture can be expressed by the following equation:"}, {"title": "A. Overall Architecture and Problem Definition", "content": "$I_{SR} = H_{UP}(H_{SR}(I_{LR})) + F_{Bicubic},$ \nwhere $H_{UP}$ denotes learnable upsampling layer and the $F_{Bicubic} \\in R^{rH \\times rW \\times B}$ denotes the bicubic interpolation residual. $H_{SR}(\\cdot)$ is our LKCA-Net backbone, to clarify, $H_{SR}$ consists with one 3 x 3 convolution to extract shallow features $F_o$, and N stacked LKB modules to extract hierarchical features. This procedure can be expressed as:\n$F_o = Conv_{3\\times3}(I_{LR}),$ \n$F_N = H_{LKB,N}(H_{LKB,N-1}(\\dots(H_{LKB,1}(F_o))\\dots)),$ \nwhere $F_N \\in R^{H \\times W \\times C}$ represents the feature map extracted by the backbone. $H_{LKB,N}(\\cdot)$ denotes the Nth LKB module"}, {"title": "B. Large-Kernel Channel Attention-based Block", "content": "The LKB module serves as the basic block of the LKCA-Net backbone, which is designed to extract spatial-spectral features. The structure of the LKB is illustrated in Fig. 3. The LKB consists of an $LN - Conv_{1\\times1} - GELU$ block for feature projection, one LKCA module to capture multi-scale features, and a residual connection to facilitate information flow. We design the LKCA module with multi-scale features and channel-wise attention to form an efficient and lightweight network for SHSR tasks to extract complex features. Specifically, the LKB module can be expressed as:\n$F_{LKB} = H_{LKB}(F_1),$ \nwhere $F_1$ and $F_{LKB} \\in R^{H \\times W \\times C}$ represent the input and output feature maps, and $H_{LKB}(\\cdot)$ denotes the LKB module.\nThe LKCA module adopts convolution decomposition strategies to expand the receptive field while reducing computational complexity. We construct a cascade of two depthwise dilated convolutions with different dilation rates, denoted as $(Conv_1 - Conv_2)$, where $k_1$ and $k_2$ represent the kernel sizes, and $d_1$ and $d_2$ are dilation rates. This strategy not only enlarges the receptive field but also naturally generates multi-scale features. The outputs of these two dilated convolutions are concatenated to fuse the multi-scale features.\nOne critical problem is that the depthwise convolution processes of each channel are independent, which may limit the exchange of information between channels. However, large redundant features exist within multi-scale channels. To overcome this limitation, we incorporate group convolution and a channel attention mechanism to calibrate the multi-scale channel features. The CA mechanism effectively captures dependencies between channels and improves interactions among information. It dynamically assigns weights to the channels of the feature map, adjusting the importance of each channel, helping the model to learn and restore relationships between channels. In the final integration stage, we further replace the original 1 \u00d7 1 convolution with grouped convolution to further reduce the parameters of the module. The process can be expressed as:\n$F_W = H_{LKCA}(F_U),$ \nwhere $F_U$ and $F_W \\in R^{H \\times W \\times C}$ represent the input and output feature maps respectively. $H_{LKCA}(\\cdot)$ denotes the LKCA module. In LKCA, $F_U$ is first passed through a dilated convolution with a dilation rate of 5, yielding the first attention map $A_1 \\in R^{H \\times W \\times C}$. Then, $A_1$ is passed through another dilated"}, {"title": "C. Upsampling Layer is Low Rank", "content": "In this section, we demonstrate that the parameter matrix of the learnable upsampling layer in SHSR models is low-rank. During the upsampling stage of the SHSR model, the PixelShuffle method requires the channels to satisfy Eq. 15.\n$C_{out} = C \\times r^2,$\nwhere, $C$ is the number of spectral channels in the hyperspectral image, and $r$ is the scale factor. Therefore, the number of parameters for the convolution layer before the PixelShuffle is $C_{in} \\times C \\times r^2 \\times k^2$, where k is the kernel size of the convolution.\nThere is no doubt that the parameter matrix of the learnable upsampling layer is a high-dimensional tensor. The parameters of the upsampling layer grow exponentially with the scale factor and kernel size. Therefore, we have to investigate the inherent property of this high-dimensional tensor of the upsampling layer. At first, we reshape the parameter tensor into a two-dimensional matrix $M \\in R^{C_{out}, C_{in} \\times k^2}$ and then perform singular value decomposition (SVD) on the matrix to analyze the rank of the parameter matrix.\nFor our model, the value of $C_{in}$ is 128, and the kernel size is 3. After reshaping, the parameter tensor of the learnable upsampling layer should become a matrix of size [$C_{out}$, 3 \u00d7 3 \u00d7 128], meaning that in the case of a full rank, the rank of matrix M should be 1152. Subsequently, we perform SVD decomposition on the matrix M, as shown in Equ. 16.\n$M = UCV^T = [u_1 \\enspace u_2 \\enspace \\dots \\enspace u_m]  \\begin{bmatrix} \\sigma_1 & 0 & & 0 \\\\ 0 & \\sigma_2 & & 0 \\\\ & & \\ddots & \\\\ 0 & 0 & & \\sigma_p  \\end{bmatrix} \\begin{bmatrix} v_1^T \\\\ v_2^T \\\\ \\vdots \\\\ v_n^T  \\end{bmatrix},$ \nwhere $\\sigma_1, \\sigma_2,\\dots, \\sigma_p$ are all singular values of the matrix M. After normalizing these singular values, illustrates the cumulative distribution of singular values of the matrix on three datasets with one \u00d74 scale factor. The y-axis represents the cumulative sum of the top-n singular values. The x-axis represents the indices of the singular values, with the largest singular value positioned on the far left and decreasing sequentially. The total number of indices should be 1152, and we only show the cumulative distribution of the first 600 singular values.\nFrom the curves of, it can be observed that the cumulative sum of singular values rapidly approaches 1 as the singular value index increases. The distribution of singular values exhibits a clear long-tail characteristic. This indicates that the majority of the information is concentrated in a small number of larger singular values, which demonstrates the low-rank property of the matrix. Therefore, we can approximate the matrix M via low-rank approximation.\nWe introduce the group convolution to approximate the matrix M. It illustrates the process of approximating the matrix M with a low-rank matrix. Clearly, the factor g significantly affects the trade-off between network performance and complexity. As g must be selected from integers divisible by $r^2$ to satisfy the PixelShuffle demand, $g\\in (2,4,8,16)$ when r = 4. We can observe from that the cumulative sum of singular values reaches 90% with about the first 60 singular values on the Chikusei and Houston datasets. Therefore, a rank-60 matrix is sufficient to effectively approximate the parameter matrices. For the Pavia dataset, the parameter matrix requires a rank-130 matrix to achieve a good approximation. In general, we ultimately set g to 8."}, {"title": "D. Feature Alignment", "content": "To address the performance gap caused by the low-rank approximation for the upsampling layer, we propose a feature alignment strategy based on knowledge distillation (KD). We design one feature alignment loss $L_{total}$ to align the features between the approximated layer of the student network and the teacher network. $L_{total}$ consists of two components: the supervised loss $L_{H}$ and knowledge distillation loss $L_{KD}$.\nAt first, we adopt the H loss suggested by MSDformer [30] as the supervisory loss $L_H$. H loss comprehensively considers the reconstruction loss $L_1$, spectral angular loss $L_{sam}$, and gradient loss $L_{grad}$. The definition of H loss could be formulated as\n$L_{H}(\\Theta) = L_1 + \\lambda_1L_{sam} + \\lambda_2L_{grad},$ \nwhere, $\\lambda_1$ and $\\lambda_2$ are the weight coefficients for spectral loss and gradient loss, respectively, used to balance the contributions of different loss components. We set $\\lambda_1$ = 0.5 and $\\lambda_2$ = 0.1 as suggested in [30].\nFor the KD loss, we emphasize spectral consistency for SHSR tasks by adopting the SAM loss, allowing the student network to learn the spectral features of the teacher network. We also include grad loss for spatial features. Given the significance of spectral consistency in the SHSR task, cosine similarity loss ($L_{cos}$ loss) is employed to further preserve this consistency. The loss function is defined as\n$L_{cos}(\\Theta) = 1 - \\frac{1}{N} \\sum_{n=1}^{N} cos(\\frac{H_t^r,H_s^r}{||H_t^r||_2. ||H_s^r||_2}),$ \nwhere, N represents the number of images in a training batch, $H_t^r$ is the high-resolution image reconstructed by the teacher net for the $n^{th}$ image in the training batch, and $H_s^r$ is the high-resolution image reconstructed by the student. $||. ||_2$ represents the l2 norm. Like SAM loss, $L_{cos}$ loss focuses on optimizing spectral directions without considering the magnitude, but it calculates cosine similarity for each pixel's spectral vectors, aligning student and teacher spectral directions. While SAM loss has stronger physical significance, the $cos$ loss is simpler, more efficient, and easier to optimize. Finally, the KD loss is defined as:\n$L_{KD}(\\Theta) = \\lambda_3L_{cos} + \\lambda_4L_{sam} + \\lambda_5L_{grad},$ \nwhere $\\lambda_3 = \\lambda_4 = 0.5$ and $\\lambda_5 = 0.1$ are the weighting coefficients.\nDuring training, discrepancies between the feature maps of the student and teacher networks are unavoidable. In the mid-to-late stages of training, such discrepancies may also affect the learning progress of the student network. We hope that the teacher network guides the student network in the early training stages, while gradually reducing its influence in later stages, which leads the student network to a self-learning phase. Therefore, we add a decay function D to the KD loss, which is defined as\n$D = \\frac{1}{1 + e^{d \\cdot epoch/f}},$ \nwhere d is the decay factor, f is the decay frequency, while epoch represents the current training epoch. It decreases the contribution to the total training loss progressively with the epoch increasing. The final total training loss is defined as follows\n$L_{total}(\\Theta) = D \\cdot \\alpha L_{KD} + L_{H},$ \nwhere $\\alpha$ represent the initial contributions of KD loss."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "We ablate each component step by step to investigate their performance and the effectiveness of the proposed method. All experiments are performed on the Chikusei dataset, with the super-resolution factor of 4."}, {"title": "A. Ablation Study", "content": "The number of LKB modules: Since this paper focuses on the lightweight study, it is essential to balance model performance and computational cost. To this end, we explore the impact of the number of LKB modules on the overall model performance. As can be seen from Table I, both performance and computational cost increase with the growth of LKB modules. Therefore, we ultimately select a network with 16 stacked LKB modules for our subsequent experiments. This configuration strikes a good balance between computational cost and performance, meeting the needs of a lightweight network.\nThe decay d of KD loss: As mentioned in the methodology section III-D, it is harmful to allow the teacher network to fully supervise the knowledge distillation process on training the student. Therefore, we introduce a decay function D to the KD loss in order to reduce the influence of the teacher in the later stages of training. To validate the effectiveness of this"}, {"title": "B. Datasets and Experimental Setup", "content": "In this section, we conduct a detailed analysis and evaluation of our model's performance on three publicly available hyperspectral image datasets including Chikusei, Pavia Center, and Houston 2018 datasets. We compare the proposed model with five state-of-the-art methods in the SHSR domain, such as VDSR [17], 3DFCNN [23], SSPSR [26], MSDformer [30], and ESSAformer [18]. VDSR and 3DFCNN have comparable amounts of parameters and computational costs. SSPSR, MSDformer, and ESSAformer have significantly higher amount of parameters than ours. To ensure fairness, we maintain the settings of these comparison methods as described in their original papers. We conducted experiments with super-resolution scaling factors of r = 4 and r = 8 on all datasets. Our models are trained from scratch with PyTorch and the Adam optimizer. Drop path regularization is only applied in training. The initial learning rate is $2 \\times 10^{-3}$, which is gradually decreased to a minimum of $2 \\times 10^{-4}$. All experiments are conducted on NVIDIA RTX 4090 GPUs.\nTo evaluate the performance of the model, we employ comprehensive evaluation metrics in the SHSR domain, including peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), spectral angle mapper (SAM), correlation coefficient (CC), root mean square error (RMSE), and the relative global dimensional synthesis error (ERGAS). PSNR, SSIM, and RMSE are typically used to evaluate the reconstruction accuracy of natural images, calculated on single-channel images. To well compare super-resolution performance, we record the average values across all spectral bands. Meanwhile, CC, SAM, and ERGAS are commonly used for hyperspectral image fusion tasks."}, {"title": "C. Experimental Results on Chikusei Dataset", "content": "The Chikusei dataset was captured using the Headwall Hyperspec VNIR-C imaging sensor, in an urban area of Chikusei City, Ibaraki Prefecture, Japan. It contains 128 spectral bands, with a spectral range of 363 nm to 1018 nm, a total image size of 2517 \u00d7 2335 pixels, and a ground sampling distance (GSD) of 2.5 m.\nAs suggested in SSPSR [26], we crop the original image to the central region containing valid information, with a size of 2304 x 2048 x 128. Specifically, we crop four non-overlapping regions of size 512 \u00d7 2048 \u00d7 128 from the top of the dataset for testing. The remaining regions are randomly sampled for training (10% of the data was selected for validation). For a super-resolution scaling factor of r = 4, we crop patches of size 64 \u00d7 64, with 32 pixels overlap. For r = 8, we crop patches of size 128 \u00d7 128, with 64 pixels overlap. These small patches are used as high-resolution (HR) hyperspectral images, which serve as the ground truth (GT). The corresponding low-resolution (LR) hyperspectral images are generated by downsampling these HR images by factors of 4 and 8 using Bicubic interpolation to ensure consistent scaling factors.\n shows the comparison results of LKCA-Net with five other SHSR methods. Additionally, we record the performance of the low-rank network (LKCA-LR) and the network with feature alignment (LKCA-KD), to verify the feasibility of"}, {"title": "D. Experimental Results on Houston Dataset", "content": "The Houston 2018 dataset is part of the IEEE GRSS Data Fusion Contest in 2018, captured by the ITRES CASI 1500 hyperspectral sensor. It covers the campus of the University of Houston and surrounding urban areas in Houston, Texas,"}, {"title": "E. Experimental Results on Pavia Dataset", "content": "The Pavia Centre dataset was captured using the Reflective Optics System Imaging Spectrometer (ROSIS) during a flight campaign conducted in 2001 over the central region of Pavia, northern Italy. The dataset consists of 1096 \u00d7 1096 pixels and 102 spectral bands with a wavelength range from 430 nm to 860 nm and a ground sampling distance of 1.3 m. In the Pavia Centre dataset, regions without meaningful information in the hyperspectral image are removed, leaving a final effective region of size 1096 \u00d7 715 \u00d7 102. Specifically, we crop the top 224x715 \u00d7 102 region of the dataset into three 224x224\u00d7102 non-overlapping hyperspectral patches for testing. The right-most area, which does not meet the required size for testing patches, is discarded, while the remaining regions are cropped into small blocks for training (with 10% of the training data used as validation samples).\nTable VII presents the experimental results on the Pavia dataset. With r = 4, the performance of LKCA-KD surpasses all methods except the largest model, ESSAformer. Actually, the performance of LKCA-KD is very close to that of ESSAformer, only 0.11 dB lower in MPSNR but involves 13% of the parameters and 0.6% of the computational cost. The same conclusion can be drawn for r = 8. The results on the Pavia dataset further demonstrate the effectiveness of the proposed LKCA-Net and the feature alignment strategy. Moreover, we can see that the LKCA-KD outperforms the original LKCA-Net slightly. The reason is that the Pavia dataset is the most challenging dataset among the three datasets due to the smallest number of training samples, as a result, the fewer parameters of LKCA-KD have lower overfitting risk.\n shows the visual results on a hyperspectral image from the Pavia Center test dataset. As indicated by the red-boxed region, it can be observed that our method exhibits higher smoothness in restoring the edge details of the image, even outperforming the heavyweight networks MSDformer and SSPSR. This highlights the competitive advantage of our approach when handling complex datasets."}, {"title": "V. CONCLUSIONS", "content": "This paper proposed a novel lightweight single hyperspectral image super-resolution method, which included a lightweight backbone (LKCA-Net) and a low-rank approximation upsampling layer. We designed backbone of CNN architecture, which ensured multi-scale capabilities, large receptive fields, and channel calibration while maintaining parameter and computational efficiency. It effectively enhanced feature extraction performance and provided a new benchmark for lightweight SHSR tasks. For the upsampling layer, we were the first to argue the upsampling layer is a key bottleneck in lightweight SHSR. By confirming its low-rank characteristics, we proposed a sampling low-rank approximation and a feature alignment strategy to optimize its performance. Experimental results demonstrated that our method achieved excellent performance on three public hyperspectral datasets. Compared to state-of-the-art methods, our approach achieved competitive performance and speedups of several dozen to even hundreds of times."}]}