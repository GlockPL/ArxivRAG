{"title": "Predictive Multiplicity of Knowledge Graph Embeddings in Link Prediction", "authors": ["Yuqicheng Zhu", "Nico Potyka", "Mojtaba Nayyeri", "Bo Xiong", "Yunjie He", "Evgeny Kharlamov", "Steffen Staab"], "abstract": "Knowledge graph embedding (KGE) models are often used to predict missing links for knowledge graphs (KGs). However, multiple KG embeddings can perform almost equally well for link prediction yet suggest conflicting predictions for certain queries, termed predictive multiplicity in literature. This behavior poses substantial risks for KGE-based applications in high-stake domains but has been overlooked in KGE research. In this paper, we define predictive multiplicity in link prediction. We introduce evaluation metrics and measure predictive multiplicity for representative KGE methods on commonly used benchmark datasets. Our empirical study reveals significant predictive multiplicity in link prediction, with 8% to 39% testing queries exhibiting conflicting predictions. To address this issue, we propose leveraging voting methods from social choice theory, significantly mitigating conflicts by 66% to 78% according to our experiments.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) store factual knowledge of real-world entities and their relationships in the form of triples (head entity,predicate, tail entity). KGs allow for logical reasoning and answering of queries. Knowledge graph embeddings (KGE) apply machine-learning methods on KGs to provide extra-logical reasoning capabilities exploiting similarities and analogies over knowledge structures (Ji et al., 2021).\nKGE maps entities and predicates into low-dimensional vectors that preserve semantic and structural information of KGs (Hogan et al., 2021). The learned embeddings can be applied to downstream tasks like link prediction. Given queries in the form of (head entity, predicate, ?) or (?, predicate, tail entity), candidate entities are ranked based on predictive scores provided by KGE models. The positive triples are expected to be ranked higher than the negative triples.\nThe training of the KG embedding introduces randomness into the resulting model. Sources of randomness include randomized parameter initialization, randomized sequences of positive samples, and randomized negative sampling. Given the non-convexity of the training problem, the same KG may lead to various KG embeddings because of the convergence of the training in different local minima. While learned embeddings may exhibit comparable performance in link prediction, they may suggest conflicting predictions for an individual query. This phenomenon is referred to as predictive multiplicity in recent literature (Marx et al., 2020; Watson-Daniels et al., 2023; Black et al., 2022b), it is also known as \"Rashomon effect\" and model multiplicity in earlier studies (Breiman, 2001). As an example of predictive multiplicity in link prediction, Figure 1 shows the results of two models that both have an overall accuracy of 50%, but predict entirely different facts as top 1 recommendation.\nConflicting predictions introduce considerable risks when applying KGE methods in high-stake domains such as medicine or finance. For example, they would affect treatment decisions, affecting patient health outcomes in the context of medical recommendation (Gong et al., 2021), or switch compounds for confirmatory experiments in drug discovery (Mohamed et al., 2020), potentially altering research direction and efficiency. Moreover, predictive multiplicity complicates the justification of decisions made from equally accurate models (Black et al., 2022b). For example, when equally accurate models provide contradictory decisions regarding the approval of a loan application (Alam and Ali, 2022), the random selection of a model fails to properly justify the ultimate individual decision. Despite its relevance, predictive multiplicity has been overlooked in KGE research.\nTo the best of our knowledge, this is the first work to study predictive multiplicity for KGE-based link prediction. Our contribution is two-fold: First, we formally define predictive multiplicity in the context of link prediction. Two metrics, ambiguity and discrepancy, are introduced to measure predictive multiplicity, with an upper bound derived for discrepancy. Evaluating the predictive multiplicity for six representative KGE methods on commonly used benchmark datasets, we observe significant predictive multiplicity behavior in link prediction, with conflicting predictions ranging from 8% to 39% for testing queries.\nTo address this issue, our second contribution is to investigate the effectiveness of voting methods from social choice theory in mitigating predictive multiplicity in link prediction. Applying voting methods to aggregate individual rankings yields a more robust ranking that optimizes the collective preference. Our empirical findings demonstrate significant alleviation of predictive multiplicity through voting methods, with the most effective approach reducing conflicting predictions by 66% to 78% for testing queries."}, {"title": "2 Related Work", "content": "Although prior studies show the effectiveness of KGE methods on learning complex patterns in KGs (Bordes et al., 2013; Sun et al., 2019; Nickel et al., 2011; Yang et al., 2015; Trouillon et al., 2016; Dettmers et al., 2018), predictive multiplicity of KGE methods has been overlooked.\nThe term model multiplicity was first discussed in (Breiman, 2001) with the term \"Rashomon Effect\" referring specifically to the phenomenon where there are different weights learned for linear regression with the same error rate. The term predictive multiplicity was first introduced by (Marx et al., 2020), who explored this behavior in binary classification. (Marx et al., 2020) further investigate predictive multiplicity in probabilistic classification. Recent studies also provide evidence of predictive multiplicity for deep models (Black et al., 2022a; Mehrer et al., 2020). We initiate an exploration into the predictive multiplicity behavior within the context of KGE-based link prediction.\nWhile predictive multiplicity offers flexibility in model selection without sacrificing accuracy, diverging predictions can result in unjustifiable final choices. (Black et al., 2022a) propose a method to provide consistent predictions. Given diverging predictions, they first filter them through a specified confidence threshold and select the final prediction using a majority vote. Besides classification problems, predictive multiplicity is also frequently studied for counterfactual explanations (Jiang et al., 2024; Pawelczyk et al., 2020).\nVoting methods can also be seen as ensemble methods. Ensemble strategies are employed in KGE methods (Joshi and Urbani, 2022; Xu et al., 2021) during the training phase to increase the model performance. (Joshi and Urbani, 2022) focuses on enhancing the accuracy of the triple classification task by aggregating predictions from models trained using different KGE algorithms. (Xu et al., 2021) demonstrate that combining multiple low-dimensional models can outperform a single high-dimensional model. However, our approach aggregates rankings using social choice theory in testing time, aiming to alleviate predictive multiplicity by providing more robust rankings."}, {"title": "3 Notations and Preliminaries", "content": "We consider a KG $G \\subset E \\times R \\times E$ defined over a set E of entities and a set R of relations. The elements in G are called triples and denoted as $<h, r, t>$. A KGE model $M_{\\theta}: E\\times R\\times E\\rightarrow \\mathbb{R}$ allocates each triple with a predictive score that measures the plausibility that the triple holds (Bordes et al., 2013). The parameters $\\theta$ are learned to let $M_{\\theta}$ assign higher predictive scores to positive triples (real facts) while assigning lower predictive scores to negative triples (false facts). This can be achieved for example by minimizing margin-based ranking loss (Bordes et al., 2013):\n$L = \\sum_{t_r \\in T} \\sum_{t_r^- \\in T^-} max(0, \\gamma - M_\\theta(t_r) + M_\\theta(t_r^-))$, (1)\nor cross-entropy loss (Trouillon et al., 2016):\n$L = \\sum_{t_r \\in T \\cup T^-} log(1 + exp(-Y_{t_r} \\cdot M_\\theta(t_r)))$, (2)\nwhere $\\gamma$ is a margin hyperparameter, $t_r$ refers to a triple (h, r, t), T,T\u00af are the sets of positive and negative triples, respectively. The label of a triple, denoted as $Y_{t_r}$, takes values from the set $\\{\u22121,1\\}$. Here, $Y_{t_r} = 1$ indicates the triple as positive, while $Y_{t_r} = -1$ indicates that the triple is negative. The negative triples are typically generated by randomly replacing the head entity or the tail entity in a positive triple with a random entity sampled from the entity set."}, {"title": "3.2 Social Choice Theory", "content": "Social choice theory studies collective decision-making processes, where individual preferences are aggregated to determine a group's overall preference (Brandt et al., 2016). In this section, we recall some basics of social choice theory from (Shoham and Leyton-Brown, 2009).\nWe consider a finite set of candidates $C = \\{C_1,...,C_m\\}$ and a finite set of voters $V = \\{1, ..., n\\}$, who have different preferences on candidates in C. We represent preferences by a linear order $\\succ$ and let\n$\\bullet\\ \\ c_1 \\succ c_2 \\ \\text{iff} \\ \\ c_1 \\succeq c_2 \\land c_2 \\nsucceq c_1 \\ \\ \\ \\text{(strict preference)}$\n$\\bullet \\ \\ c_1 \\sim c_2 \\ \\text{iff} \\ \\ c_1 \\succeq c_2 \\succ c_2 \\succeq c_1 \\ \\ \\ \\text{(indifference)}$\nWe let $\\succ_i$ denote the preference ordering of the i-th voter. A preference profile $p : [\\succeq_1,\u2026\u2026\u2026, \\succeq_n]$ is a list of preference orderings. Next, we introduce some interesting voting methods from social choice theory (Brandt et al., 2016).\nDefinition 1 (Scoring Rule). A score vector is a vector $w \\in \\mathbb{R}^m$ such that $w_1 \\geq w_2 \\geq ... \\geq w_m$ and $w_1 > w_m$. Any score vector induces a scoring rule, in which each voter awards $w_1$ points to the top-ranked candidate, $w_2$ points to the second-ranked, and so on. The candidate with the highest total sum of scores wins.\nDefinition 2 (Majority Voting). Majority voting is a scoring rule with the score vector (1, 0, . . ., 0).\nDefinition 3 (Borda Voting). Given m candidates, Borda voting is a scoring rule with the score vector (m - 1, m - 2,..., 0).\nDefinition 4 (Range Voting (Smith, 2000)). Given m candidates, range voting is a scoring rule with a score vector $w \\in [-1,1]^m$.\nAdditionally, we introduce several properties desirable for the link prediction task in Appendix A."}, {"title": "4 Predictive Multiplicity in Link Prediction", "content": "A query $q \\in Q$ is of the form (h, r, ?) or (?, r, t). We let $t_r(q,e)$ denote the corresponding triple (h, r, e) or (e, r, t), respectively. A KGE model $M_{\\theta}$ can be used to rank the candidate entities for query q. We define the ranking $\\succeq_{M_{\\theta},q}$ by $e_1 \\succeq_{M_{\\theta},q} e_2$ iff $M_{\\theta},q(t_r(q, e_1)) \\geq M_{\\theta},q(t_r(q, e_2))$. We let $R_{M_{\\theta}g,q}(e)$ denote the rank position of a specific candidate entity $e \\in E$, that is\n$R_{\\succeq_{M_{\\theta},q}(e) = 1 + |\\{d \\in E | d \\succ_{M_{\\theta},q}e\\}|   \\ \\ \\ \\  \\ (3)$\nThen the link prediction task can be formulated as a binary classification problem: determine whether a triple is ranked within the top-K predictions:\n$T_K(M_{\\theta}, t_r(q, e)) = 1[R_{\\succ_{M_{\\theta}g,q}(e) \\leq K]    \\ \\ \\ \\ (4)$\nThe performance of link prediction is commonly evaluated by Hits@K. The test set T contains testing queries (q, e) consisting of a query q and a correct answer e. We define the Hits@K function $H_K$ of a KGE model $M_{\\theta}$ as\n$H_K (M_\\Theta) =  \\frac{1}{|T|} \\sum_{(q,e) \\in T} 1[R_{\\succ_{M_\\theta g,q}(e) \\leq K]    \\ \\ \\ \\ (5)$\nWe study KGE models that perform similarly in link prediction task in terms of Hits@K, i.e. competing models. Following (Marx et al., 2020), we will now define a e-level set for similar performing models and e as the error tolerance.\nWe let M denote a hypothesis class of KGE models. A baseline model $M_{\\theta}^\\star \\in M$ is the KGE model that achieves the highest Hits@K on the validation dataset throughout the hyperparameter optimization process. $D(M_{\\theta}, M_{\\theta}^\\star)$ measures the difference between baseline model and a competing model with respect to Hits@K.\n$D(M_\\Theta, M_\\Theta^\\star) = H_K(M) \u2013 H_K(M_\\Theta^\\star)    \\ \\ \\ \\ (6)$\nDefinition 5 (e-level set). Given a baseline KGE model $M_{\\theta}^\\star$ and a hypothesis class M, the e-level set around $M_{\\theta}^\\star$ is the set of all models $M_\\theta \\in M$ with a performance difference at most e in the link prediction task.\n$S_\\epsilon(M_\\Theta^\\star) := \\{M_\\Theta \\in M\\ \\ \\ |\\ \\  D(M_\\Theta, M_\\Theta^\\star) \\leq \\epsilon\\}    \\ \\ \\ \\ (7)$"}, {"title": "4.3 Measuring Predictive Multiplicity", "content": "Ambiguity and discrepancy are two measures that have been used to quantify predictive multiplicity in classification tasks (Marx et al., 2020; Watson-Daniels et al., 2023). We next define them for link prediction.\nTo make the notation more concise, we use $\\Delta(M_\\theta, \\tau)$ to denote whether a competing model $M_{\\theta}$ provides conflicting predictions compared to the baseline model $M_{\\theta}^\\star$ for a testing query $\\tau = (q, e)$.\n$\\Delta(M_\\theta, \\tau) = 1 [T_K(M_\\theta, t_r(\\tau)) \\neq T_K(M_\\Theta^\\star, t_r(\\tau))]   \\ \\ \\ \\ (8)$\nDefinition 7 (Ambiguity). Given a testing query set T, the ambiguity of link prediction over the e-level set $S_\\epsilon(M_\\Theta^\\star)$ is the proportion of testing queries that obtain a different prediction by a competing model $M_\\theta \\in S_\\epsilon(M_\\Theta^\\star)$:\n$\\alpha_{\\epsilon}(M_\\Theta^\\star) := \\frac{1}{|T|}  \\sum_{\\tau \\in T} \\max_{M_\\Theta \\in S_\\epsilon(M_\\Theta^\\star)} \\Delta(M_\\theta, \\tau)     \\ \\ \\ \\   (9)$\nDefinition 8 (Discrepancy). The discrepancy of link prediction over the e-level set $S_\\epsilon(M_\\Theta^\\star)$ is the maximum percentual disagreement between the baseline model and a competing model $M_{\\Theta}\\in S_{\\epsilon}(M_\\Theta^\\star)$:\n$\\delta_{\\epsilon}(M_\\Theta^\\star) :=  \\max_{M_\\Theta \\in S_{\\epsilon}(M_\\Theta^\\star)}  \\frac{1}{|T|} \\sum_{\\tau \\in T} \\Delta(M_\\theta, \\tau)   \\ \\ \\ \\   (10)$\nAmbiguity measures the proportion of testing queries that exhibit predictive multiplicity, while discrepancy captures the largest fraction of test queries for which the predicted answers vary upon switching the baseline model with a competing model."}, {"title": "4.4 Bound on Predictive Multiplicity", "content": "In Proposition 1, we bound the number of queries with conflicting predictions between the baseline model and a competing model in the e-level set. We provide a proof in Appendix B.\nProposition 1 (Bound on Discrepancy). The discrepancy between the baseline model $M_{\\theta}^\\star$ and any competing model $M_\\theta \\in S_{\\epsilon}(M_\\Theta^\\star)$ obeys:\n$\\delta_{\\epsilon}(M_\\Theta^\\star) \\leq 2 \\cdot (1 \u2013 H_K(M_\\Theta^\\star)) + \\epsilon     \\ \\ \\ \\ (11)$\nThe upper bound illustrates how the extent of predictive multiplicity depends on Hits@K of the baseline model. Specifically, a less accurate baseline model theoretically provides greater potential for predictive multiplicity."}, {"title": "5 Alleviating Predictive Multiplicity using Social Choice Theory", "content": "The predictive multiplicity can be alleviated by improving the robustness of the rankings. Here, robustness means models with similar performance should also provide similar rankings for testing queries. Social choice theory provides a theoretical framework for aggregating individual preferences to determine a group's overall preference (Brandt et al., 2016). Voting methods from social choice theory can help \"smooth out\" the randomness in rankings by aggregating individual models (Potyka et al., 2024). Intuitively, the candidate entities that are constantly ranked high for all models should also be ranked high in final rankings.\nWe next describe ranking aggregation using voting methods with a running example and adapt range voting (Smith, 2000) to aggregate the predictive scores for the final ranking."}, {"title": "5.1 Ranking Aggregation using Voting Methods", "content": "For link prediction, given a query q and a KGE model $M_\\theta$, the ranking of candidate entities for a query is denoted as $\\succeq_{M_\\theta,q}$. By training KGE models with N different random seeds, we obtain a profile for each query $p_q = [\\succeq_{M_{\\theta_1},q}\u2026\u2026\u2026, \\succeq_{M_{\\theta_N},q}]$. A ranking aggregation process takes pq as input and outputs a single ranking.\nWe illustrate how to aggregate rankings with voting methods in link prediction task with the following running example."}, {"title": "6 Experiments", "content": "In this section, we measure the predictive multiplicity in link prediction and apply voting methods from social choice theory. Our goals are (i) to measure the predictive multiplicity for the link prediction task; (ii) to investigate to which extent voting methods can alleviate predictive multiplicity.\nModels and Datasets. The main experiments are conducted for six representative KGE models (TransE (Bordes et al., 2013), RotatE (Sun et al., 2019), RESCAL (Nickel et al., 2011), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016), and ConvE (Dettmers et al., 2018)) on four public benchmark datasets (WN18 (Bordes et al., 2013), WN18RR (Dettmers et al., 2018), FB15k (Bordes et al., 2013), and FB15k-237 (Toutanova and Chen, 2015)). A small dataset Nations (Hoyt et al., 2022) is additionally used for investigating the change of predictive multiplicity with respect to the error tolerance \u20ac. The statistics of benchmark datasets are summarized in Table 4.\nExperiment Settings. For training KGE, we use the implementation of LibKGE (Broscheit et al., 2020). All experiments were conducted on a Linux machine with a 40GB NVIDIA A100 SXM4 GPU."}, {"title": "6.1 Evaluating Predictive Multiplicity", "content": "The e-level set, as defined in Definition 5, is too large to be evaluated in practice. As usual, we will use empirical notions of ambiguity and discrepancy that are based on a sample of the e-level set that we denote by $S_\\epsilon(M_\\Theta^\\star)'$.\nConstructing the Subset of e-level Set. To construct $S_\\epsilon(M_\\Theta^\\star)'$, we first obtain the baseline model $M_{\\theta}^\\star$ by performing 60 trials of hyperparameter search using the strategy in (Ruffinelli et al., 2019) (more details in Appendix C) and set \u20ac to 0.01 (a commonly used value in the literature (Marx et al., 2020; Watson-Daniels et al., 2023)). Subsequently, we train a potential competing model using the training configurations of the baseline model with a different random seed. If the performance difference between the potential competing model and the baseline model is less than e, we add it in $S_\\epsilon(M_\\Theta^\\star)'$. Due to computational constraints, we limit the size of $S_\\epsilon(M_\\Theta^\\star)'$ to 10 in our experiment."}, {"title": "6.2 Further Analysis", "content": "We conduct experiment for ComplEx on Nations to investigate the influence of e on predictive multiplicity. The procedure follows Algorithm 1 with thirty values of e spanning the range from 0 to 0.06. We represent the results in Figure 2. Our observations confirm the expectation in section 4.4: both predictive multiplicity metrics increase with larger values of e. Employing voting methods consistently reduces both ambiguity and discrepancy across all e values, with a more pronounced effect observed for larger 6. Notably, even at e = 0, conflicting predictions persist, underscoring the necessity to report predictive multiplicity even for equally accurate models. Additionally, we observe that the change of e has negligible effects on Hits@K, as detailed in Appendix D.3."}, {"title": "6.2.2 Investigating the Number of Models for Aggregation", "content": "In Figure 3, we investigate the predictive multiplicity metrics in relation to the number of models employed for ranking aggregation. Employing a larger number of models for aggregation yields a more notable alleviation of predictive multiplicity. Remarkably, even with a relatively small number of aggregated models, substantial improvements in predictive multiplicity can be attained. Furthermore, change of the number of models for aggregation does not notably affect Hits@K (Figure 11 - 14 in Appendix D.4)."}, {"title": "6.2.3 Investigating the Predictive Multiplicity wrt. Entity/Relation Frequency", "content": "Most entities/relations only have a few facts in KGs (Xiong et al., 2018). There are more possible embeddings or more uncertainty for those relations/entities since they are less constrained by the existing facts in KG during training. Intuitively, there might be more significant predictive multiplicity behavior for queries containing those entities/relations.\nWe conduct hypothesis tests using Spearman's coefficient ($\\rho$) to assess the correlation between entity/relation frequency (i.e., the number of triples containing the target entity/relation) and predictive multiplicity metrics ($\\alpha_\\epsilon$ and $\\delta_\\epsilon$). $\\rho$ ranges from -1 to 1, indicating the strength and direction of the correlation: close to 1 implies a positive monotonic relationship, while close to -1 implies a monotonic negative relationship.\nWe count entity/relation frequencies (Ent. Fre and Rel. Fre) as variable 1 and calculate $\\alpha_\\epsilon$ and $\\delta_\\epsilon$ for six KGE methods on entity/relation- specific subsets of all datasets as variable 2. Results in Table 6 show a significant negative correlation, confirming our conjecture. Notably, applying range voting weakens this correlation, potentially due to its effectiveness in alleviating predictive multiplicity for queries with higher uncertainty."}, {"title": "7 Discussing Other Influential Factors of Predictive Multiplicity", "content": "In this section, we discuss additional factors that may influence predictive multiplicity, namely expressiveness and inference patterns. We briefly introduce these two notions and then discuss some observations regarding their relationship to predictive multiplicity.\nExpressiveness. The expressiveness of KGE models refers to the ability of modeling an arbitrary KG. Following (Pavlovi\u0107 and Sallinger, 2023; Wang et al., 2018), we call a KGE model fully expressive if we can find a parameter set such that the model predicts all training triples correctly. Intuitively, more expressive models can represent more possible embeddings that fit the training graph, thereby allowing more \"room\" for multiplicity.\nInference Patterns. Inference patterns refer to the logic rules used to derive new triples from the observed facts in KGs. The generalization capabilities of KGE is usually analysed based on inference patterns that KGE model can capture (Abboud et al., 2020). For instance, TransE can capture inverse patterns, wherein $r_1(X, Y)$ implies $r_2(Y, X)$, suggesting that the testing triple (e1, r2, e2) can be correctly predicted with low uncertainty if (e2, r1, e1) is present in the training graph. Theoretically, if the KGE method effectively captures the inference patterns for the testing triple, we would expect fewer conflicts from competing models.\nObservations. According to (Wang et al., 2018)[Table 1], RESCAL and ComplEx are more expressive than DistMult when considering similar embedding dimensions. We observe that RESCAL and Complex associate with larger values of ambiguity and discrepancy than DistMult in Table 5, aligning with our conjecture regarding expressiveness. Furthermore, WN18 and FB15k are known to suffer from test leakage due to inverse relations (Toutanova and Chen, 2015), meaning that many test triples can be easily derived by the inverse pattern. WN18RR and FB15k-237 delete inverse relations to address this issue (Toutanova and Chen, 2015; Dettmers et al., 2018). In Figure 4, we note a consistent trend where competing models exhibit fewer conflicting predictions on WN18 and FB15k compared to WN18RR and FB15k237. This observation supports our conjecture regarding inference patterns, as the absence of even a single inference pattern notably increase the number of conflicting predictions."}, {"title": "8 Conclusion", "content": "In this paper, we define and measure the predictive multiplicity in link prediction. We measure the predictive multiplicity with empirical ambiguity and discrepancy for representative KGE methods on commonly used benchmark datasets. Our empirical study reveals significant predictive multiplicity in link prediction, and we demonstrate the effectiveness of applying voting methods. We also discuss several potential factors that could influence predictive multiplicity in link prediction.\nFurthermore, according to Proposition 1, predictive multiplicity depends on the accuracy of the baseline model and error tolerance (\u20ac). A less accurate baseline model or larger e allows for more predictive multiplicity. Given the typically low accuracy in link prediction and the existence of conflicting predictions even when \u20ac = 0, a considerable number of conflicting predictions may arise from competing models in practice, posing significant risks in safety-critical domains. Hence, we advocate for the measurement, reporting, and mitigation of predictive multiplicity in link prediction within these domains."}, {"title": "9 Limitations", "content": "In Section 7, we offer conjectures regarding the relationship between influential factors and predictive multiplicity. Our findings only show that our conjectures are potentially reasonable, but no conclusions can be drawn based on them. A systematic analysis necessitates quantifying expressiveness, inference patterns, which falls outside the scope of our paper but is a promising avenue for future research.\nTo mitigate predictive multiplicity, employing voting methods derived from social choice theory emerges as a straightforward yet effective strategy. However, voting-based ranking aggregation requires training multiple competing models from scratch, which can be time/computational consuming. Addressing predictive multiplicity during the training phase is considered as next step. Furthermore, more advanced voting methods such as partial Borda voting (Cullinan et al., 2014) could be explored in the future, which aggregates only partial rankings to reduce memory requirements during the aggregation step."}, {"title": "10 Ethics Statement", "content": "In this study, we emphasize the importance of reporting and dealing with predictive multiplicity to ensure fair and transparent decision-making processes for KGE-based applications. Failure to account for predictive multiplicity may lead decision-makers to select models that align with their personal preferences, potentially resulting in unfair outcomes for individuals. By neglecting to report predictive multiplicity of KGE models, decision-makers risk undermining the integrity and equity of the decision-making process."}, {"title": "A Properties of Voting Methods from Social Choice Theory", "content": "All voting methods were proposed to aggregated preferences in an intuitive \"fair\" way. However, for some cases, they may fail unintendedly. Thus, precisely defined properties - appealing behaviors that the voting methods satisfy, are investigated in social choice theory (Brandt et al., 2016).\nWe introduce some properties from (Brandt et al., 2016) that are desirable for link prediction. Recall that a social choice function is a function f mapping from the set of all possible profiles P to a non-empty subset of possible candidate C. Given a finite set of voters N = {1, ..., n} and a profile $p = [\\succeq_1,\u2026\u2026\u2026, \\succeq_n]$, f is called:\n$\\bullet\\ \\ $anonymous: if f does not depend on the identity voters, i.e., if for every bijective function $\\pi$: V \u2192 V, we have $f([\\succeq_1,\u2026\u2026\u2026,\\succeq_n]) = f([\\succeq_{\\pi(1)},\u2026\u2026\u2026, \\succeq_{\\pi(n)}])$.\n$\\bullet\\ \\ $neutral: if f does not depend on the identity of candidates, i.e., if two candidates are exchanged in every preference ordering in p, the outcome will change accordingly.\n$\\bullet\\ \\ $Pareto-optimal: if candidate $c_A$ is ranked higher than candidate $c_B$ in all preference orderings, then $c_B \u2209 f(p)$.\n$\\bullet\\ \\ $reinforcing: If $p_1, p_2$ are disjoint profiles and $f(p_1) \u2229 f(p_2) \u2260 0$ then $f(p_1) \u2229 f (p_2) = f (p_1 \u222a p_2)$.\n$\\bullet\\ \\ $monotonic: if whenever a profile p is changed to p' by having one voter lifting the winning candidate, $f(p) = f(p')$.\nTheorem 1 ((Young, 1975)). Suppose that V is a voting method that requires voters to rank the candidates. Then, V is anonymous, neutral and reinforcing if and only if the method is a scoring rule.\nAccording to Theorem 1, majority vote and Borda vote as scoring rules are anonymous, neutral and reinforcing.\nNote simply averaging the predictive scores does not satisfy some relevant properties for providing such as anonymity. That means KGE models with higher predictive scores for the top ranked entity would dominate the final decision. Therefore, we do not consider averaging as baseline in our paper.\nA social welfare function $f_W$ is a mapping from the set of all possible profiles P to a set of all linear orders on C. We next introduce some properties of $f_W$.\n$f_W$ is:\n$\\bullet\\ \\ $weakly Paretian: for $c_1, c_2 \u2208 C$, if $c_1 \\prec_i c_2$ for all $i \u2208 N$, then $c_1 \\prec c_2$.\n$\\bullet\\ \\ $independent of irrelevant alternatives (IIA): if for any $c_1, c_2 \u2208 C$, the relative ranking of $c_1$ and $c_2$ only depends on the relative rankings of $c_1$ and $c_2$ provided by the voters - but not on how the voters rank some third candidate $c_3$.\n$\\bullet\\ \\ $a dictatorship: if there exists a voter $i^\\star \u2208 N$ such that, for all $c_1, c_2 \u2208 C$, $c_1 \\prec^{i^\\star} c_2$ implies $c_1\\prec c_2$.\nTheorem 2 ((Arrow, 1951)). When there are three or more alternatives, then every $f_W$ that is weakly Paretian and IIA must be a dictatorship.\nMajority vote and Borda vote are both weakly Paretian and non-dictatorship (Brandt et al., 2016), therefore according to Theorem 2, they are not IIA. However, range vote as a cardinal voting method meet the Arrow's conditions and additionally provide \"maximum information\" (i.e. provide their opinion of the maximum possible number of candidates) (Vasiljev, 2014; Smith, 2000)."}, {"title": "B Proof of Proposition 1", "content": "Proof. Given a set of testing queries $T = \\{(q_1, e_1), ..., (q_n, e_n)\\}$, we let $\\hat{y} \\in \\mathbb{R}^n$, $\\hat{y}_i = T_K(M_\\Theta^\\star, t_r(q_i, e_i))$ be the vector that contains a 1 if the baseline model regards $e_i$ as a valid answer. Similarly, we let $y' \\in \\mathbb{R}^n$, $y'_i = T_K(M_\\theta, t_r(q_i, e_i))$ be the corresponding vector for a competing model $M_\\theta \u2208 S_\\epsilon(M_\\Theta^\\star)$.\nLet 1 \u2208 \u211dn be a vector consisting only of ones. Then we can express the proportion of testing triples not ranked in top-K as $\\frac{1}{n} ||1 \u2013 \\hat{y}||_1$ and $\\frac{1}{n} ||1 \u2013 y\u2019||_1$ for the baseline and competing model, respectively. We let $\\delta(M_A, M_B)$ denote the discrepancy between two models $M_A, M_B \u2208 M$.\n$\\delta(M_A, M_B) := \\frac{1}{n} \\sum_{\\tau \\in T} 1[T_K(M_A, \\tau) \u2260 T_K(M_B, \\tau)]$"}, {"title": "C More Experiment Settings", "content": "C.1 Personal Identification Issue in FB15k and FB15k237\nWhile FB15k and FB15k237 contain information about individuals, it typically focuses on well-known public figures such as celebrities, politicians, and historical figures. Since this information is already widely available online and in various public sources, its inclusion in Freebase doesn't significantly compromise individual privacy compared to datasets containing sensitive personal information.\nC.2 Pseudocode for Constructing $S_{\\epsilon}(M_{\\Theta}^\\star)'$\nAlgorithm 2 Pseudocode for $S_{\\epsilon}(M_{\\Theta}^\\star"}]}