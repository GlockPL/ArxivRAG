{"title": "Unexplainability of Artificial Intelligence Judgments in Kant's Perspective", "authors": ["Jongwoo Seo"], "abstract": "Kant's Critique of Pure Reason, a major contribution to the history of epistemology, proposes a table of categories to elucidate the structure of the a priori principle of human judgment. The technology of artificial intelligence (AI), based on functionalism, claims to simulate or replicate human judgment. To assess this claim, it is necessary to study whether AI judgment possesses the characteristics of human judgment. This paper argues that AI judgments exhibit a form that cannot be understood in terms of the characteristics of human judgments according to Kant. Because the characteristics of judgment overlap, we can call this Al's uncertainty. Then, I show that concepts without physical intuitions are not easy to explain when their functions are shown through vision. Finally, I illustrate that even if AI makes sentences through subject and predicate in natural language, which are components of judgment, it is difficult to determine whether AI understands the concepts to the level humans can accept. This shows that it is questionable whether the explanation through natural language is reliable.", "sections": [{"title": "Introduction", "content": "Philosophy has explored the concept of human intelligence for thousands of years, and the study of intelligence is a traditional philosophical topic. Logic attempts to explain the structure of knowledge, and epistemology seeks to understand how knowledge is obtained. In computer science, the powerful learning capabilities of deep learning (LeCun, Bengio, and Hinton 2015) have attracted attention. In particular, artificial neural networks (ANNs) (LeCun, Bengio, and Hinton 2015) are commonly used to mimic human intelligence. However, the information that has been produced with respect to the type of judgment used by artificial intelligence (AI) remains insufficient. Indeed, the nature of the judgment made by AI is often considered a black box (Adadi and Berrada 2018). Philosophy, with its tools to explain and analyze intelligence, should investigate the mimicry of the intelligence of ANNs.\nFirst, I examine this topic broadly using Kant's four categories of judgment to show that unexplainability can occur in all of them. Then, explainable AI technology is introduced. I then discuss the SoftMax function (Sharma, Sharma, and Athaiya 2017) that can force AI judgments to make possibility judgments. Next, the limitations of vision-based explainable AI technology are examined based on the fact that concepts whose functions are revealed in vision but have no physical intuitions are difficult to explain. The self and self-consciousness are taken as examples. Finally, I show that even by using natural language processing technology that reveals subjects and predicates for judgments a machine makes, it is difficult to determine whether a machine understands the judgments it makes at a level that a human would accept. Therefore, this shows that explanation through natural language processing is not reliable."}, {"title": "Background", "content": "(Kant 1855), which translated Kant's book into English, is being used in this work because Kant's Critique of Pure Reason was not originally written in English. This paper investigates AI judgments. For this, it is necessary to understand what judgment is. Judgments are divided in philosophy into subjects and predicates. For example, in the sentence \"Socrates is wise,\" \"Socrates\" is the subject, and \"is wise\" is the predicate. Basic information about judgment can be obtained in Kant's discussion of analytic and synthetic judgment (Kant 1855, p. 7).\nIn all judgments wherein the relation of a subject to\nthe predicate is cogitated, (I mention affirmative judg-\nments only here; the application to negative will be\nvery easy,) this relation is possible in two different\nways. Either the predicate B belongs to the subject A,\nas somewhat which is contained (though covertly) in\nthe conception A; or the predicate B lies completely\nout of the conception A, although it stands in connec-\ntion with it. In the first instance, I term the judgment\nanalytical, in the second, synthetical. Analytical judg-\nments (affirmative) are therefore those in which the\nconnection of the predicate with the subject is cog-\nitated through identity; those in which this connec-\ntion is cogitated without identity, are called syntheti-\ncal judgments. The former may be called explicative,\nthe latter augmentative judgments; because the former\nadd in the predicate nothing to the conception of the\nsubject, but only analyse it into its constituent con-"}, {"title": "Kant", "content": "ceptions, which were thought already in the subject,\nalthough in a confused manner; the latter add to our\nconceptions of the subject a predicate which was not\ncontained in it, and which no analysis could ever have\ndiscovered therein. For example, when I say, \u201cAll bod-\nies are extended,\" this is an analytical judgment. For\nI need not go beyond the conception of body in order\nto find extension connected with it, but merely anal-\nyse the conception, that is, become conscious of the\nmanifold properties which I think in that conception,\nin order to discover this predicate in it: it is therefore\nan analytical judgment. On the other hand, when I say,\n\"All bodies are heavy,\" the predicate is something to-\ntally different from that which I think in the mere con-\nception of a body. By the addition of such a predicate\ntherefore, it becomes a synthetical judgment. Judg-\nments of experience, as such, are always synthetical.\nIn his exploration of the possibility of synthetic a priori\njudgments, Kant intricately maps the capabilities of intelli-\ngence, and he illustrates and defines the forms of judgment\n(Kant 1855, p. 58).\nIf we abstract all the content of a judgment, and con-\nsider only the intellectual form thereof, we find that\nthe function of thought in a judgment can be brought\nunder four heads, of which each contains three mo-\nmenta.\nBased on this claim, he presents"}, {"title": "Artificial intelligence", "content": "A. M. Turing provided a striking test to assess the question\nof whether machines can think which is now known as the\nTuring test (French 2000; Turing 2009). According to this,\nif a machine provides responses that are indistinguishable\nfrom the way that a human would respond, then the two\nshould not be differentiated. This aligns with functionalism\n(Levin 2004) in the philosophy of mind.\nWhat makes something a thought, desire, pain (or any\nother type of mental state) depends not on its internal\nconstitution, but solely on its function, or the role it\nplays, in the cognitive system of which it is a part.\nComputer scientists generally and unwittingly adopt func-\ntionalist perspectives on the human mind. Many of these\nscholars contend that when computers perform human-like\nfunctions, we can recognize them as AI.\nAs a prominent AI algorithm, ANNs incorporate inter-\nconnected artificial neurons. Generally, ANNs are trained\nto generate correct output values that correspond to certain\ngiven input values. The significance of ANNs as a repre-\nsentative algorithm is evident in studies demonstrating their\nability to efficiently train approximate solutions for all func-\ntions (Cs\u00e1ji et al. 2001; Hornik, Stinchcombe, and White\n1989; Sch\u00e4fer and Zimmermann 2006; Zhou 2020). This\nimplies that ANNs can find all functions, and therefore, it\ncan be assumed that all functions can be computed by using\nANNS.\nI would like to emphasize that the process of forward\npropagation, through which artificial intelligence makes\njudgments, is completely mathematically explained. How-\never, what judgments are made is not explained like human\njudgment. During their training, all of the error contributions\ngenerated by neurons are calculated using a backpropagation\nalgorithm (LeCun, Bengio, and Hinton 2015), which enables\nthe ANN to learn by minimizing errors. This backpropaga-\ntion algorithm provides information in a readable form using\nnumbers. However, this form cannot be considered to be the\nequivalent of a human judgment. We know why AI makes\njudgments in numerical format, but we do not know expres-\nsions resembling human judgment. The field that aims to\ninterpret AI judgments like human judgments is called ex-\nplainable AI (Adadi and Berrada 2018)."}, {"title": "Unexplainability of AI based on Kant's epistemology", "content": "When AI makes a judgment, it produces numerical outputs.\nFor instance, in image classification, let us say that if an im-\nage contains a face, the AI outputs 1; otherwise, it outputs\n0. However, the determination of whether the judgment cor-\nresponds to the form of \u201cUniversal,\u201d \u201cParticular,\u201d or \u201cSingu-\nlar\" in Table 1 is challenging. For instance, when an ANN\nlearns to classify a human face, people may require addi-\ntional information to distinguish among statements such as\n\"This image has a human face,\u201d \u201cSome of these types of im-\nag\u00e9s have a human face,\u201d and \u201cAll these types of images have\na human face.\" Human beings' expressions of their judg-\nments are made in their exact form with natural language."}, {"title": "", "content": "However, AI outputs are represented as numbers. Therefore,\nwhen AI makes a specific judgment, we cannot classify its\nform in quantity in Table 1.\nIn logic, only \"Affirmative\u201d (A is B) and \u201cNegative\u201d (A is\nnot B) are deemed necessary. However, Kant distinguishes\n\"Infinite\" from \u201cNegative\u201d (A is (not B)) (Kant 1855, p. 59).\nIf I say of the soul, \u201cIt is not mortal,", "The soul is not mortal,\" I have, in\nrespect of the logical form, really affirmed, inasmuch\nas I thereby place the soul in the unlimited sphere of\nimmortal beings. Now, because, of the whole sphere\nof possible existences, the mortal occupies one part,\nand the immortal the other, neither more nor less is\naffirmed by the proposition, than that the soul is one\namong the infinite multitude of things which remain\nover, when I take away the whole mortal part. But by\nthis proceeding we accomplish only this much, that\nthe infinite sphere of all possible existences is in so\nfar limited, that the mortal is excluded from it, and\nthe soul is placed in the remaining part of the extent\nof this sphere. But this part remains, notwithstanding\nthis exception, infinite, and more and more parts may\nbe taken away from the whole sphere, without in the\nslightest degree thereby augmenting or affirmatively\ndetermining our conception of the soul.\nIt is not clear whether ANN outputs correspond to forms of\n\\\"Affirmative,": "Negative,\u201d or \u201cInfinite.\u201d For example, let us\ntake again AI that is trained to output 1 when an image con-\ntains a human face and 0 when it does not. The judgments it\nmakes during training with a value of 0 could be interpreted\nas \"This image has a nonhuman face\" or \"This image does\nnot have a human face.\"\nLet me examine several cases in detail. While \"This im-\nage has a human face,\" \"This image does not have a human\nface,\" and \"This image has a nonhuman face\" are all plau-\nsible interpretations, some are ruled out as a result of the\ncharacteristics of the task. First, we can imagine that the AI\nonly uses the \"Affirmative\u201d form of a judgment when it out-\nputs 1 or 0. When it outputs 1, it states, \"This image has a\nhuman face.\" However, when it outputs 0, no predicate can\nbe affirmed, except for the affirmation of the negative predi-\ncate itself. Hence, we conclude that ANN utilizes a combina-\ntion of the \"Affirmative\u201d and \u201cInfinite\u201d forms of judgment.\nSecond, we can imagine that the AI makes judgments that\nonly take the \u201cNegative\u201d form. Here, an output of 0 would\nmean that \"This image does not have a human face,\" and an\noutput of 1 would not produce any corresponding predicate,\nwith the exception of the", "Affir-\nmative\" and \"Negative\u201d judgments. The use of only the \"In-\nfinite\" form does not provide a solution. Additionally, with\nthe exception of the two combinations of judgments deter-\nmined above, a combination of the ": "nfinite\u201d and \u201cNegative\u201d\njudgments would also seem impossible. If the ANN utilizes\na combination of the", "Affirmative": "nd", "Infinite": "orms of\njudgment, the question then arises: Is this type of learning\nacceptable? The ANN could be affirming that something is"}, {"title": "", "content": "a nonhuman face but lacks further specification.\nFrom a logical point of view, the \"Infinite\" form is\nidentical to the \u201cAffirmative\u201d form, and previous studies\nhave found that ANNs perform their tasks effectively: They\nshould output true values where we intended the output to\nbe true and false where we intend the output to be false.\nHowever, due to the differences in content according to the\nquality form of the judgments, an ANN may decide not to\nmake judgments in the way that we intend. For instance, it\ncould learn to make judgments on an entirely different basis\nfrom humans. An ANN may process this in a mixed way,\nutilizing both \"Negative\" and \"Infinite\u201d forms.\nConsidering Kant's relation of judgment, this case\npresents even more difficult challenges than before. As sum-\nmarized in Table 1, ", "Hypothetical": "ndicates the relation between\nthe two judgments, and \u201cDisjunctive\u201d signifies the relation\nbetween multiple judgments. For example, let's consider a\nmodel that classifies objects as either a car, a dog, or a cat.\nIt's not always clear whether the model's judgment is based\non a single proposition, a causal relationship, or multiple\npropositions. The judgment could simply be \u201cThis image has\na dog.\u201d It could also be", "color.": "lso, it could be", "dog.": "n this last case,\nAI can make an accurate judgment without even possessing\na full understanding of the concept of a dog. If this kind of\nlearning occurs, it suggests that the learning process doesn't\nalign with our intention. Considering that Al is typically\ntrained to classify objects within specific predefined classes,\nthis phenomenon is quite common. Even when employing\nvarious technologies to elucidate decisions by highlighting\nthe dog's location in an image, it is uncertain whether AI\ncomprehends the concept of a dog. This uncertainty arises\nbecause the AI might have made this determination based\non distinguishing it from cats and cars.\nIn the modality form, the \"Assertorical\u201d form could be\nused to solve the XOR problem. Logically, this problem can\nbe solved as shown in Table 2. Here, the top logical ex-\npression is continuously transformed downward, using the\nfact that (A\u2192B) is logically equivalent to \u201cIf A, then B"}, {"title": "", "content": "and 0 from the right node in the input layer. As a result of 0\nfrom the left node and 1 from the right node, 1 is presented\nin the output layer.\n(A\u2227\u00acB)\u2228(\u00acA\u2227B)\n\u00ac(\u00acA\u2228B)\u2228\u00ac(A\u2228\u00acB)\n\u00ac(if A, then B)\u2228\u00ac(if \u00acA, then \u00acB)\nif (if A, then B), then \u00ac(if \u00acA, then \u00acB)"}, {"title": "", "content": "Figure 1 shows that, although the value of true is defined\nas 1 and false as 0, there is no way to know whether AI has\nsolved a logic problem or a classification problem. ANNs\ncan solve the problem as a logical problem, as shown in Ta-\nble 2, or as a problem of classification. Thus, we observe\nthat unexplainability can exist in any of the four categories\nof judgment. Inspired by the uncertainty principle in quan-\ntum mechanics (Busch, Heinonen, and Lahti 2007), we can\ncall this AI's uncertainty. In this case, all forms of judgment\noverlap.\nIn the field of explainable AI research, various technolo-\ngies are being studied (Adadi and Berrada 2018; Selvaraju\net al. 2017). I believe that by developing technologies that\nprovide insights into how AI judgments are made, we can\nachieve some level of explainability. Computer scientists\nhave begun revealing which part of an image Al focuses on\nwhen making judgments. These tools assist us in inferring\nthe subject and predicate. However, neither a subject nor a\npredicate is explicitly stated.\nAmong the four categories that are shown in Table 1,\nmodality seems easier to put into explainable form than\nany of the others. This is because of the SoftMax function\n(Sharma, Sharma, and Athaiya 2017), which is applied to the\nfinal layer, ensuring that the cumulative sum of all options\nequals 1. This makes outputs satisfy the definition of prob-\nability. For example, when looking at an image and deter-\nmining whether there is a dog or a cat, the SoftMax function"}, {"title": "Limitation of vision-based explainable AI", "content": "makes it possible to indicate a 63% probability that there is\na dog and a 37% probability that there is a cat. According\nto Kant, the modality form has the following characteristics\n(Kant 1855, pp. 60\u201361).\nThe modality of judgments is a quite peculiar func-\ntion, with this distinguishing characteristic, that it\ncontributes nothing to the content of a judgment (for\nbesides quantity, quality, and relation, there is nothing\nmore that constitutes the content of a judgment), but\nconcerns itself only with the value of linking verb in\nrelation to thought in general.\nBecause it does not contribute to the content of the judg-\nment, it allows us to explain it in the \"Problematical\" form\neven if we do not know what judgment the machine makes.\nTherefore, even before the advent of explainable AI, it was\npossible to obtain judgments in the form of \u201cProblematical\"\nonly through the SoftMax function. What we want to know\nthrough explainable AI may be the characteristics of the en-\ntire judgment shown in Table 1, but another way is to ar-\ntificially consider only the content of the judgment and in-\ntentionally fix the modality of the judgment. In other words,\nit is not that we will see what kind of judgment was made,\nbut that we will treat it as a possibility judgment no matter\nwhat judgment was made. People might claim that we can\ntransform some judgments into possibility judgments. For\nexample, if something is said to be inevitably and logically\ntrue, the probability is 100%. Can \u201cactually exist,\u201d \u201cneces-\nsary,\u201d and \"possible\" be the same? Treating all judgments as\npossibility judgments seems problematic.\nWe may encounter a range of things that cannot be explained\nwhen dealing with concepts that have no visual intuitions\nbut whose functions are revealed in vision. I address this\ntopic by beginning with an experiment conducted by (Seo\nand Lee 2021). To predict the speed of a ball, it is crucial to\nobserve its movement. However, if the camera that is filming\nit also begins to move in the same manner the ball does, the\nball will always appear stationary in the camera's images.\nHence, the position of the camera must also be taken into\naccount. Coordinate information, such as those provided by\nthe global positioning system (El-Rabbany 2002) can be uti-\nlized in this regard. Moreover, the camera's position can\nbe estimated by using fixed background information. If the\nbackground screen moves, it means that the camera moves.\nIf it moves only to a minimal degree, the camera moves only\nminimally, Figure 2 provides an image of this scenario.\nPerformance decreased when boxes were eliminated (Seo\nand Lee 2021). This indicates that the AI used boxes to\npredict the speed. From this, I conducted additional exper-\niments. Specifically, I seek to demonstrate that the AI uti-\nlizes the boxes for information from boxes by employing\nGrad-CAM (Selvaraju et al. 2017). Grad-CAM focuses on\na classification problem, so it cannot be used for the task\nthat predicts the speed itself. Therefore, it is trained to clas-\nsify class 1 for high speeds based on the average of final\nspeeds in the training set and class 2 for low speeds. When"}, {"title": "", "content": "AI predicts speeds higher than the average, I examine where\nit focuses on its attention.\nI employ a combination of a convolutional neural net-\nwork (CNN) (Lawrence et al. 1997), which processes two-\ndimensional spatial information (e.g., photos), and a long\nshort-term memory (LSTM) (Hochreiter and Schmidhuber\n1997) which processes data obtained along the time axis.\nThis is called a long-term recurrent convolutional network\n(LRCN) (Donahue et al. 2015). The training data comprise\n200 videos, each containing 20 images captured at 0.2-\nsecond intervals. The trained model consists of 4 CNN lay-\ners, 2 LSTMs, and 1 fully connected layer (LeCun, Bengio,\nand Hinton 2015). If the values in parentheses represent the\ninput channel, the output channel, the square kernel size, and\nthe stride in order, the parameters for the CNN layers are\n(3,15,5,2), (15,13,5,2), (13,7,5,2), and (7,3,5,2). The LSTM\nhas two layers and outputs 40 nodes. Finally, the LRCN out-\nputs the speed in the last fully connected layer. The learning\nrate is set to 1e-4, and Adam (Kingma and Ba 2014) is used.\nCross entropy loss function (Wang et al. 2020a) is employed,\nand the learning epoch is set to 2000. Figure 3 presents the\nresult. I use the activation map of the CNN layer that is first\nused in the forwarding process, which processes input val-\nues.\nFigure 3 displays the areas where AI focuses on. The area\nmarked in white is the area that AI focuses on. These white\nareas are near the balls and the boxes. This indicates that the\nspeed of the ball is predicted in relation to the information\nfrom the balls and boxes, as discovered by (Seo and Lee\n2021). It is intriguing that even though I only provided the\ncorrect classes for the speed of the ball and the video, the AI\nbegan to give its attention to the balls and boxes, indirectly\nutilizing the boxes for self-location.\nWe could imagine the machine thinking as follows: While\ntracking the position of the ball and the box, considering that\nmy location has moved by this much, the speed is likely to be\nat this level. Here, we ask the crucial question: Does the fact\nthat a machine seeks self-location mean that it understands\nthe concept of self? Humans address the problem using self-\nconsciousness. Do machines also have such concepts, based\non functionalism? Problems arise from the following. The\nconcepts of dogs and cats have physical intuitions. How-\never, it seems that the concept of self has no such accom-\npanying intuitions. Let us investigate the concepts relating"}, {"title": "", "content": "to humans, animals, and AI.\nWe can think that the structure of human cognition is\nthe same as each other, assuming no illness is present.\nThe cognitive structure itself is completely given and be-\ncomes fully experienced. Research seeks to elucidate how\ncognition functions, leading to investigations of epistemol-\nogy and logic. In epistemology, Kant argues for the divi-\nsion of self-consciousness into transcendental and empir-\nical aspects. This perspective and other questions persist\nin philosophy and in the empirical sciences. First of all,\nseveral attempts have been made to scientifically examine\nour self-consciousness. Bodily self-consciousness has been\nexplored (Blanke 2012; Blanke, Slater, and Serino 2015;\nIonta et al. 2011; Lenggenhager, Mouthon, and Blanke 2009;\nLenggenhager et al. 2007; Serino et al. 2013). In particu-\nlar, (Blanke 2012) includes attempts to determine the neural\nmechanisms that underlie self-identification, self-location,\nand the first-person perspective. The question of whether in-\ndividuals with autism possess the concept of self and how\nit can be enhanced has been investigated (Duff and Flat-\ntery Jr 2014; Huang et al. 2017; Lind and Bowler 2009;\nLombardo et al. 2007; Mitchell and O'Keefe 2008). On the\nother hand, Kant discussed self-consciousness in the follow-\ning paragraph (Kant 1855, pp. 81\u201383).\nThe I think must accompany all my representations,\nfor otherwise something would be represented in me\nwhich could not be thought; in other words, the rep-\nresentation would either be impossible, or at least be,\nin relation to me, nothing. [...] For the manifold rep-\nresentations which are given in an intuition would not\nall of them be my representations, if they did not all\nbelong to one self-consciousness, that is, as my repre-\nsentations (even although I am not conscious of them\nas such), they must conform to the condition under\nwhich alone they can exist together in a common self-\nconsciousness, because otherwise they would not all\nwithout exception belong to me. [...] For example, this\nuniversal identity of the apperception of the manifold\ngiven in intuition, contains a synthesis of representa-\ntions, and is possible only by means of the conscious-\nness of this synthesis. For the empirical consciousness\nwhich accompanies different representations is in it-\nself fragmentary and disunited, and without relation\nto the identity of the subject. This relation, then does"}, {"title": "", "content": "not exist because I accompany every representation\nwith consciousness, but because I join one represen-\ntation to another, and am conscious of the synthesis\nof them. Consequently, only because I can connect a\nvariety of given representations in one consciousness,\nis it possible that I can represent to myself the iden-\ntity of consciousness in these representations; in other\nwords, the analytical unity of apperception is possible\nonly under the presupposition of a synthetical unity.\nThe thought, \"These representations given in intu-\nition, belong all of them to me,\u201d is accordingly just the\nsame as, \"I unite them in one self-consciousness, or\ncan at least so unite them;\" and although this thought\nis not itself the consciousness of the synthesis of rep-\nresentations, it presupposes the possibility of it; that\nis to say, for the reason alone, that I can comprehend\nthe variety of my representations in one conscious-\nness, do I call them my representations, for otherwise\nI must have as many-coloured and various a self as\nare the representations of which I am conscious.\nWhen a human solves the task given to the AI, it is neces-\nsary that all of the intuitions of all images coming in at ev-\nery moment belong to a single self-consciousness. Because,\nif I borrow a phrase from Kant, self-consciousness is con-\nscious of the synthesis of representations. Therefore, self-\nconsciousness is needed to solve the task. And the concept\nof self is needed for considering self-location.\nAnimals cannot directly share their experiences with us,\nso we are in need of evidence to clarify that they are aware of\nthe concepts of self and self-consciousness. Kant also grap-\npled with this difficulty (McLear 2011). Animals seem to\nhave a concept of self. The empirical investigations that have\nresearched this concept in animals have primarily focused\non self-location. For instance, place cells, which are found\nin the hippocampus of mice are known as cells for self-\nlocation (Barry and Burgess 2014; Chen et al. 2019; Danjo,\nToyoizumi, and Fujisawa 2018; Knierim 2015; Nadel and\nEichenbaum 1999). Place cells are activated with respect to\nthe mouse's location. Specific neurons are activated in spe-\ncific spatial positions, and these change as the mouse moves.\nAlso, Mice acquire self-location through background infor-\nmation (Chen et al. 2019). The 2014 Nobel Prize in Phys-\niology or Medicine was awarded for the discoveries of the\ncell. I want to emphasize that animals' ability to understand\nself-location is not a mere claim but a well-established fact.\nThese studies might be pieces of evidence that animals pro-\ncess temporal and spatial information, implying the exis-\ntence of the concepts of self and self-consciousness."}, {"title": "", "content": "The experiment reported in Figure 3 could help un-\nderstand the acquisition of the concept of self and self-\nconsciousness in humans and animals. This concept appears\nto have evolved from the predator-prey relationship. In Fig-\nure 3, we can regard the ball as either the prey or the predator\nand the camera as the opposite in each case. This situation\ncould aid the development of the concepts of self and self-\nconsciousness. Some studies (Bard et al. 2006; Gallup Jr,\nAnderson, and Shillito 2002; Haikonen 2007) have shown\nthat if an animal can recognize itself in a mirror, it has the\nconcept of self-consciousness. However, mirrors do not exist\nin nature except still water surfaces. I believe that the con-\ncepts of self and self-consciousness can be derived from the\nrelationship between predator and prey.\nWhat sets AI apart from humans and animals is that we\ncan create its cognition structure to be similar to our own, as\nit is a technology we create, not a natural science that must\nbe verified empirically. This leads us to interpret that AI has\na cognitive structure that resembles ours, even if only par-\ntially, provided that sufficient conditions are provided. These\ncharacteristics can lead to attaching the word \"Artificial\" to\nan existing concept. Explainable AI involves a long journey\nthat involves identifying the parts of AI that align with hu-\nman cognition and striving to make it similar to ours. How-\never, Al researchers are not obligated to always make AI\nexplainable.\nBased on this generous attitude, previous investigations\n(Bringsjord et al. 2015; Chella, Frixione, and Gaglio 2008;\nZhang and Zhou 2013) have argued that machines have self-\nconsciousness. However, we cannot have any confidence in\nthe concepts that do not have visual intuitions. These con-\ncepts could be explainable in other ways, but they cannot\nbe interpreted in a vision-based explainable way. We can\nonly confidently conclude that there is a relationship be-\ntween these and other parts. Even if vision-based explain-\nable technology develops, it cannot explain anything about\nconcepts that appear in vision but do not have physical intu-\nitons.\nWe can emphasize that AI is created to perform specific\ntasks. In natural language processing, the chat of AI is an\nimportant topic. When a certain sentence is inputted, the AI\noutputs a response (Deng et al. 2019; Zhou et al. 2018). Re-\ncent technologies have displayed chatbots that can pass the\nTuring test. Nevertheless, we do not assert that chatbots have\nself-consciousness. They are just set up to behave that way.\nUsing human chat data, AI is trained to output appropriate\nsentences. As another example, a generative adversarial net-\nwork consists of a discriminator and a generator (Creswell"}, {"title": "Vision, natural language, and definition in Kant's perspective", "content": "et al. 2018; Goodfellow et al. 2014). Here, the generator pro-\nduces random fake data, and the discriminator is responsible\nfor distinguishing between the real and the fake data. Mak-\ning them compete with each other, we train the generator\nto produce data that are as similar to real data as possible.\nFor another example, reinforcement learning studies how an\nagent adapts to the environment through changing actions\nand states after dividing a system into an agent and environ-\nment (Mazyavkina et al. 2021; Sutton and Barto 2018). Both\ncases involve humans dividing two groups in advance and\nmaking them interact. I doubt whether these computer pro-\ngrams acquire the concept of the self in this process. Even\nwithout this concept, they can address problems because we\nspecifically ask them to do this. There is no inherent reason\nwhy they must possess these concepts.\nWhen explaining AI judgment, in addition to highlighting\npixels, it could be helpful to have a textual output. But we\ncould have doubts. To make a judgment, AI must understand\nthe concepts used in a judgment. Does the machine truly"}]}