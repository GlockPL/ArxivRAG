{"title": "TMI-CLNET: TRIPLE-MODAL INTERACTION NETWORK FOR CHRONIC LIVER DISEASE PROGNOSIS FROM IMAGING, CLINICAL, AND RADIOMIC DATA FUSION", "authors": ["Linglong Wu", "Xuhao Shan", "Ruiquan Ge", "Ruoyu Liang", "Chi Zhang", "Yonghong Li", "Ahmed Elazab", "Huoling Luo", "Yunbi Liu", "Changmiao Wang"], "abstract": "Chronic liver disease represents a significant health challenge worldwide and accurate prognostic evaluations are essential for personalized treatment plans. Recent evidence suggests that integrating multimodal data, such as computed tomography imaging, radiomic features, and clinical information, can provide more comprehensive prognostic information. However, modalities have an inherent heterogeneity, and incorporating additional modalities may exacerbate the challenges of heterogeneous data fusion. Moreover, existing multimodal fusion methods often struggle to adapt to richer medical modalities, making it difficult to capture inter-modal relationships. To overcome these limitations, We present the Triple-Modal Interaction Chronic Liver Network (TMI-CLNet). Specifically, we develop an Intra-Modality Aggregation module and a Triple-Modal Cross-Attention Fusion module, which are designed to eliminate intra-modality redundancy and extract cross-modal information, respectively. Furthermore, we design a Triple-Modal Feature Fusion loss function to align feature representations across modalities. Extensive experiments on the liver prognosis dataset demonstrate that our approach significantly outperforms existing state-of-the-art unimodal models and other multi-modal techniques. Our code is available at https://github.com/Mysterwll/liver.git.", "sections": [{"title": "1. INTRODUCTION", "content": "Chronic liver disease poses a significant threat to human health and safety. Chronic hepatitis caused by viral infections of hepatitis B virus (HBV) and hepatitis C virus (HCV) may progress to cirrhosis and hepatocellular carcinoma (HCC) [1]. Therefore, providing clinicians with timely and accurate prognostic results to guide early interventions and treatment decisions is of critical importance in mitigating the health burden of chronic liver diseases. In clinical practice, patients present various non-imaging data including demographic information, complete blood count reports, and lipid analysis reports alongside routine imaging examinations such as Computed Tomography (CT). Meanwhile, radiomics can extract quantitative features from medical imaging data to describe disease characteristics. By integrating comprehensive domain knowledge with auxiliary information, we can develop a holistic and precise prognostic model for chronic liver disease, thereby offering robust support for clinical practice.\nNumerous studies have explored multimodal approaches across various fields [2], often merging different modalities through simple concatenation or attention mechanisms [3, 4]. However, these conventional methods fail to accurately capture the intricate relationships between different modalities and struggle when dealing with a larger number of data types, thereby falling short of the demands of more complex datasets.\nTo address the challenges, we pioneeringly introduce a multimodal approach for prognosis assessment in chronic liver disease. Our work makes three notable contributions: (1) To our knowledge, this is the pioneering study to simultaneously integrate CT imaging, radiomic features, and clinical information into a unified multi-modal learning framework. (2) We design a unique TCAF module to address the heterogeneity among different modalities. This module effectively extracts cross-modal information to generate comprehensive global feature representations. (3) We introduce a TMFF loss to align feature representations across these three modalities during training, ensuring consistent semantic matching."}, {"title": "2. METHODOLOGY", "content": "The overall architecture of our network, as illustrated in Figure 1, consists of three main components: a feature extraction module, a multi-modal interaction module, and a classification head module.\nIn the feature extraction module, we employed a pre-trained 3D ResNet-50 as the visual encoder to extract deep visual features [5]. The radiomics data were tensorized and then processed using a Multilayer Perceptron (MLP) as the radiomics encoder to achieve more abstract feature representations. For clinical text, we utilized a pre-trained BioBERT model as the text encoder, selecting the output of the last hidden layer as the text feature representation [6]. In the classification head module, a pre-trained 1D DenseNet-121 is employed as the classification head."}, {"title": "2.1. Multi-modal Interaction Module", "content": "In this module, we employed linear layers from the outset to map the high-dimensional features obtained from the feature extraction module to a uniform size. Following this, the normalized high-dimensional features are processed by the Intra-Modal Aggregation (IMA) module to enhance their quality. At the core of the IMA is a multi-head self-attention mechanism with 16 attention heads. The primary role of the IMA is to consolidate intra-modal information before feature fusion, thereby improving the effectiveness of the fusion process and enhancing the overall performance of the model."}, {"title": "2.2. Triple-modal Cross-Attention Fusion Module", "content": "The architecture of this module is illustrated in Figure 2. After processing through the IMA module, the enhanced features from different modalities pass through linear layers to calculate their respective query Q, key K, and value V components for attention computation. The Q of each modality is multiplied by the transposed K of the adjacent modality, followed by a softmax layer. This result is then multiplied by the V of the adjacent modality to compute the attention scores for the given modality. Formally, this can be expressed as:\n$$F_{hidden} = softmax(\\frac{Q_iK_j}{\\sqrt{d_k}})V_j = j = i \\mod 3 + 1,$$"}, {"title": "2.3. Loss Function", "content": "TMFF Loss. To align features across different modalities, we drew inspiration from the Similarity Distribution Matching (SDM) loss [7] and developed a TMFF loss specifically for clinical texts, radiomics, and deep image features. The SDM loss was originally designed for the global alignment of visual and textual embeddings and is defined as:\n$$L_{v2t} = KL(P_i || q_i) = \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{n} p_{i,j} \\log \\frac{p_{i,j}}{q_{i,j}}$$\nwhere q denotes the genuine matching probability, and p represents the fraction of a particular cosine similarity score relative to the total sum. The bidirectional SDM loss aggregates both v2t and t2v losses. Thus, the SDM loss between visual and textual modalities can be described as:\n$$L_{SDM} = L_{v2t} + L_{t2v}.$$\nTo align the features of the three modalities, we computed their pairwise SDM losses: $L_{SDM}^{it}$ (images and text), $L_{SDM}^{rt}$ (radiomics and text), and $L_{SDM}^{ir}$ (images and radiomics). Given the asymmetry of the SDM loss across different modality pairs and the similarity between image and radiomics features, aligning images with text should correspond to aligning radiomics with text. Therefore, we define the TMFF loss as follows:\n$$L_{multi} = \\lambda (\\frac{L_{SDM}^{it} + L_{SDM}^{rt}}{2}) + (1 - \\lambda) L_{SDM}^{ir},$$\nwhere \\lambda \u2208 [0, 1] is a scalar weight coefficient that controls the relative importance of the alignment terms.\nOverall Loss Function. We employ a joint loss function to optimize the entire TMI-CLNet network. The overall loss function is formulated as a weighted sum of the task-specific and the multi-modal alignment losses. Mathematically, this can be expressed as:\n$$L_{total} = L_{task} + \\alpha L_{multi},$$\nwhere $L_{task}$ is the commonly used cross-entropy loss function, and \u03b1 is a weighting hyperparameter that balances the contributions of $L_{task}$ and $L_{multi}$. The value of \u03b1 is determined experimentally, with a typical value set to 1."}, {"title": "3. EXPERIMENTS", "content": "Our study utilized a private liver prognosis dataset\nprovided by a partner hospital, comprising 184 patients with\nImplementation Details. All experiments employed 5-fold cross-validation to validate the model's stability and generalization capabilities. The batch size was set to 2, the learning rate to 0.0001, and the number of epochs to 300. The Adam optimizer was used for parameter optimization. All experiments were conducted using an NVIDIA A100 GPU."}, {"title": "3.2. Experimental Results", "content": "Comparison with Other Methods. The first four methods rely on single-modal data as baselines, employing classic deep learning techniques such as ResNet50 [8] and ViT [9] for images, and machine learning techniques like Support Vector Machine (SVM) [10] and Random Forest (RF) [11] for radiomics data. The next three methods represent advanced approaches for handling multi-modal data. SimpleFF [12] sequentially integrates deep learning and radiomics data. HF-BSurv [13] progressively fuses multi-modal data using a factorized bilinear model. Lastly, MMD [14] offers a universal framework for multi-modal feature fusion, accommodating both complete and missing modalities.\nIntegrating CT images with other modalities can sometimes degrade performance such as SimpleFF. This decline may be attributed to the inherent heterogeneity among different modalities, posing challenges for effective multi-modal feature fusion. In contrast, HFBSurv considers modality heterogeneity and cross-modal relationships, surpassing all single-modality methods. Our approach demonstrates superior performance across all metrics compared to other methods. Specifically, our framework achieved an accuracy of 83.12%, an F1 score of 0.7805 and an AUC of 0.8223. These results represent improvements of 3.67%, 0.0572, and"}, {"title": "Ablation Study for Modalities", "content": "Each experiment included the relevant encoders and classification heads. We used a direct connection for unimodal fusion, whereas, for bimodal fusion, we employed cross-attention mechanisms. As shown in Table 2, the results from the bimodal ablation experiments were similar to or slightly lower than those obtained with CT imaging alone. Specifically, the combination of vision and radiomics performed poorly, likely due to inconsistencies in the representation and dimensionality between deep features and handcrafted features, which simple cross-attention mechanisms are unable to effectively handle. However, our method achieved the best performance and this supports the effectiveness of our approach in handling modality heterogeneity and leveraging cross-modal information."}, {"title": "Ablation Study for IMA and TCAF", "content": "To further validate the effectiveness of our modules in reducing intra-modality redundancy and extracting cross-modal information, we conducted ablation experiments on the IMA and TCAF modules. The results, presented in Table 3, show a significant performance improvement when each module was integrated individually. When both IMA and TCAF modules were used together, the accuracy increased by 7.64%. These results demonstrate that the network effectively integrates multi-modal information when all modules are applied, highlighting the strength of our approach in handling complex data."}, {"title": "Ablation Study for Loss Weights", "content": "In the TMFF loss, we introduce weight coefficients A to control the relative importance of individual pairwise SDM losses. This ablation study examines the impact of different weight coefficients. By employing the TMFF loss and adjusting the coefficients A to 0.2, 0.4, 0.6, and 0.8, we observe varying degrees of performance improvements. When A is set to 0.6, our model surpasses the baseline without TMFF loss by 6.45% in accuracy and 0.0687 in AUC. These results underscore the significance of TMFF loss in aligning and integrating features across different modalities."}, {"title": "Interpretative Visualization", "content": "In this study, we utilized XGradCAM [15] to analyze the decision-making process of our proposed TMI-CLNet. We selected four intermediate layers of the visual encoder as target layers, averaged their class activation maps, and converted them into heatmaps. Compared to a purely visual encoder, our fusion network focuses more on the liver region and reduces attention to task-irrelevant tissue areas. This indicates that, guided by clinical and radiomic information, the model excels in extracting relevant features from CT data."}, {"title": "4. CONCLUSION", "content": "This study presented TMI-CLNet, which integrates CT imaging, radiomic features, and clinical infomation to provide early prognosis assessments for patients with chronic liver disease. By introducing the TCAF module and the TMFF loss function, the proposed model can address the heterogeneity among different modalities, thus achieving remarkable performance. Experimental results demonstrate the efficacy of our method. In the future, we will focus on expanding the dataset, conducting multi-center validation, and improving computational efficiency to enhance the robustness and scalability of the model. Additionally, our approach provides a valuable perspective and can be extended to other diseases or modalities, making it suitable for various future applications across different fields."}, {"title": "5. COMPLIANCE WITH ETHICAL STANDARDS", "content": "This study was conducted in accordance with the principles of the Declaration of Helsinki. Approval was granted by the Ethics Committee of Longgang Central Hospital of Shenzhen (2024.5.8/No.2024052)."}]}