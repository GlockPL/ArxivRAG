{"title": "Towards User-Focused Research in Training Data Attribution for Human-Centered Explainable AI", "authors": ["Elisa Nguyen", "Johannes Bertram", "Evgenii Kortukov", "Jean Y. Song", "Seong Joon Oh"], "abstract": "While Explainable AI (XAI) aims to make AI understandable and useful to humans, it has been criticised for relying too much on formalism and solutionism, focusing more on mathematical soundness than user needs. We propose an alternative to this bottom-up approach inspired by design thinking: the XAI research community should adopt a top-down, user-focused perspective to ensure user relevance. We illustrate this with a relatively young subfield of XAI, Training Data Attribution (TDA). With the surge in TDA research and growing competition, the field risks repeating the same patterns of solutionism. We conducted a needfinding study with a diverse group of AI practitioners to identify potential user needs related to TDA. Through interviews (N=10) and a systematic survey (N=31), we uncovered new TDA tasks that are currently largely overlooked. We invite the TDA and XAI communities to consider these novel tasks and improve the user relevance of their research outcomes.", "sections": [{"title": "1 INTRODUCTION", "content": "Understanding AI model behaviour is critical for using Al models in practice, especially in high-stakes domains such as medicine, law, or finance [55]. End-users require the understanding of model behaviours to make informed decisions, while developers need this to build reliable models with limited biases [16]. Explainable AI (XAI) aims to provide insights into Al systems' functioning for humans. It has produced a variety of explanation methods that address different model architectures and data modalities [16, 26].\nDespite the progress, the XAI community has faced criticism for its predominant techno-centric focus on solutions as opposed to the problem of helping users understand model behaviour [18]. Historically, XAI research progressed bottom-up: starting with methods providing qualitative observations, then structuring the field with quantitative evaluations, and eventually considering application in real-world environments including users (see Figure 1). Consequently, XAI was mainly driven by \"[solutionism (seeking technical solutions) and] formalism (seeking abstract, mathematical solutions)\" [18, p.2]. An example of this bottom-up approach is the development of the subfield of feature attribution. Feature attribution explanations quantify each input feature's contribution to the model prediction, highlighting key inputs. One of the first feature attribution methods in the context of deep learning was presented in 2013 introducing the visualisation of neural network input gradients as saliency map explanations and providing 15 qualitative examples as evidence that saliency maps explain [68]. Several other methods followed, either building on this work and proposing"}, {"title": "2 RELATED WORK", "content": "Our work proposes a top-down approach to XAI research, grounded in user needs, which positions it in human-centered explainable AI (HCXAI). We apply this approach to the relatively young subfield of training data attribution (TDA) to provide early insights into users and their needs. To this end, we utilise methods from needfinding. In the following, we relate our work to HCXAI, TDA and needfinding."}, {"title": "2.1 Human-centred explainable Al", "content": "The aim of explainable AI (XAI) research is to study methods that explain the behaviour of AI models [26]. The field has developed various explanation methods over the years to explain different types of tasks and models. Yet, explanations are only meaningful in front of an audience. This human aspect was often neglected in XAI studies [50, 64]. To overcome this gap, Human-centred XAI (HCXAI) emphasises the human in the explanation process and focuses on user perspectives in XAI [20]. In HCXAI, explanations of black-box models are studied holistically as sociotechnical systems centred around the user [19]. HCXAI explores how to design AI technology and explanations that focus on user understanding and practicality [20]. Work in this area covers multiple aspects that affect the user, e.g. human-AI-collaboration [40, 44, 67, 71], design guidelines [28, 45, 76], evaluation of human understanding of explanations [28, 39, 64, 65] and understanding user needs and requirements [10, 35, 37, 38]. Our work contributes to the HCXAI field by proposing an early consideration of user needs to inform ongoing research in XAI methods. We focus particularly on TDA explanations which have not been explored widely in HCXAI before."}, {"title": "2.2 Training data attribution", "content": "Training data attribution (TDA) explains model predictions by pointing to training data samples relevant to the model predictions [29]. TDA differs from the mainstream XAI approach known as feature attribution (FA) where the model predictions are attributed to the features of an input given to the model, disregarding the impact of training data. Interest in TDA has recently accelerated with the paradigm shift towards data-centric AI (DCAI) [36], where the importance of using the right training data is emphasised over other factors like model architecture and optimisation algorithms. Under the context of DCAI, TDA has been utilised to enhance data quality by cleaning faulty data labels [72] and detecting model biases [11, 34, 60, 77]. The recent rise of foundational models, such as large language models (LLMs) and diffusion-based image-generation models, further contributed to the interest in TDA, as it has proven effective"}, {"title": "2.3 Needfinding", "content": "Needfinding is an exploratory and qualitative approach to user research to understand users' needs beyond what simply asking users may tell [57]. It aims at studying the true user needs, regardless of solutions, which may not be apparent to the users themselves. Needfinding stems from the field of design thinking [74], which is an iterative design approach driven by user needs. Needfinding is an iterative process involving users where the subsequent studies extract greater information about their problems and needs through field studies, interviews, and lab studies. In XAI, needfinding has been used to study general needs such as user expectations for XAI [10] or end-user transparency needs in AI decision-support systems [46, 73] by using semi-structured interviews, scenario-based study designs and qualitative analyses of case-study related text files. Needfinding has also been applied to more narrow questions, like extracting user needs for web-search explanations through surveys and semi-structured interviews [37] or understanding explainability needs in human-AI collaborations [40]. Our work extends this overall research direction by studying the user needs of TDA explanations through a two-stage study involving semi-structured interviews and a systematic survey study."}, {"title": "3 TRAINING DATA ATTRIBUTION", "content": "Our work studies user needs for training data attribution (TDA) explanations. This section provides background on TDA. TDA is links specific model behaviour to the training data, treating the data as the root cause for learned behaviours [29]. By providing training data relevant to what the model has learnt, TDA gives insights into the model. TDA has been applied to enhancing data quality and reducing model biases, as well as answering questions around data valuation, memorisation and copyright issues in foundational models in the recent years [14, 24, 46, 80], attracting unprecedented attention.\nFormal definition. The TDA community has focused on a particular instance of attributing a model prediction on a single test sample on a single training sample. More formally, let us consider a training sample $z_{\\text{train}} := (x_{\\text{train}}, y_{\\text{train}})$ and a test sample $z_{\\text{test}} := (x_{\\text{test}}, y_{\\text{test}})$, where x indicates the input and y indicates the true answer the model is supposed to predict. We define the attribution score $\\tau$ for $z_{\\text{train}}$ and $z_{\\text{test}}$ as the change in the correctness of the model prediction on $z_{\\text{test}}$, measured via the model loss $L(f_{\\theta}(x_{\\text{test}}), y_{\\text{test}})$, before and after removing the sample $z_{\\text{train}}$ from the training procedure [30]:\n$\\tau(z_{\\text{train}}, z_{\\text{test}}; f_{\\theta}) := L(f_{\\theta \\setminus z_{\\text{train}}}(x_{\\text{test}}), y_{\\text{test}}) - L(f_{\\theta}(x_{\\text{test}}), y_{\\text{test}})$.\nHere, we indicate the model trained with all training samples as $f_{\\theta}$ and the one trained without $z_{\\text{train}}$ as $f_{\\theta \\setminus z_{\\text{train}}$.\nMain focus of TDA research. Computing $\\tau$ directly by re-training a model without each training sample $z_{\\text{train}}$ is computationally prohibitive. To identify the most influential training sample, the re-training has to be repeated as many times as the number of training samples. The main focus of the TDA community has been on efficient approximations of Equation 1. In 2017, Koh & Liang [41] introduced a gradient-based approximation called influence functions (IF). Since IF was still computationally prohibitive due to the need to compute and invert a massive Hessian matrix, subsequent"}, {"title": "4 UNDERSTANDING USER NEEDS OF TDA EXPLANATIONS THROUGH NEEDFINDING STUDY", "content": "Our work studies user needs of training data attribution (TDA) explanations to facilitate user need-driven TDA research. To this end, we conduct a two-stage needfinding study. First, we identify use cases for TDA explanations using semi-structured interviews. The experimental setup is described in \u00a7 4.1. \u00a7 4.2 presents the interview findings. We derive a design space representing different types of information that may be needed from TDA in \u00a7 4.3. Second, we conduct a systematic survey study targeted at extracting user needs from this space (\u00a7 4.4). IRB approval was obtained from one of the authors' institutions."}, {"title": "4.1 Interview study", "content": "In the interview study, we conduct semi-structured interviews with users of machine learning (ML) applications outside of ML research. This serves as an exploratory study to gain an impression of potential usage scenarios and user needs for TDA RQ1 \u2013 What do users need TDA explanations for?\n4.1.1 Participants. To get a realistic impression of user needs for TDA explanations in practice, we target participants who work with ML applications outside of academia. Our inclusion criteria are: Participants should (1) have at least one month of experience in working with ML systems and (2) work in a high-risk application area according to the EU AI Act [55] (e.g., health care, law enforcement, complete list in Appendix A). This criterion serves to find users who are likely to use explanations, as these areas are subject to further regulations [15, 26]. Recruiting participants poses a challenge, especially in high-risk application areas. Hence, we use purposive sampling [25] and approach participants from the authors' network. We recruit 10 participants from various domains and degrees of experience (see Table 1).\n4.1.2 Interview process. The interviews were conducted during June - September 2023, either in person or remotely through video call using Zoom 1. All interviews are one-on-one conversations in English, except for P10 in German. Participants were first briefed about the purpose of the study and data processing. Upon receiving informed consent, we began the interview recording. In general, the interviews lasted between 30 and 60 minutes. Each interview addresses the following topics to answer RQ1 - What do users need TDA explanations for?:"}, {"title": "4.2 Interview findings", "content": "The interview study aims to provide insights into the potential usefulness of TDA, answering RQ1 - What do users need TDA explanations for? The analysis yields six theme areas, which are common across all participants (see light"}, {"title": "4.2.1 Role of ML systems", "content": "Our participants work with different types of ML models across several domains and in different positions (see Table 1). We find that ML systems are used as work assistants (P1,P10,P11), decision support systems (P3,5,6,7,8,9), and for automation in systems like autonomous vehicles or machinery (P2, P4). These roles provide context for our analysis, as the needs and usage scenarios of XAI may differ based on the level of user interaction. For example, P4 develops automated computer vision models for animal population monitoring with minimal human involvement, while P10 uses a chatbot to handle smaller tasks like answering employee questions. Our analysis is set against the backdrop of these use cases."}, {"title": "4.2.2 Workflow with ML systems", "content": "As ML systems take different roles, the workflows differ. We notice that ML systems are not an essential part of the workflow for system users, as opposed to developers. System users only integrate the ML system in their work when it delivers helpful suggestions and report bugs or ignore it otherwise (P6, P10). We recognise a separation of domain knowledge (collaboration with domain experts): \"Because personally, I cannot know if the model is doing the correct thing [...] business have to tell me\" (P3). For developers whose job revolves around the ML system, we identify themes that are relevant for understanding usage scenarios of XAI and TDA. We find that the development workflow is heavily centred around data: \"[What] drives your model is your data. [...]\" (P8) (data-focused debugging process, data quality checks). Often, standard model architectures are employed with standard hyperparameters (standard model choices) and the main work lies in data curation (P2, P4, P3, P7, P8, P9). This theme area shows that there are two distinct user groups, system users who utilise ML systems and model developers who build ML systems. They have different workflows and potential XAI use cases."}, {"title": "4.2.3 Role of training data", "content": "The training data plays different roles to system users and model developers. While it is the most important variable in a model for developers (P3, P4, P8), training data is generally not interesting to system users (P1, P10). System users often do not have capacity to interact much with the ML system to verify model behaviour, even if they an interest in the underlying mechanisms exists, because they are preoccupied with their own tasks (interesting but no capacity) (P6). However, for developers, training data is a lever for model performance (increasing training data as a bug fix, training data artifacts as main bug causes) (P2, P4, P3, P7, P8, P9, P11). These insights underline that XAI needs to be intuitive, and TDA is particularly suited for model development and less for system users."}, {"title": "4.2.4 Challenges in working with ML systems", "content": "In the interviews, we ask participants about challenges in their ML workflow to explore if XAI can address them. We identified six themes in this theme area. We confirm Kim et al.'s [40] observation that system users' main challenge for working with ML systems is trust calibration (P1, P6, P8, P11). In addition, we find that system users fear they may misuse the ML system due to lacking know-how (P1, P10, P11), highlighting the need for user training to make effective use of ML systems in practice. For model developers, we identify several challenges: Data quality issues are often the root cause of model malfunction (data quality issues) (P2, P4, P3, P7, P8, P9, P11), in turn impacting model validation and evaluation. For instance, participants encounter difficulties due to missing data or absent labels (P2, P4, P3, P7, P8, P9). Additionally, resource constraints (P2, P4, P11) and the stochastic nature of ML models (P2) are pain points for the development and evaluation of ML systems. Our analysis shows that the challenges faced by system users require sociotechnical consideration, whereas challenges faced by model developers are more technical with data quality and evaluation being most pressing. We believe that the latter is a suitable usage scenario for TDA."}, {"title": "4.2.5 Use of XAI in practice", "content": "This theme area groups six themes about the use or lack of XAI in practice. Our analysis reveals that XAI is usually not a standard part of an ML system in practice (XAI is not used, XAI is little used) (P1, P4, P6, P7, P9, P10, P11). In fact, the uses of XAI, in particular feature attribution, among our participants are limited to model development (P2, P3, P8). XAI offers explanations for per-example debugging or act as a sanity check for model reasoning (P8, P9). Unexpectedly, developer participants also use XAI to understand real-world phenomena modelled by their ML system (P3) and as a communication means to get customer buy-in for their ML systems (P8). While explanations have different purposes, we note that participants use XAI tools mainly as an out-of-the-box functionality (e.g., the SHAP library [47]). We find that implementation thresholds must be low for the adoption of XAI in practice."}, {"title": "4.2.6 User perspectives on TDA", "content": "None of the participants were familiar with TDA, highlighting a gap between research and practice. Our analysis, based on discussions about the concept of TDA, identifies four themes reflecting participants' views on XAI and TDA. Overall, participants find XAI useful for debugging and communication, though they noted its usefulness is use-case dependent and not always guaranteed. They expect TDA not to be an exception (P3, P6, P8, P11), but are optimistic about its potential, especially for model development. For instance, P2 mentioned that TDA could save time by identifying faulty training data. One participant suggested combining TDA with feature attribution for debugging (P3). We observed that participants have different notions of interesting training samples, with some focused on finding faulty data (P2, P11) and others on identifying samples similar to a specific one (P6, P7, P8). In conclusion, we identify that TDA could be valuable for model debugging, leading us to focus on this user group and use case in the remainder of this work."}, {"title": "4.3 Defining a design space of TDA explanations", "content": "The interview analysis reveals model developers as a key audience for TDA explanations, with model debugging as a promising use case. Before addressing RQ2 \u2013 What do users need from TDA explanations?, we translate these insights into a design space of TDA explanations in model debugging. The space shall illustrate how explanatory information about training samples and model output could look like, forming the basis for the study addressing RQ2. Interview participants provided rich information about what they usually look out for in the training data when debugging a model. We coded this information into the following themes: Data quality issues, and different notions of interesting training samples. We present the descriptions of the corresponding codes in Table 2 and use them to identify different axes of information that could be represented by TDA explanations. Specifically, we find that different data artefacts are debugged using different actions, with users interested in various metrics and considering different numbers of training samples. From these observations, we define a three-dimensional design space (see Figure 3). In the following, we detail the reasoning behind each axis and connect it to the codes in Table 2 via the ID column."}, {"title": "4.3.1 Axis 1: Action defining relevance.", "content": "The first axis corresponds to the actions participants propose for different types of data artefacts: Removing data, changing the label, reweighting data and collecting more data (x-axis of Figure 3). Participants look at data to remove when it is clearly malicious data and likely hurts model learning, for instance mislabelled (#3) or outdated, historical data (#5). Additionally, we find that participants look for data that they wish to change, for example filling in or imputing missing data (#2), or correcting wrong data scales (#4) and labels (#3). Especially when data is hard to obtain (e.g. in the medical domain), removing data is not an option and correcting data is preferred (P6). Another action is the reweighting of training samples and collecting more data related to the error. These actions emphasise specific samples in the training process to encourage the model to learn them. This is particularly relevant to learning outliers (#6) and handling distribution shifts (#1).\nRemoval, change and addition of data have different effects on a model and therefore represent different quantifications of attribution: For instance, removal as in traditional TDA (Equation 1 in \u00a7 3) or by addition of data. Essentially, these examples map to the questions: How would the model behaviour change if one were to (a) exclude a training sample? or (b) add more training samples like the test sample to the dataset? By understanding preferences in this axis, we aim to identify what is needed for data attribution to be actionable and reflect the debugging process."}, {"title": "4.3.2 Axis 2: Metric.", "content": "The second axis corresponds to the sample-wise metrics participants used when referring to model behaviour: the loss, the probability of the true class label, and the probability of the predicted class label (y-axis of Figure 3). The interviews show not all participants think of model behaviour in terms of sample loss, but also in classification probability. While high loss can be an indicator for wrong or outlier samples (#3, #6), participants voiced their interest in samples where the model has high or low classification probability (#7, #8). Such samples provide context of the global model behaviour to the model developer, and can therefore be helpful for their work.\nThe metric can be seen as a measure by which participants primarily perceive model behaviour in the debugging process. By understanding metric preferences, we aim to identify the metric that helps model developers most in debugging, testing whether the loss as in traditional TDA is the most actionable measure."}, {"title": "4.3.3 Axis 3: Number of training samples.", "content": "The third axis corresponds to the number of training samples participants need to make sense of a model error through the training data. We identify two main options: A single sample and a group of samples (z-axis of Figure 3). We found that model developers usually do not inspect just one training data"}, {"title": "4.4 Survey study", "content": "Building on the findings of the interview study, we aim to study the needs of model developers and understand what kind of information is useful, addressing RQ2 \u2013 What do users need from TDA explanations? We design an online survey that explores the design space of TDA explanations (see Figure 3) using scenario-based design [12]. We piloted the survey to ensure clear phrasing and compatibility across devices. Data was collected from June - August 2024.\n4.4.1 Participants. We recruit participants with prior experience in developing machine learning models to closely align with potential real users. To recruit expert participants, we used snowball sampling alongside social media ads on X and LinkedIn, reaching out to contacts and asking them to share the study link. We expect thematic saturation in the qualitative analysis and meaningful statistics in the quantitative analysis at around 30 replies. Of 34 responses, we excluded three for low-quality answers (short and non-sensical), leaving 31 participants (nine female). 18 participants work in industry while 14 work in academia, with most having 1-5 years of machine learning experience. Four do not work with deep models. Participants come from diverse fields, including biology, health, logistics, and more. While the data types used by participants are diverse too, the majority of participants work with image or tabular data, as used in our study (see Figure 4). The participants were compensated with 25\u20ac via Tremendous\u00b9.\n4.4.2 Scenario-based design. The main part of the study is constructed with scenario-based design [12] to base the analysis on a tangible usage scenario. We create two scenarios of model debugging in an interactive mock-up of a model development suite (see Figure 5). The choice of data types (image and tabular data) is based on the dominant modalities of interview participants. The study puts participants in two imaginary scenarios where they are model developers in a company that builds (1) a bird classification app, and (2) a credit scoring app. The company recently acquired a data-centric tool to help model developers debug their models by understanding errors and identifying training data relevant to those errors. The tool is customisable to adapt to the developer's preferences. The customisation choices, based on the TDA design space (see Figure 3) are:"}, {"title": "4.4.3 Survey structure.", "content": "The survey structure is shown in Figure 6. Participants are first briefed on the study's objective and data processing. After providing consent and confirming they meet inclusion criteria, they are asked preliminary demographic questions for context to the later analysis. In the remaining study, we aim to extract the participant's preferences through a usage scenario (\u00a7 4.4.2). The participants are presented with the scenario description and prompted with \"Imagine you are a model developer in a company [...]\" to immerse them in the use case. Then, we ask the participants follow-up questions to understand the reasoning behind their choices through free-text questions."}, {"title": "4.4.4 Data analysis.", "content": "We analyse the survey study both quantitatively and qualitatively. For the quantitative analysis, we aim to understand whether a specific notion of TDA is preferred across participants because previous TDA studies have been centred around a single notion of TDA (\u00a7 3). Since the traditional TDA setting is already a part of the design space, studying preferences gives an answer to whether participants really preferred this particular scenario. We further assess if participants prefer a specific scenario if not, it is a strong signal that the research should be diversified into multiple possible scenarios serving different user needs.\nTo this end, we compute the distribution of the participants' choices (action, metric and number of samples). Furthermore, we test whether the collected data indicates a statistically significant preference for a specific setting. We perform a $\\chi^2$ test as the data is categorical [58]. For the number of training samples axis, we perform Fisher's exact test as the choice frequencies are low [17]. Specifically, our hypotheses are:\n$H_0$: There is no clear preference for a specific TDA setting.\n$H_1$: There is a clear preference for a specific TDA setting.\nIn other words, we test the goodness of fit of our observations against a uniform distribution across categories of an axis. Since we conduct multiple tests on the same data, we apply a Bonferroni correction [79] and define statistical significance at p < 0.05. Additionally, we compute each axis category's entropy H(X) and normalised entropy $H(X)_{\\text{norm}}$ (normalised by $\\log_2(k)$ with k as the number of categories in an axis, e.g. three for the metric axis) to get an indicator of the diversity of preferences [3].\n$H(X) = \\sum \\limits_{x} p(x) \\log p(x)$\nIf the entropy is high, the participant's preferences are highly diverse, meaning there is no agreement on a certain preferred notion of the relevance of training data.\nThe qualitative analysis aims to understand the reasons behind participant choices and get deeper insight into user preferences. The free-text responses in the follow-up questions are analysed using inductive thematic coding [9] by two coders. Per question, we analyse the common themes among participants to extract the reasons behind user preferences and needs. The analysis consists of several coding workshops and is depicted in Figure 7. We measure interrater agreement by the percentage of overlapping themes. The final agreement is 97.1%."}, {"title": "4.5 Survey findings", "content": "We conduct the survey study to answer RQ2 \u2013 What do users need from TDA explanations? The survey data analysis indicates a clear connection between users' intuition of what causes a model error and their TDA preferences. We elaborate further in the following.\n4.5.1 Quantitative results. The quantitative analysis builds on the customisation choices of the debugging interface (see Quantitative data collection in Figure 6). Table 3 shows the frequency of selections and the associated p-values. We test if there exists a setting of action, metric and number of training samples which is consistently chosen and particularly useful to model developers. We are especially interested in whether there is a clear preference for removal, loss and one training sample as these correspond to the traditional notion of TDA introduced in \u00a7 3. For the action axis, there is no statistically significant preference for any choice and the distribution is rather uniform across choices. While the results may show a higher frequency of participants choosing loss as a metric, we observe no statistically significant preference. Hence, we fail to reject $H_0$ for the action and metric axes and conclude that there is no dominant preference for a particular TDA setting. For the number of samples axis, however, we find a statistically significant preference for a group of 10 samples.\nThe entropy analysis supports this conclusion (see Table 4). As a measure of diversity, normalised entropy scores range from 0.588 to 0.997, showing a highly diverse (>0.5) distribution of choices. This is especially true for the Action defining relevance and Metric axes with high scores (>0.8). There is less diversity in the number of training samples but a clear preference for choosing groups of samples, confirming Ilyas et al.'s [33] intuitions for studying group attribution. In conclusion, the quantitative analysis reveals that the preferred TDA explanations represent highly diverse information with a clear need for group attribution.\n4.5.2 Qualitative results. The inductive thematic coding analysis of the free-text answers resulted in four thematic areas and theme types covering 30 themes (see Figure 8): (1) Action depends on the user's hypothesis, (2) Individual metric preferences, (3) Group attribution preferences, and (4) Reliability of explanations. For the theme types, we find themes that correspond to the user's hypothesis about the error and observe that these are often coded together with a corresponding proposed action. However, preferred metrics and preferred number of training samples do not correspond to the user's hypothesis. Additionally, we identify four themes that do not correspond to any category but give interesting insights and context to the analysis. The full description of each theme is in Appendix D."}, {"title": "5 DISCUSSION", "content": "This work presents a needfinding study with Al practitioners for training data attribution (TDA) explanations to inform user-focused TDA research in a top-down approach. We summarise our findings to provide answers to the initial research questions (\u00a7 5.1), identify research gaps that need to be addressed for more user-focused TDA (\u00a7 5.2) and discuss limitations (\u00a7 5.3)."}, {"title": "5.1 Answers to the research questions", "content": "5.1.1 RQ1: What do users need TDA explanations for? To answer this question, we conducted the interview study (\u00a7 4.1). Our findings suggest that TDA as an explanation method has the potential to serve similar high-level purposes as other explanation techniques, namely enabling users to understand model behaviour, especially unexpected model behaviour. By offering insights into model behaviour, TDA can assist end-users in calibrating trust by providing context for model decisions and support model developers in debugging errors by identifying root causes in the training data. This data-centric approach is particularly valuable in model development, leading us to identify model developers as the primary audience for TDA explanations and model debugging as the key use case.\n5.1.2 RQ2: What do users need from TDA explanations? To answer this question, we designed a survey study that extracts user preferences for the information represented by TDA (\u00a7 4.4). Our study reveals that users need different types of information to understand model behaviour, depending on their hypotheses of why the model errs during development. This necessitates TDA explanations that are flexible and adaptable to individual needs. These needs vary based on the application domain and the user's level of expertise. Experienced users, may form hypotheses about model errors based on their past experiences. In contrast, less experienced users may form hypotheses based on what they learnt previously about machine learning models. Users have a preference for group attribution, as groups provide more information than individual samples. Additionally, users need TDA explanations to be reliable considering how data-centric actions might affect the model as a whole. Reliability entails two main aspects: First, the information needs to be reliable for it to be actionable and effective for the task (i.e., model debugging). Second, TDA explanations should also account for any broader changes in the model's behaviour that may affect the attribution to training samples.\n5.1.3 RQ3: To what extent do existing TDA approaches align with user needs? To answer this question, we reflect on existing TDA approaches in light of the identified needs: TDA explanations need to be adaptable to the user and provide reliable and actionable insight while maintaining consistency with the overall model behaviour. From these needs, we identify different TDA tasks that users need, next to the overarching need for reliability (see Table 5).\nTraditional TDA measures the influence of a training sample on model predictions by analysing the impact of its removal through leave-one-out retraining [30]. This method essentially evaluates model predictions in the context of a counterfactual change in the training data. It has been studied across different tasks, such as identifying mislabelled data, understanding adversarial vulnerabilities, and detecting domain mismatches [41]. Mislabel identification, in particular, is a common evaluation strategy employed in several studies [54, 61, 66]. Consequently, traditional TDA partially addresses the need for adaptability to different use cases. However, since TDA quantifies the relevance of a training sample by its removal, it is particularly suited for scenarios where data elimination is necessary, such as malicious"}, {"title": "5.2 Overlooked research topics for user-focused training data attribution", "content": "This work adds to a collection of human-centred explainable AI (HCXAI) works calling to centre XAI research around the user (e.g., [10, 19, 38, 46, 48, 65]). By focusing on TDA explanations specifically, we identify several research areas that are necessary to study to advance TDA research towards a focus on users.\n\u2022 Mental models and TDA: Our study supports previous work [38, 65] that the most useful explanations depend on the user's mental model of why the model errs: We find that what information is actionable to users depends in their hypothesis for reasons of model behaviour (Theme area: Action depends on user's hypothesis). Therefore, future research should explore how TDA influences and is influenced by mental models, as well as how TDA can be adapted to reflect mental models (e.g. by defining relevance through the action of upweighting as opposed to removal), potentially leading to more aligned and actionable explanations that are well-defined.\n\u2022 User-centred group attribution: The survey findings reveal a clear need for attributing model behaviour to groups of training samples, as it provides more informative insights (Theme area: Group attribution preference). While existing approaches [7, 42, 54] address this, our findings highlight the need for creating groupings that make sense to users (e.g., based on ground truth or predicted class), an aspect not yet widely explored in TDA research.\n\u2022 Holistic understanding of model errors: Our thematic analysis of survey responses highlights the importance of understanding the model error itself before attempting to fix it (Theme: Grouping erroneous test samples). If the root cause is a spurious correlation, the error likely extends beyond the current test sample and may"}, {"title": "5.3 Limitations", "content": "This study has several limitations. First, the use of purposive and snowball sampling techniques in participant recruitment may have introduced selection bias."}]}