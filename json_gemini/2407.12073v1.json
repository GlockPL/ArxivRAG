{"title": "Relational Representation Distillation", "authors": ["Nikolaos Giakoumoglou", "Tania Stathaki"], "abstract": "Knowledge distillation (KD) is an effective method for transferring knowledge from a large, well-trained teacher model to a smaller, more efficient student model. Despite its success, one of the main challenges in KD is ensuring the efficient transfer of complex knowledge while maintaining the student's computational efficiency. Unlike previous works that applied contrastive objectives promoting explicit negative instances, we introduce Relational Representation Distillation (RRD). Our approach leverages pairwise similarities to explore and reinforce the relationships between the teacher and student models. Inspired by self-supervised learning principles, it uses a relaxed contrastive loss that focuses on similarity rather than exact replication. This method aligns the output distributions of teacher samples in a large memory buffer, improving the robustness and performance of the student model without the need for strict negative instance differentiation. Our approach demonstrates superior performance on CIFAR-100, outperforming traditional KD techniques and surpassing 13 state-of-the-art methods. It also transfers successfully to other datasets like Tiny ImageNet and STL-10. The code will be made public soon.", "sections": [{"title": "1. Introduction", "content": "Knowledge distillation (KD) is a technique that facilitates the transfer of knowledge from a larger, well-trained model (teacher) to a smaller, more efficient model (student). This is achieved by minimizing the Kullback-Leibler (KL) divergence between their outputs, allowing the student model to approximate the performance of the teacher model while maintaining lower computational complexity. This process is particularly beneficial for deployment in resource-constrained environments. A critical aspect of KD is representation learning, which enables the student model to acquire meaningful feature representations that capture the underlying data distribution. Effective representation learning in KD can significantly boost the performance of the student model across various domains, such as natural language processing, computer vision, and speech recognition [10, 15,22]. Despite these advantages, a major challenge in KD is the efficient transfer of complex knowledge from the teacher to the student model. Ensuring that the student model captures the abstract features and nuanced information present in the teacher model, without the need for similar computational capacity, remains a significant bottleneck.\nRecent advancements in KD have introduced various strategies to enhance the transfer process, addressing the bottlenecks associated with representation learning. Techniques such as adversarial training, attention transfer, and contrastive representation distillation have been proposed to further align the representations learned by the student model with those of the teacher model. Adversarial training involves using adversarial examples to improve the robustness of the student model, while attention transfer focuses on aligning the attention maps of the teacher and student models to ensure that both models focus on similar regions of the input data [?, 31]. Contrastive representation distillation aims to improve the quality of learned representations by encouraging the student model to produce similar representations for similar inputs while differentiating dissimilar inputs. Additionally, methods like BookKD reduce distillation costs by decoupling knowledge generation and learning processes, leading to improved performance with minimal resource consumption [35]. Other approaches like structured KD focus on training compact networks by distilling structured knowledge from cumbersome networks [15]. These methods aim to capture richer information from the teacher model, leading to more robust and generalized student models. The continuous evolution of KD techniques underscores its pivotal role in developing efficient deep learning models capable of high performance with lower computational costs, while overcoming the challenges of effective knowledge transfer.\nOur proposed method, Relational Representation Distillation (RRD), introduces a novel approach to address these challenges by maintaining relational consistency between the teacher and student models. By leveraging a large memory"}, {"title": "buffer of teacher samples to align their output distributions,\nour method ensures consistent relational structures, thereby\nenhancing the robustness and performance of the student\nmodel.", "content": "Our contributions are threefold:\n1. We propose a novel relational consistency framework\nfor KD that leverages a memory buffer to maintain re-\nlational structures between teacher and student models.\n2. We introduce a relational consistency loss that aligns\nthe similarity distributions of teacher and student out-\nputs, improving the robustness and generalization of\nthe student model.\n3. We validate the effectiveness of RRD hthrough compre-\nhensive testing on standard benchmarks, showcasing\nconsiderable gains in both accuracy and robustness.\nRRD surpasses other methods with a 19.22% relative\nimprovement\u00b9 over conventional KD. When integrated\nwith KD, it demonstrates a 55.18% relative improve-\nment over standard KD, underscoring its robust poten-\ntial in enhancing model performance significantly.\nThe rest of this paper is organized as follows. Section 2\nreviews related work in KD and contrastive learning. Section\n3 details our proposed methodology. Section 4 presents our\nexperimental setup and results, and Section 5 concludes the\npaper."}, {"title": "2. Related Work", "content": "The study of [10] on KD introduced the idea of trans-\nferring knowledge from large, complex models to smaller,\nmore efficient models without losing their generalization ca-\npabilities. This technique involves temperature scaling in the\nsoftmax outputs to effectively capture and convey the knowl-\nedge from the teacher model. Numerous enhancements to\nthis method have been proposed. For example, using in-\ntermediate representations or \"hints\" to guide the learning\nprocess [22], and aligning the attention maps of teacher and\nstudent models to ensure they focus on similar areas during\ntraining [31]. Other methods preserve relational knowledge\nbetween samples [27] or align the correlation structures be-\ntween teacher and student models [20]. Additionally, some\ntechniques use variational inference to improve the knowl-\nedge transfer process [1]. Further developments include\nfocusing on structural relationships between data points to\nensure the student model learns relational information [18],\nand maintaining the internal dynamics of neural networks\nduring distillation [9, 19]. Other notable techniques include\nnetwork compression through factor transfer [12], optimiz-\ning networks for better transfer learning [30], promoting se-\nlectivity in the distillation process [11], and using contrastive\nlearning objectives to enhance representation learning [26].\nSelf-supervised learning has significantly impacted rep-\nresentation learning by leveraging unlabeled data. Methods\nsuch as SimCLR [3] and MoCo [7] use contrastive losses\nto learn useful representations by maximizing agreement\nbetween different augmented views of the same data point.\nReSSL [34] introduces relational self-supervised learning,\nwhich explores the relationships between data points. These\napproaches have inspired various KD methods, including\nour proposed method, which adapts relational consistency\nfrom self-supervised learning to the KD framework.\nOur method, RRD, differentiates itself from state-of-the-\nart methods by focusing on maintaining relational consis-\ntency between the teacher and student models. Unlike tra-\nditional KD methods that often rely on direct alignment of\nlogits or intermediate features, RRD leverages a large mem-\nory buffer of teacher samples to align the relational structures\nof the output distributions. This approach not only enhances\nthe robustness and performance of the student model but\nalso provides a more flexible and scalable solution for KD."}, {"title": "3. Methodology", "content": "This section presents our methodology to improve the\nefficiency and accuracy of KD. Our method, Relational Rep-\nresentation Distillation (RRD), focuses on maintaining rela-\ntional consistency between the teacher and student models by\nleveraging a large memory buffer of teacher samples to align\ntheir output distributions. By ensuring consistent relational\nstructures, RRD enhances the robustness and performance\nof the student model. Figure 1 shows an overview of the\nproposed RRD method."}, {"title": "3.1. Preliminary", "content": "KD involves transferring knowledge from a high-capacity\nteacher neural network, denoted as $f_T$, to a more compact\nstudent neural network, $f_S$. Consider $x_i$ as the input to these\nnetworks, typically an image. We represent the outputs at the\npenultimate layer (just before the final classification layer,\nor logits) as $z_i^T = f_T(x_i)$ and $z_i^S = f_S(x_i)$ for the teacher\nand student models, respectively. The primary objective of\nKD is to enable the student model to approximate the perfor-\nmance of the teacher model while leveraging the student's\ncomputational efficiency. The overall distillation process can\nbe mathematically expressed as:\n$\\mathcal{L} = \\mathcal{L}_{sup}(y_i, z_i^S) + \\lambda \\cdot \\mathcal{L}_{distill}(z_i^T, z_i^S)$ (1)\nwhere $y_i$ represents the true label for the input $x_i$ and $\\lambda$ is\na hyperparameter that balances the supervised loss and the\ndistillation loss. The loss $\\mathcal{L}_{sup}$ is the alignment error between"}, {"title": "3.2. Relational consistency", "content": "Our method is inspired by self-supervised learning. Tra-\nditional contrastive self-supervised learning uses instance\ndiscrimination as a pretext task and relies on $(K+1)$-softmax\nclassification, where different instances are pushed apart, and\naugmented views of the same instance are expected to have\nidentical features. Applying such properties to KD imposes\noverly strict constraints, where a contrastive loss encour-\nages the representations from the teacher and student models\nfor the same input data to be similar, while simultaneously\npushing apart representations from different data inputs:\n$\\mathcal{L}_{contrast}(z, z') = - \\log \\frac{\\exp(\\phi(z, z')/\\tau)}{\\sum_{k=1}^M \\exp(\\phi(z, z')/\\tau)}$ (2)\nwhere $\\mathcal{L}_{contrast}$ is the InfoNCE [28] loss, typically employed\nin self-supervised methods, $\\phi$ is a similarity function, $\\tau$ is\na temperature parameter, and $M$ is the number of negative\nsamples.\nIn this way, we do not encourage explicit negative in-\nstances (those to be pushed away) for each instance; instead,\nwe leverage the pairwise similarities to explore their relation-\nships. We pull the features of the student $f_S$ and teacher $f_T$.\nAs a result, our method relaxes Equation (2), where different\nteacher and student outputs do not always need to be pushed\naway from each other; and teacher and student need to share\nsimilar but not exactly the same features.\nConcretely, given an input image $x_i$, and the outputs\n$z_i^T = f_T(x_i)$ and $z_i^S = f_S(x_i)$ for the teacher and student\nmodels, respectively, we calculate their similarity measured\nby $\\phi(z_i^T, z_i^S)$. A softmax layer can be adopted to process the\ncalculated similarities, which then produces a relationship\ndistribution:\n$\\overline{p}_{i}^{T} = \\frac{\\exp(\\phi(z_i^T, z_i^S)/T_t)}{\\sum_{k=1}^M \\exp(\\phi(z_k^T, z_i^S)/T_t)}$ (3)\nwhere $T_t$ is the temperature parameter for the teacher net-\nwork. At the same time, we can calculate the relationship\ndistribution for the student model as:\n$\\overline{p}_{i}^{S} = \\frac{\\exp(\\phi(z_i^T, z_i^S)/T_s)}{\\sum_{k=1}^M \\exp(\\phi(z_k^T, z_i^S)/T_s)}$ (4)\nwhere $T_s$ is a different temperature parameter for the student\nnetwork. We propose to push the relational consistency\nbetween $\\overline{p}^T$ and $\\overline{p}^S$, similar to [34], by minimizing the KL\ndivergence, which can be formulated as:\n$\\mathcal{L}_{relational}(z, z') = KL(\\overline{p}^T || \\overline{p}^S) = H(\\overline{p}^T, \\overline{p}^S) - H(\\overline{p}^T)$ (5)\nwhere KL denotes the KL divergence between $\\overline{p}^T$ and $\\overline{p}^S$.\nSince $\\overline{p}^T$ will be used as a target, the gradient will be clipped\nhere to avoid the model collapsing, thus we only minimize\nthe cross-entropy term $H(\\overline{p}^T, \\overline{p}^S)$ in our implementation.\nHowever, the quality of the target similarity distribution\n$\\overline{p}^T$is crucial for reliable and stable training. To ensure this,\nwe maintain a large memory buffer $Q$, storing the feature\nembeddings from teacher batches. The relational consis-\ntency between the teacher and student models is enforced\nby aligning the similarity distributions of their outputs using\nthe KL divergence."}, {"title": "4. Experiments", "content": "We evaluate our RRD framework in the KD task of model\ncompression of a large network to a smaller one.\nDatasets. (1) CIFAR-100 [13] contains 50,000 training\nimages with 500 images per class and 10,000 test images. (2)\nSTL-10 [5] consists of a training set of 5,000 labeled images\nfrom 10 classes and 100,000 unlabeled images, and a test\nset of 8,000 images. (3) Tiny ImageNet (TIN-200) [6] has\n200 classes, each with 500 training images and 50 validaton\nimages.\nSetup. We experiment on CIFAR-100 with student-\nteacher combinations of various capacity, such as ResNet [8]\nor Wide ResNet (WRN) [32], VGG [25], MobileNet [23],\nand ShuffleNet [16,33] (more details about the network ar-\nchitectures are described in the supplementary material). We\nuse $M = 16384$ negative samples and set the temperature\nparameter of the student to $T_s = 0.04$ and of the teacher to\n$T_t = 0.07$. Both the student and teacher outputs are pro-\njected to a 128-dimensional space. We use a projection head\nof a single linear layer, followed by l2 normalization. We\ntrain for a total of 240 epochs. More details on the training\ndetails are described in the supplementary material."}, {"title": "4.1. Results on CIFAR-100", "content": "Table 1 and Table 2 present the top-1 accuracies of stu-\ndent networks trained using different distillation techniques\nacross various teacher-student architectural pairings. Ta-\nble 1 examines pairings where both student and teacher\nmodels share similar architectural styles, while Table 2 fo-\ncuses on cross-architecture distillations. Our proposed loss\nconsistently outperforms the conventional KD technique, as\nindicated by the green upward arrows (\u2191). While the stan-\ndalone performance of our method is comparable to CRD,\nits integration with KD not only achieves higher accura-\ncies but in some cases, surpasses the performance of the\nteacher networks, such as in the distillation of WRN-40-2 to\nShuffleNet-v1. The enhanced performance of our distillation\nmethod can be credited to multiple factors that collectively\nimprove the transfer of knowledge from the teacher to the\nstudent model. Our approach uses a unique loss function\nthat complements KD's primary focus on matching the soft-\nened output logits of the teacher and student. We introduce\nan additional layer of representational alignment that en-\nsures not only the final outputs but also the intermediate\nfeature representations of the student closely match those\nof the teacher. This dual focus allows the student model to\nmimic the teacher's outputs and develop more robust and\ngeneralizable internal representations."}, {"title": "4.2. Capturing inter-class correlations", "content": "Cross-entropy loss overlooks the relationships among\nclass logits in a teacher network, often resulting in less ef-\nfective knowledge transfer. Distillation techniques that use\n\"soft targets\", such as those described by [10], have success-\nfully captured these relationships, improving student model\nperformance. Figure 4 assesses the effectiveness of differ-\nent distillation methods on the CIFAR-100 KD task using\nWRN-40-2 as the teacher and WRN-40-1 as the student. We\ncompare students trained without distillation, with attention\ntransfer [31], with KL divergence [10], and with our pro-\nposed RRD method. Our findings show that RRD achieves\nclose alignment between teacher and student logits, as evi-\ndenced by reduced differences in their correlation matrices.\nWhile RRD does not match CRD [26] in terms of exact\ncorrelation alignment, it significantly enhances learning ef-\nficiency and reduces error rates. The smaller discrepancies\nbetween teacher and student logits indicate that the RRD\nobjective captures a substantial portion of the correlation\nstructure in the logits, resulting in lower error rates, though\nCRD achieves a slightly closer match."}, {"title": "4.3. Transferability of representations", "content": "Our study investigates how knowledge is transferred from\na larger teacher network (WRN-40-2) to a smaller student\nnetwork (WRN-16-2), aiming to develop versatile represen-\ntations suitable for a range of tasks and datasets. We apply\nthis technique by having the student network either learn\ndirectly from the CIFAR-100 dataset or via distillation. In\nthis setup, the student network is employed as a static feature\nextractor for images from the STL-10 and TIN-200 datasets,\nboth adjusted to a 32 \u00d7 32 resolution. We evaluate the adapt-\nability of these features by training a linear classifier on\nthe final feature layer to conduct classifications with 10 cat-\negories for STL-10 and 200 categories for TIN-200. We\ndocument the impact of various distillation approaches on\nthe transferability of these features in Table 3. Our findings\nreveal that, except for the FitNet method, all distillation tech-\nniques significantly improve the feature transferability on\nboth datasets. Notably, while the teacher network achieves\nthe highest performance on CIFAR-100, its features show"}, {"title": "4.4. Visualization of t-SNE embeddings", "content": "We provide t-SNE [29] visualizations to compare the em-\nbeddings generated by various KD methods and the teacher\nnetwork on the CIFAR-100 dataset. Figure 3 displays the\nembeddings from the teacher network, a WRN-40-2, and\nthe student network, WRN-40-1, under standard training\nas well as distillation using AT and RRD where we limit\nthe dataset to the first 10 classes of CIFAR-100 to offer a\nclearer understanding of the embedding space. We observe\nimproved consistency in the embedding distributions be-\ntween the teacher and student networks, indicating that RRD\neffectively transfers the knowledge of the teacher's feature\nspace to the student. Our relational consistency approach\nensures that the spatial relationships in the embedding spaces\nof both the student and teacher models are preserved. This\nalignment not only enhances the student's performance but\nalso maintains the integrity of the feature representations\nlearned by the teacher."}, {"title": "4.5. Ablation study", "content": "There are two main hyperparameters in our objective: the\nnumber of negative samples $M$ in the memory buffer $Q$ of\nEquation (3) and Equation (4); and temperature parameters\n$T_s$ and $T_t$ of Equation (3) and Equation (4) that modulate the\nsoftmax probability. We also ablate the hyperparameter $B$\nthat balances the KD loss. For the ablation study, we adopt\nWRN-40-2 as the teacher and WRN-16-2 as the student.\nExperiments are conducted on CIFAR-100, and the results\nare shown in Figure 4.\nNumber of negatives $M$. We validated different values\nfor $M$: 256, 1024, 2048, 4096, 8192, 16384, and 32768.\nAs shown in Figure 4a, increasing $M$ leads to improved\nperformance. However, the difference in error rate between\n$M = 4096$ and $M = 16384$ is less 0.5%. Therefore, we\nuse $M = 16384$ for reporting the accuracy, while in practice\nlower $M$ should suffice. Going beyond M 16384 proves\nto harms performance."}, {"title": "5. Conclusions", "content": "Our method offers a significant advancement in KD by\nmaintaining relational consistency between teacher and stu-\ndent models. RRD leverages a large memory buffer of\nteacher samples to align their output distributions, ensur-\ning consistent relational structures throughout the learning\nprocess. Unlike traditional approaches, RRD uses pairwise\nsimilarities and a relaxed contrastive loss to enhance the\nrobustness and performance of the student model without\nexplicit negative instances. Through comprehensive test-\ning on CIFAR-100, TIN-200, and STL-10 datasets, RRD\nconsistently outperforms traditional KD techniques and state-\nof-the-art methods, demonstrating substantial improvements\nin accuracy, robustness, and transferability of learned repre-\nsentations. Our experiments on model compression highlight\nRRD's ability to provide a scalable solution."}]}