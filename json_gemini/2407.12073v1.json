{"title": "Relational Representation Distillation", "authors": ["Nikolaos Giakoumoglou", "Tania Stathaki"], "abstract": "Knowledge distillation (KD) is an effective method for transferring knowledge from a large, well-trained teacher model to a smaller, more efficient student model. Despite its success, one of the main challenges in KD is ensuring the efficient transfer of complex knowledge while maintaining the student's computational efficiency. Unlike previous works that applied contrastive objectives promoting explicit negative instances, we introduce Relational Representation Distillation (RRD). Our approach leverages pairwise similarities to explore and reinforce the relationships between the teacher and student models. Inspired by self-supervised learning principles, it uses a relaxed contrastive loss that focuses on similarity rather than exact replication. This method aligns the output distributions of teacher samples in a large memory buffer, improving the robustness and performance of the student model without the need for strict negative instance differentiation. Our approach demonstrates superior performance on CIFAR-100, outperforming traditional KD techniques and surpassing 13 state-of-the-art methods. It also transfers successfully to other datasets like Tiny ImageNet and STL-10. The code will be made public soon.", "sections": [{"title": "1. Introduction", "content": "Knowledge distillation (KD) is a technique that facilitates the transfer of knowledge from a larger, well-trained model (teacher) to a smaller, more efficient model (student). This is achieved by minimizing the Kullback-Leibler (KL) divergence between their outputs, allowing the student model to approximate the performance of the teacher model while maintaining lower computational complexity. This process is particularly beneficial for deployment in resource-constrained environments. A critical aspect of KD is representation learning, which enables the student model to acquire meaningful feature representations that capture the underlying data distribution. Effective representation learning in KD can significantly boost the performance of the student model across various domains, such as natural language processing, computer vision, and speech recognition [10, 15,22]. Despite these advantages, a major challenge in KD is the efficient transfer of complex knowledge from the teacher to the student model. Ensuring that the student model captures the abstract features and nuanced information present in the teacher model, without the need for similar computational capacity, remains a significant bottleneck.\nRecent advancements in KD have introduced various strategies to enhance the transfer process, addressing the bottlenecks associated with representation learning. Techniques such as adversarial training, attention transfer, and contrastive representation distillation have been proposed to further align the representations learned by the student model with those of the teacher model. Adversarial training involves using adversarial examples to improve the robustness of the student model, while attention transfer focuses on aligning the attention maps of the teacher and student models to ensure that both models focus on similar regions of the input data [?, 31]. Contrastive representation distillation aims to improve the quality of learned representations by encouraging the student model to produce similar representations for similar inputs while differentiating dissimilar inputs. Additionally, methods like BookKD reduce distillation costs by decoupling knowledge generation and learning processes, leading to improved performance with minimal resource consumption [35]. Other approaches like structured KD focus on training compact networks by distilling structured knowledge from cumbersome networks [15]. These methods aim to capture richer information from the teacher model, leading to more robust and generalized student models. The continuous evolution of KD techniques underscores its pivotal role in developing efficient deep learning models capable of high performance with lower computational costs, while overcoming the challenges of effective knowledge transfer.\nOur proposed method, Relational Representation Distillation (RRD), introduces a novel approach to address these challenges by maintaining relational consistency between the teacher and student models. By leveraging a large memory buffer"}, {"title": null, "content": "of teacher samples to align their output distributions, our method ensures consistent relational structures, thereby enhancing the robustness and performance of the student model.\nOur contributions are threefold:\nWe propose a novel relational consistency framework for KD that leverages a memory buffer to maintain relational structures between teacher and student models.\nWe introduce a relational consistency loss that aligns the similarity distributions of teacher and student outputs, improving the robustness and generalization of the student model.\nWe validate the effectiveness of RRD hthrough comprehensive testing on standard benchmarks, showcasing considerable gains in both accuracy and robustness. RRD surpasses other methods with a 19.22% relative improvement\u00b9 over conventional KD. When integrated with KD, it demonstrates a 55.18% relative improvement over standard KD, underscoring its robust potential in enhancing model performance significantly.\nThe rest of this paper is organized as follows. Section 2 reviews related work in KD and contrastive learning. Section 3 details our proposed methodology. Section 4 presents our experimental setup and results, and Section 5 concludes the paper."}, {"title": "2. Related Work", "content": "The study of [10] on KD introduced the idea of transferring knowledge from large, complex models to smaller, more efficient models without losing their generalization capabilities. This technique involves temperature scaling in the softmax outputs to effectively capture and convey the knowledge from the teacher model. Numerous enhancements to this method have been proposed. For example, using intermediate representations or \"hints\" to guide the learning process [22], and aligning the attention maps of teacher and student models to ensure they focus on similar areas during training [31]. Other methods preserve relational knowledge between samples [27] or align the correlation structures between teacher and student models [20]. Additionally, some techniques use variational inference to improve the knowledge transfer process [1]. Further developments include focusing on structural relationships between data points to ensure the student model learns relational information [18], and maintaining the internal dynamics of neural networks during distillation [9, 19]. Other notable techniques include\n${Average relative\\newline1 Acc\\^{2}_{RRD}-Acc\\^{2}_{KD}\\newlineimprovement is calculated as:\\frac{Acc\\^{2}_{KD}-Acc\\^{2}_{van}}  where Acc\\^{2}_{RRD}, Acc\\^{2}_{KD}, and Acc\\^{2}{van}\\newlinerepresent the accuracies of RRD, KD, and vanilla training of the i-th\\newlinestudent model, respectively [26].}$ network compression through factor transfer [12], optimizing networks for better transfer learning [30], promoting selectivity in the distillation process [11], and using contrastive learning objectives to enhance representation learning [26].\nSelf-supervised learning has significantly impacted representation learning by leveraging unlabeled data. Methods such as SimCLR [3] and MoCo [7] use contrastive losses to learn useful representations by maximizing agreement between different augmented views of the same data point. ReSSL [34] introduces relational self-supervised learning, which explores the relationships between data points. These approaches have inspired various KD methods, including our proposed method, which adapts relational consistency from self-supervised learning to the KD framework.\nOur method, RRD, differentiates itself from state-of-the-art methods by focusing on maintaining relational consistency between the teacher and student models. Unlike traditional KD methods that often rely on direct alignment of logits or intermediate features, RRD leverages a large memory buffer of teacher samples to align the relational structures of the output distributions. This approach not only enhances the robustness and performance of the student model but also provides a more flexible and scalable solution for KD."}, {"title": "3. Methodology", "content": "This section presents our methodology to improve the efficiency and accuracy of KD. Our method, Relational Representation Distillation (RRD), focuses on maintaining relational consistency between the teacher and student models by leveraging a large memory buffer of teacher samples to align their output distributions. By ensuring consistent relational structures, RRD enhances the robustness and performance of the student model. Figure 1 shows an overview of the proposed RRD method."}, {"title": "3.1. Preliminary", "content": "KD involves transferring knowledge from a high-capacity teacher neural network, denoted as $f_T$, to a more compact student neural network, $f_S$. Consider $x_i$ as the input to these networks, typically an image. We represent the outputs at the penultimate layer (just before the final classification layer, or logits) as $z_T^i = f_T(x_i)$ and $z_S^i = f_S(x_i)$ for the teacher and student models, respectively. The primary objective of KD is to enable the student model to approximate the performance of the teacher model while leveraging the student's computational efficiency. The overall distillation process can be mathematically expressed as:\n$\\mathcal{L} = \\mathcal{L}_{sup}(y_i, z_i^S) + \\lambda \\cdot \\mathcal{L}_{distill}(z_i^T, z_i^S)$  (1)\nwhere $y_i$ represents the true label for the input $x_i$ and $\\lambda$ is a hyperparameter that balances the supervised loss and the distillation loss. The loss $\\mathcal{L}_{sup}$ is the alignment error between"}, {"title": "3.2. Relational consistency", "content": "Our method is inspired by self-supervised learning. Traditional contrastive self-supervised learning uses instance discrimination as a pretext task and relies on (K+1)-softmax classification, where different instances are pushed apart, and augmented views of the same instance are expected to have identical features. Applying such properties to KD imposes overly strict constraints, where a contrastive loss encourages the representations from the teacher and student models for the same input data to be similar, while simultaneously pushing apart representations from different data inputs:\n$\\mathcal{L}_{contrast}(z, z') = -\\log \\frac{\\exp(\\phi(z_i, z_i') / \\tau)}{\\sum_{k=1}^{M} \\exp(\\phi(z_i, z_k') / \\tau)}$  (2)\nwhere $\\mathcal{L}_{contrast}$ is the InfoNCE [28] loss, typically employed in self-supervised methods, $\\phi$ is a similarity function, $\\tau$ is a temperature parameter, and $M$ is the number of negative samples.\nIn this way, we do not encourage explicit negative instances (those to be pushed away) for each instance; instead, we leverage the pairwise similarities to explore their relationships. We pull the features of the student $f_S$ and teacher $f_T$. As a result, our method relaxes Equation (2), where different teacher and student outputs do not always need to be pushed away from each other; and teacher and student need to share similar but not exactly the same features.\nConcretely, given an input image $x_i$, and the outputs $z_T^i = f_T(x_i)$ and $z_S^i = f_S(x_i)$ for the teacher and student models, respectively, we calculate their similarity measured by $\\phi(z_T^i, z_S^i)$. A softmax layer can be adopted to process the calculated similarities, which then produces a relationship distribution:\n$\\hat{p}_i^T = \\frac{\\exp(\\phi(z_T^i, z_T^k)/T_t)}{\\sum_{k=1}^{M} \\exp(\\phi(z_T^i, z_T^k)/T_t)}$  (3)\nwhere $T_t$ is the temperature parameter for the teacher network. At the same time, we can calculate the relationship distribution for the student model as:\n$\\hat{p}_i^S = \\frac{\\exp(\\phi(z_S^i, z_S^k)/T_s)}{\\sum_{k=1}^{M} \\exp(\\phi(z_S^i, z_S^k)/T_s)}$  (4)\nwhere $T_s$ is a different temperature parameter for the student network. We propose to push the relational consistency between $\\hat{p}^T$ and $\\hat{p}^S$, similar to [34], by minimizing the KL divergence, which can be formulated as:\n$\\mathcal{L}_{relational}(z, z') = KL(\\hat{p}^T || \\hat{p}^S) = H(\\hat{p}^T, \\hat{p}^S) - H(\\hat{p}^T)$  (5)\nwhere KL denotes the KL divergence between $\\hat{p}^T$ and $\\hat{p}^S$. Since $\\hat{p}^T$ will be used as a target, the gradient will be clipped here to avoid the model collapsing, thus we only minimize the cross-entropy term $H(\\hat{p}^T, \\hat{p}^S)$ in our implementation.\nHowever, the quality of the target similarity distribution $\\hat{p}^T$ is crucial for reliable and stable training. To ensure this, we maintain a large memory buffer Q, storing the feature embeddings from teacher batches. The relational consistency between the teacher and student models is enforced by aligning the similarity distributions of their outputs using the KL divergence."}, {"title": "4. Experiments", "content": "We evaluate our RRD framework in the KD task of model compression of a large network to a smaller one.\nDatasets. (1) CIFAR-100 [13] contains 50,000 training images with 500 images per class and 10,000 test images. (2) STL-10 [5] consists of a training set of 5,000 labeled images from 10 classes and 100,000 unlabeled images, and a test set of 8,000 images. (3) Tiny ImageNet (TIN-200) [6] has 200 classes, each with 500 training images and 50 validaton images.\nSetup. We experiment on CIFAR-100 with student-teacher combinations of various capacity, such as ResNet [8] or Wide ResNet (WRN) [32], VGG [25], MobileNet [23], and ShuffleNet [16,33] (more details about the network architectures are described in the supplementary material). We use $M = 16384$ negative samples and set the temperature parameter of the student to $T_s = 0.04$ and of the teacher to $T_t = 0.07$. Both the student and teacher outputs are projected to a 128-dimensional space. We use a projection head of a single linear layer, followed by l2 normalization. We train for a total of 240 epochs. More details on the training details are described in the supplementary material."}, {"title": "4.2. Capturing inter-class correlations", "content": "Cross-entropy loss overlooks the relationships among class logits in a teacher network, often resulting in less effective knowledge transfer. Distillation techniques that use \"soft targets\", such as those described by [10], have successfully captured these relationships, improving student model performance. Figure 4 assesses the effectiveness of different distillation methods on the CIFAR-100 KD task using WRN-40-2 as the teacher and WRN-40-1 as the student. We compare students trained without distillation, with attention transfer [31], with KL divergence [10], and with our proposed RRD method. Our findings show that RRD achieves close alignment between teacher and student logits, as evidenced by reduced differences in their correlation matrices. While RRD does not match CRD [26] in terms of exact correlation alignment, it significantly enhances learning efficiency and reduces error rates. The smaller discrepancies between teacher and student logits indicate that the RRD objective captures a substantial portion of the correlation structure in the logits, resulting in lower error rates, though CRD achieves a slightly closer match."}, {"title": "4.3. Transferability of representations", "content": "Our study investigates how knowledge is transferred from a larger teacher network (WRN-40-2) to a smaller student network (WRN-16-2), aiming to develop versatile representations suitable for a range of tasks and datasets. We apply this technique by having the student network either learn directly from the CIFAR-100 dataset or via distillation. In this setup, the student network is employed as a static feature extractor for images from the STL-10 and TIN-200 datasets, both adjusted to a 32 \u00d7 32 resolution. We evaluate the adaptability of these features by training a linear classifier on the final feature layer to conduct classifications with 10 categories for STL-10 and 200 categories for TIN-200. We document the impact of various distillation approaches on the transferability of these features in Table 3. Our findings reveal that, except for the FitNet method, all distillation techniques significantly improve the feature transferability on both datasets. Notably, while the teacher network achieves the highest performance on CIFAR-100, its features show"}, {"title": "4.5. Ablation study", "content": "There are two main hyperparameters in our objective: the number of negative samples M in the memory buffer Q of Equation (3) and Equation (4); and temperature parameters $T_s$ and $T_t$ of Equation (3) and Equation (4) that modulate the softmax probability. We also ablate the hyperparameter B that balances the KD loss. For the ablation study, we adopt WRN-40-2 as the teacher and WRN-16-2 as the student. Experiments are conducted on CIFAR-100, and the results are shown in Figure 4.\nNumber of negatives M. We validated different values for M: 256, 1024, 2048, 4096, 8192, 16384, and 32768. As shown in Figure 4a, increasing M leads to improved performance. However, the difference in error rate between $M = 4096$ and $M = 16384$ is less 0.5%. Therefore, we use $M = 16384$ for reporting the accuracy, while in practice lower M should suffice. Going beyond M 16384 proves to harms performance.\nWe normalize the outputs $z_T^i$ and $z_S^i$ before computing the loss, ensuring that the representations lie on a unit hypersphere. To compute $\\mathcal{L}_{relational}$, we further encode $z_T^i$ and $z_S^i$ using a projection head to match the dimensions. This ensures that the representations from both models are compatible for comparison and alignment.\nThe final objective function, which includes the supervised loss and standard KL divergence, is given by:\n$\\mathcal{L} = \\mathcal{L}_{sup}(y_i, z_i^S) + \\lambda \\cdot \\mathcal{L}_{distill}(z_i^T, z_i^S) + \\beta \\cdot \\mathcal{L}_{relational}(z, z')$  (6)\nwhere $\\beta$ is a hyperparameter that balances the KD loss $\\mathcal{L}_{relational}$. We experiment with $\\beta$ in ablation studies to understand its impact on the final performance."}, {"title": "5. Conclusions", "content": "Our method offers a significant advancement in KD by maintaining relational consistency between teacher and student models. RRD leverages a large memory buffer of teacher samples to align their output distributions, ensuring consistent relational structures throughout the learning process. Unlike traditional approaches, RRD uses pairwise similarities and a relaxed contrastive loss to enhance the robustness and performance of the student model without explicit negative instances. Through comprehensive testing on CIFAR-100, TIN-200, and STL-10 datasets, RRD consistently outperforms traditional KD techniques and state-of-the-art methods, demonstrating substantial improvements in accuracy, robustness, and transferability of learned representations. Our experiments on model compression highlight RRD's ability to provide a scalable solution."}]}