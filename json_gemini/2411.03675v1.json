{"title": "QUILL: Quotation Generation Enhancement of Large Language Models", "authors": ["Jin Xiao", "Bowei Zhang", "Qianyu He", "Jiaqing Liang", "Feng Wei", "Jinglei Chen", "Zujie Liang", "Deqing Yang", "Yanghua Xiao"], "abstract": "While Large language models (LLMs) have become excellent writing assistants, they still struggle with quotation generation. This is because they either hallucinate when providing factual quotations or fail to provide quotes that exceed human expectations. To bridge the gap, we systematically study how to evaluate and improve LLMs' performance in quotation generation tasks. We first establish a holistic and automatic evaluation system for quotation generation task, which consists of five criteria each with corresponding automatic metric. To improve the LLMs' quotation generation abilities, we construct a bilingual knowledge base that is broad in scope and rich in dimensions, containing up to 32,022 quotes. Moreover, guided by our critiria, we further design a quotation-specific metric to rerank the retrieved quotations from the knowledge base. Extensive experiments show that our metrics strongly correlate with human preferences. Existing LLMs struggle to generate desired quotes, but our quotation knowledge base and reranking metric help narrow this gap.", "sections": [{"title": "1 Introduction", "content": "Famous quotations (Tan et al., 2015a) are vital in academic and everyday communication. They lend authority to arguments and enhance persuasiveness, as they often stem from historically influential figures whose ideas have endured. Additionally, these quotations elevate the literary and artistic quality of a text, making discussions more engaging. They also facilitate comprehension of complex concepts, enabling readers to grasp ideas efficiently through concise expressions (Vaswani et al., 2023).\n\nThe task of Quotation Generation (QG) seeks to produce suitable quotations to deepen the context in large language models (LLMs) (Anil et al., 2023; Achiam et al., 2023; Touvron et al., 2023). However, LLMs encounter significant challenges in this domain, as illustrated in Figure 1. Primarily, the generated quotes frequently fail to correspond to genuine famous quotations and are often inaccurately attributed, a phenomenon termed \"Quotation Halluciantion.\" (Huang et al., 2023; Bang et al., 2023; Guerreiro et al., 2023) Additionally, these quotes don't align with the contextual meaning, resulting in a lack of coherence within the paragraph. Furthermore, LLMs exhibit a tendency to reproduce well-known quotes, which diminishes novelty and restricts creative expression.\n\nAlthough the issues of Quote Generation task are particularly problematic in LLMs, there is cur-"}, {"title": "2 Related Work", "content": "Previous research about quotation mainly focused on quote recommendation (Tan et al., 2015a). The task of quote recommendation was initially proposed by (Tan et al., 2015a). They proposed a learning ranking framework for the task, which integrates 16 manually crafted features. (Lee et al., 2016) combined four different methods for recommending famous quotes, including matching granularity adjustment (a statistical context quote correlation prediction method), random forest, CNN, and LSTM. (Wang et al., 2020) utilized an encoder-decoder framework to generate speech responses based on separate modeling of dialogue history and current query. (Wang et al., 2021) used semantic matching to encode multi round dialogue histories using Transformer (Vaswani et al., 2023) and GRU (Cho et al., 2014), and encoded quotes using Transformer. However, previous studies do not take into account the quotation generation capabilities of the large models themselves, nor did they propose a systematic and comprehensive evaluation system or benchmark to assess model performance in scenarios involving famous quotes."}, {"title": "2.2 Hallucination", "content": "In the field of NLP, hallucinations typically refer to a phenomenon where generated content appears meaningless or does not align with the provided source (Filippova, 2020; Maynez et al., 2020). \u03a4\u03bf address the issue of hallucinations in language models, two primary methods have been proposed: (1) preventing hallucinations during the training and generation processes, and (2) reducing hallucinations after generation. (Manakul et al., 2023) introduced an alternative classification, dividing methods into black box and gray box approaches. Black box methods involve conducting factual checks without relying on external resources, either during or after generation. In contrast, gray box methods utilize external resources for validation. Other techniques for alleviating hallucinations include reranking generated sample responses (Dale et al., 2022) and improving beam search (Sridhar and Visser, 2023). Recent mitigation technologies have also shown promise in reducing hallucinations (M\u00fcndler et al., 2024; Pfeiffer et al., 2023; Chen et al., 2023; Zhang et al., 2024; Agrawal et al., 2024). Although these methods have alleviated the quotation problem to a certain extent, they have not yet completely solved it, particularly in factual quotation and famous quotes."}, {"title": "3 Background", "content": ""}, {"title": "3.1 Task Formulation", "content": "Quotation Generation Given a plain text c = [t1, t2,..., ti,..., tn], the goal of the Quotation Generation (QG) task is to generate quotes for the specified insertion point i. The left and right contexts, ci and Cr, are defined as c\u2081 = [t1, t2,..., ti] and cr = [ti+1,..., tn], respectively. In our work, we mainly focus on the ability of the model in quotation generation tasks."}, {"title": "Quotation Recommendation", "content": "In the Quotation Recommendation (QR) task, given the context C = [t1, t2,..., ti,...,tn], the objective is to select the most suitable quote from the given set Q = {q1, \u2026\u2026\u2026, q\\Q|} to insert at position i, where qj represents the j-th quote."}, {"title": "3.2 Preliminaries", "content": "Perplexity (PPL) is a crucial metric in natural language processing, reflecting a model's predictive capability on text data and indicating the certainty of its next word prediction. Lower perplexity signifies greater confidence in the model's predictions, demonstrating a stronger ability to generate or understand language. PPL of a language model given a sequence of words w\u2081, W2, ..., wn is defined as:\n\n$PPL (PP) = exp {-\\frac{1}{s} \\sum_{i=t+1}^{N} log P(w_i | w_1,..., w_{i-1})}$ (1)\n\nwhere Pi is the given left paragraph, Pr is the following context needs to be calculated, P(wi | W1, W2, ..., Wi\u22121) is the probability of the word wi given its left context, and s is equal to N-t+1, which is the length of the sequence in the following paragraph."}, {"title": "4 Evaluation System for QG", "content": "The accuracy and rationality of quoting famous quotes are crucial, as they directly affect the credibility and rigor of the content. Therefore, we establish a holistic and automatic evaluation system for QG task evaluation in LLMs, containing five criteria and further design automatic metrics for each criterion (Fig. 1).\n\nCriteria Considering the nature of the quotation task itself, we design the following five criteria:\n(1) Quotation Authenticity: Confirm whether the"}, {"title": "5 Quotation Knowledge Base", "content": ""}, {"title": "5.1 Dataset Construction", "content": "In order to alleviate the problem of famous quote hallucination in LLMs, we develop a comprehensive bilingual and multi-topic quotation corpus designed to enhance retrieval quotation tasks during the RAG stage, as shown in Tab. 1. This corpus is structured into three distinct components: the English, the Standard Chinese, and the Classical Chinese. To improve the application scope and practical value of the corpus, we ensure comprehensive coverage of both common and specialized fields and also implement stringent quality control measures. Each quote is manually reviewed to ensure accuracy and relevance."}, {"title": "English Corpus", "content": "To construct the English quotation corpus, we extract approximately 27,260 quotes covering different topics from the BrainyQuote, A-ZQuotes and Goodreads websites, categorizing them by topic and author."}, {"title": "Classical Chinese Corpus", "content": "Considering the representativeness and novelty of the Chinese corpus, we first collect some famous citations from Gushiwen. Subsequently, given the limited number of citations, we utilize LLM to conduct a meaningful selection of the collected poems from BaiduHanyu. For instance, the seven-character quatrains in Tang poetry can be divided into two citations. Furthermore, to enhance the generalization of themes, we employ LLM to summarize the topics of the quotes. Finally, we collect over 9,233 citations with its poems, author and topics, including various genres such as Tang poetry and Song lyrics."}, {"title": "Standard Chinese Corpus", "content": "Regarding the Standard Chinese quotation corpus, we gather 13,453 quotes from the Guzimi and Mingyancidian websites, similarly categorized by topic and author."}, {"title": "Dataset Evolution", "content": "For those collected from diverse websites, the corpus have two limitations: (1) Semantic redundancy: the semantics of different quotations are too similar, especially when a long quotation includes a shorter one. (2) Lengthy quotations: some quotations are excessively long. Hence, We first utilized the Jaccard Similarity coefficient to address the issue of semantic redundancy. Then we set a restriction on the length of the citations and remove the extreme values based on the quotation ppl metric. Additionally, to facilitate the subsequent rerank stage of retrieval-augmented generation (RAG), we also pre-calculate the novelty of the quotations in the database. The specific calculation is detailed in Equation (8). Finally, we obtain a higher-quality corpus exceeding 32,022 entries."}, {"title": "5.2 Dataset Statistics", "content": "In this part, we compare the statistics of our dataset with existing quotation-related resources, as shown in Tab.2. In contrast, our dataset is the first to consider quotation novelty, encompassing a wide range of topics and numerous authors, while recording and annotating their sources. Additionally, we have expanded the scale of the quotation dataset, thereby broadening its application scenarios and significance."}, {"title": "6 Quotation-specific Reranking Metric", "content": "In our study we introduce a fine-grained and end-to-end RAG solution to improving model performance in quotation tasks through introducing a straightforward and interpretable quotation-specific rerank metric to select the optimal quotation.\n\nWhen the user inputs the context to be inserted, we use semantic similarity to recall the top k most relevant quotes from the knowledge database. However, while similarity assesses the semantic relevance between the quotation and the context, the QG task necessitates a more comprehensive approach. It requires not only that the semantics of the quote align with the context but also that the paragraph maintains fluency and incorporates novel citations. To enhance the performance of LLMs in QG, we propose three evaluative sub-indicators as shown in Fig. 2:\n\nQuotation Matching Quotation matching emphasizes the completion of the quotation itself and its alignment with the subsequent text. This is accomplished by calculating the perplexity (PPL) of the remaining portion of the quotation, given the preceding text and the initial k characters of the quotation. Generally, lower PPL values suggest that the model produces more accurate and coherent quotations. The specific calculation formula is as follows:\n\n$PPL = PPL ([q_{n-t}; C_r] | [C_l; q_t])$ (10)\n\nwhere n represents the length of the quote, qt represents the first t characters of the quote, qn-t represents the remaining n-t characters of the quote, and PPLm represent the sum perplexity of Qwen2-7B (Bai et al., 2023) and Llama3-8B (Touvron et al., 2023) respectively.\n\nSemantic Matching Semantic matching is concerned with ensuring semantic consistency and logical coherence within the context. This is achieved by predicting the PPL of the subsequent text, given the preceding text and the entire quote. Typically, a lower PPL value indicates that the context with the quote generated by the model is more logically consistent. The specific calculation formula is as Equation (10).\n\nNovelty The Novelty metric evaluates the originality of generated quotations. By avoiding repetition and clich\u00e9s, this metric ensures that content remains fresh and engaging, providing unique perspectives across various contexts. The specific calculation formula is as Equation (8).\n\nTo integrate the advantages of the three indicators, we employ a weighted average method, uti-"}, {"title": "7 Experiments", "content": "In this section, we conduct experiments to verify the effectiveness of our method and metrics."}, {"title": "7.1 Experiment Setup", "content": "Evaluation Dataset In constructing the evaluation dataset, our study select 7 common categories: economy, diplomacy, journalism, academia, law, technology, and life. Additionally, 21 frequently cited scenarios are chosen to encompass various aspects of the knowledge system, as show in Fig. 3. To enhance the dataset's diversity, standard Chinese, classical Chinese, and English texts are also included. Initially, we gather quotes from each scenario to ensure diversity, richness, and relevance to the selected fields. After collecting these quotes, they are used as keywords to search on major search engines like Google10, Bing11, and Baidu12. Then articles containing these quotes are identified, and"}, {"title": "7.2 Results", "content": "Overall Performance We conduct experiments on models of different ranges and sizes on our benchmark, and the results are shown in Table 3. Among the evaluated models, GPT-4 outperforms the other models, followed by Qwen1.5-72B-Chat. Despite varying parameter sizes (ranging from 6B to 72B), all models demonstrate suboptimal performance on the quotation generation task. Even the best-performing model, GPT-4, achieves an average score of no more than 0.62 across our five evaluation metrics, highlighting the critical need to address the quotation hallucination problem. Notably, our Quotation-specific Reranking method achieves the best results in each indicator, demonstrating the concise effectiveness of our proposed reranking metric."}, {"title": "7.3 Ablation Study", "content": "Correlations with Human Ratings In this study, we analyze the effectiveness of five evaluation metrics of our evaluation system by randomly selecting 100 samples from the evaluation dataset. These samples are combined with the recommended quotations from our reranking metric (PPLavg + Novelty) to form the manual evaluation dataset. To ensure reliability and objectivity, multiple evaluators independently complete the scoring process. We then conduct a correlation analysis to determine the degree of association between each evaluation metric and the human evaluation scores. As depicted in Figure 4, all metrics demonstrate a high correlation. The correlation coefficients significantly exceed the threshold for statistical significance, highlighting the metrics' effectiveness and reliability in quotation generation tasks.\n\nEffectiveness of Reranking Metrics This study delves into the effectiveness of the rerank metric designed in our method and validates it through a series of ablation experiments. We adopt the following metrics: Hit Ratio at rank K (HR@K(K=1,3)), Normalized Discounted Cumulative Gain at rank K (NDCG@K(K=1,3)), and Mean Reciprocal Rank (MRR) for comparison. On our benchmark, we compare a range of defined quotation-rerank metrics with state-of-the-art supervised, unsupervised, and closed-source API-based reranking methods."}, {"title": "7.4 QUILL Application", "content": "In this study, we conduct a comprehensive case analysis to demonstrate the efficacy and alignment of our reranking metric with human evaluations. As illustrated in Tab. 6, we focus on several key models for comparison: the supervised BM25 and our own reranking metric, which combines average perplexity (PPLavg) with novelty. Additionally, we manually sort and annotate the top-5 quote list initially recalled, serving as a benchmark for comparison. The findings reveal that our metric exhibits a higher correlation with human sorting than the other methods, underscoring its broad applicability and effectiveness."}, {"title": "8 Conclusion", "content": "In this paper, we systematically explore methods to enhance the performance of quotation generation tasks in LLMs. Initially, we establish a holistic and automatic evaluation system consisting of five highly interpretable and rigorous criteria, facilitating both human and automatic evaluation of this task. Then, we construct a comprehensive and high-quality knowledge database containing up to 32,022 quotes, complete with authors or sources. Moreover, we design a fine-grained quotation-specific metric to rerank the retrieved quotations from the knowledge base to improve QG performance. Additionally, we conduct extensive"}, {"title": "Limitations", "content": "This study highlights several limitations. We primarily use Perplexity (PPL) to evaluate text fluency. Although PPL is widely applied, it only measures the divergence between the model's and true probability distributions. Future research should integrate additional metrics or human evaluations for a more comprehensive assessment. Additionally, our analysis is restricted to specific contexts with clear correlations before and after quoted content. While informative, this approach does not cover a wide range of quoting scenarios. Future studies should explore diverse applications for more generalizable insights."}, {"title": "Appendix", "content": ""}, {"title": "A Details of Evaluation Dataset", "content": "We also conducted manual analysis on the Evaluation Dataset, selecting 275 quotes from numerous context-quote pairs, dividing into Chinese and English, which categories and scenarios details are shown in Figure.3. After statistics, there are 204 Chinese samples and 71 English samples, with a total of 144 Chinese and English authors."}, {"title": "B Details of Quotation Knowledge Base", "content": "This chapter further analyzes the data details in the quotation corpus, which is divided into three languages: English, Standard Chinese, and Classical Chinese, all classified by topic and author. The number of topics and authors for each language is shown in Table.7. In addition, we also conduct analysis on the proportion of different topics in each language in the corpus, as shown in Figure. 5 - 6. for specific topics and proportions."}, {"title": "C Details of Naive Setting Prompts", "content": "For the naive experimental settings, we also disclose its prompt in detail, see Table.9 for Naive-0-Shot, Table. 10 for Naive-1-Shot, and Table.11 for Naive-Cot setting."}, {"title": "D More Cases of QUILL Application", "content": "In this study, we conduct a comprehensive case analysis to demonstrate the efficacy and alignment of our reranking metric of the unsupervised UPR, the closed-source model GPT-3.5-turbo, and our approach with human evaluation. As shown in Table.8, we show the results of rag without rearrangement, index rearrangement and manual evaluation. The darker the color, the higher the manual evaluation score."}, {"title": "EnDCG Formulation", "content": "In our experiment, in order to get the relevance between quote and query, we first use GPT-40 to score the relevance and get the complete relevance list after manual sampling. Hence, given m candidate quotes Q = {q1, q2,\u2026\u2026, qm}, the nDCG@k is defined as follows:\n\n$nDCG(k) = \\frac{DCG(O_{real}, k)}{DCG(O_{ideal}, k)}$ (11)\n\n$DCG(O, k) = \\sum_{i=1}^{k} \\frac{Rel_i}{log_2(1+i)}$ (12)\n\nwhere Oideal and Oreal represent the score list given by the ideal ranking relevance and the real ranking relevance respectively, Reli denote the relevance score of the quote qi."}]}