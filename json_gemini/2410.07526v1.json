{"title": "MKGL: Mastery of a Three-Word Language", "authors": ["Lingbing Guo", "Zhongpu Bo", "Zhuo Chen", "Yichi Zhang", "Jiaoyan Chen", "Yarong Lan", "Mengshu Sun", "Zhiqiang Zhang", "Yangyifei Luo", "Qian Li", "Qiang Zhang", "Wen Zhang", "Huajun Chen"], "abstract": "Large language models (LLMs) have significantly advanced performance across a spectrum of natural language processing (NLP) tasks. Yet, their application to knowledge graphs (KGs), which describe facts in the form of triplets and allow minimal hallucinations, remains an underexplored frontier. In this paper, we investigate the integration of LLMs with KGs by introducing a specialized KG Language (KGL), where a sentence precisely consists of an entity noun, a relation verb, and ends with another entity noun. Despite KGL's unfamiliar vocabulary to the LLM, we facilitate its learning through a tailored dictionary and illustrative sentences, and enhance context understanding via real-time KG context retrieval and KGL token embedding augmentation. Our results reveal that LLMs can achieve fluency in KGL, drastically reducing errors compared to conventional KG embedding methods on KG completion. Furthermore, our enhanced LLM shows exceptional competence in generating accurate three-word sentences from an initial entity and interpreting new unseen terms out of KGs.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) are important resources for many data-driven applications, offering structured repositories of factual information that empower a variety of intelligent tasks [1, 2]. Yet, the strides made through the rapid advancement of large language models (LLMs) have challenged the conventional reliance on KGs. Nonetheless, LLMs are often critiqued for their susceptibility to generating factually incorrect or nonsensical outputs-a phenomenon known as the \u201challucination problem\" [3, 4]. Many recent studies propose to resort KGs to mitigate this problem [5-8].\nIn this paper, we investigate the capacity of LLMs to assimilate and generate knowledge graph facts proficiently. For example, the natural language sentence, \u201cWendee Lee is an actor in Mighty Morphin Power Rangers,\u201d translates into a KG triplet format as (Wendee Lee, actor of, Mighty Morphin Power Rangers). It is worth noting that, English names such as Wendee Lee and Mighty Morphin Power Rangers, while can serve as identifiers for entities, are perceived as atomic elements within the KG framework. They are indivisible and distinct from their constituent words or characters.\nWhen the LLMs interpret these text identifiers as mere sequences of tokens, they risk producing output that misrepresents entities or relations, therefore compromising the integrity of KG-based tasks. Consequently, existing research that integrates LLMs with KGs tends to limit its scope to relatively straightforward tasks. Examples of these limitations include validating the correctness of"}, {"title": "2 Related Works", "content": "We category the related works into two groups:\nKnowledge Graph Completion KG completion can be regarded as a classification problem like many NLP tasks [17-21], such as node classification and semantic role labeling. However, its label space is significantly larger than most NLP tasks. For example, the WN18RR [22] dataset contains over 40,000 different entities, making it impractical to simply feed them all as possible results and let the LLM select one as output. Most conventional KG completion methods are embedding-based methods, including the triplet-based methods [23-27], e.g, TransE [23], ComplEx [25], RotatE [26]; the GNN-based methods [15, 28\u201332], e.g., DAN [15], CompGCN [28], CoKE [29]; and other neural-based methods [22, 33-35], e.g., ConvE [22] and RSN [34]. Despite differences in their neural methods and input forms, all these methods focus on relational information and are not good at utilizing other types of information such as textual attributes.\nPretrained Language Models for Knowledge Graphs Leveraging Pretrained language models for KG completion has been explored for many years [36]. Some works treat BERT as a GNN model to encode graph features [30, 37], while others consider the textual information of KGs and use pretrained BERT to encode the textual labels of entities and relations [14, 38-40]. The resulting outputs are regarded as the entity and relation embeddings or their concatenations.\nWith the rapid advancements in leveraging LLMs for KG completion, recent works have begun design-ing prompts or instructions to guide LLMs in this task. Initially, the results were not promising [41], as it appeared that even state-of-the-art LLMs without context information could not outperform basic KG embedding models like TransE. However, subsequent works such as KGLlama [42] and KOPA [13] discovered that LLMs might perform better in triplet classification, i.e., estimating the correctness of a given triplet.\nMore recently, KICGPT [10] has proposed leveraging in-context learning [43, 44] to provide explicit instructions and guide the behavior of LLMs. This involves a triplet-based KG embedding model to generate the initial rankings of the top-k entities, followed by a multi-round interaction with the LLM, providing textual information and triplet demonstrations for the query entity and relation. The LLM should then re-rank the initial list. KICGPT has achieved state-of-the-art results on KG completion tasks. However, its performance not only depends on the LLM and the instructions but also on the pretrained KG embedding model. Additionally, KICGPT cannot be deployed offline due to the demand of commercial LLMs [45]. It also cannot provide embeddings for downstream tasks.\nIn contrast, the proposed MKGL has an embedding module based on the LLM token embeddings and KG relational information, which overcomes the weaknesses of existing KG embedding methods that cannot provide embeddings for unseen entities. The context information is implicitly encoded into the KGL token embeddings and efficiently captured by the LLM during fine-tuning."}, {"title": "3 Mastery of KG Language", "content": "In this section, we discuss the details of MKGL. We first introduce the general architecture of an LLM and how to convert a KG triplet into a fine-tuning instruction. Then, we present the details of constructing KGL token embeddings and scores. Finally, we illustrate how to train an MKGL and analyze its complexity."}, {"title": "3.1 Preliminaries", "content": "We start by a brief introduction to KGs and LLMs."}, {"title": "3.2 Instruct an LLM in KG Language", "content": "Recent studies reveal that LLMs harbor the potential to acquire unfamiliar natural languages [46, 47]. Given this premise, it is of particular interest to investigate how LLMs might interpret and operate within our KGL. We first design a prototype instructional text for this purpose. For a given triplet (Wendee Lee, actor of, Mighty Morphin Power Rangers), suppose that the task is to predict the tail entity Mighty Morphin Power Rangers, the instructional text is formatted as follows:\nInstruction 3.1. Supposed that you are a linguist versed in an esoteric three-word knowledge graph language. Given the following dictionary comprising two words from this language, please kindly\nHere, <kgl: Wendee Lee> denotes the definitive KGL token (corresponding to $e_i$ in previous sections and Figure 1) assigned to the entity Wendee Lee. We enrich the tokenizer's vocabulary with all pertinent KGL tokens, thereby enabling it to translate these KGL tokens into token IDs, which append sequentially to the LLM's original vocabulary range. It is worth noting that we only provide at most one example KGL sentence for each KGL word. Our intention is to introduce the schematics of KGL sentences to the LLM, rather than leveraging augmented KG data for in-context learning. To mitigate potential biases, the example sentences are sampled randomly."}, {"title": "3.3 In-Context Learning versus Special Token Embedding", "content": "The practice of incorporating supplementary context information alongside instructional prompts, known as in-context learning (ICL), has proven effective in enhancing performance across many NLP tasks [43, 44]. However, the concatenation of retrieved context on KGs with the input text can easily exceed the input length constraints of LLMs. Processing such long input sequences remains computationally intensive even with truncation. To address these constraints, we propose an alternative approach to encode context information into compact vector representations. Our experiments in Section 4.6 also demonstrate its superiority in terms of both efficiency and performance."}, {"title": "3.4 LORA-based KGL Context Retriever", "content": "We propose the low-rank adaption (LoRA)-based KGL context Retriever $\\mathcal{R}_{context}$ to effectively aggregate textual and KG information into KGL token embeddings. Typically, the vocabulary scope of a KG (comprising both entities and relations) usually surpasses that of an LLM. For instance, WN18RR is a KG completion dataset set sampled from WordNet [48]. It has over 40,000 unique entities, while the vocabulary size of Llama-2-7b is 32,000. Therefore, initializing new token embeddings for each KG elements and optimizing them from scratch would be prohibitively resource-intensive.\nMoreover, the dynamic nature of real-world KGs consistently introduces new entities. This is analogous to the evolution of human language, where new words are often synthesized or derived from existing ones. Drawing inspiration from this linguistic adaptability, we propose leveraging existing textual tokens to generate new KGL tokens, thereby avoiding the computational burden of learning unique embeddings for every KG element.\nScale Down As illustrated in Figure 2, the first step is to reduce the dimensionality of LLM token embeddings to lower computational demands during text and KG context aggregation. Inspired by LoRA [12], we leverage a projection matrix $W_T \\in \\mathbb{R}^{d\\times r}$ to transform the token embedding matrix $T \\in \\mathbb{R}^{N \\times d}$ into a reduced space $\\mathbb{R}^{N \\times r}$:\n$T' = TW_T$,\nwhere $T' \\in \\mathbb{R}^{N \\times r}$ represents the compact token embedding matrix.\nRetrieve Text Information We leverage a text encoder to encode the textual token embeddings of each KGL token into a unified vector. For example, the entity name \u201cMighty Morphin Power Rangers\" would be converted into individual token embeddings $t_{ei,0}, t_{ei,1}, ..., t_{ei,n}$, which are then aggregated into a single vector for the entity $e_i$:\n$t_{e_i} = \\mathcal{E}_{text}(t_{ei, 0}, t_{ei,1},..., t_{ei,n})$,"}, {"title": "3.5 Reconstructing Vocabulary or Constraining the Output Space", "content": "While recent studies have adapted LLMs to various tasks by either restricting the output space or reformulating tasks into multiple-choice questions [9, 10, 51\u201353], such strategies pose challenges for KG completion. Specifically, the existing methods are inapplicable to entity constrastive learning as their main objective is optimized against text tokens instead of entities. Also, they incur significantly slow inference times, as the LLM must traverse to the output tree's leaf nodes to generate predictions. Even then, the generation of top-k results, dependent on beam search parameters, may not accurately reflect the true likelihoods of entities.\nIn contrast, in this paper we propose a new approach to reconstruct the KGL scores through LLM's score layer and hidden states, providing a one-shot probability distribution for all candidates. Our method seamlessly integrates with contrastive loss and negative sampling techniques [54], making it highly compatible with prevalent KG completion frameworks. This compatibility also ensures that MKGL has the potential of being applied for downstream KG embedding tasks [32, 55]."}, {"title": "3.6 LoRA-based KGL Score Retriever", "content": "We propose a LoRA-based KGL score retriever $\\mathcal{R}_{score}$ to produce the probability distribution of KGL tokens, which can be formulated as follows:\n$\\begin{aligned}\nS' &= SW_s, &\\text{(Down Scaling)}\\\\\ns''_{text} &= \\mathcal{S}_{text}(s'_{j,0}, s'_{j,1},..., s'_{j,n}), &\\text{(Text Information Retrieval)}\\\\\nS'_{e_j|e_i,r_k} &= \\mathcal{S}_{kg}([h'_n, s''_{j}], \\mathcal{N}(e_j)) &\\text{(Conditioned Retrieval)}\\\\\nP_{e_j|e_i,r_k} &= S''_{e_j|e_i,r_k} W_o &\\text{(Score Estimation)}\n\\end{aligned}$\nThe score retriever also starts from a down-scaling layer to reduce the dimensionality of the score matrix $S \\in \\mathbb{R}^{N \\times d}$ to $S_R \\in \\mathbb{R}^{N \\times r}$ with $W_s$, and similarly scales down the LLM's output hidden vector $h_n$ with $W_H$. Subsequently, the text information (i.e., the token score vectors $s'_{j,0}, s'_{j,1},..., s'_{j,n}$) associated with target entity $e_j$ is fed to the score text encoder $\\mathcal{S}_{text}$ to construct the KGL score vector $s''_{j}$. It is then concatenated with the LLM hidden state $h'_n$ to obtain the conditioned input $[h'_n, s''_{j}]$. Upon gathering the neighboring information of the target entities via a multi-layered PNA $\\mathcal{S}_{kg}$, an output matrix $W_o \\in \\mathbb{R}^{r \\times 1}$ is employed to map the result $s'''_{j|e_i,r_k} \\in \\mathbb{R}^{r}$ to the 1-d probability estimate $P_{e_j|e_i,r_k} \\in \\mathbb{R}$."}, {"title": "3.7 Complexity", "content": "It is clear that the primary computational cost for MKGL lies in the LLM. By employing LoRA-based KGL retrievers to retrieve context vectors instead of texts, we can significantly reduce the major expenditure. For instance, our retrievers can reduce the average input lengths from 811.2 to 91.4 on the FB15k-237 dataset, compared to using one-hop neighbors for in-context learning. All operations within the LoRA-based retrievers are performed under low dimensionality. Furthermore, the token embeddings and score matrix of the LLM are frozen during fine-tuning, thus ignoring their gradient computation. In the worst case, the complexity of text information retrieval is $O(N_{kgl}L_{kgl}r)$, where $N_{kgl}, L_{kgl}, r$ are the number of KGL tokens, maximum text token lengths of KGL tokens, and the reduced dimensionality, respectively. Subsequently, the complexity of KG information retrieval in the worst case is linear to the number of triplets, i.e., $O(|T|N_{layer}r)$, where $|T|$, $N_{layer}$ denote the number of triplets in the KG and the number of PNA layer, respectively."}, {"title": "4 Experiments", "content": "In this section, we evaluate the performance of the proposed MKGL through extensive experiments, comparing it against both LLM-based and KG embedding methods. The source code and datasets are available at github.com/zjukg/MKGL."}, {"title": "4.1 Datasets", "content": "We evaluate MKGL on the FB15k-237 and WN18RR datasets, which are widely used by most KG completion methods [22, 23, 26, 28, 34, 56, 57]. We also evaluate MKGL on the inductive version of"}, {"title": "4.2 Settings", "content": "For our experiments, we employ Llama-2-7b [11] as the base LLM and train MKGL using 8 A100 GPUs. A standard LoRA adaptation is applied to the query and value layers of the LLM. Full hyper-parameter details are available in Appendix D. We evaluate performance using MRR (mean reciprocal rank of target entities) and Hits@k (percentage of target entities ranked in the top k).\nOur baselines include conventional KG embedding methods such as TransE [23], RotatE [26], and TuckER [56]; GNN-based methods like CompGCN [28], DAN [15], and CoKE [29]; methods that integrate language models including KG-BERT [14], StAR [38], KGLM [40], FTL-LM [39], and DET [30]; and LLM-based methods: KG-Llama [42], GPT 3.5 [41], and KICGPT [10]. In the inductive scenario, we compare against rule-based reasoning methods such as RuleN [60], NeuralLP [33], DRUM [61], GraIL [58] and RED-GNN [59], acknowledging that standard methods fail to predict relations without entity embeddings."}, {"title": "4.3 Knowledge Graph Completion", "content": "The knowledge graph completion results are presented in Table 2. MKGL outperforms other baselines in nearly all metrics. Notably, MKGL and KICGPT significantly surpass other LLM-based methods, demonstrating the importance of KG relational information. Contrarily, many BERT-based methods fall short against GNN-based methods, suggesting that merely incorporating text information may not yield the anticipated benefits. In summary, the proposed MKGL clearly outshines its counterparts, particularly those founded on commercial LLMs.\nTo our knowledge, existing LLM-based methods have not addressed the inductive KG completion challenge. We benchmark MKGL against the state-of-the-art inductive methods. Although we can"}, {"title": "4.4 Knowledge Graph Language Modeling", "content": "Beyond its capability as a KG completion tool, MKGL also serves as a language model for KG languages. To evaluate its proficiency in generating KGL sentences, we employ a sequence generation loss and remove the relation context from the input prompts. We leverage the second-to-last output of the LLM for relation prediction.\nThe results are shown in Figure 3. The left section contrasts the sequence prediction results against standard KG completion, revealing only a modest loss in performance. MKGL still outperforms many conventional methods, especially on WN18RR dataset. The right panel displays sample sentences generated by MKGL, illustrating its potential to discover legitimate KGL sentences absent from the existing KG. We observe that WN18RR is more difficult than anticipated as it contains many plausible entities that challenge even an LLM's discernment."}, {"title": "4.5 Ablation Study", "content": "We conduct ablation studies to assess the importance of each module, as detailed in Table 4. The unmarked cells indicate that we either substitute the text retrieval module with a learnable embedding module or remove the KG retrieval module. Clearly, the method with complete features achieves best results, while replacing or removing either module significantly impacts performance. Notably, removing the KG retrieval module yields more performance loss on WN18RR, as many entities in this dataset have similar names. For example, there are 14 different entities named \u201ccall\u201d. In this case, incorporating with KG information becomes necessary."}, {"title": "4.6 Computational Cost", "content": "We examine the computational efficiency of our method (MKGL) relative to \u201cin-context\u201d baselines. Specifically, we develop several variants: LLM randomly-initialized new entity token embeddings (NewToken), LLM with KGL context from 1-hop neighbors (NewToken (1-hop)), LLM with KGL context from 2-hop neighbors (NewToken (2-hop)), and MKGL without score retriever (MKGL w/o"}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we propose MKGL to instruct the LLM in the language of KGs. MKGL employs a context retriever that efficiently provides LLMs with pertinent textual and relational context, markedly reducing input lengths relative to in-context-learning and supervised fine-tuning methods. Meanwhile, MKGL also leverages a score retriever to supply score information and aid in KGL inference. Extensive experiments confirm the superiority of MKGL in terms of both performance and computational efficiency. The proposed context and score retrievers point out a new direction in incorporating LLMs with semantic data, such as question answering and entity linking. They may also shed lights on a more broaden area where the input cannot be precisely represented by text, e.g., node classification and protein representation learning. Furthermore, the construction of KGL vocabulary enables contrastive learning not only limited on tokens, which may provide insights on general machine learning. Therefore, there are plenty of future directions. We would like to pretrain LLM using the mixture corpora of KG and natural languages, such that the LLM could understand and create responses with linked data."}, {"title": "A Limitations", "content": "We would like to discuss the potential limitations of our method from the following three aspects:\nEfficiency. As MKGL is an LLM-based fine-tuning method, it inevitably demands more computational resources. In the main experiments, MKGL significantly outperforms all the conventional and LLM-based methods. The later analysis also reveal that the trainable parameters and runtime of MKGL are less than general fine-tuning framework. Therefore, we believe that MKGL is still an efficient LLM-based method.\nRobustness. MKGL leverage multiple retrievers to retrieve text and KG information for constructing both input embeddings and score estimations, which may accumulate more errors during fine-tuning. Even though, most modules are learnable with back-propagation. To avoid biased evaluation and occasional results, we also report the averaged results of multiple runs with variance statistics. Thus, we believe MKGL is a robust method.\nGenerality. The advances in LLMs have revolutionized many NLP tasks, and it is important for an LLM-based method where the LLM makes use of the proposed modules and whether the performance can continually improves as the LLM get promotion. We have conducted experiments to visualize the KGL embeddings and compare the performance with different base LLMs. The results empirically demonstrate the generality and potential of MKGL."}, {"title": "B Broader Impacts", "content": "Our work focuses on the integration of Large Language Models (LLMs) with Knowledge Graphs (KGs) through the introduction of a specialized KG Language (KGL), has substantial broader impacts spanning technological advancements, societal implications, and educational benefits. Here, we outline the diverse and far-reaching impacts of our research.\nTechnological Advancements Our research contributes to the cutting-edge of artificial intelligence, pushing the boundaries of what LLMs can achieve when combined with the structured knowledge represented by KGs. This can potentially unlock new capabilities in AI, ranging from more accurate context-aware natural language understanding to enhanced machine reasoning across diverse domains such as healthcare, finance, and legal systems. Additionally, our approach of using a specialized language (KGL) and the method of contextual embedding augmentation can inspire novel AI architectures and learning paradigms that bridge the gap between unstructured and structured forms of knowledge.\nSocietal Implications The enhancement of AI systems with a more profound understanding of structured knowledge has broad societal implications. For one, it could lead to the development of AI assistants that provide more accurate, consistent, and reliable information, thus improving decision-making in critical sectors. In healthcare, for instance, AI systems equipped with our technology could offer more precise recommendations by thoroughly understanding medical knowledge graphs. Moreover, by reducing the propensity for errors and hallucinations, our approach could foster greater trust in AI technologies among the general public, paving the way for wider acceptance and integration into daily life.\nEthical Considerations As with any advancement in AI, our work prompts important ethical considerations. Ensuring our technology is used responsibly involves critical discussions around privacy, bias, and transparency, especially as Al systems become more adept at interpreting and generating human-like text. We advocate for the continued examination of these aspects in tandem with technological development to ensure AI benefits society equitably and ethically. Our work, by facilitating error reduction in AI outputs, also contributes to the broader effort of minimizing harm and bias in AI-generated content.\nEducational Benefits Our integration of LLMs with KGs presents a novel avenue for educational tools and applications. AI tutors or educational platforms powered by our enhanced LLMs can offer students personalized and accurate learning experiences. Such systems could better understand and integrate complex academic content structured within knowledge graphs, from history timelines to"}, {"title": "C Principal Neighborhood Aggregation", "content": "Graph Neural Networks (GNNs) have emerged as a powerful family of neural models for learning on graph-structured data [17, 18, 62]. Among the recent advances is the principal neighborhood aggregation (PNA) mechanism [49], which enhances the representational capacity of GNNs by diversifying the aggregation functions applied to neighboring nodes.\nPNA leverages a combination of multiple aggregators such as sum, mean, and standard deviation, together with a scalable degree-specific weighting scheme. This approach is designed to address the shortcomings associated with simple aggregation functions that may fail to capture the complexity and diversity of neighborhood structures in graphs.\nThe key component of PNA is its aggregation scheme, which is formally defined as follows:\n$a_v^{(l+1)} = \\delta\\left( AGG_{\\mathcal{R}}\\left(\\left\\{ h_u^{(l)}, \\forall u \\in \\mathcal{N}(v) \\right\\} \\right), W^{(l)} \\right)$\nPlease note that the symbols used in describing PNA are independent to the main paper for clarity. Here, $a_v^{(l+1)}$ is the aggregated information for node $v$ at layer $l + 1$, $\\mathcal{N}(v)$ denotes the set of neighbors of $v$, $h_u^{(l)}$ represents the hidden features of neighbor nodes at layer $l$, $\\oplus$ is a concatenation operator over all aggregators in the set $\\mathcal{R}$, $AGG_{\\mathcal{R}}$ is an aggregation function (e.g., sum, mean, max), $\\delta$ is a nonlinear activation function such as ReLU, and $W^{(l)}$ is a learnable weight matrix at layer $l$.\n$a_v^{(l+1)} = \\delta\\left( AGG_{\\mathcal{R}}\\left(\\left\\{ h(h_u^{(l)}), \\forall (v, r, u) \\in T \\right\\} \\right), W^{(l)} \\right)$\nPNA's distinctive blend of multiple aggregation functions and degree-aware weighting significantly enhances the expressive power of GNNs, allowing for more complex feature representations and, consequently, improved performance on downstream tasks. We also follow the KG embedding methods [15, 28, 63] to incorporate the relation embeddings into PNA as relational PNA."}, {"title": "D Implementation Details", "content": "We introduce Algorithm 1 to demonstrate the fine-tuning process of MKGL for KG completion. We first construct input instructions following Instruction 3.1 and tokenize them into IDs. For those in the score of original LLM vocabulary, their embeddings can be looked up from T, while those of out of scope will be retrieved by our context retriever $\\mathcal{R}_{context}$. After assembling the input embeddings, we feed them to the LLM to obtain output hidden states and then obtain the scores from the score retriever $\\mathcal{R}_{score}$. Finally, we optimize MKGL by minimizing the constrastive loss $\\mathcal{L}$. The main hyper-parameter settings are summarized in Table 5."}, {"title": "E Dataset Details", "content": "We use the following benchmark datasets to evaluate the performance of MKGL, and summarize the statistics in Table 6:\n\u2022 FB15k-237: This dataset is a subset of the original FB15k dataset [23] and is created by removing inverse relations that may lead to test set leakage.\n\u2022 WN18RR: This dataset is a subset of the original WN18 dataset [23] and is created by removing inverse relations that may lead to test set leakage.\n\u2022 FB15k-237-ind: The 'ind' suffix denotes the inductive setting adopted in FB15k-237 [58]. It includes new entities in the validation and test sets that are not present during training, thus requiring models to generalize beyond the transductive assumptions of previously seen entities.\n\u2022 WN18RR-ind: Similarly to FB15k-237-ind, the WN18RR-ind dataset is adapted for inductive KG completion on the WordNet [48].\nThese datasets have been instrumental in the development and benchmarking of advanced KG completion models, enabling comparison of different approaches and understanding of their effectiveness in both conventional and inductive settings."}, {"title": "F Additional Experiment Results", "content": "F.1 More Examples on Knowledge Graph Language Modeling\nWe present additional examples of KGL modeling in Table 5, which demonstrates that MKGL can not only generate KGL sentences seen during training but also produce previously unseen triplets within the testing set.\nF.2 Details Results on Inductive Knowledge Graph Completion\nWe present detailed results on all inductive KG completion benchmarks in Table 7, where MKGL consistently and significantly outperforms all state-of-the-art baselines."}, {"title": "F.3 Different Layer Numbers", "content": "We conduct experiments to analyze the influence of layer numbers in the KGL retrievers. The results are illustrated in Figure 6. Clearly, increasing the number of layers enhances performance across all datasets and metrics. Additionally, we observe that a small number of layers (i.e., 2) significantly impairs performance."}, {"title": "F.4 Different Encoders", "content": "We conduct experiments to explore the impact of different encoders in the retrievers. The results are depicted in Figure 7. We find that the MKGL is not highly sensitive to the choice of encoders. The performance when using GAT [50] is slightly lower than when using PNA."}]}