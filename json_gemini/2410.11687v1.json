{"title": "STATE-SPACE MODELS CAN LEARN IN-CONTEXT BY GRADIENT DESCENT", "authors": ["Neeraj Mohan Sushma", "Yudou Tian", "Harshvardhan Mestha", "Nicolo Colombo", "David Kappel", "Anand Subramoney"], "abstract": "Deep state-space models (Deep SSMs) have shown capabilities for in-context learning on autoregressive tasks, similar to transformers. However, the architectural requirements and mechanisms enabling this in recurrent networks remain unclear. This study demonstrates that state-space model architectures can perform gradient-based learning and use it for in-context learning. We prove that a single structured state-space model layer, augmented with local self-attention, can reproduce the outputs of an implicit linear model with least squares loss after one step of gradient descent. Our key insight is that the diagonal linear recurrent layer can act as a gradient accumulator, which can be 'applied' to the parameters of the implicit regression model. We validate our construction by training randomly initialized augmented SSMs on simple linear regression tasks. The empirically optimized parameters match the theoretical ones, obtained analytically from the implicit model construction. Extensions to multi-step linear and non-linear regression yield consistent results. The constructed SSM encompasses features of modern deep state-space models, with the potential for scalable training and effectiveness even in general tasks. The theoretical construction elucidates the role of local self-attention and multiplicative interactions in recurrent architectures as the key ingredients for enabling the expressive power typical of foundation models.", "sections": [{"title": "1 INTRODUCTION", "content": "The current generation of Large Language Models (LLMs) and foundation models are extremely capable and have started proliferating in several real-world applications. These models are based on the transformer architecture (Vaswani et al., 2017), and a big part of their capability has been at-tributed to in-context learning (Wei et al., 2023; Lu et al., 2023). In-context learning in transformers is relatively well studied (Wies et al., 2023; Pan et al., 2023; Guo et al., 2023; Garg et al., 2023). A prominent explanation for the mechanism used by transformers to do in-context learning is that the model performs in-context learning by gradient descent (von Oswald et al., 2023; Aky\u00fcrek et al., 2024). But, the quadratic dependence of transformers on the input length makes them computa-tionally expensive. To mitigate this, there has been much work on alternatives based on recurrent networks, such as state-space models (Gu et al., 2021; Gu & Dao, 2023) and linear recurrent net-works (Orvieto et al., 2023). These recurrent models can perform inference efficiently since the computational complexity of recurrent networks is linear in the sequence length. At the same time, linear recurrent networks allow parallelization across the sequence during training using associative scan. The latest versions of these models are competitive with transformers at scale (Dao & Gu, 2024; De et al., 2024), and also capable of in-context learning (Grazzi et al., 2024). However, the mechanism they use for in-context learning remains unclear."}, {"title": "2 BACKGROUND", "content": "A sequence model operates on an input sequence $S = \\{s_t\\}_{t=1}^T \\in \\mathbb{R}^{T\\times f}$, where T is also referred to as the context-length and f is the feature dimension.\nContemporary sequence models based on transformers interleave self-attention with MLP layers to perform sequence processing. The self-attention performs sequence-mixing while the MLPs per-form channel-mixing. The most common form of self-attention has been the scaled dot-product self-attention, which embeds the sequence into a query $Q = SW_Q$, key $K = SW_K$ and value $V = SW_V$, where $W_Q, W_K \\in \\mathbb{R}^{f\\times m}$, $W_v \\in \\mathbb{R}^{f\\times d}$, and then calculates the output of attention as\n$SA(S) = \\text{softmax}(\\frac{QK^T}{\\sqrt{m}})V$.\nLocal self-attention (Beltagy et al., 2020) uses the same form as Eq. 1, but on an input sequence that is a subset of the full sequence, with a sliding window.\nBy discarding the softmax (and scaling for simplicity), self-attention can be written in a linear form\n$LSA(S) = QK^TV$.\nDeep SSMs are sequence models that replace the self-attention with a linear recurrent network, to perform the sequence-mixing. In the most general form, the recurrent SSM block consists of a recurrent state $Z_t \\in \\mathbb{R}^{d\\times m}$ updated iteratively as\n$Z_t = A(s_t) * Z_{t-1}+ B(s_t)$,\nwhere A, B : $ \\mathbb{R}^f \\rightarrow \\mathbb{R}^{d\\times m}$, and * is some multiplication operator (e.g. matrix or element-wise multiplication)."}, {"title": "3 SSMS CAN EMULATE GRADIENT DESCENT ON LINEAR REGRESSION TASKS", "content": "We will now show that an SSM as described in Section 2 can perform gradient descent on an implicit linear model to minimize a least squares loss (for particular choices of parameters). Extensions to non-linear regression models are considered in Section 3.3.\nConsider a linear regression problem. The goal is to minimize the corresponding least squares loss using gradient descent. Performing mini-batch (batch size > 1) gradient descent on the parameters of this implicit model involves two steps: (i) to accumulate gradients of the loss with respect to the parameters, and (ii) apply the accumulated gradient to the initial value of the parameters of the linear model to calculate the updated parameters. Predictions can be made with the updated parameters by combining them linearly with the input.\nAssume the training samples for the linear regression problem are provided as a sequence of inputs and targets. A large enough SSM can then accumulate the gradients of the loss function in its state if a local self-attention-like layer processes the sequence inputs before the recurrence\u00b9.\nGiven the accumulated gradients, the next-step emission of the SSM is equivalent to i) updating the parameters of the implicit model gradient and ii) computing the model output with the updated parameters. Multiple steps of gradient descent can be achieved by stacking multiple layers, while nonlinearity in the implicit model can be handled by adding nonlinear input-output embedding lay-ers. We argue that the architecture that allows a single layer to perform gradient descent provides the inductive bias for the model to do in-context learning."}, {"title": "3.1 SINGLE STEP 1-DIMENSIONAL LINEAR REGRESSION", "content": "Consider a linear regression model with 1-d output for simplicity\n$y = w^T x$,\nA SSM layer with input-dependent recurrence would be able to simulate a local self-attention layer, but with significantly increased computational and conceptual complexity."}, {"title": "3.2 SINGLE STEP N-DIMENSIONAL LINEAR REGRESSION", "content": "In this section, we generalize the construction above to the N-dimensional case. Without loss of generality, we assume the input and the target, \u00e6 and y, have both dimensions $f$ i.e. $x, y \\in \\mathbb{R}^f$. If this is not the case, the input and output dimensionality can be matched by defining appropriate embeddings\u00b3. We can then treat the N-dimensional system as \u0192 1-D linear regression problems, one for each element of y.\nProposition 2 Given a diagonal linear recurrent layer augmented with local self-attention with sliding window of size 3, and tokens $s_{2j} = x_j$ and $s_{2j+1} = y_j$, for j = 1, ..., N, $x_j, y_j$ drawn from a linear model, one can construct recurrent matrix $A(s_j)$, input $B(s_j)$ and output matrix $U(s_j)$ such that each recurrent step for every token $s_j$ produces $\\hat{y}_{j+1} = -(\\Delta W)^T x_{j+1}$ as output, where AW is one step of gradient descent, i.e. $\\Delta W = n\\nabla_w L$. The test input $x_{N+1}$ is contained in token $s_{2N+2}$, and produces the test prediction $\\hat{y}_{N+1}$.\nSimilar to Eq. 11, we show the above by writing the SSM as\n$Z_t = Z_{t-1} + y_t x_t$,\nwhere $Z_t$ corresponds to the parameters of the implicit linear model, $W \\in \\mathbb{R}^{f\\times f}$, and we assume, for simplicity, that $W = 0$. The output is\n$\\hat{o}_t = \\beta Z_t x_{t+1}$.\nTo see how this can be written in the form of Eq. 3, let the input sequence consist of the training dataset of the implicit linear regression problem (as before). This time, we cast the training dataset into a standard sequence $s_1, s_2, ..., $ where\n$s_{2j} = x_j,$\n$s_{2j+1} = y_j,$"}, {"title": "3.3 GENERALISING TO ANY REGRESSION PROBLEM", "content": "Multi-step gradient descent: The proposed construction can be extended to multi-step gradient descent. Since each layer of a GD-SSM produces the parameters of the implicit linear model updated by one step of GD, this is equivalent to stacking together multiple layers. In our derivations above, we assumed the initial parameter of the implicit linear model is 0. In the multi-step GD, all layers other than the first will correspond to a non-zero initialised implicit linear model. Technically, extra gradient steps in the implicit model introduce one additional term in the gradient accumulation equation. Each of the two terms requires a separate (parallel) recurrence and is performed by a dedicated layer. At the end, the states are combined to obtain the multiple-step GD update, with minor extra computational burdens.\nNon-linear regression: Non-linear regression can be handled by adding MLP layers to the GD-SSM. In the previous sections, we let GD-SSM accumulate the gradients of a linear regressor. Additional MLP layers can learn to transform the state of these linear layers into quantities corresponding to the gradient of the implicit non-linear model.\nRegularisation terms in the loss: As the recurrent layers only accumulate the gradients, we can separate the gradient calculation and accumulation from its application. This has a practical advantage. Any input-independent regularisation term, e.g. L2 norms, can be added to the model without changing the recurrence structure."}, {"title": "4 TRAINED LINEAR RECURRENT NETWORKS DO EMULATE GRADIENT DESCENT ON LINEAR REGRESSION TASKS", "content": "We investigated if the GD-SSM variant of the general SSM architecture does do gradient descent in-context learning. To do this, we trained a randomly initialised model on various in-context learning"}, {"title": "4.1 SINGLE STEP 1-DIMENSIONAL LINEAR REGRESSION", "content": "We first tested the simplest case of one-step gradient descent on a linear regression problem with scalar predictions/targets. This corresponds directly to the construction in Section 3.1. To do this, similar to Garg et al. (2023); von Oswald et al. (2023), we randomly generated linear regression tasks consisting of training and test points, and trained the model to make a prediction for the test input using the training input as context.\nWe generated randomly sampled linear regression tasks \u03c4 in the following way: Each task (context) 7 consisted of a sequence of in-context training data $D_{\\tau} = \\{(X_{\\tau,i}, Y_{\\tau,i})\\}_{i=1}^{N}$ and test point $(X_{\\tau,N+1}, Y_{\\tau,N+1})$. To generate this, the $x_{\\tau,i}$s are sampled from a uniform distribution $X_i \\sim U(-1,1)^f$. Then, for each task \u03c4, the parameters of its implicit linear model $w_\u03c4$ is sampled from a normal distribution, so that each element $[w_\u03c4]_i \\sim N(0, 1)$. This is used to calculate the $y_{\\tau,i}$s for each corresponding $x_{\\tau,i}$ using $y_{\\tau,i} = w_\u03c4^T x_{\\tau,i}$.\nThe sequence $S = \\{s_{\\tau,1}, ..., s_{\\tau,N}\\}$ is constructed so that $s_{\\tau,t} = c_{\\tau,t} = [x_{\\tau,t} y_{\\tau,t}, x_{\\tau,t+1}]$, with [...] denoting the vector concatenation operation, and $c_t$ is the constructed context vector. Note that this includes the query $x_{\\tau,N+1}$ in $c_N$. We will use a more general construction in the next section. The outputs of the GD-SSM at time T = N is the prediction for N+1 i.e. $SSM(S_\u03c4)_N = \\hat{y}_{\u03c4,N+1}$, with target $y_{\u03c4,N+1} = w_\u03c4^T x_{\u03c4,N+1}$. The model was trained to minimize the expected squared prediction error, averaged over linear regression tasks \u03c4:\n$\\min_{\\theta} E_{\\tau} [||\\hat{y}_{\\theta}(c_{\\tau,1},..., c_{\\tau,N}) - y_{\\tau, \\text{test}}||_2]$,\nwhere \u03b8 are the randomly initialised parameters of the GD-SSM. We evaluate our model on multiple metrics:\nL2 norm between the difference in predictions $||\\hat{y}_{\\theta}(x_{\\tau, \\text{test}}) \u2013 \\hat{y}_{\\theta}^{GD}(x_{\\tau, \\text{test}})||_2$ where $\\hat{y}_{\\theta}^{GD}$ is the prediction from the GD based construction."}, {"title": "4.2 SINGLE STEP N-DIMENSIONAL LINEAR REGRESSION", "content": "In the more general case where we allow the targets $y_{\\tau i}$ to be of arbitrary dimension, we also relax other assumptions in the form of the input. Notably, instead of constructing the inputs in a particular way, we let the sequence be similar to a more natural sequence of $x_1, y_1, x_2, y_2, ..., $ i.e. $s_{2j} = x_j; s_{2j + 1} = y_j$. This requires the introduction of a local self-attention mechanism with attention window of 3 as discussed in Section 3.2. The local self-attention mechanism allows the model to access inputs from multiple adjacent timesteps, and not just the current one. We perform the same comparisons as in Section 4.1, and find that the model trained from a random initialisation has an excellent match with the construction, both in the output/loss metrics and in the parameters it ends up with ."}, {"title": "4.3 MULTI-STEP AND NON-LINEAR REGRESSION", "content": "To perform multi-step GD, we tested a GD-SSM with multiple-layers, where each layer of the GD-SSM does 1-step of GD as shown in Appendix A.2. The token construction is identical to that used for N-dimensional linear regression, and each layer includes the local self-attention mechanism. In Figure 4A, we show that that trained network is able to reach the same loss as the multi-step GD.\nFinally, to be able to handle any regression task, we tested GD-SSM with an MLP layer on non-linear regression tasks. Again, we see that the trained model is able to reach the same level of performance as performing GD directly on the dataset ."}, {"title": "5 RELATED WORK", "content": "In-context learning in transformers has been studied extensively, and various mechanisms have been proposed to explain it (Hendel et al., 2023; von Oswald et al., 2023; Aky\u00fcrek et al., 2022). Of those, the most prominent is that the self-attention mechanism performs gradient descent on a linear loss. A construction with linear self-attention was demonstrated by von Oswald et al. (2023). While linear-self attention can be written as an RNN (Katharopoulos et al., 2020b), the results of von Oswald et al. (2023) depends on the tokens being constructed in a specific way, which limits the generality of their construction. Moreover, the most common type of self-attention used is softmax scaled dot-product self-attention rather than linear self-attention, and the basic linear self-attention mechanism used in von Oswald et al. (2023) is not competitive with softmax self-attention. Whereas, we show a construction that uses local-self attention with state-space models, which has been shown to be competitive with softmax self-attention transformers (De et al., 2024).\nLiu et al. (2024) formulate the SSM layer as an online optimization objective with exact solution. Similarly, Sun et al. (2024) use the state of the SSM to perform GD. But both these papers consider online updates, that is minibatch size 1 updates. One layer of their model is not capable of performing larger minibatch updates. Moreover, without local self-attention, their online optimiza-tion objective is limited to using single sequence tokens with a single-layer. And finally their goal is not to explain how in-context learning happens with existing architectures, but rather to develop entirely new SSM variants using the idea that an optimization process can be used for compressing information as well, which is the key property required of a recurrent network."}, {"title": "6 DISCUSSION", "content": "This work establishes a clear connection between state-space models (SSMs) and gradient-based in-context learning. We have demonstrated, both theoretically and empirically, that SSMs can emulate gradient descent on implicit regression models, providing a mechanistic explanation for their in-context learning capabilities. Our construction, GD-SSM, reveals the crucial role of architectural features such as local self-attention and multiplicative output interactions in enabling this behavior. These findings not only explain the success of recent SSM variants in in-context learning tasks but also provide valuable insights for the design of future sequence models.\nThe alignment between our theoretical construction and the behavior of trained models suggests that the gradient descent mechanism may be a natural inductive bias in these architectures. This understanding opens new avenues for analyzing and improving sequence models, potentially leading to more efficient and effective architectures for a wide range of tasks.\nFuture research could explore the implications of this mechanism in larger-scale models, more complex tasks, and real-world applications. Additionally, investigating how this understanding can be leveraged to enhance the design and training of state-space models could yield significant advance-ments in the field of sequence modeling."}]}