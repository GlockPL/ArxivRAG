{"title": "SimuDICE: Offline Policy Optimization Through World Model Updates and DICE Estimation", "authors": ["Catalin E. Brita", "Stephan Bongers", "Frans A. Oliehoek"], "abstract": "In offline reinforcement learning, deriving an effective policy from a pre-collected set of experiences is challenging due to the distribution mismatch between the target policy and the behavioral policy used to collect the data, as well as the limited sample size. Model-based reinforcement learning improves sample efficiency by generating simulated experiences using a learned dynamic model of the environment. However, these synthetic experiences often suffer from the same distribution mismatch. To address these challenges, we introduce SimuDICE, a framework that iteratively refines the initial policy derived from offline data using synthetically generated experiences from the world model. SimuDICE enhances the quality of these simulated experiences by adjusting the sampling probabilities of state-action pairs based on stationary DIstribution Correction Estimation (DICE) and the estimated confidence in the model's predictions. This approach guides policy improvement by balancing experiences similar to those frequently encountered with ones that have a distribution mismatch. Our experiments show that SimuDICE achieves performance comparable to existing algorithms while requiring fewer pre-collected experiences and planning steps, and it remains robust across varying data collection policies.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) [34] has demonstrated numerous successes in domains such as games [10] and robotics [36], largely due to simulation-based trial and error [26,31]. While feasible in game environments (where the game can be considered a simulator) or in simple real-world scenarios that can be accurately simulated, such direct or easy access to the environment is often not possible. Furthermore, in some areas such as medicine [27] the deployment of a new policy, even just for the sake of performance evaluation, may be risky or costly.\nOffline RL [23], also known as batch RL [22], addresses the challenge of training agents from static, pre-collected datasets, which are typically composed of off-policy data [34]. A key issue in this context is the mismatch between the state visitation distribution of the target (candidate) policy and the behavioral (logging) policy. The problem is exacerbated throughout the learning process as this mismatch grows, potentially leading to issues such as divergence in off-policy learning [2,38], making direct online-to-offline transitions challenging [8,19].\nThe agent has to perform offline policy evaluation during the learning process, a task that is particularly challenging due to the policy-induced state-action distribution mismatch. Early work on this problem tackled the challenge using products of importance sampling [29]. Subsequent approaches have focused on improving the variance by directly learning density ratios [12,25]. The DIstribution Correction Estimation (DICE) family of algorithms [28,40,42,43] achieves impressive results by leveraging optimization techniques to directly estimate stationary distribution corrections, significantly reducing variance.\nMost prior work in offline RL consists of model-free methods. These studies show that directly using off-policy RL algorithms yields poor results due to distribution mismatch and function approximation errors. To address this, various modifications were proposed, such as Q-network ensembles [8,39], regularization towards the behavioral policy [15,18,39], and implicit Q-learning [17].\nModel-based RL (MBRL) involves learning a dynamics model of the environment that can be used for policy improvement. MBRL has shown significant success in online learning, demonstrating excellent sample efficiency [9,10,11]. However, applying MBRL algorithms directly to offline datasets presents challenges due to the same distribution mismatch issue. Specifically, it is difficult to obtain a globally accurate model because the dataset may not cover the entire state-action space. Consequently, planning with a learned model without safeguards against model inaccuracy can lead to \u2018hallucinated' states [13] and 'model exploitation' [4,14,20], leading to the possibility of poor policy performance.\nMost of the prior work in offline MBRL [3,35] pre-train a one-step forward model via maximum likelihood estimation to be a simple mimic of the world and then uses it to improve the policy, without any change to the model. This results in an objective mismatch, namely the objective function used for model training (accurately predicting the environment) is unrelated to its utilization (policy optimization). Recent works have identified objective mismatch in the model training and utilization as problematic [7,21].\nWe introduce SimuDICE\u00b9, an algorithm that iteratively improves the target policy by adjusting the sampling probabilities within a world model. Unlike prior methods that focus solely on generating safe experiences, we extend that approach by integrating DICE estimations, usually used for offline policy evaluation. These estimations reveal how the reward distributions shift from the behavioral dataset to the target policy, which leads us to choose to explore those transitions further. Our experiments show that incorporating the DICE estimations with a model prediction confidence safeguard achieves superior results with less data and fewer planning steps compared to uniform sampling experiences."}, {"title": "2 Related Work", "content": "Offline Reinforcement Learning (RL) In offline RL, agents are trained only from pre-collected datasets, avoiding the risks associated with real-time data collection. One of the main issues in offline RL is the distribution mismatch between the behavioral (logging) policy and the target policy that is being optimized. Fujimoto et al. [8] tackled this by stabilizing Q-learning to reduce bootstrapping errors caused by this mismatch, specifically through the introduction of a method to limit the overestimation of Q-values. Building on this, Wu et al. [39] proposed a conservative Q-learning approach known as Behavior-Regularized Actor-Critic (BRAC), which further mitigates Q-value overestimation by incorporating a penalty term that keeps the learned policy close to the behavior policy. Levine et al. [24] emphasize the broader challenges in offline RL, highlighting the necessity of techniques that effectively manage limited and biased datasets. Most prior work in this area focuses only on model-free approaches, exploring algorithms such as the Q-network ensembles [8,39], behavioral policy regularization [15,18,39], and implicit Q-learning [17].\nDistribution mismatch correction techniques Various approaches have been developed to mitigate the policy-induced state-action distribution mismatch. Precup et al. [29] tackle the problem using products of importance sampling ratios, though this approach suffers from large variance. To correct the distribution mismatch without incurring a large variance, Hallak et al. [12] and Liu et al. [25] propose learning the density ratio between the state distribution of the target policy and sampling distribution directly.\nDualDICE [28] is a relaxation of previous methods and enables learning from multiple unknown behavior policies. They achieved impressive results by using a change of variable technique in the density ratio calculation. GenDICE [42] is a generalization of DualDICE, stabilizing estimations in the average reward setting. GradientDICE [43] outlines that GenDICE is not a convex-concave saddle-point problem in all settings and proposes a provably convergent method under linear function approximation. Despite their differences, all these algorithms use minimax optimizations, allowing them to be combined under the regularized Lagrangians of the same linear problem [40].\nModel-based offline RL Model-based RL (MBRL) uses a learned model of the environment to generate additional experiences, thereby enhancing sample efficiency. The Dyna-Q algorithm [33] was among the first to integrate model-based techniques with reinforcement learning, combining planning and learning into a unified framework. Over time, MBRL has become a key approach in online reinforcement learning, with several algorithms demonstrating exceptional sample efficiency by incorporating sophisticated dynamics models that closely simulate the environment [10,11].\nHowever, in offline settings where interactions with the environment are restricted, inaccuracies in the learned models can lead to the generation of un-"}, {"title": "3 Preliminaries", "content": "In this section, we introduce the theoretical framework and describe the problem setting. We also present the Distribution Correction Estimation (DICE) algorithm, which is the basis for updating sampling probabilities in SimuDICE.", "sections": [{"title": "3.1 Theoretical framework", "content": "We consider a Markov Decision Process (MDP) [30], in which the environment is defined by a tuple M = (S, A, R, T, \\mu_0, \\gamma) where S represents the state space, A is the action space, R is a reward function, T is the transition probability function, \\mu_0 is the initial state distribution, and \\gamma \\in [0, 1) is the discount factor. A policy in an MDP decides what action the agent should take given some state s. Formally, it is a mapping \\pi : S \\rightarrow \\Delta(A), where \\pi(s) represents the probability distribution over actions A in state s. The goal of the agent is to maximize the cumulative expected reward (its return), given by Eq. (1).\n\\rho(\\pi) = E_{s_0 \\sim \\mu_0} \\Big[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\Big]  a_t \\sim \\pi(\\cdot | s_t) (1)\nTo evaluate the performance of a policy, we define two functions: the Value function V^{\\pi}(s), which is the expected return of policy \\pi from state s (Eq. 2), and the Q-value function Q^{\\pi}(s, a), which represents the expected return following a policy starting from state s with action a (Eq. 3).\nV^{\\pi}(s) = E_{\\pi} \\Big[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\Big]  s_0 = s (2)\nQ^{\\pi}(s,a) = E_{\\pi} \\Big[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\Big]  s_0 = s, a_0 = a (3)"}, {"title": "Bellman equation", "content": "The Bellman equation provides a recursive definition of the Q-value by decomposing it into immediate reward and the discounted value of the next state-action pair. Eq. (4) shows how the Bellman equation is applied to the Q-value policy function Q^{\\pi}(s, a).\nQ^{\\pi}(s,a) = R(s, a) + E_{s'\\sim T(s,a),a'\\sim\\pi(\\cdot | s')} [Q^{\\pi}(s', a')] (4)\nThe Bellman operator \\mathcal{B}^{\\pi} iteratively applies the Bellman equation to update the Q-values until convergence, leading to a formulation as in Eq. (5).\n\\mathcal{B}^{\\pi}Q(s,a) = R(s, a) + \\gamma E_{s'\\sim T(s,a),a'\\sim\\pi(\\cdot | s')} [Q(s', a')] (5)"}, {"title": "3.2 Problem setting", "content": "Offline Reinforcement Learning The focus of this work is offline RL. Unlike online RL where the agent actively interacts with the environment to gather data and update its policy, offline RL aims to derive the optimal policy from a pre-collected dataset of experiences. Specifically, we assume access to a finite dataset D = \\{ (s_0^{(i)}, a_0^{(i)}, r^{(i)}, s'^{(i)}) \\}_{i=1}^{N} where s_0 \\sim \\mu_0, (s^{(i)}, a^{(i)}) \\sim d_D are samples from an unknown distribution d_D, r^{(i)} \\sim R(s^{(i)}, a^{(i)}), and s'^{(i)} \\sim T(s^{(i)}, a^{(i)}).\nModel-Based RL MBRL involves learning an MDP \\hat{M} = (S, A, \\hat{R}, \\hat{T}, \\mu_0, \\gamma) which uses the learned transitions \\hat{T} instead of the true transitions T, and the learned reward function \\hat{R} instead of the true reward function R. In this work, we assume the initial distribution \\mu_0 is unknown and learned from the data."}, {"title": "3.3 DualDICE estimation", "content": "In this section, we will elaborate on the DICE estimation algorithm used in SimuDICE, namely DualDICE [28], which is often used for off-policy evaluation. They obtained impressive results by reducing the off-policy evaluation problem to density ratio estimation and doing a change of variable optimization trick. The policy value can be rewritten using the importance weighing trick (Eq. 6).\n\\rho(\\pi) = E_{(s,a)\\sim d_{\\pi}} [r(s, a)] = E_{(s,a)\\sim d_D} \\Big[ \\frac{d^{\\pi} (s, a)}{d^D(s,a)}r(s,a)\\Big] (6)\nwhere d^{\\pi} is the discounted state visitation distribution (Eq. 7).\nd^{\\pi} (s, a) := (1 - \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t \\cdot Pr[s_t = s, a_t = a | s_0 \\sim \\mu_0, \\pi] (7)\nEq.(6) can be rewritten in the offline setting as a weighted average (Eq. 8), reducing the problem to estimating the density ratios (Eq. 9) for policy correction."}]}, {"title": "4 SimuDICE", "content": "For clarity and ease of understanding, we start by presenting an idealized version of SimuDICE, discussing its theoretical foundations. We then proceed to describe the practical implementation used in our experiments. Algorithm 1 presents the broad framework, while Figure 1 shows an illustration of the algorithm."}, {"title": "5 Experiments", "content": "Our experiments aim to answer the following questions: 1) how do the quality (value) and size (number of experiences collected) of the behavior policy impact the policy learned by SimuDICE; 2) how does the policy derived by SimuDICE compare with other algorithms across different off-policy datasets; and 3) what effect do various components and parameters have on SimuDICE's performance?\nTo answer the above questions, we consider commonly studied benchmark tasks from the Gymnasium Library [37]. Our experimental setup follows previous work [8,19,39]. We focus on Toy Text environments: Taxi, FrozenLake, and Cliff Walking, as shown in Figure 2. For each environment, we consider three distinct logged datasets. These datasets are collected following Wu et al. [39], with 500 timesteps of environment interaction each. First, we partially train a policy (\\pi_{\\rho}) to achieve values of approximately 0.1, 0, and -2.38, respectively. We generate three noisy \\pi_{\\rho} variants using epsilon-greedy strategies (\\epsilon = 0.1, 0.4, 0.7), introducing different noise magnitudes. The evaluation was performed on the Taxi environment using the 9 datasets collected from the other environments.\nWe evaluate the learned policies by performing rollouts in the (real) environment, strictly for evaluation purposes. These rollouts are not available to the algorithm and are not used for any learning. This evaluation protocol is consistent with prior work [8,19,39]. We report results as average per-step reward over 500 plays and 20 seeds, using identical hyperparameters across all environments. Unless stated otherwise, we used the following hyperparameters: \\alpha: 0.1, \\gamma: 0.99, 10 planning steps, 1 iteration, regularization parameter (\\lambda): 1000, play episodes: 500, and environment steps: 100.", "sections": [{"title": "5.1 Algorithm comparisons", "content": "We compare SimuDICE with two other methods: offline Q-learning and a variant of SimuDICE that uses uniform sampling probabilities, which we refer to as offline Dyna-Q. To evaluate their effectiveness across different planning scenarios, we assess both SimuDICE and offline Dyna-Q using 10 and 20 planning steps.\nFigure 3 shows that SimuDICE consistently outperforms or matches the performance of the other algorithms across various settings. In particular, in the"}, {"title": "5.2 Ablation study", "content": "In this section, we conduct an ablation study to evaluate the effect of various parameters on the performance of SimuDICE. Our analysis focuses only on the Taxi environment, as it represents the most complex scenario out of the three. For this ablation study, we have changed \\alpha to 0.05 to visualize the results better.\nPlanning Steps: How does the number of planning steps affect the model's performance?\nWe carry out an experimental evaluation to determine how different numbers of planning steps affect the agent's performance. Figure 4 shows that while the number of planning steps improves the performance, the relationship is not linear. The improvement is particularly evident in low-data cases, with the difference starting to decrease with the number of experiences in the offline dataset.\nDifferent Sampling Probabilities Formulas: How does the algorithm's performance change when we alter the method for estimating sampling probabilities?\nWe compare three different formulas for altering the sampling probabilities using the w_{\\pi/D} weights and model confidence estimation. This comparison assesses how effectively these approaches prioritize 'valuable' synthetic experiences for the world model. To ensure normalization, likelihoods are converted into probabilities by dividing each by their total sum.\nFormula 1: The default in SimuDICE (Eq. 15).\nFormula 2: Similar to Formula 1 but discourages experiences with distribution shifts instead of encouraging them (Eq. 16).\nFormula 3: Applies lambda-regularized DICE estimations w_{\\pi/D} over a uniform sampling model, ignoring model confidence (Eq. 17)."}]}, {"title": "6 Discussion", "content": "This work introduced SimuDICE, a framework for policy optimization in offline reinforcement learning, and assessed its effectiveness across various scenarios.\nSimuDICE addresses the data needs and the state-action distribution mismatch. In this section, we discuss the findings, their implications, and their limitations.\nKey Findings and Implications Based on the experimental results, we derive the following key findings and implications of the proposed method:\nImproved Sample Efficiency: Our experiments show that SimuDICE achieves greater sample efficiency compared to uniform sampling experiences during the planning stage and vanilla Q-learning. This advantage is particularly evident in data-rich environments where the collected data diverges significantly from the optimal policy. We attribute this improvement to the adjustment of sampling probabilities using DICE estimations.\nDistribution Mismatch Correction: Our results show that in addition to SimuDICE being more sample-efficient than the methods we compared it with, it also requires fewer planning steps to achieve comparable or superior policies. This suggests that we can achieve equal or better policies while generating fewer synthetic experiences using the world model, which implicitly reduces the risk of 'hallucination'. While the DICE estimations play an important role in this improvement, it is worth mentioning that the confidence prediction estimation was added for completeness as a safeguard to balance exploitation-exploration for these experiences.\nObjective Mismatch: Our ablation study reveals no significant performance gain from updating the sampling probabilities multiple times during a run. We believe this may be due to the simplicity of the environments used in our experiments, as the approach was originally theorized for more complex settings [7,21]. Additionally, the basic nature of the components, such as the world model, may have influenced these results."}, {"title": "7 Conclusion", "content": "We have presented SimuDICE, a framework for optimizing policies in model-based offline reinforcement learning by adjusting the world model's sampling probabilities using DualDICE estimation and the estimated prediction confidence. The main innovation of SimuDICE lies in its ability to correct the state-action distribution mismatch between the behavior policy and target policy through a bi-objective optimization that balances experience realism and diversity, thereby preventing mode collapse and the generation of hallucinated states.\nOur experiments show that modifying sampling probabilities offers advantages over uniform sampling, achieving similar average per-step rewards with fewer pre-collected experiences and planning steps. SimuDICE also demonstrates greater robustness to variations in data collection policies.\nFuture work includes (1) incorporating a world model that can generate novel experiences, using a more stable and robust DICE estimator, and implementing a policy that can handle continuous state-action spaces; (2) further exploring the impact of altered sampling probabilities on the stability and robustness of SimuDICE across various data collection policies; and (3) evaluating the algorithm's performance in more complex, continuous environments where greater divergence between the behavioral and target policies is present.", "sections": [{"title": "Limitations", "content": "In this part, we outline the limitations of this study and indicate some possible future work directions.\nSimple environments: While SimuDICE showed promising results in deterministic grid-world environments, their simplicity limits the generalizability of the findings. Therefore, future research should focus on evaluating SimuDICE in more complex settings to assess its effectiveness better.\nSensitivity to sample probabilities formula: In the ablation study, we show that the performance of SimuDICE is affected by different variations of the sampling probability formula. Given the limited number of scenarios the algorithm was tested on, a more extensive evaluation across a wider range of environments is necessary to assess its effectiveness.\nComparison with other algorithms: This study conducted a comparison between SimuDICE, vanilla offline Q-learning, and offline Dyna-Q [33], where the latter is effectively SimuDICE with uniform sampling probabilities. While this comparison gives an intuition of its improvements, a comparison with other algorithms might provide more insights into its performance."}]}]}