{"title": "SEA: State-Exchange Attention for High-Fidelity Physics-Based Transformers", "authors": ["Parsa Esmati", "Amirhossein Dadashzadeh", "Vahid Goodarzi", "Nicolas Larrosa", "Nicol\u00f2 Grilli"], "abstract": "Current approaches using sequential networks have shown promise in estimating field variables for dynamical systems, but they are often limited by high rollout errors. The unresolved issue of rollout error accumulation results in unreliable estimations as the network predicts further into the future, with each step's error compounding and leading to an increase in inaccuracy. Here, we introduce the State-Exchange Attention (SEA) module, a novel transformer based module enabling information exchange between encoded fields through multi-head cross-attention. The cross-field multidirectional information exchange design enables all state variables in the system to exchange information with one another, capturing physical relationships and symmetries between fields. In addition, we incorporate a ViT-like architecture to generate spatially coherent mesh embeddings, further improving the model's ability to capture spatial dependencies in the data. This enhances the model's ability to represent complex interactions between the field variables, resulting in improved rollout error accumulation. Our results show that the Transformer model integrated with the State-Exchange Attention (SEA) module outperforms competitive baseline models, including the PbGMR-GMUS Transformer-RealNVP and GMR-GMUS Transformer, with a reduction in error of 88% and 91%, respectively, achieving state-of-the-art performance. Furthermore, we demonstrate that the SEA module alone can reduce errors by 97% for state variables that are highly dependent on other states of the system. The repository for this work is available at: https://github.com/ParsaEsmati/SEA", "sections": [{"title": "1 Introduction", "content": "Solving partial differential equations (PDE) has been a primary concern of many fields in science and engineering, including physics [Salvini et al., 2024], chemistry [Grilli et al., 2020], and material sciences [Grilli et al., 2018, Esmati et al., 2024]. In many cases where a direct analytical solution of the PDE is impossible to obtain, numerical simulations are used. To solve these equations numerically, the domain is discretized into smaller cells using a discretization method such as finite element or finite volume methods. In some cases, the domain of interest is divided into millions of smaller elements forming large matrices representing the equation [Moukalled et al., 2016]. Solving these discretized equations typically follows an iterative technique, which can take days and sometimes weeks of runtime to converge to the full solution on the defined temporal domain [Liu et al., 2024, Jiang et al., 2023]. In some cases to resolve some specific features of the system a fine mesh is required. The graphical representation of small droplet [Um et al., 2018], cracking and brittle behaviour in thin components [Pfaff et al., 2014], multiphase and turbulent flows in computational fluid dynamics (CFD)[Kochkov et al., 2021, Thuerey et al., 2020, Heyse et al., 2021a,b] are all instances of such scenario.\nFiner mesh however comes at the price of computational cost, which may not be feasible in industry settings. Consequently, there is a critical need for frameworks that can either bypass these detailed simulations or accelerate the solvers.\nRecent advances in sequential networks have shown promising results in estimating the state variables of dynamical systems over time. However, they have not yet been effectively utilized to bypass or accelerate numerical models [Yousif et al., 2022]. Key challenges include lengthy training times and steep gradients in rollout errors [Sun et al., 2023, Han et al., 2022]. Additionally, many of these networks are task-specific, necessitating retraining for each unique test case [Li et al., 2020a]. If a model is to be integrated into a solver, steep error accumulation necessitates frequent retraining of networks to maintain efficacy when bypassing numerical solvers. This leads to unnecessary computational costs, making it challenging to integrate these models with solvers. Ideally, a model should be capable of learning the underlying physics and constitutive laws to minimize error gradients. Such a model could reduce retraining frequency and enable reuse through transfer learning approaches [Yosinski et al., 2014].\nTransformer-based architectures, in particular, are at the forefront of advancing the sequential estimation of system variables [Sun et al., 2023, Zhao et al., 2023]. The output of numerical models is similar to videos, as both involve a temporal axis and a set of cells (pixels) discretizing the spatial axis. This resemblance highlights the potential of applying techniques from vision models, to enhance the accuracy and efficiency of predicting state variables in physical simulations [Lee et al., 2024, Sun et al., 2019, Arnab et al., 2021, Bertasius et al., 2021]. Inspired by the spatio-temporal cross-attention mechanisms used in vision models [Lin et al., 2022, Chen et al., 2021], we propose a State-Exchange Attention (SEA) module for physics-based transformers, designed to mimic partial differential equations (PDEs) by explicitly capturing variable coupling. The effective multidirectional information exchange between fields enables the correction of some fields by others, allowing the model to learn the complex features of the physical system. This work employs a Vision Transformer (ViT) like [Dosovitskiy et al., 2020] encoder for mesh embedding. The embedded mesh at each instance is then used in the temporal State-Exchange Attention (SEA) integrated Transformer to predict the system's future states.\nIn summary, the contributions of this work include:\n1. Design and integration of a novel SEA module for physics-domain Transformer models. This module enables learning the underlying physics through a multidirectional information exchange process between the state variables.\n2. Assembly of a full ViT-SEA integrated framework that demonstrates state-of-the-art results in generating the complete sequence of the physical system given the initial sequence and specific time-invariant parameters representing the model.\n3. Comprehensive evaluation of SEA module, and ViT-SEA integrated transformer across different computational fluid dynamics cases, showing over 60% reduction in error in all cases compared to state-of-the-art models."}, {"title": "2 Related work", "content": "In recent years, deep learning has led to major advancements in modeling physical systems, with contributions ranging from applying computer vision techniques to enhance the resolution of coarse meshes to incorporating physical symmetries and constraints through innovative modifications to learning objectives.\nThe work of [Raissi et al., 2019] demonstrated the feasibility of directly incorporating physical information into the objective function, including initial conditions and necessary physical residuals. Other examples of such approaches include [Jeon et al., 2024, Haghighat et al., 2021, Yu et al., 2022]. While these techniques embed physical knowledge into the objective function, they lack generality, as the obtained parameters tend to be case-specific. The broader underlying physics is not fully captured, and only the physical symmetries specific to a particular case are addressed.\nAnother class of methods is neural operators [Li et al., 2020b]. These methods generalize well across the PDEs they are trained on and are not case-specific. However, these models require data to be represented in higher dimensions to capture complex relationships. The application of an integral kernel in a high-dimensional representation of complex geometries with dense data points poses a significant computational challenge [Li et al., 2020a]. While neural operators can employ different architectures to reduce rollout error, they lack explicit mechanisms for integrating temporal data, unlike transformers, which handle sequence dependencies robustly through their attention mechanisms. This limitation can affect their effectiveness in applications that require sequential data processing.\nA notable trend in recent years involves the use of encoder-decoder pairs to process dynamical states in latent space. The work of [Wiewel et al., 2019] is an early example of this approach. In general, the input must first be encoded into the latent space while preserving the context. A sequential network, such as LSTM, GRU, or other variants, is then trained on the encoded data. Thus, these approaches typically require two components: an encoder-decoder pair and a temporal model. Another example of this approach is Mesh Graph Networks (MGN) [Pfaff et al., 2020] and Graph-Network-based Simulators (GNS) [Sanchez-Gonzalez et al., 2020]. The encoder modules of these models are based on graph networks. After processing nodes, edges, and features, they use a simple multi-layer perceptron (MLP) to compute the derivatives of the features over time and update the state using a forward-Euler scheme. While the encoder-decoder pair is a powerful tool, especially for unstructured mesh spaces, the lack of a robust time-stepping model significantly limits performance, with rollout error becoming a dominant issue.\nCurrent state-of-the-art models that demonstrate optimal performance on baseline datasets typically combine a graph network encoder with more advanced time-stepping algorithms. Notably, the study by [Han et al., 2022] employs a Graph Mesh Reducer (GMR) for encoding, along with a sequential time-stepping network and a Graph Mesh Up-Sampling (GMUS) decoder. This research explored various sequential models, including LSTM, GRU, and Transformers, with the latter achieving the lowest rollout error. Building on these principles, [Sun et al., 2023] introduced a modified version of GMR-GMUS, adding a RealNVP normalizing flow model to the time-stepping transformer. While adding a normalizing flow model does not yield a fully tractable model, it allows for the direct maximization of log-likelihood over the final data point, improving the overall objective and resulting in the most competitive baseline model reported thus far. However, these models still cannot fully address the rollout error in a systematic way that incorporates our physical understanding of the system.\nAs a result, improving rollout errors and reducing training time remain significant challenges in this field. In this work, we demonstrate that both objectives can be addressed by separating the fields and allowing them to learn the inherent physical relationships and symmetries. To this end, we introduce the SEA module, implemented on top of a Vision Transformer (ViT) based mesh autoencoder."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Problem statement", "content": "Temporal generation of the states in a dynamical system from an initial condition is analogous to video generation tasks, where the process is conditioned on both the initial state and an external input. Similarly, the evolution of a dynamical system can be conditioned on known parameters, such as the Reynolds number in fluid flow and the system's initial condition. Given this similarity to autoregressive generative models like ART-V [Weng et al., 2024], we formulate the temporal evolution as an autoregressive generation of states, conditioned on both the initial state and a time-invariant parameter.\nHowever, autoregression on the mesh space is challenging due to the large size of the elements and the number of variables stored on each element. Hence the mesh must be embedded into a manageable embedding, and our formulation becomes an autoregressive sequence generation in latent space."}, {"title": "3.2 ViT mesh autoencoder", "content": "The backbones commonly used to create embedding spaces in image and video models, such as Latent Diffusion Models (LDM) [Rombach et al., 2022] and Video Diffusion Models (VDM) [Ho et al., 2022], cannot be directly applied to mesh data due to their inherent structured pixel inductive biases. Therefore, our proposed autoencoder must specifically overcome these biases for unstructured mesh space. Given that the temporal model employs a transformer backbone, the embedding must generate tokens compatible with the transformer's input requirements. These tokens are generated similarly to that of ViT [Dosovitskiy et al., 2020]. Following the approach used in ViT, the space is divided into multiple patches, with each patch containing a number of cells. Let \u03a9 denote the domain in which our study is conducted, with dimension d. Assume that the domain is discretized into a set of nodes {xi}i=1N, where each node xi represents a point in Rd. To construct patches, we define a partitioning function fp : Rd \u2192 {1,2,..., M}, which assigns each node xi to one of M patches based on its coordinates.\nAssuming the boundaries between patches are showing with B = {b1, b2,..., bm} then the function fp is defined as follows:\n\nfp(xi) = \\begin{cases} 1 & \\text{if } x_i < b_1, \\\\ j & \\text{if } b_{j-1} \\leq x_i < b_j \\text{ for } 2 \\leq j \\leq m, \\\\ m+1 & \\text{if } x_i \\geq b_m. \\end{cases}\n\nTo address the challenge of irregular and unstructured meshes, which lead to varying numbers of nodes per patch, each patch is padded to align with the length of the largest patch. A padding value of 0 is used throughout the framework. Moreover, bias terms are excluded in the embedding process, and the Gaussian Error Linear Unit (GELU) activation function [Hendrycks and Gimpel, 2016] is applied to ensure that the padded elements do not influence the spatial encoding. To achieve spatially aware embeddings and coherent reconstructions, we apply a multi-head self-attention mechanism (MHSA). This padding and embedding strategy is illustrated in Figure 1.\nThe output generated by the Vision Transformer (ViT) embedding module is subsequently flattened and utilized as tokens within the temporal model. The complete process is thoroughly explained in appendix A."}, {"title": "3.3 Temporal and State-Exchange Attention model", "content": "The State-Exchange Attention module is integrated into the temporal model in this work to enhance the autoregressive generation of sequences in time. The temporal model utilizes a transformer architecture to capture the temporal dependencies of the state variables. This transformer includes the adaptive layer-norm by [Peebles and Xie, 2023] and the rotational positional embedding (RoPE) as developed in [Su et al., 2024] and adapted by state of the art autoregressive image generation models [Lu et al., 2024, Sun et al., 2024]. The adaptive layer-norm employed in this work is modified to take the continuous time invariant parameters as input and act as a secondary conditioning mechanism on the temporal model. Full detail of the temporal model is provided in appendix B.\nThis work initializes a decoder block for each state variable in the given PDE For instance, the Navier-Stokes equations governing the fluid dynamics, requires the resolution of variables such as velocity and pressure each of these are assigned an expert decoder. The SEA module is designed to allow the exchange of information amongst these experts with cross attention. We further investigate other modes of information exchange in appendix D.\nThe flow of information through the expert layers and the SEA module can be formulated by building on the well-known attention mechanism. For clarity, the terms regarding the positional embedding are omitted here. We start from the encoded groups of variables presented in section 3.1, denoted by Zjt. To simplify the notation and remove the explicit dependence on t, we represent the sequence of encoded variables across all time steps as a single matrix, Zj, which stacks the encodings of all time steps. The attention mechanism then reads:\n\nAttention(Zj) = \\text{softmax}\\left(\\frac{Q(Z^j)K(Z^j)^T}{\\sqrt{d_k}}V(Z^j)\\right)\n\nWhere K(Zj), Q(Zj), and V(Zj) represent the key, query, and value matrices, and are obtained from the linear transformation of the input Zj by a set of learnable weights WK, WQ, and Wv. Additionally dk represents the model dimension.\nFurthermore the adaptive layer-norm is demonstrated by 'AdaLN' and hence following a pre-norm convention the multihead self-attention (MHSA) becomes:\n\n(Z^j)_{SA} = Z^j + \\text{Attention} \\left(\\text{AdaLN}(Z^j)\\right), \\forall j \\in G\n\nGiven the autoregressive task at hand all the attention mechanisms including the presented MHSA have causal mask to improve autoregressive generation.\nThe field information flows through the information exchange module after the temporal self attention. This information exchange module is represented by the state-exchange attention in Figure 2, where we allow the state variables to exchange relevant information with a causal cross attention mechanism. This is formulated as:\n\n(Z^j)_{SEA} = (Z^j)_{SA} + \\sum_{k \\in G \\ k \\neq j} f_{SEA}(\\text{AdaLN}((Z^j)_{SA}), \\text{AdaLN}((Z^k)_{sa})), \\forall j \\in G\n\nHere, fSEA(\u00b7, \u00b7) represents our information exchange mechanism SEA. This module takes in two arguments: the first is the adaptive layer norm of the expert for which the attention is taking place, and the second is the adaptive layer norm from the expert to which the module is attending. These are represented by AdaLN(Zjk) and AdaLN(Zksa), where k and j are non-equal embedding field indices.\nTo enhance efficiency during information exchange, we adopt a bottleneck mechanism inspired by expert-based architectures such as [Lee et al., 2024]. Introducing a bottleneck at this stage helps keep the model scalable by enabling selective information exchange in a lower-dimensional space. Another instance of such strategy is the Perceiver architecture [Jaegle et al., 2021], where cross-attention creates a bottleneck for high-dimensional data from different modalities. In our case, we employ a simpler method, using a down-projection with learnable parameters, following the approach in [Lee et al., 2024]. If the down- and up-projection matrices for mapping to the bottleneck are denoted by W\u2193 and W\u2191, respectively, the State-Exchange Attention mechanism is fully described by Equation 4. The arguments to this function are the AdaLN of (Zjk)sa and (Zksa), as shown in Equation 3. However, for clarity in illustrating the equation, we use Ai and Ak to represent the input arguments here.\n\nf_{SEA}(A^j, A^k) = W_\\uparrow \\text{ softmax}\\left(\\frac{Q(W_\\downarrow A^j)K(W_\\downarrow A^k)^T}{\\sqrt{d_k}}\\right)V(W_\\downarrow A^k)\n\nIn the presented equation K(W\u2193Aj), Q(W\u2193Aj), and V(W\u2193Ak) represent the key, query and value matrices obtained form the down projection of the inputs.\nThe conditioning of the generations on external parameters such as the Reynolds number is done using an indirect method of adaptive layer norm and a direct method of time invariant parameter injection module (TIPI). These components replace the more computationally intensive attention mechanism commonly used in physics domain autoregressive models. The TIPI component processes the time-invariant parameters using a multilayer perceptron (MLP) with a GELU activation function. The MLP maps the time-invariant parameters \u0398 to the model's embedding dimension through learnable parameters. As shown in Equation 5, the information injector, represented by TIPI[\u00b7, \u00b7], injects these processed parameters into the current state by summing the model's embedded information with the upscaled time-invariant parameters.\n\n(Z^j)_{Output} = (Z^j)_{SEA} + \\text{MLP}\\left(\\text{AdaLN} \\left(\\text{TIPI}\\left[ (Z^j)_{SEA}, \\Theta \\right]\\right)\\right)\n\nFigure 2 illustrates the architecture's detailed schematic."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Procedure", "content": "In this section, we evaluate the proposed model on two benchmark datasets and compare its performance with other frameworks. First, the complete model is tested on the cylinder flow benchmark, a widely used dataset in computational fluid dynamics. This evaluation includes the comparison of the full model with recent physics domain autoregressive models.\nWe then explore a multiphase case, where the model is tasked to resolve the interface by taking into account the fluxes caused by the velocities using the SEA module. In this section, only the temporal aspects of the model are varied (SEA module), while the ViT mesh autoencoder is fixed to isolate and eliminate the impact of encoding method on model's performance. To this end, separate decoders are assigned, one for velocity and another for volume fraction, similar to structure depicted in 2. The inclusion of the volume fraction allows us to study the extent of the model's capability to resolve interfaces and the effect of State-Exchange Attention on capturing the multiphase scenarios.\nFor consistent comparison with state-of-the-art models [Sun et al., 2023, Han et al., 2022], relative mean squared error is used to quantify the errors. The model was trained on an A100 GPU for approximately 2 hours for both datasets. Furthermore, a consistent Transformer architecture was adopted in both cases, utilizing 1 layer and 8 attention heads. The embedding dimension of the model for the cylinder flow case was set to 1024, in line with the literature, while a dimension of 2048 was used for the multiphase flow case to effectively capture the interface. Full details of the configurations and datasets are provided in Appendix G and F, respectively."}, {"title": "4.2 Evaluation of ViT mesh autoencoder", "content": "The autoencoder used in the following experiments was trained exclusively with a reconstruction objective. The complete training procedure for this model is detailed in Appendix A. Reconstruction errors, compared to recent graph-based autoencoders, are presented in Table 1."}, {"title": "4.3 Evaluation of temporal model on general case", "content": "We assess our complete architecture using the 2D cylinder flow, a benchmark dataset employed by other leading baseline models. In this case, the Navier-Stokes equation governs the motion of the fluid throughout the domain. Consequently, the trajectory to track is the velocity and pressure. The mesh was initially tokenized at all time steps based on the explained ViT mesh encoder. These tokens were then fed through the SEA integrated model illustrated in Figure 2. During the training, the entire sequence was fed to the network for each batch. During inference, the model was set to estimate the trajectory autoregressively, and hence, the test errors correspond to the total rollout error. For the cylinder flow dataset, the error was evaluated over the case with Reynold's number of 400. This case was chosen to keep consistent with the other competitive models.\nThe recorded rollout error, and its comparison with other baseline models is illustrated in Figure 3."}, {"title": "4.4 Evaluation of temporal model on multi-phase", "content": "Evaluation of the presented model is extended to include a multiphase flow scenario, which complements the analysis presented in Section 4.3. The evaluated test case in this experiment corresponds to an immiscible collapse of two blocks of liquid due to density differential. In studies of multiphase flows, a critical aspect is the precise identification of fluid interfaces. This is accomplished by using a volume fraction state variable, denoted by \u03b1, which indicates the region occupied by the fluid. The value of \u03b1 ranges from 0 in one fluid to 1 in the other, effectively distinguishing between the two phases. To capture the phase, an additional block is assigned to the volume fraction which can communicate with other fields (Velocity in this case). Given the minor variations of pressure, we discard this variable here and only focus on the importance of the field communication between velocities and volume fraction through SEA module.\nWe investigate three possible variations of the Transformers to achieve this. First, we estimate the rollout error of the model with the SEA module. Second, we evaluate a basic model that encodes the fields into different latent spaces with no mode of information exchange. Finally, we assess a model that encodes all fields together into a single latent space, referred to as the Field Fusion Encoder (FFE) Transformer. This latter model corresponds to the Transformer architecture used in the PbGMR-GMUS Transformer-RealNVP and GMR-GMUS Transformer, as indicated in the provided results. The rollout error of these models are presented in Figure 5.\nFrom the comparative results presented in Figure 5, it is evident that the models with the SEA module outperform the other variations. Most of the error observed in the average error plot in Figure 5 corresponds to the error in the volume fraction. The mean volume fraction errors over all time steps are 0.12, 0.25, and 4.61 for the SEA-integrated Transformer, basic Transformer, and FFE Transformer, respectively. This represents approximately a 52% and 97% reduction in error with the integration of the SEA module, compared to the basic and FFE Transformers. Additionally, improvements of 48.5% and 40% are observed in the averaged velocity components.\nFurther demonstration of the contour maps is provided in Figure 6 where the actual interface tracking capability of the SEA enhanced transformer can be observed. Further visual results on the velocities are provided in Appendix H."}, {"title": "5 Discussion", "content": "The presented ViT-based mesh autoencoder and State-Exchange Attention (SEA) integrated Transformer module were evaluated through two different experiments on computational fluid dynamics (CFD) problems. A significant improvement in relative mean squared error was observed when the SEA module was deployed. During the cylinder flow evaluation, the full ViT-SEA integrated transformer framework achieved over 80% improvement compared to all competitive baselines. This improvement was accompanied by a lower gradient in the error, demonstrating a form of self-correction through information exchange between the velocity and pressure fields. The isolated SEA module was then tested on a multiphase case, resulting in a 97% improvement in the volume fraction compared to the field fusion encoder transformer, where the entire field is encoded into the same latent space. In the multiphase case, it was evident that the velocities exhibited a marginal error difference; approximately 40-50%; however, the volume fraction dominated the overall improvement. This is due to the significance of velocity in the displacement of the interface, whereas the volume fraction did not provide any valuable information to the velocity field. The error reduction observed with the deployment of the SEA module suggests that SEA module provides the necessary tools for the underlying physics of the governing to be captured.\nThe presented module, however, may face challenges when scaling to equations involving a large number of state variables. For instance, in multiphase flows with more than two phases or in grain growth within materials where each grain is represented by a state variable, the model would require a corresponding number of transformers to operate in parallel. This could lead to inefficiencies as the number of variables increases significantly."}, {"title": "6 Conclusion", "content": "The presented work introduces SEA, a novel module that enables the state variables of a physical system to exchange information within the transformer architecture. Transformers integrated with SEA demonstrated state-of-the-art performance, surpassing previously established benchmarks by other transformer-based models with over 60% error reduction. This significant improvement underscores the effectiveness of SEA in capturing complex interactions within physical systems. Future work will explore the scaling complexity of SEA to handle more state variables and its application in other domains."}, {"title": "Appendix", "content": ""}, {"title": "A ViT autoencoder for Mesh detail", "content": "Embedding mesh data presents a greater challenge compared to images, where a structured pixel grid allows for the natural application of convolutional kernels. The unstructured nature of meshes requires novel approaches for embedding, such as graph neural networks [Pfaff et al., 2020], or systematic methods for padding within the kernels. However, determining the appropriate padding and their position in the convolution is not a simple task.\nViT models, originally developed for image classification, partition images into patches and apply a linear projection to each patch. Inspired by this structure, we developed a ViT-like mesh autoencoder, where patches are generated on the mesh, and each patch is encoded individually. In this approach, the number of cells per patch becomes readily available, allowing all patches to be padded to the length of the patch with the maximum number of cells. To enhance the processing capability during the embedding stage, we replace the original patch projection in the ViT with an MLP layer. The self-attention block applied after this step further refines the embedding, creating spatial coherence by allowing each patch to capture the global context in addition to the local context encoded by the MLP.\nThe following list outlines the evolution of the data structure as it passes through the ViT-based mesh autoencoder. Here, B denotes the batch size, T represents the number of time steps, C is the total number of mesh cells, F refers to the number of fields (analogous to channels in image or video contexts), P is the number of patches, Cp is the number of cells per patch, and D is the embedding dimension. In this study, batches are derived from the generated trajectories, as described in Section F.\nInput: [B,T, C, F]\nPatchify (Patched): [B,T,C, F] \u2192 [B,T, P, Cp, F]\nPermute: [B,T, P,Cp, F] \u2192 [B,T, P, F, Cp]\nCollapse batch with time and field with cell: [B * T, P, F, Cp] \u2192 [B * T, P, F * Cp]\nEncode with MLP (Embd): [B *T, P, F * Cp] \u2192 [B * T, P, D]\nGlobal awareness with MHSA (z): [B * T, P, D] \u2192 [B * T, P, D]\nDecode with MLP (P): [ B * T, P, D] \u2192 [B * T, P, F * Cp]\nThe presented procedure is done for fields within each field group separately. The field groups refer to the groups of fields with the same physical dimension as formalised in Section 3.1.\nFor completeness, we also provide a variational version of the encoder in the repository, incorporating a weighted KL divergence penalty, following the approach in [Rombach et al., 2022]. However, in our"}, {"title": "B Temporal transformer detail", "content": "The transformer used to capture temporal dependencies is designed with a decoder-only architecture, incorporating the SEA and TIPI modules. In this work, we develop and evaluate two variations of information exchange methods, namely exchange by addition and exchange via cross-attention, as shown in Figure 8. Both methods operate within an information bottleneck, where the data is down-projected to retain only the relevant information required for exchange. To ensure improved gradient flow and prevent information loss due to this bottleneck, we introduce a residual connection from the output of the multi-head self-attention (MHSA) to the flow outside the SEA module. The down-projection within the bottleneck is set to a factor of 2, following the approach in [Lee et al., 2024]. The architecture referred to as the basic transformer does not incorporate any connections or information exchange between the decoders assigned to different variables.\nAdditionally, the TIPI module is incorporated as a direct conditioning mechanism, while AdaLN serves as an indirect conditioning technique. In the final design, the primary TIPI module utilizes the addition-based approach, as shown in Figure 9. However, we also explored injection via attention, which is commonly used as a conditioning technique in physics-based models.\nThe adaptive layer normalization (AdaLN) technique, used as the indirect conditioning method, is adopted from [Peebles and Xie, 2023]. In this approach, conditioning is applied by replacing the standard layer normalization with scale and shift parameters derived from the time-invariant parameter \u0398. To remain consistent with [Peebles and Xie, 2023], we use an MLP to compute the"}, {"title": "C Training procedures", "content": "Autoencoder: The autoencoder was trained wtih the objective of the reconstruction error with L2 loss:\n\nL = E_{x~D} [||X \u2013 X' ||_2^2]\n\nWhere X' represents the model's output, and the expectation is done with respect to the data drawn from the batch.\nFor a consistent comparison with the literature we provide the errors using relative mean squared error where we refer to this as relative reconstruction error, and relative rollout error for the autoencoder and temporal models respectively. The relative rollout MSE is formulated as:\n\nRelMSE = \\frac{E_{x~D}\\left[\\sum_{i=1}^N ||X_i - X'_i||^2\\right]}{\\sum_{i=1}^N ||X_i||^2}\n\nHere the summation is done on the cell dimension.\nTemporal model: The temporal model is trained using L2 loss in a teacher forcing manner, where the model generates the encoded field for the subsequent time step after observing the full trajectory. To accurately assess the autoregressive performance of the model, we conduct an autoregressive evaluation every 250 epochs, allowing us to track the learning history for the required autoregressive task. This evaluation includes monitoring both the encoded and decoded errors. We observe that, although the magnitude of variations in error is not perfectly aligned, the trends in the embedding space and decoded space errors follow a similar trajectory, peaking simultaneously. The following figure provides an example of the autoregressive learning curves for both the embedded space and the decoded space, using the ViT mesh autoencoder described in earlier sections."}, {"title": "D Ablation study", "content": "Ablation of the information exchange method is performed using a vanilla architecture. The architecture incorporates layer normalization, a common practice in transformer models within the physics domain. We also initialize with absolute positional embeddings. Following our exploration of the vanilla transformer with the information exchange module, we extend the ablation studies to include the components of the main architecture. Specifically, we show that adding TIPI and AdaLN as conditioning mechanisms further reduces error and eliminates the need for an explicit attention mechanism for conditioning."}, {"title": "E Theoretical error", "content": "To gain deeper insight into the problem and the sources of error", "coupling": "n\n\\frac{\\partial \\psi^1"}, {"condition": "n\nH(\\Psi) > \\text{max"}, {"have": "n\nI"}]}