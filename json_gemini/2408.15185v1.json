{"title": "PoseWatch: A Transformer-based Architecture for\nHuman-centric Video Anomaly Detection Using\nSpatio-temporal Pose Tokenization", "authors": ["Ghazal Alinezhad Noghre", "Armin Danesh Pazho", "Hamed Tabkhi"], "abstract": "Video Anomaly Detection (VAD) presents a significant challenge in computer vision, particularly due\nto the unpredictable and infrequent nature of anomalous events, coupled with the diverse and dynamic\nenvironments in which they occur. Human-centric VAD, a specialized area within this domain, faces\nadditional complexities, including variations in human behavior, potential biases in data, and sub\nstantial privacy concerns related to human subjects. These issues complicate the development of\nmodels that are both robust and generalizable. To address these challenges, recent advancements\nhave focused on pose-based VAD, which leverages human pose as a high-level feature to mitigate pri\nvacy concerns, reduce appearance biases, and minimize background interference. In this paper, we\nintroduce PoseWatch, a novel transformer-based architecture designed specifically for human-centric\npose-based VAD. PoseWatch features an innovative Spatio-Temporal Pose and Relative Pose (ST\nPRP) tokenization method that enhances the representation of human motion over time, which is also\nbeneficial for broader human behavior analysis tasks. The architecture's core, a Unified Encoder Twin\nDecoders (UETD) transformer, significantly improves the detection of anomalous behaviors in video\ndata. Extensive evaluations across multiple benchmark datasets demonstrate that PoseWatch con\nsistently outperforms existing methods, establishing a new state-of-the-art in pose-based VAD. This\nwork not only demonstrates the efficacy of PoseWatch but also highlights the potential of integrating\nNatural Language Processing techniques with computer vision to advance human behavior analysis.", "sections": [{"title": "1 Introduction", "content": "Video Anomaly Detection (VAD) is a rapidly\ngrowing area within Computer Vision that focuses\non automatically identifying unusual events or\nbehaviors in video sequences (Ren et al., 2021;\nPatrikar and Parate, 2022; Abbas and Al-Ani,\nwe adopt the self-supervised learning approach,\naligning with recent research trends, to address\nthe inherent challenges in human-centric anomaly\ndetection.\nRegardless of the training methodology\nemployed, human-based VAD is primarily cat\negorized into two strategies: pixel-based (Wang\net al., 2023; Zaheer et al., 2022; Georgescu et al.,\n2021a; Ristea et al., 2022; Wang et al., 2022) and\npose-based (Morais et al., 2019; Markovitz et al.,\n2020; Jain et al., 2021; Rodrigues et al., 2020;\nZeng et al., 2021; Yu et al., 2023) methods. For\nvideo processing and training, a general technique\ninvolves analyzing the pixel data of frames over\ntime. This holds true for VAD as well, where\npixel-based approaches examine the raw pixel\nvalues in video frames to identify anomalies,\nleveraging the fine granularity provided by the\npixels in each frame. Nonetheless, focusing specif\nically on human behaviors, the examination of\nevery pixel can result in the analysis of excessive,\nredundant pixels, introducing undesirable noise\ninto the system (Hirschorn and Avidan, 2023).\nSuch noise can range from relatively harmless\ndisturbances like background changes to more\ncritical issues like demographic attributes (skin\ncolor, clothing, gender, etc.), potentially inducing\nbiases within the system. While such noise and\nbiases might not show their effect through avail\nable metrics and datasets, they becomes critical\nin the deployment of the models in the real world.\nPose-based methods have been developed to\nmitigate these effects. These methods concentrate"}, {"title": "2 Related Works", "content": "The field of anomaly detection has evolved, adapt\ning to scientific advancements, particularly within\nthe realm of Artificial Intelligence (AI) (Patrikar\nand Parate, 2022; Zhu et al., 2020). The trend\nstarted from handcrafted methods (Co\u015far et al.,\n2016; Cheng et al., 2015; Yuan et al., 2014) with\napproaches utilizing algorithms such as histogram\nof optical flow (Kaltsa et al., 2015). With the"}, {"title": "3 PoseWatch", "content": "The architecture of PoseWatch, depicted in\nFigure 2, embodies two main components: the\nSpatio-Temporal Pose and Relative Pose (ST\nPRP) tokenization, alongside the Unified Encoder\nTwin Decoders (UETD) transformer core. Pose\nWatch leverages a shared encoder, a Current Tar\nget Decode (CTD), and a Future Target Decoder\n(FTD). The CTD and FTD branches provide com\nplementary insights by capturing distinct patterns\nwithin the input sequences. This synergy enhances\nthe overall model\u2019s robustness, as it minimizes the\ninfluence of individual branch errors through the"}, {"title": "3.1 ST-PRP Tokenization", "content": "The tokenization process aims to provide a rich\nand informative input sequence to the transformer\nmodel. We define the absolute pose sequence as\nfollows:\n$S_{to} = [P_{to}, P_{to+1}, p_{to+2},..., p_{to+B-1}]$\nwhere $S_{i}$ is the absolute pose sequence of\nperson i, $P_{is}$ pose data containing (x,y) coor\ndinates of the joints, to is the starting frame of\nthe sequence, and \u1e9e is the input window size.\nThis will provide the model with basic informa\ntion about the position of a person's joints in each\nsequence frame. However, a person's movement\npatterns through the sequence also reveal critical\ninformation for anomaly detection. To accentuate\nthe global movement of humans through space,\nin addition to the absolute pose sequence, we\nalso leverage the relative pose sequence shown\nin Equation (2). $AS_{i}$ is constructed to highlight\nthe overall movements of the subjects relative to\nthe coordinates of the first pose of the current\nsequence as shown in Equation (3).\n$AS_{to} = [\\Delta P_{to}, \\Delta P_{to+1}, \\Delta P_{to+2},..., \\Delta P_{to+B-1}]$\n$\\Delta P = P_i - P_{to}$\n$\\Delta P$ is relative pose data containing relative\ncoordinates of the joints, to is the starting frame\nof the sequence, and \u1e9e is the input window size.\nThe transformer's design exclusively employs\ninter-token self-attention, ignoring any intra-token\nattention mechanisms Vaswani et al. (2017). Con\nsequently, the way attention is applied depends\nsignificantly on how the input data is tokenized.\nIn order to utilize the full potential of the\ntransformer self-attention module we introduce\na Spatio-Temporal Pose and Relative Pose Tok\nenization (ST-PRP). After conducting extensive\nexperiments with various tokenization strategies,\nas detailed in Section 6, we identified ST-PRP as\nthe best-performing approach. ST-PRP tokeniza\ntion, as depicted in Figure 3, employs \u1e9e tokens,"}, {"title": "3.2 UETD Transformer Core", "content": "At the heart of PoseWatch is the UETD module,\nwhich processes tokens generated by the ST-PRP\nthrough a dual-branch structure. This core com\nponent is trained in a self-supervised manner,"}, {"title": "3.2.1 CTD Branch (PoseWatch-C):", "content": "The CTD branch works on the basis that a net\nwork trained on the normal data samples in the\ntraining set will learn how to encode normal pose\nsequences to the latent space and generate cur\nrent sequence with a relatively low Mean Squared\nError (MSE) loss. However, if this model is given\nabnormal pose sequences since it has not seen such\ndatapoints during the training, its generative abil\nity is compromised leading to a relatively larger\nMSE loss indicating abnormal behavior.\nThe ST-PRP Tokenization output serves as\nthe input for PoseWatch-C, being treated as a\nsequential data stream. The PoseWatch CTD\nbranch has an encoder-decoder structure. We\nchose our design to use a non-autoregressive strat\nagy to take advantage of the parallelization of\nthe transformer's structure. In the CTD branch,\nwe chose the target sequence of the decoder to\nbe equal to the current sequence TC. Both the\nencoder and the decoder are chosen to have 12\nheads. Considering the real-time nature of the\nanomaly detection task, we choose a minimal\n4 number of layers with the feed-forward layer\ndimensions set to 64. Unlike most NLP tasks,\nwe do not need to define a start token and end\ntoken for the sequences since the input and out\nput sequences have fixed lengths. PoseWatch does\nnot use masking strategies for both the input and\ntarget sequences since these sequences are always\navailable even in the inference time. The output\nof the CTD branch for the input $TC_{to}$ is:\n$TC_{to} = [Token_1, Token_2,\u2026\u2026\u2026, Token_{B-1}]$\nwhere $Token_{in}$ is the nth generated token of\nthe ith person in the to sequence $TC_{to}$.\nFinally, the MSE loss between the generated\nsequence $TC'_{toi}$ and the input sequence $TC_{to}$ is\nused both as the training loss and calculating the\nCTD Score ($CS_{to+B/2}$) in the inference time."}, {"title": "3.2.2 FTD Branch (PoseWatch-F):", "content": "As illustrated in Figure 2, the PoseWatch FTD\nand CTD branches utilize a shared encoder."}, {"title": "3.2.3 Pose Watch Hybrid\n(PoseWatch-H):", "content": "In order to be able to capture all anomalous\npatterns detected by both the CTD and the\nFTD branches, we combine the scores from these\nbranches to calculate the Hybrid Score or HS of\nthe ith person. We use a weighted sum strategy\ndescribed in Equation (6). Before combining the\nscores, we normalize them to ensure they are in\nthe same range.\n$HS_{to+}=\n_i^\\u03b2\n0.5. Norm(CS_{to+\n_i}^\\u03b2\n2\n)+\n0.5. Norm(FS_{to+\n_i}^\\u03b2\n2\n)$\nIn the last step, we find the maximum anomaly\nscore across all people available in the scene to\nfind one score for each frame:\n$HS_{to+\\frac{\\beta}{2}} = max_{i \\in N}(HS_{ito+\\frac{\\beta}{2}})$\nwhere N is the set of available people in the frame.\nIn addressing the real-time demands of\nanomaly detection, our commitment to mini\nmal model complexity is evident, exemplified by\nonly choosing 4 layers for both the encoder and\ntwin decoders. Additionally, diverging from vision\ntransformers employed in various computer vision\ntasks, our strategy involves tokenized poses with\nreduced dimensions, resulting in PoseWatch-H\nonly having 0.5 million parameters and an average\nend-to-end latency of 5.96 ms."}, {"title": "4 Experimental Setup", "content": "ShanghaiTech Campus (SHT) (Liu et al.,\n2018) dataset is the primary benchmark for\nhuman-centric video anomaly detection, offering\nover 317,000 frames from 13 scenes. It includes\n274,515 normal training frames and 42,883 test\nframes with both normal and anomalous events in\nits unsupervised split. The dataset features unique\nanomalies as well as various lighting conditions\nand camera angles, with 130 abnormal events. In\nline with previous SotA approaches (Markovitz\net al., 2020; Hirschorn and Avidan, 2023; Yu\net al., 2023), AlphaPose (Li et al., 2019) is uti\nlized for pose extraction and tracking to ensure\nfair comparison.\nHR-ShanghaiTech (HR-SHT) (Morais\net al., 2019) represents a human-related adapta\ntion of the SHT dataset. Notably, the only distinc\ntion lies in its exclusive focus on human-centric\nanomalies.\nCharlotte Anomaly Dataset (CHAD)\n(Danesh Pazho et al., 2023) is a new large\nscale high-resolution multi-camera VAD dataset\nwith about 1.15 million frames, including 1.09 mil\nlion normal and 59,172 anomalous frames. Unique\nfor its detailed annotations, including bounding\nboxes and poses for each subject, CHAD offers a\nmore challenging environment compared to SHT.\nThe experiments are conducted on the unsuper\nvised split. CHAD is selected since it sets a unified\nbenchmark for pose-based anomaly detection by\nproviding extracted poses to eliminate the vari\nations in the final pose-based anomaly detection\naccuracy."}, {"title": "4.1 Metrics", "content": "AUC-ROC or the Area Under the Receiver\nOperating Characteristic Curve is used to evaluate\nthe discriminative power of models for binary clas\nsification. It plots the True Positive Rate (TPR)\nagainst the False Positive Rate (FPR) at various\nthresholds. A higher AUC-ROC value indicates\nbetter model performance in class separation.\nEER or the Equal Error Rate represents the\npoint at which the False Positive Rate (FPR)\nand False Negative Rate (FNR) are equal. Owing\nto the valuable insights that EER provides for\nanomaly detection, several works such as (Li et al.,\n2023, 2022b) report it, but it is not used as\nwidely as AUC-ROC. EER finds a balancing point\nbetween both error rates, indicating an optimal\ntrade-off between security (inferred from FNR)\nand usability (inferred from FPR). Notably, EER\nis not influenced by imbalanced data, which is cru\ncial for anomaly detection problem. On its own,\nthis metric is not informative enough to evaluate\na model (Sultani et al., 2018), but in conjunction\nwith AUC-ROC, it provides additional valuable\ninsights (Li et al., 2013)."}, {"title": "4.2 Training Strategy and\nHyper-partameters", "content": "For all the training instances, we employed Adam\nOptimizer, and the training batch size was set to\n256 and 512 for FTD and CTD branches respec\ntively. As for all the training instances dropout\nrate and weight decay have been set to 0.1 and\n5e 5 respectively. The training procedures were\nconducted on a workstation equipped with three\nNVIDIA RTX A6000 graphic cards and an AMD\nEPYC 7513 32-core processor. A conventional grid\nhyper-parameter search was systematically uti\nlized to find the optimal set of hyper-parameters.\nSHT (Liu et al., 2018) has been recorded at 24\nFPS. Thus, we consider the input sequence length\nto be 24, equivalent to 1s. PoseWatch-C under\nwent training for 20 epochs with a learning rate of\nle-5. In the next step, we freeze the parameters of\nthe trained pose encoder and train the PoseWatch\nF decoder for 30 epochs with a learning rate of\n2e-3.\nHR-SHT (Morais et al., 2019) contains the\nsame videos in the training set as SHT. Thus, we\ndo not have a separate training for it. We use the\nmodel trained on SHT and validate it on the HR\nSHT subset as well.\nCHAD (Danesh Pazho et al., 2023) is\nrecorded at 30 FPS. Thus, we chose the input\nsequence length to be 1s or 30 frames. PoseWatch\nC is trained for 30 epochs with a learning rate of\n2e-3. In the next step, We freeze the parameters\nof the pose encoder and train the PoseWatch-F\ndecoder for 30 epochs with a learning rate of 5e-4.\nThe training approach is entirely self\nsupervised, identifying the best model through the\nminimization of MSE loss on the training data.\nThis model is then subjected to a single evalua\ntion on the test set to assess its anomaly detection\nefficacy."}, {"title": "5 Results", "content": "Table 1 presents the performance of PoseWatch\nand its variants on three distinct datasets: SHT\n(Liu et al., 2018), HR-SHT (Morais et al., 2019),\nand CHAD (Danesh Pazho et al., 2023). Unfortu\nnately, most works do not provide their code base\nfor further examinations on the newer dataset."}, {"title": "5.1 Comparison With Pose-based\nApproaches", "content": "Based on Table 1 and Figure 4 PoseWatch\nH establishes itself as the most accurate model\nfor VAD with an average AUC-ROC that sur\npasses the previous state-of-the-art by 2.04%, as\ndemonstrated across three benchmark datasets.\nSpecifically, on the CHAD dataset, PoseWatch-H\noutperforms the prior best model by 2.14%, high\nlighting its robustness in complex environments.\nOn the SHT and HR-SHT datasets, PoseWatch\nH secures the second-highest AUC-ROC scores of\n85.75% and 87.23%, respectively, trailing STG-NF\nby only 0.15% and 0.17%. However, STG-NF's\napproach, which relies on probabilistic modeling\nwith a fixed normal distribution, limits its gen\neralizability, particularly on the diverse CHAD\ndataset, where it underperforms by 6.44%. This"}, {"title": "5.2 Comparison With Pixel-based\nApproaches", "content": "This manuscript mainly focuses on pose-based\napproaches. However, we will further explore an\nadditional comparative analysis between Pose\nWatch and pixel-based methodologies to have a\nbetter understanding of Pose Watch's capabilities.\nThe datasets common between pose-based and\npixel-based approaches are SHT (Liu et al., 2018)\nand with one reported instance (Wang et al.,\n2022), HR-SHT (Morais et al., 2019). Notably,\nCHAD has not yet been employed in pixel-based\nstudies. Therefore, we further compare Pose Watch\nwith SotA pixel-based algorithms on SHT and\nHR-SHT.\nAs outlined in Section 1 and Section 2, pixel\nbased approaches have historically been regarded\nas more precise than pixel-based approaches.\nNonetheless, recent studies indicate a shift in\nthis trend. In Table 3, we present a compara\ntive analysis of PoseWatch against current SotA\npixel-based algorithms. PoseWatch-H achieves an\naverage AUC-ROC of 86.49% across the SHT and\nHR-SHT datasets, underscoring its overall superi\nority. Notably, PoseWatch-H demonstrates supe\nrior performance with AUC-ROC scores of 85.75%\nand 87.23% on the SHT and HR-SHT datasets,\nrespectively. This superior performance is note\nworthy considering that PoseWatch is a pose\nbased approach. This approach inherently exhibits\nlower bias and reduced susceptibility to back\nground noise, while simultaneously promoting\ngreater privacy and adhering to ethical standards.\nFurthermore, despite processing less information\nthan its pixel-based counterparts, PoseWatch-H\nconsistently achieves higher AUC-ROC."}, {"title": "6 Ablation Study", "content": "Table 4 Evaluating AUC-ROC of PoseWatch on SHT (Liu et al., 2018), HR-SHT (Morais et al., 2019), and CHAD\n(Danesh Pazho et al., 2023) datasets: A comparative analysis of our design variants with and without incorporating\nrelative motion. The best result of each branch is highlighted in gray."}, {"title": "6.1 Impact of Relative Pose", "content": "The results detailed in Table 4 empirically validate\nthe effectiveness of using relative pose tokeniza\ntion, as theoretically outlined in Section 3.1. This"}, {"title": "6.2 Exploring Diverse Tokenization\nStrategies", "content": "We employed diverse tokenization strategies to\noptimize the synergy between temporal and spa\ntial attention. This experiment aims to identify\nthe most effective method for the transformer core\nto interpret and analyze pose behavior more accu\nrately, optimizing its overall anomaly detection\ncapabilities. The input window of size \u1e9e and the\nnumber of keypoints k remain consistent across all\nimplemented strategies, ensuring a uniform basis\nfor comparability. For each tokenization method,\nthe best model was selected after a grid hyper\nparameter search; details can be found in the\nsupplementary materials.\nTemporal Pose and Relative Pose (T\nPRP) tokenization prioritizes the temporal\nmotion between video frames by encapsulating the\ninformation of an individual frame, encompass-\ning its pose and relative pose represented as (x, y)\nand (Ax, Ay) coordinates, within a single token\nTable 5 Evaluating AUC-ROC of PoseWatch on SHT (Liu et al., 2018), HR-SHT (Morais et al., 2019), and CHAD\n(Danesh Pazho et al., 2023) datasets: A comparative analysis of our design with different methods of tokenization. The\nbest result of each branch is highlighted in gray."}, {"title": "7 Conclusion", "content": "In this paper, we introduced methodologies that\npave the way for advanced human-centric Video\nAnomaly Detection (VAD). The novel proposed\nSpatio-Temporal Pose and Relative Pose (ST\nPRP) tokenization method, which serves as a key\ncomponent for high-level human behavior anal\nysis. Combined with our new Unified Encoder\nTwin Decoders (UETD) transformer core, the\nproposed Pose Watch architecture demonstrates\nsuperior performance in self-supervised human\ncentric VAD. Extensive benchmarking against\nSotA methods confirms PoseWatch's accuracy and\nrobustness. We hope that our contributions will\nserve as a foundation for future advancements in\nthe field."}, {"title": "Appendix A Ablation\nStudies Hyper-\nparameters", "content": "This section provides an in-depth exposition of\nthe architectural and training hyperparameters of\nthe ablation studies (Section 6). It is aimed at\nensuring reproducibility of the results."}, {"title": "A.1 Relative Pose Ablation Setup\nand Hyperparameters", "content": "To reveal the benefit of incorporating relative\nmovement, we use the best PoseWatch model\nwhich includes 12 heads and 4 layers with the\nfeed-forward layer size set to 64. This variant is\nTable A1 The training hyperparameters used for the\nRelative Pose Ablation Study (Section 6.1) on SHT (Liu\net al., 2018), HR-SHT (Morais et al., 2019) and CHAD\n(Danesh Pazho et al., 2023) datasets. LR and DR refer\nto the Learning Rate and Dropout Rate.\ntrained with and without relative movement data\nusing the hyperparameters shown in Table A1.\nThe optimal hyperparameters are chosen using a\nsystematic grid search. In all training instances,\nwe have used the Adam optimizer with a weight\ndecay of 5.0e 5 and trained branches for 30\nepochs. For the training, the same strategy is used\nas in Section 4 in the original manuscript; first,\nthe PoseWatch-C (the unified encoder and CTD\ndecoder) is trained. In the next step, the unified\nencoder is frozen and the FTD decoder is trained.\nSince both the SHT and HR-SHT utilize identical\nvideos in their training sets, the hyperparameters\nfor both models remain consistent."}, {"title": "A.2 Tokeniation Ablation Setup\nand Hyperparamters", "content": "To effectively compare various tokenization\nsetups, we carried out a systematic grid search.\nThis approach was not just to identify the opti\nmal training hyperparameters, but also to deter\nmine the best architectural choices. This thorough\nprocess ensures that each tokenization setup is\nevaluated at its highest potential, leading to more\naccurate and reliable comparison results.\nTable A2 presents the architectural design\nparameter choices obtained from the grid search\non SHT dataset (Liu et al., 2018). These param\neters are kept the same for other datasets to"}]}