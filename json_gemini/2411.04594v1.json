{"title": "Verification of Neural Networks against Convolutional Perturbations via Parameterised Kernels", "authors": ["Benedikt Br\u00fcckner", "Alessio Lomuscio"], "abstract": "We develop a method for the efficient verification of neural networks against convolutional perturbations such as blurring or sharpening. To define input perturbations we use well-known camera shake, box blur and sharpen kernels. We demonstrate that these kernels can be linearly parameterised in a way that allows for a variation of the perturbation strength while preserving desired kernel properties. To facilitate their use in neural network verification, we develop an efficient way of convolving a given input with these parameterised kernels. The result of this convolution can be used to encode the perturbation in a verification setting by prepending a linear layer to a given network. This leads to tight bounds and a high effectiveness in the resulting verification step. We add further precision by employing input splitting as a branch and bound strategy. We demonstrate that we are able to verify robustness on a number of standard benchmarks where the baseline is unable to provide any safety certificates. To the best of our knowledge, this is the first solution for verifying robustness against specific convolutional perturbations such as camera shake.", "sections": [{"title": "1 Introduction", "content": "As neural networks are increasingly deployed in a range of safety-critical domains such as autonomous vehicles, aviation, or robotics, concerns about their reliability are rising. Networks have been shown to be vulnerable to Adversarial Attacks, perturbations that are often imperceptible but change the output of the network on a given instance (Szegedy et al. 2014; Madry et al. 2017). Such adversarial examples have been shown to also exist in the physical world and pose a threat to algorithms deployed in practical applications (Eykholt et al. 2018; Tu et al. 2020).\nNeural Network Verification has been put forward as a way to address these issues by formally proving that for a given input, a network is robust to a set of specified perturbations, often referred to as local robustness (Katz et al. 2017; Gehr et al. 2018; Singh et al. 2018a).\nAlgorithms are usually divided into complete and incomplete approaches. Given enough time, complete methods are guaranteed to provide a definitive answer to the verification problem. Meanwhile, incomplete methods may not be able to answer the verification problem, returning an undecided result. Complete approaches often employ an exact encoding of the network at hand. They rely on techniques such as Mixed Integer Linear Programming (MILP) (Tjeng, Xiao, and Tedrake 2019; Anderson et al. 2020; Bunel et al. 2020) or Satisfiability Modulo Theories (SMT) (Pulina and Tacchella 2012; Katz et al. 2017). Incomplete verifiers on the other hand employ Semidefinite Programming (Raghunathan, Steinhardt, and Liang 2018; Dathathri et al. 2020; Fazlyab, Morari, and Pappas 2020) or bound propagation (Wang et al. 2018a,b; Singh et al. 2019b; Xu et al. 2021; Wang et al. 2021). They usually overapproximate the true behaviour of the neural network and can be made complete by combining them with a Branch and Bound (BaB) strategy. Stronger verifiers either employ tighter relaxations such as SDP-based ones or linear constraints that reason over multiple neurons simultaneously (Singh et al. 2019a; M\u00fcller et al. 2022; Ferrari et al. 2022; Zhang et al. 2022) State-of-the-art (SoA) verifiers achieve low runtimes through exploiting GPU-enabled parallelism (Brix et al. 2023b,a).\nEarly works usually verified robustness against norm-based perturbations, often referred to as white noise, which covers a limited number of scenarios and rarely appears in the real world (Pulina and Tacchella 2010, 2012; Singh et al. 2018a; Katz et al. 2019). A number of other perturbations were later proposed. Photometric perturbations such as brightness, contrast, hue or saturation changes as well as more expressive bias field perturbations can be encoded by prepending suitable layers to a neural network (Kouvaros and Lomuscio 2018; Henriksen et al. 2021; Mohapatra et al. 2020). Verifiers can equally be extended to handle more complex geometric perturbations such as rotations, translations, shearing or scaling, although the efficient verification against such perturbations requires further modifications and extensions (Singh et al. 2019b; Balunovic et al. 2019; Mohapatra et al. 2020; Kouvaros and Lomuscio 2018). Other works focus on the efficient verification of robustness to occlusions (Mohapatra et al. 2020; Guo et al. 2023) or semantically rich perturbations in the latent space of generative models (Mirman et al. 2021; Hanspal and Lomuscio 2023).\nMore relevant to this work are previous investigations of camera shake effects. Guo et al. (2020) examine the performance of networks in the presence of motion blur and find that it is highly problematic, significantly degrading the performance of the models. The phenomenon can be modeled by applying a convolution operation using suitable kernels to a given input image (Sun et al. 2015; Mei et al. 2019). Using different kernels, convolution operations can similarly be used to implement other image transformations which include box blur (Shapiro and Stockman 2001, pp.153\u2013154) and sharpen (Arvo 1991, pp.50\u201356). Since many semantically interesting and realistic perturbations can be modelled using convolution, being able to verify robustness to perturbations in a kernel space is highly valuable. Some attempts at verifying the robustness of models to such perturbations have been made before. Paterson et al. (2021) encode contrast, haze and blur perturbations but only perform verification for haze while resorting to empirical testing for contrast and blur. One previous work presents a general method which, if successful, certifies robustness of a network to all possible perturbations represented by a kernel of the given size (Mziou-Sallami and Adjed 2022). However, this generality comes at a cost. It leads to loose bounds and a high dimensionality of the perturbation which makes verification difficult, even more so for large networks. The universality also implies that counterexamples which are misclassified by the network may be difficult to interpret.\nWe propose a new method which aims at the tight verification of networks against convolutional perturbations with a semantic meaning. Our key contributions are the following:\n\u2022 To enable an efficient symbolic encoding of the perturbations, we show how an arbitrary constant input can efficiently be convolved with a linearly parameterised kernel using standard convolution operations from a machine learning library.\n\u2022 We present parameterised kernels for motion blur perturbations with various blurring angles as well as box blur and sharpen.\n\u2022 Using standard benchmarks from past editions of the Verification of Neural Networks Competition (Brix et al. 2023b,a) as well as self-trained models, we show experimentally that verification is significantly easier with our method due to the tighter bounds and the low dimensionality of the perturbation. Our ablation study demonstrates that the existing method is unable to verify any properties on the networks we use. At the same time, our method certifies a majority of the properties for small kernel sizes and perturbation strengths while still being able to certify robustness in a number of cases for large kernel sizes and strengths."}, {"title": "2 Background", "content": "The notation used for the remainder of the paper is as follows: we use bold lower case letters a to denote vectors with a[i] representing the i-th element of a vector, bold upper case letters A to denote matrices with A[i, j] to denote the element in the i-th row and j-th column of a matrix and $||\\mathbf{a}||_{\\infty} = \\sup_i |a[i]|$ for the $l_{\\infty}$ norm of a vector. Since we focus on neural networks for image processing, we assume that the input of a given network will be an image and therefore refer to single entries in an input matrix as pixels."}, {"title": "2.1 Feed-forward Neural Networks", "content": "A feed-forward neural network (FFNN) is a function $f(\\mathbf{x}) : \\mathbb{R}^{n_0} \\rightarrow \\mathbb{R}^{n_L}$ which is defined using the concatenation of $L \\in \\mathbb{N}$ layers. Each layer itself implements a function $f_i$ and it holds that $f(\\mathbf{x}) = f_L(f_{L-1}(...(f_1(\\mathbf{x}))))$. Given an input $\\mathbf{x}_0$ the output of a layer $1 \\leq i \\leq L$ is calculated in a recursive manner by applying the layer's operations to the output of the previous layer, i.e. $\\mathbf{x}_i = f_i(\\mathbf{x}_{i-1})$. The operation encoded by the $i$-th layer is $f_i: \\mathbb{R}^{n_{i-1}} \\rightarrow \\mathbb{R}^{n_i}$ where $n_i$ is the number of neurons in that layer. We assume each layer operation $f_i$ to consist of two components: firstly, the application of a linear map $\\mathbf{a}_i: \\mathbb{R}^{n_{i-1}} \\rightarrow \\mathbb{R}^{n_i}$, $\\mathbf{x}_{i-1} \\mapsto \\mathbf{\\hat{x}}_i = W_i \\mathbf{x}_{i-1} + \\mathbf{b}_i$ for a weight matrix $W_i \\in \\mathbb{R}^{n_i \\times n_{i-1}}$ and a bias vector $\\mathbf{b}_i \\in \\mathbb{R}^{n_i}$ which yields the pre-activation vector $\\mathbf{\\hat{x}}_i$. And secondly the element-wise application of an activation function $\\sigma_i: \\mathbb{R}^{n_i} \\rightarrow \\mathbb{R}^{n_i}$ yielding the post-activation vector $\\mathbf{x}_i$. In the verification literature networks are often assumed to use the piece-wise ReLU function $ReLU(x) = \\max(0, x)$, but verification is equally possible for other activations like sigmoid or tanh functions (Shi et al. 2024). The last layer of a given network normally does not include an activation function, $\\sigma_L$ would therefore be the identity map. In this work we focus on networks performing image classification where the input $\\mathbf{x}_0$ is an image that needs to be categorised as belonging to one out of $c$ classes. The final layer outputs $n_L = c$ classification scores and the predicted class for an image is $j = \\arg\\max_i X_L[i]$ ."}, {"title": "2.2 Neural Network Verification", "content": "Given a trained network $f$, the verification problem consists of formally proving that the output of the network will always be contained in a linearly definable set $\\mathcal{O} \\subset \\mathbb{R}^{n_L}$ for all inputs in a linearly definable input set $\\mathcal{I} \\subset \\mathbb{R}^{n_0}$. Formally we aim to show that\n$\\forall \\mathbf{x}_0 \\in \\mathcal{I}: f(\\mathbf{x}_0) \\in \\mathcal{O}$\nInspired by adversarial attack paradigms, most works study the local robustness of networks to white noise contrained by the $l_{\\infty}$ norm (Bastani et al. 2016; Singh et al. 2018b). Given an input $\\mathbf{x} \\in \\mathbb{R}^{n_0}$ which the network correctly classifies as belonging to class $j'$, they do so by defining the input and output sets as\n$\\mathcal{I} = {\\mathbf{x} \\in \\mathbb{R}^{n_0} | ||\\mathbf{x} - \\mathbf{\\hat{x}}||_{\\infty} \\leq \\epsilon}$\n$\\mathcal{O} = {\\mathbf{y} \\in \\mathbb{R}^{n_L} | y[j'] > y[j] \\forall j \\neq j'}$\nwhere $\\epsilon$ is the perturbation size for which the verification query should be solved.\nSoA verifiers often employ bound propagation of some kind (Liu et al. 2019; Meng et al. 2022). If the bounds obtained at the final layer are tight enough, they can be used to answer the verification problem. The key issue for most algorithms is the nonlinearity of the activation functions in the networks. Convex relaxations of the functions are usually employed, but they induce an overapproximation error which can become significant for larger networks (Liu et al. 2019). Most methods employ a BaB mechanism which allows for a refinement of the network encoding if the problem cannot be solved with the initial encoding due to the relaxations being too coarse (Palma et al. 2021; Ferrari et al. 2022; Shi et al. 2024). One branching strategy is input splitting which partitions the input space into subspaces and has been found to be particularly effective for networks with low input dimensions (Wang et al. 2018b; Botoeva et al. 2020). On the other hand, neuron splitting is used for networks with high-dimensional perturbations where input splitting is less effective (Botoeva et al. 2020). This strategy splits the input space of a single neuron in the network into subspaces to allow for a more precise encoding of the activation functions. In the simple case of piece-wise ReLU activation functions, this can be done by splitting the function into its two linear pieces (Ferrari et al. 2022). Verification for high-dimensional perturbations such as norm-based ones is normally more challenging than verification for low-dimensional properties like brightness or contrast (Wang et al. 2018a). This is due to the dimensionality of the perturbations and the BaB strategies mentioned before. Possible return values of verifiers include safe (the network is robust under the given perturbation), undecided (the verifier could neither verify nor falsify the query, for example due to overly coarse relaxations) or unsafe (a concrete counterexample for which the network returns an incorrect result was found in the space of allowed perturbations)."}, {"title": "2.3 Convolution", "content": "Convolution is a mathematical operation which is frequently used for processing inputs in signal processing. Most relevant for us is the discrete convolution operation on two-dimensional inputs, in our case images. For a given input matrix, it computes each element in the output matrix by multiplying the corresponding input value and its neighbours with different weights and then summing over the results. Given a two-dimensional input matrix $I$ and a kernel matrix $K$ we define the convolution of $I$ with $K$, often written as $I * K$, as follows (Goodfellow, Bengio, and Courville 2016, pp.331-334):\n$I * K[i, j] = \\sum_{k,l} I[i + k, j + l]K[k,l]$\nHere $(i, j)$ is a tuple of valid indices for the result, the output shape of $I * K$ can be computed based on a number of parameters (Dumoulin and Visin 2018).\nSignal processing works often assume that the kernel is flipped for convolution, but in line with common machine learning frameworks we omit this and refer to the above operation as convolution. The elements of $K$ are usually normalised to sum to 1, for example if $I$ is an image, since this preserves the brightness of the image. The output of a convolution is usually of a smaller size than the input, if an output of identical size is required, padding can be added to the image before the convolution. In early image processing algorithms linear filtering methods such as convolutions were frequently used for purposes such as edge detection or denoising with kernels being carefully designed by experts (Szeliski 2022, pp.100-101). Convolutional neural networks learn these kernels from data in order to perform a variety of tasks such as classification or object detection. When using suitable kernels, convolution can be used to apply effects such as sharpening to an image [Mei et al. 2019, Szeliski 2022, p.101]. We focus on the verification against camera shake/motion blur while also considering box blur and sharpen to demonstrate the generalisation of our method to other perturbations."}, {"title": "2.4 Convolutional Perturbations", "content": "Mziou-Sallami and Adjed (2022) propose an algorithm for verifying the robustness of a network to convolutional perturbations. They define the neighbourhood of a pixel in the input space for a given kernel size $k$, input image $I \\in \\mathbb{R}^{d_1 \\times d_2}$ and a pixel location tuple $(i, j)$ as the set of all pixels inside a box of size $k \\times k$ centered at the position $(i, j)$. Fields of the box that lie outside the bounds of the input image are disregarded. A lower bound $l$ and upper bound $u$ for each pixel is calculated as the minimum and maximum element in that neighbourhood, respectively. These bounds are tight in the sense that they are attainable: the lower bound for a pixel can be realised through a kernel which is 1 at the location of the minimum in the pixel's neighbourhood and zero elsewhere. A similar construction is possible for the maximum. If this operation is repeated for each pixel in the input image, one obtains two matrices $L \\in \\mathbb{R}^{d_1 \\times d_2}$ and $U \\in \\mathbb{R}^{d_1 \\times d_2}$ with lower and upper bounds for each pixel. A standard verifier can be used to certify robustness for a network on the given input by treating the perturbation as an $l_{\\infty}$, assuming that each pixel can vary independently between its lower and upper bound.\nIf verification is successful, the network is certified to be robust to any convolutional perturbation for which the values of the kernel lie in the $[0, 1]$ interval. However, the counterexamples found by the method are often difficult to interpret and the bounds for the inputs can be extremely loose. The assumption that variations of pixels are not coupled also means that the perturbation is extremely high dimensional, leading to long runtimes and even looser bounds in layers deeper in the network. The combination of these factors means that even for the smallest kernel size of 3, Mziou-Sallami and Adjed (2022) obtain a verified accuracy of 30% in the best case and 0% in the worst case for small classifiers trained on the MNIST and CIFAR10 datasets. The approach is therefore unlikely to scale to larger networks."}, {"title": "3 Method", "content": "Phenomena such as motion blur cannot be modeled using standard techniques such as $l_{\\infty}$ perturbations since the computation of each output pixel's value is based not only on its original value, but also on the values of its neighbouring pixels. We parameterise specific kernels to model perturbations using convolution, allowing for the certification of robustness to specific types of convolutional perturbations while yielding tighter bounds. A major advantage of our approach is its simplicity. It can be implemented using standard operations from a machine learning library used to calculate the parameters of a linear layer prepended to the network to be verified. No special algorithms as in the case of geometric perturbations are required to perform efficient verification (Balunovic et al. 2019)."}, {"title": "3.1 Convolution with Parameterised Kernels", "content": "In our work we use linearly parameterised kernels in the convolution operation. We refer to a kernel as linearly parameterised if each entry in the kernel matrix is an affine expression depending on a number of $m$ variables. When convolving a constant input with such a kernel, the result is again a linear expression because of the linearity of the convolution operation.\nTheorem 1. Assume we are given an input image $I$ and a parameterised kernel $K$ defined as\n$K = \\sum_{i=1}^{m} A_i z_i + B$\nwhere $z_i \\in \\mathbb{R}$. $A_i$ and $B$ are a number of coefficient matrices and a bias matrix, respectively, which have the same shape as $K$. Then it holds that\n$I * K = \\sum_{i=1}^{m} (I * A_i) z_i + I * B$   (1)\nProof: See Appendix B.1\nThis theorem allows us to compute the result of a convolution with a parameterised kernel by separately convolving the input with each coefficient matrix and the bias matrix (Equation 1). The $z_i$ variables can be ignored during these computations which means that the convolution operations need to only be executed on two constant matrices. Standard convolution implementations from a machine learning library can thus be used."}, {"title": "3.2 Parameterised Kernels for Modelling Camera Shake", "content": "To enable the verification of a network to a range of perturbation strengths, we propose modeling a linear transition from the identity kernel to the desired perturbation kernel $P$ using a single variable $z \\in [0, 1]$. We assume that we are given two initial conditions for the kernel it should be equal to the identity kernel for $z = 0$ and equal to the desired perturbation kernel such as those in Figure 1 for $z = 1$. An affine function is then unambiguously defined by these two points that it passes through and we can compute the slope and intercept for each entry in the kernel. Example 1 shows this derivation for the 3 \u00d7 3 motion blur kernel with a blur angle of $\\phi = 45\u00b0$ introduced in Section 2.3.\nExample 1. The initial conditions for our parameterisation are:\n$P_{z=0} = \\begin{pmatrix} 1 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 0\\end{pmatrix}$, $P_{z=1} = \\begin{pmatrix} 0 & 0 & \\frac{1}{3}\\\\ 0 & \\frac{1}{3} & 0\\\\ \\frac{1}{3} & 0 & 0\\end{pmatrix}$\nWe assume each kernel entry is of the form $p(z) = az + b$ where $z \\in \\mathbb{R}$ is a variable and $a, b \\in \\mathbb{R}$ are parameters. Since we have two unknowns $a,b$ and two points that the function passes through from the initial conditions, we can solve for a, b to obtain our parameters for each kernel entry. In the camera shake case for $\\phi = 45\u00b0$ we have three types of entries: The center entry, the non-center entries on the antidiagonal running from the top right to the bottom left of the matrix, and the entries that do not lie on the antidiagonal.\nParameterisation for the Center Entry For the center entry we have $p(0) = 1$ and $p(1) = \\frac{1}{3}$. This results in the constraints\n$p(0) = 1 = a \\cdot 0 + b$   (2)\n$p(1) = \\frac{1}{3} = a \\cdot 1 + b$   (3)\nSolving for a, b yields $a = -\\frac{2}{3}, b = 1$ and therefore $p(z) = -\\frac{2}{3}z + 1$.\nParameterisation for the Antidiagonal Entries For non-center entries on the antidiagonal the initial conditions are\n$p(0) = 0 = a \\cdot 0 + b$   (4)\n$p(1) = \\frac{1}{3} = a \\cdot 1 + b$   (5)"}, {"title": "3.3 Integration of Convolutional Perturbations into Neural Network Verifiers", "content": "Given the parameterised kernels from Section 3.2, we can use Theorem 1 to devise a method for easily verifying the robustness of neural networks to the perturbations the kernels encode. Since the parameterised kernels we introduce only depend on a single variable we omit the indexing of the variable z and the coefficient matrix A. We borrow the popular idea of using additional layers that encode a perturbation which are then prepended to a network for verification (Mohapatra et al. 2020; Kouvaros and Lomuscio 2018; Guo et al. 2023). Assume a trained neural network $f$ is given together with a correctly classified input image in vectorised form $\\mathbf{x} \\in \\mathbb{R}^{O_c O_w O_h}$ where $O_c, O_w, O_h$ are the image's number of channels, width and height, respectively. We first reshape the vector $\\mathbf{x}$ into the original shape of the image $(O_c, O_w, O_h)$ to obtain an input tensor $I$. This step is necessary for the convolution operation to be applicable. We then separately convolve $I$ with $A$ and $B$ to obtain $R_A := (I * A)$ and $R_B := (I * B)$. For inputs with multiple channels each channel is convolved independently with the same kernels $A, B$ so the output of the convolution has the same number of channels as the input.\nTo encode the perturbation in a network layer, we reshape the resulting matrices of these convolutions to be vectors $\\mathbf{r}_A, \\mathbf{r}_B \\in \\mathbb{R}^{O_c O_w O_h}$ again. We then prepend a matrix multiplication layer to the network which computes $\\Lambda \\cdot z + B$ where $\\Lambda = \\text{reshape}(R_A) = \\mathbf{r}_A, B = \\mathbf{r}_B$ are parameters for the layer that are set for each verification query and $z \\in \\mathbb{R}$ is the input to the network controlling the strength of the perturbation. Similarly to the existing approaches which encode properties to verify in a layer prepended to the network, the image information is now encoded in the parameters of this new layer. Despite the fact that all parameterisations used in this work only depend on one variable, the method does generalise to the case where the number of coefficient matrices in the parameterisation $m$ is greater than one. In those cases one performs $m + 1$ separate convolutions and obtains $m$ matrices $R_{A_1},..., R_{A_M}$ and one matrix $R_B$ which are reshaped into vectors $\\mathbf{r}_{A_1},...,\\mathbf{r}_{A_m},B$. Assuming $\\mathbf{r}_{A_i} \\in \\mathbb{R}^{O_c O_w O_h}$ are column vectors, they can be concatenated horizontally to form a parameter matrix $\\mathbb{A} \\in \\mathbb{R}^{O_c O_w O_h, m}$. The input to the network in this case would be a vector $\\mathbf{z} \\in \\mathbb{R}^{m}$ for parameterising the perturbation. The prepended layer then computes the matrix-vector product $\\mathbb{A} \\cdot \\mathbf{z}$, adds the bias $B$ to it and feeds the resulting vector of size $O_c O_w O_h$ into the first layer of the original network. The robustness of the resulting network can be checked using standard neural network verifiers. Since the input to the augmented network is low-dimensional, in our case even just one-dimensional, input splitting as a branching strategy is particularly effective for verification (Botoeva et al. 2020)."}, {"title": "4 Evaluation", "content": "To evaluate the method described in the previous section, we extend Venus (Kouvaros and Lomuscio 2021), a robustness verification toolkit that uses both Mixed-Integer Linear Programming and Symbolic Interval Propagation to solve verification problems. The toolkit is extended to process modified vnnlib files as used in the most recent Verification of Neural Networks Competition (VNNCOMP23) with those files encoding the convolutional perturbations (Brix et al. 2023a). Venus uses PyTorch (Paszke et al. 2019) for efficient vectorised computations, we enable its SIP solver and its adversarial attacks and otherwise use the default settings. Our method is implemented in PyTorch and reads a vnnlib file, builds the appropriate parameterised kernels as described in Section 3.2 and convolves the input with them using Py-Torch's Conv2d operation. An additional layer encoding the perturbation is prepended to the network as described in Section 3.3, and verification with input splitting is run on the augmented network using Venus. The experiments are conducted on a server running Fedora 35 which is equipped with an AMD EPYC 7453 28-Core Processor and 512GB of RAM.\nThe performance of robustness verification for the proposed perturbations is evaluated on three benchmarks from previous editions of the Verification of Neural Networks Competition (M\u00fcller et al. 2022). mnist_fc is a classification benchmark which consists of three different networks with 2, 4 and 6 layers with 256 ReLU nodes each that is trained on the MNIST dataset. Oval21 contains three convolutional networks trained on the CIFAR10 dataset. Two of them consist of two convolutional layers followed by two fully-connected layers while the third one has two additional convolutional layers, the number of network activations ranges from 3172 to 6756. Sri_resnet_a is a ReLU-based ResNet with one convolutional layer, three ResBlocks and two fully-connected layers trained using adversarial training on the CIFAR10 dataset. Our resnet18 benchmark is created using a self-trained ResNet18 model from the TorchVision package in version 0.16.0 processing CIFAR10 inputs which has 11.7m parameters. For each VNNCOMP benchmark we take the properties from the official VNNCOMP repository and change the perturbation type to one of ours before running the experiments. The only other modification we make is that we set the timeout for each query to 1800 seconds. For resnet18 we select 50 correctly classified instances from the CIFAR10 test set for verification.\nFor each of the benchmarks we test perturbations with kernel sizes of 3, 5, 7 and 9. We vary the upper bound of the perturbation strength which we simply denote as strength in the following, the lower bound for the strength is zero. A perturbation strength of 0.4 means, for example, that for the parameterised kernel the variable z is allowed to vary within the interval [0, 0.4] The results for mnist_fc are presented in Table 1, those for resnet18 in Table 2. The ones for oval21 can be found in Table 4 and those for sri resnet_a in Table 5 which we move to Appendix A due to space constraints.\nWe find that our method scales well to very large networks such as ResNet18 and can provide safety certificates for models for which noise-based certification fails. Verification on all benchmarks is fast due to the low dimensionality of our perturbations. Verification for small perturbation strengths is successful for nearly all instances, irrespective of the kernel size s. For small kernel sizes such as s = 3 we further observe that verification is successful even for very large strengths. For large kernel sizes and large perturbation strengths unsafe cases are more likely to be found which is not surprising given that the degree of corruption for e.g. box blur with a kernel size of 9 and a perturbation strength of 1 is substantial. The differences in robustness to different types of perturbations are also noteworthy. While robustness deteriorates severely for box blur and camera shake when larger kernel sizes or perturbation strengths are considered, networks retain a high verified robustness against sharpen perturbations. This intuitively makes sense. While blurring often induces an information loss which can make it hard to restore the original information of the image, sharpening emphasises the image texture and can strengthen edges in the image. This robustness to large sharpen perturbation strengths can be observed for both MNIST and CIFAR10.\nFor mnist_fc we also observe that the robustness of the network to camera shake perturbations is highly dependent on the perturbation angle, especially for the largest kernel size of 9. While the networks are highly vulnerable to camera shake along the 45\u00b0 and 135\u00b0 axis, they are much more robust to blurring along the 0\u00b0 axis. Motion blurring along the 90\u00b0 axis affects the networks to the least degree with verified accuracies still being extremely high for strong perturbations, even for a kernel size of 9 and a perturbation strength of 1.0. The differences in robustness to different camera shake angles are also observable for resnet18, oval21 and sri_resnet_a, even though they are less prominent.\nTo compare our approach to the existing work on convolutional perturbations, we also reimplement the method presented by (Mziou-Sallami and Adjed 2022) and refer to it as baseline. The verification queries are once again solved by Venus. Since the perturbations are now high-dimensional, we use activation splitting instead of input splitting as the branching strategy. The results are shown in Table 3. As expected, the baseline perturbations lead to loose bounds due to the high dimensionality of the perturbations combined with the already loose bounds for each pixel's value for large neighbourhoods. Even for a small kernel size of 3 we find that no properties can be verified for the networks we consider, indicating that the baseline method does not scale to networks or input sizes larger than those presented in the original paper. Its key advantage is that if verification is successful for a specific input and kernel size, robustness is certified for any kernel of the respective size while our method needs to run a query for each type of parameterised kernel. However, it is obvious that the baseline's method of modeling the perturbations is too loose to verify any of the properties.\nIn summary, we show that our method enables the verification of networks against a range of perturbations that can be modeled through parameterised kernels. We demonstrate that it provides tight bounds, permitting the verification of much larger networks, and that it allows for an efficient verification due the low dimensionality of the perturbations."}, {"title": "5 Conclusion", "content": "Verification against camera shake and related convolutional perturbations is important since these phenomena are likely to appear in the real world, for example in vision systems of autonomous vehicles or planes. So far, verification against such perturbations was only possible using an algorithm that yielded counterexamples which are difficult to interpret and produced loose bounds, preventing the robustness verification of larger networks or networks with large input sizes. Our algorithm based on parameterised kernels is easy to implement and allows for the verification of network robustness to a number of semantically interesting perturbations. We demonstrate the effectiveness of the method on a number of standard benchmarks from VNNComp and prove that it is able to verify properties for networks that are too complex for the existing baseline. We further show that our method can be used to identify weaknesses of models to specific types of convolutional perturbations that might otherwise remain hidden such as motion blurring along axes of a specific angle. Since it is easy to design parameterised kernels for additional perturbation types besides those presented, we expect that more convolutional perturbations of practical interest will be developed in the future, contributing to more thorough robustness checks for deployed AI systems."}, {"title": "B.1 Proof of Theorem 1", "content": "Assume we are given a kernel which is linearly parameterised as\n$K = \\sum_{i=1}^{m} A_i z_i + B$\nwhere $z_i \\in \\mathbb{R}$. $A_i$ and $B$ are a number of coefficient matrices and a bias matrix, respectively, which have the same shape as $K$. If we we want to compute the result of convolving $I$ with $K$, we can exploit the linearity of the convolution operation (Szeliski 2022, pp.95\u201396) to obtain that\n$I * K = \\sum_{i=1}^{m} (I * A_i) z_i + I * B$\nProof. For a kernel $K$ that is parameterised as specified above and an input $I$ it holds that\n$I*K=I*"}]}