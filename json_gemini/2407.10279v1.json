{"title": "AlphaDou: High-Performance End-to-End Doudizhu AI Integrating Bidding", "authors": ["Chang Lei", "Huan Lei"], "abstract": "Artificial intelligence for card games has long been a\npopular topic in AI research. In recent years, com-\nplex card games like Mahjong and Texas Hold'em have\nbeen solved, with corresponding AI programs reach-\ning the level of human experts. However, the game of\nDou Di Zhu presents significant challenges due to its\nvast state/action space and unique characteristics in-\nvolving reasoning about competition and cooperation,\nmaking the game extremely difficult to solve. The RL\nmodel DouZero, trained using the Deep Monte Carlo\nalgorithm framework, has shown excellent performance\nin DouDiZhu. However, there are differences between\nits simplified game environment and the actual Dou Di\nZhu environment, and its performance is still a consid-\nerable distance from that of human experts. This paper\nmodifies the Deep Monte Carlo algorithm framework\nby using reinforcement learning to obtain a neural net-\nwork that simultaneously estimates win rates and ex-\npectations. The action space is pruned using expecta-\ntions, and strategies are generated based on win rates.\nThis RL model is trained in a realistic DouDiZhu en-\nvironment and achieves a state-of-the-art level among\npublicly available models.", "sections": [{"title": "Introduction", "content": "Games can be broadly classified into two categories:\nperfect-information games (PIGs) and imperfect-\ninformation games (IIGs). In PIGs, players can observe\nall game states, such as in Shogi, Go, and Chess. In con-\ntrast, IIGs involve scenarios where participants cannot\naccess complete information about other players, such\nas in heads-up Texas Hold'em. Reinforcement learning\n(RL) has been successfully applied to create numer-\nous game Als. RL algorithms have achieved remark-\nable success in both PIGs and IIGs, exemplified by Al-\nphaGo (Silver et al. 2016) and AlphaZero (Silver et al.\n2017) in Go, AlphaStar (Vinyals et al. 2019) in Star-\nCraft II, OpenAI Five (OpenAI et al. 2019) in Dota\n2, Suphx (Li et al. 2020) in Mahjong, Douzero (Zha\net al. 2021) in Doudizhu, NukkiAI (Bouzy, Rimbaud,\nand Ventos 2020) in Contract Bridge, and AlphaHol-\ndem (Zhao et al. 2022a) in Hold'em.\nHowever, AI has not performed perfectly in cer-\ntain gambling games that require bidding. NukkiAI\nonly outperformed professional human players in non-\nbidding 1v1 Bridge, and Douzero did not consider the\nbidding phase during training. These AIs function more\nas playing machines rather than proficient gamblers.\nThe bidding phase contains rich strategic information\nthat significantly influences player strategies. When an\nopponent has a strong hand, they tend to bid high for\nhigher potential rewards, while players should adopt\na conservative strategy to minimize losses. Conversely,\nwhen opponents bid low, players can employ more ag-\ngressive strategies to increase their gains.\nThis work aims to develop a high-performance end-to-end Doudizhu AI model that incorporates bidding.\nDoudizhu, also known as Fighting the Landlord, is the\nmost popular card game in China. Doudizhu is a 3-\nplayer IIG where players bid based on their hands, and\nthe winning bidder becomes the Landlord. The remain-\ning players form the Peasants team to oppose the Land-\nlord. If any Peasant player wins, the entire team wins.\nThe Landlord wins double rewards, while each Peasant\nplayer receives a single reward if the team wins, and\nvice versa. Rewards are related to the bid score and the\noccurrence of \"bombs\" (four cards of the same rank)\nor \"rockets\" during the game. Players with good hands\ntend to bid high to become the Landlord for higher\nreturns. Moreover, Doudizhu has a large, flexible, and\ndiverse action space with thousands of possible states\n(1083) and actions (27,472) due to card combinations\nand complex rules (Zha et al. 2019). Additionally, re-\nwards in Doudizhu are sparse and highly variable, only\nawarded at the end of the game and influenced by the\nbidding phase, the number of \"bombs\" during the game,\nand \"spring\" rewards. These characteristics make train-\ning a Doudizhu AI extremely challenging, and existing\nDoudizhu Als exhibit certain issues.\nPrevious research on Doudizhu AI has primarily fo-\ncused on the playing phase, neglecting the bidding\nphase or employing completely random bid strategies.\nDeltaDou (Jiang et al. 2019) is the first AI program\nto achieve human-level performance compared to top\nhuman players, using an AlphaZero-like algorithm with\nBayesian methods to infer hidden information and sam-"}, {"title": "The Game of Doudizhu", "content": "Doudizhu is a three-player card game that is extremely\npopular in China and is considered a typical gambling\ngame. Among the three players, two are Peasants who\nneed to cooperate to compete against the third player,\nthe Landlord. The game comprises two phases: 1) Bid-\nding and 2) Cardplay."}, {"title": "Bidding", "content": "The Bidding Phase determines the roles of the players.\nAt the start of the game, each player receives seventeen\ncards from a shuffled deck in a counterclockwise man-\nner, with three cards left in the middle of the table. In\nthe Bidding Phase, a randomly selected player begins\nthe bidding process, followed by the others in sequence.\nEach player can only bid once, with options to bid 1\npoint, 2 points, 3 points, or pass. A subsequent player\nmust either choose a higher bid or pass. The first player\nto bid 3 points becomes the Landlord, or if all players\ncomplete their bids, the player with the highest bid be-\ncomes the Landlord, while the other two players become\nPeasants. The Landlord has the privilege to reveal the\nthree remaining cards for all players to see and then in-\ncorporates these cards into his/her hand. Notably, if all\nthree players choose to pass, the game results in a draw,\nand a new game starts with a fresh deal. The bid score\nimpacts the final game rewards: if the Landlord wins,\nhe/she gains points equal to twice the bid score from\nboth Peasants. Conversely, if any Peasant wins, both\nPeasants receive points equal to the bid score from the\nLandlord, who loses double the points. This scenario as-s\nsumes the absence of Bombs, Rockets, and Spring (refer\nto the following section)."}, {"title": "Cardplay", "content": "During the Cardplay phase, players take turns playing\ncards. Each game consists of multiple rounds, starting\nwith a player playing a valid card combination (e.g.,\nsolo, pair). The first round is initiated by the Land-\nlord. Subsequent players must either pass or defeat the\nprevious hand by playing a higher-ranked combination\n(an action has a rank, refer to Appendix A). The round\ncontinues until two consecutive players pass. Then, the\nplayer who played the last hand starts the next round.\nThe objective is to clear all cards from one's hand to\nwin. Each \"Bomb\" and \"Rocket\" can double the game's\nstakes. If the Landlord wins, they receive double the re-\nwards, whereas if the Peasant team wins, each Peasant\nplayer receives single rewards. Rewards are influenced\nby the bid score and the presence of Bombs or Rockets.\nBombs surpass any action. The only way to defeat a\nBomb is with a higher-ranked Bomb or a Rocket. The\nRocket is the highest action in the game and can beat\nany Bomb or action. When a Bomb or Rocket is played,\nthe points at stake double. For instance, if the winning\nbid is 3 points at the start, it becomes 6 points if a Bomb\nis played and 12 points if another Bomb is played. With\ntwo Bombs played, the Landlord stands to win/lose 24"}, {"title": "", "content": "points, and each Peasant stands to win/lose 12 points.\nThe game concludes when a player clears all their cards.\nTo encourage more aggressive play, Doudizhu includes\na \"Spring\" reward: if throughout the game, the Peasant\nteam makes no plays other than passing, or the Land-\nlord only passes once, it is termed as Spring or Anti-\nSpring, respectively, doubling the reward (equivalent to\nplaying an additional Bomb).\nFor more information, readers may also refer to the\nWikipedia page on Doudizhu."}, {"title": "AlphaDou", "content": "The goal of AlphaDou is to incorporate the Bid Phase\nin the training and testing stages of the Doudizhu AI,\nenabling the AI to fully engage in a complete gambling\ngame. \"End-to-end\" here means that this framework di-\nrectly accepts game state information and outputs ac-\ntions, without requiring handcrafted feature encoding\nas input or iterative reasoning during decision-making.\nAlphaDou uses a reinforcement learning (RL) frame-\nwork to achieve this goal, driven solely by game re-\nwards. The Bid Phase introduces significant variance\nin game rewards, so we implemented a series of mea-\nsures to reduce reward variance and make the model's\nstrategy more flexible."}, {"title": "Card Representation and Neural Architecture", "content": "For any card combination, excluding jokers, we encode\nthe remaining card combination into a one-hot 4\u00d713\nmatrix, with 13 columns representing the cards 3, 4,\n5, 6, 7, 8, 9, T, J, Q, K, A, 2. The i-th row (where\ni \u2208 {0,1,2,3,4}) indicates whether the number of that\ncard type is greater than i; if true, it is 1, otherwise it\nis 0. This is then flattened into a 1 \u00d7 52 vector, with an\nadditional 1 \u00d7 2 matrix indicating the presence of the\nBlack and Red Jokers. Figure 1(a) demonstrates this\nencoding process.\nDuring the bidding phase, we record the observed\ndata as shown in Table 1 and generate a 5 \u00d7 54 obser-\nvation matrix for each possible move. All observation\nmatrices are combined into a batch \u00d7 5 \u00d7 54 matrix as\ninput data, where the batch size is the number of valid\nmoves.\nIn the card-playing phase, our recorded data is di-\nvided into parts a and b. The observation data in Table\n2 is used to generate a 72 \u00d7 54 observation matrix for\neach possible move. All observation matrices are com-\nbined into a batch \u00d7 5 \u00d7 54 matrix as input data part a,\nwhere the batch size is the number of valid moves. We\nalso encode the number of bombs played in the game\nas a one-hot 1 x 15 matrix, indicating the number of\nbombs from 0 to 14 that have been played. A 1 \u00d7 3\nmatrix records the bid scores of the first, second, and\nthird players (with -1 if they did not participate in the\nbidding). The bid data and the bomb count are con-\ncatenated and repeated batch times to form a batch \u00d7\n18 matrix as data part b. Although there might be du-"}, {"title": "Deep Monte-Carlo", "content": "Monte Carlo (MC) methods are a class of methods\nthat estimate strategies and value functions by mod-\neling sample paths. Monte Carlo methods are very ef-\nfective in episodic tasks to estimate the value function\nby taking every-visit MC approach (Sutton 1998).\n1. Generating sample trajectories using a speci-\nfied policy \u03c0: Starting from an initial state, simu-\nlate using the current policy until a terminal state is\nreached, generating a complete state-action-reward\nsequence.\n2. Calculating returns and updating Q(s,a) val-"}, {"title": "DMC with Probability and Value Factorization", "content": "Considering the significant gap between the distribu-\ntion of winning scores and losing scores, we perform\nValue Factorization on the Q-value (Wang, Wu, and Lai\n2023). Given that there are no ties in Doudizhu and the\noutcomes of the game (win or lose) are mutually exclu-\nsive, we have:\n$\\Q(s,a) = pw(s, a)Qw(s, a) + (1 \u2212 pw(s, a))Q\u0131(s, a)$$\nwhere pw(s, a) is the winning probability given (s,a),\nand $Qw(s, a)$ and $Q\u0131(s, a)$ are the Q-values for winning\nand losing, respectively."}, {"title": "", "content": "When updating the Q-Net, we do not directly min-\nimize the Mean Square Error, MSE(reward, predicted\nQ-value), to update the Q-Net. Instead, we simultane-\nously optimize the winning probability $p_{w}(s, a)$, and the\nQ-values $Q_{w}(s, a)$ and $Q_{l}(s, a)$ for winning and losing.\nWe divide the training data D into two mutually ex-\nclusive datasets for winning and losing. For outcome\n$\\u$:\n$\\D = Dw U Di$\n$\\Dw = {(s, a, Rw, u) | u = 1}$$\n$\\D\u2081 = {(s, a, R\u2081, u) | u = \u22121}$$\n$Q_{w}(s, a)$ is trained using Dw, while $Q_{l}(s, a)$ is trained\nusing Dr. The winning probability $p_{w}(s, a)$ is trained\nusing {$u$} in D.\nThe final loss function is:\n$\\L = X1Lp + A2Lq$\nwhere a\u2081 and a2 are two hyperparameters controlling\nthe weights. The winning probability $p_{w}(s, a)$ is derived\nfrom the neural network output $p(s, a)$:\n$\\p_{w}(s, \u03b1) = \\frac{p(s, a) +1}{2}$$\nThe loss function for the probability is:"}, {"title": "", "content": "$\\Lp = MSE(p(s, a), u)$$\nThe loss function for the Q-value is:\n$\\Lq = \\frac{D_{w}}{|D|}MSE_{D_{w}}(Qw(s,a), R_{w})+\\frac{D_{i}}{|D|}MSE_{D_{i}}(Q\u0131(s,a), R_{l})$$\nWhen generating a strategy, we calculate $Q(s,a) =$\n$p_{w}(s,a)Q_{w}(s, a) + (1 \u2212 pw(s,a))Q_{l}(s,a)$, and consider\nwhether factors affecting the final reward still exist:\nwhether the spring bonus can no longer be obtained,\nand whether there are no bomb cards left in this game.\nIf these factors are excluded, theoretically $Qw(s,a) =$\n$Q\u0131(s, a)$, but the absolute values of the neural network"}, {"title": "", "content": "outputs are not always equal, which introduces errors\nin calculating Q(s, a). In this case, we directly choose\nthe move with the highest winning probability $p_{w}(s, a)$.\nIf factors affecting the final reward still exist, we\nprune the moves based on Q(s, a). We consider moves\nwhose difference from max Q(s, a) is within a certain\npercentage range $\u03c1 = 0.05$ as selectable moves, forming\nthe pruned set of selectable moves:\n$\\A_{cut} \u2208 {a | \\frac{|Q(s,a)- max Q(s, a) |}{max Q(s, a)}< \u03c1}$$\nThen, we choose the move with the highest winning\nprobability $p_{w}(s, a)$ within Acut:\n$\\a_{best} = max p_{w}(s, a), a \u2208 A_{cut}$$\nWe use the epsilon-greedy method to introduce ex-"}, {"title": "EXPERIMENTS", "content": "In this chapter, we compare the performance of the Al-\nphaDou card-playing model (CardModel) with Douzero\nand Douzero Resnet. Douzero Resnet is the state-of-\nthe-art Dou Dizhu AI based on the Douzero algorithm,\nreplacing the LSTM neural network in Douzero with\nResNet, significantly improving performance compared\nto Douzero. The weights and code for Douzero Resnet\nare open-sourced at https://github.com/Vincentzyx/\nDouzero_Resnet. We also compare the AlphaDou bid\nmodel (Bid Model) with a supervised learning bid\nmodel (Douzero Resnet Bid). The Douzero Resnet Bid\nwe used is derived from Douzero Resnet: fixing the land-\nlord player's hand, randomly distributing 1000 sets of\nfarmer hands and landlord hands, loading the Douzero\nResnet model for games, obtaining a mean score from\nthe results of 1000 games, using the hand as input,\nand the mean score as the label for supervised learn-\ning. When applying the model, a threshold is set for\nbidding: a model output greater than -0.1 bids 1 point,\ngreater than 0 bids 2 points, and greater than 0.1 bids\n3 points. The Douzero demonstration website https:\n//www.douzero.org/bid also has a bidding model, but\nits bidding method is not based on a 3-point system,\nand it does not have an interface reserved for testing.\nOur AI system is trained on a server with 4 Intel(R)\nXeon(R) Gold 6330 CPUs @ 2.10GHz and a GeForce\nRTX 4090 GPU in the Ubuntu 20.04 operating system.\nThe code is available in the supplement."}, {"title": "Compare Bid Model to Douzero Resnet Bid", "content": "Dou Dizhu has three players, and we catego-\nrize them into three positions\u2014first, second, and\nthird according to the order of bidding. To evalu-\nate the performance of the Bid model, we initially set\nall three positions to a combination of Douzero and\nDouzero Resnet Bid, recording the scores for each posi-\ntion after 4000 games (control group). Next, we succes-\nsively replace the Douzero Resnet Bid at each position\nwith the Bid Model and conduct the same 4000 games\nto observe whether the scores at each position improve\ncompared to the control group. Since Douzero's card\nplaying is unaffected by the Bid model, we use Douzero\nas the card-playing model in this experiment.\nMetrics. Following (Jiang et al. 2019), given an al-\ngorithm A and an opponent B, we use two metrics to\ncompare the performance of A and B:\n\u2022\nWP (Winning Percentage): The number of\ngames won by A divided by the total number of\ngames.\n\u2022\nADP1 (Average Difference in Points 1): The\naverage difference of points scored per game between\nA and B. The base point is 1. Each bomb will double\nthe score.\n\u2022\nADP2 (Average Difference in Points 2): The\naverage difference of points scored per game between\nA and B. The base score is 1 to 3 points, determined\nby the highest bid during the bidding phase. Each\nbomb will double the score. Spring bonuses will also\ndouble the score.\nAdditionally, we evaluate each position's:\n\u2022\nLP (Landlord Percentage): The number of\ngames in which A became the Landlord Player di-\nvided by the total number of games.\n\u2022\nDR (Draw Rate): The number of draw games di-\nvided by the total number of games."}, {"title": "", "content": "The test results are shown in Table 4.\nFor each test group, the Bid Model shows signifi-\ncant improvement in WP, ADP2, and LP compared\nto Douzero Resnet Bid, while also achieving a lower\nDraw Rate. In Dou Dizhu, the Landlord Player wins\ndouble the rewards, so accurately determining whether\na player should become the Landlord Player is crucial\nfor scoring. The Bid Model is more aggressive, tend-\ning to become the Landlord Player more often, whereas\nDouzero Resnet Bid is more conservative. One reason\nis that the Bid Model adjusts its bids by considering\nthe bids of other players; when opponents bid low, the\nBid Model may bid high even if the player's hand is\nnot exceptionally good but relatively better than the\nopponents' hands.\nWhen two positions are replaced with Bid Models,\nthe ADP and LP of the position still using Douzero\nResnet Bid significantly decrease, indicating that the\nmore accurate judgment of the Bid Models exploits the\nDouzero Resnet Bid."}, {"title": "Compare Card Model to Benchmarks with Random Bidding Phase", "content": "To evaluate the performance of the Card Model, we fol-\nlowed the approach of (Jiang et al. 2019) and douzero\n(Zha et al. 2021), initiating a competition between the\nLandlord and the Peasants. We reduce variance by play-\ning each deck twice. Specifically, for two competing al-\ngorithms A and B, they will first play with A as the\nLandlord and B as the Peasants for a given deck. Then,\nthey swap roles, with A as the Peasants and B as the\nLandlord, and play the same deck again. A total of 4,000\ngames were conducted. Considering that the Bid result\nis random, we set the initial score of the game to 2\npoints for the Landlord's win and 1 point for the Peas-\nants' win, with each bomb doubling the final score (we\ndefine this scoring method as ADP1). The Card Model\nneeds to decide the playing strategy based on the bid-\nding process, and the random bidding process will lead\nto a decline in model performance because the random\ntesting deck distribution deviates from the training pro-\ncess."}, {"title": "Case Study: Bid Model vs Douzero Resnet Bid", "content": "In these cases, we use the following abbreviations: \"P\"\nfor \u201cPass\u201d, \u201cT\u201d for card \u201c10\u201d, \u201cJ\u201d for Jack, \u201cQ\u201d for\nQueen, \u201cK\u201d for King, \u201cA\u201d for Ace, \u201cB\u201d for Black Joker,\nand \u201cR\u201d for Red Joker. Each action is represented as\n\u201cposition: action,\u201d where \u201cposition\u201d can be \u201cL\u201d for\nLandlord, \u201cD\u201d for Peasant-Down, or \u201cU\u201d for Peasant-\nUp. For example, \"L:TT\" denotes the Landlord play-\ning a Pair (10 10), and \u201cD: 22\u201d indicates Peasant-Down\nplaying a Pair (22). The actions are separated by com-\nmas (e.g., \u201cL:J,D:Q,U:Pass\u201d).\nCompared to threshold bidding, reinforcement learn-\ning bidding has a more flexible handling of different\nbidding situations. Here, we analyze a hand with the\ncards 333444569TTJJQKK2. The hand score given by\nDouzero Resnet Bid is -0.987, indicating that this hand\nis very weak. Firstly, the only high card is 2, and the\nabsence of 7, 8, A, B, and R means that there is a high\nprobability that other players' hands form bombs. Us-\ning Douzero Resnet Bid, the choice would be \"O points\".\nFor the same hand, the Bid Model gives different bid-\nding scores based on the bidding order. In different\nsituations, the Bid Model's bidding strategy varies, as"}, {"title": "Case Study: Card Model vs Douzero Resnet vs Douzero", "content": "Figure 2 shows the cards held by the landlord, the\nlandlord's next player, and the landlord's previous\nplayer. The landlord does not have any joker cards, but\ntheir hand is well-structured and strong. The landlord\nchooses to play 44, which brings the game to the first\ncritical point: the landlord's next player Douzero plays\nKK, while Card Model and Douzero Resnet choose to\nplay AA. Playing KK allows the landlord to regain card\nrights with AA, whereas playing AA prevents the land-\nlord from taking card rights.\nFirst, let's analyze the scenario where the farmer\nplays KK and the landlord regains card rights with"}, {"title": "", "content": "AA. In this situation, the landlord has the advantage,\nwith Douzero Resnet and Douzero opting to play 5557,\nwhile Card Model chooses to play 888999TJ, as shown\nin Figure 3. In this scenario, if the opponent plays\nQQQ5 after 5557, the only way to regain card rights\nis by playing 222K. The landlord's previous player then\nuses the rocket to obtain forced card rights, leaving the\nlandlord with 7888999K, resulting in failure and bomb\npenalty for the landlord. Conversely, Card Model's play\nof 888999TJ, an airplane type, is a rare hand. The\nopponent has a low probability of suppressing it. If\nthe farmer uses the rocket to gain forced card rights,\nthe landlord's remaining hand is 5557K222. With three\n2s being the highest cards, the landlord can still win\nand receive a bomb reward. If the farmer opts to pass\nagainst the airplane type, the landlord can play 5557\nand have 222K left. The farmer can only prevent the\nlandlord from playing all their cards in the next hand\nby using the rocket. However, using the rocket leaves\nthe landlord with three 2s, and the farmer cannot win.\nThe farmer can only avoid using the rocket to evade the\nbomb penalty. The Card Model landlord values card\nrights more and plays more conservatively, leading to a\nhigher win rate."}, {"title": "", "content": "From the analysis above, it is clear that the farmer's\nchoice to play KK leads to quick failure. Only by play-\ning AA can the farmer have a chance to win. Card\nModel and Douzero Resnet are more sensitive to po-\ntential dangers. After the landlord's next player plays\nAA and gains card rights, the game reaches the second\ncritical point, as shown in Figure 4. Douzero Resnet\nopts to play 89TJQ to gain card rights before playing\n3336, while Card Model directly plays 3336.\nAnalyzing Douzero Resnet's play, playing 89TJQ re-\nduces hand complexity. When 3336 is played next,\nthe landlord plays 555T, leaving the landlord's next\nplayer with 777KK2. If they play 7772, leaving KK,\nthe landlord can play 2227. Even if the landlord's pre-\nvious player uses the rocket for forced card rights, the\nlandlord's remaining AA will be the highest cards and\nwin, earning the bomb reward. Returning to 777KK2,\nif 777K is played, leaving K and 2, it can secure a\nwin. However, leaving two single cards is unwise, es-\npecially when neither K nor 2 is the highest card (the\njoker hasn't appeared yet). This incomplete informa-\ntion game makes the 777K strategy unlikely, leading to\nthe farmer's almost certain failure.\nNow, consider Card Model's play of directly playing"}, {"title": "", "content": "3336. After the landlord plays 555T, the landlord's next\nplayer is inclined to play 777K. This is because the re-\nmaining K can combine with 89TJQ to form 89TJQK,\nretaining the single 2, ensuring the farmer's victory. The\nCard Model farmer can maintain hand diversity in com-\nplex situations, keeping more possibilities open, which\nalso results in a higher win rate."}, {"title": "Conclusion", "content": "The game of DouDiZhu is an extremely challenging in-\ncomplete information game. It has a vast state/action\nspace and unique characteristics involving reasoning\nabout competition and cooperation, making the game\nparticularly difficult to solve. Research on DouDiZhu\ntypically simplifies the game by not considering the bid-\nding phase and the \"spring\" bonus, as including these\nfactors increases the variance in rewards, making the\nmodel harder to converge. Additionally, the inclusion\nof bidding can cause deviations in the card distribution\ncompared to when bidding is not included.\nThis paper first incorporates factors like bidding and\nthe spring bonus to make the research environment\nmore closely resemble the actual DouDiZhu game en-\nvironment. Secondly, it modifies the Deep Monte Carlo\nalgorithm framework, using reinforcement learning to\nobtain a neural network that simultaneously estimates\nwin rates and expectations. The action space is pruned\nusing expectations, and strategies are generated based\non win rates. This modification allows the DMC algo-\nrithm to produce strategies that are not solely depen-\ndent on value (expectation) but also consider win rates,\nresulting in a state-of-the-art (SOTA) DouDiZhu rein-\nforcement learning model, which we named AlphaDou.\nWe compared AlphaDou with the baseline program\nDouZero, achieving a win rate of 0.6167 in an envi-\nronment with bidding. Even when there are differences\nbetween the training environment with bidding and the\ntesting environment without bidding, AlphaDou still\nachieved a win rate of 0.5970 and an average score per\ngame of 0.4343, making it the SOTA RL model. RL"}, {"title": "B.Mixture of the Policy Makes the AI Stronger", "content": "Douzero has open-sourced two model weights: douzero-wp, which uses win rate as the reward, and douzero-adp,\nwhich uses expectation as the reward. Douzero generates strategies based on the maximum output of douzero-adp.\nWe propose the following two methods to consider both douzero-wp and douzero-adp models simultaneously to\ngenerate strategies:\n1. Bomb check: Check for factors that influence the final reward. If none exist, choose the move with the highest\nwin rate based on douzero-wp output. Factors influencing the final reward include spring reward and bomb reward.\nDouzero does not consider the spring reward, so this step is to determine the presence of a bomb.\n2. Mixed strategy (Mix): Prune moves based on the expectation derived from douzero-adp. We consider moves\nwith an expectation difference within a certain percentage range ($\u03c1 = 0.05$) from the maximum expectation as viable\nmoves. Then, select the move with the highest win rate among the viable moves.\nWe can derive four different RL models for generating strategies: douzero, douzero with only Bomb check (Bomb\ncheck), douzero with only mixed strategy (Mix), and douzero with Bomb check followed by mixed strategy (Bomb\ncheck & Mix). We tested the performance of these four models against douzero in fixed 4000 game scenarios at\ndifferent positions (landlord, farmer). The specific results are displayed in the Table 10. It can be seen that both\nBomb Check and Mixed strategy yield better strategies than the standalone douzero-adp. Bomb Check followed by\nMixed strategy achieves the best strategy."}, {"title": "A.The Combinations and Ranks of Cards", "content": "One of the challenges in the game of DouDiZhu is the vast state/action space, which includes numerous card combi-\nnations. For certain categories, players can choose a \"kick-out\" card, which can be any card from their hand, directly\nleading to a large action space. For the landlord player, the winning condition is to play all their cards, while farmer\nplayers do not always need to play all their cards; their teammate clearing their hand also signifies victory. This re-\nquires considering using larger cards as kick-out cards and retaining smaller cards to coordinate with the teammate's\nplays. Players need to carefully strategize their moves to win the game. The classification of card types in DouDiZhu\nis shown in the Table 9. Note that \"Bombs\" and \"Rockets\" break category rules and can dominate all other categories."}]}