{"title": "AlphaDou: High-Performance End-to-End Doudizhu AI Integrating Bidding", "authors": ["Chang Lei", "Huan Lei"], "abstract": "Artificial intelligence for card games has long been a popular topic in AI research. In recent years, complex card games like Mahjong and Texas Hold'em have been solved, with corresponding AI programs reaching the level of human experts. However, the game of Dou Di Zhu presents significant challenges due to its vast state/action space and unique characteristics involving reasoning about competition and cooperation, making the game extremely difficult to solve. The RL model DouZero, trained using the Deep Monte Carlo algorithm framework, has shown excellent performance in DouDiZhu. However, there are differences between its simplified game environment and the actual Dou Di Zhu environment, and its performance is still a considerable distance from that of human experts. This paper modifies the Deep Monte Carlo algorithm framework by using reinforcement learning to obtain a neural network that simultaneously estimates win rates and expectations. The action space is pruned using expectations, and strategies are generated based on win rates. This RL model is trained in a realistic DouDiZhu environment and achieves a state-of-the-art level among publicly available models.", "sections": [{"title": "Introduction", "content": "Games can be broadly classified into two categories: perfect-information games (PIGs) and imperfect- information games (IIGs). In PIGs, players can observe all game states, such as in Shogi, Go, and Chess. In contrast, IIGs involve scenarios where participants cannot access complete information about other players, such as in heads-up Texas Hold'em. Reinforcement learning (RL) has been successfully applied to create numerous game Als. RL algorithms have achieved remarkable success in both PIGs and IIGs, exemplified by AlphaGo (Silver et al. 2016) and AlphaZero (Silver et al. 2017) in Go, AlphaStar (Vinyals et al. 2019) in StarCraft II, OpenAI Five (OpenAI et al. 2019) in Dota 2, Suphx (Li et al. 2020) in Mahjong, Douzero (Zha et al. 2021) in Doudizhu, NukkiAI (Bouzy, Rimbaud, and Ventos 2020) in Contract Bridge, and AlphaHoldem (Zhao et al. 2022a) in Hold'em.\nHowever, AI has not performed perfectly in certain gambling games that require bidding. NukkiAI only outperformed professional human players in non- bidding 1v1 Bridge, and Douzero did not consider the bidding phase during training. These AIs function more as playing machines rather than proficient gamblers. The bidding phase contains rich strategic information that significantly influences player strategies. When an opponent has a strong hand, they tend to bid high for higher potential rewards, while players should adopt a conservative strategy to minimize losses. Conversely, when opponents bid low, players can employ more aggressive strategies to increase their gains.\nThis work aims to develop a high-performance end- to-end Doudizhu AI model that incorporates bidding. Doudizhu, also known as Fighting the Landlord, is the most popular card game in China. Doudizhu is a 3- player IIG where players bid based on their hands, and the winning bidder becomes the Landlord. The remain- ing players form the Peasants team to oppose the Land- lord. If any Peasant player wins, the entire team wins. The Landlord wins double rewards, while each Peasant player receives a single reward if the team wins, and vice versa. Rewards are related to the bid score and the occurrence of \"bombs\" (four cards of the same rank) or \"rockets\" during the game. Players with good hands tend to bid high to become the Landlord for higher returns. Moreover, Doudizhu has a large, flexible, and diverse action space with thousands of possible states (1083) and actions (27,472) due to card combinations and complex rules (Zha et al. 2019). Additionally, rewards in Doudizhu are sparse and highly variable, only awarded at the end of the game and influenced by the bidding phase, the number of \"bombs\" during the game, and \"spring\" rewards. These characteristics make train- ing a Doudizhu AI extremely challenging, and existing Doudizhu Als exhibit certain issues.\nPrevious research on Doudizhu AI has primarily fo- cused on the playing phase, neglecting the bidding phase or employing completely random bid strategies. DeltaDou (Jiang et al. 2019) is the first AI program to achieve human-level performance compared to top human players, using an AlphaZero-like algorithm with Bayesian methods to infer hidden information and sam-"}, {"title": "The Game of Doudizhu", "content": "Doudizhu is a three-player card game that is extremely popular in China and is considered a typical gambling game. Among the three players, two are Peasants who need to cooperate to compete against the third player, the Landlord. The game comprises two phases: 1) Bid- ding and 2) Cardplay."}, {"title": "Bidding", "content": "The Bidding Phase determines the roles of the players. At the start of the game, each player receives seventeen cards from a shuffled deck in a counterclockwise man- ner, with three cards left in the middle of the table. In the Bidding Phase, a randomly selected player begins the bidding process, followed by the others in sequence. Each player can only bid once, with options to bid 1 point, 2 points, 3 points, or pass. A subsequent player must either choose a higher bid or pass. The first player to bid 3 points becomes the Landlord, or if all players complete their bids, the player with the highest bid be- comes the Landlord, while the other two players become Peasants. The Landlord has the privilege to reveal the three remaining cards for all players to see and then in- corporates these cards into his/her hand. Notably, if all three players choose to pass, the game results in a draw, and a new game starts with a fresh deal. The bid score impacts the final game rewards: if the Landlord wins, he/she gains points equal to twice the bid score from both Peasants. Conversely, if any Peasant wins, both Peasants receive points equal to the bid score from the Landlord, who loses double the points. This scenario as- sumes the absence of Bombs, Rockets, and Spring (refer to the following section)."}, {"title": "Cardplay", "content": "During the Cardplay phase, players take turns playing cards. Each game consists of multiple rounds, starting with a player playing a valid card combination (e.g., solo, pair). The first round is initiated by the Land- lord. Subsequent players must either pass or defeat the previous hand by playing a higher-ranked combination (an action has a rank, refer to Appendix A). The round continues until two consecutive players pass. Then, the player who played the last hand starts the next round. The objective is to clear all cards from one's hand to win. Each \"Bomb\" and \"Rocket\" can double the game's stakes. If the Landlord wins, they receive double the re- wards, whereas if the Peasant team wins, each Peasant player receives single rewards. Rewards are influenced by the bid score and the presence of Bombs or Rockets. Bombs surpass any action. The only way to defeat a Bomb is with a higher-ranked Bomb or a Rocket. The Rocket is the highest action in the game and can beat any Bomb or action. When a Bomb or Rocket is played, the points at stake double. For instance, if the winning bid is 3 points at the start, it becomes 6 points if a Bomb is played and 12 points if another Bomb is played. With two Bombs played, the Landlord stands to win/lose 24"}, {"title": "AlphaDou", "content": "The goal of AlphaDou is to incorporate the Bid Phase in the training and testing stages of the Doudizhu AI, enabling the AI to fully engage in a complete gambling game. \"End-to-end\" here means that this framework di- rectly accepts game state information and outputs ac- tions, without requiring handcrafted feature encoding as input or iterative reasoning during decision-making. AlphaDou uses a reinforcement learning (RL) frame- work to achieve this goal, driven solely by game re- wards. The Bid Phase introduces significant variance in game rewards, so we implemented a series of mea- sures to reduce reward variance and make the model's strategy more flexible."}, {"title": "Card Representation and Neural Architecture", "content": "For any card combination, excluding jokers, we encode the remaining card combination into a one-hot $4 \\times 13$ matrix, with 13 columns representing the cards 3, 4, 5, 6, 7, 8, 9, T, J, Q, K, A, 2. The $i$-th row (where $i \\in \\{0,1,2,3,4\\}$) indicates whether the number of that card type is greater than $i$; if true, it is 1, otherwise it is 0. This is then flattened into a 1 \u00d7 52 vector, with an additional 1 \u00d7 2 matrix indicating the presence of the Black and Red Jokers.\nDuring the bidding phase, we record the observed data and generate a 5 \u00d7 54 observation matrix for each possible move. All observation matrices are combined into a batch \u00d7 5 \u00d7 54 matrix as input data, where the batch size is the number of valid moves.\nIn the card-playing phase, our recorded data is di- vided into parts a and b. The observation data is used to generate a 72 \u00d7 54 observation matrix for each possible move. All observation matrices are com- bined into a batch \u00d7 5 \u00d7 54 matrix as input data part a, where the batch size is the number of valid moves. We also encode the number of bombs played in the game as a one-hot 1 x 15 matrix, indicating the number of bombs from 0 to 14 that have been played. A 1 \u00d7 3 matrix records the bid scores of the first, second, and third players (with -1 if they did not participate in the bidding). The bid data and the bomb count are con- catenated and repeated batch times to form a batch \u00d7 18 matrix as data part b. Although there might be du-"}, {"title": "Deep Monte-Carlo", "content": "Monte Carlo (MC) methods are a class of methods that estimate strategies and value functions by mod- eling sample paths. Monte Carlo methods are very ef- fective in episodic tasks to estimate the value function by taking every-visit MC approach (Sutton 1998).\n1. Generating sample trajectories using a speci- fied policy \u03c0: Starting from an initial state, simu- late using the current policy until a terminal state is reached, generating a complete state-action-reward sequence.\n2. Calculating returns and updating Q(s,a) val- ues: For each state-action pair (s, a) in every tra-"}, {"title": "DMC with Probability and Value Factorization", "content": "Considering the significant gap between the distribu- tion of winning scores and losing scores, we perform Value Factorization on the Q-value (Wang, Wu, and Lai 2023). Given that there are no ties in Doudizhu and the outcomes of the game (win or lose) are mutually exclu- sive, we have:\n$$Q(s,a) = p_w(s, a)Q_w(s, a) + (1 \u2212 p_w(s, a))Q_l(s, a)$$\nwhere $p_w(s, a)$ is the winning probability given (s,a), and $Q_w(s, a)$ and $Q_l(s, a)$ are the Q-values for winning and losing, respectively.\nWhen updating the Q-Net, we do not directly min- imize the Mean Square Error, MSE(reward, predicted Q-value), to update the Q-Net. Instead, we simultane- ously optimize the winning probability $p_w(s, a)$, and the Q-values $Q_w(s, a)$ and $Q_l(s, a)$ for winning and losing. We divide the training data D into two mutually ex- clusive datasets for winning and losing. For outcome u:\n$$D = D_w U D_l$$\n$$D_w = \\{(s, a, R_w, u) | u = 1\\}$$\n$$D_l = \\{(s, a, R_l, u) | u = \u22121\\}$$\n$Q_w(s, a)$ is trained using $D_w$, while $Q_l(s, a)$ is trained using $D_l$. The winning probability $p_w(s, a)$ is trained using $\\{u\\}$ in D.\nThe final loss function is:\n$$L = \u03bb_1L_p + \u03bb_2L_q$$\nwhere $\u03bb_1$ and $\u03bb_2$ are two hyperparameters controlling the weights. The winning probability $p_w(s, a)$ is derived from the neural network output $p(s, a)$:\n$$p_w(s, \u03b1) = \\frac{p(s, a) +1}{2}$$\nThe loss function for the probability is:"}, {"title": "Compare Bid Model to Douzero Resnet Bid", "content": "Dou Dizhu has three players, and we catego- rize them into three positions\u2014first, second, and third according to the order of bidding. To evalu- ate the performance of the Bid model, we initially set all three positions to a combination of Douzero and Douzero Resnet Bid, recording the scores for each posi- tion after 4000 games (control group). Next, we succes- sively replace the Douzero Resnet Bid at each position with the Bid Model and conduct the same 4000 games to observe whether the scores at each position improve compared to the control group. Since Douzero's card playing is unaffected by the Bid model, we use Douzero as the card-playing model in this experiment.\nMetrics. Following (Jiang et al. 2019), given an al- gorithm A and an opponent B, we use two metrics to compare the performance of A and B:\n\u2022 WP (Winning Percentage): The number of games won by A divided by the total number of games.\n\u2022 ADP1 (Average Difference in Points 1): The average difference of points scored per game between A and B. The base point is 1. Each bomb will double the score.\n\u2022 ADP2 (Average Difference in Points 2): The average difference of points scored per game between A and B. The base score is 1 to 3 points, determined by the highest bid during the bidding phase. Each bomb will double the score. Spring bonuses will also double the score.\nAdditionally, we evaluate each position's:\n\u2022 LP (Landlord Percentage): The number of games in which A became the Landlord Player di- vided by the total number of games.\n\u2022 DR (Draw Rate): The number of draw games di- vided by the total number of games.\nThe test results are shown."}, {"title": "Compare Card Model to Benchmarks with Random Bidding Phase", "content": "To evaluate the performance of the Card Model, we fol- lowed the approach of (Jiang et al. 2019) and douzero (Zha et al. 2021), initiating a competition between the Landlord and the Peasants. We reduce variance by play- ing each deck twice. Specifically, for two competing al- gorithms A and B, they will first play with A as the Landlord and B as the Peasants for a given deck. Then, they swap roles, with A as the Peasants and B as the Landlord, and play the same deck again. A total of 4,000 games were conducted. Considering that the Bid result is random, we set the initial score of the game to 2 points for the Landlord's win and 1 point for the Peas- ants' win, with each bomb doubling the final score (we define this scoring method as ADP1). The Card Model needs to decide the playing strategy based on the bid- ding process, and the random bidding process will lead to a decline in model performance because the random testing deck distribution deviates from the training pro- cess."}, {"title": "Compare Card Model to benchmarks with Bidding Phase", "content": "We conducted another 4,000 matches and with the bid model set to Bid Models. The game scores are divided into ADP1 and ADP2. ADP1 is consistent with the one mentioned above, and ADP2 is calculated based on the results of the Bid Models. For example, if the landlord wins with 3 points, the landlord scores 6 points, the peasants score 3 points, and each bomb will double the final score.\nThe Card Model still dominates all other algorithms. Compared to the random Bidding Phase, the WP of the Card Model against Douzero Resnet and Douzero has significantly improved. This indicates that the Card Model can adjust its playing strategy based on the Bid results to achieve higher returns. Notably, with the Bid- ding Phase, the WP of Douzero Resnet against Douzero also increased (0.5809 > 0.5702), but Douzero Resnet does not adjust its playing strategy based on the bid results. We believe this is because the bid results pro- vided by the Bid Model are favorable to the landlord, and if a model's landlord strength is very strong, its overall win rate will correspondingly increase.\nTable 7 shows the WP and ADP of the Card Model (Landlord) and Douzero Resnet (Landlord) against Douzero (Peasant). It can be seen that the strength of Douzero Resnet (Landlord) is quite similar to that of the Card Model (Landlord). Therefore, after adjust- ing the bid strategy, the overall win rate of Douzero Resnet against Douzero will increase. Correspondingly, we find that although the strength of Douzero Resnet (Landlord) is similar to that of the Card Model (Land-"}, {"title": "Case Study: Bid Model vs Douzero Resnet Bid", "content": "In these cases, we use the following abbreviations: \"P\" for \u201cPass\u201d, \u201cT\u201d for card \u201c10\u201d, \u201cJ\u201d for Jack, \"Q\" for Queen, \u201cK\u201d for King, \u201cA\u201d for Ace, \u201cB\u201d for Black Joker, and \"R\" for Red Joker. Each action is represented as \"position: action,\u201d where \u201cposition\u201d can be \u201cL\u201d for Landlord, \"D\" for Peasant-Down, or \u201cU\u201d for Peasant- Up. For example, \"L:TT\" denotes the Landlord play- ing a Pair (10 10), and \u201cD: 22\u201d indicates Peasant-Down playing a Pair (22). The actions are separated by com- mas (e.g., \"L:J,D:Q,U:Pass\").\nCompared to threshold bidding, reinforcement learn- ing bidding has a more flexible handling of different bidding situations. Here, we analyze a hand with the cards 333444569TTJJQKK2. The hand score given by Douzero Resnet Bid is -0.987, indicating that this hand is very weak. Firstly, the only high card is 2, and the absence of 7, 8, A, B, and R means that there is a high probability that other players' hands form bombs. Us- ing Douzero Resnet Bid, the choice would be \"0 points\". For the same hand, the Bid Model gives different bid- ding scores based on the bidding order. In different situations, the Bid Model's bidding strategy varies, as"}, {"title": "Case Study: Card Model vs Douzero Resnet vs Douzero", "content": " shows the cards held by the landlord, the landlord's next player, and the landlord's previous player. The landlord does not have any joker cards, but their hand is well-structured and strong. The landlord chooses to play 44, which brings the game to the first critical point: the landlord's next player Douzero plays KK, while Card Model and Douzero Resnet choose to play AA. Playing KK allows the landlord to regain card rights with AA, whereas playing AA prevents the land- lord from taking card rights.\nFirst, let's analyze the scenario where the farmer plays KK and the landlord regains card rights with"}, {"title": "Conclusion", "content": "The game of DouDiZhu is an extremely challenging in- complete information game. It has a vast state/action space and unique characteristics involving reasoning about competition and cooperation, making the game particularly difficult to solve. Research on DouDiZhu typically simplifies the game by not considering the bid- ding phase and the \"spring\" bonus, as including these factors increases the variance in rewards, making the model harder to converge. Additionally, the inclusion of bidding can cause deviations in the card distribution compared to when bidding is not included.\nThis paper first incorporates factors like bidding and the spring bonus to make the research environment more closely resemble the actual DouDiZhu game en- vironment. Secondly, it modifies the Deep Monte Carlo algorithm framework, using reinforcement learning to obtain a neural network that simultaneously estimates win rates and expectations. The action space is pruned using expectations, and strategies are generated based on win rates. This modification allows the DMC algo- rithm to produce strategies that are not solely depen- dent on value (expectation) but also consider win rates, resulting in a state-of-the-art (SOTA) DouDiZhu rein- forcement learning model, which we named AlphaDou. We compared AlphaDou with the baseline program DouZero, achieving a win rate of 0.6167 in an envi- ronment with bidding. Even when there are differences between the training environment with bidding and the testing environment without bidding, AlphaDou still achieved a win rate of 0.5970 and an average score per game of 0.4343, making it the SOTA RL model. RL models trained in complex environments can also per- form excellently in more simplified environments."}, {"title": "B.Mixture of the Policy Makes the AI Stronger", "content": "Douzero has open-sourced two model weights: douzero-wp, which uses win rate as the reward, and douzero-adp, which uses expectation as the reward. Douzero generates strategies based on the maximum output of douzero-adp. We propose the following two methods to consider both douzero-wp and douzero-adp models simultaneously to generate strategies:\n1. Bomb check: Check for factors that influence the final reward. If none exist, choose the move with the highest win rate based on douzero-wp output. Factors influencing the final reward include spring reward and bomb reward. Douzero does not consider the spring reward, so this step is to determine the presence of a bomb.\n2. Mixed strategy (Mix): Prune moves based on the expectation derived from douzero-adp. We consider moves with an expectation difference within a certain percentage range (p = 0.05) from the maximum expectation as viable moves. Then, select the move with the highest win rate among the viable moves.\nWe can derive four different RL models for generating strategies: douzero, douzero with only Bomb check (Bomb check), douzero with only mixed strategy (Mix), and douzero with Bomb check followed by mixed strategy (Bomb check & Mix). We tested the performance of these four models against douzero in fixed 4000 game scenarios at different positions (landlord, farmer). The specific results are displayed. It can be seen that both Bomb Check and Mixed strategy yield better strategies than the standalone douzero-adp. Bomb Check followed by Mixed strategy achieves the best strategy."}]}