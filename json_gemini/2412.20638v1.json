[{"title": "Predicting Long Term Sequential Policy Value Using Softer Surrogates", "authors": ["Hyunji Nam", "Allen Nie", "Ge Gao", "Vasilis Syrgkanis", "Emma Brunskill"], "abstract": "Performing policy evaluation in education, healthcare and online commerce can be challenging, because it can require waiting substantial amounts of time to observe outcomes over the desired horizon of interest. While offline evaluation methods can be used to estimate the performance of a new decision policy from historical data in some cases, such methods struggle when the new policy involves novel actions or is being run in a new decision process with potentially different dynamics. Here we consider how to estimate the full-horizon value of a new decision policy using only short-horizon data from the new policy, and historical full-horizon data from a different behavior policy. We introduce two new estimators for this setting, including a doubly robust estimator, and provide formal analysis of their properties. Our empirical results on two realistic simulators, of HIV treatment and sepsis treatment, show that our methods can often provide informative estimates of a new decision policy ten times faster than waiting for the full horizon, highlighting that it may be possible to quickly identify if a new decision policy, involving new actions, is better or worse than existing past policies.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement learning has had many successes and offers the compelling framework of explicitly evaluating and optimizing decision policies to optimize for long-term outcomes. However, in many important settings - such as education, healthcare and online commerce - the horizon for the reward values of interest is very long. For example, it is common to consider how the use of an intelligent tutoring system over a school year impacts end-of-year external assessment results (Zheng et al., 2019), the impact of medical treatments on 5-year survival rates, and the impact of ads or promotions over a multi-month customer churn or engagement (Zhang et al., 2023). Performing policy optimization or evaluation in such settings can require substantial amounts of real time, and is a significant barrier to testing, deploying and iterating on adaptive sequential decision policies for such settings. This has motivated several recent works that consider how to estimate long-horizon outcomes of decision policies using short-horizon data (Saito et al., 2024; Cheng et al., 2021; Tang et al., 2022; Zhang et al., 2023; Tran et al., 2024).\nOur aim is to estimate the long-term value of a new target policy that involves previously unexplored actions, using only short-horizon data from this new policy and full-horizon historical data from a behavior policy. Without further assumptions, this task will generally be impossible to solve - the short-term data may not include all reachable states and rewards, and the historical policy and process may be entirely different than the target policy and the decision process of interest. To tackle this, surrogate indices have been proposed (Athey et al., 2019), which assumes a set of observed features that are made available from a short horizon, and renders the long-term delayed outcome independent of the decision policy.\nUnfortunately surrogacy given short-horizon observations is a strong assumption when it comes to sequential decision making settings, where the decision policy will continue to take actions through the full horizon. The natural application of surrogacy indices to our setting would be if observation of the short-term state trajectory made the future expected sum of rewards independent of the decision policy deployed during the remaining time steps. While there are settings where that will be true (such as if no reward occurs after the short horizon sequence), in general surrogacy is unlikely to hold.\nHowever, the \"dynamic invariance\" assumption of Battocchi et al. (2021) relaxes the standard surrogacy assumption by only requiring that the behavioral policy's estimand and the new target policy's estimand have the same relationship with the surrogates. For example, if the rewards and both the target and the behavioral policies are only functions of states, then even though a binary indicator for intervention $D$ changes the policy from $\\pi_1$ to $\\pi_2$, and therefore, the standard surrogacy assumption does not hold since the future trajectory of states due to the intervention $D$ are not independent given the surrogates, identification is still"}, {"title": "3 PROBLEM SETTING", "content": "We consider sequential decision processes. We assume there is a $H$-horizon episodic decision process $M_b$ consisting of a set of states $S$, actions $A_b$, a dynamics model $P(s_{t+1} | s_0, A_0, s_1, A_1,..., s_t, a_t)$ and reward model $R(s)$. We assume the reward model depends only on the current or the full history of states but not on actions. We assume access to a historical dataset $D_b$ which consists of a set of $N_b$ state and reward trajectories $T_i = (s_{0i}, r_{0i}, s_{1i}, r_{1i}, ..., s_{Hi}, r_{Hi})$ where each trajectory was collected by following a fixed behavior policy $\\pi_b$ in decision process $M_b$, which is a function mapping either the current state $s_t$ or a history of $s_0, s_1, .., s_t$ to action $a_t$. Importantly, we do not assume the decision process is Markov, the dataset contains the taken actions, knowledge of $\\pi_b$ nor that the behavior policy is Markov. We use $G_i = \\sum_{j=1}^H r_{ji}$ to denote the total sum of rewards in a particular trajectory\u00b2.\nThe value of a policy in a particular episodic decision process $M$ is defined as the expected sum of $H$-step rewards in that process: overloading notation and writing $E_\\pi$ to denote the expectation over the distribution of trajectories induced by policy $\\pi$, we can express the value as $V_{H,M} = E_{\\tau = (s_0,r_0,...,s_H,r_H) \\sim \\pi,M} \\sum_{j=1}^H r_j$. Let $t_h = (s_0, r_0,..., s_h, r_h)$.\nWe assume there is a second episodic decision process $M_2$ which has the same state space $S$ and same reward model $R(s)$ but may have a different transition dynamics as the previous historical process but whose"}, {"title": "4 ESTIMATORS", "content": "There are multiple challenges involved in obtaining a good estimate of $V_{H,M_2}$: The action space differs, the dynamics models may be different, and we do not assume knowledge of the behavior policy nor the reward model nor the dynamics models. We only have access to the historical dataset $D_b$ collected from executing the fixed behavioral policy $\\pi_1$ in the process $M_b$, and assume that the state spaces between $M_b$ and $M_2$ are the same, and the reward model $R(s)$ is defined over the states (or a history of states).\nWe now present a quantity that we can estimate, followed by describing conditions under which this quantity is the desired quantity of the long-term expected return of the target policy.\nDefine $V_{\\pi_2,\\pi_b}$ as the expected sum of returns of following the target policy $\\pi_2$ for $h$ steps in decision process $M_2$ to reach state $s_h$ (denote this sub-trajectory by $\\tau_{h2}$, and then following behavior policy $\\pi_b$ in decision process $M_1$ starting in state $s_h$, for the remaining $h + 1$ to $H$ steps of the episode, conditioned on the initial trajectory $(s_0,...,s_h)$ (call this latter sub-trajectory $\\tau_{2b}^H$).\n$V_{\\pi_2,\\pi_b} = E_{(s_0,r_0,s_1,r_1,......,s_h,r_h) \\sim \\pi_2,M_2} [\\sum_{j=1}^h r_j + E_{(s_{h+1},r_{h+1},......,s_H,r_H) \\sim \\pi_b,M_b | (s_0,r_0,..., s_h,r_h)} \\sum_{j'=h+1}^H r_{j'}] = E_{\\tau_h} [\\sum_{j=1}^h r_j + E_{\\tau_{2b}^H} [\\sum_{j'=h+1}^H r_{j'}] ]$\nIntuitively, this considers the expected return of following the new target policy over a short horizon in the new decision process, and then following the old behavior policy for the remaining time steps till the full horizon in the old decision process, conditioned on the observed initial trajectory in the new process."}, {"title": "4.1 Estimating the Target Value", "content": "Before describing our proposed policy estimators, we first consider how estimates of $V_{\\pi_2,\\pi_b}$ relate to our primary goal of estimating $V_{H,M_2}$. Consider the following assumption:\nAssumption 1. The expected future return given an initial sequence $s_{0:h}$ is the same under the behavior policy and the historical decision process, and the target policy and the new decision process:\n$E_{s_{h+1},r_{h+1},......,s_H,r_H | \\pi_b, M_b, s_0,... s_h, r_h} [\\sum_{i=h+1}^H r_i] = E_{s_{h+1},r_{h+1},......,s_H,r_H | \\pi_2, M_2, s_0,... s_h, r_h} [\\sum_{i=h+1}^H r_i]$    (1)\nUnder Assumption 1 $V_{\\pi_2,\\pi_b} = V_{H,M_2}$ (see Supplement).\nNote that the standard surrogacy assumption would require that these match for all possible decision policies and sequential data generation processes, so that the future return is independent of the decision process and policy conditioned on $s_{0:h}$. Assumption 1 is slightly weaker, only focusing on the historical decision policy and process, and the target policy and process.\nWe recognize this assumption is still quite strong. In practical settings, we will often be interested in evaluating if a new target policy is better or worse than"}, {"title": "4.2 Short Long Regression Estimation", "content": "We first introduce a Short Long Regression Estimator, which builds a regression function that takes in a short-horizon trajectory over states and rewards, and uses this to predict the full-horizon reward if the remaining trajectory was generated by following the behavior policy in the historical decision process:\n$f(\\tau_h) = \\sum_{i=1}^h r_i + E_{(s_{h+1},...,s_H,r_H) |\\pi_b, M_b/\\tau_h} [\\sum_{j'=h+1}^H r_{j'}]$\t(2)\n$V_{\\pi_2,\\pi_b}$ is then computed by evaluating $f$ over the short-horizon target policy dataset:\n$\\hat{V}_{H,M_2} = \\frac{1}{N_2} \\sum_{k=1}^{N_2} f(\\tau_h) \\sim E_{\\tau_h \\sim \\pi_2, M_2} [f(\\tau_h)]$\t\t(3)\nWhile $f$ can be learned directly using the data available in the behavior dataset, when data is limited and the function approximator is powerful, it may be helpful to change the loss to prioritize accurate predictions over the short-horizon sub-sequences most likely to occur in the target policy. To accomplish this we can use distribution shift aware methods or inverse propensity weighting when fitting the regressor function.\n$f_w(\\tau_h) = \\frac{p(\\tau_h |\\pi_2, M_2)}{p(\\tau_h |\\pi_b, M_b)} [\\sum_{i=1}^H r_i]$\t\t(4)\nNote that the density ratios involve probability distributions over the $h$-step state trajectories, which may yield a high dimensional distribution. Since we only require estimating the ratio, we use the popular technique of reducing density ratio estimation to a classification problem Sugiyama et al. (2007), which allows us to invert the problem and build a classifier $h(t_h)$ to predict if a sample came from the behavior or target policy using the short-horizon historical and target data.\nIn our experiments we will compare the weighted and unweighted functions (Equations 2 and 4) as an input to our regression estimate (Equation 42)."}, {"title": "4.3 Doubly Robust Short Long Estima- tion", "content": "One of the limitations to the prior estimator is that it relies on the machine learning regression estimate to predict the long-term returns of a particular trajectory. When the true model does not lie inside the considered regressor class, this can introduce asymptotic bias and error. In addition, the density ratio estimator may also have error and introduce additional errors.\nA recent powerful approach is to instead use double machine learning (Chernozhukov et al., 2023) or debiased machine learning (Battocchi et al., 2021; Kallus and Mao, 2024) to construct an estimator that is more robust to error in the \"nuisance\" parameters that contribute to the estimate of interest. We now construct such a double machine learning estimator for our setting. Define $Z = (\\tau_{hb}, G_b, \\tau_{h2})$ where $\\tau_{hb}$ is a H-step trajectory generated by $M_1$ using behavior policy $\\pi_b$, and $\\tau_{h2}$ is a $h$-step trajectory from the target policy:\nm((Z, f, a) = a(s_{0:h})(G_b - f(\\tau_{hb}) + f(\\tau_{h2})\t(5)\nwhere\na(s_{0:h}) = \\frac{p(\\tau_h | \\pi_2, s_0)}{p(\\tau_h | \\pi_1, s_0)}\t(6)\nrepresents the learned density ratio estimates over the short-horizon returns, and $f$ is the learned regressor defined in the prior subsection.\nNote the expected value of $m$ with the true function parameters $f_0, a_0$ (the true regression function and the true density ratios) satisfies the following moment condition\nM(f_0,a_0) = E_Z[m(Z, f_0, a_0)] = 0\t\nM(f_0, a_0) = M(f_0, \\alpha_0) - V_{\\pi_2,\\pi_b}\t(7)\nIn other words, the expected value of $m$ is an unbiased estimator $V_{\\pi_2,\\pi_b}$ given the (unknown) true regressor and density ratios.\nOf course, we do not have access to the true unknown regressor function and density ratios. Instead we construct an estimate of $V_{\\pi_2,\\pi_b}$ by using k-fold cross-fitting (Chernozhukov et al., 2023) using one fold of the data to estimate the nuisance parameters $(\\tilde{a}^{(k)}, f^{(k)})$ and the rest to evaluate the finite sample moment conditions given the estimated nuisance parameters, summed over the folds. The k-th cross fit estimate is\nM_k(f^{-k},a^{-k}) = \\frac{1}{N_b} \\sum_{i=1}^{N^{-k}} a^{-k}(T_i)(G_H(s_{0:h}) - f^{-k}(T_{bi})) + \\frac{1}{N_2}\\sum_{j=1}^{N_2}(T_{hj})\\t(8)\nwhere the first sum is over a subset of the historical data gathered by $\\pi_b$ in decision process $M_b$, and the second term is a sum over a subset of the short-horizon data sampled from the target policy $\\pi_2$ in decision process $M_2$. The second term serves as a baseline and is a"}, {"title": "5 THEORY", "content": "Overlap is a standard assumption for off-policy reinforcement learning. Here we present an adaptation of that assumption, specifically only on the short-horizon state distribution (not action distribution). Note that all proofs, when omitted, are presented in the supplement.\nAssumption 2. (Coverage). The behavior policy in the historical process strictly overlaps the distribution of short-horizon state sequences generated under the target policy in the target domain:\n$\\frac{P_0(\\tau_h | \\pi_2, M_2)}{P_0(\\tau_h | \\pi_b, M_b)} > 0$ for all $P_0(\\tau_h | \\pi_2, M_2) > 0$\nWe also define\n$\\epsilon_2(f^{(k)}) = \\sqrt{E_{\\tau_h \\sim \\pi_2, M_2} [(f^{(k)} (\\tau_h) - f_0(\\tau_h))^2]}$\n$\\epsilon_b(f^{(k)}) = \\sqrt{E_{\\tau_h \\sim \\pi_b, M_b} [(f^{(k)} (\\tau_h) - f_0(\\tau_h))^2]}$\n$\\epsilon_b(a^{(k)}) = \\sqrt{E_{\\tau_h \\sim \\pi_b, M_b} [(a(\\tau_h) - a_0(\\tau_h))^2]}$\nas the $L_2$ error between the estimated nuisance parameters and the true nuisance parameters. Note that we are defining these errors with respect to the two different data distributions: that induced by the data generation process from either the historical data process $M_b$ and policy $\\pi_b$, and that induced by the target data process $M_2$ and policy $\\pi_2$.\nWe now present a bound in the error of our DR estimator (Equation 9):\nTheorem 1 (Variance-Based Rate for DR). Assume that $|G_H|, |f^{(k)} (\\tau_h)|, |f_0(\\tau_h)|$ are a.s. bounded by $C_1H$ and $a^{(k)} (\\tau_h), a_0(\\tau_h)$ are a.s. bounded by $C_2$, where $C_1$ and $C_2$ are constants. Then w.p. at least $1 - \\delta$:\n$|\\hat{V}^{DR}_{H,M_2} - V_{\\pi_2,\\pi_b}| \\le \\sqrt{\\frac{12 Var_{\\tau_h \\sim \\pi_2, M_2} (f_0(\\tau_h)) log(4K/\\delta)}{N_2}}$\n$+ \\sqrt{\\frac{2E_{\\tau_h \\sim \\pi_b, M_b} [a_0 (\\tau_h)^2 (G_H(\\tau_h) - f_0(\\tau_h))^2] log(4K/\\delta)}{N_b}}$\n$+ \\frac{1}{K} \\sum_{k=1}^K \\epsilon_b(f^{(k)}) \\epsilon_b(a^{(k)}) + \\frac{2HK log(4K/\\delta)}{N_2}$\n$+ max_k \\epsilon_2(f^{(k)}) \\frac{2K log(4K/\\delta)}{N_2} + \\frac{4C_1C_2HK log(4K/\\delta)}{N_2}$\n$+ 3C_1Hmax_k {\\epsilon_b (f^{(k)}) + \\epsilon_b(a^{(k)})\\sqrt{\\frac{2K log(4K/\\delta)}{N_b}}$\nWe are interested in understanding when the first two terms (which depend on the variance under the the true, unknown, nuisance parameters) will dominate the bound. Note that terms four and six are dominated by the first two terms, as the first two terms have a slower rate dependence on $N_2$ and $N_b$, respectively. As long as $f$ is consistent for the target policy and data distribution, the fifth term will be dominated by the first term. Similarly if the nuisance parameters are consistently estimated under the behavior policy distribution, seventh term will be dominated by the second term. Finally the third term is a product of the nuisance error bounds under the historical distribution, which implies for this to be lower order than the first two terms, it may be particularly important for $f$ to well model the historical distribution. This further suggests that fitting $f$ using unweighted regression on the historical dataset (Equation 2) may have beneficial properties. We will examine this further in our empirical simulations.\nTheorem 5 immediately implies the following:\nCorollary 1. Given Assumption 2, if our estimates of the regression function $f$ and propensity weights / density ratio $a$ are asymptotically consistent, at any rate, then $\\hat{V}^{DR}_{H,M_2}$ is a consistent estimator of $V_{\\pi_2,\\pi_b}$.\nWe also now characterize the doubly robust property of our $\\hat{V}^{DR}_{H,M_2}$ estimator.\nTheorem 2. Define the error in the predicted value as $\\triangle(\\tau_h, f) = f(\\tau_h) - f_0(\\tau_h)$ and the density ratio relative to the true density ratio as $\\delta(\\tau_h) = \\frac{a(\\tau_h)}{a_0(\\tau_h)}$. Under Assumptions 1 and 2, then the bias of $\\hat{V}^{DR}_{H,M_2}$ is\n$V_{\\pi_2,\\pi_b} - \\hat{V}^{DR}_{H,M_2} = \\frac{1}{K} \\sum_{k=1}^K E_{\\tau \\sim \\pi_2, M_2} [\\triangle(\\tau_{h2}, f^{(k)}) (1 - \\delta^{(k)} (\\tau_{h2}))]$,\nthe average of the product-bias terms across the folds."}, {"title": "6 EXPERIMENTS", "content": "We now investigate the performance of our proposed estimators - namely, i) Short Long Regression Estimator, ii) unweighted version of Short Long Regression Estimator, iii) Doubly Robust Short Long Estimator, and iv) DR Short Long Estimator with weighted regressor as introduced in Section 4. Additionally we include results from a synthetic domain showing that the DR estimator is more robust to mis-specification in the regression and density modeling in Section 9.\nOne may ask how the new target policies are suggested as alternatives to the existing behavioral policies. We model two possible scenarios for proposing new policies with the HIV simulator by Ernst et al. (2006) and the sepsis treatment model by Oberst and Sontag (2019). In HIV we consider when new actions (i.e., dosages of drugs) substitute the old actions taken by the behavioral policy; in sepsis a new treatment method is introduced additionally to the existing interventions already available to the behavioral policy."}, {"title": "6.1 Clinical Simulator Experiments", "content": "Our goal in both settings is to estimate the long-term value of a target policy (i.e., returns from the full-horizon) using only the short-horizon data from this new policy and some historical data under a different behavioral policy."}, {"title": "6.1.1 Domains", "content": "HIV Treatment Human Immunodeficiency Virus (HIV) is a retrovirus that can lead to the lethal Acquired Immune Deficiency Syndrome (AIDS) devastating a person's immune system. In the simulator designed by Ernst et al. (2006), the patient's state is represented by 6 features representing the number of healthy and HIV-infected cells in the range between (0,108). Treatments are based on two drugs: Reverse Transcriptase Inhibitors (RTI) and Protease Inhibitors (PI). The behavioral policy chooses actions from $A_b$ = {small RTI & high PI, high RTI & small PI, small RTI & PI, and high RTI & PI}; while the target policy chooses actions from a different action space, $A_2$ = {high RTI, high PI, high both, and no treatment}. Notably the small dosages from $A_b$ are replaced with zero. As a result 3 out of the 4 discrete actions possible under the new policy are not observed in the historical dataset by the behavioral policy. The reward is observed at every time step based on the number of free virus particles and the HIV-specific T cells. Thus, the return $G$ is the sum of these rewards over 200 steps, and the policy value is the average return over the initial patient population. We include 2500 trajectories in the historical dataset, $D_b$, and only 500 trajectories in the new dataset $D_2$ under $\\pi_2$, considering that in real world scenarios, new policies are likely first tested on a smaller scale compared to the behavioral policies which have been in practice for a long time.\nManaging Sepsis for ICU Patients Sepsis is a life-threatening organ dysfunction in response to infections and one of the leading causes of mortality in the United States (Liu et al., 2014). We use the sepsis treatment model for ICU patients by Oberst and Sontag (2019), following the settings in Namkoong et al. (2020). The patient's state is represented by a binary indicator for diabetes, and four vital signs (i.e., heart rate, blood pressure, oxygen concentration, glucose level) that take values in a subset of {very_high, high, normal, low, very_low}, leading to the size of the state space = 1440. The behavioral policy chooses from the following two binary treatment options, {antibiotics, mechanical ventilation}, leading to |$A_b$| = 4, modeled after Gao et al. (2024). The target policy has an additional option of vasopressors based on the original implementation by Oberst and Sontag (2019), thus making the action space larger, |$A_2$| = 8. The full horizon length is 20 and at each time step, the reward is either 0, +1 (if the patient is successfully discharged from the hospital), or -1 (if the patient dies). $D_b$ has 5000, and $D_2$ has 500 trajectories. More details about both simulators are in the supplement due to space."}, {"title": "6.1.2 Baselines", "content": "We compare to the following baselines. i) Online model-based RL: Prior work has used the online short-horizon data to estimate the target value $V_{H,M_2}$ (Tran et al., 2024; Shi et al., 2023). Here we estimate the transition models to estimate the target policy's"}, {"title": "6.1.3 Results", "content": "We first specify how we define ground truth long-term policy value of the target policy. In HIV the decision process is deterministic, and we compute $V_{H,M_2}$ by averaging over simulating $\\pi_2$ for the full horizon in each initial state. In Sepsis the decision process is stochastic, and we define the ground truth $V_{H,M_2}$ as a Monte Carlo average of 5000 simulation rollouts.\nTable 1 shows the estimation errors $V_{\\pi_2,\\pi_b} - V_{H,M_2}$ made by our proposed estimates and the baselines in the HIV and sepsis domains. Encouragingly, we find that at a short horizon of $h = 20$ in HIV (versus the full horizon of $H = 200$) and $h = 2$ in sepsis (versus the full"}, {"title": "7 DISCUSSION", "content": "When the short-horizon length is not necessarily bound by logistical constraints (e.g., clinical trials are only scheduled to run for one week, but there may be opportunities to continue them for longer duration), practitioners may consider different observation horizons to balance practical trade-offs between more signal versus increased observation cost. Other future direction would be to combine our proposed methods with active learning to guide the design of a behavioral policy such that the data collected using this behavioral policy can improve efficiency of evaluating the long-term value of an unknown target policy."}, {"title": "8 APPENDIX: THEORY", "content": ""}, {"title": "8.1 Under Assumption 1, $V_{\\pi_2,\\pi_b} = V_{\\pi_2}$", "content": "Proof.\n$\\begin{aligned}V_{\\pi_2,\\pi_b} &= E_{(s_0,...,r_h) \\sim \\pi_2, M_2} [\\sum_{j=1}^h r_j + E_{(s_{h+1},...,s_H,r_H) \\sim \\pi_b, M_b|(s_0,......,r_h)} [\\sum_{j'=h+1}^H r_{j'}] ]\\\\&= E_{(s_0,...,r_h) \\sim \\pi_2, M_2} [\\sum_{j=1}^h r_j + E_{(s_{h+1},...,s_H,r_H) \\sim \\pi_2, M_2|(s_0,......,r_h)} [\\sum_{j'=h+1}^H r_{j'}] ]\\\\&= V_{\\pi_2}\\end{aligned}$"}, {"title": "8.2 Baseline Regression Estimator", "content": "The second term of the DR estimator represents the estimated value of $V_{\\pi_2"}, "using the regression estimate, and leverages the on policy data samples from the target policy $\\pi_2$. We note that in our setting there are several choices for this quantity, and below we briefly motivate our choice. Consider two possible options:\n$\\frac{1}{N_2} \\sum_{j=1}^{N_2} f(s_{0:h})a(s_{0:h}) \\approx E_{s_{0:h} \\sim \\pi_b, M_b} [f(s_{0:h})a(s_{0:h})"], "f(s_{0": "h"}, {"s_{0": "h"}, ["f (s_{0:h})a(s_{0:h})"], {"0": "h"}, {"s_{0": "h"}, {"h+1": "H"}, ["sum_{i=1}^H r(s_i)$. Then the error in the estimate for Equation 12 is only due to the finite sample approximation of the expectation, and will generally decrease as $O(\\frac{1}{\\sqrt{N_2}})$.\nWe now consider the estimate in Equation 11 which uses the density ratio (short-horizon trajectory propensity weights) $a(s_{0:h})$. Let $a^*$ be the true (unknown) density ratio and a be the density ratio estimated using the historical data and on policy short horizon data:\n$\\frac{1}{N_2} \\sum_{j=1}^{N_2} f(s_{0:h})a(s_{0:h}) = \\frac{1}{N_2} \\sum_{j=1}^{N_2} (f(s_{0:h})a(s_{0:h}) - (f(s_{0:h})a^* (s_{0:h}) + (f(s_{0:h})a^* (s_{0:h}))$\t\t(13)\n$\\frac{1}{N_2} \\sum_{j=1}^{N_2} f(s_{0:h}) (a(s_{0:h}) - a^*(s_{0:h})) + (f(s_{0:h})a^*(s_{0:h})$\t\t(14)\n$= E_{s_{0:h} \\sim \\pi_1, M_1} [f (s_{0:h})a^*(s_{0:h})"], {"f(s_{0": "h"}, {"0": "h"}, {"s_{0": "h"}, ["f (s_{0:h})a^*(s_{0:h})"], {"f(s_{0": "h"}, {"0": "h"}, {"0": "h"}, {"f(s_{0": "h"}, {"0": "h"}, {"s_{0": "h"}, ["f (s_{0:h})a^* (s_{0:h})"], {}]