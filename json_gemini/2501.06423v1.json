{"title": "AlgoPilot: Fully Autonomous Program Synthesis Without Human-Written Programs", "authors": ["Xiaoxin Yin"], "abstract": "Program synthesis has traditionally relied on human-provided specifications, examples, or prior knowledge to generate functional algorithms. Existing methods either emulate human-written algorithms or solve specific tasks without generating reusable programmatic logic, limiting their ability to create novel algorithms. We introduce AlgoPilot, a groundbreaking approach for fully automated program synthesis without human-written programs or trajectories. AlgoPilot leverages reinforcement learning (RL) guided by a Trajectory Language Model (TLM) to synthesize algorithms from scratch. The TLM, trained on trajectories generated by random Python functions, serves as a soft constraint during the RL process, aligning generated sequences with patterns likely to represent valid algorithms. Using sorting as a test case, AlgoPilot demonstrates its ability to generate trajectories that are interpretable as classical algorithms, such as Bubble Sort, while operating without prior algorithmic knowledge. This work establishes a new paradigm for algorithm discovery and lays the groundwork for future advancements in autonomous program synthesis.", "sections": [{"title": "Introduction", "content": "Program synthesis is the task of automatically generating programs that satisfy a given specification, typically expressed as input-output examples, formal constraints, or high-level descriptions. The ultimate goal of program synthesis is to bridge the gap between human intent and executable code, enabling systems that can generate reliable and correct programs with minimal human intervention. The development of deep learning has significantly advanced program synthesis by enabling models to learn"}, {"title": "Related Work", "content": "Historically, program synthesis can be traced back to early theoretical work in the 1960s, such as Church's synthesis problem, which posed the challenge of finding a program satisfying a logical specification. Since then, the field has evolved significantly, driven by advances in computational power, formal verification techniques, and learning-based methods.\nIn the early 2000s, program synthesis gained momentum with the introduction of inductive program synthesis approaches, where programs are synthesized from input-output examples. Notable among these is the Programming by Example (PBE) paradigm, which allows users to specify program behavior through examples rather than formal specifications. Tools such as FlashFill by Gulwani et al. [10] demonstrated the practical feasibility of program synthesis in spreadsheet automation, enabling non-programmers to perform complex tasks via example-driven specifications. Around the same time, SKETCH [11] introduced a counterexample-guided inductive synthesis (CEGIS) approach, which iteratively refines candidate programs based on counterexamples generated by a verifier. Additionally, Angelic Programming [12] explored angelic non-determinism to simplify program search by focusing on high-level intent. These pioneering efforts laid the foundation for modern approaches to program synthesis, setting the stage for incorporating machine learning, neural networks, and reinforcement learning techniques in subsequent years.\nIn recent years, deep learning has significantly advanced program synthesis by enabling models to learn complex program structures and semantics directly from data. Traditional search-based and symbolic methods often struggle with large search spaces, while neural approaches offer a more scalable and flexible alternative. Deep learning models, particularly sequence-to-sequence architectures and transformers, have been successfully applied to generate programs from natural language descriptions, input-output examples, or partial specifications [1, 2]. For instance, DeepCoder by Balog et al. [2] demonstrated the potential of neural-guided search by predicting program sketches and efficiently guiding symbolic solvers. Similarly, RobustFill by Devlin et al. [1] employed sequence-to-sequence models to synthesize string transformation programs from input-output pairs, showcasing the capabilities of deep neural networks in capturing program patterns. The advent of large-scale pre-trained language models, such as OpenAI's Codex [13], has further pushed the boundaries by generating executable code from natural language prompts. These advancements illustrate the growing synergy between deep learning techniques and program synthesis, opening up"}, {"title": "AlgoPilot: Automated Learning of Algorithm without Human Help", "content": "In this section we present AlgoPilot, the first approach that can learn to create an algorithm purely on its own, without human-created algorithms or trajectories. AlgoPilot uses reinforcement learning to learn to accomplish a algorithmic task. In this study we use sorting with double-loop as the example, and the approach can be applied to many other types of algorithms.\nComparing with existing methods that learns from trajectories of human-written algorithms [3] [4] [5], we generate many random python functions with double-loops, and train a language model to predict the next token in a trajectory of a randomly generated python function. The prediction of this language model becomes a soft constraint for its reinforcement learning process, which guides the RL model to generate trajectories that can likely be generated by a real algorithm. This enables AlgoPilot to train a model which generates trajectories that are highly similar to those of a real algorithm."}, {"title": "Learning to Sort with Reinforcement Learning", "content": "Before going into the details of AlgoPilot, we first explain our setting of learning to sort with reinforcement learning. We use a Sorting Environment to manage the state and dynamics of the sorting task. Each episode begins with generation of a random array of integers with a specified length (6 to 14). The agent interacts with the environment through a series of actions, which includes actions like \"Compare\" and \"Swap\", along with tokens representing various comparison outcomes and list indices.\nWhen the agent performs a Compare action, it selects two indices from the list to compare their values, receiving feedback on whether the first is less than, equal to, or greater than the second. If the agent chooses to Swap two elements, the environment swaps them in the underlying array, and gives a reward or penalty based on whether the two elements are swapped into the right order. The environment also imposes a maximum number of steps per episode to encourage efficiency and prevent endless interactions."}, {"title": "Reinforcement Learning Agent", "content": "We use a simple transformer model for this reinforcement learning task, which predicts actions within the sorting environment. Unless otherwise specified, our default transformer has 4 layers, 8 heads, and a hidden dimension of 192. Sinusoidal positional vectors are used for the position embeddings. We use the TransformerEncoderLayer of Pytorch, and Relu activation function.\nOur reinforcement learning model is very different from many existing approaches such as Neural Turing Machines (NTMs) [6], Differentiable Neural Computers (DNCs) [7], and AlphaDev [8]. The above models observe the whole input array simultaneously, which enables them to directly select the smallest element or sorting the array. In contrast, our transformer-based agent can only use \"Compare\" operations to learn about the underlying array, which is consistent with most sorting algorithms including Selection Sort, Bubble Sort, QuickSort and Merge Sort.\nPlease note that although our agent can learn to produce a sequence of \"Compare\u201d and \"Swap\" actions to sort an array, such sequences seldom follow any algorithmic patterns, and it is difficult to create an algorithm based on such sequences."}, {"title": "Experiment Results", "content": "Before going into more details of AlgoPilot, we first present our experiment results of our simple reinforcement learning agent in this sorting environment. All our experiments were done using a computer with a NVIDIA A6000 GPU, an Intel i7-12700K CPU, with Ubuntu 18.04 and Pytorch 2.0.0.\nThe sorting environment is configured as follows. The reward of sorting success is 0.5, which uses a gamma of 0.99 (i.e., it barely decays over the steps). All other rewards have a gamma of 0.7, which decays quickly over several steps. Each step has a reward of -0.3, and each swap that moves a smaller element to the front has a reward of 1.0.\nWe study random arrays of different sizes (from 6 to 14, step 2). For each array size, we allow the model to perform at most 3\u00b7 array_size\u00b2 operations. If the array is sorted before this limit on #operations is hit, we say the model successfully sorts this array.\nWe use an Epsilon-greedy policy for exploration-exploitation trade-off. Epsilon is set to 0.5 at the beginning, and will gradually drop to 0.05, which decays by a factor of e every 1000 episodes. We run 50,000 episodes in each test. Learning rate is set to 1e-4.\nWe focus on two metrics for our reinforcement learning model: Success rate and Number of operations (both Compare and Swap)."}, {"title": "Random Function Generator", "content": "In order to train a language model for trajectories (which are sequences of operations in our study) of programs, we need to create a random function generator which generates random functions of a particular type. In this study we focus on functions with double-loops. With help from ChatGPT-01, we create a random function generator that can generate a python function with a double-loop, as shown in Algorithm 1 and Algorithm 2."}, {"title": "Trajectory Language Model (TLM)", "content": "Given a sequence like \"len16 Compare 0 1 more Compare 1 1 less Swap Compare 2 1 more Swap Compare 3 1 more ...\", we want to train a language model to predict the probability of seeing each token given the prefix. Since the vocabulary is very small (with 36 different tokens), we prefer a medium-sized language model. We choose QWen-0.5B [14], which has the best benchmark results among all models with < 1B parameters on the Hugging Face Open LLM Leaderboard2."}, {"title": "Guided Reinforcement Learning", "content": "In this subsection we will describe how AlgoPilot uses the Trajectory Language Model (TLM) to guide the training of its reinforcement learning model described in Section 3.1. As shown in Section 3.1, although the model can learn to sort an array using Compare and Swap operations, the trajectory of such operations does not follow any algorithmic pattern. Therefore, we would like to add an additional reward (or penalty) to the reinforcement learning environment according to the TLM. If an operation has high probability according to the TLM, a relatively high reward should be given to agent of reinforcement learning. If an operation has low probability, a low or negative reward should be given.\nHere we revisit the reward settings of the sorting environment. The reward of sorting success is 0.5, which uses a gamma of 0.99 (i.e., it barely decays over the steps). All other rewards have a gamma of 0.7, which decays quickly over several steps. Each"}, {"title": "Discussions and Future Work", "content": "Let us revisit the procedure of AlgoPilot in creating an algorithm for using a double-loop to sort an array:\n1. Random Function Generator\nUse a random function generator to generate millions of random python function with a double-loop, and randomly add the two operations \"Compare\u201d and \u201cSwap\u201d.\n2. Trajectory Language Model (TLM)\nTrain a language model using the trajectories of the random functions, which can predict the probability of observing the next token after a prefix.\n3. Reinforcement Learning\nUse reinforcement learning to train a transformer model for sorting, using only Compare and Swap operations, with the environment only returning a simple feedback for each operation (greater, equal, less for Compare and sorted for Swap).\n4. Guided Reinforcement Learning\nUse the Trajectory Language Model (TLM) to enhance the reinforcement learning process by adding an additional reward when the next operation has a high probability according to the TLM. This guides the model to learn to generate a trajectory that can be produced by an algorithm.\n5. Algorithm Creation\nUse an LLM (such as GPT-40-mini) to generate an algorithm or a python function based on the trajectory.\nWe can see that the above procedure does not use any prior knowledge of a sorting algorithm, except that in Step 5 (Algorithm Creation) where the LLM is probably aware of all popular algorithms. Although it often looks obvious and trivial to create"}, {"title": "Appendix A Expected Numbers of Compare and Swap operations of Quicksort", "content": "We consider the classic (textbook) QuickSort algorithm with the following characteristics:\n1. Pivot Selection: Choose one pivot uniformly at random from the current subarray.\n2. Partitioning Scheme: Utilize the Lomuto partition scheme:\n\u2022 Compare each of the other $n-1$ elements to the pivot.\n\u2022 Each time an element smaller than the pivot is found, increment a pointer and perform a swap.\n\u2022 Finally, perform one additional swap to place the pivot in its correct position at the boundary.\n3. Recursion: Recursively sort the subarrays to the left and right of the pivot.\nWe aim to count:\n\u2022 #Comparisons: The number of times array elements are compared against the pivot.\n\u2022 #Swaps: The number of times two elements in memory are exchanged.\nBoth counts are treated as random variables due to the random selection of pivots."}, {"title": "Expected Number of Comparisons", "content": "Let $C(n)$ denote the (random) number of comparisons QuickSort makes on an array of size n. The recurrence relation for the expected number of comparisons is:\n$C(n) = (n - 1) + C(k) + C(n \u2212 k \u2212 1)$,\nwhere:\n\u2022 $(n - 1)$ is the number of comparisons in the current partition.\n\u2022 k is the rank of the chosen pivot (0 \u2264 k \u2264 n \u2212 1), selected uniformly at random.\n\u2022 $C(k)$ and $C(n \u2212 k \u2212 1)$ are the recursive calls on the resulting subarrays.\nTaking expectations on both sides:"}, {"title": "Expected Number of Swaps", "content": "Let $S(n)$ denote the (random) number of swaps performed using the Lomuto partition scheme. The expected number of swaps in a single partition step is approximately:\n$\\frac{n-1}{2}$+\\frac{n+1}{2} =1$\\nConsidering recursion, the recurrence relation is:\n$S(n) = S(k) + S(n \u2212 k \u2212 1) + (swaps in current partition)$,\nTaking expectations:\n$\\frac{1}{n}E[S(n)] = \\sum{E[S(k)] + E[S(n \u2212 k \u2212 1)]} + \\frac{n+1}{2}$"}, {"title": "Final Exact Formulas", "content": "Expected Number of Comparisons:\n$E[\\#Comparisons] = 2(n + 1)Hn \u2013 2n$\n\u2022 Expected Number of Swaps:\n$E[\\#Swaps] = (n + 1)Hn \u2013 2n$\nWhere $H_n$ is the n-th harmonic number, defined as:\n$Hn=1+\\frac{1}{2}+\\frac{1}{3} + ... +\\frac{1}{n}$\nAs n\u2192\u221e, we have:\n$Hn=ln n+y,$\nwhere y is Euler's constant. Therefore, the asymptotic behaviors are:\n$E[\\#Comparisons] ~ 2n ln n,$\n$E[\\#Swaps] ~n ln n$."}, {"title": "Caveats and Variations", "content": "1. Partition Scheme: The exact constants differ if using Hoare's partition scheme instead of Lomuto's. Hoare's scheme generally results in fewer swaps.\n2. Pivot Selection Strategy: Different pivot selection strategies, such as median-of-three, can alter the constants and lower-order terms in the expected counts, although the overall (nlogn) scaling remains unchanged.\n3. Worst-Case Behavior: Deterministic worst-case scenarios (e.g., always selecting the largest element as pivot) can lead to O(n\u00b2) comparisons and swaps. However, this is avoided in expectation with random pivot selection."}, {"title": "LLM prompt for algorithm generation from a trajectory", "content": "Given a trajectory generated by our model in guided reinforcement learning, we use the following prompt to let GPT generate a python function that is likely to generate the trajectory (with small discrepancies allowed):\nBelow is the sequence of actions of a python function that operates on an array of integers named \"a\", and its will receive feedbacks from an environment after it takes actions. Action \"Compare i j\" means the python function compares a[i] and a[j]. If a[i] is greater than a[j], the environment will return \"more\". If a[i] is smaller than a[j], it will return \"less\". Otherwise it will return \"equal\". Action \"Swap\""}]}