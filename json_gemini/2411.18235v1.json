{"title": "Certified Training with Branch-and-Bound: A Case Study on Lyapunov-stable Neural Control", "authors": ["Zhouxing Shi", "Cho-Jui Hsieh", "Huan Zhang"], "abstract": "We study the problem of learning Lyapunov-stable neural controllers which provably satisfy the Lyapunov asymptotic stability condition within a region-of-attraction. Compared to previous works which commonly used counterexample guided training on this task, we develop a new and generally formulated certified training framework named CT-BaB, and we optimize for differentiable verified bounds, to produce verification-friendly models. In order to handle the relatively large region-of-interest, we propose a novel framework of training-time branch-and-bound to dynamically maintain a training dataset of subregions throughout training, such that the hardest subregions are iteratively split into smaller ones whose verified bounds can be computed more tightly to ease the training. We demonstrate that our new training framework can produce models which can be more efficiently verified at test time. On the largest 2D quadrotor dynamical system, verification for our model is more than 5X faster compared to the baseline, while our size of region-of-attraction is 16X larger than the baseline.", "sections": [{"title": "1 Introduction", "content": "Deep learning techniques with neural networks (NNs) have greatly advanced abundant domains in recent years. Despite the impressive capability of NNs, it remains challenging to obtain provable guarantees on the behaviors of NNs, which is critical for the trustworthy deployment of NNs especially in safety-critical domains. One area of particular concern is safe control for robotic systems with NN-based controllers (Chang et al., 2019; Dai et al., 2021; Wu et al., 2023; Yang et al., 2024). There are many desirable properties in safe control, such as reachability w.r.t. target and avoid sets (Althoff & Kochdumper, 2016; Bansal et al., 2017; Dutta et al., 2019; Everett et al., 2021; Wang et al., 2023b), forward invariance (Ames et al., 2016; Taylor et al., 2020; Zhao et al., 2021; Wang et al., 2023a; Huang et al., 2023), stability (Lyapunov, 1992; Chang et al., 2019; Dai et al., 2021; Wu et al., 2023; Yang et al., 2024), etc.\nIn particular, we focus on the Lyapunov (Lyapunov, 1992) asymptotic stability of NN-based controllers in discrete-time nonlinear dynamical systems (Wu et al., 2023; Yang et al., 2024), where we aim to train and verify asymptotically Lyapunov-stable NN-based controllers. It involves training a controller while also finding a Lyapunov function which intuitively characterizes the energy of input states in the dynamical system, where the global minima of the Lyapunov function is at an equilibrium state. If it can be guaranteed that for any state within a region-of-attraction (ROA), the controller always makes the system evolve towards states with lower Lyapunov function values, then it implies that starting from any state within the ROA, the controller can always make the system converge towards the equilibrium state and thus the stability can be guaranteed. Such stability requirements have been formulated as the Lyapunov condition in the literature. This guarantee is for an infinite time horizon and implies a convergence towards the equilibrium, and thus it is relatively stronger than reachability or forward invariance guarantees.\nPrevious works (Wu et al., 2023; Yang et al., 2024) typically used a counterexample-guided procedure that basically tries to find concrete inputs which violate the Lyapunov condition and then train models on counterexamples. After the training, the Lyapunov condition is verified by a formal verifier for NNs (Zhang et al., 2018; Xu et al., 2020, 2021; Wang et al., 2021a; Zhang et al., 2022; Shi et al., 2024). However, the training process has very limited consideration on the computation of verification which is typically achieved by computing verified output bounds given an input region. Thereby, their models are often not sufficiently \u201cverification-friendly\u201d, and the verification can be challenging and take a long time after training (Yang et al., 2024).\nIn this paper, we propose to consider the computation of verification during the training, for the first time on the problem of learning Lyapunov-stable neural controllers. To do this, we optimize for verified bounds on subregions of inputs"}, {"title": "2 Related Work", "content": "Learning Lyapunov-stable neural controllers. On the problem of learning (asymptotically) Lyapunov-stable neural controllers, compared to methods using linear quadratic regulator (LQR) or sum-of-squares (SOS) (Parrilo, 2000; Tedrake et al., 2010; Majumdar et al., 2013; Yang et al., 2023; Dai & Permenter, 2023) to synthesize linear or polynomial controllers with Lyapunov stability guarantees (Lyapunov, 1992), NN-based controllers have recently shown great potential in scaling to more complicated systems with larger region-of-attraction. Some works used sampled data points to synthesize empirically stable neural controllers (Jin et al., 2020; Sun & Wu, 2021; Dawson et al., 2022; Liu et al., 2023) but they did not provide formal guarantees. Among them, although Jin et al. (2020) theoretically considered verification, they assumed an existence of some Lipschitz constant which was not actually computed, and they only evaluated a finite number of data points without a formal verification.\nTo learn neural controllers with formal guarantees, many previous works used a Counter Example Guided Inductive Synthesis (CEGIS) framework by iteratively searching for counterexamples which violate the Lyapunov condition and then optimizing their models using the counterexamples, where counterexamples are generated by Satisfiable Modulo Theories (SMT) solvers (Gao et al., 2013; De Moura & Bj\u00f8rner, 2008; Chang et al., 2019; Abate et al., 2020), Mixed Integer Programming solvers (Dai et al., 2021; Chen et al., 2021; Wu et al., 2023), or projected gradient descent (PGD) (Madry et al., 2018; Wu et al., 2023; Yang et al., 2024). Among these works, Wu et al. (2023) has also leveraged a formal verifier (Xu et al., 2020) only to guarantee that the Lyapunov function is positive definite (which can also be achieved by construction as done in Yang et al. (2024)) but not other more challenging parts of the Lyapunov condition; Yang et al. (2024) used \u03b1,\u03b2-CROWN (Zhang et al., 2018; Xu et al., 2020, 2021; Wang et al., 2021a; Zhang et al., 2022; Shi et al., 2024) to verify trained models without using verified bounds for training. In contrast to those previous works, we propose to conduct certified training by optimizing for differentiable verified bounds at training time, where"}, {"title": "3 Methodology", "content": "3.1 Problem Settings\nCertified training problem. Suppose the input region-of-interest of the problem is defined by B \u2286 Rd for input dimension d, and in particular, we assume B is an axis-aligned bounding box B = {x | b < x < b, x \u2208 Rd} with boundary defined by b, b \u2208 Rd (we use \u201c<\u201d for vectors to denote that the \u201c<\u201d relation holds for all the dimensions in the vectors). We define a model (or a computational graph) ge : Rd \u2192 R parameterized by 0, where go generally consists of one or more NNs and also additional operators which define the properties we want to certify (such as the Lyapunov condition in this work). The goal of certified training is to optimize for parameters such that the following can be provably verified (we may omit 0 in the remaining part of the paper):\n$\\forall x \\in B, g_\\theta(x) \\le 0,$\n(1)\nwhere any go (x) > 0 can be viewed as a violation. Unlike previous certified training works (Gowal et al., 2018; Mirman et al., 2018; Zhang et al., 2020; M\u00fcller et al., 2022) which only considered certified adversarial robustness guarantees on small local regions as {x : ||x - x0|| \u2264 6} around a finite number of examples x0 \u2208 B in the dataset, we require Eq. (1) to be fully certified for any x \u2208 \u0412.\nNeural network verifiers typically verify Eq. (1) by computing a provable upper bound \u011f such that \u011f \u2265 g(x) (\u2200x \u2208 B) provably holds, and Eq. (1) is considered as verified if \u011f \u2264 0. For models trained without certified training, the upper bound computed by verifiers is usually loose, or it requires a significant amount of time to further optimize the bounds or gradually tighten the bounds by branch-and-bound at test time. Certified training essentially optimizes for objectives"}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\nDynamical systems. We demonstrate our new certified training work on learning Lyapunov-stable neural controllers with state feedback in several nonlinear discrete-time dynamical systems following Wu et al. (2023); Yang et al. (2024), as listed in Table 1: Inverted pendulum is about swinging up the pendulum to the upright equilibrium; Path tracking is about tracking a path for a planar vehicle; and 2D quadrotor is about hovering a quadrotor at the equilibrium state. For inverted pendulum and path tracking, there are two different limits on the maximum allowed torque of the controller, where the setting is more challenging with a smaller torque limit. Detailed definition of the system dynamics (f in Eq. (2)) is available in existing works: Wu et al. (2023) for inverted pendulum and path tracking, and Tedrake (2009) for 2D quadrotor."}, {"title": "5 Conclusion", "content": "To conclude, we propose a new certified training framework for training verification-friendly models where a relatively global guarantee can be verified for an entire region-of-interest in the input space. We maintain a dynamic dataset of subregions which cover the region-of-interest, and we split hard examples into smaller subregions throughout the training, to ease the training with tighter verified bounds. We demonstrate our new certified training framework on the problem of learning and verifying Lyapunov-stable neural controllers. We show that our new method produces more verification-friendly models which can be more efficiently verified at test time while the region-of-attraction also becomes much larger compared to the state-of-the-art baseline.\nA limitation of this work is that only low-dimensional dynamical systems have been considered, which is also a common limitation of previous works on this Lyapunov problem (Chang et al., 2019; Wu et al., 2023; Yang et al., 2024). Future works may consider scaling up our method to higher-dimensional systems. Since splitting regions on the input space can become less efficient if the dimension of the input space significantly increases, future works may consider applying splits on the intermediate bounds of activation functions (potentially with sparsity), which has been commonly used in state-of-the-art NN verifiers (mentioned in Section 2) for verifying trained models on high-dimensional tasks such as image classification.\nAlthough our new certified training framework is generally formulated, we have only focused on demonstrating the training framework on Lyapunov asymptotic stability. Given the generality of our new framework, it has the potential to enable broader applications, such as other safety properties including reachability and forward invariance mentioned in Section 2, control systems with more complicated settings such as output feedback systems, or even applications beyond control. These will be interesting directions for future work."}]}