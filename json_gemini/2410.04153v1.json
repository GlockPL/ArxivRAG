{"title": "NEURO-SYMBOLIC Entity AliGNMENT VIA VARIA- VARIATIONAL INFERENCE", "authors": ["Shengyuan Chen", "Qinggang Zhang", "Junnan Dong", "Wen Hua", "Jiannong Cao", "Xiao Huang"], "abstract": "Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying equivalent entity pairs. Existing methods can be categorized into symbolic and neural models. Symbolic models, while precise, struggle with substructure het- erogeneity and sparsity, whereas neural models, although effective, generally lack interpretability and cannot handle uncertainty. We propose NeuSymEA, a proba- bilistic neuro-symbolic framework that combines the strengths of both methods. NeuSymEA models the joint probability of all possible pairs' truth scores in a Markov random field, regulated by a set of rules, and optimizes it with the varia- tional EM algorithm. In the E-step, a neural model parameterizes the truth score distributions and infers missing alignments. In the M-step, the rule weights are updated based on the observed and inferred alignments. To facilitate interpretabil- ity, we further design a path-ranking-based explainer upon this framework that generates supporting rules for the inferred alignments. Experiments on bench- marks demonstrate that NeuSymEA not only significantly outperforms baselines in terms of effectiveness and robustness, but also provides interpretable results.", "sections": [{"title": "INTRODUCTION", "content": "Knowledge graphs (KGs) are crucial for organizing structured knowledge about entities and their relationships, enhancing search capabilities across various applications. They are widely used in question-answering systems (Zhang et al., 2023a; Dong et al., 2023), recommendation sys- tems (Catherine & Cohen, 2016), social network analysis (Tang et al., 2008), Natural Language Processing (Weikum & Theobald, 2010), etc.. Despite their utility, real-world KGs often face issues like incompleteness, domain specificity, and language constraints, which hinder their effectiveness in cross-disciplinary or multilingual contexts. To address these issues, entity alignment (EA) aims to merge disparate KGs into a unified, comprehensive knowledge base by identifying and linking equivalent entities across different KGs. For example, aligning entities between a biomedical KG and a pharmaceutical KG allows for mining cross-discipline relationships through the aligned en- tities, such as identifying the same drugs and their effects on different diseases to enhance drug repurposing efforts. This alignment enables more nuanced exploration and interrogation of inter- connected data, providing richer insights into how entities function across multiple domains.\nTo perform entity alignment, existing methods can be broadly categorized into symbolic models and neural models. Symbolic models (Suchanek et al., 2012; Jim\u00e9nez-Ruiz & Cuenca Grau, 2011; Qi et al., 2021) provide interpretable and precise inference by mining ground rules, but they strug- gle with aligning long-tail entities, especially those without aligned neighbors. In such cases, the lack of supporting rules leads to low recall. Conversely, neural models, such as translation mod- els (Chen et al., 2017; Sun et al., 2018) and Graph Convolutional Networks (GCNs) (Mao et al., 2020; 2021; Wang et al., 2018; Mao et al., 2022; Li et al., 2024b;a), excel in recalling similar enti- ties by embedding them in a continuous space, yet they often fail to distinguish entities with similar representations, causing a drop in precision as the entity pool grows. Neuro-symbolic models aim to combine the strengths of both approaches, offering the interpretability and precision of symbolic models alongside the high recall capabilities of neural models."}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 PROBLEM STATEMENT", "content": "A knowledge graph G comprises a set of entities E, a set of relations R, and a set of relation triples T where each triple (ei, rk, ej) \u2208 T represents a directional relationship between its head entity and tail entity. Given two KGs G = {E,R,T}, G' = {E',R',T'}, and a set of observed aligned entity pairs O = {(ei, ei)|e\u00a1 \u2208 E, e \u2208 E'}=1, the goal of entity alignment is to infer the missing alignments by reasoning with the existing alignments. This problem can be formulated in a proba- bilistic way: each pair (e, e'), e \u2208 E, e' \u2208 E' is associated with a binary indicator variable v(e,e'). V(e,e') = 1 means (e, e') is an aligned pair, and v(e,e') = 0 otherwise. Given some observed align- ments vo = {v(e,e') = 1}(e,e')\u2208O, we aim to predict the labels of the remaining hidden entity pairs H = E \u00d7 E'\\O, i.e., vh = {v(e,e')}(e,e')\u2208H."}, {"title": "2.2 SYMBOLIC REASONING FOR ENTITY ALIGNMENT", "content": "Given an aligned pair (ej, e'j), a new aligned pair (ei, e\u015b) can be inferred with confidence score Wp,p' if they are each connected to the existing pair via a relational path p and p' respectively, formally:\n$W_{p,p'} : (e_j = e'_j) \\land p(e_i, e_j) \\land p' (e'_i, e'_j) \\Rightarrow (e_i = e'_i),$\n(1)"}, {"title": "3 NEURO-SYMBOLIC REASONING FRAMEWORK FOR ENTITY ALIGNMENT", "content": ""}, {"title": "3.1 VARIATIONAL EM", "content": "Given a set of observed labels vo, our goal is to maximize the log-likelihood of these labels, i.e., log pw (vo). Directly optimizing this objective is intractable because it requires computing an in- tegral over all the hidden variables. Instead, we optimize the evidence lower bound (ELBO) of the log-likelihood as follows:\n$p_\\omega(v_O) \\geq E_{q(v_H)} [log p_\\omega(v_O, v_H) - log q(v_H)] = ELBO(q, v_O; w),$\n(3)\nhere, q(v) is a variational distribution of the hidden variables v\u04a3. This inequality holds for all q because pw (vo) = ELBO(q, vo; w) + $D_{KL}(q(v_H)||p_w(v_H | v_O))$, where $D_{KL}(q(v_H)||p_w(v_H | v_O)) \\geq 0$ is the KL-divergence between q(v\u00ed) and pw(VH | vo). Under this framework, the log- likelihood pw (vo) can be optimized using an EM algorithm: during the E-step, we fix w and update the variational distribution q; during the M-step, we update w to maximize the log-likelihood of all the entity pairs, i.e., $E_{q(v_H)} [log p_w(v_O, v_H)]$, as illustrated in Figure 1.\nExplicitly representing the variational distribution q is parameter intensive, which requires \u2248 |E||E' variables because the observed pairs are very sparse. To this end, we parameterize q with a neural model as qe, with @ being the parameters of the neural model."}, {"title": "3.2 E-STEP: INFERENCE", "content": "In this step, we fix w and update qe to minimize the KL divergence DKL. Directly minimizing DKL is intractable, as it involves computing the entropy of qe. Therefore, we follow Qu & Tang (2019) and optimize the reverse KL divergence of qe and pw, leading to the following objective:\n$\\Phi_{v_H,\\theta} = \\sum_{(e,e') \\in H} E_{p_\\omega(v_{(e,e')} | v_O)} q_\\theta(v_H).$\n(4)\nTo optimize this objective, we first use the symbolic model with weighted rules to predict pw (v(e,e') |\nvo) for each (e, e') \u2208 H. If pw (v(e,e') | vo) > \u03b4, where d is a threshold, we treat this entity pair as a positive label; otherwise, we regard the pair as a negative pair that can be selected during negative sampling process of the neural model.\nThe observed labels can also be used as training data for supervised optimization. The objective is:\n$\\Phi_{v_O,\\theta} = \\sum_{(e,e') \\in O} log q_\\theta(v_{(e,e')} = 1).$\n(5)\nThe final objective for qe is obtained by combining these two objectives: \u0444\u04e9 = \u03a6\u03c5\u03bc,\u03b8 + \u03a6\u03c5\u03bf,\u03b8\u00b7"}, {"title": "3.3 M-STEP: RULE WEIGHT UPDATE", "content": "In this step, we fix qe and update the rule weight w to maximize ELBO(q, vo; w). Since the right term of the ELBO in equation 3 is constant when qe is fixed, the objective is equivalent to maximiz- ing the left term $E_{q_\\theta(v_H)} [log p_w(v_O, v_H)]$, which is the log-likelihood function.\nSpecifically, we start by predicting the labels of hidden variables using the current neural model. For each (e, e') \u2208 H, we predict the labels \u00db(e,e')(0) and obtain the prediction set \ufffdH(0) =\n{\u00db(e,e') (0)} (e,e')\u2208H. In this way, maximizing the likelihood practically becomes maximizing the following objective:\n$\\phi_\\omega = log p_w (v_O, \\tilde{v}_H(\\theta)).$\n(6)\nTo obtain the pseudo-label \u00fb(e,e') using qe, we employ the trained neural model to compute the matching score of any entity pair (e, e') \u2208 H. However, this strategy can easily introduce false positives into the pseudo-label set especially when the number of entities is large. To mitigate this, we consider one-to-one matching to sift only the most confident pairs. Practically, we first sort all pairs by their confidence score, then we annotate the pairs as positive following the order of the confidence. If a pair contains an entity observed in the annotated pairs, then this pair is skipped. This simple greedy strategy significantly reduces the amount of false positives."}, {"title": "4 OPTIMIZATION AND INFERENCE", "content": ""}, {"title": "4.1 EFFICIENT OPTIMIZATION VIA LOGICAL DEDUCTION", "content": "Inference and learning with logic rules of length L can be computationally intensive, as the search space for paths grows exponentially with increasing L. To enhance reasoning efficiency, we decom- pose a rule in equation 1 using logic deduction, inspired by Cheng et al. (2023) in KG completion:\n$W_{p,p'} : (e_j = e'_j) \\land (\\bigwedge_{k=1}^L r_k(e_{k-1}, e_k)) \\land (\\bigwedge_{k=1}^L r'_k(e'_{k-1}, e'_k)) \\Rightarrow (e_i = e'_i).$\n(7)\nHere Ak=1 rk (ek-1, ek) represents the path formed by r1, r2, ..., r\u2081 connecting ei to ej with eo = Ci and ek = ej. This can be reorganized as a series of single-step logic reasoning:\n$W_{p,p'} : (e_j = e'_j) \\land (\\bigwedge_{k=1}^L \\Delta r_k(e_{k-1}, e_k)) \\Rightarrow (e_i = e'_i).$\n(8)\nIn this way, each logic rule of length L can be viewed as a deductive combination of L short rules of length 1. At each step, following Suchanek et al. (2012), we perform one-step inference to update"}, {"title": "4.2 INFERENCE WITH INTERPRETABILITY", "content": "To predict new alignments, there are two approaches: using the symbolic model or the neural model. The symbolic model infers alignment probabilities with the optimized weights w. Due to scalability concerns, symbolic methods generally adopt a lazy inference strategy that only preserves the confi- dent pairs implied by the neighbor structure during inference. On the other hand, the neural model computes similarity scores for all entity pairs (e, e') \u2208 H using the learned parameters 0, generating a ranked candidate list for each entity.\nThe evaluation of these models thus differs. Symbolic models are generally evaluated by precision, recall, and F1-score for their binary outputs, while neural models are assessed using hit@k and mean reciprocal ranks (MRR) for their ranked candidate lists. Following the practices in Qi et al. (2021) and Liu et al. (2022), we unify the evaluation metrics by treating the recall metric of symbolic models as equivalent to hit@1, facilitating comparison with neural models.\nTo enhance the interpretability of predictions, we adapt the optimized symbolic model into an ex- plainer. For any given entity pair, the explainer generates a set of supporting rule path pairs that justify their alignment, each associated with a confidence score indicating its significance. The ex- plainer operates in two modes: (1) hard-anchor mode, which generates supporting paths only from prealigned pairs, and (2) soft-anchor mode, which includes paths from both prealigned and inferred pairs, providing more informative interpretations.\nBy integrating a breadth-first search algorithm (detailed in Appendix A.2), the explainer efficiently generates high-quality interpretations. For truly aligned pairs, it typically produces high-confidence interpretations, while for non-aligned pairs, the interpretations may result in an empty set (indicating no supporting evidence) or have low confidence scores."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 EXPERIMENTAL SETTINGS", "content": "In this section, we conduct experiments to answer the following questions. RQ1: Can NeuSymEA outperform existing neural, symbolic, and neuro-symbolic methods in terms of alignment perfor- mance? RQ2: Can symbolic and embedding-based methods complement each other in our frame- work? RQ3: How does the incorporated embedding-based model affect the alignment performance?"}, {"title": "5.2 RESULTS", "content": ""}, {"title": "5.2.1 \u0421\u043eMPARISON WITH BASELINES", "content": ""}, {"title": "5.2.2 EvOLUTION OF RULES AND EMBEDDINGS", "content": "We study how rules and embeddings evolve and interact with each other during the EM steps, with results shown in Figure 2. Results in the left subplot indicate that in each EM iteration, the number of rule-inferred pairs grows consistently with high precision, implying that the embedding model continuously improves the inference performance of rules. These precise pairs, in turn, enhance the performance of the neural model. As shown in the right subfigure, the MRR of the neural model converges within a few iterations."}, {"title": "5.2.3 INTERPRETATIONS BY THE EXPLAINER", "content": "We investigate the interpretations generated by the explainer on the fr-en dataset. The left subfigure of Figure 3 shows the probability density of confidence scores for supporting rules (with a maxi- mum rule length of 3) associated with entity pairs. Positive pairs are derived from the test set, while negative pairs are created by replacing one entity in each test pair with another randomly sampled entity. The distinct confidence distributions indicate that positive pairs generally have more evi- dence for alignment, which aligns with intuition. However, the probability density distribution also reveals that some positive pairs do not have high confidence scores. Upon further examination, we found that many test pairs are isolated, i.e., they lack directly aligned neighbors. Despite this, NeuSymEA successfully generates supporting rules for isolated pairs by exploiting multihop dependencies. In Table 2, we provide several examples of supporting rules and their associated confidence scores for the queried entity pairs.\nTo examine the impact of rule length on the explainer's effectiveness, the middle and right subfigures in Figure 3 show the number of supporting rules for test positive pairs as the maximum rule length increases. Compared to hard anchor mode, the explainer in soft anchor mode generates more high- quality supporting rules by leveraging inferred pairs as complementary anchor pairs, mitigating the sparsity issue. We also observe that increasing the maximum rule length leads to more high- quality rules; however, the number of high-confidence rules grows more slowly than lower- confidence rules. This can be attributed to our method for calculating confidence: the logical deduction-based approach computes a rule's confidence as the product of the confidences of its decomposed length-one sub-rules (as in Equation (11)). For example, a rule with two sub-rules, each with confidence 0.8, results in an overall confidence of 0.8 \u00d7 0.8 = 0.64. Considering this, the confidence score tends to decrease when the rule length increases, thus increasing the maximum length tends to discover supporting rules with lower scores."}, {"title": "5.2.4 ROBUSTNESS IN LOW RESOURCE SCENARIO", "content": "Figure 5 demonstrates the model performance under low-resource settings. As the percentage of training data decreases, all models experience noticeable drops in Hit@1 performance. Despite this, NeuSymEA (both NeuSymEA-L and NeuSymEA-D) exhibits remarkable robustness across all datasets, consistently outperforming other models even with limited data. Notably, with only 1% of pairs used as training data, NeuSymEA-L achieves a Hit@1 score exceeding 0.7 on fr-en, rivaling or even surpassing the performance of some state-of-the-art models trained on 20% of the data. While models such as LightEA, Dual-AMN, and BootEA show improvement with additional training data, they still fall short of NeuSymEA. In contrast, traditional models like GCNAlign"}, {"title": "6 RELATED WORK", "content": "Neuro-symbolic reasoning on knowledge graphs. Knowledge graphs (KGs) inherently contain logical rules within the relational patterns of their edges. Reasoning with these patterns is crucial for KG tasks such as KG completion (Chen et al., 2024a; Cheng et al., 2022; 2023; Zhang et al., 2024; Liu et al., 2024; Fang et al., 2023) and error detection (Zhang et al., 2022; 2023b). Neuro- symbolic methods aim to merge symbolic reasoning with neural representation learning, leveraging the precision and interpretability of symbolic approaches alongside the scalability and high recall of neural methods. In KG completion task, Guo et al. (2016) employ horn rules to regularize the learning of KG embeddings; Cheng et al. (2022) and Cheng et al. (2023) model the rule-based predictions as distributions conditioned on the input relational sequences, and parameterize these distributions using a recurrent neural network; Qu et al. (2020) and Cheng et al. (2022) enhances rule grounding by augmenting the fact set using a pre-trained KG-embedding model; Qu & Tang (2019), Zhang et al. (2020) and Chen et al. (2024a) models the joint probability of neural model and the symbolic model with a Markov random field. Despite extensive studies of neuro-symbolic reasoning in KG completion(DeLong et al., 2024; Cheng et al., 2024), they only consider single-KG structures, thus cannot be directly adopted to entity alignment which requires consideration of inter-KG structures.\nEntity alignment. Recent models have sought to combine symbolic and neural approaches for entity alignment. For instance, Qi et al. (2021) enhances probabilistic reasoning by utilizing KG embeddings, employing a KG-embedding model to measure similarities during both updating and inference processes. Liu et al. (2022) implements self-bootstrapping with pseudo-labeling in a neu- ral framework, using rules to choose confident pseudo-labels. However, it relies solely on unit-length rules, which restricts its effectiveness for long-tail entities. Furthermore, neither of these approaches offers interpretability for the inferred alignments."}, {"title": "7 CONCLUSIONS", "content": "In this work, we introduced NeuSymEA, a neuro-symbolic framework for entity alignment that integrates symbolic and neural methods within a variational EM framework. NeuSymEA addresses challenges such as substructure heterogeneity, sparsity, and uncertainty. Empirical results show it outperforms existing approaches while offering interpretable alignment predictions via a path- ranking-based explainer. This demonstrates NeuSymEA's potential to advance knowledge fusion by enabling effective and uncertainty-aware entity alignment."}]}