{"title": "Robust Real-Time Mortality Prediction in the Intensive Care Unit using Temporal Difference Learning", "authors": ["Thomas Frost", "Ken Li", "Steve Harris"], "abstract": "The task of predicting long-term patient outcomes using supervised machine learning is a challenging one, in part because of the high variance of each patient's trajectory, which can result in the model over-fitting to the training data. Temporal difference (TD) learning, a common reinforcement learning technique, may reduce variance by generalising learning to the pattern of state transitions rather than terminal outcomes. However, in healthcare this method requires several strong assumptions about patient states, and there appears to be limited literature evaluating the performance of TD learning against traditional supervised learning methods for long-term health outcome prediction tasks. In this study, we define a framework for applying TD learning to real-time irregularly sampled time series data using a Semi-Markov Reward Process. We evaluate the model framework in predicting intensive care mortality and show that TD learning under this framework can result in improved model robustness compared to standard supervised learning methods and that this robustness is maintained even when validated on external datasets. This approach may offer a more reliable method when learning to predict patient outcomes using high-variance irregular time series data.", "sections": [{"title": "1. Introduction", "content": "Patients in the Intensive Care Unit (ICU) are amongst the sickest in any hospital and often follow a complex trajectory from admission to discharge (or death). With an average ICU mortality rate between 7-19%, there is a need for accurate prediction models to help stratify patient risk during admission.\nTraditional approaches rely heavily on scoring systems such as APACHE, SOFA, and SAPS, which provide a mortality risk score from patient features collected at ICU admission. These methods have been externally validated but are cited to have area under the receiver operating characteristic curve (AUROC) scores frequently limited to the 0.70-0.79 range.\nSupervised machine learning algorithms (such as gradient boosted ensembles, artificial neural networks, and Bayesian networks) demonstrate improvement on this baseline, with AUROC scores ranging from 0.80-0.95. However, many of these studies lack high-quality external validation; are limited to one-time predictions at the point of admission (as opposed to ongoing/real-time predictions); and/or limit their mortality time-horizon to the short-term (\u226472 hours).\nWe theorise that the over-fitting of models in these tasks may be related to the significant variance of each patient's trajectory, with diminishing relevance of current features to outcomes far in the future. In reinforcement learning (the branch of machine learning tasked with optimal sequential decision-making), models face a similar challenge of attributing distant rewards under noisy trajectories to present states and actions (the so-called \"credit assignment problem\"). A common solution to this has been the use of temporal difference (TD) learning, in which the model is bootstrapped using its own predictions for future states rather than the actual observed (distant) rewards. This is demonstrated conceptually in Figure 1.\nSeveral reinforcement learning publications already exist exploring the use of TD learning in predicting mortality risk for a given set of actions in intensive care. However, to date these works focus on using TD learning to generate counterfactual predictions based on a hypothetical decision policy, making it difficult to evaluate their accuracy without real-world policy deployment. Additionally, most such studies artificially aggregate time series data into regular intervals (e.g., every four hours), with only one exception applying TD learning to irregular health data.\nIn this paper, we generate a set of models using either TD learning or supervised learning for the task of inpatient mortality prediction. The models are trained on more than 65,000 undifferentiated patients using the MIMIC-IV dataset. We describe the mathematical framework for patient states and state transitions, the CNN-LSTM base model architecture, and the training process for each model category. We report the AUROC scores for all models on both a hold-out test segment of the MIMIC-IV dataset, as well as an external validation dataset (SICdb) of 21,000 patients. We show that models trained with TD learning outperform both supervised learning and clinical score baselines for the task of long-term mortality prediction, and suffer much less overfitting when tested on an external validation dataset. We discuss our interpretations of these results and plans for further work in this area."}, {"title": "2. Methods", "content": null}, {"title": "2.1. Temporal Difference Learning and Semi-MRP Framework", "content": null}, {"title": "2.1.1. MARKOV REWARD PROCESS", "content": "The Markov Reward Process (MRP) describes a Markov chain of states, in which \"memoryless\" states transition stochastically to new states over time, and generate rewards R in the process. It is defined by the tuple {S, Pss', R, \u03b3}:\n\u2022S: The current state.\n\u2022 Pss': The state transition probability matrix from S to S', which is independent of any states prior to S."}, {"title": "2.1.2. TD LEARNING", "content": "One can train a value function V(Ot) to predict the expected sum of (discounted) rewards R collected at all future steps from a set of observations Ot as follows:\n$V(O_t) \\leftarrow E [\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}]$\nThis equation requires complete episode trajectories using Monte Carlo sampling, which is unbiased but potentially of high variance, particularly as the future trajectory becomes very long. In contrast, via mathematical induction of (1), we arrive at (2), the formula for TD learning. This equation uses bootstrapping to reduce the variance by predicting a single step in the future (at the risk of bias).\n$V(O_t) \\leftarrow E [R_{t+1} + \\gamma V(O_{t+1})]$"}, {"title": "2.1.3. SEMI-MARKOV REWARD PROCESS", "content": "One limit of MRPs is their dependence on state transitions modelled under regular discrete intervals. To allow for irregularly sampled health data, we instead use a semi-MRP (3), in which transitions are allowed to occur at variable intervals:\n$V(O_t) \\leftarrow E_{k \\sim D} [R_{t:t+k} + \\gamma^k V(O_{t+k})]$\nWe assume that k is sampled from a (potentially unknown) stochastic distribution D, with Rt:t+k representing the discounted sum of rewards captured in the interval between t and t + k."}, {"title": "2.1.4. CHOICE OF REWARD UNDER SMRP", "content": "We can now convert the formula in Equation (3) to a loss function for predicting the terminal outcome (death/discharge) of a patient. We define the following:\n\u2022 The terminal reward is patient mortality, with R = 1 for death and R = 0 for survival (i.e., successful discharge).\n\u2022 There are no interim rewards prior to the terminal state.\n\u2022 \u03b3 = 1, i.e., the predicted risk of death is the averaged risk from all immediate future states.\nThe above conditions allow us to convert (3) to (4), producing an averaged mortality risk at time t from 0 to 100%:\n$V(O_t) \\leftarrow E_{k \\sim D} \\begin{cases} R, & \\text{if } O_{t+k} \\text{ is terminal} \\\\ V(O_{t+k}), & \\text{otherwise} \\end{cases}$"}, {"title": "2.2. Data Selection and Pre-Processing", "content": null}, {"title": "2.2.1. MIMIC-IV", "content": "The Medical Information Mart for Intensive Care IV (MIMIC-IV v3.0) database is a collection of de-identified electronic healthcare data for more than 360,000 patients admitted to Beth Israel Deaconess Medical Center, USA between 2008-2022. Our training data consisted of 65,000 patients across 85,000 hospital admissions with at least one of the required input features. These included a mixture of elective post-operative and emergency surgical/medical admissions. The input features consisted of a range of biomarkers, intravenous medications, and demographic information. The patients were divided into train (80%), validation (10%), and test (10%) groups. All input data were standardised based on mean and standard deviation values computed from the training group. Additional per-feature processing steps are described in Appendix C."}, {"title": "2.2.2. SICDB", "content": "For external validation, we employed the Salzburg Intensive Care database (SICdb v1.0.6), a publicly available European dataset containing de-identified healthcare data for more than 27,000 ICU admissions across 21,000 patients admitted to the University Hospital Salzburg between 2013 and 2021."}, {"title": "2.3. Data Representation and State Definition", "content": null}, {"title": "2.3.1. TIME SERIES AS 1D SEQUENCES", "content": "Each patient's time series data was represented as a 1D sequence of tuples {v, t, f, \u2206\u03c5, \u2206t}. Here, v represents the measurement value, t the time-point relative to the current state marker (2.3.2), and f the feature label. Additionally, Av denotes the change in value (if available) since the last measurement of the same feature, and At indicates the time interval between these measurements."}, {"title": "2.3.2. STATE MARKERS AND INPUT DATA CONSTRUCTION", "content": "In our framework, each measurement in a patient's admission is treated as a unique \"state marker\", from which the model can learn the patient's latent state at that point in time. Specifically, the model is provided with the observation data Ot, which consists of the state marker and up to 396 retrospective measurements taken over the previous 7 days (plus age, gender, and weight). The model processes the data as an ordered sequence of real measurements (2.3.1 and Figure 2B), with the temporal component encoded within the tuple rather than the sequence position itself. The chosen model architecture (2.4.1) can accommodate sequences of variable lengths, and thus there is almost no missing data or requirement for data imputation (with the exception of patient weight, described in Appendix C)."}, {"title": "2.3.3. CHOICE OF NEXT STATE", "content": "When determining our distribution D, it is important that our step sizes are large enough to demonstrate a clear trajectory, but not so large as to suffer excessive variance. We defined an eligibility window of 24 hours, with the start of the window delayed x hours into the future. The \"next state\" marker is chosen as the first available measurement occurring inside this window - if no measurements are made within the eligible period, we assume the patient has reached a terminal state (i.e., the current state marker was measured in the final 24 hours before discharge or death). We experimented with a range of possible delays when training the TD model, with x set as either 4, 16, 24, 48, 72, or 120 hours. The best-performing model (on the validation dataset) was then chosen for further analysis."}, {"title": "2.4. Models", "content": null}, {"title": "2.4.1. MODEL ARCHITECTURE", "content": "The model architecture is demonstrated in Figure 2A. The input data consists of a sequence of observed measurements, in the form of a measurement tuple {v, t, f, Av, \u2206t} (2.3.1). Each tuple is embedded using five one-to-many multi-layer perceptron (MLP) networks, corresponding to each component of the tuple, before summing the embeddings together to create an embedded measurement en. The sequence of embedded measurements is then processed through CNN (for sequence length reduction) and LSTM (for sequence processing) layers, a popular machine learning architecture which has the advantage of being able to process sequences of variable length. The final hidden state of the terminal LSTM is decoded by two densely connected MLPs to give a single output for mortality risk. ReLU activation is used in all hidden layers, with sigmoid activation for the final output layer."}, {"title": "2.4.2. CANDIDATE MODELS", "content": "We trained six groups of models using the same base architecture but different learning targets: our TD model, and five baseline models trained with supervised learning. The TD model was trained according to Equation (4), with the terminal reward set to 28-day mortality in the terminal state. The target for the supervised models was each state's observed mortality at one of several pre-defined time horizons (1 day, 3 days, 7 days, 14 days, and 28 days). This allows for a comparative evaluation of TD model performance across a spectrum of well-defined temporal horizons."}, {"title": "2.4.3. MODEL TRAINING", "content": "Each candidate model was independently trained and evaluated five times. We used a binary cross-entropy loss, but included an optional class balancing factor for the supervised baselines (in which the loss is weighted according to the normalised inverse class frequency). This can often optimise supervised learning performance when training on class-imbalanced datasets such as MIMIC-IV.\nUsing the same network for both the predicted and target values can lead to training instability. To address this, we implement a separate identical \"target network\" that provides stable targets for the main network, and is gradually synchronised with the main network after each update. More details for this and other training hyperparameters can be found in Appendix D."}, {"title": "2.4.4. MODEL EVALUATION", "content": "The discriminative ability of all models was evaluated on the internal and external test data using the area under the receiver operating characteristic curve (AUROC) for each of the possible mortality labels i.e., how well does the model's predicted score discriminate between classes when the label is set to mortality at 1 day, 3 days, 7 days, 14 days, or 28 days. AUROC scores were also calculated using the Sequential Organ Failure Assessment (SOFA) score for samples in MIMIC-IV to provide a clinical baseline."}, {"title": "2.5. Statistical Testing", "content": "The mean performance of the TD model was compared to the mean performance of each baseline model in each evaluation using a one-tailed paired Student's t-test, with Benjamini-Yekutieli correction to account for multiple testing under dependency. No comparison was made between the baseline models themselves."}, {"title": "2.6. Software", "content": "All training with performed using Python 3.11. Models were custom-built using PyTorch 2.3.1, and evaluated using the TorchEval 0.0.7 evaluation and SciPy 1.13 statistical testing packages."}, {"title": "3. Results", "content": null}, {"title": "3.1. Dataset Baseline Characteristics", "content": "The baseline characteristics of the two datasets are summarised in Table 1. Both datasets exhibit similar ICU lengths of stay, with comparable means and standard deviations in the number of days in ICU. The patient cohorts in both datasets share similar median ages and gender distributions, which suggests a level of comparability in terms of the general patient population. However, there is a notable difference in mortality outcomes, with the MIMIC-IV dataset showing a significantly higher average hospital and 1-year mortality rate. This may relate to differences in underlying co-morbidities and admission diagnoses for the two populations, as well as differences in the collection period."}, {"title": "3.2. Effect of State Interval Size on TD Performance", "content": "The impact on TD model performance when trained with different sized delays between states is shown in Figure 3. The overall best-performing model was trained with a state-to-state delay of 24 hours (although a longer delay of 48 hours performed better for 7-day and 28-day mortality prediction). We thus chose the TD-24hr model as our benchmark model for subsequent analyses. For a 24-hour delay between states, the distribution of exact intervals between \"states\" and \"next states\" can be seen in Figure 4, with 50% of \"next state\" markers occurring within 1 hour of the eligible period, and 90% occurring within 4 hours."}, {"title": "3.3. Model Evaluation on the Internal Dataset", "content": "The evaluation results for each model group\u00b9 on the MIMIC-IV dataset are summarised in Table 1. In nearly all cases, the supervised models trained with class weighting outperformed those without full results are reported in Appendix A."}, {"title": "3.4. External Model Validation", "content": "Each model was evaluated externally on the SICdb dataset, with results presented in Table 3 and Figure 5B. All models exhibited some degree of performance degradation on the unseen data, and this decline is most pronounced when predicting long-term mortality. When compared to the supervised models, the TD model showed noticeably less deterioration in predictive performance over the extended time horizons, and consistently outperformed all supervised models in all mortality predictions beyond the ultra-short (1-day) horizon."}, {"title": "3.5. Statistical Analysis", "content": "Each model was trained five times, and the resulting mean and standard deviation was used to construct 95% confidence intervals, which are demonstrated in Figure 5. Most models were found to have relatively tight 95% CI bounds for their internal performance, with these bounds typically widening when evaluated on the external dataset. Student's t-test values were also calculated, with adjustments to account for multiple testing. The TD model outperformed the baseline supervised models to high degrees of statistical significance (p < 0.01), with the significance of these results increasing for longer prediction horizons and during external evaluation."}, {"title": "4. Discussion", "content": "In this report, we have demonstrated that models trained with temporal difference learning are able to produce long-term mortality predictions superior to those derived from conventional supervised learning (and far superior to clinical scores such as SOFA). This makes intuitive sense - as mentioned earlier, ICU patients can have lengths of stay that range from a matter of hours to many months, and an admission with a long duration can be expected to have greater variance for its final outcome. As machine learning algorithms become more complex, these models are then at risk of over-fitting to noise for these distant labels, and may deteriorate significantly when validated externally. We observe this in our results, with the supervised models trained directly on longer-term labels tending to perform worse on both the internal and external evaluations.\nOn the other hand, models trained on identical input data using temporal difference learning can learn more accurate predictions for long-term outcomes by generalising to the near-term trajectories of each state. This shortens the horizon of the target label to just a single step in the future, reducing the variance and degree of over-fitting, and does not appear to suffer significant bias for a sufficiently sized training dataset. Importantly, the prediction accuracy remains robust even when validated on an external dataset collected from another continent. When combined with our choice of model architecture, this produces a model that generalises well; can be applied at any stage in an ICU patient's admission; can process any length of inpatient stay; and does not suffer from data missingness.\nFrom a purely theoretical standpoint, there are several reasons why TD learning could fail when applied to healthcare settings. First, the states are only partially observable, and it may not be possible for the model to infer an accurate latent state St for the patient given a limited set of visible observations Ot. Second, the irregular distribution of time intervals (Figure 4) could limit the model's ability to consistently infer transitions between states. Third, two patients will never truly occupy the same continuous state space, and the model may fail to group \"similar\u201d patient observations together as part of trajectory mixing. Fourth, patient states may not observe a consistent transition probability over time, a key assumption of the SMRP. Despite all of these potential limitations, we have managed to show experimentally that models trained using TD learning are still able to converge to a coherent and accurate prediction for the complex ICU patient.\nThere remain several avenues for further research. Ideally, TD learning should be evaluated on multiple architectures (e.g., Transformers, RNNs), with a sensitivity analysis to assess the impact of training dataset size. For instance, how does the potential for bias in TD learning evolve with smaller datasets, and at what sizes might it become significantly problematic? We aim to explore these questions in future work."}, {"title": "5. Conclusions", "content": "This report proposes a framework for applying temporal difference learning to outcome prediction tasks when using irregular time series health data. We subsequently implement this framework for the task of mortality estimation in the ICU, chosen for its partially observable data and long, complex trajectories. When compared to standard supervised learning methods, models trained with temporal difference learning can be shown to predict distant health outcomes with a higher level of accuracy, and show greater resilience when evaluated on external unseen datasets. This work has important implications both for the implementation of temporal difference learning within healthcare reinforcement learning, and also for the wider field of health risk estimation."}, {"title": "Appendix D. Model Training Hyperparameters", "content": "The AdamW optimiser was used for each backpropagation update. The learning rate and weight decay were set as per Equation (5) and Equation (6) (Wang and Aitchison, 2024). Each model was trained for a maximum of 10 epochs, with the best iteration selected based on per-epoch evaluation on the validation data. The main and target networks were initialised with identical parameters at the start of training the target network then receives a soft update of the main network's parameters after every update as per Equation (7), with a = 0.99.\n$\\text{Learning rate} = \\frac{1}{\\sqrt{\\text{Ntrainable parameters}}}$\n$\\text{Weight decay} = \\frac{1}{\\text{learning rate} * \\text{Nbatches per epoch}}$\n$\\theta_{target} \\leftarrow \\alpha * \\theta_{target} + (1 - \\alpha) * \\theta_{train}$"}]}