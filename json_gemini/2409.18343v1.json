{"title": "Improving Agent Behaviors with RL Fine-tuning\nfor Autonomous Driving", "authors": ["Zhenghao Peng", "Wenjie Luo", "Yiren Lu", "Tianyi Shen", "Cole Gulino", "Ari Seff", "Justin Fu"], "abstract": "A major challenge in autonomous vehicle research is model-\ning agent behaviors, which has critical applications including construct-\ning realistic and reliable simulations for off-board evaluation and forecast-\ning traffic agents motion for onboard planning. While supervised learn-\ning has shown success in modeling agents across various domains, these\nmodels can suffer from distribution shift when deployed at test-time. In\nthis work, we improve the reliability of agent behaviors by closed-loop\nfine-tuning of behavior models with reinforcement learning. Our method\ndemonstrates improved overall performance, as well as improved tar-\ngeted metrics such as collision rate, on the Waymo Open Sim Agents\nchallenge. Additionally, we present a novel policy evaluation benchmark\nto directly assess the ability of simulated agents to measure the quality\nof autonomous vehicle planners and demonstrate the effectiveness of our\napproach on this new benchmark.", "sections": [{"title": "Introduction", "content": "Transformer-based architectures\nhave demonstrated state-of-the\nart performance in a variety of\ntasks in language [24], vision [26],\nand robotics [43]. The success\nof these models is credited to a\nwidely adopted \"pre-training and\nfine-tuning\" scheme [40]. In the\npre-training phase, the model ac-\nquires knowledge from a very large amount of training data; during fine-tuning,\nthe model behaviors are rectified to align with human preferences and expec-\ntations. While supervised learning can be used for fine-tuning, previous work\nhas shown superior performance with reinforcement learning (RL) fine-tuning\nin language tasks [25] and text-to-image generation [3]. In autonomous driving"}, {"title": "Related Work", "content": "Pre-training and Fine-tuning of Transformer-based Models\nTransformer-based models have been applied to various domains such as text\ngeneration [4], image generation [26], robotics [43], drug discovery [18], disease\ndiagnose [42], and generalist medical AI [22]. Many large Transformer-based\nmodels are trained in the \"pre-training then fine-tuning\" manner, where su-\npervised fine-tuning [40] or reinforcement learning with human feedback [25]\nholds the promise to align the model behaviors to human preferences. In the\nautonomous driving domain, similar Transformer-based architectures have been\napplied to various tasks, ranging from perception [19], motion prediction [23],\nself-driving policies [10] and simulation [31,38,39]. In this work, we focus on the\nmotion prediction problem, where predictors forecast the future trajectories of\nthe target agents by observing history information [23, 29, 30]. Unlike founda-\ntional models in other domains such as large language models [24] and vision"}, {"title": "Behavior Modeling for Autonomous Driving", "content": "Modeling the behavior of traffic participants is a critical task in many au-\ntonomous driving systems, particularly for constructing realistic simulation to\ntest the AD planners. Most existing simulators [6, 12, 41] rely on hand-crafted\nrules for traffics and maps generation. However, the data distributions for the\nmap structure, traffic flow, the interaction between traffic participants and other\nelements do not realistically represent the real world. Modern data-driven simula-\ntors [9,13,14,33] address this by replaying the behaviors of the traffic participants\nfrom real-world scenarios recorded by an autonomous vehicle (log-replay). Yet,\nthe downside of log-replay is that re-simulation may become unrealistic when\nthe planner behavior diverges from the original logged behavior. For example, if\nan AD planner is more cautious than the human driver and brakes earlier, the\ntrailing vehicle might collide into it, leading to a false positive collision.\nIn this work, we mainly focus on the simulation agents task and evaluate our\nsolutions on the Waymo Open Sim Agent Challenge (WOSAC) [20]. Many exist-\ning WOSAC submissions apply the marginal motion prediction models [2, 7,29],\nwhich typically take initial states and predict the positions of traffic participants\nat all future steps in a single inference (one-shot). Those marginal models do not\nexplicitly model interactions between agents during the prediction horizon. The\nautoregressive (AR) models naturally fit to the driving behavior modeling, es-\npecially in the context of closed-loop simulation [11,28,31,38]. AR decoding [28]\nallows the interactions between agents to be modeled via a self-attention mech-\nanism at each step of the decoding process. However, closed-loop training of the\nAR behavior prediction models remains an understudied area. We propose to\nimprove a pre-trained AR model with closed-loop fine-tuning and evaluate the\nperformance on the WOSAC benchmark.\nIn contrast to prior research on combining behavior cloning and reinforce-\nment learning [17,36,37], our approach eliminates the need for an external sim-\nulator and a dynamics model. Since our model predicts actions for all agents,\nit functions as a simplified simulation environment itself. From an algorithmic\nperspective, we avoid back-propagation through time (BPTT) used in existing\nworks [11,36]. Instead, we propose that RL can be conducted with a minimalistic\npolicy gradient algorithm [35]. This allows us to use non-differentiable rewards\n(such as a boolean collision indicator) and non-differentiable models with dis-\ncrete outputs which would not be possible with BPTT."}, {"title": "Preliminaries", "content": "Motion Prediction. A driving scenario includes static information such as the\nmap topology and dynamic information such as the states of traffic participants"}, {"title": "Improving Agent Behaviors with RL Fine-tuning for Autonomous Driving", "content": "and traffic lights. At each time step, the state of a traffic participant is repre-\nsented by a feature vector containing the position, velocity, and heading angle\nin the global frame, and the object type (vehicle, cyclist, pedestrian). For traf-\nfic lights, the feature vector contains their position and state (green, yellow,\nred or unknown). Given the history states of N traffic participants with indices\nI = [1...N], the goal of motion prediction is to predict future trajectories, i.e.\nthe positions in future steps, of these agents.\nBehavior Modeling as a Multi-Agent RL Problem. We consider driving behavior\nmodeling as a Multi-agent Markov Decision Process (Multi-agent MDP). The\nMulti-agent MDP is defined by the tuple (I, S, {A}, T, {Ri}, N, {Oi}, \u03b3),\nwhere S is the joint state space, A = \u00d7iAi is the joint action space, the transition\nfunction is T : S \u00d7 A \u2192 S, the reward functions are Ri(St, At,i, St+1), Vi \u2208\nI, the observation space is \u03a9 = \u00d7i\u03a9i, the observation functions Oi(s), and\nthe discount factor is \u03b3. In this Multi-agent MDP, the goal is to learn action\npolicies \u03c0\u03af: Ai \u00d7 \u03a9i \u2192 [0,1] for each agent. Each agent aims to maximize its\nexpected cumulative return: $\\pi_{i} = \\arg \\max E_{\\tau \\sim P_{\\theta}, \\forall j \\in I} [\\sum_{t=1}^{T} \\gamma^{t}r_{t,i}]$, where \u03c4 =\n(So, ao, ..., ST, aT) is the joint future rollout obtained by executing the learned\npolicy model conditioned on the initial state. Here, at = {at,i}i\u2208I is the joint\nactions.\nAutoregressive Encoder-Decoder Architecture. We\nuse MotionLM [28], an encoder-decoder transformer\nbased autoregressive motion prediction model.\nThe model's encoder takes a set of tokens repre\nsenting the initial states of the scenarios as input\nand generates a scene embedding. These initial\nstates include the traffic lights states, map topol\nogy, and the history information of all traffic par\nticipants. During inference, we run the decoder for\nTpred prediction steps to autoregressively generate\nthe prediction of all agents. At each prediction\nstep, the decoder takes a set of motion tokens as\nwell as the scene embedding as input and gener\nates a distribution of N output tokens. The decoder consists of multiple layers,\neach applying self-attention among input tokens and cross-attention to the scene\nembedding. All N motion tokens at step t can attend to each other and all pre-\nvious tokens as shown in Fig. 3, where each row represents a query token and\neach column a key token and green blocks indicate key tokens that the query\ncan attend. After running Tpred prediction steps, the Tpred \u00d7 N output motion\ntokens form complete trajectories for N agents. This autoregressive approach en-\nsures that each agent's action is based on a temporally causal relationship with\nthe previous actions of all traffic participants, leading to improved modeling of\ninteraction between agents within the prediction horizon.\nWe modify the original MotionLM model by adopting a scene-centric input\nformat and predicting the motion of all agents, rather than using a pre-selected"}, {"title": "Method", "content": "As shown in Algorithm 1, our method has two stages. In the pre-training stage,\nwe reconstruct the ground truth actions from the data and use the maximum\nlikelihood objective to match the joint action distribution of observed behaviors\nin the dataset:\n$\\max_{\\pi_{\\theta}} E_{D} \\sum_{t=1}^{T_{pred}} \\sum_{i\\in I}log \\pi_{\\theta}(a^{GT}_{t,i}).$\n(1)\nThe second stage of our learning process fine-tunes the model using reinforcement\nlearning (RL). We formalize the problem as a Multi-agent MDP for our behavior\nmodeling task as follows:\nAction. The action space for each agent Ai is a Verlet-wrapped delta action\nspace [28], where each action represents the X, Y acceleration in scene coordi-\nnates. To reconstruct the ground truth action targets, we first infer the acceler-\nations by differentiating the observed positions in the data. These accelerations\nare then discretized into a 13x13 uniformly spaced grid, where outliers are clipped\nto the minimum and maximum values of 6 m/s\u00b2.\nState. The state space S contains map features, the joint state of all objects\nand traffic lights.\nTransition Dynamics. Agents transit to new positions computed by adding\nprevious positions with an offset: post+1,i = (at,i\u2206 + velt,i)\u2206 + post,i wherein\nthe velocity velt,i and the position post,i are in st and A is the time interval\nbetween steps.\nObservation. We define observations to consist of a historic context c, previous\nactions of all objects and the agent identity: Ot,i = (c, a1, ..., at\u22121, i). Here, the\ncontext c = ({mi}M, STprev, \u2026\u2026\u2026, So) is a set containing M map features and the\nobject and traffic light states for history steps t = Tprev, ..., 0."}, {"title": "RL Fine-tuning", "content": "We propose to fine-tune a pre-trained autoregressive motion predictor with RL.\nThe reward function for each agent at each step is defined as:\n$r_{t,i} = -||Post,i \u2013 GTt,i||2 \u2013 \\lambda Collt,i,$\n(2)\nwhere Post,i is the position of agent i in stept and GTt,i is the corresponding\nposition in the logged trajectory. Collt,i is a Boolean indicator and will be 1 if\nthe bounding box of agent i intersects with others' bounding boxes. This reward\nfunction, while simple, captures the key objectives of preserving the behavioral\nrealism as well as satisfying the safety constraint of collision avoidance."}, {"title": "Improving Agent Behaviors with RL Fine-tuning for Autonomous Driving", "content": "Algorithm 1: Pre-train and fine-tune an autoregressive motion predic-\ntor.\ninput: Large-scale driving dataset D.\noutput: A Sim Agent policy \u03c0\u03b8.\n1 Initialize model parameters \u03b8 and model \u03c0\u03b8.\n2 for pre-training iterations j = 1, ... do\n3 Retrieve Shistory, SGT from D.\n4 Construct target actions agr = {avt, vt, i} and construct the observation\nOt,i with GT actions.\n5 Run the model with the observation and get the predicted actions\n{at,i, vt, i}.\n6 Update \u03c0\u03b8 via Eq. 1.\n7 for fine-tuning iterations j = 1, ... do\n8 Retrieve Shistory from D; Set ao \u2190.\n9 for t = 1, ..., Tpred do \u25b7 Autoregressive Rollout.\n10 Ot\u2190 (Shistory, ao, ..., at-1, I). \u25b7 Get obs.\n11 at \u2190 \u03c0\u03bf(10t). \u25b7 Decode next actions.\n12 Reconstruct predicted states Spred = {\u015dt,i, Vt, i} by translating actions\nat,i, Vt, i.\n13 Compute per-agent per-step rt,i via Eq. 2\n14 Compute normalized return via Eq. 4 and update \u03c0\u03bf via Eq. 5\nDuring fine-tuning, we run the model for Tpred prediction steps. The encoder\nfirst encodes the scene context c as a shared scene embedding, before the autore-\ngressive decoding. At each prediction step t, the fixed scene embedding and the\ntx N tokens are fed to the autoregressive decoder and sample N new actions.\nSpecifically, at prediction step t = 1, we project the agents' current positions\nthrough a MLP and get the agent embeddings: idi = MLP(Poso,i), i = 1, ..., N.\nThe agent and scene embeddings serve as the input tokens to the decoder. After\nseveral layers of self-attention and cross-attention, N actions are sampled from\nthe categorical distributions constructed from the output of the decoder. The\nembeddings of those sampled actions will be added with corresponding idi and\nconcatenated with the tokens in previous steps to form the input tokens for the\nnext step. Compared to the decoding process of a language model, we output\nN tokens concurrently at each prediction step instead of one token. Our model\nautoregressively rolls out the actions in Tpred time steps. After collecting the\nrollout trajectories, we translate the actions to sequences of 2D positions for\ncomputing the rewards following Eq. 2. The return (i.e. the \"reward-to-go\"), for\nstep t and each agent i is:\n$R_{ti} = \\sum_{t'=t}^{T_{pred}} \\gamma^{t'-t}r_{t'i}$\n(3)\nWe normalize the return across the training batch, here Mean and Std are\nthe average and the standard deviation computed across all time steps in all"}, {"title": "Improving Agent Behaviors with RL Fine-tuning for Autonomous Driving", "content": "scenarios for all agents in the training batch:\n$\\hat{R_{t,i}} = (R_{t,i} - Mean(R))/Std(R).$\n(4)\nWe then apply the REINFORCE [35] method to compute a policy gradient for\noptimizing the model by differentiating the following surrogate objective:\n$\\max_{\\pi_{\\theta}}E_{D} \\sum_{t=1}^{T_{pred}} \\sum_{i\\in I}log \\pi_{\\theta}(a_{t,i}|O_{t,i})\\hat{R}_{t,i}.$\n(5)"}, {"title": "Policy Evaluation for Sim Agents", "content": "A key limitation of common \u201cimitative\u201d metrics (such as ADE), which compare\nmodel rollouts to ground truth trajectories, is the weak connection between the\nmetric and the actual goal of assessing the AD planner performance. A low ADE\nmetric does not guarantee good driving behaviors. Log-replay, for example, has a\nperfect ADE of zero but would be a poor choice for sim agents because it is non-\nreactive. To create an evaluation that has a direct connection to measuring the\nperformance of the AD planners, we propose a new policy evaluation framework\nfor sim agents, inspired by the RL policy evaluation literature [32].\nOur policy evaluation framework involves ranking and scoring the perfor-\nmance of a predetermined collection of AD planner policies. This is analogous\nto a real-world use case where one must decide which planner to deploy from\na collection of candidate software releases. A better sim-agent model will give\na more accurate signal on which policy would be best when deployed in the\nreal world. As shown in Fig. 4, we first prepare a batch of AD planner policies\nwith known performance ranking. Then, we evaluate the performance of these\nAD planners when the traffic agents in the scenario are controlled by a sim\nagent model. Therefore, we will generate the estimated performance for those\nAD planners for the specific sim agent. We then measure the discrepancy be-\ntween the estimated performance and the ground truth performance of those\nplanners. This discrepancy becomes the measurement of the sim agents model's\nability to assess the performance of the planners. Policy evaluation covers two\nimportant use cases for the sim agent models in the deployment of autonomous\nvehicles: evaluation, where we wish to estimate the performance of agents in\nthe simulation, and selection, where we wish to determine a ranking or order\nbetween different deployment candidates.\nChoice of policies. In order to perform policy evaluation, we must have a fixed set\nof policies on hand to rank or evaluate. To generate a large variety of planning\npolicies with both good and bad performance, we propose to use a random\nshooting search-based policy family, parameterized by the number (J) and depth\n(D) of trajectories sampled. We compute a \u201cground truth\u201d score for each policy\nby evaluating it with log playback agents. Note that the choice of ground truth\nis an important design decision. Any sim agent could serve as a ground truth,"}, {"title": "Improving Agent Behaviors with RL Fine-tuning for Autonomous Driving", "content": "but we need to pick one that is the most fair to all models and we believe log\nplayback to be the most neutral.\nOur random shooting policy operates in a model-predictive control (MPC)\nfashion: at each time step, our random shooting policy samples from a fixed\nlibrary of J trajectories, which are generated by maintaining a single steering\nwheel angle and acceleration for D steps. Note that this action specification\nis different from the action space of the MotionLM architecture we described\nin Sec. 4. We found this simple strategy to work much better than randomly\nselected actions. The trajectories are then scored by a reward function and the\nfirst step of the best scoring trajectory is executed. This process repeats for the\nentirety of the rollout. We used 16 different settings of J, ranging between 9 to\n81, and we used 4 values of depth D\u2208 [6,8,12,16]. We then used the product\nof these two sets, for a total of 64 different policies evaluated.\nReward Function. The reward function used for selecting actions from a set of\ncandidate trajectories is a linear combination of collisions, as well as off-road\nand route-following infractions. We use a modified reward function from Eq. 2\nby replacing the L2 norm from the ground truth (which is not available to the\nplanner at execution time) with additional terms for following a reasonable path.\nWe instead give the planner a high-level route in the form of waypoints, and we\nuse a weighted sum of \u221210C \u2013 O \u2013 R + 10\u22124P, where C\u2208 {0,1} denotes the\ncollision indicator and is 1 when a collision between AV and another object\nhappens, O \u2208 {0,1} denotes the offroad indicator and is 1 when the AV is too\nclose to the road edge, R \u2208 {0,1} denotes the off-route indicator which is 1 when\nthe AV's lateral distance to the GT trajectory exceeds a threshold, and P is the\nprojection of the AV's displacement between two time steps when projected\nonto the logged trajectory and measures the route-following behavior. We use\nWaymax [9]'s utility function to compute those metrics."}, {"title": "Experiments", "content": "We now describe our method's experimental results on the Waymo Open Sim\nAgents Challenge (WOSAC) [20] and on the Policy Evaluation task introduced\nin Sec. 4.2. Our experiments are designed to answer the following questions:"}, {"title": "Conclusion", "content": "We studied the viability of applying the popular \"pre-training and fine-tuning\"\nscheme to modeling traffic agents for AD simulation. We drew the connection be-\ntween a multi-agent driving behavior model and a simulation environment - the\nmulti-agent behavior model itself can be used to perform rollouts for closed-loop\ntraining. By using an on-policy RL algorithm with a simple reward, we are able to\nfine-tune a pre-trained large multi-agent behavior model to effectively align the\ntraffic agent behaviors with human expectations, such as collision avoidance. The\nexperimental results show that our method can significantly improve the per-\nformance of the pre-trained model on the Waymo Open Sim Agent Challenge\n(WOSAC) [20]. We also proposed a novel policy evaluation task and demon-\nstrated that the model fine-tuned by our method can achieve more reliable AD\ntesting result.\nLimitations. There are several limitations to the approach we have discussed\nin this paper. We use a simple transition and action model (based on predicting\naccelerations and integrating them to estimate positions) as the environment\ndynamics model, which could produce kinematically unrealistic behaviors dur-\ning a rollout. A more realistic solution would be to embed a low-level controller\ninto simulation that attempts to reach the positions predicted by the model. In\naddition, we studied a reward function (Eq. 2) that encourages collision avoid-\nance and minimizes divergence in closed-loop simulation. The reward function\ncan be extended to induce various driving behaviors, such as encouraging adver-\nsarial behavior (e.g. using the negative of the ego vehicle's reward) to stress-test\nchallenging scenarios."}]}