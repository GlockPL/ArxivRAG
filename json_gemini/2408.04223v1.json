{"title": "VideoQA in the Era of LLMs: An Empirical Study", "authors": ["Junbin Xiao", "Nanxin Huang", "Hangyu Qin", "Dongyang Li", "Yicong Li.", "Fengbin Zhu", "Zhulin Tao", "Jianxing Yu", "Liang Lin", "Tat-Seng Chua.", "Angela Yao"], "abstract": "Video Large Language Models (Video-LLMs) are flourishing and has advanced many video-language tasks. As a golden testbed, Video Question Answering (VideoQA) plays pivotal role in Video-LLM developing. This work conducts a timely and comprehensive study of Video-LLMs' behavior in VideoQA, aiming to elucidate their success and failure modes, and provide insights towards more human-like video understanding and question answering. Our analyses demonstrate that Video-LLMs excel in VideoQA; they can correlate contextual cues and generate plausible responses to questions about varied video contents. However, models falter in handling video temporality, both in reasoning about temporal content ordering and grounding QA-relevant temporal moments. Moreover, the models behave unintuitively - they are unresponsive to adversarial video perturbations while being sensitive to simple variations of candidate answers and questions. Also, they do not necessarily generalize better. The findings demonstrate Video-LLMs' QA capability in standard condition yet highlight their severe deficiency in robustness and interpretability, suggesting the urgent need on rationales in Video-LLM developing.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are revolutionizing research in various X+Language tasks (with X be vision, robotics, sciences, etc.), ushering in a new era with unprecedented advancements in computer science. Video Question Answering (VideoQA), as a multimodal and multifaceted challenge that requires both linguistic and visual reasoning, is at the forefront of this revolution. It also serves as a litmus test for evaluating the comprehensive multimodal understanding and reasoning capabilities of cutting-edge Video-LLMs 1. While VideoQA performance steadily improves with Video-LLMs, the pursuit of higher QA accuracy alone is insufficient to demonstrate the models' capability, neither in video understanding nor in question-answering. For example, a recent study reveals that the models can answer a large portion of questions (correctly) with irrelevant video inputs or even without video at all. Also, there is a surge of works studying the hallucination problem of multimodal LLMs in visual description. These observations raise the concern about whether these models can provide faithful answers in real-world applications. In this regard, we conduct a comprehensive study to Video-LLM's behavior in VideoQA. We delve into the models' success and failure modes from multiple aspects to provide insights towards more trustworthy and human-like machine intelligence.\n1 In this paper, we denote Video-LLMs as video-language models that use LLMs with \u2265 1 billion parameters."}, {"title": "2 Probes and Analyses", "content": ""}, {"title": "3.1 Overview", "content": "We explore the following probes to examine Video-LLMs' behavior in VideoQA. The probes are related to either video understanding or question answering:\n1. Temporal Understanding. VideoQA characterizes temporal dynamics compared with image VQA. Hence, our first study is to check if Video-LLMs can (or to what extent) interpret the chronological order of video content by answering paired adversarial temporal questions, i.e., questions elicited by different time words but are about the same video content, e.g., from \"A after B\" to \"B before A\".\n2. Visual Grounding. To discern possible reasons for model failures in following the video content to answer questions, we further study the problem of visual grounding, which is to check whether or to what extent the Video-LLMs' predictions anchored on the relevant video content versus irrelevant contexts.\n3. Multimodal VQA Reasoning. Both temporal understanding and visual grounding are evaluated on MCQA because there is no suitable OEQA benchmarks for such tests. As such, we delve deeper into multimodal reasoning to check how much Video-LLMs can reason faithfully from video and question to the correct answer, without influence from short-cuts of candidate answers.\n4. Robustness. Are Video-LLMs robust (like human) to general data perturbation? We further study if model decisions are invariant and equivariant to more general video and question perturbations that are not specific to a particular short-cut problem.\n5. Generalizability. All the above experiments curate new data for testing, which may have probed model generalization to a certain extent. Yet for a better study, we consider two classic generalization problems of long-tailed and out-of-distribution (OOD). Long-tailed is to predict long-tailed answers within the same dataset. OOD is to answer questions of completely different types or related to different kinds of videos (e.g., short to long videos, 3rd- to 1st-person view videos) across different datasets."}, {"title": "3.2 Tested Models", "content": "We select prominent Video-LLMs that have reported state-of-the-art performance on VideoQA and summarize them in Tab. 1. These models exploit cross-modal pretrained visual encoders (e.g., CLIP, EVA-CLIP and UMT-L) and powerful LLMs (e.g., DeBERTa-V2, Flan T5, LLAMA, Vicuna, GPT-4) to perform VideoQA.\nModels in the 1st block of Tab. 1 are specially finetuned on target datasets to achieve VideoQA. Models in the 2nd block are general-purpose Video-LLMs and most of they do a zero-shot evaluation on VideoQA. As exception, VideoChat2 includes the training data of NEXT-QA (one of our major testbed) for instruction tuning, and thus should be considered a normal evaluation versus zero-shot. Also, for Video-LLaMA, we adopt the implementation in (Xiao et al., 2024), which is finetuned on NExT-QA by adding adpation parameters as in LLaMA-Adapter. Additionally, we include the recent GPT-40 as a representative of commercial models for off-the-shelf use. In the 3rd block, we include LLoVi which is a tool-based Video-LLM established on LLaVA, LLaViLA and GPT-4. As non-LLM baselines (the bottom block), we consider VIOLETv2, CoVGT and Temp[CLIP] as representative methods for their good performance achieved by fully finetuning small language models, such as BERT and ROBERTA.\nAll models except for Video-LLaMA, we use their official checkpoints (or API for GPT-40). It is noteworthy that different model often tackles a different type of QA, either MCQA or OEQA. For GPT-40, we uniformly sample video frames and feed them along with the questions into the API and prompt it for QA. Related details are presented in the Appendix .4."}, {"title": "3.3 Datasets & Evaluation Measures", "content": "Our major experiments are conducted on NEXT-QA for multi-choice QA, and MSVD-QA for open-ended QA. We select the datasets for their high quality and popularity with completed models of both non-LLMs and LLMs. Moreover, we add other datasets for specific probes as necessary. Concretely, we test on NEXT-GQA for visually grounded QA (Sec. 3.5), NExT-OOD for multi-choice biases in multimodal VQA reasoning (Sec. 3.6) as well as generalization (Sec. 3.8.2), ActivityNet-QA for robustness in answering open-ended temporal questions with shuffled frames (Sec. 3.7). To probe generalization across different types of videos, we add EgoSchema and Perception Test datasets (Sec. 3.8.2). The former features long-form video understanding from an ego-centric viewpoint, while the latter emphasizes visual perception and reasoning of fine-grained human-object interactions. Other details are listed in Tab. 2. NEXT-OOD shares the same statistics as NEXT-QA so we do not list it in the table. Finally, for evaluation, we report both accuracies and flip rates of specific predictions before and after the probes, where the flip rate denotes the proportion of changed predictions."}, {"title": "3.4 Temporal Understanding", "content": "VideoQA distinguishes from image VQA for its factoring a rich set of temporal questions signaled by time words \"before/after/when\". Video-LLMs demonstrate increasingly high accuracy in answering temporal questions, so we try to verify if these models really interpret content order in videos. To this end, we derive two new sets of questions from the original temporal questions in NEXT-QA.\nConsidering the example in Fig. 3. The original question asks the person's next action after \"taking the black item away\" and should be answered with"}, {"title": "3.5 Visual Grounding", "content": "natural question arises - to what extent are the models' predictions grounded on the relevant video content? To study this, we conduct experiments on visual grounding. Specifically, we experiment on the recent NExT-GQA dataset. This dataset extends NEXT-QA with temporal location labels of question-answer pairs. With NEXT-GQA, we compare model performance with different visual inputs as follows: NormalVQA follows the common practice of performing VideoQA by uniformly sampling frames over the whole video. BlindQA removes the video inputs to train and test the models with only the QA pairs to check reliance on pure language priors. PosVQA and NegVQA provides only QA-relevant (positive) or only irrelevant segments (negative) as inputs to the model (see Fig. 6 as an illustration). Finally, we evaluate Grounded QA (GQA) accuracy, i.e., jointly answer the questions and also localize the QA-relevant video moments. By comparing QA performance with and without requirements on visual grounding, we study how much the models' predictions are anchored on the relevant video content. Note that similar experiments are also conducted in NExT-GQA (Xiao et al., 2024), but their analyses are focused on the connection between visual grounding and VQA, with also their major findings established on non-LLM techniques. In this paper, we inherent such settings but target a thorough analysis of Video-LLMs' behavior in visual grounding and a comparison with non-LLM techniques.\nThe results in Fig. 5 deliver the following major observations: 1) Fig. 5a shows that both LLM and non-LLM methods are poor at substantiating their answers with relevant video content. For example, from NormalVQA to Grounded VQA, i.e. when the correct predictions are required to be visually grounded, the accuracies decrease by more than 40% for all models.\n2) Compared with the non-LLM method (CoVGT), specilized Video-LLMs (e.g., FrozenGQA and SeViLA) benefit more from short-cut learning. This is because they remarkably surpass the non-LLM method on normal VQA accuracy (by ~10%) but are only marginally better on GQA (e.g., by ~2%) (Fig. 5a). Also their LLMs alone (e.g., DeBERTa-V2-XL and Flan-T5-XL) already wins over CoVGT's language model (ROBERTa) by more than 7% according to the corresponding BlindQA accuracies. A further comparison of NormalVQA vs. BlindQA (Fig. 5a), NormalVQA vs. PosVQA vs. NegVQA (Fig. 5b) indicates that the short-cuts are primarily from language priors and spurious vision-text correlations. For example, the BlindQA baselines without any visual inputs retain more than 70% of the corresponding NormalVQA' performance. Moreover, visual signals that come from answer-irrelevant video segments yield surprisingly high performance that is competitive to model variants with answer-relevant (ground-truth) visual segments (NegVQA vs. PosVQA)."}, {"title": "3.6 Multimodal VQA Reasoning", "content": "The analyses of both temporal understanding and visual grounding are based on MCQA because current OEQA benchmarks do not support such tests, e.g., no"}, {"title": "4 Discussion", "content": "We summarize the major limitations of Video-LLMs and discuss possible directions for improvement.\nRobustness and Trustworthiness. All probes showed that the open-sourced Video-LLMs be it general-purpose or fine-tuned, perform well for standard question-answering, but suffer under adversarial language modifications while being unaffected by analogous video modifications. These findings reveal a lack of robustness and interpretability and underscores the need for rationales in Video-LLM developing. Some recent studies in multimodal chain-of-thought and modular multi-agent approaches show improved robustness and interpretability. Yet, one needs to mind the inefficiency issues of such techniques. In addition, video multimodal benchmarks are emerging, but almost all of them target comprehensive video understanding, with little attention on interpretability. Related benchmarks such as VIP and NEXT-GQA are thus valuable efforts in developing trustworthy Video-LLM techniques, and deserve more research efforts.\nEfficient and Fine-Grained Long-Form Video Understanding. Our observations about GPT-40's behavior in VideoQA (refer to Fig. 1 and 5), as well as the rapid evolution of SOTA results on common VideoQA benchmarks (e.g., MSVD-QA) suggests that coarse and holistic understanding of short videos (less than 20s) are solvable by scaling up existing Video-LLMs or operating powerful image MLLMs (like GPT-40) frame by frame. Fine-Grained Long-Form Video Understanding manifest two major challenges for existing Video-LLMs:\nThe first is a 'needle in the haystack' challenge search in the long video for short moments (or key frames) that carry the correct answer. Existing benchmarks such as NEXT-GQA and Ego4D-QA embody such challenges, yet effective techniques are lacking. Recent solutions show promise, but they are inefficient, brute-force methods that traverse the whole video frame-by-frame (or segment-by-segment) and perform LLM inference multiple times. Thus, efficiently understanding long videos is still an open problem.\nThe second challenge is fine-grained understanding of very long-form videos, with large temporal certificates, as defined in EgoSchema. Our studies in Sec. 3.8.2 show that existing Video-LLMs targeting short clips do not generalize well to such videos. The aforementioned traversal solutions are ineffective as the task requires reasoning over and weaving together multiple frames or segments to achieve holistic understanding. Recent memory- and caption-based solutions as well as the direct long-frame-encoding solutions offer some insights to handle long videos, but understanding the videos in efficient and fine-grained manner remains a key challenge.\nEgocentric and Embodied QA. We primarily analyzed VideoQA with datasets featuring third-person view (exo-centric) videos and QAs, be it effective as benchmark evaluation for Video-LLMs. Yet noteworthy an important real-world application for VideoQA is geared towards reliable embodied QA assistance. In this regard, first-person view (ego-centric) VideoQA are of greater interest and relevance. Sec. 3.8.2 shows that Video-LLMs pre-trained on third-person view VideoQA datasets can generalize to these application scenarios, but the performance are weak compared with the tool-based Video-LLMs that directly use ego-centric pretrained visual descriptions as well as the well-prompted general-purpose MLLMs (e.g. GPT-40). Therefore, the efforts of leveraging existing MLLMs for ego-centric VideoQA will be of critical importance to initiate successful embodied assistance applications."}, {"title": "5 Conclusion", "content": "This paper has vetted Video-LLMs' performance in VideoQA by probing their success and failure modes with well-designed adversarial tests. While Video-LLMs generally show better QA accuracy, the decrease rates and flip rates align with or even higher than non-LLM methods when faced with adversarial challenges. Specifically, Video-LLMs show significant limitation in coping with video temporality, of both reasoning the chronological content order and grounding the temporal moments to substantiate the answers. Also, they are unresponsive towards video perturbation while being susceptible to simple language variations of questions and candidate answers. Additionally, Video-LLMs, after finetuned, are not necessarily generalize better. Understanding these limitations is crucial for developing future Video-LLMs and VideoQA techniques, where we also conclude some promising directions to proceed."}]}