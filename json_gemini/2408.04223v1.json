{"title": "VideoQA in the Era of LLMs: An Empirical Study", "authors": ["Junbin Xiao", "Nanxin Huang", "Hangyu Qin", "Dongyang Li", "Yicong Li", "Fengbin Zhu", "Zhulin Tao", "Jianxing Yu", "Liang Lin", "Tat-Seng Chua", "Angela Yao"], "abstract": "Video Large Language Models (Video-LLMs) are flourishing and has advanced many video-language tasks. As a golden testbed, Video Question Answering (VideoQA) plays pivotal role in Video-LLM developing. This work conducts a timely and comprehensive study of Video-LLMs' behavior in VideoQA, aiming to elucidate their success and failure modes, and provide insights towards more human-like video understanding and question answering. Our analyses demonstrate that Video-LLMs excel in VideoQA; they can correlate contextual cues and generate plausible responses to questions about varied video contents. However, models falter in handling video temporality, both in reasoning about temporal content ordering and grounding QA-relevant temporal moments. Moreover, the models behave unintuitively - they are unresponsive to adversarial video perturbations while being sensitive to simple variations of candidate answers and questions. Also, they do not necessarily generalize better. The findings demonstrate Video-LLMs' QA capability in standard condition yet highlight their severe deficiency in robustness and interpretability, suggesting the urgent need on rationales in Video-LLM developing.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are revolutionizing research in various X+Language tasks (with X be vision, robotics, sciences, etc.), ushering in a new era with unprecedented advancements in computer science. Video Question Answering (VideoQA), as a multimodal and multifaceted challenge that requires both linguistic and visual reasoning, is at the forefront of this revolution. It also serves as a litmus test for evaluating the comprehensive multimodal understanding and reasoning capabilities of cutting-edge Video-LLMs. While VideoQA performance steadily improves with Video-LLMs, the pursuit of higher QA accuracy alone is insufficient to demonstrate the models' capability, neither in video understanding nor in question-answering. For example, a recent study reveals that the models can answer a large portion of questions (correctly) with irrelevant video inputs or even without video at all. Also, there is a surge of works studying the hallucination problem of multimodal LLMs in visual description. These observations raise the concern about whether these models can provide faithful answers in real-world applications. In this regard, we conduct a comprehensive study to Video-LLM's behavior in VideoQA. We delve into the models' success and failure modes from multiple aspects to provide insights towards more trustworthy and human-like machine intelligence."}, {"title": "VideoQA Background", "content": "2.1 Preliminaries\nVideoQA is the task of answering visual content-related questions for videos. It extends image VQA to dynamic scenarios, such as the movement state of objects (e.g., \"slow\" or \"fast\"?, \"put down\" or \"take away\"?), action repetitions and their transitions over time. VideoQA is studied through multiple-choice (MCQA) and open-ended (OEQA) questions. MCQA provides candidate answers along with each question for models to select the correct one. It is favored in answering inference- and explanatory-type questions (e.g., \"why and how\") that require relatively longer answers. OEQA treats each answer as a category and requires the models to predict a correct answer from a predefined answer list. It is mainly adopted to assess factoid QA where the questions pertain to visual recognition (e.g., \"what is\"). We refer to the recent survey for a more detailed introduction.\n2.2 VideoQA Technique Evolution\nThe VideoQA techniques generally follow the corresponding development of vision and text representation models. The development can be roughly divided into three stages. The first is 1) CNN + RNN where video and text are encoded by convolutional neural networks (e.g., ResNet) and recurrent neural networks (e.g., LSTM) respectively. The second stage 2) CNN/ViT+BERT, features improved visual and language representations, where the models also benefit from self-supervised cross-modal pre-training at scale and fine-tuned small language models (e.g., BERT and RoBERTa) on the target datasets. Finally, the third (and the current) stage 3) CLIP+LLM features pre-trained cross-modal visual encoders (e.g., CLIP-VIT) and frozen LLMs (e.g., Flan T5, LLaMA) with instruction tuning of projection and adaptation modules. In the transition of each stage, there are significant performance jumps. Especially in the LLM stage, leading models like GPT-4V/o and Gemini rival human performance in answering standard visual questions.\n2.3 LLMs for VideoQA\nExisting LLM-based VideoQA methods can be roughly classified into 3 groups: 1) General-purpose MLLMS, where models are not specialized to VideoQA and are instead capable of handling various vision-language tasks such as image/video QA and captioning. Typical models are InstructBLIP, Video-ChatGPT, Video-LLAMA, VideoChat, VideoChat2, Video-LLaVA, LLaMA-VID, and LLaVA-NeXT, and commercial models such as GPT-4V/4O, Gemini. These models do not finetune on the target datasets but are instruction tuned and prompted to perform different tasks, e.g. VideoQA. We refer to the recent survey for a more detailed introduction. 2) Specialized Video-LLMs exploit frozen LLMs or general-purpose image MLLMs (e.g., BLIP-2, LLaVA, LLaMA-Adapter) while finetuning adaptor or LORA modules on target VideoQA datasets. Successful examples include FrozenBiLM, SeViLA, LLAMA-VQA. Additionally, Video-LLAMA is a general-purpose Video-LLM, but in this paper we study its finetuned version that is similar to LLaMA-Adapter but with Video Q-Former to encapsulate short clips of a video. 3) Tool-based Video-LLMs. These methods are parameter-free and focused on the use or collaboration of different MLLMS (LLaVA, GPT-4V, LaViLA) and LLMs (LLaMA, GPT-3.5 and GPT-4) as tools (e.g., planning, captioning, and question-answering) for VideoQA. Representative examples include ViperGPT, MoReVQA, LLoVi, IG-VLM"}, {"title": "Empirical VQA Study", "content": "Existing literature analyzed image VQA with non-LLM techniques (e.g., RNNs or BERT) and focus on specific issues such as data imbalance, robustness, or language bias. While analyze the behavior of VQA models from multiple aspects of generalization to long-tailed data as well as complete image and question understanding, they focus on image QA and the techniques examined are from the CNN+RNN stage. In this paper, we will revisit some of the probes and check if the problems get alleviated in the LLM era. To our best knowledge, no previous work has comprehensively studied Video-LLMs' behavior in VideoQA. Several works explore specific phenomena with models from the stage of CNN/ViT+BERT. For example, have revealed the single-frame bias issue, where a large number of questions can be answered with a single video frame. have uncovered that existing VLMs (to be distinguished from MLLMs) that fully finetune BERT lack a sense of time. have investigated the problem of imbalanced distribution of candidate answers in MCQA, and show that the models tend to exploit the short-cuts in candidate answers for answer prediction. Recently, have studied visually grounded QA and disclose that existing VLMs and MLLMs are poor at substantiating their predictions with visual evidence. The above works each focuses on a particular aspect of VideoQA; they study either MCQA or OEQA with non-LLM models. Furthermore, most of they only mention the problems to induce their own solutions or datasets, without delving into the extent of the problems. Given the emerging and revolutionary effect of LLMs for question answering, in this paper, we concentrate on LLM-based techniques and present the first comprehensive analysis for Video-LLMs in VideoQA. We study both MCQA and OEQA either for classification or generation, and additionally compare with non-LLM techniques in face of the same probes. Additionally, we notice that there is a surge of new benchmarks targeting the analysis of Video-LLMs in a broad aspect of video understanding (e.g., diverse video categories such as TV/Magic/Fashion shows and related challenges). In this study, we focus on VideoQA of daily activities for its significance towards human-machine interaction. While we use common names for the probes (e.g., temporal understanding, multimodal reasoning, ...), the specific probes and data are entirely different."}, {"title": "Probes and Analyses", "content": "3.1 Overview\nWe explore the following probes to examine Video-LLMs' behavior in VideoQA. The probes are related to either video understanding or question answering:\n1. Temporal Understanding. VideoQA characterizes temporal dynamics compared with image VQA. Hence, our first study is to check if Video-LLMs can (or to what extent) interpret the chronological order of video content by answering paired adversarial temporal questions, i.e., questions elicited by different time words but are about the same video content, e.g., from \"A after B\" to \"B before A\".\n2. Visual Grounding. To discern possible reasons for model failures in following the video content to answer questions, we further study the problem of visual grounding, which is to check whether or to what extent the Video-LLMs' predictions anchored on the relevant video content versus irrelevant contexts.\n3. Multimodal VQA Reasoning. Both temporal understanding and visual grounding are evaluated on MCQA because there is no suitable OEQA benchmarks for such tests. As such, we delve deeper into multimodal reasoning to check how much Video-LLMs can reason faithfully from video and question to the correct answer, without influence from short-cuts of candidate answers.\n4. Robustness. Are Video-LLMs robust (like human) to general data perturbation? We further study if model decisions are invariant and equivariant to more general video and question perturbations that are not specific to a particular short-cut problem.\n5. Generalizability. All the above experiments curate new data for testing, which may have probed model generalization to a certain extent. Yet for a better study, we consider two classic generalization problems of long-tailed and out-of-distribution (OOD). Long-tailed is to predict long-tailed answers within the same dataset. OOD is to answer questions of completely different types or related to different kinds of videos (e.g., short to long videos, 3rd- to 1st-person view videos) across different datasets."}, {"title": "Temporal Understanding", "content": "VideoQA distinguishes from image VQA for its factoring a rich set of temporal questions signaled by time words \"before/after/when\". Video-LLMs demonstrate increasingly high accuracy in answering temporal questions, so we try to verify if these models really interpret content order in videos. To this end, we derive two new sets of questions from the original temporal questions in NEXT-QA. Considering the example, The original question asks the person's next action after \"taking the black item away\" and should be answered with \"pat the animal\". Our first edit, called Temporal Exchange (TE), exchanges the actions in the question and the correct answer, by asking for the action before \"the person pats the animal\" (right of Fig. 3). The models should be able to answer \"take the black item away\" for the derived question if it correctly answered the original question. A second edit called Temporal Description (TD), focuses on the time order and isolates the effect of language correlation by asking in a coarse manner of \"what did xx do or what is happening/doing\". The bottom of Fig. 3 shows a TD example, where the question provides little to no contextual information compared to the original and TE questions. Moreover, all answers for the question describe the same visual content with different orders though only one is correct. Correctly answering TD questions requires stronger action order reasoning compared to the original and TE questions. To obtain the testing examples, we collect about 1K temporal questions (TO) originated from NEXT-QA and derive the TE and TD examples by prompting GPT-4 and parsing the syntactic structure of the questions respectively. Regarding prompting for TE examples, we split the questions into two groups of \"before/after\" and \"when\", and design different prompts for them to achieve better results. Regarding parsing for TD questions, we mainly detect the time words \"before/after/when\", keep the question parts (e.g., \"what did the person do?\") as new questions, and recombine the contextual parts (e.g., \"she took the black item away\") with the correct answers (e.g., \"pat animal\") while varying the time words to form different options. More details are presented in Appendix .1. For both the generated TE and TD examples, we further manually check and refine as necessary to ensure the quality of the derived QAs. The results in Fig. 2 demonstrate that most models are brittle towards such temporal tests. Specifically, the high flip rates in Fig. 2a show that both LLM and non-LLM methods tend to change their answers for the edited questions. Moreover, the accuracy in Fig. 2b also show that most models' performances shrink. The effects are specially prominent for the TD test, where most models flip more than 70% of their predictions and decline their accuracy by more than 30%, even though the involved video contents remain the same (also refer to prediction examples in Fig. 4). Among the models, GPT-40 significantly outperforms others in the TD test even though it does not encode any temporal information (we specify in the prompts that the images are ordered video frames). By jointly considering the accuracy in Fig. 2b, we speculate that the models trained on the target datasets likely overfit to dataset statistics rather than reason about time faithfully. Additionally, LLaMA-VQA shows the smallest flip rate in TE test. We attribute such strength of LLaMA-VQA to its auxiliary training objective by inversely predicting the questions given the answers, which implicitly suits the TE probes by enhancing the relation between the answer and the contextual term in the question. Moreover, Fig. 2b shows that models using advanced LLMs such as LLAMA and Vicuna perform better on the TE test but worse on the TD test than non-LLM method CoVGT. This indicates that LLM-based methods are better at exploiting language priors or commonsense in the TE questions for predicting answers, even though they seem weaker than non-LLM methods in understanding video content order. Again, we find that GPT-40 achieves the highest accuracy in both TE and TD tests, showcasing strong zero-shot temporal QA capability. The above observations, together with the findings in some related studies, suggest that the end-to-end learning approach (even with LLMs) may not be a good choice to cope with temporal relations. It seems more favourable to explicitly reason across the sequence of video frames in a way like GPT-40, and alternatively, to convert the video frames into time-aware descriptions and then feed them into LLMs for question answering, such as in MoReVQA and LLoVi. Yet how to effectively and efficiently coordinate visual captioning and question-answering is still open challenge that deserves further efforts."}, {"title": "Visual Grounding", "content": "With the observation that model answers flip a lot even with paired questions about the same video content, a natural question arises - to what extent are the models' predictions grounded on the relevant video content? To study this, we conduct experiments on visual grounding. Specifically, we experiment on the recent NExT-GQA dataset. This dataset extends NEXT-QA with temporal location labels of question-answer pairs. With NEXT-GQA, we compare model performance with different visual inputs as follows: NormalVQA follows the common practice of performing VideoQA by uniformly sampling frames over the whole video. BlindQA removes the video inputs to train and test the models with only the QA pairs to check reliance on pure language priors. PosVQA and NegVQA provides only QA-relevant (positive) or only irrelevant segments (negative) as inputs to the model. Finally, we evaluate Grounded QA (GQA) accuracy, i.e., jointly answer the questions and also localize the QA-relevant video moments. By comparing QA performance with and without requirements on visual grounding, we study how much the models' predictions are anchored on the relevant video content. Note that similar experiments are also conducted in NExT-GQA, but their analyses are focused on the connection between visual grounding and VQA, with also their major findings established on non-LLM techniques. In this paper, we inherent such settings but target a thorough analysis of Video-LLMs' behavior in visual grounding and a comparison with non-LLM techniques. The results deliver the following major observations: 1) Fig. 5a shows that both LLM and non-LLM methods are poor at substantiating their answers with relevant video content. For example, from NormalVQA to Grounded VQA, i.e. when the correct predictions are required to be visually grounded, the accuracies decrease by more than 40% for all models. 2) Compared with the non-LLM method (CoVGT), specilized Video-LLMs (e.g., FrozenGQA and SeViLA) benefit more from short-cut learning. This is because they remarkably surpass the non-LLM method on normal VQA accuracy (by ~10%) but are only marginally better on GQA (e.g., by ~2%). Also their LLMs alone (e.g., DeBERTa-V2-XL and Flan-T5-XL) already wins over CoVGT's language model (ROBERTa) by more than 7% according to the corresponding BlindQA accuracies. A further comparison of NormalVQA vs. BlindQA, NormalVQA vs. PosVQA vs. NegVQA indicates that the short-cuts are primarily from language priors and spurious vision-text correlations. For example, the BlindQA baselines without any visual inputs retain more than 70% of the corresponding NormalVQA' performance. Moreover, visual signals that come from answer-irrelevant video segments yield surprisingly high performance that is competitive to model variants with answer-relevant (ground-truth) visual segments (NegVQA vs. PosVQA)."}, {"title": "Multimodal VQA Reasoning", "content": "The analyses of both temporal understanding and visual grounding are based on MCQA because current OEQA benchmarks do not support such tests, e.g., no sufficient temporal questions or no temporal location labels. Thus, we want to study if (or to what extent) the models' failure of reasoning is because of the short-cuts in candidate answers. We call this probe as multimodal reasoning because such short-cuts dispense with the reasoning between videos and questions, leading to a direct jump to the answers. Accordingly, we study two kinds of short-cuts related to candidate answers. A video-answer (VA) short-cut exists when only one candidate answer contains the visual objects presented in the video, while most other answers do not involve video-related objects. As such, models may ignore the question and choose the video-relevant answer as final prediction. A similar question-answer (QA) short-cut exists when only one candidate answer contains question-relevant keywords. This shortcut allows models ignore the video and also bypass interpreting the question as well; they may simply associate common keywords in question and answer for prediction. Note that the short-cuts may not necessarily bring correct answers but can result in false positives, if for example the video-object related answers are not the correct ones. This is possible because the correct answers often contain the actions (or verbs) presented in the videos though without the objects. To study the problem, we randomly sample 30% (~1.2K to save API budgets) of the validation data from NEXT-QA. We then check for the samples that have the aforementioned VA and QA short-cuts. For the target samples, we curate new answers for each question on the basis of the original candidate answers to avoid the possibility of having such shortcuts, i.e., do little change to make most of the candidate answers contain the corresponding video objects or question keywords as shown in the bottom of Fig. 7. In our implementation, we prompt GPT-4 to achieve the goal, followed by human checking and correction as necessary. Specifically, to eliminate VA short-cut, we first obtain from the related video a collection of visual object labels. We begin by detecting object labels from the sampled video frames by using image-tagging model RAM and then filter for redundancy before merging the labels together. With the object labels and the original QAs, we then prompt GPT-4 to incorporate into each candidate answer the appropriate visual label to make it relate to the video. For QA short-cut, we follow a similar practice to prompt GPT-4 to reword each answer option to contain keywords in the question. Note that the identification of samples with short-cuts are completed at the same time by asking GPT-4 to first check before modifying the answers (details in Appendix .2). We removed samples whose candidate answers are the same as the original ones as these samples are identified by the GPT-4 as qualified samples without VA and QA short-cuts. Eventually, we obtain two different sets of samples with both size of 1K for probing of VA and QA short-cuts respectively. The high short-cut rates (1K out of 1.2K) is because our short-cuts are defined on nouns while NEXT-QA features verbs in its answers. The results in Fig. 8 show that almost all models, regardless of the language models used, suffer substantial declines in accuracy (10% ~ 20%) when answering questions with edited distractor answers see prediction examples. The flip rates are also high with 21% ~ 37% even though the triplets of remain the same. This posits that the models largely rely on the short-cuts in candidate answers and are limited in faithful multimodal reasoning from video and question to correct answers. Again, GPT-40's zero-shot performance surpasses others on the edited data. Compared with non-LLM method (CoVGT), all Video-LLMs, especially VideoChat2, show higher flip rates when confronted with the edited options. This suggests that Video-LLMs are better at exploiting the short-cuts of candidate answers for question answering. By comparing the performances on QA and VA tests, we find that most Video-LLMs experience greater performance loss on edited VA data compared to QA data, especially for GPT-40 whose performance degenerates marginally on QA questions (2.6%) but more on VA type questions (9.9%). Such a discrepancy suggests that the models are primarily exploiting spurious vision-text correspondence for answer prediction. Additionally, we extend our experiments to the recent NEXT-OOD dataset. NEXT-OOD is also designed to reduce candidate answer biases in NEXT-QA. It aims for a balanced distribution of objects and actions in the correct and distractor answers, whereas our VA and QA short-cuts pertains to"}, {"title": "Robustness", "content": "The brittle behavior of models in the above studies prompts us to conduct a more general test on model robustness towards common perturbations on the videos and questions. To this end, we consider two kinds of perturbation: data diversifying and data poisoning, and experiment on both MCQA and OEQA. For data diversifying, we modify the original questions by rephrasing them or prepending spoken phrases (see examples in the bottom of Fig. 10), both are achieved by prompting GPT-4 with human correction as necessary. Robust models should be invariant and not flip their predictions for this perturbation, since the meanings of questions keep unchanged. For data poisoning, we attack the input data by shuffling video frames, shuffling question words, and removing keywords from the questions (e.g., question words 6W1H2, nouns, and verbs). Examples are presented in the first three cases of Fig. 10. Intuitively, models should suffer significant performance loss with poisoned data. For data shuffling, we repeat the experiment 3 times with different random seeds and find there is negligible variance in performance (\u00b1 0.1%). Therefore, we report only the median results for brevity. Additionally, for data poisoning, we revisit the issue of \"answering incomplete questions\" explored by , analogous to human behavior of answering by listening to only a part of the question. We realize this by progressively truncating the question, starting from the first word and increasing the omitted portion. For probing of model robustness in data diversifying, the results (both accuracy and keep rate (1 - flip rate)) show that almost all models deteriorate performances, especially in OEQA. Yet, we surprisingly find that GPT-40' accuracy gets improved by 3% when adding spoken phrases, we attribute this strength of GPT-40 to its learning from numerous real-life user cases. Additionally, we highlight the weak behavior of the general-purpose Video-LLMs in answering questions with spoken prefixes in OEQA. For example, the accuracy degradation rates and flip dates of Video-LLaVA and LLaMA-VID are both higher than those of non-LLM method VIOLETv2. Finally, comparing the same model (LLaMA-VID) of different LLM sizes (7B and 13B) reveals that larger LLMs improve accuracy but not necessarily invariance to data diversity. For better interpretation, we visualize some predictions in Fig. 12. The general purpose Video-LLMs tend to change their answers with the edited questions even though the questions' meanings remain the same. Generally, they prefer giving more specific answers, but always result in hallucinations or ignore the actual questions (e.g., refer to examples at the bottom of Fig. 12). A likely reason could be that the models are not instruction-tuned to answer such edited questions. Another cause could be that the modified questions are often longer and appear more complex than the original ones. Consequently, LLMs are induced for more detailed responses point by point, which increases the risk of hallucination. In conclusion, the experiments demonstrate that the models are not sufficiently robust to language diversifying. For probing of model robustness in data poisoning, Fig. 11c shows that after shuffling the video frames, all models' accuracies have little to no change, be it on MCQA or OEQA. Also, the flip rates are small. The exception, however, is GPT-40, which has a ~ 4 to 5x higher flip rate than other models. This result echos the findings where GPT-40 conditions better on the video content for question answering. Yet, for all models, the negligible fluctuation of"}, {"title": "Generalizability", "content": "Video-LLMs, by virtue of using LLMs, are assumed to encode some common sense and open-world knowledge. To that end, their decisions should be less affected by the QA training data and generalize better. Our previous probes implicitly test model generalization to a certain extent by curating new data. This section studies two classic generalization scenarios more explicitly: generalization to long-tailed data within dataset and generalization to out-of-distribution (OOD) data across datasets. Both long tails and OOD data conflict and do not reflect the training data on which the models are developed. It is worth mentioning that the general-purpose and tool-based Video-LLMs have already shown excellent zero-shot performance in our previous probes. Thus, we focus on analyzing the specialized Video-LLMs that are fintuned on the target datasets in this section. 3.8.1 Generalization to long-tailed data\nFor this experiment, we investigate the models' predictions at a per-answer basis. We consider open-ended QA with close-set classification setting where each answer is treated as a class label, e.g. MSVD-QA. Specifically, we analyze an answer' frequency and its accuracy, as well as the frequency of it being a false positive prediction. An answer's accuracy stands for the proportion of correctly answered questions over all questions with that answer as ground-truth answer. We experiment with FrozenBiLM and VIOLETv2 as representatives of Video-LLMs and non-LLMs for close-set classification respectively. We present the results of FrozenBiLM. VIOLETv2 shares almost the same distribution and is omitted from the plot for clarity. The answers' frequency distribution and accuracy distribution has a correlation coefficient of only r = 0.2 for both Frozen-BiLM and VIOLETv2. In contrast, the coefficient value between answers' frequency distribution and the frequency distribution of an answer being a false positive prediction is 0.9 for both methods. The experimental results suggest that there is little to no correlation between an answer's frequency and its accuracy for both LLM and non-LLM methods. However, there is a strong positive correlation between an answer's frequency and the frequency of it being a false positive prediction, and this stands true for both LLM- and non-LLM based methods. While we suspect that the LLM in Frozen-BiLM may not be large enough (~1B), the experiments indeed suggest that finetuned (or specialized) Video-LLMs are similar to non-LLM method in generalizing to long-tailed data distribution. 3.8.2 Generalization to OOD data\nThe test samples in all previous probes are either derived from the same types of questions or from the same dataset. Here, we focus on generalization across different question types and datasets."}, {"title": "Discussion", "content": "We summarize the major limitations of Video-LLMs and discuss possible directions for improvement. Robustness and Trustworthiness. All probes showed that the open-sourced Video-LLMs be it general-purpose or fine-tuned, perform well for standard question-answering, but suffer under adversarial language modifications while being unaffected by analogous video modifications. These findings reveal a lack of robustness and interpretability and underscores the need for rationales in Video-LLM developing. Some recent studies in multimodal chain-of-thought and modular multi-agent approaches show improved robustness and interpretability. Yet, one needs to mind the inefficiency issues of such techniques. In addition, video multimodal benchmarks are emerging, but almost all of them target comprehensive video understanding, with little attention on interpretability. Related benchmarks such as VIP and NEXT-GQA are thus valuable efforts in developing trustworthy Video-LLM techniques, and deserve more research efforts. Efficient and Fine-Grained Long-Form Video Understanding. Our observations about GPT-40's behavior in VideoQA as well as the rapid evolution of SOTA results on common VideoQA benchmarks suggests that coarse and holistic understanding of short videos (less than 20s) are solvable by scaling up existing Video-LLMs or operating powerful image MLLMs (like GPT-40) frame by frame. Fine-Grained Long-Form Video Understanding manifest two major challenges for existing Video-LLMs: The first is a 'needle in the haystack' challenge search in the long video for short moments (or key frames) that carry the correct answer. Existing benchmarks such as NEXT-GQA and Ego4D-QA embody such challenges, yet effective techniques are lacking. Recent solutions show promise, but they are inefficient, brute-force methods that traverse the whole video frame-by-frame (or segment-by-segment) and perform LLM inference multiple times. Thus, efficiently understanding long videos is still an open problem. The second challenge is fine-grained understanding of very long-form videos, with large temporal certificates, as defined in EgoSchema. Our studies show that existing Video-LLMs targeting short clips do not generalize well to such videos. The aforementioned traversal solutions are ineffective as the task requires reasoning over and weaving together multiple frames or segments to achieve holistic understanding. Recent memory- and caption-based solutions as well as the direct long-frame-encoding solutions offer some insights to handle long videos, but understanding the videos in efficient and fine-grained manner remains a key challenge. Egocentric and Embodied QA. We primarily analyzed VideoQA with datasets featuring third-person view (exo-centric) videos and QAs, be it effective as benchmark evaluation for Video-LLMs. Yet noteworthy an important real-world application for VideoQA is geared towards reliable embodied QA assistance. In this regard, first-person view (ego-centric) VideoQA are of greater interest and relevance. shows that Video-LLMs pre-trained on third-person view VideoQA datasets can generalize to these application scenarios, but the performance are weak compared with the tool-based Video-LLMs that directly use ego-centric pretrained visual descriptions as well as the well-prompted general-purpose MLLMs. Therefore, the efforts of leveraging existing MLLMs for ego-centric VideoQA will be of critical importance to initiate successful embodied assistance applications."}, {"title": "Conclusion", "content": "This paper has vetted Video-LLMs' performance in VideoQA by probing their success and failure modes with well-designed adversarial tests. While Video-LLMs generally show better QA accuracy, the decrease rates and flip rates align with or even higher than non-LLM methods when faced with adversarial challenges. Specifically, Video-LLMs show significant limitation in coping with video temporality, of both reasoning the chronological content order and grounding the temporal moments to substantiate the answers. Also, they are unresponsive towards video perturbation while being susceptible to simple language variations of questions and candidate answers. Additionally, Video-LLMs, after finetuned, are not necessarily generalize better. Understanding these limitations is crucial for developing future Video-LLMs and VideoQA techniques, where we also conclude some promising directions to proceed. Limitations We note that Video-LLM research is emerging, and there are other powerful models we may have missed testing. We thus release all our test data: https://github.com/doc-doc/VideoQA-LLMs."}]}