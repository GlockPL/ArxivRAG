{"title": "FashionR2R: Texture-preserving Rendered-to-Real Image Translation with Diffusion Models", "authors": ["Rui Hu", "Qian He", "Gaofeng He", "Jiedong Zhuang", "Huang Chen", "Huafeng Liu", "Huamin Wang"], "abstract": "Modeling and producing lifelike clothed human images has attracted researchers' attention from different areas for decades, with the complexity from highly articu-lated and structured content. Rendering algorithms decompose and simulate the imaging process of a camera, while are limited by the accuracy of modeled variables and the efficiency of computation. Generative models can produce impressively vivid human images, however still lacking in controllability and editability. This paper studies photorealism enhancement of rendered images, leveraging generative power from diffusion models on the controlled basis of rendering. We introduce a novel framework to translate rendered images into their realistic counterparts, which consists of two stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG). In DKI, we adopt positive (real) domain finetuning and negative (rendered) domain embedding to inject knowledge into a pretrained Text-to-image (T2I) diffusion model. In RIG, we generate the realistic image corre-sponding to the input rendered image, with a Texture-preserving Attention Control (TAC) to preserve fine-grained clothing textures, exploiting the decoupled features encoded in the UNet structure. Additionally, we introduce SynFashion dataset, featuring high-quality digital clothing images with diverse textures. Extensive experimental results demonstrate the superiority and effectiveness of our method in rendered-to-real image translation.", "sections": [{"title": "1 Introduction", "content": "Modeling and simulating digital humans and clothing has achieved significant progress [1, 2, 3, 4, 5], while leveraging these 3D assets for fashion e-commerce still remains a challenging problem. Due to the imperfection of 3D models and the approximation in rendering algorithms, rendered images cannot yet replace fashion photos taken by a camera, with deficiency in the realism of rendered human faces and skin, clothing shape and fabric, etc. This paper studies transferring rendered fashion images into their realistic counterparts, which is inherently an Image-to-Image (I2I) translation problem.\nExisting works on improving the realism of rendered images mainly resort to retrieving and blending real image patches [6], or train a GAN-based network [7, 8, 9] due to lack of paired training data. Another line of works can tackle this problem as general I2I translation [10, 11, 12, 13]. However, these methods may still suffer from several limitations: Firstly, their image generation pipelines have limited power to utilize real image resources for highly-detailed enhancement and may suffer from instability and mode collapse from adversarial training. Moreover, they either focus on indoor/outdoor\n* Work done during an internship at Style3D Research.\nThese authors contributed equally to this work.\n#Corresponding author.\nPreprint. Under review."}, {"title": "2 Related Works", "content": "Improving the realism of rendered images has been a long-standing problem due to the inherent limitations of rendering pipelines and the rich potential for commercial applications. CG2Real [6] proposes to retrieve similar images from a large collection of real photos and then applies local style transfer to upgrade color, tone and texture of the CG image. Deep CG2Real [7] adopts a two-stage deep learning framework to first transfer OpenGL images to PBR (Physically-Based Rendering) images, and then translates PBR to real images, disentangling lighting and texture in a CycleGAN-like [23] framework. [8] enhances photorealism under the guidance of a set of input G-buffers and learns the network with a perceptual discriminator. [9] proposes to learn a rendered image generator for human faces, which can encode the same face identity but different \"style\" from a real face image generator, based on StyleGAN [24, 25]. These methods all utilize limited data for generative training, while we propose to adapt diffusion models pretrained on large datasets for better image generation quality. Besides, applying these methods to fashion images often leads to the failure to preserve fine-grained clothing textures."}, {"title": "2.1 Rendered-to-real Image Translation", "content": "Transferring a rendered fashion image into its realistic counterpart is inherently an image-to-image (I2I) translation problem, which has attracted wide interest in different realms of research [26, 27,"}, {"title": "2.2 Image-to-image Translation", "content": "Transferring a rendered fashion image into its realistic counterpart is inherently an image-to-image (I2I) translation problem, which has attracted wide interest in different realms of research [26, 27,"}, {"title": "3 Method", "content": "In diffusion framework, the forward diffusion process begins by generating noisy images xt from clean images x0 sampled from a specified data distribution, accompanied by"}, {"title": "3.1 Preliminaries", "content": "their respective noise labels e. These pairs are used to train a score estimator [70] \u000f\u03b8 usually based\non the UNet architecture. The score estimator can serve as an effective approximation of the score\nfunction \u2207x log p(x) which directs the inverse denoising process to generate new data samples.\nWith distinguished capabilities in synthesizing images, the Latent Diffusion Model (LDM) [14] is\nselected as the backbone of our method. The LDM employs a pre-trained AutoEncoder to transform\nthe diffusion process from pixel space to latent space and integrates a conditional branch, facilitating\nfaster training and more flexible embedding of conditions. Specifically, the pre-trained Encoder\nE(\u00b7)first encodes images into latent space z = E(x). Following this, the score estimator network \u000f\u03b8\nis trained by taking the latent z, step t and conditions c as input to predict the noise labels:\nmin E z=E(x),\u000f\u223cN(0,I),t\u223cU(1,T) ||\u000f \u2212 \u000f\u03b8 (zt, t, c)||2  (1)\nFor text to image generation task, condition c is usually the text embedding generated from text\nprompt y through a tokenizer and a pretrained CLIP [71] model c = \u03c4(y). The intermediate noisy\nlatent zt is generated through the formula [63]:\nzt = \u221a\u03b1(t)z0 + \u221a1 \u2212 \u03b1(t)\u000f, \u000f \u223c N(0, I) (2)\n\u03b1 is the cumulative product of the noise coefficients at each step. During the sampling process, the\ntrained score estimator takes random Gaussian noise as input, along with text embedding as condition.\nIt progressively predicts the noise added at each step, completing the denoising process to obtain z0.\nThe final image is obtained by the pretrained decoder z0 = D(z0).\nTextual Inversion. Textual inversion [16] introduces a new paradigm to T2I generation models,\nallowing the model to learn a new concept by setting a placeholder token \"[C]\" and obtaining the\ncorresponding text embedding v\u02c6 as a learnable vector. This vector is then trained and optimized using\na few images represent this new concept:\nv\u02c6 = arg min E z=E(x),\u000f\u223cN(0,I),t\u223cU(1,T) ||\u000f \u2212 \u000f\u03b8 (zt, t, v)||2  (3)\nDuring training, the network parameters are all fixed, only the embedding is optimized.\nDDIM Sampling and Inversion. Inversion is an effective method for finding the corresponding\nnoise map of an image and achieving training-free control during the generation process. DDIM\ninversion is widely used due to its clear principles and easy implementation. The DDIM sampling\nprocess is [17]:\nzt\u22121 =\u221a \u03b1t\u22121\u03b1tzt \u2212 \u221a 1 \u2212 \u03b1t\u03b1t\u000f\u03b8 (zt, t, c) + \u221a 1 \u2212 \u03b1t\u22121\u03b1t\u03b1t\u22121\u000f\u03b8 (zt, t, c) (4)\nBy simply assuming zt\u22121 \u2248 zt and rewriting the sampling process in reverse direction, the following\nDDIM Inversion [17] formula is given:\nzt\u22121 \u2248\u221a \u03b1t\u22121\u03b1tzt \u2212 \u221a 1 \u2212 \u03b1t\u22121\u03b1t\u000f\u03b8 (zt\u22121, t, c) + \u221a 1 \u2212 \u03b1t\u03b1t\u22121\u000f\u03b8 (zt\u22121, t, c) (5)\nUnlike direct noise addition, the DDIM Inversion allows for the original information of the image to\nbe well preserved, enhancing the stability in the subsequent generation process."}, {"title": "3.2 Overall Pipeline", "content": "Given a computer-rendered fashion image xcg, the goal of our method is to transform it into a corresponding realistic image xr while preserving the garment's detailed textures. Defining realism and helping model understand what is \"realistic\" remains an open question. The challenge can be divided into two sub-tasks: one is making the fashion image appear realistic by enhancing aspects like wrinkles, lighting and color, which reflect true-to-life expressions. Another one is to maintain the texture details of the garment to achieve fine-grained, controllable generation.\nAs shown in Fig. 1, our method comprises two stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG). During the DKI phase, we infuse the model with information from both the rendered and realistic domains through fine-tuning and domain inversion. In the subsequent generation phase, we utilize negative domain embedding Und to stimulate the model's potential for generating realistic images and employ self-attention control to preserve texture details. For a better understanding, details will be further elaborated in Section 3.3 and Section 3.4."}, {"title": "3.3 Domain Knowledge Injection", "content": "To enhance the ability of the base SD model \u000f\u03b8 to generate realistic images, especially concerning the appearance of garments and models, we use real studio-shot images xtr to fine-tune the base model. This process injects real domain information into the model, thereby increasing its potential to generate authentic visual details, the fine-tuning process can be formulated as:\n\u000f\u2217 \u03b8 = arg min E z=E(xtr),\u000f\u223cN(0,I),t\u223cU(1,T) ||\u000f \u2212 \u000f\u03b8 (zt, t, \u03c4tr)||2  (6)\nwhere e is the pretrained SD model, Utr is the embedding of the text description of the Xtr.\nFor the source domain rendered data, we hope that the model can understand its characteristics and deviated from the rendered data manifold as much as possible during the generation process. After the first step fine-tuning, we assume that the model has already enhanced its representation of the real domain manifold. If we can make the model deviate from the rendered data manifold, it can better express the characteristics of realistic images.\nwe ex-pand the concept of Textual Inversion\nto Domain Inversion. We train a neg-ative domain embedding on a fine-tuned base model using a large num-ber of rendered images. This negative\ndomain embedding guides the model\nto avoid certain content, here is the\nrendered domain characteristics, dur-ing the generation process.\n \u00d4nd = arg min E z=E(xcg),\u000f\u223cN(0,I),t\u223cU(1,T) ||\u000f \u2212 \u000f\u03b8 (zt, t, v)||2  (7)\nDuring the training of negative domain embedding, we freeze the parameters in the fine-tuned model\ne, and find the Und through direct optimization with a certain number of rendered images."}, {"title": "3.4 Target Domain Knowledge Injection", "content": "After domain knowledge injection, we can use the negative domain embedding to guide the model in generating realistic images. During each denoising step, the negative domain embedding guidance is defined by:\n\u03b8 (zt, t, Und) = w \u00b7 \u03b8 (zt, t, v\u00f8) + (1 \u2212 w) \u00b7 \u03b8 (zt, t, Und) (8)\nwhere vo denotes the embedding of Null text. With a guidance scale w larger than 1, the negative\ndomain embedding becomes effective. Unlike traditional CFG guidance, here we do not use any\npositive prompts processed through CLIP to obtain the embedding as conditions. Instead, we directly"}, {"title": "3.5 Realistic Image Generation", "content": "where y is the parameter that indicates how many steps before the TAC should be applied and f is\nthe feature size of different layers, only those layers exceeding the specified size F undergo TAC,\nparticularly in the shallow layers. Specifically, the cg-domain self-attention features are derived from\nthe reverse sampling process starting from the noisy latent, which is obtained by performing DDIM\ninversion on the input image latent. In contrast, the r-domain self-attention features differ due to the\nincorporation of negative domain guidance and the self-attention injection."}, {"title": "4 Experiments", "content": "To evaluate our method and conduct comprehensive comparisons, we introduce a high-quality\nrendered fashion image dataset, named Synthetic Fashion (SynFashion), with the professional garment\ndesign software Style3D Studio. SynFashion consists of 10k rendered images in 20 categories,\nincluding pants, T-shirt, lingerie and swimwear, half skirt, hoodie, coat, jacket, set, home-wear,\nhat, Hanfu, jeans, shorts, down jacket, vest and camisole, shirt, suit, dress, sweater and trench coat.\nFor each category, we use Style3D Studio to build 10 to 40 projects in different 3D geometry with\ncorresponding texture and design, and then randomly sample several new textures to change its\nappearance. There are overall 375 projects in 3D and 500 additional texture collected from Internet.\nFor each textured 3D geometry, we render four views, including front, back, and two randomly\nsampled views. After rendering, we crop the enlarged garment area of each image and resize it to 768\n\u00d7 1024. Due to legal issues, some of the images contain a digital human figure but not the complete\nface. To supplement the evaluation on rendered human faces, we also conduct experiments on the\npublic available Face Synthetics dataset [1] with its first 10k images."}, {"title": "4.1 Datasets", "content": "We implement our method with pretrained Stable Diffusion (SD) model and finetune the base model with 2500 realistic images at a 1024 \u00d7 1024 resolution for source domain knowledge injection. The finetuning uses images from iMaterialist (Fashion) 2019 FGVC dataset [72], based on the publicly available SD v1.5, and is conducted on 2 RTX 4090 with a batch size of 6. Based on the finetuned model, we train our negative domain embedding with 2500 rendered images on a single RTX 4090 with a batch size of 1. The rendered images are resized to the resolution of 512x 512. The placeholder embedding size is 75 and the learning rate is 5e-4. During sampling, we perform DDIM sampling with default 50 denoising steps with a denoising strength of 0.3 as default. The y is set to 0.9 as default, which means that the TAC is performed on the first 90% of sampling steps. Only the attention maps in the first and second shallow layers are used for TAC. Note that the denoising strength and y may be changed to obtain different level of image translation. We compare our method with three state-of-the-art unpaired image-to-image translation method, CUT, SANTA and UNSB, and one diffusion-based style transfer method VCT. For CUT, SANTA and UNSB, we train the models for about 400 epochs following the official code with same training data."}, {"title": "4.2 Implementation Details", "content": "Qualitative Results Fig. 3 and Fig. 4 show the visual comparison between our method, CUT [10], SANTA [11], UNSB [12] and VCT [13] on the SynFashion and Face Synthetics datasets. As can be seen from the figures, both the CUT and SANTA methods exhibit some degree of image degradation and fail to effectively learn the concept of image realism from data across rendered and real domains, thus enable to generate realistic images. The diffusion based style transfer method VCT maintains image quality but fails to extract realistic image features from the guidance image, also resulting in the loss of image details. Compared to previous methods, the UNSB method achieves better consistency in terms of content, but like CUT and SANTA, it performs poorly in maintaining color fidelity and the realism effect is not good. The proposed method effectively enhances the overall realism of the image, particularly in capturing the facial and hand features of models, as well as the texture and wrinkle details of the garment.\nQuantitative Results The absence of ground truth for rendered-to-real translation and domain gap between the source rendered and target real domains make quantitative evaluation challenging.\nFollowing the previous work [8], we use KID to evaluate the realism of the generated images and the average SSIM and LPIPS to assess content similarity. For each dataset, we use the 7500 testing result images from each method and calculate the KID against the realistic images and the SSIM/LPIPS against the rendered images. As shown in Tab. 1, our method shows significant improvements in terms of realism as well as overall texture and content consistency. The standard deviations here show the variance over test inputs for a fixed model to demonstrate the stability and generalization ability.\nWe adopt user studies to provide more quantitative insight into perceived realism, image quality, and consistency to input rendered images. We follow StyleDiffusion [73] in style-transfer and compare our method to previous works in pairs. Specifically, we randomly sample 100 image pairs from each dataset for user evaluation. Each pair contains one image generated by our method and a corresponding image generated by another comparison method, presented side by side in random order. Users are asked to assess the images based on three criteria: 1) which result appears more realistic, 2) which result demonstrates overall better image quality, and 3) which result shows better consistency with the reference image.\nWe collected approximately 2,000 votes per question from 20 users and present the percentage of votes where existing methods were preferred over ours in the Tab. 2. Lower percentages indicate that our method was favored over the competitors. Our approach garnered a strong preference in terms of overall realism and image quality, while also showing a clear advantage in maintaining consistency with the reference images."}, {"title": "4.3 Results", "content": "We conduct ablation study on two datasets in a drop-one-out manner and evaluate the performance of each module in the proposed method during inference and analyze the impact on the final results. As shown in Fig. 5, without source DKI (embedding), the fine-tuned base model tends to recover the input rendering image with DDIM inversion. Without target DKI (fine-tuning), the rendering effect slightly decreases but the output is still not real enough due to lack of concentrated knowledge on real human and clothing. Without TAC, the semantic structure such as face identity and clothing design can significantly deviate from the input. The quantitative results are in Tab. 3.\nFig. 6 shows the trade-off between image realism and texture preservation. With a high denoising strength, the generated images resemble realistic images more closely but retain fewer details from the original rendered image. Increasing the TAC ratio helps to better preserve the texture details and facial features. Unlike other content preservation techniques such as inpainting, which can lead to potential visual incoherence, our TAC seems to blend the attention features smoothly into the generation process and cause no obvious coherence issues."}, {"title": "4.4 Ablation Study and Further Analysis", "content": "In this paper, we introduce a novel diffusion-based framework for rendered-to-real fashion image translation and create a high-quality rendered fashion image dataset (SynFashion), which includes 10k images with multiple classes. With Domain Knowledge Injection (DKI) and Texture-preserving Attention Control (TAC), our method can successfully translate the rendered fashion image into its realistic counterpart with significant realism improvement and texture details preservation. Extensive experimental results demonstrate the superiority and effectiveness of our method."}, {"title": "5 Conclusion", "content": "While our method achieves superior results on this challenging task, there are still several problems to be further explored. In this work, we simply use DDIM inversion to extract texture-related attention features. However, the inversion process slows down the generation, requiring approximately one minute to translate an image with a resolution of 768 \u00d7 1024. This could potentially be accelerated by recent inversion-free methods. We test the inference time and resource consumption for a 512x512 image on an RTX 3090, as shown in Tab. 4. Note that comparing to VCT, which is also based on diffusion, our method takes much less memory and time during testing as we do not need to perform additional optimization for each testing image. Our method cannot handle real-time applications for now, but has potential for improvement with future integration with SD Turbo or SD Lightning. Additionally, for different images, finding the optimal balance between the TAC ratio and denoising strength may require more empirical refinements to achieve the best result. Due to limitations on computational resources, experiments were not conducted on more advanced models such as SDXL [74]. Given that our method is based on SD1.5 and for human-related content generation, potential negative societal impacts of exploiting this method could be violation of portrait rights, racial bias, or inappropriate content in generation when the denoising strength is high. Relative solutions can include but are not limited to using authorized, diverse and balanced training data and training detection models to prevent inappropriate content generation."}, {"title": "6 Limitations and social impacts", "content": "Donoising Strength\nSource Image\n\n90%\n\n0%\nTarget Image\nRatio of TAC control steps\nFigure 6: A visual example of tuning TAC ratio and denoising strength.\nTable 4: Comparison of memory required and testing time across different methods."}, {"title": "7 Acknowledgments", "content": "This work is supported in part by the National Key Research and Development Program of China\n(No: 2021YFF0501503) and by the Talent Program of Zhejiang Province (No: 2021R51004)."}]}