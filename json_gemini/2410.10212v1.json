{"title": "Large Language Model-Enhanced Reinforcement Learning for Generic Bus Holding Control Strategies", "authors": ["Jiajie Yu", "Yuhong Wang", "Wei Ma"], "abstract": "Bus holding control is a widely-adopted strategy for maintaining stability and improving the operational efficiency of bus systems. Traditional model-based methods often face challenges with the low accuracy of bus state prediction and passenger demand estimation. In contrast, Reinforcement Learning (RL), as a data-driven approach, has demonstrated great potential in formulating bus holding strategies. RL determines the optimal control strategies in order to maximize the cumulative reward, which reflects the overall control goals. However, translating sparse and delayed control goals in real-world tasks into dense and real-time rewards for RL is challenging, normally requiring extensive manual trial-and-error. In view of this, this study introduces an automatic reward generation paradigm by leveraging the in-context learning and reasoning capabilities of Large Language Models (LLMs). This new paradigm, termed the LLM-enhanced RL, comprises several LLM-based modules: reward initializer, reward modifier, performance analyzer, and reward refiner. These modules cooperate to initialize and iteratively improve the reward function according to the feedback from training and test results for the specified RL-based task. Ineffective reward functions generated by the LLM are filtered out to ensure the stable evolution of the RL agents' performance over iterations. To evaluate the feasibility of the proposed LLM-enhanced RL paradigm, it is applied to various bus holding control scenarios, including a synthetic single-line system and a real-world multi-line system. The results demonstrate the superiority and robustness of the proposed paradigm compared to vanilla RL strategies, the LLM-based controller, and conventional space headway-based feedback control. This study sheds light on the great potential of utilizing LLMs in various smart mobility applications.", "sections": [{"title": "1. Introduction", "content": "Bus holding control is widely recognized as an efficient strategy for maintaining the stability of bus systems and minimizing passenger waiting time by preventing bus bunching (Wu et al., 2017; Gkiotsalitis and Cats, 2021). It can be achieved by holding an early bus at the stop until it adheres to the predefined schedule or target headway, classified as schedule-based or headway-based holding control, respectively (van der Werff et al., 2019). The latter has been proven to be more suitable for high-frequency bus systems (Dai et al., 2019). Numerous optimization-based and model-based feedback holding control strategies have been proposed to determine the appropriate holding duration for buses (Gkiotsalitis and Cats, 2019; Berrebi et al., 2018; Wang et al., 2024c). These studies demonstrate the capability of bus holding control and provide analytical solutions for its implementation. However, the model-based methods require accurate knowledge of bus propagation and passenger arrival patterns, which are difficult to obtain in real-world applications characterized by fluctuating traffic dynamics (Wang and Sun, 2020; Rodriguez et al., 2023; Lee et al., 2022). Consequently, model-free methods are gaining traction in developing bus holding control strategies.\nDrawing on their broad applicability in traffic control tasks, Reinforcement Learning (RL) methods have been explored for bus holding control, demonstrating promising performance outcomes while enabling the model-free property (Geng et al., 2023; Chen and Chen, 2022; Yu et al., 2023b). For example, Wang and Sun (2020) utilized a Multi-Agent Reinforcement Learning (MARL) method to derive the real-time bus holding control strategy in a single bus line system. The robustness of the MARL-based bus holding control strategy was further enhanced by Wang and Sun (2023b) through the use of implicit quantile networks and meta-learning. Wang and Sun (2023a) applied the MARL method to design the holding strategy in a bus system with multiple lines sharing a common corridor. The cooperation between two bus lines was defined and achieved through weight evolution in the reward function. These studies demonstrate that RL-based methods offer significant advantages for formulating bus holding control strategies, particularly for real-time applications in dynamic environments with fluctuating passenger demands.\nAlthough existing RL-based bus holding control strategies have shown promise, they also exhibit several limitations. First, these strategies are tailored for specific operational scenarios such as single-line or multi-line services, thereby limiting their transferability and adaptiveness across varying real-world scenarios. In practical control tasks, numerous scenarios can arise due to the varied configurations of real-world bus systems. Therefore, the transferability and adaptiveness of bus holding control strategies are crucial, posing challenges in developing generic strategies capable of handling multiple scenarios (Ke et al., 2020; Han et al., 2023). Second, the evaluation metrics and objectives used in these strategies, such as the overall efficiency of bus systems or the variance in bus headways over an operation period, tend to be delayed and sparse (Karnchanachari et al., 2020; Eschmann, 2021). RL agents typically receive accurate feedback from the environment only at the end of a task or after a delay following the action execution (Yu et al., 2023d; Chan et al., 2024). Consequently, decomposing these sparse objectives (i.e., sparse rewards), which do not provide clear signals to evaluate the effectiveness of individual actions, into dense learning signals (i.e., dense rewards) at each step within the RL framework typically necessitates substantial input from human experts and iterative manual adjustments. (Cao et al., 2024; Xie et al., 2023). This challenge is not unique to bus holding control but is also common in many other control scenarios, such as robotic control, potentially compromising the efficacy of RL methods in these real-world applications. Developing more generic and automated designs for RL-based control applications requires further exploration.\nThe recent advent of large language models (LLMs) has significantly enhanced the AI domain with their substantial capabilities (Cao et al., 2024). Leveraging pre-trained world knowledge, LLMs exhibit notable in-context learning and reasoning abilities, surpassing previous AI technologies (Yu et al., 2023c; Huang and Chang, 2022; Wei et al., 2023). The LLMs can understand and process multi-modal information and incorporate domain-specific knowledge through tools such as plugins, APIs, and expandable models (referred to as LLM-as-agent) or through Supervised Fine-Tuning (SFT) using domain-specific data. The expansive capabilities of LLMs pave the way for potential advancements in Artificial General Intelligence (AGI) (Ge et al., 2024). As LLMs surpass human performance in large-scale data analysis and processing speed, they hold the potential to enhance the efficacy of RL in complex control tasks by substituting human expert input with contributions from LLMs. This substitution may also provide an opportunity to increase the automation in the formulation phase of RL-based control method (Yang et al., 2024).\nTo improve the transferability and effectiveness of RL-based control methods in real-world applications, such as bus holding control, LLMs offer significant advantages in information processing and RL performance enhancement (Cao et al., 2024). Figure 1 illustrates the integration and enhancement of RL-based control methods through LLMs. From the perspective of information processing, LLMs are capable of managing complex environmental data and task instructions, extracting essential features from multi-modal observations, and analyzing the reasons for failure experiences in task execution (Pang et al., 2023; Adeniji et al., 2023). Regarding RL performance enhancement, LLMs can improve RL by assisting in the design of the agent, interpreting improvement suggestions from feedback, and generating executable code for reward functions (Cao et al., 2024). Yu et al. (2023d) integrated LLMs into RL-based robotic tasks by defining reward parameters. They prompted the LLM to generate several reward function candidates, selecting the best-performing one for the RL agents, which improved the pass rates of the robotic tasks. Li et al. (2024) designed an iterative framework to automatically initialize and improve the reward functions for an RL-based game player. They prompted the LLM to formulate the Chain of Thought (CoT) to provide interpretable reward functions. With feedback from failure trajectories and reward modifications, the RL agent achieved significant improvements in success rate and learning efficiency in the game. These studies suggest that integrating LLMs in the formulation of RL-based control methods can eliminate the need for expert input and extensive manual trial-and-error, and make the reward function more interpretable (Cao et al., 2024; Li et al., 2024). However, the interaction patterns between LLMs and RL in these studies overlook the inefficient outputs from LLMs due to the misunderstanding of input prompts, which can disrupt the improvement loops of the reward function and lead to unstable performance for RL agents"}, {"title": "2. Literature review", "content": "To summarize, the limitations and emerging challenges of existing studies on RL-based bus holding control strategies and LLM applications are primarily reflected in the following aspects: 1) designing an appropriate dense reward function for the RL method in control scenarios with sparse and delayed goals is challenging; 2) existing RL-based bus holding control strategies lack adaptiveness and generalization across different bus systems; 3) existing LLM-enhanced RL methods do not ensure stable and reliable performance for the RL agents due to the occasional inefficient output of LLMs.\nTherefore, this study develops a novel LLM-enhanced RL paradigm for general control tasks, and we apply the paradigm to various bus holding control scenarios. In this paradigm, the LLM is utilized to generate interpretable reward functions for RL agents. Several LLM-based modules are designed for reward initialization, reward modification, agent performance analysis, and reward refinement. Through the cooperation of these modules, the reward function can be iteratively and automatically improved. The proposed iteration rules ensure stable improvements and reliable performance for RL agents. The capability and adaptiveness of the proposed paradigm across multiple bus holding control scenarios are verified through numerical tests.\nThe main contributions of this study are summarized below:\nAn LLM-enhanced RL paradigm is designed for general control tasks. This paradigm can automatically initialize and improve RL reward function without requiring human expertise input and manual trial-and-error. The designed iteration rule ensures the reliability and interpretability of the final reward function.\nThe LLM-enhanced RL paradigm is applied to derive bus holding control strategies, demonstrating applicability to various scenarios, including single-line and multi-line systems. The adaptiveness of the proposed paradigm to generic bus holding control scenarios is verified.\nThe proposed bus holding control strategy is tested with both synthetic and real-world bus systems, showing promising performances and robustness compared to vanilla RL, LLM-based, and conventional space headway-based feedback controllers.\nThe remainder of the paper is organized as follows. Section 2 provides the literature review on related studies. Section 3 formulates the RL-based bus holding control strategy. Section 4 exhibits the overall framework of the proposed LLM-enhanced RL paradigm and the function of each module. Section 5 describes the settings of two numerical tests and evaluates the performance of the proposed paradigm. The conclusions and perspectives are summarized in Section 6."}, {"title": "2.1. Bus operation control", "content": "To enhance the fidelity of bus propagation simulations, model-based bus holding control strategies incorporate a diverse set of factors, such as bus queueing, overtaking between buses, passenger boarding behaviors, dynamic target headways, real-time bus route state forecasting, multiple bus services, and charging scheduling for electric"}, {"title": "2.2. Reward design of RL", "content": "Designing effective reward signals for RL, particularly in the context of sparse rewards, presents significant challenges that can impede the learning process through reliance on undirected exploration (Eschmann, 2021). Addressing this, the design of appropriate reward signals to expedite learning and ensure stable convergence is a fundamental issue (Mao et al., 2020). Devidze et al. (2021) introduced an optimization framework to craft explicable reward functions, which strategically balances the informativeness and sparseness of rewards within a discrete optimization perspective. Eschmann (2021) highlighted the importance of a feedback loop involving the engineer, where the reward function and the learning algorithm are iteratively refined based on task requirements by expert input. Zhou and Li (2022) developed a probabilistic framework capable of deducing the most suitable programmatic reward function from expert demonstrations, which was experimentally proven to surpass existing baselines in complex settings. Knox et al. (2023) critically assessed the reward functions reported in the literature, applying eight sanity checks to each and uncovering pervasive flaws in RL reward designs for autonomous driving applications. Mao et al. (2020) integrated system-wide global rewards with local rewards for individual agents in a MARL-based packet routing scenario, employing a dynamic weighting factor to adjust the focus between local and global rewards during training, thereby enhancing learning efficiency and policy effectiveness.\nThese methods for reward design generally necessitate explicit definitions of desired behavior or extensive expert demonstrations. However, the advent of LLMs offers novel prospects for more automated reward design. For instance, Kwon et al. (2023) generated reward signals aligned with user-defined objectives by prompting the LLM with descriptions of desired behaviors, testing the efficacy of these rewards in various scenarios such as the Ultimatum Game, matrix games, and negotiation tasks. Xie et al. (2023) utilized LLMs to design dense reward functions for RL-based robotic manipulation and refined these reward functions with human feedback, surpassing the performance of RL agents trained with expert-designed rewards in the majority of tasks and addressing the challenge of language ambiguity. This approach enables more effective training of RL agents, closely aligning with specific human objectives without the necessity for extensive labeled data or manually crafted reward functions. Consequently, this study leverages LLMs to generate reward functions and evaluate the performance of"}, {"title": "2.3. LLM-based applications in transportation", "content": "In the transportation domain, LLMs have been deployed as controllers in various applications, including traffic signal control (Lai et al., 2023; Wang et al., 2024b; Da et al., 2023), urban itinerary planning (Tang et al., 2024), personal mobility generation (Wang et al., 2024a), and urban planning (Zhou et al., 2024). These applications highlight LLMs' adeptness in scenario understanding and natural language processing. Despite their capabilities, LLMs may not be well-perceived for real-time control tasks due to their limited generation speed and the non-interpretability of direct output actions. Specifically, the slow generation speed of LLMs cannot meet the latency requirements of real-time, dense control tasks (Nam et al., 2024). Moreover, the outputs of the LLM-based controller are the control action, instruction, or signal only in most cases, lacking interpretability which is crucial for addressing the black-box problem of AI techniques in practical applications (Lin et al., 2023; Castelvecchi, 2016). These limitations hinder the implementation of LLMs on real-time control tasks.\nCompared to LLM-based controllers, the integration of LLMs with RL can mitigate the impact of slow responses of LLMs through the intermediate interface of offline reward function generation, facilitating the practical application of LLMs in real-time control tasks (Carta et al., 2023). For instance, Han et al. (2024) leveraged LLMs to generate and evolve RL reward functions through iterative feedback for autonomous driving in highway scenarios. They applied a similar design of reward function candidates to Yu et al. (2023d) in each iteration, potentially boosting reward improvement efficiency. Pang et al. (2024) combined LLMs with RL in traffic signal control, where LLMs assess and adjust the RL agent's decisions to ensure their reasonableness. These studies highlight the benefits of integrating LLMs with RL to enhance RL-based control systems through various methods of integration. However, the reliability of the responses from LLMs is not taken into account in these studies. Therefore, this study proposes a rule for the feedback loop within the LLM-enhanced RL paradigm. This rule is designed to filter out inefficient outputs from LLMs, thereby ensuring a reliable and stable method for formulating the bus holding control strategies."}, {"title": "3. RL-based bus holding control for generic scenarios", "content": "In bus holding control, early buses require holding control at control points (e.g., bus stops and the departure area) to homogenize the time headways in the system and reduce the average waiting time for passengers (Xuan et al., 2011). However, unreasonable holding control can lead to excessive delays for onboard passengers, resulting in conflicting objectives of balancing bus headways and minimizing passengers' travel time. In multi-line services, the interaction between different bus lines must also be considered. In RL-based control methods, integrating these diverse objectives requires careful design of the dense rewards at each control step.\nThe control action in bus holding control is taken every time a bus arrives at the control point, i.e., the bus stop in this study, and the travel time of buses between two adjacent stops is inconsistent. Thus, bus holding control is asynchronous control in which RL agents do not act simultaneously, and the default MARL method cannot tackle the asynchronous control when considering the cooperation among agents (Wang and Sun, 2021; Shen et al., 2024). In this study, as we focus on the LLM-enhanced reward design in the RL method, the asynchronous issue in bus holding control is addressed in a simplified way: through control action discretion (to homogenize the duration of decision steps) and knowledge sharing among agents (to stabilize learning in the decentralized control system (Li et al., 2021)).\nIn the RL method of this study, each bus stop is represented by an RL agent. The RL agents are decentralized and observe only partial information around their respective controlled stops from the environment. Thus, the agents within the same bus system are considered homogeneous, allowing them to share experiential knowledge from their memory buffers during training to enhance exploration efficiency (Wang and Sun, 2020). Sharing experiential knowledge also improves cooperation among agents and stabilizes training in the decentralized system (Li et al., 2021). During testing, the learned policy parameters can be shared among agents in the same system due to their homogeneity. Given the strong correlation and consistent performance between bus time headway and space headway (Yu et al., 2023a; Ampountolas and Kring, 2021), space headway is utilized as the direct observation for the RL agents. The long holding duration is discretized into the combination of a series of homogeneous smaller actions (e.g., 5 seconds). Buses traveling between two bus stops do not participate in the control process, thus their states are not considered by agents or recorded in the memory buffer. Real-time space"}, {"title": "3.1. Problem statement", "content": "In bus holding control, early buses require holding control at control points (e.g., bus stops and the departure area) to homogenize the time headways in the system and reduce the average waiting time for passengers (Xuan et al., 2011). However, unreasonable holding control can lead to excessive delays for onboard passengers, resulting in conflicting objectives of balancing bus headways and minimizing passengers' travel time. In multi-line services, the interaction between different bus lines must also be considered. In RL-based control methods, integrating these diverse objectives requires careful design of the dense rewards at each control step.\nThe control action in bus holding control is taken every time a bus arrives at the control point, i.e., the bus stop in this study, and the travel time of buses between two adjacent stops is inconsistent. Thus, bus holding control is asynchronous control in which RL agents do not act simultaneously, and the default MARL method cannot tackle the asynchronous control when considering the cooperation among agents (Wang and Sun, 2021; Shen et al., 2024). In this study, as we focus on the LLM-enhanced reward design in the RL method, the asynchronous issue in bus holding control is addressed in a simplified way: through control action discretion (to homogenize the duration of decision steps) and knowledge sharing among agents (to stabilize learning in the decentralized control system (Li et al., 2021)).\nIn the RL method of this study, each bus stop is represented by an RL agent. The RL agents are decentralized and observe only partial information around their respective controlled stops from the environment. Thus, the agents within the same bus system are considered homogeneous, allowing them to share experiential knowledge from their memory buffers during training to enhance exploration efficiency (Wang and Sun, 2020). Sharing experiential knowledge also improves cooperation among agents and stabilizes training in the decentralized system (Li et al., 2021). During testing, the learned policy parameters can be shared among agents in the same system due to their homogeneity. Given the strong correlation and consistent performance between bus time headway and space headway (Yu et al., 2023a; Ampountolas and Kring, 2021), space headway is utilized as the direct observation for the RL agents. The long holding duration is discretized into the combination of a series of homogeneous smaller actions (e.g., 5 seconds). Buses traveling between two bus stops do not participate in the control process, thus their states are not considered by agents or recorded in the memory buffer. Real-time space"}, {"title": "3.2. Design of the RL agent", "content": "To implement bus holding control with the RL method, the state, action, and reward need to be designed to follow the Markov decision process (Sutton and Barto, 2018). Every time a bus dwells at a stop and completes the passenger boarding and alighting process, the corresponding agent needs to decide whether to hold the bus for the next control step (i.e., 5 seconds in this study). If the action is to hold the bus, the agent will continue making decisions at the end of each holding period until either a no-holding signal is received or the predefined maximum holding time is reached. Thus, the action of the agents is a binary variable representing to hold or not to hold the bus at the next action step (i.e., the next control step). If the agent chooses not to hold the bus, the bus will depart from the stop and proceed to the next stop. During the bus's travel between stops, no action is taken regarding the bus.\nThe state of the agent is an array comprising a set of observations from the environment, including the space headways, the number of onboard passengers, and holding duration information of the current bus dwelling at a stop. We formulate the agents' state in a generic way that is not specific to any particular scenario. Given the set of all bus lines in a bus system as M, the state of the current dwelling bus b at stop i at time step t, \\(S_{i,b,t}\\), is defined as in Eqs. (1) and (2).\n\\(S_{i,b,t} =  \begin{bmatrix} S_{i,b,t}^{headway} \\ P_{i,b,t} \\ t_{i,b,t}^{holding} \\end{bmatrix}\\)\n(1)\n\\(S_{i,b,t}^{headway} =  \begin{bmatrix} h_{i,b,t}^{m+} \\ h_{i,b,t}^{m-} \\end{bmatrix}, \\ \\ \\ \\  \\  \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\   \\forall m \\in M\\)\n(2)\nwhere \\(S_{i,b,t}^{headway}\\) is the set of space headways of the current dwelling bus b at stop i at time step t; \\(h_{i,b,t}^{m+}\\) and \\(h_{i,b,t}^{m-}\\) are the forward and backward space headways between the current dwelling bus b and its nearest forward and backward buses from bus line m at time step t, respectively; \\(n_{i,b,t}^{onboard}\\) is the number of on-board passengers of the current dwelling bus b at stop i at time step t; \\(t_{i,b,t}^{holding}\\) is the holding time the current dwelling bus b has already spent at stop i at time step t. When multiple buses are dwelling at the same stop simultaneously, the state of each bus is recorded separately, and the agent corresponding to this stop is duplicated into several instant versions to make decisions for each bus individually. The exploration and exploitation transitions of these instant agents are all updated to a common memory buffer for knowledge sharing.\nRegarding single bus line systems, \\(S^{headway}\\) only contains one pair of headways between buses within the same line. For multi-line systems, the headways between buses from all different bus lines also need to be included in the state. Thus, there are |M| pairs of headways in \\(S^{headway}\\) for multi-line services, where |M| is the number of bus lines in the entire system. It is worth noting that not all bus stops in a multi-line system serve all bus lines. As depicted in the right part (white-background box) of Fig. 2, bus stops located on branches outside the shared corridor only serve one bus line. In these cases, the forward and backward space headways between the current dwelling bus and buses from different bus lines are set to 0. This setting for headways is described by Eqs. (3) and (4).\n\\(h_{i,b,t}^{m+} =  \begin{cases} x_{i,b,t}^{m+} - x_{i,b,t} ,  \\forall m \\in M_{i} \\ 0 ,  m \\notin M_{i}  \\end{cases}\\)\n(3)\n\\(h_{i,b,t}^{m-} =  \begin{cases} x_{i,b,t} - x_{i,b,t}^{m-} ,  \\forall m \\in M_{i} \\ 0 ,  m \\notin M_{i}  \\end{cases}\\)\n(4)\nwhere \\(x_{i,b,t}\\) is the travel distance of the current dwelling bus of stop i along its bus route; \\(x_{i,b,t}^{m+}\\) and \\(x_{i,b,t}^{m-}\\) are the travel distances of the nearest forward and backward buses of the current bus b from bus line m along the shared bus route with standardized origins; \\(M_{i}\\) is the set of bus lines that bus stop i serves. This formulation allows for a general state representation of heterogeneous bus stops in multi-line systems.\nAs the major contribution of this paper, the reward is not defined manually, instead, it is generated by LLMs. All agents within the same bus system share the same reward function. Although agents controlling bus stops located in the branch and the shared corridor may have different objectives in the multi-line service, we use homogeneous agents for control to evaluate whether LLMs can provide a generic reward function for such heterogeneous scenarios. The potential success in designing a generic reward function may enhance the adaptiveness and transferability of RL-based bus holding control strategies."}, {"title": "3.3. Modeling dynamics of bus propagation", "content": "A dynamic simulation environment of bus systems that allows overtaking is built considering the passenger modeling and bus capacities (Wu et al., 2017). Assuming that passengers' arrivals follow Poisson processes (Berrebi et al., 2018; Wang and Sun, 2020), the passenger p traveling with bus line m and originating from stop i is defined as in Eq. (5).\n\\(P_{i,m,p} = [t_{i,m,p}^{arrive}, [a_{i,m,p}^{alight}, t_{i,m,p}^{board}, t_{i,m,p}^{alight}]\\)\n(5)\nwhere \\(t_{i,m,p}^{arrive}\\) is the arrival time of passenger p at stop i traveling with bus line m; \\([a_{i,m,p}^{alight}\\) is the alighting stop ID of passenger p; \\(t_{i,m,p}^{board}\\) and \\(t_{i,m,p}^{alight}\\) are the boarding and alighting times of passenger p, respectively, which are updated during the simulation. Then, the total waiting time, \\(T^{wait}\\), and total travel time, \\(T^{travel}\\), for all passengers are derived from Eqs. (6) and (7) to evaluate the control strategies.\n\\(T^{wait} = \\sum_{m \\in M} \\sum_{i \\in I_{m}} \\sum_{p \\in P_{m,i}}  (t_{i,m,p}^{board} - t_{i,m,p}^{arrive}),\\)\n(6)\n\\(T^{travel} = \\sum_{m \\in M} \\sum_{i \\in I_{m}} \\sum_{p \\in P_{m,i}}  (t_{i,m,p}^{alight} - t_{i,m,p}^{arrive})\\)\n(7)\nwhere \\(I_{m}\\) is the set of all bus stops within bus line m; \\(P_{m,i}\\) is the set of all passengers traveling with bus line m and originating from stop i; \\(T_{i,m,p}^{wait}\\) and \\(T_{i,m,p}^{travel}\\) are the waiting and travel time of passenger p traveling with bus line m and originating from stop i. In multi-line systems, passengers whose origin and destination stops are both shared among several lines are categorized as shared passengers. Shared passengers can board buses from any line that reaches their destinations. The specific bus line that a shared passenger belongs to is determined based on the bus line they actually board during the simulation.\nThe dwell time for bus b at stop i with arrival time step t, denoted as \\(D_{i,b,t}\\), is the maximum boarding time and alighting time of passengers, computed as in Eq. (8).\n\\(D_{i,b,t} = max[a_{b}B_{i,b,t}, a_{a} A_{i,b,t}],\\)\n(8)\nwhere \\(B_{i,b,t}\\) and \\(A_{i,b,t}\\) are the number of boarding and alighting passengers at stop i for bus b with the bus arrival time step t, respectively; \\(a_{b}\\) and \\(a_{a}\\) are the time taken for each passenger to board and alight from the bus, respectively.\nThe number of boarding passengers, \\(B_{i,b,t}\\), is determined by the bus's capacity limit and the number of waiting passengers at the stop, as given in Eq. (9). For bus b belongs to bus line m, the number of waiting passengers for bus line m at stop i, denoted \\(B_{i,m,t}\\), is derived from the set of passengers whose origins are at stop i and also includes hold-up passengers who could not board the previous bus due to capacity constraints, as shown in Eq. (10). The number of alighting passengers, \\(A_{i,b,t}\\), is obtained from the set of onboard passengers of bus b, as depicted in Eq. (12).\n\\(B_{i,b \\in b_{m},t} = min [B_{i,m,t}, c - n_{b,t}^{onboard}],\\)\n(9)\n\\(B_{i,m,t} = |[P_{i,m.p}| \\forall p if t < t_{i,m,p}^{arrive} < t + n_{i,m,t-1}^{holdup}],\\)\n(10)\n\\(n_{b,t}^{holdup} = max[0, B'_{i,m,t-1} - (c-n_{b,t-1}^{onboard} + A_{i,b,t-1})],\\)\n(11)\n\\(A_{i,b,t} = |[P_{b}^{Ponboard}| \\forall p if t_{i,m,p}^{alight} \\rightarroW i]|\\),\n(12)\nwhere \\(b_{m}\\) is the set of buses operating on bus line m; c is the bus capacity; \\(n_{b,t}^{onboard}\\) is the number of onboard passengers on bus b at time step t, and \\(n_{b,t}^{onboard}\\) refers to the same quantity when bus b dwells at stop i at time t; | | denotes the cardinality of the inside set; t' is the arrival time of the previous bus from line m at the stop i; \\(N_{i,m,t-1}^{holdup}\\) is the number of passengers who could not board the previous bus and are held up at the stop i due to capacity constraints; \\([P_{b}^{Ponboard} = [t_{i,m,p}^{arrive}, t_{i,m,p}^{alight}] - [t_{i,m,p}^{board}, t_{i,m,p}^{alight}]\\) for passenger p represented by P, who have boarded bus b. This formulation of \\([P_{b}^{Ponboard}\\) is intended to simulate the state of passengers onboard the buses. The set of onboard passengers for each bus is updated with the simulation by appending boarding passengers and removing those alighting."}, {"title": "3.4. Action-value function approximation", "content": "The bus propagation for bus b, updated every second, is calculated using Eq. (13).\n\\(x_{b \\in b_{m}", "g_{i,b}": "x_{b,t-1} \\in x_{m} and t > t_{a b,i} + D_{i,b},t + g_{i,b}  \\end{cases}\n(13)\nwhere \\(x_{b,t}\\) is the travel distance of bus b along its route at time step t, with \\(x_{b,t} = x_{i,t}\\) when bus b dwells at stop i; \\(v_{b,t}\\) is the average bus speed from time stept - 1 to t; \\(x_{m}\\) is the set of distances corresponding to all stops on the bus line m; when bus dwells, assuming the current dwelling stop ID of bus b is i, \\(t_{a b,i}\\) is the arrival time of bus b at stop i; \\(g_{i,t}\\) is the holding time of this dwelling bus b at stop i, determined by the RL agents. The average value and standard deviation of time headways within each bus line can be calculated from the comprehensive observation of bus trajectories, which can be used for evaluating the performances of control strategies.\nIn this study, the Deep Q-Network (DQN) is employed to approximate the action-value function for the bus holding control agents (Mnih et al., 2015). The agents aim to interact with the environment by selecting actions that maximize future rewards (Mnih et al., 2013). To differentiate between the time step t (i.e., per second) in the environment simulation and the decision step (i.e., 5 seconds in this study) in the RL method, we denote the current decision step of RL agents as f. The future discounted return, \\(R_{t} = \\sum_{t'=t}^{T}\\gamma^{t'-t}r_{t'}\\), is defined as the discounted sum of future rewards, where T is the maximum number of decision steps within an episode, \\(\\gamma\\) is the discounted factor, and \\(r_{t'}\\) is the reward of the single decision step f'. The optimal action-value function Q*(s, a) is the maximum expected return achieved by selecting an action a with the state s, as described in Eq. (14).\n\\(Q*(s, a) = max_{\\pi} E [R_{t} | s_{t} = s, a_{t} = a, \\pi"}]}