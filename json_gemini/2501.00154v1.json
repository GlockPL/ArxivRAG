{"title": "Probabilistic Explanations for Linear Models", "authors": ["Bernardo Subercaseaux", "Marcelo Arenas", "Kuldeep S. Meel"], "abstract": "Formal XAI is an emerging field that focuses on providing explanations with mathematical guarantees for the decisions made by machine learning models. A significant amount of work in this area is centered on the computation of \u201csufficient reasons\u201d. Given a model M and an input instance x, a sufficient reason for the decision M(x) is a subset S of the features of x such that for any instance z that has the same values as x for every feature in S, it holds that M(x) = M(z). Intuitively, this means that the features in S are sufficient to fully justify the classification of x by M. For sufficient reasons to be useful in practice, they should be as small as possible, and a natural way to reduce the size of sufficient reasons is to consider a probabilistic relaxation; the probability of M(x) = M(z) must be at least some value \u03b4 \u2208 (0, 1], for a random instance z that coincides with x on the features in S. Computing small 8-sufficient reasons (8-SRs) is known to be a theoretically hard problem; even over decision trees traditionally deemed simple and interpretable models strong inapproximability results make the efficient computation of small 8-SRs unlikely. We propose the notion of (d, \u0454)-SR, a simple relaxation of d-SRs, and show that this kind of explanations can be computed efficiently over linear models.", "sections": [{"title": "1 Introduction", "content": "Explaining the decisions of Machine Learning classifiers is a fundamental problem in XAI (Explainable AI), and doing so with formal mathematical guarantees on the quality, size, and semantics of the explanations is in turn the core of Formal XAI (Marques-Silva and Ignatiev, 2022). Within formal XAI, one of the most studied kinds of explanations is that of sufficient reasons (Darwiche and Hirth, 2020), which aim to explain a decision M(x) = 1 by presenting a subset S of the features of the input x that implies M(z) = 1 for any z that agrees with x on S. In the language of theoretical computer science, these correspond to certificates for M(x).\nExample 1. Consider a binary classifier M defined as\nM(x) = (x\u2081 \u2227 x3) \u2227 (x2 \u2228 x1) \u2227 (x4 \u2228 x3),"}, {"title": "2 Probabilistic Sufficient Reasons", "content": "The main idea of probabilistic sufficient reasons is to relax the condition \u201call completions of the explanation y have the same class as x\u201d to \u201ca random completion of y has the same class as x with high probability\u201d."}, {"title": "2.1 The size of 8-SRS", "content": "Interestingly, even a 0.999999-SR can be arbitrarily smaller, in terms of defined features, than the smallest sufficient reason (i.e., 1-SR) for a pair (M, x), even when M is a linear model, as we will illustrate in Example 2. Before providing the example, let us define linear models.\nDefinition 3. A (binary) linear model L of dimension d is a pair (w, t), where w \u2208 Qd and t \u2208 Q. Its classification over an instance x is defined simply as\nL(x) = { 1 if x\u22c5w\u2265t 0 otherwise.\nExample 2. Consider a linear model L of dimension d = 1000 with parameters t = 1250 and\nw = (1000, 1, 1, 1, 1, ..., 1).\nLet the instance x be (1, 1, 1, 1, 1, . . . , 1), so that clearly L(x) = 1. One can easily see that any 1-SR for x under L has size 251, as it must include the first feature and any 250 other features. However, if we consider y = (1, 1, 1, 1, ..., 1), then a simple application of the Chernoff-Hoeffding concentration bound (in the appendix for the completeness) gives that\nPr[L(z)=1] \u2265 0.999999.\nz\u223cU(y)\nThis suggests that we might say L(x) = 1 \u201cbecause\" x\u2081 = 1; formally, y is a 0.999999-SR, and 251 times smaller than any 1-SR for L(x)."}, {"title": "3 Approximating 8-Sufficient Reasons", "content": "Unfortunately, computing small 8-SRs is computationally challenging, even when attempting to find approximate solutions. Let us contextualize our main result by summarizing first what is known about the complexity of computing 8-SRs and their deterministic predecessors, 1-SRs."}, {"title": "3.1 A Simple Relaxation: (\u03b4, \u03b5)-min-SR", "content": "In light of the hardness results for 8-SRs, it is natural to consider a further relaxation that would allow for tractability. Consider for instance a customer of a bank who wants a 0.95-SR for why their application for a loan was rejected. Such an explanation would consist of a small number of features of their application profile that are relevant to the decision since 95% of applicants with such a profile would also get rejected. We expect that, in such a scenario, the user would not particularly care if the explanation she obtains holds for 95% of potential applicants or for 94.9997% of them. In other words, the value of 8 is chosen in a trade-off between the size of the explanation and the desired level of confidence or \u201cexplanation power\u201d. We posit that in such a trade-off, the user is more sensitive to increases in the explanation size than they are to a minor perturbation in 8, the probability guarantee. This motivates the following definition:\nDefinition 4 ((\u03b4, \u03b5)-min-SR). Given a model M, an instance x, and values \u03b4, \u03b5 \u2208 (0, 1), we say a partial instance y of size is a (\u03b4, \u03b5)-min-SR if there exists a value \u03b4* \u2208 [\u03b4 \u2013 \u03b5, \u03b4 + \u025b] such that y is a minimum d*-SR for x under M.\nNote that, even though the guarantee of a (\u03b4, \u03b5)-min-SR is symmetric around 8, our definition is such that the ability of efficiently computing (\u03b4, \u03b5)-min-SRs is enough for the following two tasks:"}, {"title": "3.2 Estimating the Probability of Acceptance", "content": "The hardness of computing Prz~U(y)[M(z) = 1] is about computing it to arbitrarily high precision, i.e., with an additive error within O(2-", "0,1}": "Then\n\u03bc(M) := 1M \u03a3Mi=11[f(xi) = 1]\nM\nis an unbiased estimator for\n\u03bc := Prx\u2208{0,1}n [f(x) = 1],\nand\nPr[|(M) \u2013 \u03bc| \u2264 t] \u2265 1 \u2013 2 exp(-2t\u00b2M),\nwhich is at least 1 \u2013 \u03b3 for M = log(2/\u03b3).\nAs a consequence of the previous idea, although a minimum 6-SR might be hard to compute, this crucially depends on the value of d. In order to deal with this, our algorithm will sample a value 8* uniformly at random from [\u03b4 \u2013 \u03b5, \u03b4 + \u03b5], and then compute a minimum 8*-SR. Intuitively, the idea is that as d* is chosen at random, it will be unlikely that a value that makes the computation hard is chosen.\nBefore proving Theorem 2, we need to prove a lemma concerning the easiness of selecting the features of the desired explanation."}, {"title": "3.3 Feature Selection", "content": "Even if we were granted an oracle computing the probabilities Prz\u2208D(y) [M(z) = 1], that would not be necessarily enough to efficiently compute a minimum 8-SR. Indeed, for decision trees, the counting problem can be easily solved in polynomial time (Barcel\u00f3 et al., 2020), and yet the computation of 8-SRs of minimum size is hard, even to approximate (Arenas et al., 2022; Kozachinskiy, 2023). Intuitively, the problem for decision trees is that, even if we were told that the minimum 8-SR has exactly k features, it is not obvious how to search for it better than enumerating all () subsets. The case of linear models, however, is different, at least under the uniform distribution. In this case, every feature i that is not part of the explanation will take value 0 or 1 independently with probability 1/2, and contribute to the classification according to its weight wi. In other words, we can sort the features according to their weights (with some care about signs), and select them greedily to build a small 8-SR. A proof for the deterministic case (d = 1) was already given in Barcel\u00f3 et al. (2020) and sketched earlier on by Marques-Silva et al. (2020a).\nDefinition 5. Given a linear model L = (w, t), and an instance x, both having dimension d, we define the score of feature i \u2208 [d] as\nsi := wi \u22c5 (2xi \u2013 1) \u22c5 (2L(x) \u2013 1).\nIn other words, the sign of si is +1 if the feature is \u201chelping", "hurting": "t. The magnitude of si is proportional to the weight of the feature i. Changing the value of feature i in an instance x would decrease w\u22c5 x by si if L(x) = 1, and increase it by si if L(x) = 0. For the uniform distribution (or more generally, any distribution in which all features are Bernoulli variables with the same parameter), we can prove the following lemma that basically states that, for linear models it is good to choose features greedily according to their score.\nLemma 1. Given a linear model L, and an instance x, if y(0), ..., y(d) are the partial instances of x such that y(k) x is defined only in the top k features of maximum score, then\nPr[L(z)=L(x)] \u2265 Pr[L(z)=L(x)]\nz\u223cU(y(k+1))\nz\u223cU(y(k))"}, {"title": "4 Locally Minimal Probabilistic Explanations", "content": "Due to the complexity of finding even subset-minimal 8-SR, Izza et al. (2024) have proposed to study \"locally minimal\" d-SR, which are 6-SRs such that the removal of any feature from the explanation would decrease its probabilistic guarantee below d. Interestingly, we can generalize a proof from Arenas et al. (2022) to show that, over lineal models even in the more general case of product distributions (distributions over {0,1}d that are products of independent Bernoulli variables of potentially different parameters), every locally minimal d-SR is a subset-minimal 8-SR. This allows leveraging the previous results of Izza et al. (2024) to subset-minimal 8-SRs in the case of linear models.\nTheorem 3. For linear models, under any product distribution, every locally minimal d-SR is a subset-minimal 8-SR.\nProof sketch. Define the \u201clocality\" gap LGAP(y) of a locally minimal d-SR y as the smallest value g such that y*| - |y|1 = g for some y* \u2286 y that is a d-SR. If g = 0, then y is globally minimal, and we are done. If g were to be 1, then y would not be locally minimal, a contradiction. Therefore, we can safely assume g \u2265 2 from now on. Let L, y be such that y is locally minimal 8-SR and LGAP(y) \u2265 2. We will find a contradiction by the following method:\n\u2022 Let y* be the d-SR such that |y\\y*| = LGAP(y).\n\u2022 Every feature in y\\y* is either \u201cgood\u201d, if its score is positive, or \u201cbad\u201d if its score is negative.\n\u2022 Fix any feature i in y\\y*. If i is good, then y* \u2295 i, meaning the partial instance obtained by taking y and setting its i-th feature to xi, has a probability guarantee greater or equal than that of y* (the proof of this fact is very similar to the proof of Lemma 1), and the gap has reduced. On the other hand, if i is bad, then y \u2295 i, meaning the partial instance obtained from y by setting yi = 1, has greater-equal probability than y, contradicting the fact that y is locally minimal."}, {"title": "5 Approximations on the Size of Explanations", "content": "A natural question at this point is whether the size of a (d, \u025b)-min-SR is necessarily similar to the size of a (6, 0)-min-SR (i.e., a smallest 8-SR). It turns out that this is not the case, and it can happen that in order to get a slightly better probabilistic guarantee (i.e., \u03b4 + \u025b instead of 6), the number of features needed under any explanation significantly increase. In general, if we let MIN(M, x, d) denote the size of the smallest 8-SR for (M, x), we can prove the following generalization of Example 2.\nProposition 2. For any d \u2208 (0,1), y > 0, and any \u025b > 0 such that d + \u025b \u2264 1, there are pairs (L, x) where L is a linear model of dimension d, and x an instance of dimension d, such that\nMIN(L, x, \u03b4 + \u03b5) / MIN(L, x, \u03b4) = \u03a9 (1/\u03b3).\nAs a consequence, we may say informally that approximations on d do not neccesarily lead to approximations on the explanation size."}, {"title": "6 Conclusions and Future Work", "content": "We have proved a positive result for the case of linear models, showing that a (\u03b4, \u03b5)-min-SRs can be computed efficiently, and also a more abstract reason suggesting that linear models might be easier to explain than, e.g., decision trees. However, a variety of natural questions and directions of research remain open. First, in practical terms, even though the runtime of Theorem 2 is polynomial and only has a quasi-linear dependency on d, our future work includes lowering the dependency in 1/\u025b and 1/y; on a dataset with d = 500, setting \u025b = 0.1 and y = 0.01 is already computationally expensive. We acknowledge, in terms of practical implementations, the work of Bounia and Koriche (2023); Izza et al. (2024) that allows for computing small probabilistic explanations over decision trees significantly faster than the exact SAT approach of Arenas et al. (2022). Similarly, Izza et al. (2023b) showed solid practical results with different kinds of classifiers, including linear models (i.e., Naive Bayes). Despite our results having better theoretical guarantees over linear models, a natural direction of future work is to improve the practical efficiency of our algorithm for high-dimensional models.\nSecond, our theoretical result has some natural directions for generalization. We considered only binary features, whereas in order to offer a practically useful tool to the community, we will need to understand how to compute (approximate) probabilistic explanations for mixtures real-valued features and categorical features, for example under the \u201cextended linear classifier\" definition of Marques-Silva et al. (2020b). Another fascinating theoretical question is handling the generalization of our setting to that of product distributions (i.e., feature i takes value 1 with probability pi and 0 otherwise) can also be solved efficiently. A straightforward extension of our techniques does not seem to work on such a generalized setting, since the feature selection argument of ?? no longer holds. Therefore, we believe that new techniques will be needed.\nThird, it would be interesting to allow for a more declarative way of specifying the probabilistic guarantees or constraints on the explanations. While a recent line of research has studied the design of languages for defining explainability queries with a uniform algorithmic treatment (Arenas et al.,"}]}