{"title": "Relational Conformal Prediction for Correlated Time Series", "authors": ["Andrea Cini", "Alexander Jenkins", "Danilo Mandic", "Cesare Alippi", "Filippo Maria Bianchi"], "abstract": "We address the problem of uncertainty quantification in time series forecasting by exploiting observations at correlated se-quences. Relational deep learning methods leveraging graph representations are among the most effective tools for obtain-ing point estimates from spatiotemporal data and correlated time series. However, the problem of exploiting relational structures to estimate the uncertainty of such predictions has been largely overlooked in the same context. To this end, we propose a novel distribution-free approach based on the con-formal prediction framework and quantile regression. Despite the recent applications of conformal prediction to sequential data, existing methods operate independently on each target time series and do not account for relationships among them when constructing the prediction interval. We fill this void by introducing a novel conformal prediction method based on graph deep learning operators. Our method, named Conformal Relational Prediction (COREL), does not require the rela-tional structure (graph) to be known as a prior and can be applied on top of any pre-trained time series predictor. Additionally, COREL includes an adaptive component to handle non-exchangeable data and changes in the input time series. Our approach provides accurate coverage and archives state-of-the-art uncertainty quantification in relevant benchmarks.", "sections": [{"title": "1 Introduction", "content": "Many recent advancements in deep learning methods for time series forecasting rely on learning from large collections of (related) time series [1, 2]. In many application domains, such time series are characterized by a rich spatiotemporal dependency structure that can be exploited by introducing inductive biases in the forecasting architecture [3], to steer the learning procedure toward the most plausible models. Accounting for the existing dependencies, usually represented as a graph, allows the resulting models to obtain accurate pre-dictions with a reduced sample complexity [4, 3]. Besides the accuracy of the point estimates, the reliability of the forecasts is a critical aspect of the problem and a key element to enable effective decision-making in many applications [5, 6]. Uncertainty quantification methods [7, 8] can improve reliability by providing confidence intervals on the forecasting error magnitude, allowing for making more informed decisions [9]. This is particularly true for risk-sensitive applications such as healthcare [10] and load forecasting [11]. In this context, inter-series (spatiotemporal) dynamics offer both a challenge and an opportunity. Indeed, while these dependencies can lead to wide prediction intervals (PIs) if overlooked, they may also provide additional knowledge to reduce uncertainty [12].\nExisting probabilistic forecasting frameworks often rely on strong distributional assumptions and major modifications of the base point predictor [1, 13]. As such, they cannot be used to quantify uncertainty given a pre-trained forecasting model. In such a setting, conformal prediction (CP) [8, 14] methods are particularly appealing. CP is an uncertainty quantification framework that estimates confidence intervals with marginal coverage guarantees from observed predic-tion residuals. One of the main assumptions of standard CP methods is that of exchangeability between the data used to estimate the confidence intervals and the test data points, i.e., the assumption that their joint probability distribution is invariant to the ordering of the associated sequence of random variables [15]. Although this assumption does not usually hold when operating on time series [16], several methods have successfully adapted CP to estimate forecast uncertainty [17, 18, 19, 20, 21]. Nevertheless, existing CP ap-proaches operate on each (possibly multivariate) time series independently and cannot account for dependencies among correlated time series.\nIn this paper, we propose Conformal Relational Prediction (COREL), a novel CP approach leveraging graph representa-tions and graph deep learning (GDL) for quantifying uncer-tainty in correlated time series forecasting. In our framework, a spatiotemporal graph neural network (STGNN) [4, 3] is trained on a calibration set to approximate the quantile func-tion of the distribution of prediction residuals. Relationships among time series, assumed to be sparse, are learned end-to-end from the observed residuals owing to a graph structure learning module integrated into the processing. Our approach estimates the error quantile function for each time series at each time step, by conditioning the shared uncertainty quan-tification model on past observations at neighboring nodes (as defined by the learned graph structure). Finally, an adaptive component is added to handle potential non-stationarities by"}, {"title": "2 Preliminaries", "content": "relying on a small set of parameters specific to each time se-ries. Our approach can be applied to the residuals generated by any point forecasting model, even those that completely disregard potential relationships among the input time series. Our main novel contributions can be summarized as fol-lows.\n\u2022 The first application of GDL to CP for time series;\n\u2022 A novel, sound, and effective CP method able to quan-tify uncertainty from observations across a collection of correlated time series;\n\u2022 A family of graph-based architectures to estimate uncer-tainty that share most of the learnable parameters among the processed time series, while including node-level pa-rameters that dynamically adapt to changes in each target sequence.\nEmpirical results show that COREL achieves state-of-the-art performance compared to existing CP approaches for time series in several datasets and under different scenarios."}, {"title": "2.1 PROBLEM FORMULATION", "content": "Consider a collection of N sparsely correlated time series. Denote by $x_i^t \\in \\mathbb{R}$ the scalar target variable associated with the i-th time series at time step t; $X_t \\in \\mathbb{R}^{N\\times 1}$ indicates the N stacked target variables w.r.t. the entire time series collection. $X_{t:t+T}$ indicates the sequence within time interval [t, t + T); conversely, with the shorthand $X_{<t}$ refers to observations up to time step t (excluded). Time series are as-sumed to be homogenous, i.e., all the variables (observables) describe the same physical quantity (e.g., temperature or en-ergy consumption). Analogously, $U_t \\in \\mathbb{R}^{N\\times d_U}$ indicates the $d_U$-dimensional exogenous covariates associated with each time series. We assume that the i-th time series is generated by a stochastic time-invariant process such as\n$x_i^t \\sim p\\left(x_i^t | X_{<t}, U_{<t}\\right).$ (1)\nLet us hypothesize the existence of a sparse predictive causal-ity \u00e0 la Granger [22], i.e., we assume that the values of a single time series are related to the values of a (small) sub-set of other time series in the collection. The extension of the framework to collections of multivariate time series is straightforward, but we focus on the univariate case to sim-plify the presentations. The problem of dealing with non-stationary processes will be discussed in Sec. 3.4.\nForecasting We are interested in a model that makes point forecasts by predicting the unknown H-steps-ahead (H > 0) observation $X_{t+H}$ given a window W \u2265 1 of past ob-servations $X_{t-W:t}$ and the associated exogenous variables $U_{t-W:t}$ as\n$X_{t+H} = F_\\Theta\\left(X_{t-W:t}, U_{t-W:t}\\right).$ (2)\n$F_\\Theta$ denotes a generic parametric model family, i.e., a simple recurrent neural network (RNN) for univariate time series. Given a trained model, our objective is to build a confidence interval around predictions $X_{t+H}$. Note that the following easily extends to multi-step predictions $X_{t:t+H}$, but we fo-cus on forecasting the single time step H to simplify the presentation and discussion."}, {"title": "Uncertainty quantification", "content": "Our objective is to estimate PIs, $C_\\alpha^t\\left(X_{t+H}\\right)$, such that\n$P\\left(x_i^t \\in C_\\alpha^t\\left(X_{t+H}\\right)\\right) \\geq 1 - \\alpha,$ (3)\nwhere \u03b1 is the desired confidence level. If the interval satisfies Eq. 3, we say that the PI achieves marginal coverage 1 \u2013 \u03b1. Similarly, we say that the PI provides conditional coverage if\n$P\\left(x_i^t \\in C_\\alpha^t\\left(X_{t+H}\\right) | X_{<t}, U_{<t}\\right) \\geq 1 - \\alpha.$ (4)\nConditional coverage provides stronger guarantees and it is often harder to achieve [15]. In the following, we will omit the dependence of the interval on the forecasts and simply write $C_\\alpha^t$. Among uncertainty quantification methods, we are interested in post-hoc approaches that can build confi-dence intervals for any given pre-trained point predictor $F_\\Theta$ without requiring any modification of the base forecasting architecture."}, {"title": "2.2 CONFORMAL PREDICTION", "content": "As anticipated in Sec. 1, standard CP methods [8, 14] are a class of distribution-free uncertainty quantification tech-niques that build PIs from empirical quantiles of conformal scores. In the forecasting setting, we consider as conformal scores the prediction residuals,\n$r_i^t = x_i^t - \\hat{x}_i^t,$ (5)\nand use $R_t$ to denote residuals w.r.t. the entire time series collection. Under appropriate assumptions, CP methods can build valid and informative PIs [14, 16]. Split conformal prediction (SCP) [8] is arguably the most common approach and exploits scores computed on a calibration set that is disjoint from the training data (i.e., a post-hoc approach).\nAs mentioned, most standard CP methods rely on the assumption that calibration and test data are exchangeable which allows the procedure to treat them symmetrically and obtain valid (marginal) coverage guarantees [15]. Since this assumption does not hold when dealing with time series data, there have been several recent results extending the CP frame-work beyond exchangeability [23, 17, 24, 18]. In particular, Barber et al. [16] showed that approximate coverage can be achieved by reweighting the residuals to account for the lack of exchangeability between calibration and test set. Auer et al. [21] learn such a reweighting scheme through an attention-based architecture. Differently, Xu and Xie [19] introduce SCPI, a method based on fitting a quantile random forest [25] on the most recent prediction residuals at each time step. Similar to SCPI, our approach relies on quantile regression to build PIs but differently from existing methods, it exploits observations in arbitrary sets of time series by relying on GDL operators."}, {"title": "2.3 QUANTILE REGRESSION", "content": "Quantile regression [26] is an established statistical frame-work that consists of learning a model of the quantile func-tion (the inverse c.d.f.) of a target distribution from obser-vations. In particular, given $y \\sim p(y|x)$ and observations $(x_1, y_1), ..., (x_N, y_N)$, a standard approach to estimate the"}, {"title": "2.4 GRAPH DEEP LEARNING FOR TIME SERIES FORECASTING", "content": "\u03b1-quantile is to train a model by minimizing the so-called pinball loss\n$\\ell_\\alpha\\left(\\hat{q}^\\alpha(x), y\\right) =\\begin{cases}(1 - \\alpha)(\\hat{q}^\\alpha(x) - y), & \\hat{q}^\\alpha(x) \\geq y\\\\\\alpha(y - \\hat{q}^\\alpha(x)), & \\hat{q}^\\alpha(x) < y\\end{cases}$ (6)\nwhere $\\hat{q}^\\alpha(x)$ is the estimate of the \u03b1-quantile w.r.t. x.\nQuantile networks Quantile regression has been incorpo-rated in several probabilistic forecasting architectures [1]. The simplest approach consists of using a multi-output net-work to predict a set of quantiles of interest and interpolate among them to approximate the entire quantile function [27]. More complex approaches rely on, e.g., splines [28]. Con-versely, implicit quantile networks (IQNs) [29, 30, 31] ap-proximate the quantile function by being trained to minimize the loss in Eq. 6 given the quantile level \u03b1 as input and sampling a random \u03b1 for each sample in a mini-batch.\nGraph neural networks (GNNs) [32, 33] process graph-structured data by incorporating the graph topology as an in-ductive bias, e.g., by relying on message-passing layers [34]. STGNNs [4, 3] leverage message-passing layers within se-quence modeling architectures to process spatiotemporal data and collections of time series where dependencies are rep-resented as a (possibly dynamic) graph. We consider as ref-erence architectures time-then-space (TTS) models [35, 3] where each time series in the collection is processed indepen-dently from the others by a temporal encoder whose output is then fed into a stack of GNN layers. In particular, we adopt the following template architecture:\n$h_i^{l,0} = \\text{SEQENC}\\left(x_{i,t-W:t}, u_{i,t-W:t}\\right),$ (7)\n$H^{l+1} = \\text{GNN}_l\\left(H^l, A\\right),, l = 0, ..., L-1$ (8)\n$\\hat{y}_t = \\text{READOUT} \\left(h_i^{L}\\right),$ (9)\nwhere $A \\in \\mathbb{R}^{N\\times N}$ is the graph adjacency matrix and $\\hat{y}_t$ a generic node-level prediction associated with the problem at hand. SEQENC( \u00b7 ) and GNNl(\u00b7) denote, respectively, any sequence modeling architecture, e.g., an RNN, and any GNN layer, e.g., based on message-passing. Representations can then be mapped into predictions $Y_t$ by using any readout block, e.g., a multilayer perceptron (MLP). STGNNs have been used as forecasting architecture ($Y_t = X_{t+H}$) with great success. In the following, we will exploit this framework as a backbone for estimating the residual quantile distribution. We refer to Sec. 4 and Jin et al. [4] for more discussion on the application of STGNNs in the context of time series analysis."}, {"title": "3 Conformal relational prediction", "content": "Our objective is to build PIs by exploiting relational depen-dencies across the residuals of the target time series. We model the dependencies as edges of a graph and learn them under the assumption that the relational structure is sparse, which reduces the computational costs and act as an induc-tive bias on the structure learning architecture. By relying on such representation, we can leverage GDL methods for time series to process the data. In particular, we train a STGNN on the residuals of the calibration set to predict the quantiles of the error distribution. Conditioning the prediction on the recent history of related time series allows for taking the de-pendency structure of the data into account when estimating uncertainty: a key aspect in applying conformal prediction to non-exchangeable data [16]. Compared to existing meth-ods [19, 21] that only capture temporal dependencies, our approach allows for modeling spatiotemporal dependencies among different time series. Sec. 3.1 presents the details of the proposed conformal inference procedure by assuming that the relational structure at each time step is defined by an adjacency matrix $A \\in \\mathbb{R}^{N\\times N}$ (Sec. 3.1). We then show how to learn the graph structure directly from data and make the model adaptive in Sec. 3.2 and Sec. 3.4, respectively. Fi-nally, we discuss the theoretical properties of the approach in Sec. 3.3. Fig. 1 shows an overview of the architecture."}, {"title": "3.1 RELATIONAL QUANTILE PREDICTOR", "content": "Consider a standard SCP setup, where the training data are split into training and calibration sets. For the moment, we disregard possible nonstationarities in the data considering the problem setup introduced in Sec. 2.1 and encode spatial dependencies in the adjacency matrix $A \\in \\mathbb{R}^{N\\times N}$. While the training set is used to fit the point predictor $F_\\Theta$, we use the prediction residuals in the calibration set (Real) to learn the quantile function of the error distribution at each time step.\nWe implement the quantile regressor as a hybrid global-local STGNN, which mixes global (shared) parameters with local, target-specific components [36]. Sharing most learn-able parameters across all time series reduces sample com-plexity, while local parameters allow for processing each series differently. Specifically, we keep all processing blocks shared and associate a learnable node embedding $v_i \\in \\mathbb{R}^{d_V}$ with each time series [37]. More specifically, our model is a quantile network (see Sec. 2.3) composed of the following processing layers:\n$h_i^{1,0} = \\text{ENC} \\left(r_{i,t-1}, v_i\\right),$ (10)\n$Z_t = \\text{STGNN} \\left(H^1_t, A\\right),$ (11)\n$\\hat{Q}_t^{\\alpha,i} = \\text{QDEC} \\left(\\alpha, z_i^t, v_i\\right),$ (12)\nwhere $r_{i,t-1}$ are prediction residuals (Eq. 5) and $\\hat{q}_t$ is the predicted \u03b1-quantile at time step t + H for the i-th time series. ENC(\u00b7) denotes any encoding layer,e.g., a linear transformation or an MLP. For the STGNN block, several designs are possible (e.g., see Jin et al. [4]); the one we follow is the template in Sec. 2.4. QDEC(\u00b7) is a readout mapping the representations at each node to the prediction of the quantile of specified level \u03b1. We refer to the family of quantile networks defined in Eq. 10\u201312 as relational quantile predictors (RelQPs) and use the notation\n$\\hat{Q}_t^{\\alpha} = Q_\\psi \\left(\\alpha, V; R_{t-W:t}, A\\right),$ (13)\nwhere $Q_\\psi$ indicates the shared (global) part of the network and $\\hat{Q}_t^{\\alpha} \\in \\mathbb{R}^N$ denotes the predicted \u03b1-quantiles at time step"}, {"title": "3.2 LEARNING THE RELATIONAL STRUCTURE", "content": "t w.r.t. the full time series collection. Note that the frame-work can easily accommodate further inputs at the encoding block (e.g., we can condition the regression on $X_{<t}$ and $U_{<t}$). The model is trained by, at each time step in the cali-bration set, the pinball loss (Eq. 6) w.r.t. the full-time series collection. Through the message-passing layers, the residuals of each time series contribute to estimating the quantiles of the error distribution at neighboring nodes. In practice, we restrict the input of the regressor to the most recent observa-tions rather than considering the full sequence (the window length here can also be different from the one used by the point predictor).\nBuilding the confidence intervals Given the trained quan-tile network $Q_\\psi$, we build the PIs for each target (test) time step as\n$C_\\alpha^t = \\left[X_{t+H} + \\hat{Q}_t^{\\alpha/2}, X_{t+H} + \\hat{Q}_t^{1-\\alpha/2}\\right]$ (14)\nor\n$\\beta = \\text{arg} \\min_\\beta \\left| \\hat{Q}_t^{\\alpha/2 + \\beta} - \\hat{Q}_t^{-\\alpha/2 + \\beta} \\right|$ (15)\n$C_\\alpha^t = \\left[X_{t+H} + \\hat{Q}_t^{\\alpha/2 + \\beta}, X_{t+H} + \\hat{Q}_t^{-\\alpha/2 + \\beta}\\right],$ (16)\nwhere $\\hat{C}_\\alpha$ indicates the estimated PI. While both Eq. 14 and Eq. 16 correspond to the same confidence level, the PI in Eq. 16 can be narrower, at the expense of the additional com-putation needed to obtain \u03b2 [18]. In practice, one can choose between the two approaches given computational constraints and the difference in performance observed on a validation set. Note that $\\hat{C}_\\alpha^t$ can provide only approximate coverage, as it is subject to approximation errors of the true quantile func-tion. Sec. 3.3 will discuss this aspect in detail. A potential drawback of COREL is that residuals cannot be assumed ex-changeable in most practical scenarios. The error distribution can be non-stationary, making it difficult to obtain any cov-erage guarantee. To mitigate the problem, Sec. 3.4 discusses an efficient and scalable approach to make the framework adaptive by updating local components of the architecture over time. Finally, it is worth noting that the use of relational components in COREL relies on the actual presence of the associated dependencies in the data. In practical applications, the presence of spatiotemporal correlations in the residu-als can be verified through ad-hoc statistical tests [12, 38], whose outcome can support the adoption of COREL.\nAssuming the dependency structure across time series to be unknown, we integrate a graph learning module into the architecture to derive the operational graph topology directly from the residuals. To do so, we adopt a probabilistic structure learning framework [39, 40, 41]. In particular, we associate each edge with a score $p_{ij}$ and learn a distribution over K-NN graphs parametrized by the matrix $\\Phi \\in \\mathbb{R}^{N\\times N}$ [40, 42]. Notably, we consider graphs obtained by sampling, for each i-th node, K elements without replacement from the categorical distribution\n$\\Phi = \\mathbb{E}_\\xi(R_{<t}, V, ...)$ (17)\n$M_i = \\text{Categorical}\\left(\\frac{\\exp{\\xi_{ik}}}{\\sum_{k=1}^N \\exp{\\xi_{ij}}}\\right)_{k\\in \\{1,...,N\\}}$ (18)\nwhere $\\mathbb{E}_\\xi(\\cdot)$ is a generic trainable encoder with parameters $\\xi$. In practice, sampling can be done efficiently by exploiting the GumbelTopK trick [43] and scores $\\Phi$ can be parametrized directly as $\\Phi = \\xi$.\nEnd-to-end learning To propagate gradients through the sampling, we rely on the continuous relations introduced by Xie and Ermon [44] paired with a straight-through gradient estimator [45] to obtain discrete samples. Optionally, we sparsify the gradients by backpropagating only through a random subset of the zero entries of A (more details are provided in App. D)."}, {"title": "3.3 THEORETICAL ANALYSIS AND FURTHER DISCUSSION", "content": "We start the discussion by providing an intuitive bound on the approximate coverage provided by COREL.\nProposition 3.1. Let $P_{t+H}^\\mathcal{D}(X_{t+H}) = P_{t+H}(X_{t+H} | X_{<t}, U_{<t})$ and $P_{\\psi}(X_{t+H}) = p_{\\psi}(X_{t+H} | X_{<t}, U_{<t})$ be the true conditional data-generating distribution at the test point t + H and the probability distribution associated with the learned quantile function Q\u03c8, respectively. Then\n$P_{t+H}(X_{t+H} \\in C_\\alpha^t(X_{t+H})) \\geq 1 - \\alpha - TV\\left(P_\\psi, P_{t+H}^\\mathcal{D}\\right)$"}, {"title": "3.4 ADAPTATIVITY", "content": "where $TV(\\cdot)$ denotes the total variation function.\nThe proof relies on the properties of the total variation of probability measures and can be found in App. A. Here, differently from the problem settings introduced in Sec. 2.1, we do not assume the process to be time-invariant. Prop. 3.1 links the conditional coverage gap to the approximation error in estimating the quantile function of the residuals. By mak-ing assumptions on the expressivity of the quantile regressor in Eq. 13 and on the stationarity process (e.g., by assuming a strongly mixing process), we can expect the total variation between the learned and true distribution to shrink asymptot-ically as the size of the calibration set increases. Moreover, in this case, monitoring the coverage gap on a validation set offers an estimate of the actual miscoverage on test data. Con-versely, if we expect the process to be non-stationary, Q\u03c8 has to be updated over time to keep the coverage gap contained. Within this context, the next section discusses a simple and sample-efficient approach to make COREL adaptive. Finally, the bound provided in Prop. 3.1 shares similarities with the one in [16], which bounds the miscoverage gap for CP from weighted empirical quantiles. Prop. 3.1 can be seen as an analogous result that holds when estimates obtained from empirical quantiles are replaced with a parametric function approximator.\nThe RelQP model introduced in Sec. 3.1 can yield arbitrarily large coverage gaps in the presence of distribution shifts from the calibration set, where the model is trained. Adopting a re-training approach such as in Xu and Xie [19] would be impractical due to the higher sample complexity entailed by the deep learning approach that we adopt. Therefore, to miti-gate this issue while keeping the computational complexity under control, we update only the local components of the model over time, i.e., the learnable node embeddings V [37]. This allows for keeping most of the parameters of the model fixed and fine-tuning only a small number of weights for each node. Empirically, we show that this procedure can effectively improve the quality of the uncertainty estimates."}, {"title": "4 Related Work", "content": "The problem of quantifying forecast uncertainty is central in fundamental and applied research in time series forecast-ing [9, 6]. Among deep learning approaches [1], many gen-erative architectures have been proposed as means to obtain probabilistic forecasts [13, 46, 47, 48]. Most related to our approach are those methods that exploit quantile regression to produce probabilistic forecasts [27, 28, 49, 31]. Similarly to COREL, these quantile regression techniques do not usu-ally require strong assumptions on the underlying data dis-tribution. Regarding probabilistic graph-based forecasting architecture, the existing literature is limited [4, 3]. Wu et al. [50] investigate the combination of STGNNs with standard uncertainty quantification techniques for deep learning. Pal et al. [51] use an STGNN to implement a state-space model and quantify uncertainty within a Bayesian framework. Wen et al. [52] propose a probability predictor based on com-bining STGNNs with a diffusion model [53]. Zambon et al. [54] introduce a framework for designing probabilistic graph state-space models that can process collections of time se-ries. However, all these methods cannot operate on top of an existing pre-trained model and require training an ad-hoc forecasting model. Conversely, COREL is trained, within a CP framework, on predicting the quantiles of the error dis-tribution of an existing model, rather than on forecasting the target variable.\nConformal prediction Related work on CP for time series has been already discussed in Sec. 2.2 and Sec. 3. Related to our method, Mao et al. [55] propose a CP approach for (static) spatially correlated data. Jiang et al. [56] propose to quan-tify the uncertainty in predicting power outages by fitting a quantile random forest [25] on time series from neighbor-ing geographical units. COREL can be framed among the CP methods that learn a model of conformal scores distribu-tion [19, 57]. Differently from existing methods that operate on each time series separately, the estimates are conditioned on errors at both the target time series as well as at neigh-boring nodes. To the best of our knowledge, no previous CP method has been designed to specifically operate on col-lections of correlated time series and exploit graph deep learning operators. CP methods for multivariate time series do exist [58, 59], but operate on a single multidimensional time series. Moreover, although global-local models are pop-ular among forecasting architectures [36, 1], COREL is the first CP architecture of this kind. Finally, CP methods have also been applied to static graphs and used to quantify the uncertainty of GNNs, both in inductive [60] and transduc-tive settings [61]. More recently, Davis et al. [62] proposed a CP method for node classification with GNNs in dynamic networks. These methods often assume node/edge exchange-ability [60, 61] or are limited to node classification tasks [63]."}, {"title": "5 Experiments", "content": "We validate COREL across three experimental settings. In the first one (Sec. 5.1), we compare it against state-of-the-art CP methods operating on the residuals produced by different forecasting models. Then, we analyze COREL in a controlled environment (synthetic dataset). Finally, we assess the effec-tiveness of the procedure described in Sec. 3.4 in adaptively improving the PIs. We implement COREL as an RNN fol-lowed by two message-passing layers. To approximate the quantile function, we train the model by minimizing the pin-ball loss over a discrete set of quantiles, similarly to Wen et al. [27]. PIs are constructed as in Eq. 14; App. G shows results for the alternative construction in Eq. 16. To learn the graph, we directly parametrize the score matrix \u03a6 by associating a learnable parameter with each of its entries. We use as metrics: 1) the difference between the specified confidence level 1 \u2013 \u03b1 and the observed coverage on the test set (\u25b3Cov); 2) the width of the PI (PI-Width); 3) the Winkler score [64], which is computed as the width of the PI plus penalty for each observation outside of the predicted interval proportional to the actual error (Winkler). More details and results are provided in the appendix."}, {"title": "5.1 TIME SERIES FORECASTING BENCHMARKS", "content": "We consider the following datasets, each coming from a different application domain: METR-LA from the traffic forecasting literature [65]; a collection of air quality measure-ments from different Chinese cities (AQI) [66]; a collection of energy consumption profiles acquired from smart meters within the CER smart metering project (CER-E) [67, 68]. We follow the preprocessing steps of previous works [65, 69, 37] and adopt 40%/40%/20% splits for training, calibration, and testing, respectively. For each dataset, we first train 3 differ-ent baseline models: a simple RNN with gated recurrent unit (GRU) cells [70], a decoder-only Transformer [71], a sim-ple TTS STGNN by following the template in Sec. 2.4. The latter uses a pre-defined graph that models the dependencies across the time series. After training, we evaluate each base-line on the calibration set and save the associated residuals, which are then used as input to the different CP methods. More details on the datasets and base models are provided in App. C.\nBaselines We compared COREL against the following methods: 1) SCP, the standard split CP; 2) SeqCP, where, analogously to Xu and Xie [18], we compute empirical quan-"}, {"title": "5.2 CONTROLLED ENVIRONMENT", "content": "tiles using only the most recent K residuals at each time step; 3) NexCP [16], which computes empirical quantiles by as-signing exponentially decaying weights to past residuals; 4) SCPI [19], which estimates the residuals' quantile function from the last few steps of each time series with a random for-est; 5) HopCPT which reweights past residuals by learning attention scores with a Modern Hopfield Network [72]. We in-clude in our comparison a model called CORNN, where we use the same architecture as COREL but remove the message-passing layers. Except for HopCPT which uses a custom procedure [21], model selection is performed on a validation set by optimizing the Winkler score.\nResults Tab. 1 reports the results across the datasets and the base prediction models. The first observation is that COREL outperforms the competitors in terms of Winkler score in almost all cases. We observed a few exceptions only when the baseline is itself an STGNN, as it is already expected to take care of modeling spatiotemporal dependencies. However, note that the STGNN base model has access to a pre-defined graph, which is not always available in practical applications. In terms of coverage, COREL achieves good results, with the exception of some cases in the CER-E dataset. How-\nWe evaluated the behavior of COREL in a controlled envi-ronment by simulating a diffusion process on a graph. In particular, the experiment relied on the GPVAR benchmark introduced by Zambon and Alippi [12], with a setup anal-ogous to [37]. Data were generated recursively from the auto-regressive polynomial graph filter:\n$H_t = \\sum_{l=1}^{L_Q}\\sum_{q=1}^{Q} \\Theta_{l} A^{l-1} X_{t-q},$ \n$X_{t+1} = a \\tanh\\left(\\Theta H_t\\right) + b \\tanh \\left(X_{t-1}\\right) + N_t,$ (19)\nwhere parameters $\\Theta \\in \\mathbb{R}^{Q \\times L_Q}$, $a \\in \\mathbb{R}^{N}$, $b \\in \\mathbb{R}^{N}$ are kept fixed across nodes and $N_t \\sim \\mathcal{N}(0, \\sigma^2 I)$. We ran the simula-tion on a graph with 60 nodes and with a topology analogous to previous works [37]; more details are provided in App. C. We use both a RNN and STGNN as base point predictors. As shown by Zambon and Alippi [12], STGNNs can obtain a forecasting accuracy near the theoretical optimum in this dataset, which results in i.i.d. residuals. As such, we would expect standard SCP to be sufficient in this scenario. The objective of this experiment is to show that COREL is effec-tively able to capture and leverage existing spatiotemporal dependencies."}, {"title": "5.3 ADAPTABILITY", "content": "ever, we note that our model selection prioritized the Winkler score which emphasizes the width of the prediction bands besides the coverage. CORNN, the simplified version of our approach, obtains good overall performance and is competi-tive against the state-of-the-art but, despite providing good coverage, it is outperformed by COREL in most scenarios in terms of Winkler score. Among the competitors, HopCPT provides competitive results in METR-LA and CER-E but with a larger coverage gap. Except for SeqCP, the other base-lines obtain good coverage in most settings at the expense of drastically wider PIs.\nIn this experiment, we evaluate how effectively"}]}