{"title": "VFM-Det: Towards High-Performance Vehicle Detection via Large Foundation Models", "authors": ["Wentao Wu", "Fanghua Hong", "Xiao Wang", "Chenglong Li", "Jin Tang"], "abstract": "Existing vehicle detectors are usually obtained by training a typical detector (e.g., YOLO, RCNN, DETR series) on vehicle images based on a pre-trained backbone (e.g., ResNet, ViT). Some researchers also exploit and enhance the detection performance using pre-trained large foundation models. However, we think these detectors may only get sub-optimal results because the large models they use are not specifically designed for vehicles. In addition, their results heavily rely on visual features, and seldom of they consider the alignment between the vehicle's semantic information and visual representations. In this work, we propose a new vehicle detection paradigm based on a pre-trained foundation vehicle model (VehicleMAE) and a large language model (T5), termed VFM-Det. It follows the region proposal-based detection framework and the features of each proposal can be enhanced using VehicleMAE. More importantly, we propose a new VAtt2Vec module that predicts the vehicle semantic attributes of these proposals and transforms them into feature vectors to enhance the vision features via contrastive learning. Extensive experiments on three vehicle detection benchmark datasets thoroughly proved the effectiveness of our vehicle detector. Specifically, our model improves the baseline approach by +5.1%, +6.2% on the $AP_{0.5}$, $AP_{0.75}$ metrics, respectively, on the Cityscapes dataset. The source code of this work will be released at https://github.com/Event-AHU/VFM-Det.", "sections": [{"title": "I. INTRODUCTION", "content": "VEHICLE detection is the premise of fine-grained vehicle analysis and plays an important role in intelligent video surveillance. Nowadays, many object detectors can be adopted for vehicle detection, such as YOLO series [1]\u2013[4], R-CNN series [5]-[8], DETR series [9]-[12], and other detectors [13]-[16]. Although good performance can already be achieved using these models, however, the detection performance in challenging scenarios is still unsatisfactory. Many researchers employ multi-modal data to address these issues, however, the popularity of multi-modal devices is still low, which limits the scope of use of their methods.\nRecently, pre-training and fine-tuning techniques have become very popular in the artificial intelligence (AI) community. The researchers usually first pre-train a large backbone network (for example, BERT [19], GPT series [20]-[23], ViT [24], CLIP [25]) in a self-supervised or unsupervised way, then, adapt it to various downstream tasks using Parameter Efficient Fine Tuning (PEFT) strategies (e.g., prompt tuning [26], adapter [27], side tuning [28], [29], etc). Inspired by these works, some researchers attempt to tune these pre-trained big models for object detection. Specifically, Li et al. [12] develop a backbone network for the object detection task using the plain and non-hierarchical vision Transformer. Lin et al. [30] address the multi-domain universal detection problem using a pre-trained large vision model. Fang et al. [31] used pre-trained vanilla ViT network for object detection and instance segmentation. They achieve better results even compared with strong hierarchical architectures, such as Swin-Transformer [32], MViTv2 [33] and ConvNeXt [34].\nAlthough a better detection performance can be achieved, however, these models may be still limited by the following issues: 1). Generalized Large Model vs. Large Vehicle Model: Current pre-trained big vision model based object detectors are proposed for universal or general object detection which usually adopts the generalized pre-trained large backbone for their work. However, these large models may only achieve sub-optimal results for the vehicle detection problem which is a specific downstream task. 2). Semantic Gap Problem: Existing vehicle detection algorithms mainly rely on the feature representations obtained using the pure pre-trained large vision models, but ignore the semantic gap between the semantic label information of the target object (e.g., the vehicle) and the visual representations. Given the two issues above, it is natural to raise the following question: \"How can we achieve high-performance vehicle detection using pre-trained models and fully bridge the aforementioned semantic gaps between the vision features and semantic categories?\"\nIn this work, we propose a new vehicle detection framework by adapting the pre-trained large vehicle foundation model VehicleMAE [17] and a large language model (T5 [18] adopted in this work) into the Mask RNN detector, as illustrated in Fig. 1. More in detail, on the basis of region proposal-based detection framework, our model takes the raw image as the input and adopts a backbone network (e.g., ResNet [35]) for feature extraction. After that, it adopts the Region Proposal Network (RPN) to get the features of interest. A detection head is introduced for location regression and label classification. In this work, we propose to enhance the baseline detector from the following two aspects. First, the large vision model VehicleMAE which is specifically pre-trained on massive vehicle images is adopted to enhance the feature representations of each proposal. Second, we predict the semantic cues of each proposal using an attribute estimate network which takes the vision features of each proposal and all the attributes as the input. We transform the estimated attributes into a unified semantic representation using GRU network [36] and bridge the aforementioned semantic gaps by aligning the vision features and unified semantic representation based on contrastive learning. The experiments thoroughly proved the effectiveness of our proposed strategy for the vehicle detection.\nTo sum up, the key contributions of this paper can be summarized as the following three aspects:\n1). We propose a novel vehicle detection framework, termed VFM-Det, by adapting the pre-trained vehicle large model VehicleMAE and successfully improving the final results significantly.\n2). We propose a new semantic gap reduction module, termed VAtt2Vec, for vehicle detection by learning the representations of vehicle attributes and aligning its vision features using contrastive learning.\n3). Extensive experiments on three large-scale vehicle detection benchmark datasets thoroughly proved the effectiveness of our proposed vehicle detection framework.\nThe following of this paper is organized as follows: In section II, we mainly introduce the related works about the pre-trained models, object detection task, and attribute representation learning. In section III, we concentrate on outlining our proposed framework, more in detail, we first review the scheme of the proposal-based detection framework and then give a review of our proposed new vehicle detection model. After that, we focus on the pre-trained VehicleMAE-based perceptron, vehicle attribute representation learning, and loss functions used for training. In section IV, we introduce our experiments by describing the datasets and evaluation metrics, implementation details, comparisons with other detectors, and related analysis. We also summarize the limitations of our model and propose possible improvements as our future works in section IV-F, and conclude this work in section V."}, {"title": "II. RELATED WORKS", "content": "In this section, we will introduce the works most related to ours, including Object Detection, Pre-trained Big Models, and Attribute Representation Learning\u00b9.\n### A. Object Detection\nThe three main categories of deep learning-based object detection are one-stage detectors [1], [38], two-stage detectors [5], and Transformer-based detectors [9], [12]. Among them, the one-stage detectors based on sliding windows directly classify and locate semantic targets through dense sampling, avoiding the step of screening potential object regions in the image. YOLO [1] adopts a direct regression approach to obtain detection results, effectively improving the detection speed. RetinaNET [38] proposes a focal loss function, which handles the foreground-background imbalanced problem well. YOLO-V3 [3] adopts multi-scale feature map extraction, improving the detection effect of the YOLO series on small targets significantly. After that, a series of follow-up works [4], [39], [40] are proposed to further improve the YOLO detector. Two-stage object detection first extracts the proposal from the image, and then makes secondary modifications grounded in the proposal to obtain the detection results. R-cnn [5] first attempts to address the detection task using neural networks and improve the overall performance significantly. Faster RCNN [6] proposes an RPN module to generate candidate boxes that solve the issues caused by selective search well. Mask RCNN [7] introduces the Rol align layer instead of Rol pooling operator to avoid pixel-level dislocation caused by spatial quantization.\nAlong with the outstanding performance of Transformer networks in many domains, some researchers have begun to think about the integration of Transformer and object detection. To be specific, the detectors [41] based on Transformer, DETR [9] first introduce Transformer into object detection. Zhu et al. [10] proposed the deformable attention module, which improved the training speed and detection performance on small objects of the original DETR algorithm. DINO [11] adopts contrastive denoising training and mixed query selection methods, further enhancing the performance of the DETR model. VITDET [12] uses SFP instead of a feature pyramid structure, eliminating hierarchical constraints of backbone networks. Different from these works, in this paper, we propose to enhance vehicle detection using pre-trained large vision and language models and achieve a higher performance than our baseline significantly.\n### B. Pre-trained Big Models\nSelf-supervised/unsupervised pre-training is currently the focus of research. Currently, there are two mainstream self-/unsupervised pre-trained algorithms: contrastive learning and reconstruction-based pre-trained. Specifically, contrastive learning methods aim to train the network to distinguish whether given input pairs are aligned. For single-modal input pre-training, SimCLR [42], MoCo [43], etc., generate a"}, {"title": "III. METHODOLOGY", "content": "In this section, we will first provide an overview of our proposed VFM-Det framework, and then, we review the region proposal-based detection framework. After that, we will present the detailed network architectures, focusing on the VehicleMAE [17] based Perceptron, Contrastive Learning between vision features and unified attribute representations. Finally, we will introduce the detection head and loss functions used for the optimization of our whole framework.\n### A. Overview\nIn this work, based on the aforementioned region proposal-based object detection framework, we propose applying pre-trained foundation models to the vehicle detection tasks, as shown in Fig. 2. Specifically, we first feed the input image into the ResNet-50 backbone network to get the feature maps. Then, the RPN generates a group of candidate bounding boxes, and their features can be obtained via the Rol Align layer. More importantly, we crop the proposals out and feed them into the pre-trained vehicle foundation model VehicleMAE to extract more fine-grained vehicle-specific features. These features are concatenated with the features output by Rol Align along the channel dimension and then fed into the detection head and classification head to obtain the target's location and category. In addition, we also propose a new VAtt2Vec module to bridge the gaps between the vision features used for vehicle detection and semantic labels that can describe the vehicles in high-level semantic features. To be specific, this module takes the vision features of each proposal and all the defined vehicle attributes as the input and predicts the attributes using an attribute head. Note that, the given attributes are encoded using a large language model T5 [18]. The predicted attributes are further fused and transformed into a unified feature representation via a GRU [36] module. The visual feature and unified semantic attribute feature are used for contrastive learning. Extensive experiments on three vehicle detection benchmark datasets demonstrate that the pre-trained large vision and language models improve vehicle detection performance significantly.\n### B. Review: Region Proposal-based Detection\nThe key ideas of the region proposal-based detection framework can be divided into two stages, i.e., the candidate region generation and classification. To be specific, we first generate a series of candidate regions using an efficient algorithm, which contains positive and negative objects. Then, a classification head and a bounding box regression head are adopted for each proposal to determine target object is present or not, and also predict its locations and scales more accurately. One can find that the high-quality candidate region generation is a key procedure in the region proposal-based detection framework and the widely used modules including selective search, edge boxes, RPN, etc. Selective Search produces candidate regions by dividing the image into multiple regions and evaluating the properties of each region; edge boxes generate candidate regions guided by image edge information; RPN predicts the location and size of candidate regions by training neural networks. The representative region proposal-based detectors contains RCNN [5], Faster R-CNN [6], Mask R-CNN [7], etc. In this work, we integrate our proposed strategies into the Mask R-CNN detector to validate their effectiveness. The subsequent subsection will provide further information, correspondingly.\n### C. VehicleMAE based Perceptron\nBased on the Mask R-CNN [8], we propose to enhance the detection performance on the vehicles using a specifically pre-trained foundation vision model VehicleMAE [17]. Specifically, given the input image $I$ and the candidate region information $C$ generated by the RPN module, for each input image, the RPN generates 512 candidate regions. We first crop all proposal images from the input image based on the candidate region information $C$, and resize them into 224 \u00d7 224, and obtain the image set $I_{pro} \\in R^{224\\times224\\times3}$. For each image in the image set $I_{pro}$, we divide it into 196 non-overlapping patches, then, project them into token embeddings $P_{rat} \\in R^{1\\times768}$, $j \\in {1,2,...,196}$, using a convolutional layer (kernels size is 16 \u00d7 16). After integrated CLS-token, we have obtained the feature $F_{CLS} \\in R^{197\\times768}$. We introduce position encoding $Z_{pos} \\in R^{197\\times768}$ to encode the spatial coordinates of input tokens. More in detail, the position encoding is randomly initialized and added with the token embedding, i.e., we have $F_{ros} = Z_{pos} + F_{CLS}$. Significantly, we fixed the parameters of the pre-trained VehicleMAE network and introduced learnable tokens $K \\in R^{8\\times768}$ to achieve more efficient fine-tuning. These learnable tokens are placed between the CLS-token and patch-token to obtain feature $F_{emb} \\in R^{205\\times768}$. After obtaining the input vision embeddings $P_{emb}$, we input them into the VehicleMAE encoder (i.e., ViT-B/16) which contains 12 Transformer blocks. The output of the foundation vision model VehicleMAE is $F \\in R^{205\\times768}$.\nTo align with the dimension of Rol Align output features from the ResNet50 backbone network $F_{roi} \\in R^{256\\times16\\times16}$, we use a 256-dimensional linear projection layer to project the output of VehicleMAE encoder $F$ to $F \\in R^{205\\times16\\times16}$. Finally, we concatenate the Rol Align output features $F_{roi}$ and $F$ along the channel dimension to obtain the visual feature $F \\in R^{461\\times16\\times16}$ for vehicle detection.\nBy introducing the pre-trained foundation vehicle-specific vision model, the detection performance can be improved. However, this detector still ignores the alignment between the vision features and semantic high-level features, which may still bring us a sub-optimal result. In the following subsection, we will exploit the vehicle attributes guided visual feature learning to further improve overall performance.\n### D. Vehicle Attribute Representation Learning\nTo integrate the vehicle attributes into our detection framework, in this work, we define a total of 47 vehicle attribute tags which can be categorized into six groups based on attribute labels from the CompCars [61] dataset. More in detail, the six groups are color, number of doors, model, displacement, top speed, and number of seats. We utilize the pre-trained large language model T5 [18] to obtain the text embeddings $t_{emb} \\in R^{1\\times768}$, $i \\in {1, 2, ..., 47}$, for these 47 vehicle attribute tags. By concatenating all the text embeddings, we obtain the text feature $F_{t} \\in R^{47\\times768}$ and feed them into the attribute prediction head along with the visual feature $F_{ve} = F$ output by the encoder of VehicleMAE.\nIn the attribute prediction head, we first project the text features $F_{t}$ into the same dimension through 1\u00d71 convolution layer with visual features $F_{ve}$. Here, the feature dimension output by the T5 model is identical to the visual feature dimension, therefore there is no need for projection. After that, we introduce learnable visual embedding $e_{vmb} \\in R^{1\\times768}$ and text tokens $e_{t}^{emb} \\in R^{1\\times768}$, to sum with the corresponding features to maintain modality information, respectively. The produced visual features $F_{ve}$ and text features $F_{t}$ are then concatenated to get the feature $F$:\n$F = [F_{t} + e_{t}^{emb}, F_{ve} + e_{vmb}] $"}, {"title": "E. Detection Head and Loss Function", "content": "For the detection head, we keep the same structure as the original Mask R-CNN [8]. We input the concatenated visual features $F$ into the detection head, where they are first projected through two MLP networks and then fed into two fully connected layers. They are respectively used to predict the class score for each proposal and the bounding box regression parameters corresponding to each candidate region, which are further utilized to compute the final box coordinates.\nBased on Mask R-CNN [8], we introduce a new loss function for contrastive learning between vision features and vehicle semantic attributes. Firstly, we normalize the image $F$ and text features $V_{a}$ using the $L_{2}$ norm, and then compute the cosine embedding loss between the normalized features. The following is the formula:\n$L_{va} = \\frac{1}{N} \\sum_{i=1}^{N} CEL(\\frac{V_{a}}{|V_{a}|_{2}},\\frac{F}{||F||_{2}})$ \nwhere $CEL$ stands for Cosine Embedding Loss, N represents the batch size, and $|| * ||_{2}$ refers to the $L_{2}$ norm. The final expression for the total loss function $L = L_{cls} + L_{reg} + L_{va}$. The bounding box regression loss $L_{reg}$ and classification loss $L_{cls}$ are identical to those defined in Mask R-CNN [8]. For further information, we advise readers to review their work."}, {"title": "IV. EXPERIMENTS", "content": "In Section IV-A, we will introduce the three vehicle object detection datasets and evaluation metrics. Then, the implementation details are introduced in Section IV-B. In Section IV-C, we will compare our method with the state-of-the-art (SOTA) detector and pre-trained models, respectively. After that, we study the effectiveness of each key component in our VFM-Det model in Section IV-D. We also give the analysis of tradeoff parameters. In Section IV-E and IV-F respectively introduced the visualization and limitation analysis.\n### A. Datasets and Evaluation Metrics\n\u2022 Datasets. We have demonstrated the effectiveness and generalization of our proposed VFM-Det on three vehicle target detection datasets. Below is a brief introduction to these datasets. We conducted validation on three vehicle detection datasets, including the Cityscapes [62] dataset, the UA-DETRAC [63] dataset, and the COCO2017 [64] dataset. Cityscapes: This is a dataset of urban street scenes. It contains 3,257 high-resolution images from 50 cities in Germany. The dataset covers street scenes under different lighting conditions such as morning, daytime, and nighttime. Each image has a resolution of 2048 x 1024 and is annotated for objects such as buildings, roads, pedestrians, and vehicles. In our experiments, we selected four categories related to the vehicle target, i.e., car, bus, truck, and caravan. Thus, the dataset contains 2846 training images and 481 testing images. UA-DETRAC: This is a multi-target detection dataset for urban road scenes. For a more detailed introduction, please refer to [63]. To facilitate training and testing, we constructed a dataset for vehicle object detection by sampling images every 10 frames from the video dataset, it contains 8,178 training images and 5,608 testing images. The dataset includes three vehicle categories, including car, bus, and vans. COCO2017: This is an object detection dataset. For a more detailed introduction, please refer to [64]. We select images of the vehicle categories for training using the official API. After processing, the dataset contains 16270 training images and 707 testing images. In our training phase, we selected three vehicle target categories, i.e., car, bus, and truck.\n\u2022 Evaluation Metrics. In the work, we have selected three commonly used metrics for object detection, namely $AP_{[0.5:0.95]}$, $AP_{0.5}$, and $AP_{0.75}$. Specifically, $AP_{[0.5:0.95]}$ refers to the calculation of AP values at various IoU (Intersection over Union) thresholds ranging from 0.5 to 0.95 with a step size of 0.05, followed by computing the average of these values. $AP_{0.5}$ represents the AP value when the IoU is set to 0.5, while $AP_{0.75}$ corresponds to the AP value when the IoU is set to 0.75. To be specific, the formulaic expression of AP can be written as:\n$AP = \\frac{1}{M}\\sum_{j=1}^{M}(Precision_{i})$\n$Precision = TP/(TP + FP)$ \nwhere TP and FP represent the number of true positives, and false positives, respectively.\n### B. Implementation Details\nIn our training phase, the learning rate is set as 0.02, the momentum is 0.9, and the weight decay is 0.0001. The SGD is selected as the optimizer to train our model. The batch size is 2 and training for a total of 26 epochs on every dataset. All the experiments are implemented using Python based on the deep learning toolkit PyTorch [65]. A server with RTX3090 GPU is used for the training.\nIn this work, the attribute head in the VAtt2Vec module was pre-trained using the CompCars dataset. To better adapt to the attribute recognition task, we reconstructed the annotated attribute information in the CompCars dataset. Specifically, we modified the maximum speed attribute in the CompCars dataset from its original numerical form to five ranges, i.e., unknown, less than 150, greater than 150 and less than 200, greater than 200 and less than 250, and greater than 250. Furthermore, based on the classification standards for car engine displacement, we transformed the displacement attribute from its original numerical form to four categories: unknown, small, middle, and large. As a result, our reconstructed dataset comprises six attribute groups, 47 labels, and 44,481 images. A detailed list of attributes is as shown in Table III. The attribute head in our VAtt2Vec module was pre-trained on the organized CompCars dataset for 20 epochs.\n### C. Compare with SOTA Detectors\nIn this experiment, we validated the effectiveness of our proposed VFM-Det model through three vehicle object detection datasets. We compared our baseline model and other advanced object detection models on each dataset. Additionally, we also compared the VehicleMAE with other general pre-training models on the VFM-Det model.\n\u2022 Results on Cityscapes datasets. We compared a comparison with other detection algorithms on the Cityscapes dataset, as shown in Table I, our method VFM-Det, achieved the best performance, reaching 46.9%, 66.5%, and 51.6% on the $AP_{[0.5:0.95]}$, $AP_{0.5}$, and $AP_{0.75}$ metrics, respectively. Compared to our baseline method Mask R-CNN, we improved by 5.2%, 5.1%, and 6.2% on the three evaluation metrics, respectively. Compared to VitDet, we improved by 1.7%, 2.4%, and 1.5% on the three evaluation metrics, respectively. Compared with other unsupervised pre-training models, the VehicleMAE we used achieved the best performance. As shown in Table II, compared to DINO, we improved by 1.9%, 0.9%, and 2.3% on the three evaluation metrics, respectively. Compared to MAE, we improved by 4.5%, 4.7%, and 5.2% on the three evaluation metrics, respectively. Additionally, we compared the MAE model pre-trained on our proposed vehicle pre-training dataset, achieving improvements of 3.2%, 3.5%, and 3.8% on the three evaluation metrics, respectively.\n\u2022 Results on UA-DETRAC datasets. We conducted a comparison with other detection algorithms on the UA-DETRAC dataset. As shown in Table I, our VFM-Det model achieved 51.7%, 73.7%, and 63.1% on the $AP_{[0.5:0.95]}$, $AP_{0.5}$, and $AP_{0.75}$ metrics, respectively. In contrast, our baseline algorithm achieved 48.0%, 70.5%, and 58.0% on these metrics, respectively. Notably, our model outperformed Mask R-CNN by 3.7% on the $AP_{0.5}$ metric. Existing detection algorithms such as VitDet, RetinaNet and DetectoRS fell short of our model in $AP_{0.75}$ metrics. Additionally, we compared our model with other unsupervised pre-training models. As presented in Table II, our model improved by 3.2% on the $AP_{0.5}$ metric compared to MoCoV3 and by 2.1% compared to MAE. Our model also outperformed other unsupervised pre-training algorithms on other metrics.\n\u2022 Results on COCO2017 datasets. We also conducted tests on the vehicle category within the commonly used object detection dataset, COCO2017. As shown in Table I, our baseline method Mask R-CNN algorithm achieved 45.9%, 68.0%, and 50.8% on the $AP_{[0.5:0.95]}$, $AP_{0.5}$, and $AP_{0.75}$ metrics, respectively. In contrast, our proposed VFM-Det model achieved 51.5%, 75.3%, and 56.6% on these metrics, representing improvements of 5.6%, 7.3%, and 5.8% over Mask R-CNN, respectively. When compared to other detection algorithms, our model outperformed VitDet with improvements of 1.1%, 2.9%, and 0.8% on the respective metrics. In Table II, we compared our algorithm with other unsupervised pre-training models on the COCO2017 dataset. Our algorithm improved by 4% over MoCoV3 on the $AP_{0.5}$ metrics, and by 3.9% over MAE, respectively. These experimental results thoroughly proved the superiority of our model."}, {"title": "D. Ablation Study", "content": "In this section, to make it easier for readers to understand the contributions of each module in our framework, we conduct comprehensive ablation studies to validate and analyze the effectiveness of the related settings.\nEffects of VehicleMAE Encoder. In this paper, we introduce VehicleMAE encoder to enhance the features of the proposal and concatenate it with the original features. As shown in Table IV, after introducing VehicleMAE encoder, we tested it on the Cityscapes dataset and found that the results improved to 45.0%, 65.4%, and 48.9% on the three metrics. These experimental results indicate that enhanced proposal features can improve detection performance.\nEffects of VAtt2Vec. In the VAtt2Vec module, we introduce the $L_{va}$ loss function. As shown in Table IV, based on the VehicleMAE encoder, the VAtt2Vec module leads to improvements of 1.9%, 1.1%, and 2.7% on the three metrics when tested on the Cityscapes dataset. When both the VehicleMAE encoder and VAtt2Vec modules are introduced, the results are further improved to 46.9%, 66.5%, and 51.6%. The effectiveness of our VAtt2Vec module can be demonstrated by the experimental results. Since the input features of the attribute head originate from the VehicleMAE encoder, we have not conducted ablation studies specifically targeting the VAtt2Vec alone.\nEffects of the number of learnable tokens. Given the large number of proposals generated during the training process, we fixed the parameters of the VehicleMAE encoder during model training to enhance computational efficiency and conserve resources. This action not only effectively reduced computational complexity but also preserved the crucial features and knowledge learned by the large model during previous training. However, fixing the VehicleMAE encoder may result in sub-optimal outcomes, because of discrepancies in the pre-training and detection datasets. Therefore, we introduced a certain number of learnable tokens to enable the pre-trained VehicleMAE encoder to better adapt to new data and tasks. We delved into the influence of the number of introduced learnable tokens on model performance. As shown in Table V, when the number of learnable tokens was 4, the respective indicators were 44.8%, 64.5%, and 50.6%. When the number increased to 8, each indicator improved by 2.1%, 2.0%, and 1.0%, respectively. However, when the number further increased to 12, each indicator decreased by 2.4%, 2.8%, and 1.9%, respectively. Based on these experimental results, we decided to introduce 8 learnable tokens in the VehicleMAE encoder. These comparative experiments fully demonstrated the effectiveness of introducing learnable tokens.\nEffects of different attribute encoders. In this paper, for each candidate region image, a set of corresponding vehicle attributes is predicted. We utilize a large language model to extract features from these attribute information, and then fuse the attribute features into a unified textual representation through a GRU [36] module. The model is optimized by computing the cosine similarity loss between the textual and visual features. Therefore, the capability of the chosen large language model will directly impact the performance of the detector. We compared five large language models: T5 [18], CLIP [25], BERT [19], ALBERT [66], and MPNet [67], in the Table VI. Among them, T5 [18] achieves optimal performance in the $AP_{[0.5:0.95]}$ and $AP_{0.5}$ metrics, with values of 46.9% and 66.5%, respectively. However, it falls slightly behind CLIP [25] by 0.4% in the $AP_{0.95}$ metric. Consequently, we choose to use the large language model T5 [18] as the text encoder.\nEffects of different feature fusion strategies. In this paper, we retain the image features extracted by the original Mask R-CNN through ResNet50, while simultaneously inputting the candidate region images generated by the RPN into the pre-trained large vehicle model, VehicleMAE, for feature extraction through its encoder. Finally, these two sets of features are fused for detection. Therefore, the method of feature fusion is crucial to the model's performance. We compare three feature fusion strategies: concatenate, weighted fusion, and linear fusion. The effectiveness evaluation results are summarized in Table VII. Concatenating the two features achieves 46.9%, 66.5%, and 51.6%. Compared to weighted fusion, concatenate operation improves the metrics by 2%, 3.2%, and 2%, and compared to linear fusion, it improves by 1%, 2.3%, and 0.7%. Consequently, the concatenate operation is chosen as the feature fusion method in this paper.\nEffects of different usage methods of attribute vectors. Utilizing the attribute heads pre-trained on the CompCars dataset, we conducted attribute prediction for each proposal, obtaining a set of attribute features encoded by T5. Subsequently, these features were fused through a GRU module to derive the textual features of the proposal. We compared two distinct methods of utilizing the attribute vectors, as summarized in Table VIII. When the textual features were directly concatenated with the image features and then fed into the classification and regression heads, the results were 45.9%, 64.6%, and 50.3%, respectively. Alternatively, introducing a cosine similarity loss for image-text contrastive learning yielded results of 46.9%, 66.5%, and 51.6%. Compared to the concatenation approach, the contrastive learning method improved the metrics by 1%, 1.9%, and 1.3%, respectively. We believe that the primary reason for this improvement lies in the ability of contrastive learning to reduce redundant information and noise while leveraging the complementary between features. Additionally, there exists a domain gap between the CompCars dataset and the proposals, and the contrastive learning approach effectively mitigates this impact compared to concatenation. Consequently, our method adopts the contrastive learning approach."}, {"title": "E. Visualization", "content": "In this section, we visualize the detection results of our model, VFM-Det, the attribute results of the proposals detected by the VAtt2Vec module, and the feature maps of the proposals processed by the VehicleMAE backbone.\nWe present the detection results on road images, where the green boxes represent the detection results of our proposed algorithm, the white boxes indicate the detection results of Mask R-CNN, and the red boxes represent the ground truth. As shown in Fig. 5, our approach can accurately detect vehicle targets. In Fig. 6, we present the detection results of the VAtt2Vec module for proposal attributes. Additionally, as depicted in Fig. 7, we employed GradCAM\u00b2 to visualize the attention maps of the last Transformer block in the VehicleMAE encoder. It can be observed that the attention is primarily focused on the vehicle targets, indicating that our VehicleMAE encoder is capable of extracting more effective proposal features."}, {"title": "F. Limitation Analysis", "content": "Based on the above experiments, we found that introducing the pre-trained vehicle-specific foundation model VehicleMAE, significantly enhances the detection performance for vehicles. However, due to the large number of proposals, even though we fixed the parameters of the VehicleMAE encoder, it still introduced more computational overhead. Therefore, further reducing the complexity of our detector is an important research direction [68]. On the other hand, the attribute prediction head is trained on a small dataset which may limit the performance of attribute prediction. This module also makes our detector unable to be optimized end-to-end. We will address these two issues in our future work."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel vehicle detection paradigm, named VFM-Det, by extending the region proposal-based detectors based on pre-trained foundation vision and language models. For the input image, we first feed it into a ResNet50 backbone network to obtain image features and generate a set of proposals through a region proposal network. Subsequently, we crop these proposals from the image and utilize the VehicleMAE encoder, which is pre-trained on a large-scale vehicle image dataset, to extract features from the proposals, thus enhancing the raw features. More importantly, we introduce a novel VAtt2Vec module, which predicts the semantic attributes of the vehicle for these proposals based on the features extracted by the VehicleMAE encoder. These attributes are then converted into a unified feature vector, and the model is optimized by computing similarity constraints with visual features. We evaluate and compare our VFM-Det on three vehicle detection datasets, and extensive experiments thoroughly proved the effectiveness and superiority of our proposed vehicle detector."}]}