{"title": "AGENTREFINE: ENHANCING AGENT GENERALIZATION THROUGH REFINEMENT TUNING", "authors": ["Dayuan Fu", "Keqing He", "Yejie Wang", "Wentao Hong", "Zhuoma Gongque", "Weihao Zeng", "Wei Wang", "Jingang Wang", "Xunliang Cai", "Weiran Xu"], "abstract": "Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research.", "sections": [{"title": "INTRODUCTION", "content": "Language agents (Mialon et al., 2023; Sumers et al., 2023), which harness the powerful capabilities of large language models (LLMs) to perceive environments, make decisions, and take actions, have emerged as an effective solution to complex real-world problems. Plenty of agent projects such as AutoGPT (Sig), GPT-Engineer (gpt), and BabyAGI (yoh) have employed LLMs as the core controllers, showing potential for practical applications. Both prompt engineering (Yao et al., 2022; Fu et al., 2024; Zhao et al., 2024) and framework practice (Yao et al., 2024; Shinn et al., 2024) have been proposed to enhance the agent capability of top-tier commercial LLMs like GPT-4. Recently, open-sourced LLMs (Dubey et al., 2024; Jiang et al., 2023) are emerging as effective alternatives to GPT models and show promising results.\nMany efforts have been made to enhance the agent capability of open-sourced LLMs via finetuning. Deng et al. (2024); Qin et al. (2023) carefully define single task schema and collect agent data for specific vertical fields. Further, Zeng et al. (2023); Chen et al. (2024); Hu et al. (2024) extend to"}, {"title": "RETHINK THE GENERALIZATION OF AGENT-TUNING", "content": "Current agent-tuning works lack generalization to new agent tasks. Figure 1 compares the performance between held-in and held-out agent tasks, where Agent-FLAN utilizes the Alfworld environment to gather training data and subsequently makes direct predictions for the held-out tasks. We observe a clear performance drop between the two settings.\nMemorizing true trajectories leads to overfitting. To further figure out the reason behind the poor generalization, we employ a study on the robustness of Agent-FLAN. Figure 2 displays the different"}, {"title": "METHODOLOGY", "content": null}, {"title": "DATA CONSTRUCTION", "content": "Inspired by the Tabletop Role-playing game (TRPG), AgentRefine data's construction process can be divided into three parts: script generation, trajectory generation, and verification, as shown in Figure 4. The script generation requires the LLM to generate a script with the environment, tasks, and available actions based on the persona. In the trajectory generation phase, the LLM is required to simultaneously play the roles of both Dungeon Master (DM) and player to generate multi-turn agent data containing errors and refine steps based on the script. The verification will verify the script and trajectory, giving LLM the mistake it has made within a given persona and the LLM will regenerate the script/trajectory based on the verifier's response.\nScript Generation We first sample a persona pi from diverse personas (Chan et al., 2024), and prompt the LLM to generate a script with the environment, tasks, and available actions based on pi. The environment will include locations, items, and player information that may appear in the interaction. To assist the LLM in understanding the environment, we prompt the LLM to display the hierarchical relationships between locations/items in JSON format. We also require the LLM to generate some interfering locations/items, to ensure that some erroneous steps are likely to occur during trajectory generation. After generating the environment, the LLM will generate a clear and specific task. Finally, the LLM will generate a series of available actions. For each action, we require the LLM to generate an action name, validation code (a regular expression), and valid parameters. The structure of the script can be seen in Appendix L.\nTrajectory Generation Given a script, the LLM can simulate multi-turn interactions between the DM and the player within one call. Specifically, the DM's turn is divided into three stages: thinking, observing, and evaluating. In the thinking stage, we require the LLM to evaluate the player's state and known information so far and analyze the observations the player can obtain based on the last"}, {"title": "GENERATION SETUP", "content": "We use gpt-40-2024-05-13 to generate the script and trajectory. We will save all trajectories that can pass verification in 4 LLM calls (including script generation and trajectory generation). We primarily adopt the 1-shot trajectory example approach in trajectory generation and the 3-shot script examples in script generation to help LLM follow the format and give a diversified result. In Appendix 5, we use deepseek-v2.5 (Liu et al., 2024) as the open-source LLM to generate the script and trajectory."}, {"title": "REFINEMENT TUNING", "content": "After generating the complete trajectory, we convert the trajectory into a Refinement Tuning dataset DRT, specifically, the user turn is the DM's observation, while the assistant turn is the Player's thought and action, in ReAct (Yao et al., 2022) format. To prevent interference from error turns generated by the LLM, we changed the loss function J(\u03b8), as shown in Equation 1 where N is the total turn number of a given data x, Ti, Aj, Oj is the thought, action, and observation in turn j. If Aj is correct 1(Aj) = 1 else 1(Aj) = 0.\n\nJ(\u03b8) = Ex~DRT \u2211 log (\u03c0\u03bf (\u03a4\u03af, \u0391i I, {Tj, Aj, 0}j=0i-1)1(\u0391i))"}, {"title": "EXPERIMENTS", "content": null}, {"title": "EXPERIMENT SETUP", "content": "Training We use the LLaMA3-base series models (Dubey et al., 2024) for most of our experiments. For mistral (Jiang et al., 2023), we use mistral-v0.3. We applied the original llama3 (or mistral)'s multi-turn chat template. We use LLaMA-Factory (Zheng et al., 2024) to train our models. The training hyperparameter details can be seen in Appendix D.\nTasks We select 5 tasks: SciWorld (Wang et al., 2022), Alfworld (Shridhar et al., 2020), BabyAI (Chevalier-Boisvert et al., 2018), PDDL (Vallati et al., 2015), and Jericho (Hausknecht et al., 2020), all of them are testing models' decision-making ability. We use the AgentBoard (Ma et al., 2024) framework for experiments, this framework can determine whether the agent has completed all tasks (success rate) and whether the agent has reached key nodes (progress rate). The Held-in task refers to Alfworld, while the Held-out tasks are the results obtained by the weighted average of other tasks based on AgentBoard (Ma et al., 2024) We change AgentBoard's prompts from Act-only to ReAct and the historical thought, action, and observation will be transformed into the chat format instead of plaintext. We adjusted the example prompts on Llama-3-8B-Instruct and never changed them during this work. (except \u00a74.3). The max turn is 30 for all these tasks in inference. To further prove AgentRefine's generalization, followed by the choice in ReAct (Yao et al., 2022), We choose a reasoning task HotpotQA (Yang et al., 2018) in the ablation experiment. We use Wikipedia search in LATS (Zhou et al., 2023) as the environment, randomly sample 300 questions from HotpotQA, and test the exact match (EM) and F1 score of those methods. The max turn is 8 for HotpotQA task in inference. It should be emphasized that we will only use environment feedback in the inference and we will not use GPT4's judgement as the feedback.\nBaseline For the close-source model, we choose GPT-40 (gpt-4o-2024-05-13) and GPT4o-mini (gpt-4o-mini-2024-07-18). For the open source model, we choose Meta-Llama-3-8B-Instruct, Meta-Llama-3-70B-Instruct, and Mistral-7B-Instruct-v0.3. For fine-tuned mode, we choose Agent-FLAN (Chen et al., 2024), AgentGym (Xi et al., 2024), and AgentGen (Hu et al., 2024) as the baseline. They are all trying to solve the agent generalization problem. Agent-FLAN is an improvement of AgentTunning (Zeng et al., 2023), focusing on training \"thought\" in ReAct. AgentGym uses lots of environments to ensure generalization and AgentGen uses LIMA (Zhou et al., 2024) to synthesize diversified agent-tuning data. Agent-FLAN includes Alfworld in its training set. AgentGym includes Alfworld, BabyAI, and SciWorld in its training set. These datasets will be seen as Held-in test tasks for the corresponding method. Since Agent-FLAN and AgentGym's original model is LLaMA2-Chat, for a fair comparison, we reproduce them under LLaMA3 and Mistral. Since AgentGym has not open sourced, we only report the result in (Hu et al., 2024)"}, {"title": "MAIN RESULTS", "content": "Table 1 shows the performance comparison of AgentRefine and other methods across different families and sizes. It is important to emphasize that some methods sample training data in the same environment as the task; in such cases, we consider this task for these methods to be held-in. We identify the held-in metrics for each method with an underscore. It can be observed that compared to other agent works, our method shows significant advantages in held-out tasks. For example, it leads Agent-FLAN by 13.3% in Sciworld Success Rate. Notably, in some tasks, AgentRefine can even match the performance of the GPT-40 series. This demonstrates the strong generalization capability of AgentRefine. We also observe that AgentRefine can not outperform held-in training methods. However, in \u00a7 4.3, we will demonstrate that these held-in methods simply memorize the mapping between observation and action, and a very small perturbation can render these methods ineffective. Furthermore, we also notice that LLaMA-3-8B-Instruct exhibits very strong performance in many tasks. We attribute this to its extensive use of Alignment data and additional RL training. In subsequent experiments, we also mix alignment data and AgentRefine and achieve further gains.\nEffect of Refinement Tuning To further investigate the effectiveness of Refinement Tuning, we mask the loss of refinement trajectory tokens. Table 2 shows that after masking the refinement, the model's performance over 5 tasks drops dramatically. For instance, there is approximately 43% performance drop in Sciworld which, to some extent, reflects the necessity of Refinement Tuning for Agent tasks. we also re-generated a training set without error and refinement trajectories, which"}, {"title": "ROBUSTNESS ANALYSIS", "content": "Previous work has extensively trained on held-in tasks but shows poor performance on held-out tasks. One possible reason is that models simply memorize the key-value pairs between observation"}, {"title": "DIVERSITY ANALYSIS", "content": "Thought Diversity Figure 6 illustrates the distribution of chain-of-thought diversity across three agent datasets. We extracted the thought content from all ReAct rounds and vectorized them. We randomly sampled 8100 data from all thoughts and visualized them via dimensionality reduction using t-SNE (Van der Maaten & Hinton, 2008). Compared to Agent-FLAN and AgentGym, the data of AgentRefine are more widely distributed and numerous in Figure 6, indicating a higher diversity of thoughts in AgentRefine. This suggests that the AgentRefine data can better teach the model to think diversely, achieving a broader exploration space.\nEnvironment Diversity Figure 7 shows the similarity relationship between the AgentRefine environment and the test datasets. We randomly selected the instructions from 100 data (50 from AgentRefine and 10 from each test set) and removed the one-shot examples from the test sets. As shown in Figure 3, the similarity between the AgentRefine environment and the test environments is less than 0.5 (bottom left and top right sections), indicating a certain degree of difference between our environment and the test environments."}, {"title": "SYNTHESIS FROM OPEN SOURCE MODEL", "content": "In the main experiment, we use GPT-40 to synthesize the AgentRefine data. In this chapter, we attempt to replace it with open-source models to complete the data synthesis process. Table 5 shows our results under 4000 training data. It can be observed that, compared to Agent-FLAN, which used GPT-4 for data synthesis, the AgentRefine data synthesized with the open-source model DeepSeek-v2.5 exhibits significant advantages on the held-out tasks. For example, it leads Agent-FLAN by 11.6% in the BabyAI Success Rate metric, further proving the advantages of AgentRefine. Additionally, we observe a noticeable gap between the data synthesized with DeepSeek and the data synthesized with GPT-40. This indicates that using more capable models for data synthesis does indeed yield higher-quality training data and results in greater performance gains."}, {"title": "GENERLIZATION IN REASONING TASK", "content": "Figure 8 presents the results on the reasoning task, HotpotQA (Yang et al., 2018). The result shows that AgentRefine outperforms other methods on Hot-potQA's EM and F1 metrics. It proves that AgentRefine's generalization still works on reasoning problems."}, {"title": "CASE STUDY", "content": "Figure 9 presents examples of Agent-FLAN and AgentRefine in Jericho and Sciworld. The cases show that Refinement Tuning can enhance the diversity and quality of the model's thinking, which helps improve the model's exploration breadth and efficiency and avoid always getting stuck in loops in a new environment.\nIn Jericho, Agent-FLAN mistakenly believes it is not in the cell and attempts to go to cell. After failing, it chooses to check valid actions. Although check valid actions is a correct choice,"}, {"title": "GPT-4 JUDGEMENT'S RELIABILITY", "content": "Figure 10 shows the comparison of GPT-4 and human judgement on whether a turn needs to be refined. We randomly sampled 50 trajectories from the generated trajectory. In each trajectory, we randomly sampled 1 right turn and 1 wrong turn. We asked the human annotator to label the correctness of the turn. The human annotator receives the historical thought, action, and observation before the right/wrong turn as well as the right/wrong turn's thought, and action in ReAct format. It also receives the script corresponding to the trajectories. The results show that in the turns that GPT-4 labeled right, 94% are aligned with human judgment, and in the turns that GPT-4 labeled wrong, 82% are aligned with human judgment. This indicates that GPT-4's judgement is reasonable."}, {"title": "GENERALIZATON BETWEEN GENERAL DATA AND AGENT DATA", "content": null}, {"title": "RELATED WORK", "content": "Agent Finetuning To enhance the decision-making capabilities of open-source models, a series of works currently focus on training Agent trajectories. A small number of models choose the decompose-then-execution paradigm (Yin et al., 2024), while the majority opt for using ReAct (Yao et al., 2022). Most works sample from the dataset and train the model using methods such as SFT or DPO (Rafailov et al., 2024) to improve their ability to handle Held-in problems(Zeng et al., 2023; Hu et al., 2024; Xi et al., 2024; Chen et al., 2024). AgentTuning, Agent-FLAN, and AgentGen attempt to train generalizable agent models. AgentTuning and Agent-FLAN have found that using general data like ShareGPT can improve generalization. AgentGym aims to enhance generalization by enabling the model to continuously learn new tasks and treating all tasks as Held-in. AgentGen is the first to attempt direct environment synthesis, improving generalization by enhancing the diversity of training data. In this work, we demonstrate that the above approaches still have limitations in terms of generalization, specifically in terms of easily overfitting on single data sets, getting stuck in reasoning, and learning incorrect reasoning patterns. To address this issue, we increased the diversity of training agent data through synthetic data, significantly alleviating the model's overfitting problem. Additionally, we add refinement steps in the trajectory. We show that whether the training data includes the refinement process affects the model's reasoning pattern, and adding synthetic refinement processes greatly enhances the generalization performance of LLMs.\nData Synthesis Due to the impending depletion of web data, the use of synthetic data has become a research hotspot. The synthesis can be divided into query synthesis and response synthesis. Most agent-tuning approaches synthesize the response in different ways like the plan (Yin et al., 2024), ReAct format (Zeng et al., 2023), JSON format (Zhang et al., 2024), chat format (Chen et al., 2024), pair format (Xiong et al., 2024), or evaluation of the state knowledge (Qiao et al., 2024), etc. The other way is to synthesize queries, like evolving a given query (Xu et al., 2023) or using pre-train data as a seed to generate new data (Chan et al., 2024). Among agent research, only AgentGen explores query synthesis. AgentRefine tries to synthesize queries and responses at the same time and uses a verifier to supervise the quality of the responses.\nSelf-Refine Self-refine refers to the process where a model iteratively generates better results through feedback. SELF-REFINE (Madaan et al., 2024; Huang et al., 2023) finds GPT-4 can find and correct mistakes itself in a compulsory pipeline - generate answer, asking a refinement advise and use the question and the advise to generate answer again. AgentRefine trains models to develop step-level refinement abilities. This means the model can spontaneously adjust its decision processes based on feedback from the environment, rather than relying on compulsory guidance from a pipeline at instance-level. AgentRefine is also the first approach to identify the connection between step-level refinement and agent generalization."}, {"title": "CONCLUSION", "content": "In this work, we study the generalized agent abilities for open-source LLMs via agent tuning. Current work performs well on held-in evaluation sets but fails to generalize to held-out sets because of overfitting to several manual agent environments. We present the AgentRefine approach to enable the model to correct its mistakes based on the environment feedback. Experiments demonstrate that AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent benchmarks. Our analysis shows that self-refinement enables the robustness of agent capability and the diversity of agent environments and thoughts further enhances the performance. We hope to provide new insight for future agent research."}, {"title": "ETHICS STATEMENT", "content": "When using a large amount of open-source resources for data synthesis, an important issue is the generation of harmful and malicious data. In our work, we use Persona-Hub, a synthesized dataset that has undergone security processing. We use it to synthesize tasks and environmental information, which pass our secondary review and are safe to use. However, our method may have potential risks of misuse, such as enhancing LLM's capabilities in malicious agent tasks, like generating attack codes. Therefore, adhering to ethical guidelines is crucial to ensuring the responsible use of this technology."}, {"title": "TASKS STATISTIC", "content": "Table 6 presents the number of test data and domains in the 5 tasks. These number calculates the Held-out Task score. Specifically, Held \u2013 outTaskscore = (BabyAIscore * 112 + SciWorldscore * 90 + PDDLscore * 60 + Jerichoscore * 20)/282"}, {"title": "THE HISTORY OF AGENT-TUNING", "content": "In recent years, LLM-Based Agents have become a popular paradigm. However, improving LLM performance on agent tasks during the post-training phase remains a challenging issue. Previous work typically sampled and trained in fixed environments (with Held-in data that is distributionally similar to the test data)(Xi et al., 2024), which significantly improved performance on specific tasks (test sets that are distributionally similar to the training data). However, performance drops sharply once the task changes.\nAgentTuning (Zeng et al., 2023) was the first to recognize this issue by adding a portion of general alignment data to the single-agent data, alleviating the problem and demonstrating initial generalization capabilities. Agent-FLAN (Chen et al., 2024) further improved the single-agent data, enhancing the model's generalization in agent tasks.\nIn our work, we demonstrate that the above approaches still have significant limitations in terms of generalization, specifically in terms of easily overfitting on single data sets, getting stuck in reasoning, and learning incorrect reasoning patterns (as discussed in Figure 2, Figure 9, and Section 4.3, etc.). To address this issue, we increased the diversity of training agent data through synthetic data, significantly alleviating the model's overfitting problem. Additionally, we add refinement steps in the trajectory. We show that whether the training data includes the refinement process affects the model's reasoning pattern, and adding synthetic refinement processes greatly enhances the generalization performance of LLMs."}, {"title": "SYNTHESIS DATA WITH PERSONA", "content": "Persona represents diverse and rich information content. Persona hub (Chan et al., 2024) contains 1,000,000,000 personas after filtering via diverse. If the filter cosine similarity is 0.5, it can still generate 1 million diverse personas. The persona hub also demonstrated that the data generated via the persona hub has similar diversity to the persona data and its scaling experience shows that data generated via the persona hub is not yet saturated at the size of 1M under math problem."}, {"title": "TRAINING HYPER PARAMETER", "content": "For all models, the learning rate is 5e-6 with a cosine learning rate scheduler and no warm-up steps. The batch size is 64. The max length is 8192 for 7/8b models and 4096 for 70b models due to limited storage for DeepSpeed (Rasley et al., 2020) usage. Aligned with Agent-FLAN, we choose AgentRefine with 32000 data for the default training setting. Aligned with AgentGen (Hu et al., 2024), we train our model for 10 epochs and select the checkpoint with the best average results to report. We also modified the LLaMA-Factory's SFT loss to Equation 1. Other settings are aligned with LLaMA-Factory's default settings."}, {"title": "COMPARISON AMONG AGENT DATASETS", "content": "Table 7 compares the number of trajectories, the methods to obtain environments and trajectories, the held-in tasks in the AgentBoard benchmark, and the availability of refinement steps among Agent-FLAN, AgentGym, AgentGen, and AgentRefine. AgentRefine can easily scale its data and includes refinement steps in the training set. AgentGen and our work are contemporary. Our commonality lies in synthesizing diverse environments, but we place more emphasis on enhancing refinement abilities."}, {"title": "IND FILTERING EXPERIMENTS", "content": "To remove the interference from IND data, we perform an experiment where we train model using data that excludes all IND training data. Agent-FLAN removes 672 samples out of 34440 samples, and AgentGym removes 5350 samples out of 14485 samples. The result in Table 8 shows that AgentRefine outperforms the other two methods in all tasks. This demonstrates that our method significantly improves over previous methods."}, {"title": "REFLEXION EXPERIMENT", "content": "Table 9 presents the results with Reflexion (Shinn et al., 2024). It shows that AgentRefine outperforms other methods when adding Reflexion, especially in Alfworld, since AgentRefine isn't trained on any Alfworld data, yet it outperforms AgentGym, and Agent-FLAN, whose models are trained on Alfworld data. This indicates that AgentRefine can utilize Reflexion more effectively than other methods."}, {"title": "STANDARD DEVIATIONS", "content": "Table 10 shows the average and standard deviation for each task. We use the results from Table 4 (decoding temperature = 1.0 with 10 sample times). AgentRefine's average performance exceeds that of other methods by at least 2 standard deviations in most OOD tasks. This demonstrates that our method represents a significant improvement over previous methods."}, {"title": "ROBUSTNESS ANALYSIS WITH DIFFERENT COMPONENTS", "content": "Table 11 presents the contribution to robustness among different components. When training on 4000 data, the standard deviation of the success score is almost double that of the baseline which means the number of the training data is the most important factor for the model's robustness."}, {"title": "MODEL'S INSTRUCTION-FOLLOWING ABILITY", "content": "We use MT-bench (Zheng et al., 2023) to test models' instruction-following ability and use gpt-40-2024-05-13 to judge the score.\nThe score of AgentRefine is approximately 0.2 points higher than that of Agent-FLAN regardless of whether ShareGPT is incorporated. After incorporating ShareGPT, both show an improvement of about 2 points."}, {"title": "PERTURBATION DETAILS", "content": "We have made 5 perturbation in Alfworld:\nPerturbation 1: change clean {obj} with {recep}, cool {obj} with {recep}, heat {obj}with {recep} to clean {obj} using {recep}, cool {obj} using {recep}, heat {obj} using {recep} in the instruction\nPerturbation 2: change go to {recep} to move to {recep} in the instruction\nPerturbation 3: change take {obj} from {recep} to from {recep} take {obj} in the instruction\nPerturbation 4: delete all space between item name and item number in the instruction.\nPerturbation 5: remove all IND data in the training set and retrain the model.\nWe also revise the environment to adjust to these changes."}, {"title": "SCRIPT GENERATION", "content": null}, {"title": "TRAJECTORY GENERATION", "content": null}]}