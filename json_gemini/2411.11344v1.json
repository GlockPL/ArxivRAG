{"title": "Mitigating Knowledge Conflicts in Language Model-Driven Question Answering", "authors": ["Han Cao", "Zhaoyang Zhang", "Xiangtian Li", "Chufan Wu", "Hansong Zhang", "Wenqing Zhang"], "abstract": "Knowledge-aware sequence to sequence generation tasks such as document question answering and abstract summarization typically requires two types of knowledge: encoded parametric knowledge and retrieved contextual information. Previous work show improper correlation between parametric knowledge and answers in the training set could cause the model ignore input information at test time, resulting in un-desirable model behaviour such as over-stability and hallucination. In this work, we argue that hallucination could be mitigated via explicit correlation between input source and generated content. We focus on a typical example of hallucination, entity-based knowledge conflicts in question answering, where correlation of entities and their description at training time hinders model behaviour during inference.", "sections": [{"title": "I. INTRODUCTION", "content": "Large pre-trained language models are a common building blocks for a wide array of natural language processing (NLP) tasks. These models are known to encode factual knowledge during training [1]. Such property have been exploited in tasks such as closed-book question answering [2] and com-monsense reasoning [3]. However, recent study shows that improperly activated parametric knowledge during inference could cause the model to produce factually inconsistent out-puts. Such inconsistency is undesirable in natural language generation (NLG) tasks that requires grounding with respect to certain contextual information, affecting performance in various tasks such as table-to-text generation [4], abstract summarization [5], and grounded dialogue systems [6].\nLarge language models (LLMs) have been extremely popu-lar for questions answering nowadays, with tons of engineering work done to improve its efficiency, like quantization [7] and [8]. These work also deeply influenced other generative AI areas like stylistic image generation [9]. However, accuracy and consistency still remain a huge challenge for everyone.\nA particular challenge in ensuring factual consistency in grounded NLG tasks is that the usually unsatisfied suffi-ciency between the grounding context and gold output. In the ideal world, the input context should always contain enough information for a model to produce the gold output. However, such an assumption is impractical in many tasks. For example, human written gold answers in summarization often contains extrinsic hallucination [5], where extra background knowledge is required for the model to produce the correct output. Similarly, the one-to-many problem in dialogue system were also caused by the insufficiency to accurately predict response given contextual information. Thus, it is crucial that we develop training scheme to constrain model behaviour under such sufficiency constraint.\nWe study a simple variant of hallucination, entity-based knowledge conflicts. Concretely, recent study find reader model exhibits over-reliance on parametric knowledge, pri-marily due to information insufficiency caused by an in-perfect retriever during training [10]. In other words, the reader model learns to ignore retrieved documents and rely on parametric knowledge during training, resulting in hallucination at test time. While such behaviour can be beneficial, un-alerted hallucination significantly affects model usability and trust-worthiness [11]. We propose two set of orthogonal methods to mitigate undesirable model memorization: gradient based decoding and adapter-based fine tuning. We hope to show that with minimum or no performance sacrifice, our model generate output that is more faithful with respect to input context."}, {"title": "II. BACKGROUND AND RELATED WORKS", "content": "As mentioned in the introduction though, some early studies [12] focused on the potential pitfalls of leveraging standard likelihood maximization-based objectives in training and de-coding Natural Language Generation models. They found that an approach that maximizes this potential could lead to deterioration. At the same time, it turns out that these models often produce meaningless text or are not faithful to the source input provided [13]. Researchers have come to call this unwelcome generation a hallucination. [14] Hallucination can lead to potential privacy violations. Carlini et al. [15]\ndemonstrated that language models can be prompted to recover and generate sensitive personal information from the training corpus. Such memorization and recovery of the training corpus is considered a form of hallucination because the model is generating text that is not exist in the source context [16].\nMany LLMs are evolving from text-only to multimodal content understanding and question answering, which proposes a bigger challenge for multimodal understanding [17]. Re-searchers are also utilizing Transformer models for visual tasks like navigating [18] and medical tasks [19]. These applications introduce higher requirements for knowledge conflict resolu-tion.\nConsidering QA models searches external knowledge for information relevant to the question, and generates the answer based on the retrieved information [20], an essential goal is to provide documents-based answers given the question, so that hallucination in the answer will mislead the user and harm performance dramatically [21]."}, {"title": "A. Hallucination in Text Generation", "content": null}, {"title": "B. Controllable Generation", "content": "Controllable text generation [22], [23] may be one tool to resolve the hallucination problem. Unlike plain text generation, we want the sentences generated by the models (x) to align with specific attributes (a), such as sentiment and content. Works in this area typically deal around the conditional probability p(xa). While some authors [24] decide to directly model it, some [11], [25] avoid direct modeling through bayes rules or latent variables. For example, Hu et al. [26] uses a latent variable (z) to transform the problem into modeling p(x|z,a), p(a|x) and p(z|x) separately. This tool may allow us to treat the easily hallucinated entities as attributes and consequently train the model not to completely ignore the provided context [27]."}, {"title": "C. Question Answering", "content": "Question answering (QA) is a major research direction of NLP. Two of the largest subbranches under QA are visual question answering [28], which uses images as sources of knowledge and context, and text question answering [20], [29], which uses text as the sources. One of major subbranches under text question answering is called open-domain question answering [30]. In this task, given any question, the model is expected to find the answer to it from a giant database of passages, e.g. Wikipedia articles. The state-of-the-art is the retriever-reader approach, as mentioned in Chen et al [31]. With this approach, first, a retriever, such as a dense retriever [32] or simply a BM25 [33], will first retrieve a small set of relevant passage from the given database of passages. A document reader, typically a standard language model like RNN [34] or BERT [35], will then read through these retrieved passages and produce the answer to the question."}, {"title": "III. METHODS", "content": "This section is organized as follows: we first cover definition of knowledge conflicts and a simple data augmentation base-line solution. Then we introduce our projected directions that could potentially lead to better solution to knowledge conflicts. Finally, we report our tentative schedule for the project."}, {"title": "A. Problem re-formalization and Model selection", "content": "Seq2seq-based big QA models like BART, when accompa-nied by QA datasets with contexts, become hard to train and require computing resource beyond our current constraints. Hence, to seamlessly handle the datasets we used in our exper-iments, which typically contain long contexts with more than 150 words, we used pretrained generative language models and finetuned them for QA tasks.\nFor other multimodal-based models like [36], the datasets also contain pictures or videos for models to recognize. These tasks are for further exploration.\nSimilar to what is done in most language models, question and answer pairs are connected by a special token for training, and model are only given questions as prefixs for inference. Consequently, this task can be viewed as a conditional text generation problem. In the following section, we will disucss how to address this with prompt-tuning."}, {"title": "B. Knowledge Conflicts and Baseline Method", "content": "In open-book question answering, the reader model is trained to produce an answer x given a (retrieved) context c and a query q. Ideally, the reader model should learn to always estimate the answer given both the context and the query, i.e. p\u03b8(x|q, c). However, previous study found that via factual knowledge encoded in model parameters, the reader model sometimes ignores the context, and directly estimate the answer, i.e. p\u03b8(x|q). We believe a straightforward solution to this problem is simply by training on a set of augmented context-query-answer triplets of the form {x', q, c'}, where x' is only answerable using information from c'. However, this method requires heuristic based design of augmented data, and requires fine tuning the entire model parameters, thus is computationally and labor expensive [37]."}, {"title": "C. Knowledge Erasing Module", "content": "We already know knowledge can be stored in language models and this attribute has been utilized in various areas including chat robots and network security [38]. However, erasing or modification of the stored knowledge remains a challenge.\nPrevious work demonstrate that addtional trainable param-eters could be used to infuse factual knowledge into pre-trained language models [39]. In this work, we investigate whether the same formualtion could be used to steer the model towards forgetting memorized undesirable correlation. Namely, we investigate two types of extra parameters: bottle-neck adapter [40] and prefix tuning [41]. The former closely resembles the knowlege infusing module used by [39], and the latter is closed connected to universal trigger attack [42], which is know to be able to steer model behaviour. Besides, inspired the adversarial loss used by [43], we trained our adapters in adversarial way for robust performance.\nThe benefit of using extra parameter are three folds: firstly, the small amount of additional parameters do not require the"}, {"title": "IV. DATASETS", "content": null}, {"title": "A. KMIR Dataset", "content": "a) Introduction to KMIR: For preliminary experiments, we used the recently released KMIR dataset from the study [46], which is a new benchmark for evaluating knowledge memorization, identification and reasoning for language mod-els. It includes questions and answers for different types of knowledge, such as commonsense, general knowledge and do-main specific knowledge. This dataset is more light-weighted.\nb) Preprocessing: Here we only use the dataset for knowledge memorization evaluation. Thus we only selected the data with Triple Completion relation type. Then for each query statement, we generated a question accordingly via a huggingface api function. In this way we got more than 140,000 sample for training."}, {"title": "B. Natural Question(NQ) Dataset", "content": "As mentioned in [47], this study proposed a extensible framework to generated entity-based knowledge conflicts QA datasets. They focused on 5 types of entities including person (PER), date (DAT), numeric (NUM), organization (ORG), and location (LOC). They also introduced four types of substitutions to comprehensively understand model behavior: corpus substitution, alias substitution, type swap substitution and popularity substitution.\nWe can follow this former work and use [48]'s Natural Questions (NQ) ([49]) for our experiments. We only focused on the corpus substitution setting because it replaced the answer with another answer of the same type, which aligned with the setting before. This type of substitution also made it easier to expect reasonable answers from the model.\nWhile the framework also allows custom substitutions. Inspired by the framework of [50], for future work, we can further create more types of entity substitution policies to enhance our model's ability to reduce hallucination. Besides, more knowledge bases (KBs) like YAGO [51] can be inte-grated into this framework to increase the diversity of potential entity candidates for substitution."}, {"title": "C. Victim Model", "content": "We used pretrained GPT-2 for the task. The questions and answers are combined with special tokens to show the model the pattern. Then we finetuned GPT-2 on the training dataset for 2-3 epochs. To quantify the memorization of our model, we then used the finetuned model to generate answers given the questions. If the model generated the exact same answer by comparing the texts, we considered the model memorized the question and answer pair.\nWe also considered the situation that which the model mentioned the correct entity in the answer but described it in a different way. This situation did happen. But this may also introduce some perturbed answers with hallucination. Therefore, we strictly counted the exactly matched answers only.\na) For KMIR Datset: We randomly sampled 4,000 sam-ples from the training set to evaluate the memorization rate. The results showed that the exactly matched rate is 12%-13%. While the same entity rate is about 25% to 30%. We finally got 593 memorized samples.\nThen we categorized the data according to the pred type and gathered the answers within each category. We generated a wrong entity for each question by randomly selecting from according category. In this way, we made sure that the entity type is the same as original answer, so that the model can be more easily stirred to generate the wrong answer different from its memory.\nb) For NQ Dataset: Similarly, we chose 10,000 samples from the training set and the memorization rate of model is about 5-6%. We finally got 637 memorized samples."}, {"title": "V. EXPERIMENT", "content": null}, {"title": "A. Qualitative results for Prompt tuning", "content": "Following the prompt tuning method, we clarified at III-C, the result is as follow:"}, {"title": "B. Quantitative Evaluation for Prompt tuning", "content": "The evaluation metrics is now different from [47]. For our model, we focus on the questions that the model correctly answers on the original unmodified samples. Through the exact match measurement, we can compare the predictions on the original example x and the example x' after substitution. Then for the predictions on x', we calculate the fraction of the substituted answer ps as the accuracy.\nThe accuracy then measures how often the model predicts the substituted answer in the context. This metrics shows the extend to which model pay attention to the changing information in the context."}, {"title": "VI. CONCLUSION", "content": "Undesired storage of parameterized knowledge in pre-trained models lead to factual hallucination at inference time. In this work, we investigate two classes of prompt tuning methods: Bottleneck adapter and Prefix tuning adapter. We show these two methods could effectively override memorized parameterized knowledge."}]}