{"title": "Differentiable SVD based on Moore-Penrose Pseudoinverse for Inverse Imaging Problems", "authors": ["Yinghao Zhang", "Yue Hua"], "abstract": "Low-rank regularization-based deep unrolling networks have achieved remarkable success in various inverse imaging problems (IIPs). However, the singular value decomposition (SVD) is non-differentiable when duplicated singular values occur, leading to severe numerical instability during training. In this paper, we propose a differentiable SVD based on the Moore-Penrose pseudoinverse to address this issue. To the best of our knowledge, this is the first work to provide a comprehensive analysis of the differentiability of the trivial SVD. Specifically, we show that the non-differentiability of SVD is essentially due to an underdetermined system of linear equations arising in the derivation process. We utilize the Moore-Penrose pseudoinverse to solve the system, thereby proposing a differentiable SVD. A numerical stability analysis in the context of IIPs is provided. Experimental results in color image compressed sensing and dynamic MRI reconstruction show that our proposed differentiable SVD can effectively address the numerical instability issue while ensuring computational precision.", "sections": [{"title": "1. Introduction", "content": "Low-rank regularization has found extensive application in various fields [13]. Examples include, but are not limited to, recommendation systems [38], visual tracking [33], 3D reconstruction [2], and salient object detection [28]. In the context of inverse imaging problems (IIPs), low-rank regularization plays a pivotal role and has demonstrated remarkable effectiveness in applications such as matrix/tensor completion [7], medical image reconstruction [39] and restoration of remote sensing images [40].\nGenerally, the low-rank regularization method for IIPs consists of two essential components or, in other words, two stages of theoretical construction: 1) the development of a reconstruction model incorporating low-rank regularization, such as the rank minimization model [8]/Robust Principal Component Analysis (RPCA) model [7]. Due to the non-convex nature of rank, the nuclear norm of matrices/tensors [24] is commonly employed as a convex envelope to constrain the rank. 2) the design of iterative optimization algorithms to solve the model, such as the use of Alternating Direction Method of Multipliers (ADMM) [5], Proximal Gradient Descent (PGD) [4], and similar algorithms.\nSpecifically, the degradation model of IIPs can be commonly described as follows:\n$A(X) + n = b,$\nwhere $A(\\cdot)$ is the degradation operator, $X$ is the to-be-reconstructed variable, $b$ is the observed data, and $n$ is the noise. The goal of IIPs is to recover the unknown variable $X$ from the observed data $b$. To succinctly illustrate, we take the example of a matrix inverse problem, where $X$ is a two-dimensional matrix. With the introduction of low-rank regularization, the reconstruction problem can be described as follows:\n$\\min \\frac{1}{2} ||A(X) - b||_2^2 + \\lambda ||X||_*,$"}, {"title": null, "content": "where $||\\cdot||_*$ denotes the nuclear norm of the matrix, and $\\lambda$ is the regularization parameter. Adopting the PGD algorithm [4, 6] to solve the above problem, the iterative update formula is as follows:\n$\\begin{cases} Z = X^n - \\rho A^H (A(X^n) - b) \\\\ X^{n+1} = SVT(Z, \\lambda \\rho). \\end{cases}$"}, {"title": null, "content": "where the subscript indicates the iteration step, $Z$ is the intermediate variable, $\\rho$ is the stepsize of the gradient descent step, $A^H$ is the adjoint of $A$, $SVT$ denotes singular value thresholding algorithm [6], and $SVT(Z, \\lambda \\rho) = U \\max(S - \\lambda \\rho, 0) V^H$ with the singular value decomposition (SVD) $Z = USV^H$.\nIn the process of solving nuclear norm regularization problems, performing SVD on the matrix and applying a soft-thresholding operation to the decomposed singular values is an efficient and concise approach [6]. In iterative optimization algorithms, the computation of SVD is only concerned with its forward process, and it does not involve the backward process requiring SVD derivatives. Consequently, the instability in the derivatives of SVD [37] does not adversely affect the normal operation of the algorithm.\nRecently, Deep Unrolling Networks (DUNs) [26, 41] have introduced a third stage on top of the traditional two-stage construction of iterative optimization algorithms. Specifically, DUNs unfold the iterative algorithm, such as (3), into a deep neural network. In this process, a fixed number of (typically 10) iterations of the optimization algorithm are implemented within a deep learning framework. This enables the construction of an end-to-end neural network and facilitates adaptive learning of hyperparameters within the iterative algorithm using supervised learning. Furthermore, DUNs can replace certain transformations within the iterative algorithm with neural networks, enhancing the ability to learn from data. Overall, DUNs leverage the physics-driven capabilities of traditional algorithms while harnessing the powerful learning capabilities of deep learning. As a result, they have demonstrated remarkable performance in various IIPs [21, 23].\nGiven the widespread application of traditional low-rank regularization models, the natural extension of DUNs to iterative algorithms, and the significant success achieved by DUNs, it is inevitable for low-rank regularization-based DUNs to become a research hotspot [32, 43, 44]. However, in DUNs, computing SVD introduces challenges regarding the backpropagation of SVD. The derivative of SVD is undefined when there are repeated singular values [11, 37], and it can even result in excessively large derivatives, leading to overflow for a certain training data type (e.g., when using Float32 in PyTorch, the maximum value without overflow is 3.41 \u00d7 1038) when two singular values are very close. This numerical instability makes the training process of DUNs prone to collapse. In some cases, the training process of DUNs can enter a cycle of numerical instability, rendering the training process unable to proceed normally. Therefore, addressing the backpropagation issue of SVD is a crucial step for enabling low-rank regularization-based DUNs.\nWhile the paper [37] attempted to address the non-differentiability issue of SVD, it mainly focused on the SVD of positive semi-definite matrices or, in other words, the eigenvalue decomposition of matrices. However, in IIPs, the commonly used SVD is often applied to trivial matrices for SVT. To the best of our knowledge, the differentiability analysis of the trivial SVD remains unexplored. In this paper, we start from the derivation of the derivative of SVD and identify that the non-differentiability issue of SVD is essentially attributed to an underdetermined system of linear equations arising in the derivation process, particularly in the presence of repeated singular values. To overcome this, we propose a differentiable SVD by utilizing the Moore-Penrose pseudoinverse to obtain the minimum norm least-squares solution of the system. Additionally, we provide a Numerical stability analysis for the derivative of this differentiable SVD in the context of IIPs.\nTo demonstrate the applicability of our approach, we evaluate our proposed differentiable SVD in two IIPs, i.e., color image compressive sensing and dynamic magnetic resonance imaging (MRI) reconstruction. A state-of-the-art low-rank regularization-based DUN [43, 44] is employed as the baseline model. The experimental results show that our proposed differentiable SVD can effectively address the numerical instability issue of SVD in DUNs, thereby improving the performance."}, {"title": "2. Related Works", "content": null}, {"title": "2.1. DUNs with Low-Rank Regularization for IIPs", "content": "DUNs with low-rank regularization have obtained substantial success in a wide of applications [21, 23, 26, 41]. Similar to traditional iterative optimization methods, they can generally be classified into two categories. The first, here termed LR-DUNs, involves directly applying low-rank regularization to the target of interest, denoted as $X$, for reconstruction. The second category, termed RPCA-DUNS, decomposes $X$ into a low-rank component $L$ and a sparse component $S$. In this case, low-rank regularization is applied to the separated component $L$.\nLR-DUNs have demonstrated remarkable effectiveness in various fields such as matrix completion [31], high dynamic range imaging [25], hyperspectral image denoising [45], spectral CT image reconstruction [10], MRI image reconstruction [18, 43], and more. These methods mostly employ the matrix nuclear norm regularization for low-rank constraints. However, when dealing with tensor data, unfolding it into a matrix often disrupts its high-dimensional low-rank structure. To address this issue, Zhang et al. [42, 43] proposed a framework for LR-DUNs based on tensor singular value decomposition. This approach utilizes the low-rank characteristics of tensors in the CNN-transformed domain and achieves state-of-the-art performance in high-dimensional MRI reconstruction tasks. A similar idea has also been evaluated in the context of video synthesis applications [44]. Additionally, RPCA-DUNs have also achieved impressive results in various applications, including but not limited to foreground-background separation [35], SAR imaging [3], radar interference mitigation [30], and cloud removal [16]. Specifically, methods like CORONA [32] and L+S-Net [15] have achieved significant improvement in tasks such as ultrasound clutter suppression and dynamic MRI reconstruction.\nThe methods mentioned above primarily employ SVT to leverage the low-rank priors. This highlights the simplicity, efficiency, and ease of use characteristics of SVT. However, SVT requires performing SVD on the input data, and the derivative of SVD becomes numerically unstable when encountering repeated singular values [37]. This instability can pose challenges during the training of DUNs, especially considering that the training process involves frequent updates to the internal data of the DUNs through backpropagation. Given the large number of training iterations, often in the order of tens of thousands or even hundreds of thousands, the probability of encountering repeated or very close singular values is quite high. Also, there are alternative methods for effective low-rank utilization, such as partial separation [11, 22], which decomposes $X \\in \\mathbb{C}^{m \\times n}$ into $UV$, where $U \\in \\mathbb{C}^{m \\times r}$ and $V \\in \\mathbb{C}^{r \\times n}$. However, this approach necessitates manually selecting the rank $r$ beforehand, which to some extent restricts its performance."}, {"title": "2.2. SVD Gradient", "content": "Townsend [34] conducted a detailed analysis of the gradient of SVD. Ionescu et al. [17] provided the derivative for the SVD of symmetric matrices, which have a simplified form compared to the derivative of the trivial SVD. However, these methods did not consider the issue of numerical instability associated with SVD.\nIn addressing the differentiability issue of SVD for positive semi-definite matrices or, equivalently, the differentiability issue of eigenvalue decomposition for matrices, Wang et al. [36] observed that employing the power iteration (PI) method [27] could gradually yield the eigenvalues and eigenvectors of a matrix through a deflation procedure. Therefore, by substituting the PI method for the exact solution of the SVD for symmetric matrices, they could leverage the gradients of PI in the backward pass to circumvent the numerical instability associated with SVD. However, the imprecise solution in the forward pass could impact the performance of the model. Consequently, they proposed a strategy where accurate SVD is used in the forward pass, while the gradients of PI are utilized in the backward pass. Furthermore, Wang et al. [37] discovered the derivative of the SVD of positive semi-definite matrices depends on the matrix $K$ [17] with elements:\n$K_{ij} = \\begin{cases} \\frac{1}{\\sigma_i - \\sigma_j}, & i \\neq j \\\\ 0, & i = j \\end{cases}$"}, {"title": null, "content": "where $\\sigma$ denotes the singular values of the matrix. It is evident that when two singular values are equal, the corresponding elements of $K$ will become infinite, leading to numerical instability. Wang et al. [37] proposed a solution based on Taylor expansion to address this issue. When $\\sigma_i > \\sigma_j$, the $K$th degree Taylor expansion of the element $K_{ij}$ can be expressed as:\n$K_{ij} = \\frac{1}{\\sigma_i - \\sigma_j} = \\frac{1}{\\sigma_i (1 - \\frac{\\sigma_j}{\\sigma_i})} = \\frac{1}{\\sigma_i} (1 + (\\frac{\\sigma_j}{\\sigma_i}) + (\\frac{\\sigma_j}{\\sigma_i})^2 + ... + (\\frac{\\sigma_j}{\\sigma_i})^K ),$"}, {"title": null, "content": "where the second equality holds due to the $K$th degree Taylor expansion of function $f(x) = \\frac{1}{1-x}, x \\in [0,1)$ at the point $x = 0$. At the element where $\\sigma_i < \\sigma_j$, we have,\n$K_{ij} = \\frac{1}{\\sigma_i - \\sigma_j} = - \\frac{1}{\\sigma_j (1 - \\frac{\\sigma_i}{\\sigma_j})},$"}, {"title": null, "content": "and the same $K$th degree expansion could be used.\nHowever, this approximation method encounters three issues: 1) The Taylor series of the function $f(x) = \\frac{1}{1-x}$ has a convergence radius of $|x| < 1$. The starting point for applying Taylor expansion is to address the scenario when two singular values are equal, i.e., $\\frac{\\sigma_i}{\\sigma_j} = 1$. In such cases, it cannot be guaranteed that $(1 + (x)^2 + (x)^3 + ... + (x)^N), N \\rightarrow \\infty$ converges to $\\frac{1}{1-x}$, thereby rendering the $k$-th order approximation, $(1 + (\\frac{\\sigma_j}{\\sigma_i})^2 + (\\frac{\\sigma_j}{\\sigma_i})^3 + ... + (\\frac{\\sigma_j}{\\sigma_i})^K )$, ineffective in approximating $\\frac{1}{1 - \\frac{\\sigma_j}{\\sigma_i}}$. 2) During the training process, the repetition of singular values is a probabilistic event and does not occur in every training step. However, this Taylor approximation method alters the gradients of SVD in any scenario, potentially leading to inaccurate backpropagation. 3) Indeed, the $K$ matrix is not the intrinsic and fundamental reason for the non-differentiability of SVD. We will delve into a detailed analysis in the next section.\nTo the best of our knowledge, there is currently no comprehensive analysis of the differentiability of the trivial SVD. The work based on Taylor expansion [37] can be extended to the trivial SVD. Specifically, the derivatives of the trivial SVD rely on the matrix $F$ [34] with elements:\n$F_{ij} = \\begin{cases} \\frac{1}{\\sigma_i^2 - \\sigma_j^2}, & i \\neq j \\\\ 0, & i = j \\end{cases}$"}, {"title": null, "content": "Therefore, we can approximate the elements of the $F$ matrix through a similar $K$-th order Taylor expansion. In this paper, we refer to this approach as SVD-taylor. SOUL-Net [10] employs this SVD-taylor strategy to address the numerical instability associated with the SVT in DUN. This paper is the only one we have found that effectively considers the non-differentiability of SVD in the context of DUN. Moreover, the inspiration for gradient clipping strategies can also be applied to the matrix $F$. For instance, when two singular values are equal or very close, elements of $F$ can be clipped to a relatively large value, such as 1e16. In this paper, we name this approach SVD-clip. Additionally, through an analysis of the code of TensorFlow [1], we found that when $F$ contains infinite values, TensorFlow always sets the corresponding element to zero. We term this approach SVD-tf. All of the above methods involve adjustments to the $F$ matrix. However, we observed that the numerical instability of $F$ is merely a manifestation of the non-differentiability of SVD. The true underlying cause is an underdetermined system of linear equations in the derivation process when facing repeated singular values. Therefore, we employ the Moore-Penrose pseudoinverse to obtain the minimum norm least squares solution to this system, proposing a differentiable SVD, termed SVD-inv. Moreover, some works [14, 19] suggest that smaller-sized matrices have a lower probability of having repeated singular values, making the computation of their SVD gradients more stable. Consequently, they propose dividing the original large-sized matrix into smaller ones for computation. However, this approach involves manual interventions in the original problem and may not accurately measure the global singular vectors [37], leading to suboptimal results."}, {"title": "3. Differentiable SVD based on Moore-Penrose Pseudoinverse", "content": "In this section, we start from the derivation of the SVD gradient, revealing the deep-seated reason for the non-differentiability of SVD when repeated singular values occur. Specifically, we highlight that a system of linear equations in the derivation process becomes underdetermined in the case of repeated singular values. Subsequently, we utilize the Moore-Penrose pseudoinverse to provide the least squares solution to this system, thereby rendering SVD differentiable."}, {"title": "3.1. The gradient of the proposed SVD-inv", "content": "Assuming there is an SVD operation for the input $A$ in a deep neural network, i.e., $A = USV^H$, and the network has a loss function $L(U, S, V)$ that depends on the SVD decomposition $U, S, V$. From the backpropagation, We can obtain the partial derivatives of $L$ with respect to $U, S, V$, abbreviated as $\\overline{U}, \\overline{S}, \\overline{V}$. The total derivative of $L$ can be written as,\n$d\\mathcal{L} = \\text{tr}(\\overline{U}^H dU) + \\text{tr}(\\overline{S}^H dS) + \\text{tr}(\\overline{V}^H dV) = \\text{tr}(\\frac{\\partial L}{\\partial A}^H dA)$"}, {"title": null, "content": "where $\\text{tr}(\\cdot)$ denotes the trace of the input matrix, and the second equality holds due to that the loss function $L$ can also be written as $L(A)$. So, to correctly derive the derivative $\\frac{\\partial L}{\\partial A}$, it is necessary to express $dU, dS, dV$ in terms of $dA$, meaning finding their relationships with $dA$. Furthermore, by dividing both sides of the equation by $dA$, the remaining terms will constitute the derivative of $\\frac{\\partial L}{\\partial A}$.\nTheorem 3.1. Given the SVD $A = USV^H$ where $A \\in \\mathbb{C}^{m \\times n}$, $U \\in \\mathbb{C}^{m \\times k}$, $V \\in \\mathbb{C}^{n \\times k}$, $S \\in \\mathbb{R}^{k \\times k}$ and $\\text{rank}(A) < k < \\min(m, n)$, the following relationships hold,\n$dU = U (F \\odot [U^H dAV S + SV^H dA^H U] + T \\odot [U^H dAV])$\n$+ (I_m - UU^H) dAV S^{-1},$\n$dS = I_k \\odot [U^H dAV],$\n$dV = V (F \\odot [SU^H dAV + V^H dA^H US])$\n$+ (I_n - VV^H) dAV S^{-1},$"}, {"title": null, "content": "where $I_k$ represents the $k \\times k$ identity matrix, $\\odot$ denotes the Hadamard product, $F_{ij} = \\begin{cases} \\frac{1}{\\sigma_i^2 - \\sigma_j^2}, & \\sigma_i \\neq \\sigma_j \\\\ 0, & \\text{else} \\end{cases}$, $T_{ij} = \\begin{cases} 1, & \\sigma_i = \\sigma_j \\neq 0 \\&i \\neq j \\\\ 0, & \\text{else} \\end{cases}$, and $S^{-1} = \\begin{cases} \\frac{1}{\\sigma_i}, & \\sigma_i \\neq 0 \\\\ 0, & \\text{else} \\end{cases}$.\nProof. Taking the total differential of the SVD, we have,\n$dA = dU S V^H + U dS V^H + U S dV^H.$"}, {"title": null, "content": "This equation is the only breakthrough for us to find the relationship between $dU, dS, dV$, and $d A$. However, there are three unknowns here, and only one equation is insufficient to obtain a closed-form solution. We notice that both $U$ and $V$ satisfy the following relation,\n$U^H U = V^H V = I_k.$\nTaking the differential of the above equation gives,\n$dU^H U + U^H dU = dV^H V + V^H dV = 0.$"}, {"title": null, "content": "Thus, the matrices $d\\Omega_U = U^H dU$ and $d\\Omega_V = V^H dV$ are skew-symmetric. If we fix an $m \\times (m - k)$ matrix $U_1$ such that $[U, U_1]$ is an unitary matrix, then we may expand $dU$ as,\n$dU = U d\\Omega_U + U_1 dK_U,$"}, {"title": null, "content": "where $d K_U$ is an unconstrained $(m - k) \\times k$ matrix. Similarly, we could obtain,\n$dV = V d\\Omega_V + V_1 dK_V,$"}, {"title": null, "content": "where $d K_V$ is an unconstrained $(n-k)\\times k$ matrix. Substituting (15) and (16) into (14), we can easily verify the correctness of both equations. At this point, we have established a well-posed system of three linear equations, i.e., (12), (15), and (16).\nLeft-multiplying (12) by $U^H$ and right-multiplying by $V$ gives,\n$U^H dAV = d\\Omega_U S + dS + S d\\Omega_V^H.$"}, {"title": null, "content": "Since $d\\Omega_U$ and $d\\Omega_V$ are skew-symmetric, they have zero diagonal and thus the products $d\\Omega_U S$ and $S d\\Omega_V^H$ also have zero diagonal. Therefore, we could split (17) into two components: diagonal and off-diagonal.\nLetting $dP := U^H dAV$ and using $\\odot$ as the Hadamard product, the diagonal of (17) is,\n$dS = I_k \\odot dP = I_k \\odot U^H dAV.$"}, {"title": null, "content": "Thus, Equation (10) holds.\nThe off-diagonal is,\n$\\bar{I}_k \\odot dP = d\\Omega_U S - S d\\Omega_V^H,$"}, {"title": null, "content": "where $\\bar{I}_k$ denotes the $k \\times k$ matrix with zero diagonal and ones else.\nThe elemental form of (19) can be written as,\n$(dP)_{ij} = (d\\Omega_U)_{ij} \\sigma_j - \\sigma_i (d\\Omega_V)_{ij}, i > j,$"}, {"title": null, "content": "and, the corresponding $j$th element has the following relationship,\n$(dP)_{ji} = (d\\Omega_U)_{ji} \\sigma_i - \\sigma_j (d\\Omega_V)_{ji} = - (d\\Omega_U)_{ij} \\sigma_i - \\sigma_j (d\\Omega_V)_{ij},$"}, {"title": null, "content": "where the subscript $*$ denotes the conjugate operation. From (20) and (21), we can observe that, due to the skew symmetry of $d\\Omega_U$ and $d\\Omega_V$, it is only necessary to solve for the elements in half of the region of $d\\Omega_U$ and $d\\Omega_V$ (here we choose the lower triangular region). For the element at the $ij$-th position, only the $ij$-th and $ji$-th elements of $dP$ are relevant. Thus, (20) and (21) form a system of linear equations in two variables,\n$\\begin{bmatrix} \\sigma_j & -\\sigma_i \\\\ -\\sigma_i & \\sigma_j \\end{bmatrix} \\begin{bmatrix} (d\\Omega_U)_{ij} \\\\ (d\\Omega_V)_{ij} \\end{bmatrix} = \\begin{bmatrix} (dP)_{ij} \\\\ (dP)_{ji} \\end{bmatrix}$"}, {"title": null, "content": "1) When $\\sigma_i \\neq \\sigma_j$, the matrix $\\begin{bmatrix} \\sigma_j & -\\sigma_i \\\\ -\\sigma_i & \\sigma_j \\end{bmatrix}$ is of full rank, thus we could obtain the closed solution by directly left-multiplying its inverse, i.e,\n$\\begin{bmatrix} (d\\Omega_U)_{ij} \\\\ (d\\Omega_V)_{ij} \\end{bmatrix} = \\frac{1}{\\sigma_j^2 - \\sigma_i^2} \\begin{bmatrix} -\\sigma_j & \\sigma_i \\\\ \\sigma_i & -\\sigma_j \\end{bmatrix} \\begin{bmatrix} (dP)_{ij} \\\\ (dP)_{ji} \\end{bmatrix}$"}, {"title": null, "content": "2) When $\\sigma_i = \\sigma_j = 0$, we have $dP_{ij} = dP_{ji} = 0$. At these points, $d\\Omega_U$ and $d\\Omega_V$ are independent of $dP$, or equivalently, $dA$. Therefore, we can also straightforwardly set them to zero, resulting in a form similar to (23):\n$\\begin{bmatrix} (d\\Omega_U)_{ij} \\\\ (d\\Omega_V)_{ij} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$"}, {"title": null, "content": "3) When $\\sigma_i = \\sigma_j = \\sigma \\neq 0$, the coefficient matrix turns to $\\begin{bmatrix} \\sigma & -\\sigma \\\\ -\\sigma & \\sigma \\end{bmatrix}$, which is a singular matrix, hence non-invertible. Thus, we have revealed that the fundamental reason for the non-differentiability of SVD when repeated singular values occur lies in the fact that the system of linear equations (22) does not have an exact solution. In this paper, we propose to use the Moore-Penrose pseudoinverse to obtain the minimum norm least-squares solution, i.e.,\n$\\begin{bmatrix} (d\\Omega_U)_{ij} \\\\ (d\\Omega_V)_{ij} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2\\sigma} & -\\frac{1}{2\\sigma} \\\\ -\\frac{1}{2\\sigma} & \\frac{1}{2\\sigma} \\end{bmatrix} \\begin{bmatrix} (dP)_{ij} \\\\ (dP)_{ji} \\end{bmatrix}$"}, {"title": null, "content": "Moreover, substituting $\\sigma_i$ and $\\sigma_j$ with $\\sigma$ in (20) and (21), we have, $(dP)_{ij} = -(dP)_{ji}$, i.e., at this point, $dP$ also exhibits the antisymmetric property, which makes the two linear equations equivalent, thereby leading to the absence of an exact solution. Substituting this relationship into (25), we ultimately obtain the approximate solution in the case of two equal singular values, that is,\n$(d\\Omega_U)_{ij} = \\frac{(dP)_{ij}}{\\sigma},$\n$(d\\Omega_V)_{ij} = 0,$"}, {"title": null, "content": "Therefore, using the equations (23), (24), and (26), we rewrite these relationships in matrix form as follows:\n$d\\Omega_U = F \\odot [dPS + S dP^H] + T \\odot dP,$\n$d\\Omega_V = F \\odot [S dP + dP^H S],$"}, {"title": null, "content": "where $F_{ij} = \\begin{cases} \\frac{1}{\\sigma_i^2 - \\sigma_j^2}, & \\sigma_i \\neq \\sigma_j \\\\ 0, & \\text{else} \\end{cases}$, and $T_{ij} = \\begin{cases} 1, & \\sigma_i = \\sigma_j \\neq 0 \\& i \\neq j \\\\ 0, & \\text{else} \\end{cases}$.\nFinally, finding $dK_U$ and $dK_V$ in (15) and (16), we will obtain (9) and (11). Left-multiplying (12) and (15) by $U^H$, we could find,\n$U^H dA = dK_U S V^H,$\nand thus,\n$U^H dAV = dK_U S.$"}, {"title": null, "content": "The diagonal matrix $S$ in our setting may have zero singular values in the last diagonal elements, which means that the corresponding last columns of $dK_U S$ are all zeros and thus $U^H dAV$ is the same. It is indicated that there are no constraints on the last columns of $d K_U$ w.r.t. the zero singular values in $S$, since we have mentioned before in (15) that $d K_U$ is an unconstrained matrix. Therefore, we set the unconstrained elements in $d K_U$ as zeros, and then we could obtain,\n$dK_U = U^H dAV S^{-1},$\nwhere $S^{-1} = \\begin{cases} \\frac{1}{\\sigma_i}, & \\sigma_i \\neq 0 \\\\ 0, & \\text{else} \\end{cases}$, and similarly,\n$dK_V = V^H dA^H U S^{-1}.$"}, {"title": null, "content": "Substituting (28) and (32) into (15) and using the property $U_1 U_1^H = I_m - UU^H$, (9) holds. Similarly, we can also obtain that (11) holds.\nTheorem 3.2. Suppose that at some stage during the computation of the loss function $L$, we take a matrix $A$ and compute its SVD, i.e., $A = USV^H$. The derivative of $L$ with respect to $A$ is,\n$\\frac{\\partial L}{\\partial A} = U [(F \\odot [U^H \\overline{U} - \\overline{V} V]) S + T \\odot (U^H \\overline{U})]V^H$\n$+ (I_m - UU^H) \\overline{U} S^{-1}V^H$\n$+ U(I_k \\odot \\overline{S})V^H$\n$+ U S (F \\odot [\\overline{V}^H V - V^H \\overline{V}])V^H$\n$+ U S^{-1} \\overline{V}^H (I_n - VV^H).$"}, {"title": null, "content": "Proof. Substituting Theorem 3.1 into (8) and using the property of the matrix trace, tr(ABC) = tr(CAB) = tr(BCA), we could easily obtain (34). Since the derivation is similar to that in the paper [34], the reader can refer to Section 1.1 in Ref. [34] for the detailed proof of this theorem.\n3.2. Numerical stability analysis in the context of IIPs\nIn the context of IIPs where SVT is adopted to utilize the low-rank properties, suppose that at some stage during the computation of the loss function $L$, we take a matrix $A = USV^H$ and conduct SVT on it to obtain $B = U \\hat{S} V^H = U \\max(S - \\tau, 0) V^H$. In general, most of the information in a matrix is retained in the large singular values, while small singular values are thresholded. Therefore, $\\tau$ is typically not set too small; here, we choose its value as $\\tau > 1e-10$.\nLemma 3.3. In this specific situation, the partial derivative, $\\overline{U}$, have the following form,\n$\\overline{U} = \\overline{B} V S.$\nProof. It can be easily obtained using the chain rule and property of the Kronecker product, i.e., $(B^T \\otimes A) \\text{vec}(X) = \\text{vec}(AXB)$.\nSubstituting (35) into (34), we can obtain,\n$F \\odot [U^H \\overline{U} - \\overline{V}^H V] = F \\odot [U^H \\overline{B} V \\hat{S} - \\hat{S} V^H \\overline{B}^H U],$ \n$T \\odot (U^H \\overline{U}) = T \\odot (U^H \\overline{B} V \\hat{S}).$"}, {"title": null, "content": "Assuming the number of zero elements on the diagonal of the $k \\times k$ matrix $\\hat{S}$ is denoted as $d$, we refer to the non-zero singular values in"}]}