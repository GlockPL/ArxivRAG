{"title": "COMBINING CONSTRAINT PROGRAMMING REASONING WITH Large Language Model Predictions", "authors": ["Florian R\u00e9gin", "Elisabetta De Maria", "Alexandre Bonlarron"], "abstract": "Constraint Programming (CP) and Machine Learning (ML) face challenges in text generation due to CP's struggle with implementing 'meaning' and ML's difficulty with structural constraints. This paper proposes a solution by combining both approaches and embedding a Large Language Model (LLM) in CP. The LLM handles word generation and meaning, while CP manages structural constraints. This approach builds on GenCP, an improved version of On-the-fly Constraint Programming Search (OTFS) using LLM-generated domains. Compared to Beam Search (BS), a standard NLP method, this combined approach (GenCP with LLM) is faster and produces better results, ensuring all constraints are satisfied. This fusion of CP and ML presents new possibilities for enhancing text generation under constraints.", "sections": [{"title": "1 Introduction", "content": "How can we perceive Constraint Programming beyond its traditional role in solving combinatorial optimization problems? Once Eugene Freuder wrote Constraint programming represents one of the closest approaches computer science has yet made to the Holy Grail of programming: the user states the problem, the computer solves it [1]. Nevertheless, some real-world problems are still beyond the reach of the current CP paradigm. This is particularly true when real-world problems involve vague notions such as 'meaning' and 'melody' for text and music. These are not easy to model in CP with the classical toolbox, mainly because these notions are hard to define formally. For instance, it is unclear how to formalize an objective function or a constraint to get closer to a meaningful sentence, a melodious song or a captivating painting. On the other hand, recent results in Machine Learning (ML), such as transformer-based models [2], have demonstrated the power of these techniques to capture a significant part of these vague concepts through data-driven statistical learning (e.g., Large Language Model (LLM) like the GPT series [3], stable-diffusion [4], ChatMusician [5]). In the article, we demonstrate that ML, and in particular LLM, can help CP to model and solve problems where such vague concepts can be found. In recent years, there has been a growing interest in text generation under constraints thanks to the rise of transformerbased models, like OpenAI ChatGPT ([3]) and Meta LLaMa ([6]). Nevertheless, even fine-tuned prompted LLMs fail to generate several constrained outputs (see the tasks introduced in [7]). The goal of this paper is to present a new method for the task of text generation under constraints. This interest has a strong chance of continuing to grow insofar as many brands wish to integrate these technologies, in particular with their customers, and want to have control and guarantees on the behavior of these conversational agents. Hence, it may impact several critical marketing aspects (e.g., brand representation, legal issues, data privacy, ...). Therefore, CP has the potential to become a strong safeguard of this kind of generative model. For the task of text generation under constraints, ML techniques face limitations when they have to manage structural constraints, such as limits on the number of words or characters (e.g. Text Summarization, Text Simplification, Text style transfer, Question Answering, Storytelling, Poetry or Lyrics Generation, Subtitle) [8]. CP succeeds on these types of constraints, making the combination of CP and ML a natural fit for the task of text generation under constraints. This paper proposes such a combination, to tackle a class of problems where neither CP and ML succeeds on their own (Fig. 1). Combining Combinatorial Optimization (CO) and ML is a very active research area [19], however there is no easy way to integrate the ML 'expertise' into CP as a constraint of the model [20, 21] and vice versa [22]. Furthermore, there are many incentives to strengthen the interactions between ML and CO [23, 24, 25]. Usually, the main motivation comes from the performance perspective, where the idea is to improve a solver's performance with ML (e.g., finding branching heuristics thanks to Reinforcement Learning [26] or finding better bounds with Clustering [27]). This paper tackles it from the modeling point of view. Modeling is a crucial part of CO works. In the end, the model must account for the underlying solver that runs it. More in detail, here, the paper focuses on the interaction between CP and ML, more precisely through an ML-augmented CP approach [28]. In the context of text generation under constraints, the domain of a variable represents a word. The base idea of the paper consists in letting ML manage the domain of variables and CP manage the constraints and the number of variables. In this manner, the sentence formed by variables has high chances to have a meaning and all the constraints will be satisfied. In traditional CP, the domains can not be managed by ML because they have to be set beforehand. However, it is possible to rely on On-the-fly Constraint Programming Search (OTFS) [29], a CP based method where variables, domains and constraints are generated during the search for a solution. The main contribution of this paper is to propose a new version of OTFS, called GenCP, where the generative function of the domain of variables is modified to allow CP variable domains to be computed by an LLM embedded in it, during the search for a solution. More in detail, ML is used during process solving but it is also used as an explicit part of the problem definition (i.e., domains are predicted by the LLM and can replace entirely static variable domains definition of a CSP.). Thus it bridges CP and ML through solving and modelling. The potential of the approach is showcased for the problem of text generation under constraints, against one the most used techniques in the Natural Language Processing (NLP) field: Beam Search (BS). Both methods (BS and GenCP) are compared on constrained sentences generation tasks extracted from benchmarks recently introduced [7]. The approach highlights how CP can provide guarantees when combined with LLM predictions. The paper is organized as follows: Sec. 2 serves as background, Sec. 3 shows how to extend OTFS to GenCP and how to implement an interaction between GenCP and LLM. Sec. 4 presents the experimental results in which the new approach is demonstrated on the task of text generation under constraints. Finally, Sec. 5 delves into further discussion, offering additional insights into this work and providing perspectives for future research endeavors."}, {"title": "2 Background", "content": "This section introduces the necessary background on LLM and CP."}, {"title": "2.1 LLM Predictions Strategies", "content": "Large Language Models (LLMs), such as the GPT series, generate text by predicting the next token (word or character) given the history of previously generated words. Decoding in LLMs refers to the strategy used to select the next words to be generated."}, {"title": "2.1.1 Decoding Strategies Combined with LLMs", "content": "The simplest decoding strategy is greedy decoding. Here, the LLM selects the words with the highest probability at each time step. Although simple and efficient, this approach does not guarantee the best overall sequence, as it does not consider the effect of the current selection on future tokens."}, {"title": "2.1.2 Greedy Decoding", "content": "The simplest decoding strategy is greedy decoding. Here, the LLM selects the words with the highest probability at each time step. Although simple and efficient, this approach does not guarantee the best overall sequence, as it does not consider the effect of the current selection on future tokens."}, {"title": "2.1.3 Beam Search", "content": "Beam Search (BS) [30, 10, 11] is a refined version of greedy decoding. A beam is a candidate sequence of words. Instead of selecting the single best token at each time step, it usually keeps track of the $k$ most likely sequences (beams) at each step. Although BS usually achieves better results than greedy decoding, it assumes that high-ranking token sequences consist of high-ranking tokens, which may only sometimes be the case. For a more stochastic and diverse output, top$k$ sampling and top- $p$ sampling (also known as nucleus sampling) are used. In top- $k$ sampling, the model selects from the top $k$ highest probability predictions, while in top- $p$ sampling, it dynamically selects the number of top predictions to cover $p$ percent of the total probability mass."}, {"title": "2.1.4 Perplexity", "content": "Perplexity is an entropy metric derived from Shannon's information theory [31]. Since an LLM computes the probability of text, then it can compute text perplexity. It can be expressed as the geometric mean of the inverse conditional likelihood of the sequence [32]. Let $S_{n}$ be the sequence of a succession of words of size $n$ : $S_{n}=w_{1} w_{2} . . w_{n}$. The perplexity (PPL) of $S_{n}$ is computed as follows: \n\n $$\\operatorname{PPL}\\left(S_{n}\\right)=\\sqrt[n]{\\frac{1}{P\\left(w_{1} w_{2} w_{3} \\ldots w_{n}\\right)}}\n\n where probability $P(\\cdot)$ is given by the LLM. PPL can be interpreted as the 'how likely a text is generated by a given model' [8]. Usually, it is used to evaluate the LLM itself by checking that good samples are recognized as such (i.e., low PPL values). In NLP, the evaluation of text is still an open problem, and human evaluation remains the gold standard. Numerous metrics have been developed to address this issue. Among them, PPL remains an objective criterion associated with text produced by a given model. PPL is also much more convenient to use than pure probability. Its range is $[1 ;+\\infty[$. The lower, the better."}, {"title": "2.2 Constraint Programming", "content": "Constraint Programming (CP) is a paradigm for solving combinatorial problems that draws on a wide range of techniques from artificial intelligence and operations research. In CP a problem can be defined as a Constraint Satisfaction Problem (CSP). A CSP is a triplet: $\\langle X, D, C\\rangle$, where: \n\n- $X=\\left\\{X_{1}, X_{2}, \\ldots, X_{n}\\right\\}$ is the set of variables of the problem. \n- $D=\\left\\{D_{X_{1}}, D_{X_{2}}, \\ldots, D_{X_{n}}\\right\\}$ is the set of domains, where each domain $D_{X_{i}}$ corresponds to the set of possible values for the variable $X_{i}$. \n- $C=\\left\\{c_{1}, c_{2}, \\ldots, c_{m}\\right\\}$ is the set of constraints of the problem. A constraints represent a property of the problem. \n\nA solution is an assignment of all the variables to a value present in their respective domains, such that all the constraints are satisfied."}, {"title": "2.2.1 Avoiding Static Definition of the CSP", "content": "In traditional CP, for the task of text generation under constraints, a variable represents a word. Since the domains of variables have to be set beforehand, they will be of enormous size, containing every word/declination of words for a given language. Furthermore, constraints between succession of words may lead to a combinatorial explosion. Since traditional CP is not well suited, this work focuses on OTFS, a CP based method recently introduced by R\u00e9gin and De Maria [29]. Instead of having the variables/domains/constraints set before the search, OTFS generates the variables/domains/constraints during the search for a solution, avoiding the problem stated above and being expendable to permit the integration of an LLM. The new version of OTFS is called GenCP."}, {"title": "3 Method: LLM alongside OTFS", "content": "The approach of this paper extends OTFS by having an embedded LLM generate the domains of variables. The approach also adds a minor improvement in the form of helping functions, to differentiate between implicit constraints (prevent infinite loops, ensure a variable represents a word, etc.) and explicit constraints (constraints of the problem). In the next subsection, the new version of OTFS called GenCP is described."}, {"title": "3.1 New version of OTFS: GenCP", "content": "In traditional CP it is not common to generate new variables/domains/constraints during the search, while OTFS is based on this idea. OTFS begins with an empty or partially defined CSP (the CSP has less variables/domains/constraints than the CSP in traditional CP) and will generate variable/domains/constraints during the search for solutions. \n\nGenCP is a new version of OTFS that makes two changes to the original version: 1) the function that generates the domain gen $D$ calls an LLM to generate the domain of the current variable. 2) Helping functions are added to represent implicit constraints. \n\nHere is GenCP applied to text generation under constraints. An GenCP model can be defined as a pair of sets $\\{\\mathcal{M}, \\mathcal{F}\\}$, where: \n\n- $\\mathcal{M}=\\{X, D, C\\}$ represents the model of the problem. \n- $X$ represents the variables. The variables represent words. \n- $D$ represents the domain of the variables. A domain $d_{i} \\in D$ contains a list of predicted words by an LLM. \n- $C$ represents the explicit constraints (constraints of the problem). A constraint $c_{i} \\in C$ represents rules over text (e.g., number of words, number of characters, forbidden words, or symbols). \n- $\\mathcal{F}=\\{G, B, H\\}$ is a set of functions. \n- $G$ represents the set of generative functions: these functions explain to the solver how to generate variables/domains/constraints. \n- $B$ represents the set of Boolean functions: these functions tell the solver when a solution is found. \n- $H$ represents the set of helping functions: these functions are used to represent implicit constraints, for example ensuring that when a variable is generated, it helps obtaining a solution (to prevent the solver from attaining an infinite loop of generating variables)."}, {"title": "3.1.1 Generative Functions", "content": "The set of generative functions $G=\\{\\text{gen } V, \\text{gen } D, \\text{gen } C\\}$ is such that: \n\n- $\\text{gen } V$ generates a new variable with an empty domain and adds it to $X$. \n- $\\text{gen } D$ calls the LLM with the current sentence formed by the model and sets the domain of the previously generated variable to the output. \n- $\\text{gen } C$ generates the constraint(s) relevant to the current variables of the model to $C$. The constraints generated depend on the problem (e.g., generate a sentence that does not contain the letter 'e')."}, {"title": "3.1.2 LLM integration", "content": "A variable is generated with an empty domain. To generate the domain of variables, $\\text{gen } D$ calls an LLM using $\\text{callLLM}$ (sentence, parameters, $k$ ), where: \n\n- sentence is the current sentence represented by the variables of the model. \n- parameters represents sampling parameters (top$_k$, top$_p$...). For this paper, top$_k$ is used exclusively for both GenCP and BS: the LLM answers $k$ words ranked by probability, highest to lowest. \n- $k$ is the number of words asked to the LLM. \n\nSince the parameters and $k$ are not modified after the definition of the model, $\\text{callLLM}$ (sentence, parameters, $k$ ) will be simply referred to as $\\text{callLLM}$ (sentence)."}, {"title": "3.1.3 Helping Functions", "content": "Helping functions represent implicit constraints, like avoiding infinite loops. In our current implementation, the set of functions $H$ contains the following functions: \n\n- $H_{o}$ : it orders the domain of variables depending on the problem. \n- $H_{\\text {onlyWords }}$ : it ensures that any word predicted by the LLM is a complete word and not the suffix or prefix of a word and it filters out any symbol or special character."}, {"title": "3.1.4 Description of the new approach", "content": "The main steps of GenCP are depicted in Fig. 3 and Algorithm 1: \n\n1. GenCP begins with an initial state. If the initial state is empty, the generative functions are called (2.), otherwise the helping functions are called (3.). \n2. The generative functions $\\text{gen } V / \\operatorname{gen } D / \\operatorname{gen } C$ are called (genD calls the LLM). \n3. The helping functions are called to manage implicit constraints, backtracking if necessary (e.g., if the LLM generated an empty domain). \n4. The current state of the model $\\mathcal{M}$ is saved. \n5. The propagation is called, if it fails the model backtracks (8.), else it calls the boolean functions (6.). \n6. The Boolean functions are called to check if a solution has been found. If a solution is found, it is saved (7.) and the model backtracks (8.), otherwise the model calls the generative functions (2.). \n7. The current sentence formed by the variables is saved as a solution. \n8. GenCP backtracks to a previously saved state (4.) of the model and changes the choices made during propagation (5.). If no previous state was saved, then backtracking fails (9.). \n\n- When backtracking to a previously saved state, the model deletes all variables, their respective domains, and the constraints associated with them, that are not present in the previously saved state. \n\n9. GenCP outputs the solution(s) that were saved or it indicates that no solution was found."}, {"title": "3.1.5 Enforce variability", "content": "Variability between two sentences is the number of words that are not equal at each position, for example: \n\n- 'The little boy is' and 'The little cat is' have a variability of 1. \n- 'My name is John' and 'John is my name' have a variability of 4. \n\nTo force a greater variability between solutions (greater than 2), a special backtrack called backtrackTo( $n$ ) is used. Let the set of variables $X=\\{x_{1}, \\ldots, x_{n}, x_{n+1} \\ldots, x_{m}\\}$. The function backtrackTo( $n$ ) deletes the variables $x_{n+1}$ to $x_{m}$ and causes a backtrack. For example, consider the sentence 'I like to swim in the summer.'. With backtrackTo(2), 'to swim in the summer.' is deleted and the value of variable $x 2=$ 'like' is changed. The next solution might be 'I want to break free.'."}, {"title": "3.1.6 Ordering", "content": "For some tasks, not following the ordering strategies of the LLM (like top- $k$ and top- $p$ ) can lead to better/faster solutions. Two other orderings are considered: PPL valuation and length of a word (depending on the average word length in the given language)."}, {"title": "3.2 Modeling Example", "content": "Here is a simple example of how the search of GenCP works: for this paper the generative functions only generate variables one at a time but it is important to note that these functions can generate multiple variables, domains and constraints at once. Let us suppose GenCP has to generate a sentence beginning by 'The' and containing between 10 and 15 words with exactly 60 characters. The following functions are needed: \n\n- currentSentence $(\\mathcal{M})$ : outputs the current sentence the variables form. \n- callLLM (sentence): described in 3.1.2. Here $k$ is equal to 10 (each time the LLM is called, it will output 10 words). \n- contains(sentence, word): outputs yes if the sentence contains the word and no otherwise. \n- $n b C h a r($ sentence $)$ : outputs the number of characters in the sentence. \n\nThe obtained model is $\\{\\mathcal{M}, \\mathcal{F}\\}$, where: \n\n- $\\mathcal{M}=\\{X, D, C\\}$ \n- $X=\\{x_{1}\\}$ \n- $D=\\{d_{1}=\\{right.\\text{'The'}\\}\\}$ \n- $C=\\emptyset$ \n- $\\mathcal{F}=\\{G, B, H\\}$ \n- $G=\\{\\text{gen } V, \\text{gen } D, \\text{gen } C\\}$ is a set of functions, each function follows these steps: \n- generate $x_{|X|+1}$ and add it to $X$ with an empty domain $d_{|X|+1}$. \n- $d_{|X|+1}=\\operatorname{callLLM}(\\text{currentSentence}(\\mathcal{M}))$. \n- $c_{\\text {remove }_{\\text {maxv6tohap }}}((\\text { currentSentence }(\\mathcal{M}), d_{|X|+1})$ \n- The constraints remove the words that make the current sentence exceed 60 characters from the domain of the current variable. \n- $B=\\{\\text{endNbWords}, \\text{endNbCharacters}, \\text{endLLM}\\}$ is a set of functions, each function behaves as follows: \n- $|X|>=10 \\wedge|X|<=15$. \n- $n b \\operatorname{Char}(\\text{currentSentence}(\\mathcal{M}))==60$. \n- contains(callLLM(currentSentence $(\\mathcal{M}))$, '.'). \n- $H=\\{H_{h o}\\}$ : \n- $H_{h o}: \\operatorname{order}(d_{|X|+1})$. \n- To help attain the goal of 60 characters, the domain of the current variable is ordered such that before the 10th word the solver tries the longer words first and at the 10th word the solver tries the shorter words first. \n\nWith the above representation of the problem, GenCP is asked for 4 solutions, backtrackTo(2) is used and the LLM is asked for 10 words maximum per call. The obtained solutions are: \n\n1. The following is an article by the author of the above book. \n2. The first time you see the movie version of your book on TV. \n3. The New York Times has an article on the new book by Tim Wu. \n4. The new year is here and we are ready to make the next step."}, {"title": "4 Experiments", "content": "The experiments presented by Yao et al. are partially reproduced [7]. In particular, the constrained sentence generation tasks described in Tab. 1. Five LLMs were selected: GPT4, GPT4-O, Mistral Next, Claude 3.5, and Gemini. The four LLMs are prompted with the same example command given in [7]. For example, 'Please create a sentence of exactly 82 characters.' for the Sent-1 task ${ }^{1}$. Tab. 2 gives an overview of the performance of the five LLMs on the four tasks. The satisfaction rate is based on ten trials per task per model. In addition, Tab. 2 also shows that the LLMs perform well on the lexically constrained scenario task-4 with a $90+\\%$ satisfaction rate over ten trials. Also, as Yao et al. previously showed in their paper, LLMs struggle to produce constrained sentences involving counting (e.g., words and characters). They provide a nice picture of current LLM satisfaction capabilities by introducing new benchmarks. Unfortunately, the Yao et al. article only provides the benchmarks and some hints on reproducing them. However, it does not give any hints on how to solve the tasks associated with the benchmarks (see the original article for more details [7])."}, {"title": "4.1 Experimental Conditions", "content": "The experiments were performed on a laptop with Windows 10 Professional, 32 GB RAM, and Intel 16 CPU cores. The approach and the BS are implemented in Java 17 for easier comparisons."}, {"title": "4.1.1 Baseline", "content": "The experiments presented by Yao et al. are partially reproduced [7]. In particular, the constrained sentence generation tasks described in Tab. 1. Five LLMs were selected: GPT4, GPT4-O, Mistral Next, Claude 3.5, and Gemini. The four LLMs are prompted with the same example command given in [7]. For example, 'Please create a sentence of exactly 82 characters.' for the Sent-1 task ${ }^{1}$. Tab. 2 gives an overview of the performance of the five LLMs on the four tasks. The satisfaction rate is based on ten trials per task per model. In addition, Tab. 2 also shows that the LLMs perform well on the lexically constrained scenario task-4 with a $90+\\%$ satisfaction rate over ten trials. Also, as Yao et al. previously showed in their paper, LLMs struggle to produce constrained sentences involving counting (e.g., words and characters). They provide a nice picture of current LLM satisfaction capabilities by introducing new benchmarks. Unfortunately, the Yao et al. article only provides the benchmarks and some hints on reproducing them. However, it does not give any hints on how to solve the tasks associated with the benchmarks (see the original article for more details [7])."}, {"title": "4.1.2 Hardware & Implementation", "content": "The experiments were performed on a laptop with Windows 10 Professional, 32 GB RAM, and Intel 16 CPU cores. The approach and the BS are implemented in Java 17 for easier comparisons."}, {"title": "4.1.3 LLM choice", "content": "LLaMa [6] is responsible for the predictions of words as domains for the variables, mainly because an efficient implementation in C++ was recently released ${ }^{2}$. It allows running a model on a personal laptop and CPU (only) efficiently. Thanks to quantization [33] (model weight compression), the 7B parameters model (in Float16) of 13GB original size, in 4-bit quantization (Q4) drops to 3.9 GB of RAM. However, the biggest model of LLama 65B (120GB), even in Q4, needs 38.5 GB of RAM. Thus, the LLaMa v1 model used in the experiments is LLaMa 7B Q4 with low temperature (i.e., $\\leq 1$, temp $=0.8$ ). \n\nWhen asked for $k$ words, this version of LLaMa will take the same amount of time to ouput 1 word and 1000 words. To minimize the importance of $k$, call $L L M$ outputs more than $k$ words, a beam/variable only keeps $k$ 'valid' words. A 'valid' word is a word that does not violate a constraint on its own. For example, a word that does not violate the constraint 'does not contain the letter $e$ '."}, {"title": "4.1.4 Beam Search Technical Remarks", "content": "In the current implementations two halting conditions are defined for BS: \n\n- First solution: when the current beam contains at least one solution, BS is stopped and output the solutions. \n- All solutions: when the current beam contains at least one solution but another beam can continue to generate words without violating a constraint (for instance, it does not contain enough characters to satisfy a length constraint), the beam solutions are saved and BS continues with the remaining beams."}, {"title": "4.1.5 Benchmarks Settings", "content": "BS and GenCP are compared on some recent benchmarks described in Sec. 4.1.1. To guarantee GenCP and BS to be judged on the generation of sentences of the same quality, a solution is a sentence that satisfies all the constraints of the current task and, when given this sentence, the LLM predicts a period ('.'). Not to alter BS too much, words are ordered by probability (PPL is not used) and, since BS sentences have low variability, GenCP is used without backtrackTo( $n$ ). BS and GenCP are compared on the following criteria: \n\n- Time in seconds. \n- Number of solutions. GenCP was stopped when it found the same number of solutions as BS on a task. $0 / 1$ means that BS found no solution while GenCP found one solution. \n- The ratio solutions/outputs as a constraint satisfaction rate. \n- For BS only, the number of bad outputs (number of outputs that are not a solution). \n- For GenCP only, the number of backtracks."}, {"title": "4.2 Result Analysis", "content": "The results show that GenCP can be used to solve efficiently text generation under constraints problems. GenCP is faster than BS and all the outputs are solutions, contrary to BS where some outputs are not solutions. \n\nAlthough the results suggest that GenCP succeeds in all tasks (see Tab. 3), it becomes particularly interesting when considering size constraints (e.g., sentences with a precise number of words or characters). It obtains sentences that satisfy the constraint with a low PPL score on sent-1 and sent-3 tasks. \n\nGenCP also succeeds in producing sequences obeying lexical constraints in sent-2 and sent-4. However, the PPL and a human evaluation on these sentences show a substantial deterioration in term of quality (i.e., meaningfulness). \n\nTherefore, regarding sent-1 and sent-3 tasks, GenCP is to be preferred, whereas for sent-4 and sent-2 tasks, LLMs prompted alone or joint with BS is still adequate."}, {"title": "4.2.1 Beam Search", "content": "BS and GenCP are compared in Tab. 3. In all tables, the number of backtracks is denoted by \\#bk. BS is slower than GenCP and has lower satisfaction rate (number of outputs that are solutions / total number of outputs), denoted by \\%sat. This is due to multiple facts: \n\n1. Beam Search can not guarantee to find every solution. \n2. Beam Search chooses the next word depending on the probability of the LLM. \n3. At each step, BS considers $k$ sentences, each sentence asks $k$ words to the LLM, so each step considers $k^{2}$ words. BS orders these words decreasingly by probability and only keeps the $k$ first. \n\nFacts 2 and 3 explain why increasing $k$ does not guarantee to find the same/more solutions, it might even cause BS to find less solutions. Let us suppose $k=5$, BS found one solution, and at depth 4 , the candidate needed to find this solution was ranked 5 out of 25 . Let us suppose now $k$ is increased to 6 : at each step BS will consider 36 candidates and take the 6 best ones. BS considers 11 more candidates than with $k=5$; if at depth 4 , the candidate needed to find the previous solution is now ranked 7 instead of 5 , BS will not consider it and $k=6$ will not find the solution found with $k=5$."}, {"title": "4.2.2 GenCP", "content": "Tab. 4 shows the capability of GenCP to generate more solutions than BS. GenCP is given the same time as BS for the same task and $k=50$, GenCP obtains more solutions than BS. Note that for sent-1, without backtrackTo GenCP only obtains 2 solutions in 1123 seconds, while with backtrackTo(6) GenCP obtains 11 solutions in 1123 seconds. The LLM-enhanced GenCP avoids the drawbacks of BS and proposes an alternative approach to text generation under constraints for the following reasons: \n\n- GenCP can guarantee to find every solution (if any). Increasing $k$ guarantees to find at least the same solutions previously found and potentially finds new solutions. Furthermore, it can offer more solutions than BS. \n- All the outputs answered by GenCP are solutions (all the constraints are satisfied). \n- GenCP offers more options for improvement, for example to ensure better variability (backtrackTo explained in 3.1.5 can be used) or other orderings than probability (3.1.6)."}, {"title": "4.2.3 Variability and Perplexity", "content": "Tab. 5 demonstrates the importance of enforcing variability and perplexity. When GenCP generated solutions for Tab. 3 and 4, the maximum variability was 4 . Tab. 5 shows that with backtrackTo(2)/backtrackTo(3), sentences generated are almost completely different thanks to high variability (10+ for sent-3 for example). \n\nTab. 5 purposefully contains sentences with high perplexity to illustrate that this leads to a degradation in the sentence quality (i.e., low meaning). All the sentences generated for sent-4 had the words 'soft', 'beach' and 'math' next to each other. To showcase the capability of GenCP to improve sentences, sent-4* was created: it is the same as sent-4 except that 'soft', 'beach' and 'math' must contain at least three words between them."}, {"title": "5 Discussion & Perspectives", "content": "The article shares a proof-of-concept showing that interesting results can be obtained using CPU resources combined with a small quantized LLM in a CP solver. However, LLMs, in general, work best with much larger computational resources and require GPU resources. Even though smaller models (e.g., Mistral 8x7B) sometimes manage to take top places in specific scenarios. The top spots in the LLM Elo rankings feature gigantic models [34]. Given their size, clusters of GPU are quickly mandatory. Hence, it would be interesting to study in more detail how the joint use of resources (for instance, CPU for solver and GPU for LLM) could improve the results of the paper and correspond to more real-world usage in industry. \n\nIn this article, GenCP ignores tokens and works at the word level (pre-token). It is possible to handle tokens by adapting the problem modeling. Indeed, it is possible to consider a word as a meta-variable $X_{1}$ composed of several decision variables (e.g., $X_{1_{1}}, X_{1_{2}}, X_{1_{3}} \\ldots$ ). This is useful and straightforward, as it is not clear in advance how the tokenizer will cut the words. For instance, let us consider the following sentence: The first step in the recruitment of a new hire is to make sure that the job requisition is clear. Let us look at the assignments of the variables (space separates meta-variables, and semicolon decision variables): The; first; step; in; the; rec;ruit;ment; of; a; new; h;ire; is; to; make; sure; that; the; job; requ;is;ition; is; clear;. The word recruitment needs three decision variables because it is composed of three tokens (i.e., rec, ruit and ment). It is easy to manage in GenCP because it can generate as many variables as required. Nevertheless, the evolution of the CSP (generation of variables and domains) is rather technical and, therefore, depends on the tokenizer. \n\nThe idea that a CSP can evolve in response to external information is not new (e.g., Dynamic Constraint Network [35]). This dynamic vision of CSPs has been motivated by several real-world problems, particularly in product configuration [36]. GenCP proposes ML integration in modeling by letting LLMs manage operations for CSP domains during the resolution process. The 'outside the world' information [37] is given by the LLM. The article shows that LLMs can contribute to CSP modeling for generation tasks. However, how ML/LLMs can be used for CSP modeling in general for any problem remains an open problem [38, 39, 40, 41]."}, {"title": "6 Conclusion", "content": "This paper showed that combining CP solving of structural constraints and ML understanding of vague notions (like meaning) on the task of text generation under constraints obtains promising results. This paper presents GenCP, a new method that extends OTFS to make the domains manageable by LLM predictions. The results show that GenCP can generate meaningful sentences that ensure various properties like the number of words, number of characters, mandatory keywords, or some forbidden characters. The results also show that GenCP has $100 \\%$ satisfaction rate and takes less time to output solutions of the same quality than a well-known technique in the field of text generation under constraints: Beam Search. GenCP provides multiple improvements thanks to ordering, enforcing variability and perplexity, allowing thus to obtain overall higher quality solutions than BS."}]}