[{"title": "COMBINING CONSTRAINT PROGRAMMING REASONING WITH\nLARGE LANGUAGE MODEL PREDICTIONS", "authors": ["Florian R\u00e9gin", "Elisabetta De Maria", "Alexandre Bonlarron"], "abstract": "Constraint Programming (CP) and Machine Learning (ML) face challenges in text generation due\nto CP's struggle with implementing \u201cmeaning\u201d and ML's difficulty with structural constraints. This\npaper proposes a solution by combining both approaches and embedding a Large Language Model\n(LLM) in CP. The LLM handles word generation and meaning, while CP manages structural con-\nstraints. This approach builds on GenCP, an improved version of On-the-fly Constraint Programming\nSearch (OTFS) using LLM-generated domains. Compared to Beam Search (BS), a standard NLP\nmethod, this combined approach (GenCP with LLM) is faster and produces better results, ensuring\nall constraints are satisfied. This fusion of CP and ML presents new possibilities for enhancing text\ngeneration under constraints.", "sections": [{"title": "1 Introduction", "content": "How can we perceive Constraint Programming beyond its traditional role in solving combinatorial optimization prob-\nlems? Once Eugene Freuder wrote Constraint programming represents one of the closest approaches computer\nscience has yet made to the Holy Grail of programming: the user states the problem, the computer solves it [1].\nNevertheless, some real-world problems are still beyond the reach of the current CP paradigm. This is particularly true\nwhen real-world problems involve vague notions such as \u201cmeaning\u201d and \u201cmelody\u201d for text and music. These are not\neasy to model in CP with the classical toolbox, mainly because these notions are hard to define formally. For instance,\nit is unclear how to formalize an objective function or a constraint to get closer to a meaningful sentence, a melodious\nsong or a captivating painting. On the other hand, recent results in Machine Learning (ML), such as transformer-based\nmodels [2], have demonstrated the power of these techniques to capture a significant part of these vague concepts\nthrough data-driven statistical learning (e.g., Large Language Model (LLM) like the GPT series [3], stable-diffusion\n[4], ChatMusician [5]). In the article, we demonstrate that ML, and in particular LLM, can help CP to model and solve\nproblems where such vague concepts can be found.\nIn recent years, there has been a growing interest in text generation under constraints thanks to the rise of transformer-\nbased models, like OpenAI ChatGPT ([3]) and Meta LLaMa ([6]). Nevertheless, even fine-tuned prompted LLMs\nfail to generate several constrained outputs (see the tasks introduced in [7]). The goal of this paper is to present\na new method for the task of text generation under constraints. This interest has a strong chance of continuing to\ngrow insofar as many brands wish to integrate these technologies, in particular with their customers, and want to have\ncontrol and guarantees on the behavior of these conversational agents. Hence, it may impact several critical marketing\naspects (e.g., brand representation, legal issues, data privacy,...). Therefore, CP has the potential to become a strong\nsafeguard of this kind of generative model.\nFor the task of text generation under constraints, ML techniques face limitations when they have to manage structural\nconstraints, such as limits on the number of words or characters (e.g. Text Summarization, Text Simplification, Text\nstyle transfer, Question Answering, Storytelling, Poetry or Lyrics Generation, Subtitle) [8]. CP succeeds on these types\nof constraints, making the combination of CP and ML a natural fit for the task of text generation under constraints.\nThis paper proposes such a combination, to tackle a class of problems where neither CP and ML succeeds on their\nown (Fig. 1)."}, {"title": "2 Background", "content": "This section introduces the necessary background on LLM and CP."}, {"title": "2.1 LLM Predictions Strategies", "content": ""}, {"title": "2.1.1 Decoding Strategies Combined with LLMs", "content": "Large Language Models (LLMs), such as the GPT series, generate text by predicting the next token (word or character)\ngiven the history of previously generated words. Decoding in LLMs refers to the strategy used to select the next words\nto be generated."}, {"title": "2.1.2 Greedy Decoding", "content": "The simplest decoding strategy is greedy decoding. Here, the LLM selects the words with the highest probability at\neach time step. Although simple and efficient, this approach does not guarantee the best overall sequence, as it does\nnot consider the effect of the current selection on future tokens."}, {"title": "2.1.3 Beam Search", "content": "Beam Search (BS) [30, 10, 11] is a refined version of greedy decoding. A beam is a candidate sequence of words.\nInstead of selecting the single best token at each time step, it usually keeps track of the k most likely sequences (beams)\nat each step.\nAlthough BS usually achieves better results than greedy decoding, it assumes that high-ranking token sequences\nconsist of high-ranking tokens, which may only sometimes be the case. For a more stochastic and diverse output, top-\nk sampling and top-p sampling (also known as nucleus sampling) are used. In top-k sampling, the model selects from\nthe top k highest probability predictions, while in top-p sampling, it dynamically selects the number of top predictions\nto cover p percent of the total probability mass."}, {"title": "2.1.4 Perplexity", "content": "Perplexity is an entropy metric derived from Shannon's information theory [31]. Since an LLM computes the proba-\nbility of text, then it can compute text perplexity. It can be expressed as the geometric mean of the inverse conditional\nlikelihood of the sequence [32]. Let $S_n$ be the sequence of a succession of words of size n: $S_n = w_1 w_2 ... w_n$. The\nperplexity (PPL) of $S_n$ is computed as follows:\n$PPL(S_n) = \\sqrt[n]{\\frac{1}{P(w_1 w_2 w_3 ... w_n)}}$,\nwhere probability $P(\\cdot)$ is given by the LLM. PPL can be interpreted as the \u201chow likely a text is generated by a given\nmodel\" [8]. Usually, it is used to evaluate the LLM itself by checking that good samples are recognized as such (i.e.,\nlow PPL values).\nIn NLP, the evaluation of text is still an open problem, and human evaluation remains the gold standard. Numerous\nmetrics have been developed to address this issue. Among them, PPL remains an objective criterion associated with\ntext produced by a given model. PPL is also much more convenient to use than pure probability. Its range is $[1; +\\infty[$.\nThe lower, the better."}, {"title": "2.2 Constraint Programming", "content": "Constraint Programming (CP) is a paradigm for solving combinatorial problems that draws on a wide range of tech-\nniques from artificial intelligence and operations research. In CP a problem can be defined as a Constraint Satisfaction\nProblem (CSP). A CSP is a triplet: $(X, D, C')$, where:\n\u2022 $X = \\{X_1, X_2, ..., X_n\\}$ is the set of variables of the problem.\n\u2022 $D = \\{D_{X_1}, D_{X_2}, ..., D_{X_n} \\}$ is the set of domains, where each domain $D_{X_i}$ corresponds to the set of possible\nvalues for the variable $X_i$.\n\u2022 $C = \\{C_1, C_2, ..., C_m\\}$ is the set of constraints of the problem. A constraints represent a property of the\nproblem.\nA solution is an assignment of all the variables to a value present in their respective domains, such that all the con-\nstraints are satisfied."}, {"title": "2.2.1 Avoiding Static Definition of the CSP", "content": "In traditional CP, for the task of text generation under constraints, a variable represents a word. Since the domains\nof variables have to be set beforehand, they will be of enormous size, containing every word/declination of words\nfor a given language. Furthermore, constraints between succession of words may lead to a combinatorial explosion.\nSince traditional CP is not well suited, this work focuses on OTFS, a CP based method recently introduced by R\u00e9gin\nand De Maria [29]. Instead of having the variables/domains/constraints set before the search, OTFS generates the\nvariables/domains/constraints during the search for a solution, avoiding the problem stated above and being expendable\nto permit the integration of an LLM. The new version of OTFS is called GenCP."}, {"title": "3 Method: LLM alongside OTFS", "content": "The approach of this paper extends OTFS by having an embedded LLM generate the domains of variables. Figure 2\ngraphically depicts the interplay between those components. The approach also adds a minor improvement in the form\nof helping functions, to differentiate between implicit constraints (prevent infinite loops, ensure a variable represents\na word, etc.) and explicit constraints (constraints of the problem). In the next subsection, the new version of OTFS\ncalled GenCP is described."}, {"title": "3.1 New version of OTFS: GenCP", "content": "In traditional CP it is not common to generate new variables/domains/constraints during the search, while\nOTFS is based on this idea. OTFS begins with an empty or partially defined CSP (the CSP has less vari-\nables/domains/constraints than the CSP in traditional CP) and will generate variable/domains/constraints during the\nsearch for solutions."}, {"title": "3.1.1 Generative Functions", "content": "The set of generative functions $G = \\{genV, genD, genC'\\}$ is such that:\n\u2022 $genV$ generates a new variable with an empty domain and adds it to $X$.\n\u2022 $genD$ calls the LLM with the current sentence formed by the model and sets the domain of the previously\ngenerated variable to the output.\n\u2022 $genC$ generates the constraint(s) relevant to the current variables of the model to $C$. The constraints generated\ndepend on the problem (e.g., generate a sentence that does not contain the letter \u201ce\u201d)."}, {"title": "3.1.2 LLM integration", "content": "A variable is generated with an empty domain. To generate the domain of variables, genD calls an LLM using\ncallLLM(sentence, parameters, k), where:\n\u2022 sentence is the current sentence represented by the variables of the model.\n\u2022 parameters represents sampling parameters (top_k, top_p...). For this paper, top_k is used exclusively for\nboth GenCP and BS: the LLM answers k words ranked by probability, highest to lowest.\n\u2022 k is the number of words asked to the LLM.\nSince the parameters and k are not modified after the definition of the model,\ncallLLM(sentence, parameters, k) will be simply referred to as callLLM(sentence)."}, {"title": "3.1.3 Helping Functions", "content": "Helping functions represent implicit constraints, like avoiding infinite loops. In our current implementation, the set of\nfunctions $H$ contains the following functions:\n\u2022 $H_O$: it orders the domain of variables depending on the problem.\n\u2022 $H_{onlyWords}$: it ensures that any word predicted by the LLM is a complete word and not the suffix or prefix\nof a word and it filters out any symbol or special character."}, {"title": "3.1.4 Description of the new approach", "content": "The main steps of GenCP are depicted in Fig. 3 and Algorithm 1:\n1. GenCP begins with an initial state. If the initial state is empty, the generative functions are called (2.),\notherwise the helping functions are called (3.).\n2. The generative functions $genV/genD/genC$ are called (genD calls the LLM).\n3. The helping functions are called to manage implicit constraints, backtracking if necessary (e.g., if the LLM\ngenerated an empty domain).\n4. The current state of the model M is saved.\n5. The propagation is called, if it fails the model backtracks (8.), else it calls the boolean functions (6.).\n6. The Boolean functions are called to check if a solution has been found. If a solution is found, it is saved (7.)\nand the model backtracks (8.), otherwise the model calls the generative functions (2.).\n7. The current sentence formed by the variables is saved as a solution.\n8. GenCP backtracks to a previously saved state (4.) of the model and changes the choices made during propa-\ngation (5.). If no previous state was saved, then backtracking fails (9.).\n\u2022 When backtracking to a previously saved state, the model deletes all variables, their respective domains,\nand the constraints associated with them, that are not present in the previously saved state.\n9. GenCP outputs the solution(s) that were saved or it indicates that no solution was found."}, {"title": "3.1.5 Enforce variability", "content": "Variability between two sentences is the number of words that are not equal at each position, for example:\n\u2022 \"The little boy is\" and \"The little cat is\" have a variability of 1.\n\u2022 \"My name is John\" and \"John is my name\u201d have a variability of 4."}, {"title": "3.1.6 Ordering", "content": "For some tasks, not following the ordering strategies of the LLM (like top-k and top-p) can lead to better/faster\nsolutions. Two other orderings are considered: PPL valuation and length of a word (depending on the average word\nlength in the given language)."}, {"title": "3.2 Modeling Example", "content": "Here is a simple example of how the search of GenCP works: for this paper the generative functions only generate\nvariables one at a time but it is important to note that these functions can generate multiple variables, domains and\nconstraints at once. Let us suppose GenCP has to generate a sentence beginning by \u201cThe\u201d and containing between 10\nand 15 words with exactly 60 characters. The following functions are needed:\n\u2022 currentSentence(M): outputs the current sentence the variables form.\n\u2022 callLLM(sentence): described in 3.1.2. Here k is equal to 10 (each time the LLM is called, it will output\n10 words).\n\u2022 contains(sentence, word): outputs yes if the sentence contains the word and no otherwise.\n\u2022 nbChar(sentence): outputs the number of characters in the sentence.\nThe obtained model is {M, F}, where:\n\u2022 M = {X, D,C'}:\n\u2022 X = {x1}.\n\u2022 D = {d\u2081 = {\u201cThe"}], "steps": "ngenerate $x_{|x|+1}$ and add it to X with an empty domain $d_{|x|+1}$.\n$d_{|x|+1}$ = callLLM(currentSentence(M)).\n$C_{removeover60char((currentSentence(M),dx|+1)}$.\nThe constraints remove the words that make the current sentence exceed 60 characters from the domain\nof the current variable.\n\u2022 B = {endNbWords, endNbCharacters, endLLM} is a set of functions, each function behaves as fol-\nlows:\n$|X| >= 10 ^ |X| <= 15$.\n$nbChar(currentSentence(M)) == 60$.\n$contains(callLLM(currentSentence(M)), \u201c.\u201d)$.\n\u2022 H = {Hho}:\n$H_{ho}$: order($d_{|x|+1}$).\nTo help attain the goal of 60 characters, the domain of the current variable is ordered such that before the\n10th word the solver tries the longer words first and at the 10th word the solver tries the shorter words\nfirst.\nWith the above representation of the problem, GenCP is asked for 4 solutions, backtrackTo(2) is used and the LLM\nis asked for 10 words maximum per call. The obtained solutions are:\n1. The following is an article by the author of the above book.\n2. The first time you see the movie version of your book on TV.\n3. The New York Times has an article on the new book by Tim Wu.\n4. The new year is here and we are ready to make the next step."}, {"title": "3.3 Illustrated Example", "content": ""}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Conditions", "content": ""}, {"title": "4.1.1 Baseline", "content": "The experiments presented by Yao et al. are partially reproduced [7]. In particular, the constrained sentence generation\ntasks described in Tab. 1. Five LLMs were selected: GPT4, GPT4-O, Mistral Next, Claude 3.5, and Gemini. The\nfour LLMs are prompted with the same example command given in [7]. For example, \"Please create a sentence of\nexactly 82 characters.\" for the Sent-1 task\u00b9. Tab. 2 gives an overview of the performance of the five LLMs on the four\ntasks. The satisfaction rate is based on ten trials per task per model. In addition, Tab. 2 also shows that the LLMs\nperform well on the lexically constrained scenario task-4 with a 90+% satisfaction rate over ten trials. Also, as Yao et\nal. previously showed in their paper, LLMs struggle to produce constrained sentences involving counting (e.g., words\nand characters). They provide a nice picture of current LLM satisfaction capabilities by introducing new benchmarks.\nUnfortunately, the Yao et al. article only provides the benchmarks and some hints on reproducing them. However, it\ndoes not give any hints on how to solve the tasks associated with the benchmarks (see the original article for more\ndetails [7])."}, {"title": "4.1.2 Hardware & Implementation", "content": "The experiments were performed on a laptop with Windows 10 Professional, 32 GB RAM, and Intel 16 CPU cores.\nThe approach and the BS are implemented in Java 17 for easier comparisons."}, {"title": "4.1.3 LLM choice", "content": "LLaMa [6] is responsible for the predictions of words as domains for the variables, mainly because an efficient imple-\nmentation in C++ was recently released\u00b2. It allows running a model on a personal laptop and CPU (only) efficiently.\nThanks to quantization [33] (model weight compression), the 7B parameters model (in Float16) of 13GB original size,\nin 4-bit quantization (Q4) drops to 3.9GB of RAM. However, the biggest model of LLama 65B (120GB), even in Q4,\nneeds 38.5 GB of RAM. Thus, the LLaMa v1 model used in the experiments is LLaMa 7B Q4 with low temperature\n(i.e., \u2264 1, temp = 0.8).\nWhen asked for k words, this version of LLaMa will take the same amount of time to ouput 1 word and 1000 words.\nTo minimize the importance of k, callLLM outputs more than k words, a beam/variable only keeps k \"valid\" words.\nA \"valid\" word is a word that does not violate a constraint on its own. For example, a word that does not violate the\nconstraint \"does not contain the letter e\"."}, {"title": "4.1.4 Beam Search Technical Remarks", "content": "In the current implementations two halting conditions are defined for BS:\n\u2022 First solution: when the current beam contains at least one solution, BS is stopped and output the solutions.\n\u2022 All solutions: when the current beam contains at least one solution but another beam can continue to generate\nwords without violating a constraint (for instance, it does not contain enough characters to satisfy a length\nconstraint), the beam solutions are saved and BS continues with the remaining beams."}, {"title": "4.1.5 Benchmarks Settings", "content": "BS and GenCP are compared on some recent benchmarks described in Sec. 4.1.1.\nTo guarantee GenCP and BS to be judged on the generation of sentences of the same quality, a solution is a sentence\nthat satisfies all the constraints of the current task and, when given this sentence, the LLM predicts a period (\u201c.\u201d). Not\nto alter BS too much, words are ordered by probability (PPL is not used) and, since BS sentences have low variability,\nGenCP is used without backtrackTo(n)."}, {"title": "4.2 Result Analysis", "content": "The results show that GenCP can be used to solve efficiently text generation under constraints problems. GenCP is\nfaster than BS and all the outputs are solutions, contrary to BS where some outputs are not solutions.\nAlthough the results suggest that GenCP succeeds in all tasks (see Tab. 3), it becomes particularly interesting when\nconsidering size constraints (e.g., sentences with a precise number of words or characters). It obtains sentences that\nsatisfy the constraint with a low PPL score on sent-1 and sent-3 tasks.\nGenCP also succeeds in producing sequences obeying lexical constraints in sent-2 and sent-4. However, the PPL and\na human evaluation on these sentences show a substantial deterioration in term of quality (i.e., meaningfulness).\nTherefore, regarding sent-1 and sent-3 tasks, GenCP is to be preferred, whereas for sent-4 and sent-2 tasks, LLMs\nprompted alone or joint with BS is still adequate."}, {"title": "4.2.1 Beam Search", "content": "BS and GenCP are compared in Tab. 3. In all tables, the number of backtracks is denoted by #bk. BS is slower than\nGenCP and has lower satisfaction rate (number of outputs that are solutions / total number of outputs), denoted by\n%sat. This is due to multiple facts:\n1. Beam Search can not guarantee to find every solution.\n2. Beam Search chooses the next word depending on the probability of the LLM.\n3. At each step, BS considers k sentences, each sentence asks k words to the LLM, so each step considers $k^2$\nwords. BS orders these words decreasingly by probability and only keeps the k first.\nFacts 2 and 3 explain why increasing k does not guarantee to find the same/more solutions, it might even cause BS to\nfind less solutions.\nLet us suppose k = 5, BS found one solution, and at depth 4, the candidate needed to find this solution was ranked 5\nout of 25. Let us suppose now k is increased to 6: at each step BS will consider 36 candidates and take the 6 best ones.\nBS considers 11 more candidates than with k = 5; if at depth 4, the candidate needed to find the previous solution is\nnow ranked 7 instead of 5, BS will not consider it and k = 6 will not find the solution found with k = 5."}, {"title": "4.2.2 GenCP", "content": ""}, {"title": "4.2.3 Variability and Perplexity", "content": "Tab. 5 demonstrates the importance of enforcing variability and perplexity. When GenCP generated solutions for\nTab. 3 and 4, the maximum variability was 4. Tab. 5 shows that with backtrackTo(2)/backtrackTo(3), sentences\ngenerated are almost completely different thanks to high variability (10+ for sent-3 for example)."}, {"title": "5 Discussion & Perspectives", "content": ""}, {"title": "5.1 GPU and CPU Interplay", "content": "The article shares a proof-of-concept showing that interesting results can be obtained using CPU resources combined\nwith a small quantized LLM in a CP solver. However, LLMs, in general, work best with much larger computational\nresources and require GPU resources. Even though smaller models (e.g., Mistral 8x7B) sometimes manage to take\ntop places in specific scenarios. The top spots in the LLM Elo rankings feature gigantic models [34]. Given their size,\nclusters of GPU are quickly mandatory. Hence, it would be interesting to study in more detail how the joint use of\nresources (for instance, CPU for solver and GPU for LLM) could improve the results of the paper and correspond to\nmore real-world usage in industry."}, {"title": "5.2 Token Management", "content": "In this article, GenCP ignores tokens and works at the word level (pre-token). It is possible to handle tokens by\nadapting the problem modeling. Indeed, it is possible to consider a word as a meta-variable $X_1$ composed of several\ndecision variables (e.g., $X_{11}, X_{12}, X_{13}...)$. This is useful and straightforward, as it is not clear in advance how the\ntokenizer will cut the words. For instance, let us consider the following sentence: The first step in the recruitment\nof a new hire is to make sure that the job requisition is clear. Let us look at the assignments of the variables (space\nseparates meta-variables, and semicolon decision variables): The; first; step; in; the; rec;ruit;ment; of; a; new; h;ire;\nis; to; make; sure; that; the; job; requ;is;ition; is; clear;. The word recruitment needs three decision variables because\nit is composed of three tokens (i.e., rec, ruit and ment). It is easy to manage in GenCP because it can generate as many\nvariables as required. Nevertheless, the evolution of the CSP (generation of variables and domains) is rather technical\nand, therefore, depends on the tokenizer."}, {"title": "5.3 CSP Modeling", "content": "The idea that a CSP can evolve in response to external information is not new (e.g., Dynamic Constraint Network [35]).\nThis dynamic vision of CSPs has been motivated by several real-world problems, particularly in product configuration\n[36]. GenCP proposes ML integration in modeling by letting LLMs manage operations for CSP domains during the\nresolution process. The \"outside the world\u201d information [37] is given by the LLM. The article shows that LLMs can\ncontribute to CSP modeling for generation tasks. However, how ML/LLMs can be used for CSP modeling in general\nfor any problem remains an open problem [38, 39, 40, 41]."}, {"title": "6 Conclusion", "content": "This paper showed that combining CP solving of structural constraints and ML understanding of vague notions (like\nmeaning) on the task of text generation under constraints obtains promising results. This paper presents GenCP, a\nnew method that extends OTFS to make the domains manageable by LLM predictions. The results show that GenCP\ncan generate meaningful sentences that ensure various properties like the number of words, number of characters,\nmandatory keywords, or some forbidden characters. The results also show that GenCP has 100% satisfaction rate\nand takes less time to output solutions of the same quality than a well-known technique in the field of text generation\nunder constraints: Beam Search. GenCP provides multiple improvements thanks to ordering, enforcing variability and\nperplexity, allowing thus to obtain overall higher quality solutions than BS."}]