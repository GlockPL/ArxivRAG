{"title": "NON-MYOPIC GENERATION OF LANGUAGE MODELS FOR REASONING AND PLANNING", "authors": ["Chang Ma", "Haiteng Zhao", "Junlei Zhang", "Junxian He", "Lingpeng Kong"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities in reasoning and planning by breaking down complex problems into sequential steps. Despite their success in various domains, such as mathematical problem-solving and coding, LLMs face challenges in ensuring reliable and optimal planning due to the inherent myopic nature of autoregressive decoding. This paper revisits LLM reasoning from an optimal control perspective, proposing a novel method, Predictive-Decoding, that leverages Model Predictive Control to enhance planning accuracy. By reweighting LLM distributions based on foresight trajectories, Predictive-Decoding aims to mitigate early errors and promote non-myopic planning. Our experiments show significant improvements across a wide range of tasks in math, coding, and agent-based scenarios. Furthermore, Predictive-Decoding demonstrates computational efficiency, outperforming search baselines while utilizing inference compute more effectively. This study provides insights into optimizing LLM planning capabilities. Code is available at this repo.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) are extensively pretrained on large corpus to predict the next tokens. Models like GPT-4 (Achiam et al., 2023) have demonstrated a capacity for step-by-step reasoning and planning, breaking down complex problems into sequential steps that progressively lead to solutions (Wei et al., 2022). This sequential planning capability has led to significant advancements in mathematical problem-solving (Cobbe et al., 2021) and programming tasks (Chen et al., 2021). It has also enabled essential applications such as tool use (Qin et al., 2023) and the development of LLM agents (Yao et al., 2022). Consider a scenario where an agent is tasked with cooking a meal: given an instruction like \u201cPut salt on steak\" and an initial observation such as \u201cThere is a shelf in front of you,\" the LLM assigns the highest probability to the most appropriate action. After executing this action, the LLM generates subsequent actions, each building upon the previous steps. This process creates a sequential chain of actions that advances toward task completion, guided by continuous interaction with environmental observations:\np( \u201cGo to the shelf.\u201d | \u201cThere is a shelf in front of you.\u201d)\np( \u201cPick up the saltbottle.\u201d | \u201cGo to the shelf. A saltbottle is on the shelf.\u201d),\nwhere \"A saltbottle is on the shelf\" is the new observation after going to the shelf. This iterative process continues until the specified goal state is achieved. However, this autoregressive decoding process could easily lead to irreversible errors, as LLMs tend to follow the most natural local follow-ups. For instance, given a different observation \u201cA spice bottle is on the shelf\", the LLM is likely to generate actions \"Pick up the spice bottle\" followed by \"Shake the bottle over the steak\". Such a sequence of actions totally conflicts with the intended instruction \"Put salt on the steak\". This contrasts with reinforcement learning paradigms, which use reward-based training to teach models to anticipate long-term action consequences and learn optimal decision policies (Silver et al., 2014). The short-sightedness of LLMs raises critical questions about their planning capabilities: (i) Can an LLM proactively avoid erroneous steps without necessitating their occurrence? (ii) What is the degree of optimality achievable in LLM-based planning strategies?"}, {"title": "2 PROBLEM FORMULATION: AN OPTIMAL CONTROL VIEW", "content": "One of the fundamental problems in decision-making is whether we can solve a long-term goal step by step (Qin & Badgwell, 1997). Traditionally, reinforcement learning approaches have relied on multiple iterations of training (Silver et al., 2014) and searching (Silver et al., 2017) to achieve optimal results. However, recent advancements demonstrate that LLMs possess the capability for sequential planning (Wei et al., 2022). Sequential planning can achieve optimal outcomes if non-myopic, while also offering other benefits, as shown in Table 1. In this section, we first outline the context of the problem and reconsider LLM planning from the perspective of trajectory optimization.\nPlanning as Trajectory Optimization We consider Partially Observable Markov Decision Processes (POMDPs) defined by tuple (g, S, A, T, R), with goal g, state space S, valid actions space A, transition function $T : S \\times A \\rightarrow S$, and $R : r(s, a|g) \\rightarrow R+$ is the reward function that measures how well each step aligns with the goal (with the goal dependency omitted for simplicity). Global-Optimal Planning aims to find an action sequence that maximizes cumulative reward over a set number of steps. We model this as a Trajectory Optimization process: find a sequence of actions $a_{0:T}$ that maximizes an objective $I$ (Return) factorized over per-timestep rewards $r (s_t, a_t)$ over a planning horizon with T steps:\n$\\max J (s_0, a_{0:T}) = \\sum_{t=0}^{T} r (s_t, a_t)$,\nsubject to $s_{t+1} = T(s_t, a_t), \\forall t\\in [0, T \u2013 1]$\nConversely, a Myopic Planner maximizes the return on a shorter horizon. For instance, a planner that maximizes the immediate reward at current step, i.e $a_t = \\max_{a_t}r(s_t, a_t)$, is myopic, while non-myopic planning works towards global-optimality.\nPrevious approaches to trajectory optimization follow a straightforward intuition: using large language models (LLMs) to generate diverse trajectories through sampling or search, and then selecting the best one (Wang et al., 2022; Yao et al., 2024; Hao et al., 2023; Xie et al., 2024; Shinn et al., 2024). However, a significant drawback is the exponential growth of the solution space. Each action step offers |A| possibilities, leading to $|A|^T$ potential solutions after T steps. This exponential scaling complicates the optimization process, and despite attempts to prune trajectories and accelerate search (Zhuang et al., 2023; Wang et al., 2024), most iterative planning methods are either time-consuming or perform poorly with limited iterations (Chen et al., 2024b). Additionally, iterative methods are often unsuitable for closed-loop scenarios, where agents interact with their environment in real-time, as each decision alters the environment and is irreversible.\nSequential Planning with Model Predictive Control Model Predictive Control (Qin & Badgwell, 1997) introduces a different paradigm for planning: instead of optimizing an entire action sequence at once, this method selects the best action $a_t$ at each timestep $a_0,..., a_{t-1}$, fixing each as it progresses and then optimizing the subsequent steps. This transforms Eq. 1 into a series of optimization problems, each with linear size solution space |A|. The colors denote: the current action to decide $a_t$, history,"}, {"title": "LLM Autoregressive Planning", "content": "This approach is widely adopted in LLM reasoning and planning, encompassing Chain-of-Thought (CoT; Wei et al. 2022), ReAct (Yao et al., 2022), and Voyager (Wang et al., 2023a). Research has demonstrated that with carefully designed prompts and in-context examples, LLM autoregressive planning can achieve competitive performance in various tasks (Cobbe et al., 2021; Chen et al., 2021; Shridhar et al., 2021). We further investigate whether LLM autoregressive planning is global-optimal."}, {"title": "3 DIAGNOSING THE DEFICIENCY OF LLM PLANNERS", "content": "Comparing Eq.3 with Eq.2, we observe that LLM-based action selection at time step t differs from the optimal control approach. Instead of maximizing future outcomes in the tail subproblem, LLMs select actions based on the immediate conditional probability arg max, $P_{LLM}(a_t | s_0, a_{0:t-1})$. Consequently, the optimality of LLM planning hinges on two hypotheses: (1) Non-myopic: LLMs inherently plan ahead, with current step probabilities accounting for the success of future steps. (2) Evaluation capability: The probability distribution in LLMs can effectively substitute for $I$, distinguishing successful trajectories from failed ones. In the following sections, we will examine and investigate the validity of these two hypotheses.\n3.1 FINDING 1: LLM AUTOREGRESSIVE PLANNING IS MYOPIC\nWe first examine whether LLM inherently plan ahead. Next token prediction is greedy; however previous evidence suggests that extensive pretraining could allow LLMs to implicitly plan ahead for future tokens (Wu et al., 2024a). Here we analyze the myopic gap for LLM planning.\nDefinition 3.1. (Myopic Gap for LLM Planning) Given a language model with distribution $P(a_0, a_1,...,a_T)$, let $P$ be the support set of the distribution. $a_{0:T}$ are generated autoregressively following Eq. 3. Then the myopic gap for planning is:\n$p* = \\max_{a_{0:T} \\in P} P(a_0, a_1,..., a_T) \u2013 \\hat{P}(a_0, \\hat{a}_1,..., \\hat{a}_T)$"}, {"title": "3.2 FINDING 2: LLM STRUGGLES TO IDENTIFY MISTAKES IN PLANNING EARLY", "content": "In this section, we explore another bottleneck in LLM sequential planning: Can LLMs evaluate intermediate steps and identify mistakes early on?\nWe perform an LLM score calibration analysis by comparing LLM evaluations of intermediate steps with ground truth human annotations. We collect samples of trajectories $a_{0:t}, t \\in (0,T]$ and use the LLM to evaluate these trajectories $P_{LLM}(a_0, a_1,...,a_t)$. GPT-3.5-Turbo is used to evaluate trajectories on the agent task AlfWorld, while Llama3-8B evaluates steps of the GSM8K trajectory. The LLM score for Llama3 is calculated using LLM probability, and the score for proprietary GPT-3.5 is obtained through prompt-based self-evaluation (Xie et al., 2024). Human annotators label each step in the AlfWorld trajectory with scores from 0, 0.25, 0.5, 0.75, 1, where 0 indicates that the task is unlikely to be completed and 1 indicates a high likelihood of successful completion. Each step in the GSM8K trajectory is labeled as either Correct or Incorrect following Lightman et al. (2023).\nAs shown in Table 2, GPT-3.5-Turbo could barely estimate intermediate progress simply with the current trajectory $a_{0:t}$, with only p = 0.133 correlation to human ground truth and Calibration Error 0.165. However, after given future trajectory (foresight of 3 steps), LLM evaluation improves. This is in line with previous observations (Uesato et al., 2022; Lightman et al., 2023) that LLMs are more natural at evaluating complete trajectories, rather than intermediate actions.\nThe evaluation of intermediate steps for Llama3-8B on GSM8K shows a calibration error of 0.332 when compared to human ground truth. We observe that incorporating foresight into the evaluation could help avoid early mistakes. As shown in Figure 3, the score for the first incorrect step falls within the high-density region of correct step scores but becomes easily distinguishable from correct steps after a few iterations. Consequently, the GSM8K calibration score decreases to 0.323 when foresight of three steps is applied (detailed in Table 2). However, the correlation does not improve, as the scores of positive steps slightly drops, leading to some overlap with ambiguous incorrect steps."}, {"title": "4 BEYOND MYOPIC GENERATION FOR BETTER PLANNING", "content": "Our findings presented above highlight critical issues with current LLM decoding methods, particularly their tendency toward myopic planning. Additionally, the findings suggest that incorporating global planning strategies can significantly enhance performance. By incorporating future generations, we can calibrate erroneous actions, which directly motivates our proposed method. In this section, we introduce Predictive-Decoding, which follows MPC principles to reduce myopic planning in LLMs. The pipeline is introduced in following sections and the pseudocode is presented in Algorithm 1 in Appendix A.\n4.1 PREDICTIVE-DECODING\nOur decision-making strategy draws inspiration from model predictive control (MPC) (Qin &\nBadgwell, 1997). It solves the T sub-optimization problems (Eq. 2) and addresses myopia with foresight. Here in order to ensure non-myopic planning, our main objective is to generate a\u00b4t according to:\n$a*_{t} = arg \\max_{a_{>t},s_{>t}} E_{a_{t}} P_{LLM} (a_{t}, a_{>t}, s_{>t} | a'_{<t}, s'_{<t}), \\forall a_t \\in P$,\nwhere P is the support set of distribution $P_{LLM}(a_t | a'_{t}, s'_{<t})$, and $a_{>t} \\sim P_{LLM}(. | a'_{t}, s'_{<t}, a_t)$. Note that based on results from \u00a73.2, LLMs could accurately evaluate future steps after incorporating a few steps of foresight. Therefore here we evaluate the future constrained to $T_0$ steps of foresight, i.e.\n$a_{>t} := a_{t+1:t+T_0}$\nWe use constrained decoding and sampling to apply soft constraints on outputs, promoting diverse generation and preventing overfitting to our partial ($T_0$ steps) foresight. Our method follows the sampling-importance-resampling (SIR) technique from energy-based models (EBM) (Smith &\nGelfand, 1992; Ji et al., 2023) to achieve the optimization goal.\nProposition 4.1. The distribution that solves the optimization problem in Eq.5 is in the form of:\n$Pr(a_t) \\propto P_{LLM} (a_t | a_{<t}, s_{<t}) exp [E_{a_{>t},s_{>t}} P_{LLM} (a_t, a_{>t}, s_{>t} | a_{<t}, s_{<t})/\\tau]$.\nThe detailed proof is in Appendix B.2. Specifically we first use the LLM to sample a set with size K of foresight rollouts $a_{t:t+T_0}$ given prefix $a'_{t}$ in parallel, as well as obtaining the exponential value of generation probability $w_k$ of each rollout. Then we obtain the categorical distribution Categorical (${\\frac{w_{1}}{\\sum_{k=1}^{K} w_{k}}, ..., \\frac{w_{K}}{\\sum_{k=1}^{K} w_{k}}})$. The next action step is determined as the first step in the sampled rollout from this distribution. In the limit K \u2192 \u221e and temperature \u03c4 \u2192 0, the method recovers the exact maximum value of the distribution. In practice, we can choose K according to computational budget and set \u03c4 according to whether we want the model to fit more towards the distribution or generate more diversely. The computation overhead of this method mainly stems from the tokens decoded additionally for foresight. The parallel sampling of multiple rollouts is generally fast for LLM inference infrastructures (Kwon et al., 2023).\nIn previous studies, generating text with a lookahead approach has been applied to controllable lan- guage model generation (Deng et al., 2020; Lu et al., 2021; Qin et al., 2022). These methods typically involve generating extra tokens to verify if constraints are met, accepting previous generations more likely to satisfy future constraints. Our approach builds on these work, focusing on future generations at the action level rather than the token level, better aligning with large language models' needs for reasoning and planning. While (Deng et al., 2020; Qin et al., 2022) also use EBM sampling, our method is tailored for pretrained models without additional training. Unlike controllable generation,"}, {"title": "4.2 RECYCLING TRAJECTORY ROLLOUTS", "content": "In previous work, foresight in generation has been applied for controllable tasks (Deng et al., 2020; Lu et al., 2021), typically using token-level constraints with a short lookahead of only a few tokens. In contrast, generating future actions involves creating K samples with longer foresights, which could lead to increased computation with complexity O(K$T_0$). Moreover, insufficient sampling number K can inaccurately model the distribution pr(at), while larger K is inefficient. Inspired by work on accelerating LLM inference (Fu et al., 2024), we design a memory pool to recycle sampled trajectories that could reduce the number of sampling K.\nWe observe that different sampled trajectories at different time-steps often overlap. For example, when an agent needs to heat a tomato, the trajectory starting from \u201cgo to the microwave \u2192 put the tomato in a microwave \u2192 heat the tomato with microwave"}, {"title": "5 EXPERIMENTS", "content": "In our experiments, we evaluate Predictive-Decoding in two settings: (i) without additional supervision, using only LLM self-evaluation (\u00a75.2); (ii) with an additional reward (heuristics or reward model) to analyze Predictive-Decoding's sample efficiency in trajectory optimization (\u00a75.3).\n5.1 EXPERIMENT SETTINGS\nBenchmarks Our evaluation covers three domains: math, coding, and agents. Math tasks\nGSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) are essential reasoning benchmarks,\nwhile coding tasks HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) are also closely\nrelated to reasoning ability. We also evaluate on two agent tasks AlfWorld (Shridhar et al., 2021) and\nPDDL (from Agentboard, Ma et al., 2024) to understand planning ability in closed-loop interactions.\nEvaluation Settings We use Llama 3 (Dubey et al., 2024) and Mistral-v0.3 (Jiang et al., 2023)\nfor math tasks, and Deepseek-Coder (Guo et al., 2024) for coding tasks. For agent planning\nwhich requires stronger models, we include the proprietary GPT-3.5 (Achiam et al., 2023) alongside\nLlama3.1-70B. We use vLLM inference infrastructure for efficiency (Kwon et al., 2023). To\nensure fair comparisons, we use standardized prompts from Guo et al. (2024), Gao et al. (2023),\nCobbe et al. (2021), and Ma et al. (2024). Hyperparameter and prompt details are stated in Appendix F\nFLOPS calculation Many of our experiments report computational efficiency to better illustrate\nperformance-efficiency tradeoff. We using FLOPS as the metric, Kaplan et al. (2020), i.e. FLOPS \u2248\n6nP, where P is the number of parameters in the LLM, and n the number of generated tokens. We\nrecord the average number of generated tokens per example for this calculation."}, {"title": "5.2 PREDICTIVE DECODING MAIN RESULTS", "content": "Results on Math Tasks Following (Xie et al., 2024), we use the Program Aided Language Model (PAL) format for each step, and execute the generated code in a Python environment to obtain answers. As shown in Table 3, Predictive-Decoding significantly improves problem-solving accuracy: improve by 7.2% on GSM8K and 4.3% on MATH with Llama 3, and 13.3% and 6.8% with Mistral-0.3. Beam Search achieves similar gains, which reduces myopia but requires over 3x the computation. Our method matches or exceeds Beam Search performance and can be further enhanced with Self-Consistency, outperforming both autoregressive and beam search methods in that setting. Guided-Decoding performance well using Codex, but its performance drops using Llama3 due to its overconfident confidence scores. Its performance improves when guided by a reward model (in \u00a7 5.3).\nResults on Coding Tasks Table 4 presents results for two code generation tasks evaluated using Pass@1 and Pass@10 settings. Pass@10 selects the best from 10 generations, highlighting both accuracy and diversity. Predictive-Decoding outperforms all baselines with Llama3, achieving a strong quality-diversity balance, as detailed in the subsequent temperature analysis. Interestingly, beam search shows less stability in the Pass@10 setting for coding tasks, suggesting that search-based methods may result in lower diversity in generation."}, {"title": "5.3 PLANNING AS REWARD OPTIMIZATION", "content": "Recall that Predictive-Decoding samples from a distribution to solve the trajectory optimization problem using LLM evaluation as the proxy for calculating future return. Similarly we can define such distribution for any target. The same sampling technique could maximize the objective $I$ left to right, achieving global-optimal planning. We examine how Predictive-Decoding performs with two different types of objective:\nGuiding Agent Planning with Heuristics We employ human-designed reward heuristics J to guide LLM planning, using a semantic matching score to compare the current observation to the goal state, following the approach recommended by Ma et al. (2024). The heuristic rewards are sparse, requiring multiple steps to increase rewards. As shown in Table 7, even without foresight, the reward enhances LLM agent performance. Predictive-Decoding further overcomes the sparsity with foresight and improves the utility of reward functions.\nGuiding LLM Reasoning with Reward Model We use Math-Shepherd (Wang et al., 2023b) as the reward model for GSM8K. We compare Predictive-Decoding against other reward model based planning methods, including: (1) Reward Model weighted Self-Consistency; (2) Reward Model based Ranking, selecting the top result from multiple samples; (3) Guided-Decoding; and (4) Monte Carlo Tree Search. Results in Table 8 demonstrate that all al- gorithms improve by over 10% with Math-Shepherd. Although strong baselines like Guided-Decoding and MCTS are effective, they require extensive computation. Predictive-Decoding achieves a 1.5% improve- ment over Guided-Decoding with only 65% of the computation and a 2.4% improvement over MCTS with just 50% of the computation. We further com- pare the sample efficiency of these methods in \u00a75.4."}, {"title": "5.4 DISCUSSION: INFERENCE SCALING LAW", "content": "Snell et al. (2024) recently stressed the importance of inference scaling law, which states that LLM inference could benefit from using more computation to support more expressive algorithms. Wu et al. (2024b) further discuss the efficiency-performance trade-off of different search algorithms. However, it is yet to be discussed if searching is the most efficient and effective choice for LLM reasoning.\nWe follow setting in \u00a75.3 using reward model Math-Shepherd. In Figure 6, we plot the inference scaling law of various methods and observe that: (i) Sampling-based methods have better computation efficiency compared to search-based methods. However autoregressive generation using simple reward ranking tends to saturate when more computation is available. (ii) Predictive-Decoding which directly samples from optimal solution space achieves better scaling law than all sampling and search baselines. Predictive-Decoding performance also consistently improves with more computation."}, {"title": "6 RELATED WORK", "content": "LLM-based Planning and Reasoning One major development is the emergence of LLM step-by-step reasoning abilities (Wei et al., 2022). This capability is further enhanced by more expressive inference algorithms like searching (Yao et al., 2022; 2024; Hao et al., 2023; Wang et al., 2023a; Zheng et al., 2023; Xie et al., 2024; Zhao et al., 2024; Sun et al., 2024). However, most of these algorithms are computation-intensive. Recent work discusses the inference time scaling law for LLM reasoning (Wu et al., 2024b; Snell et al., 2024). This highlights the need for an efficient yet global-optimal method. Our work attempts to close this research gap by introducing Predictive-Decoding. Another line of work studies the expressiveness of LLM reasoning. Feng et al. (2024) confirms LLM reasoning's expressiveness equals dynamic programming. Li et al. (2024) discusses Turing completeness in chain-of-thought reasoning. Our theory does not contradict their claims but examines whether any pretrained LLM could find such an optimal solution.\nModel Predictive Control Model Predictive Control (MPC) is a widely used control strategy that involves solving an optimization problem at each time step by forecasting future results (Witkin &\nKass, 1988; Qin & Badgwell, 1997). Similarly, in model-based reinforcement learning (RL), an agent aims to maximize future success by using a dynamics model to simulate state transitions (Silver et al., 2017; Ha & Schmidhuber, 2018; Anthony et al., 2017; Racani\u00e8re et al., 2017; Nagabandi et al., 2018). These algorithms enable agents to think ahead, envisioning the outcomes of various potential actions, and making deliberate selections among alternatives. Our method follows this line of work.\nCombinatorial Optimization via Sequential Sampling Sampling-based methods have been extensively used to solve combinatorial optimization efficiently (Sun & Yang, 2023; Janner et al., 2022; Qin et al., 2022; Du et al., 2024), by constructing an energy-based model where the generative probability reflects the objective. However, these methods often use non-autoregressive sampling, which undermines the Markov property of sequences critical to tasks like planning and language modeling. Autoregressive diffusion models (Wu et al., 2023; Chen et al., 2024a) have been proposed to sample based on optimization constraints while enabling causal generation. However, these methods rely on diffusion training loss and cannot be directly applied to LLMs. In this work, we follow Deng et al. (2020); Ji et al. (2023) and use the sampling-importance-resampling (SIR) technique, which enables us to maintain autoregressive next-token prediction while achieving global optimality. This method can be easily combined with model predictive control and used on any LLM."}, {"title": "7 CONCLUSIONS AND LIMITATION", "content": "In this paper, we analyzed the limitations and potential of Large Language Models (LLMs) in planning and reasoning tasks, particularly focusing on their myopic tendencies. We introduced Predictive-Decoding, which employs Model Predictive Control to enhance non-myopic planning, significantly improving performance across various domains. Our experiments confirmed Predictive-Decoding's effectiveness in boosting planning accuracy and computational efficiency. These results open promising avenues for future research into optimizing LLM reasoning and incorporating foresight into decision-making, paving the way for more robust and efficient LLM applications.\nThe main limitation of our work is that we are targeted towards improving reasoning and planning at inference times, without exploring how our method could aid in generating training data an advantage of search-based methods. Additionally, Predictive-Decoding is limited to tasks with clear reasoning steps, like math or coding. While it potentially could aid global awareness in free-form generation, such as long-context tasks, we leave this for future research."}, {"title": "A PSEUDO CODE", "content": "Input: prompt, the language model, maximum number of iterations T, sampling number K, sampling temperature \u03c4, environment env, rollout length T0.\nOutput: Action sequence $a'_0, ... a'_{T}$.\nSet $s_0$ \u2190 Initialize env, finish \u2190 False\nfor t = 1, 2, ..., T do\n\u25b7 Sample Foresight.\nInput prompt and $s'_{0}, a'_{0}, ..., s'_{t}$ to language model context;\nfor k = 1, 2, ..., K do // In parallel\nSample $a^{k}_{t+1}, s^{k}_{t+1},..., a^{k}_{t+T_0}, s^{k}_{t+T_0+1} \\sim P_{LLM}(\u00b7|context)$;\n$p_k \u2190 P_{LLM}(a^k_{t}|context)$;\n$w_k \u2190 exp (p_k/\\tau)$;\nend for\n\u25b7 Re-sample based on foresight.\nSample $j \\sim Categorical ($\\frac{w_{1}}{\\sum_{k=1}^{K} w_{k}}, ..., \\frac{w_{K}}{\\sum_{k=1}^{K} w_{k}}$).\nSet $a'_{t} \u2190 a^j_{t}$ ;\n\u25b7 Takes the action $a'_{t}$\nUpdate $s_{t+1}$, finish \u2190 Execute $a'_{t}$ in environment env;\nbreak if finish is True;\nend for\nReturn the action sequence $a'_{0}, ... a'_{T}$."}, {"title": "B THEORETICAL RESULTS", "content": "We begin by introducing a stochastic setting, where trajectory \u03c4 = (a0, a1,...,\u0430\u0442) is a chain of actions that could take values in set A. A policy model models the distribution of trajectories as \u03c0\u03bf(\u03c4), denoting the probability that the policy model samples the trajectory.\nFor the best scenario, the policy model is perfect and the sample \u03c4' ~ \u03c0\u03bf(\u03c4) maximizes the objective J(T) and T' is the best solution. However, in most cases our LLM is not the perfect policy model and prone to make errors during sampling. Instead, we sample multiple solutions T1, T2,..., Tk ~ \u03c0\u03c1(\u03c4) and selects the best solution \u03c4' = maxi=1,...,k J(Ti). A different method is using model predictive control during sampling. We thereby discuss the number of k necessary to obtain the correct \u03c4\u00b4.\nProposition B.1. Given the optimal trajectory \u03c4\u00b4 = (a\u00b4, ..., a\u00b4T) that maximizes J. T1, T2,..., Tk ~ \u03c0\u03c1(\u03c4). The best at is the first step of the trajectory \u03c4\u00b3 that maximizes J.\nIf am satisfies:\n$a_m = arg \\max_ {a_0} [\\max_ {i=1...k} J(a_0, T_{i}| T_{0} |\\rightarrow a_0)] \\\\ T_{i\\rightarrow a_0}  \\sim \\pi_\\theta (.|a_0)$"}, {"title": "5."}]}