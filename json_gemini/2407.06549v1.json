{"title": "AutoTask: Task Aware Multi-Faceted Single Model for Multi-Task Ads Relevance", "authors": ["Shouchang Guo", "Sonam Damani", "Keng-hao Chang"], "abstract": "Ads relevance models are crucial in determining the relevance between user search queries and ad offers, often framed as a classification problem. The complexity of modeling increases significantly with multiple ad types and varying scenarios that exhibit both similarities and differences. In this work, we introduce a novel multi-faceted attention model that performs task aware feature combination and cross task interaction modeling. Our technique formulates the feature combination problem as \u201clanguage\u201d modeling with auto-regressive attentions across both feature and task dimensions. Specifically, we introduce a new dimension of task ID encoding for task representations, thereby enabling precise relevance modeling across diverse ad scenarios with substantial improvement in generality capability for unseen tasks. We demonstrate that our model not only effectively handles the increased computational and maintenance demands as scenarios proliferate, but also outperforms generalized DNN models and even task-specific models across a spectrum of ad applications using a single unified model.", "sections": [{"title": "Introduction", "content": "Ads relevance modeling is a crucial application in advertising technology as it directly impacts search engine revenue, user experience, and advertiser satisfaction. It assesses whether, and how relevant, a user's search query is to ad offers and ranks the offers accordingly. The overall process starts by extracting features from query-offer pairs using NLP processors Lu et al. [2020], Zhang et al. [2022], Chen et al. [2020] and other types of feature extractors Shen et al. [2014]. We focus on the stage after feature extraction, where the extracted features are combined with a relevance model to determine the final relevance score (the probability that a query-offer pair is relevant). This modeling stage is often formulated as a classification problem, and it is worth noting that all the extracted features representations are numerical, no longer natural language, for model inputs."}, {"title": "", "content": "In our ads product, due to the diverse types of ad scenarios, varying user search behaviors, and distinct ad properties and quality assessments, we face the challenge of addressing these different ads scenarios' modeling needs at scale. The modeling aims to leverage the similarity across query/ad types and address their differences, making it a multi-task classification problem. This multi-classification issue has two specific requirements from product needs: 1) Only a single task is presented for the serving of each ads product. 2) The model needs to generalize well to unseen ads types for new product onboarding.\nMost of the standard multi-task approaches use shared network to exploit task similarity and train task specific modules with task specific data to produce tailored models for each task Liu et al. [2019], Ma et al. [2018], Zhao et al. [2018], Dai et al. [2016], Zhang et al. [2014, 2019], Bhattacharjee et al. [2022], Lopes et al. [2023]. However, this approach can be limited as:\n\u2022 Model development and maintenance efforts grow rapidly with increased number of tasks to support.\n\u2022 The model has limited capacity for generalization with the task specific training.\n\u2022 When an imbalanced amount of data for different tasks are presented for training, we need a more sophisticated model to automatically determine task importances and effectively address varying tasks.\nIn this work, we propose to model the multi-task feature combination and relevance classification as a NLP problem using language models such as Radford et al. [2019], Vaswani et al. [2017], Bahdanau et al. [2014], Brown et al. [2020]. We propose a multi-faceted model with two sets of auto-regressive attentions:\n\u2022 Task Aware Feature Modeling: We propose a new paradigm of token ID encoding to introduce a new dimension of task awareness and representations. With the new task awareness and auto-regressive attention design, the model exploits within task feature modeling and effectively addresses task differences.\n\u2022 Cross Task Interaction Modeling: We shuffle and structure the multi-task data into task blocks with a random mixture of tasks, and model the cross task interactions with auto-regressive attention to exploit task similarities. The mixture design also set the foundation for single task inference at test time.\nWith the new method proposed, we demonstrate that our single model for multi-task relevance:\n\u2022 Effectively models all tasks with superior performance compared to a generalized DNN feature combiner.\n\u2022 Presents better performance than task specific models for most task scenarios."}, {"title": "", "content": "\u2022 The token ID encoding introduced is crucial for boosting the model's capacity to generalize to unseen tasks.\n\u2022 Operates as a natural language model, therefore, it can unify feature combination with semantic information processing by accepting both extracted features and natural languages as inputs for relevance prediction.\nOur model is also light-weighted and suitable for online serving."}, {"title": "Related Works", "content": "In the field of multi-task learning, there are 3 major types of architecture designs Crawshaw [2020], Zhang and Yang [2021]. The first type involves a shared feature extractor followed by task-specific output branches or modules for each task Ma et al. [2018], Zhao et al. [2018], Dai et al. [2016], Zhang et al. [2014, 2019], Liu et al. [2019], Bhattacharjee et al. [2022], Lopes et al. [2023]. The second type features separate networks for each task and supports information exchange between parallel layers across individual task networks. This setup enhances inter-task interaction and is presented in Gao et al. [2019], Ruder et al. [2019], Misra et al. [2016].\nMost model designs produce outputs for multiple tasks at once from given inputs. There is limited work on the third type, which inferences a single task once at one time and can be used for multiple tasks Maninis et al. [2019]. The work is enabled by task-specific modules Perez et al. [2018], Rebuffi et al. [2018]. Our work falls into the this third category and is facilitated by viewing the multi-task problem as an NLP task and by introducing new designs such as task ID encoding for the modeling."}, {"title": "Multi-Faceted Single Model", "content": null}, {"title": "Problem Statement", "content": "Our ads product addresses distinctive advertising needs across various types of user searches and ads types for serving. This leads to a multi-task classification problem. While the core requirement of query-ad offer relevance modeling remains consistent across different ad scenarios, queries and ad offers for various ad types may exhibit distinct properties and data distributions.\nImportantly, in our case of multi-scenario ads ranking setup, during online inference, the relevance model receives only a predetermined single task or ad scenario, rather than having features from all tasks simultaneously inputted into the model. Therefore, the model training can utilize information from all tasks, while during inference, the model only sees one task at a time.\nThe multi-scenarios problem can be written as:\n$\\hat{\\theta} = \\arg \\min_\\theta \\sum_i L (M (\\theta; X_i), y_i)$ \t\t\t\t\t\t\t\t\t\t\t\t(1)"}, {"title": "Facet 1: Task Aware Feature Modeling", "content": "As only one predetermined single task is presented for inference, we aim to develop a robust model capable of comprehensively capturing task-specific information. To achieve this, we propose a task aware model with task ID encoding design and auto-regressive attention focusing on task specific feature modeling.\nWe propose to model the feature dimension $d$ as the sequence dimension in auto-regressive attention. However, we face the challenge of not having representative enough information for each feature in the sequence and the lack of task specific distinctions.\nTask ID Encoding To effectively represent the task specific features, we introduce a new di-"}, {"title": "", "content": "mension leveraging the task ID. For input $X_i$ of size $b \\times d$, we place task ID $i$ as the last element of feature dimension $d$ for each task $i$. We encode the task IDs of size $b \\times 1$ into a batch of one-hot vectors of size $b \\times N$, then expand the one-hot encoding across the new sequence dimension to become of size $b \\times d \\times N$. The one-hot encoding of task ID and the original input are then concatenated along the representation dimension to form the processed input of size $b \\times d \\times (N + 1)$, providing new and expanded representations for task specification and awareness.\nThe proposed task ID encoding thereby naturally forms tokens for the GPT model. We build a customized GPT model exploiting both auto-aggressive attention and tranformer architectures. The token embedding and position embedding operations convert the inputs into embeddings of size $b \\times d \\times e_1$, with sequence dimension $d$ and embedding dimension $e_1$. These embeddings are then processing by transformer layers of the GPT model to obtain transformed representations $T_1$ of size $b \\times (d * e_1)$."}, {"title": "Facet 2: Cross Task Interaction Modeling", "content": "We aim to develop a multi-task model that can handle single-task inference and can leverage cross-task information through explicit modeling of task interactions. To achieve this, we propose to use auto-regressive attention with flexible sequence length and task blocks.\nThe self-attention mechanism effectively models the similarity across tasks and aggregates the desired information for each task. Moreover, the causal nature of the auto-regressive attention alleviates the need of seeing all the tasks all at once. We structure the multi-task data into blocks with a random mixture of tasks, treat the block dimension as sequence dimension, and apply flexible sequence length for cross task attention modeling. These enable the same network for training multi-tasks and inferencing a single task.\nSpecifically, we reshape the mixture of multi-task input $X_i$ to have a shape of $\\frac{b}{t} \\times t \\times d$, where $b' = \\frac{b}{t}$ is the new batch dimension, and $t$ is the sequence block size for task interaction model-ing. We then naturally model the task blocks with another customized GPT model by treating the features as tokens for language models. With token and position embedding followed by transformers, we similarly obtain transformed representations $T_2$ of size $b \\times e_2$ for the tasks, where $e_2$ is the embedding dimension size.\nThe multi-faceted representations $T_1$ and $T_2$ focus on task aware feature modeling and task interactions, respectively. Our model further fuses the representations by concatenating them with linear layers to produce logits and scores for the classification tasks."}, {"title": "Experiments", "content": null}, {"title": "Experimental Setup", "content": null}, {"title": "Datasets", "content": "The training set consists of a large-scale sample of impressed query-ad pairs from Microsoft Advertising's search logs. We preprocess the query-ad pairs using feature extractors to obtain a consistent set of representations for 11 different ad scenarios: Hotel, Store, Tours Activity, Automobile, Credit Card, Health Insurance, Insurance Service, Cruise, Real Estate, Doctor & Clinic, and Home Service. Doctor & Clinic and Home Service are tested without any prior training.\nThe labels are annotated by humans or LLMs, using guidelines and prompts tailored to different ad scenarios and relevance levels. Given the volume of the training set, we also utilize distilled labels of relevance probability from a powerful BERT-based teacher model, similarly as in Devlin et al. [2018], Sanh et al. [2019], Jiao et al. [2019], for a large initial training set. The test data contains 258,000 samples with LLM or human labeling for each ad scenarios, and includes tasks (Clinic and Home) that are unseen during model training."}, {"title": "Training Configuration", "content": "For the model to be light-weighted for online serving, we apply only one customized transformer layer of the GPT-2 model with reduced embedding dimensions for both the task aware feature model and the task interaction model. The embedding dimensions $e_1$ and $e_2$ are set that $d * e_1 = l_2$ for an equal number of representations of $T_1$ and $T_2$ before concatenation. The model use the Adam optimizer with a learning rate of 6e-4. We normalize the feature dimension $d$ to have zero mean and standard deviation of 1 as model inputs. The model is trained initially for 30 epoch using teacher model labeled initial training set, followed by fine-tuning with LLM labeled training data for 45 epochs. The loss function $L$ is the binary cross entropy loss."}, {"title": "Comparisons and Results", "content": null}, {"title": "Baseline Comparisons", "content": "We compare the proposed approach to a production model and present ROC AUCs and PR AUCs of the models on the test set. The baseline model is a multi-task DNN model with shared fully connected layers followed by task-specific branches of fully connected layers to address the need for single task inference. The task-specific branches are optimized with task-specific data for each ad scenario. It has a generalization branch trained with data from all tasks to serve as a general model.\nThe proposed model is compared to both the general model and task-specific models of the baseline. As the task-specific models are calibrated Caruana and Niculescu-Mizil [2004] with each task's calibration data to maintain a stable relevance score distribution for online serving, we also calibrate the proposed model when comparing it to the task-specific baselines. The proposed model is compared to the general branch of the baseline without calibration. We set task ID as 0 for the unseen tasks."}, {"title": "Results", "content": "All the tables present ROC AUC on the upper panel and PR AUC on the lower panel. Table 1 presents comparison to the general model of the baseline without task-specific calibration. Our proposed method consistently outperforms the general model with higher ROC and PR AUCs. More importantly, the proposed model provides substantial gains for the unseen tasks of Clinics and Home ads. As shown in Table 2, the proposed single model provides better AUCs than most of the task-specific models, especially for the more important ad scenarios, including Hotel, Store, Tours Activity, Automobile, and Credit Card."}, {"title": "Ablation Study", "content": null}, {"title": "Ablation Results", "content": "Table 3 lists ablation study results (without calibration) for some example ad scenarios and the two unseen tasks. The \u2018No Task ID' row presents the proposed model trained without adding task ID"}, {"title": "", "content": "to the input features. The \u2018Task ID Number' represents the case where task ID numbers are added as the last element of input features for training. The \u2018Task ID Encoding' row shows the proposed approach using task ID number and task ID encoding for the multi-faceted attentions in both task interaction and feature representation modeling. Performance varies between scenarios with and without task ID numbers; each approach excels in different advertising contexts, sometimes outperforming the other and vice versa. However, the proposed encoding provides gains for most cases (including the remaining ad tasks not in the table) and demonstrates superior performance for unseen tasks."}, {"title": "Discussion", "content": "We observe that treating the task and feature modeling as language tasks with token embedding and position embedding improves the convergence during training. The design of auto-regressive attention instead plain attention without masks, and merging $T_1$ and $T_2$ representations with concatenation instead of summation, would also improve model performance. Additionally, we notice that for the multi-faceted design, the task aware feature model and the cross task interaction model exhibit slightly different optimal learning rates if trained separately. Therefore, the model may benefit from more diligent hyper-parameter tuning. With optimized learning rates and parameters, the proposed task aware feature attention model with task ID encoding might demonstrate greater capacity for desired outcomes."}, {"title": "Conclusion", "content": "We propose a new task aware, multi-faceted single model that could function effectively across a range of tasks and generalize well to unseen tasks. With the proposed task aware feature modeling and cross task interaction modeling, our model demonstrates higher performance compared to some task-specific models and provides substantial improvements for new tasks.\nWe believe that this new way of treating non-NLP multi-task modeling as language tasks and the new task aware design via task ID encoding can benefit broader multi-task applications."}]}