{"title": "Geometric Insights into Focal Loss: Reducing Curvature for Enhanced Model Calibration", "authors": ["Masanari Kimura", "Hiroki Naganuma"], "abstract": "The key factor in implementing machine learning algorithms in decision-making situations is not only the accuracy of the model but also its confidence level. The confidence level of a model in a classification problem is often given by the output vector of a softmax function for convenience. However, these values are known to deviate significantly from the actual expected model confidence. This problem is called model calibration and has been studied extensively. One of the simplest techniques to tackle this task is focal loss, a generalization of cross-entropy by introducing one positive parameter. Although many related studies exist because of the simplicity of the idea and its formalization, the theoretical analysis of its behavior is still insufficient. In this study, our objective is to understand the behavior of focal loss by reinterpreting this function geometrically. Our analysis suggests that focal loss reduces the curvature of the loss surface in training the model. This indicates that curvature may be one of the essential factors in achieving model calibration. We design numerical experiments to support this conjecture to reveal the behavior of focal loss and the relationship between calibration performance and curvature.", "sections": [{"title": "1. Introduction", "content": "In recent years, neural networks have been used successfully in many fields, including computer vision [14, 32], natural language processing [8, 9], and signal processing [12, 15]. Such successful results are grounded in the outstanding accuracy of neural networks. However, for real-world applications of machine learning algorithms, merely high accuracy is not sufficient in many cases. Especially in decision-making situations, the confidence level of the model in its own predictions is important. For example, when the output of a machine learning model is used to assist human decision-making rather than being used directly, such as in pathology classification or credit forecasting, the focus is not only on the accuracy of the model itself but also on how much the output of the model can be trusted. In many cases, model confidence is quantified by interpreting the output of the final layer activation function as a classification probability. In fact, the output vector of the softmax function is treated as a probability vector because its value range is [0, 1]. However, although the softmax output seems to be regarded as class classification probabilities, it is known that there is a discrepancy with reality. For example, even if a human observer is unsure whether an image is a dog or a cat, the machine learning model may determine that the image is a dog with a 99% probability. This phenomenon is called overconfidence [17, 31] and is known to be one of the most important issues to be solved in practical applications of machine learning algorithms.\nThe task of addressing these problems is known as model calibration [10, 23, 30]. The goal of model calibration is to ensure that the model output probabilities can be interpreted as confidence levels. There are several metrics for evaluating model calibration, including ECE and its variants, and our objective is to improve them to guarantee the validity of the machine learning model as a confidence level for the output vector of the model. Focal loss [22, 23] is one of the most popular techniques used to improve model calibration performance. It was originally proposed as a heuristic for object detection in computer vision, but its effectiveness for model calibration was later reported. There are many variants and related studies of focal loss due to its simplicity of conception and formulation [7, 19-21, 28]. The main idea of focal loss is weighting the cross-entropy according to the classification probability. Focal loss is a generalization of cross-entropy with"}, {"title": "2. Preliminary", "content": "In this section, we provide the preliminary knowledge and background required for our discussion.\nThe output of the softmax function employed in the output layer of recent neural networks for class classification is often regarded as a vector of probabilities that the input vector belongs to the respective class. The value of this softmax output is often also interpreted as the confidence of the model with respect to the input. However, it is known that these values differ significantly from the actual expected confidence of the models. This phenomenon is called overconfidence [17, 31] and is one of the major challenges to the adoption of machine learning algorithms in decision making. For example, when the output of a machine learning model is used to assist human decision-making rather than being used directly, such as in pathology classification or credit forecasting, the focus is not only on the accuracy of the model itself but also on how much the output of the model can be trusted.\nThe task of ensuring that the output of a machine learning model and its confidence level are in line with the actual conditions is called model calibration [6, 10, 16]. The goal of model calibration is to ensure that the estimated class probabilities are consistent with what would naturally occur:\n$\\mathbb{P}\\left(\\arg \\max _{y \\in \\mathbb{Y}} p(y \\mid x ; \\theta)=y \\mid p(y \\mid x ; \\theta)=s\\right)=s,$\nfor all $s \\in [0,1]$ and $x \\in \\mathbb{X}$, where $\\mathbb{X}$ is an input space, $\\mathbb{Y}$ is an output space and $p(\\cdot \\mid \\cdot ; \\theta)$ is a model parameterized by $\\theta \\in \\Theta$ with a parameter space $\\Theta$. The most popular calibration performance metric is the Expected Calibration Error (ECE) [24], which we also use in this study.\nDefinition 1 (Expected Calibration Error [24]). For $(x, y)$, let $\\hat{y} \\triangleq \\arg \\max _y p(y \\mid x ; \\theta)$. The expected calibration error is defined as\n$E C E(\\theta):=\\mathbb{E}[\\mid \\mathbb{P}(\\hat{y}=y \\mid p(y \\mid x ; \\theta)=s)-s \\mid ]$,\nwhere s is the prediction probability.\nIt is known that improvements in ECE can lead to achieving a confidence of the model that is in line with human intuition. One technique known to be effective in achieving model calibration is the following focal loss [22, 23].\nDefinition 2 (Focal Loss [22]). For some input vector $x \\in \\mathbb{X}$ and $\\gamma \\geq 0$, the focal loss is defined as follows.\n$L_{F L}(\\theta ; x, \\gamma):=-\\sum_{y=1}^{\\mathcal{Y}}\\left(1-p(y \\mid x ; \\theta)\\right)^{\\gamma} \\ln p(y \\mid x ; \\theta) q(y \\mid x)$,\nwhere $q(y \\mid x)$ is the ground truth probability."}, {"title": "3. Geometric Reinterpretation of Focal Loss", "content": "In this study, we analyze its behavior using the geometrical reinterpretation of focal loss. In particular, we show that focal loss behaves as a reduction in the curvature of the loss surface. This result is confirmed by both the reformulation of focal loss as an optimization under the entropy constraint and the Taylor expansion of focal loss. Moreover, based on our analysis and the reports of existing studies that focal loss is effective for model calibration, we can expect that curvature reduction is one of the essential factors for calibration. This conjecture suggests that regularization that explicitly reduces the curvature of the loss surface may be useful in improving model calibration performance. Then, we design numerical experiments to support the above conjecture and investigate the behavior of focal loss.\nOur contributions are summarized as follows:\n\u2022 We reinterpret focal loss geometrically and show that it behaves as the reduction of curvature (Theorem 2).\n\u2022 We provide the conjecture of the connection between curvature and calibration performance (Conjecture 1 and Fig. 3).\n\u2022 We design and conduct the numerical experiments to demonstrate our theoretical hypothesis (Fig. 2).\nFinally, this paper is organized as follows: we provide the preliminary knowledge and background required for our discussions in Section 2, we derive the analysis of the focal loss from the perspective of geometry in Section 3, we design and conduct the numerical experiments in Section 4 and we provide the conclusion and discussion in Section 5."}, {"title": "3.1. Focal Loss as Curvature Reduction", "content": "In this study, we reinterpret focal loss geometrically and investigate its behavior. First, we consider the following lower bound for focal loss.\nLemma 1. For $\\gamma \\geq 0$, we have\n$L_{F L}(\\theta ; x, \\gamma) \\geq L_{C E}(\\theta ; x)-\\gamma H(y \\mid x, \\theta)$,\nwhere $L_{C E}(\\theta ; x, \\gamma)=-\\sum_{y=1}^{\\mathcal{Y}} q(y \\mid x) \\ln p(y \\mid x ; \\theta)$ is the cross-entropy loss and $H(y \\mid x, \\theta)$ is the conditional entropy.\nProof. From Definition 2, we have\n$L_{F L}(\\theta ; x, \\gamma)=-\\sum_{y=1}^{\\mathcal{Y}}\\left(1-p(y \\mid x ; \\theta)\\right)^{\\gamma} \\ln p(y \\mid x ; \\theta) q(y \\mid x)$\n$\\geq-\\sum_{y=1}^{\\mathcal{Y}}(1-\\gamma p(y \\mid x ; \\theta)) \\ln p(y \\mid x ; \\theta) q(y \\mid x)$\n$\\geq-\\sum_{y=1}^{\\mathcal{Y}} q(y \\mid x) \\ln p(y \\mid x ; \\theta)$\n$\\geq-\\gamma \\max _k q(k \\mid x) \\sum_{y=1}^{\\mathcal{Y}}\\left|p(y \\mid x ; \\theta) \\ln p(y \\mid x ; \\theta)\\right|$\n$\\geq L_{C E}(\\theta ; x)-\\gamma H(y \\mid x, \\theta)$.\nHere, the first inequality follows from Bernoulli's inequality and the second from H\u00f6lder's inequality."}, {"title": "From Eq. 4, learning procedure with focal loss can be regarded as maximizing conditional entropy term under the constraint", "content": "$\\int_{\\Theta} p(\\theta) L_{C E}(\\theta ; x) d \\theta \\geq \\delta_{\\gamma}<\\delta$,\nfor some $\\delta>0$ and $\\delta_{\\gamma} \\geq 0$.\nTheorem 1. Among all distributions defined on $\\Theta$ with a given $\\delta_{\\gamma}$, the distribution with the largest entropy is the Maxwell-Boltzmann distribution\n$\\hat{p}(\\theta)=\\alpha \\cdot e^{-\\beta \\cdot L_{C E}(\\theta)}$\n$\\alpha \\cdot \\exp \\left[-\\beta \\cdot \\sum_{y=1}^{\\mathcal{Y}} q_{y \\mid x} \\ln p(y \\mid x ; \\theta) d x\\right]$\nwhere the constants $\\alpha$ and $\\beta$ are determined from the following constraints\n$\\int_{\\Theta} p(\\theta) d \\theta=1 \\Leftrightarrow \\alpha=\\left(\\int_{\\Theta} e^{-\\beta \\cdot L_{C E}(\\theta)} d \\theta\\right)^{-1}$,\n$\\int_{\\Theta} L_{C E}(\\theta) p(\\theta) d \\theta=\\delta_{\\gamma} \\Leftrightarrow \\int_{\\Theta} L_{C E}(\\theta) e^{-\\beta \\cdot L_{C E}(\\theta)} d \\theta=\\frac{\\delta_{\\gamma}}{\\alpha}$.\nProof. From chain rule, the conditional entropy is written as\n$H(y \\mid x ; \\theta)=H(y, \\theta \\mid x)-H(\\theta)$.\nThen, maximizing conditional entropy $H(y \\mid x ; \\theta)$ is equal to minimizing entropy $H(\\theta)$. In order to minimize the entropy $H(\\theta)=-\\int_{\\Theta} p(\\theta) \\ln p(\\theta) d \\theta$ subject to constraints\n$\\int_{\\Theta} p(\\theta) d \\theta=1$\n$\\int_{\\Theta} p(\\theta) L_{C E}(\\theta) d \\theta=\\delta_{\\gamma}$,\nconsider the Lagrangian\n$\\mathcal{L}=-p(\\theta) \\ln p(\\theta)-\\beta L_{C E}(\\theta) p(\\theta)-\\eta p(\\theta)$.\nThe Euler-Lagrange equation [5] for Eq.11 is\n$\\ln p(\\theta)=\\beta L_{C E}(\\theta)-\\eta+1$,\nwith solution\n$p(\\theta)=\\alpha \\cdot e^{-\\beta \\cdot L_{C E}(\\theta)}$,\n$\\alpha=e^{1-\\eta}$.\nFrom Eq. 13 and Eq. 14, we have\n$\\int_{\\Theta} e^{-\\beta \\cdot L_{C E}(\\theta)} d \\theta \\int_{\\Theta} L_{C E}(\\theta) e^{-\\beta \\cdot L_{C E}(\\theta)} d \\theta \\stackrel{?}{=}\\delta_{\\gamma}$.\nConsider\n$\\Phi(\\beta)=\\int_{\\Theta} e^{\\beta L_{C E}(-\\theta)} d \\theta \\int_{\\Theta} L_{C E}(\\theta) e^{-\\beta L_{C E}(\\theta)} d \\theta$."}, {"title": "which is an decreasing function of B. Since", "content": "$\\lim _{\\beta \\rightarrow+\\infty} \\Phi(\\beta)=0$,\n$\\lim _{\\beta \\rightarrow 0} \\Phi(\\beta)=+\\infty$,\nthe continuity of $\\Phi(\\beta)$ implies that Eq. 15 has a solution and this is unique. Hence, a unique pair $(\\alpha, \\beta)$ satisfies the constraints of the problem. Therefore, the Maxwell-Boltzmann distribution is unique.\nThis can be regarded as the learning process by focal loss trying to learn a model with parameters that do not deviate too much from the submanifold composed of the family of distributions expressed in Eq. 7. Using Eq. 7 as the prior of parameter distribution as $\\pi=\\hat{p}$, we give the following PAC-Bayes bound by using Thiemann's bound[29].\nCorollary 1. Maxwell-Boltzmann distribution in Eq. 7 induces the following bound.\n$\\mathbb{P}\\left[\\mathbb{E}_{\\theta \\sim \\varsigma} \\mathbb{E}_{\\mathbb{P}(\\Theta)}\\left[\\hat{R}(\\theta)\\right] \\leq J(\\varsigma, \\lambda)+\\sqrt{\\frac{D_{K L}[\\varsigma \\mid \\mid \\pi]+\\ln 2}{\\eta \\lambda(1-\\lambda)}}\\right] \\leq \\epsilon$\nfor some $\\epsilon>0$ and $\\lambda \\geq 0$, where $J(\\varsigma, \\lambda)=\\mathbb{E}_{\\theta \\sim \\varsigma}[\\hat{R}(\\theta)], n$ is the sample size of training data and $\\hat{R}(\\theta), \\hat{R}(\\theta)$ are expected and empirical error, respectively. Here, $D_{K L}[\\varsigma \\mid \\mid \\pi]=\\int \\varsigma(\\theta) \\ln (\\varsigma(\\theta) / \\pi(\\theta)) d \\theta$ is the Kullback\u2013Leibler divergence (or KL-divergence). The minimum of the right-hand side of this bound is achieved by\n$\\varsigma_{\\lambda}(\\theta)=\\frac{\\pi(\\theta) e^{-\\lambda \\hat{R}(\\theta)}}{\\mathbb{E}_{\\theta \\sim \\pi}\\left[e^{-\\lambda \\hat{R}(\\theta)}\\right]}$ (fixed $\\lambda$),\n$\\lambda=\\sqrt{\\frac{2}{n} \\frac{2}{\\mathbb{E}_{\\theta \\sim \\varsigma}[\\hat{R}(\\theta)]}}\\left(\\frac{D_{K L}[\\varsigma \\mid \\mid \\pi]+\\ln 2}{\\epsilon}+1\\right)-1$ (fixed $\\varsigma$).\nThis upper bound can be decomposed into an expected error term and a KL-divergence term. We can see that if the estimators are consistent, the first term approaches a minimum as $n \\rightarrow \\infty$, and the second term also vanishes because the denominator includes the sample size. On the other hand, for a fixed $n$, the bound can be made tighter by reducing the KL-divergence in the second term. This induces the following regularizer naturally.\nCorollary 2. Focal Loss equips the following regularizer:\n$D_{K L}[\\varsigma \\mid \\mid \\pi]=\\int \\varsigma(\\theta) \\ln \\frac{\\varsigma(\\theta)}{\\pi(\\theta)} d \\theta=\\int \\varsigma(\\theta) \\ln \\frac{\\varsigma(\\theta)}{c \\cdot \\exp \\left[\\int_{\\mathbb{X}} \\int_{\\mathbb{Y}} q(x, y) \\ln p(y \\mid x ; \\theta) d x\\right]} d \\theta$.\nKL-divergence for small changes in parameters can be expressed using the Fisher information matrix. Here, it is known that the space composed of probability distributions is a Riemannian manifold with parameters as the coordinate system, from the framework of information geometry [1, 2]. On this Riemannian manifold, the Fisher information matrix behaves as a metric, characterizing the curvature of the manifold. From the above discussion, we can characterize the geometric behavior of focal loss as follows.\nTheorem 2. Learning with the focal loss reduces the local sharpness of the likelihood, if prior and posteriors are close enough.\nProof. From assumption, let\n$\\Delta \\xi_{i}=\\xi-\\xi^{\\prime}$,\nwhere $\\xi$ and $\\xi^{\\prime}$ are parameters of $\\varsigma$ and $\\pi$. Consider the quadratic approximation of $d_{K L}(\\xi)=D_{K L}[\\varsigma|| \\xi]$ as\n$d_{K L}(\\xi)=d_{K L}\\left(\\xi^{\\prime}\\right)+\\sum_{i} \\frac{\\partial d_{K L}}{\\partial \\xi_{i}}\\left(\\xi^{\\prime}\\right) \\Delta \\xi_{i}$\n$\\frac{1}{2} \\sum_{i, j} \\frac{\\partial^{2} d_{K L}}{\\partial \\xi_{i} \\partial \\xi_{j}}\\left(\\xi^{\\prime}\\right) \\Delta \\xi_{i} \\Delta \\xi_{j}-o(\\mid \\mid \\Delta \\xi \\mid \\mid^{2})$."}, {"title": "First, since DKL [P||q] = 0 if and p = q, we have", "content": "$d_{K L}\\left(\\xi^{\\prime}, \\xi^{\\prime}\\right)=D_{K L}\\left[\\varsigma^{\\prime}|| \\xi^{\\prime}\\right]=0$.\nNext, diagonal part of the first variation of the KL-divergence is\n$\\frac{\\partial}{\\partial \\xi_{i}} D_{K L}\\left[\\varsigma|| \\xi\\right]=\\int \\varsigma(\\theta) \\frac{\\partial}{\\partial \\xi_{i}} \\ln p(\\theta ; \\xi) d \\theta$\n$=-\\frac{\\partial}{\\partial \\xi_{i}} D_{K L}\\left[\\varsigma|| \\xi\\right]=-\\int \\varsigma(\\theta) \\frac{\\partial}{\\partial \\xi_{i}} \\ln p(\\theta ; \\xi^{\\prime}) d \\theta$\n$=-\\int \\frac{\\varsigma(\\theta)}{p(\\theta ; \\xi^{\\prime})} \\frac{\\partial}{\\partial \\xi_{i}} p(\\theta ; \\xi^{\\prime}) d \\theta$\n$=-\\mathbb{E}_{\\xi^{\\prime}}\\left[\\frac{\\partial}{\\partial \\xi_{i}} \\ln p(\\theta ; \\xi)\\right]$\n$=-\\frac{\\partial}{\\partial \\xi_{i}} \\ln p(\\theta ; \\xi^{\\prime})=0$.\nFinally, the diagonal part of the Hessian of the KL-divergence is\n$\\frac{\\partial^{2}}{\\partial \\xi_{i} \\partial \\xi_{j}} D_{K L}\\left[\\varsigma|| \\xi\\right]=\\int \\varsigma(\\theta) \\frac{\\partial^{2}}{\\partial \\xi_{i} \\partial \\xi_{j}} \\ln p(\\theta ; \\xi) d \\theta$\n$\\int \\varsigma(\\theta) \\frac{\\partial^{2}}{\\partial \\xi_{i} \\partial \\xi_{j}} \\ln p(\\theta ; \\xi) d \\theta$\n$=-\\int \\frac{\\partial}{\\partial \\xi_{i}} \\left(\\frac{\\varsigma(\\theta)}{p(\\theta ; \\xi^{\\prime})}\\right) \\frac{\\partial}{\\partial \\xi_{j}} p(\\theta ; \\xi) d \\theta$\n$\\frac{\\partial^{2}}{\\partial \\xi_{i} \\partial \\xi_{j}} D_{K L}\\left[\\varsigma|| \\xi\\right]=-\\int \\frac{\\partial}{\\partial \\xi_{i}} \\frac{\\varsigma(\\theta)}{p(\\theta ; \\xi^{\\prime})} \\frac{\\partial}{\\partial \\xi_{j}} p(\\theta ; \\xi) d \\theta$\n$=-\\mathbb{E}_{\\xi^{\\prime}}\\left[\\frac{\\partial^{2}}{\\partial \\xi_{i} \\partial \\xi_{j}} \\ln p(\\theta ; \\xi)\\right]$\n$=g_{i j}\\left(\\xi^{\\prime}\\right)$,\nwhere $I(\\xi)=\\left(I_{i j}(\\xi)\\right)$ is the Fisher information matrix. Then, we have\n$D_{K L}[\\varsigma|| \\xi]=\\frac{1}{2} \\sum_{i, j} g_{i j}\\left(\\xi^{\\prime}\\right) \\Delta \\xi_{i} \\Delta \\xi_{j}+o(\\mid \\mid \\Delta \\xi \\mid \\mid^{2})$.\nFrom Eq. 26 and Eq. 18,\n$\\mathbb{P}_{\\xi \\sim \\varsigma} \\mathbb{E}_{\\mathbb{P}(\\Theta)}, \\mathbb{E}_{\\theta \\sim \\varsigma}[\\hat{R}(\\theta)] \\leq J(\\varsigma, \\lambda)+\\sqrt{\\frac{\\frac{1}{2} \\sum_{i, j} g_{i j}\\left(\\xi^{\\prime}\\right) \\Delta \\xi_{i} \\Delta \\xi_{j}+\\ln 2}{\\eta \\lambda(1-\\lambda)}}<\\epsilon$\nwith $o(\\mid \\mid \\Delta \\xi \\mid \\mid^{2})$, and the second term of the right-hand side measures the curvature of the log-likelihood.\nHere, we can also relate focal loss to curvature from another path. The Taylor expansion of the negative log-likelihood function $l(\\theta)=p_{\\theta}:=-\\ln p(\\cdot ; \\theta)$ around the maximum likelihood estimator $\\theta_{0}$ is given by\n$l(\\theta) \\approx l\\left(\\theta_{0}\\right)+\\left.\\frac{1}{2}\\left(\\theta-\\theta_{0}\\right)^{T} \\frac{\\partial^{2}}{\\partial \\theta^{2}}\\left(\\theta-\\theta_{0}\\right)\\right|_{\\theta_{0}}$\nand second term corresponds to the curvature around $\\theta_{0}$, and is the Fisher information matrix. In addition, the expansion of $l_{F L}(\\theta)=-\\left(1-p_{\\theta}\\right)^{\\gamma} \\ln p_{\\theta}:=-\\left(1-p(\\cdot ; \\theta)\\right)^{\\gamma} \\ln p(\\cdot ; \\theta)$ yields the following curvature term:\n$\\left.\\frac{\\left(1-p_{\\theta_{0}}\\right)^{n}}{2} \\frac{\\partial^{2}}{\\partial \\theta^{T} \\partial \\theta} l(\\theta)\\left(\\theta-\\theta_{0}\\right)\\right|_{\\theta_{0}}$.\nSince $\\left(1-p_{\\theta_{0}}\\right) \\geq 0$ and $\\gamma \\geq 0$, the role of the parameter $\\gamma$ is to control the curvature. In fact, the larger $\\gamma$ makes the curvature term smaller. Theorem 2 and Eq. 28 both suggest that focal loss behaves as curvature reduction from different approaches. Therefore, we can see that"}, {"title": "3.2. Characterization of Curvature", "content": "There are several approaches to characterizing the curvature of loss surfaces. The most straightforward are the Hessian $H_{\\theta}$ of a function and its contractions. As contractions, traces $\\operatorname{Tr}\\left(H_{\\theta}\\right)$ and maximum eigenvalues $\\Lambda_{\\max }\\left(H_{\\theta}\\right)$ are often used. As another characterization of curvature, the following concept of continuity can be utilized.\nDefinition 3 (Lipschitz continuity [27]). A function $f: \\mathbb{X} \\rightarrow \\mathbb{R}$ is called Lipschitz continuous on $\\mathbb{X}$ with Lipschitz constant $L$ if\n$\\mid f(u)-f(v) \\mid \\leq L \\mid \\mid u-v \\mid \\mid \\forall u, v \\in \\mathbb{X}$.\nIn general, if the curvature is small, it means that changes in the neighborhood are relatively small. This leads to a small Lipschitz constant.\nFurther, consider the gradient norm $\\mid \\mid \\nabla f \\mid \\mid$. This quantity can be regarded as quantifying the speed of change of the function, which means that the gradient norm is small when the Lipschitz constant is small. Here, it is important to note the effect of the focal loss parameter $\\gamma$ on its gradient. Let $g(p, \\gamma)$ be the gradient of focal loss with $\\gamma$, and its explicit form can be written as\n$g(p, \\gamma)=\\left(1-p\\right)^{\\gamma}\\left(\\gamma p\\left(1-p\\right)^{\\gamma-1}-\\left(1-p\\right)^{\\gamma} \\ln p\\right)$,\nwhere p is the classification probability. Consider the left-hand limit of $g(p, \\gamma)$ for p and $\\gamma$ as\n$\\lim _{p \\rightarrow 1^{-}} g(p, \\gamma)=\\left\\{\n0 \\quad(\\gamma=0 \\text { or } 0.5<\\gamma),\n0.5 \\quad(\\gamma=0.5),\n+\\infty \\quad(0<\\gamma<0.5)\n\\right.$,\nand\n$\\lim _{\\gamma \\rightarrow 0.5} g(p, \\gamma)=0.5 p-\\ln p+\\gamma \\ln p$.\nThis can be confirmed numerically from Fig. 1. Thus, it can be seen that $\\gamma>0.5$ is suggested to be desirable. In our numerical experiments, the hyperparameters are configured according to this suggestion."}, {"title": "4. Numerical Experiments", "content": "In this section, we design numerical experiments to answer the following questions and confirm our results.\ni) Does focal loss behave as curvature reduction? (Fig. 2)\nii) Can we find a connection between curvature and model calibration performance? (Fig. 3)\niii) Is explicit curvature regularization effectively improving calibration? (Fig. 4)\n4.1. Focal Loss and Curvature Reduction\nFigure 2 shows the changes in the maximum eigenvalue of the Hessian, $\\Lambda_{\\max }\\left(H_{\\text {val }}\\right)$, with respect to the hyperparameter $\\gamma$ in focal loss training. Here, $H_{\\text {val }}$ denotes the Hessian of the validation loss. As predicted by our theoretical analysis (Theorem 2), $\\Lambda_{\\max }\\left(H_{\\text {val }}\\right)$ consistently decreases across all model architectures as $\\gamma$ increases. This confirms that focal loss indeed reduces the curvature of the loss surface.\n4.2. Curvature and Calibration Performance\nFigure 3 explores the relationship between the trace of the Hessian, $\\operatorname{Tr}\\left(H_{\\text {val }}\\right)$, and the Expected Calibration Error (ECE) during focal loss training on CIFAR-100 [18] with various DNN architectures\u00b9. Data points with the same color represent experiments using different values of $\\gamma$. The results show that for all network architectures, ECE achieves a minimum value when $\\operatorname{Tr}\\left(H_{\\text {val }}\\right)$ is reduced to a certain extent, rather than when it's maximized. However, excessively low values of $\\operatorname{Tr}\\left(H_{\\text {val }}\\right)$ also lead to performance degradation in ECE. This suggests that applying an appropriate degree of regularization to reduce the curvature (reflected by $\\operatorname{Tr}\\left(H_{\\text {val }}\\right)$) is crucial for achieving optimal calibration performance.\n4.3. Explicit Curvature Regularization\nFocal loss has been demonstrated to improve ECE by reducing curvature to a certain extent. A natural question arises: can we achieve similar improvements in ECE by explicitly minimizing curvature? To address this question, we introduce an experimental protocol based on explicit curvature regularization via Hessian trace minimization. If explicit regularization of Hessian trace leads to improvement in ECE, then this supports the conjecture that curvature is one of the key factors in model calibration performance.\nTo implement this approach, we incorporate a regularization term into the loss function that penalizes large values of"}, {"title": "5. Conclusion and Discussion", "content": "In this study, we provided the analysis of focal loss from a geometrical point of view. First, we reformulated the learning process by focal loss, suggesting that its behavior is like regularization with curvature reduction. This reformulation allows us to view focal loss as restricting the parameter search to the neighborhood of the submanifold created by the probability distribution family that maximizes entropy. This consequently leads to equivalence with regularization such that the curvature does not increase during optimization. We also demonstrated that such a transformation is supported by the asymptotic expansion around the maximum likelihood estimator of focal loss. Based on these arguments, we then conjecture that the curvature of the function is one of the key factors for model calibration.\nTo empirically validate these theoretical insights, we conducted a series of numerical experiments to probe the relationships between focal loss, curvature reduction, and model calibration performance. Our findings demonstrate that controlling curvature, implicitly through focal loss or explicitly through direct regularization, significantly enhances model calibration. This emphasizes the pivotal role of curvature considerations in achieving well-calibrated machine learning models.\nIn conclusion, our analysis and experimental results agree that effective curvature control, whether implicit or explicit, is indispensable for optimizing model calibration. This study deepens our understanding of focal loss from a geometrical perspective and enriches our understanding of the calibration of deep neural networks.\nFinally, we suggest the potential impact of this study by listing the following open questions.\n\u2022 Effect of other calibration algorithms on curvature. This study suggested inductively that focal loss can control curvature and that curvature control leads to improved model calibration. The next question is whether other model calibration algorithms control curvature in the same way."}]}