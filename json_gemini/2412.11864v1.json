{"title": "Investigating Mixture of Experts in Dense Retrieval", "authors": ["Effrosyni Sokli", "Pranav Kasela", "Georgios Peikos", "Gabriella Pasi"], "abstract": "While Dense Retrieval Models (DRMs) have advanced In-\nformation Retrieval (IR), one limitation of these neural models is their\nnarrow generalizability and robustness. To cope with this issue, one can\nleverage the Mixture-of-Experts (MoE) architecture. While previous IR\nstudies have incorporated MoE architectures within the Transformer lay-\ners of DRMs, our work investigates an architecture that integrates a sin-\ngle MoE block (SB-MOE) after the output of the final Transformer layer.\nOur empirical evaluation investigates how SB-MoE compares, in terms\nof retrieval effectiveness, to standard fine-tuning. In detail, we fine-tune\nthree DRMs (TinyBERT, BERT, and Contriever) across four benchmark\ncollections with and without adding the MoE block. Moreover, since MoE\nshowcases performance variations with respect to its parameters (i.e., the\nnumber of experts), we conduct additional experiments to investigate\nthis aspect further. The findings show the effectiveness of SB-MoE espe-\ncially for DRMs with a low number of parameters (i.e., TinyBERT), as\nit consistently outperforms the fine-tuned underlying model on all four\nbenchmarks. For DRMs with a higher number of parameters (i.e., BERT\nand Contriever), SB-MoE requires larger numbers of training samples to\nyield better retrieval performance.", "sections": [{"title": "1 Introduction", "content": "Neural Information Retrieval (IR) models, including several Dense Retrieval\nModels (DRMs), have shown a potential to enhance retrieval performance com-\npared to sparse lexicon-based models such as BM25 [24]. DRMs can be trained to\ncapture the semantic context of queries and documents [22]. Nonetheless, their\ntraining requires large labeled datasets and there is a trade-off between gener-\nalizability and task-specific performance since DRMs often struggle to robustly\nadapt to different tasks or domains without additional fine-tuning.\nIn this paper, we investigate how an enhanced bi-encoder DRM architecture\nleveraging Mixture-of-Experts (MoE) [11] performs compared to the original un-\nderlying model across various dense retrieval scenarios. Since MoE consists of"}, {"title": "2 Related Work", "content": "DRMs often outperform lexicon-based models (e.g., BM25 [24]), since they can\ncapture the semantic context of queries and documents by projecting them in\na shared dense vector space and leverage similarity functions to score the docu-\nments according to an input query [7,14,32]. We leverage three DRMs, namely\nContriever [10], BERT [5], and TinyBERT [12]. Contriever is a state-of-the-\nart BERT-based model that exploits contrastive learning, a Machine Learning\ntechnique that uses pairs of positive and negative examples to learn meaningful\nand distinctive representations of queries and documents. TinyBERT leverages\nknowledge distillation [25] to transfer knowledge from its larger counterpart,\nBERT, to a tinier version, reducing training times and computational expenses.\nA common downside of DRMs is their continuous adaptation needs, which\noften leads to low generalizability and overall robustness [20,28]. To that re-\ngard, numerous MoE [11] approaches have been proposed in the literature, as"}, {"title": "3 Methodology", "content": "SB-MoE builds upon a bi-encoder DRM architecture [23], which allows for inde-\npendent encoding of documents and queries to enhance scalability and to enable\nthe computation of relevance scores through a similarity function (e.g., cosine\nsimilarity). While it is possible to employ separate encoders [15], using a single\nencoder for both queries and documents improves robustness, without signifi-\ncantly affecting performance [10,23].\nFigure 1 presents the SB-MoE architecture, which consists of three main parts:\n(1) the query and document level employed experts, which are added after the\noutput of the final Transformer layer of the underlying bi-encoder; (2) the gating\nfunction, which is trained in an unsupervised manner to combine the experts'\noutput for a given input; and (3) the pooling module used in the final stage to\naggregate the experts' representations and produce the final embedding to be\nused to measure similarity between the query and documents.\nThe experts receive the input embedding directly from the underlying model\n(Fig. 1) and apply a series of transformations. The output is n modified represen-"}, {"title": "4 Experimental Analysis", "content": "This section presents the empirical evaluation conducted to answer the following\nresearch questions (RQs):\nRQ1 How does SB-MoE compare, in terms of effectiveness, to standard model\nfine-tuning?\nRQ2 How does the number of experts impact the retrieval effectiveness of\nSB-MoE?"}, {"title": "4.1 Experimental Settings.", "content": "Datasets. In our experiments, we use four publicly available datasets: (i) Natural\nQuestions (NQ) [17], an open domain Q&A dataset based on a corpus consisting\nof 2.6 million Wikipedia passages, with 132k training queries with 1.2 average\nrelevance judgments per query and 3.5k test queries; (ii) HotpotQA [31], a Q&A\ndataset with multihop questions that also relies on a Wikipedia-based corpus of\n5.2 million documents, with 85k training queries with 2 average relevance judg-\nments per query and 7.4k test queries; and (iii) the Multi-Domain Benchmark\nproposed by Bassani et al. [1], from which we exploit the Political Science (PS)\nand Computer Science (CS) domains for the task of domain-specific academic\nsearch. Both collections consist of 4.8 million documents, with PS, containing\n160k training queries with an average of 3.8 relevance judgments per query and\n5.7k testing queries, while CS contains 550k training queries with an average\nof 3.25 relevance judgments per query and 6.5k testing queries. We use these\ndatasets with distinct characteristics to evaluate SB-MoE under different condi-\ntions and study its consistency across various training sample sizes and tasks.\nSB-MoE and Training Hyper-parameters. For RQ1 we employ 6 distinct experts\nfor all models and datasets, and for RQ2 we vary the number of experts in\nthe range of 3-12. Our selection is based on previous studies on the impact of\nthe number of experts on model performance [19,33], which reveal that a high"}, {"title": "4.2 Results and Discussion", "content": "RQ1. As presented in Table 1, SB-MoE exhibited a noticeable improvement in\nterms of NDCG@10 and Recall@100, especially with models having fewer pa-\nrameters. For instance, on TinyBERT, SB-MoE leads to consistent performance\ngains in both metrics across all datasets, with a marked increase in HotpotQA,\nwhere SB-MOEALL achieved an NDCG@10 score of .171 compared to .158 of the\nfine-tuned version. However, for larger models like BERT and Contriever, the\nintegration of SB-MoE had a marginal impact. For instance, on HotpotQA with\nBERT, SB-MoE achieved similar or slightly worse retrieval performance compared\nto Fine-tuned. These results suggest that in models already equipped with a sub-\nstantial number of parameters, SB-MoE's advantages may not be so prominent,\npotentially due to redundancy when additional experts are employed. Therefore,\nthe integration of SB-MoE particularly benefits lightweight models.\nRQ2. As SB-MoE seems to benefit significantly lightweight models, we leverage\nTinyBERT to understand the impact of the number of experts, by configuring\nSB-MoE with 3, 6, 9, and 12 experts and evaluating across all datasets. This\nanalysis is presented in Figure 2. Our investigation revealed that performance\ngains with different numbers of experts vary depending on the dataset, indicating\nthat optimization is collection-dependent. Moreover, variations in the number\nof experts of SB-MoE can lead to the maximization of different performance\nmeasures, as observed in the case of NQ, where the employment of 12 experts\nmaximizes NDCG@10, but Recall@100 is maximized with 9 experts. Therefore,\nour findings indicate that the number of employed experts is a hyper-parameter\nthat requires tuning with respect to the domain and the addressed retrieval task."}, {"title": "5 Conclusions", "content": "In this work, we conduct an experimental investigation on the effectiveness of in-\ntegrading a single Mixture-of-Experts block (SB-MoE) into Dense Retrieval Mod-\nels (DRMs) to examine its potential in neural IR. Results show that SB-MoE"}]}