{"title": "Bidirectional Gated Mamba for Sequential Recommendation", "authors": ["Ziwei Liu", "Qidong Liu", "Yejing Wang", "Wanyu Wang", "Pengyue Jia", "Maolin Wang", "Zitao Liu", "Yi Chang", "Xiangyu Zhao"], "abstract": "In various domains, Sequential Recommender Systems (SRS) have become essential due to their superior capability to discern intricate user preferences. Typically, SRS utilize transformer-based architectures to forecast the subsequent item within a sequence. Nevertheless, the quadratic computational complexity inherent in these models often leads to inefficiencies, hindering the achievement of real-time recommendations. Mamba, a recent advancement, has exhibited exceptional performance in time series prediction, significantly enhancing both efficiency and accuracy. However, integrating Mamba directly into SRS poses several challenges. Its inherently unidirectional nature may constrain the model's capacity to capture the full context of user-item interactions, while its instability in state estimation can compromise its ability to detect short-term patterns within interaction sequences.\nTo overcome these issues, we introduce a new framework named SelectIve Gated MAmba (SIGMA). This framework leverages a Partially Flipped Mamba (PF-Mamba) to construct a bidirectional architecture specifically tailored to improve contextual modeling. Additionally, an input-sensitive Dense Selective Gate (DS Gate) is employed to optimize directional weights and enhance the processing of sequential information in PF-Mamba. For short sequence modeling, we have also developed a Feature Extract GRU (FE-GRU) to efficiently capture short-term dependencies. Empirical results indicate that SIGMA outperforms current models on five real-world datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Over the past decade, sequential recommender systems (SRS) have demonstrated promising potential across various domains, including content streaming platforms [27] and e-commerce [32]. To harness this potential and meet the demand for accurate next-item predictions [5], an increasing number of researchers are focusing on refining existing architectures and proposing novel approaches [24, 33].\nRecently, Transformer-based models have emerged as the leading approach in sequential recommendation due to their outstanding performance [3]. By leveraging the powerful self-attention mechanism [15, 31], these models have demonstrated a remarkable ability to deliver accurate predictions. However, despite their impressive performance, current transformer-based models are proven inefficient since the amount of computation grows quadratically as the length of the input sequence increases [15]. Other approaches, such as RNN-based models e.g., GRU4Rec [12] and MLP-based models e.g., MLP4Rec [21], are proven to be efficient due to their linear complexity. Nevertheless, they have struggled with handling long and complex patterns [38]. All these methods above seem to have\nsuffered from a significant trade-off between effectiveness and efficiency. Consequently, a specially designed State Space Model (SSM) called Mamba [6] has been proposed. By employing simple input-dependent selection on the original SSM [8, 22], it has demonstrated remarkable efficiency and effectiveness.\nHowever, two significant challenges hinder the direct adoption of Mamba in SRS:\n\u2022 Context Modeling: While previous research has demonstrated Mamba's reliability in capturing sequential information [6, 36], its unidirectional architecture imposes significant limitations in SRS applications. By only capturing users' past behaviors, Mamba cannot leverage future contextual information, potentially leading to an incomplete understanding of user preferences [22, 30]. For instance, if a user consistently purchases household items but begins to show interest in sports equipment, a model that does not consider future contexts may struggle to recognize this shift, resulting in sub-optimal next-item predictions [13, 19].\n\u2022 Short Sequence Modeling: This challenge is primarily driven by the long-tail user problem, a common issue in sequential recommendation. Long-tail users refer to such users who interact with only a few items but typically receive lower-quality recommendations compared to the normal ones [16, 17]. Furthermore, the instability in state estimation caused by limited data in short sequences [6, 29, 39] exacerbates this problem when Mamba is directly applied to SRS, highlighting the need for effective short sequence modeling. For illustration, we compare two leading baselines, Mamba4Rec [22] and SASRec [14], against our proposed framework on the Beauty dataset.\nTo address these challenges and better leverage Mamba's strengths, we propose an innovative framework called SelectIve Gated MAmba for Sequential Recommendation (SIGMA). Our approach introduces the Partially Flipped Mamba (PF-Mamba), a specialized bidirectional structure that captures contextual information [13, 22]. We then introduce an input-dependent Dense Selective Gate (DS Gate) to allocate the weights of the two directions and further filter the information. Additionally, we develop a Feature Extract GRU (FE-GRU) to better model short-term patterns in interaction sequences [10],\noffering a possible solution to the long-tail user problem. Our contributions are summarized as follows::\n\u2022 We identify the limitations of Mamba when applied to SRS, attributing them to its unidirectional structure and instability in state estimation for short sequences.\n\u2022 We introduce SIGMA, a novel framework featuring a Partially Flipped Mamba with a Dense Selective Gate and a Feature Extract GRU, which respectively address the challenges of context modeling and short sequence modeling.\n\u2022 We validate SIGMA's performance on five public real-world datasets, demonstrating its superiority."}, {"title": "2 METHODOLOGY", "content": "In this section, we will introduce a novel framework, SIGMA, which effectively addresses the aforementioned problems by adopting PF-Mamba with a Dense Selective Gate and a Feature Extract GRU. We will first present the overview of our proposed framework; then detail the important components of our architecture; and lastly introduce how we conduct our training and inference procedures.\n2.1 Framework Overview\nIn this section, we present an overview of our proposed framework in Figure 2.\nFirstly, we employ an embedding layer to learn the representation for input items. After getting the high-dimensional interaction representation, we propose a G-Mamba block to selectively extract the information. Specifically, the G-Mamba block consists of a bidirectional Mamba path and a GRU path, which respectively address challenges in context modeling and short sequence modeling. Then, a Position-wise Feed-Forward Network (PFFN) is adopted to improve the modeling ability of user actions in the hidden representation. Finally, processed by the prediction layer, we can get an accurate next-item prediction.\n2.2 Embedding Layer\nFor existing SRSs, It is necessary to map the sequential information in user-item interaction to a high-dimensional space [23] to effectively capture the temporal dependencies. In our framework, we choose a commonly used method for constructing the item embedding. Here, we denote the user set as $U = \\{u_1, u_2, \u2026, u_{|U|}\\}$\nand the item set as $V = \\{0_1, 0_2, \u2026, 0_{||}\\}$. So for a chronologically ordered interaction sequence, it can be expressed as $S_u = [s_1, s_2,..., s_{n_u}]$, where $n_u$ represents the length of the sequence for user $u \\in U$. For simplicity, we omit the mark $(u)$ in the following sections. Regarding this interaction sequence as the input tensor, we denote $D$ as the embedding dimension and use a learnable item embedding matrix $E \\in \\mathbb{R}^{|V|\\times D}$ to adaptively projected $s_i$ into the representation $h_i$. The whole interaction sequence can be output as:\n$H_0 = [h_1,h_2,\\ldots,h_N]$\\nwhere $N$ denotes the length of user-item interactions."}, {"title": "2.3 G-Mamba Block", "content": "In this section, we will detail the design of our proposed G-Mamba Block. Starting with the input sequence processed by the Embedding Layer, this block introduces two paralleling paths i.e., PF-Mamba and FE-GRU, which respectively address the context modeling challenge and short sequence modeling challenge. Specifically, for the contextual information loss caused by the unidirectional structure of Mamba [6, 19], we introduce the Partially Flipped Mamba. It modifies the original unidirectional structure to a bi-directional one by employing a reverse block that partially flips the first $r$ items in the interaction sequence. Next, a Dense Selective Gate is proposed to properly allocate the weights of the two directions depending on the input sequence [26, 41]. Additionally, for the long-tail user problem, we introduce the Feature Extract GRU, effectively capturing short-term preferences [10, 17].\n2.3.1 Partially Flipped Mamba. This module is proposed to address the context modeling challenge by leveraging the bi-directional structure. Current bi-directional methods like Dual-path Mamba [13] or Vision Mamba [44] usually just flip the whole input sentence to enable the global capturing capability. Although it allows the model to have a better understanding of the context, it significantly reduces the influence of short-term patterns in interaction sequences, leading to the loss of important interest dependencies. To address this issue, we introduce a partial flip method and integrate it with the Mamba block to construct a bi-directional structure. Followed by embedding sequence $H_0$ in Equation (1), the partially flip function adaptively reverses the first $n$ items while remaining the last $r$ items in the input tensor from $H_0 = [h_1,h_2,\\ldots, h_n, h_{n+1}\\cdots,h_N]$\nto $H^f = [h_n,\\ldots, h_2, h_1, h_{n+1}, \\cdots, h_N]$. $r$ is a pre-defined hyper-parameter that equals $N - n$, which determines the range of the remaining items, i.e., what extent we focus on the short-term preferences. After processing the input sequence, we utilize two Mamba blocks to construct a bi-directional architecture and process these\ntwo sequences as follows:\n$M_0 = \\text{Mamba} (H_0) \\in \\mathbb{R}^{L\\times D}$\\n$M_f = \\text{Mamba} (H^f) \\in \\mathbb{R}^{L\\times D}$\\nwhere $L$ and $D$ respectively represent the sequence length and hidden dimension at each time step. These two feature representations will then get a dot product with an input-dependent DS Gate to further learn the user preferences.\n$M = G_1 (H_0) \\cdot M_0 + G_1 (H^f) \\cdot M_f$\nwhere $G_1$ represents the designed DS Gate and $M = [m_1, m_2, \\ldots, m_N]^T$ denotes the output from PF-Mamba.\n2.3.2 Dense Selective Gate. To allocate the weights of two Mamba blocks and further filter the information according to the input sequence, we design an input-dependent Dense Selective Gate. It starts with a dense layer and a Conv1d layer to extract the original sequential information from the context, which can be formalized as follows:\n$G_0 = \\text{Conv1d} (\\text{How}) + b))$\nwhere $H_0$ is denoted as the output of embedding layer followed by Equation (1). Then, we introduce a forget gate and a SiLU gate [26] to generate the weights from the interaction sequence:\n$\\delta_1 (G_0) = G_0 W^{(1)}+b^{(1)}$\n$G_0 (G_0) = \\sigma (\\delta_1 (G_0))$\nwhere $W^{(1)} \\in \\mathbb{R}^{D\\times D}$ is the weight, $b^{(1)} \\in \\mathbb{R}^{D}$ is bias; $G_0$ denoted as the symbol of forget gate; $\\sigma(\\cdot)$ represents the Sigmoid activation function [9]. By employing this $G_0$, we can control the information flow in $G_0$ to selectively retain or suppress certain information [2]. Apart from the $G_0$, we also employ a SiLU function to further\nimprove the capability for capturing more complex patterns and\nfeatures [25]. Therefore, We can conclude our DS Gate as follows:\n$G_1(H_0) = \\text{SiLU} (\\delta_1 (G_0)) + G_0 (G_0)$\nThis method allows the PF-Mamba to balance two directions of the input sequence and produce a global representation.\n2.3.3 Feature Extract GRU. To handle Mamba's undesirable performance on short sequence modeling, we introduce one more GRU path called Feature Extract GRU in our SIGMA framework. Considering efficiency and effectiveness, we only introduce one more convolution function before the GRU cell to extract and mix the features [40]. By employing this one-dimensional convolution with a well-designed kernel size, we can aggregate and extract information from the short-term pattern of the input embedding sequence. Then, we can extract the hidden representation by utilizing GRU's impressive capability in capturing short-term dependencies. The whole processing procedure can then be formalized as follows:\n$C = \\text{Conv1d} (H_0) = [C_1, C_2,.., C_n]$\n$z_t = \\sigma (W_z \\cdot [f_{t-1}, c_t] + b_z)$\n$r_t = \\sigma (W_r\\cdot [f_{t-1}, c_t] + b_r)$\n$\\tilde{f_t} = \\tanh (W. [r_t \\odot f_{t-1}, c_t] + b)$\n$f_t = z_t \\odot f_{t-1} + (1 - z_t) \\odot \\tilde{f_t}$\nwhere $\\sigma(\\cdot)$ is the sigmoid activation function, $c_t$ is the input of GRU module in tth time step, $f_t$ represents the $t^{th}$ hidden states, $z_t$ and $r_t$ are the update gate and the reset gate, respectively. $b_z, b_r, b$ are bias, $W_z, W_r, W$ are trainable weight matrices. The final output of FE-GRU can be denoted as $F_0 = [f_1, f_2,\\ldots, f_N] \\in \\mathbb{R}^{L\\times D}$.\nMixing Layer. To capture user-item interactions globally and get the comprehensive hidden representation, we introduce another layer to mix the outputs of the FE-GRU and PF-Mamba for the next-item prediction. The procedure can be formalized as follows:\n$Z_0 = \\alpha_1 M + \\alpha_2 F_0 \\in \\mathbb{R}^{L\\times D}$\nwhere $\\alpha_1, \\alpha_2$ are all trainable parameters. Then, we employ a linear layer to capture complex relationships:\n$\\tilde{Z_0} = Z_0W^{(2)} + b^{(2)}$\nwhere $W^{(2)} \\in \\mathbb{R}^{D\\times D}$ is the weight, $b^{(2)} \\in \\mathbb{R}^{D}$ is bias.\n2.4 PFFN Network\nTo capture the complex features, we further leverage a position-wise feed-forward network (PFFN Net) [14, 22]:\n$\\tilde{R_0} = \\text{GELU} (\\tilde{Z_0}W^{(3)} + b^{(3)}) W^{(4)} + b^{(4)}$\nwhere $W^{(3)} \\in \\mathbb{R}^{D\\times 4D}, W^{(4)} \\in \\mathbb{R}^{4D\\times D}, b^{(3)} \\in \\mathbb{R}^{4D}, b^{(4)} \\in \\mathbb{R}^{D}$ are parameters of two dense layers, $R_0$ represents the user representation. After that, we employ a layer normalization and a residual path to stabilize the training process and ensure that the gradients flow more effectively through the network. To maintain generality, the subscript (0) here only denotes that the final user representation is obtained by 1 SIGMA layer. Actually, we can stack more such layers to better capture complex user preferences."}, {"title": "2.5 Train and Inference", "content": "In this subsection, we will present some details about the training and inference progress in our framework. As mentioned in Equation (10), we get the mixed hidden state representation $R_0$, which involves the sequential information for the first $N$ items. Assuming the embedding for items as $H_{item} = [h_{item}, h_{item},\\ldots, h_{item}] \\in \\mathbb{R}^{K\\times D}$, where $K$ denotes the total number of items. The details for the next-item prediction can be formalized as follows:\n$logits_{ik} = R_{i,j} \\cdot H_{item}$\n$P_{ik} = \\frac{\\text{exp}(logits_{ik})}{\\sum_{j=1}^M \\text{exp}(logits_{il})}$\nWhere $logits_{ik}$ and $P_{ik}$ respectively represent the prediction scores and corresponding probability of the i-th sample for the k-th item. And we can formulate our Cross Entropy Loss (CE) [42] and minimize it as:\n$L_{CE} = -\\frac{1}{B} \\sum_{i=1}^{B} log P_{i, y_i}$\nWhere $y_i$ represents the actual positive sample for i-th sample and $B$ represents the batch size following the previous formulas. By constantly updating the loss in each epoch, we can obtain the optimal weighting parameters and correspondingly get an accurate next-item prediction."}, {"title": "3 EXPERIMENT", "content": "In this section, first we introduce the experiment setting. Then we present extensive experiments to evaluate the effectiveness of our proposed SIGMA. All reported experimental results are the average values obtained from five independent runs of the algorithm\n3.1 Experiment Setting\n3.1.1 Dataset. We conduct comprehensive experiments on five representative real-world datasets i.e., Yelp\u00b9, Amazon-based\u00b2 (Beauty, Sports and Games) and MovieLens-1M\u00b3. The statistics of datasets after preprocessing are shown in Table 1. For the grouped user analysis, all datasets are categorized into three subsets based on user interaction length: \"Short\" (0 \u2013 5), \"Medium\" (5 \u2013 20), and \u201cLong\u201d (20-inf). Additionally, we arrange user interactions sequentially by time across all datasets and use the leave-one-out method for data"}, {"title": "3.3 Efficiency Comparison", "content": "In this section, we analyze the efficiency of SIGMA compared to other baselines by examining the inference time per batch and GPU memory usage during inference. The results, presented in Table 3, offer several valuable insights. First, we can find that the Mamba-based methods, including our SIGMA, can achieve higher efficiency remarkably compared with the transformer-based methods, except for LinRec. The reason lies in the simple input-dependent selection mechanism of Mamba. Then, though the efficiency-specified LinRec also owns comparable efficiency, it slightly downgrades the effectiveness of the transformer. By comparison, our SIGMA can achieve a better efficiency-effectiveness trade-off. To further analyze the efficiency of our SIGMA from a theoretical view, we refer to the complexity analysis in Appendix A. Besides, Additional experiments on the other three datasets are provided in Appendix F."}, {"title": "3.4 Grouped Users Analysis", "content": "This section presents the recommendation quality for users with different number of interactions to figure out the effctiveness of SIGMA on enhancing long-tail users' experience. We illustrate the results on Beauty and Sports in Figure 3. More results on other datasets are in Appendix G. From this figure, we can find that:\n\u2022 Mamba4Rec (denoted as Mamba in the figure) that adopts the vanilla Mamba structures for SRS presents the poor performance for 'short' and 'medium' users. While ECHO, which designs a bi-directional modeling module for SRS, achieves a slightly better results while is still worse than SASRec, a SRS baseline.\n\u2022 Our SIGMA defeat all baselines on all groups, where FE-GRU contributes to the short-sequence modeling and PF-Mamba boosts the overall performance."}, {"title": "3.5 Ablation Study", "content": "In this section, we analyze the efficacy of three key components within SIGMA, including PF-Mamba (partial flipping and DS gate), and FE-GRU. We design three variants: (1) w/o partial flipping: this variant uses the original interaction sequence without partial flipping; (2) w/o DS gate: the second variant linearly combines the output of two Mamba blocks; (3) w/o FE-GRU: this variant drops the Feature Extract GRU. We test these variants on Beauty and present results in Table 4 and Figure 4. Results on other datasets are included in Appendix H.\nFrom Table 4 and Figure 4, we can conclude that:\n\u2022 With the bi-directional interaction sequences, partial flipping contributes to improve the recommendation performance for all users.\n\u2022 DS gate significantly boosts the SIGMA by balancing the information from two directions.\n\u2022 FE-GRU is crucial for enhancing the experience of users with few interactions with strong short sequence modeling ability. And it has a huge impact on the overall performance, highlighting the importance of tackling the long-tail user problem."}, {"title": "3.6 Hyperparameter Analysis", "content": "In this section, we conduct experiments on Beauty to analyze the influence of two significant hyperparameters: (i) r, the remaining range in the partial flipping method; (ii) L, the number of stacked SIGMA layers. The results are respectively visualized in Figure 5 and Table 5.\nFrom Figure 5, we can find that our proposed SIGMA framework achieves the best results when r = 5, offering two valuable insights as follows: (i) when r is relatively large (r = N represents \"w/o flipping\"), it is challenging for SIGMA to leverage the limited bi-directional information (N - r items are flipped); (ii) when r is relatively small (r = 0 represents \"whole flipping\"), users may lose the short-term preference due to the exceeding flipping range, which is reflected as a varied Hit@10 and NDCG@10 performance in Figure 5. These phenomenons justify the significance of partial flipping with a proper r, defending the effectiveness of SIGMA."}, {"title": "3.7 Case Study", "content": "In this section, we leverage a specific example in ML-1M to illustrate the effectiveness of partial flipping in SIGMA. Specifically, we choose a user (ID: 5050) and present the interaction sequence before and after the partial flipping in the left part of Figure 6. With r = 1, only the last item 2762 is remained at the original position and other items are flipped. From this example, we can find that this user prefers comedy and romance movies (pink balls), as well as action and thriller movies (blue balls). Without the flipping, baselines focus on the most recent interactions on action and thriller movies and provide incorrect recommendations of the same genres (movie 3753 and 2028). While our SIGMA, with PF-Mamba, notices the previous preference on comedy and romance movies, makes the accurate recommendation of movie 539. Furthermore, we also present the overall performance for user-5050 in Table 6, where SIGMA significantly defeat baselines."}, {"title": "4 RELATED WORK", "content": "4.1 Sequential Recommendation\nAdvancements in deep learning have transformed recommendation systems, making them more personalized and accurate in next-item prediction. Early sequential recommendation frameworks have adopted CNNs and RNNs to capture users' preferences but faced\nissues like catastrophic forgetting when dealing with long-term dependencies [3, 16]. Then, the transformer-based models have emerged as powerful methods with their self-attention mechanism, significantly improving performance by selectively capturing the complex user-item interactions [14]. However, they have suffered from inefficiency due to the quadratic computational complexity [15]. Therefore, to address the trade-off between effectiveness and efficiency, we propose SIGMA, a novel framework that achieves remarkable performance.\n4.2 Selective State Space Model\nCurrently, SSM-based models have been proven effective in time-series prediction due to their ability to capture the hidden dynamics [8]. To further address the issues of catastrophic forgetting and long-term dependency in sequential processing, a special SSM called Mamba was introduced. Originating from the structured state space model (S4)[8, 29], Mamba has been proven to provide Transformer-level performance with better efficiency, particularly for long sequences [36]. However, it still suffers from some challenges when adopted in the realm of recommendation i.e., context modeling and short sequence modeling, which are mainly caused by Mamba's original structure and the inflexibility in hidden state transferring. Correspondingly, we introduce a special bi-directional module called Partially Flipped Mamba and a Feature Extract GRU in our SIGMA framework, which somewhat addresses these problems and explores a novel way to leverage Mamba in SRS."}, {"title": "5 CONCLUSION", "content": "In this paper, we analyze the challenges of applying Mamba to SRS and propose a novel framework, SIGMA, to address these challenges. We introduce a bidirectional PF-Mamba, featuring a well-designed DS gate, to allocate the weights of each direction and address the context modeling challenge, enabling our framework to leverage information from both past and future user-item interactions. Furthermore, to address the challenge of short sequence modeling, we propose FE-GRU to enhance the hidden representations for interaction sequences, mitigating the impact of long-tail users to some extent. Finally, we conduct extensive experiments on five real-world datasets, verifying SIGMA's superiority and validating the effectiveness of each module."}]}