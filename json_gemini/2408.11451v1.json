{"title": "Bidirectional Gated Mamba for Sequential Recommendation", "authors": ["Ziwei Liu", "Qidong Liu", "Yejing Wang", "Wanyu Wang", "Pengyue Jia", "Maolin Wang", "Zitao Liu", "Yi Chang", "Xiangyu Zhao"], "abstract": "In various domains, Sequential Recommender Systems (SRS) have become essential due to their superior capability to discern intricate user preferences. Typically, SRS utilize transformer-based architectures to forecast the subsequent item within a sequence. Nevertheless, the quadratic computational complexity inherent in these models often leads to inefficiencies, hindering the achievement of real-time recommendations. Mamba, a recent advancement, has exhibited exceptional performance in time series prediction, significantly enhancing both efficiency and accuracy. However, integrating Mamba directly into SRS poses several challenges. Its inherently unidirectional nature may constrain the model's capacity to capture the full context of user-item interactions, while its instability in state estimation can compromise its ability to detect short-term patterns within interaction sequences.\nTo overcome these issues, we introduce a new framework named SelectIve Gated MAmba (SIGMA). This framework leverages a Partially Flipped Mamba (PF-Mamba) to construct a bidirectional architecture specifically tailored to improve contextual modeling. Additionally, an input-sensitive Dense Selective Gate (DS Gate) is employed to optimize directional weights and enhance the processing of sequential information in PF-Mamba. For short sequence modeling, we have also developed a Feature Extract GRU (FE-GRU) to efficiently capture short-term dependencies. Empirical results indicate that SIGMA outperforms current models on five real-world datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Over the past decade, sequential recommender systems (SRS) have demonstrated promising potential across various domains, including content streaming platforms [27] and e-commerce [32]. To harness this potential and meet the demand for accurate next-item predictions [5], an increasing number of researchers are focusing on refining existing architectures and proposing novel approaches [24, 33].\nRecently, Transformer-based models have emerged as the leading approach in sequential recommendation due to their outstanding performance [3]. By leveraging the powerful self-attention mechanism [15, 31], these models have demonstrated a remarkable ability to deliver accurate predictions. However, despite their impressive performance, current transformer-based models are proven inefficient since the amount of computation grows quadratically as the length of the input sequence increases [15]. Other approaches, such as RNN-based models e.g., GRU4Rec [12] and MLP-based models e.g., MLP4Rec [21], are proven to be efficient due to their linear complexity. Nevertheless, they have struggled with handling long and complex patterns [38]. All these methods above seem to have"}, {"title": "2 METHODOLOGY", "content": "In this section, we will introduce a novel framework, SIGMA, which effectively addresses the aforementioned problems by adopting PF-Mamba with a Dense Selective Gate and a Feature Extract GRU. We will first present the overview of our proposed framework; then detail the important components of our architecture; and lastly introduce how we conduct our training and inference procedures."}, {"title": "2.1 Framework Overview", "content": "In this section, we present an overview of our proposed framework in Figure 2.\nFirstly, we employ an embedding layer to learn the representation for input items. After getting the high-dimensional interaction representation, we propose a G-Mamba block to selectively extract the information. Specifically, the G-Mamba block consists of a bidirectional Mamba path and a GRU path, which respectively address challenges in context modeling and short sequence modeling. Then, a Position-wise Feed-Forward Network (PFFN) is adopted to improve the modeling ability of user actions in the hidden representation. Finally, processed by the prediction layer, we can get an accurate next-item prediction."}, {"title": "2.2 Embedding Layer", "content": "For existing SRSs, It is necessary to map the sequential information in user-item interaction to a high-dimensional space [23] to effectively capture the temporal dependencies. In our framework, we choose a commonly used method for constructing the item embedding. Here, we denote the user set as $U = \\{u_1, u_2, \u2026, u_{|U|}\\}$\nand the item set as $V = \\{0_1, 0_2, \u2026, 0_{||}\\}$. So for a chronologically ordered interaction sequence, it can be expressed as $S_u = [s_1, s_2,..., s_{n_u}]$, where $n_u$ represents the length of the sequence for user $u \\in U$. For simplicity, we omit the mark $(u)$ in the following sections. Regarding this interaction sequence as the input tensor, we denote D as the embedding dimension and use a learnable item embedding matrix $E \\in R^{|V|\\times D}$ to adaptively projected $s_i$ into the representation $h_i$. The whole interaction sequence can be output as:\n$H_0 = [h_1,h_2, ,h_N]$ (1)\nwhere N denotes the length of user-item interactions."}, {"title": "2.3 G-Mamba Block", "content": "In this section, we will detail the design of our proposed G-Mamba Block. Starting with the input sequence processed by the Embedding Layer, this block introduces two paralleling paths i.e., PF-Mamba and FE-GRU, which respectively address the context modeling challenge and short sequence modeling challenge. Specifically, for the contextual information loss caused by the unidirectional structure of Mamba [6, 19], we introduce the Partially Flipped Mamba. It modifies the original unidirectional structure to a bi-directional one by employing a reverse block that partially flips the first $n$ items in the interaction sequence. Next, a Dense Selective Gate is proposed to properly allocate the weights of the two directions depending on the input sequence [26, 41]. Additionally, for the long-tail user problem, we introduce the Feature Extract GRU, effectively capturing short-term preferences [10, 17]."}, {"title": "2.3.1 Partially Flipped Mamba", "content": "This module is proposed to address the context modeling challenge by leveraging the bi-directional structure. Current bi-directional methods like Dual-path Mamba [13] or Vision Mamba [44] usually just flip the whole input sentence to enable the global capturing capability. Although it allows the model to have a better understanding of the context, it significantly reduces the influence of short-term patterns in interaction sequences, leading to the loss of important interest dependencies. To address this issue, we introduce a partial flip method and integrate it with the Mamba block to construct a bi-directional structure. Followed by embedding sequence $H_0$ in Equation (1), the partially flip function adaptively reverses the first $n$ items while remaining the last $r$ items in the input tensor from $H_0 = [h_1,h_2,\u2026, h_n, h_{n+1}\u00b7\u00b7\u00b7,h_N]$ to $H' = [h_n,\u2026, h_2, h_1, h_{n+1}, \u00b7\u00b7\u00b7, h_N]$. $r$ is a pre-defined hyperparameter that equals $N - n$, which determines the range of the remaining items, i.e., what extent we focus on the short-term preferences. After processing the input sequence, we utilize two Mamba blocks to construct a bi-directional architecture and process these two sequences as follows:\n$M_0 = Mamba (H_0) \\in R^{L\\times D}$\n$M' = Mamba (H_f) \\in R^{L\\times D}$ (2)\nwhere L and D respectively represent the sequence length and hidden dimension at each time step. These two feature representations will then get a dot product with an input-dependent DS Gate to further learn the user preferences.\n$M_0 = G_1 (H_0) \\cdot M_0 + G_1 (H'_f) \\cdot M'$\n(3)\nwhere $G_1$ represents the designed DS Gate and $M_1 = [m_1, m_2, ,m_N]^T$ denotes the output from PF-Mamba."}, {"title": "2.3.2 Dense Selective Gate", "content": "To allocate the weights of two Mamba blocks and further filter the information according to the input sequence, we design an input-dependent Dense Selective Gate. It starts with a dense layer and a Conv1d layer to extract the original sequential information from the context, which can be formalized as follows:\n$G_0 = Convld (How)+ b))$ (4)\nwhere $H_0$ is denoted as the output of embedding layer followed by Equation (1). Then, we introduce a forget gate and a SiLU gate [26] to generate the weights from the interaction sequence:\n$\\delta_1 (G_0) ) = G_0W^{(1)}+b^{(1)}$\n$G_0 (G_0) = \\sigma (\\delta_1 (G_0))$ (5)\nwhere $W^{(1)} \\in R^{D\\times D}$ is the weight, $b^{(1)} \\in R^D$ is bias; $G_0$ denoted as the symbol of forget gate; $\\sigma(\\cdot)$ represents the Sigmoid activation function [9]. By employing this $G_0$, we can control the information flow in $G_0$ to selectively retain or suppress certain information [2]. Apart from the $G_0$, we also employ a SiLU function to further improve the capability for capturing more complex patterns and"}, {"title": "2.3.3 Feature Extract GRU", "content": "To handle Mamba's undesirable performance on short sequence modeling, we introduce one more GRU path called Feature Extract GRU in our SIGMA framework. Considering efficiency and effectiveness, we only introduce one more convolution function before the GRU cell to extract and mix the features [40]. By employing this one-dimensional convolution with a well-designed kernel size, we can aggregate and extract information from the short-term pattern of the input embedding sequence. Then, we can extract the hidden representation by utilizing GRU's impressive capability in capturing short-term dependencies. The whole processing procedure can then be formalized as follows:\n$C = Convld (H_0) = [C_1, C_2,.., c_n]$\n$z_t = \\sigma (W_z \\cdot [f_{t-1}, c_t] + b_z)$\n$r_t = \\sigma (W_r\\cdot [f_{t-1}, c_t] + b_r)$\n$f'_t = tanh (W. [r_t \\odot f_{t-1}, c_t] + b)$\n$f_t = z_t \\odot f_{t-1} + (1 - z_t) \\odot f'_t$ (7)\nwhere $\\sigma(\\cdot)$ is the sigmoid activation function, $c_t$ is the input of GRU module in $t^{th}$ time step, $f_t$ represents the $t^{th}$ hidden states, $z_t$ and $r_t$ are the update gate and the reset gate, respectively. $b_z, b_r, b$ are bias, $W_z, W_r, W$ are trainable weight matrices. The final output of FE-GRU can be denoted as $F_0 = [f_1, f_2,\u2026, f_N] \\in R^{L\\times D}$.\nMixing Layer. To capture user-item interactions globally and get the comprehensive hidden representation, we introduce another layer to mix the outputs of the FE-GRU and PF-Mamba for the next-item prediction. The procedure can be formalized as follows:\n$Z_0 = a_1M + a_2F_0 \\in R^{L\\times D}$ (8)\nwhere $a_1, a_2$ are all trainable parameters. Then, we employ a linear layer to capture complex relationships:\n$\\delta$\n$Z'_0 = Z_0W^{(2)}+ b^{(2)}$ (9)\n$\\delta$\nwhere $W^{(2)} \\in R^{D\\times D}$ is the weight, $b^{(2)} \\in R^D$ is bias."}, {"title": "2.4 PFFN Network", "content": "To capture the complex features, we further leverage a position-wise feed-forward network (PFFN Net) [14, 22]:\n$\\delta$\n$R_0 = GELU (Z'_0W^{(3)} + b^{(3)})W^{(4)}+ b^{(4)}$ (10)\n$\\delta \\delta \\delta \\delta$\nwhere $W^{(3)} \\in R^{D\\times4D}, W^{(4)} \\in R^{4D\\times D}, b^{(3)} \\in R^{4D}, b^{(4)} \\in R^D$ are parameters of two dense layers, $R_0$ represents the user representation. After that, we employ a layer normalization and a residual path to stabilize the training process and ensure that the gradients flow more effectively through the network. To maintain generality, the subscript (0) here only denotes that the final user representation is obtained by 1 SIGMA layer. Actually, we can stack more such layers to better capture complex user preferences."}, {"title": "2.5 Train and Inference", "content": "In this subsection, we will present some details about the training and inference progress in our framework. As mentioned in Equation (10), we get the mixed hidden state representation $R_0$, which involves the sequential information for the first N items. Assuming the embedding for items as $H_{item} = [h_{item}, h_{item},..., h_{item}] \\in R^{K\\times D}$, where K denotes the total number of items. The details for the next-item prediction can be formalized as follows:\n$\\text{logits}_{ik} = R_{ij} H_{item}^j$\n$d$\n$\\text{exp(logits}_{ik})$\n$P_{ik} = \\frac{\\sum_{j=1}^{M} \\text{exp(logits}_{il})$ (11)\nWhere $\\text{logits}_{ik}$ and $P_{ik}$ respectively represent the prediction scores and corresponding probability of the i-th sample for the k-th item. And we can formulate our Cross Entropy Loss (CE) [42] and minimize it as:\n$L_{CE} = -\\frac{1}{B} \\sum_{i=1}^{B} \\text{log } P_{i, y_i}$ (12)\nWhere $y_i$ represents the actual positive sample for i-th sample and B represents the batch size following the previous formulas. By constantly updating the loss in each epoch, we can obtain the optimal weighting parameters and correspondingly get an accurate next-item prediction."}, {"title": "3 EXPERIMENT", "content": "In this section, first we introduce the experiment setting. Then we present extensive experiments to evaluate the effectiveness of our proposed SIGMA. All reported experimental results are the average values obtained from five independent runs of the algorithm"}, {"title": "3.1 Experiment Setting", "content": "We conduct comprehensive experiments on five representative real-world datasets i.e., Yelp\u00b9, Amazon-based\u00b2 (Beauty, Sports and Games) and MovieLens-1M\u00b3. The statistics of datasets after preprocessing are shown in Table 1. For the grouped user analysis, all datasets are categorized into three subsets based on user interaction length: \"Short\" (0 \u2013 5), \"Medium\" (5 \u2013 20), and \u201cLong\u201d (20-inf). Additionally, we arrange user interactions sequentially by time across all datasets and use the leave-one-out method for data"}, {"title": "4 RELATED WORK", "content": "Advancements in deep learning have transformed recommendation systems, making them more personalized and accurate in next-item prediction. Early sequential recommendation frameworks have adopted CNNs and RNNs to capture users' preferences but faced"}, {"title": "4.1 Sequential Recommendation", "content": "issues like catastrophic forgetting when dealing with long-term dependencies [3, 16]. Then, the transformer-based models have emerged as powerful methods with their self-attention mechanism, significantly improving performance by selectively capturing the complex user-item interactions [14]. However, they have suffered from inefficiency due to the quadratic computational complexity [15]. Therefore, to address the trade-off between effectiveness and efficiency, we propose SIGMA, a novel framework that achieves remarkable performance."}, {"title": "4.2 Selective State Space Model", "content": "Currently, SSM-based models have been proven effective in time-series prediction due to their ability to capture the hidden dynamics [8]. To further address the issues of catastrophic forgetting and long-term dependency in sequential processing, a special SSM called Mamba was introduced. Originating from the structured state space model (S4)[8, 29], Mamba has been proven to provide Transformer-level performance with better efficiency, particularly for long sequences [36]. However, it still suffers from some challenges when adopted in the realm of recommendation i.e., context modeling and short sequence modeling, which are mainly caused by Mamba's original structure and the inflexibility in hidden state transferring. Correspondingly, we introduce a special bi-directional module called Partially Flipped Mamba and a Feature Extract GRU in our SIGMA framework, which somewhat addresses these problems and explores a novel way to leverage Mamba in SRS."}, {"title": "5 CONCLUSION", "content": "In this paper, we analyze the challenges of applying Mamba to SRS and propose a novel framework, SIGMA, to address these challenges. We introduce a bidirectional PF-Mamba, featuring a well-designed DS gate, to allocate the weights of each direction and address the context modeling challenge, enabling our framework to leverage information from both past and future user-item interactions. Furthermore, to address the challenge of short sequence modeling, we propose FE-GRU to enhance the hidden representations for interaction sequences, mitigating the impact of long-tail users to some extent. Finally, we conduct extensive experiments on five real-world datasets, verifying SIGMA's superiority and validating the effectiveness of each module."}]}