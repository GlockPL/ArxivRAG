{"title": "TECHNICAL REPORT: SMALL LANGUAGE MODEL FOR JAPANESE CLINICAL AND MEDICINE", "authors": ["Shogo Watanabe"], "abstract": "This report presents a small language model (SLM) for Japanese clinical and medicine, named NCVC-slm-1. This 1B parameters model was trained using Japanese text classified to be of high-quality. Moreover, NCVC-slm-1 was augmented with respect to clinical and medicine content that includes the variety of diseases, drugs, and examinations. Using a carefully designed pre-processing, a specialized morphological analyzer and tokenizer, this small and light-weight model performed not only to generate text but also indicated the feasibility of understanding clinical and medicine text. In comparison to other large language models, a fine-tuning NCVC-slm-1 demonstrated the highest scores on 6 tasks of total 8 on JMED-LLM. According to this result, SLM indicated the feasibility of performing several downstream tasks in the field of clinical and medicine. Hopefully, NCVC-slm-1 will be contributed to develop and accelerate the field of clinical and medicine for a bright future.", "sections": [{"title": "1 Introduction", "content": "Large language model (LLM), based on Transformer [1], is widely spreading in many fields. Transformer-based language model is formulated by auto-regressive language model and implemented using causal masking on self-attention layer as follow:\n\n$p(x) = \\prod_{i=1}^{n} p(s_i | s_1, ..., s_{n-1})$\n\nIn the context of language processing, the variables $s_i$ and $p(x)$ represent word tokens and probabilities, respectively.\nThe most famous Transformer-based language model, Generative Pre-training Transformer (GPT) [2, 3, 4, 5] series, demonstrated surprising performances regardless of simply architecture and training manner. In particularly, ChatGPT ignited this main stream. Since Scaling laws [6, 7] was proposed, various LLMs have been developed and released in recent years [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]. In the field of clinical and medicine, some LLMs were developed and released [20, 21, 22, 23, 24, 25]. Gigantic LLMs achieve or exceed human-level ability on several benchmarks. Larger models show higher performance. However, they require high computational cost (even if using some quantization or light-weighting techniques). Besides, huge LLMs have some disadvantages. Too large models increase inference time and slow response time, therefore they are low maneuverability and inconvenience to quickly try and error. In addition, they require high-end hardware to run. It is difficult to use large models in local environments. Therefore, client-server systems are an option to use large models, but they involve the risk of leaking personal or confidential information to a third-party server.\nOn the other hand, small language model (SLM) is proposed such as Phi series (phi-1, phi-1.5, and phi-3) [26, 27, 28], LLaMA 3.2 1B, gemma-2-2B, and so on. These SLMs indicate the feasibility of better performance and reduce computational costs. In particular, the paper of phi-1 proposed textbooks approach, which use only textbook quality"}, {"title": "2 Materials and Methods", "content": "NCVC-slm-1 was trained using two main corpus stimulated by textbooks approach."}, {"title": "2.1 Dataset", "content": ""}, {"title": "2.1.1 Common Corpus", "content": "The first dataset is a widely used common corpus. NCVC-slm-1 used Japanese Wikipedia [29] and OSCAR [30] (OSCAR-2301). Following the policy that fewer parameter model and high-quality dataset, text corpus was focused on high-quality data only. Japanese Wikipedia was used only text extracted from articles. OSCAR is a large dataset based on Common Crawl. Although OSCAR is applied some filtering and removing duplication, web-based text datasets contain various topics documents, not proper contents, and noise. For screening corpus, some following strongly additional filtering were applied: filtering documents containing adult content words and copyright notices, screening documents containing not completing sentences, and removing duplication documents and sentences. These processes filtered approximately 68% of data size. Furthermore, text quality filtering based on transformer-based classifier were performed [26]. First, 100,000 documents were randomly sampled from corpus and annotated text quality using Llama3-8B-Instruct (Llama-3-ELYZA-JP-8B-instruct). Annotation prompt format is \u3042\u306a\u305f\u306f\u8aa0\u5b9f\u3067\u512a\u79c0\u306a\u65e5\u672c\u4eba\u306e\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u5165\u529b\u3055\u308c\u305f\u6587\u7ae0\u304c\u6559\u79d1\u66f8\u30ec\u30d9\u30eb\u306e\u54c1\u8cea\u304b\u3069\u3046\u304b\u3092\u5224\u65ad\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306f'\u9ad8'\u307e\u305f\u306f\u2019\u4f4e\u2019\u306e\u307f\u3092\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\u3002(In English: You are an honest and excellent Japanese assistant. Determine whether the input text is of textbook quality. Please answer 'high' or 'low' only.). Second, the last token embedding vector of the last hidden state was calculated on all documents. A random forest [31] classifier was trained using its embedding vector as features and annotated labels. A trained text quality classifier filtered documents that were considered low quality. Unfortunately, approximately only 1% of data size was filtered in this process."}, {"title": "2.1.2 Medicine Textbooks", "content": "The second corpus is textbook quality dataset comprising clinical and medical knowledge scraped from the Web. However, the amounts of scraping text focusing on clinical and medicine contents were only 49 MB. In order to augment the shortage of text about clinical and medicine, synthesized textbooks approach were performed. Synthesized textbooks contain the topics of general and various diseases and drugs based on J-Medic and the list of items on the National Health Insurance drug price list announced on the website of Ministry of Health, Labour and Welfare, Japan. Their textbooks were 5 versions of previous language models (ELYZA-Llama2-7B-Fast-Instruct [32], ELYZA-Llama3-8B-instruct [33], StableLM-7B-Alpha-Instruct [34], StableLM-7B-Beta-Instruct, and Phi-3-mini-128K-instruct [28]). These instructed models generated topics of 91,000 diseases and 9,000 drugs. Moreover, ELYZA-Llama3-8B-instruct generated exercises text which similar to Japanese national medical licensing examinations and Japanese national pharmacists licensing examinations. Synthesized exercises dataset were prepared 5 samples per each topics."}, {"title": "2.2 Tokenizer", "content": "Generally speaking, refined pre-processing is considered an important process to enhance accuracy or performance. Huge LLMs may cover this process. However, given the scalability of NCVC-slm-1, proper pre-processing is considered to be necessary. Pre-processing is performed in 2 steps text cleaning and morphological analysis before tokenization.\nFirst, text cleaning is applied to raw text. This process contains replacing specific characters, normalizing Unicode and spelling inconsistencies, removing personal information and web-specific notations, unifying symbols, reducing redundancy, and so on. Japanese characters do not only have full-width and half-width character sets but also are used mixing punctuations frequently. Here, full-width period and comma are replaced with full-width\u3002and\u3001 respectively. This is because their symbols are normalized to half-width symbols by subsequent Unicode normalization. In NCVC-slm-1, Unicode normalization is used Normalization Form Compatibility Composition (NFKC). Furthermore, it is normalized spelling inconsistencies such as hyphens, ch\u014donpus, and tildes. Restoration is applied continuous three dots to ellipsis(\u2026) and \u00b0C to \u00b0C. In the perspective of protecting privacy, it is applied removing personal information such as phone numbers and e-mail addresses using regex processing. In addition, web-specific notations are removed such as URL, @ account names, and # tags. Symbols include similar meaning are unified because of reducing the wastes of vocabularies. After these cleanings, other than the using characters are deleted because of limiting the space of characters. In order to eliminate superfluous repetitions, the unnecessary continuous characters are removed. Finally, white-space are replaced with meta-space (U+2581) because white-space is used in the pre-processing of subword tokenizer.\nSecond, morphological analysis are applied to each sentence. Different from English, Japanese sentences were not explicitly split by such as white-space. According to previous researches, [35, 36, 37] The application of morphological analysis can enhance the precision of model performance evaluation in comparison to the absence of such an analysis in the context of Japanese text. The difference between morphological analyzers hardly affects by performance. Therefore, MeCab [38] is used as morphological analyzer. Although custom dictionaries, which include latest words such as neologd, is not necessary to improve the model performance, custom dictionary were made referred to J-Medic [39] in order to specialize clinical and medicine words.\nIn tokenization, Unigram tokenizer [40] is used as tokenizer. Morphological analyzed text are split by white-space at pre-tokenizer. The vocabulary size was decided to be 32768 (= $2^{15}$) for NCVC-slm-1 base model. Special tokens are following: <|begin_of_text|> (means begin of text), <|end_of_text]> (means end of text and uses as padding tokens), and \\n (means new line). The hyperparameters of training Unigram tokenizer are set following: shrinking factors is 0.75, max piece length is 16, and number of sub-iterations is 8. The corpus for training tokenizer used Japanese Wikipedia and medicine textbooks (including both scraping and synthesized) in order to prioritize clinical and medicine words."}, {"title": "2.3 Modeling", "content": "NCVC-slm-1 is based on phi-1, LLaMA series, and the advantages of previous language models. Basic hyperparameters are following: 24 layers, 2048 hidden dimensions, 8192 internal hidden dimensions in MLP, 64 head dimensions, and 32 heads. The maximum sequence length is 2048 at pre-training. Positional encoding (PE) plays an important role to Transformer-based model because of not considering the order of inputs explicitly. Rotary positional encoding [41] is one of the better approaches for PE. Similarly to several language models, NCVC-slm-1 used rotary PE in every query and key of self-attention layers. A normalization architecture is applied pre-normalization because of stabilizing model training [42]. The total number of parameters are approximately 1 billions (more accurately 1.2B), and the reserved GPU memory is only 2.2 GB memory with bfloat16 [43] precision.\nNCVC-slm-1 is modified and updated on several components from phi-1.\nThe activation function is replaced gaussian error linear units (GELU) (gelu_new) [44] with sigmoid linear units (SiLU) [45]. Although GELU has the features of differentiable at near 0 and similar effects to dropout and it has been used instead of ReLU [46] since around 2018, approximal GELU implementation requires the high computation burden. In preliminary experiments, 28% GPU memory reduction were observed by replacing GELU (gelu_new) with SiLU. In phi-3, SiLU also used as non-linear activation function instead of GELU (gelu_new).\nLayer normalization (LayerNorm) [47] is replaced with root mean square normalization (RMSNorm) [48] inspired by LLaMA series. RMSNorm reduces the computation burden comparing with LayerNorm. In actually, the series of phi-3 also used RMSNorm.\nMulti-head attention (MHA) is replaced with grouped query attention (GQA) [49] inspired by LLaMA series. Although MHA is replaced with multi-query attention (MQA) [50] on the previous study [13], due to concerns that MQA is degraded the model performance, GQA was applied to NCVC-slm-1 in finally. The number of group is applied 8 refer to the proposed paper."}, {"title": "2.4 Training", "content": ""}, {"title": "2.4.1 Self-supervised Pre-training", "content": "According to Chinchilla scaling law, 1B parameters model requires 20B tokens [55]. The phi-1 paper reported total seen 50B tokens (of which 7B tokens were unique) for pre-training. On the other hand, excessive epoch learning, which repeatably seeing the same training data, declines language model performance [56]. Based on these previous findings, self-supervised pre-training was performed total 24000 global steps (save model per 3200 global steps) \u2013 this is equivalent to 5.5 epochs and total seen approximately 50B tokens (of which 9B tokens were unique). Effective batch size were performed 1024 with flash attention 2 [57], distributed data parallel, and DeepSpeed with zero redundant optimizer (ZERO) stage 2 [58, 59]. Similar to other language model training, tokens are packed to a length of 2048 using concatenation or truncation for computational efficiency. The batch size of 1024 is consisted of 8 minibatches per a GPU, 4 GPUs, and 32 gradient accumulation steps. Dropout [60] was applied to all attention and residual layers with 0.1 in order to mitigate a generalization performance degradation using repeat tokens. Loss function is cross entropy for next token prediction manner. Optimizer algorithm was selected AdamW [61] with $\u03b2_1$, $\u03b2_2$, and weight decay was 0.9, 0.95, and 0.1, respectively. Learning rate scheduling was used WarmupCosineLR, warmup to 1.0 \u00d7 $10^{-3}$ by 750 global steps and then cosine decay to 0. Self-supervised pre-training spent 14 days using 4 NVIDIA 6000 Ada 48GB memory."}, {"title": "2.4.2 Fine-tuning", "content": "Fine-tuning was performed based on instruction tuning [62]. Instruction tuning dataset was used 8 JMED-LLM [63] dataset except test samples. The effective of with or without synthesized exercises dataset also was evaluated. Base models were used total seen 20B tokens (NCVC-slm-1-instruct-20B) and total seen 50B tokens (NCVC-slm-1-instruct-50B). When instruction tuning is applied, additional 3 special tokens (<|system|>, <|user|>, and <|assistant|>) are added to the tokenizer and the dimension of token embedding layer is extended 32768 to 32771. The batch size is 256 (8 minibatches per a GPU, single NVIDIA A6000 48GB memory, and 32 gradient accumulation steps) in JMED-LLM only fine-tuning. The batch size is 1024 (the same as pre-training) in fine-tuning with synthesized exercises dataset. Loss function is cross entropy similar to self-supervised pre-training. However, the loss function is calculated only positions from immediately after <|assistant|> to <|end_of_text|>. AdamW optimizer is used and $\u03b2_1$, $\u03b2_2$, and weight decay was 0.9, 0.95, and 0.01, respectively. Learning rate scheduling was used WarmupCosineLR, warmup to 1.0 \u00d7 $10^{-4}$ by 50 global steps and then cosine decay to 0. Instruction tuning was run 890 or 4850 global steps and spent 9 hours on JMED-LLM only or 21 hours on with synthesized exercises, respectively. In order to enhance the model performance, noisy embeddings fine-tuning (NEFTune) [64] were applied to hidden states after token embedding layer in training. The strength of noise was set to \u03b1 = 5."}, {"title": "2.5 Validation", "content": "In this report, two benchmarks are used for performance evaluation. IgakuQA [65] is a benchmark test for Japanese national medical licensing examinations. IgakuQA contains 5 examinations in 2018\u20132022. JMED-LLM [63] is proposed by LLM-JP to evaluate language model performance for clinical and medicine in the various aspects. JMED-LLM is consisted of 6 major tasks including question and answer (Q&A), document classification, and named entity recognition (NER). JMMLU-Med is a Q&A task with respect to biology and medicine extracted from JMMLU. CRADE is a classification task for the possibility of adverse events in case reports. RRTNM is a classification task for the stage of cancer refer to radiology reports of lung cancer patients. SMDIS is a classification task for detection of illness or symptoms from tweets. JCSTS is classification task for similarity of two sentences in case reports. MRNER-disease and MRNER-medicine is a NER for extracting symptoms or pharmaceuticals from medical reports, respectively. NRNER is a NER for extracting symptoms and pharmaceuticals from nursing records. JMMLU-Med, CRADE, RRTNM, SMDIS, and JCSTS are evaluated by Cohen's kappa [66] and accuracy. NER tasks are evaluated by partial or exact F1 score."}, {"title": "3 Results", "content": ""}, {"title": "3.1 Self-supervised Pre-training", "content": "Figure 3 illustrates the logging of loss in self-supervised pre-training. The horizontal axis represents global steps and seen tokens. The vertical axis is global step and cross entropy loss, respectively. Loss function rapidly decreases until approximately 3200 global steps and slowly on subsequent steps. Despite the tendency towards stagnation, the value of loss function continues to decrease slightly. The loss spike, which destroy a model training, was not observed in this experiments."}, {"title": "3.2 IgakuQA", "content": "Table 2 shows the scoring board on IgakuQA. NCVC-slm-1-base and -instruct is displayed on the table. The scores of other LLMs were cited from the original article [65] and the blog post [25]. The instruct models are tuned by instructing only JMED-LLM dataset. The acceptance criteria is guessed approximately 75%. The score of NCVC-slm-1 models remains below 20%. There are no obvious difference between seen 20B and 50B tokens on both base and instruction. The scores of intermediate models saved per 3200 global steps are illustrated in Appendix A.4."}, {"title": "3.3 JMED-LLM", "content": "Table 3 and 4 show the scoring board on JMED-LLM. Similar to IgakuQA, base and instruct is displayed on the table. These models are the same as on IgakuQA. The scores of other LLMs were cited from [67]. NCVC-slm-1 base models did not cope with the problems at all and consequently achieved the lowest scores in most tasks. On the other hand, instruction tuning models were the highest score of 6 tasks (CRADE, SMDIS, JCSTS, MRNER-disease, MRNER-disease, and NRNER). Nevertheless, the score of JMMLU-Med and RRTNM were comparable or below even if base models are enough fine-tuning. GPT-40 demonstrates the highest performance on these two tasks and is superior to other models."}, {"title": "3.4 Ablation Study", "content": "In order to inspect the base model, it was performed to visualized token embedding space, to demonstrate token analogy, and to visualize attention map in packing tokens. NCVC-slm-1-base seen 20B tokens was used as the base model for ablation study. Considering the arrangement of pages, these figures are posted to Appendix. See more details on Appendix A.2, A.3, and A.6."}, {"title": "4 Discussion", "content": "Transformer-base LLMs have achieved grateful successes along with Scaling laws and drastically developed. However, it is said that the rate of performance improvement stagnates in recent years. Furthermore, it rises up the problem of the shortage of high-quality data for training contrary to the size of model parameters. From various perspectives, which include the limit of scaling up the size of model or energy problems, it may be to require smaller and smarter language models optimized the specific fields and particular tasks in the next stages. It is hoped that this report will contribute to the development of SLMs in conjunction with other previous studies.\nIt is considered that the development of a smart SLM require high-quality dataset. Based on this intuition, strong screening and filtering was applied to pre-training corpus in this project. Unlike an expectation, Transformer-based classifier of text quality was hardly filtering. There are several possible reasons for this result. One is that corpus was already enough filtering because 94% of randomly sampled 100,000 texts were labeled to high-quality. Another one is that the instruction prompt for annotation was not optimized. In general, annotation is a very tough task. Moreover, it is too difficult to define how good text is. Although text quality annotations were automated by LLM to reduce the burden of annotation and to avoid the subjective evaluation, the criteria for classifying text quality were also unclear and LLM sometimes did not follow the instruction (e.g. LLM answered not in Japanese but in English or generated non-relation text etc.). It is not easy to operate LLMs by only instruction prompt. If you automate to classify text quality, it may be needed to develop an optimized annotation model. Furthermore, the computation costs were not small to derive the hidden vector. Considering benefits and efforts, it may be not effective to use this quality filtering.\nIn order to augment the clinical and medicine contents, synthesized textbooks approach were also performed following the phi-1 paper. In this project, only 0.23B tokens were prepared despite considering variation of models and topics. This is very fewer than total tokens of corpus. It is necessary to add much more clinical and medicine knowledge and to train various documents such as radiological, nursing, and summary report. Synthesized approach has been focused on LLM training recently. Considering the problem of training data shortage, this approach will be utilized more and more. On the other hand, synthesized approach contains the problem of curse of recursion [68]. Therefore, it must be concerned with the proportion of real and synthesized data. Well, where is more high-quality text? Valuable text may be buried in traditional analog textbooks. In the current stage, it cannot be to use the contents of these textbooks for model training. But, it will be necessary to cooperate with us for a bright future.\nIn other language models which mainly developed in English, a lot of Japanese words are split to character-level or decomposed by fallback to UTF-8. Character- or Unicode-level tokenization is effective to avoid unknown tokens, but"}, {"title": "5 Conclusion", "content": "A SLM specialized particular language, specific field, and well fine-tuning is capable of performing useful against particular tasks. This type of SLM is expected to to assist human jobs in local environment that isolated network connections or limited computational costs."}, {"title": "A.1 Text Generation Samples", "content": "Text Generation Samples\n\u8133\u5352\u4e2d\u3068\u5faa\u74b0\u5668\u75c5\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002\n\u5faa\u74b0\u5668\u75c5\u306f\u3001\u8133\u6897\u585e\u30fb\u5fc3\u7b4b\u6897\u585e\u304c\u591a\u304f\u3001\u9ad8\u9f62\u8005\u306e\u767a\u75c7\u5272\u5408\u304c\u9ad8\u3044\u75c5\u6c17\u3067\u3059\u3002\u307e\u305f\u3001\u5faa\u74b0\u5668\u75c5\u306f\u7cd6\u5c3f\u75c5\u3092\u5408\u4f75\u3057\u305f\u5272\u5408\u304c\u975e\u5e38\u306b\u9ad8\u304f\u3001\u751f\u6d3b\u7fd2\u6163\u75c5\u3068\u3057\u3066\u306f\u3001\u9ad8\u8102\u8840\u75c7\u30fb\u9ad8\u8840\u5727\u75c7\u30fb\u7cd6\u5c3f\u75c5\u306a\u3069\u304c\u3042\u308a\u307e\u3059\u3002\u5faa\u74b0\u5668\u75be\u60a3\u306e\u4e2d\u3067\u3082\u300c\u72ed\u5fc3\u75c7\u300d\u300c\u5fc3\u7b4b\u6897\u585e\u300d\u306f\u6bd4\u8f03\u7684\u591a\u304f\u3001\u300c\u5fc3\u623f\u7d30\u52d5\u300d\u3084\u300c\u51a0\u6523\u7e2e\u6027\u72ed\u5fc3\u75c7\u300d\u3068\u3044\u3063\u305f\u5fc3\u81d3\u306e\u75c5\u6c17\u306f\u3042\u307e\u308a\u77e5\u3089\u308c\u3066\u3044\u307e\u305b\u3093\u3002\u3057\u304b\u3057\u3001\u5fc3\u4e0d\u5168\u306f\u5fc3\u75be\u60a3\u306e\u3046\u3061\u6700\u3082\u983b\u5ea6\u306e\u9ad8\u3044\u75c5\u6c17\u3067\u3059\u3002\u5faa\u74b0\u5668\u75c5\u306e\u6cbb\u7642\u65b9\u91dd\u306f\u3001\u307e\u305a\u3001\u75c7\u72b6\u304c\u8efd\u3044\u5834\u5408\u306f\u3001\u8840\u5727\u3092\u4e0b\u3052\u308b\u3053\u3068\u306b\u7126\u70b9\u3092\u5f53\u3066\u307e\u3059\u3002\u305d\u3057\u3066\u3001\u75c7\u72b6\u304c\u91cd\u3044\u5834\u5408\u306f\u3001\u5fc3\u81d3\u306e\u6a5f\u80fd\u3092\u56de\u5fa9\u3055\u305b\u3001\u305d\u308c\u306b\u3088\u308a\u5408\u4f75\u75c7\u3092\u6291\u3048\u308b\u3053\u3068\u306b\u7126\u70b9\u3092\u5f53\u3066\u307e\u3059\u3002\u305d\u3057\u3066\u3001\u5fc3\u81d3\u79fb\u690d\u30fb\u51a0\u52d5\u8108\u30d0\u30a4\u30d1\u30b9\u8853\u30fb\u4e0d\u6574\u8108\u6cbb\u7642\u85ac\u306a\u3069\u306e\u88dc\u52a9\u7642\u6cd5\u304c\u7a4d\u6975\u7684\u306b\u7528\u3044\u3089\u308c\u3066\u3044\u307e\u3059\u3002"}, {"title": "A.2 Token Embedding Space Visualization", "content": "Figure 4 and 5 illustrate token embedding spaces using manifold learning algorithms and applying dimensional reduction for visualization. Figure 4 illustrates a token embedding space applied t-distributed Stochastic Neighbor Embedding (t-SNE) [69]. A hyperparameter perplexity was set to be 15. Figure 5 illustrates a token embedding space applied Uniform Manifold Approximation and Projection (UMAP) [70]. A hyperparameter neighbor was set to be 15. The certain clusters has been observed in 2-dimensional space. Some stop-words or function words such as postpositional particle and special tokens are clustered at the red circles."}, {"title": "A.3 Token Analogy", "content": "Word analogy was famous to Word2Vec [71]. A well trained language model acquires the ability to analogy the relationships between words. It shows below some examples with respect to diseases.\nExamples of token analogy\n\u8133\u51fa\u8840 - \u8133 + \u6d88\u5316\u7ba1 = (cerebral hemorrhage - brain + gastrointestinal = )\n\u6d88\u5316\u7ba1\u51fa\u8840 0.85 (gastrointestinal bleeding)\n\u8133\u5185\u51fa\u8840 0.64 (intracerebral hemorrhage)\n\u8133\u8840\u7ba1\u969c\u5bb3 0.63 (cerebrovascular disease)\n\u8840\u4fbf 0.62 (hematochezia)\n\u80c3\u6f70\u760d 0.61 (peptic ulcer)\n\u5c3f\u8def\u7d50\u77f3 - \u5c3f\u8def + \u80c6\u7ba1 = (urinary calculi - urinary tract + bile duct)\n\u7d50\u77f3 1.00 (calculus)\n\u80c6\u77f3 0.97 (gallstone)\n\u80c6\u77f3\u75c7 0.96 (gallstone disease)\n\u5c3f\u7ba1\u7d50\u77f3 0.94 (ureteric stone)\n\u80c6\u7ba1\u7d50\u77f3 0.92 (bile duct stones)\n\u8133\u5352\u4e2d - \u8133 + \u5fc3\u81d3 = (stroke - brain + heart)\n\u5fc3\u75be\u60a3 0.99 (heart disease)\n\u5fc3\u4e0d\u5168 0.95 (heart failure)\n\u8133\u6897\u585e 0.85 (ischemic stroke)\n\u6025\u6027\u5fc3\u7b4b\u6897\u585e 0.84 (acute myocardial infarction)\n\u4e0d\u6574\u8108 0.82 (arrhythmia)\n\u8133\u6897\u585e - \u8133 + \u5fc3\u81d3 = (ischemic stroke - brain + heart)\n\u5fc3\u4e0d\u5168 1.00 (heart failure)\n\u7b4b\u6897\u585e 1.00 (myocardial infarction)\n\u6897\u585e 1.00 (infarction)\n\u5fc3\u75be\u60a3 1.00 (heart disease)\n\u6025\u6027\u5fc3\u7b4b\u6897\u585e 0.96 (acute myocardial infarction)\n\u5927\u8178\u764c - \u5927\u8178+ \u809d\u81d3 = (colorectal cancer - colon + liver)\n\u809d\u7d30\u80de\u764c 1.00 (hepatocellular carcinoma)\n\u809d\u764c 1.00 (liver cancer)\n\u809d\u8ee2\u79fb 0.95 (liver metastases)\n\u809d\u969c\u5bb3 0.92 (hepatic impairment)\n\u80c3\u764c 0.88 (stomach cancer)"}, {"title": "A.4 Base Model Performance", "content": "Figure 6 illustrates the comparison of between total seen tokens and average score on IgakuQA. It is observed the cycle of deterioration and improvement."}, {"title": "A.5 Instruction Tuning", "content": "Figure 7 illustrates the loss logging of instruction tuning. The horizontal and vertical axis is global step and cross entropy loss, respectively."}, {"title": "A.6 Attention Map in Packing Tokens", "content": "In pre-training, packing tokens are usually implemented for computation efficiency. The height and width of attention maps are query and key tokens. The horizontal ticks represent just sequence positions. The vertical ticks represent the position of <|end_of_text|> tokens. In causal language models, masking are applied and only lower triangular matrix are valid, therefore attention map scores are all zero on the upper right area. It is observed that the sequence of packed tokens are split into clusters by <|end_of_text|> in deeper and deeper layers."}]}