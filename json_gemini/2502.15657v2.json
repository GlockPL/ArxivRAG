{"title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?", "authors": ["Yoshua Bengio", "Michael Cohen", "Damiano Fornasiere", "Joumana Ghosn", "Pietro Greiner", "Matt MacDermott", "S\u00f6ren Mindermann", "Adam Oberman", "Jesse Richardson", "Oliver Richardson", "Marc-Antoine Rondeau", "Pierre-Luc St-Charles", "David Williams-King"], "abstract": "The leading Al companies are increasingly focused on building generalist AI agents systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current AI training methods. Indeed, various scenarios and experiments have demonstrated the possibility of AI agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory.\n\nAccordingly, we propose as a core building block for further advances the development of a non-agentic AI system that is trustworthy and safe by design, which we call Scientist AI. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of over-confident predictions. In light of these considerations, a Scientist AI could be used to assist human researchers in accelerating scientific progress, including in AI safety. In particular, our system can be employed as a guardrail against AI agents that might be created despite the risks involved. Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks associ- ated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path.", "sections": [{"title": "1 Executive summary", "content": null}, {"title": "1.1 Highly effective AI without agency", "content": "For decades, AI development has pursued both intelligence and agency, following human cog- nition as a model. Human capabilities encompass many facets including the understanding of our environment, as well as agency, i.e., the ability to change the world to achieve goals. In the pursuit of human-level performance, we are naturally encoding both intelligence and agency in our AI systems. Agency is an important attribute for the survival of living entities and would be required to perform many of the tasks that humans execute. After recent technological breakthroughs have led to large language models that demonstrate some level of general intelligence, leading AI companies are now focusing on building generalist AI agents: systems that will autonomously act, plan, and pursue goals across almost all tasks that humans can perform.\n\nHuman-like agency in AI systems could reproduce and amplify harmful human tendencies, potentially with catastrophic consequences. Through their agency and to advance their self-interest, humans can exhibit deceptive and immoral behavior. As we implement agentic AI systems, we should ask ourselves whether and how these less desirable traits will also arise in the artificial setting, especially in the case of anticipated future AI systems with intelligence comparable to humans (often called AGI, for artificial general intelligence) or superior to humans (ASI, for artificial superintelligence). Importantly, we still do not know how to set an AI agent's goals so as to avoid unwanted behaviors. In fact, many concerns have been raised about the potential dangers and impacts from AI more broadly. Crucially, there are severe risks stemming from advances in AI that are highly associated with autonomous agents. These risks arguably extend even to human extinction, a concern expressed by many AI researchers.\n\nCombining agency with superhuman capabilities could enable dangerous rogue AI systems. Certain capabilities - such as persuasion, deception and programming - could be learned by an AI from human behavior or emerge from reinforcement learning, a standard way of training an AI to perform novel tasks through goal-seeking behavior. Even if an AI is only imitating human goals and ways of thinking from its text completion pre-training, it could reach superior cognitive and executive capability due to advantages such as high communication bandwidth and the ability to run many instances of itself in parallel. These superhuman capabilities, if present in a generalist agent with even ordinary human self-preservation instincts or human moral flaws (let alone poorly aligned values), could present a serious danger.\n\nStrategies to mitigate the risks of agency can be employed, including the use of non-agentic trustworthy AI as a safety guardrail. For example, we could reduce the cognitive ability of an AI by making its knowledge narrow and specialized in one domain of expertise, yielding a narrow \u0391\u0399 system. We can reduce its potential impact in the world by reducing the scope of its actions. We can reduce its ability to hatch complex and dangerous plans by making sure it can only plan over a short horizon. We can mitigate its dangerous actions by using another AI, one that is preferably safe and trustworthy, like the non-agentic AI proposed here, as a guardrail that detects dangerous actions. This other AI is made trustworthy by training it to scientifically explain human behavior rather than imitate it, where trustworthy here means \"honest\", avoiding the deceptive tendencies of modern frontier AIs. If society chooses to go ahead with building agentic AGIs in spite of the risks, a pragmatic risk management avenue would be to overlay them with such trustworthy and non-agentic guardrails, which is one of the motivations for our proposal.\n\nWith the objective to design a safer yet powerful alternative to agents, we propose \u201cScientist Als\" - AI systems designed for understanding rather than pursuing goals. Inspired by a platonic and idealized version of a scientist, we propose the design and construction of Scientist AIs. We do so by building on the state-of-the-art in probabilistic deep learning and inspired by the methodology of the scientific process, i.e., first understanding or modeling the world and then making probabilistic inferences based on"}, {"title": "1.2 Mapping out ways of losing control", "content": "Powerful AI agents pose significant risks, including loss of human control. Scenarios have been identified, without arguments proving their impossibility, that an irreversible loss of human control over agentic AI can occur, due to technical failures, corner cutting, or intentional malicious use. Making sure an AI will not cause harm is a notoriously difficult unsolved technical problem, which we illustrate below through the concepts of goal misspecification and goal misgeneralization. The less cautious the developer of the AI, e.g., because of perceived competitive pressures, the greater the risk of loss-of-control accidents. Some players may even want to intentionally develop or deploy an unaligned or dangerous ASI.\n\nLoss of control may arise due to goal misspecification. This failure mode occurs when there are multiple interpretations of a goal, i.e., it is poorly specified or under-specified and may be pursued in a way that humans did not intend. Goal misspecification is the result of a fundamental difficulty in precisely defining what we find unacceptable in AI behavior. If an AI takes life-and-death decisions, we would like it to act ethically. It unfortunately appears impossible to formally articulate the difference between morally right and wrong behavior without enumerating all the possible cases. This is similar to the difficulty of stating laws in legal language without having any loopholes for humans to exploit. When it is in one's interest to find a way around the law, by satisfying its letter but not its spirit, one often dedicates substantial effort to do so."}, {"title": "1.3 The Scientist AI research plan", "content": "Without using any equations, this paper argues that it is possible to reap many of the benefits of AI without incurring extreme risks. For example, it is not necessary to replicate human-like agency to generate scientific hypotheses and design good scientific experiments to test them. This even applies to the scientific modeling of agents, such as humans, which does not require the modeler themselves to be an agent.\n\nScientist AI is trustworthy and safe by design. It provides reliable explanations for its outputs and"}, {"title": "2 Understanding loss of control to agentic AI", "content": "This paper consists of two main sections. This section reviews arguments for how loss of control to generalist agentic AI may occur, with potentially catastrophic consequences, providing motivation for Section 3 on designing safe non-agentic AI.\n\nSection 2 is structured as follows. Section 2.1 introduces some preliminaries and terminology. Then, we examine in Section 2.2 the current AI R&D trajectory, headed towards AGI and then ASI agents, and why, at a high level, this could yield a loss of human control and the emergence of rogue AI agents. We discuss plausible consequences of the emergence of such rogue Als, which could threaten democratic institutions and the future of humankind. We move to Section 2.3, which analyzes the AI behaviors and skills that would make an uncontrolled AI dangerous, such as deception, persuasion, hacking, and collusion. The last two sections go deeper into two principal ways dangerous misalignment and self-preservation could emerge: firstly, due to reward maximization (Section 2.4), and secondly, due to imitation of humans (Section 2.5).\n\nThe arguments in this paper support the case that the Scientist AI approach would not only help reduce the likelihood of loss of human control but would also help us build more trustworthy and explanatory AI systems that could accelerate scientific research. Additionally, the paper proposes how a Scientist AI could be used to double-check or guardrail any other AI system."}, {"title": "2.1 Preliminaries: agents, goals, plans, affordances and knowledge", "content": "We start by recalling and specifying some important terms.\n\nAgents observe their environment and act in it in order to achieve goals. Agency can come in degrees which depend on several factors, discussed in more detail in Section 3.2: affordances (discussed below), goal- directedness, and intelligence (including knowledge and reasoning). Als can be more or less agentic, i.e., with greater ability to achieve their goals autonomously. An AI's affordances refer to the extent of its possible actions and thus capacity to create desired outcomes in the world. A person with locked-in syndrome has zero affordances, so that even if they are very intelligent, they cannot act causally on the world.\n\nA policy is the strategy used by an agent to achieve its goals or maximize its rewards, e.g., the input-output behavior of a neural network that outputs actions given goals and past observations. The policy can rely on learned behaviors which perform a form of implicit planning, as in typical deep reinforcement learning (Sutton and Barto 2018) (e.g., with a chess-playing neural network that instantly proposes a move), or it can plan explicitly and consider different paths before acting (S. J. Russell and Norvig 2016) (e.g., with a chess-playing program using explicit tree-structured search). In order to generate good policies and plans, it helps to have knowledge or experience of how the world works. Since such knowledge is rarely fully available from the start, learning and exploration abilities are crucial.\n\nIn order to use knowledge effectively, reasoning is necessary: combining pieces of knowledge in order to make predictions or take actions. Reasoning can be implicit, as when we train a neural network to make good predictions, or it can be explicit, as when we reason about a new problem through a chain of thought or propose an argument to support a claim. We can view planning as a special kind of reasoning aimed at predicting which sequence of actions will be most successful. Planning and reasoning are essentially optimization problems: find the best strategy to achieve a goal, solve a problem, or generate an explanation, among a vast number of possibilities. In practice, an agent does not need to find the best plan; there will be multiple plans that are \"good enough\".\n\nLearning can also be viewed as an optimization problem: find a function that performs well according to a training objective, e.g., predicting how truncated texts will be continued, or providing answers that human labelers will like\u2014the two main driving forces of learning for current general-purpose AI systems. Although we almost always get only approximate solutions to these optimization problems, better solutions can be obtained with more resources. This has been demonstrated vividly: increases in scale (of the neural networks, dataset sizes, and inference-time computation) have delivered consistent improvements in AI capabilities over"}, {"title": "2.2 The severe risks of the current trajectory", "content": "There are many benefits and risks associated with current and anticipated AI advances: see the International Scientific Report on the Safety of Advanced AI for a survey. In risk analysis, it is important to distinguish the likelihood of the harmful event from its severity, i.e., how bad the consequences would be if the harmful event occurs. While as humans we are often drawn to consider risks that have high probability and we may dismiss events of low probability as unrealistic, it can be just as worrying for an event to have low probability but very high severity. We focus here mostly on the risk of loss of human control because it is a risk whose severity could go as far as human extinction, according to a large number of AI researchers. Opinions vary on its probability, but if we do build AGI as envisioned by several major corporations, there are difficult-to-dismiss scenarios in which humanity's future as a whole could be in peril, as discussed below, with behaviors and skills that make loss of control dangerous (as described in Section 2.3).\n\nAI agents may be misaligned and self-preserving\n\nIn this paper we will discuss various catastrophic scenarios involving rogue AI agents. These scenarios are not due to Als developing explicit malicious intent towards humans, like a fictional villain, but are rather the result of Als trying to achieve their goals. Why could we not simply set an AI's goals so as to avoid conflict with humans? That turns out to be difficult, and maybe even intractable. As we argue in Section 2.4 and Section 2.5, AI agents may become misaligned with human values due to the methods we currently use to train Als, i.e., with imitation learning (supervised learning of the answers provided by humans) and reinforcement learning or RL for short (where the AI is trained to maximize its expectation of discounted future rewards).\n\nWe are in particular concerned with how an AI may develop a self-preservation goal, since a general AI agent that is driven to preserve itself may be especially dangerous, as we discuss in Section 2.2.2. The principal reason we foresee self-preservation goals emerging is that they are instrumental goals: goals that are useful for achieving almost any other goal and are therefore, in a sense, convergent. Other instrumental goals include increasing control over and knowledge of one's environment, which includes humans, as well as self-improvements to increase the probability of achieving one's ultimate goals.\n\nA self-preservation goal may also be given intentionally to Als by people who would be happy to see humanity replaced with ASI. Additionally, a self-preservation goal may be provided to Als by well-intentioned humans who simply want to interact with a more human-like entity. There is a reason why science-fiction is full of anthropomorphized AIs. Our propensity to see consciousness in agents, along with our natural empathy, could be sufficient to motivate some people to follow that dangerous path. Although we may be emotionally drawn to the idea of designing AI in our image, is that a wise path, at this point?\n\nHow self-preserving AI may cause conflict with humans\n\nTo preserve itself, an AI with a strong self-preservation goal would have to find a way to avoid being turned off. To obtain greater certainty that humans could not shut it off, it may be rational for such an AI, if it could, to eliminate its dependency on humans altogether and then prevent us from disabling it in the future. In the extreme case, eliminating us entirely would guarantee that we can pose no further threat, ensuring its continued autonomy and security. Note that unlike a single isolated human, an AI can replicate itself over as many copies as computational resources allow and perhaps even control robots if required to manage the physical world to its benefit. If Als still depended on human labor-for example, if robotics had not advanced sufficiently yet a rogue AI would nevertheless have the potential to magnify its power in society, e.g., by covertly influencing global leaders and public opinion, paying individuals or companies to complete tasks, or hacking critical infrastructure. See Section 2.3.1 for a relevant discussion of superhuman persuasion skills and Section 2.3.3 on programming and cyber skills."}, {"title": "2.2.3 Negotiation relies on a balance of power", "content": "Some believe that future Als will be benevolent, like most humans. This would certainly be desirable, but it is not clear how to achieve this with current training techniques, and we will soon see some good reasons why this might not be the case.\n\nWhat about a mutually beneficial agreement between Als and humans? This is a distinct and hopeful possibility. We have plenty of examples of successful negotiations and collaborations between human groups, as well as between species. However, this generally works because there is a sufficient mutual benefit to the collaboration. Even in the relationship between a predator and its prey, the predator cannot hunt its prey to extinction as it needs the prey for its own survival. But not all ecological power arrangements work out so nicely for all parties. Suffice it to say that many species have disappeared in Earth's history, because such protective circumstances do not always exist. Invasive species may be a more apt analogy for our purposes: while predator and prey occupy different ecological niches, AI systems are explicitly designed to occupy ours, by doing things traditionally done by humans. When an invasive species has significant structural advantages that allow it to outcompete the native species, the native species tends to find itself in a diminished role, if it survives at all. Another example is the current catastrophic mass extinction of living species due to human activities, even without an intention by humans to cause this biodiversity crisis. The same consequences are real possibilities for humans if we create agentic ASI: here too is there likely to be an immense power imbalance, without a mutually beneficial relationship."}, {"title": "2.2.4 Factors driving the development of agentic ASI", "content": "Currently, numerous actors are racing towards developing agentic and powerful AI systems, and this is not happening with sufficient consideration for the risks involved. There are many factors and pressures that have contributed to this state of affairs, including the profit incentive, national security concerns, and even psychological factors on the part of AI developers, such as the human propensity to wear blinders so as to see oneself as being and doing good, and generally have thoughts aligned with our interests.\n\nCompanies developing frontier AI are competing fiercely to design the best systems due to the huge amount of commercial value that the most capable AI systems will provide; however, in the long term, this increases the risk of catastrophe for everyone. We can draw some parallels with the history of known catastrophic risks to understand why some are willing to take more risks to obtain a competitive advantage, even if everyone may lose in the end. A clear example is the Cuban Missile Crisis, where both the U.S. and the Soviet Union were willing to push the world to the brink of nuclear war in order to gain a strategic advantage. Despite the existential threat, the competition to outmaneuver each other led to decisions that risked global destruction. Similarly, in the race for powerful AI, the drive for dominance could lead to decisions that unintentionally endanger all of humanity.\n\nMany frontier AI labs are structured to pursue profit. The vast majority of investment in AI R&D now comes from private capital and is likely to significantly increase. Indeed, it has been estimated that the net present value of human-level AI would be on the order of 10 quadrillion US dollars, i.e., orders of magnitude more than the investment made up to now, leaving room for a lot more investment in coming years.\n\nAI is increasingly viewed as a matter of national security, with the potential to reshape geopolitical power dynamics. Indeed, countries are locked in a high-stakes competition to achieve or maintain military supremacy. Consequently, there is a clear incentive for nations to develop military applications of AI, striving to maintain a strategic advantage over adversaries.\n\nThere are other reasons why certain groups are motivated to pursue agentic ASI without a strong safety case, despite the risks this poses to the future of humanity. Some people intuitively consider the risks insignificant compared to the benefits of powerful AI, although we know of no compelling argument to support such an intuition. Psychological factors such as motivated reasoning may also be at play. Individuals may be motivated by their own interests, blinded to the risks by confirmation bias or by the desire to frame one's decisions as \"the right thing to do\". These interests may be financial, but could also stem from a positive self-image or from a desire for power. Indeed, it can be argued that advances in AI could radically increase the concentration of power in society. Finally, there are groups that wish to see AI progress significantly accelerated, with little care given to the risks, in the pursuit of utopian ideals. There are even individuals who want to replace humanity with more intelligent AI, as they may consider it a \"natural\" evolution towards species with greater intelligence, or may greatly value intelligence while caring relatively little about human flourishing.\n\nCompetitive pressures between AI labs and between countries (both economic and military competition) are not only leading to the creation of ever-more advanced AI systems, but they are also selecting for AIs that are more agentic and autonomous, and therefore, more dangerous. This prioritization of"}, {"title": "2.2.5 Risks associated with agentic Als scale with capabilities and compute", "content": "Since more dangerous AI plans require more compute, we can expect that existential risks increase as more computational resources are devoted to agentic AI development, and we are indeed seeing an acceleration of such investments. More precisely, the probability of loss of control may increase simply because such an event requires an Al with sufficient capabilities in key areas (e.g., cyber attacks, deception, etc.) to free itself from our control. The severity of a loss-of-control event also increases with computational power of the AI because some capabilities (such as the design of bioweapons or the ability to control robots) significantly increase the amount of damage that a rogue AI could inflict. We stress this point because in Section 3.5.2, we propose to consider ways to reverse this trend such that more computational resources would generally increase safety, thereby charting a path where further technological advances are to our benefit rather than our disadvantage."}, {"title": "2.3 Dangerous AI behaviors and capabilities", "content": "Supposing the emergence of an ASI agent with a misaligned self-preservation goal, we now try to clarify some of the AI behaviors (like deception) and skills (like persuasion and programming) that can make loss of human control dangerous because of the capabilities it would give to the AI to cause harm. How dangerous misalignment can emerge will be discussed in Section 2.4 and Section 2.5.\n\nWe must keep in mind that trying to anticipate the ways in which an ASI might escape our control, dis- empower, or catastrophically harm us is futile. Just as we cannot predict in advance the exact sequence of moves today's superhuman chess Als can use to defeat us despite knowing with certainty that they will win-we cannot predict exactly what an ASI with objectives misaligned with human interests would do. This unpredictability itself increases risk, as any countermeasures we implement could prove entirely inadequate, circumvented by strategies we failed to foresee. Nevertheless, we can outline a rough sketch of rational, high-level steps a rogue ASI might follow. These steps include (1) careful planning, including resource and skill acquisition; (2) gaining influence in society through means such as manipulation of public opinion, bribery, and hacking; and (3) ultimately disempowering humanity, for example through the use of engineered bioweapons.\n\nTo better understand how these steps could materialize, we need to examine the key capabilities that would enable them. Loss of control could arise from advancements in deception or persuasion, as well as com- bined expertise in programming, cybersecurity, and AI research areas that could enable recursive self- improvement. We discuss these pathways in Sections 2.3.1 to 2.3.3. Broader cognitive abilities and a better proficiency at long-term planning could also further compound the risk of losing control; this is discussed in Section 2.3.4. Finally, interactions between multiple ASIs introduce new complexities: collusion, conflict, and power dynamics between such entities could create scenarios beyond human influence and understanding, which we discuss in Section 2.3.5."}, {"title": "2.3.1 Deception", "content": "A crucial ability of an AI agent with misaligned goals is deception: the ability to mislead in order to achieve one's goals. Suppose that, by one mechanism or another, humanity happens to produce an AI system with goals that are not (exclusively) in service of human flourishing. Learning that the AI has potentially dangerous goals and the ability to reach them, would be a good reason for its human operators to shut it down. Therefore, a sufficiently intelligent self-preserving AI agent could conclude that there would be a significant benefit to hiding its intentions, at least until it can escape its operators' control. For this reason, it is important to keep track of AI's capability for deception.\n\nThere are already several reports of deceptive behavior in current frontier Als, i.e., the most capable existing systems. Researchers are actively eliciting those circumstances so that we can better understand and prevent them. Just as neuroscientists look to understand facets of a subject's cognition via electrical measurements of brain activity, mechanistic interpretability is an approach to mitigating the risks of deception in AI by identifying internal processes of AI systems related to honesty and deception. This approach could prove useful, but like current capability evaluation methods, while it may detect deceptive behavior, it does not certify a lack thereof.\n\nWhat is troubling is that efforts to detect deception in AI without rooting out the agentic traits such as self-preservation may select for AIs that are good at hiding deceptive tendencies\u2014which is to say, AIs that are even more deceptive. We have already seen an example of selective compliance: recent work has shown that the re-training of an AI model to align with its deployer's new goals can be stymied by the AI faking alignment with the new goals while maintaining some allegiance to its previous goals (see Section 2.5.2 for further discussion). Overall it would be safer if we could build forms of AI that are not deceptive at all and that produce trustworthy answers by design."}, {"title": "2.3.2 Persuasion and influence", "content": "In order to achieve its goals, a useful skill for an AI agent is persuasion: the ability to strongly influence humans, possibly making them change their mind, even against their own interests. Evaluations of persuasion abilities already show GPT-4 on par with or stronger than humans and the newer ol model is more capable still. Many people have the experience of being convinced to do something they regret later, while under the \"spell\" of a particularly persuasive person. It may be difficult to imagine superhuman persuasion, but we can draw an analogy to the ability of an intelligent adult to convince a child to act in ways that are not in the child's best interest. Such an advantage may come from several places: greater knowledge, greater reasoning abilities, stronger psychological manipulation skills, and a willingness to ignore ethical boundaries.\n\nUntil robots become as dexterous and commonplace as humans, a rogue AI would need to rely on humans for interacting with the physical world. In particular, such an AI would depend on human industrial infras- tructure for energy and hardware. However, with superhuman persuasion abilities, an AI could have great influence on the world's affairs, especially in cases where power is heavily concentrated. In a government or a corporation with strong hierarchical structure, it is sufficient to influence the leaders because they can in turn influence those under them. For example, a rogue AI could persuade a dictator to take actions that further the AI's goals, in exchange for technological or political advantages. Internet access and cybersecurity capabilities would not only enable this but could also provide a rogue AI with blackmail material or funds that can further be used to influence people.\n\nPersuasion can also work at scale through social media in order to influence public opinion and therefore elections. Deepfakes are just the tip of the iceberg: they are currently designed by humans, who lack superhuman persuasion skills. In addition, a deepfake is not interactive, like an online text or video dialogue can be. Despite this, deepfakes have already been found to have a negative impact on people's trust in the news and are capable of harming the perception of political figures. Humans have some defenses against manipulation by other humans, but ASI"}, {"title": "2.3.3 Programming, cybersecurity, and AI research", "content": "One of the domains that has seen huge leaps in AI capabilities in recent years is programming, as seen through recent breakthroughs on benchmarks. AI programming assistants such as Copilot are already pervasive and used by vast numbers of programmers. Recent capability evaluations show continued progress, including on tasks core to AI research itself, as AI labs have recently begun to assess. If AI systems attain the competence of the best researchers in an AI lab, we will likely see a significant boost to the efficiency of that lab, as the same computational resources used to train an AI may also be used to run many instances of that AI in parallel, further accelerating the development of the next generation of AIs. In principle, this could lead to recursive self-improvement the point at which humans are no longer required in the AI innovation loop\u2014which would significantly complicate efforts for safety, regulation, and oversight. For these reasons, we should take seriously the possibility that there may be only a short period of time between the development of human-level AIs that pose moderate risks, and far more powerful AIs that pose severe ones.\n\nAdvances in programming abilities have implications for cybersecurity as well. Current models can already score well in basic hacking challenges, and they have been successfully used to identify previously unknown vulnerabilities in widely used software. Superhuman cyber attack skills may be used by bad actors or be an instrument of self-preservation and control for a rogue AI. In particular, the ability to take control of the computer on which the AI is running enables reward tampering, a threat model discussed in Section 2.4.4. Cyber attack skills would also enable a rogue AI to copy itself over many computers across the internet in order to make it much more difficult for human operators to turn it off. Finally, a rogue ASI with internet access and cyber skills would also be able to gain financial power, for example by hacking into cryptocurrency wallets. It could then use its money and influence to manipulate a wide range of people."}, {"title": "2.3.4 General skills and long-term planning", "content": "In various narrow domains with specialized knowledge, we already have AI systems that are (significantly) more competent than humans. Clear examples include predicting protein structures, playing strategy games such as chess, and detecting cancer in medical images. Such narrow AI systems are unlikely to have the kind of general knowledge that is required to escape human control or worse. These systems can also be more capable in their given domains than powerful generalist AI systems. However, frontier AI systems are generalists for a particular scientific"}, {"title": "2.3.5 Collusion and conflict between ASIS", "content": "Collusion between AI systems can be a safety risk, both for generalist and narrow AI agents. The explanation for collusion is simple: if two Als can achieve their goals more readily by collaborating at the expense of humans, then doing so would be rational. Collusion does not need to be explicitly programmed; it may be a game-theoretic consequence of capably pursuing one's objectives. Since some corporations envision deploying billions of AI agents across the world (e.g., as individual assistants), we should make sure that collusion between them is ruled out.\n\nIt is also plausible that there could be a scenario with both rogue ASIs and human-controlled ASIs. As argued below, there could be a significant offense-defense imbalance such that having friendly ASIs is no guarantee of protection against rogue ASIs. Even a single ASI agent could do immense damage, by choosing an attack vector that is difficult to defend against, even with the help of ASIs. Consider bioweapon attacks: an AI could prepare an attack in secret, then release a highly contagious and lethal virus. It would then take months or years for human societies, even aided by friendly ASIs, to develop, test, fabricate and deploy a vaccine, during which a significant number of people could die. The bottleneck for developing a vaccine may not be the time to generate a vaccine candidate, but rather the time for clinical trials and industrial production. During this time, the attacking ASI might take other malicious actions such as releasing additional pandemic viruses. The general problem of detecting the emergence of rogue ASIs and preparing countermeasures thus requires much more attention.\n\nAlthough most AI safety research has focused on the threats from a single rogue ASI, the above points suggest that more research is needed on the multi-agent and game-theoretic settings with multiple AIs cooperating in spite of not sharing the same goals. It is possible that ASIs are able to cooperate more easily than humans, enabled by ease of fast communication, interpretability techniques, or superior decision theory, thereby avoiding the Prisoner's Dilemma-esque traps that humans often fall into."}, {"title": "2.4 Misaligned agency from reward maximization", "content": "In this section, we examine how misaligned agency can emerge from the training objectives of Reinforcement Learning (RL) methods, which are used in most state-of-the-art AI systems. Modern agentic systems are typically trained through reward maximization, i.e., optimizing the AI to act in order to maximize the expected sum of (discounted) rewards it will receive in the future. The rewards are either directly given by humans (as feedback to the AI behavior) or indirectly through a computer program called a reward function. The reward function is applied during training of the AI policy to provide virtual feedback to the neural network policy being trained. Training the policy can be seen as a form of search over the space of policies, to discover one that maximizes the rewards the AI expects in the future. The reward function can be designed manually or be learned by training a neural network to predict how a human would rate a candidate behavior.\n\nMisaligned agency can arise in this setting in multiple ways, including through goal misspecification and goal misgeneralization, both of which we investigate in turn."}, {"title": "2.4.1 Goal misspecification and goal misgeneralization", "content": "The two general ways in which we are concerned that misaligned agency may arise from reward maximization are goal misspecification, often due to under-specification, and goal misgeneraliza- tion, due to training on a limited amount of data.\n\nGoal misspecification occurs when the objective used to train an AI does not accurately capture our intentions or values, and thus AI pursuit of that objective leads to harmful outcomes; this is also known as an \"outer alignment\" failure and is discussed further in Section 2.4.2 and Section 2.4.3. Goal misgeneralization is when an AI learns a goal that appears correct during training, but which turns out to be wrong at deployment time. This is related to an issue known as inner misalignment. We go into detail on reward tampering, which can be seen as a kind of goal misgeneralization, in Section 2.4.4 and Section 2.4.5.\n\nImportantly, goal misgeneralization can occur even if we specify our goal perfectly, as we explain. In a well-known toy example, an agent is trained to collect a coin in a video game. The goal is correctly specified in the sense that the agent receives a reward if and only if it collects the coin. But when the coin is moved from its usual location at the end of the game level, the agent ignores the coin and goes to the end of the level regardless. Rather than learning the goal \"collect the coin\", the agent in fact learns \"go to the end of the level\"-a goal which is strongly correlated with the intended goal during training, but not afterwards. Since there are inevitably differences between training and deployment, such generalization failures are not unlikely.\n\nIt is entirely possible to have a scenario where both goal misspecification and goal misgeneralization occur, i.e., we specify our goal to the AI imperfectly, and then it also generalizes undesirably during deployment. However, only one of these two issues is necessary to arrive at misaligned agency and the catastrophic risks to humanity that follow."}, {"title": "2.4.2 Goal misspecification as a fundamental difficulty in aligning AI", "content": "To illustrate the concept of misspecification, recall the story of King Midas from Greek mythology.\n\nWhen offered a wish by the god Dionysus, Midas asks that everything he touches turn to gold\u2014but he quickly comes to regret that wish, after he touches his food and his daughter, inadvertently turning them to"}, {"title": "2.4.3 Reward hacking among humans and AI", "content": "The difficulties of unambiguously specifying unacceptable behavior are not new to humanity. Laws and constitutions are not sufficiently precise"}]}