{"title": "Migician: Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models", "authors": ["You Li", "Heyu Huang", "Chi Chen", "Kaiyu Huang", "Chao Huang", "Zonghao Guo", "Zhiyuan Liu", "Jinan Xu", "Yuhua Li", "Ruixuan Li", "Maosong Sun"], "abstract": "The recent advancement of Multimodal Large Language Models (MLLMs) has significantly improved their fine-grained perception of single images and general comprehension across multiple images. However, existing MLLMs still face challenges in achieving precise grounding in complex multi-image scenarios. To address this, we first explore a Chain-of-Thought (CoT) framework that integrates single-image grounding with multi-image comprehension. While partially effective, it remains unstable and struggles to capture abstract visual information due to its non-end-to-end nature. Therefore, we introduce Migician, the first multi-image grounding model capable of performing free-form and accurate grounding across multiple images. To support this, we present the MGrounding-630k dataset, which comprises data for several multi-image grounding tasks derived from existing datasets, along with newly generated free-form grounding instruction-following data. Furthermore, we propose MIG-Bench, a comprehensive benchmark specifically designed for evaluating multi-image grounding capabilities. Experimental results demonstrate that our model achieves significantly superior multi-image grounding capabilities, outperforming the best existing MLLMs by 21.61% and even surpassing much larger 70B models. Our code, model, dataset, and benchmark are fully open-sourced at https://migician.github.io/.", "sections": [{"title": "1. Introduction", "content": "Multimodal Large Language Models (MLLMs) have exhibited significant advancements recently, demonstrating exceptional cross-modal understanding capabilities and achieving outstanding performance in various vision-language tasks [1, 7, 12, 15, 20, 28, 46]. As these models continue to evolve, their capabilities have expanded beyond image-level understanding to include fine-grained visual grounding [5, 37, 48]. This enables MLLMs to process region-"}, {"title": "2. Related Work", "content": "Multimodal Large Language Models Recent developments in multimodal large language models (MLLMs) have shifted from single image-text understanding towards more versatile capabilities [3, 23, 38, 45]. Among these efforts, some focus on enabling models to achieve fine-grained visual grounding, either through simple instruction tuning [5, 33] or by integrating additional auxiliary visual components [4, 48, 51]. However, these models primarily focus on visual grounding within a single image. Some other studies explore multi-image understanding tasks, such as multi-image comparison, reasoning, and temporal comprehension [3, 17, 23, 24, 45, 47]. Nevertheless, fine-grained visual grounding at the multi-image level remains an under-explored area. To the best of our knowledge, our proposed Migician is the first MLLM designed to address the challenge of multi-image grounding.\nMLLM Benchmarks Most existing benchmarks for evaluating MLLMs focus on single-image tasks [9, 22]. A few recent benchmarks have started assessing the performance of MLLMs on multi-image understanding [10, 17, 27, 29, 36], but they primarily emphasize image-level comprehension. The most relevant benchmark to our work is MC-Bench [42], a contemporaneous study. MC-Bench evaluates the multi-context grounding capabilities of MLLMs by asking them to accurately locate the corresponding object based on a text prompt in the correct image from a given pair. However, it exhibits limitations in the fixed number of input images and the restricted forms of queries. In contrast, the proposed MIG-Bench in this work offers more flexible task formats, focusing on evaluating models' capabilities in free-form multi-image understanding."}, {"title": "3. Task Definition", "content": "The task of free-form multi-image grounding is to identify and localize relevant visual regions across a set of images based on a free-form query. Unlike traditional grounding tasks with fixed input formats, the query in free-form multi-image grounding can be an arbitrary combination of text and images, making it highly flexible and versatile. For-"}, {"title": "4. Methods", "content": "In this section, we delve into the methods for enabling free-form multi-image grounding capabilities in MLLMs. Since free-form MIG requires the ability to perform visual grounding while simultaneously understanding multiple images, we begin by investigating a Chain-of-Thought (CoT) framework"}, {"title": "4.1. A Chain-of-Thought Framework", "content": "Although some existing MLLMs such as Qwen2-VL-7B [38] demonstrate strong multi-image understanding and single-image grounding capabilities, we find that directly prompting them to perform MIG tasks often leads to significant performance degradation as illustrated in Figure 3(a). To better explore the potential of existing models for MIG tasks, we design a Chain-of-Thought (CoT) framework that enables the model to effectively leverage and combine its exitsing abilities during the MIG execution.\nSpecifically, we decompose the MIG task into two sub-"}, {"title": "4.2. Data Construction", "content": "The CoT framework has demonstrated that an MLLM with both multi-image understanding and single-image grounding capabilities inherently holds strong potential for free-form MIG. In the following section, we employ instruction tuning to explicitly bridge these capabilities in existing MLLMs to achieve MIG. For this purpose, we first construct an instruction tuning dataset for MIG, named MGrounding-630k, with its statistics presented in Figure 4. This dataset is primarily constructed through the following two ways.\nTransforming Existing Data. By analyzing the tasks and annotation types of existing datasets, we identify multiple multi-image grounding (MIG) tasks whose data could be derived through transformation of the existing. Specifically,"}, {"title": "4.3. Instruction Tuning for MIG", "content": "Using the constructed dataset, we perform instruction tuning based on Qwen2-VL-7B [38] to develop Migician, enabling it to achieve end-to-end free-form MIG capabilities.\nTwo-Stage Training. To effectively equip the model with free-form MIG capabilities, we propose a two-stage training approach. In the first stage, the model learns to perform multi-image grounding by training on the six representative MIG tasks of MGrounding-630k, acquiring the ability to simultaneously comprehend multiple images and execute visual grounding. In the second stage, the model is further fine-tuned on free-form MIG instruction data in MGrounding-630k, enabling it to adapt to more flexible and diverse instruction types and transfer the MIG skills learned in the first stage to a broader range of scenarios. To prevent the model from forgetting its existing capabilities during training, we also incorporate single-image understanding, multi-image understanding, and single-image grounding data into each training stage. More details are in Appendix D.\nModel Merging. After the second stage of fine-tuning, we observe a trade-off between model performance and flexibility: while the model adapts to the free-form MIG instructions, there is a performance drop in common multi-"}, {"title": "5. MIG-Bench", "content": "To thoroughly assess the multi-image grounding abilities of current MLLMs, we have meticulously curated MIG-Bench. This benchmark consists of 5.9k images and 4.3k testing instances, covering 10 tasks. The distribution of these tasks is illustrated in Figure 4. The benchmark tasks are divided into two categories: spontaneous and referential multi-image grounding. Spontaneous grounding tasks require the model to recognize and ground differences or common objects across images, with a total of 1.4k testing instances. Referential grounding tasks, on the other hand, require the model to utilize different forms of reference queries (i.e., textual, visual, or multimodal) to locate the target objects, comprising 6 distinctive tasks and 2.9k testing instances. The details of these tasks are provided in Figure 2 and Appendix A.\nTo ensure diversity, the images are sourced from a variety of sources, existing datasets, web images and manually captured photos. For existing datasets, we use examples that exhibits significant movement from GOT-10k_val [13] for"}, {"title": "6. Experiments", "content": "Migician undergoes development based on the Qwen2-VL-7B [38] foundation model with a global batch size of 48, a total of 25,000 steps for the two-stage training procedure, and a learning rate of 5e-6, using 8\u00d7A100-80G GPUs. For the evaluation in our proposed MIG-Bench, we use the conventional metric Acc0.5 in referring expression comprehension [19]. This metric measures the accuracy of object localization, defining a prediction as correct if the Intersection over Union (IoU) with the ground truth bounding box is greater than 0.5."}, {"title": "6.1. Implementation Details", "content": null}, {"title": "6.2. Results on MIG-Bench", "content": "As shown in Table 1, Migician achieves the state-of-the-art performance across all tasks on MIG-bench, with an average improvement of 21.61% compared to the second-best model, Qwen2-VL-72B (38.88%), despite having significantly fewer parameters.\nNote that there is a substantial gap between human performance and that of all MLLMs across all tasks, indicating that MLLMs have significant potential for improvement in free-form MIG. In particular, for 7B-scale models, even advanced multi-image models like InternVL2-8B and Qwen2-VL-7B struggle to perform, particularly in tasks such as multi-view grounding, region locating, and correspondence.\nFor models equipped with preliminary grounding capabilities, such as mPLUG-Owl3, InternVL2 series, and Qwen2-VL series, their inherent localization ability provides an implicit advantage over other baselines. Furthermore, the proposed single-image CoT method (+CoT) effectively integrates the grounding and multi-image understanding capabilities of the MLLMs where different abilities assist each other in different reasoning steps, achieving comprehensive improvements on multi-image grounding tasks. Moreover, this approach is effective for all the aforementioned models."}, {"title": "6.3. Results on Multi-Image Understanding Benchmarks", "content": "As shown in Table 2, Migician not only establishes its multi-image grounding ability, but also remarkably stimulates its general multi-image understanding ability. In particular, Migician achieves the best average results on the multi-image understanding benchmarks. It surpasses the second-best model (Mantis) on MuirBench by 9.77%, achieving SOTA performance on MMIU and shows a 1.40% improvement on the large-scale MIBench. We attribute this to the training on a mixture of multi-image understanding and grounding data, which indicates that our proposed MGrounding-630k can enhance general multi-image comprehension."}, {"title": "6.4. Results on Single-Image Grounding Benchmarks", "content": "As presented in Table 4, Migician not only acquires free-form multi-image grounding capabilities but also demonstrates continual and consistent performance improvements on the RefCOCO series single-image grounding benchmark, surpassing specialized grounding models such as Griffon v2 and GroundingGPT by a large margin. Additionally, Migician outperforms Qwen2-VL-7B in terms of average scores."}, {"title": "7. Analysis", "content": null}, {"title": "7.1. Effects of Different CoT Strategies", "content": "The CoT framework in Section 4.1, after obtaining a referring expression, has the MLLM perform grounding in each image in a polling manner (denoted as single-image CoT), which incurs significant inference overhead. Here, we explore multi-image CoT, where the MLLM directly performs grounding across all images based on the obtained referring expression. As shown in Table 5, multi-image CoT achieves some effectiveness but it still falls significantly behind single-image CoT. In contrast, our proposed Migician is able to perform end-to-end reasoning, offering significant advantages in both efficiency and effectiveness."}, {"title": "7.2. Visual Search in High-Resolution Images", "content": "Finding visual details in high-resolution images is a challenging task, and many recent works have explored this area [41]. Typically, these tasks involve images with very high resolution, where the relevant visual information is often quite small, posing significant challenges to the model's grounding ability. In this section, we analyze and demonstrate that the multi-image grounding capability of Migician can be leveraged to efficiently address this task. Specifically, we slice a single high-resolution image into multiple sub-images and directly transform the problem into a multi-image grounding task. By utilizing the MIG ability of Migician, we can locate the regions relevant to the input question. Afterward, the model combines the identified region with the original image to generate the answer for the input question.\nWe test this approach on the V*Bench [41] and list the results in Table 3. In the table, we refer to the method that directly asks Migician to answer the question based on the original image as Migicianzero_shot, while Migician slice denotes the method that transforms this task into a MIG task as mentioned before. Its detailed implementation could be found in Appendix E. The results remarkably demonstrate the effectiveness of using the MIG approach for high-resolution image visual search. Notably, on the Attribute recognition task, Migician even surpasses the specialized visual searching system SEAL [41]."}, {"title": "7.3. Effects of Different Data on Multi-Image Understanding", "content": "As observed in Table 6, Migician shows an improvement in multi-image understanding. We further conduct an ab-"}, {"title": "8. Conclusion", "content": "In this work, we explore the task of multi-image grounding and propose Migician, the first MLLM to overcome the barriers between fine-grained visual grounding and multi-image inputs. With our proposed large-scale MGrounding-630k dataset, Migician seamlessly integrates grounding across multiple images, enabling free-form multi-image grounding. To further advance research in this area, we introduce MIG-Bench, a comprehensive benchmark for evaluating the multi-image grounding capabilities of MLLMs. Experimental results demonstrate that our model significantly outperforms existing methods. We hope this work will inspire further developments in multi-image grounding and contribute to the creation of more versatile multimodal models in the future."}, {"title": "Limitation", "content": "Despite our comprehensive discussion of the MIG challenge, there still remain several limitations. First, due to the computational budget, we haven't verified the effectiveness of our training methods on larger 70B scale models. Secondly, in spite of intensive grounding training, our model is still confronted with inaccurate grounding issue, especially in complicated or messy scenarios. Lastly, our training methods and benchmark construction mainly focus on the REC task. Although Migician possesses decent REG capacity, this topic is still insufficiently discussed."}, {"title": "A. Benchmark Tasks Definition", "content": null}, {"title": "A.1. Spontaneous Grounding", "content": "Our benchmark evaluates the spontaneous grounding through three distinct tasks below, which aim at assessing model's ability to autonomously discover insidious connections across various images and accurately recognize then locate the target.\nSpot the Difference Given two similar images with a single subtle difference, the model is instructed to recognize and ground this difference in the second image, requiring keen perceptual skills.\nCommon Object Grounding It refers to automatically recognizing and grounding the common object appearing in all images within an image group, which in our bench, each shares one definite common object.\nRobust Image Difference Grounding Models must focus on the primary difference between two images captured from slightly different perspectives, ignoring other minor variations caused by shifts in the viewpoint."}, {"title": "A.2. Reference Grounding", "content": "Textual Reference Query This challenge, which mainly includes Group Grounding, tests a model's ability to link a textual reference to a target object within its corresponding specific image. Given a set of images and one textual query, the model must identify the correct image then accurately ground the target object within it.\nVisual Reference Query These tasks examine model's ability to effectively utilize visual reference information and incorporate it into the searching process.\n(1) Visual Referring Grounding. In this task, a pair of images is provided\u2014a source image with a clear object and a target image containing multiple elements. The model must locate the referenced object in the target image.\n(2) Region Locating. Models are tasked with identifying multiple region images within a source image, which often requires perceptive and discerning observation as the model may encounter person recognition, similar object distinguishing, tiny item searching and etc.\n(3) Object Tracking. This task involves tracking a target object across a sequence of video frames. The object is highlighted with a red bounding box in the first frame, and the model must follow it throughout the sequence.\n(4) Multi-view Grounding. Here, the model must locate the same target object across multiple images taken from distinct viewpoints.\nVisual+Textual Reference Query These tasks combine information from both modalities to assess cross-modal reasoning abilities.\n(1) Correspondence. The model must ground semantically or functionally similar regions within a target image. This finer-grained task focuses on object regions rather than whole objects, demanding an in-depth understanding of visual semantics.\n(2) Reasoning. This task requires the model to perform reasoning-based grounding by integrating cross-modality information. Several examples are shown in Figure2.\nOur comprehensive benchmark offers a rich, multi-faceted evaluation across various real-world scenarios and domains, extending beyond simple image pairs to include longer and more complex image contexts. By ensuring that each task is well-defined and unambiguous, we facilitate objective and definitive assessments.."}, {"title": "B. Single-Image CoT Failure Patterns", "content": "As shown in Figure 5, the four representative failure patterns are (a) special multi-image format, (b) abstract visual information, (c) CoT error propagation, (d) step-2 inference error.\nWhen the multiple images are formatted in a special pattern, where our target object is missing in the target image, like, the information in this image is insufficient to perform grounding.\nThe abstract visual information refers to situations where the intricate visual cannot be adequately converted in textual description to perform accurate grounding. In Figure 5, the simple description \"a close-up of a woman's face\" cannot distinguish which face the target is in Image-1.\nEach reasoning step of CoT could be incorrect, which could potentially leads to the error propagation issue [44]. In Figure 5, the conclusion draw from the first step is incorrect, which directly leads to the mistake in the second step.\nThe last failure pattern refers to cases where the erroneous reasoning step appears at the second step, failing to accurately ground or follow instruction."}, {"title": "C. MGrounding-630k Data Curation Details", "content": null}, {"title": "C.1. Transforming Existing Data", "content": "Static Diff Describing the differences of the two nearly same pictures is a well discussed topic, yet they focus on the coarse-grained semantic feature, failing to precisely recognize the part of differences. After comprehensive survey on this area, we have collected high-quality and fully labeled image difference data from Spot-the-diff [16], Img-diff [18], MagicBrush [50] and CLEVR-change [32].\nDuring the construction process, we ensure the diversity of the content by (1) incorporating numerous prompt formats generated by GPT-4, (2) constructing CoT process to assist"}, {"title": "C.2. Synthesizing Free-form MIG Data", "content": "The algorithm for CLIP adaptive similarity image input is shown in Algorithm 1. We further display our prompt template for image caption generation, bounding box label refinement and instruction tuning data generation in the following pages.\nSpecifically, we deploy Qwen2-VL-7B for detailed image caption generation and Qwen2-VL-72B for bbox label refinement. The inference process is accelerated through VLLM framework [21]."}, {"title": "D. Details of Two-Stage Training", "content": "This section outlines the data proportions and their respective sources for the two training stages, as summarized in Table 10.\nIn stage 1, we leverage both single-image and multi-image datasets encompassing general understanding and grounding tasks to comprehensively enhance the model's capabilities. At this stage, the stage-1 subset from MGrounding-630k constitutes the largest portion of the training data, with a total of 530k examples. The total training examples for stage-1 is 1 million.\nIn stage 2, the focus shifts to stimulating the model's free-form MIG abilities by integrating all free-form grounding"}, {"title": "E. Evaluation Implementation", "content": "When directly requiring the model to generate bounding box coordinates for each image, due to their limited multi-image grounding ability and insufficient instruction following ability, the answer obtained in this way is largely unfaithful and mostly unsatisfactory in instruction following, failing to objectively reflecting the real grounding ability of the model.\nConsidering current model's feeble performance, we transform from directly generating all answers to polling every single image, which facilitates definite and objective evaluation. Empirically, directly generating all the bounding box coordinates for all images results in lower performance. Yet as illustrated in Table 8, Migician still demonstrates great robustness to the variation of evaluation format.\nThe performance of Migician is presented in Table 4. Although mainly targeted at multi-image grounding, Migician still maintains well on conventional single-image grounding task."}, {"title": "F. Multi-Task Learning", "content": "Our whole training process involves the learning process of multiple distinct tasks. How does the actual learning efficiency alter compared with learning these tasks separately, can they contribute to each other or comprise to some extent?\nWe conduct experiments that only expose the model to omni-task dataset and the results are shown in Table 7. It clearly reveals the conflicts of learning various tasks, with mixes multi-task training consistently surpassing omni-task learning by a huge margin. When we directly merge the checkpoints of all these trained specialized models [14], the merged model fail at excelling at most tasks, with the average performance falling behind simple multi-task learning."}, {"title": "G. Case Study", "content": "We provide detailed cases comprehensively reflecting the free-form MIG ability of Migician in Figure 6, 7, as well as our instruction tuning data details examples in Figure 8."}]}