{"title": "PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets", "authors": ["Jaeyoung Kim", "Sihyeon Lee", "Hyeon Jeon", "Keon-Joo Lee", "Hee-Joon Bae", "Bohyoung Kim", "Jinwook Seo"], "abstract": "Acute stroke demands prompt diagnosis and treatment to achieve optimal patient outcomes. However, the intricate and irregular nature of clinical data associated with acute stroke, particularly blood pressure (BP) measurements, presents substantial obstacles to effective visual analytics and decision-making. Through a year-long collaboration with experienced neurologists, we developed PhenoFlow, a visual analytics system that leverages the collaboration between human and Large Language Models (LLMs) to analyze the extensive and complex data of acute ischemic stroke patients. PhenoFlow pioneers an innovative workflow, where the LLM serves as a data wrangler while neurologists explore and supervise the output using visualizations and natural language interactions. This approach enables neurologists to focus more on decision-making with reduced cognitive load. To protect sensitive patient information, PhenoFlow only utilizes metadata to make inferences and synthesize executable codes, without accessing raw patient data. This ensures that the results are both reproducible and interpretable while maintaining patient privacy. The system incorporates a slice-and-wrap design that employs temporal folding to create an overlaid circular visualization. Combined with a linear bar graph, this design aids in exploring meaningful patterns within irregularly measured BP data. Through case studies, PhenoFlow has demonstrated its capability to support iterative analysis of extensive clinical datasets, reducing cognitive load and enabling neurologists to make well-informed decisions. Grounded in long-term collaboration with domain experts, our research demonstrates the potential of utilizing LLMs to tackle current challenges in data-driven clinical decision-making for acute ischemic stroke patients.", "sections": [{"title": "1 INTRODUCTION", "content": "Acute stroke, characterized by the sudden obstruction of blood vessels\nin the brain, demands prompt decision-making to improve treatment\nefficacy and reduce long term disability. The widespread adoption of\nElectronic Medical Records (EMRs) has significantly expanded the\nvolume of patient data available for stroke research [34]. However,\nthis rapid data growth has also amplified the presence of legacy data,\nhuman errors and inconsistencies. Data pertaining to stroke cases often\nfeature irregular time intervals, inconsistent terminologies, and a lack\nof standardized structure across organizations. Consequently, these\nfactors contribute to the creation of large and complex datasets, posing\nsignificant challenges for the effective and efficient analysis of data.\nWhile a growing body of visual analytics research focuses on ad-\ndressing these issues, there remains a notable gap in visual analytics\ntools specifically designed for time-sensitive diseases such as acute is-\nchemic stroke. Previous efforts, like Stroscope [13] and TimeSpan [30],\nmade remarkable strides in handling data irregularity and supporting\ndata analysis. However, as data complexity and volume have increased,\nmodern clinical datasets now encompass data from tens to hundreds of\nthousands of unique patients [5, 25, 26]. This scale highlights the need\nfor more advanced visual analytics systems and novel analysis work-\nflows tailored to the challenges of large, complex acute ischemic stroke\ndata. To address these challenges, we collaborated with domain experts\nto identify bottlenecks in the existing analysis workflow. We found that\nneurologists often struggle with the cognitive load imposed by large\ndatasets [8] and visual clutter, even in familiar visual representations\n(e.g., bars and lines). The prevailing analysis workflows, which rely\nheavily on neurologists' cognitive abilities, are inadequate for exploring\nsuch large, complex datasets, leading to prolonged analysis times and\npotentially overlooking critical patterns and insights.\nBased on this understanding, we established three key domain goals:\n(1) facilitating cohort construction and exploration, (2) supporting the\ndiscovery of meaningful temporal patterns, and (3) providing clinical\nevidence alongside derived outcomes. Additionally, we identified the\ncrucial importance of preserving patient privacy while working with\nsensitive medical data. These goals, along with seven key design re-\nquirements (see Sec. 4.5), informed the design and implementation\nof PhenoFlow, a visual analytics system driven by human-Large Lan-\nguage Model (LLM) collaboration to explore large, complex acute\nischemic stroke data. PhenoFlow introduces a novel analysis workflow,\nemploying LLMs for cohort construction and facilitating an iterative ex-\nploration process. This workflow empowers neurologists to effectively\nexplore and analyze vast and complex medical datasets with reduced\ncognitive load. To address the privacy concerns, PhenoFlow employs\na novel approach that utilizes metadata to generate inferences, synthe-\nsize executable code for cohort construction, and create visualizations\nwithout directly accessing raw patient data. To mitigate potential errors\ngenerated by the LLM, PhenoFlow incorporates a visual inspection\nview that allows users to debug the intermediate results and validate the\nfinal LLM-derived output. This visual feedback mechanism, which in-\ncludes visualizations of data distributions, helps to ensure the accuracy\nand reliability of the results (see Sec. 5.1).\nWe also introduce the slice-and-wrap visualization technique, de-\nsigned specifically to address the challenges posed by unevenly spaced\ntime series data. This technique leverages temporal folding [15] to split\ntemporal points into segments based on predefined intervals or biologi-\ncal cycles. Each segment is then 'wrapped' in a circular visualization,\nwhich is superimposed to facilitate the exploration of recurring patterns.\nWe opt for a circular layout for three reasons: (1) it displays lengthy\nsequences in a space-efficient way [36], (2) it aligns with the periodic\ncharacteristics inherent in blood pressure (i.e., biological clock), and (3)\nit avoids excessive visual clutter. Moreover, overlaying of information\non tracks enables efficient comparison and summarization [31]. How-\never, the identification of not only recurring patterns but also abnormal\npatterns is important to support decision-making in acute ischemic\nstroke scenarios. To intuitively reveal less frequent yet abnormal pat-\nterns, we juxtapose linear layout bar charts with interactive baselines.\nThis combination proves adept at revealing both recurring and abnormal\npatterns in irregular data (see Sec. 5.2).\nTo validate the effectiveness of PhenoFlow, we conducted a series\nof case studies. We engaged neurologists to use the system and analyze\nreal-world patient data from the CRCS-K dataset [5]. The results\ndemonstrate that our visual analytics approach enhances physicians'"}, {"title": "2 RELATED WORK", "content": "This section provides an overview of the prior research that has in-\nfluenced the design of PhenoFlow. We first discuss previous studies\non visualizing time-oriented data, including stroke patient data, high-\nlighting their contributions and limitations in handling large, complex\ndatasets (Sec. 2.1). We then delve into the growing adoption of LLMs\nin the clinical domain, examining their applications, challenges, and\npotential for enhancing medical research workflows (Sec. 2.2).\nAnalyzing time-oriented data, encompassing both time-series data and\ntemporal event sequences, is an important task in medical decision-\nmaking scenarios. However, time-oriented data in the clinical field\noften suffer from irregularity and uncertainty in both data and temporal\ndimensions [3]. In particular, blood pressure (BP) data, which is pri-\nmary data in stroke research, is measured at irregular time intervals and\ndoes not necessarily follow a regular, predictable schedule.\nA straightforward and effective method for tackling such data is trans-\nforming time-series data to temporal event sequences (i.e., temporal\nabstraction) [6, 18, 27, 33]. By doing so, irregular time-series data can\nbe converted to discrete observations collected over time and arranged\nin sequence [20]. These temporal event sequences can be visualized\nwith respect to the time axis. Lifelines [38] employed this approach.\nHowever, when analyzing large datasets, this approach can incur sub-\nstantial cognitive load. To reduce the complexity, LifeLines2 [54]\nand IDMVis [57] adopted temporal folding, which involves folding\nor splitting long data streams into daily, weekly, monthly, or yearly\nsegments to find cyclic patterns [15]. Similar to prior work, PhenoFlow\nemploys both temporal abstraction and temporal folding approaches\nto construct each patient's BP sequence at a cohort level. We aggre-\ngate each patient's BP measurements within a 24-hour segment (i.e.,\ncircadian rhythm) to efficiently summarize their BP trajectories. How-\never, these approaches come with a loss of information. Therefore, we\nalso provide support for visualizing BP data as time-series data at an\nindividual level to ensure comprehensive analysis.\nRecent research by Scheer et al. [43] provides a comprehensive\noverview of visualization techniques for time-oriented medical data.\nThe authors identify strategies employed in existing tools, such as\njuxtaposition, superposition, and explicit encoding. For example, to\nvisually address irregular time-series data (i.e., BP data), Stroscope [13]\nproposed a ripple graph that maps uncertainty between two temporal\nmeasurements by varying color intensity. Another approach that deals\nwith irregularity is using animation. TimeRider [41] reveals temporal"}, {"title": "3 BACKGROUND", "content": "This section provides an explanation on a conceptual overview of\nacute ischemic stroke and the related data that PhenoFlow aims to\nsupport. We introduce the CRCS-K dataset [5] and describe its key\ncharacteristics that informed the design of PhenoFlow.\nWe utilized the CRCS-K dataset [5], a multicenter cohort dataset con-\ntaining 324 clinical variables from about 100,000 acute ischemic stroke\npatients, collected from 17 university hospitals in South Korea since\n2011. Multiple groups of neurologists have reviewed the data, ensuring\nits credibility and relevance for stroke research. The dataset is seg-\nmented into five distinct parts, encompassing demographic information,\nrisk factors, treatment and examination details, follow-up data, and\nvital BP measurements.\nGiven the dataset's extensive longitudinal span and scale, careful\ncuration of data is necessary. Neurologists E1 and E2, co-authors of\nthis paper and experts in acute ischemic stroke from Seoul National\nUniversity Bundang Hospital and Korea University Guro Hospital,\nrespectively, have led the data curation process. With 35 and 24 years of\nexperience in the field, respectively, they selected 60 clinical variables\nand 5 BP-related variables crucial for acute ischemic stroke research.\nThis meticulous selection process supports PhenoFlow's development,\nensuring it effectively meets the nuanced demands of acute ischemic\nstroke research.\nAcute ischemic stroke, which accounts for approximately 85% of all\nstroke cases [2], occurs when a blood clot or other obstruction blocks\nblood flow to the brain, leading to rapid cell death and neurological\ndeficits [4]. In acute ischemic stroke, when BP surpasses 150-160\nmmHg for mean arterial pressure, the autoregulation mechanism that\nnormally maintains stable cerebral blood flow may become impaired.\nThis impairment can lead to unstable cerebral blood flow, potentially\nworsening brain damage [24]. Consequently, BP serves as a crucial\nindicator for assessing patients' conditions [28, 29]. Moreover, clinical\nevents such as treatment interventions (e.g., thrombolysis or thrombec-\ntomy) and stroke recurrence contribute to understanding the underlying\nreasons for fluctuations in patients' BP.\nPhenoFlow incorporates three key types of data: event data, clinical\ndata, and BP data.\nEvent data capture significant medical events within patients'\nrecords, such as Intravenous Thrombolysis (IVT), Intra-Arterial Throm-\nbolysis (IAT), and stroke recurrence. These events are discrete data\npaired with time. They can be recorded not only at specific points but\nalso as intervals, indicating events that happen over periods of time\n(e.g., IA surgery from start to end)."}, {"title": "4 PROBLEM DEFINITION AND DESIGN", "content": "In this section, we describe the design process and present the domain\ngoals and design requirements. We characterized the problem at a\ndomain level and connected it to specific visual analysis tasks.\nOur goal is to develop a visual analytics system to support neurologists.\nTo understand their expectations and needs, we conducted a design\nprocess based on the framework established by Sedlmair et al. [44].\nThe process was divided into two primary phases.\n(Phase I) Initial exploration and domain analysis. In the first phase,\nour aim was to understand the current workflow of the experts and\nidentify their specific needs. Over a span of six months, we conducted\nweekly meetings with two neurologists. We observed their existing\nworkflows and comprehensively reviewed domain literatures pertaining\nto acute ischemic stroke. Based on the insights collected from this\nprocess, we formulated initial domain goals and visual analysis tasks.\n(Phase II) Prototype development and iterative refinement. In the\nsubsequent seven months, we focused on the development and refine-\nment of PhenoFlow. We first developed an initial system and presented\nit to the neurologists (E1 and E2). They were encouraged to explore the\nsystem and articulate their thoughts through think-aloud methods [52].\nBased on their feedback, we iteratively refined PhenoFlow.\nStroke research, mirroring the clinical diagnosis process [17, 56], fol-\nlows a five-stage process. The first three stages include data wrangling,\ncohort interpretation, and hypothesis generation. In the subsequent\nstages, neurologists delve into data exploration and explore insights via\ndescriptive statistics. This cycle of data exploration and insight genera-\ntion is iteratively repeated until a working hypothesis is established.\nIn Phase I, we identified that the data wrangling, patient cohort in-\nterpretation, and iterative data exploration were significant bottlenecks\nwithin the neurologists' analysis workflow. Neurologists explained that,\nwhile descriptive statistics offer valuable information, they may lack\nintuitiveness. Conversely, visualizations provide an intuitive grasp, but\ninterpreting their meaning, particularly in the context of large datasets,"}, {"title": "5 DESIGN OF PHENOFLOW", "content": "PhenoFlow is a visual analytics tool designed specifically for acute\nischemic stroke patients. It enables a wider range of data exploration\nby leveraging a Large Language Model (LLM) as a data wrangler.\nIn the previous workflow, neurologists had to fulfill both the roles of\ndata wranglers and researchers. However, in our proposed workflow,\nneurologists can concentrate on clinical decision-making while the\nLLM handles the data wrangling tasks.\nWhile previous research has explored the use of Large Language Mod-\nels (LLMs) as medical task solvers [16, 21, 23, 37, 42, 55], LLMs still\nexhibit hallucinations in medical domain tasks [50]. Furthermore, the\nprecise performance of LLMs in this domain remains uncertain. There-\nfore, in designing the human-LLM collaboration workflow, we aimed\nto keep many possibilities open and collaborated with neurologists to\naddress various medical tasks using the LLM. We will briefly discuss\nour exploratory phase and then explain the final design of our workflow.\nExploratory Phase: This research began in early 2023 when there\nwas no prior work testing the performance of LLMs in medical tasks.\nInitially, we collaborated iteratively with neurologists to identify five\ndistinct medical research tasks: (1) generating medical hypotheses from\ndata, (2) recommending literature for medical research, (3) suggesting\nresearch methods for validating hypotheses, (4) performing medical\ndata wrangling tasks, and (5) generating and interpreting visualiza-\ntions. We utilized GPT4-32k for these tasks and employed few-shot\nprompting to guide the LLMs. To address complex medical knowl-\nedge, we initially used retrieval-augmented generation (RAG) that\nincluded prior medical research papers. However, due to bias, retrieval errors, and computational overhead, we later discontinued the use of\nRAG. To reduce hallucination and enable inspection, we adopted a\nself-reflection approach [46]. We utilized the PICO (Population, Inter-\nvention, Comparison, Outcomes) hypotheses and research protocols\nfrom our collaborators' published studies for testing. Our initial goal:\nwas to utilize LLMs as universal companions in medical research.\nResults: We found that LLMs struggled with more creative and\nexpansive tasks, such as (1) generating medical hypotheses and (2)\nrecommending literature for medical research. When provided with\nthe PICO hypothesis as input and asked to recommend a medical\nhypothesis and relevant literature, LLMs exhibited hallucinations in\nhigh-temperature settings (e.g., suggesting non-existent literature and\ngenerating PICO hypotheses involving incorrect knowledge). In low-\ntemperature settings, LLMs only modified part of the variables in the\nPICO hypothesis. However, LLMs demonstrated impressive perfor-\nmance in (3) research method recommendation tasks. When presented\nwith the PICO hypothesis, LLMs successfully recommended research\nmethods similar to those used in actual studies. This result surprised\nour collaborator (E2), but in terms of reproducibility, LLMs did not\nconsistently yield the same results. These findings posed two significant\nchallenges: reproducibility (i.e., consistency) and explainability.\nIn the (4) data wrangling tasks, LLMs demonstrated much better\nresults than in previous tasks. We discovered that LLMs are capable\nof extracting data that meet user requirements from large and intricate\ndatasets. Furthermore, with proper instructions, they can handle miss-\ning data by leveraging existing information. However, we encountered\nanother critical challenge in these results: the privacy issue. Lastly,\nin (5) generating and interpreting visualization tasks, LLMs still strug-\ngled to address complex visualizations effectively. Similarly to data\nwrangling tasks, privacy concerns also exist in these tasks.\nFinal Design: The prior results led us to design a collaborative work-\nflow incorporating a LLM as a data wrangler. The LLM addresses three\nmedical data wrangling tasks: (1) standardizing inconsistent terminolo-\ngies, (2) extracting pertinent data from large, complex datasets, and\n(3) providing interpretable explanations. Ideally, these tasks would be\nundertaken by a professional data wrangler or with comprehensive doc-\numentation of the data. However, such resources are often unavailabl\nor impractical to employ in real-world clinical settings.\nTo ensure reproducibility, we employed an LLM-based code gen-\neration approach with few-shot prompting to synthesize executable\ncode from user requests. This executable code ensures the consistency\nof the derived results. To offer explainability for the yielded results,\nwe utilized a combination of natural language explanations and small\nmultiples. The natural language explanations are derived through"}, {"title": "6 EVALUATION", "content": "In this section, we present the results of case studies conducted with\nfour experienced neurologists. Prior to the hour-long case study session,\nwe introduced PhenoFlow to participants by using a demo scenario.\nThe demo scenario only utilized patients' demographic information.\nThen, participants were encouraged to explore the CRCS-K dataset [5]\nwhile articulating their thoughts through think-aloud methods [52].\nFurther feedback was gathered through post-study interviews. None of\nthe experts were collaborators or co-authors of this paper."}, {"title": "7 FUTURE DIRECTIONS", "content": "The series of case studies demonstrates that PhenoFlow empowers\nneurologists to derive meaningful insights and expand their explo-\nration range in the complex and extensive medical dataset. Despite\nthe promising results, we found there are still limitations and areas for\nfuture research. We further discuss future directions for integrating\nLLM into visual analytics tasks within the medical domain.\nMost visual analytics systems assume that the target data has undergone\nsome degree of data wrangling, but unfortunately, the majority of real-world medical datasets do not align with this assumption. Numerous\nexperts have stated that data wrangling for medical data is messy, repet-\nitive, and resource-intensive, requiring a significant amount of man-\npower. Furthermore, the current data wrangling methods for medical\ndatasets struggle to keep up with the rapid accumulation of data. This\nchallenge persists even when neurologists analyze the wrangled data.\nGiven that they are not data analysts, extracting relevant insights from\nthese extensive and complex datasets to answer their research questions\nbecomes a daunting task. To tackle this challenge, we employed LLM\nas a data wrangler, with the LLM data wrangling capability emerging as\none of the most appreciated features of PhenoFlow. Although we have\nnot formally studied the impact of varying data quality on performance,\nthrough iterative testing of the LLM data wrangler with the extensive\nCRCS-K dataset, we believe this workflow can be extended to other\ndiseases or domains with large structured datasets.\nHowever, we want to emphasize that LLM is a \"fragile\" data wran-\ngler, possessing inherent power but facing challenges in 1) directing\nits unpredictable behavior, 2) understanding and explaining its actions,\nand 3) ensuring consistent outcomes. Prompt engineering emerges as\nthe most effective means to influence LLM, but a universally reliable\n'magic' prompt for medical tasks remains elusive. To ensure verifiabil-\nity and credibility, LLM workflows should be designed with multiple\nreasoning steps while keeping the design concise. While granting LLMs\nmore freedom (i.e., high temperature), constructing long task chains,\nor incorporating complex modules may enhance their performance on\ncomplex medical tasks, it also raises the risk of unpredictable behavior.\nAdditionally, the highly disorganized metadata or data structure may\npose difficulties for the LLMs during the inference step. Therefore, we\nadvocate for integrating LLMs in workflows where 1) results can be\nvalidated, 2) tasks don't demand a high level of complexity or creativity,\nand 3) LLMs can either reduce repetitive tasks for domain experts or\naugment their expertise like data wrangling."}]}