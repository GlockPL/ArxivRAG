{"title": "PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets", "authors": ["Jaeyoung Kim", "Sihyeon Lee", "Hyeon Jeon", "Keon-Joo Lee", "Hee-Joon Bae", "Bohyoung Kim", "Jinwook Seo"], "abstract": "Acute stroke demands prompt diagnosis and treatment to achieve optimal patient outcomes. However, the intricate and irregular nature of clinical data associated with acute stroke, particularly blood pressure (BP) measurements, presents substantial obstacles to effective visual analytics and decision-making. Through a year-long collaboration with experienced neurologists, we developed PhenoFlow, a visual analytics system that leverages the collaboration between human and Large Language Models (LLMs) to analyze the extensive and complex data of acute ischemic stroke patients. PhenoFlow pioneers an innovative workflow, where the LLM serves as a data wrangler while neurologists explore and supervise the output using visualizations and natural language interactions. This approach enables neurologists to focus more on decision-making with reduced cognitive load. To protect sensitive patient information, PhenoFlow only utilizes metadata to make inferences and synthesize executable codes, without accessing raw patient data. This ensures that the results are both reproducible and interpretable while maintaining patient privacy. The system incorporates a slice-and-wrap design that employs temporal folding to create an overlaid circular visualization. Combined with a linear bar graph, this design aids in exploring meaningful patterns within irregularly measured BP data. Through case studies, PhenoFlow has demonstrated its capability to support iterative analysis of extensive clinical datasets, reducing cognitive load and enabling neurologists to make well-informed decisions. Grounded in long-term collaboration with domain experts, our research demonstrates the potential of utilizing LLMs to tackle current challenges in data-driven clinical decision-making for acute ischemic stroke patients.", "sections": [{"title": "1 INTRODUCTION", "content": "Acute stroke, characterized by the sudden obstruction of blood vessels in the brain, demands prompt decision-making to improve treatment efficacy and reduce long term disability. The widespread adoption of Electronic Medical Records (EMRs) has significantly expanded the volume of patient data available for stroke research [34]. However, this rapid data growth has also amplified the presence of legacy data, human errors and inconsistencies. Data pertaining to stroke cases often feature irregular time intervals, inconsistent terminologies, and a lack of standardized structure across organizations. Consequently, these factors contribute to the creation of large and complex datasets, posing significant challenges for the effective and efficient analysis of data.\nWhile a growing body of visual analytics research focuses on ad- dressing these issues, there remains a notable gap in visual analytics tools specifically designed for time-sensitive diseases such as acute is- chemic stroke. Previous efforts, like Stroscope [13] and TimeSpan [30], made remarkable strides in handling data irregularity and supporting data analysis. However, as data complexity and volume have increased, modern clinical datasets now encompass data from tens to hundreds of thousands of unique patients [5, 25, 26]. This scale highlights the need for more advanced visual analytics systems and novel analysis work- flows tailored to the challenges of large, complex acute ischemic stroke data. To address these challenges, we collaborated with domain experts to identify bottlenecks in the existing analysis workflow. We found that neurologists often struggle with the cognitive load imposed by large datasets [8] and visual clutter, even in familiar visual representations (e.g., bars and lines). The prevailing analysis workflows, which rely heavily on neurologists' cognitive abilities, are inadequate for exploring such large, complex datasets, leading to prolonged analysis times and potentially overlooking critical patterns and insights.\nBased on this understanding, we established three key domain goals: (1) facilitating cohort construction and exploration, (2) supporting the discovery of meaningful temporal patterns, and (3) providing clinical evidence alongside derived outcomes. Additionally, we identified the crucial importance of preserving patient privacy while working with sensitive medical data. These goals, along with seven key design re- quirements (see Sec. 4.5), informed the design and implementation of PhenoFlow, a visual analytics system driven by human-Large Lan- guage Model (LLM) collaboration to explore large, complex acute ischemic stroke data. PhenoFlow introduces a novel analysis workflow, employing LLMs for cohort construction and facilitating an iterative ex- ploration process. This workflow empowers neurologists to effectively explore and analyze vast and complex medical datasets with reduced cognitive load. To address the privacy concerns, PhenoFlow employs a novel approach that utilizes metadata to generate inferences, synthe- size executable code for cohort construction, and create visualizations without directly accessing raw patient data. To mitigate potential errors generated by the LLM, PhenoFlow incorporates a visual inspection view that allows users to debug the intermediate results and validate the final LLM-derived output. This visual feedback mechanism, which in- cludes visualizations of data distributions, helps to ensure the accuracy and reliability of the results (see Sec. 5.1).\nWe also introduce the slice-and-wrap visualization technique, de- signed specifically to address the challenges posed by unevenly spaced time series data. This technique leverages temporal folding [15] to split temporal points into segments based on predefined intervals or biologi- cal cycles. Each segment is then 'wrapped' in a circular visualization, which is superimposed to facilitate the exploration of recurring patterns. We opt for a circular layout for three reasons: (1) it displays lengthy sequences in a space-efficient way [36], (2) it aligns with the periodic characteristics inherent in blood pressure (i.e., biological clock), and (3) it avoids excessive visual clutter. Moreover, overlaying of information on tracks enables efficient comparison and summarization [31]. How- ever, the identification of not only recurring patterns but also abnormal patterns is important to support decision-making in acute ischemic stroke scenarios. To intuitively reveal less frequent yet abnormal pat- terns, we juxtapose linear layout bar charts with interactive baselines. This combination proves adept at revealing both recurring and abnormal patterns in irregular data (see Sec. 5.2).\nTo validate the effectiveness of PhenoFlow, we conducted a series of case studies. We engaged neurologists to use the system and analyze real-world patient data from the CRCS-K dataset [5]. The results demonstrate that our visual analytics approach enhances physicians' ability to derive meaningful insights and increases their capacity for navigating complex, large datasets with minimal cognitive load.\nWhile the case studies demonstrated promising results, we acknowl- edge limitations and areas for future research. One key issue raised by neurologists during our case studies is the challenge of validating findings generated through visual analytics tools. To address this, we propose developing techniques that enhance the visual interpretation skills of domain experts and enable them to better understand and trust the insights derived from visualizations. Another limitation relates to the fragility of LLMs as data wranglers. While LLMs have shown impressive data wrangling capabilities, ensuring their consistent and reliable behavior remains a challenge. We discuss potential approaches to mitigate this issue. Based on these insights, we propose several future directions for developing visual analytics tools that incorporate LLMs for exploring large, complex medical datasets (see Sec. 7.1).\nIn summary, our main contributions are:\n\u2022 Three domain goals and seven design requirements identified through close collaboration with domain experts.\n\u2022 PhenoFlow, a visual analytics tool driven by human-LLM col- laboration for exploring large, complex medical data. It inte- grates several LLM-based techniques with novel visualization approaches.\n\u2022 Case studies that demonstrate PhenoFlow's strengths, limitations, ability to discover meaningful patterns and trends in acute is- chemic stroke data."}, {"title": "2 RELATED WORK", "content": "This section provides an overview of the prior research that has in- fluenced the design of PhenoFlow. We first discuss previous studies on visualizing time-oriented data, including stroke patient data, high- lighting their contributions and limitations in handling large, complex datasets (Sec. 2.1). We then delve into the growing adoption of LLMs in the clinical domain, examining their applications, challenges, and potential for enhancing medical research workflows (Sec. 2.2).\n2.1 Time-Oriented Data and Stroke Visualization\nAnalyzing time-oriented data, encompassing both time-series data and temporal event sequences, is an important task in medical decision- making scenarios. However, time-oriented data in the clinical field often suffer from irregularity and uncertainty in both data and temporal dimensions [3]. In particular, blood pressure (BP) data, which is pri- mary data in stroke research, is measured at irregular time intervals and does not necessarily follow a regular, predictable schedule.\nA straightforward and effective method for tackling such data is trans- forming time-series data to temporal event sequences (i.e., temporal abstraction) [6, 18, 27, 33]. By doing so, irregular time-series data can be converted to discrete observations collected over time and arranged in sequence [20]. These temporal event sequences can be visualized with respect to the time axis. Lifelines [38] employed this approach. However, when analyzing large datasets, this approach can incur sub- stantial cognitive load. To reduce the complexity, LifeLines2 [54] and IDMVis [57] adopted temporal folding, which involves folding or splitting long data streams into daily, weekly, monthly, or yearly segments to find cyclic patterns [15]. Similar to prior work, PhenoFlow employs both temporal abstraction and temporal folding approaches to construct each patient's BP sequence at a cohort level. We aggre- gate each patient's BP measurements within a 24-hour segment (i.e., circadian rhythm) to efficiently summarize their BP trajectories. How- ever, these approaches come with a loss of information. Therefore, we also provide support for visualizing BP data as time-series data at an individual level to ensure comprehensive analysis.\nRecent research by Scheer et al. [43] provides a comprehensive overview of visualization techniques for time-oriented medical data. The authors identify strategies employed in existing tools, such as juxtaposition, superposition, and explicit encoding. For example, to visually address irregular time-series data (i.e., BP data), Stroscope [13] proposed a ripple graph that maps uncertainty between two temporal measurements by varying color intensity. Another approach that deals with irregularity is using animation. TimeRider [41] reveals temporal aspects using an animated scatter plot. Inspired by these prior works, we developed two visualizations to handle irregular BP data. At the cohort level, we mapped the data density of each segment of BP sequences to opacity. At the individual level, we propose a novel slice-and-wrap design that superimposes multiple circular visualizations to support the visual exploration of recurring patterns. This visualization is juxtaposed with a linear bar graph, which facilitates the exploration of less frequent but abnormal patterns, offering a comprehensive view of the individual patient's BP data.\nAnother study that helps clinicians visually analyze stroke patient data is TimeSpan [30]. TimeSpan supports the exploration of both multi-dimensional and temporal attributes of acute ischemic stroke patients. Similar to TimeSpan [30] and Stroscope [13], our solution's primary goal is to support the exploration of both multi-dimensional and temporal data of acute ischemic stroke patients. However, PhenoFlow goes beyond prior work by enabling the exploration of voluminous and complex clinical datasets (see Sec. 3.1). By leveraging a large language model as a data wranger, PhenoFlow empowers neurologists to efficiently navigate and derive insights from extensive, multifaceted stroke datasets, addressing the challenges posed by the increasing scale and complexity of modern clinical data."}, {"title": "2.2 LLMs for Clinical Research", "content": "The adoption of natural language processing (NLP) techniques in the medical domain has significantly increased in recent years. Especially after the introduction of GPT-3 [7] and GPT-4 [1], several LLMs have emerged that can achieve expert-level performance in the medical domain [47, 48]. Concurrent with this trend, we explored opportunities in the clinical data visualization domain to reduce the cognitive load on neurologists during the analysis process and enhance their efficiency with the human-LLM collaborative workflow. He et al. classified LLMs' fundamental tasks in the healthcare domain into six areas: Named Entity Recognition (NER), Relation Extraction (RE), Text Classification (TC), Semantic Textual Similarity (STS), Question Answering (QA), and Dialog [22]. Among these areas, PhenoFlow utilizes LLMs for STS and QA, which are part of the data wrangling tasks.\nSTS evaluates the extent to which two phrases or sentences con- vey the same meaning. In the clinical domain, STS is often used to address challenges related to inconsistent terminologies and human errors in electronic health records (EHRs). The National NLP Clinical and BioCreative/Open Health NLP challenge [40] demonstrated that STS can help reduce mistakes and disorganization in medical datasets. Our dataset also includes human errors and inconsistent terminologies introduced during the data transcription and integration process. These obstacles not only hinder the interpretability of data but also generate substantial cognitive loads for neurologists during the data wrangling. QA typically refers to a task that involves generating or retrieving answers for given questions. Traditional QA systems often encounter challenges within the medical domain, primarily stemming from the vast amount of domain-specific knowledge [19]. However, with the advent of powerful LLMs such as GPT-4 [1], prompting and in-context learning-based QA systems have been developed. Hemidi et al. [21] evaluated the performance of ChatGPT, Google Bard, and Claude for patient QA tasks from EHRs. Similarly, several prior studies report the QA performance of GPT-3.5 and GPT-4 in various medical domains such as dementia, bariatric, and general surgery [37, 42, 55]. While LLMs show impressive results, there are mixed findings in the literature. Some studies suggest that LLMs outperform medical experts [23], while others indicate that LLMs do not significantly surpass experts [16]. Moreover, modern LLMs such as GPT-4, Claude, and Bard maintain proprietary architectures, posing challenges for quantitative performance evaluation in many uncertain areas.\nTo overcome the hurdles in medical data analysis and evaluate the performance of LLMs in this domain, we adopted GPT-4 with few-shot prompting, multi-step reasoning, and self-reflection. We tested the performance of LLMs for multiple medical research tasks and then finalized the design of the workflow, which leverages the strengths of LLMs while mitigating their limitations through human supervision and visual feedback mechanisms. The proposed workflow aims to reduce the cognitive load on neurologists during the data analysis process and enhance the efficiency of the analysis. We will discuss our approach and results in detail in later section (see Sec 5.1)."}, {"title": "3 BACKGROUND", "content": "This section provides an explanation on a conceptual overview of acute ischemic stroke and the related data that PhenoFlow aims to support. We introduce the CRCS-K dataset [5] and describe its key characteristics that informed the design of PhenoFlow.\n3.1 Collaborator and Dataset Description\nWe utilized the CRCS-K dataset [5], a multicenter cohort dataset con- taining 324 clinical variables from about 100,000 acute ischemic stroke patients, collected from 17 university hospitals in South Korea since 2011. Multiple groups of neurologists have reviewed the data, ensuring its credibility and relevance for stroke research. The dataset is seg- mented into five distinct parts, encompassing demographic information, risk factors, treatment and examination details, follow-up data, and vital BP measurements.\nGiven the dataset's extensive longitudinal span and scale, careful curation of data is necessary. Neurologists E1 and E2, co-authors of this paper and experts in acute ischemic stroke from Seoul National University Bundang Hospital and Korea University Guro Hospital, respectively, have led the data curation process. With 35 and 24 years of experience in the field, respectively, they selected 60 clinical variables and 5 BP-related variables crucial for acute ischemic stroke research. This meticulous selection process supports PhenoFlow's development, ensuring it effectively meets the nuanced demands of acute ischemic stroke research.\n3.2 Acute Ischemic Stroke\nAcute ischemic stroke, which accounts for approximately 85% of all stroke cases [2], occurs when a blood clot or other obstruction blocks blood flow to the brain, leading to rapid cell death and neurological deficits [4]. In acute ischemic stroke, when BP surpasses 150-160 mmHg for mean arterial pressure, the autoregulation mechanism that normally maintains stable cerebral blood flow may become impaired. This impairment can lead to unstable cerebral blood flow, potentially worsening brain damage [24]. Consequently, BP serves as a crucial indicator for assessing patients' conditions [28, 29]. Moreover, clinical events such as treatment interventions (e.g., thrombolysis or thrombec- tomy) and stroke recurrence contribute to understanding the underlying reasons for fluctuations in patients' BP.\n3.3 Data Characteristics\nPhenoFlow incorporates three key types of data: event data, clinical data, and BP data.\nEvent data capture significant medical events within patients' records, such as Intravenous Thrombolysis (IVT), Intra-Arterial Throm- bolysis (IAT), and stroke recurrence. These events are discrete data paired with time. They can be recorded not only at specific points but also as intervals, indicating events that happen over periods of time (e.g., IA surgery from start to end).\nClinical data include discrete clinical features, such as the Modified Rankin Score (mRS), risk factors, demographics, medications, and the Trial of Org 10172 in Acute Stroke Treatment (TOAST) classifica- tion. These data are all categorical and remain unchanged over time, representing static entries at the time of collection.\nBP (Blood pressure) data record hemodynamic information from patients' onset to discharge. These measurements are typically taken at irregular time intervals, which can lead to complexity in analysis. To address this issue, neurologists often aggregate BP measurements using a fixed time interval, such as the circadian rhythm (24-hour cycle). Our collaborators specifically requested to follow the circadian rhythm when starting the analysis. As a result, PhenoFlow was designed to aggregate and visualize BP data in 24-hour increments by default.\nDepending on the analytical approach, BP measurements can be treated as either time-series or temporal event sequences. When the fluctuation of BP values (i.e., patterns) is important, BP data are treated as time-series data. The most commonly studied patterns, such as repeated spikes, sustained high BP, and sharp drops [32], typically indicate a negative health transition in the patient.\nConversely, when understanding the general health transition pattern of multiple patients, BP data are treated as temporal event sequences. For example, most acute ischemic stroke patients are hospitalized with high BP ranges, typically greater than 160 mmHg for systolic blood pressure (SBP). Neurologists may want to construct a cohort that shares similar clinical conditions, and analyze BP trajectories of patients in the cohort from admission to discharge. This exploration can be related to certain clinical questions, such as \"Given patients who share similar clinical conditions, should clinicians aggressively lower BP to target levels using medication, or would a conservative approach of waiting for natural BP normalization be more advisable?\"\nPhenoFlow's design takes into account these characteristics of BP data. At a cohort level, PhenoFlow treats BP measurements as temporal event sequences to support a more concise exploration. At an individual level, PhenoFlow addresses BP measurements as time-series data to support pattern analysis and comparison."}, {"title": "4 PROBLEM DEFINITION AND DESIGN", "content": "In this section, we describe the design process and present the domain goals and design requirements. We characterized the problem at a domain level and connected it to specific visual analysis tasks.\n4.1 Design Process\nOur goal is to develop a visual analytics system to support neurologists. To understand their expectations and needs, we conducted a design process based on the framework established by Sedlmair et al. [44]. The process was divided into two primary phases.\n(Phase I) Initial exploration and domain analysis. In the first phase, our aim was to understand the current workflow of the experts and identify their specific needs. Over a span of six months, we conducted weekly meetings with two neurologists. We observed their existing workflows and comprehensively reviewed domain literatures pertaining to acute ischemic stroke. Based on the insights collected from this process, we formulated initial domain goals and visual analysis tasks.\n(Phase II) Prototype development and iterative refinement. In the subsequent seven months, we focused on the development and refine- ment of PhenoFlow. We first developed an initial system and presented it to the neurologists (E1 and E2). They were encouraged to explore the system and articulate their thoughts through think-aloud methods [52]. Based on their feedback, we iteratively refined PhenoFlow.\n4.2 Current Analysis Workflow and Limitations\nStroke research, mirroring the clinical diagnosis process [17, 56], fol- lows a five-stage process. The first three stages include data wrangling, cohort interpretation, and hypothesis generation. In the subsequent stages, neurologists delve into data exploration and explore insights via descriptive statistics. This cycle of data exploration and insight genera- tion is iteratively repeated until a working hypothesis is established.\nIn Phase I, we identified that the data wrangling, patient cohort in- terpretation, and iterative data exploration were significant bottlenecks within the neurologists' analysis workflow. Neurologists explained that, while descriptive statistics offer valuable information, they may lack intuitiveness. Conversely, visualizations provide an intuitive grasp, but interpreting their meaning, particularly in the context of large datasets, presents a considerable challenge. Although previous works have aimed to facilitate interpretation using familiar visual representations [13, 30], the growing size and complexity of datasets also escalate the cogni- tive load associated with interpreting visual encodings. Furthermore, efficient visual encodings for irregularly spaced data (i.e., BP data) are lacking. Neurologists emphasized the need for a more intuitive visual analytics tool that allows for visual exploration of large medical datasets with reduced cognitive load."}, {"title": "4.3 Domain Goals", "content": "In collaboration with experts, we have identified three domain goals:\n(G1) Define and explore patient cohorts with various patient at- tributes but with less cognitive load: Acute ischemic stroke analy- sis requires the examination of multiple patient attributes to interpret patient outcomes. However, neurologists are not data wranglers or analysts. Therefore, as data volumes and research question complexity increase, this process becomes a significant bottleneck.\n(G2) Identify both recurring and abnormal patterns in irregular BP data: The main attribute in acute ischemic stroke research is BP data [28, 29]. Unlike typical time series data, BP data cannot be contin- uously and evenly measured. While statistical methods exist to handle such data, visually identifying meaningful patterns in these irregular data remains challenging.\n(G3) Provide clinical evidence with derived outcomes: Neurologists aim to present clinical evidence alongside the tool's results. For in- stance, the visualization of a sudden spike in BP might be dismissed as a temporary or stress-related response. However, if such a pattern coin- cides with specific clinical events, such as recurrence of conditions or medication administration, it could significantly strengthen the insight that these spike patterns indicate a worsening of the patient's condition."}, {"title": "4.4 Visual Analysis Tasks", "content": "We translated domain goals into specific visual analysis tasks and vali- dated them in Phase I. The first four tasks (T1-T4) were directly derived from our discussions on domain goals. The last one (T5) was added during Phase II. These five tasks ensure that the design of PhenoFlow is seamlessly aligned with neurologists' needs.\nT1. Iteratively define, filter, and refine patient cohorts. Constructing patient cohorts is crucial in acute ischemic stroke research. Similar to the information-seeking mantra, neurologists often start their explo- ration by grouping patients who share similar phenotypes into cohorts. Through this process, they establish, refine, and validate their hypothe- ses. Acute ischemic stroke is a complex disease influenced by multiple factors that can affect patient outcomes. As a result, the process of con- structing and refining cohorts often involves multiple iterations (G1).\nT2. Compare multiple BP trajectories. BP trajectories unveil the health transition of patients or cohorts in acute ischemic stroke. Neu- rologists often need to examine them in a detailed view and compare them with a comparison group, either between patients or cohorts. The objective of this task is to identify distinctive or similar patterns in BP trajectory and facilitate further exploration. (G2).\nT3. Summarize BP trajectories of multiple patients within a cohort. Neurologists are interested in identifying the health transition pattern, primarily derived from BP trajectories, for patients within a cohort. BP trajectories play a crucial role in answering clinical questions (e.g., \"Has a patient's BP reached a target value?\"). Thus, summarizing the BP trajectories of multiple patients in a target cohort is essential for understanding the cohort's characteristics (G1).\nT4. Display clinical evidence with BP trajectory. As mentioned earlier, BP trajectory is a crucial factor in interpreting patients' health status in acute ischemic stroke research. However, it is hard to reach clinical insights solely relying on it. Reaching clinical insights often requires further analysis of multiple clinical factors. Therefore, to rein- force the reliability and interpretability of findings, clinical evidence should be displayed alongside BP trajectories (G3).\nT5. Iteratively define and manage sub-cohorts. Neurologists often engage in multiple iterations of cohort construction and refinement. This process typically begins with the creation of cohorts, which are primarily informed by medical knowledge. Then they partition these cohorts into smaller sub-cohorts based on various clinical attributes, such as outcomes, risk factors, and BP trajectories. Effective manage- ment of sub-cohorts is particularly crucial for narrowing the scope of research and refining hypotheses. Therefore, maintaining the context of these sub-cohorts and facilitating their construction is essential."}, {"title": "4.5 Design Requirements", "content": "Based on our observations and feedback from experts, we listed key seven design requirements to guide the design of PhenoFlow.\nR1. Incorporate familiar visual representations. Experts acknowl- edged the necessity of new visualizations to extract insights from ir- regular, large-scale datasets. However, they also emphasized that these visualizations should not impose additional cognitive loads. As a result, we have incorporate familiar visual representations, such as bars and lines, in the design of PhenoFlow. Despite this, during Phase II, we discovered that neurologists still suffer significant cognitive loads when engaging in visual exploration. This discovery prompted us to define the next design requirement.\nR2. Enable exploration in a more natural manner. During Phase II, we observed that neurologists experienced cognitive load during the data wrangling process, which includes cohort construction and refinement. For instance, when constructing and reviewing a cohort with multiple conditions, such as \"Male patients over 50 years of age, diagnosed with LAA or SVO, and elevated systolic blood pressure (SBP)>180\", experts had to select multiple conditions and review the re- sults with multiple visualizations. Addressing this process interactively and visually was mentally demanding and sometimes led to confusion. Instead, they expressed a preference for handling the data wrangling process through natural language interactions and focusing on visual exploration of clinical questions. This design requirement led us to develop a human-LLM collaboration workflow.\nR3. Visualize all patient trends in a single view. Neurologists ex- pressed the need for a comprehensive view of BP patterns within a cohort. Although summarized trends can offer a global overview of the target cohort, irregularities in measurements may introduce bias towards patients with more data points. To address this concern, they requested us to visually represent the data density in this view.\nR4. Integrate descriptive statistics with visualization. During the iterative exploration process, neurologists typically develop a prelimi- nary understanding of constructed cohorts using raw-level data, such as distribution, variance, and standard deviations. These data enable them to quickly and roughly validate their hypotheses. Therefore, they required a view that combines visualization and descriptive statistics.\nR5. Facilitate easy identification of anomalous data. Neurologists often focus on values outside the normal range, which may signal anomalous patient statuses. Therefore, clear visual identification of out-liers is essential to quickly detect potential issues and make informed decisions regarding patient care.\nR6. Visualize recurring spatio-temporal patterns. In retrospective studies, it's challenging to ascertain the exact patient situations at spe- cific times. However, during Phase II, E2 observed that the frequency of BP measurements may reflect the patient's health condition. For instance, a consistent, low-frequency pattern of measurements may suggest the stability of a patient's condition, while an irregular, high-frequency pattern could indicate instability. To enhance the credibility of such findings, it's important to consider not only the magnitude of BP fluctuations (i.e., spatial patterns) but also the timing of measure- ments (i.e., temporal patterns). This led us to develop a slice-and-wrap visualization, which will be elaborated on in the following sections.\nR7. Enable flexible interactions. Neurologists often start their explo- ration by identifying specific ranges of interest both in BP and time. However, these conditions can vary significantly among patients. To address this variability, they require flexible interaction techniques that allow for adjusting the range of interest. Additionally, at the co- hort level, patients may exhibit varying clinical attributes or outcomes. Experts have expressed the need to filter out data that shows weak correlations with their research interests."}, {"title": "5 DESIGN OF PHENOFLOW", "content": "PhenoFlow is a visual analytics tool designed specifically for acute ischemic stroke patients. It enables a wider range of data exploration by leveraging a Large Language Model (LLM) as a data wrangler. In the previous workflow, neurologists had to fulfill both the roles of data wranglers and researchers. However, in our proposed workflow, neurologists can concentrate on clinical decision-making while the LLM handles the data wrangling tasks.\n5.1 Human-LLM Collaboration Workflow (R1 and R2)\nWhile previous research has explored the use of Large Language Mod- els (LLMs) as medical task solvers [16, 21, 23, 37, 42, 55], LLMs still exhibit hallucinations in medical domain tasks [50]. Furthermore, the precise performance of LLMs in this domain remains uncertain. Therefore, in designing the human-LLM collaboration workflow, we aimed to keep many possibilities open and collaborated with neurologists to address various medical tasks using the LLM. We will briefly discuss our exploratory phase and then explain the final design of our workflow.\nExploratory Phase: This research began in early 2023 when there was no prior work testing the performance of LLMs in medical tasks. Initially, we collaborated iteratively with neurologists to identify five distinct medical research tasks: (1) generating medical hypotheses from data, (2) recommending literature for medical research, (3) suggesting research methods for validating hypotheses, (4) performing medical data wrangling tasks, and (5) generating and interpreting visualiza- tions. We utilized GPT4-32k for these tasks and employed few-shot prompting to guide the LLMs. To address complex medical knowl- edge, we initially used retrieval-augmented generation (RAG) that included prior medical research papers. However, due to bias, retrieval errors, and computational overhead, we later discontinued the use of RAG. To reduce hallucination and enable inspection, we adopted a self-reflection approach [46]. We utilized the PICO (Population, Inter- vention, Comparison, Outcomes) hypotheses and research protocols from our collaborators' published studies for testing. Our initial goal was to utilize LLMs as universal companions in medical research.\nResults: We found that LLMs struggled with more creative and expansive tasks, such as (1) generating medical hypotheses and (2) recommending literature for medical research. When provided with the PICO hypothesis as input and asked to recommend a medical hypothesis and relevant literature, LLMs exhibited hallucinations in high-temperature settings (e.g., suggesting non-existent literature and generating PICO hypotheses involving incorrect knowledge). In low-temperature settings, LLMs only modified part of the variables in the PICO hypothesis. However, LLMs demonstrated impressive perfor- mance in (3) research method recommendation tasks. When presented with the PICO hypothesis, LLMs successfully recommended research methods similar to those used in actual studies. This result surprised our collaborator (E2), but in terms of reproducibility, LLMs did not consistently yield the same results. These findings posed two significant challenges: reproducibility (i.e., consistency) and explainability.\nIn the (4) data wrangling tasks, LLMs demonstrated much better results than in previous tasks. We discovered that LLMs are capable of extracting data that meet user requirements from large and intricate datasets. Furthermore, with proper instructions, they can handle miss- ing data by leveraging existing information. However, we encountered another critical challenge in these results: the privacy issue. Lastly, in (5) generating and interpreting visualization tasks, LLMs still strug- gled to address complex visualizations effectively. Similarly to data wrangling tasks, privacy concerns also exist in these tasks.\nFinal Design: The prior results led us to design a collaborative work- flow incorporating a LLM as a data wrangler. The LLM addresses three medical data wrangling tasks: (1) standardizing inconsistent terminolo- gies, (2) extracting pertinent data from large, complex datasets, and (3) providing interpretable explanations. Ideally, these tasks would be undertaken by a professional data wrangler or with comprehensive doc- umentation of the data. However, such resources are often unavailabl or impractical to employ in real-world clinical settings.\nTo ensure reproducibility, we employed an LLM-based code gen- eration approach with few-shot prompting to synthesize executable code from user requests. This executable code ensures the consistency of the derived results. To offer explainability for the yielded results, we utilized a combination of natural language explanations and small multiples. The natural language explanations are derived through self-reflection during the reasoning process, while the small multiple visualizations are generated based on the synthesized executable code. We believe these two approaches are complementary, as explanations in natural language are concise but may not be as intuitive, while expla- nations in visualizations may take up more space but are more easily understood. Lastly, to address privacy issues, we only utilize metadata (e.g., column names, data types, and field coding information) for han- dling user requests. This approach leverages in-context learning, which refers to the ability of LLMs to learn from and adapt to the context provided in the input. This metadata acts as a form of in-context infor- mation that the LLM uses to better understand the user's request and generate more accurate and relevant outputs."}, {"title": "5.1.1 Natural Language Cohort Construction", "content": "In PhenoFlow, natural language cohort construction involves both co- hort definition and refinement. When experts provide their requests in natural language, the LLM data wrangler goes through a four-step pro- cess to conduct data wrangling. In the first stage, the LLM normalizes inconsistent terminologies. These inconsistencies can be caused by users' requests or the data itself. For example, if neurologists want to construct a cohort of patients who have undergone intra-arterial throm- bolysis, they can use abbreviations such as IAT, ambiguous terminology (e.g., thrombolysis therapy), or specific procedure codes.\nNext, the LLM tries to identify the region of interest (ROI) within data and fields to achieve the user's requirement by utilizing provided metadata as in-context information. Modern medical datasets are often very large and split into multiple files. Therefore, the ROI refers not only to the target field but also to the data location. Then, the LLM starts query inference. Query inference is the final step before generating executable codes. In this stage, the LLM performs multi-step reasoning (i.e., self-reflection) with the data generated in previous stages (e.g., normalized terminologies, metadata, and ROI). The inferences gener- ated in this process are further utilized as natural language explanations in the visual inspection view. Based on the query inference, the LLM generates executable code to extract relevant patient data from mul- tiple data sources and fields. To guide the LLM in synthesizing code, we employed few-shot prompting techniques in this stage. PhenoFlow extracts field names and data from the generated code to create small multiples that will be included in the visual inspection view."}, {"title": "5.1.2 Visual Inspection View", "content": "The visual inspection view enables neurologists to examine and under- stand the result derived by the LLM data wrangler. Neurologists first expand the visual inspection view by clicking the button \u2013 for space efficiency", "Justification": "Neurologists are familiar with interpreting and validating natural language clinical explanations. Therefore, they can efficiently inspect the LLM data wrangler's behavior with natural language. However, we found that they missed errors when the related condition becomes complex and generates a long explanation. We also tried to compress the length of the natural language explanation with prompt engineering. However, this approach leads to the loss of neces- sary information and generates hallucinations due to overcompression. Also, there is a risk that these explanations may include hallucinations.\nTo address this issue, we employed visualization of relevant fields and data. Because these visualizations are generated with the data itself, they do not include hallucinations. Moreover, they are intuitive and easily understandable. To reduce space consumption, we also implemented visualizations that include multiple field information. However, similar to natural language explanation, this visualization also imposes extra cognitive load for neurolog"}]}