{"title": "Scaling of Search and Learning: A Roadmap to Reproduce 01 from Reinforcement Learning Perspective", "authors": ["Zhiyuan Zeng", "Qinyuan Cheng", "Zhangyue Yin", "Bo Wang", "Shimin Li", "Yunhua Zhou", "Qipeng Guo", "Xuanjing Huang", "Xipeng Qiu"], "abstract": "OpenAI ol represents a significant milestone in Artificial Inteiligence, which achieves expert-level performances on many challanging tasks that require strong reasoning ability. OpenAI has claimed that the main techinique behinds o1 is the reinforcement learining (OpenAI, 2024a;b). Recent works use alternative approaches like knowledge distillation to imitate ol's reasoning style, but their effectiveness is limited by the capability ceiling of the teacher model. Therefore, this paper ana- lyzes the roadmap to achieving o1 from the perspective of reinforcement learning, focusing on four key components: policy initialization, reward design, search, and learning. Policy initialization enables models to develop human-like reasoning behaviors, equipping them with the ability to effectively explore solution spaces for complex problems. Reward design provides dense and effective signals via reward shaping or reward modeling, which is the guidance for both search and learning. Search plays a crucial role in generating high-quality solutions during both training and testing phases, which can produce better solutions with more computation. Learning utilizes the data generated by search for improving policy, which can achieve the better performance with more parameters and more searched data. Existing open-source projects that attempt to reproduce ol can be seem as a part or a variant of our roadmap. Collectively, these components underscore how learning and search drive ol's advancement, making meaningful contributions to the development of LLM.", "sections": [{"title": "1 Introduction", "content": "The field of Artificial Intelligence (AI) has witnessed unprecedented exploration and advancement of Large Language Models (LLMs) over the past two years. LLMs have progressively evolved to handle increasingly sophisticated tasks such as programming and solving advanced mathematical problems. OpenAI o1 represents a significant milestone in AI, which can generate very long reasoning process and conduct human-like reasoning actions like clarifying and decomposing questions, reflecting and correcting previous mistakes, exploring new solutions when encountering failure modes. The ol model dramatically transcended the reasoning capabilities of preceding LLMs, achieving performance comparable to PhD-level proficiency. Its remarkable reasoning achievements signify OpenAI's progression to the second stage (\u201cReasoner\u201d) in their five-stage roadmap to Artificial General Intelligence (AGI).\n\nThe blog and system card of ol demonstrate that the performance of ol consistently improves with increasing the computation of reinforcement learning and inference (OpenAI, 2024a;b). This suggests that ol could drive two paradigm shifts in AI: from (self-)supervised learning toward reinforcement learning, and from scaling solely training computation to scaling both training and inference computation.\n\no1 scales up the train-time compute with reinforcement learning and the test-time compute with more thinking. We take search as a way to implement the thinking process of o1, since search is scalable (Sutton, 2019) and there are many successful researches that use search for training and decision in reinforcement learning, like AlphaGo (Silver et al., 2016) and AlphaGo Zero (Silver et al., 2017). In this paper, we take reinforcement learning as the core in the roadmap to 01. Our roadmap is illustrated in Figure 1 and consists of four components: policy initialization, reward design, search, and learning. We believe these four parts are the keys to constructing LLM with strong reasoning abilities like 01.\n\nAs depicted in Figure 2, our roadmap starts with policy initialization. In the context of LLMs, the pol- icy (\u03c0(als)) typically refers to the probability distribution for generating the next token/step/response (action) based on a given context (state). Policy initialization brings human-like reasoning behaviors to LLMs, like task composition, self-evaluation and self-correction. Next, we enter the reward design, which aims to provide guiding signals for search and learning. The reward design can take or reshape the reward signal from the environment or learn a reward model from preference data. Both policy initialization and reward designs are preparations for search and learning. Search plays an important role in generating high-quality solutions at both training and testing phases, which can produce better solutions with more computation. Learning utilizes the data generated by search for improving the policy. The data used for learning is derived from the interaction of the LLM with the environment, rather than being manually curated by human experts, therefore eliminating the need for costly data annotation and enabling the potential for achieving superhuman performance.\n\nPolicy Initialization Training an LLM from scratch using reinforcement learning is exceptionally challenging due to its vast action space. Fortunately, we can leverage extensive internet data to pre- train a language model, establishing a potent initial policy model capable of generating fluent language outputs. Moreover, prompt engineering and supervised fine-tuning help models acquire human-like reasoning behaviors, enabling them to think systematically and validate their own results. These approaches enable models to thoroughly explore their solution spaces, leading to more comprehensive problem-solving capabilities.\n\nReward Design Both search and learning require guidance from reward signals to improve the policy. There are different levels of action granularity, each corresponding to varing levels of reward signals granularity, which can be explored further. Additionally, these signals are often sparse or even nonexistent in many environments. To transform sparse outcome reward to dense process reward, there are some reward shaping methods (Ng et al., 1999). For the environment where the reward signal is unavailable, like the task of story writing, we can learn a reward model from preference data"}, {"title": "2 Background", "content": "Since this roadmap is designed from perspective of reinforcement learning, we introduce some background of reinforcement learning and its connection to LLM in this section. Unlike other learning paradigms, reinforcement learning learns through interaction with the environment, rather than learning from a static training dataset. In reinforcement learning, an agent learns by receiving rewards from the environment as it explores. Figure 3 illustrates the interaction between agent and environment in reinforcement learning for LLM.\n\nAgent Agent is the entity that interacts with environment, which makes decision according to its policy. Formally, a policy \u03c0 is a mapping from states to actions. It is often represented as a probability distribution (\u03c0(\u03b1|s)) over actions given a state s, where the agent selects actions based on these probabilities."}, {"title": "3 Policy Initialization", "content": "In reinforcement learning, a policy defines how an agent selects actions in response to environmental states. As discussed in Section 2, LLMs operate with actions at three granularity levels: solution-level, step-level, and token-level. Solution-level actions represent the coarsest granularity, treating the entire solution as a single action. Step-level actions operate at an intermediate granularity, where individual steps serve as discrete actions. Token-level actions provide the finest granularity, treating each individual token as an action. Taking token-level actions as an example, the action space contains thousands of tokens from the vocabulary, establishing a well-initialized policy becomes essential for effective model performance (Brown et al., 2020).\n\nAs illustrated in Figure 4, the initialization process of LLMs consists of two primary phases: pre- training and instruction fine-tuning. During pre-training, models develop fundamental language understanding through self-supervised learning on large-scale web corpora (Sun et al., 2024d; Weber et al., 2024; Liu et al., 2024f), following established power-law relationships between computational resources and performance (Kaplan et al., 2020; Hoffmann et al., 2022). Instruction fine-tuning then transforms LLMs from simple next-token prediction to generating human-aligned responses (Wei et al., 2022a; Chung et al., 2024). For models like 01, incorporating human-like reasoning behaviors is crucial to enable more sophisticated exploration of solution spaces. We summarize six key behaviors that can be activated through prompts or learned through expert trajectories distillation from LLMs."}, {"title": "3.1 Pre-Training", "content": "Pre-training establishes basic language understanding and reasoning capabilities in LLMs through exposure to massive text corpora (Radford & Narasimhan, 2018; Lee et al., 2024). For o1-like models, these core competencies serve as the basis for advanced behaviors developed through subsequent learning and search."}, {"title": "3.1.1 Language Understanding and Generation", "content": "Pre-training cultivates diverse language capabilities through extensive exposure to natural lan- guage (Radford & Narasimhan, 2018). At the syntactic level, models learn grammatical structures ranging from basic word order patterns to complex dependencies (Manning, 2022). This syntactic foundation enables pragmatic understanding, including discourse markers and contextual language use, allowing models to adapt different styles across tasks (Dam et al., 2024). Generation capabilities progress from basic grammatical coherence to sophisticated features like long-range consistency and complex narrative structures (Tian et al., 2024b). Through multilingual training data, models develop cross-lingual abilities, enabling zero-shot transfer and cultural understanding across languages (Scao et al., 2022; Alves et al., 2024). Research shows language understanding emerges hierarchically: syn- tactic patterns appear early, while logical consistency and abstract reasoning develop later, suggesting the importance of training duration and data composition beyond model scaling (He et al., 2024; Ahuja et al., 2024)."}, {"title": "3.1.2 World Knowledge Acquisition and Storage", "content": "Pre-training enables comprehensive knowledge acquisition across factual, procedural, and conceptual domains through diverse corpora processing (Radford et al., 2019; Brown et al., 2020). Models develop rich semantic networks of factual knowledge from encyclopedic sources and academic literature, enabling cross-domain reasoning and novel insights (Chang et al., 2024). Domain expertise emerges from specialized technical content, manifesting in advanced capabilities like mathematical proofs and scientific analysis (Shao et al., 2024; Yang et al., 2024). Procedural knowledge develops through exposure to instructional content and programming languages, enhancing systematic problem- solving abilities (Ruis et al., 2024; Brahman et al., 2023; Haluptzok et al., 2023). Mathematical and logical foundations form through formal mathematical texts, establishing logical reasoning capabilities (Sun et al., 2023; Ahn et al., 2024). Recent studies demonstrate that knowledge storage exhibits efficient compression and generalization properties (An et al., 2024), with abstract concepts requiring more extensive training compared to factual knowledge (Allen-Zhu & Li, 2024)."}, {"title": "3.1.3 Basic Reasoning Abilities", "content": "Pre-training develops foundational reasoning capabilities through diverse reasoning patterns, emerg- ing hierarchically from simple inference to complex reasoning. Pattern matching and analogical reasoning emerge as primary mechanisms, enabling generalization across domains (Webb et al., 2022; Yasunaga et al., 2024; Yu et al., 2024b). Logical inference abilities develop through exposure to massive code and mathematical proofs (Hui et al., 2024; Yang et al., 2024), while sequential processing capabilities emerge from procedural texts and mathematical derivations (Lewkowycz et al., 2022). These abilities enable complex problem decomposition and logical coherence."}, {"title": "3.2 Instruction Fine-Tuning", "content": "Instruction fine-tuning transforms pre-trained language models into task-oriented agents through specialized training on instruction-response pairs across diverse domains (Zhang et al., 2023a; Cheng et al., 2024a). This process alters the model's behavior from pure next-token prediction to purposeful behavior (Wu et al., 2024b; Zhao et al., 2024b). The effectiveness of instruction fine-tuning depends primarily on two key factors: the diversity of the instruction dataset (Sanh et al., 2022) and the quality of instruction-response pairs (Li et al., 2024a; Liu et al., 2024g). Research efforts have expanded both dimensions significantly, with Wang et al. (2022b) developing a comprehensive dataset encompassing over 1,600 distinct NLP tasks. FLAN (Wei et al., 2022a) demonstrated that models fine-tuned on high- quality instruction data can effectively generalize to novel tasks. Chung et al. (2024) enhanced this capability through scaling across task count, model size, and incorporation of step-by-step reasoning protocols. Smaller-scale models like Alpaca (Taori et al., 2023) achieved remarkable instruction- following capabilities through carefully curated high-quality training data. Self-Instruct (Wang et al., 2023b) introduced methods for automated generation of instruction-response pairs using LLMs themselves. Hayati et al. (2024) further established that fine-tuning on complex, multi-step instructions significantly enhances model capabilities and generalization."}, {"title": "3.3 Human-like Reasoning Behaviours", "content": "While instruction-tuned models demonstrate general task competency and user intent understanding, models like o1 require a more sophisticated repertoire of human-like reasoning capabilities to fully leverage their potential. As shown in Table 1, similar to Zhang (2024), our analysis of ol's behavior patterns identified six human-like reasoning behaviors that help o1 better explore the solution space. We examine the implementation of these reasoning behaviors through two complementary perspectives: supervised fine-tuning and prompt engineering (richards, 2024).\n\nProblem Analysis Problem analysis serves as a crucial initialization process where the model reformulates and analyzes the problem before solving it (Kondrakunta et al., 2018). This process involves multiple steps: explicit problem restatement to verify understanding, identification of implicit constraints, and transformation of abstract requirements into concrete, actionable specifications. Deng et al. (2023) advance this concept through Proactive Chain-of-Thought, where models actively analyze potential ambiguities before proceeding with problem-solving. In ol's blog on cipher solving, this manifests as careful observation of ciphertext patterns and explicit problem reformulation. As shown in Table 1, in coding tasks, it restructures inputs into matrices and precisely generates expected outputs. Problem analysis reduces ambiguity in problem interpretation, constructing a more advantageous initial state for the subsequent phase (Lee et al., 2023).\n\nTask Decomposition When encountering complex problems, humans typically decompose them into several manageable subtasks (Zhou et al., 2023a). As shown in Table 1, in coding tasks, o1 decomposes the problem into several subtasks, including capturing input strings, removing spaces, and parsing input strings. Bursztyn et al. (2022) introduce Compositional Fine-Tuning (CFT), a technique that explicitly divides a target task into its constituent components. Recent studies show that models can effectively perform such decompositions when guided by carefully structured prompts (Khot et al., 2023; Dua et al., 2022). Importantly, the decomposition process is adaptive and context-aware, with the model dynamically adjusting the granularity and structure of subtasks based on the problem's complexity and uncertainty levels (Prasad et al., 2024; Zhou et al., 2023b).\n\nTask Completion Following problem analysis and task decomposition, the model generates solu- tions through step-by-step reasoning based on clarified problems and decomposed subtasks (Plaat et al., 2024; Sun et al., 2023). This behavior forms the foundation for all other reasoning processes, where successful solutions lead to subsequent subtask processing, while problematic solutions trigger the generation of alternatives or self-correction behaviors. Step-by-step generation significantly enhances models' complex reasoning capabilities (Wei et al., 2022b; Zhang et al., 2023c). For LLMs, this ability can be activated through prompts containing reasoning processes (Wei et al., 2022c), or even through simple instructions like \u201cLet's think step by step\" (Kojima et al., 2022). Smaller models can acquire this capability through distillation on extensive step-by-step reasoning data Shridhar et al. (2023); Hsieh et al. (2023). Recent research indicates that sampling multiple solutions substantially\""}, {"title": "3.4 Speculation About the Policy Initialization of o1", "content": "During the progression from pre-training to instruction following, the model gradually constrains its action space. Policy initialization plays a critical role in developing o1-like models, as it establishes foundational capabilities that influence subsequent learning and search processes. The policy ini- tialization phase encompasses three essential components: pre-training, instruction fine-tuning, and the development of human-like reasoning behaviors. While these reasoning behaviors are implicitly present in LLMs after instruction fine-tuning, their effective deployment requires activation through either supervised fine-tuning or carefully crafted prompts. Below, we outline several foundations for effectively utilizing these human-like reasoning behaviors.\n\nLong-Text Generation Capabilities During inference, LLMs need to generate a large number of tokens to encompass complex and diverse reasoning behaviors, which requires sophisticated long-context modeling capabilities. While current LLMs have significantly improved in processing long texts (Wang et al., 2024e), their capacity for generating lengthy content remains limited. Bai et al. (2024) introduces AgentWrite, an agent-based automated data construction pipeline. By fine-tuning on the constructed LongWriter-6k dataset, the approach substantially enhances LLMs' long-text generation capabilities. Similarly, Quan et al. (2024) proposes Self-Lengthen, which iteratively fine-"}, {"title": "3.5 Challenges of Policy Initialization for Reproducing 01", "content": "While policy initialization establishes crucial foundations for ol-like models, several significant challenges emerge in implementing this approach effectively.\n\nHow to balance sampling-efficiency and sampling-diversity? Policy initialization faces a critical trade-off between sharping the action probability distributions for efficient sampling and maintaining sufficient diversity for exploration. While learning from human demonstrations helps constrain the action space, excessive convergence to fixed strategies can limit the discovery of superior approaches during search phases (Li et al., 2024b). This challenge is evident in comparing AlphaGo and AlphaGo Zero, initialization from human data provides a strong starting point but may inadvertently restrict exploration of potentially better strategies.\n\nHow to ensure domain generalization of reasoning behaviors? Current research focuses on repli- cating ol's behaviors in specific domains such as mathematics (Chen et al., 2024a) and coding (Zhang et al., 2024g). However, ol's behaviors are not limited to domain-specific reasoning behaviors. For instance, in safety tasks, models need to perform behaviors that verify whether generated content complies with safety guidelines. Therefore, during the policy initialization process, models should not be restricted to domain-specific reasoning behaviors. Since it is impractical to specify corresponding reasoning behaviors for all tasks, designing reasoning behaviors with strong domain generalization capabilities becomes crucial."}, {"title": "4 Reward Design", "content": "In reinforcement learning, the agent receives feedback from the environment in the form of a reward signal and seeks to maximize its long-term reward by improving its policy. The reward function, denoted as r(st, at), represents the reward associated with the agent's action at in state st at time step t. The reward signal is crucial in guiding both the training and inference processes, as it defines the desired behavior of the agent through numerical scoring. While the same optimal policy can be learned from various reward designs (Ng et al., 1999), a well-designed reward signal can accelerate both the convergence of learning and the efficiency of the search process.\n\nThis section provides an overview of the current reward design methods for large language models (LLMs). We begin by comparing the outcome reward and the process reward. Next, we introduce reward design methods that have been specifically proposed for LLMs. We also review reward design"}, {"title": "4.1 Outcome Reward vs Process Reward", "content": "The outcome reward involves assigning a score based on whether the outputs of a large language model (LLM) meet predefined expectations (Cobbe et al., 2021; Shao et al., 2024). For instance, Cobbe et al. (2021) uses outcome reward to assess whether the solution provided by the LLM to a mathematical problem is correct. Since ground truth is generally available for tasks such as solving mathematical problems, outcome reward is often straightforward.\n\nAlthough the outcome reward is relatively easy to construct, it lacks supervision for intermediate steps. For instance, a solution to a mathematical problem that leads to the correct answer may still contain errors in the intermediate steps (Lightman et al., 2024). As a result, using the outcome reward may cause the LLM to generate incorrect solution steps, which could negatively affect performance. Moreover, outcome reward is sparse, as it does not reward intermediate steps, making it challenging to learn step-level policies. Despite these limitations, it remains widely used in reinforcement learning for LLMs due to its simplicity.\n\nIn contrast to the outcome reward, the process reward provides a reward signal not only for the final step but also for intermediate steps (Lightman et al., 2024; Yin et al., 2024a). For example, Lightman et al. (2024) involves human annotators rewarding the intermediate steps in a mathematical solution generated by an LLM. The outcome reward can be seen as a special case of the process reward, where the rewards for intermediate steps are all set to zero.\n\nDepending on the granularity of the actions, the process can be classified into token-level (Rafailov et al., 2024) and step-level (Lightman et al., 2024). The step-level segmentation is flexible. Kuhn et al. (2023) uses information entropy to guide step segmentation while methods in mathematical problem-solving typically use newline characters as the split symbol (Lightman et al., 2024).\n\nAlthough process rewards show promise, they are more challenging to learn than outcome rewards. For instance, the reward design in Lightman et al. (2024) depends on human annotators, making it costly and difficult to scale. There are some automatic ways to transform outcome rewards to process rewards, which is called reward shaping in traditional reinforcement learning. We introduce the methods of reward shaping in Section 4.2.3."}, {"title": "4.2 Reward Design Methods", "content": "Since outcome reward can be seen as a special case of process reward, many reward design methods can be applied to the modeling of both outcome reward and process reward. The resulting models are typically referred to as the Outcome Reward Model(ORM) and the Process Reward Model(PRM)."}, {"title": "4.2.1 Reward from Environment", "content": "The most straightforward approach to designing a reward is to directly utilize the reward signal from the environment, or learn a model to simulate the reward signal from the environment.\n\nFrom Realistic Environment Many environments can provide effective reward signals, for exam- ple, code generation can receive reward signals from a compiler or interpreter. Dou et al. (2024); Shojaee et al. (2023); Dong et al. (2024) have shown that the quality of code generation is improved with compiler feedback. Zhang et al. (2024e) generates test cases for input questions and uses the testing outcomes as the reward signal. (Shi et al., 2022) measures the similarity of different programs based on their execution results. Zheng et al. (2024) uses the execution results of a program as guidance for refining a code generation model. For some real-world tasks, the environment is just a sandbox. In web shopping (Liu et al., 2024e), the environment is represented by a real HTML page, and the reward is calculated based on the score the LLM obtains through clicks and searches. In more general scenarios, sandboxes are created to allow the policy model to interact directly with, such as in environments like MineDojo (Fan et al., 2022) or Alfworld (Shridhar et al., 2021).\n\nFrom Simulating Environment While some environments can provide valid feedback, interacting with them to obtain the reward signal can be costly, or the feedback may not be available at test time. For example, during testing, we might not have test cases to validate whether the program generated by the LLM is correct. In such cases, a reward model is required to simulate the reward signal from the environment. For instance, Lightman et al. (2024) trains a verifier to predict whether the model's solution to a mathematical problem is correct at test time.\n\nSimulating the reward signal enables the LLM to obtain feedback at any time. However, using such a reward model during learning or search can lead to the problem of distribution shift. As the policy is updated during learning and search, the reward model, which is trained on data from interactions between the old policy and the environment, may fail to adapt to the new policy. This issue, known as reward optimization, has been discussed in Gao et al. (2023); Stroebl et al. (2024). Thus, the reward model must be updated in tandem with the policy improvements. In more general domains, simulating the environment aligns with the concept of a world model, where the transition probabilities of states must be further simulated, allowing for a more accurate reward model. We discuss the world model in Section 4.5.2.\n\nFrom AI Judgment Al Judgment involves using general AI assistants to provide the reward signal, serving as an alternative to relying on realistic environments, thereby avoiding the high costs associated with environment construction and interaction. For instance, it is common to use a powerful LLM, such as GPT-4, to assess the performance of an AI assistant (Zheng et al., 2023b). While AI judgment can be viewed as a form of reward model, it does not face the issue of reward optimization, as it is not dependent on the policy model. As a result, AI judgment remains effective even when the policy model undergoes updates. The LLM is an implementation of the world model, which also highlights the effectiveness of building a world model to provide reward signals."}, {"title": "4.2.2 Reward Modeling from Data", "content": "For some environment, the reward signal from environment is unavailable, which can not be simulated. For example, it is difficult to judge whether the response from an AI assistant is good or not. Fortunately, it is easier to collect the expert data or the preference data than giving a reward. With the expert data or preference data, we can also learn a model to provide effective rewards.\n\nLearning rewards from preference data is widely recognized in the LLM community, particularly due to the success of RLHF (Bai et al., 2022a). However, learning rewards from expert data, also known as Inverse Reinforcement Learning, has not been widely used for LLMs, which may represent a promising technique for 01."}, {"title": "Learning Rewards from Preference Data", "content": "Preference data is collected by ranking multiple re- sponses from LLMs to the same question. Using the Bradley-Terry model, an outcome reward can"}, {"title": "4.2.3 Reward Shaping", "content": "The reward signal from some environments may not be effective; for example, it could be an outcome reward rather than a process reward. In such cases, the reward can be reshaped to make it denser and more informative, a process known as reward shaping. For instance, Kumar et al. (2024) attempts to train an LLM to self-correct via reinforcement learning and discovers that proper reward shaping can prevent learning collapse. Wang et al. (2024c) reshaped the outcome reward by estimating the value function using Monte Carlo sampling, employing the Q-value Q\u03c0(st, at) as the reward during intermediate steps. However, since the value function depends on the policy \u03c0, the value function estimate from one policy may not be a valid reward function for another policy. This is empirically validated by Xiong et al. (2024), who found that using a value function from another policy for reward shaping is harmful.\n\nNg et al. (1999) defines potential-based reward shaping and proves that two reward functions can lead to the same optimal policy if and only if they satisfy the following equation:\n\nF(st, at) = r(st, at) + \u03b3\u03c6(St+1) \u2013 \u0444(st),\n\nwhere F(st, at) and r(st, at) are two reward functions that yield the same optimal policy, y is the discount factor, and $ is a potential function. Potential-based reward shaping suggests that the reward function r(st, at) can be reshaped without altering the optimal policy, as long as the shaping satisfies Equation 1. Setlur et al. (2024) uses the theory of potential-based reward shaping to transform outcome rewards into process rewards.\n\nThe reward learned from preference data can also be reshaped. For example, Rafailov et al. (2023), Rafailov et al. (2024) and Zhong et al. (2024) demonstrate that DPO implicitly makes potential-based reward shaping (Ng et al., 1999) on the reward learned from preference data.\n\nWhile reward shaping can be advantageous, it may also be harmful. Improper reward shaping can negatively affect both learning and search processes. As a result, reward shaping requires careful design and often entails incorporating inductive bias."}, {"title": "4.3 Speculation About the Reward Design of o1", "content": "We now turn to discuss our assumptions regarding the reward design for o1. Given that o1 could handle multi-task reasoning, its reward model is likely to incorporate multiple reward design methods. For complex reasoning tasks, such as mathematics and code, where responses typically involve long chains of reasoning, a process-reward model is more likely to be employed to supervise the"}, {"title": "4.4 Challenges of Reward Design for Reproducing 01", "content": "How to overcome distribution shift? The reward model learns from an existing dataset distribution and the out-of-distribution problem still remains significant today (Gao et al., 2023), especially as the LLM continues to explore and learn from feedback. The reward provided by the proxy model deviates from the golden reward when the distribution of the policy model changes, as the trajectory becomes unseen during the training of the reward model and heavily depends on its generalization. Scaling the parameters of the reward model and increasing the amount of data can mitigate this issue but does not completely solve it. Iteratively training the reward model provides a more direct solution, but it still requires human involvement in the loop.\n\nHow to design fine-grained reward for language model? Unlike in Atari games or robotic environments, language presents a unique challenge because the definition of a step or action can vary in granularity: token-level, step-level or solution-level. In many situations, such as fitting the model to human preferences, it is more natural to judge the entire solution rather than each token individually, as higher-order semantics in language emerge from the combination of tokens. However, using token combinations as actions results in an action space that is too large to define or learn a reward function for. The potential action space grows exponentially, leading to a long tail of actions. Meanwhile, the sparsity of the reward signal increases with the length of each step as we mentioned before.\n\nHow to choose data when modeling reward for complex tasks? As the complexity of tasks in- creases, choosing the appropriate type of feedback becomes increasingly challenging. Recent studies have shown that using preference-based feedback for tasks like code generation or mathematical reasoning may actually degrade the performance of the policy model (Wen et al., 2024). Additionally, the question of how much data is necessary to accurately capture the intended behavior remains underexplored. The difficulty of evaluating whether a reward is effective also grows as the complexity of the task increases."}, {"title": "4.5 Generalization", "content": "The previous section primarily focused on reward design for specific tasks, such as math and coding. When addressing more general tasks, we need to create a broader environment. Practically, according to OpenAI's 5-stages plan for AGI, 01 has been a strong reasoner, the next stage is to train an agent that can interact with the world and address real-world problems. To achieve this goal, we need a reward model to provide the reward signal for the agent taking actions in a realistic environment. In this section, we focus on how to construct a general reward signal, which can be divided into two components: the reward ensemble and the world model."}, {"title": "4.5.1 Reward Ensemble", "content": "An intuitive way to build a reward signal for a general task is through an ensemble rewards in specfic domains. Quan (2024) trains the reward model as a form of MoE (Jiang et al., 2024a; Zeng et al., 2024a), where each expert is trained on a different task to provide the corresponding reward signal, and the outputs are aggregated finally. Nguyen et al. (2024) frames this problem as a multi-armed bandit question, learning to select the most suitable reward model. In short, the crucial question is for these methods is how to combine the reward signals effectively."}, {"title": "4.5.2 One for all: World Model", "content": "The world model can not only provide the reward signal but also predict the next state (Ha & Schmidhuber, 2018; Bar et al., 2024). Some works claim that a video generator is a world model since the video generator can predict the images in the future time steps (Liu et al., 2024b; Bar et al., 2024; Wang et al., 2024d; Brooks et al., 2024). Dawid & LeCun (2023) also proposes a framework where world model do not need to predict the next state, instead they predict the representation of the next state, which is easier and more efficient than predicting a image. This is inline with Muzero (Schrittwieser et al., 2020), which also let the environment simulator to predict the representation of the next state in the game of go. The current works of world model focus on modeling of the next state prediction, but we believe modeling the reward signal is also critical and challenging for an agent to accomplish real-environment tasks."}, {"title": "5 Search", "content": "For LLMs, performing random sampling during generation has become a mainstream method to im- prove output quality, with techniques such as nucleus sampling being prominent examples (Holtzman et al., 2020). Furthermore, many studies (Kulal et al., 2019; Chen et al., 2021) have observed that the pass@k metric improves consistently as the number of model samples increases. Brown et al. (2024) show that even small models can outperform large models leveraging search. This suggests that language models have the potential to explore the correct solution by more sampling during inference, which requires consuming more inference-time computations. Search refers to the process of finding the correct solution through multiple attempts or strategic exploration based on certain guidance, such as rewards or heuristic rules. Well-known inference strategies like self-consistency (Wang et al., 2023a) and Best-of-N (BON) (Cobbe et al., 2021) both can be seen as search methods. For model like 01, which are designed to solve complex reasoning tasks, search may play an important role in both training and inference processes. In this section, we first analyze the possible role of search in o1, and then introduce the details of various promising search methods which have the potential to be used in the construction of o1-like models."}, {"title": "5.1 The role of Search in 01", "content": "Search often relies on guiding signals thus can be viewed as a process of strategy iteration, we call it search policy. Compared to naive sampling, search policies are usually more likely to find better solutions. Solutions generated by search policies can either be directly used as the final output or incorporated into training algorithms, like Expert Iteration (Anthony et al., 2017"}]}