{"title": "Pre-train and Fine-tune: Recommenders as Large Models", "authors": ["Zhenhao Jiang", "Chenghao Chen", "Hao Feng", "Yu Yang", "Jin Liu", "Jie Zhang", "Jia Jia", "Ning Hu"], "abstract": "In reality, users have different interests in different periods, regions,\nscenes, etc. Such changes in interest are so drastic that they are\ndifficult to be captured by recommenders. Existing multi-domain\nlearning can alleviate this problem. However, the structure of the\nindustrial recommendation system is complex, the amount of data\nis huge, and the training cost is extremely high, so it is difficult to\nmodify the structure of the industrial recommender and re-train\nit. To fill this gap, we consider recommenders as large pre-trained\nmodels and fine-tune them. We first propose the theory of the in-\nformation bottleneck for fine-tuning and present an explanation\nfor the fine-tuning technique in recommenders. To tailor for recom-\nmendation, we design an information-aware adaptive kernel (IAK)\ntechnique to fine-tune the pre-trained recommender. Specifically,\nwe define fine-tuning as two phases: knowledge compression and\nknowledge matching and let the training stage of IAK explicitly\napproximate these two phases. Our proposed approach designed\nfrom the essence of fine-tuning is well interpretable. Extensive on-\nline and offline experiments show the superiority of our proposed\nmethod. Besides, we also share unique and important lessons we\nlearned when deploying the method in a large-scale online plat-\nform. We also present the potential issues of fine-tuning techniques\nin recommendation systems and the corresponding solutions. The\nrecommender with IAK technique has been deployed on the home-\npage of a billion-scale online food platform for several months and\nhas yielded considerable profits in our business.", "sections": [{"title": "1 Introduction", "content": "Shipping billions of items to meet users' tastes is the basic function-\nality of recommendation systems [27-29], which is an important\npart of online information distribution platforms such as news\nfeeds, e-commerce, social media, etc. At present, the mainstream\nrecommender systems, based on deep learning [16, 20], use an\nend-to-end training strategy to mine user interest and rank items\naccording to user preference [6]. With the development of business,\nthere are more and more topics/conditions in the domain of rec-\nommender algorithms. Multi-scene [15, 44], multi-region [12, 36],\nand multi-period [1, 42] are three representatives becoming the\nresearch frontier. Multi-scene describes that online recommender\nservice consists of many scenes, such as home page, channel page,\nin-shop related page, etc. Users may click back and forth on dif-\nferent scenes with distinct behaviors. Therefore, it is necessary to\nexplicitly model user-item interaction behavior in different scenes.\nMulti-region mainly includes multi-country and multi-city, which\ndescribes the differences in life habits and preferences of users\namong different regions, as well as the differences in the supply\nof items. For example, traditionally, turkey is for Thanksgiving\nin America while zongzi is for the Dragon Boat Festival in China.\nMulti-period describes that users have different behaviors at differ-\nent times. For example, morning vs. evening on takeaway platforms,"}, {"title": "2 related works", "content": "There are mainly three issues related to this work: multi-domain\nlearning, fine-tuning technique and multi-task learning.\nMulti-Domain Learning: Multi-domain recommendation [30,\n37, 56] aims to improve recommendation performance in each do-\nmain using knowledge transferred from other domains. SAR-Net\n[40] employs the paradigm of a mixture of experts to give rec-\nommendation results and accommodates an attention mechanism\n[9] to extract the relationship of the user's interests in different\ndomains. STAR [41] presents a star topology including shared pa-\nrameters and domain-specific parameters, which can realize using\none model to serve all domains in CTR prediction. AFT [14] learns\nthe feature translations between different domains under a genera-\ntive adversarial network [13] framework. It explicitly models the\nrelationships between items, domains and users' representations\nin general and specific domains. ADIN [18] is able to adaptively\ntreat the commonalities and diversities across scenarios, tracing the\nuser's interest in different domains with shared networks and spe-\ncific networks. HKGCL [24] handles each relevant domain as a hi-\nerarchy in the interaction network, and the hierarchical knowledge\ngraph is aggregated based on the LightGCN aggregation strategy\nto learn knowledge representations for users and items.\nFine-Tuning in Recommendation: Although fine-tuning tech-\nniques are widely used in NLP, there are few studies on fine-tuning\nin recommendation. [59] employs a simple fine-tuning technique to\naddress mainstream bias [23] in recommendation. C2-CRS [58] em-\nploys user's feedback to fine-tune a conversational recommender\nsystem [43]."}, {"title": "3 preliminary", "content": "In this paper, we use the ideology of information bottleneck [39, 47]\nto explain the fine-tuning technology. We first introduce informa-\ntion bottleneck in this section.\nInformation bottleneck theory is originally developed from the\nrate-distortion theory of data compression in information theory\n[2, 52], which discusses the problem of how to retain as much infor-\nmation as possible when compressing data. To open the black box\nof deep neural network, information bottleneck for deep learning\nis proposed as follows [38, 48].\n\\hat{X} = arg min \u2013 I(X; Y) + \u03b2I(X; X),", "latex": ["\\hat{X} = arg min \u2013 I(X; Y) + \u03b2I(X; X)"]}, {"title": "4 Explanation of Fine-tuning in Recommender", "content": "Here, we present a new explanation of fine-tuning from the perspec-\ntive of information bottleneck. The original information bottleneck\ndescribes a minimum sufficient representation of samples that con-\ntains specific information. OLR is pre-trained on all domain data on\na large scale to fully learn the general business knowledge, but it is\ndifficult to deal with the impact of sudden changes in user interest\ncaused by changes in conditions in downstream tasks (e.g. human\nhabits determine that there is a huge gap between user-item dis-\ntribution of breakfast and that of dinner). Thus, each downstream\ntask has its hidden characteristics which is the learning objective of\nfine-tuning. This means that some of the general business knowl-\nedge learned by the pre-trained model is helpful in understanding\nthe downstream task, and some is not. Therefore, the knowledge\nor information learned in the pre-trained model needs to be com-\npressed, and the information related to the downstream task should\nbe retained as much as possible.\nTheorem 1 (Information Bottleneck for Fine-tuning). Given the\ngeneral knowledge in the pre-trained model G and the knowledge\nin the given downstream task T, the optimization objective of Infor-\nmation Bottleneck for Fine-tuning is to find the minimum sufficient\nrepresentation \\hat{G}:\nG = arg min \u2013 I(\\hat{G};T) + \u03b2I(\\hat{G};G).", "latex": ["G = arg min \u2013 I(\\hat{G};T) + \u03b2I(\\hat{G};G)."]}, {"title": "5 proposed method", "content": "In this paper, we mainly focus on multi-scene, multi-region and\nmulti-period in food recommendations. Let S, R, P denote three top-\nics, respectively. Given a set of downstream tasks D = SURU\nP = {s\u2081, ..., s\u1d62} \u222a {r\u2081, ..., r\u2c7c} \u222a {p\u2081, ..., p\u2096 } where s\u1d62 denotes the i-th\nscene, r\u2c7c denotes the j-th city, and p\u2096 denotes the k-th mealtime.\nHere, a task is defined as modeling user interest in the given do-\nmain/condition. Therefore, there are i + j + k downstream tasks, the\ntasks under the same topic are independent, and the tasks under\ndifferent topics are orthogonal. G is the general knowledge and\nTm is specific knowledge in the m-th task. The challenge is how to\neffectively extract the knowledge related to Tm from G to assist the\nmodel in decision-making on the m-th task, in addition, to accom-\nplish the above goal while compressing business knowledge of the\nlarge model as much as possible. This can be formulated as follows.\narg min ||F\u03b8(G) - T\u2098 || + |G|,\nwhere F\u03b8 is the learner with parameter \u03b8 and |G| expresses the\ninformation of business knowledge.", "latex": ["arg min ||F\u03b8(G) - T\u2098 || + |G|"]}, {"title": "6 discussion on the hidden issues", "content": "In this section, we report two concerns about the fine-tuning tech-\nnique in recommender systems. These issues are significant for\nunderstanding the role of fine-tuning in the field of recommenda-\ntion systems.\nPseudo cold start issue. In practice, the amount of data used\nfor fine-tuning is much smaller than that used for pre-training, and\nobviously the historical records of user-item interactions in some\ndomains are lost in the downstream dataset. For example, we select\nthe last seven days of scene A data to fine-tune OLR, but user B has\nnot entered scene A in the last seven days. The IAK module does not\nlearn the user's behavior in scene A from the downstream dataset.\nWhen the model is deployed on the server, the user B enters scene\nA, which the IAK module encounters for the first time, creating a\npseudo cold start issue. Models can only estimate based on the\npre-trained general knowledge.\nUser/item overlapping issue. Traditional fine-tuning meth-\nods completely isolate samples for different domains. Models serv-\ning scene A are fine-tuned using only scene A's samples. But that\ndoesn't make sense in the realm of recommendation systems. In a\nrecommendation system, samples from different domains are highly\ncorrelated. Taking multi-period as an example, there is not much\ndifference in the distribution of items for lunch and dinner. So the\ninformation of an item at noon is helpful for understanding the\ncharacteristics of the item at night. It's not appropriate to think of\nthem as independent tasks."}, {"title": "7 experiments", "content": "We conducted extensive experiments to evaluate the performance\nof IAK, and the following research questions (RQs) were answered:\n\u2022 RQ1 Does OLR outperform state-of-the-art baselines in mul-\ntiple conditions?\n\u2022 RQ2 Does IAK fine-tuning technique further improve the\nperformance of OLR?\n\u2022 RQ3 Does IAK fine-tuning technique further improve the\nperformance of baselines?\n\u2022 RQ4 Can IAK address multiple topics at the same time?\n\u2022 RQ5 How do critical components affect the performance of\nIAK?\n\u2022 RQ6 How does the issue in Section 6 affect IAK's perfor-\nmance?\n\u2022 RQ7 Does OLR+IAK work in real large-scale online recom-\nmendation scenarios?\n\u2022 RQ8 How does IAK affect recommendation results?"}, {"title": "A Mathematical Derivation", "content": "For the second term, based on the non-negative nature of KL diver-\ngence, the following derivation is valid:\n\u21d2\n\u2190\nDKL(p(\u011c)||r(\u011c)) \u2265 0\nf (p(\u011c) log p(\u011c) \u2013 p(\u011c) logr(\u011c))d\u011c \u2265 0\nfp(G)logp(\u011c)d\u011c \u2265 fp(\u011c) logr(\u011c)d\u011c,\nwhere r(G) is the variational approximation to p(\u011c).\nAccording to (2) and (11),\nI(G; G) = P(G, p(\u011c, G) log\nwhere knowledge G and G can be approximated with training\nsamples.", "latex": ["DKL(p(\\hat{G})||r(\\hat{G})) \u2265 0", "\\int (p(\\hat{G}) \\log p(\\hat{G}) \u2013 p(\\hat{G}) \\log r(\\hat{G}))d\\hat{G} \u2265 0", "\\int p(G)\\log p(\\hat{G})d\\hat{G} \u2265 \\int p(\\hat{G}) \\log r(\\hat{G})d\\hat{G}", "I(G; G) = P(G, p(\\hat{G}, G) log"]}]}