{"title": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models", "authors": ["Yuncheng Yang", "Yulei Qin", "Tong Wu", "Zihan Xu", "Gang Li", "Pengcheng Guo", "Hang Shao", "Yucheng Shi", "Ke Li", "Xing Sun", "Jie Yang", "Yun Gu"], "abstract": "The cultivation of expertise for large language models (LLMs) to solve tasks of specific areas often requires special-purpose tuning with calibrated behaviors on the expected stable outputs. To avoid huge cost brought by manual preparation of instruction datasets and training resources up to hundreds of hours, the exploitation of open knowledge including a wealth of low rank adaptation (LoRA) models and instruction datasets serves as a good starting point. However, existing methods on model and data selection focus on the performance of general-purpose capabilities while neglecting the knowledge gap exposed in domain-specific deployment. In the present study, we propose to bridge such gap by introducing few human-annotated samples (i.e., K-shot) for advancing task expertise of LLMs with open knowledge. Specifically, we develop an efficient and scalable pipeline to cost-efficiently produce task experts where K-shot data intervene in selecting the most promising expert candidates and the task-relevant instructions. A mixture-of-expert (MoE) system is built to make the best use of individual-yet-complementary knowledge between multiple experts. We unveil the two keys to the success of a MoE system, 1) the abidance by K-shot, and 2) the insistence on diversity. For the former, we ensure that models that truly possess problem-solving abilities on K-shot are selected rather than those blind guessers. Besides, during data selection, instructions that share task-relevant contexts with K-shot are prioritized. For the latter, we highlight the diversity of constituting experts and that of the fine-tuning instructions throughout the model and data selection process. Extensive experimental results confirm the superiority of our approach over existing methods on utilization of open knowledge across various tasks. Codes and models will be released later.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent few years have witnessed significant development of large language models (LLMs) across a broad spectrum of tasks and domains. The open-source community offers an array of competitive pre-trained/foundation models with general-purpose language understanding and generation capabilities [7], [36], [87], allowing for the follow-up special-purpose tuning customized for specific tasks and domains. To match specialist capabilities, one common exploration to boost pre-trained base models within specialty areas is instruction tuning on expert-curated contents [57], [71], [111]. However, such a training-intensive approach poses challenges to the manual collection and annotation of instruction-response pairs, which impedes agile development and deployment. On the other hand, extravagant computing resources are required to grasp the missing domain knowledge from scratch, making it difficult to appropriately calibrate for the expected responses.\nFortunately, a vast amount of fine-tuned and aligned models [95], together with instruction datasets across various domains and tasks [62], [92], [100], are available online. Such models often exist in the form of LoRA [32] adapters, which are derived from few strong base models. Given any task of interest, one can choose pertinent models from the existing LoRA bank or library as a good starting point. Accordingly, researchers have developed various approaches to utilize and select the publicly released LORA models [33], [63], [67] either in an off-line or an on-line manner for expeditious task adaptation. Meanwhile, great efforts have been put into the selection of high-quality open instruction datasets [74] for improving the instruction following capabilities, where the most challenging [15], [18], [48], [101] and influential [79], [98], [99], [109] datapoints are preferred.\nDespite the tremendous progress made in model and data selection, most existing methods target at lifting LLMs for better alignment with human preference in general domains and common tasks. It remains an under-explored problem to advance specific task expertise of LLMs by fully exploiting the open knowledge such as public LoRA models and datasets. To fulfill such task-oriented knowledge augmentation, in this paper, we propose to bridge the gap between general and \"vertical\" domain knowledge by resorting to a few human-verified instruction samples (i.e., K-shot) from the task of interest. Such K-shot data play a steering role in guiding the selection of the most appropriate candidate models and beneficial datapoints to solve the task at hand.\nGiven a small amount of labeled, real-world instructions from the task of interest (K-shot), we aim at developing a universally effective and easily scalable pipeline that leverages publicly available models and datasets to advance the task expertise of state-of-the-art (SOTA) LLMs.\nIn this context, we encounter three primary challenges:\nGiven a collection of LLMs including one foundation model and its fine-tuned LoRA variants, how can we take full advantage of K-shot to efficiently pinpoint the models with the highest potential for solving tasks of interest?"}, {"title": "2 RELATED WORKS", "content": "Parameter-efficient fine-tuning methods (PEFTs) are a set of techniques that diverge from traditional full-parameter fine-tuning. In PEFTs, only a certain subset of a model's parameters are adjusted to better suit specific tasks of interest."}, {"title": "2.1.1 LoRA", "content": "Low-rank adaptation (LoRA) [32] and its variants [25], [54], [60], [110] use low-rank matrices to approximate additive weights during training. These methods are beneficial as they do not necessitate additional computational resources during inference, where the updated weights can be integrated into the model without any complications."}, {"title": "2.1.2 Prompt Tuning", "content": "Prompt-based methods incorporate randomly initialized soft tokens to the input, usually as a prefix, and train their embeddings while maintaining the LLM's weights constant, as suggested in [44], [49], [118]. While these methods perform competitively, they do entail a substantial computational load during inference."}, {"title": "2.1.3 Adapters", "content": "Adapter methods involve the training of extra modules (for instance, fully connected layers) on top of the frozen pre-trained model [72], [117]. Unfortunately, these adapters are not effortlessly integrated into the original architecture, thereby reducing inference efficiency."}, {"title": "2.2 Mixture-of-Expert Models", "content": "The MoE technique [38], [83] combines several specialized expert models to efficiently scale up models for improved generalization. The principle behind MoE is that each expert is adept at handling a specific region of the input distributional space, and their combined decision-making outperforms any individual expert. Recent studies [29], [45], [50], [56], [97] focus on utilizing the MoE as a PEFT technique. Other methods [20], [33], [64], [67], [114] underscore the use of existing LoRA experts for convenient assembly, where the fine-tuning of their parameters is saved. Specifically, both [35] and [10] route to one expert by comparing the input query with the averaged embeddings of the datasets used to train each individual expert. Such reliance on the fine-tuning datasets restricts the practicability of these methods. [28], [52], [58] all investigate external models such as GPT4 [3] and reward model [7] for estimation of the routing policy, which increases their cost of deployment and causes isolation between the routing model and the candidate experts. In spite of the development of merging techniques, few efforts [53], [69], [84] have been made to combine cross-domain experts for multi-task problems.\nVery recently, PHATGOOSE [63] proposes a post-hoc adaptive token-wise gating mechanism to recycle from a collection of specialized experts. It aims at improving the zero-shot generalization of the pre-trained base model by constructing a routing system and performing dynamic token-by-token assignment. Despite the shared MoE principal, PHATGOOSE differs from the proposed method in three key aspects. First, the problem setting of PHATGOOSE is fundamentally different from ours in that they are not targeted at improving specific tasks of interest. There exists no model and data selection procedures in PHATGOOSE for acquiring task-relevant skills. Second, PHATGOOSE assumes that the contributors of the LoRA models provide additional gate modules that are implemented as sigmoid layers in front of each LORA module. However, it is almost impossible to enforce the same gate training pipeline on the entire open-source community. In practice, one can only find the released LORA modules from the repositories of Huggingface and Github without gating vectors. Third, our MoE system adapts to different tasks of interest by polishing the routing weights and the constituting experts simultaneously, while PHATGOOSE keeps the LoRA modules and gating vectors fixed with lower flexibility. Arrow [67] is another contemporary method which maintains the LoRA library and proposes a routing mechanism to select the most input-relevant LoRAs. Although Arrow is featured by its training-free routing, it does not consider the potential conflicts when the inputs are not representative enough of the given task. The statistics of the LORA parameters (e.g., singular vectors) are directly used as the proxy for the routing weights, which is prone be biased towards the original datasets used to train LoRA modules. Furthermore, they do not introduce the K-shot datapoints from downstream tasks for calibrating the expert behavior, and neglect the exploitation of open-source datasets for optimizing the expert collaboration."}, {"title": "2.3 Data Selection for Efficient Tuning", "content": "Existing methods on data selection for pre-training and instruction tuning of LLMs can be categorized into: 1) quality-based, 2) diversity-based, and 3) importance-based [74]."}, {"title": "2.3.1 Quality", "content": "[59] explores perplexity, L2-Norm error, and memorization as quality scores to rank and prune pre-training corpora. [48] presents a novel self-guided approach to autonomously select high-quality samples from open-source datasets using the instruction-following difficulty (IFD) metric. [18] employs the GPT3.5 to score instruction-response pairs and filters out samples with scores below a threshold. [15] introduces instruct mining to automatically select high-quality instruction-following data for LLM fine-tuning. [27] proposes a quality evaluation model to extract high-quality subsets from the original dataset and designs an algorithm to further select a seed instruction dataset with extensive coverage."}, {"title": "2.3.2 Diversity", "content": "Geometry-based sampling methods are the most intuitive and widely-used ones for maximizing diversity of the selected samples [40], [82], [116]. In particular, k-center greedy [80] is favored in diversity sampling on massive pre-training and instruction-tuning corpus [11], [16], [27], [96]. It performs iterative and greedy selection on data that exhibit the most dissimilarity with the already selected set in the embedding space."}, {"title": "2.3.3 Importance", "content": "Two kinds of gradient-based methods on importance estimation have been developed: 1) gradient matching [8], [109], [113], i.e., the gradients of the entire set being approximated by the weighted gradients of the selected set, and 2) gradient influence [9], [13], [73], i.e., the influence of each training datapoint on the testing set being measured by upweighted gradient multiplication. For importance-oriented sampling, [99] adopts importance resampling to select a subset from a large-scale unlabeled dataset that shares similar distributions with the target set.\nAlthough these methods strike a balance between data quality and quantity, they fail to incorporate the data selection pipeline into the task-oriented model development. On the contrary, the proposed method focuses on improving the downstream performance given limited K-shot real-word datapoints under the context of expert fusion. Correspondingly, we simultaneously consider the quality and importance where the resemblance of an open-source instruction to the K-shot set is prioritized during selection. In addition, we treasure the diversity of the selected dataset as its variety helps polish the coordination between experts in token-wise routing. To the best of our knowledge, the proposed method is the pioneer that integrates comprehensive data selection into the advancement of a MoE system for task expertise, where the concepts of quality, diversity, and importance play a critical role throughout the selection of both expert and data candidates."}, {"title": "3 METHODOLOGY", "content": "The open-source community has witnessed a significant increase in the number of LoRA models and high-quality datasets. To ensure experimental reproducibility, rationality, and comparability, we have selected thirty-eight representative and widely-used instruction datasets from the Huggingface [95] to construct a rich and reliable LoRA bank."}, {"title": "3.1.1 Data Sources", "content": "Specifically, the data to construct our LoRA Bank are summarized as follows: ARC [22], Winogrande [1], GSM8K [23], PiQA [12], CommonSenseQA [85], RACE [42], MBPP [6], MathQA [5], Esnli [14], ECQA [4], CREAK [66], GPT4Tools [103], AQUA [51], QASC [39], QED [43], StrategyQA [30], SensemakingQA [89], Toolformer [78], HellaSwag [107], SiQA [77], BoolQ [21], Dolly [31], WizardLM [100], WebGPT [65], Lima [115], Code-Alpaca [91], ThoughtSource [68], CAMEL [46]. We choose these datasets for two reasons: 1) they provide a comprehensive coverage of mainstream tasks and domains, and 2) their quality is confirmed through massive downloads worldwide and positive feedback comments from researchers."}, {"title": "3.1.2 Data Preprocessing", "content": "All raw datasets are processed into the following format: {\"instruction\": (instruction), \"input\": (input (can be empty)), \"output\": (output)}. For certain CoT datasets, we directly follow [75] to use the template from FLAN [57] and transform the original data into the data format above."}, {"title": "3.1.3 LoRA Fine-Tuning", "content": "For each dataset, we fine-tune pre-trained base models and derive their LORA weights. These LoRA models are used for subsequent model selection. We follow [90] to set the hyper-parameters for optimization: a batch size of 2, gradient accumulation steps of 16, and an initial learning rate of 5e-5. Uniformly, all models underwent training for three epochs to ensure that each dataset is fully mastered regardless of the task difficulty. A cosine decaying schedule is adopted for adjusting the learning rate over iterations. The LoRA modules are applied on the linear embedding layers of the Query and Value matrices in self-attention. For all LoRAs, the rank was set to 16. To reduce training overhead, sequences with length exceeding 1024 were truncated."}, {"title": "3.2 K-shot Guided Expert Model Selection", "content": "In a bank of LoRA models, it becomes essential to appropriately select the most relevant task expert candidates [104], [105]. However, few studies have explored model selection options for language models, especially in the context of K-shot settings. Furthermore, K-shot labeled data are typically sparse and uniform in the input embedding space, which means that the evaluation results of models on such limited data cannot reflect their true expertise level.\nIn this section, we propose to select highly-potential models that would perform well on any task of interest before conducting the task-specific fine-tuning. We hypothesize that two following perspectives should be prioritized during model selection: 1) Does the model possess the necessary knowledge for the tasks of interest? and 2) Can the model adequately follow the instructions under the tasks of interest? Accordingly, we first consider two indicators in estimation of the empirical risk on K-shot samples: 1) the specific evaluation metric for performance measurement, and the perplexity of a LLM in modeling the answers in an auto-regressive manner. Specifically, exact match accuracy is adopted in the present study as the default evaluation metric on the generated responses. If the answer choice precisely matches the model's output, it is considered correct; otherwise, it is deemed incorrect. However, the empirical risk merely computed by performance is prone to be affected by poor instruction following capabilities of LLMs, where models that produce correct but unparsable answers are severely under-estimated. The post-processing techniques cannot handle all the corner cases of answers that are formatted unexpectedly. On the other hand, models that randomly guess the answer choices in QA tasks might be over-estimated. Such misjudgement originates from the low informative evaluation metric that fails to comprehensively assess whether the model can understand and handle the given task. Therefore, it calls upon the perplexity of the model on K-shot sequences as a straightforward complement. To reduce biased measurement merely from one indicator, we take into consideration of three aspects: 1) perplexity, 2) performance, and 3) diversity."}, {"title": "3.2.1 Perplexity", "content": "The perplexity of a LLM on auto-regressive modeling of the answers serve as an effective indicator of the model's capability. Given a model $m$ parameterized by $\\Theta_m$, an input sequence $x$, and its expected output sequence $y$, the perplexity of language modeling is defined with cross-entropy:\n$PPL(x, y, \\theta_m) = exp(-\\sum_{i=1}^{y}log\\ P(y(i)|x, Y(<i); \\theta_m)), (1)$\nwhere $P(y(i)|X,Y(<i))$ is the predicted probability of the i-th token $y(i)$ of $y$ given the sequences $x$ and $Y(<i)$.\nHowever, as explained in the drawbacks of exact match on multiple-choice problems, the perplexity computed solely on the ground-truth answer options in many QA tasks still suffers from inaccurate estimation of model capabilities. To address this issue, we utilized an advanced open model [61] to expand the answers with CoT rationales. Subsequently, we calculated the reasoning perplexity of LoRA models by considering both the predicted answer choices and the rationales behind them:\n$PPL(x, y, \\theta_m) = exp(-\\sum_{i=1}^{\\hat{y}}log\\ P(\\hat{y}(i)|x, \\hat{y}(<i); \\theta_m)), (2)$\n$\\hat{y} = \\Phi(x, y), (3)$\nwhere $\\Phi(x, y)$ represents the CoT expansion process given both the input $x$ and the output $y$. For such CoT rationales, we target at multiple-choice datasets whose outputs are only answer options (e.g., A and B). We use the CoT-formatted answers rewritten by the WizardLM2-8x22B [61] for reasoning explanations (see Fig. 3 for detailed prompts). Such rationales improve the transparency and interpretability of the decision-making process, which benefits accurate model selection by estimating the uncertainty of any LLM on the rationales under the context of instruction. Besides, answers with reasoning process in advance are more effective in fine-tuning models, where the justified rationales well calibrate the response towards the ultimate correct answers. As illustrated in Fig. 2, models with higher reasoning perplexity might achieve better performance before task-specific tuning, but their performance gains brought by fine-tuning are not as strong as models with lower reasoning perplexity.\nFor the K-shot $D_k = \\{(X_1,Y_1), (X_2,Y_2), ..., (x_k,y_k)\\}$, $|D_K| = K$, the total reasoning perplexity is defined as:\n$PPL_R(M, D_K) = \\frac{1}{K} \\sum_{(x_i,Y_i) \\in D_K} PPL(X_i, \\Phi(X_i, Y_i), \\theta_m). (4)$"}, {"title": "3.2.2 Performance", "content": "The evaluation metric measured on K-shot directly reflects a model's ability to solve the corresponding task in an \u201cend-to-end\" way. We denote such evaluation results as the performance, which is defined by any metric calculated by comparing the generated responses $\\tilde{y}$ and the ground-truth answers $y$. In line with the task requirement, a post-processing step might be involved to extract formatted answers from responses. It can be implemented through certain function $f(\\cdot)$, i.e., $\\tilde{y}' = f(\\tilde{y})$. Common post-processing operations include threshold-based truncation and probability-to-category mapping. Out of simplicity, the performance of a model $m$ on the dataset $D_K$ can be defined as the accuracy of matching the post-processed $\\tilde{y}'$ with the ground-truth $y$:\n$Acc(m, D_k, f) = \\frac{1}{K} \\sum_{(x_i,Y_i) \\in D_K} 1(\\tilde{y}' = Y_i),$ (5)\n$\\tilde{Y}' = f(\\tilde{y}),$\nwhere $1(.)$ is the indicator function that equals to 1 when the condition holds true and 0 otherwise. $\\tilde{y}$ is the predicted response by $o_m$ given the input $X_i$."}, {"title": "3.2.3 Diversity", "content": "To fully harness the task-related knowledge inherent in models from the LORA Bank, our objective is to select all candidate models that are likely to solve the downstream tasks. To expand the knowledge base, it is essential to \u201cdeduplicate\" the selected models so that each candidate model contributes to the accumulation of skills. Therefore, diversity is ensured so that different selected experts possess distinct abilities. Contrary to existing studies [41], [47], [84] that primarily focus on combining experts without considering their relationships, we highlight the concept of group diversity for retrieving expert candidates. It refers to the variety of the selected models in a group. A high intra-group diversity lays a solid foundation for exploitation of task-relevant yet complementary knowledge. Given a group of $N$ expert models $B_N = \\{M_1,M_2, ..., m_N\\}$, the group diversity $\\Omega_{B_N}$ is defined as the inverse of the sum of cosine similarities between the parameters of each model pair $E(m_i)$ and $E(m_j)$:\n$\\Omega_{B_N} = (\\frac{2}{N(N-1)} \\sum_{m_i\\in B_N}\\sum_{m_j\\in B_N} \\frac{E(m_i)E(m_j)}{||E(m_i) || ||E(m_j) ||} -1), (6)$\nwhere $E(m_i)$ denotes the flattened matrices of layers in the model $m_i$. The cosine similarity between the paired $m_i$ and $m_j$ is computed matrix by matrix and averaged over all layers."}, {"title": "3.2.4 Selection Mechanism", "content": "The overall pipeline of model selection is illustrated in Fig. 4 and detailed in Alg. 1. One important principle behind our pipeline design is to comprehensively employ indicators from various aspects, which reduces the bias by partial measurement.\nWe first calculate the reasoning perplexity and the performance in accuracy for each model in the bank B based on Eq. 4 and Eq. 5. We sort models by perplexity and performance respectively from smallest to largest and from largest to smallest. The rankings by perplexity and performance are respectively denoted as $R_P$ and $R_A$. Then, we select M candidate models $B_M$ from the LORA Bank B with minimum sum of rankings $R_P$ and $R_A$. In this way, models in lack of task-relevant knowledge and skills are filtered out in advance to greatly reduce the computation overhead on diversity measurement. Subsequently, we calculate the group diversity $\\Omega_{B_N}$ on all combinations of N-tuples $B_N$ from $B_M$, and sort these tuples by intra-group diversity from largest to smallest for the ranking $R_D$. Finally, we sum up all the rankings of $R_P$, $R_A$, and $R_D$ and choose the top N models $B_N \\subset B_M$, $|B| = N$ for initializing our MoE system $B_E$.\nCompared with other methods, our selection approach is more robust to variation of K-shot across tasks due to its all-sided evaluation in perplexity, performance, and diversity. Besides, it consumes fewer computational resources for two reasons: 1) Low-quality and impotent models are eliminated before the exhaustive computation of intra-group diversity. 2) Each model performs evaluation only once on K-shot data."}, {"title": "3.3 Mixture-of-Experts Initialization", "content": "Upon selecting the most promising experts, our objective is to efficiently utilize their potentials at respective domains. The linear arithmetic composition offers a straightforward approach to benefit from our LoRA bank [20], [33], [108]:\n$W = W + \\sum_{i=1}^{N} w_iW_i, (7)$\nwhere $W$ indicates the original parameter of a pre-trained model and $\\Delta W_i$ denotes the i-th LoRA variant with $\\sum_{i=1}^{N} W_i = 1$. Nevertheless, such a naive method of assigning weights to different LoRAs does not promise a dynamic and flexible routing mechanism, restricting its quick adaptation to any task of interest.\nA more reasonable strategy would be a mixture of expertise. Specifically, we train the router in such a manner that the model autonomously allocates different tokens to appropriate experts. Given an input $x$, the output of a N-expert MoE system at the l-th layer $g^l(x)$ is computed as the weighted sum of the outputs from each expert:\n$g^l(x) = \\sum_{i=1}^{N} G^l_i(x).g^l_i(x), (8)$\nwhere $G^l_i(x)$ represents the i-th element of the gating vector $G^l(x) \\in \\mathbb{R}^N$, controlling the contribution of the i-th expert. $g^l_i(x)$ is the output from the i-th expert at the l-th layer. A simple yet efficient implementation of the gating network $G^l(\\cdot)$ is a single fully-connected layer $W$, where the gating vector is obtained by matrix multiplication between $W$ and $g^{l-1}(x)$. Only the top-k activated experts are selected with their gating weights re-normalized via softmax function:\n$G'(x) = softmax (top-k (g(x))), (9)$\nwhere top-k(.) keeps the value of the largest k elements unchanged and returns the other elements as -\u221e. We fine-tune all LORA experts and routers simultaneously (see Fig. 5)."}, {"title": "3.4 K-shot Guided Sim-Div Data Selection", "content": "In situations where few annotated data are available under the task of interest, it is prone to overfitting if our MoE system is directly fine-tuned on these datapoints. A common approach to address this issue is data augmentation [26], [81], [94], [112], which mitigates the risk of degeneration via lexical or semantic augmentation on the original text inputs. Such manually designed techniques tend to work on traditional discriminative models by imposing perturbations to the decision boundary. However, for generative LLMs, vanilla data augmentation methods do not inherently inject any new knowledge into the model, nor do they fundamentally improve the diversity of the maintained instruction dataset.\nIn the light of this statement, we propose to leverage open-source data for task-oriented augmentation. It has three advantages including: 1) high cost-efficiency of utilizing the massive and free open-source datasets, 2) prevention of overfitting by introducing diverse and beneficial instructions, and 3) improvement of token-wise collaboration between experts via acquiring novel knowledge. When it comes to the selection of specific data instances, a new question is raised: What type of open-source data should be favored to improve the performance on our task of interest? In the present study, we assume that datapoints that share a similar distribution with samples in the downstream tasks ought to be prioritized. Accordingly, we propose the similarity-first and diversity-aware principle to guide the data selection process."}, {"title": "3.4.1 Data Encoding", "content": "In the first step, we perform encoding of the raw instruction texts for their projection into the embedding space. Given the K-shot $D_K$ and a set of $S$ open-source samples $D_S = \\{(X_1,Y_1), (X_2,Y_2),..., (x_s,y_s)\\}$, $|D_S| = S$, we aim to find the most relevant datapoints in $D_S$ based on their similarity with samples in $D_K$. Given a pre-trained encoding model $h$ parameterized by $\\theta_h$, we obtain the representation of each sample $u_i$ with both input and output via:\n$u_i = h([x_i, Y_i]; \\theta_h), (10)$\nwhere $[,]$ denotes the concatenation operation in python syntax. The embeddings of samples $(x_i, Y_i)$ from $D_K$ and $D_S$ are respectively denoted as $U_K$ and $U_S$. In practice, we employ the pre-trained BGE model [17] as h, which is fine-tuned on the XLM-ROBERTa [24] with self-distillation on corpus of multiple languages, tasks, and granularities. Specifically for extraction of $u_i$, we use the prefix prompt of query_instruction_for_retrieval: \"Represent the following sentence for similar task retrieval: ([xi, Yi])\u201d."}, {"title": "3.4.2 Similarity-First", "content": "We calculate the cross-dataset similarity between $D_K$ and $D_S$ to select a subset $D_C \\subset D_S$ that resembles $D_K$ the most. First, we define a distance $d(\u00b7, \u00b7)$ to measure the similarity between two encoded samples $u_i$ and $u_j$. Without losing generality, we demonstrate an intuitive implementation of similarity metric via cosine distance:\n$sim(u_i, u_j) = 1 - d_{cos}(U_i, U_j) = \\frac{U_i \\cdot U_j}{|| U_i || || U_j ||}, (11)$\nwhere each pair of $u_i \\in U_K$ and $u_j \\in U_S$ constructs the entry $A_{ij}$ of the similarity matrix $A \\in \\mathbb{R}^{K \\times S}$:\n$A_{ij} = sim(u_i, u_j). (12)$\nThen, we pinpoint top C samples in Ds that share the most similarity with DK by maximizing along the rows of A:\n$D_C = arg max \\sum_{(x_j, y_j)\\in D_C} (max_i A_{ij}). (13)$\n$D_C \\subset D_S, |D_C|=C$\nThe reasons why $D_C$ is first selected out of $D_S$ are two-fold: 1) It ensures that the samples which enjoy a high level of similarity with $D_K$ are prioritized while those dissimilar ones would not be unexpectedly introduced into the candidate set during the subsequent selection. 2) It reduces the computation overhead of diversity measurement since the entire set $D_S$ shrinks into a much smaller candidate set $D_C$ with $C < S$."}, {"title": "3.4.3 Diversity-Aware", "content": "We remove duplicates in $D_C$ to improve its overall diversity for the selected dataset $D_A$. A greater level of diversity not only improves the token-wise cooperation between experts on broader domains and topics but also reduces the overfitting of the MoE system on K-shot datapoints. Specifically, we remove a subset of data with excessively high semantic similarity. We use the same distance metric defined in Eq. 11 and compute the intra-dataset similarity matrix I. The pairwise similarity is measured on both $u_i \\in U_C$ and $u_j \\in U_C$, where $U_C = \\{U_i|(X_i, Y_i) \\in D_C\\}$:\n$I_{ij} = sim(u_i, u_j). (14)$\nWe follow the SemDeDup [2], [86] to perform semantic deduplication by thresholding with T. If the similarity between any two instructions exceeds t, we discard the one whose similarity with K-shot $D_K$ is lower. By this means we maintain the overall diversity of the final selected dataset $D_A$. The entire process of data selection is elaborated in Alg. 2."}, {"title": "3.5 Mixture-of-Experts Fine-Tuning", "content": "We combine the augmented dataset $D_A$ and the K-shot dataset $D_K$ for optimizing both the routing weights and the experts of our MoE system $\\Theta_{MOE}$, where the cross-entropy loss is employed to supervise the language modeling on the outputs of the last L-th MoE layer (Eq. 8):\n$L(x, y, \\Theta_{MOE}) = \\sum_{i=1}^{y}log P(y(i)|x, Y(<i); \\Theta_{MOE}),$ (15)\nwhere $P(y(i)|x, Y(<i); \\Theta_{MOE}) = softmax(g^L([x,y(<i)]))_{y(i)} \u00b7$"}, {"title": "4 EXPERIMENTS", "content": "In this section, we validate the effectiveness of our proposed method through a series of experiments on various tasks of interest. To begin with, we use six popular open-source datasets, namely ARC-Challenge, ARC-Easy [22", "12": "BoolQ [21", "6": "and GSM8K [23", "85": "and SiQA [77"}]}