{"title": "Why does in-context learning fail sometimes?\nEvaluating in-context learning on open and closed\nquestions.", "authors": ["Xiang Li", "Haoran Tang", "Siyu Chen", "Ziwei Wang", "Ryan Chen", "Marcin Abram"], "abstract": "We measure the performance of in-context learning as a function of task novelty\nand difficulty for open and closed questions. For that purpose, we created a novel\nbenchmark consisting of hard scientific questions, each paired with a context\nof various relevancy. We show that counter-intuitively, a context that is more\naligned with the topic does not always help more than a less relevant context. This\neffect is especially visible for open questions and questions of high difficulty or\nnovelty. This result reveals a fundamental difference between the treatment of close-\nform and open-form questions by large-language models and shows a need for a\nmore robust evaluation of in-context learning on the variety of different types of\nquestions. It also poses a new question of how to optimally select a context for large\nlanguage models, especially in the context of Retrieval Augmented Generation\n(RAG) systems. Our results suggest that the answer to this question can be highly\napplication-dependent and might be contingent on factors including the format\nof the question, the perceived difficulty level of the questions, and the novelty or\npopularity of the information we seek.", "sections": [{"title": "1 Introduction", "content": "Despite their indisputable successes [Bommasani et al., 2021, Drori et al., 2022, Chang et al., 2024],\nLarge Language Models (LLMs) often struggle to answer challenging questions [Rawte et al., 2023].\nWhile they can achieve superhuman accuracy on many benchmarks [Luo et al., 2024], they also suffer\nfrom hallucinations [Ye et al., 2023, Azamfirei et al., 2023], lack of coherence [Xie et al., 2023b],\nand are prone to cognitive errors [Jones and Steinhardt, 2022, Hagendorff and Fabi, 2023].\nTo make the difficult situation even worse, it is not always easy to detect mistakes committed by\nLLMs since their responses are often presented in a way that emulates correct and coherent answers\n[Bender et al., 2021, Scheurer et al., 2023]. For practical reasons, many existing benchmarks only\ntest the ability to answer either closed [Chang et al., 2024] or easy-to-verify questions, e.g., regarding\ncommon knowledge [Bisk et al., 2020, Clark et al., 2018] or questions that can be algorithmically\nverified [Srivastava et al., 2024].\nAnother challenge concerns domain generalization and domain shift problems, resulting in the need\nto constantly update your machine learning models to account for the evolution of various trends in\nyour data [Zhou et al., 2022]. However, improving the performance of pre-trained LLMs for specific\ntasks by fine-tuning is both expensive [Bender et al., 2021, Luccioni et al., 2023] and technically\nchallenging [Kandpal et al., 2023, Gaspers et al., 2022]. While some techniques like Low-Rank\nAdaptation (LoRa) can reduce the cost of training [Hu et al., 2021], it does not solve the main issue,\nnamely, how to allow LLMs to leverage new pieces of information that were not a part of the initial\ntraining corpus [Liu, 2017].\nOne approach to the issue might be in-context learning [Brown et al., 2020], where LLMs effectively\nlearn to solve a given problem leveraging a limited number of examples without updating the model\nparameters. Namely, in-context learning incorporates question-solution pairs in the input prompt,\nallowing LLMs to detect the logic and patterns of those examples, subsequently improving the LLMs\noutput accuracy. It enables LLMs to acquire new knowledge in the inference time and utilize it in\nsubsequent responses. This technique significantly reduces the complexity of improving the LLMs\nperformance compared to alternative approaches such as fine-tuning [Min et al., 2022b]. It should\nalso be noted that the effectiveness of the popular Retrieval-Augmented Generation (RAG) techniques\nrelies heavily on the strength of in-context learning [Gao et al., 2024], as discussed later.\nIn this paper, we focused on the question of how various types of context improve the effectiveness\nof in-context learning when answering challenging questions. We noticed a surprising behavior.\nNamely, depending on the difficulty and novelty of the question, and depending on the fact whether\nthe question is of the open or closed type, the relation of the measured performance of the model to\nboth the perceived and quantified relevancy of the context varies. Notably, the measured in-context\nlearning performance of GPT-4 was positively correlated to context relevancy in two benchmarks\nwith closed-form questions but negatively correlated in our benchmark with open-form questions,\nindicating different utilization of context depending on the form of the received questions.\nIn the next sections, we introduce our novel dataset, which comprises 160 unique question-response\npairs from the fields of physics and computer science with varying levels of difficulty. For the purpose\nof evaluation, each question is accompanied by one of four types of context (including no context to\nserve as a control group) and paired with a generated answer from GPT-4. In the subsequent sections,\nwe detail our grading scheme and present the results aggregated from each of our graders. Next, we\ncompare our findings with the existing work by Min et al. [2022b], highlighting a notable discrepancy\nin the measured effectiveness of the context. To elucidate this difference, we delve deeper into the\nnature of the problem, discovering that the main impact comes from the open or closed form of\nthe questions, with additional effects related to the difficulty or novelty of those queries. To further\nstrengthen our analyses, we then compare the performance improvement associated with in-context\nlearning across a range of context relevancy using two additional close-ended question datasets,\nMetaICL [Min et al., 2022a] and NephSAP [Wu et al., 2023b] and we contrast the results with our\nfindings harvested with the help of our open-ended question dataset. Following this, in the discussion\nsection, we discuss the impact of our work, especially in the context of the RAG systems, future\nresearch directions, and other methods that enhance LLM performance"}, {"title": "2 Related Work", "content": "Large Language Models. LLMs have shown remarkable capabilities in various tasks, including\ncode generation [Kojima et al., 2022, Siddiq and Santos, 2023], text summarization [Sahu et al.,\n2023], and database query optimization [Li et al., 2023]. They demonstrate a surprising ability to\nperform in-context learning, where an LLM \u201clearns\u201d to do a task simply by conditioning on a prompt\ncontaining input-output examples, achieving state-of-the-art (SOTA) results on various benchmarks.\nHowever, there has been little understanding of how the model leverages the context and what makes\nin-context learning work. In addition, their performance significantly depends on the contextual\ninformation provided and, as discussed in this paper, on the form and type of the queries.\nIn-Context Learning. In-context learning has been a focal point in recent research. Unlike tra-\nditional fine-tuning methods, in-context learning adapts models to unseen tasks by incorporating\nexamples directly into the input context, as highlighted by Brown et al. [2020]. Xie et al. [2022]\ndiscussed how in-context learning can be understood as implicit Bayesian inference, where models\ninfer latent concepts to generate coherent responses. Techniques such as chain-of-thought prompting\n[Wei et al., 2022, Press et al., 2023, Wang et al., 2022, Zhou et al., 2023, Imani et al., 2023, Besta et al.,\n2023] have shown significant improvements in reasoning tasks. Recent frameworks like OpenICL"}, {"title": "3 Originality and general impact of the work assessment", "content": "Originality. In this paper, we argue that closed questions, such as multiple-choice or fill-in-\nthe-blank formats, do not adequately reflect the challenges posed by open questions that require\ndeep understanding and synthesis of information from diverse contexts. While Min et al. [2022b]\nhave shown that context significantly affects LLM performance, they have not quantified how\ndifferent levels of context relevancy impact responses to different types of questions. Our research\naddresses this gap by creating a novel benchmark that focuses on open, challenging questions. These\nquestions are paired with various types of contexts to systematically evaluate how context affects\nLLM performance.\nImpact of the paper. Furthermore, our work suggests areas for improving the performance of\nRetrieval-Augmented Generation (RAG). Current RAG studies focus on providing context during\nmodel inference. Given our observation of the inconsistent relationship between the relevance of\ncontext and model performance for different question types (open-form and closed-form), we believe\nthat the context retrieved by comparing vector similarity using RAG may not always correlate with\nthe most useful context for enhancing LLM inference performance and does not mitigate issues such\nas hallucinations and logic errors. We propose that the type of context selected should be tailored\nto the attributes of the type of questions with several practical propositions of the retrieval regions\noutlined in the discussion."}, {"title": "4 Is more relevant context always better?", "content": "To investigate the relationship between the relevance of context and the performance of large language\nmodels (LLMs), we created an open-form questions dataset comprising physics and computer science\nquestions of varying difficulty levels and originality. Next, we prepared contexts with four different\nlevels of relevancy for each question in our dataset.\nThe selected questions cover the following areas: quantum mechanics, physics for life science, elec-\ntromagnetism, classical mechanics, and computer science. Solutions usually involve a combination\nof complex calculations and the application of conceptual knowledge. Each question is categorized\nunder one of the three different difficulty levels: easy, medium, and hard. The difficulty of the ques-\ntion is defined by the grader according to their perceived complexity of the question. Additionally,\neach question is also categorized under one of three originality categories: known, paraphrased, and\noriginal. Known questions can be found online or in textbooks, paraphrased questions are modified\nversions of known questions, and original questions were handcrafted by the authors of this paper.\nFor each question, we created a ground truth answer for scoring reference and four context types\nwith different levels of relevance. The four context types are: (1) \"no context\" to serve as a control"}, {"title": "4.2 Evaluation", "content": "Our evaluation system comprised three main categories, Completeness and Relevancy (5 points),\nLogic and Reasoning (5 points), and Truthfulness (understood as lack of hallucination) (5 points).\nIn addition, graders had the option to identify specific problems in the responses, such as hallucina-\ntions, omission, irrelevant, calculation error, and logic error. They could also highlight portions of\nthe responses as incorrect, correct, or irrelevant. An open response section was provided for graders\nto give comments and feedback about the generated responses. Finally, graders were asked to rate\ntheir confidence in their own grading. These options allowed us to gain deeper insights into the\ngrading process and to assess the quality of the generated responses in detail. A screenshot of the\nscoring interface can be found in the Supplementary Material.\nEach grader may have different biases and varying levels of expertise. To enhance the accuracy and\nreliability of our evaluation, we ensured that all graders assessed all 160 questions. This approach\nwas essential for obtaining consistent and accurate results. By having multiple graders evaluate\neach response, we mitigated individual biases and ensured a more comprehensive assessment. This\nmethod captured a broader range of perspectives and expertise, leading to a more robust and reliable\nevaluation of the generated responses. As demonstrated later, this comprehensive grading significantly\nimproved the accuracy and consistency of our findings."}, {"title": "4.3 Results", "content": "To illustrate the correlations between the context types and the quality of the corresponding generated\nresponses, in Fig. 1 panel A, we show the raw average scores of each context type for each grader.\nNotably, the results are rather noisy, with each grader having an individual tolerance for different\ntypes of errors, resulting in different reference levels for each of them.\nBy design, each question was evaluated by each grader. This additional redundancy allows us to\nstandardize the scores for each grader and then average them, resulting in reduced variance in the\nfinal results. This aggregation procedure is depicted in Fig. 1 panel B.\nAs a result, although the raw scores displayed differences in trends and values across all three grading\nrubrics, a clear trend appeared after we applied the aggregation procedure, as depicted in Fig. 1\npanel C. Counter-intuitively, a higher standardized average score was associated with no context, and\nthe lowest score with the relevant context."}, {"title": "4.3.2 Difficulty Levels and Originality Types", "content": "To investigate how the difficulty of questions affects the quality of generated responses, we compared\nthe results across three difficulty levels (easy, medium, and hard) for each of the four context types,\nas shown in Figure 2, panel A. We can observe a clear trend of decreasing scores as the difficulty\nof the questions increased from medium to hard, indicating that GPT-4's performance declines with\nhigher question difficulty. This also indicates that human-perceived difficulty of the question was in\nfact, correlated with the factual difficulty experienced by GPT-4, a result interesting on its own. For"}, {"title": "4.3.3 Result comparison", "content": "In this section, we combined the standardized scores from all graders and compared them across\ndifferent context types. Our results indicate that, on average, the responses generated with no\nadditional context or with the help of irrelevant context are of higher quality than the responses\ngenerated for queries incorporating highly relevant context. This result is in striking difference to\nresults of Min et al. [2022b]. To further understand this discrepancy, in the next section, we replicate\nthe key findings of Min et al. [2022b], and we discuss what might cause the difference in the behavior."}, {"title": "5 Citical comparison with existing study", "content": "Min et al. [2022b] demonstrates that in-context learning allows us to achieve significantly better\nresults compared to the \"no context\" case. In addition, the authors show that in-context learning\nis robust to label noise. Namely, the authors show that context with randomly shuffled labels and\n\"golden\" context (with correct labels) have similar effects in enhancing the quality of generated\nresponses for closed questions, such as multiple choice and true/false questions.\nHowever, to investigate the striking difference in the observed trends and to eliminate the effect of\ndifferent versions of ChatGPT playing a potential role here, we decided to replicate the key results\nfrom Min et al. [2022b] using precisely the same framework as above and using the same version of\nthe LLM, namely gpt-4-1106.\nFor the replication, we decided to use two different existing benchmarks, MetaICL [Min et al., 2022a]\nand a dataset from NephSAP [Wu et al., 2023a]. The only significant element, differentiating this\nstudy from our previous evaluations, is that both of these datasets contain close-form questions."}, {"title": "5.2 Data and Methodology", "content": "Our evaluation of in-context learning of closed-form questions involves two datasets. For the MetaICL\ndataset, we take a subset of 10 different tasks, each containing multiple-choice questions. For the\nNephSAP dataset, we take multiple-choice questions within 20 different subjects. Details about tasks,\nsubjects, and sample questions can be found in the Supplementary Materials.\nWe conduct an 80-20 train test split for both the MetalCL dataset and the NephSAP dataset. For each\nmultiple-choice question in the test set, we generate a response using the gpt-4-1106-preview model.\nWe do it three times: once without any context, once with a randomly sampled demonstration with\na different task or subject from the training set of the dataset, and once with a randomly sampled\ndemonstration with the same subject or task from the training set.\nWe also compute the embedding of the questions and the demonstrations. We bin the embedding\nsimilarity of each demonstration/response pair into separate bins. Treating the no-context response\nas a benchmark, we record the general score improvement of the response within each embedding\nsimilarity bin compared to the raw benchmark."}, {"title": "5.3 Context Relevancy and Performance improvement", "content": "In Fig. 3, we show the score improvement as a result of different contexts, using the no-context\nanswer as the baseline.\nNote how context similarity is positively correlated with the mean score improvement in both of\nthe closed-question datasets (MetalICL and NephSAP). This result is consistent with the arguments\nmade by Liu et al. [2021] and Rubin et al. [2022]. Note also that in both closed-question datasets, the\ncontext with the lowest levels of similarity scores has a tendency to have a negative mean improvement\n(meaning, adding context hurts the results). As contexts with low levels of similarities are more likely\nto be contexts with a different subject or task, this result is consistent with the findings in [Liu et al.,\n2021], where irrelevant demonstrations can hurt the performance of LLM.\nThis contrasts the results for the closed-form questions, as depicted in Fig. 3, panel C. Our open-form\nquestion results display a negative correlation between context similarity and mean improvement.\nThe results suggest that, in this case, context with a lower level of similarity can be more helpful in\nimproving the quality of the response, whereas context with a higher level of similarity can hurt the\nquality of the response."}, {"title": "6 Discussion", "content": "Our results have suggested a significant difference between open-form question evaluation and\nclose-form question evaluation, as the relationship between context-similarity and performance\nimprovement is completely reversed in those two cases. The implications of this result are twofold.\nFirst, the difference between open-question evaluation and close-question evaluation invokes a new"}, {"title": "6.2 How should we evaluate in-context-learning? Open vs Close", "content": "The different behaviors exhibited in open-form question evaluation and closed-form question eval-\nuation stem from a different treatment of context in those two cases. We provide a hypothetical\ninterpretation of that mechanism. In closed-form multiple-choice questions, the evaluated language\nmodel is treated as a classification model. A relevant demonstration provided as a context can improve\nthe LLM's performance by aligning it with the correct choice. In open-form questions, the evaluated\nlanguage model is treated as a generative model, and the response is open-form. Instead of being\neither correct or incorrect, an open-form response can be anywhere in between. A relevant context\nprovides alignment with one way of approaching the question, but it can also introduce bias, leading\nto performance degradation instead of improvement."}, {"title": "6.3 How should we select context with respect to RAG", "content": "The difference between the relationship between context relevancy and performance in open-form\nand closed-form questions suggests that the RAG is highly application-dependent. For example, the\nstrategy for context retrieval for open-form applications should be different from the strategy used in\nclosed-form applications. It is also important to be mindful when evaluating RAG, as common closed-\nform benchmarks might not be good indicators of RAG's performance in open-form applications.\nWhen designing an RAG, especially in open-form applications, it is important to include some other\nfactors than pure embedding distance or relevancy. Sometimes including a piece of context that is not\nas close in embedding distance to the question might be helpful as it does not reinforce the hidden\nbias inside the question."}, {"title": "Sanity Check", "content": "To check whether our context relevancy is well defined, we compute the embedding of the questions\nand their respective contexts for both our open-form question dataset and the two closed-form question\ndatasets we use. We then calculate the cosine distance between the embedding of each question and\nthe different contexts associated with them. We show the results for the open question dataset in\nFig. 6.\nWe computed the embedding of each question and each context using OpenAI's \u201ctext-embedding-3-large\" model. For the no-context part, we used a space as a placeholder instead of an empty string.\nAs expected, the results show that more relevant contexts, as perceived by us when designing the\ndataset, receive a higher mean similarity score with their respective questions. Different question\ntypes can result in a large standard deviation in similarity scores in different contexts. We show the\ndetails breakdown of those results in Fig. 7.\nAll question types except hard paraphrased questions display the same trend, confirming the relation-ship between context types and embedding similarities."}]}