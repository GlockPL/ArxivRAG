{"title": "SEADAG: SEMI-AUTOREGRESSIVE DIFFUSION FOR\nCONDITIONAL DIRECTED ACYCLIC GRAPH GENERA-\nTION", "authors": ["Xinyi Zhou", "Xing Li", "Yingzhao Lian", "Yiwen Wang", "Lei Chen", "Mingxuan Yuan", "Jianye Hao", "Guangyong Chen", "Pheng Ann Heng"], "abstract": "We introduce SeaDAG, a semi-autoregressive diffusion model for conditional gen-\neration of Directed Acyclic Graphs (DAGs). Considering their inherent layer-wise\nstructure, we simulate layer-wise autoregressive generation by designing different\ndenoising speed for different layers. Unlike conventional autoregressive gener-\nation that lacks a global graph structure view, our method maintains a complete\ngraph structure at each diffusion step, enabling operations such as property con-\ntrol that require the full graph structure. Leveraging this capability, we evaluate\nthe DAG properties during training by employing a graph property decoder. We\nexplicitly train the model to learn graph conditioning with a condition loss, which\nenhances the diffusion model's capacity to generate graphs that are both realistic\nand aligned with specified properties. We evaluate our method on two represen-\ntative conditional DAG generation tasks: (1) circuit generation from truth tables,\nwhere precise DAG structures are crucial for realizing circuit functionality, and (2)\nmolecule generation based on quantum properties. Our approach demonstrates\npromising results, generating high-quality and realistic DAGs that closely align\nwith given conditions.", "sections": [{"title": "1 INTRODUCTION", "content": "The success of diffusion models in various domains (Dhariwal & Nichol, 2021; Kong et al., 2021)\nhas led to significant interest in their application to graph generation (Vignac et al., 2023; Kong\net al., 2023). In this work, we focus on conditional Directed Acyclic Graph (DAG) generation.\nDAGs are essential and widely used data structures in various domains, including logic synthesis (Li\net al., 2022; Liu & Young, 2023; Wang et al., 2023; Pei et al., 2024) and bioinformatics (Zhou &\nCai, 2019). Compared to general graphs, DAGs possess an inherent layer-wise structure with\nintricate node inter-dependencies that can significantly influence the overall graph properties. In\nlogic synthesis, for example, minor structural alterations in lower layers can propagate errors to\nhigher layers, resulting in substantial functional differences. Modeling these layer-wise structural\nfeatures of DAGs requires specially designed model architectures and generation mechanisms (An\net al., 2024). Recognizing these challenges, many established DAG synthesis and optimization\ntools (Mishchenko et al., 2006; Flach et al., 2014; Li et al., 2024b; Wang et al., 2024) employ a\nsequential synthesis approach, allowing for effective propagation and modeling of localized changes\nat each step on the global DAG structure.\nRecent studies have explored DAG generation using Autoregressive (AR) diffusion models (Li et al.,\n2024a), owing to their improved efficiency and enhanced ability to model node-edge dependen-\ncies (Kong et al., 2023). However, existing approaches generally face the following challenges:\n(1) Their strict part-by-part generation impedes information flow from later components to earlier\nones, whereas in DAG structures with complex inter-layer interactions, subsequent layer structure\ncan often influence the message passing in preceding layers (Wu & Qian, 2016). (2) They lack a\nglobal graph structure view until the final generation step. This limitation is particularly problem-\natic in conditional generation scenarios, where graph properties or functions generally cannot be\nevaluated without a complete structure (Fang et al., 2022). This incomplete view of conventional\nAR methods hinders effective property guidance during both training and sampling (Vignac et al.,\n2023). (3) Many existing works do not employ explicit condition learning during training, with con-\nditional guidance applied only during sampling (An et al., 2024; Vignac et al., 2023). However, our\nexperiments suggest that incorporating explicit graph condition learning during training enables the\nmodel to more effectively balance condition satisfaction and graph realism.\nTo address above issues while preserving the benefits of AR models, we propose SeaDAG, a SEmi-\nAutoregressive diffusion-based DAG generation model that enables graph property control, as illus-\ntrated in Figure 1. Our approach simulates layer-wise autoregressive generation while being able\nto output a complete graph at every diffusion step. We achieve this by applying different denoising\nspeeds to nodes and edges within different layers, inspired by recent advances in natural language\ngeneration (Wu et al., 2023). Therefore, layers with higher-noise can be generated conditioned on\nother less-noisy layers.\nIn summary, SeaDAG offers several key advantages over existing methods:\n(1) SeaDAG fully exploits the hierarchical and layer-wise nature of DAGs. We design a de-\nnoising process that mimics sequential AR generation, enabling the model to effectively capture\ninter-layer dependencies.\n(2) SeaDAG evolves all layers simultaneously, albeit at different rates, Unlike the strict part-by-\npart generation, this simultaneous evolution enables more flexible generation and message passing\namong layers.\n(3) SeaDAG maintains a complete graph structure at each timestep, akin to one-shot methods.\nLeveraging this, we incorporate explicit condition learning by employing a property decoder dur-\ning training to evaluate graph properties. By using a condition loss, we explicitly teach the model to\nlearn the relationship between DAG structures and properties, which helps the model simultaneously\nsatisfy the conditions while producing realistic graph structures.\nTo demonstrate the broad applicability of our method, we evaluate it on two significant conditional\ngraph generation tasks from distinct domains. First, we address an important challenge in electronic\ndesign automation (Liu et al., 2023): circuit generation from truth tables. This task was selected\ndue to the pervasive use of DAGs in circuit design, representing a classic application of DAGs in\nreal-world engineering. Second, to showcase our model's versatility, we tackle molecule generation"}, {"title": "2 METHODS", "content": "2.1 PRELIMINARY\nDirected acyclic graph A directed acyclic graph with n nodes $V = \\{n_1, n_2 . . . n_n\\}$ can be repre-\nsented as $G = (X, E)$. $X \\in \\mathbb{R}^{n \\times k_x}$ is the node type matrix. Each row $x_i \\in \\mathbb{R}^{k_x}$ is a one-hot vector\nencoding the type of node $n_i$ among $k_x$ possible types. $E \\in \\mathbb{R}^{n \\times n \\times k_e}$ is the edge type matrix. Each\nentry $e_{ij} \\in \\mathbb{R}^{k_e}$ is a one-hot vector encoding the type of the directed edge from $n_i$ to $n_j$ among $k_e$\npossible types. In this work, for a directed edge $e_{ij}$, we refer to $n_i$ as the child node and $n_j$ as the\nparent node. The ith row of E encodes all parents of $n_i$, denoted as $Pa(n_i)$. The jth column encodes\nall children of $n_j$, denoted as $Ch(n_j)$. We define leaf nodes as nodes without children, while root\nnodes are those without parents. The level of node $n_i$ is defined as the length of the longest directed\npath from any leaf node to $n_i$ (Bezek, 2016):\nlevel$(n_i) = \\max_{n_j \\in Ch(n_i)} \\text{level}(n_j) + 1$, if $n_i$ is not a leaf node, (1)\nlevel$(n_i) = 0$, if $n_i$ is a leaf node. (2)\nDAG representation of circuits The And-Inverter Graph (AIG) is a powerful representation of\nlogic circuits. In an AIG, leaf nodes carry input signals to the circuit, while root nodes produce the\ncircuit's output signals. Intermediate nodes are computing units that perform logical conjunction\n(AND operation) on two binary inputs $a, b \\in \\{0,1\\}$, outputting $a \\cdot b$. Edge $e_{ij}$ from node $n_i$ to\n$n_j$ indicates that the output of $n_i$ serves as an input to $n_j$. Edges can optionally perform logical\nnegation (NOT operation) on the signal they carry. This representation naturally forms a DAG\nstructure. The structure of an AIG uniquely defines its functionality, which can be represented as\na canonical truth table mapping all possible combinations of input signals to their corresponding\noutput signals. Figure 2(a) illustrates a simple AIG and its truth table.\nThis representation is particularly useful because any combinational logic circuits can be expressed\nusing only AND and NOT operations, making AIG a universal and efficient model for circuit anal-\nsis and synthesis. In this work, we generate AIGs that can realize a given truth table functionality.\nFor node types, we have three category ($k_x = 3$): input gates, AND gates and output gates. For\nedges, we define three types ($k_e = 3$): non-existing edges representing the absence of a connection,\nnormal edges for direct connections, and negation edges that perform a logical NOT operation.\nDAG representation of molecules We adopt the approach of Jin et al. (2018) to convert molecules\ninto junction trees. This method first extracts valid chemical substructures, e.g. rings, bonds and in-\ndividual atoms, from the training set. The vocabulary size is the number of node types $k_x$. Each\nmolecule is then converted into a tree structure representing the scaffold of substructures. We trans-\nform this tree into a DAG by designating the tree root as the DAG root and converting tree edges"}, {"title": "2.2 SEMI-AUTOREGRESSIVE DIFFUSION FOR DAG", "content": "In this section, we introduce the proposed semi-autoregressive diffusion generation for DAG. This\napproach considers the hierarchical nature of DAGs while maintaining a complete graph structure\nat every step of the sampling process. The training pipeline of SeaDAG is illustrated in Figure 3.\nDiscrete graph denoising diffusion The forward diffusion process independently adds noise to\neach node $x_i$ and each edge $e_{ij}$ from timestep 0 to T, where T is the maximum diffusion step.\nThe forward process is defined using transition matrices: $[Q_X]_{ij} = q(x_t = e_j | x_{t-1} = e_i)$ and\n$[Q_E]_{ij} = q(e_t = e_j | e_{t-1} = e_i)$. Here, $e_z$ denotes a one-hot vector with 1 in its ith position.\nConsequently, the node and edge types at time t can be sampled from the following distributions:\n$q(x_t|x^{t-1}_i) = Q_X x^{t-1}_i$, $q(e_t|e^{t-1}_{ij}) = Q_E e^{t-1}_{ij}$, (3)\n$q(x_i^t|x_i^0) = Q_X x_i^0$, $q(e_{ij}^t|e_{ij}^0) = Q_E e_{ij}^0$, (4)\nwhere $Q_X = Q_X...Q_X$ and $Q_E = Q_E......Q_E$. Following Vignac et al. (2023), we use marginal\ndistribution to define transition matrices: $Q_X = a_t I + (1 - a_t) \\mathbb{1}m_X$, $Q_E = a_t I + (1 - a_t) \\mathbb{1}m_E$,\nwhere $m_X, m_E$ are marginal distributions of node and edge types. We use cosine noise schedule\nfollowing Nichol & Dhariwal (2021).\nSemi-Autoregressive diffusion To leverage the inherent layer-wise structure of DAGs and the\ndependency-modeling advantages of sequential generation, we introduce different diffusion speed\nfor different layers. We define the timestep $t \\in [0,T]$ as the global timestep. We denote\nthe normalized node level as $l_i = \\text{level}(n_i)/\\max_{n_j \\in V} \\text{level}(n_j)$. We then design a function\n$T : [0, T] \\times [0, 1] \\rightarrow [0, T]$ that maps the global timestep t and normalized level $l_i$ to a node-specific\nlocal timestep $\\tau_i = T(t,l_i)$ for node $n_i$, or an edge-specific local timestep $\\tau_{ij} = T(t,l_i)$ for edge\n$e_{ij}$. By designing T such that $T(t,l_i) >= T(t,l_j)$ when $l_i > l_j$, we assign larger timesteps to\nnodes at higher levels and edges pointing to higher levels. This configuration results in a bottom-up\ngeneration process, where layers at the bottom of the DAG are denoised at a higher speed. Con-\nversely, we can achieve top-down generation by reversing this relationship. In our experiments, we\nimplement T as:\n$T$", "latex": ["q(x_t|x^{t-1}_i) = Q_X x^{t-1}_i", "q(e_t|e^{t-1}_{ij}) = Q_E e^{t-1}_{ij}", "q(x_i^t|x_i^0) = Q_X x_i^0", "q(e_{ij}^t|e_{ij}^0) = Q_E e_{ij}^0", "Q_X = a_t I + (1 - a_t) \\mathbb{1}m_X", "Q_E = a_t I + (1 - a_t) \\mathbb{1}m_E"]}, {"title": "2.3 CONDITIONAL GENERATION", "content": "Our approach incorporates explicit condition learning by employing a property decoder to compute\na condition loss during training. This section details the implementation of these key components.\n2.3.1 CONDITION LOSS\nTo incorporate graph properties as generation conditions, we extend our network to accept an addi-\ntional condition input cond: $f_\\theta(G_t, \\text{cond}) = (p_\\theta(X), p_\\theta(E))$. For AIG generation from truth tables,\nwe concatenate truth table vectors with other node features. Each column of binary values in the"}, {"title": "3 RELATED WORKS", "content": "Different diffusion mechanisms have been proposed for graph generation. One-shot generation mod-\nels apply noise addition and denoising processes across the entire graph structure simultaneously,\npredicting all nodes and edges at each timestep (Yan et al., 2024; Vignac et al., 2023). In contrast,\nAR diffusion models generate the graph sequentially, either producing one part of the graph at each\ndiffusion step (Kong et al., 2023) or having a separate denoising process for each part (Zhao et al.,\n2024). These AR approaches offer advantages in generation efficiency and are better at modeling\ndependencies between nodes and edges by allowing each step to condition on previously generated\nparts. (Kong et al., 2023). However, they face challenges such as sensitivity to node ordering (You\net al., 2018b) and the production of only partial, incomplete graph structures during the sampling\nprocess. This limitation precludes operations that require the entire graph structure, such as validity\nchecks and property guidance (Vignac et al., 2023; Yu et al., 2023). Latent diffusion models have\nalso been explored for graph generation (Zhou et al., 2024), with studies indicating their potential to\nenhance performance in 3D graph generation tasks (You et al., 2023)."}, {"title": "4 EXPERIMENTS", "content": "In our experiments, we focus on evaluating two key aspects of the methods: (1) Conditional gen-\neration: We assess whether the methods can generate graphs that satisfy the given condition. (2)\nGraph quality: We evaluate whether the generated graphs are realistic and resemble real graphs in\ntheir distribution.\n4.1 DATA AND BASELINES\nAIG dataset We generate a dataset of random AIGs with 8 inputs, 2 outputs and a maximum of 32\ngates. For each AIG, we compute its corresponding truth table. The dataset comprises 12950 AIGs\nin the training set, 1850 in the validation set, and 256 in the test set. The generalization experiments\non AIGs with more than 8 input and 2 outputs are available in Appendix A.2.\nMolecule dataset For molecule generation, we evaluate SeaDAG on the standard QM9 benchmark\n(Wu et al., 2017), which contains molecules with up to 9 heavy atoms. We adopt the standard dataset\nsplit, utilizing 100k graphs for training and 20k for validation. We generate 10k molecules for\nevaluation. We further conduct a molecule optimization experiment using the ZINC dataset (Irwin\net al., 2012), which has 219k molecules in training set and 24k molecules for validation. More\ndataset statistics are provided in Appendix E.1.\nBaselines We evaluate SeaDAG against several recent graph generation diffusion models as well\nas state-of-the-art molecule generation models. For one-shot graph diffusion baselines, we compare\nSeaDAG with SwinGNN (Yan et al., 2024), EDP-GNN (Niu et al., 2020), GDSS (Jo et al., 2022)\nand DiGress (Vignac et al., 2023). For AR graph diffusion baselines, we compare SeaDAG with\nPard (Zhao et al., 2024) and GRAPHARM (Kong et al., 2023). For graph diffusion baselines that\noperate in latent space, we compare SeaDAG with LDM-3DG (You et al., 2023) and EDM (Hooge-\nboom et al., 2022). For molecule generation baselines, we compare SeaDAG with GraphAF (Shi"}, {"title": "4.2 CONDITIONAL GENERATION", "content": "Conditional AIG generation Ta-\nble 1 presents the evaluation results\nfor AIG generation. To achieve con-\nditional generation, we extend base-\nline models to accept truth tables as\nadditional input. We report two met-\nrics: Validity, which means the per-\ncentage of generated AIGs that are\nstructurally valid, and function Ac-"}, {"title": "4.3 GRAPH QUALITY EVALUATION", "content": "In addition to evaluating the methods' ability to meet the conditions, we also assess the realism of\nthe generated graphs and how closely they resemble the real graph distribution."}, {"title": "4.4 MOLECULE OPTIMIZATION VIA CONDITIONAL GENERATION", "content": "Table 4: Molecule Optimization on ZINC. We report the best property scores achieved by each\nmethod. While other methods use optimization algorithms, SeaDAG is solely trained for conditional\ngeneration, yet it produces molecules with properties comparable to those of other methods.\nWe demonstrate a practical application of our conditional DAG generation in the domain of molecule\noptimization. On the ZINC dataset, we train SeaDAG to conditionally generate molecules based on\ntwo properties: penalized logP and Quantitative Estimate of Druglikeness (QED), which are two\ncommon target properties for molecule optimization (Popova et al., 2019). We then sample from\nSeaDAG with target property scores. Detailed implementations could be found in Appendix E.6.\nWithout using explicit optimization techniques, SeaDAG achieves top property scores comparable\nto several optimization-based baselines (Guimaraes et al., 2017; Jin et al., 2018; You et al., 2018a)\nas shown in Table 4. Notably, for the penalized logP property, SeaDAG attains scores surpassing\nthe highest values observed in the training set. This suggests SeaDAG's capacity to extrapolate\nbeyond the training distribution by effectively learning the intrinsic relationships between molecular\nstructure and associated properties."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced SeaDAG, a semi-autoregressive diffusion model for conditional DAG\ngeneration. Our approach demonstrates significant improvements in generating graphs that are real-\nistic and realize given conditions. Future research could focus on enhancing the method's efficiency\nto match that of fully autoregressive models."}, {"title": "A MORE EXPERIMENT RESULTS", "content": "A.1 MCTS-BASED REFINEMENT FOR CONDITION ALIGNMENT\nWe implement an MCTS-based post-processing step to enhance the condition properties of the\nDAGs generated by the diffusion model.\nState representation Each state is a DAG structure G = (X, \u0395).\nAction space and transition model The action space is defined as a set of random graph edits.\nAn action, i.e. a graph edit, for an AIG or a molecule junction tree is defined as follows:\n\u2022 To sample an action for an AIG, we first randomly sample a gate from the AND gates and\noutput gates. We then change the input gates (i.e. the children) of the chosen gate to nodes\nrandomly sampled from possible candidate input gates. Specifically, an AND gate has two\ninputs and a output gate only requires one input. The candidate gates are the set of gate\nnodes with lower levels. Finally, for the new edges from the new inputs to the chosen gate,\nwe determine their types by sampling from normal type and negation type.\n\u2022 To sample an action for a molecule junction tree, we first randomly select a node from the\njunction tree. Then, with a probability of 0.5, we either modify the node's type or alter\nits parent node. In the case of type modification, we randomly assign a new type to the\nselected node. In the case of changing parent, we randomly select a new parent from the\nnodes at higher levels in the tree.\nApplying an action to a state is to modify the DAG structure using the edit defined by the action\nand result in a new DAG structure. Our design of the action space ensures that the resulting DAG is\nalways valid.\nReward function We employ the property decoder & to compute the reward function for a state.\n\u2022 For AIGs, we aim to maximize the function accuracy. Therefore, we use the truth table\ndecoder & to decode the output signals of the AIG and compute the accuracy as reward.\n\u2022 For molecules, we aim to minimize the MAE between the molecule property and the target\nproperty. Therefore, we employ & to predict the molecule property from the junction tree\nand compute the negative of MAE loss as reward.\nMCTS Algorithm Structure Since the action space, namely the set of possible edits to a DAG,\nis finite but is still very large, we employ the progressive widening strategy (Coulom, 2007; Chaslot\net al., 2008) to balance adding new children and selecting among existing children. We employ UCB\nselection strategy (Auer et al., 2002) to select the best child. In the simulation phase, we employ a\nrandom action sampling strategy until reaching the predefined maximum depth limit. Upon reaching\nthis limit, we evaluate the reward of the terminal state and back-propagated it through the tree. We\nconduct 500 such simulations for each decision point. After these simulations, we select the best\nchild node as the next state. This process is repeated for 50 steps for each DAG.\nAfter the MCTS refinement, we evaluate the resulting DAG structure. If it performs worse than the\noriginal DAG, we reject the refinement and retain the original structure.\nWe evaluate the performance of our MCTS-based DAG structure refinement in Table 5. MCTS\nis applied to 256 generated AIGs and 600 generated molecule junction trees for each of the five"}, {"title": "A.2 MODEL GENERALIZATION", "content": "We evaluate SeaDAG's generalization capacity by testing its ability to generate AIGs with varying\nnumbers of input and output gates, as well as diverse total gate counts. Our model uses truth table\ncolumns as node-level features, with a fixed length parameter of 256 (28) rows corresponding to\nthe 8 input gates in the training AIGs. To accommodate AIGs with $N_{\\text{input}} > 8$, we randomly\nsample 256 rows from the $2^{N_{\\text{input}}}$ rows of the full truth table. For AIGs with $N_{\\text{input}} < 8$, we pad\nthe truth table to 256 rows by randomly duplicating $256 - 2^{N_{\\text{input}}}$ rows. Then the truth table can\nbe concatenated to node features and the AIG can be generated in the same way. Figure 7 plots\nthe function accuracy of SeaDAG on AIGs with different number of input gates, output gates and\ntotal gates. Although it is only trained on AIGs with 8 input gates and 2 output gates, SeaDAG\ndemonstrates robust performance on AIGs with very different configurations from the training set.\nMeanwhile, it maintains stable performance across a diverse range of AIG sizes."}, {"title": "A.3 ABLATION STUDY", "content": "We conduct ablation study on the two key elements in our method: the condition loss and the semi-\nautoregressive diffusion scheme.\nAs shown in Table 6, incorporating condition loss during training significantly improves the func-\ntion accuracy for AIG generation. This suggests that merely providing the condition as an additional\ninput to the model may be insufficient. The condition loss appears to enhance the model's ability\nto learn the relationship between DAG structure and its property conditions. Conversely, the model\nwithout semi-autoregressive generation performs poorly in terms of graph validity (note that we cor-\nrect invalid AIG graphs when calculating Accuracy). This finding further supports our argument in\nSection 4.3 that semi-autoregressive diffusion helps the model learn structural features and depen-\ndencies within DAGs, leading to the generation of higher quality and more realistic DAG structures.\nTable 7 presents the ablation study results for molecule generation. Consistent with our previous\nfindings, the model trained without semi-autoregressive generation shows significant performance"}, {"title": "B SEADAG PARAMETERIZATION AND TRAINING", "content": "B.1 DENOISING NETWORK IMPLEMENTATION\nWe extend the model employ in Dwivedi & Bresson (2020) and Vignac et al. (2023) to implement\nour denoising network fo. First, we extract the graph features of $G_t = (X_t, E_t)$ following Vignac\net al. (2023), resulting in node level feature $F_X \\in \\mathbb{R}^{n \\times d_x}$, edge level feature $F_e \\in \\mathbb{R}^{n \\times n \\times d_e}$ and\ngraph level feature $y \\in \\mathbb{R}^{d_y}$. For clarity of presentation, we omit the timestep superscript t for\n$F_X, F_e, y$. These features encode (1) the node and edge types and (2) the graph structure features of\n$G_t$. We refer the reader to Vignac et al. (2023) for details on the structure features. Global timestep\nt is encoded in graph level feature y. Node level li and node-specific local timestep $\\tau_i^t$ is encoded in\nnode level feature $F_X$.\nWe then incorporate the condition information into graph features.\nTruth table condition as node features Each column in truth tables are a series of {0, 1} values\nof one input gate or output gate. The signal values for AND gates are unknown in the condition and\ntherefore we first pad 0 for the values for AND gate. For AIGs with 8 input gates, the padded truth\ntable could be represented as $T \\in \\{0,1\\}^{n \\times 2^8}$. To compress this representation, we convert each\n8-bit sequence in the last dimension of T to its corresponding integer value. These integer values\nare then normalized by dividing by 256, resulting in a compressed representation with values in the\nrange [0, 1]. We concatenate the compressed truth table T with node level features $F_X$. With a slight\nabuse of notation, we continue to denote these augmented node features as $F_X$.\nProperty condition as graph level features We concatenate the molecule property condition with\nthe graph level feature y. With a slight abuse of notation, we continue to denote the resulting graph\nlevel feature as y.\nModel architecture After the condition information is incorporated into graph features\n($F_X, F_e, y$), we process the them through several graph transformer layers to update the features."}, {"title": "B.2 TRAINING SEADAG", "content": "The training algorithm for SeaDAG is detailed in Algorithm 1. Note that for AIG generation, the\ntruth table implicitly specifies the numbers of input and output gates, thereby partially determining\nthe node types. For example, we can designate the first $N_{\\text{input}}$ nodes as input gates and the last\n$N_{\\text{output}}$ nodes as output gates, with the remaining nodes naturally serving as AND gates. $N_{\\text{input}}$ and\n$N_{\\text{output}}$ are the numbers of input and output gates that can be inferred from the truth table. Given\nthis predetermined information, the diffusion of node types becomes unnecessary for AIG graphs.\nInstead, the model only needs to generate the edge connections. To implement this, we omit the\naddition of noise to node types during the forward process and maintain fixed node types in the\nbackward process. Consequently, we exclude the node cross-entropy loss from the model's loss\nfunction."}, {"title": "B.3 PROPERTY DECODER FOR CONDITION LOSS", "content": "In this section, we introduce the implementation of property decoder $. We denote the probability\ndistribution of node types and edge types predicted by the network fo as $p_\\theta(X) \\in \\mathbb{R}^{n \\times k_\\alpha}$, $p_\\theta(E) \\in\n\\mathbb{R}^{n \\times k_e}$.\nAIG function decoder As discussed in the previous section, the node types are fixed in AIG\ngeneration and we only need E to decode the AIG structure. Given the graph structure of an AIG,\nthe logic function represented by an AIG can be directly determined and the output signals are\nreadily computable. However, to ensure that this operation is differentiable and can be incorporated\ninto the training process, several aspects require special consideration:\n\u2022 Input selection. To determine the inputs (children) for each gate, we employ a softmax\noperation across all candidate input gates. Candidate gates are those with levels lower than\nthe current gate. For AND gates, we select two inputs, while for output gates, we select\none input.\n\u2022 Edge type selection. The type of each edge (normal or NOT) is determined by computing\nan edge type score. This score is calculated as $\\tanh(p_\\theta(E)_{ij1} - p_\\theta(E)_{ij2})$, where $p_\\theta(E)$\nrepresents the predicted edge type distribution from nodes i to j. A positive score indicates\na higher likelihood of a normal edge, while a negative score suggests a higher probability\nof a NOT edge.\nUtilizing the aforementioned differentiable operations, we can compute the output signals as contin-\nuous values. The decoded output signals are then compared against the ground truth output signals\nspecified in the condition truth table to compute the condition loss using BCE. Note that the loss\ncomputation only involves the output signal portion, as the input signals, which are all possible\ncombinations of input values, are the same for every AIG."}, {"title": "C PROOFS", "content": "Proof of Equation 10 During inference", "from\n$G_t$": "n$p_\\theta(G_{t-1"}, "G_t) = \\prod_{1<i<n} p_\\theta(x_i^{t-1}|G_t) \\prod_{1<i,j<n} p_\\theta(e_{ij}^{t-1}|G_t)$. (24)\nTo compute $p_\\theta(x_i^{t-1}|G_t)$, we marginalize over the network predictions:\n$p_\\theta(x_i^{t-1}|G_t) = \\sum_{k\\in\\{1...k_x\\}} p_\\theta(x_i^{t-1} = e_k, G_t) p_\\theta(x_i = e_k|G_t)$, (25)\nwhere $p_\\theta(x_i = e_k|G_t)$ is the network prediction and $p_\\theta(x_i^{t-1} = e_k, G_t)$ is computed as follows:\n$p_\\theta(x_i^{t-1} = e_k, G_t) = \\frac{p_\\theta(x_i^{t-1} = e_k | x_i^t) p_\\theta(x_i^t)}{q(x_i^t | x_i^{t-1})}= \\frac{p_\\theta(x_i^{t-1} = e_k | x_i^t)}{q(x_i^t | x_i^{t-1})} p_\\theta(x_i^t)$. (26)\nq(x_ix_i^t)\nPoo(x_ix_i^t)=p_o(x_ix_i^t), the node types in G\u00b2 are the same. We sample\nto be\n+X\u2081\n\u03a3\u0395 ... \u03a3\u03a3 \u03a0\u03b1\u03c0\nThe\n\u03a3.\u03a3\u03a3 ... \u03a3\u03a0\nT\nT\n+19\nProof of model equivariance In this section, we prove that the network we use is equivariant to\nnode permutations. For any node permutation \u03c3 : [n"], "following": "n\u2022 Equivariant feature extraction: the graph features are either permutation equivariant (node\nlevel and edge level features) or permutation invariant (graph level features). If the features\nof Gt are (Fx, Fe"}