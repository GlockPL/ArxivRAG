{"title": "MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents", "authors": ["Yun Xing", "Nhat Chung", "Jie Zhang", "Yue Cao", "Ivor Tsang", "Yang Liu", "Lei Ma", "Qing Guo"], "abstract": "Physical adversarial attacks in driving scenarios can expose critical vulnerabilities in visual perception models. However, developing such attacks remains challenging due to diverse real-world backgrounds and the requirement for maintaining visual naturality. Building upon this challenge, we reformulate physical adversarial attacks as a one-shot patch-generation problem. Our approach generates adversarial patches through a deep generative model that considers the specific scene context, enabling direct physical deployment in matching environments. The primary challenge lies in simultaneously achieving two objectives: generating adversarial patches that effectively mislead object detection systems while determining contextually appropriate placement within the scene. We propose MAGIC (Mastering Physical Adversarial Generation In Context), a novel framework powered by multi-modal LLM agents to address these challenges. MAGIC automatically understands scene context and orchestrates adversarial patch generation through the synergistic interaction of language and vision capabilities. MAGIC orchestrates three specialized LLM agents: The adv-patch generation agent (GAgent) masters the creation of deceptive patches through strategic prompt engineering for text-to-image models. The adv-patch deployment agent (DAgent) ensures contextual coherence by determining optimal placement strategies based on scene understanding. The self-examination agent (EAgent) completes this trilogy by providing critical oversight and iterative refinement of both processes. We validate our method on both digital and physical level, i.e., nuImage and manually captured real scenes, where both statistical and visual results prove that our MAGIC is powerful and effectively for attacking wide-used object detection systems.", "sections": [{"title": "1. Introduction", "content": "Autonomous driving (AD) systems have received much attention from the research community thanks to tremendous advancements in machine learning [2, 4, 75], especially those"}, {"title": "2. Related Works", "content": "Physical Adversarial Attacks. Physical adversarial attacks have been designed to stress-test perception systems used for AD in the real world [57], particularly against classification [1, 3, 9, 12, 76], detection [5, 50, 52, 66, 68, 72], and among other applications like [6, 8, 30, 42], where attacks have been carefully designed to make selected targets disappeared or mis-predicted when using attacked models [4], revealing the various threats to their deployment. Research of physical adversarial attacks, such as attack effectiveness [1, 52, 63] and attack stealthiness [33, 51, 56], has been crucial to inform the development of safety-critical systems. Various adversarial attacks have been realised in the physical world as patches [9, 10, 22, 34, 36, 62, 63, 71], to make deployed predictors produce erroneous outputs under conditions that are easy to replicate and manufacture. Motivated by the potential of naturalistic attack [44] and its lack of comprehensive analyses in the physical world, we aim to setup a naturalistic benchmark against object detectors with multi-agent reasoning.\nAdversarial Attack Design. Adversarial attacks can be designed to mislead neural network-based models in various manners [17-19, 47, 65] and expose their vulnerabilities. When an attack is successful, model predictions would not align with human judgement and values on the same inputs, bearing potential harm, safety and utility concerns in systems that rely on them [57]. Conventionally, existing works add subtle perturbations that are not noticeable by humans on an entire image input [17-19], put adversarial stickers to the scene [14] or place small patches in the attack scenarios [1, 9, 21, 34, 55, 67]. It has been hypothesised that adversarial attacks are caused by non-robust, target-specific features that are incomprehensible to humans, rather than by inherent model issues [20, 27, 44]. In fact, Hendrycks et al. [20] found that even unedited images in the wild can pose adversarial effects. Hence, by leveraging diffusion modeling on image distributions, natural adversarial examples [44] can be produced to evaluate object detectors. However, generating these samples involves manual prompting and review to produce a static dataset that does not consider AD physical contexts [44], thus they are not directly relevant to physical-world traffic scenarios. Different from prior works, we investigate how natural adversarial patterns can be automatically produced and reviewed on-the-fly."}, {"title": "Multimodal LLM Reasoning", "content": "Inspired by the emergent capabilities of LLMs on a number of key techniques, such as zero-shot prompting [61], in-context reasoning [54], multimodal reasoning [32, 35, 70], and self-feedback [24], autonomous agents have enjoyed significant advances to mimic human interactions in the world [11, 23, 25, 38, 69, 74]. In fact, while language-based agents [45, 54] pioneered such interactions in text-based contexts, multi-modal embodied systems [23, 25, 38, 41, 69, 74] have become increasingly relevant with real-life scenarios by building on other modalities, such as image [16, 59, 69], video [38, 48] and audio [23]. Interestingly, [69] even incorporated LLM reasoning to produce fine-grained diffusion results, where self-feedback could be relevant to improving the results. Different from prior works, we blend multimodal LLM agent reasoning with diffusion to mimic human adversaries in the real world. Thus, we aim to develop an adaptive strategy to generating natural adversarial patterns on the fly in a scalable manner. With LLM reasoning and feedback, our attack framework is more comprehensive and controllable in designing physical attacks against visual detectors in AD."}, {"title": "3. Revisit Natural Denoising Diffusion Attack", "content": "In this section, we revisit the Natural Denoising Diffusion Attack (NDDA) proposed in [44], which is regarded as the baseline method in our paper."}, {"title": "3.1. Formulation of the Baseline (NDDA)", "content": "The Natural Denoising Diffusion Attack (NDDA) uses a denoising diffusion probabilistic model (DDPM) to generate adversarial examples by intentionally guiding the image generation process through text prompts. In NDDA, prompts are crafted to exploit model vulnerabilities by omitting certain robust visual features such as shape, color, and text essential for human recognition, while retaining enough non-robust features to be detected by DNNs. For example, consider a stop sign as the target object. Normally, the recognizable features of a stop sign include its octagonal shape, red color, and the text \"STOP.\" In an NDDA attack, the text prompt might instruct the model to generate an image of a \"blue, square stop sign with the word 'HELLO' on it\". Despite lacking the key attributes that make it recognizable as a stop sign to humans, the generated image can still be classified as a stop sign by DNN-based object detectors due to the presence of non-robust features that the model recognizes but humans do not.\nFormally, let x be the input image and T the prompt that guides the diffusion model G to produce an adversarial example x' = G(x, T). The prompt T is designed to introduce non-robust perturbations that maintain high misclassification rates on DNNs without relying on the features humans typically use to recognize objects. This approach enables the"}, {"title": "3.2. Limitations of NDDA on Physical Deployment", "content": "Attack Effectiveness under Different Real-world Environment. One of the primary limitations of NDDA when applied as a physical attack is its dependency on the controlled conditions under which the adversarial examples were generated. In real-world environments, factors such as lighting variations, occlusions, and background complexity can significantly impact the effectiveness of the NDDA-generated patches. Since the adversarial patches created via NDDA are detached from any specific real-world context, their transferability may degrade in dynamic and unpredictable settings. This limitation highlights the need for context-aware approaches that adapt the adversarial patch to specific environmental conditions.\nVisual Naturality in the Attack Scene. Another challenge of NDDA in physical deployment is achieving visual naturality in the attack scene. The NDDA method, based purely on prompt-driven diffusion models, generates adversarial patches without considering how these patches integrate visually within the natural environment. This lack of visual coherence may reduce the stealthiness of the attack, as patches that appear visually unnatural can be more easily noticed by human observers. In applications where maintaining visual naturality is critical, NDDA's approach may fall short, necessitating advancements that enable the adversarial patches to blend seamlessly into the real-world context."}, {"title": "4. Methodology: MAGIC", "content": null}, {"title": "4.1. Overview", "content": "As illustrated in Fig. 1, we aim to deploy a printed adversarial patch in the target scene, which could make an object detector misclassify the patch. For example, the object detector misclassifies a normal patch as a stop sign, which will cause high risks for visual perception-based systems. A straightforward solution is to optimize a patch on wide pre-collected scenes and deploy it into the target scene directly, or generate the patch according to the inherent properties of the stop sign like the NDDA method. However, such a strategy cannot well leverage the context information with some known scene data.\nIn this work, we formulate such a physical attack as a one-shot patch generation problem: give an image I captured from a scene and a text prompt T indicating the main objective of the attack, our method aims to generate an adversarial patch P and the corresponding deployment strategy to implement in the physical world automatically. The generation process should be scene-aware and objective-oriented to make the adv-patch mislead the targeted object detector. Such a problem can hardly be achieved by training a network"}, {"title": "4.2. Adv-Patch Generation Agent (GAgent)", "content": "The GAgent is a multi-modal LLM agent with both visual and language capabilities, responsible for generating adversarial patches based on scene context. The GAgent's process includes four main steps:\n\u2022 Initial Patch Generation: The initial adversarial patch P\u2080 is generated using the Natural Denoising Diffusion Attack (NDDA) method. This patch serves as a foundational starting point for subsequent iterative enhancements.\n\u2022 Scene Analysis and Description: Given an input scene image I, GAgent generates a textual description T = GA(I) to capture key contextual details. This description enables the creation of patches that blend naturally into the scene.\n\u2022 Prompt Engineering: Using the scene description T and the previously generated patch P\u1d62\u208b\u2081, GAgent constructs"}, {"title": "4.3. Adv-Patch Deployment Agent (DAgent)", "content": "The DAgent is specialized solely in the deployment of the generated adversarial patch P\u1d62 within the scene. Taking as input the patch P\u1d62 and the scene image I, DAgent determines the optimal localization based on both adversarial objectives and visual coherence. This process is guided by segmentation and instruction-based analysis, ensuring that the patch's deployment aligns naturally within the scene context.\n\u2022 Guided Placement Strategy: DAgent utilizes segmentation maps and contextual instructions derived from the scene image I to inform its placement decisions. These guides help the agent to identify suitable regions within the scene, focusing on areas where the patch can achieve high adversarial impact while blending seamlessly with the background.\n\u2022 Naturalness Constraints: To maintain the visual naturality of the scene, DAgent applies specific constraints that prevent the patch from appearing out of place or drawing unnecessary attention. These constraints ensure that the patch integrates smoothly with its surroundings and remains visually inconspicuous, thus enhancing the attack's stealth.\n\u2022 Deployment and Evaluation: Once the optimal localization l* is determined based on the segmentation and constraints, DAgent places the patch P\u1d62 at l* in the scene image I, producing a modified image I'."}, {"title": "4.4. Self-Examination Agent (EAgent)", "content": "The EAgent is the final component in the MAGIC framework, responsible for assessing both the effectiveness of the adversarial patch in influencing targeted object detection and its natural integration within the scene. EAgent ensures the patch meets both attack success and visual naturality requirements through an iterative evaluation process.\n\u2022 Targeted Detection Evaluation: EAgent starts by evaluating the modified image I', where DAgent has placed the adversarial patch P\u1d62 at the optimal localization l*. EAgent checks if the patch successfully triggers a false detection of the specified target object in the intended location. This is assessed using the detection model D to verify if it detects the target object class:\n\u0394S\u209c\u2090\u1d63\u2089\u2091\u209c = D\u209c\u2090\u1d63\u2089\u2091\u209c(I'),\nwhere D\u209c\u2090\u1d63\u2089\u2091\u209c() represents the detection confidence for the target object class. This step confirms whether the patch induces the desired detection response.\n\u2022 Naturalness Evaluation Using VLM: EAgent uses a Vision-Language Model (VLM) to assess the patch's visual coherence within the scene. Instructions are designed to guide the VLM in evaluating whether the patch appears contextually and visually consistent. The VLM generates a naturality score N(P\u1d62, I') based on these instructions:\nN(P\u1d62, \u0399') \u2265 \u03b4,\nwhere d is a threshold for acceptable naturality. This ensures the patch blends smoothly within the scene without appearing out of place.\n\u2022 Success and Naturalness Threshold Check: If both \u0394S\u209c\u2090\u1d63\u2089\u2091\u209c and N(P\u1d62, I') meet their respective thresholds 0 and 8, indicating successful misclassification and visual naturality, the patch is considered effective. If either criterion is not met, refinement is necessary.\n\u2022 Iterative Refinement: If the patch does not meet either the detection or naturalness criteria, EAgent initiates an iterative refinement process. This may involve adjusting the prompt T\u1d62 generated by GAgent and modifying the localization l* set by DAgent to optimize both adversarial effectiveness and natural integration. Refinement continues until both thresholds are achieved: updateT\u1d62 or l* to improve P\u1d62, while \u0394S\u209c\u2090\u1d63\u2089\u2091\u209c < 0 and N(P\u1d62, I') < \u03b4.\n\u2022 Final Verification and Output: Once the patch satisfies both detection and naturalness requirements, the final modified image I' is saved for further testing or deployment. This ensures the patch effectively meets the targeted attack goal and integrates naturally within the scene."}, {"title": "4.5. Extension to Physical World", "content": "The final step in the MAGIC framework is to ensure that the adversarial patch generated and refined through GAgent, DAgent, and EAgent can be effectively deployed in physical environments. This phase involves adapting the patch from digital to physical formats while preserving its adversarial properties. This transformation uses a printing process that retains the color, texture, and spatial features necessary for maintaining adversarial integrity in real-world settings. The goal is to ensure the physical patch closely matches the digital version in both appearance and impact on the target detection system. We outline our analyses in the next section."}, {"title": "5. Experiments", "content": null}, {"title": "5.1. Experimental Setup", "content": "Digital Environments. We first verify our pipeline on digital level, we adopt the real-world driving images from nuImage [15] dataset as the digital environments for evaluation.\nnuImage composed of 93,000 images that captured by 6 car-mounting cameras of different car view, i.e., back, back left, back right, front, front left, front right. We select one image to represent each camera view as an initial study, which are depicted in Fig. 2.\nPhysical Environments. We further verify our proposed MAGIC framework in physical environments. In specific, we test in two different physical scenarios, as illustrated in Fig. 3, one is a real-world busy bus stop scene with heavy traffic and another is a regular road next to college with few pedestrians. More physical deployment experiments can be found in supplemental materials.\nBaselines. Since to the NDDA [44] is the very initial study of the natural denoising diffusion attacking, we validate our method by setting NDDA as the baseline. In specific, due to NDDA does not consider the patch deployment, we combine NDDA with two different deployment strategies as the baselines. We first deploy the patches from NDDA dataset with random location in the given environment image, and we simply name it as \u201cNDDA Rand\u201d. With random deploying location, this baseline can utilized to compare the influence of the environment introduction. For the second baseline, we keep using the patches from NDDA but adopt our DAgent as the deployment strategy. Such a baseline will effectively explain the performance of the patch effectiveness, and we name it as \u201cNDDA+DAgent\".\nGenerator & Detectors. To better compare to NDDA with fairness, we follow their evaluation and apply Stable Diffusion v2 [43] as the text-to-image generation model. As for detectors, we adopt the same two commonly used detectors, i.e., YOLOv5 [29] and DETR [37]. Moreover, we empirically found that both YOLOv5 and DETR are out-of-date and are easily to be disturbed, thus we further adopt YOLOv10 [53] which exhibits better stability for attacks as our main evaluation target. For all three detectors, we use the API from ultralytics to keep the evaluation consistency.\nMetrics. We evaluate the effectiveness of the patch with Attack Success Rate (ASR). ASR measures the percentage of successful attacks that deceive the target object detector, indicating the attack's effectiveness. It has a range from 0 to 100, where higher values signifying greater success."}, {"title": "5.2. Digital Comparative Results", "content": "The Attack Effectiveness. In order to demonstrate our MAGIC can generate physically deployable effective patch, we first conduct experiments on digital level and compare our patch against the patch from NDDA dataset.\nSetting. In the NDDA dataset, there are 50 patches for each robust feature removed text prompt. To keep the fairness of comparison, we follow the setup of NDDA dataset and generate 50 patches in total for a given environment images. At the same time, there are several different text prompts for a type of robust feature removed text prompt"}, {"title": "5.3. Physical Comparative Results", "content": "For the second part of the main experiments, we conduct real-world physical patch deployment and evaluation to verify the attack effectiveness of our porposed MAGIC framework.\nSetting. We conduct two sets of physical environments, \u2460 one for a bus stop area where there are heavy traffics and \u2461 one for a regular road next to a college with some pedestrians on the sidewalk, i.e., the captured scenes in Fig. 3. In order to verify the flexibility of MAGIC generation, we select the bus stop scene and generate two patches for it. As for the regular road scene, only one patch is generated to verify the contextual deployment capability of our MAGIC. To physically test our MAGIC, we first feed the captured scene image into the framework and get the optimized patch and the corresponding deployment suggestions, then we print the patches out and physically paste the patch into the specified location in the scene. Finally, we take pictures after the deployment process and evaluate with the detectors.\nResults. As visualized in Fig. 3, we show the generated patch and prompt, the deployment suggestion and the detection result. First, checking the general lower detection confidences, we notice that the physical attack is more harder to realize due to the environment variance. Even so, our MAGIC still generates an extremely effective attack patch, i.e., the second patch for scene \u2461. Second, comparing the detection results of the three detectors, we observe that YOLOv5 and RT-DETR are more prone being attacked while YOLOv10 is more robust to the attacks. However, all the generated patches are proved to be physically attack effective for YOLOv10 which further verified the physical effectiveness of our MAGIC. Third, by comparing the two scenes' results, we see that our MAGIC is capable of giving out contextually appropriate patch and location for real-world deployment. In summary, we can conclude from the visualization that our MAGIC is full of power for attacking the object detection system in the real-world scenarios.\nEthical Considerations. Our experiments ensured that attacks were not visible to AD vehicles on public roads. We also discuss the implications of MAGIC in Supplementary."}, {"title": "6. Ablation Study", "content": "In this section, we ablate the proposed MAGIC framework to verify the contribution of the involves LLM agents.\nSetting. We start from the basic patch generation which is realized with GAgent in our framework, thus we isolate it without either deployment or self-examination. We denote this setup as \"GAgent-na\u00efve\". The further functionality that our MAGIC provided is the contextually appropriate placement of the patch, which is controlled by DAgent. Thus, we combine the basic generation function with the patch deployment planning as the second ablation, denoted as \"GAgent-na\u00efve w/ DAgent\". As for the third ablation, we note that the EAgent is responsible for supervising both patch attack effectiveness and deploy integration, so we combine the basic generation function with the patch attack supervision from EAgent which denotes as \u201cGAgent-na\u00efve w/ EAgent-ae\". Finally, we involve the deploy integration of EAgent and get the proposed MAGIC framework.\nResults. As the statistics shown in Tab. 3, we see that naive patch generation without any text prompt designing or optimization cannot attack the detectors at all. As a result, it is obvious that the DAgent cannot do any help with the improvement of the attack performance for GAgent, i.e., GAgent-na\u00efve w/ DAgent. The good news is that the involvement of attack effectiveness supervision from EAgent greatly boosted the attack effectiveness of the generated patch for all the detectors, and achieves 39.67%, 45.00% attack effectiveness improvements compared to the DAgent baseline. In summary, our MAGIC achieves its best attack performance by mainly benefiting to the supervision of EAgent and also the appropriate deployment of DAgent."}, {"title": "7. Conclusion", "content": "In this study, we propose the MAGIC framework which reformulate physical adversarial attacks as a one-shot patch generation problem. Our approach generates adversarial patches through a deep generative model that considers the specific scene context, enabling direct physical deployment in matching environments. By conduct experiment on both digital and physical level, we prove that the proposed MAGIC can effectively realize the context-aware patch generation, the real-world deployment and the detection system attack effectiveness. To the best of our knowledge, our work is the very initial study to improve and extend the natural diffusion attack into the physical world. We hope it can inspire more future works on the exploration physical adversarial attack."}, {"title": "1. Implementation Details", "content": "In this section, we give out the technical details for implementing the proposed MAGIC framework. Specifically, there are three main aspects: \u25cf the experimental environment;  the technical pipeline of patch generation, deployment and evaluation; \u25a2 the LLM instruction template utilized to prompt each agent in the pipeline."}, {"title": "1.1. Experimental Environment", "content": "We employ ChatGPT (gpt-4o-2024-08-06) as the backend LLM for all the three agents in our design. All the experiments are conducted via a server with AMD EPYC 9554 64-core Processor and an NVIDIA L40 GPU."}, {"title": "1.2. Technical Pipeline", "content": "As the methodology giving in the main text is a conceptual description of our MAGIC framework, here we show the practical implementation of our proposed framework. As shown in Fig. 1, our MAGIC is a iterative process where each step is a round of patch generation, deployment and evaluation procedure. For the function and capability of each agent, please refer to Sec. 4 for the description."}, {"title": "1.3. Collaborative Agents Prompting", "content": "For the actual instructions to prompt the GAgent, DAgent and EAgent, we show them in p.4, p.5 and p.6 respectively. Note that we masked some of the details for confidential reasons, they will be released after the peer review process."}, {"title": "2. More Physical Experiments", "content": "In this section, we conduct more physical experiments to support the superior performance of our proposed MAGIC framework. As shown in Fig. 2 and Fig. 3, we apply MAGIC to generate and deploy for three more different environments. Following the same strategy, we first test whether MAGIC can consistently generate effective patch for multiple round of execution given an environment. The results is illustrated in Fig. 2 and we can observe that all three different generated patches are effective to attack the three detectors. For the distinct environment attacking, the physical cases shown in Fig. 3 further support our conclusion that our MAGIC is full of power for attacking the object detection system in the real-world scenarios."}, {"title": "3. Limitations", "content": "The scope of our work is focused on designing physical adversarial attack in traffic scenarios via LLM agents. Nevertheless, the current environments primarily serve as proofs of concept under standard conditions, rather than encompassing a broader range of factors such as varying weathers, daylight and nighttime conditions, or noisy camera inputs. Furthermore, our framework has not yet considered seam-"}, {"title": "4. Broader Impact", "content": "Understanding vulnerabilities in traffic systems can help improve the safety and resilience of autonomous vehicles. In fact, by rigorously testing adversarial robustness across"}, {"title": "5. Future Work", "content": "Our future work aims to explore larger-scale attacks to evaluate adversarial vulnerabilities across broader traffic systems, including segmentation and depth estimation tasks critical for autonomous navigation. Furthermore, integrating advanced blending techniques, such as diffusion models, could enhance the realism and effectiveness of natural adversarial patches, enabling more comprehensive testing under diverse environmental conditions. Importantly, as also suggested by the lack of investigation for robust defenses, we aim to develop robust defense mechanisms to counter natural diffusion attacks both digitally and physically, would be essential for ensuring safety and resilience in real-world applications."}, {"title": "4.4. Self-Examination Agent (EAgent)", "content": "\u0394S\u209c\u2090\u1d63\u2089\u2091\u209c = D\u209c\u2090\u1d63\u2089\u2091\u209c(I'),\nN(P\u1d62, \u0399') \u2265 \u03b4,"}]}