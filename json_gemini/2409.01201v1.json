{"title": "ENCLAP++: ANALYZING THE ENCLAP FRAMEWORK FOR OPTIMIZING AUTOMATED AUDIO CAPTIONING PERFORMANCE", "authors": ["Jaeyeon Kim", "Minjeong Jeon", "Jaeyoon Jung", "Sang Hoon Woo", "Jinjoo Lee"], "abstract": "In this work, we aim to analyze and optimize the EnCLAP framework, a state-of-the-art model in automated audio captioning. We investigate the impact of modifying the acoustic encoder components, explore pretraining with different dataset scales, and study the effectiveness of a reranking scheme. Through extensive experimentation and quantitative analysis of generated captions, we develop EnCLAP++, an enhanced version that significantly surpasses the original.", "sections": [{"title": "1. INTRODUCTION", "content": "Automated audio captioning (AAC), a cross-modal translation involving transcribing audio signals into concise and meaningful natural language descriptions [1], remains a particularly challenging task with a substantial performance gap between human and machine. One significant contributor to the performance gap can be attributed to the intrinsic complexity of the task, as distinguishing between various sound events, especially between similar and ambiguous ones, requires extensive real-world knowledge. Furthermore, the scarcity of high-quality data, with the most widely used datasets, AudioCaps [2] and Clotho [3] containing only 50K and 20K captions, respectively, poses an additional challenge. To address these challenges, prior studies have employed pretrained audio encoders trained on audio classification tasks [4, 5, 6], leveraged the text generation capabilities of pretrained language models like GPT-2 [7, 8, 9] and BART [10, 11], and incorporated auxiliary loss terms, including keyword prediction loss [12] or sentence embedding loss [13], to improve the semantic quality of captions and provide additional training signal.\nBuilding on the previous line of research, Kim et al. [14] proposed the EnCLAP framework which integrates a set of pretrained models with an auxiliary training task. Specifically, EnCLAP utilizes two acoustic feature encoders, EnCodec [15] and CLAP [16], to generate timestep-level and sequence-level representation of the input audio sequence, respectively. EnCLAP utilizes pretrained BART as the caption decoder to leverage these features and generate captions. Furthermore, Kim et al. also introduced masked codec modeling (MCM), an auxiliary task which involves masking a part of the input codec sequence and predicting it, to enhance the acoustic awareness of the caption decoder. The caption decoder was trained jointly using cross-entropy loss for caption generation and MCM loss. The combination of these approaches allowed EnCLAP to achieve state-of-the-art performance on the AudioCaps dataset.\nAlthough EnCLAP exhibits impressive performance, the study by Kim et al. lacks sufficient experimental evaluation for determining the optimal models for the model components. Notably, the authors do not investigate alternative sequence-level acoustic features beyond CLAP. Furthermore, for timestep-level acoustic features, while they demonstrate that discrete codec input outperforms continuous input, their analysis is restricted to a single setup using EnCodec, without exploring other options or configurations. Additionally, Kim et al. acknowledge the issue of overfitting in larger model variants but do not investigate the use of large-scale weakly-labeled datasets [17, 6], which contain noisy and model-generated captions. Therefore, the EnCLAP framework has potential for further optimization.\nIn this work, we extend and optimize the EnCLAP framework through a comprehensive examination of its components. We explore alternative acoustic feature encoder components and assess their efficacy. We also investigate the impact of large-scale training incorporating weakly-labeled datasets on the framework's performance. Furthermore, we adopt a sampling-and-reranking approach [6] as an alternative to beam search decoding and evaluate its effectiveness. Finally, we conduct a qualitative analysis of the generated captions to examine the effects of each component on the outputs. Based on our findings, we present EnCLAP++, an improved version of the EnCLAP model that achieved second place in the DCASE2024 Challenge Task6."}, {"title": "2. EXPERIMENTAL DESIGN", "content": "Neural audio codecs are autoencoder models designed to encode waveforms into sequences of discrete codes. Recent advancements [18, 15, 19] typically employ residual vector quantization (RVQ) for compression, utilizing multiple codebooks to quantize the residuals of preceding codebooks. Ultimately, the input waveforms are transformed into a set of parallel discrete code sequences, each of which is associated with a unique codebook. Neural audio codecs have demonstrated success as the acoustic representation format in generative audio models [20, 21, 22].\nKim et al. [14] demonstrate that language models achieve superior performance when used with discrete input sequences compared to continuous input sequences. However, their study does not explore the impact of different configurations within the discrete input sequence setup. To address this limitation, we conduct experiments to examine the effects of different codec settings on the model performance. Specifically, we investigate the effect of codebook size,\nsample rate, and codec type on the final outcome.\nThe original EnCLAP employed a version of EnCodec [15] that compresses a 24kHz audio signal into 16 discrete code sequences at a rate of 75Hz, with a codebook size of 1024. We experiment with two additional variants of EnCodec, which yield 8 and 32 code sequences, respectively, as well as a variant that processes 48kHz audio signal input. As for the alternative codec, we use a variant of Descript Audio Codec (DAC) [19] that closely resembles the original EnCodec setup, which transforms 24kHz audio signal into 32 code sequences at a rate of 75Hz. We opted for DAC as the alternative codec due to its superior performance in audio compression, as well as downstream tasks [19, 23]."}, {"title": "2.2. Sequence-level Acoustic Embedding", "content": "While EnCLAP employs CLAP [16] as its sequence-level acoustic feature encoder, preceding studies in audio captioning have predominantly utilized models pretrained on the AudioSet [24] dataset for audio classification task [4, 5, 6]. In this work, we investigate alternative candidates for the sequence-level acoustic encoder component. Specifically, we examine the sequence-level representation capabilities of a model pretrained on AudioSet with audio tagging task and its variants, which have gone through additional audio-text retrieval training. We compare the audio captioning performance of these models with the original CLAP setup and assess the impact of additional retrieval training on downstream performance.\nFor the baseline sequence-level encoder, we use ConvNext-Tiny [25] pretrained on AudioSet classification, referred to as CNext, and three of its variants that have undergone additional training on datasets of varying scales. Specifically, the three dataset configurations are: (1) Clotho [3], (2) AudioCaps [2] and Clotho, and (3) WavCaps [17], AudioCaps, and Clotho. We use m-LTM framework [26] and bge text encoder [27] for retrieval training. We assess the performance of these models against the original CLAP version."}, {"title": "2.3. Large-scale Pretraining", "content": "The original EnCLAP described two versions of the model, denoted as \"base\" and \"large\", based on the size of the underlying BART [10] model used. The study highlights the issue of overfitting, especially in the large variant with smaller training datasets. To mitigate this issue, we draw on the recent trend in audio captioning, which involves leveraging weakly-labeled datasets for pretraining [17, 6]. In particular, we evaluate a large-scale pretraining setup, where the model is pretrained on the WavCaps, and finetuned on Clotho, against the original EnCLAP dataset setup, where the model is pretrained on AudioCaps and finetuned on Clotho. From WavCaps, we filter out\naudio clips that fall outside the 1-30 second duration range, as well as overlapping clips from AudioCaps and Clotho. We evaluate both setups using both the base and large variants of our model."}, {"title": "2.4. Generation and Reranking", "content": "Previous works, including EnCLAP, have utilized beam search decoding for caption generation. However, Wu et al. [6] demonstrates that the sampling-then-reranking approach yields more diverse and informative captions. Wu et al. proposes two scores for candidate reranking: the encoder reranking score and the decoder reranking score. The encoder reranking score is the cosine similarity score between the input audio representation and the generated caption representation computed using a retriever model. The decoder reranking score is the log-likelihood of the generated caption given the input audio. In this study, we explore the benefits of incorporating the reranking scheme into the EnCLAP framework. Specifically, we compare the original beam search scheme against three reranking setups: encoder reranking, decoder reranking, and hybrid reranking. We use CLAP as the retriever model for computing the encoder reranking score. We perform a fluency error-based filtering before the reranking procedure, following Wu et al..\nFor sampling, we use nucleus sampling with a probability threshold of 0.95 and a temperature of 0.5 to generate 30 candidates. For hybrid reranking, we rank the candidates by the weighted sum of the encoder reranking score and the decoder reranking score using weights of 0.6 and 0.4, respectively."}, {"title": "2.5. Quantitative Evaluation Metric", "content": "We adopt both widely used AAC metrics, METEOR, CIDEr, SPICE, and SPIDEr, and more recently proposed AAC metrics, SPIDEr-FL, FENSE [28], and Vocab to evaluate various aspects of the generated captions. All metrics are calculated using the aac-metrics library. METEOR is a machine translation evaluation metric, based on unigram precision and recall. CIDEr and SPICE assess the syntactic and semantic quality of the generated captions, respectively, while SPIDEr is a linear combination of them. SPIDEr-FL is SPIDEr score penalized by the fluency error. FENSE is the combination of the SentenceBERT similarity score and the fluency error penalty. Vocab shows the diversity of the vocabularies in the generated captions."}, {"title": "2.6. Qualitative Analysis", "content": "Although quantitative metrics provide valuable insights into relative improvements in model performance, they are inherently limited, particularly in tasks such as audio captioning, where no single objective\ntruth exists. Thus, in addition to reporting quantitative metrics, we perform a qualitative analysis of the generated captions. Specifically, we identify the examples with the largest improvement in the evaluation metric between the baseline and the best-performing variant and manually examine the enhancement in the caption quality."}, {"title": "3. RESULTS AND ANALYSIS", "content": null}, {"title": "3.1. Timestep-level Acoustic Embedding", "content": "Table 1 shows that substituting the EnCodec encoder with an alternative variant does not enhance the model's performance and, in fact, leads to incremental degradation. This indicates that changing the timestep-level feature encoder across different EnCodec models has a negligible effect on the performance in the audio captioning task. Contrastively, replacing the EnCodec encoder with the DAC encoder leads to a modest improvement in the model performance. We believe that the DAC's superior ability to preserve the information in the original audio signal contributes to the enhancement. Therefore, we adopt DAC as the timestep-level acoustic feature encoder in subsequent experiments."}, {"title": "3.2. Sequence-level Acoustic Embedding", "content": "As illustrated in Table 1, the model using CNext as the sequence-level acoustic encoder falls behind the CLAP variant. However, the results indicate that additional retrieval training boosts the audio captioning performance and further, increasing the dataset size narrows the performance gap relative to the CLAP variant. Nevertheless, none of the CNext variants fully surpass the CLAP variant in terms of performance. We attribute the performance gap to the fact that CLAP was trained on a much larger scale than CNext, even with additional training, which is consistent with our findings within the CNext variants. Consequently, we will proceed with the original CLAP variant in subsequent experiments."}, {"title": "3.3. Large-scale Pretraining", "content": "The third section of Table 1 demonstrates the effect of augmenting the pretraining dataset with a large-scale weakly-labeled dataset. Notably, our results for the original dataset setup replicate the phenomenon observed in the original EnCLAP work, where the large variant performs worse than the base variant. While variants with large-scale pretraining also exhibit this issue, the performance degradation is significantly less pronounced. Given that large-scale pretraining substantially improves the base variant, we infer that even the base variant can benefit from larger datasets. Our hypothesis is that larger datasets are necessary to fully utilize the capabilities of the large variant models."}, {"title": "3.4. Generation and Reranking", "content": "We investigated sampling and reranking techniques using the base variant pretrained on WavCaps from Sec 3.3. The results are presented in the last section of Table 1. Our findings indicate that encoder reranking enhances both the diversity of words and the semantic content of the generated captions. However, this improvement in semantic quality comes at the expense of syntactic quality. In contrast, decoder reranking alone yields results comparable to beam search, while when encoder and decoder reranking are combined, there is a significant improvement in semantic quality without any degradation in syntactic quality."}, {"title": "3.5. Qualitative Analysis", "content": "Timestep-level Acoustic Embedding. The variant without DAC tends to focus on the most prominent event in a clip, but frequently overlooks background and supplementary acoustic events. This shortcoming can be attributed to the inherent constraint of relying on a single vector to represent the entire clip, which can lead to a loss\nof details. The inclusion of DAC, a timestep-level representation, enables the model to capture more fine-grained details of the scene.\nSequence-level Acoustic Embedding. While the model without CLAP generally succeeds in capturing the atmosphere of the acoustic scene, it tends to confound the overall semantic meaning of the scene. Thus, its captions describe an event similar to the actual event, but is actually different. We believe this comes from the lack of world knowledge to clear up the ambiguity. Thus, the variant with CLAP does not suffer from this issue. We attribute this to the model's lack of world knowledge, which fails to resolve ambiguities. Consequently, its generated captions describe an event that is similar to, yet distinct from, the actual event. In contrast, the variant with CLAP does not suffer from this issue.\nGeneration and Reranking. The captions produced by beam search variants are typically shorter and more concise, often omitting scene details. In contrast, the reranking variant generates more detailed captions that closely align with the label captions."}, {"title": "3.6. Results on AudioCaps", "content": "Based on observations from Section 3, we propose EnCLAP++, an improved version of EnCLAP that incorporates DAC, large-scale pretraining, and hybrid reranking. We evaluate EnCLAP++ on the AudioCaps dataset and present the results in Table 3. The assessment shows that both EnCLAP++-base and EnCLAP++-large outperform their respective EnCLAP counterparts, demonstrating the effectiveness of our mix of optimizations across different datasets."}, {"title": "3.7. Results on DCASE Challenge 2024", "content": "We submitted a variant of EnCLAP++ to the DCASE Challenge 2024. This variant employs a large version of BART and is pretrained on an extensive dataset that combines WavCaps, AudioCaps, and Clotho-Chatmix [6]. Due to the challenge regulations, we could not use CLAP because of potential overlap with the evaluation dataset. Therefore, we adopted CNext from Sec 2.2, which was additionally trained with text-retrieval on WavCaps, AudioCaps, and Clotho, as the sequence-level representation.\nThe overall results are presented in Table 4. Our model achieved second place in the challenge, which was ranked based on the FENSE metric. Additionally, our model outperformed all other models on the METEOR and SPICE metrics."}, {"title": "4. CONCLUSION", "content": "This study presents an analysis of the EnCLAP framework and its components. Our investigation reveals that replacing the EnCodec encoder with the DAC encoder, augmenting the pretraining dataset with large-scale weakly-labeled data, and the incorporating of a reranking scheme enhances the model's performance in audio captioning. Notably, our modified variant, EnCLAP++ shows significant improvement over the original model. Future directions for our research involve extending the EnCLAP framework to incorporate recent advances in large language models, thereby enhancing its capabilities."}]}