{"title": "Cross-Lingual Conversational Speech Summarization with Large Language Models", "authors": ["Max Nelson", "Shannon Wotherspoon", "Francis Keith", "William Hartmann", "Matthew Snover"], "abstract": "Cross-lingual conversational speech summarization is an important problem, but suffers from a dearth of resources. While transcriptions exist for a number of languages, translated conversational speech is rare and datasets containing summaries are non-existent. We build upon the existing Fisher and Callhome Spanish-English Speech Translation corpus by supplementing the translations with summaries. The summaries are generated using GPT-4 from the reference translations and are treated as ground truth. The task is to generate similar summaries in the presence of transcription and translation errors. We build a baseline cascade-based system using open-source speech recognition and machine translation models. We test a range of LLMs for summarization and analyze the impact of transcription and translation errors. Adapting the Mistral-7B model for this task performs significantly better than off-the-shelf models and matches the performance of GPT-4.", "sections": [{"title": "1. Introduction", "content": "Despite the advances in automatic speech recognition (ASR) since the advent of deep neural networks, conversational speech remains a significant challenge. Due to the lack of data and challenging recording conditions, word error rates (WER) remain high. Even when accurately transcribed, conversational speech is difficult to read; making it beneficial to build models of cross-lingual summarization that can generate more human-readable versions of conversations. Faithful summarization captures the important information in the conversation without the distractions of hesitations and other speech disfluencies.\nRecent advances in large language models (LLMs) have allowed them to match or even surpass the capabilities of special purpose models for a number of tasks, including summarization. For some datasets, LLMs have been found to not only surpass previous summarization models, but to meet the performance of humans [1]. LLMs have the additional benefit of flexibility. With prompting and finetuning, the model can be made to take advantage of additional information, or to provide a contextual summary based on additional instructions.\nSummarization of speech has a long history [2]. The majority of the work has focused on single-speaker audio with a clear goal (e.g., voicemail [3], broadcast news [4]). Two-party conversations have also been a domain of interest. Meeting summarization was explored in [5]. [6] released a large corpus of interviews where a host interviews a guest. Another major area is the summarization of call center interactions [7]. Given the audio from an interaction between a customer and a call center employee, the goal is to describe the nature of the call and whether and how a request was fulfilled. To our knowledge, there is no prior work on the cross-lingual summarization of conversational speech outside of the call-center domain.\nThe classic approach to cross-lingual speech summarization has been a cascaded pipeline where audio is automatically transcribed and then fed to a summarization system [8]. A benefit of this approach is that the individual models can be trained independently, taking advantage of non-parallel data. More recent work has explored direct summarization where a single model is used to directly summarize the audio [9]. The model can be directly optimized for the task, as opposed to individual components being optimized for intermediate objectives.\nSummarization can be either abstractive or extractive. In a cross-lingual conversational speech domain, extractive summarization can be problematic. Both transcription and translation errors are propagated, and the extracted utterances can be incomplete or incoherent [10]. Because of these issues, we focus on abstractive summarization. While direct approaches can be powerful, we focus on a cascaded approach which allows us to incorporate and compare open-source models. We aim to establish an evaluation framework for conversational speech summarization and to evaluate the ability of LLMs to accomplish the task. We leave comparisons against direct summarization to future work. Our contributions are as follows:\n\u2022 We provide a first of its kind public conversational speech summarization dataset\u00b9 by building upon existing datasets.\n\u2022 We compare a range of LLMs and provide baseline performance using open-source tools and models.\n\u2022 We demonstrate that by fine-tuning a relatively small, quantized LLM, we achieve performance competitive with GPT-4."}, {"title": "2. Technical Approach and Dataset Creation", "content": null}, {"title": "2.1. Datasets", "content": "We focus on the Fisher and Callhome corpora of Spanish conversational telephone speech (CTS). The audio and Spanish transcripts of which are available through the LDC2. The corpora contain crowd-sourced English translations [11] of Spanish telephone calls. The translations are publicly available\u00b3. We focus on this dataset due to the large amount of transcribed audio compared to other CTS datasets and the availability of translations. We are unaware of similar CTS datasets with English"}, {"title": "2.2. Summary Generation", "content": "While we have human generated translations for this dataset, there are no existing summaries. The collection of human summaries for this dataset would be expensive and time-consuming. Instead, we generate summaries using GPT-44. We justify this decision in two ways. The first is that summaries generated by GPT-4's predecessors have been judged comparable to human summaries on some datasets[1]. Given the difficulty of summarizing conversational speech, there is no guarantee that summaries generated by humans would be substantially better. Second, GPT-4 is given access to reference translations when generating the summaries. During evaluation, an LLM will be given input that contains both ASR and machine translation (MT) errors. The goal is to generate a summary from errorful input that can match the reference summary. Even if the reference summaries are deficient, obtaining a similar result in the presence of errors would still signify a significant achievement.\nFor both the training and the test sets, we present GPT-4 with a conversation and ask it to generate a summary. Since we know summaries capturing the same content can differ significantly in style, we generate four total summaries for each test conversation by sampling outputs with a temperature of 0.5. We aim for our evaluation set to be useful even for evaluating models with limited context windows, so we set the maximum number of words in a conversation to 1200 words. Given this limit, the context, prompt, and a reasonable length summary will all fit within a context window of 2048 tokens. If a conversation in either set exceeds the 1200 word limit, we split it into equal-sized chunks and treat each individual chunk as a separate conversation. Across both the train and test sets, the summaries range in size from 144 to 443 words, with a median size of 268 words. We plan to release these reference summaries to the community.\nA breakdown of the number of conversations, chunks, utterances, and audio hours for each dataset is shown in Table 3. When building our summarization dataset, each chunk represents one training example or one datapoint for evaluation. While any of the individual testsets would be sufficient for ASR"}, {"title": "2.3. LLM Adaptation", "content": "Along with testing off-the-shelf LLMs, we also experiment with supervised fine-tuning for task adaptation in order to understand the potential for improvement and establish strong baselines for future work to compare against. We use LoRA[12] finetuning to adapt the models. All fine-tuning experiments are run with 4-bit quantization and fp16 precision. A LoRA adaptor is learned for every linear layer in the model with r = 64. The training data are GPT-4 reference summaries paired with either the reference English transcripts (Ref) or with the outputs of our Whisper-NLLB speech translation system (MT from ASR). In other words we we create two training samples for each GPT-4 summary that differ only in the input. We vary which inputs we use during fine-tuning to evaluate the extent to which domain-matched input improves summarization quality. For fine-tuning experiments that make use of both the reference English and MT from ASR inputs we train the model for a single epoch over all training data. When finetuning on only the reference English or MT from ASR transcripts we train for two epochs in order to keep the number of update steps constant across experiments.5"}, {"title": "3. Experimental Setup", "content": null}, {"title": "3.1. ASR Model", "content": "We use the Whisper-large-v3 model [13] for ASR. While a dataset and language-specific model could likely outperform the Whisper model in the CTS domain, the Whisper model is publicly available and is chosen due to its wide use and reproducibility. The WER of the Whisper model on each of the five test sets is shown in the second column of Table 2. We measure the WER after downcasing the the output and removing punctuation. This postprocessing is not applied when used in the cascaded pipeline."}, {"title": "3.2. MT Model", "content": "We use the NLLB 1.3 Billion parameter dense model [14] for machine translation. As with Whisper for ASR, a domain-specific model would likely outperform the NLLB model on CTS, but we use NLLB for better reproducibility. The BLEU scores for NLLB on the test sets are also shown in Table 2. We include punctuation when computing the reported BLEU scores, we also tested scoring without punctuation and found it to have negligible impact on the scores so we exclude those results for legibility. The third column in Table 2 uses the Whisper ASR output as input to NLLB, while the last column uses the reference Spanish transcriptions. On average the BLEU scores drop by about 8 points when using ASR output as opposed to reference transcriptions. Note that while some of the test sets contain multiple translations, we only report BLEU scores using a single reference so that the numbers are comparable across test sets. In the remaining sections we explore the impact of cascaded ASR and MT errors on downstream summarization."}, {"title": "3.3. LLMs for Summarization", "content": "As described in Section 2.2, we use GPT-4 to generate the reference summaries. We then evaluate a range of open-source and API-based models against the GPT-4 generated references. The API-based models we consider are GPT-3.5 [15] and GPT-4. The open-source models we consider are the 7 and 13 billion parameter versions of Llama 2 [16], the 7 billion parameter Mistral [17], and the 45 billion mixture-of-experts model Mixtral-8x7 [18]. We focus on these open-source models due to their low compute requirements which make them more amenable to real-world applications. For the same reason all inference is run with 4-bit quantization and fp16 precision. All open-source models tested are the officially released chat or instruct tuned versions. It is well-known that the performance of LLMs can vary dramatically depending on the prompt [19]. We follow the guidelines for prompt structure released by the publishers of each of the individual models. Our exact prompt structure will be released with the reference summaries. In addition to applying these models off-the-shelf, we also run a set of further supervised fine-tuning experiments with the Mistral 7B model."}, {"title": "4. Results", "content": null}, {"title": "4.1. Zero-shot", "content": "We evaluate the quality of summaries with ROUGE-L [20]. We explored a number of other metrics, but most gave a similar ordering in terms of model performance. While we recognize the pitfalls of focusing on a single metric [21], we only report ROUGE-L due to space concerns.\nBaseline results are shown in Table 3. Each row represents a different input condition. While the performance of each model becomes progressively worse as more error is introduced through MT and ASR-as opposed to using reference transcripts and translations the drop in performance is less than anticipated. The difference between a summary generated from a reference translation and the cascade of AST and MT is no more than 10% relative across all models. We believe there are two possible reasons for this result and they merit future investigation. Either the errors from transcription and translation do not impact the model's ability to summarize the key information, or the metric is not able to measure the impact.\nThe performance of the open-source models follows the expected ranking, with the larger MoE model Mixtral outperform-"}, {"title": "4.2. Finetuning", "content": "In addition to testing off-the-shelf API-based and open-source models, we also fine-tune Mistral-7B to provide a much stronger baseline. Table 3 includes results from a fine-tuned model that is trained with both reference and MT of ASR inputs. Compared to the unadapted version, fine-tuning improves the performance of the Mistral-7B model by almost 10 ROUGE points. After fine-tuning, the performance of the model is comparable to, or even outperforms, GPT-4.\nIn order to determine the value of including both the reference transcript and MT of ASR inputs in fine-tuning, we run a set of experiments in which we vary our fine-tuning dataset. Table 4 shows performance, aggregated across datasets, when we fine-tune on only the reference transcripts, only the MT of ASR transcripts, and the combination of both. We also experiment with a fourth condition in which we include both the reference transcripts and MT of ASR transcripts, but use a separate source prompt during fine-tuning and inference for the two. The intuition behind this experiment is that there are likely different error distributions between the two types of inputs and it might help the LLM to signal what type of input is being provided.\nWe find that ROUGE scores are relatively flat across different fine-tuning sets. Fine-tuning on reference transcripts does result in the model that performs best at summarizing reference transcripts, although the differences are small. The same cannot be said for testing on MT of ASR outputs, where the inclusion of MT of ASR data in the train set does not reliably yield an improvement in ROUGE. This is potentially reflective of the fact that even before fine-tuning models seemed very robust to MT and ASR errors. In Table 5 we show how performance varies as we vary the amount of fine-tuning data. We see a roughly linear increase in ROUGE as the amount of data increases.\nIn Table 6 we show an example summary using GPT-4 where the input comes either from reference translations or ASR+MT. The second summary contains several phrases highlighted in red. We can trace these back to errors in either the ASR or MT. The statements \"so poor' and 'comotose.\" likely arise from a combination of errors and the true statement should be, \"You eat all you earn.\" When the summmary mentions Speaker B offering \u201cmundane responses\" and saying, \"Thank you, I'm all right,\" it is a reflection of hallucinations in both the ASR and the MT. The Whisper model tends to output \"Gracias\" as a filler word and the NLLB model translates it as \"Thank you, I'm all right.\" Interestingly, this demonstrates that errors in ASR+MT not only impact the factual information in the summary, but also the implied tone."}, {"title": "5. Conclusions", "content": "We have established an evaluation framework for CTS summarization. Using GPT-4, we created reference summaries for a well-known Spanish CTS corpus with existing English translations. Our experiments establish a baseline for a cascaded approach to summarization using publicly available models. While GPT-4 outperforms existing open-source models, we are able to match the performance of GPT-4 by fine-tuning the Mistral-7B model. This demonstrates the efficacy of using large, API-based models like GPT-4 to generate evaluation and adaptation data for cross-lingual speech summarization.\nWe plan to explore several extensions to this work in the future. Moving beyond general summarization, we want to explore contextual summarization where the summary can be guided by input from the user to focus on specific information."}]}