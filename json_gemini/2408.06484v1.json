{"title": "Cross-Lingual Conversational Speech Summarization with Large Language Models", "authors": ["Max Nelson", "Shannon Wotherspoon", "Francis Keith", "William Hartmann", "Matthew Snover"], "abstract": "Cross-lingual conversational speech summarization is an important problem, but suffers from a dearth of resources. While transcriptions exist for a number of languages, translated conversational speech is rare and datasets containing summaries are non-existent. We build upon the existing Fisher and Callhome Spanish-English Speech Translation corpus by supplementing the translations with summaries. The summaries are generated using GPT-4 from the reference translations and are treated as ground truth. The task is to generate similar summaries in the presence of transcription and translation errors. We build a baseline cascade-based system using open-source speech recognition and machine translation models. We test a range of LLMs for summarization and analyze the impact of transcription and translation errors. Adapting the Mistral-7B model for this task performs significantly better than off-the-shelf models and matches the performance of GPT-4.\nIndex Terms: ASR, machine translation, summarization", "sections": [{"title": "1. Introduction", "content": "Despite the advances in automatic speech recognition (ASR) since the advent of deep neural networks, conversational speech remains a significant challenge. Due to the lack of data and challenging recording conditions, word error rates (WER) remain high. Even when accurately transcribed, conversational speech is difficult to read; making it beneficial to build models of cross-lingual summarization that can generate more human-readable versions of conversations. Faithful summarization captures the important information in the conversation without the distractions of hesitations and other speech disfluencies.\nRecent advances in large language models (LLMs) have allowed them to match or even surpass the capabilities of special purpose models for a number of tasks, including summarization. For some datasets, LLMs have been found to not only surpass previous summarization models, but to meet the performance of humans [1]. LLMs have the additional benefit of flexibility. With prompting and finetuning, the model can be made to take advantage of additional information, or to provide a contextual summary based on additional instructions.\nSummarization of speech has a long history [2]. The majority of the work has focused on single-speaker audio with a clear goal (e.g., voicemail [3], broadcast news [4]). Two-party conversations have also been a domain of interest. Meeting summarization was explored in [5]. [6] released a large corpus of interviews where a host interviews a guest. Another major area is the summarization of call center interactions [7]. Given the audio from an interaction between a customer and a call cen-"}, {"title": "2. Technical Approach and Dataset Creation", "content": ""}, {"title": "2.1. Datasets", "content": "We focus on the Fisher and Callhome corpora of Spanish conversational telephone speech (CTS). The audio and Spanish transcripts of which are available through the LDC2. The corpora contain crowd-sourced English translations [11] of Spanish telephone calls. The translations are publicly available\u00b3. We focus on this dataset due to the large amount of transcribed audio compared to other CTS datasets and the availability of trans-"}, {"title": "2.2. Summary Generation", "content": "While we have human generated translations for this dataset, there are no existing summaries. The collection of human summaries for this dataset would be expensive and time-consuming. Instead, we generate summaries using GPT-44. We justify this decision in two ways. The first is that summaries generated by GPT-4's predecessors have been judged comparable to human summaries on some datasets[1]. Given the difficulty of summarizing conversational speech, there is no guarantee that summaries generated by humans would be substantially better. Second, GPT-4 is given access to reference translations when generating the summaries. During evaluation, an LLM will be given input that contains both ASR and machine translation (MT) errors. The goal is to generate a summary from errorful input that can match the reference summary. Even if the reference summaries are deficient, obtaining a similar result in the presence of errors would still signify a significant achievement.\nFor both the training and the test sets, we present GPT-4 with a conversation and ask it to generate a summary. Since we know summaries capturing the same content can differ significantly in style, we generate four total summaries for each test conversation by sampling outputs with a temperature of 0.5. We aim for our evaluation set to be useful even for evaluating models with limited context windows, so we set the maximum number of words in a conversation to 1200 words. Given this limit, the context, prompt, and a reasonable length summary will all fit within a context window of 2048 tokens. If a conversation in ei-ther set exceeds the 1200 word limit, we split it into equal-sized chunks and treat each individual chunk as a separate conversation. Across both the train and test sets, the summaries range in size from 144 to 443 words, with a median size of 268 words. We plan to release these reference summaries to the community.\nA breakdown of the number of conversations, chunks, utterances, and audio hours for each dataset is shown in Table 3. When building our summarization dataset, each chunk rep-resents one training example or one datapoint for evaluation. While any of the individual testsets would be sufficient for ASR"}, {"title": "2.3. LLM Adaptation", "content": "Along with testing off-the-shelf LLMs, we also experiment with supervised fine-tuning for task adaptation in order to understand the potential for improvement and establish strong baselines for future work to compare against. We use LoRA[12] finetuning to adapt the models. All fine-tuning experiments are run with 4-bit quantization and fp16 precision. A LoRA adaptor is learned for every linear layer in the model with r = 64. The training data are GPT-4 reference summaries paired with either the reference English transcripts (Ref) or with the outputs of our Whisper-NLLB speech translation system (MT from ASR). In other words we we create two training samples for each GPT-4 summary that differ only in the input. We vary which inputs we use during fine-tuning to evaluate the extent to which domain-matched input improves summarization quality. For fine-tuning experiments that make use of both the reference English and MT from ASR inputs we train the model for a single epoch over all training data. When finetuning on only the reference English or MT from ASR transcripts we train for two epochs in order to keep the number of update steps constant across experiments.5"}, {"title": "3. Experimental Setup", "content": ""}, {"title": "3.1. ASR Model", "content": "We use the Whisper-large-v3 model [13] for ASR. While a dataset and language-specific model could likely outperform the Whisper model in the CTS domain, the Whisper model is publicly available and is chosen due to its wide use and reproducibility. The WER of the Whisper model on each of the five test sets is shown in the second column of Table 2. We measure the WER after downcasing the the output and removing punctuation. This postprocessing is not applied when used in the cascaded pipeline."}, {"title": "3.2. MT Model", "content": "We use the NLLB 1.3 Billion parameter dense model [14] for machine translation. As with Whisper for ASR, a domain-specific model would likely outperform the NLLB model on CTS, but we use NLLB for better reproducibility. The BLEU scores for NLLB on the test sets are also shown in Table 2. We include punctuation when computing the reported BLEU scores, we also tested scoring without punctuation and found it to have negligible impact on the scores so we exclude those results for legibility. The third column in Table 2 uses the Whisper ASR output as input to NLLB, while the last column uses the reference Spanish transcriptions. On average the BLEU scores drop by about 8 points when using ASR output as opposed to reference transcriptions. Note that while some of the test sets contain multiple translations, we only report BLEU scores using a single reference so that the numbers are comparable across test sets. In the remaining sections we explore the impact of cascaded ASR and MT errors on downstream summarization."}, {"title": "3.3. LLMs for Summarization", "content": "As described in Section 2.2, we use GPT-4 to generate the reference summaries. We then evaluate a range of open-source and API-based models against the GPT-4 generated references. The API-based models we consider are GPT-3.5 [15] and GPT-4. The open-source models we consider are the 7 and 13 billion parameter versions of Llama 2 [16], the 7 billion parameter Mistral [17], and the 45 billion mixture-of-experts model Mixtral-8x7 [18]. We focus on these open-source models due to their low compute requirements which make them more amenable to real-world applications. For the same reason all inference is run with 4-bit quantization and fp16 precision. All open-source models tested are the officially released chat or instruct tuned versions. It is well-known that the performance of LLMs can vary dramatically depending on the prompt [19]. We follow the guidelines for prompt structure released by the publishers of each of the individual models. Our exact prompt structure will be released with the reference summaries. In addition to applying these models off-the-shelf, we also run a set of further supervised fine-tuning experiments with the Mistral 7B model."}, {"title": "4. Results", "content": ""}, {"title": "4.1. Zero-shot", "content": "We evaluate the quality of summaries with ROUGE-L [20]. We explored a number of other metrics, but most gave a similar ordering in terms of model performance. While we recognize the pitfalls of focusing on a single metric [21], we only report ROUGE-L due to space concerns.\nBaseline results are shown in Table 3. Each row rep-resents a different input condition. While the performance of each model becomes progressively worse as more error is introduced through MT and ASR-as opposed to using reference transcripts and translations the drop in performance is less than anticipated. The difference between a summary generated from a reference translation and the cascade of AST and MT is no more than 10% relative across all models. We believe there are two possible reasons for this result and they merit future investigation. Either the errors from transcription and translation do not impact the model's ability to summarize the key information, or the metric is not able to measure the impact.\nThe performance of the open-source models follows the ex-pected ranking, with the larger MoE model Mixtral outperform-"}, {"title": "4.2. Finetuning", "content": "In addition to testing off-the-shelf API-based and open-source models, we also fine-tune Mistral-7B to provide a much stronger baseline. Table 3 includes results from a fine-tuned model that is trained with both reference and MT of ASR inputs. Compared to the unadapted version, fine-tuning improves the performance of the Mistral-7B model by almost 10 ROUGE points. After fine-tuning, the performance of the model is com-parable to, or even outperforms, GPT-4.\nIn order to determine the value of including both the ref-erence transcript and MT of ASR inputs in fine-tuning, we run a set of experiments in which we vary our fine-tuning dataset. Table 4 shows performance, aggregated across datasets, when we fine-tune on only the reference transcripts, only the MT of ASR transcripts, and the combination of both. We also experi-ment with a fourth condition in which we include both the refer-ence transcripts and MT of ASR transcripts, but use a separate source prompt during fine-tuning and inference for the two. The"}, {"title": "5. Conclusions", "content": "We have established an evaluation framework for CTS summa-rization. Using GPT-4, we created reference summaries for a well-known Spanish CTS corpus with existing English transla-tions. Our experiments establish a baseline for a cascaded ap-proach to summarization using publicly available models. While GPT-4 outperforms existing open-source models, we are able to match the performance of GPT-4 by fine-tuning the Mistral-7B model. This demonstrates the efficacy of using large, API-based models like GPT-4 to generate evaluation and adaptation data for cross-lingual speech summarization.\nWe plan to explore several extensions to this work in the future. Moving beyond general summarization, we want to explore contextual summarization where the summary can be guided by input from the user to focus on specific information."}]}