{"title": "SEAL: Towards Safe Autonomous Driving via Skill-Enabled Adversary Learning for Closed-Loop Scenario Generation", "authors": ["Benjamin Stoler", "Ingrid Navarro", "Jonathan Francis", "Jean Oh"], "abstract": "Verification and validation of autonomous driving (AD) systems and components is of increasing importance, as such technology increases in real-world prevalence. Safety-critical scenario generation is a key approach to robustify AD policies through closed-loop training. However, existing approaches for scenario generation rely on simplistic objectives, resulting in overly-aggressive or non-reactive adversarial behaviors. To generate diverse adversarial yet realistic scenarios, we propose SEAL, a scenario perturbation approach which leverages learned scoring functions and adversarial, human-like skills. SEAL-perturbed scenarios are more realistic than SOTA baselines, leading to improved ego task success across real-world, in-distribution, and out-of-distribution scenarios, of more than 20%. To facilitate future research, we release our code and tools: https://github.com/cmubig/SEAL", "sections": [{"title": "I. INTRODUCTION", "content": "With the growing deployment of autonomous driving (AD) technologies in real-world settings, ensuring the safety of such systems has only increased in importance and public concern [1], [2]. As AD verification and validation approaches continue to evolve, scenario-based testing via datasets and simulation has emerged as a core methodology, where alternatives such as on-road testing via a sufficiently large number of miles driven can be prohibitively expensive, risky, and infeasible [3], [4]. While validation of system behavior under normal operating circumstances is valuable, testing AD behavior under safety-critical and other corner-case circumstances is vital for Safety of the Intended Functionality (SOTIF) standards [5]\u2013[7]."}, {"title": "", "content": "Scenarios are often curated in the form of large datasets of real-world recorded driving traces, providing a basis for assessing human behaviors and for training machine learning models [8]\u2013[10]. AD subsystems are then asked to perform tasks such as forecasting the future motion of various road users or controlling the behavior of a particular vehicle in a simulated reconstruction [11]\u2013[14]. However, the presence of critical scenarios in collected datasets is exceedingly low, a problem identified as the \u201ccurse-of-rarity\u201d in autonomous driving [15]\u2013[17]. Thus, programmatically generating safety-critical scenarios has become necessary. To ensure that generated scenarios retain realistic properties, it is appealing to perturb the behavior of one or more agents in a principled way, rather than using first principles to painstakingly assemble a scenario from scratch [18]\u2013[21]. In this setting, one agent is referred to as the ego agent, while the modified background traffic participants are adversary agent(s), who attempt to attack the ego in some way."}, {"title": "", "content": "State-of-the-art (SOTA) approaches in perturbation-based scenario generation have coupled a dynamic scenario generation framework with an ego control policy being trained with closed-loop objectives [21]\u2013[23], in contrast with previous less-efficient staged approaches [24], [25]. These approaches can still be sub-optimal, however, in that they can struggle to provide useful training stimuli to a closed-loop agent. In particular, we identify three key issues in recent SOTAs: 1) they have a limited view of safety-criticality, e.g., focusing only on inducing collisions or near-misses; 2) they lack reactivity to an ego agent's behavior diversity; and 3) their optimization objectives tend to maximize \u201cunrealistic\" and overly-aggressive adversarial behavior, limiting their usefulness for balanced model training."}, {"title": "", "content": "Therefore, in this paper, we propose and evaluate a method for Skill-Enabled Adversary Learning (SEAL), which yields significantly improved downstream ego behavior, in closed-loop training with safety-critical scenario generation. Our method addresses the identified limitations in prior art by introducing two novel components, as shown in Figure 1. First, we introduce a learned scoring function to anticipate how a reactive ego agent will respond to a candidate adversarial agent behavior. We quantify both collision closeness and induced ego behavior deviation, thus providing a broadened understanding of safety criticality. Second, we develop a reactive adversary policy; in particular, inspired by human cognition, we leverage a hierarchical framework that is akin to how humans operate vehicles [26] and we create an adversarial prior that selects human-like skill primitives to increase criticality while maintaining realism."}, {"title": "", "content": "Furthermore, we argue that safety-critical scenario generation approaches should be evaluated in terms of behavior realism and usability in improving ego policy development, rather than induced criticality alone. Much prior work, however, relies on evaluating ego policies by leveraging their scenario generation approach on a set of base scenarios, wherein safety-critical behavior is effectively in-distribution with respect to the training examples and the family of heuristic scene perturbations [21], [27]. Thus, we extend recent work on scenario characterization [16] to instead identify real (non-generated) yet still safety-relevant scenarios in datasets, thereby creating a realistic, out-of-distribution evaluation setting. We argue that while ego performance on in-distribution generated scenes is informative, performance on real challenging scenes ultimately matters the most."}, {"title": "", "content": "In summary, our paper comprises three main contributions:\n1) We propose two novel techniques for safety-critical per- turbation: (i) a learned scoring function, to select over candidate trajectories; and (ii) a reactive, adversarial skill policy for increased realism in adversary behavior.\n2) We design an improved evaluation setting for closed- loop training, utilizing real-world safety-relevant scenar- ios in contrast to just in-distribution generated scenes.\n3) We provide results on several key experiments, showing an increase of more than 20% in ego task success rate over SOTA baselines, across scenarios generated closed-loop by our proposed framework, across scenarios gen- erated closed-loop by previous SOTA baseline frame- works, and across real-world safety-relevant scenarios."}, {"title": "II. RELATED WORK", "content": "A. Scenario Generation in Autonomous Driving"}, {"title": "", "content": "Approaches for generating scenarios that reproduce the distribution of normal driving behavior have been extensively explored. Some methods ensure the diversity of generated traffic behavior [28], [29], while others aim for controllability through rule-based or language-driven specifications [30]\u2013[32]. However, due to the rarity of safety-critical events in recorded data [15]\u2013[17], other approaches have focused on directly generating corner-case scenarios by injecting adversarial behaviors. Earlier works in safety-critical scenario generation relied on gradient-based optimization approaches that require access to vehicle dynamics [24], [27], [33], a limitation in model-free settings. Other methods, such as diffusion-based approaches [25], [34], are compute-intensive and impractical to be used in a closed-loop manner. Efficient methods like CAT [21] and GOOSE [35], which leverage trajectory prediction priors and reinforcement learning (RL) respectively, are non-reactive to the ego agent and often prioritize simple collision objectives. In contrast, our approach efficiently generates reactive, nuanced adversarial behavior, providing a stronger training signal and improving closed-loop agent performance."}, {"title": "B. Robust Training and Evaluation in Autonomous Driving", "content": "Several techniques for robustifying AD policies against safety-critical and out-of-distribution scenarios have been explored. Formal methods, such as Hamilton-Jacobi (HJ) reachability, have been utilized in various driving tasks, but struggle with dimension scaling [36], [37]. Similarly, domain randomization has been used as a form of data augmentation (e.g., randomizing vehicle control parameters [38] or scenario initial states [39]) but requires excessive sampling to cover a sufficient domain size. Thus, adversarial training has been increasingly used, either as a fine-tuning scheme [24], [25] or in a fully closed-loop training pipeline [21], [33], providing adaptive, continuous feedback to an ego agent."}, {"title": "", "content": "Evaluation of robust training and scenario generation approaches is crucial. Many works evaluate generated scenarios against fixed rule-based or replay ego planners alone [12], [24], [34], [35], [40], offering limited insights into the efficacy of adversarial agents against more sophisticated ego agents. Additionally, adversarially-trained ego policies are often tested on scenarios perturbed by the same adversarial method used in training [21], [25], [27], [33], leading to in-distribution evaluations. Conversely, we focus on out-of-distribution evaluation of well-trained, reactive ego policies, in both adversarial scenarios perturbed by other SOTA approaches, as well as real safety-relevant scenarios."}, {"title": "", "content": "Out-of-distribution evaluation has been well-explored in AD trajectory prediction [14], [16], [41], [42], but these approaches often aim to characterize an entire scene without focusing on a single ego driver or identifying a specific adversary. In AD control tasks, some prior work has explored out-of-distribution settings, such as CARNOVEL [43], [44], which tests unseen scenario types like roundabouts. Additionally, Lu et al. [45] evaluate across real-world scenarios of various difficulty levels, but do not hold out the hardest scenes during training. Our approach thus addresses this gap by offering a more comprehensive and rigorous evaluation, across a wide set of adversarial and real-world scenarios."}, {"title": "III. PRELIMINARIES", "content": "In this section, we define relevant notation and task defi-nitions used in the rest of this paper. Let $(x, y)^{(t)}$ represent the location of an agent (i.e., vehicle, pedestrian, or cyclist) in the ground plane at some given time t. We then define an agent's trajectory as the ordered set $X = \\{(x,y)^{(t)} | t \\in \\{1, 2, ..., T\\}\\}$ over T timesteps at some fixed time delta."}, {"title": "Base Scenario:", "content": "We define a base scenario, S, as the tuple $(X, M, ego, adv)$, with $X = \\{X_i | i \\in \\{1,2, ..., N\\}\\}$, consisting of the set of all agent trajectories observed, where $X_i$ denotes the trajectory of an agent with the ID of i, and N is the total number of agents. All relevant map and scenario meta information (such as lane connectivity, traffic light locations, etc.) is given as M. Finally, ego and adv refer respectively to the agent IDs of the ego vehicle (to be controlled in simulation) and the adversarial vehicle (to be perturbed to induce criticality)."}, {"title": "Scenario Perturbation Task:", "content": "For this task, K re-simulations of a base scenario S are performed as episodes, where agents start from the same state as the base scenario and follow a behavior prescribed by some policy (i.e., a reactive policy or predefined trajectory), which may be different than their original trajectory. Let $X^{(k)}$ represent the observed trajectories in the k-th re-simulation of S. The perturbation-based safety critical scenario generation task is thus assigning behaviors to roll-out for all non-ego agents, conditioned on the base scenario S and K previous episodes, $\\{X^{(k)} | k \\in \\{1, 2, ..., K\\}\\}$, such that the resulting $X^{(K+1)}$ satisfies some specified desired properties of criticality. Importantly, we treat the ego agent's behavior as a black box: while we are able to observe previous behavior as $X_{ego}^{(k)}$, we have no access to the model or any privileged information on ego's decision-making process."}, {"title": "IV. APPROACH: SKILL-ENABLED ADVERSARY\nLEARNING FOR SCENARIO GENERATION", "content": "To increase the criticality while maintaining the realism of a scenario, we propose the Skill-Enabled Adversary Learning (SEAL) approach for perturbation-based scenario generation. We share a similar insight to CAT [21] of utilizing a probabilistic trajectory prediction model, $\\pi_{gen}$, to produce diverse candidate future paths that an adversary could take, conditioned on S. However, there are several challenges with directly providing a future path for the adversary to follow from $\\pi_{gen}(S)$:"}, {"title": "", "content": "1) Focusing solely on inducing collisions neglects a broader understanding of safety criticality, such as forc-ing hard braking maneuvers, swerving, or other such deviations from normal ego-driving behavior.\n2) Having an adversary follow a predefined path prevents reactivity to an ego agent's decision making.\n3) The heuristically-selected route often results in unrealis-tic behavior, with the adversary driving directly towards the ego vehicle in a non-human-like manner without attempting to avoid a collision."}, {"title": "", "content": "To address these challenges, we introduce two key com-ponents: a learned scoring function to select trajectories more flexibly than heuristic methods, and an adversarial skill policy that enables more reactive and human-like behavior."}, {"title": "A. Learned Scoring Function", "content": "Many previous works rely on heuristic approaches to select the best trajectory from a candidate set to be assigned to the behavior of the adversary agent, $X_{adv}^{(K+1)}$. For instance, CAT [21] compares bounding box overlaps across the pre-vious K episodes in all candidate routes, selecting the one which collides with the most previous ego roll-outs at the earliest time step or is closest to a collision, otherwise. We instead aim to select among candidate trajectories in a more flexible way that captures both closeness to collision as well as likelihood of anticipated ego behavior deviation (e.g., causing the ego to swerve or execute a hard-brake maneuver).\nWe frame the problem as a supervised regression task. First, we build a dataset of simulated outcomes, where we roll out and observe all trajectory pairs of ego and adversarial agents, $(X_{ego}^{(K+1)}, X_{adv}^{(K+1)})$. To keep ego behavior as a black-box in downstream closed-loop training, we have the ego follow a reactive heuristic policy during this stage. We then obtain ground truth values from the collected demonstrations, using the following scoring functions, similar to measure functions used in prior work [16], [19]:"}, {"title": "", "content": "$f_{coll} = exp \\left(- \\frac{1}{b} \\underset{t}{min} ||x_{ego}^{(t)} - x_{adv}^{(t)}||^2 \\right)$ (1)\n$f_{diff} = 1 - exp \\left(- \\frac{1}{b} \\underset{t}{ \\sum} ||x_{ego}^{(k-1),t} - x_{ego}^{(k),t}||^2 \\right)$ , (2)"}, {"title": "", "content": "where $b \\in \\mathbb{R}$ is a hyperparameter controlling sensitivity to distance values. Both Equation (1) and Equation (2) map to [0, 1], where 1 indicates maximal criticality and 0 indicates minimal. Equation (1) captures collision closeness between the ego and adversary over a given roll-out, while Equation (2) captures ego behavior difference between two episodes. However, instead of only assessing past episodes, we propose to predict these measures for a roll-out yet to happen by training a neural network, $\\pi_{score}$ (detailed in Section IV-C). This $\\pi_{score}$ network aims to predict $f_{coll}$ and $f_{diff}$ conditioned on a previous $X_{ego}$ and the proposed $X_{adv}^{(K+1)}$. The final score for ranking candidate trajectories"}, {"title": "", "content": "is the sum of the predicted $f_{coll}$ and $f_{diff}$ values from $\\pi_{score}$, averaged over the K previous ego roll-outs."}, {"title": "B. Adversarial Skill Learning", "content": "We design a reactive policy, $\\pi_{adv}$, to guide the adversary's behavior, unlike recent works [21], [35], where the selected adversary follows a predefined trajectory. This adversarial policy observes and acts in a closed-loop simulator along-side the ego policy. In this context, skill-based hierarchical policies are appealing approaches as they capture maneuvers at a higher abstraction, compared to the low-level actions of a simulator, corresponding more closely to how humans operate vehicles [26]."}, {"title": "", "content": "We build upon prior work [47], which utilizes expert demonstrations to extract paired observation and action se-quences as state-conditioned \"skills\" which are then embed-ded using a Variational AutoEncoder (VAE). Additionally, a state-conditioned prior network is trained to map from a state to a useful location in the VAE's latent space to be decoded into a reconstructed skill for the agent to follow."}, {"title": "", "content": "In our work, we separate the demonstrated skills into adversarial (i.e., those ending in a collision or near-miss) and benign skills (i.e., those avoiding a collision while staying on road). We then train two prior networks in parallel with a shared-skill VAE: benign skills flow through a \"benign\" prior while adversarial skills flow through an analogous \u201cadversarial\u201d prior. In this way, the adversarial agent policy, $\\pi_{adv}$, leverages the adversarial prior to select skills which are likely to lead to safety critical outcomes, while still being a demonstrated, human-like skill, thereby improving behavior realism. Figure 2 visualizes the learned skill spaces over uniformly sampled states; regions of overlap correspond to skills which may be useful to both an adversarial and benign agent (e.g., lane-keeping, smooth kinematics, etc.) while distinct regions correspond to skills only useful for that particular agent (e.g., for an adversary: cutting-off another vehicle, hard-braking in a dangerous way, etc.)."}, {"title": "", "content": "To integrate this skill module with the trajectory genera-tion and ranking discussed in Section IV-A, we first select the highest ranking candidate trajectory, $X_{adv}^{(K+1)}$. We derive goals and subgoals from this selected trajectory to provide to $\\pi_{adv}$ as navigation information. Skills are then executed in a hierarchical manner as in [47]: at the start of the episode or when a skill has completed, a new skill is selected based on the current observation and adversarial prior. The agent then decodes that skill, in a closed-loop manner, into raw actions."}, {"title": "", "content": "To further increase safety-criticality, the adversary initially exactly follows $X_{adv}^{(K+1)}$ before switching to this adversarial skill policy at a fixed offset before the anticipated point of maximal collision risk."}, {"title": "C. SEAL Implementation Details", "content": "For training and validating both the learned scoring function and skill spaces, we leverage the well-established Waymo Open Motion Dataset (WOMD) [8] dataset, as well as a subset of scenes therein labeled by Waymo as containing interacting agents. A further subset of 500 of these scenes has been used by prior work, and we henceforth refer to this set as WOMD-Normal [21], [48]. We split these scenes into 400 training and 100 evaluation scenarios."}, {"title": "", "content": "For $\\pi_{gen}$, we utilize a pre-trained DenseTNT [11] trajectory prediction model, as used by CAT. We use the MetaDrive simulator [49] and its included IDM policy [50] as the heuris-tic reactive agent to collect imperfect demonstration data, described and utilized in both Section IV-A and Section IV-B. For data augmentation, all agents in the scene follow the IDM policy and produce useful demonstrations, rather than collecting examples from solely the ego."}, {"title": "", "content": "We implement $\\pi_{score}$ as a VectorNet-style polyline en-coder [51], followed by a multilayer perceptron decoder to the predicted values of $f_{coll}$ and $f_{diff}$. We use an MSE loss objective on the sum of the two values, ensuring equal weight to both predicted measures. For $\\pi_{adv}$, we leverage the skill embedding framework from [47], with identical architectures and loss functions across our two parallel prior networks. We empirically set the hyperparameter b in Equation (1) and Equation (2) to 8, use a skill time horizon of 10, and fix K to 5 (consistent with CAT)."}, {"title": "V. EXPERIMENTAL SETUP", "content": "We leverage SEAL to generate scenes for two primary purposes: providing data augmentation during closed-loop training of reinforcement learning (RL) agent policies, and providing a means of evaluating such agents' capabilities."}, {"title": "A. Policy Training", "content": "For closed-loop training of an ego agent policy, we lever-age the WOMD-Normal set along with the MetaDrive sim-ulator [49], described in Section IV-C. Then, we follow the curriculum training approach proposed by CAT [21], where a random base scenario S from the train split is selected and has a random chance of being perturbed; this perturbation chance increases throughout the training process. Agents observe the environment via simulated LiDAR returns and navigation information based on their original destination in X. Agents act on the environment with normalized steering and acceleration forces as a; the ego and adversarial agents follow either a policy or a predefined trajectory, while all other agents follow their original trajectory in X."}, {"title": "", "content": "We utilize ReSkill [47] as our underlying RL algorithm, a recent SOTA approach in hierarchical RL. We use our skill-space built in Section IV-B, utilizing the benign prior rather than the adversarial one. The action learned by the ReSkill agent is a remediating Aa adjustment to the action decoded based on the current skill and state pair, a'. Thus, the action sent to the environment is a = a' + \u2206a. Actions are performed at a 10Hz rate, and all agents are trained for one million timesteps in total, empirically sufficient for consistent policy convergence."}, {"title": "B. Evaluation Settings", "content": "Many previous works evaluate agent performance, in-distribution, on a held-out subset of their own gener-ated scenes [21], [25], [27], [33]. For additional com-prehensiveness, we propose to utilize a recent scenario characterization approach, SafeShift [16], for identify-ing real-world safety-relevant base scenarios, denoted as WOMD-SafeShift-Hard. We start by identifying scenar-ios containing interacting agents labeled by Waymo. We then apply SafeShift's hierarchical scoring to these agents and select scenarios where the interacting agents have trajectory scores in the top 20th percentile across WOMD, randomly sampling 100 scenes therein. The ego and adversary agents are assigned to the interacting agents with the higher and lower trajectory score, respectively."}, {"title": "", "content": "We baseline SEAL against two recent SOTA safety criti-cal scenario generation approaches, that can be utilized in a closed-loop manner: CAT [21] and GOOSE [35]. CAT heuristically chooses a trajectory from $\\pi_{gen}$ to apply to the adversarial agent; we use the same $\\pi_{gen}$ function for both CAT and SEAL, for fairness. GOOSE learns to iteratively modify control points of a NURBS [52] curve fit to the original adversary's trajectory, observing the outcome of each roll-out. We train GOOSE against the MetaDrive IDM agent using the WOMD-Normal training set and GOOSE's \u201cde-celeration\" task goal-induce a collision while maintaining kinematic feasibility. For consistency, we limit the number of GOOSE policy steps (i.e., observed roll-outs) to K = 5."}, {"title": "C. Metrics", "content": "Within MetaDrive, episodes are terminated when the ego agent either arrives safely at its goal (Success), collides with another agent (Crash), or violates an off-road con-straint (i.e., crosses a road edge or yellow median; Out of Road). As such, we report these corresponding rates as the key metrics for ego performance, following prior work [21]. For evaluating generated scenario quality, we examine the induced ego Success rate, across all tested ego methods. We derive a realism metric based on distributional measures, similar to prior work [12], [13], [32]. In particular, we utilize the Wasserstein distance (WD) over adversarial \"profiles\"\u2014normalized histograms constructed from the adversary's yaw rates, acceleration values, out of road rates, and rates of collisions with non-ego agents. All WD values are compared to profiles derived from the original $X_{adv}$ in S. Furthermore, we perform a simple average over the four derived WD values to comprise an overall Realism meta-metric."}, {"title": "VI. RESULTS", "content": "We report the interquartile mean (IQM) and interquartile range (IQR) over four seeds, as recommended for statistical robustness [53]. These statistical summaries are computed independently over each metric, so Success, Crash, and Out of Road may not sum to 100%. We also evaluate a non-reactive ego replay policy (Replay), which rolls out the original $X_{ego}$ trajectory, as well as a ReSkill [47] agent trained without any adversarial scenario generation (No Adv). Note that due to re-simulation limitations, Replay in WOMD-Normal and WOMD-SafeShift-Hard may have a nonzero failure rate."}, {"title": "Downstream Performance.", "content": "Our closed-loop training results are summarized in Table I. SEAL-trained policies average a 21.5% increase in Success rate relative to the top baseline"}, {"title": "", "content": "in each evaluation setting, achieving a strong balance be-tween Crash and Out of Road rates. While a baseline-trained policy may have slightly better performance on one failure type, it is achieved by sacrificing performance against the other."}, {"title": "Scenario Generation Quality.", "content": "To directly assess scenario generation quality, we aggregate metrics in Table II, aver-aged over all ego methods. Note that WOMD-Normal and WOMD-SafeShift-Hard can have non-zero WD values due to early episode terminations from ego policies' fail-ures. Although CAT scenes induce a lower ego Success rate than SEAL scenes, SEAL scenes exhibit the highest Realism among scenario generation approaches, a 35.7% improvement, contributing to SEAL-trained policies' supe-rior downstream performance. We also showcase qualitative examples of the tested scenario generation approaches in Figure 4, against Replay. CAT and GOOSE perturbations both result in aggressive, colliding scenarios, while the SEAL perturbation creates a more nuanced near-miss scenario, reactively avoiding a direct collision."}, {"title": "Ablation Studies.", "content": "To further investigate how different components of SEAL affect downstream training, we per-form extensive ablation studies shown in Figure 3. We study the effect of our learned scoring function by com-paring it to the heuristic, bounding box overlap approach used by CAT (Learned Obj and Heuristic Obj, respectively). Similarly, we compare our adversarial skill policy (Adv Skill Prior) with a benign prior variant (Benign Skill Prior) and a predefined trajectory fol-lowing policy (TrajPred Adv). Note that the combination of TrajPred Adv with Heuristic Obj reduces to CAT. Our full SEAL approach performs the best across all evaluation settings; both the learned scoring function and adversarial skill policy appear to be needed."}, {"title": "VII. CONCLUSION", "content": "As autonomous driving (AD) systems advance, ensur-ing safety remains essential. While recent safety-critical scenario generation techniques show promise, they often lack the realism, reactivity, and nuance needed to provide strong training signals for closed-loop agents. We thus introduced Skill-Enabled Adversary Learning (SEAL) as a perturbation-based safety-critical scenario generation ap-proach, combining a learned scoring function and an adver-sarial skill policy. In all test settings across both real-world challenging scenarios and generated scenarios by SEAL and other SOTA methods\u2014SEAL-trained policies achieved significantly higher success rates, with a more than 20% relative increase. Upon deeper analysis, SEAL-generated scenes contain less aggressive but more realistic adversaries, helping to explain the observed ego agent improvements. We argue that realism metrics, downstream task utility, and out-of-distribution evaluation settings are vital in assessing adversarially-perturbed scenarios."}, {"title": "", "content": "While SEAL is quite effective, future improvements are still possible. Incorporating finer-grained objectives into the scoring function could enable more adaptive and controllable generation beyond safety criticality alone. Additionally, en-hancing realism metrics to reflect human decision-making at the skill-level could provide deeper insights into scenario quality. We encourage future work to explore these topics."}]}