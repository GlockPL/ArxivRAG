{"title": "Doob's Lagrangian: A Sample-Efficient Variational Approach to Transition Path Sampling", "authors": ["Yuanqi Du", "Michael Plainer", "Rob Brekelmans", "Chenru Duan", "Frank No\u00e9", "Carla P. Gomes", "Alan Apsuru-Guzik", "Kirill Neklyudov"], "abstract": "Rare event sampling in dynamical systems is a fundamental problem arising in the natural sciences, which poses significant computational challenges due to an exponentially large space of trajectories. For settings where the dynamical system of interest follows a Brownian motion with known drift, the question of conditioning the process to reach a given endpoint or desired rare event is definitively answered by Doob's h-transform. However, the naive estimation of this transform is infeasible, as it requires simulating sufficiently many forward trajectories to estimate rare event probabilities. In this work, we propose a variational formulation of Doob's h-transform as an optimization problem over trajectories between a given initial point and the desired ending point. To solve this optimization, we propose a simulation-free training objective with a model parameterization that imposes the desired boundary conditions by design. Our approach significantly reduces the search space over trajectories and avoids expensive trajectory simulation and inefficient importance sampling estimators which are required in existing methods. We demonstrate the ability of our method to find feasible transition paths on real-world molecular simulation and protein folding tasks.", "sections": [{"title": "1 Introduction", "content": "Conditioning a stochastic process to obey a particular endpoint distribution, satisfy desired terminal conditions, or observe a rare event is a problem with a long history (Schr\u00f6dinger, 1932; Doob, 1957) and wide-ranging applications from generative modeling (De Bortoli et al., 2021; Chen et al., 2021a; Liu et al., 2022, 2023c; Somnath et al., 2023) to molecular simulation (Anderson, 2007; Wu et al., 2022; Plainer et al., 2023; Holdijk et al., 2023), drug discovery (Kirmizialtin et al., 2012, 2015; Dickson, 2018), and materials science (Xi et al., 2013; Selli et al., 2016; Sharma et al., 2016).\nTransition Path Sampling. In this work, we take a particular interest in the problem of transition path sampling (TPS) in computational chemistry (Dellago et al., 2002; Weinan and Vanden-Eijnden, 2010), which attempts to describe how molecules transition between local energy minima or metastable states under random fluctuations or the influence of external forces. Understanding such transitions has numerous applications for combustion, catalysis, battery, material design, and protein folding (Zeng et al., 2020; Klucznik et al., 2024; Blau et al., 2021; No\u00e9 et al., 2009; Escobedo et al., 2009)."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Transition Path Sampling", "content": "Consider a forward or reference stochastic process with states $x_t$ and the density of transition probability $P_{t+dt}(Y|X_t = x) := p(x_{t+dt} = Y|x_t = x)$. Starting from an initial point $x_0 = A$, the probability density of a discrete-time path is given as\n\n$p(x_T,...,x_{dt}|x_0 = A) = \\prod_{t=dt}^{T-dt} P(X_{t+dt} | X_t) \\cdot p(x_{dt} | x_0 = A)$.\n\nThe problem of rare event sampling aims to condition this reference stochastic process on some event at time $T$, for example, that the final state belongs to a particular set $x_T \\in B$. We are interested in"}, {"title": "2.2 Doob's h-transform", "content": "Doob's h-transform addresses the question of conditioning a reference Brownian motion to satisfy a terminal condition such as $x_T \\in B$, thereby providing an avenue to solve the transition path sampling problem described above. Without loss of generality, and to provide a unified treatment of the dynamics in (3)-(4), we consider the forward or reference stochastic differential equation (SDE),\n\n$\\text{Pref}_{0:T}:\\qquad dx_t = b_t(x_t) dt + \\Xi_t dW_t, \\qquad x_0 \\sim p_0,$\n\nwith drift vector field $b_t : \\mathbb{R}^N \\to \\mathbb{R}^N$ and diffusion coefficient matrix $\\Xi_t \\in \\mathbb{R}^{N \\times N}$ such that $G_t := \\Xi_t\\Xi_t^\\intercal$ is positive definite. We denote the induced path measure as $\\text{Pref} \\in \\mathcal{P}(C([0, T] \\to \\mathbb{R}^N))$, i.e. a measure over continuous functions from time to $\\mathbb{R}^N$.\nRemarkably, Doob's h-transform (Doob, 1957; S\u00e4rkk\u00e4 and Solin, 2019, Sec. 7.5) shows that conditioning the reference process (5) on $x_T \\in B$ results in another Brownian motion process.\nProposition 1. [Jamison (1975, Thm. 2)] Let $h_B(x,t) := p_T(x_T \\in B|x_t = x)$ denote the conditional transition probability of the reference process in (5). Then,\n\n$\\text{P}_{0:T}:\\qquad dx_{t|T} = (b_t(x_{t|T}) + 2G_t\\nabla_x \\log h_B(x_{t|T}, t)) \\cdot dt + \\Xi_t dW_t \\qquad x_0 \\sim p_0$\n\nwhere we use $x_{t|T}$ to denote a conditional process. The SDE in (6) is associated with the following transition probabilities for $s < t < T$,\n\n$P_t(y|x_s = x, x_T \\in B) = \\frac{h_B(y,t)}{h_B(x,s)}P_t(y|x_s = x),$\n\nNote that all of our subsequent results hold for the case when $B$ is a point-mass, with the only change being that the h-function becomes a density, $h_B(x,t) = p_T(B|x_t = x)$.\nSee App. A.1 for proof, and note that (7) is simply an application of Bayes rule $p_t(y | x_s = x, x_T \\in B) = \\frac{p_T(x_T \\in B|x_t = y)p_t(y|x_s = x)}{p_T(x_T \\in B|x_s = x)}$ with the unconditioned or reference transition probability as the prior. Furthermore, the conditioned transition probabilities in (7) allow us to directly construct the transition path (2). Using Bayes rule, we have\n\n$p(x_{T-dt},...,x_{dt} | x_0 = A, x_T \\in B) = \\frac{h_B(x_{T-dt}, T - dt)}{h_B(A, 0)} p(x_{T-dt}...,x_{dt} | x_0 = A)$\n\nafter telescoping cancellation of h-functions and rewriting the denominator in (2) as $h_B(A, 0)$ Thus, we can solve the TPS problem by exactly solving for the h-function and simulating the SDE in (6).\nFinally, the h-process and temporal marginals $p_t(x|x_0 = A, x_T \\in B)$ of the conditioned process satisfy the following forward and backward Kolmogorov equations, which will be useful in deriving our variational objectives in the next section. Note, we use $(\\nabla_x,\\cdot) = \\text{div}(\\cdot)$ for the divergence operator, and we use $p_{t|0,T}$ to indicate the dependence on both $x_0 = A$ (via the initial condition of (8a)) and $x_T \\in B$ (via the h-transform $h_B$). See App. A.1 for the proof."}, {"title": "3 Method", "content": "We first present a novel variational objective whose minimum corresponds to the Doob h-transform in Sec. 3.1, and then propose an efficient parameterization to solve for the h-transform in Sec. 3.2."}, {"title": "3.1 Doob's Lagrangian", "content": "Consider reference dynamics given in the form of either (3) or (4), with known drift $b_t$ or energy $U$. We will restrict our attention to conditioning on a terminal rare event of reaching a given endpoint $x_T = B$, along with an initial point $x_0 = A$. We approach solving for Doob's h-transform via a least action principle where, in the following theorem, we define a Lagrangian action whose minimization yields the optimal $q_{t|0,T}(x) = p_{t|0,T}(x)$ and $v_{t|0,T}(x) = \\nabla_x \\log h_B(x, t)$ from Prop. 1 and 2.\nTheorem 1. The following Lagrangian action functional has a unique solution which matches the Doob h-transform in Prop. 2,\n\n$\\mathcal{S} = \\min_{q,v} \\int_0^T dt \\int dx q_{t|0,T}(x)\\langle v_{t|0,T}(x), G_t v_{t|0,T}(x)\\rangle,$\n\ns.t.\n\n$\\frac{\\partial q_{t|0,T}(x)}{\\partial t} = -(\\nabla_x, q_{t|0,T}(x) (b_t(x) + 2G_t v_{t|0,T}(x))) + \\sum_{ij} (G_t)_{ij} \\frac{\\partial^2}{\\partial x_i \\partial x_j} q_{t|0,T}(x),$\n\n$q_0(x) = \\delta(x - A), \\qquad q_T(x) = \\delta(x - B)$.\n\nThe optimal $q_{t|0,T}(x)$ obeys (8a), and $v_{t|0,T}(x) = \\nabla_x \\log h_B(x,t) = \\nabla_x s_B(x, t)$ obeys (8b)-(8c).\nThis objective will form the basis for our computational approach, with proof of Thm. 1 deferred to App. A.2. We proceed briefly to contextualize our variational objective and highlight several optimization challenges which will be solved by our proposed parameterization in Sec. 3.2.\nUnconstrained Dual Objective. Introducing Lagrange multipliers to enforce the constraints in (9b)-(9c) and eliminating $v_{t|0,T}$, we obtain an alternative, unconstrained version of (9a).\nCorollary 1. The Lagrangian objective in Thm. 1 which solves Doob's h-transform is equivalent to\n\n$\\mathcal{S} = \\min_q \\max_{s_B} s_B(B,T) - s_B(A, 0) - \\int_0^T dt \\int dx q_{t|0,T} \\Big[ \\frac{\\partial s_B}{\\partial t} + \\langle \\nabla s_B, G \\nabla s_B \\rangle + \\langle \\nabla s_B, b_t \\rangle + \\langle \\nabla, G \\nabla s_B \\rangle \\Big],$\n\nif $q_{t|0,T}$ satisfies (9c). Note $v_{t|0,T}(x) = \\nabla_x s_B(x,t)$, with $s_B(x,t) = \\log h_B(x,t)$ at optimality. This objective is similar to the objectives optimized by Action Matching methods (Neklyudov et al., 2023, 2024). Notably, the objective in Cor. 1 is expressed directly in terms of the (log) of the h-function for fixed conditioning information $x_T = B$. We also note that the Hamilton Jacobi-style quantity, whose expectation appears in the final term, is zero at optimality in (8c) of Prop. 2."}, {"title": "3.2 Computational Approach", "content": "We now propose a family of Gaussian (mixture) path parameterizations $q_{t|0,T}$ which overcome the computational challenges posed in the previous section, while still maintaining expressivity. We present all aspects of our proposed method in the context of the first-order dynamics (3) in Sec. 3.2.1, before presenting extensions to mixture paths and the second-order setting (4) in Sec. 3.2.2-3.2.3."}, {"title": "3.2.1 First-Order Dynamics and General Approach", "content": "Tractable Drift $v_{t|0,T}$ for Variational Doob Objective. We begin by considering a modification of the Fokker-Planck constraint in (9b), with all drift terms absorbed into a single vector field $u_{t|0,T}$,\n\n$\\frac{\\partial q_{t|0,T}(x)}{\\partial t} = -(\\nabla_x, q_{t|0,T}(x) u_{t|0,T}(x)) + \\sum_{ij} (G_t)_{ij} \\frac{\\partial^2}{\\partial x_i \\partial x_j} q_{t|0,T}(x)$.\n\nFor arbitrary $q_{t|0,T}$, solving for any $u_{t|0,T}(x)$ satisfying (12) can be a difficult optimization problem, whose solution is not unique without some cost-minimizing assumption (Neklyudov et al., 2023).\nTo sidestep this optimization, and address Challenge 2, we restrict attention to variational families of $q_{t|0,T} \\in \\mathcal{Q}$ where it is analytically tractable to calculate a vector field $u$ which satisfies (12). We first consider the family of Gaussian paths $\\mathcal{Q}_G$, in similar fashion to (conditional) flow matching methods (Lipman et al., 2022; Tong et al., 2023; Liu et al., 2023a), with proof in App. B."}, {"title": "4 Related Work", "content": "(Aligned) Schr\u00f6dinger Bridge Matching Methods. Many existing \u2018bridge matching' approaches (Shi et al., 2023; Peluchetti, 2021, 2023; Liu et al., 2022; Lipman et al., 2022; Liu et al., 2023b) for SB and generative modeling rely on convenient properties of Brownian bridges and would require calculating h-transforms to simulate bridges for general reference processes. Our conditional Gaussian path parameterization is similar to Liu et al. (2023a); Neklyudov et al. (2024), where analytic bridges are not available for SB problems with nonlinear reference drift or general costs.\nSomnath et al. (2023); Liu et al. (2023b) attempt to solve the SB problem given access to aligned data $x_0, x_T \\sim q_{\\text{data}}$ assumed to be drawn from an optimal coupling. While the method in Somnath et al. (2023) involves approximating an h-transform, their goal is to obtain an unconditioned vector field $v_t$ to simulate a Markov process. However, De Bortoli et al. (2023) use Doob's h-transform to argue the learned Markov process will not preserve the empirical coupling unless $q_{\\text{data}}$ is the optimal coupling for the SB problem, and show that an 'augmented' $v_{0,t}^{\\text{data}}$ which conditions on $x_0$ can correct this issue.\nAfter training on a dataset of $x_0, x_T \\sim q_{0,T}^{\\text{data}}$ pairs using our method, we could consider using an (augmented) bridge matching objective (Shi et al., 2023; De Bortoli et al., 2023) to distill our learned $u_{t|0,T}^{(q)}$ into a vector field $v_t$ or $v_{0,t}$ which does not condition on the endpoint. Our use of a Gaussian path parameterization with samples from a fixed endpoint coupling and no Markovization step corresponds to a simplified version of the conditional optimal control step in Liu et al. (2023a).\nTransition Path Sampling. We refer to the surveys of Dellago et al. (2002); Weinan and Vanden-Eijnden (2010); Bolhuis and Swenson (2021) for an overview of the TPS problem. Least action principles for TPS have a long history, building upon the Freidlin-Wentzell (Freidlin and Wentzell, 1998) and Onsager-Machlup (Onsager and Machlup, 1953; D\u00fcrr and Bach, 1978) Lagrangian functionals in the zero-noise limit and finite-noise cases. In particular, the Onsager-Machlup functional relates maximum a posteriori estimators or \u2018most probable (conditioned) paths' to the minimizers of an action functional similar to Thm. 1, where example algorithms include (Vanden-Eijnden and Heymann, 2008; Sheppard et al., 2008). By contrast, our approach targets the entire posterior over transition paths using an expressive variational family. While Lu et al. (2017) provide analysis for the Gaussian family, we draw connections with Doob's h-transform and extend to mixtures of Gaussians.\nShooting methods are among the most popular for sampling the posterior of transition paths. From a path that satisfies the boundary conditions (obtained, e.g., using high-temperature simulations), shooting picks points and directions to propose alterations, then simulates new trajectories and accepts or rejects using Metropolis-Hastings (MH) (Juraszek and Bolhuis, 2008; Borrero and Dellago, 2016; Jung et al., 2017; Falkner et al., 2023; Jung et al., 2023). While the MCMC corrections yield theoretical guarantees, shooting methods involve expensive molecular dynamics (MD) simulations and need to balance high rejection rates with large changes in trajectories. One-way shooting methods sample paths efficiently but yield highly correlated samples. Two-way shooting methods, which we compare to in Sec. 5, are more expensive but typically sample diverse paths faster. Recent machine learning approaches (e.g. Plainer et al. (2023); Leli\u00e8vre et al. (2023)) aim to reduce the need for MD.\nFinally, various related methods rely on iterative simulation of SDE in (10) during training to learn the control drift term. Yan et al. (2022); Holdijk et al. (2023) are motivated from the perspective of stochastic optimal control, while Das et al. (2021); Rose et al. (2021) develop actor-critic methods using closely-related ideas from soft reinforcement learning. The variational method in Das et al. (2019) optimizes the rate function quantifying the probability of the rare events, while Singh and Limmer (2023) solves the Kolmogorov backward equation to learn the Doob's h-transform. However, all of these methods may be inefficient if the desired terminal state is sampled infrequently."}, {"title": "5 Experiments", "content": "We investigate the capabilities of our approach across a variety of different settings. We first illustrate features of our method on toy potentials before continuing to real-world molecular systems, including a commonly-used benchmark system, alanine dipeptide, and a small protein, Chignolin. The code behind our method is available at https://github.com/plainerman/variational-doob. Before diving into the experiments, we introduce the evaluation procedure and baseline methods.\nEvaluation metrics. In our evaluation, we emphasize two key quantities: accuracy and efficiency. Efficiency is evaluated by the number of calls to the potential energy function, which requires extensive computation and dominates the runtime of larger molecules. For accuracy, we evaluate the log-likelihood of each sampled path and the maximum energy point (saddle point/transition state) along each sampled path. A good method samples many probable paths (i.e., high log-likelihood) and an accurate transition state (i.e., small maximum energy). See App. D for further details.\nBaselines. We compare our approach against the MCMC-based two-way shooting method with uniform point selection with variable or fixed length trajectories. We found that two-way shooting"}, {"title": "5.1 Synthetic Datasets", "content": "M\u00fcller-Brown Potential. The M\u00fcller-Brown potential is a popular benchmark to study transition path sampling between metastable states. It consists of three local minima, and we aim to sample transition paths connecting state A and state B with a circular state definition. In Fig. 2, we visualize the potential and the sampled paths and can see that the same ensemble is sampled for both our method and two-way shooting. Our method exhibits a slightly reduced variance for unlikely transitions. In Table 1, we can observe that MCMC-based methods require many potential evaluations to achieve a good result, which comes from the low acceptance rate (especially when fixing the lengths of trajectories). Our method requires fewer energy evaluations (1 million vs. 1 billion) while finding paths with similar energy and likelihood. Note that the likelihood for variable approaches has been omitted, as it is governed by the number of steps in the trajectory and cannot be compared directly.\nGaussian Mixture. We further consider a potential in which the states are separated by a symmetric high-energy barrier that allows for two distinct reaction channels. In Fig. 3, we observe that a single Gaussian path cannot model a system with multiple modes of transition paths. Nevertheless, this issue can be resolved using a mixture of Gaussian paths, with slightly increased computational cost.\nThe Case for Neural Networks. According to our empirical study, the neural network parameteriza- tion of the Gaussian distribution statistics $\\mu_{t|0,T}, \\Sigma_{t | 0,T}$ is an invaluable part of our framework. As an ablation, we consider parameterizing $\\mu_{t | 0,T}, \\Sigma_{t | 0,T}$ as piecewise linear splines whose intermediate points are updated using the same gradient-based optimizer as used for neural network training. In App. D.3, we report results comparing the W1 distance of learned marginals using neural network versus spline parameterizations, observing that splines yield inferior results even after an order of magnitude more potential function evaluations. We thus conclude that spline parameterizations are not competitive for learning transition paths and continue to focus on our neural-network approach."}, {"title": "5.2 Second-order Dynamics and Molecular Systems", "content": "Experiment Setup. We evaluate our methods on real-world high-dimensional molecular systems governed by the second-order dynamics (4): alanine dipeptide and Chignolin. Alanine dipeptide is a well-studied system of 22 atoms (66 total degrees of freedom), where the molecule can be described by two collective variables (CV): the dihedral angles $\\phi, \\psi$. Chignolin is a larger system consisting of 10 residues with 138 atoms (414 total degrees of freedom) that cannot be summarized as easily. We use an AMBER14 force field (Maier et al., 2015) implemented in OpenMM (Eastman et al., 2017) but use DMFF (Wang et al., 2023) to backpropagate through the energy evaluations."}, {"title": "6 Conclusion, Limitations and Future Work", "content": "In this paper, we propose an efficient computational framework for transition path sampling with Brownian dynamics. We formulate the transition path sampling problem by using Doob's h-transform to condition a reference stochastic process, and propose a variational formulation for efficient optimization. Specifically, we propose a simulation-free training objective and model parameterization that imposes boundary conditions as hard constraints. We compare our method with MCMC-based baselines and show comparable accuracy with lower computational costs on both synthetic datasets and real-world molecular systems. Our method is currently limited by rigidly defining states A and B to be a point mass with Gaussian noise instead of any arbitrary set. Finally, our method might be improved by accommodating variable length paths."}, {"title": "A Proofs", "content": ""}, {"title": "A.1 Proofs from Sec. 2.2 (Doob's h-Transform Background)", "content": "Proposition 1. [Jamison (1975, Thm. 2)] Let $h_B(x,t) := p_T(x_T \\in B|x_t = x)$ denote the conditional transition probability of the reference process in (5). Then,\n\n$\\text{P}_{0:T}:\\qquad dx_{t|T} = (b_t(x_{t|T}) + 2G_t\\nabla_x \\log h_B(x_{t|T}, t)) \\cdot dt + \\Xi_t dW_t \\qquad x_0 \\sim p_0$\n\nwhere we use $x_{t|T}$ to denote a conditional process. The SDE in (6) is associated with the following transition probabilities for $s < t < T$,\n\n$P_t(y|x_s = x, x_T \\in B) = \\frac{h_B(y,t)}{h_B(x,s)}P_t(y|x_s = x),$\n\nNote that all of our subsequent results hold for the case when $B$ is a point-mass, with the only change being that the h-function becomes a density, $h_B(x,t) = p_T(B|x_t = x)$.\nProof. See Jamison (1975) for a simple proof based on Ito's Lemma, assuming smoothness and strict positivity of h."}, {"title": "A.2 Proofs from Sec. 3.1 (Lagrangian Action Minimization for Doob's h-Transform)", "content": "We begin by proving Cor. 1, whose proof actually contains the initial steps needed to prove our main theorem Thm. 1. In both proofs, we omit conditioning notation $q_t \\leftarrow q_{t|0,T}$ for simplicity and assume $q_t(x)s_t(x) \\to 0$ vanishes at the boundary $x \\to \\pm \\infty$, which is used when integrating by parts in $x$.\nCorollary 1. The Lagrangian objective in Thm. 1 which solves Doob's h-transform is equivalent to\n\n$\\mathcal{S} = \\min_q \\max_{s_B} s_B(B,T) - s_B(A, 0) - \\int_0^T dt \\int dx q_{t|0,T} \\Big[ \\frac{\\partial s_B}{\\partial t} + \\langle \\nabla s_B, G \\nabla s_B \\rangle + \\langle \\nabla s_B, b_t \\rangle + \\langle \\nabla, G \\nabla s_B \\rangle \\Big],$\n\nif $q_{t|0,T}$ satisfies (9c). Note $v_{t|0,T}(x) = \\nabla_x s_B(x,t)$, with $s_B(x,t) = \\log h_B(x,t)$ at optimality. Proof. Consider the following action functional\n\n$\\mathcal{S} = \\min_{q,v} \\int dt \\int dx q_t(x)\\langle v_t(x), G_t v_t(x) \\rangle,$\n\ns.t.\n\n$\\frac{\\partial q_t(x)}{\\partial t} = -(\\nabla_x, q_t(x)(b_t(x) + 2G_tv(x))) + \\sum_{ij} (G_t)_{ij} \\frac{\\partial^2}{\\partial x_i \\partial x_j} q_t(x),$\n\n$q_0(x) = \\delta(x - A), \\qquad q_T(x) = \\delta(x - B)$.\n\nThe Lagrangian of this optimization problem is\n\n$\\mathcal{L} = \\int dt \\int dx \\Big[ q_t\\langle v_t, G_tv_t\\rangle + s_t \\Big[ \\frac{\\partial q_t}{\\partial t} + \\langle \\nabla, q_t(b_t + 2G_tv_t)\\rangle - \\sum_{ij} (G_t)_{ij} \\frac{\\partial^2}{\\partial x_i \\partial x_j} q_t \\Big] \\Big],$\n\nwhere $s_t$ is the dual variable and we omit the optimization arguments, with $\\mathcal{S} = \\min_{q,v} \\max_s \\mathcal{L}$. Swapping the order of optimizations under strong duality, we take the variation with respect to $v_t$ in an arbitrary direction $h_t$. Using $G_t = G_t^\\intercal$, we obtain\n\n$\\frac{\\delta \\mathcal{L}}{\\delta v_t}[h_t] = \\int dx q_t\\langle (G_t + G_t^\\intercal)v_t, h_t\\rangle - q_t\\langle 2G_t\\nabla s_t, h_t\\rangle = 0$\n\nSubstituting into the above, we have\n\n$\\mathcal{L} = \\int dt \\int dx s_t \\frac{\\partial q_t}{\\partial t} - q_t\\langle \\nabla s_t, G_t \\nabla s_t \\rangle + s_t\\langle \\nabla, q_t b_t\\rangle - s_t\\langle \\nabla, G_t q_t\\nabla s_t \\rangle$.\n\nIntegrating by parts in $t$ and in $x$, assuming that $q_t(x)s_t(x) \\to 0$ as $x \\to \\pm \\infty$, yields\n\n$\\mathcal{L} = \\int dx q_T s_T - \\int dx q_0 s_0 + \\int dt \\int dx -q_t \\frac{\\partial s_t}{\\partial t} - q_t\\langle \\nabla s_t, G_t \\nabla s_t \\rangle + \\langle s_t, \\nabla q_t b_t \\rangle + \\langle \\nabla s_t, G_t \\nabla q_t \\rangle\\Big]$.\n\nIntegrating by parts in $x$ again. Enforcing $q_T(x) = \\delta(x - B)$ and $q_0(x) = \\delta(x - A)$ and recalling $\\mathcal{S} = \\min_q \\max_s \\mathcal{L}$ after eliminating $v_t$, we recover the optimization in the statement of the corollary.\nTheorem. 1. The following Lagrangian action functional has a unique solution which matches the Doob h-transform in Prop. 2,\n\n$\\mathcal{S} = \\min_{q,v} \\int_0^T dt \\int dx q_{t|0,T}(x)\\langle v_{t|0,T}(x), G_t v_{t|0,T}(x) \\rangle,$\n\ns.t.\n\n$\\frac{\\partial q_{t|0,T}(x)}{\\partial t} = -(\\nabla_x, q_{t|0,T}(x) (b_t(x) + 2G_t v_{t|0,T}(x))) + \\sum_{ij} (G_t)_{ij} \\frac{\\partial^2}{\\partial x_i \\partial x_j} q_{t|0,T}(x),$\n\n$q_0(x) = \\delta(x - A), \\qquad q_T(x) = \\delta(x - B)$."}, {"title": "B Gaussian Path Parameterizations", "content": "Proposition 3. For the family of endpoint-conditioned marginals $q_{t|0", "Sigma_{t|0,T})$,\n\n$u_{t|0,T}^{(q)}(x)": "frac{\\partial \\mu_{t|0,T}}{\\partial t} + \\frac{1}{2} \\Sigma_{t|0,T} \\frac{\\partial \\Sigma_{t|0,T}^{-1}}{\\partial t} \\Sigma_{t|0,T} (x - \\mu_{t|0,T}) \\text{\n\nsatisfies the Fokker-Planck equation (12) for $q_{t|0,T}$, and diffusion coefficients $G_t = \\Xi_t \\Xi_t^\\intercal$.\nProof. Consider the following identities for the Gaussian family of marginals $q_t(x) = \\mathcal{N}(x|\\mu_t, \\Sigma_t)$, where we omit conditioning $q_t \\leftarrow q_{t|0,T}$ for simplicity of notation,\n\n$\\log q_t(x) = -\\frac{1}{2} (x - \\mu_t)^\\top \\Sigma_t^{-1} (x - \\mu_t) - \\frac{1}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\text{det} \\Sigma_t,$\n\n$\\nabla_x \\log q_t (x) = -\\Sigma_t^{-1}(x - \\mu_t),$\n\n$\\frac{\\partial}{\\partial t} \\log q_t(x) = -\\frac{1}{2} (x - \\mu_t)^\\top \\Sigma_t^{-1} \\frac{\\partial \\mu}{\\partial t} - \\frac{1}{2} \\frac{\\partial \\mu}{\\partial t} \\Sigma_t^{-1} (x - \\mu_t) - \\frac{1}{2} \\text{tr} \\Big( \\Sigma_t^{-1} \\frac{\\partial \\Sigma_t}{\\partial t} \\Big)$.\n\nWe begin by solving for a vector field $u_t(x)$ that satisfies the continuity equation (where $u$ denotes the drift of an ODE)\n\n$\\frac{\\partial q_t}{\\partial t} = - \\langle \\nabla_x, q_t u_t \\rangle = -q_t \\langle \\nabla_x, u_t \\rangle - \\langle \\nabla_x"}]}