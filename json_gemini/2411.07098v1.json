{"title": "A Multi-Agent Approach for REST API Testing with Semantic Graphs and LLM-Driven Inputs", "authors": ["Myeongsoo Kim", "Tyler Stennett", "Saurabh Sinha", "Alessandro Orso"], "abstract": "As modern web services increasingly rely on REST APIs, their thorough testing has become crucial. Furthermore, the advent of REST API specifications such as the OpenAPI Specification has led to the emergence of many black-box REST API testing tools. However, these tools often focus on individual test elements in isolation (e.g., APIs, parameters, values), resulting in lower coverage and less effectiveness in detecting faults (i.e., 500 response codes). To address these limitations, we present AutoRestTest, the first black-box framework to adopt a dependency-embedded multi-agent approach for REST API testing, integrating Multi-Agent Reinforcement Learning (MARL) with a Semantic Property Dependency Graph (SPDG) and Large Language Models (LLMs). Our approach treats REST API testing as a separable problem, where four agents\u2014API, dependency, parameter, and value\u2014collaborate to optimize API exploration. LLMs handle domain-specific value restrictions, the SPDG model simplifies the search space for dependencies using a similarity score between API operations, and MARL dynamically optimizes the agents' behavior. Evaluated on 12 real-world REST services, AutoRestTest outperforms the four leading black-box REST API testing tools, including those assisted by RESTGPT (which augments realistic test inputs using LLMs), in terms of code coverage, operation coverage, and fault detection. Notably, AutoRestTest is the only tool able to identify an internal server error in Spotify. Our ablation study underscores the significant contributions of the agent learning, SPDG, and LLM components.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern web services increasingly depend on REST (Representational State Transfer) APIs for efficient communication and data exchange [1]. These APIs enable seamless interactions between various software systems using standard web protocols [2]. REST APIs function through common Internet methods and a design that allows the server and client to operate independently [3]. The prevalence of REST APIs in the tech industry is evident from platforms like APIs Guru [4] and Rapid API [5], which host extensive collections of API specifications.\nIn recent years, various automated testing tools for REST APIs have been developed [6]\u2013[17]. These tools follow a sequential process: selecting an operation to test, identifying operations that depend on the selected operation, determining parameter combinations, and assigning values to those parameters. Feedback from response status codes is then used to adjust the exploration policy at each step, either encouraging or penalizing specific choices. Although significant research has been dedicated to optimizing each individual step\u2014operation selection, dependency identification, parameter selection, and value generation\u2014these tools treat each step in isolation rather than as part of a coordinated testing strategy. This isolated approach can result in suboptimal testing strategies and a high number of invalid requests. For instance, one endpoint might benefit from extensive parameter exploration, which another might require focusing on historically successful parameter combinations.\nConsequently, these tools tend to suffer from low coverage, especially for large REST services (e.g., Language Tool, Genome Nexus, and Market in the ARAT-RL evaluation [13], and Spotify and OhSome in the NLP2REST evaluation [18]). To address these limitations, we propose AutoRestTest, a new approach that integrates a Semantic Property Dependency Graph (SPDG) and Multi-Agent Reinforcement Learning (MARL) with LLMs to enhance REST API testing. Instead of traversing all operations to find dependencies by matching input/output properties, AutoRestTest uses an embedded graph that prioritizes the properties by calculating the cosine similarity between input/output names.\nSpecifically, AutoRestTest employs four specialized agents to optimize the testing process. The dependency agent manages and utilizes the dependencies between operations identified in the SPDG, guiding the selection of dependent operations for API requests. The operation agent selects the next API operation to test, prioritizing operations likely to yield valuable test results based on previous results, such as successfully processed dependent operations. The parameter agent chooses parameter combinations for the selected operation to explore different configurations. Finally, the value agent manages the generation of realistic parameter values, using dependent operations (if available), default values, or values generated by LLMs with few-shot prompting [19].\nAutoRestTest allows its agents to collaborate to optimize the testing process using the multi-agent value decomposition Q-learning approach [20], [21]. When selecting actions, each agent is employed independently using the epsilon-greedy strategy for exploitation-exploration balancing. However, during policy updates using the Q-learning equation, AutoRestTest uses value decomposition to consider the joint actions across all agents. Through centralized policy updates, each agent converges toward selecting the optimal action while accounting for the actions of the other agents.\nWe evaluated AutoRestTest on 12 real-world RESTful services used in previous studies [13], [18], including well-known services such as Spotify, and compared its performance with that of four state-of-the-art REST testing tools recognized as top-performing tools in recent studies [13], [15], [22], [23]: RESTler [7], EvoMaster [24], MoRest [15], and ARAT-RL [13]. To ensure a fair comparison, we used the enhanced version of specifications generated by RESTGPT [25], which augments realistic testing inputs using LLMs to the REST API documents. To measure effectiveness, we used code coverage for the open-source services, operation coverage for the online services, and internal server error-detection ability for all the services. These are the most commonly used metrics in this field [22], [26].\nAutoRestTest demonstrated superior performance across all coverage metrics compared to existing tools. It achieved 58.33% method coverage, 32.06% branch coverage, and 58.25% line coverage, significantly outperforming ARAT-RL, EvoMaster, RESTler, and MoRest by margins of 12-27%. For closed-source services, AutoRestTest processed 25 API operations compared to 9-11 operations by other tools. These results provide evidence that AutoRestTest is more effective in providing comprehensive API testing than the other tools.\nIn fault detection, AutoRestTest identified 42 operations with internal server errors, outperforming ARAT-RL (33), EvoMaster (20), MoRest (20), and RESTler (14). Notably, AutoRestTest was the only tool that detected an internal server error for Spotify [27]. We reported the errors for FDIC, OhSome, and Spotify, which are actively maintained. The OhSome error has been confirmed and fixed [28], whereas we are still waiting for feedback from the developers for the other ones.\nAblation studies revealed the critical role of each component. Removing temporal difference Q-learning caused the largest performance drop, with method, line, and branch coverage falling to 45.6%, 18.2%, and 45.9% respectively. SPDG removal reduced coverage to 46.7%, 18.7%, and 47.6%, while LLM removal led to 47.4%, 19.3%, and 45.8%. Overall, removing any component decreased coverage by 10.7-13.9%, demonstrating each component's importance.\nThe main contributions of this work are:\n\u2022 A novel REST API testing technique that (1) reduces the operation dependency search space using a similarity-based graph model, (2) employs Multi-Agent Reinforcement Learning to consider optimization among the testing steps, and (3) leverages Large Language Models to generate realistic test inputs.\n\u2022 Empirical results showing that AutoRestTest can outperform current state-of-the-art REST API testing tools, enhanced by the state-of-the-art specification tool, by covering more operations, achieving higher code coverage, and triggering more service failures.\n\u2022 An artifact including the AutoRestTest tool, the benchmark services used in the evaluation, and detailed empirical results, which serves as a comprehensive resource for further research and application [29]."}, {"title": "II. BACKGROUND AND MOTIVATING EXAMPLE", "content": "REST APIs are a type of web API that conform to RESTful architectural principles [1]. These APIs enable data exchange between client and server through established protocols like HTTP [30]. The foundation of REST lies in several key concepts: statelessness, cacheability, and a uniform interface [3]. In RESTful services, clients interact with web resources by making HTTP requests. These resources represent various types of data that a client might wish to create, read, update, or delete. The API endpoint, defined by a specific resource path and an HTTP method, determines the action on the resource. Common HTTP methods include POST (create), GET (read), PUT (update), and DELETE (delete). Each pairing of an endpoint with an HTTP method constitutes an operation.\nWhen a web service processes a request, it responds with headers, a body (if applicable), and an HTTP status code indicating the result. Successful operations typically return 2xx codes, while 4xx codes denote client-side errors, and a 500 status code indicates an internal server error.\nThe OpenAPI Specification (OAS) [31], which evolved from Swagger [32] in version 2 to the OAS in version 3, is a crucial standard in the realm of RESTful API design and documentation. As an industry-standard format, OAS delineates the structure, functionalities, and anticipated behaviors of APIs in a standardized and human-accessible way."}, {"title": "C. Large Language Models", "content": "Large Language Models (LLMs), like the Generative Pre-trained Transformer (GPT) series, are at the forefront of advancements in natural language processing (NLP) [33]. LLMs, trained on extensive text collections, can understand, interpret, and generate human-like text [34]. The GPT series, including GPT-3, exemplifies these advanced LLMs, showcasing remarkable skill in producing human-like text for various applications, from education to customer service [19], [35], [36]. Their versatility in handling diverse language-related tasks, from writing coherent text and translating languages to test case generation [37], highlights their advanced capabilities in human-like understanding [34]."}, {"title": "D. Multi-Agent Reinforcement Learning", "content": "Reinforcement learning (RL) is a branch of machine learning where an agent learns to make decisions by interacting with its environment [38]. In this process, the agent selects actions in various situations (states), observes the outcomes (rewards), and learns to choose the best actions to maximize the cumulative reward over time. RL involves a trial-and-error approach, where the agent discovers optimal actions by experimenting with different options and adjusting its strategy based on the observed rewards. Additionally, the agent must balance between exploring new actions to gain more knowledge and exploiting known actions that provide the best reward based on its current understanding. This balance between exploration and exploitation is often controlled by parameters, such as  in the e-greedy strategy [38].\nMulti-Agent Reinforcement Learning (MARL) is an advanced extension of reinforcement learning that involves multiple agents interacting in a shared environment to achieve individual or collective goals [20]. MARL addresses the complexities of agent interactions, including cooperation, competition, and communication [39]. Techniques such as cooperative learning, competitive learning, and communication and coordination methods enable agents to develop optimal strategies. Applications of MARL span various domains, including autonomous driving and robotic coordination, where agents must work together or compete to optimize their performance [40]\u2013[42]. MARL is expected to be used in areas requiring complex multi-agent decision-making [43]."}, {"title": "E. Motivating Example", "content": "Figure 1 depicts the /register endpoint in the Market API's OpenAPI specification. This endpoint is vital for creating user credentials needed in other components of the API. Current REST API testing tools struggle to generate valid requests for this operation due to the strict interparameter dependencies and value restrictions. Moreover, these tools fail to effectively prioritize information gained from the operation success in subsequent requests.\nAutoRestTest addresses these issues through structured compartmentalization involving REST agents, LLMs, the SPDG, and MARL. The REST parameter agent uses reinforcement learning to discover successful parameter sets, like email, name, and password. Through updating its policy, the parameter agent learns which parameter combinations lead to successful registration while avoiding invalid combinations, significantly improving request efficiency. The value agent employs hierarchical learning to identify the optimal data source for the /register endpoint. Recognizing it as a starting point for dependency sequences, it uses LLMs to create context-aware values that satisfy the specification's strict validation patterns. Upon success, the dependency agent uses its embedded edges from the SPDG to ensure the user information is propagated to subsequent requests. The operation agent prioritizes dependent endpoints like /customer/cart and /customer/orders, while the parameter and value agents leverage their experience from the /register endpoint to optimize requests to these operations. MARL orchestrates the collaboration of these agents, using value decomposition to ensure the joint actions from all agents result in the correct configuration of the API test sequence."}, {"title": "III. OUR APPROACH", "content": "Figure 2 illustrates the architecture of AutoRestTest, highlighting its core components: the SPDG, the REST agents, and the request generator. The overall workflow consists mainly of two phases: initialization and testing execution."}, {"title": "1) Initialization Phase:", "content": "The process begins with parsing the OpenAPI specification to extract endpoint information, parameters, and request/response schemas. Using this parsed specification, the Dependency Agent constructs the SPDG, where edges represent potential dependencies between operations based on semantic similarity. These initial dependencies are later validated and refined through the testing process based on actual server responses.\nThe REST Agents (Operation, Parameter, Value, and Dependency Agents) are then initialized with their respective Q-tables. Each agent serves a specific purpose in the testing process: the Operation Agent selects API operations to test, the Parameter Agent determines parameter combinations, the Value Agent generates appropriate parameter values for each parameter, and the Dependency Agent manages operation dependencies from the SPDG."}, {"title": "2) Testing Execution Phase:", "content": "The testing execution follows an iterative process. In each iteration, the Operation Agent first selects the next API operation based on its learned Q-values and exploration strategy. Next, the Parameter Agent determines which parameters to include, considering both required and optional parameters from the specification. The Value Agent then generates parameter values using dependencies identified by the Dependency Agent, LLM-generated values, or default assignments for basic parameter types. Using the SPDG, the Dependency Agent identifies any dependencies between the selected parameters and those used in previous operations. Finally, the Request Generator constructs the API request, with the mutator component modifying 20% of requests to test error handling and trigger potential 500 response codes.\nOnce the request is sent to the Service Under Test (SUT) and a response is received, the response is used to update the Q-tables of all agents through reinforcement learning, validate and refine SPDG dependencies, and store any 500 responses for the final testing report. This cycle continues until the testing criteria are met, with the SPDG being continuously refined through multi-agent reinforcement learning. Identified dependencies are validated using server responses (e.g., 2xx codes for successfully processed requests), helping to ensure the accuracy of the dependency graph over time."}, {"title": "B. Q-Learning and Agent Policy", "content": "Both the SPDG and REST agent modules use the Q-learning algorithm [44] with value decomposition within their respective agents. During initialization, each agent creates a Q-table data structure that maps available actions to their expected cumulative rewards. When request generation begins, AutoRestTest addresses the two primary components of Q-learning\u2014action selection and policy optimization\u2014to facilitate effective communication between agents."}, {"title": "1) Action Selection:", "content": "During action selection, agents independently choose between exploiting their best-known option or exploring new options randomly. To balance these choices, all agents follow an epsilon-greedy strategy: with probability $\\epsilon$, the agent selects a random action (exploration), and with probability $1 - \\epsilon$, it selects the action with the highest Q-value (exploitation), likely yielding the best results.\nAutoRestTest utilizes epsilon-decay to guarantee all actions are adequately explored in its initial stages. Starting with an epsilon value of 1.0, this value decreases linearly to 0.1 over the duration of the tool's operation. This strategy, commonly used in practice, has been shown to improve performance by balancing exploration and exploitation [45]."}, {"title": "2) Policy Optimization:", "content": "After receiving a response from the server, agents update their Q-table values using the temporal difference update rule of the Q-learning algorithm, derived from the Bellman equation [38]. This update aims to maximize the expected cumulative reward for each action taken. The Q-learning update rule is given by:\n$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\delta$\nwhere $\\delta$ represents the temporal difference error, calculated as:\n$\\delta = r + \\gamma \\max_{a'} Q(s', a') \u2013 Q(s,a)$\nwhere $\\alpha$ is the learning rate, $\\gamma$ is the discount factor, $r$ is the received reward, $s$ is the current state, $s'$ is the next state, $a$ is the current action, and $a'$ is an action in state $s'$.\nGiven the complexity of the multi-agent environment, AutoRestTest leverages value decomposition to optimize the joint cumulative reward, which has demonstrated improvements for policy acquisition over independent learning. This approach assumes that the joint Q-value can be decomposed additively as follows [21]:\n$Q(s, a) = \\sum_{i=1}^n Q_i(s, a_i)$\nwhere $Q_i$ and $a_i$ represent the Q-value and action for agent $i$ in shared state $s$."}, {"title": "Using this redefined temporal difference error, each agent updates its Q-table to converge towards the optimal joint Q-value, as depicted in the following temporal difference equation:", "content": "$Q_i(s, a_i) \\leftarrow Q_i(s, a_i) + \\alpha\\left[r + \\gamma \\max_{a'}\\sum_{i=1}^n Q_i(s', a'_i) - \\sum_{i=1}^n Q_i(s, a_i)\\right]$\nThis value decomposition approach enables each agent to select actions independently while maintaining centralized policy updates, simplifying the implementation while enhancing coordination across agents [21].\nFor reward delegation, the dependency, value, and parameter agents are optimized to reward actions that generate 2xx status codes, whereas the operation agent rewards the selection of operations that generate 4xx and 5xx status codes. This balance in behavior is intended to maximize coverage by encouraging repeated attempts at creating successful requests for operations that frequently yield 4xx and 5xx status codes. For the hyperparameters $\\alpha$ and $\\gamma$, we used 0.1 and 0.9, respectively, as used in ARAT-RL [13] and other works [46], [47]."}, {"title": "C. Semantic Property Dependency Graph", "content": "The construction of the SPDG begins with parsing the OpenAPI specification to extract information about API endpoints, parameters, and request/response schemas. The specification parser extracts this information and initializes the graph structure.\nAlgorithm 1 describes the SPDG initialization using the OpenAPI specification of the SUT. In line 3, it begins by parsing the specification to extract details such as endpoints, parameters, and request/response schemas, which are stored in a structured format for further processing. Lines 4-9 then iterates through each operation in the API, creating corresponding nodes in the graph with the operation's ID, parameters, and responses. Dependencies between operations are identified by initializing an empty list for similarity scores and comparing each node with every other node using cosine similarity with pre-trained word embeddings (e.g., GloVe [48]). If the similarity score exceeds a threshold value (0.7), the operations are considered dependent, and the dependency is added as an edge in the graph. In line 17, if no similarity exceeds the threshold, the algorithm connects the node to the top-five most similar nodes to ensure adequate linkage. Finally, in line 22 the SPDG is returned.\nThe dependency agent manages and utilizes the dependencies between operations identified in the SPDG. It uses a Q-table to represent the validity of these dependencies, functioning similarly to a weighted graph, where edges are assigned values that reflect confidence in each dependency. Specifically, the Q-table encodes edges from the SPDG, categorizing dependencies by parameter type (query or body) and target (parameters, body, or reponse). Higher Q-values on an edge indicate greater confidence in the reliability of that operation dependency, motivating the agent to prioritize these relationships on future requests.\nFor each parameter and body property in a selected operation, the dependency agent refers to its Q-table to identify a dependent parameter, body, or response, as well as the associated operation ID. The dependency agent then consults AutoRestTest's tables storing successful parameters, request body properties, and decomposed responses to ensure that the selected dependency has available values. Notably, AutoRestTest recursively deconstructs response objects, allow-", "title_1": "1) Dependency Agent:"}, {"title": "D. REST Agents", "content": "The operation agent is tasked with selecting the next API operation to test. It employs reinforcement learning to prioritize operations that are likely to yield valuable test results based on prior experiences. The agent's state is binary, with its action space encompassing all possible operations. Each operation's Q-value is initialized to 0 and is updated based on response codes from the server. The Q-table for the operation agent stores cumulative rewards representing the proportion of unsuccessful requests for each operation in the provided service.\nThe operation agent acts as the forerunner of the REST API testing process, selecting an operation that dictates the state of the remaining agents. While the remaining agents coordinate to generate valid test cases for a given operation, the operation agent is tasked with identifying unsuccessful operations for retesting. Consequently, while the remaining agents update their Q-value using the value decomposition temporal difference equation discussed in Section III-B, the operation agent updates its Q-values independently using a structured reward system: +2 for server errors (5xx), +1 for client errors (4xx), -1 for successful responses (2xx), -3 for authentication failures (401), and 10 for invalid methods (405). This reward structure encourages the agent to prioritize exploring problematic endpoints while severely penalizing systematically invalid requests and those with consistently successful configurations.\nConsider the customer registration endpoint in the Market API, shown in Figure 1. The Q-value associated with the endpoint starts at 0. After initial attempts at processing the operation fail, the Q-table value might increase (e.g., to 1), prompting the agent to prioritize further testing on this endpoint. Conversely, a successful test case will decrease the Q-value, directing the agent to explore other challenging endpoints.\nThe parameter agent is responsible for selecting parameters for the chosen API operation. It ensures that parameters used across requests are both valid and varied, covering a range of testing scenarios while addressing interparameter dependencies. For each operation, the parameter agent initializes a state containing the operation ID,", "title_1": "1) Operation Agent:"}, {"title": "available parameters, and required parameters, and defines its action space as possible parameter combinations. The Q-values associated with each state-action pair are initialized to 0.", "content": "Consider the Market API's customer registration endpoint shown in Figure 1. The parameter agent initializes with the following state: s = {createCustomerUsingPOST, [email, name, password, links], [email, name, password]}, where the first array contains all available parameters from the request body schema, and the second list contains the required parameters according to the endpoint's specification. The agent initializes a Q-table for this operation, mapping various parameter combinations to Q-values.\nThis setup ensures that all possible arrangements of parameters (up to 10 by default, to account for space restrictions) are sufficiently represented in the agent's action space. The agent updates its Q-values using the value decomposition temporal difference equation discussed in Section III-B and the following reward structure: -1 for server errors (5xx), -2 for client errors (4xx), and +2 for successful responses (2xx). Importantly, unused parameter combinations receive a neutral update (effectively 0) in the Q-learning process, maintaining their initial Q-values. These unused combinations are prioritized over options that received negative rewards and are deprioritized relative to those with positive rewards. For example, in the registration endpoint, if the parameter combination (email, name, password) consistently yields positive rewards, the unused combination (email, name, password, links) retains its initial Q-value and may be selected during exploration to test scenarios with optional parameters. This strategy ensures comprehensive coverage across all parameter combinations.\nFor the registration endpoint, its Q-values might evolve as follows:\n\u2022 (email, name, password): 0.8 \u2013 Complete required parameter set\n\u2022 (email, name, password, links): 0.3 All parameters, including optional ones\nThese Q-values suggest that the inclusion of the links parameter presents challenges, resulting in a lower Q-value for this combination.\nThrough this reward structure, AutoRestTest effectively identifies valid parameter sets and addresses challenges related to undocumented interparameter dependency requirements, particularly in complex scenarios like user registration where certain parameters must be present and correctly formatted."}, {"title": "is responsible for generating and assigning values to parameters specified by the parameter agent. For each parameter in an operation, the value agent maintains a state containing the operation ID, parameter name, parameter type, and OpenAPI schema constraints, with its actions corresponding to possible data sources for parameter value assignment. The Q-values for each state-action pair are initialized to 0.", "content": "Consider the Market API's customer registration endpoint in Figure 1. The value agent initializes with the following states:\n\u2022 For email: s = {createCustomerUsingPOST, email, string, {pattern: \u201c[\\w-]+(\\.[\\w-]+)*@([\\w-]+\\.)+[a-zA-Z]+$\u201d}}\n\u2022 For name: s = {createCustomerUsingPOST, name, string, {pattern: \u201c[\\pL \u2018-]+$\u201d, maxLength: 50}}\n\u2022 For password: s = {createCustomerUsingPOST, password, string, {pattern: \u201c[a-zA-Z0-9]+$\u201d, minLength: 6, maxLength: 50}}\nTo generate a diverse set of inputs, the value agent can select from the following data sources:\n\u2022 Operation Dependencies: When selected, the value agent collaborates with the dependency agent to map dependent values to the selected parameter. For example, the registration endpoint might reuse email addresses from prior successful registrations to test duplicate user scenarios.\n\u2022 LLM Values: If chosen, the value agent creates (or parses if already created) values using few-shot prompting with LLMs [19]. For instance, the LLM might generate \u201cjohn.doe@example.com\u201d for the email parameter based on the specified pattern.\n\u2022 Default Assignments: When this option is chosen, the value agent assigns a basic default value according to the selected parameter's type. For example, the name parameter might receive values like \u201cJohn Smith\u201d that match the pattern, while simpler parameters without constraints might default to \u201cString\u201d for strings or 1 for integers.\nOnce a request is completed with values from the chosen data source, the agent updates its Q-values based on the temporal difference equation, using the same reward strategy as the parameter agent to refine its value generation strategy.\nFor instance, when testing the registration endpoint, the Q-values across parameters might average as follows:\n\u2022 LLM: 0.5\n\u2022 DEFAULT: 0.2\n\u2022 DEPENDENCY: -0.7\nIn analyzing these Q-values, we observe that since the registration endpoint is required for account creation, it has little dependencies to derive values from, which results in a lower Q-value for the dependency source. While default values are effective for simpler fields like the name parameter, the LLM-generated values perform better for complex pattern-constrained fields such as email and password."}, {"title": "E. Request Generator", "content": "The request generator is responsible for constructing and dispatching API requests to the SUT. It works closely with the REST agents to ensure that the generated requests are both effective and comprehensive. Upon receiving responses from the SUT, the response handler processes these results and provides feedback to the REST agents, allowing refinement of future requests.\nThe mutator's purpose is to generate invalid requests through slight altercations to the optimized request input, uncovering unexpected behaviors like 500 responses. This is a crucial part of REST API testing frameworks, as state-of-the-art tools employ similar strategies such as mutating parameter types, values, and headers (e.g., using an invalid content type in the header). The mutator follows these conventions and mutates 20% of the generated requests, a strategy used by the most recent tool [13].\nInteraction with REST Agents. The request generator interacts with the REST agents to construct API requests, relying on them for detailed information about the operation to test, the parameters to use, and appropriate values for those parameters. Specifically:\n\u2022 The operation agent selects the API operation to test.\n\u2022 The parameter agent identifies and optimizes the parameters for the chosen operation.\n\u2022 The value agent generates realistic and effective values for the parameters.\nUsing this information, the request generator constructs a complete API request.\nInteraction with the SUT. After constructing the API request, the request generator dispatches it to the SUT, which processes the request and returns a response. The response handler then analyzes this response to detect any errors or unexpected behaviors. Insights from these analyses are fed back to both the REST agents and the dependency agent, allowing them to refine their strategies for future requests.\nThe interactions between the dependency module, the REST agents, the request generator, and the SUT establish a robust feedback loop that enhances the overall effectiveness of the testing process. This collaborative approach ensures that the generated requests are not only comprehensive but also tailored to uncover potential issues within REST APIs."}, {"title": "IV. EVALUATION", "content": "In this section, we present the results of empirical studies conducted to assess AutoRestTest. Our evaluation aims to address the following research questions:\n1) RQ1: How does AutoRestTest compare with state-of-the-art REST API testing tools in terms of achieved code coverage and operation coverage?\n2) RQ2: In terms of error detection, how does AutoRestTest perform in triggering 500 (Internal Server Error) responses compared to state-of-the-art REST API testing tools?\n3) RQ3: How do the main components of AutoRestTest\u2014MARL, SPDG, and LLM-based input generation\u2014contribute to its overall performance?"}, {"title": "Experiment Setup", "content": "The experiments were conducted on two cloud VMs, each equipped with a 48-core Intel(R) Xeon(R) Platinum 8260 processor with 128 GB RAM. To ensure consistent test conditions, we restored the database and restarted the web service before each test to eliminate potential dependency conflicts. Default settings for the database and web service were used for each REST API. We allocated dedicated resources to each service and testing tool, running them sequentially to prevent interference. Throughout the experiments, we closely monitored CPU and memory usage to ensure optimal performance without encountering resource constraints.\nFor a comprehensive evaluation, we used the same set of REST API testing tools and services that ARAT-RL used [13], which is the latest REST API testing tool to our knowledge. Moreover, it has the largest benchmark dataset among the tools we considered. Specifically, Morest [15] considered six services, RESTler used one service, and EvoMaster used five services, while the ARAT-RL benchmark dataset included 10 RESTful services.\nIn addition to these services, we included the services from the RESTGPT study [25]. Out of the total 16 services, we excluded SCS and NCS because they were written by EvoMaster's authors, and we aimed to avoid potential bias. We also excluded OCVN due to authentication issues. Lastly, we excluded OMDB, which is a toy online service with only one API operation that all testing tools can process in a second. Ultimately, we used 12 services: Features Service, Language Tool, REST Countries, Genome Nexus, Person Controller, User Management Microservice, Market Service, Project Tracking System, OhSome, YouTube-Mock, and Spotify. Table I lists the open-source services along with the lines of code and the number of API operations in each service.\nFor a fair comparison, since our tool utilizes LLM calls, we used the enhanced specification generated by RESTGPT, which augments realistic testing inputs to the specification using LLMs. Moreover, we used GPT-3.5-Turbo as RESTGPT utilized this model. From a recent survey that describes the settings and metrics for REST API testing [26], we decided to use a one-hour time budget with ten repetitions to get the results. To measure the effectiveness and error-finding ability, we decided to use code coverage for the open-source services, the number of successfully processed operations in the specification, and the number of 500 status codes, which are the most popular metrics. To collect code coverage, we used Jacoco [53]. To collect the number of processed operations, we used the script from the NLP2REST repository [54]. To collect the number of errors (e.g., 500 status codes), we used the script available in the ARAT-RL repository [55]. This script collects 500 status codes by tracking the HTTP responses, and remove the duplicated 500 codes using the server response message for each operation."}, {"title": "RQ1: Effectiveness", "content": "The effectiveness of AutoRestTest is evaluated based on its ability to comprehensively cover more code compared to other tools. Figure 3 illustrates the line, branch, and method coverage achieved by each testing tool on the 9 open-source services in our benchmark; additionally, it shows the average coverage across these APIs."}, {"title": "RQ2: Fault-Detection Capability", "content": "The fault-detection capability of AutoRestTest is evaluated by analyzing its ability to identify faults, particularly 500 Internal Server Errors, in the services. Table III shows the number of 500 Internal Server Errors detected by AutoRestTest and compares it with ARAT-RL, EvoMaster, MoRest, and RESTler. As the data in the table show, AutoRestTest detected a total of 42 500 Internal Server Errors across the evaluated REST APIs, far outperforming the other tools on this metric.\nAutoRestTest's superior fault-detection ability is substantiated by its performance on the services in our benchmark. For instance, it identified significantly more errors in the OhSome service (20 errors) compared to ARAT-RL (12 errors), with none detected by EvoMaster, MoRest, or RESTler. Additionally, AutoRestTest was the only tool to detect an error in the Spotify service. It is important to note that both OhSome and Spotify are active services; Spotify, for example, has 615 million users, and the OhSome service has recent GitHub commits. We reported the detected bugs, and the OhSome errors are accepted and currently waiting for Spotify's response [27], [28]. This improvement in fault-detection is expected as AutoRestTest achieves the highest coverage among its peers, which is strongly correlated with fault-finding ability in REST API testing [22].\nTo illustrate the utility of AutoRestTest's specific components in fault-detection, first consider the following sequence of operations in the Ohsome service showcasing the capabilities of the Semantic Property Dependency Graph (SPDG). AutoRestTest begins by successfully querying the POST /elements/area/ratio endpoint from the Ohsome service with its filter2 parameter assigned to node:relation. Subsequently, AutoRestTest attempts the GET /users/count/groupBy/key endpoint, where the dependency agent applies the SPDG's semantically-created dependency edges to identify a potential connection between the"}, {"title": "filer parameter of the new operation and the filter2 parameter of the previous operation. When the dependency agent reuses the node: relation value from the previously successful filter2 parameter in the new request, the SPDG uncovers an unexpected 500 Internal Server Error. Typically, an invalid filter value would trigger a client error, but this server-side error indicates that the SPDG identified a deeper, unanticipated fault in the server's value handling. Other tools would overlook the correlation between the two parameters as a result of the minute naming differences or improper dependency modeling, and would struggle to expose this error.", "content": "In another scenario, AutoRestTest demonstrates the prowess"}]}