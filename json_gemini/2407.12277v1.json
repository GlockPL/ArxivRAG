{"title": "Multimodal Reranking for Knowledge-Intensive Visual Question Answering", "authors": ["Haoyang Wen", "Honglei Zhuang", "Hamed Zamani", "Alexander Hauptmann", "Michael Bendersky"], "abstract": "Knowledge-intensive visual question answering requires models to effectively use external knowledge to help answer visual questions. A typical pipeline includes a knowledge retriever and an answer generator. However, a retriever that utilizes local information, such as an image patch, may not provide reliable question-candidate relevance scores. Besides, the two-tower architecture also limits the relevance score modeling of a retriever to select top candidates for answer generator reasoning. In this paper, we introduce an additional module, a multi-modal reranker, to improve the ranking quality of knowledge candidates for answer generation. Our reranking module takes multimodal information from both candidates and questions and performs cross-item interaction for better relevance score modeling. Experiments on OK-VQA and A-OKVQA show that multi-modal reranker from distant supervision provides consistent improvements. We also find a training-testing discrepancy with reranking in answer generation, where performance improves if training knowledge candidates are similar to or noisier than those used in testing.", "sections": [{"title": "1 Introduction", "content": "Knowledge-intensive visual question answering (KI-VQA), compared to conventional visual question answering, provides questions that cannot be directly answered with images. It requires models to use external knowledge for answer reasoning and synthesis, as shown in Figure 1.\nA typical KI-VQA system contains a retrieval model to find relevant external knowledge, and an answer generator that performs reasoning over retrieved knowledge to produce the answer. One line of research investigates methods for an effective retrieval pipeline, which includes the choices of knowledge bases (Li et al., 2020; Gard\u00e8res et al.,"}, {"title": "2 A Knowledge-Intensive Visual Question Answering Framework", "content": "In this section, we will introduce a basic framework for KI-VQA, including image-text retrieval and answer generation, as illustrated in Figure 2."}, {"title": "2.1 Wikipedia-Based Image Text Dataset", "content": "In this work, we use a multi-modal knowledge base, Wikipedia-Based Image Text Dataset (WIT) (Srinivasan et al., 2021). In addition to previous work that uses text from encyclopedia, WIT contains images from Wikipedia and the surrounding text at different levels, including their captions and surrounding sections. Therefore, we consider WIT as a combination of image and text knowledge."}, {"title": "2.2 Image-Text Retrieval", "content": "Previous work has explored the use of different retrieval model choices (Luo et al., 2021; Gui et al., 2022; Lin et al., 2022). We follow one line of research that adopts image-text retrieval (Gui et al., 2022) using pretrained image-text language model with dual-encoder architecture (Radford et al., 2021; Jia et al., 2021). Following Gui et al. (2022), we use sliding window with a stride to generate multiple image regions from question image. Each image region is considered as a query and will be encoded by image encoder model $i(\u00b7)$. We encode captions in WIT dataset as the representation for candidates with text encoder model $t(\u00b7)$, as captions in Wikipedia are generally informative. Relevance score between an image region vi and a WIT candidate c is obtained with the inner product of their representations\n$r_t(v_i, c) = \u03c6_i(v_i)^T \u03c6_t(c)$."}, {"title": "2.3 Answer Generation", "content": "We follow previous work (Gui et al., 2022; Lin et al., 2022) that performs reasoning over top candidates within an encoder-decoder architecture. We also incorporate the multi-modal information (Salemi et al., 2023), compared to previous work that mostly uses text-based information.\nOur answer generation module is finetuned on vision language models that takes the combination of image and text as input (e.g., Chen et al., 2023b; Li et al., 2023). We first encode each top candidate separately. The input of each candidate consists of question image, candidate image and text following a template\u00b9 to compose question and candidate. We encode the image with a Vision Transformer (Dosovitskiy et al., 2021), which takes a series of image patches x = [x\u2081, ..., xn], i.e., image tokens, to produce image representations\n$E^o = [e^o_1, ..., e^o_n] = E_{EncV}(x^n)$.\nWe combine image representations and text token embeddings Et to produce fused representations with a Transformer (Vaswani et al., 2017)\n$H = [H^o; H^t_q; H^t_c] = E_{Enct} ([E^o; E^t_q; E^t_c])$,"}, {"content": "\u00b9question: <question text> candidate: <caption>"}, {"title": "3 Multi-Modal Reranking", "content": "Vanilla retrieval-generation frameworks directly use relevance score from individual image patches. However, a high relevance from a region does not necessarily imply the overall relevance. In this section, we propose multi-modal reranking, as illustrated in Figure 3, which takes multi-modal question and knowledge as input and produces relevance scores with cross-item interaction."}, {"title": "3.1 Modeling", "content": "Our ranking model is also finetuned on the multimodal pretrained language model. For each question-candidate pair, we first encode the question and candidate image separately, and obtain two series of token representations $E^q$, $E^c$. Then we concatenate the two series of image token representations, with text token embeddings Et following same template in Section 2.3 for a Transformer to produce fused token representations\n$H\" = E_{Encr} ([E^q; E^c; E^t])$.\nWe follow Zhuang et al. (2023) and use 1-step decoding to obtain the score from the unnormalized loglikelihood of a special token \u201c<extra_id_10>\u201d\n$r_{qc} = Dense (Dec (H\")) (<extra_id_10>)$."}, {"title": "3.2 Ranker Training", "content": "Because we do not have ground-truth relevance scores, we adopt distant supervision labels for reranking training. In a typical VQA setting, each answer consists of 10 answer candidate annotations. We count the number of answer candidates that occur in the knowledge candidate text as o. The distantly supervised relevance score is obtained similar to VQA accuracy (Antol et al., 2015)\n$r_{qc} = min {o/3, 1}$.\nOn OK-VQA, we split the training dataset of original dataset into sub-training and -development sets. At each training step, for a question q, we uniformly sample a candidate set C from the retrieval results, and apply pairwise logistics ranking loss (Burges et al., 2005), which compares the ranking between all pairs of candidates in the set\n$l(q) = \\sum_{c \\in C} \\sum_{c' \\in C \\atop r_{qc}>r_{qc'}} log (1+e^{r_{qc'}-r_{qc}})$."}, {"title": "3.3 Discrepancy on Applying Reranking for Answer Generation", "content": "During answer generation training, it is straightforward to apply the ranking model and use the reranked top candidates as input. However, directly applying reranking on both training and testing will instead hurt the model performance. This is because applying the ranker on the training set, from which the ranker is trained, performs much better than when applied to the unseen test set. As we will illustrate in Section 4.3, learning answer generation with higher quality ranking results while testing on lower quality ranking results will in general have a negative impact to answer generation performance. Therefore, we will keep the initial retrieval results for answer generation training, while using the reranked results for model testing."}, {"title": "4 Experiments", "content": "We conduct experiments on OK-VQA (Marino et al., 2019) and A-OKVQA (Schwenk et al., 2022). OK-VQA introduces visual questions that requires external knowledge. A-OKVQA further emphasizes commonsense reasoning over world knowledge. For both datasets, we evaluate the performance on the validation set. Following the standard setting, we use VQA accuracy as the metric."}, {"title": "5 Related Work", "content": "A typical knowledge-intensive visual question answering model involves a knowledge retrieval to find relevant information, and answer generator to produce the answer (Li et al., 2020; Gard\u00e8res et al., 2020; Luo et al., 2021; Yang et al., 2022; Gui et al., 2022; Lin et al., 2022; Salemi et al., 2023; Shao et al., 2023). Previous work on knowledge-intensive visual question answering explores knowledge bases in different modalities, such as text items (Luo et al., 2021; Gui et al., 2022), graph items (Li et al., 2020; Gard\u00e8res et al., 2020), and the composition of image items and text items (Wu et al., 2022). Our work differs from previous work by involving multi-modal knowledge items as the knowledge base, where each item contains both image and text information.\nThere is also a line of research investigating answer reranking, where they first produce a list of answer candidates, and then rerank those candidates to obtain the most reliable answer (Marino et al., 2021; Si et al., 2021; Wu et al., 2022). Instead, the focus of our work is to first retrieve a set of knowledge candidates that can help answer generation, and then improve the quality of knowledge candidate set through multimodal knowledge candidate reranking. Those selected candidates will still serves as additional knowledge input for answer generation reasoning."}, {"title": "6 Conclusion", "content": "In this paper, we introduce reranking, a critical stage for knowledge-intensive tasks, into KI-VQA. Our multi-modal reranking component takes multimodal questions and knowledge candidates as input and perform cross-item interaction. Experiments show that our proposed multi-modal reranking can provide better knowledge candidates and improve the answer generation accuracy. Experiments on the training-testing discrepancy indicate that incorporating noisier knowledge candidates during training enhances model robustness, while training with higher quality candidates than those used in testing negatively impacts performance."}, {"title": "Limitations", "content": "In this paper, we focus on applying multi-modal reranking to KI-VQA. However, because of the nature of visual data, directly adding visual information may significantly increase input size, and we will require more total memory to train the model. In this paper, to reduce the total memory use, we have a much smaller number of knowledge candidates for reasoning in answer generation module compared to previous work which only uses text-based knowledge candidates. Nevertheless, it is still important to further investigate more efficient ways to incorporate visual information.\nAlthough multi-modal reranking achieves promising performance on knowledge-intensive visual question answering, it is still an open question that whether multi-modal reranking can be used help other vision-language tasks. Besides, it is also important to develop a benchmark to evaluate multi-modal reranking models systematically, which is not covered by this work.\nSimilarly, in this work, we only use ALIGN and PaLI as the pretrained model for retrieval, reranking and answer generation. Although it is natural to extend the framework in this work to other pretrained models, it is still interesting to see how it contributes to different (large and small) models. We provide some preliminary results comparing our reranking pipeline with zero-shot multimodal large models (Alayrac et al., 2022; Li et al., 2023) in Appendix C, but we also notice that some work (Liu et al., 2023; Chen et al., 2023a) uses OK-VQA as instruction tuning data, making it hard to compare/be adopted directly.\nWe also notice that there is another line of research investigating how to effectively use large language models for knowledge-intensive visual question answering (Yang et al., 2022; Gui et al., 2022; Lin et al., 2022; Salemi et al., 2023; Shao et al., 2023). Although our preliminary results show that our framework can still provide additional improvements over same the large language model queries as in Lin et al. (2022), it is still an open question to effectively use and combine the retrieval pipeline and large language model queries."}, {"title": "Acknowledgment", "content": "This work was supported in part by the Google Visiting Scholar program and the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."}, {"title": "A Experiment Setup", "content": "We initialize image-text retrieval module with pretrained ALIGN checkpoint, and we initialize both answer generation and multi-modal reranking module with pretrained PaLI-3b checkpoint.\nIn the retrieval module, we crop a question image into a series of patches with kernel size 224 with stride 64. We use each image patch to retrieve top-20 candidates and then aggregate candidates from one question image. If there are candidates that are retrieved by multiple image patches in the same image, we will keep the one with highest relevance score. We use aggregated top-20 candidates as candidates set for answer generation training and testing.\nFor OK-VQA, the multi-modal reranker takes 8500 of examples from training set for training, and the rest of them for model development. For each question, the reranker takes aggregated candidates from top-20 image patch retrieval as the candidate set. At each training step, we will sample 20 candidates for each question and perform"}, {"title": "B Additional Comparison with Other Ranking Strategies", "content": "We also compare our model to the same multi-modal reranking model architecture trained with knowledge distillation from answer generation (Izacard and Grave, 2021a) and RankT5 (Zhuang et al., 2023) in Table 5. We can find that supervision from knowledge distillation can not provide reliable labels to train a reasonable reranking module. While both text-based reranking and multi-modal reranking can contribute to the performance, and multi-modal reranking can provide better performance. Especially, compared to RankT5 which is pretrained with over 500K items, our reranker only trained with around 8000 items. But it can still achieve competitive performance."}, {"title": "C Comparison With Zero-Shot Multi-Modal Large Models", "content": "We also provide additional comparison in Table 6 between some multi-modal large models on OK-VQA, including Flamingo-80b (Alayrac et al., 2022) and BLIP-2 (Li et al., 2023). We report their zero-shot performance compared to our model. The results show that smaller model can still achieve competitive performance when comparing to the zero-shot capability of those large models. We also note that there are some other multi-modal large models such as LLAVA 1.5 (Liu et al., 2023), MiniGPT4-V2 (Chen et al., 2023a), which are instruction tuned with OK-VQA and therefore cannot be directly compared. But in general, our proposed framework can be extended to other multi-modal language models that take the combination of image and text input."}]}