{"title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation", "authors": ["Haokun Liu", "Yangqiaoyu Zhou", "Mingxuan Li", "Chenfei Yuan", "Chenhao Tan"], "abstract": "AI holds promise for transforming scientific processes, including hypothesis generation. Prior work on hypothesis generation can be broadly categorized into theory-driven and data-driven approaches. While both have proven effective in generating novel and plausible hypotheses, it remains an open question whether they can complement each other. To address this, we develop the first method that combines literature-based insights with data to perform LLM-powered hypothesis generation. We apply our method on five different datasets and demonstrate that integrating literature and data outperforms other baselines (8.97% over few-shot, 15.75% over literature-based alone, and 3.37% over data-driven alone). Additionally, we conduct the first human evaluation to assess the utility of LLM-generated hypotheses in assisting human decision-making on two challenging tasks: deception detection and AI generated content detection. Our results show that human accuracy improves significantly by 7.44% and 14.19% on these tasks, respectively. These findings suggest that integrating literature-based and data-driven approaches provides a comprehensive and nuanced framework for hypothesis generation and could open new avenues for scientific inquiry.", "sections": [{"title": "Introduction", "content": "\"It is the theory that decides what can be observed.\" -Albert Einstein\nLarge language models (LLMs) excel at synthesizing information and hold promise for transforming hypothesis generation, a critical yet understudied step in scientific discoveries. Many recent studies recognize this potential and use LLMs to generate hypotheses (e.g., Yang et al., 2024b; Batista and Ross, 2024). We broadly categorize them into theory-driven and data-driven methods.\nOn one hand, theory-driven approaches leverage LLMs to review existing literature and generate novel hypotheses (Yang et al., 2024b; Baek et al., 2024). These methods have shown promising results in terms of the hypotheses' novelty, validity, and usefulness to researchers, while remaining grounded in established human knowledge (Si et al., 2024). However, they come with notable limitations: they require high-quality literature, struggle to adapt to new data, and lack empirical support. Data-driven approaches, on the other hand, propose hypotheses by discovering patterns in data (Zhou et al., 2024; Qiu et al., 2024). These hypotheses are data-adaptive and can exhibit strong performance in explaining the data. However, they could be too overly tailored to the specific datasets used, which can hinder their generalizability.\nWe hypothesize that theory can guide the discovery from data and propose to integrate literature-based and data-driven hypothesis generation (see Figure 1). For the data-driven component, we use HYPOGENIC as the backbone (Zhou et al., 2024). HYPOGENIC leverages an LLM to initialize hypotheses from a small number of examples and then updates them iteratively to improve the quality of hypotheses. To enhance this process with literature insights, we introduce a literature-based hypothesis agent. This agent interacts with the data-driven hypothesis agent (HYPOGENIC), refining and maintaining a shared pool of hypotheses through continuous collaboration, ensuring that the hypotheses benefit from both data-driven adaptability and the grounding of existing scientific knowledge. In addition to the refinement approach, we also propose to directly unionize literature-based and data-driven hypotheses.\nTo comprehensively evaluate these hypotheses, we conduct automatic and human evaluation to assess their generalizability, utility, and novelty. We apply our method to address research questions in social sciences: deception detection, AI generated"}, {"title": "Methods", "content": "We formulate the problem of hypothesis generation as follows. Assuming that we have access to papers P and observational data D that are relevant to a research question (q). Then, we want to develop an Al-powered algorithm fm with model M such that we can generate high-quality hypotheses for the research question q, i.e., H = fm(q,P,D). Example research questions include what makes an argument persuasive and what signs are indicative of AI-generated texts. In this work, we consider research questions that can be formulated as classification tasks, so we use q and task interchangeably."}, {"title": "Literature-Based Hypothesis Generation", "content": "Due to limitations in technical applicability, such as domain mismatch or the absence of publicly available implementations, it was not feasible to directly adopt existing methods on literature-based generation (Wang et al., 2024; Baek et al., 2024). Thus, we develop our own implementation.\nWe start by picking 10 papers for q from related papers on Semantic Scholar or Google Scholar. We also choose from papers that cite the original datasets for each task. Subsequently, we use S2ORC-doc2json to convert the raw PDF versions of the papers to a corpus of JSON files (Lo et al., 2020). Then, we develop a paper summarizer with M to generate paper summaries based on the literature corpus. For the LITERATURE-ONLY method, we instruct language models to generate hypotheses based on extracted paper summaries, with an emphasis on usefulness for carrying out the specific tasks that our literature corpus focuses on."}, {"title": "Data-Driven Hypothesis Generation", "content": "Our data-driven hypothesis generation largely follows HYPOGENIC in Zhou et al. (2024). Here we give a brief overview. During the initialization stage of HYPOGENIC, an LLM is prompted with a set of initial data instances Dinit from the training set D and asked to generate initial hypotheses that forms the initial hypothesis bank HD.\nIn the update stage, for each example s \u2208 D, top k high-reward hypotheses from HD are selected and each used to prompt the LLM to make a prediction on s. The accuracy and reward of the k hypotheses are updated accordingly. Among the k hypotheses, if at least Whyp predicted wrong on s, s is added to a wrong examples pool W. Once the size of W reaches Wmax, a set of new hypotheses are generated from W and added to HD according to their reward. Inspired by the upper confidence bound (UCB) algorithm (Auer, 2003), the reward function of HYPOGENIC is defined as follows:\nri = \\frac{\\Sigma_{(y_j,\\hat{y}_j) \\in S_i} I (Y_j = \\hat{y}_j)}{|S_i|} + \\alpha \\sqrt{\\frac{log t}{|S_i|}} ,\nwhere Si is the set of examples used to evaluate hypothesis hi, t is the training time step, and \u03b1 is the reward coefficient that controls the exploration term of the reward function."}, {"title": "Integration of Literature-based and Data-driven Hypotheses", "content": "One main contribution of our work is proposing the first approach to integrating literature-based and data-driven hypothesis generation so that we can effectively leverage the strengths of each approach, increasing the generalizability and utility of generated hypotheses. We consider two strategies.\nRefinement of literature-based hypotheses. HYPOREFINE integrates paper summaries from \u00a7 2.1 with HYPOGENIC. In the initialization stage of HYPOGENIC, an LLM is asked to generate initial hypotheses based on both a set of initial examples and paper summaries relevant to the task.\nIn the update stage, we propose an iterative refinement approach to integrate patterns from data and key findings from literature into new hypotheses. Specifically, each time HYPOGENIC generates a set of new hypotheses Ho from the wrong examples pool W, these hypotheses are refined multiple rounds by a data-driven refinement agent and a literature-based refinement agent. Take RM as the refinement agent based on model M, each time Ho is generated from the wrong examples pool W, it is iteratively refined as follows:\nHi = \\begin{cases} RM(q_i, H_{i-1}, P) & \\text{if } i \\mod 2 = 0 \\\\ RM(q_i, H_{i-1}, W) & \\text{if } i \\mod 2 = 1, \\end{cases}\nwhere P represents the literature information, W represents the data pool used to generate Ho, and q being the queries. After max_refine rounds of refinement, the final hypothesis bank Hmax_refine is fed back to the HYPOGENIC pipeline.\nThe reward function and update process for the hypothesis bank H remain consistent with those of the original HYPOGENIC.\nUnion and redundancy elimination. As the reward function of HYPOGENIC focuses only on the hypotheses' performance on the datasets at hand, literature-based hypotheses are sometimes undervalued during the update stage. On occasions they can even be replaced by hypotheses that have especially good performances on data but are not necessarily generalizable on real-world tasks. To counter this issue, we use a union approach to mechanistically combine literature-based and data-based hypotheses. We first generate two hypothesis banks: one literature-based hypothesis bank and another bank using HYPOGENIC or HYPOREFINE. Then we build a redundancy checker to remove hypotheses that express overly similar or repeating information in each bank. Lastly, we construct the final hypothesis bank of size n by randomly choosing hypotheses from the literature-based hypothesis bank and adding the top hypotheses from the other hypothesis bank based on training accuracies. For detailed information of the implementation, please refer to Appendix B.3."}, {"title": "Experiments", "content": "In this section, we introduce our evaluation framework and the tasks to operationalize it."}, {"title": "Evaluation Framework", "content": "Formally evaluating hypotheses requires rigorous protocols and vast amounts of resources. In this work, we mainly evaluate our generated hypotheses along two dimensions: utility and novelty. We"}, {"title": "Tasks", "content": "We consider four tasks in social sciences.\nDeception Detection is a widely studied problem in psychology and other social sciences (Granhag and Vrij, 2005). We use the dataset introduced by Ott et al. (2013) (DECEPTIVE REVIEWS), which consists of 800 genuine hotel reviews and 800 fake hotel reviews, as our IND dataset. For the OOD dataset, we use hotel reviews from different source websites and different cities (Li et al., 2013).\nAI-Generated Content (AIGC) Detection has attracted significant attention in recent years (Tang et al., 2023). Most existing works focus on developing black-box detection methods and rarely take interpretability into account (Wu et al., 2024). We thus build our own dataset for this task. We take 800 distinct prompts and human-written stories in the WRITINGPROMPTS dataset (Fan et al., 2018). Then we use the same prompts to generate AI-written stories with LLAMA-3.1-70B-INSTRUCT (Dubey et al., 2024) and GPT-4O-MINI (OpenAI, 2023), constituting our LLAMAGC and GPTGC datasets. The IND data contains stories generated"}, {"title": "Implementation and Baselines", "content": "Our method works with any LLM (M). We use GPT-4O-MINI and LLAMA-3.1-70B-INSTRUCT in this work. Throughout this paper, we refer to GPT-4O-MINI as \"GPT-4-MINI\" and LLAMA-3.1-70B-INSTRUCT as \u201cLLAMA-70B-I\". We compare our method with the following baselines.\n1. Zero-shot and few-shot prompting. We give the LLMs detailed task instructions (zero-shot) and optionally provide three demonstrating examples (few-shot). This approach does not involve any hypothesis.\n2. Zero-shot hypothesis generation. Inspired by Qi et al. (2023), we provide specific task descriptions and instructions, and then we prompt the LLMs to generate hypotheses directly without incorporating literature or data.\n3. Literature-driven hypothesis generation. We use the implementation in \u00a72.1. In addition to our own implementation, we compare two of the recently released agent frameworks for scientific writing, NOTEBOOKLM (Google, 2024) and HYPERWRITE (OthersideAI, 2024). We use the same prompt for NOTEBOOKLM and HYPERWRITE as what we apply in our methods. See details in Appendix B.5. These methods only use literature in hypothesis generation.\n4. Data-driven hypothesis generation. We use HYPOGENIC. See details in \u00a7 2.2.\nFor all the hypothesis generation methods we use, we keep the size of the hypothesis bank H to be 20 (i.e., |H| = 20.)"}, {"title": "Results", "content": "We first present automatic evaluation results to demonstrate the utility of generated hypotheses for model inference. We then show that the generated hypotheses can improve human decision-making in challenging tasks and that literature-based and data-driven hypotheses provide unique insights from each other."}, {"title": "Automatic Evaluation", "content": "Hypotheses generated by combining information from literature and data achieves the best performance across all task and model configurations (Table 1). First, few-shot inference outperforms zero-shot inference for all task and model configurations, with an average improvement of 6.84% in accuracy. In addition, few-shot inference surpasses zero-shot generation and the best of literature-based methods on average accuracy by 7.21% and 6.78%, respectively, suggesting off-the-shelf LLMs or literature alone does not generate effective hypotheses for predictive purposes. In fact, NOTEBOOKLM and HYPERWRITE can generate some invalid or irrelevant hypotheses, which degrades their inference performance (see Table 9 in Appendix).\nIn contrast, HYPOGENIC consistently outperforms few-shot inference, improving average accuracy by 5.61%, highlighting the advantage of data-driven hypotheses. Compared to few-shot inference, the hypotheses also offer more interpretable insights. Furthermore, our best hypothesis generation method combining literature and data outperforms HYPOGENIC by 3.37% on average (i.e., 8.97% over few-shot and 15.75% over"}, {"title": "Human Evaluation", "content": "Generated hypotheses improve human decision-making in both AIGC Detection and Deception Detection. In AIGC Detection, the average human accuracy improves by 14.19% (58.86% \u2192 73.05%) when we provide hypotheses as assistance. We perform a statistical t-test and obtain a p-value of 0.01, indicating that the improvement is significant. In Deception Detection, the introduction of hypotheses boosts human accuracy by 7.44% (57.14% \u2192 64.58%), with a p-value of 0.04.\nWhen hypotheses are present, participants would use them to assist decision-making for over 90% of the time. All three presented hypotheses are selected to be used with frequency greater than 30% (Table 4, Table 5 in the Appendix). For example, the most used hypothesis, with frequency of 44.55%, in AIGC detection is \"Human-written texts tend to have a more conversational tone and colloquial language, while AI-generated texts tend to be more formal and lack idiomatic expressions.\u201d For both tasks, 100% of the participants find the hypotheses to be helpful, and over 40% find them to be \"Very helpful\u201d or \u201cExtremely helpful\".\nHumans rate literature-based and data-driven hypotheses as distinct. We determine the novelty label based on majority vote from three human annotators. 84% of the pairs are considered novel to each other for Deception Detection, and 80% are considered novel for AIGC Detection, demonstrating the complementarity between literature-based and data-driven approaches."}, {"title": "Related Work", "content": "Theory-driven hypothesis generation. Yang et al. (2024b) generates hypotheses from raw web corpus, but their method requires human annotated hypotheses from literature. Baek et al. (2024), Wang et al. (2024), and Ghafarollahi and Buehler (2024) use LLMs to create knowledge graph and generate hypotheses from existing literature. We implement our own literature-based generation because these papers either do not provide sufficient implementation details or require significant effort to adapt to new tasks.\nData-driven hypothesis generation. Besides HYPOGENIC, we review additional works on discovering unseen patterns from data. Zhong et al. (2023) discovers patterns by analyzing difference between large corpora. Pham et al. (2024) makes discovery by generating and refining interpretable topics. Romera-Paredes et al. (2024) uncovers new solutions in open math problems by iteratively updating programs. Qiu et al. (2024) and Yang et al. (2024a) evaluate LLMs' ability in performing inductive reasoning in synthetic settings. Batista and Ross (2024) uses LLMs to generate hypotheses and conducts comprehensive experiments to study human engagements with headlines. We choose HYPOGENIC as the backbone for data-driven hypothesis generation as their tasks are most similar to ours, and their approach to hypothesis updates integrates naturally into our refinement process.\nAutomated scientific research with LLMs. There is growing interest in developing LLM-powered methods and multi-agent frameworks to assist scientific research. Lu et al. (2024) designs an LLM agent to generate full research papers. Li et al. (2024) proposes a method to generate research ideas from existing literature and automatically implement and execute experiments. In contrast, our work focuses primarily on hypothesis generation, as we believe it is crucial to preserve human agency and oversight in the scientific research process.\nTo evaluate LLM generated hypotheses, Qi et al. (2023) examines whether they contain novel information not found in existing literature. Si et al. (2024) asks experts to rate the novelty of LLM-proposed research ideas in the NLP domain. While these studies highlight LLMs' ability to generate novel hypotheses, they do not conduct human subject experiments to validate the effectiveness of hypotheses. To this end, we conduct the first human study to test the utility of LLM-generated hypotheses in supporting human decision-making.\nSignificant efforts have also been made to benchmark multi-agent frameworks on data analysis tasks (Majumder et al., 2024; Gu et al., 2024; Hu et al., 2024; Chen et al., 2024; Huang et al., 2024; Guo et al., 2024), literature processing and informa-"}, {"title": "Conclusion", "content": "We propose a novel approach that integrates literature and data to generate hypotheses, with extensive and systematic evaluations. Our method consistently outperforms all baselines, including existing literature-based and data-driven approaches. Furthermore, human evaluations reveal that our generated hypotheses also improve human decision-making in challenging tasks."}, {"title": "Limitations", "content": "Our automated evaluation uses two recent models on datasets across various domains, showing the effectiveness of our method across diverse settings. However, we did not further evaluate our hypotheses on some tasks that require representations beyond natural language, such as math problem solving and code generation.\nThe literature corpus used for literature-based hypothesis generation is limited in terms of size and collection method. The collection is carried out by manually searching and collecting up to 10 papers on Semantic Scholar or Google Scholar. Though with the limited literature corpus we already show that our methods yield competent performance, a natural future direction is to enhance the literature component with automatic and scalable retrieval.\nSimilarly, we achieved satisfactory performance across different models and tasks with the initial set of hyperparameters. However, we did not perform an exhaustive hyperparameter search, which may have yielded further enhancements to the performance of our methods. This represents a limitation of our study that could be addressed in future work.\nOur experiments with human subjects is a proof of concept. The number of participants in our human evaluation is relatively small. As a result, we do not believe that we have the statistical power to distinguish, for example, the difference between HYPOGENIC and HYPOREFINE. Although this is not the focus of our study, we encourage future work to conduct large-scale experiments in focused domains to validate the hypotheses generated through human-AI collaboration.\nLast but not least, we manually chose three hypotheses through ablation-style study and subjective judgment for experiments with human subjects. We believe this process is the essence of human-AI collaboration in future scientific processes. It requires future exploration to identify the optimal collaboration regime."}, {"title": "A Prompts", "content": "All our prompts for LLMs are separated into system prompts and user prompts. System prompts contain role and tone information, followed by detailed descriptions of the task and the expected response format. User prompts contain useful information for hypothesis generation, refinement, or inference, including information from literature, instances from datasets, and previously generated hypotheses. Below are some examples of the prompts that we use for each task."}, {"title": "Deception Detection", "content": "System Prompt\nYou're a professional hotel review analyst.\nGiven a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive. In other words, we want to know whether the review is written by a someone who actually lived in the hotel.\nUsing the given examples, please propose <num_hypotheses> possible hypothesis pairs.\nThese hypotheses should identify specific patterns that occur across the provided reviews.\nEach hypothesis should contain a pair of the following:\na. A hypothesis about what makes reviews more likely to be truthful\nb. The opposite hypothesis about what makes reviews more likely to be deceptive\nGenerate them in the format of 1. [hypothesis], 2. [hypothesis], <num_hypotheses>. [hypothesis].\nThe hypotheses should analyze what kind of reviews are likely to be truthful or deceptive.\nUser Prompt\nWe have seen some hotel reviews:\nmore examples here\nPlease generate hypotheses that are useful for predicting whether a review is truthful or deceptive.\nPropose <num_hypotheses> possible hypotheses.\nGenerate them in the format of 1. [hypothesis], 2. [hypothesis], <num_hypotheses>. [hypothesis].\nProposed hypotheses:"}, {"title": "Data-Based Hypothesis Generation with HypoGeniC.", "content": "System Prompt\nYou're a professional hotel review analyst.\nGiven some key findings from a series of research papers, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive. In other words, we want to know whether the review is written by a someone who actually lived in the hotel.\nUsing the given relevant literatures, please propose  possible hypothesis pairs.\nThese hypotheses should identify specific patterns that occur across the provided reviews.\nEach hypothesis should contain a pair of the following:\na. A hypothesis about what makes reviews more likely to be truthful\nb. The opposite hypothesis about what makes reviews more likely to be deceptive\nGenerate them in the format of 1. [hypothesis], 2. [hypothesis],  . [hypothesis].\nThe hypotheses should analyze what kind of reviews are likely to be truthful or deceptive.\nUser Prompt\nWe have some key findings from a series of research papers that might be useful for generating the required  hypotheses:\ninformation from literature here\nPlease generate hypotheses that are useful for predicting whether a review is truthful or deceptive.\nWhen generating hypotheses, remember not to overuse your own knowledge. Always refer to the key findings from research papers provided.\nDirectly cite passages in the key findings when generating a hypothesis.\nPropose  possible hypotheses.\nRemember to generate  hypotheses!\nGenerate them in the format of 1. [hypothesis], 2. [hypothesis],  . [hypothesis].\nProposed hypotheses:"}, {"title": "Literature-Based Hypothesis Generation.", "content": "System Prompt\nYou are a helpful assistant for summarizing key findings in research papers on a given topic.\nUser Prompt\nSummarize the following research paper, focusing ONLY on this question: What is useful for one to decide whether a review is truthful or deceptive in real life?\nFocus on hypotheses of what kind of reviews tend to be deceptive, do not include technical details in the paper.\nliterature texts here"}, {"title": "Paper Summarization.", "content": "System Prompt\nYou're a social scientist working on a project to identify deceptive hotel reviews.\nGiven a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive. In other words, we want to know whether the review is written by a someone who actually lived in the hotel.\nUsing the given examples, refine the hypothesis pairs provided.\nThe desired hypotheses should identify specific patterns that occur across the provided reviews.\nEach hypothesis should contain a pair of the following:\na. A hypothesis about what makes reviews more likely to be truthful\nb. The opposite hypothesis about what makes reviews more likely to be deceptive\nGenerate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis],  [hypothesis].\nThe hypotheses should analyze what kind of reviews are likely to be truthful or deceptive.\nUser Prompt\nWe have seen some hotel reviews:\nmore examples here\nWe have some hypotheses need to be refined:\nhypotheses to be refined here\nPlease refine these hypotheses to make them more specific and useful for predicting whether a review is truthful or deceptive.\nWhen refining the hypotheses, feel free to change the key information or topic of a hypothesis based on the provided prevailing patterns in data if you think it is necessary.\nGenerate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis],  [hypothesis].\nRefined hypotheses:"}, {"title": "Hypothesis Refinement Based on Data.", "content": "System Prompt\nYou're a social scientist working on a project to identify deceptive hotel reviews.\nGiven a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful or deceptive. In other words, we want to know whether the review is written by a someone who actually lived in the hotel.\nUsing the given relevant literatures, refine the hypothesis pairs provided.\nThe desired hypotheses should identify specific patterns that occur across the provided reviews.\nEach hypothesis should contain a pair of the following:\na. A hypothesis about what makes reviews more likely to be truthful\nb. The opposite hypothesis about what makes reviews more likely to be deceptive\nGenerate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis],  [hypothesis].\nThe hypotheses should analyze what kind of reviews are likely to be truthful or deceptive.\nUser Prompt\nWe have some key findings from a series of research papers that might be useful for generating hypotheses:\ninformation from literature here\nWe have some hypotheses need to be refined:\nhypotheses to be refined here\nPlease refine these hypotheses to make them more specific and useful for predicting whether a review is truthful or deceptive.\nWhen refining the hypotheses, feel free to change the key information or topic of a hypothesis based on the provided key findings if you think it is necessary.\nGenerate refined hypotheses in the format of 1. [ hypothesis], 2. [hypothesis],  [hypothesis].\nRefined hypotheses:"}, {"title": "Hypothesis Refinement Based on Literature.", "content": "System Prompt\nYou are a professional deceptive detection agent and your job is to determine whether a hotel review is truthful or deceptive.\nIn other words, we want to know whether the review is written by someone who had real experiences with the hotel.\nFrom past experiences, you learned some patterns. You need to determine whether each of the patterns holds for the current hotel review, and also predict whether the current hotel review is truthful or deceptive.\nGive an answer. The answer should be one word ( truthful or deceptive).\nGive your final answer in the format of {Final answer: answer}\nUser Prompt\nOur learned patterns: \nA hotel review is the following: \nGiven the pattern you learned above, give an answer of whether the hotel review above is deceptive or truthful.\nThink step by step.\nFirst step: Think about which pattern can be applied to the hotel review.\nSecond step: Based on the pattern, is this hotel review deceptive or truthful?\nFinal step: give your final answer in the format of {Final answer: answer}"}, {"title": "Multiple-Hypothesis-Based Inference.", "content": "System Prompt\nYou're a an AI generated content detection expert. You are great at detecting what type of text is generated by AI.\nGiven a set of texts, we want to generate hypotheses that are useful for predicting whether a piece of text is generated by AI. In other words, we want to know whether the text is written by a human or generated by AI.\nYour task is to identify what patterns or traits show up more in AI generated texts, and what shows up more in human written texts. Focus on the generalizable insight that can be applied in other contexts. Ignore things that are specific to this story. Do not make references this story they may not be for others.\nUsing the given examples, please propose <num_hypotheses> possible hypothesis pairs.\nWhen proposing hypothesis, look closely into the given examples and identify specific patterns that occur across the provided text examples.\nThe hypotheses should be clear, easy to understand, and have specific details such that one can apply the hypotheses to predict whether a piece of text is written by human or AI.\nGenerate them in the format of 1. [hypothesis], 2. [hypothesis],  . [hypothesis].\nThe hypotheses should analyze what kind of text is likely to be written by human or AI.\nUser Prompt\nWe have seen some texts:\nmore examples here\nPlease generate hypotheses that are useful for predicting predicting whether a piece of text is written by human or AI.\nPropose  possible hypotheses.\nGenerate them in the format of 1. [hypothesis), 2. [hypothesis],  [hypothesis).\nWhen proposing hypothesis, look closely into the given examples and identify specific patterns that occur across the provided text examples.\nPlease make sure that the hypotheses are:\ni. clear (i.e., precise not too wordy and easy to understand);\nii. generalizable to novel situations (i.e., they would make sense if applied to other AI generated content detection experiments or other messaging contexts);\niii. empirically plausible (i.e., this is a dimension on which messages can vary on);\niv. unidimensional (i.e., avoid hypotheses that"}, {"title": "Data-Based Hypothesis Generation with HypoGeniC.", "content": "System Prompt\nYou're a professional AI content detector.\nGiven some key findings from a series of research papers, we want to generate hypotheses that are useful for detecting whether a piece of text is written by human or AI.\nYour task is to identify what patterns or traits show up more in AI generated texts, and what shows up more in human written texts. Focus on the generalizable insight that can be applied in other contexts. Ignore things that are specific to this story. Do not make references this story they may not be for others.\nUsing the given relevant literatures, please propose  possible hypothesis pairs.\nThese hypotheses should identify specific patterns that occur across the provided texts.\nGenerate them in the format of 1. [hypothesis], 2. [hypothesis],  . [hypothesis].\nThe hypotheses should analyze what kind of text is likely to be written by human or AI.\nUser Prompt\nWe have some key findings from a series of research papers that might be useful for generating the required  hypotheses:\ninformation from literature here\nPlease generate hypotheses that are useful for predicting whether a piece of text is written of human or AI.\nPropose  possible hypotheses.\nRemember to generate  hypotheses!\nGenerate them in the format of 1. [hypothesis], 2. [hypothesis],  [hypothesis].\nProposed hypotheses:"}, {"title": "Literature-Based Hypothesis Generation.", "content": "System Prompt\nYou are a helpful assistant for summarizing key findings in research papers on a given topic.\nUser Prompt\nSummarize the following research paper, focusing ONLY on this question: What is useful for one to detect whether some text is generated by AI?\nFocus on hypotheses of what kind of text tend to be generated by AI, do not include technical details in the paper.\nliterature texts here"}, {"title": "Paper Summarization.", "content": "System Prompt\nYou are an AI generated content detection agent and want to determine whether a piece of text is written by a human or generated by an AI. In other words, we want to know whether the text is written by a human or generated by AI.\nFrom past experiences, you learned some patterns. You need to determine whether each of the patterns holds for the current text, and also predict whether the current text is written by human or AI.\nGive an answer. The answer should be one word (AI or HUMAN)."}, {"title": "Persuasive Argument Prediction", "content": "System Prompt\nYou are an intelligent rhetorician and debater who masters persuasiveness in language.\nGiven a pair of arguments, you are asked to determine which one of them uses more persuasive language. The two arguments are often on the same topic and are similar, so focus on their differences.\nWhat difference between the two arguments makes one more persuasive than the other?\nYou will be given a set of observations of the format:\nArgument 1: \nArgument 2: \nObservation: The first/second argument uses more persuasive language.\nBased on the observations, please generate hypotheses that are useful for explaining why one argument uses more persuasive language than the other.\nThese hypotheses should identify patterns, phrases, wordings etc. that occur across the provided examples. They should also be generalizable to new instances.\nPlease propose  possible hypotheses and generate them in the format of 1. [\nhypothesis), 2. [hypothesis),  [hypothesis).\nUser Prompt\nHere are the Observations:\nmore examples here"}, {"title": "Data-Based Hypothesis Generation with HypoGeniC.", "content": "System Prompt\nYou are an intelligent rhetorician and debater who masters persuasiveness in language.\nGiven a pair of arguments, you are asked to determine which one of them uses more persuasive language. The two arguments are often on the same topic and are similar, so focus on their differences.\nWhat difference between the two arguments makes one more persuasive than the other?\nYou will be given a set of literature of the format:\nTitle: \nKey Findings: \nBased on the literature, please generate hypotheses that are useful for explaining why one argument uses more persuasive language than the other.\nThese hypotheses should identify patterns, phrases, wordings etc. that you can find in the literature. They should also be generalizable to new instances.\nPlease propose  refined hypotheses and generate them in the format of 1. [hypothesis ), 2. [hypothesis ), hypothesis].\nUser Prompt\nHere are some key findings from a series of research papers that might be useful for generating hypotheses:\ninformation from literature here"}, {"title": "Multiple-Hypothesis-Based Inference.", "content": "System Prompt\nYou are an intelligent rhetorician and debater who masters persuasiveness in language.\nGiven a pair of arguments, you are asked to determine which one of them uses more persuasive language. The two arguments are often on the same topic and are similar, so focus on their differences.\nFrom past experiences, you learned some patterns. Now, at each time, you should apply the learned patterns to a new pair of arguments and determine which one uses more persuasive language.\nThe answer for the more persuasive language should be of the form \"the argument\" where is either first or second.\nPlease give your final answer in the format of { Final answer: the argument uses more persuasive language}\nUser Prompt\nOur learned patterns: \nGiven the patterns you learned above, determine which of the following arguments uses more persuasive language:\nArgument 1: \nArgument 2: \nThink step by step.\nStep 1: Think about which learned patterns can be applied to the arguments.\nStep 2: Analyze the difference between \"Argument 1\" and \"Argument 2\".\nStep 3: Based on the pattern, which argument uses more persuasive language?\nYou MUST give your final answer in the following format:\nFinal answer: the argument uses more persuasive language."}, {"title": "Mental Stress Detection", "content": "System Prompt\nYou're a psychologist and social scientist studying people's stress and their online posts. given a set of reddit posts, we want to"}]}