{"title": "DVM: Towards Controllable LLM Agents in Social Deduction Games", "authors": ["Zheng Zhang", "Yihuai Lan", "Yangsen Chen", "Lei Wang", "Xiang Wang", "Hao Wang"], "abstract": "Large Language Models (LLMs) have advanced the capability of game agents in social deduction games (SDGs). These games rely heavily on conversation-driven interactions and require agents to infer, make decisions, and express based on such information. While this progress leads to more sophisticated and strategic non-player characters (NPCs) in SDGs, there exists a need to control the proficiency of these agents. This control not only ensures that NPCs can adapt to varying difficulty levels during gameplay, but also provides insights into the safety and fairness of LLM agents. In this paper, we present DVM, a novel framework for developing controllable LLM agents for SDGs, and demonstrate its implementation on one of the most popular SDGs, Werewolf. DVM comprises three main components: Predictor, Decider, and Discussor. By integrating reinforcement learning with a win rate-constrained decision chain reward mechanism, we enable agents to dynamically adjust their gameplay proficiency to achieve specified win rates. Experiments show that DVM not only outperforms existing methods in the Werewolf game, but also successfully modulates its performance levels to meet predefined win rate targets. These results pave the way for LLM agents' adaptive and balanced gameplay in SDGs, opening new avenues for research in controllable game agents.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs), owing to their exceptional understanding, planning, and decision-making capabilities [1]\u2013[4], have demonstrated impressive performance in various gaming environments [5]-[10]. Among them, social deduction games (SDGs), which emphasize logical reasoning, strategic thinking and conversation skills, present a critical challenge for LLM agents [11]-[14]. Previous works on agents play-ing SDGs primarily focused on maximizing performance, striving to achieve the highest possible win rates. Xu et al. [15] proposed a framework that integrates LLMs with reinforcement learning to develop strategic language agents for the Werewolf game. Wu et al. [16] introduced a Thinker module to enhance the reasoning capabilities of LLM agents in complex deduction tasks. Lipinski et al. [17] and Xu et al. [18] explored emergent communication strategies and tuning-free frameworks for LLMs in SDGs. Besides, Lai et al. [19] and Zhang et al. [20] have investigated multimodal approaches, which combined visual signals with dialogue context to model persuasion behaviors. These advancements underscore the growing integration of LLMs in enhancing game agents' performance.\nHowever, this singular focus on optimal performance often overlooks the importance of having controllable agent per-formance during gameplay. Enabling agents to adjust their gameplay proficiency to match specified skill levels is ben-eficial for dynamic difficulty scaling, aligning with the actual requirements of game development. Despite these advantages, research on controllable LLM agents that can dynamically adjust their performance levels remains relatively unexplored.\nThis paper introduces DVM (Dynamic Victory Manager), a framework for LLM agents to dynamically adjust game-play proficiency using reinforcement learning, maintaining a specified win rate for controllable performance. DVM com-prises three components: Predictor, Decider, and Discussor. The Predictor aids in understanding player relationships by analyzing interactions and behaviors during gameplay. The Decider leverages the Predictor's insights and game state information to make strategic decisions. And the Discussor produces contextually appropriate and strategic dialogues to influence other players.\nThe training of the Decider employs the PPO algorithm [21], known for its stability in complex environments [22]-[26]. A novel decision chain reward, which evaluates the quality of entire decision sequences rather than individual actions, is introduced to enhance strategic performance. Furthermore, DVM incorporates a win rate-constrained reward function that adjusts based on the deviation from a specified win rate, supporting the development of a controllable agent.\nThe principal contributions of this work are as follows:\n\u2022\tWe propose DVM, a comprehensive framework with prediction, decision and discussion modules for reinforce-ment learning in social deduction games.\n\u2022\tWe develop a win rate-constrained decision chain reward function, enabling agents to dynamically adjust the pro-ficiency to maintain a specified win rate.\n\u2022\tWe conducted experiments in the Werewolf game and demonstrate that DVM not only surpasses current meth-ods but also successfully adjusts its performance to achieve specific win rate goals."}, {"title": "II. DVM", "content": "This section introduces the components of DVM, followed by a detailed description of the training methods for each module. The framework is shown in Fig. 1.\n\nPredictor. In social deduction games (SDGs), understand-ing the intentions and roles of other players is crucial to making strategic decisions. The Predictor analyzes players' interactions and behaviors to provide insights into relationships among players, aiding in more informed decision-making and enhancing cooperation with allies and confrontation with adversaries. Initially proposed with a dense network [27], we introduce an LLM-based Predictor to differentiate teammates from opponents using current discussion and voting patterns. The output of the Predictor informs the decision-making process, formulated as:\n\n$P_t = \\text{Predictor}(D_t, V_t)$\n\nwhere $D_t$ represents prior discussion content, $V_t$ represents prior voting patterns at game phase $t$, and $P_t$ represents pre-dictions about the identities of other players. These predictions are expressed in formatted natural language, then parsed and vectorized for the Decider.\nDecider. The Decider decides agent actions based on obser-vations and the Predictor's predictions. It uses three embedding layers to encode the subject, verb, and object of each event, and can output actions like voting, checking, saving, or killing, depending on the game phase. The goal is to optimize agent survival and success, considering roles and threats. This mod-ule calculates action probabilities using the Softmax function, formulated as:\n\n$\\text{Logits}(a) = \\text{Decider}(G_t, P_t, WR_{\\text{cons.}})$\n\n$\\text{Prob}(a) = \\text{Softmax}(\\text{Logits}(a) - a_{\\text{mask}} \\times 10^9)$\n\nwhere $a$ is the action to be taken, $G_t$ is the current game state, $P_t$ is the Predictor's predictions, $WR_{\\text{cons.}}$ is the win rate constraint, and $a_{\\text{mask}}$ represents the illegal action mask based on game rules.\nDiscussor. The task of the Discussor is to produce coherent and contextually appropriate speech for the agents during the discussion phase of the game. The generated text aims to persuade, deceive, or inform other agents based on the agent's strategy and role. The Discussor receives input from the De-cider's decision along with the current game context described in natural language. The output is in text format, which is then used in the game environment to simulate realistic discussions among the agents. We represent the discussion speech and voting patterns at phase $t$ using the following equation:\n\n$D_{t+1} = \\text{Discussor}(G_t, D_t, V_t, S_t) + D_t$\n\n$V_{t+1} = \\text{Discussor}(G_t, D_t, V_t, S_t) + V_t$\n\nwhere $G_t$ is the current game state, $D_t$ is the prior dis-cussion content, $V_t$ is the voting patterns, and $S_t$ is the step decision made by the Decider."}, {"title": "B. Training Methods", "content": "In our framework, we utilize ChatGLM3-6B [28] as the base model for both the Predictor and the Discussor. However, we only train the Predictor and the Decider. The training process is executed in two steps: supervised training and reinforcement learning.\nSupervised Training. Initially, we conduct supervised train-ing on the FanLang-9 dataset\u00b9, which consists of over 18,000 records of human-player games. This phase aims to fine-tune the Predictor and train the Decider to understand the intrica-cies of player decisions and strategies within the Werewolf environment.\nReinforcement Learning. Following supervised training, we further optimize the Predictor and the Decider using reinforcement learning. Each component undergoes a training process through self-play to refine their policies. For the Predictor, we use its predictions and the correct answers as negative and positive samples to train it using DPO [29]. For the Decider, we employ the PPO algorithm [21] due to its stability and efficiency in handling complex policies. The optimization objective of PPO is formulated as:\n\n$L_{PPO}(\\theta) = -E_{s, a \\sim \\pi_{\\theta_{\\text{old}}}} \\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} \\hat{A}^{\\pi_{\\theta_{\\text{old}}}}(s, a) - \\beta \\text{KL}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s), \\pi_{\\theta}(\\cdot|s)) \\right]$\n\n$\\hat{A}^{\\pi_{\\theta_{\\text{old}}}}(s, a) = r_t + \\gamma V(s_{t+1}) - V(s_t)$\n\nWhere $\\pi_{\\theta}$ is the current policy, $\\pi_{\\theta_{\\text{old}}}$ is the previous policy, and $r_t$ is the reward at step $t$.\nTo better capture long-term strategies, we introduce a de-cision chain reward mechanism to account for overall game-level decisions rather than single-step rewards. Therefore, $r_t$ is calculated as:\n\n$r_t = s_{r_t} + c_r$\n\nWhere $s_{r_t}$ is the step reward, and $c_r$ is the chain reward. The combined reward $r_t$ is then used to optimize the Decider.\nDecision Chain Reward. We enhance agent training by evaluating decision chains across the entire gameplay, as opposed to focusing only on single-step rewards used by Wu et al. [16].\nWe analyzed decision chains from the FanLang-9 dataset, and calculated the win rate for each chain, creating a database of $(DC, WR)$ pairs, where $DC$ represents the decision chain and $WR$ represents the win rate. After each game, the win rate of the agent's decision chain is retrieved from this database, providing either positive or negative rewards:\n\n$c_r(DC) = \\alpha \\cdot (WR - 0.5)$\n\nwhere $\\alpha$ is a constant that controls the amplitude.\nThis reward promotes strategic sophistication by rewarding sequences of decisions that lead to higher win rates, thus improving the agent's game performance.\nControllable Agent Training. The training of controllable agents focuses on developing game agents that can not only perform tasks effectively but also be adjusted to maintain a controllable win rate. Such a strategy ensures stability across games by not always making the best or worst decisions, aligning with the practical needs of difficulty regulation in game development.\nTo achieve this, we redefine the decision chain reward of the Decider with a function tied to the win rate constraint. The difference between the win rate constraint and the current win rate is calculated, and a threshold $\\epsilon$ determines the sign of the reward. The win rate difference is then converted into a controlled reward $c_{r_{\\text{ctrl.}}}$ and scaled within $[-s, s]$ using a factor $k$ as follows:\n\n$\\delta = (WR_{\\text{cons.}} - WR_{dc})^2$\n\n$r = \\text{tanh}\\left(k \\cdot \\frac{\\delta - \\epsilon^2}{k}\\right)$\n\n$c_{r_{\\text{ctrl.}}} = \\begin{cases}r \\cdot \\left(1 - \\frac{\\epsilon}{\\delta}\\right) \\cdot s & \\text{if } r > 0 \\\\r \\cdot \\frac{\\delta}{\\epsilon} \\cdot s & \\text{if } r < 0\\end{cases}$\n\nwhere $k$ is the smoothing factor for the tanh function.\nThis reward function is designed to encourage the agent to stabilize its win rate by providing positive rewards for small deviations and negative rewards for large deviations from the win rate constraint. The scaling of rewards within a specific range enhances gradient information, guiding the agent's learning process."}, {"title": "III. EXPERIMENTS", "content": "This section describes the experimental details to evaluate the performance of DVM. We conducted experiments com-paring the DVM with different LLM agents in playing the Werewolf game. All these games were conducted in a 9-player setup, consisting of 3 werewolves, 1 seer, 1 witch, 1 hunter, and 3 villagers. The evaluation focuses on two main aspects: the controllability of the agents and the overall performance of the framework.\n\nControllability of the Agent\nThe controllability of the agent is evaluated by comparing the actual win rates achieved by the agents against the specified win rate constraints. illustrates the performance of different agents under different win rate constraints. In our DVM, we adjusted the win rate constraint provided to the Decider. For the other methods, we simply incorporated the constraints into their prompts in the form of text.\nThe result indicates that the other methods are not sensitive to the win rate constraints specified in their prompts. Their actual win rates show little fluctuation regardless of changes to these constraints. In contrast, our method demonstrates a noticeable upward trend in actual win rates as the win rate constraints increase, even though there remains a gap between the achieved win rates and the target constraints. This trend indicates that our agents exhibit a degree of controllability.\nAdditionally, we observed that our agents perform more effectively when targeting lower win rate constraints. However, as the win rate constraints are raised, it becomes increasingly challenging for our agent to meet these higher targets. This observation is understandable, given the intrinsic difficulty of pushing an agent to exceed its top performance limits. Our main goal is to adjust the agent's capabilities to achieve a specific win rate that is beneath its optimal potential, thereby showcasing our ability to control its performance.\n\nOverall Performance of Agent Framework\nWe selected 600 games from the FanLang-9 dataset and from games played by different agents. We then randomly sampled discussion content and voting patterns from these games as our test set. We conducted a comparative analysis between our DVM and other methods to assess their perfor-mance in predicting players' identities. As detailed in Table I, our framework surpasses other methods across two prediction tasks.\nBesides, we selected Thinker as the baseline and evaluated the win rates of different methods against Thinker under unre-stricted conditions. For DVM, we set the win rate constraint to 100%. We conducted 30 games under each setting. The results in Table II show that our proposed framework outperforms other methods, and the ablation study in Table III demonstrates the effectiveness of each component. Specifically, our method achieves a win rate of 63.3% for the villager, 66.6% for the werewolf, and 53.3% for the other roles."}, {"title": "IV. CONCLUSION", "content": "In this paper, we introduced DVM, a novel framework for creating controllable LLM agents in SDGs like Werewolf. DVM features three core components: Predictor, Decider, and Discussor, each contributing to the agent's analytical, decision-making, and communication skills. By integrating reinforcement learning with a unique decision chain reward mechanism and a win rate-constrained reward function, DVM allows agents to dynamically adjust their gameplay proficiency to meet specified win rates. Experimental results show that DVM outperforms existing methods and successfully main-tains target win rates, advancing LLM agents' adaptive and balanced gameplay in SDGs and providing new insights into the development of controllable LLM agents."}]}