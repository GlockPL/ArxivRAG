{"title": "Provable Hyperparameter Tuning for Structured Pfaffian Settings", "authors": ["Maria-Florina Balcan", "Anh Tuan Nguyen", "Dravyansh Sharma"], "abstract": "Data-driven algorithm design automatically adapts algorithms to specific application domains, achieving better performance. In the context of parameterized algorithms, this approach involves tuning the algorithm's parameters using problem instances drawn from the problem distribution of the target application domain. This can be achieved by maximizing empirical utilities that measure the algorithms' performance as a function of their parameters, using problem instances.\nWhile empirical evidence supports the effectiveness of data-driven algorithm design, providing theoretical guarantees for several parameterized families remains challenging. This is due to the intricate behaviors of their corresponding utility functions, which typically admit piece-wise and discontinuity structures. In this work, we present refined frameworks for providing learning guarantees for parameterized data-driven algorithm design problems in both distributional and online learning settings.\nFor the distributional learning setting, we introduce the Pfaffian GJ framework, an extension of the classical GJ framework, that is capable of providing learning guarantees for function classes for which the computation involves Pfaffian functions. Unlike the GJ framework, which is limited to function classes with computation characterized by rational functions, our proposed framework can deal with function classes involving Pfaffian functions, which are much more general and widely applicable. We then show that for many parameterized algorithms of interest, their utility function possesses a refined piece-wise structure, which automatically translates to learning guarantees using our proposed framework.\nFor the online learning setting, we provide a new tool for verifying dispersion property of a sequence of loss functions, a sufficient condition that allows no-regret learning for sequences of piece-wise structured loss functions where the piece-wise structure involves Pfaffian transition boundaries. We use our framework to provide novel learning guarantees for many challenging data-driven design problems of interest, including data-driven linkage-based clustering, graph-based semi-supervised learning, and regularized logistic regression.", "sections": [{"title": "1 Introduction", "content": "Data-driven algorithm design [GR16, Bal20] is a modern approach that develops and analyzes algorithms based on the assumption that problem instances come from an underlying application domain. Unlike traditional worst-case or average-case analyses, this approach leverages observed problem instances to design algorithms that achieve high performance for specific problem domains. In many application domains [BNVW17, BDSV18, BKST22], algorithms are parameterized, meaning they are equipped with tunable hyperparameters which significantly influence their performance. We develop general techniques applicable to a large variety of parameterized algorithm families for the selection of domain-specific good algorithms by learning the hyperparameters from the problem instances coming from the domain.\nTypically, the performance of an algorithm is evaluated by a specific utility function. In more concrete terms, for an algorithm parameterized by $a \\in A$, consider the utility function class $U = \\{u_a : \\mathcal{X} \\rightarrow [0, H] \\mid a \\in A\\}$, where $u_a(x)$ gauges the performance of the algorithm with parameters $a$ when inputting a problem instance $x$. In the data-driven algorithm design setting, we assume an unknown underlying distribution $\\mathcal{D}$ over $\\mathcal{X}$, representing the application domain on which the algorithm operates. Consequently, designing algorithms tailored to a specific domain corresponds to the optimal selection of parameters $a$ for the given domain.\nApplications of data-driven algorithm design span various domains, including low-rank approximation [IVY19, IWW21, LLL+23], sparse linear systems solvers [LGM+20], dimensionality reduction [ALN21], among others. Its empirical success [IVY19, ACC+11, IWW21] underscores the necessity for a theoretical understanding of this approach. Intensive efforts have been made towards theoretical understanding for data-driven algorithm design, including learning guarantees for numerical linear algebra methods [BIW22], tuning regularization parameters for regression problems [BKST22, BNS24], unsupervised and semi-supervised learning [BDW18, BDL20, BS21], application in integer and mixed-integer programming [BDSV18, BPSV21], to name but a few.\nPrior theoretical work on data-driven algorithm design focuses on two main settings: distributional (also known as statistical/batch) learning [BDD+21, BIW22] and online learning [BDV18]. In the distributional learning setting, there is a learner trying to optimize hyperparameters for the algorithm within a specific domain, given access to problem instances from that domain. In this case, the question is about the sample complexity: How many problem instances are required to learn good parameters that guarantee the algorithm's performance in that domain? In the online learning setting, there is a sequence of problem instances chosen by an adversary arriving over time. The goal now is to design a no-regret learning algorithm: adjusting the algorithm's parameters on the fly so that the difference between the average utility and the utility corresponding to the best parameters in hindsight diminishes over time.\nThe main challenge in establishing learning guarantees for the utility function classes lies in the complex structure of the utility function. In other words, even a very small perturbation in $a$ can lead to a drastic change in the performance of the algorithm, making the analysis of such classes of utility functions particularly challenging. In response to this challenge, prior works take an alternative approach by analyzing the dual utility functions $U^* = \\{u_x : A \\rightarrow [0, H] \\mid x \\in \\mathcal{X}\\}$, which often admits piece-wise structured behavior [BDD+21, BNVW17, BIW22].\nBuilding upon this observation, in the distributional learning setting, Balcan et al. [BDD+21] propose a general approach that analyzes the learnability of the utility function class via the piece and boundary functions class induced by the piece-wise structure. Targeting a more restricted scenario, Bartlett et al. [BIW22] introduced a refinement for the GJ framework [GJ93], which establishes tighter guarantees for utility function classes of which the piece-wise structure involves only rational functions. In the online learning setting, Balcan et al. [BDV18] introduce the dispersion condition, which serves as a sufficient condition allowing no-regret learning for piece-wise Lipschitz functions. Essentially, the dispersion property emphasizes that if the discontinuity of utility function sequences does not densely concentrate in any small region of parameter space, no-regret learning is possible. Despite their broad applicability, these generalized frameworks exhibit inherent"}, {"title": "2 Related works", "content": "Data-driven algorithm design. Data-driven algorithm design [GR16, Bal20] is a modern approach for automatically configuring algorithms and making them adaptive to specific application domains. In contrast to conventional algorithm design and analysis, which predominantly focuses on worst-case or average-case scenarios, data-driven algorithm design posits the existence of an (unknown) underlying problem distribution that dictates the problem instance that algorithms encounter. The primary objective shifts to identifying optimal configurations for such algorithms, leveraging available problem instances at hand from the same application domain. Empirical works have consistently validated the effectiveness of data-driven algorithm approaches in various domains, including matrix low-rank approximation [IVY19, IWW21], matrix sketching [LLL+23], mixed-integer linear programming [CKFB24], among others. These findings underscore the application of the data-driven algorithm design approach in real-world applications.\nDistributional learning guarantee for data-driven algorithm design. Motivated by the empirical success of data-driven algorithm design, there is an emerged line of work that focuses on theoretically analyzing the underlying mechanism, mostly focusing on providing generalization guarantees. This includes learning guarantee for data-driven algorithm designs of low-rank approximation and sketching [BIW22], learning metrics for clustering [BDL20, BS21, BDW18], integer and mixed-integer linear programming [BPSV21, BPSV22a, BDSV18], hyperparameter tuning for regularized regression [BKST22, BNS24], decision tree learning [BS24], robust nearest-neighbors [BBSZ23] among others.\nRemarkably, Balcan et al. [BS21] introduced a general framework for establishing learning guarantees for problems that admit a specific piece-wise structure. Despite its broad applicability, the framework exhibits limitations: (1) it requires an intermediate task of analyzing the dual of piece and boundary functions, which is not trivial, and (2) it fails to incorporate extra useful information about piece and boundary functions, sometimes leading to sub-optimal bounds. Building on this insight, Bartlett et al. [BIW22] instantiates a refinement of the classical GJ framework [GJ93], offering an improved learning guarantee for data-driven algorithm design where the piece-wise structure involves only rational functions. However, it is essential to note that their framework cannot be applied beyond rational structures."}, {"title": "3 Preliminaries", "content": "We adhere closely to the problem setting and notation introduced by Balcan et al. [BDD+21]. Our focus is on parameterized algorithms, where the parameter belongs to a set $A \\subseteq \\mathbb{R}^d$. Let $\\mathcal{X}$ represent the set of input problem instances. The performance of the algorithm corresponding to parameter $a \\in A$ on input $x \\in \\mathcal{X}$ is given by the utility function $u(x, a)$ where $u : \\mathcal{X} \\times A \\rightarrow [0, H]$. The utility function class of that parameter is then defined as $U = \\{u_a : \\mathcal{X} \\rightarrow [0, H] \\mid a \\in A\\}$. We then define the dual utility function class $U^* = \\{u_x : A \\rightarrow [0, H] \\mid x \\in \\mathcal{X}\\}$, where $u_x(a) := u(x, a)$, which consists of utility functions obtained by varying the parameter $a$ for fixed problem instances from $\\mathcal{X}$. The dual-class $U^*$ plays an important role in our analysis, which we will discuss later.\nDistributional learning. In contrast to traditional worst-case or average-case algorithm analysis, we assume the existence of an underlying problem distribution $\\mathcal{D}$ over $\\mathcal{X}$, which encapsulates information about the relevant application domain. Under such an assumption, our goal is to determine how many problem instances are sufficient to learn the near-optimal parameters of the algorithm for such application-specific problem"}, {"title": "4 Pfaffian GJ framework for data-driven algorithm design", "content": "In a classical work, Goldberg and Jerrum [GJ93] introduced a comprehensive framework for bounding the VC-dimension (or pseudo-dimension) of parameterized function classes exhibiting a specific property. They proposed that if any function within a given class can be computed via a specific type of computation, named a GJ algorithm, consisting of fundamental operators such as addition, subtraction, multiplication, division, and conditional statements, then the pseudo-dimension of such a function class can be effectively upper bounded. The bound depends on the running time of the algorithm, offering a convenient approach to reduce the task of bounding the complexity of a function class into the more manageable task of counting the number of operators.\nHowever, a bound based on runtime can often be overly conservative. Recently, Bartlett et al. [BIW22] instantiated a refinement for the GJ framework. Noting that any intermediate values the GJ algorithm computes are rational functions of parameters, Bartlett et al. proposed more refined complexity measures of the GJ framework, namely the predicate complexity and the degree of the GJ algorithm. Informally, the predicate complexity and the degree are the number of distinct rational functions in conditional statements and the highest order of those rational functions, respectively. Remarkably, based on the refined complexity measures, Bartlett et al. showed a refined bound, demonstrating its efficacy in various cases, including applications on data-driven algorithm design for numerical linear algebra.\nIt is worth noting that the GJ algorithm has limitations as it can only accommodate scenarios where intermediate values are rational functions. Specifically, it does not capture more prevalent classes of functions,"}, {"title": "4.1 Pfaffian functions", "content": "We present the foundational concepts of Pfaffian chains, Pfaffian functions, and their associated complexity measures. Introduced by Khovanskii [Kho91], Pfaffian function analysis is a tool for analyzing the properties of solution sets of Pfaffian equations. We note that these techniques have been previously used to derive an upper bound on the VC-dimension of sigmoidal neural networks [KM97].\nWe first introduce the notion of a Pfaffian chain. Intuitively, a Pfaffian chain consists of an ordered sequence of functions, in which the derivative of each function can be represented as a polynomial of the variables and previous functions in the sequence.\nDefinition 2 (Pfaffian Chain, [Kho91]). A finite sequence of continuously differentiable functions $\\eta_1, ..., \\eta_q : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ and variables $a = (a_1,...,a_d) \\in \\mathbb{R}^d$ form a Pfaffian chain $C(a, \\eta_1, ..., \\eta_q)$ if there are real polynomials $P_{i,j}(a, \\eta_1, ..., \\eta_j)$ in $a_1, ..., a_d, \\eta_1, ..., \\eta_j$, for for all $i \\in [d]$ and $j \\in [q]$, such that\n$$\n\\frac{\\partial \\eta_j}{\\partial a_i} = P_{i,j}(a, \\eta_1, ..., \\eta_j).\n$$\nWe now define two complexity notations for Pfaffian chains, termed the length and Pfaffian degree, that dictate the complexity of a Pfaffian chain. The length of a Pfaffian chain is the number of functions that appear on that chain, while the Pfaffian degree of a chain is the maximum degree of polynomials that can be used to express the partial derivative of functions on that chain. Formal definitions of Pfaffian chain length and Pfaffian degree are mentioned in Definition 3.\nDefinition 3 (Complexity of Pfaffian chain). Given a Pfaffian chain $C (a, \\eta_1, ..., \\eta_q)$, as defined in Definition 2, we say that the length of $C$ is $q$, and Pfaffian degree of $C$ is $\\max_{i,j} deg(P_{i,j})$.\nGiven a Pfaffian chain, one can define the Pfaffian function, which is simply a polynomial of variables and functions on that chain.\nDefinition 4 (Pfaffian functions, [Kho91]). Given a Pfaffian chain $C (a, \\eta_1, ..., \\eta_q)$, as defined in Definition 2, a Pfaffian function over the chain $C$ is a function of the form $g(a) = Q(a, \\eta_1, ..., \\eta_q)$, where $Q$ is a polynomial in variables $a$ and functions $\\eta_1, ..., \\eta_q$ in the chain $C$.\nThe concepts of the Pfaffian chain, functions, and complexities may be a bit abstract to unfamiliar readers. To help readers better grasp the concepts of Pfaffian chains and Pfaffian functions, here are some simple examples."}, {"title": "4.2 Pfaffian GJ Algorithm", "content": "We now present a formal definition of the Pfaffian GJ algorithm, which shares similarities with the GJ algorithm but extends its capabilities to compute Pfaffian functions as intermediate values, in addition to basic operators and conditional statements. This improvement makes the Pfaffian GJ algorithm significantly more versatile compared to the classical GJ framework.\nDefinition 5 (Pfaffian GJ algorithm). A Pfaffian GJ algorithm $\\Gamma$ operates on real-valued inputs $a \\in A \\subseteq \\mathbb{R}^d$, and can perform three types of operations:\n*   Arithmetic operators of the form $v\" = v \\odot v'$, where $\\odot \\in \\{+, -, \\times, \\div\\}.\n*   Pfaffian operators of the form $v\" = \\eta(v)$, where $\\eta : \\mathbb{R} \\rightarrow \\mathbb{R}$ is a Pfaffian function.\n*   Conditional statements of the form \u201cif $v \\geq 0$ . . . else . . . \u201c.\nHere $v$ and $v'$ are either inputs or (intermediate) values previously computed by the algorithm.\nDefinition 6 (Pfaffian chain associated with Pfaffian GJ algorithm). Given a Pfaffian GJ algorithm $\\Gamma$ operating on real-valued inputs $a \\in A \\subseteq \\mathbb{R}^d$, we say that a Pfaffian chain $C$ is associated with $\\Gamma$ if all the intermediate values computed by $\\Gamma$ is a Pfaffian function from the chain $C$.\nThe main difference between the classical GJ algorithm [GJ93, BIW22] and the Pfaffian GJ algorithm is that the latter allows Pfaffian operators in its computation. By leveraging the fundamental properties of Pfaffian chains and functions, we can easily show that all intermediate functions computed by a specific Pfaffian GJ algorithm come from the same Pfaffian chain (see Appendix B.1 for details). This remarkable property enables us to control the complexity of the Pfaffian GJ algorithm by controlling the complexity of the corresponding Pfaffian chain. We formalize this claim in the following lemma."}, {"title": "5 Pfaffian piece-wise structure for data-driven distributional learning", "content": "In this section, we propose the Pfaffian piece-wise structure for function classes, a refinement of the piece-wise decomposable structure introduced by Balcan et al. [BDD+21]. Compared to their piece-wise decomposable structure, our proposed Pfaffian piece-wise structure incorporates additional information about the Pfaffian structures of piece and boundary functions, as well as the maximum number of forms that the piece functions can take. We argue that the additional information can be derived as a by-product in many data-driven algorithm design problems, but being left out in the framework by Balcan et al. [BDD+21]. We then show that if the dual utility function class of a parameterized algorithm admits the Pfaffian piece-wise structure, we can establish an improved learning guarantee, compared to the framework by Balcan et al. [BDD+21], for the algorithm by leveraging our proposed Pfaffian GJ framework. Additionally, we propose a further refined argument for the case where all dual utility functions share the same boundary structures, which leads to further improved learning guarantee."}, {"title": "5.1 Prior generalization framework for piece-wise structured utility functions in data-driven algorithm design and its limitations", "content": "In this section, we discuss the utility of the Pfaffian GJ framework in providing learning guarantees for problems related to data-driven algorithm design. Many parameterized algorithms, such as combinatorial algorithms"}, {"title": "5.2 A refined piece-wise structure for data-driven algorithm design", "content": "In this section, we propose a more refined approach to derive learning guarantees for data-driven algorithm design problems where the utility functions exhibit a Pfaffian piece-wise structure. The key difference between our proposed frameworks and the framework by Balcan et al. [BDD+21] is that we consider the following additional factors: (1) Both piece and boundary functions are Pfaffian functions from the same Pfaffian chain, and (2) the maximum number of the distinct forms that the piece function can admit. Later, we will argue that by leveraging those extra structures, we can get a better pseudo-dimension upper bound by a logarithmic factor, compared to using the framework by [BDD+21]. The Pfaffian piece-wise structure is formalized as below.\nDefinition 8 (Pfaffian piece-wise structure). A function class $H \\subseteq \\mathbb{R}$ that maps domain $\\mathcal{Y} \\subseteq \\mathbb{R}^d$ to $\\mathbb{R}$ is said to be $(k_F, k_{\\mathcal{G}}, q, M, \\Delta, d)$ piece-wise structured if the following holds: for every $h \\in H$, there are at most $k_{\\mathcal{G}}$ boundary functions of the forms $I(g_1(y) \\geq 0), ..., I(g_k(y) \\geq 0)$, where $k \\leq k_{\\mathcal{G}}$, and a piece function $f_{h,b}$ for each binary vector $b \\in \\{0, 1\\}^k$ such that for all $y \\in \\mathcal{Y}, h(y) = f_{h,b_y}(y)$ where $b_y = (I(g_1(y) \\geq 0),..., I(g_k(y) \\geq 0)) \\in \\{0, 1\\}^k$. Moreover, the piece functions $f_{h,b}$ can take on of at most $k_F$ forms, i.e. $\\{f_{h,b} \\mid b \\in \\{0, 1\\}^k\\}| \\leq k_F$, and all the piece and boundary functions are Pfaffian functions of degree at most $\\Delta$ over a Pfaffian chain $C_h$ of length at most $q$ and Pfaffian degree at most $M$.\nAn intuitive illustration of Pfaffian piece-wise structure and its comparison with the piece-wise structure by Balcan et al. [BDD+21] can be found in Figure 1. For a data-driven algorithm design problem with corresponding utility function class $U = \\{u_a : \\mathcal{X} \\rightarrow [0, H] \\mid a \\in A\\}$ where $A \\subseteq \\mathbb{R}^d$, we can see that if its dual utility function class $U^* = \\{u_x : A \\rightarrow \\mathbb{R} \\mid x \\in \\mathcal{X}\\}$ admits the Pfaffian piece-wise structure as in Definition 8, then it can be computed using the Pfaffian GJ algorithm (see Figure 2 for a visualization). Therefore, we can"}, {"title": "5.3 Analyzing via approximation function class", "content": "In many applications, we might want to analyze the utility function class $\\mathcal{U}$ indirectly by studying a surrogate utility function class $\\mathcal{V}$ that is \"sufficiently close\" to $\\mathcal{U}$. There are several reasons for this approach. For instance, if we do not know the closed form of $\\mathcal{U}$, it becomes challenging to analyze its pseudo-dimension [BNS24]. In such cases, we may consider a surrogate function class $\\mathcal{V}$ with a closed-form expression for convenience. Another scenario arises when the function class $\\mathcal{U}$ is too complex, and analyzing a simpler surrogate function class $\\mathcal{V}$ might yield better bounds [BSV20].\nIn this section, we consider a scenario where we approximate the parameterized utility function over a predefined partition $A_1, ..., A_n$ of the parameter space $A$. Formally, a partition $\\mathcal{P}$ of a set $A$ is a collection $\\{A_1, ..., A_n\\}$ is a collection of non-empty subsets $A_i$ of $A$ that are pairwise disjoint and whose union is the entire set $A$. This is a special case, as for every problem instance $x$, the dual function $u_x^*$ exhibits the same discontinuity structure, which can be leveraged to obtain a tighter bound. The following lemma highlights"}, {"title": "6 Applications of the Pfaffian piece-wise structure framework", "content": "In this section, we demonstrate how to leverage our proposed framework to establish new distributional learning guarantees, significantly expanding the scope of algorithmic families data-driven algorithm design problems."}, {"title": "6.1 Data-driven agglomerative hierarchical clustering", "content": "Agglomerative hierarchical clustering (AHC) [MC12] is a versatile, two-stage clustering approach widely employed across various domains. In the first stage, data is organized into a hierarchical clustering tree, determining the order in which data points are merged into clusters. Subsequently, in the second stage, the cluster tree is pruned according to a specific objective function, of which some common choices are k-mean k-median, and k-center [Llo82, XW05], among others, to obtain the final clusters.\nThe first stage of AHC involves carefully designing linkage functions, which measure the similarity between clusters and determine the pair of clusters to be merged at each step. These linkage functions require a pairwise distance function $d$ between data points and calculate the distance between clusters based on the distances of their constituent points in a specific manner. Common linkage functions include single-linkage, complete-linkage, and average-linkage, with numerous variants interpolating between these simple functions employed in practical applications [ABV17, SMB+03, WNN+10]. It is important to note that if two linkage functions generate the same hierarchical cluster tree, they will yield the same final clusters, irrespective of the objective function used in the subsequent pruning stage.\nAlthough linkage functions are a crucial component of AHC, they are generally chosen heuristically without any theoretical guarantees. Recently, Balcan et al. [BNVW17] proposed a data-driven, provable approach for designing linkage functions. Similar to other data-driven settings, their analysis operates under the assumption that there exists an unknown, application-specific distribution for clustering instances. They then provide learning guarantees for some simple families of linkage functions, parameterized by a single parameter, that interpolates among single, complete, and average linkage. However, they assume that the pairwise distance function $d$ is fixed, whereas in practice, multiple distance functions, each with distinct properties and benefits, are combined to achieve better performance [BBC+05]. Subsequent work by Balcan et al. [Bal20] proposes combining multiple pairwise distance functions by jointly learning their weights and the parameters of the linkage function. However, their analysis holds only under a strict assumption on the linkage function and distance functions. In particular, it requires that the merge function $m$ is $2$-point-based, meaning that the distance $m_{\\delta}(A, B)$ between two clusters $A, B$ only depends on the distance $d(a, b)$ between two points $a \\in A$ and $b \\in B$. Moreover, that pair of points must depend only on the ordering of pairwise distances, in the sense"}, {"title": "6.1.1 Problem setting", "content": "Given a set of $n$ points $S \\in \\mathcal{X}^m$, where $\\mathcal{X}$ denotes the data domain, and a distance function $\\delta : \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}_{\\geq 0}$, the goal is to partition $S$ into clusters such that the intra-cluster distance is minimized, and the inter-cluster distance is maximized. In the AHC approach, we first design a linkage function $m_{\\delta}$ based on $\\delta$, where $m_{\\delta}(A, B)$ specifies the distance between two clusters $A$ and $B$. The cluster tree construction algorithm starts with $n$ singleton clusters, each containing a single data point, and successively merges the pair of clusters $A, B$ that minimizes the cluster-wise distance $m_{\\delta}(A, B)$. This sequence of merges yields a hierarchical cluster tree, with the root corresponding to the entire set $S$, leaf nodes corresponding to individual points in $S$, and each internal node representing a subset of points in $S$ obtained by merging the point sets corresponding to its two child nodes. Subsequently, the cluster tree is pruned to obtain the final clusters via a dynamic programming procedure that optimizes a chosen objective function. Common objective functions include k-means, k-medians, and k-centers, among others. Importantly, given a fixed objective function, if two linkage functions generate the same cluster tree for a given dataset, they will yield the same final clusters.\nAs discussed previously, the point-wise distance function $\\delta$ can be a convex combination of several distance functions chosen from $\\delta = \\{\\delta_1, ..., \\delta_L\\}$, i.e., $\\delta = \\delta_{\\beta} = \\sum_{i=1}^L \\beta_i\\delta_i$, where $\\beta = (\\beta_1,..., \\beta_L) \\in \\Delta(L)$. Here $\\Delta(L)$ denotes the $(L - 1)$-dimensional simplex. The combined distance function $\\delta_{\\beta}$ is then used in the linkage function. In this work, we consider the following parameterized families of linkage functions:"}, {"title": "6.1.2 Generalization guarantees for data-driven hierarchical clustering", "content": "In this section, we will leverage our proposed framework (Theorem 5.2) to establish the upper bounds for the pseudo-dimension of $\\mathcal{H}_1$ described above.\nTheorem 6.1. Let $\\mathcal{H}_1$ be a class of functions\n$$\n\\mathcal{H}_1 = \\{u_\\mathcal{H}_1 : (S, \\delta) \\leftrightarrow u(\\mathcal{A}^{\\alpha, \\beta}_1(S, \\delta)) \\mid \\alpha \\in \\mathbb{R} \\cup \\{-\\infty, +\\infty\\}, \\beta \\in \\Delta([L])\\}\n$$\nmapping clustering instances $(S, \\delta))$ to $[0,1]$ by using merge functions from class $\\mathcal{M}_1$ and an arbitrary objective function. Then $Pdim(\\mathcal{H}_1) = O(n^4L^2)$.\nProof. The high-level idea is to show that the utility function exhibits a piece-wise structure: its parameter space is partitioned by multiple boundary functions, and within each region, the cluster tree remains unchanged, implying that the utility function is constant. We then characterize the number and complexity of the boundary functions, which we show belong to a Pfaffian system. Subsequently, we can utilize our framework to obtain a bound on the pseudo-dimension of $\\mathcal{H}_1$."}, {"title": "6.2 Data-driven graph-based semi-supervised learning", "content": "Semi-supervised learning [CSZ10] is a learning paradigm where labeled data is scarce due to expensive labeling processes. This paradigm leverages unlabeled data in addition to a small set of labeled samples for effective learning. A common semi-supervised learning approach is the graph-based method [ZG09, CSZ10], which captures relationships between labeled and unlabeled data using a graph. In this approach, nodes represent data points, and edges are constructed based on the similarity between data point pairs, measured by a given distance function. Optimizing a labeling function on this graph helps propagate labels from the labeled data to the unlabeled data.\nA large body of research focuses on how to learn such labeling functions given the graph, including using st-mincuts [BC01], optimizing harmonic objective with soft mincuts [ZGL03], label propagation [ZG02], among others. However, under the assumption that the graph well describes the relationship amongst data, it is known that all algorithms for learning the labeling function above are equivalent [ZG09]. This also highlights the importance of the graph in graph-based semi-supervised learning.\nDespite its significance, the graph is usually considered given or constructed using heuristic methods without theoretical guarantees [Zhu05, ZCP04]. Recently, Balcan and Sharma [BS21] proposed a novel data-driven approach for constructing the graph, by learning the parameters of the graph kernel underlying the graph construction, from the problem instances at hand. Each problem instance $\\mathcal{P}$ consists of sets of labeled $\\mathcal{L}$ and unlabeled data $\\mathcal{U}$ and a distance metric $d$. Assuming that all problem instances are drawn from an underlying, potentially unknown distribution, they provide guarantees for learning near-optimal graph kernel parameters for such a distribution.\nNonetheless, they consider only a single distance function, whereas in practical applications, combining multiple distance functions, each with its unique characteristics, can improve the graph quality and typically result in better outcomes compared to utilizing a single metric [BBC+05]. In this section, we consider a novel and more practical setting for data-driven graph-based semi-supervised learning, where we learn the parameters of the commonly-used Gaussian RBF kernel $w_{\\sigma,\\beta}(u, v) = \\exp(-\\delta(u, v)/\\sigma^2)$ and the weights $\\beta$ of $\\delta = \\sum_{i=1}^L \\beta_i\\delta_i$ is a combination multiple distance functions for constructing the graph."}, {"title": "6.2.1 Problem settings", "content": "In the graph-based semi-supervised learning with Gaussian RBF kernel, we are given a few labeled samples $\\mathcal{L} \\subset \\mathcal{X} \\times \\mathcal{Y}$, a large number of unlabeled points $\\mathcal{U} \\subset \\mathcal{X}$, and a set of distance functions $\\delta = \\{\\delta_1, ..., \\delta_L\\}$, where $\\delta_i : \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}_{> 0}$ for $i \\in [L]$. Here, $\\mathcal{X}$ denotes the data space, and $\\mathcal{Y} = \\{0, 1\\}$ denotes the binary classification label space. To extrapolate labels from $\\mathcal{L}$ to $\\mathcal{U}$, a graph $G_{\\sigma,\\beta}$ is constructed with the node set $\\mathcal{L}\\cup\\mathcal{U}$ and edges weighted by the Gaussian RBF graph kernel $w_{\\sigma,\\beta}(u, v) = \\exp(-\\delta(u, v)/\\sigma^2)$, where $\\sigma$ is the bandwidth parameter, and $\\delta = \\sum_{i=1}^L \\beta_i\\delta_i$ is a convex combination of the given distance functions. After constructing the graph $G_{\\sigma,\\beta}$, a popular graph labeling algorithm called the harmonic method [ZGL03] is employed. It assigns soft labels by minimizing the following quadratic objective:"}, {"title": "6.2.2 Generalization guarantee for data-driven semi-supervised learning with Gaussian RBF kernel and multiple distance functions", "content": "We now instantiate the main result in this section", "1": ".", "beta)": "upsilon^{\\sigma, \\beta} (\\mathcal{L},\\mathcal{U}, \\delta)$ is piece-wise constant and characterize the number and complexity of the boundary functions which we will show belong to a Pfaffian system. Our main result implies a bound on the pseudo-dimension of $\\mathcal{G}$. The quadratic objective minimization has a closed-form solution [ZGL03"}]}