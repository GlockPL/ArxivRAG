{"title": "Highly Parallelized Reinforcement Learning Training with Relaxed Assignment Dependencies", "authors": ["Zhouyu He", "Peng Qiao", "Rongchun Li", "Yong Dou", "Yusong Tan"], "abstract": "As the demands for superior agents grow, the training complexity of Deep Rein-forcement Learning (DRL) becomes higher. Thus, accelerating training of DRL has become a major research focus. Dividing the DRL training process into subtasks and using parallel computation can effectively reduce training costs. However, current DRL training systems lack sufficient parallelization due to data assignment between subtask components. This assignment issue has been ignored, but address-ing it can further boost training efficiency. Therefore, we propose a high-throughput distributed RL training system called TianJi. It relaxes assignment dependencies between subtask components and enables event-driven asynchronous communica-tion. Meanwhile, TianJi maintains clear boundaries between subtask components. To address convergence uncertainty from relaxed assignment dependencies, TianJi proposes a distributed strategy based on the balance of sample production and consumption. The strategy controls the staleness of samples to correct their quality, ensuring convergence. We conducted extensive experiments. TianJi achieves a convergence time acceleration ratio of up to 4.37 compared to related comparison systems. When scaled to eight computational nodes, TianJi shows a convergence time speedup of 1.6 and a throughput speedup of 7.13 relative to XingTian, demon-strating its capability to accelerate training and scalability. In data transmission efficiency experiments, TianJi significantly outperforms other systems, approaching hardware limits. TianJi also shows effectiveness in on-policy algorithms, achieving convergence time acceleration ratios of 4.36 and 2.95 compared to RLlib and XingTian. TianJi is accessible at https://github.com/HiPRL/TianJi.git.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning(RL) is a powerful approach for addressing decision-making problems. An agent is trained to interact with the environment based on its policy to maximize long-term rewards Li [2017], Chadi and Mousannif [2023]. To address complex spatial problems, value function ap-proximation methods using neural networks are commonly employed Sutton et al. [1999], Grondman et al. [2012], Schulman et al. [2015], Wilcox et al. [2022]. Deep Reinforcement Learning (DRL) combines the strong expressive power of deep learning with the decision-making capabilities of RL, offering a versatile system for flexible decision-making and controlMnih et al. [2013], Lillicrap et al. [2015], Schulman et al. [2017]. Research and applications of DRL are growing more complexBrittain et al. [2024], Wang et al. [2023], Weihs et al. [2020], Szot et al. [2023], Chen et al. [2023], Jiang et al. [2024]. In complex scenarios, the scale of the state and action spaces is vast, and computational complexity grows exponentiallyKakade [2003], Neumann and Gros [2022], Li [2023], Liu et al. [2022], Shen et al. [2023b]. Modern RL methods require substantial computational resourcesSilver"}, {"title": "2 Related Work", "content": "2.1 Parallelization of Reinforcement Learning\nMost RL methods are variants of the Temporal Difference (TD)Sutton [1988]. TD is a commonly used method in RL that updates the value function based on each step's trajectory. The TD(0), shown in Algorithm 1, represents the general RL training process. The agent interacts with the environment through multiple discrete time steps t. At each time step t, the agent observes a state s and selects an action a from a set of possible actions according to the policy @ (denoted as \u2460). The agent then interacts with the environment, which advances the simulator to yield the next state s' and a scalar reward r (denoted as \u2461). Steps \u2460 and \u2461 are repeated until the agent reaches a terminal state or the specified time step, which is simplified to one step in Algorithm 1. During training, the TD error is calculated from the collected trajectories to learn the state value function under the policy from a series of incomplete episodes (denoted as \u2462).\nComputation task \u2460, \u2461, and can be parallelized either individually or in combination. Abstracting these tasks as separate or combined components leads to varying levels of data isolation due to different degrees of computational isolation. The dependencies between these tasks are as follows:  requires the state s from \u2461 and the latest network parameters @ from (denoted as I); \u2461 needs the action a from \u2460(denoted as II); and \u2462 requires the trajectories collected from \u2461 (denoted as III). When parallelizing, it is crucial to consider the constraints imposed by these dependencies. Therefore, relaxing these dependencies is key to achieving high levels of parallelism. ShenShen et al. [2023a] indicated that the parallelism and asynchrony of A3C accelerate convergence in theory. GALA further relaxes the model dependency among workers, resulting in faster convergence than the synchronized A2C. Inspired by previous theoretical and experimental works, we relax assignment dependencies to better utilize computational resources.\n2.2 Parallelization of Existing DRL Training Systems\nThe Gorila-style architectureNair et al. [2015], Liang et al. [2018], Pan et al. [2022], Zhu et al. [2023], Mei et al. [2023] abstracts sampling-related and as actors, and \u2462 as learners, enabling parallel execution. This approach isolates sample production from consumption but requires components to exchange samples and parameters, increasing inter-component communication. Gorila contributed to increasing sampling parallelism by decoupling sampling from model training, allowing each to be parallelized independently. Subsequent developments in Gorila-style architectures include XingTian, which addressed communication bottlenecks with an asynchronous communication channel; SRL, which proposed a data flow-based abstraction to improve resource efficiency. Although these improvements optimize sample quality, communication, and task-resource mapping, they do not contribute to enhancing training parallelism. As shown in Figure 1, Gorila-style architectures are constrained by component assignment dependencies(I, II, III), resulting in significant idle periods.\nand \nThe SEED-style architecture Espeholt et al. [2019], Petrenko et al. [2020], Zhu et al. [2023], Mei et al. [2023] identifies inefficiencies in resource utilization present in the Gorila-style architecture.In Gorila, actors alternate between two dissimilar tasks, and 2. In SEED-style, model-related tasks and  are abstracted into a learner, while actors handle 2. At each environment step, the state is sent to the learner, which infers the action and then returns it to the actor. This introduces a new issue: latency. To address this issue, SEED implements a high-performance gRPC library, and SampleFactory"}, {"title": "3 This Work", "content": "We found that assignment dependencies significantly restrict parallelization. To address this, we proposed TianJi, a distributed reinforcement learning training system that relaxes these dependencies. TianJi abstracts and  into a single actor, thereby mitigating the negative impact of II. TianJi introduces decentralized data-driven training, transforming assignment dependencies I and III between components into asynchronous data exchanges (see Section 3.1). Decentralized computing leads to data isolation, resulting in inter-process data transfers. Efficient data transfer is the base of achieving high-throughput training. TianJi implements asynchronous communication with hidden overheads (see Section 3.2). Although relaxing assignment dependencies can introduce data staleness, TianJi addresses this by employing a distributed strategy based on production-consumption balance, ensuring training convergence through sample distribution adjustment. This strategy also facilitates scaling, enabling the system to overcome performance bottlenecks (see Section 3.3).\n3.1 Decentralized Data-driven Training\nExisting systems typically use a global core component to allocate and coordinate multiple instances, ensuring equivalence between the parallel and serial implementations. These implementations maintain assignment dependencies either fully or partially, resulting in significant idle time during training, as illustrated in Figure 1. Therefore, TianJi reorganizes DRL computations to enhance parallelism and introduces a decentralized data-driven training approach. In this approach, assignment dependency II is reduced to intracomponent dependency, while dependencies I and III are converted to asynchronous data exchanges.\nTianJi abstracts sampling-related computation tasks (\u2460, \u2461) as \"Actor\" and model updates (3) as \"Learner,\" parallelizing these at the component level. Figure 2 illustrates the training process of TianJi. Once computation begins, each role independently performs looped computations and triggers data exchanges between components at appropriate times. In the Learner Loop, steps (0, 2) are executed repeatedly. Step  involves loading trajectories from local storage, using an asynchronous communication trigger to determine if data reception is needed. Step involves learning and updating the model, which is then stored locally, with a trigger to decide whether sending model parameters is necessary. In the Actor Loop, steps (0-6) are executed in sequence. Step involves the agent interacting with the environment to obtain trajectories or states, which are stored locally, with a"}, {"title": "3.2 Event-driven Asynchronous Communication", "content": "Distributed components create data isolation, necessitating communication between components. Experiments have shown that communication can sometimes take more time than computationPan et al. [2022], Zhao et al. [2023]. We observed that learners do not require samples to be uniformly distributed across actors. Therefore, common practices such as group communication or uniformly requesting data from each actor can be both impractical and costly. TianJi employs an actively pushing asynchronous communication mode, where communication requests are initiated by the sender. Once the data at the sender is ready, it immediately triggers the communication, actively pushing the data to the intended recipient. The receiver uses a probing mechanism. The sender does not wait for an immediate request and continues its computations. Communication is confirmed during subsequent data transmissions, allowing computation and communication to overlap.\nWe will analyze the overlap between communication and computation in TianJi by examining critical path transitions during the sample collection phase. In this phase, the Actor collects data and sends it to the Buffer, where the samples are received. Figure 3 illustrates the critical path transitions in both \"Single Actor\" and \"Multiple Actors\" scenarios. In the \"Single Actor\" scenario, the sampling time exceeds the reception time. Causing computation overshadows communication, making the Actor's sampling the critical determinant of the path. Conversely, in the \"Multiple Actors\" scenario, increasing the number of actors effectively reduces the sampling time. When the sampling time becomes shorter than the communication time, communication can no longer be entirely concealed, shifting the key path to the Buffer. Although computation can obscure communication, this concealment has its limitations.\nThe theoretical collect time $T_c$ can be calculated. When the communication hidden limit is not reached, the communication time can be masked by the computation time, placing the critical path within the \"Actor\". When the communication hidden limit is reached, the communication time can no longer be masked by the computation time, causing the critical path to shift to the Buffer. From the Buffer's perspective, increasing the number of actors is equivalent to linearly accelerating the sample collection process. Based on the critical path transition rules, we can derive the following formula:\n$T_c = \\begin{cases}\n(T_{sp} + T_{sd}) \\times N_s, & \\text{if } \\frac{(T_{sp} + T_{sd})}{N_A} > T_{rv} \\\\\nT_{rv} \\times N_s, & \\text{others}\n\\end{cases}$ (1)\nwhere, $N_s$ is the number of collected trajectories, $T_{sp}$ is the time of a single sampling, $T_{sd}$ and $T_{rv}$ are the time of sending and receiving, respectively. $N_A$ is the number of actors.\nEvent-driven asynchronous communication eliminates redundant waiting and enables the overlap of computation and communication, thereby facilitating more efficient and rational data exchange between components."}, {"title": "3.3 Distribution Strategy", "content": "Distributed strategies handle the configuration of components and their allocation to computing resources, impacting the location of performance bottlenecks and resource utilization. When a perfor-mance bottleneck arises, efforts outside this bottleneck do not contribute to convergence. Extensive experiments have demonstrated that performance bottlenecks vary across different algorithms, appli-cations, and hardware. These studies indicate that some algorithms are more prone to bottlenecks in specific areas, resulting in fixed strategies targeting particular performance issues. However, because of the unpredictability of performance bottlenecks, fixed optimization techniques cannot guarantee overall throughput improvement. ReverbCassirer et al. [2021] proposes SPI rate limiting, which controls when items can be inserted into or sampled from a table. However, SPI reflects the ratio over a period of time, varies significantly across different scales, and involves blocking operations. SPI is both inaccurate and inefficient. Therefore, TianJi proposes a distributed strategy based on performance analysis and production-consumption balance to enable scalable training that addresses"}, {"title": "4 Evaluation", "content": "The evaluation answer the following questions: (1) How does TianJi compare to existing systems in terms of overall performance optimization (Section 4.2)? (2) How effective are the key components as demonstrated by the ablation study (Section 4.3)?\n4.1 Setting\nTestbed. We configured two hardware platforms for our experiments. The first platform is a CPU-only Slurm cluster with 8 computing nodes. Each node is equipped with 2 Intel Xeon Gold 6248"}, {"title": "4.2 Overall Performance Comparison", "content": "ApeX relaxes certain assignment dependencies and is the most similar work to TianJi. Since ApeX does not support multi-core processing or multiple learners, experiments are conducted on a single GPU machine. Figure 4(a) illustrates the episode return over time, highlighting the learning performance of each system. The current episode return is derived from clipped rewards within a single life. TianJi demonstrates superior learning performance compared to ApeX. TianJi's distributed strategy, based on production-consumption balance, corrects the sample distribution and ensures. In contrast, ApeX still encounters assignment dependencies due to the data consistency requirements of PER. Conversely, TianJi achieves true relaxation of assignment dependencies, significantly enhancing computational efficiency, as shown in Figure 4(b). ApeX's open-source implementation is nonscalable, restricting comparisons to a single node. Theoretically, PER's negative impact on computational efficiency worsens with an increasing number of components.\nAdditional performance comparisons were carried out on eight CPU machines, as illustrated in Figure 4(c). For on-policy method learning Cartpole, we compared the final time to reach an average"}, {"title": "4.3 Ablation Study", "content": "4.3.1 Data Transmission Efficiency\nWe conducted a virtual communication experiment to evaluate the data transmission efficiency of TianJi and other systems. The experiment, which followed the communication pattern of a standard DistRL algorithm, concluded after collecting 10,000 samples, with collection time recorded. As a baseline, we selected XingTian, which offers an asynchronous communication channel and exhibits superior data transmission efficiency compared to other systems. We evaluated the data transmission efficiency by measuring the time to receive the 10,000 samples(referred to as collection time) and the data transmission throughput(referred to as throughput).\nFigure 5(a) shows that TianJi consistently outperforms XingTian in data transmission efficiency for all message sizes under single-node and single-actor conditions. Figure 5(b) depicts how collection time varies with the number of actors while the message size remains fixed at 512 KB. The figure indicates that, for a fixed sample time(e.g., 0.001), communication bottlenecks occur with more than four actors, and increasing the number of actors beyond this does not reduce collection time(see Section 3.2.2 for analysis). With a sample time of 0.01, the communication bottleneck occurs with 16 actors. TianJi consistently exhibits lower sample collection times than XingTian across all conditions. Notably, TianJi has a lower communication bottleneck than XingTian. After reaching the bottleneck, the collection time depends only on receiver operations(e.g., local storage), and TianJi's reception time remains lower.Figure 5(c) shows how communication efficiency varies as the number of actors increases across multiple computing nodes, with actors evenly distributed among 4 nodes. TianJi consistently has lower collection times than XingTian in all scenarios. Figure 5(d) shows that TianJi achieves throughput close to Ethernet bandwidth. Testing was conducted between two nodes with Ethernet bandwidth of 125 MB/s and InfiniBand(IB) bandwidth of 12.5 GB/s. Under Ethernet conditions, both TianJi and XingTian show similar performance trends: as message size increases, throughput gradually rises and approaches Ethernet bandwidth. In contrast, under IB conditions, TianJi exceeds previous throughput limits, while XingTian does not. Throughput decreases after 2048 KB due to increased message generation time with larger message sizes. TianJi reaches the Ethernet bandwidth limits, demonstrating superior communication efficiency compared to XingTian.\n4.3.2 The Impact of the Distributed Strategy on Convergence\nTianJi proposes a distributed strategy based on production consumption balance to correct sample distribution and maximize training throughput. This section examines the impact of sample staleness and computational-resource mapping on training. Training was conducted on a single node with PPO. The optimal computational-resource mapping, identified by the distributed strategy, includes 2 learners and 4 actors, using 16 cores, denoted as L2A8-C16. A random computational-resource mapping, labeled L1A14-C16, was also evaluated. After introducing asynchrony, simulations with serial sample distributions were conducted using two sample ratios: 1:1 (denoted as New) and 1:8 (denoted as Staleness). Four sets of control experiments were conducted. The results, shown in Figure 6(d), indicate that optimal computational-resource mapping and production consumption balance significantly accelerate computational efficiency and enhance learning performance."}, {"title": "5 Conclusion", "content": "We found that assignment dependencies are the key to ensuring the equivalence for paralleling DRL training, but also the bottleneck that hinders the performance. Therefore, we introduce TianJi, which reduces assignment dependencies between subtasks using a decentralized, data-driven training approach combined with event-driven asynchronous communication. Additionally, TianJi proposes a distributed strategy based on balancing sample production and consumption to alleviate the con-vergence issue introduced by relaxing the dependencies. Experimental results show that relaxing assignment dependencies and improving sample quality significantly enhance computational and training efficiency. TianJi achieves a 1.6-fold speedup in convergence time and a 7.13-fold speedup in training throughput compared to XingTian when scaled to eight machines. The ablation study demon-"}, {"title": "A the Design Principles of Distributed RL System", "content": "With the increasing demand for computational power in deep reinforcement learning(DRL), acceler-ating training has become a research hotspot. Developing effective software platforms is essential for advancing research in this field. Distributed DRL systems can effectively use substantial com-putational resources to train large-scale data and models. However, what kind of code is useful for research? The deep learning community has identified several key practices, such as modular design, flexibility, usability, performance optimization, visualization, and comprehensive logging. In contrast to deep learning, RL is more irregular. Consequently, there is no consensus on developing DRL sys-tem at present. Different research objectives result in various trade-offs in software design. Users of DRL system generally fall into two categories: 1) foundational research, which focuses on advancing algorithms; and 2) application deployment, which involves training agents on computational devices. Although researchers and application deployers may prioritize different software requirements, they both demand usability, programming flexibility, versatility, and high performance. Therefore, an effective distributed DRL system should possess these characteristics to address a wide range of use cases."}, {"title": "B Usability and Programming Flexibility", "content": "As agents evolves to handle more complex interactions, creating reusable software for DRL research has become increasingly challenging. TianJi addresses this challenge by balancing usability with programming flexibility and providing standardized, streamlined interfaces. To improve the user experience, TianJi separates user interactions from internal system details. Users can initiate train-ing using configuration files without needing to understand the specifics of the system's internal implementation. For custom extensions, TianJi offers base class templates for computational and communication tasks. Users can implement custom logic by rewriting the relevant functions in these templates.\nB.1 Usage\nIn terms of usability, TianJi achieves a high level of abstraction and separation in user experience. This design enables users to start training tasks solely through configuration files, without delving into or understanding the complexities of the underlying implementation. This approach simplifies the workflow and significantly lowers the barrier to entry, allowing even users with limited programming experience to easily conduct model training. By separating user interactions from internal details, the system significantly enhances development efficiency.\nAs shown in Figure 7, the configuration file includes several categories of parameters: distribution parameters, training parameters, model parameters, and environment parameters. Each category provides a comprehensive set of options, enabling users to configure the system flexibly according to specific needs. Distribution parameters allow users to adjust component and inter-component computations, such as the number of computational tasks, computational resources, and number of rollouts. Model parameters offer fine-grained control over model architecture and hyperparameter settings, while environment parameters allow the selection of different runtime environments and their associated settings. By offering an extensive range of parameter settings, the system meets the versatility requirements of TianJi across various application scenarios.\nB.2 Custom Extensions\nRL practitioners are often not system engineers and may be unfamiliar with mapping computation flows between software and hardware. To address this, TianJi provides a non-intrusive method for implementing custom algorithms, offering significant convenience to users. TianJi features clearly defined components, enabling users to create custom extensions by modifying specific templates. For example, extending a custom task environment is straightforward; users only need to rewrite the relevant functions in the predefined templates (as shown in Figure 8) without altering other parts of the code. This design lowers the barrier to developing custom functionalities, allowing more RL practitioners to focus on algorithm development without being bogged down by underlying system implementation details."}, {"title": "C Versatility", "content": "TianJi offers excellent programming flexibility, supporting a wide range of algorithms, applications, and computational platforms. TianJi currently implements various algorithms, including on-policy, off-policy, and multi-agent reinforcement learning (MARL) algorithms. It supports multiple applica-tion scenarios, such as classic control, Atari, Multi-Agent Particle Environment (MPE), and StarCraft II Micro-RTS (SMAC). Additionally, TianJi supports a variety of computational platforms, including"}, {"title": "D Further Discussion on Distributed Strategies", "content": "TianJi introduces a distributed strategy based on balancing sample production and consumption. This strategy addresses sample quality issues and overcomes performance bottlenecks in scaling. Section D.1 examines how sample staleness affects learning efficiency, demonstrating that balancing sample production and consumption effectively corrects sample distribution. Section D.2 discusses whether accelerating processes outside performance bottlenecks improves learning efficiency. This distributed strategy, grounded in performance analysis and the principle of sample production-consumption balance, corrects sample quality issues and overcomes performance bottlenecks, ensuring convergence in training.\nD.1 Sample Staleness\nThis section discusses the impact of sample staleness on learning efficiency. Proximal Policy Opti-mization (PPO) is an on-policy algorithm in which sample production and consumption throughputs are equal. Thus, sample staleness can be controlled by adjusting the buffer size. When the batch size equals the buffer size, all samples are new, referred to as New. When the buffer size is twice the batch size, the samples consist of half new and half old samples, referred to as Lag2, and so on. As shown in Figure 12.a, sample staleness significantly affects learning efficiency. A higher proportion of old samples results in slower convergence. Thus, in fully asynchronous training, controlling the ratio of old to new samples through a producer-consumer balance to approximate the serial ratio is an effective way to accelerate convergence.\nD.2 Performance Bottlenecks\nSection D.2 discusses whether accelerating processes outside performance bottlenecks improves learning efficiency. With two fixed learners, each using four cores, the theoretical sample consumption throughput remains constant. As the number of actors increases from 2 to 24, sample production throughput also increases. According to Table 1, at L2A8, sample consumption throughput reaches its maximum. L2A8 indicates a configuration with 2 learners and 8 actors; other configurations follow this pattern. This indicates that at L2A2 and L2A4, the performance bottleneck is due to sample production throughput during exploration. Beyond L2A8, the performance bottleneck shifts to sample consumption throughput during training. At this point, increasing the number of actors further enhances sample production throughput but does not improve learning efficiency. In fact, due to factors such as increased interactions, sample consumption throughput may decrease, leading to reduced learning efficiency. Figure 12.b shows learning efficiency for different numbers of actors.Comparing L2A8 to L2A4, mitigating performance bottlenecks in sample production led to improved learning efficiency. However, comparing L2A16 to L2A8, accelerating computations outside the performance bottleneck did not improve learning efficiency. Therefore, addressing performance bottlenecks is crucial for accelerating learning. TianJi's distributed strategy is designed to achieve this, as detailed in Sections 3.3 and 4.3.3 of the main paper."}]}