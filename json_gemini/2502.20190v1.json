{"title": "Highly Parallelized Reinforcement Learning Training with Relaxed Assignment Dependencies", "authors": ["Zhouyu He", "Peng Qiao", "Rongchun Li", "Yong Dou", "Yusong Tan"], "abstract": "As the demands for superior agents grow, the training complexity of Deep Rein-forcement Learning (DRL) becomes higher. Thus, accelerating training of DRL has become a major research focus. Dividing the DRL training process into subtasks and using parallel computation can effectively reduce training costs. However, current DRL training systems lack sufficient parallelization due to data assignment between subtask components. This assignment issue has been ignored, but address-ing it can further boost training efficiency. Therefore, we propose a high-throughput distributed RL training system called TianJi. It relaxes assignment dependencies between subtask components and enables event-driven asynchronous communica-tion. Meanwhile, TianJi maintains clear boundaries between subtask components. To address convergence uncertainty from relaxed assignment dependencies, TianJi proposes a distributed strategy based on the balance of sample production and consumption. The strategy controls the staleness of samples to correct their quality, ensuring convergence. We conducted extensive experiments. TianJi achieves a convergence time acceleration ratio of up to 4.37 compared to related comparison systems. When scaled to eight computational nodes, TianJi shows a convergence time speedup of 1.6 and a throughput speedup of 7.13 relative to XingTian, demon-strating its capability to accelerate training and scalability. In data transmission efficiency experiments, TianJi significantly outperforms other systems, approaching hardware limits. TianJi also shows effectiveness in on-policy algorithms, achieving convergence time acceleration ratios of 4.36 and 2.95 compared to RLlib and XingTian. TianJi is accessible at https://github.com/HiPRL/TianJi.git.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning(RL) is a powerful approach for addressing decision-making problems. An agent is trained to interact with the environment based on its policy to maximize long-term rewards. To address complex spatial problems, value function approximation methods using neural networks are commonly employed. Deep Reinforcement Learning (DRL) combines the strong expressive power of deep learning with the decision-making capabilities of RL, offering a versatile system for flexible decision-making and control. Research and applications of DRL are growing more complex. In complex scenarios, the scale of the state and action spaces is vast, and computational complexity grows exponentially. Modern RL methods require substantial computational resources."}, {"title": "2 Related Work", "content": "Most RL methods are variants of the Temporal Difference (TD). TD is a commonly used method in RL that updates the value function based on each step's trajectory. The TD(0), shown in Algorithm 1, represents the general RL training process. The agent interacts with the environment through multiple discrete time steps t. At each time step t, the agent observes a state s and selects an action a from a set of possible actions according to the policy \\theta (denoted as \u2460). The agent then interacts with the environment, which advances the simulator to yield the next state s' and a scalar reward r (denoted as \u2461). Steps \u2460 and \u2461 are repeated until the agent reaches a terminal state or the specified time step, which is simplified to one step in Algorithm 1. During training, the TD error is calculated from the collected trajectories to learn the state value function under the policy from a series of incomplete episodes (denoted as \u2462).\nComputation task \u2460, \u2461, and can be parallelized either individually or in combination. Abstracting these tasks as separate or combined components leads to varying levels of data isolation due to different degrees of computational isolation. The dependencies between these tasks are as follows: \u2460 requires the state s from \u2461 and the latest network parameters \\theta from \u2462(denoted as I); \u2461 needs the action a from \u2460(denoted as II); and \u2462 requires the trajectories collected from \u2461 (denoted as III). When parallelizing, it is crucial to consider the constraints imposed by these dependencies. Therefore, relaxing these dependencies is key to achieving high levels of parallelism. Shen indicated that the parallelism and asynchrony of A3C accelerate convergence in theory. GALA further relaxes the model dependency among workers, resulting in faster convergence than the synchronized A2C. Inspired by previous theoretical and experimental works, we relax assignment dependencies to better utilize computational resources."}, {"title": "2.2 Parallelization of Existing DRL Training Systems", "content": "The Gorila-style architecture abstracts sampling-related \u2460 and \u2461 as actors, and \u2462 as learners, enabling parallel execution. This approach isolates sample production from consumption but requires components to exchange samples and parameters, increasing inter-component communication. Gorila contributed to increasing sampling parallelism by decoupling sampling from model training, allowing each to be parallelized independently. Subsequent developments in Gorila-style architectures include XingTian, which addressed communication bottlenecks with an asynchronous communication channel; SRL, which proposed a data flow-based abstraction to improve resource efficiency. Although these improvements optimize sample quality, communication, and task-resource mapping, they do not contribute to enhancing training parallelism. As shown in Figure 1, Gorila-style architectures are constrained by component assignment dependencies(I, II, III), resulting in significant idle periods.\nThe SEED-style architecture identifies inefficiencies in resource utilization present in the Gorila-style architecture.In Gorila, actors alternate between two dissimilar tasks, \u2460 and \u2461. In SEED-style, model-related tasks \u2460 and \u2462 are abstracted into a learner, while actors handle \u2461. At each environment step, the state is sent to the learner, which infers the action and then returns it to the actor. This introduces a new issue: latency. To address this issue, SEED implements a high-performance gRPC library, and SampleFactory"}, {"title": "3 This Work", "content": "We found that assignment dependencies significantly restrict parallelization. To address this, we proposed TianJi, a distributed reinforcement learning training system that relaxes these dependencies. TianJi abstracts \u2460 and \u2461 into a single actor, thereby mitigating the negative impact of II. TianJi introduces decentralized data-driven training, transforming assignment dependencies I and III between components into asynchronous data exchanges (see Section 3.1). Decentralized computing leads to data isolation, resulting in inter-process data transfers. Efficient data transfer is the base of achieving high-throughput training. TianJi implements asynchronous communication with hidden overheads (see Section 3.2). Although relaxing assignment dependencies can introduce data staleness, TianJi addresses this by employing a distributed strategy based on production-consumption balance, ensuring training convergence through sample distribution adjustment. This strategy also facilitates scaling, enabling the system to overcome performance bottlenecks (see Section 3.3)."}, {"title": "3.1 Decentralized Data-driven Training", "content": "Existing systems typically use a global core component to allocate and coordinate multiple instances, ensuring equivalence between the parallel and serial implementations. These implementations maintain assignment dependencies either fully or partially, resulting in significant idle time during training, as illustrated in Figure 1. Therefore, TianJi reorganizes DRL computations to enhance parallelism and introduces a decentralized data-driven training approach. In this approach, assignment dependency II is reduced to intracomponent dependency, while dependencies I and III are converted to asynchronous data exchanges.\nTianJi abstracts sampling-related computation tasks (\u2460, \u2461) as \"Actor\" and model updates (\u2462) as \"Learner,\" parallelizing these at the component level. Figure 2 illustrates the training process of TianJi. Once computation begins, each role independently performs looped computations and triggers data exchanges between components at appropriate times. In the Learner Loop, steps (\u2460, \u2461) are executed repeatedly. Step \u2462 involves loading trajectories from local storage, using an asynchronous communication trigger to determine if data reception is needed. Step \u2463 involves learning and updating the model, which is then stored locally, with a trigger to decide whether sending model parameters is necessary. In the Actor Loop, steps (\u2464-\u2465) are executed in sequence. Step \u2464 involves the agent interacting with the environment to obtain trajectories or states, which are stored locally, with a"}, {"title": "3.2 Event-driven Asynchronous Communication", "content": "Distributed components create data isolation, necessitating communication between components. Experiments have shown that communication can sometimes take more time than computation. We observed that learners do not require samples to be uniformly distributed across actors. Therefore, common practices such as group communication or uniformly requesting data from each actor can be both impractical and costly. TianJi employs an actively pushing asynchronous communication mode, where communication requests are initiated by the sender. Once the data at the sender is ready, it immediately triggers the communication, actively pushing the data to the intended recipient. The receiver uses a probing mechanism. The sender does not wait for an immediate request and continues its computations. Communication is confirmed during subsequent data transmissions, allowing computation and communication to overlap.\nWe will analyze the overlap between communication and computation in TianJi by examining critical path transitions during the sample collection phase. In this phase, the Actor collects data and sends it to the Buffer, where the samples are received. Figure 3 illustrates the critical path transitions in both \"Single Actor\" and \"Multiple Actors\" scenarios. In the \"Single Actor\" scenario, the sampling time exceeds the reception time. Causing computation overshadows communication, making the Actor's sampling the critical determinant of the path. Conversely, in the \"Multiple Actors\" scenario, increasing the number of actors effectively reduces the sampling time. When the sampling time becomes shorter than the communication time, communication can no longer be entirely concealed, shifting the key path to the Buffer. Although computation can obscure communication, this concealment has its limitations.\nThe theoretical collect time \\( T_c \\) can be calculated. When the communication hidden limit is not reached, the communication time can be masked by the computation time, placing the critical path within the \"Actor\". When the communication hidden limit is reached, the communication time can no longer be masked by the computation time, causing the critical path to shift to the Buffer. From the Buffer's perspective, increasing the number of actors is equivalent to linearly accelerating the sample collection process. Based on the critical path transition rules, we can derive the following formula:\n\\[ T_c = \\begin{cases} (T_{sp} + T_{sd}) \\times N_s, & \\text{if } \\frac{(T_{sp} + T_{sd})}{T_{rv}} > N_A \\\\ T_{rv} \\times N_s, & \\text{others} \\end{cases} \\]\nwhere, \\( N_s \\) is the number of collected trajectories, \\( T_{sp} \\) is the time of a single sampling. \\( T_{sd} \\) and \\( T_{rv} \\) are the time of sending and receiving, respectively. \\( N_A \\) is the number of actors.\nEvent-driven asynchronous communication eliminates redundant waiting and enables the overlap of computation and communication, thereby facilitating more efficient and rational data exchange between components."}, {"title": "3.3 Distribution Strategy", "content": "Distributed strategies handle the configuration of components and their allocation to computing resources, impacting the location of performance bottlenecks and resource utilization. When a performance bottleneck arises, efforts outside this bottleneck do not contribute to convergence. Extensive experiments have demonstrated that performance bottlenecks vary across different algorithms, applications, and hardware. These studies indicate that some algorithms are more prone to bottlenecks in specific areas, resulting in fixed strategies targeting particular performance issues. However, because of the unpredictability of performance bottlenecks, fixed optimization techniques cannot guarantee overall throughput improvement. Reverb proposes SPI rate limiting, which controls when items can be inserted into or sampled from a table. However, SPI reflects the ratio over a period of time, varies significantly across different scales, and involves blocking operations. SPI is both inaccurate and inefficient. Therefore, TianJi proposes a distributed strategy based on performance analysis and production-consumption balance to enable scalable training that addresses"}, {"title": "4 Evaluation", "content": "The evaluation answer the following questions: (1) How does TianJi compare to existing systems in terms of overall performance optimization (Section 4.2)? (2) How effective are the key components as demonstrated by the ablation study (Section 4.3)?"}, {"title": "4.1 Setting", "content": "Testbed. We configured two hardware platforms for our experiments. The first platform is a CPU-only Slurm cluster with 8 computing nodes. Each node is equipped with 2 Intel Xeon Gold 6248"}, {"title": "4.2 Overall Performance Comparison", "content": "ApeX relaxes certain assignment dependencies and is the most similar work to TianJi. Since ApeX does not support multi-core processing or multiple learners, experiments are conducted on a single GPU machine. Figure 4(a) illustrates the episode return over time, highlighting the learning performance of each system. The current episode return is derived from clipped rewards within a single life. TianJi demonstrates superior learning performance compared to ApeX. TianJi's distributed strategy, based on production-consumption balance, corrects the sample distribution and ensures. In contrast, ApeX still encounters assignment dependencies due to the data consistency requirements of PER. Conversely, TianJi achieves true relaxation of assignment dependencies, significantly enhancing computational efficiency, as shown in Figure 4(b). ApeX's open-source implementation is nonscalable, restricting comparisons to a single node. Theoretically, PER's negative impact on computational efficiency worsens with an increasing number of components.\nAdditional performance comparisons were carried out on eight CPU machines, as illustrated in Figure 4(c). For on-policy method learning Cartpole, we compared the final time to reach an average"}, {"title": "4.3 Ablation Study", "content": "We conducted a virtual communication experiment to evaluate the data transmission efficiency of TianJi and other systems. The experiment, which followed the communication pattern of a standard DistRL algorithm, concluded after collecting 10,000 samples, with collection time recorded. As a baseline, we selected XingTian, which offers an asynchronous communication channel and exhibits superior data transmission efficiency compared to other systems. We evaluated the data transmission efficiency by measuring the time to receive the 10,000 samples(referred to as collection time) and the data transmission throughput(referred to as throughput)."}, {"title": "4.3.2 The Impact of the Distributed Strategy on Convergence", "content": "TianJi proposes a distributed strategy based on production consumption balance to correct sample distribution and maximize training throughput. This section examines the impact of sample staleness and computational-resource mapping on training. Training was conducted on a single node with PPO. The optimal computational-resource mapping, identified by the distributed strategy, includes 2 learners and 4 actors, using 16 cores, denoted as L2A8-C16. A random computational-resource mapping, labeled L1A14-C16, was also evaluated. After introducing asynchrony, simulations with serial sample distributions were conducted using two sample ratios: 1:1 (denoted as New) and 1:8 (denoted as Staleness). Four sets of control experiments were conducted. The results, shown in Figure 6(d), indicate that optimal computational-resource mapping and production consumption balance significantly accelerate computational efficiency and enhance learning performance."}, {"title": "4.3.3 Scaling Beyond Performance Bottlenecks", "content": "Extensive experiments show that once training hits a performance bottleneck, adding resources beyond this point does not improve training performance. Predicting the location of this bottleneck is challenging because various algorithms, applications, and hardware configurations can create bottlenecks in different areas. This section will demonstrate how TianJi uses effective distribution methods to tackle different performance bottlenecks. TianJi achieves accelerated convergence beyond performance bottlenecks and scales training throughput to near-linear levels. Performance bottlenecks can be categorized into three types: sample-intensive (\u2460\u2461), training-intensive (\u2462), and communication-intensive. In the DQN(Cartpole) experiment, Figure 6(a) illustrates how sampling throughput, receiving throughput, and training throughput change as the number of actors increases in the DQN(Cartpole) experiment. The number of learners, resources, and computational load is kept constant, resulting in stable training throughput(blue dashed line). As the number of actors increases, sampling throughput(red solid line) also rises. The receiving throughput(yellow dashed line) aligns with the sampling throughput, indicating that no communication bottleneck is present at this stage. When the number of actors exceeds 4, the learner's training throughput becomes the bottleneck, and adding more actors does not accelerate performance. Figure 6(b) shows that increasing the number of physical cores for the learners and proportionally enlarging the batchsize can overcome the previous training bottleneck, shifting the equilibrium point to 16 actors. Further increasing the number of actors raises production throughput, but receiving and training throughput fall below this level. This indicates that both communication and training bottlenecks have been reached. Replicating the equilibrium configuration shown in Figure 6(b) and expanding it in groups, as depicted in Figure 6(c), shows that continuing to increase the number of actors results in a linear increase in sample receiving and training throughput. TianJi employs performance analysis and a distributed strategy based on production-consumption balance to dynamically allocate computation to resources, allowing for sustained expansion beyond performance bottlenecks."}, {"title": "5 Conclusion", "content": "We found that assignment dependencies are the key to ensuring the equivalence for paralleling DRL training, but also the bottleneck that hinders the performance. Therefore, we introduce TianJi, which reduces assignment dependencies between subtasks using a decentralized, data-driven training approach combined with event-driven asynchronous communication. Additionally, TianJi proposes a distributed strategy based on balancing sample production and consumption to alleviate the con-vergence issue introduced by relaxing the dependencies. Experimental results show that relaxing assignment dependencies and improving sample quality significantly enhance computational and training efficiency. TianJi achieves a 1.6-fold speedup in convergence time and a 7.13-fold speedup in training throughput compared to XingTian when scaled to eight machines. The ablation study demon-"}]}