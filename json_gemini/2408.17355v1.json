{"title": "Bidirectional Decoding:\nImproving Action Chunking via Closed-Loop Resampling", "authors": ["Yuejiang Liu", "Jubayer Ibn Hamid", "Annie Xie", "Yoonho Lee", "Maximilian Du", "Chelsea Finn"], "abstract": "Predicting and executing a sequence of actions without intermediate\nreplanning, known as action chunking, is increasingly used in robot learning from\nhuman demonstrations. However, its effects on learned policies remain puzzling:\nsome studies highlight its importance for achieving strong performance, while\nothers observe detrimental effects. In this paper, we first dissect the role of action\nchunking by analyzing the divergence between the learner and the demonstrator.\nWe find that longer action chunks enable a policy to better capture temporal de-\npendencies by taking into account more past states and actions within the chunk.\nHowever, this advantage comes at the cost of exacerbating errors in stochastic en-\nvironments due to fewer observations of recent states. To address this, we propose\nBidirectional Decoding (BID), a test-time inference algorithm that bridges action\nchunking with closed-loop operations. BID samples multiple predictions at each\ntime step and searches for the optimal one based on two criteria: (i) backward\ncoherence, which favors samples aligned with previous decisions, (ii) forward\ncontrast, which favors samples close to outputs of a stronger policy and distant\nfrom those of a weaker policy. By coupling decisions within and across action\nchunks, BID enhances temporal consistency over extended sequences while en-\nabling adaptive replanning in stochastic environments. Experimental results show\nthat BID substantially outperforms conventional closed-loop operations of two\nstate-of-the-art generative policies across seven simulation benchmarks and two\nreal-world tasks.", "sections": [{"title": "1 Introduction", "content": "The increasing availability of human demonstrations has spurred renewed interest in behavioral\ncloning [1, 2]. In particular, recent studies have highlighted the potential of learning from large-scale\ndemonstrations to acquire a variety of complex skills [3, 4, 5, 6, 7, 8]. However, this approach still\nstruggles with two common properties of human demonstrations: (i) strong temporal dependencies\nacross multiple steps, such as idle pauses [4] and latent strategies [9, 10], (ii) large style variability\nacross different demonstrations, including differences in proficiency [11] and preference [12]. Of-\ntentimes, both properties are prevalent yet unlabeled in collected data, posing significant challenges\nto traditional behavioral cloning, which typically learns a discriminative model to map an input state\nto a target action.\nIn response to these challenges, recent works have pursued a generative approach characterized by\ntwo key elements: (i) predicting a sequence of actions over multiple time steps and executing all or\npart of the sequence, known as action chunking [3] or receding horizon [4]; (ii) modeling the distri-\nbution of action chunks and sampling from the learned model in an independent [4, 13] or weakly\ndependent [3, 14] manner during deployment. Some studies find these elements crucial for learning\na performant policy in controlled laboratory scenarios [3, 4], while other recent work reports op-\nposite outcomes under practical conditions [6]. The reasons behind these conflicting results remain\nunclear."}, {"title": "2 Related Work", "content": "Behavioral Cloning. Learning from human demonstrations is becoming increasingly popular in\nrobot learning due to recent advances in robotic teleoperation interfaces [3, 20, 21, 22]. Generative\nBehavior cloning, which models the distribution of demonstrations, is particularly appealing due to\nits algorithmic simplicity and empirical efficacy [3, 17, 22, 23, 24, 25, 26]. However, a significant\nlimitation is compounding errors, where deviations from the training distribution accumulate over\ntime [27, 28]. These errors can be mitigated by gathering expert correction data [27, 29, 30, 31,\n32] or injecting noise during data collection [33, 34], but such strategies require additional time\nand effort from human operators. To address this, recent work proposes predicting a sequence\nof multiple actions into the future, known as action chunking, which reduces the effective control\nhorizon [3, 35, 36, 37]. By handling sequences of actions, action chunking is also better at handling\ntemporal dependencies in the data, such as idle pauses [4, 38] or multiple styles [11, 12, 39, 40]."}, {"title": "3 Analysis: Tradeoffs in Action Chunking", "content": "However, independently drawn action sequence samples may not preserve the necessary temporal\ndependencies for smooth and consistent execution. Our work provides a thorough analysis of action\nchunking and proposes a decoding algorithm to improve it.\nDecoding Algorithm. Test-time decoding algorithms have been studied in generative sequence\nmodeling for decades, with renewed attention driven by recent advances in large language model-\ning (LLM). A prominent approach focuses on leveraging internal metrics, e.g., likelihood scores,\nto improve the quality of generated sequences. Notable examples include beam search [41, 42],\ntruncated sampling [43, 44, 45], minimum Bayes risk decoding [46, 47], and many others [48, 49].\nAnother line of research explores the synergy of multiple generative models, such as contrastive\ndecoding [50] and speculative decoding [51], which jointly optimize for quality or efficiency. More\nrecently, several studies have highlighted the potential of guiding the decoding or sampling process\nthrough the use of an external discriminative model, such as a classifier [52] or reward model [53].\nIn the context of robot learning, Huang et al. [54] introduced a framework to guide LLM decoding\nfor long-horizon robotic planning. Similarly, Xu et al. [55] proposed the guidance of diffusion mod-\nels for manipulator design. To the best of our knowledge, our work is the first to explore decoding\nalgorithms for low-level robotic policy. We propose a decoding strategy centered on forward and\nbackward temporal consistency to address the inherent tradeoffs in action chunking."}, {"title": "3.1 Preliminaries", "content": "Consider a dataset of demonstrations D = {\u03c4i}Ni=1, where each demonstration \u03c4i consists of a\nsequence of state-action pairs \u03c4i = {(s1, a1), (s2, a2),\u2026\u2026,(sT, aT)} provided by a human ex-\npert. These demonstrations often exhibit strong temporal dependencies: an action at is not only\ndependent on the current state st, but also influenced by the previous k steps of states and ac-\ntions (st\u22121, at\u22121,\uff65\uff65\uff65,st\u2212k,at\u2212k) due to unobservable latent variables. Some latent variables can\nglobally influence an entire sequence (e.g., speed, left- vs right-handed), while others may locally\ninfluence specific segments within a sequence (e.g., planning strategies). Fig. 2\nillustrates the decision process of a human expert, highlighting the inherent temporal dependencies.\nTo model these temporal dependencies, recent behavior cloning methods have focused on learning\nthe joint distribution of future actions conditioned on past states (at, at+1,\uff65\uff65\uff65,at+l|st\u2212c,\u00b7\u00b7\u00b7, st),\nor more succinctly \u03c0(at:t+l|st\u2212c:t). Here, c denotes the number of past steps included as state\ninputs, and l represents the number of future steps for action outputs. Training such policies typically\ninvolves minimizing the divergence between the model distribution and the data distribution,\n$$\\pi = \\arg \\min_{\\pi} \\sum_{\\tau \\in D} \\sum_{s_{t-c:t}} L(\\pi(a_{t:t+l}|s_{t-c:t}), \\pi^*(a_{t:t+l}|s_{t-c:t})).$$\nUpon training completion, the policy is deployed by sampling a sequence of actions and executing a\nsubset or the entire sequence for h \u2208 [1, l] time steps without re-planning. This approach, commonly\nreferred to as action chunking [3], essentially takes in c states as context and executes h actions. We\nthus call it a (c, h)-policy.\nInterestingly, recent works have shown that the choices of context length c and action horizon h play\na crucial role in the empirical success of generative behavior cloning [3, 4, 6]. Specifically, extending\nthe context length c does not always improve performance, especially when human demonstrations\nare limited (refer to Appendix A.2 for more details). Instead, extending the action horizon h has\nbecome a common practice in the design of modern policies.\nHowever, a (c, h)-policy built with a short context length and a long action horizon stands in stark\ncontrast to human experts, which often consider a longer history while re-planning at each time step,\neffectively following a (k, 1)-policy. In the next section, we will analyze why, despite this difference,\nextending the action horizon can sometimes improve the learned policy in certain scenarios while\nlimiting it in others."}, {"title": "3.2 Analysis", "content": "To understand the influence of action chunking, we focus on the last time step of an action chunk,\nwhere the discrepancy between the expert policy and the learned policy is most pronounced. At this\ntime step t, the expert, which is a (k, 1)-policy written as \u03c0\u2217 := \u03c0\u2217(at|st\u2212k:t, Zt\u2212k:t), predicts at\nby conditioning on k steps of the past states and the corresponding latent variables. In contrast, a\nlearned (c, h)-policy, written as \u03c0(c,h) := \u03c0(c,h)(at | st\u2212h\u2212c:t\u2212h, at\u2212h:t\u22121) is constrained to observe\nc steps of the past states and its predicted actions over the past h \u2212 1 steps.\nConsidering that recent policies often use a short context length c and a moderate action horizon h,\nwe assume the range of temporal dependency modeled by a (c, h)-policy is limited:\nAssumption 1. The sum of context length and action horizon is less than the length of temporal\ndependency in expert demonstrations, c + h < k.\nAdditionally, since a (c, h)-policy observes only a subset of the states that the expert is conditioned\non, we assume that an optimal policy must reconstruct all missing information correctly:\nAssumption 2. An optimal \u03c0ch must infer the unobserved states based on the observed states and\nactions by modeling the transition dynamics P(st' | st'\u22121, at'\u22121) accurately for all time step t'.\nUnder these assumptions, the divergence between a learned policy and an expert policy is attributed\nto two factors: (i) the importance of unobserved states in predicting the current action, and (ii) the\ndifficulty of inferring them based on the available information. To more clearly see the influence\nof action horizon on these factors, we next compare the performance of two policies that have\nthe same context lengths but different action horizons, \u03c0h := \u03c0(c,h)(at|st\u2212h\u2212c:t\u2212h, at\u2212h:t\u22121) and\n\u03c0h+d := \u03c0(c,h+d)(at|st\u2212h\u2212d\u2212c:t\u2212h\u2212d, at\u2212h\u2212d:t\u22121), where d > 0 is an extended action horizon.\nAs illustrated in Fig. 3, each policy has access to unique information that is unavailable to the other.\n\u03c0h observes some recent states, where \u03c0h+d is only aware of the executed actions. On the other\nhand, \u03c0h+d has access to some earlier states and actions, which precede all information available to\n\u03c0h. We characterize the importance of observations as follows (formal definitions in Appendix C.1):\nDefinition (Expected Observation Advantage). If a policy can observe a state st, we say that it\nhas an observation advantage at over another policy that cannot observe it.\nDefinition (Maximum Inference Disadvantage). If a policy cannot observe a state st, the maxi-\nmum divergence arising from inferring it incorrectly is et.\nHence, we denote the observation advantage that \u03c0h gains from the observed recent states by af\nand the inference disadvantage it incurs from the earlier unobserved states by eb, whereas \u03c0h+d\nconversely gains ab but incurs ef.\nThe difficulty of inferring each unobserved state hinges on both the relevant observations as well\nas the environmental stochasticity. We quantify this difficulty as follows (formal definitions in Ap-\npendix C.1):\nDefinition (Forward Inference). Let Pf := P(st = gt|st\u22121 = gt\u22121,at\u22121) where gt and gt\u22121\nare the ground truth states in the deterministic environment at time t and t \u2212 1, respectively. In\ndeterministic environments, Pf = 1, whereas in stochastic settings, Pf is smaller."}, {"title": "Definition (Backward Inference)", "content": "Let Pb := P(st = gt|st+1 = gt+1) where gt and gt+1 are the\nground truth states in the deterministic environment at time t and t + 1, respectively. Since Pf is not\nconditioned on any action, it has higher entropy in general. In stochastic environments, Pb is small.\nGiven that the forward inference is generally easier than the backward inference, the performance\ndifference between \u03c0h and \u03c0h+d is bounded by the following (proofs are deferred to Appendix C)\nProposition 1 (Consistency-Reactivity Inequalities). Let L be a non-linear, convex loss function.\nLet S+ \u2282 {st\u2212k:t} be the states both the (c, h) and the (c, h + d) policies observe and let S\u00af :=\n{st\u2212k:t} \\ S+. Let C := {at\u2212h\u2212d:t\u22121} \u222a S+, G := {at, zt\u2212k:t} \u222a S\u00af. Then, we can bound the\nexpected loss of the (c, h + d)-policy and the (c, h)-policy as:\naf \u2212 eb (1 \u2212 P2d) < min EG [L(\u03c0h+d, \u03c0\u2217)|C] \u2212 min EG [L(\u03c0h, \u03c0\u2217)|C] \u2264 \u2212ab + ef(1 \u2212 P2d)\n\u03c0h+d\n\u03c0h\nRemark 1.1. Eq. (2) provides a general comparison of the performance of the two policies. Intu-\nitively, the advantage of each policy stems from the additional information it has access to (i.e. af\nfor \u03c0h and ab for \u03c0h+d) while the disadvantage is bounded by the maximum divergence arising from\ninferring missing information incorrectly (i.e. eb(1 \u2212 P2d) for \u03c0h and ef (1 \u2212 P2d) for \u03c0h+d).\nWe next examine two specific environmental settings: highly deterministic and highly stochastic.\nIn highly deterministic environments, while both policies need to infer the same number of unob-\nserved states, \u03c0h+d benefits from conditioning on additional actions, which may significantly aid in\ninferring the corresponding states through its action chunk. If the maximum errors ef arising from\ninferring these states are bounded, \u03c0h+d becomes strictly advantageous:\nCorollary 2 (Consistency in Deterministic Environments). In a highly deterministic environment, if\nat is temporally dependent on at least one state in {st\u2212h\u2212c\u2212d:t\u2212h\u2212c\u22121} and e f is finite,\nmin EG [L(\u03c0h+d, \u03c0\u2217)|C] < min EG [L(\u03c0h, \u03c0\u2217)|C]\n\u03c0h+d\n\u03c0h\nConversely, in highly stochastic environments, inferring the unobserved states is challenging, re-\ngardless of whether the actions are known. However, the recent states are likely more important\nthan earlier states for predicting the current action at. In this case, \u03c0h+d becomes strictly more\ndisadvantageous:\nCorollary 3 (Reactivity in Stochastic Environments). In a highly stochastic environment, if temporal\ndependency decreases over time, i.e., af > eb, then\nmin EG [L(\u03c0h+d, \u03c0\u2217)|C] > min EG [L(\u03c0h, \u03c0\u2217)|C]\n\u03c0h+d\n\u03c0h\nIn summary, there is no universally optimal action horizon across all conditions. Determining the ap-\npropriate action horizon requires careful consideration of (i) the length of temporal dependencies in\nthe demonstrations and (ii) the level of transition stochasticity present in an environment. When both\ntemporal dependencies and environmental stochasticities are significant, the vanilla action chunking\napproach leads to an inherent trade-off between these two competing factors."}, {"title": "4 Method: Bidirectional Decoding", "content": "As analyzed in \u00a73, action chunking facilitates the modeling of temporal dependencies in demon-\nstrations but sacrifices reactivity to unexpected states in stochastic environments. In this section, we\naddress this issue by bridging long action chunks with closed-loop operations. We will first outline\nthe general framework in \u00a74.1 and then describe two specific criteria in \u00a74.2."}, {"title": "4.1 Test-Time Search", "content": "Given a generative policy with context length c and prediction horizon l, an action chunk sampled\nat time t\n$$\\alpha \\sim \\pi_\\rho(a_t, a_{t+1},\\cdots, a_{t+l}|s_{t-c}, s_{t-c+1},...,s_t)$$\nis expected to adhere to a consistent latent strategy over the next l time steps. However, naive closed-\nloop operation of the policy entails executing only the first action of each predicted chunk, leading\nto a sequence of actions a(t)1, a(t+1)1, ..., a(t+l)1 sampled at different time steps. When multiple\nlatent strategies exist in the demonstrations and are learned by the policy (e.g., left versus right, stop\nversus go, fast versus slow), independently sampled action chunks may oscillate between different\nstrategies, leading to inconsistent behavior that diverges from the demonstrations.\nOur main hypothesis is that while the probability of any pair of samples sharing the same latent strat-\negy is low, the likelihood of finding a consistent pair from a large number of samples is significantly\nhigher. This intuition motivates us to cast the problem of closed-loop action chunking as searching\nfor the optimal action among a batch of plans sampled at each time step,\n$$a^* = \\arg \\min_{\\alpha \\in A} L_B(a) + L_F(a),$$\nwhere A is the set of sampled action chunks, LB and LF are two criteria measuring the temporal\ndependency with respect to the backward decision and forward plan, which we will describe next."}, {"title": "4.2 Bidirectional Criteria", "content": "Backward coherence. To preserve sufficient temporal dependency in closed-loop operations, a\nsequence of actions should (i) commit to one action chunk in the absence of unexpected changes\nand (ii) react smoothly to environmental changes. We use the action chunk selected at the previous\ntime step as a reference for enforcing coherence across time. Given the previous action chunk\na := a(t\u22121)t\u22121, a(t\u22121)t,\u00b7\u00b7\u00b7,a(t\u22121)t+l\u22121, we select action chunks that minimize the weighted sum of Euclidean\ndistances across the l \u2212 1 overlapping steps:\n$$L_B = \\sum_{\\tau=0}^{l-1} \\rho^{\\tau} \\|a_{t+\\tau}^{(t)} - a_{t+\\tau}^{(t-1)}\\|^2.$$\nHere, \u03c1 is a decay hyperparameter to account for growing uncertainty over time. This backward loss\nencourages similar latent strategies between neighboring steps, while allowing for gradual adapta-\ntion to unforeseen transition dynamics.\nForward contrast. An ideal policy should predict far enough into the future to capture the planning\ncapabilities inherent in human demonstrations. However, building such a policy can be challenging\nin practice due to modeling constraints and dataset limitations. Often, even the best policy available\nmay still produce a significant number of suboptimal plans. To address this, we introduce a forward\ncontrast objective to identify and reject these suboptimal plans. Specifically, we compare each\ncandidate plan with two sets of reference samples: one set from a stronger policy and the other set\nfrom a weaker one. We use a well-trained model as the stronger policy, and a model from an early\nunderfitting checkpoint or with a shorter prediction horizon as the weaker policy. Intuitively, the\nweaker policy cannot capture long-term planning as effectively as the stronger one. Our forward\ncontrast loss is thus framed as minimizing the average distance between a candidate plan and a set\nof positive samples while maximizing its average distance from the negative ones,\n$$L_F = \\dfrac{1}{N} (\\sum_{\\alpha^+ \\in A^+} \\sum_{\\tau=0}^{l} |a_{t+\\tau}^{(t)} - a_{t+\\tau}^{\\alpha^+}|^2 - \\sum_{\\alpha^- \\in A^-} \\sum_{\\tau=0}^{l} |a_{t+\\tau}^{(t)} - a_{t+\\tau}^{\\alpha^-}|^2)$$\nwhere A+ = A \\ {a} is the positive set predicted by the strong policy \u03c0, A\u2212 is the negative set\npredicted by the weaker one \u03c0', and N is the sample size.\nFig. 4 illustrates the combined effects of the backward coherence and forward contrast criteria on\nsample selection. Notably, not all samples in A+ and A\u2212 are necessarily subject to the same mode.\nTo mitigate this, we trim each set by removing samples that deviate significantly from the mode\nof the previous decision. This is achieved by summing over the K smallest distance values for\nin the positive and negative sets in Eq. (8). The full process of our decoding method is outlined\nin Algorithm 1. Since all steps in BID can be computed in parallel, the overall computational\noverhead remains modest on modern GPU devices."}, {"title": "4.3 Discussions", "content": "Interpretation of our method. Our method makes no changes to the learned policy; instead, it\nintervenes in the model distribution through sample selection. As illustrated in Fig. 11, randomly\nsampled sequences may be misaligned with both the previous decisions and the target demonstra-\ntions. Given a set of candidates, the backward step first identifies the behavioral mode from the past\ndecision stored in memory; the forward step then removes the samples with low likelihood under the\ntarget distribution using prior knowledge of positive and negative samples. By comparing samples\nacross time steps and model horizons, our method bridges the gap between the proposal and target\ndistributions during inference.\nRelation to recent methods. Our method builds upon the receding horizon [4] and temporal en-\nsembling [3] used in previous works, but with crucial distinctions. Receding horizon seeks a com-\npromise between temporal dependency and dynamic uncertainty by using a moderate action hori-\nzon (e.g., half of the prediction horizon), which is inevitably sup-optimal when both factors are\nprominent. Temporal ensembling strengthens dependency across chunks by averaging multiple de-\ncisions over time; however, weighted-averaging operations can be detrimental when consecutive\ndecisions fall into distinct modes. Our method more effectively addresses cross-chunk dependency\nthrough dedicated behavioral search and is not mutually exclusive with the previous methods. We\nwill demonstrate in the next section that combining our method with moving average can further\nimprove closed-loop action chunking."}, {"title": "5 Experiments", "content": "In this section, we present a series of experiments to answer the following questions:\n1. How does our theoretical analysis on action chunking manifest under different conditions?\n2. How does the proposed method affect the closed-loop operation of a policy with action chunking?\n3. How does the proposed method scale with large batch sizes and complement existing methods?\nTo this end, we will first validate our theoretical analysis through one-dimensional diagnostic simu-\nlations. We will then evaluate BID on seven tasks in three simulation benchmarks, including Push-\nT [4], RoboMimic [56], and Franka Kitchen [57]. We will subsequently examine the generality and\nscalability of our method under various base policies and sample sizes. We will finally demonstrate\nthe effectiveness of BID in two challenging real-world tasks involving dynamic objects."}, {"title": "5.1 One-Dimensional Diagnostic Simulations", "content": "Setup. We start with a diagnostic experiment in a one-dimensional state space {s0, s1,\u2026\u2026\u2026,s10},\nwhere s0 is the starting state and s10 is the goal state. The demonstrator plans to move forward by\none step in each state, except in s5 where it pauses unless the last five states visited were s5. Each\nforward move has a success probability of 1 \u2212 \u03b4, where \u03b4 denotes the level of stochastic noise in\nthe environment (as described in \u00a73.1). Given these demonstrations, we train a collection of policies"}, {"title": "5.2 Effects of BID on Closed-Loop Action Chunking", "content": "Setup. We next examine the effect of our decoding algorithm on closed-loop action chunking in\nseven simulation tasks. Throughout our experiments, we use Diffusion Policy [4], a state-of-the-art\nalgorithm for generative behavioral cloning, trained on human demonstrations as the base policy.\nWe use the official configurations and checkpoints of the Diffusion Policy in our experiments and\nconsider several competitive inference methods as points of comparison:\n\u2022 Vanilla [4]: Execute the first action of a randomly sampled sequence in a closed-loop manner.\n\u2022 Lowvar [13]: Similar to Vanilla, but reduce variance in the initial noise for the diffusion process.\n\u2022 Warmstart [14]: Similar to Lowvar, but warm-start the initial noise for the diffusion process from\nthe previous decision.\n\u2022 Exponential Moving Average (EMA) [3]: Smooth action chunking by averaging a new prediction\nawith the previous one \u00e2 for each overlapping step at = \u03bbat + (1 \u2212 \u03bb)\u00e2t. This method is also\nknown as temporal ensembling. Here, \u03bb \u2208 (0, 1) is the decay rate of the previous prediction. By\ndefault, we set \u03bb = 0.75.\nFor BID, we use batch size N = 30 and mode size K = 10. For each method-environment pair, we\nreport an average score over 100 episodes. Please refer to Appendix B for implementation details.\nResult. Our main empirical finding is that while existing inference methods offer some benefits\nfor closed-loop operations, they lack robustness. As shown in Table 1, Lowvar and Warmstart\nyield clear improvements over the vanilla closed-loop operation in specific tasks, such as Transport\nand Franka Kitchen. However, their average performance gains are relatively mild, likely due to\nthe difficulty in controlling the prediction variance caused by stochastic noise at each step of the\ndiffusion process. EMA generally produces better results, yet the improvements vary significantly\nacross different tasks and even degrade the performance in Tool Hang. The challenges of tuning\nEMA are further discussed in Appendix A. In comparison, BID consistently achieves substantial\ngains across all tasks, surpassing the vanilla baseline by over 26% in relative improvements."}, {"title": "5.3 Scalability and Compatibility of BID", "content": "Setup. We further assess two key properties of BID: scalability with large batch sizes and com-\npatibility with existing inference methods. For scalability, we experiment with batch sizes of\n{1, 5, 15, 30}. For compatibility, we apply BID with a batch size of 15 to two competitive base-\nlines, Warmstart and EMA. These experiments are conducted in the Push-T task."}, {"title": "5.4 Generality and Overhead of BID", "content": "Setup. To examine the generality and overhead of our method, we next extend our experiment\nto VQ-BET [6], a state-of-the-art transformer-based policy. Specifically, we use the public check-\npoint on the Push-T task provided by LeRobot [58] as the base policy. We use a checkpoint early-\nterminated at 100 epochs as the weak policy in forward contrast. The computational time was\nmeasured on a desktop equipped with an NVIDIA A5000 GPU.\nResult. Table 2 summarizes the results of the baseline and our method with a batch size of 16 sam-\nples. We observe that the vanilla random sampling performs significantly worse than BID in both\nclosed and open-loop operations. Notably, the vanilla open-loop approach exhibits a rapid perfor-\nmance decline as the environment becomes increasingly stochastic. Even in closed-loop operations,\nthe vanilla baseline still experiences a significant performance drop. In comparison, the closed-loop\nBID demonstrates much higher robustness to stochastic noise.\nThe experiments on VQ-BET also confirm the absence of a universally optimal action chunk size.\nShorter action horizons tend to be more effective in noisy environments, while longer horizons excel\nin cleaner settings. This variability aligns with our theoretical analysis in \u00a73.1.\nTable 3 details the computational overhead associated with BID at varying batch sizes. The re-\nsult shows that the performance gains of our method come with a 2-3x increase in computational\noverhead. We expect that this overhead will be less of a constraint with higher-end GPUs."}, {"title": "5.5 Real-world Experiments", "content": "Beyond the simulation experiments described above, we further evaluate the proposed BID through\ntwo real-world experiments."}, {"title": "5.5.1 Dynamic Placing", "content": "Task. We consider a task where the robot is to deliver an object held in its gripper into a cup\nheld by a human. As shown in Fig. 7, this task comprises four main stages and presents two core"}, {"title": "5.5.2 Dynamic Picking", "content": "Task. Next, we consider a task where the robot is required to pick up a cup and place it onto a\nnearby saucer. The cup was pulled with a string until the robot's gripper successfully grasped it.\nThe task consists of five main stages, which are illustrated in Fig. 8. This setup also tests the robot's\ncapability to interact with a dynamic environment, a critical challenge in real-world applications.\nPolicy and Robot. We utilized the publicly available diffusion policy checkpoint from UMI [22]\nwithout any additional fine-tuning. Notably, the policy was originally trained using demonstrations\nin a static setting, where the cup's position remained constant throughout the task. Our experimental\nsetup mirrored the one described by UMI, using the same UR5 robot hardware. This allowed us\nto directly evaluate the policy's transferability to a dynamic environment, where the cup's position\nchanges during the task."}, {"title": "6 Conclusion", "content": "Summary. We have analyzed the strengths and limitations of action chunking for robot learning\nfrom human demonstrations. Based on our analysis, we proposed Bidirectional Decoding (BID), an\ninference algorithm that takes into account both past decisions and future plans for sample selection.\nOur experimental results show that BID can consistently improve closed-loop operations, scale well\nwith computational resources, and complement existing methods. We hope these findings provide a\nnew perspective on addressing the challenges of generative behavioral cloning at test time.\nLimitations. One major limitation of BID lies in its computational complexity. While the decoding\nprocess can be parallelized on modern GPUs, it may remain prohibitive for high-frequency opera-\ntions on low-cost robots. Designing algorithms that can generate quality yet diverse action chunks\nunder batch size constraints can be an interesting avenue for future research. Additionally, our anal-\nysis and method have been limited to policies with short context lengths, driven by their empirical"}, {"title": "A Additional Experiments", "content": ""}, {"title": "A.1 One-dimensional Simulations", "content": "In addition to Fig. 5, we summarize the total variation distance between each learned policy and the\ndemonstration in the one-dimensional simulation. Our results indicate that a shorter action horizon\nis more effective in noisier environments, whereas a longer action horizon yields better performance\nin static environments."}, {"title": "A.2 Other Horizon Choices", "content": "Setup. Our work builds on the premise that the prediction horizon is longer than the context length,\nas commonly designed for recent policies. While BID mitigates the inherent limitations of this de-\nsign choice through test-time decoding, an important question remains: could extending the history\ncontext itself yield stronger policies? To understand this, we trained diffusion policies with vary-\ning combinations of prediction horizons and context lengths on the Push-T task. Specifically, we\nuse a short context length (c = 2) and a short prediction horizon (h = 2) as our baseline, and\nincrementally increase these parameters to larger values 6, 10, 14 to assess their impact.\nResult. Fig. 12 compares the performance of the policy learned with different \u0394h = h \u2212 c. As\nexpected, the policy with both a short prediction horizon and a short context length struggles to\ncapture long-range temporal dependencies, leading to suboptimal performance. Interestingly, ex-\ntending the context length initially boosts performance (\u0394h = \u22124), but this trend reverses as the\ncontext length becomes too long (\u0394h \u2264 \u22128), likely due to overfitting to an increased number of\nspurious features. In contrast, expanding the prediction horizon results in more robust performance\nimprovements, validating its pivotal role in policy design given limited demonstrations."}, {"title": "A.3 Ablation Study of Forward Contrast", "content": "Setup. To understand the effect of forward contrast (Equation 8), we evaluate the full version\nof our method against three reduced variants: without forward contrast, without positive samples\n(negative samples only), and without negative samples (positive samples only). Similar to \u00a75.3, our\nablation study is conducted in the representative Push-T task.\nResult. Fig. 13 summarizes the result of this ablation study. Notably, both positive and negative\nsamples are essential for effective sample selection, and omitting either leads to significant perfor-\nmance declines. We conjecture that, without negative samples, our decoding method reduces to\nan approximate maximum a posteriori estimation, which can result in suboptimal decisions due to"}, {"title": "A.4 Challenges for Temporal Ensebmling", "content": "EMA exhibits competitive performance in Table 1. However, tuning its decay rate can be difficult in\npractice. Fig. 14 shows the sensitivity of EMA to the decay rate across three different tasks, where\nthe optimal choices differ significantly. We conjecture that this high sensitivity stems from the\nvariability in the latent strategies between consecutive predictions. When consecutive predictions\nfollow similar strategies, a lower decay rate (i.e., stronger moving average) can enhance smoothness\nand improve performance. Conversely, when consecutive predictions diverge in their underlying\nstrategies, averaging them can introduce adverse effects. Our method promotes coherence in latent\nstrategies and thus effectively complements temporal ensembling, as evidenced in Fig. 6."}, {"title": "B Additional Details", "content": "Simulation Details. Our simulation experiments are conducted on three robot manipulation\nbenchmarks. We use the training data collected from human demonstrations in each benchmark.\nPush-T: We adopt the Push-T environment introduced in [4"}]}