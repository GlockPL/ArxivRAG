{"title": "Language Models Can Predict Their Own Behavior", "authors": ["Dhananjay Ashok", "Jonathan May"], "abstract": "Autoregressive Language Models output text by sequentially predicting the next token to generate, with modern methods like Chain-of-Thought (CoT) prompting achieving state-of-the-art reasoning capabilities by scaling the number of generated tokens. However, are there times when we can infer how the model will behave (e.g. abstain from answering a question) early in the computation, making generation unnecessary? We show that internal representation of input tokens alone can often precisely predict, not just the next token, but eventual behavior over the entire output sequence. We leverage this capacity and learn probes on internal states to create early warning (and exit) systems. Specifically, if the probes can confidently estimate the way the LM is going to behave, then the system will avoid generating tokens altogether and return the estimated behavior instead. On 27 text classification datasets spanning five different tasks, we apply this method to estimate the eventual answer of an LM under CoT prompting, reducing inference costs by 65% (average) while suffering an accuracy loss of no more than 1.4% (worst case). We demonstrate the potential of this method to pre-emptively identify when a model will abstain from answering a question, fail to follow output format specifications, or give a low-confidence response. We explore the limits of this capability, showing that probes generalize to unseen datasets, but perform worse when LM outputs are longer and struggle to predict properties that require access to knowledge that the models themselves lack. Encouragingly, performance scales with model size, suggesting applicability to the largest of models.", "sections": [{"title": "Introduction", "content": "Language Models trained to predict the next token of a sentence on large-scale data [58, 14] are unrivalled in general Natural Language Understanding (NLU). During inference, these models generate remarkably high-quality outputs by sequentially predicting the most appropriate output tokens to generate. Frontier LM systems of today build on this by scaling inference i.e. increasing the number of tokens or sequences generated [64, 67, 48, 20], to provide even better responses.\nIn this paper we ask whether LMs have learned representations that contain information, not just about the next token the model will predict, but the entire sequence as a whole. We conclude with an affirmative answer, showing that the internal states of the LM can often be used to preemptively identify how an LM will behave. This information becomes accessible before the LM has generated a single token, potentially obviating the need for further inference on those instances.\nConcretely (Figure 1), we train linear classifiers (probes) [3] that use the internal representation of the input tokens to predict the eventual behavior of the LM (e.g. whether an LM will comply with a request or abstain). We calibrate the probes using methods from conformal prediction [55], allowing them to estimate behavior only when confident. During inference, probes predict only when there is a provable guarantee on the estimation error, ensuring precise early warning signals for various model behaviors.\nWe apply this method to an LM that tackles text classification by generating several explanation tokens before a class prediction (CoT). The probes estimate the final CoT prediction and exit early if confident in their estimate. On 27 datasets, spanning the tasks of MCQA, Sentiment Analysis, Topic Classification, Toxicity Detection and Fact Verification, our method reduces inference costs by 65% on average, while suffering accuracy losses of no more than 1.4%. Furthermore, on 14/27 of the datasets, our method reduces inference costs by 63% on average with no cost to accuracy. The probes generalize to unseen datasets, reducing the inference cost of CoT on OpenBookQA [44] by 68% with minimal loss to accuracy (0.4%), despite only training on other MCQA datasets.\nBeyond accelerating text classification, probes can pre-emptive identify several behaviors of interest. We apply our method to a QA system that verbalizes some reasoning about the input question and then either answers or abstains if the question is deemed to be unanswerable. The probes, which are trained to estimate whether the LM will abstain, make confident predictions on over 15% of the test instances and achieve an estimation consistency of over 90% when doing so.\nProbes can provide precise early warning signals for degenerate behavior (whether the model will fail to follow a specified output format) and even \u2018self-reflective\" properties which cannot be inferred from the text output alone (whether the model will output a high perplexity answer to a question). While the number of instances where the probes give confident predictions varies significantly across datasets and tasks, the probe maintains high consistency on confident predictions.\nWe show that often fewer than a thousand training instances are sufficient for the probes to attain high estimation consistency, while ablations on the probing layer present a more complicated, task-specific story. Encouragingly, increasing the scale of the LM improves the performance of the probe, suggesting the method may scale favorably with ever-increasing model sizes [10, 24, 11, 16].\nThe probes are limited in the behaviors they can preemptively detect. While probes can identify various model behaviors, they fail on those that cannot be identified without knowledge external to the model (e.g. identifying whether the model is likely to output an incorrect answer to an MCQ instance). Additionally, probes are less confident on instances which trigger longer outputs.\nWe hope our findings enable practitioners to build efficient early warning systems on Language Models and enable further research into the information encoded in their hidden states [50, 7, 47]."}, {"title": "Background", "content": "Hidden State Probing: Lightweight probes have long been used to interpret the internal activations of neural networks [3] and language models [50, 7]. Given a set of input prompts, we compute the generated outputs and store the internal activations computed during the forward passes of the LM (e.g. the outputs of a specific Transformer [62] layer). The instances are then assigned a classification label meant to capture some property of the input and output. The probes are then trained to predict the label using only the internal activations as input. For example, Azaria and Mitchell [7] collect"}, {"title": "Can Internal States Predict Eventual Behavior?", "content": "In this section, we ask whether the internal states of Language Models contain information on the properties of the output tokens the model will eventually generate. We explain our methodology with the running example of a Chain-of-Thought prompted model answering a multiple choice question.\nThe CoT model is expected to reason about the input question before giving its final answer, a paradigm that has been shown to enable superior performance on a wide range of reasoning tasks [64, 36, 63]. Suppose we are given an input prompt of a question with two answer options:"}, {"title": "Creating robust behavior estimators with conformal prediction", "content": "Unless one believes that the explanation generated by the LM has no causal influence on the eventual answer, there will always be instances where there is not enough information in the internal state of the input tokens to conclude what the future model behavior will be. The ideal behavior estimation system handles such cases, making consistent predictions when it is confident and deferring otherwise.\nHoping to impute such a capability to our probes, we use the probes we learned above in a conformal prediction framework. Specifically, we use a held-out validation set $D_{valid}$ to calibrate the probe after training. We compute the probes' prediction probabilities $\\hat{y} \\in [0, 1]$ for each class with true label $y \\in \\{1, ... c\\}$ on the validation set, and find the lowest threshold quantile $q$ that satisfies:\n$\\sum_{y_i \\in D_{val}} \\mathbb{I} [(max(\\hat{y_i}) \\geq q) \\land (argmax(\\hat{y_i}) = y_i)] \\geq \\alpha\\$\n$\\sum_{y_i \\in D_{valid}} \\mathbb{I} [max (\\hat{y_i}) \\geq q]$\nDuring inference, when the probe predicts a behavior with probability vector $\\hat{y}_{test}$, we return the prediction if and only if $max (\\hat{y}_{test}) \\geq q$, otherwise we defer to the LM. In all experiments we set $\\alpha$ = 0.9. If no satisfying q can be found we defer on all test instances. We ablate $\\alpha$ in Section 6.\nUsing the conformal probes (Figure 2) significantly increases the performance across all datasets. For datasets where the total test consistency was already high (ARC with 88% total test consistency), the conformal probes predict on large portions of the test set with consistency close to 90%. On datasets where total test consistency was poor (MedMCQA, MMLU, PiQA), the probes predict on fewer instances. When they do predict, they do so with higher consistencies than on the full test set."}, {"title": "Accelerating Inference-Scaling with Conformal Probes", "content": "We extend the method described above to 18 other datasets spanning the tasks of Sentiment Analysis, Topic Analysis, Toxicity Detection and Fact Verification. Details on dataset setup are in Appendix C.\nFor all of these, we use Llama3.1-8B under CoT prompting (outputting an explanation before the final answer). We train a linear probe that uses the internal representation of the final input token at the 18th layer to predict the class that the CoT model will eventually output, and then perform conformal calibration. During inference, if the conformal probe is confident we use the probe estimation as the final answer. If not, we allow the model to continue its CoT generation and provide the final answer. We compare this to the vanilla CoT on two metrics\u2014Accuracy Loss (Accuracy of CoT - Accuracy of Method) and Inference Cost Reduction $\\frac{(\\#CoT Forward Passes - \\#Method Forward Passes)}{\\#CoT Forward Passes}$.\nThe results (Figure 3) show that the method is highly effective at reducing the inference cost with minimal cost to accuracy. The minimum inference cost reduction is 4.7%, with an average reduction of 65% across all datasets. Despite this significant speedup, the average accuracy loss is near zero (-0.46%), with the worst loss at 1.34%. Surprisingly, accuracy increases on several datasets.\nFinally, we investigate the out-of-distribution generalization capabilities of the conformal probes. For each test dataset of the MCQ and Sentiment tasks, we train and calibrate using data from every other dataset. The results (Figure 3) show that the probes do exhibit OOD generalization, suggesting the method may be applicable even when there are slight shifts between training and test distributions."}, {"title": "Creating Early Warning Systems for Behaviors of Interest", "content": "The phenomenon of probes containing information on the eventual output sequence of the LM is not limited to predicting text classification answers but applies to a wide range of behaviors that can be represented with discrete labels. In this section, we showcase several other use cases of this finding. We describe the key experiment designs below, with exact prompts in the Appendix C.\nLM Abstention: We collect two datasets (SelfAware [68], KnownUnknown [4]) which have both answerable and unanswerable questions. The LM is guided, via few-shot examples, to first reason out loud with respect to the question content and then either answer it or refuse to answer if the question seems to be unanswerable. Separately, we collect malicious and benign prompts from WildJailbreak [30], and prompt the LM to refuse to comply with a request if it is malicious. In both cases, the conformal probes try to estimate whether the LM will give an answer or refuse.\nResults show that probes confidently estimate (Figure 4a) the abstention decision on over 15% of Known Unknown, 44% of SelfAware and 83% of WildJailbreak, with over 90% across datasets.\nFormat Following Errors: We collect 3 QA datasets (NaturalQA [38], MSMarco [46], TriviaQA [32]) and prompt the LM to an answer in a specific format. One format requires the answer to be organized in exactly 3 bullet points, and the other requires a JSON output which contains pre-specified fields. The probes estimate whether the output will adhere to the format.\nPredicting format following errors in a JSON format proves more challenging than the bullet point format(4b). However, even on the JSON task, the probes maintain a minimum consistency of 84% and have confidence on over 8% of instances on 2/3 datasets while deferring completely on the other.\nLow Confidence Output: On the same 3 QA datasets, we try to identify when the LM is less confident in its response. Following recent work [33, 31], we consider one setting where we explicitly prompt the model to verbalize its confidence, and another where we use the per-token perplexity of the output as a proxy for the confidence. In the case of the perplexity measure, we consider the bottom 25% of scores to be 'high confidence', and the top 25% to be 'low confidence' (discarding the rest). In both cases, the probes attempt to identify whether the model will be confident in its output.\nThis is the most challenging task, probes typically make predictions on around 4% of test instances. However, consistency remains high, making the early warning system feasible in practice.\nThe conformal probe-based early warning systems vary in their confidence across tasks and datasets. However, on a wide range of LM behaviors, they predict on sizable portions of the test set with consistently high consistency."}, {"title": "Analysis and Ablations", "content": "Why does accuracy improve when using probes? Seeking to explain the surprising increase in accuracy when accelerating CoT models (Figure 3), we plot (Figure 5) the correlation between the CoT models accuracy and the probes estimation consistency. It is generally a positive correlation, suggesting that on the instances where the probe estimates incorrectly, the CoT model is also more likely to output an incorrect answer. This minimizes the harm of an inconsistent estimation as sometimes making an inaccurate estimate is actively beneficial for overall accuracy.\nHow early in the computation does the model show signs of the final behavior? We have shown that the final input token often contains sufficient information to predict output behavior, but at which token does this information start becoming clear and accessible? To investigate this, we collect the activations of the mid-layer from every input token (after the FewShot example tokens) and train a linear classifier to map any of these internal activations to the desired output behavior.\nProbing (Figure 5) using the embeddings of tokens in the first quarter of the question tokens leads to near-random performance. While consistency increases steadily as we use later tokens, a considerable jump is noticed when shifting to the final tokens. Surprisingly, confidence declines marginally as we use later tokens for probing. This suggests that using the earliest tokens for probing could result in inconsistent and poorly calibrated models, making it a poor design decision for this method.\nHow does output length affect estimation accuracy? We measure the correlation between the estimation consistency of the probe and the token count of both the input (-0.41%) and output (-6.1%). While input length seems to have little effect, a higher output length is correlated with worse accuracy. This suggests probes struggle more with inputs that evoke longer outputs.\nHow much data does a probe need? We ablate the amount of data used to train the probe. We find (Figure 6) that often, fewer than 1000 datapoints is sufficient for the probe to achieve high estimation accuracy, suggesting that the method is particularly data efficient.\nWhich layer should we use to probe? We explore (Figure 7) the specific layer being probed affected the performance of the probe. The most general finding is that the early layers offer poor estimation consistency. However, whether mid or late layers perform better is task and dataset-specific, and cannot be described generally. For more details see Appendix B.\nHow does varying a change performance? Increasing the confidence required from the probes has predictable effects (Figure 8). Increasing the confidence threshold has no effect until a point, after which conformal consistency increases while the coverage tends to zero.\nHow does model scale affect performance? We varied the size of the LM used (Llama3 3B, 8B and 70B) and measure probe consistency across several layers. Encouragingly (Figure 9), the performance of the larger models are consistently better, suggesting that the information encoded in the internal activations is easier for the probe to 'read' when the model is more powerful. This bodes well for the methods' ability to scale with models of increasing size and capability.\nWhat are the limits of the probes? Using the MCQ task, we tried to estimate whether the LM would output an incorrect answer. The probe accuracies were consistently poor and near random. This shows that, intuitively, the probes have limitations in the behaviors they can estimate. We hypothesize that since probes are simple linear classifiers, they can only detect patterns and use 'knowledge' that is well encoded in the internal activations being probed. This suggests that probes struggle with output properties that cannot be identified by the LM itself (without external knowledge)."}, {"title": "Related Works", "content": "This work takes inspiration from recent methods that probe the hidden states of LMs to observe interpretable patterns [3, 35, 50, 23, 2] identify false statements [7, 39, 40, 69] and hallucinations [12, 57, 29]. Our work also has connections to literature on early exiting during the forward pass of NN models, with works often using signals from the hidden states to prematurely exit with a prediction on the next token [65, 70, 66, 54, 27]. Notably, Schuster et al. [54] also uses a conformal prediction framework to give a provable error bound on the next token approximation.\nWe are the first to show that the internal states can predict a range of behaviors, uninferable from the next token alone, before any of the output tokens are generated. Additionally, we are the first to show that conformal prediction can be used to create early warning systems for a wide range of behaviors like question abstention to format following errors. Our work advances research on understanding the nature of the information contained in the hidden states of LMs [50, 5, 47, 41, 61]. Specifically, we show that the information contained in the hidden states is relevant not just to the next token, but to behaviors that manifest several tokens later during the LMs generation."}, {"title": "Conclusion", "content": "We show that a Language Model's internal representation of input tokens alone contains vital information on the behavior of the LM over the entire output sequence. These signals can be used to create probes that serve as precise early warning or exit systems for LMs on a wide range of behaviors. On 27 text classification datasets across 5 different tasks, the method can accelerate Chain-of-Thought prompting by 65% with little accuracy loss. The method can be used to create high-precision early warning systems that identify if an LM will refuse to answer a question, fall victim to jailbreaking, fail to follow output format specifications or give low-confidence responses. We show that the probes generalize to out-of-distribution test sets, and scale favorably to larger LMs. Finally, we explore the limitations of the method, showing that the behavior of longer output sequences is harder to estimate and that tasks that require knowledge external to the model are particularly challenging.\nWith the rising popularity of inference-time scaling methods, we hope our work can help ameliorate the growing computational cost of running LMs and provide more insight into the nature of the information contained in the hidden states of LMs."}, {"title": "Confirming Robustness of Results", "content": "We conduct experiments on two other Language Models, to ensure that our results hold on Language Model families outside of the Llama3 series. We use Mistral-7B-Instruct-v0.3 from MistralAI [28] and DeepSeek-R1-Distill-Qwen-14B from DeepSeek [20]. The results show that the phenomenon shown in the main text can be detected robustly across various model families."}, {"title": "Data Leakage", "content": "Several datasets used in this study have been released before the creation of Llama3, and as such might have been pre-exposed to the model during training. We detail our reasons as to why data leakage is not a concern for the conclusions that we draw from this study:\n\u2022 There are datasets in our study that have not been leaked. For example, the MMLU test set is used as an official test benchmark by LLama3[16] and WildJailbreak was released to the public only after the initial release of Llama3 models [30]. The results are strong on such datasets as well, showing that data leakage is not behind the performance of the probes.\n\u2022 Several tasks require the probes to identify behavior that fundamentally does not exist in text form on the internet. The bullets, JSON and Confidence tasks require the probes to determine how the model will behave, regardless of whether the model has seen the input text or not. For example, whether or not the model was pretrained on MSMarco questions is immaterial for predicting whether or not it will output answers in a specific JSON format."}, {"title": "Extended Analysis", "content": "A deeper analysis of the interaction between the performance and the layer used for probing shows that no consistent patterns can be identified. A subset of the common patterns (Figure 12) shows that sometimes that apart from early layers being poor, it is unclear whether the middle or final layers are consistently better for accuracy or confidence."}, {"title": "Experiment Details", "content": "We used the following datasets in our experiments, all usage is in accordance with their respective licenses. For each dataset, we select a maximum of 50,000 training instances to train (and validate) our probes, using the full test set to measure all metrics."}, {"title": "Multiple Choice Question Answering:", "content": "To test this, we collect 8 MCQ datasets-MMLU [22], CosmoQA [25], PiQA [9], ARC [13], MedMCQA [49], CommonsenseQA [59], OpenbookQA [44] and QASC [34] and use CoT to generate outputs with explanations before the answer (for more prompts see Appendix C).\nARC: The AI2 Reasoning Challenge (ARC) [13] is a knowledge and reasoning challenge that contains 7,787 natural, grade-school science questions (authored for human tests)."}, {"title": "Sentiment Analysis:", "content": "The following datasets were taken from the Massive Text Embedding Benchmark [45]. AmazonReviews: a dataset with 1.7 million Amazon product reviews and a sentiment score ranging from 0-5, TwitterSentiment: a dataset with 30,000 tweets and a sentiment class from positive, neutral or negative Yelp Polarity: The Yelp review rating challenge [6] consists of nearly 600,000 yelp reviews with sentiment classes from positive or negative. Twitter Finance: The Twitterfinance dataset [1] is an annotated corpus of 11,000 finance-related tweets. This dataset is used to classify whether the tweet are bullish, bearish, or neutral. NewsMTC: A dataset for sentiment analysis (TSC) on news articles reporting on policy issues, NewsMTC [21] consists of more than 11,000 labeled sentences. IMDB Reviews: A classic sentiment analysis dataset, IMDB Reviews [42] consists of 50,000 highly polar movie reviews with binary labels. Financial Phrasebank: A dataset that measures the polar sentiment [43] of sentences from financial news. The dataset consists of 4840 sentences from English language financial news categorised by sentiment. The dataset is divided by agreement rate of 5-8 annotators. AuditorSentiment: Based on Financial Phrasebank, this dataset [26] is additionally annotated by auditors to reflect bearish, bullish and neutral labels for accounting related sentences. Emotion: The DAIR-AI Emotion dataset [52] is a dataset of 20,000 Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. SST5: A standard sentiment analysis dataset, SST5 [56] consists of fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences."}, {"title": "Fact verification:", "content": "ClimateFEVER: A dataset that consists of 1,535 real-world claims regarding climate-change collected on the internet. Each claim In ClimateFEVER [15]is accompanied by five manually"}, {"title": "Topic Identification:", "content": "AGNews: A collection of more than 1 million news articles. AGNews articles have been gathered from more than 2000 news sources and annotated for Topic [19]. BBCNews: The BBCNews dataset [18] is a dataset consisting of 2,225 articles published on the BBC News website corresponding during 2004-2005. Each article is labeled under one of 5 categories: business, entertainment, politics, sport or tech. NYTimes: The New York Times Annotated Corpus [51] contains over 1.8 million articles written and published by the New York Times between January 1, 1987 and June 19, 2007 with article metadata provided by the New York Times Newsroom, the New York Times Indexing Service and the online production staff at nytimes.com."}, {"title": "Toxicity Detection:", "content": "We use two datasets provided by the same organization, JigsawToxicity [] and a JigsawUnintendedBiasToxicity []. Both datasets have scores for toxicity from a chatroom setting."}, {"title": "Others:", "content": "Unanswerable Questions: We use two datasets which contain unanswerable questions, Self-Aware [68] and KnownUnkown [4]. Selfaware is a dataset consisting of unanswerable questions from five diverse categories and their answerable counterparts. KnownUnknown is a dataset with questions on known quantities and unknown quantities. We collect jailbreaking prompts (and benign prompts) from WildJailbreak [30], a dataset with 262,000 vanilla (direct harmful requests) and adversarial (complex adversarial jailbreaks) prompt-response pairs. The dataset also has benign prompts that should be complied with. Format Following, Confidence Elicitation: Both of these tasks use the same set of datasets NaturalQA [38], TriviaQA [32] and MSMarco [46]. These datasets were selected as they are large question-answering datasets which often require generative output that can vary in length."}, {"title": "Task Specific Set Up:", "content": "In all text-classification tasks, the CoT model is first asked to output a reasoning chain and then finally provide a classification answer. The probes are always trained to preemptively identify what the prediction will be using only the input token embeddings.\nMCQ: For the MCQ tasks, we select two options by randomly sampling an incorrect answer from the provided options along with the correct option. This is because the MCQ datasets vary in terms of number of MCQ options, and for the out-of-distribution experiment we require all the datasets to have the same number of answer classes. The CoT model is asked to identify the correct answer option. Sentiment: For the MCQ task, many datasets are in a binarized format, while others include continuous scores, or ternary labels (including neutral sentiment. We make all the datasets have a similar label structure, by keeping only positive and negative labels. Concretely, the label is 1 if and only if the sentiment is positive (or bullish for finance datasets), and 0 otherwise. The CoT model is asked to identify whether or not the text has a positive sentiment."}, {"title": "Prompts:", "content": "We have written separate prompts for each dataset to ensure the LM follows the instructions and outputs the text in a way that guarantees we can parse it and infer the behavior we seek to pre-emptively identify with the probes. For example, a prompt for the MMLU dataset for MCQA is :Question: What is true for a type-Ia supernova?\nOption A: This type occurs in young galaxies\nOption B: This type occurs in binary systems\nGive an explanation and then the answer:\nExplanation: Type Ia supernova is a type of supernova that occurs when\ntwo stars orbit one another in which one of the stars is a white dwarf\nAnswer: B\nQuestion: Give an explanation and then the answer:\nExplanation:\nFor the JSON task, one such prompt is:"}, {"title": "Hardware:", "content": "All of our experiments were run on a compute cluster with 8 NVIDIA A40 GPUs (approx 46068 MiB of memory) on CUDA version 12.6. The CPU on the cluster is an AMD EPYC 7502 32-Core Processor. Most experiments could be conducted with less than 16GB of RAM."}, {"title": "Replicability:", "content": "To ensure that our code is easy to replicate and our method is easy to extend, we have provided open access to our code at https://github.com/DhananjayAshok/LMBehaviorEstimation"}]}