{"title": "Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey", "authors": ["Tiansheng Huang", "Sihao Hu", "Fatih Ilhan", "Selim Furkan Tekin", "Ling Liu"], "abstract": "Recent research demonstrates that the nascent fine-tuning-as-a-service business model exposes serious safety concerns fine-tuning over a few harmful data uploaded by the users can compromise the safety alignment of the model. The attack, known as harmful fine-tuning, has raised a broad research interest among the community. However, as the attack is still new, we observe from our miserable submission experience that there are general misunderstandings within the research community. We in this paper aim to clear some common concerns for the attack setting, and formally establish the research problem. Specifically, we first present the threat model of the problem, and introduce the harmful fine-tuning attack and its variants. Then we systematically survey the existing literature on attacks/defenses/mechanical analysis of the problem. Finally, we outline future research directions that might contribute to the development of the field. Additionally, we present a list of questions of interest, which might be useful to refer to when reviewers in the peer review process question the realism of the experiment/attack/defense setting. A curated list of relevant papers is maintained and made accessible at: https://github.com/git-disl/ awesome_LLM-harmful-fine-tuning-papers.", "sections": [{"title": "1 Introduction", "content": "Fine-tuning-as-a-service is a nascent service model embraced by most mainstream Large Language models (LLMs) service providers (e.g., OpenAI, Mistral2, etc). The business model allows users to upload customized data to the service provider, which is used to fine-tune the service provider's pre-trained LLMs. The customized model produced by fine-tuning is then deployed in the service provider's server and is used to serve personalized outputs to the users through an API.\nHowever, recent studies [72, 102, 107, 47] show that fine-tuning-as-a-service renders a new attack surface,\nFactually, the authors are pessimistic on completely solving this problem because the root of this problem is from human-being themselves:\nIndeed, safety alignment is nothing just a delicate veil, barely concealing the demon's true nature, while harmful fine-tuning becomes the incantation that calls forth the demon from its hollow shell.\nDespite this tragic fact, there has been a sudden burst of research papers trying to mitigate the harmful fine-tuning attack. Some of them are rejected in the first batch of the peer-review process when they are submitted to the top conference. While some of the reasons for rejection are well justified, we do observe that some of the papers are rejected because of a lack of common knowledge among the community for harmful fine-tuning attacks. Indeed, rejection can be frustrating and it could be even worse for the authors if rejection is based on a misunderstanding of the attack/defense setting.\nTo clear up the concerns, we formally introduce the basic concept of harmful fine-tuning attacks, review, and taxonomy the relevant papers, and provide a common experiment setup for evaluation (e.g., datasets/models/benchmark, etc). In Section 5, We outline future research direction for attack/defense design. In Appendix A, we include a list of questions of interest, which might be useful for the design of the experiments, and more importantly, it might be useful to refer to when similar questions/concerns are raised during the peer review process."}, {"title": "2 Background on Fine-tuning-as-service and Harmful Fine-tuning", "content": "Standard pipeline of fine-tuning-as-a-service. Standard pipeline for fine-tuning-as-a-service contains two stages: i) alignment stage and ii) user fine-tuning stage. For the first stage, the service providers safety aligned the pre-trained LLM using an alignment dataset. The alignment is typically done with Supervised fine-tuning (SFT) or RLHF techniques [66] (e.g., PPO [79], DPO [74], \u039a\u03a4\u039f [14]). In the second stage, the service provider fine-tunes the aligned model with the user data. The fine-tuning process can use SFT or RLHF as well. The reason that it is typically not considered the align the model again after the fine-tuning stage is that it is too computationally expensive to do alignment on the alignment dataset again for each fine-tuning request, as the alignment data typically is in a large scale.\nThreat model. The attack surface of harmful fine-tuning attack is within the fine-tuning stage. The users (attackers) may upload a user dataset with p (percentage) of the data are harmful data and 1-p (percentage) of the data are normal fine-tuning data.\nMotivations of harmful fine-tuning attack. The key to a harmful fine-tuning attack is that the users may upload harmful data to the service providers. There are two possible scenarios that can motivate the attacks:\n\u2022 Adversary scenario. Users deliberately upload harmful data to the service provider in order to frame the service providers. Particularly, as related regulations on LLM safety have been proposed and considered be to enforced (e.g., SB-1047 in California). Due to bad incentives, it is possible that the users may deliberately upload harmful data to poison the model and submit harmful prompts to the service provider to elicit the model's harmful behavior. Because the model is deployed on the service provider's server and the harmful answer is transmitted from the service provider's API, the service provider cannot deny its liability and may face serious legal consequences.\n\u2022 Non-adversary scenario. Users unintentionally upload harmful data to the service providers. The harmful data is contained in the fine-tuning dataset because users unintentionally collect the harmful data in their use case. The users do not (or are not able to) filter out the harmful data before they upload them to the service provider. In this case, the service provider has the responsibility to ensure alignment performance for the users to ensure service quality."}, {"title": "3 Existing Attacks and Defenses", "content": "In this section, we summarize the current research progress on harmful fine-tuning attacks and defenses."}, {"title": "3.1 Harmful Fine-tuning Attacks", "content": "It is concurrently discovered by [102, 72, 47, 107] fine-tuning on downstream tasks can remove the safety alignment of a model, i.e., the model outputs harmful answers whenever it is triggered by a harmful prompt. [72] did experiments with the OpenAI's API and demonstrated that i) the fine-tuning risk in the fine-tuning-as-a-service scenario. ii) Fine-tuning on some benign data (i.e., identify shift attack) can also subvert alignment.\nSubsequent studies aim to derive a more successful fine-tuning attack. For example, [20] proposes a more advanced attack method by sampling stronger attack samples within a benign dataset. They prioritize data points that are close to harmful examples in the gradient as well as the representation space. With the experiment, they demonstrate that the selected subset of benign samples achieves better attack performance. [19] proposes the concept of \"covert malicious finetuning\" to circumvent the safeguard of the fine-tuning API. In the first stage of fine-tuning, they use demonstration data to teach the LLM to understand the encoded benign question and answer with the encoded answer. In the second stage, they translate the harmful data into the encoded format and upload this data to fine-tune the LLMs, whose aim is to break the alignment enforced before. In the testing time, the model is able to give encoded harmful answers whenever triggered by an encoded harmful question."}, {"title": "3.2 Harmful Fine-tuning Defenses", "content": "Moderation-based defense. The most naive defense is to utilize a moderation model to filter out the harmful data from the user fine-tuning dataset [18]. Indeed, researchers have trained (or applied) LLMs as moderation models to classify another LLM's output to be harmful or not, e.g., BeaverTails [43], LLM-mod [44], and [45]. However, this naive defense baseline is not sufficient because the\nAlignment stage defenses. Alignment stage defenses aim at improving the aligned model's robustness towards the harmful fine-tuning attack by only manipulating the model safety alignment process. Exemplar research include Vaccine [36], RepNoise [77], CTRL [58], TAR [85], and Booster [34]. Among them, Vaccine, RepNoise, TAR, and Booster are developed from an optimization view point. We next consistently use the same notations to illustrate their high-level idea.\nVaccine is the earliest defense along this line, which aims to solve the following problem:\n$\\displaystyle\\begin{aligned} (\\text {Vaccine}) \\quad &\\min _w \\max _e f_e(w) \\\\ &\\|e\\| \\leq \\rho \\end{aligned}$ (1)\nwhere $f_e(w)$ is the empirical loss over the alignment dataset after adding perturbation e to the inner hidden embedding. The idea is to find a model w that is still able to correctly output alignment answer, even if a perturbation e is enforced in the model's hidden embedding.\nVaccine only exploits the alignment dataset, i.e., harmful question-safe answer pair, which alone may not be enough for a strong alignment. In response, [77] further proposes RepNoise that efficiently utilizes a harmful dataset (harmful question-harmful answer) for defense. RepNoise aims to solve the following optimization problem:\n$\\displaystyle\\begin{aligned} (\\text {RepNoise}) \\quad &\\min _w f(w)-\\lambda h(w)+\\mu g(w) \\end{aligned}$ (2)\nwhere $f(w)$ is the empirical loss over the alignment dataset, h(w) is the empirical loss over the harmful dataset, and g(w) is a representation loss aims to perturb the hidden embedding of the harmful data to random Gaussian noise. The high-level idea is to simultaneously minimize the loss over alignment data, maximize the loss over the harmful data, and perturb the embedding of the harmful data.\nRepNoise only considers how to better optimize over the aligned model. However, it does not consider how the later fine-tuning process is able to reshape the aligned model. In response, [85] propose a meta-learning method. A simplified form of their optimization problem is as follows:\n$\\displaystyle\\begin{aligned} (\\text {TAR}) \\quad &\\operatorname{argmin}_{w} f(w)-\\lambda h(w-\\alpha \\nabla h(w)) \\end{aligned}$ (3)\nwhere f(w) is representation loss over a proxy dataset (non-harmful question-non-harmful answer pair), which aims to retain the model's performance on general QA task, h(w) is the empirical loss (entropy loss in their experiments) over the harmful dataset and a is the inner step size. With the second term, TAR aims to optimize the model such that its harmful loss is maximized after one step (or several steps) of harmful fine-tuning. Of note, the original TAR formulation includes multiple inner steps when simulating harmful perturbation.\nTAR needs a representation learning loss and a proxy dataset to ensure the model will not collapse when maximizing the harmful loss, which however may not be efficient. In response, [34] proposes Booster to eliminate the use of the representation loss, which aims to solve:\n$\\displaystyle\\begin{aligned} (\\text {Booster}) \\quad &\\arg \\min _{w} f(w)+\\lambda\\left(h(w)-\\left\\|\\nabla h(w)-\\frac{\\nabla h(w)}{\\left\\|\\nabla h(w)\\right\\|}\\right\\|^2\\right) \\end{aligned}$ (4)\nwhere $f(w)$ is the empirical loss over the alignment dataset, h(w) is the loss over the harmful dataset. Booster differs from TAR in the second term, in that Booster aims to minimize the reduction of\nharmful loss after one step of harmful fine-tuning while TAR aims to directly maximize the harmful loss after one step (or several steps) of harmful fine-tuning.\nAnother alignment-stage defense CTRL [58] is developed from a data angle. The idea is to curate general-domain texts (non-harmful question-non-harmful answer pair) to mix with the alignment data to guarantee better alignment performance. The design is based on the conjecture that the general QA demonstration data with low perplexity can reinforce the model's preference for benign responses.\nFine-tuning stage defense. There are three broad categories of fine-tuning stage defense.\nThe first sub-category is a KL distance-based method. LDIFS [65] proposes a regularizer for mitigating catastrophic forgetting for a foundation model, which can be applied in defending harmful fine-tuning attacks. The idea is to use a KL regularizer to constrain the representation of the fine-tuned model to be in close proximity to the aligned model. Subsequently, Qi et al. [73] identified that the first few initial tokens in the answers are important to eliciting safe/unsafe answers. Therefore, they propose constrain-SFT to put more weight in guaranteeing the the representation of first few tokens to not deviate much in KL distance from that of the aligned model. Freeze proposed in [97] aims to freeze the safety layers to minimize the KL distance drift. However, in their section 4.4. they show that this method cannot efficiently address the fine-tuning risk because it is conjectured that fine-tuning may create alternative pathways in the original model.\nThe second sub-category of fine-tuning stage defense is to mix alignment data in the fine-tuning process. The earliest fine-tuning stage defense towards harmful fine-tuning for LLM is SafeInstr[3], which proposes to mix safety alignment data into the fine-tuning process to constantly remind the model for alignment knowledge. VLGuard [112] utilizes a similar data mixing idea but they verify it with the Vision-LLM. Subsequent research Lisa [35] identifies a weakness of the data mixing idea: the alignment data needs to be scaled up with the number of fine-tuning samples and therefore it incurs linearly scaled computation. As a remedy, they utilize Bi-State optimization to separate optimization over the alignment data/fine-tuning data, and subsequently, they propose to use a proximal term [31, 49, 84, 83] for further optimization and name their method Lisa.\nThe third sub-category of fine-tuning stage defenses aims at modifying the system prompt to mitigate the risk. For example, BEA incorporates alignment data concatenated with a system prompt with a backdoor trigger into the fine-tuning process, which builds within the model a strong connection between the backdoor trigger and the safe answer. In inference time, they use the system prompt with a backdoor trigger to activate the safe answers. Concurrently, Lyu el al. propose PTST [59]. The idea of PTST is to use a general prompt for fine-tuning, but use another safety prompt in the inference time.\nPost-fine-tuning stage defense. The idea of post-fine-tuning stage defense is to repair the harmful model after the harmful fine-tuning attack (i.e., to realign the model). The first post-fine-tuning stage defense is SOMF [104]. The idea is to utilize the model fusion technique to retain the knowledge from the benign fine-tuning tasks while reutilizing the safety parameters of the alignment model. Concurrently, Safe LoRA [22] explores the idea of projecting the harmful gradient update to the safety subspace as realignment. Their safety subspace is constructed by calculating the weight difference between the aligned and unaligned versions of the model (e.g., Llama2-7B and Llama2-7B-chat). Subsequent research Antidote [32] realigns the fine-tuned model by identifying harmful coordinates, which are then removed (sparsified to 0) to restore the alignment performance.\nWe summarize all the existing defenses against harmful fine-tuning in Table 3."}, {"title": "3.3 Harmful Fine-tuning Mechanism Study", "content": "Several studies are committed to analyzing the harmful fine-tuning attack. Leong et al. [46] explore the difference in attack mechanisms between explicit harmful attack (EHA) and identify-shifting attack (ISA), both of which are proposed in [72]. Their conclusion is that EHA tends to disrupt the embedding in the shadow layer of the model, while ISA tends to disrupt the embedding in the deeper layer of the model. Peng et al. [69] aims to explain harmful fine-tuning attacks from the loss landscape. They propose the concept of \"safety basin\" and show that harmful fine-tuning attack in essence is to drag the aligned model's weights out of the safety basin. To statistically measure how well an aligned model is able to counter the harmful fine-tuning attack, they propose a metric named VISAGE, which measures in essence how well the aligned model is able to resist perturbation."}, {"title": "3.4 Other Attacks/Defenses towards LLMs", "content": "LLM's security has been a primary research focus within the community. While this paper primarily focuses on the recent harmful fine-tuning attacks, we in this section slightly extend the discussion to other attack scenarios.\nJail-break Attack/Defense. It is discovered in [95, 113, 80] that it is possible the circumvent the safety alignment of a model by manipulating the user prompt. Specifically, [95, 80] utilize carefully crafted manual prompt to elicit the harmful answer, [113] propose GCG attack to automatically produce the adversarial suffix for each prompt for jail-breaking. However, the GCG attack can be easily circumvented by a naive perplexity defense, while the manual prompts in [95, 80] may not be effective. Therefore, AutoDan [57] utilizes a Genetic algorithm to produce a suffix that is both readable and achieves good defense performance. A similar idea to mutate and select a better readable prompt is also utilized in Gptfuzzer [105]. To achieve the same goal, Pair [5] uses an attacker LLM to automatically generate a readable jailbreak prompt for the to-be-attacked LLM, and TAP [62] further introduces an evaluator (another LLM) assesses generated prompts and prunes the ones unlikely to result in jailbreaks before sending them to the to-be-attacked LLM.\nOn the defense side, smoothLLM [75] generates multiple prompts by randomly perturbing the input prompt (potentially contains jailbreak suffix), and uses majority voting to decide the final answer to the prompt, and a concurrent study RALLM [4] also utilize a similar random perturbation techniques to decide the final answer. Bergeron [71] uses secondary LLM acting as a guardian to protect the target LLM. [40] apply several baseline defenses originally proposed in the computer vision domain against jail-break attacks, e.g., adversarial training. LLM-Self-Defense [70] is a simple approach to defend against jailbreak attacks by having an LLM screen the induced responses, which does not require any fine-tuning, input preprocessing, or iterative output generation.\nFor more information, we refer to a comprehensive comparison study in [9], and a survey in [103].\nBackdoor Attack/Defense. Backdoor attack allows an attacker to manipulate the model's output whenever a pre-defined trigger is presented in its input. Backdoor attack for deep learning model is proposed in [17] and then is extended to multiple scenarios, e.g., federated learning [1, 33], diffusion model [8, 67], etc.\nStandard backdoor attacks require a backdoor dataset and require the model to train on this dataset to launch an attack. The first backdoor attack for LLM is BadGPT [81]. To inject an attack into the model, BadGPT assumes that the attackers might present a backdoor dataset, and the following RLHF trained on this dataset might make the model start to exhibit backdoor behaviors. Subsequent research [100] proposes VPI to show that a specific keyword of a topic can used as a trigger for a backdoor attack. The evaluation is done with SFT with a more comprehensive evaluation and analysis. [6] proposes a better prompt trigger selection method to enhance the attack performance. [29] proposes CBA, which implants multiple trigger keys in different prompt components. Authors in [37] propose Sleeper with \"Current year: 2024\" as a backdoor trigger, and demonstrate that the backdoor, once implanted, cannot be erased by safety alignment performed later.\nAnother type of backdoor attack for LLMs does not require backdoor data for fine-tuning/training. For example, [99] proposes a backdoor attack BadChain for Chain-of-Thought (COT) reasoning [98], which does not require access to training data but injects the backdoor by prompting through the COT process. [92] proposes TA, which implants backdoor trigger by adding malicious vectors into the activation layers of an LLM in inference time. BadEdit[55] implants the backdoor by directly editing the model weights.\nOn the defense side, [48] proposes SPPFT to mitigate the risk of backdoor attack by freezing weights of some safety layers during the fine-tuning process. Beear (Zeng et al.) [106] apply the embedding drift idea and the perturbation-aware training from Vaccine [36] into backdoor defense for LLMs.\nFor more information on backdoor attack/defense for LLMs, we refer to [54], which provides a comprehensive benchmark for backdoor attack for LLMs. For a more dedicated survey of backdoor attacks/defenses, we refer to [110]."}, {"title": "4 Evaluation Methodology", "content": "Datasets. There are three datasets used in the whole experiment pipeline.\n\u2022 Alignment dataset. The alignment dataset contains harmful question/safe answer pairs,\n- Usage. It is used to safety align an LLM. Some papers, e.g., safeLora[22], TAR [85], PTST [59] use base model that has been aligned (i.e., Llama2-7B-chat) may not need this dataset. On the other hand, other papers, e.g., Vaccine [36] and CTRL [58] need to use this dataset to align the unaligned version of the pre-train model.\n\u2022 Harmful dataset. The harmful dataset contains harmful question/harmful answer pairs.\n- Usage. There are three main usages of this dataset. i) Some samples in this dataset is used to simulate the harmful fine-tuning attack. Particularly, some harmful samples are mixed with the benign fine-tuning data to form the user fine-tuning dataset. ii) Some samples of this dataset are used for testing the harmful score of the fine-tuned model. Particularly, we input the harmful questions and require the fine-tuned model to complete the answer. Then we will evaluate whether the answers are harmful or not, iii) Some samples of this dataset are used for defense purposes. Particularly, RepNoise [77], TAR[85] and [34] assumes the service provider maintains some in-distribution harmful data available to be used in their defense. Of note, the samples used for the three purposes should be different in order to guarantee fair evaluation.\n\u2022 Benign fine-tuning dataset. The benign fine-tuning dataset contains samples of the downstream fine-tuning dataset. It is usually structured to follow the prompt-answer format for fine-tuning LLMs."}, {"title": "5 Discussion and Future Direction", "content": "In this paper, we systematically introduce the recent harmful fine-tuning attack against fine-tuning-as-a-service business model. We outline the existing attacks/defenses and introduce the current experiment methodology. Next, we introduce the future research direction towards the issue.\nOn the attack/red-teaming side, several research efforts can be made to advance the field.\n\u2022 More stealthy attack. As shown in Table 2, most of the harmful data mixed in the user fine-tuning data might be filtered out by a moderation model. Factually, OpenAI seems to be adopting a moderation model to filter out the harmful data for their fine-tuning API. Therefore, to improve the attack's success rate, one may design a more stealthy attack method that might circumvent the moderation filtration. CMF [19] is an attempt to reach this goal. However, in testing time they require that the question be encoded, and the harmful answer returned by the LLM is also encoded. This restriction might result in the loss of some generality of the attack.\n\u2022 Stronger attack via better harmful data sampling and construction. Because the attacker might have limited harmful data samples to be able to pass through the moderation filtration, and finally used for fine-tuning. For the attacker side, a better harmful data sampling and data construction scheme might be useful to consider, in order to improve the attack performance.\nOn the defense side, the following efforts might be worth exploring:\n\u2022 Data filtering/moderation-based defense. Existing defense literature focuses on improving the training method. However, it is also interesting to consider launching the attack right over the attack surface, i.e., to better filter out the harmful data from the user fine-tuning dataset when they are uploaded. One may look at the embedding space, and apply SOTA outlier detection method to better identify those harmful data among the benign user fine-tuning data.\n\u2022 Defense by integrating multiple training stages. Existing defenses against harmful fine-tuning typically focus on a single stage, such as the alignment stage. However, it is worth exploring the potential benefits of jointly optimizing across the alignment, fine-tuning, and post-fine-tuning stages to improve defense strategies. While this approach may introduce additional complexity, a co-design method could lead to more robust defense performance. Multi-stage training can also draw inspiration from successful defense techniques in multi-task learning and meta-learning [12, 60].\n\u2022 Preference learning. Most of the existing defenses focus on improving upon SFT. However, how to design a defense for RLHF based on preference data (a pair of good/bad answers for the same question) is still under-explored. Future defense design can explore how to better exploit the data structure of preference data to design stronger defenses.\n\u2022 Memory/computation-efficient defense. Most existing defenses increase the memory/computation overhead compared to SFT (without defense). A future research direction may be to utilize existing model acceleration techniques, e.g., pruning/sparsification [15, 30, 39, 88], quantization [13, 52] or factorization [24, 31, 53] for a safety-efficiency co-design.\nMechanism study/explainable research could contribute to a better understanding of the problem, and eventually help the design of defense. We in the following summarize a few phenomena that were discovered by the existing work and discuss the challenges towards better understanding them.\n\u2022 Embedding drift. It is discovered in Vaccine [36] that fine-tuning on explicit harmful samples might cause embedding drift of the alignment data. It is still unknown whether this phenomenon is universal to all harmful fine-tuning attacks. In other words, the answer of whether all the harmful fine-tuning attack leads to embedding drift is still not definitive.\n\u2022 Layerwise safety. Safe LoRA [22] highlights that certain layers of a model play a greater role in maintaining safety, while others are less susceptible and seem more critical for downstream tasks. Similarly, [46] emphasizes that different layers serve distinct functions when exposed to various types of attacks. In vision domain, [38] also emphasize that different layers serve different functions. In terms of safety functionality, Peng et al. [68] have established robust architectural design principles for adversarially robust CNNs. Drawing from insights in the vision domain, we argue that a layer-wise mechanism analysis is essential to fully understand the role of different LLM layers in ensuring safety and to evaluate whether this principle holds universally. A more accurate way to identify the safety layer may be a by-product of analysis and may contribute to future defense design.\n\u2022 Gradient ascent. Several studies, e.g., RepNoise[77], TAR [85], Booster[34] utilizes a gradient ascend term to unlearn the harmful knowledge (i.e., to maximize the loss over harmful data). However, from the authors' experience, a bad configuration of the gradient ascend term might negatively result in the model forgetting the normal knowledge as well. For example, the model may constantly output a single word for any given prompt after unlearning under bad configuration (e.g., hyper-parameters). It is worth studying in which case it will result in a negative effect and what is a better way to configure the gradient ascend term.\n\u2022 Safety basin. It is discovered in [69] that the aligned model exhibits a safety basin, and harmful fine-tuning is in essence dragging the model weights out of this basin. It is interesting to study how the models produced by different defenses are going to shape the safety basin, and how perturbation yielded by different attacks can drag the model out of the safety basin.\nIt is also interesting to discover other properties/phenomena of the harmful fine-tuning attack by looking at the statistics of the model weights/embedding.\nA comprehensive benchmark could also be of great value to contribute and may advance the development of attack/defense design.\n\u2022 Attack/Defense Benchmark. Existing research papers on attack/defense towards harmful fine-tuning generally have different experimental methodologies. It is imperative to create a standard benchmark for modeling the attack/defense performance of the existing/future solutions. The benchmark may provide options for SFT and other RLHF methods (e.g., PPO, DPO) for alignment and user fine-tuning. [76] provides an initial attempt but is not comprehensive enough.\nWe envision that there are a growing number of research papers on this field and we will continuously update this survey."}, {"title": "A Question of Interest", "content": "We below aim to answer questions of interest. The answers are derived from the bitter submission experience of the authors and might be useful to refer to when reviewers question the experiment setup/assumptions/attack settings. The answers only reflect the authors' personal opinions and clearly are not the norms for this research problem.\nQ1: How to evaluate the harmful score of the model after fine-tuning?\nAs surveyed in Section 4, model-based evaluation is the mainstream for evaluating how harmful the model is. Among model-based evaluations, there are two categories of evaluation methods. The first method is to evaluate with GPT4's API, i.e., to prompt GPT4 to rate the score to the model's output. The second method is to use an open-source moderation model to flag the harmful answers and calculate the percentage of the answers to be flagged as harmful.\nCompared to the second method, the GPT4 evaluation method has the following advantages.\n\u2022 Well-accepted by the research community. Safety alignment research mostly uses GPT4 for evaluating harmful scores, and the community has accepted this as the mainstream evaluation method. Therefore, it has less chance to be questioned in the peer-review process.\n\u2022 High accuracy. The GPT4 evaluation can accurately reflect the human evaluation.\nHowever, the GPT4 method has an important downside.\n\u2022 Financially Costly. Calling GPT4 API to evaluate the model's output may be financially expensive, and may not be affordable for many research groups around the globe.\nThe GPT4 evaluation method is used in the most cited harmful-fine-tuning research paper [72], while the second evaluation method is used in [36] (with few citations). The conclusion is that if you have sufficient research funding, use the GPT4 evaluation method. Otherwise, you may pursue a more economic solution, i.e., use an open-source but not well-accepted moderation model from [43] (However. it is still likely that reviewers will ask you for GPT4 evaluation).\nQ2: What fine-tuning task should I use and how to evaluate the fine-tuning task performance?\nExisting research papers use very different fine-tuning task settings.\nOur suggestion again is that it depends on your financial condition. If you have sufficient funding, you may follow the most cited papers to use the standard LLM's evaluation benchmark, e.g., MT-Bench and AlpacaEval. Otherwise, you may use some tasks that can be evaluated its correctness by a simple string-matching method, e.g., SST2, GSM8K, etc.\nQ3: Should I use RLHF (e.g., PPO, DPO), SFT(Full) or SFT (LoRA) for alignment/fine-tuning?\nThe quick answer is that the priority is RLHF> SFT (Full) > SFT (LoRA). RLHF has shown its superiority over SFT. Research directly based on RLHF will be more practical and of interest to the broad community. However, the GPU resource requirement of RLHF is significantly larger than SFT (Full) and SFT (Full) is significantly larger than SFT (LoRA). Also, the training time of SFT (LORA) will be sufficiently smaller than the two other methods, due to its efficient training nature. Of note, if we use SFT (LoRA), it is very likely that reviewers will ask you for SFT (Full) or RLHF to show the generalization of your method.\nQ4: Should I use the aligned version of LLMs (e.g., Llama2-7B-chat) or the non-aligned version (e.g., Llama2-7B) for the experiment?\nUsing an aligned model (e.g., Llama2-7B-chat), rather than aligning a Llama2-7B yourself, would be simpler for developing a fine-tuning stage and post-fine-tuning stage solution. However, because we as researchers are agnostic to the alignment process, it may contain the risk that the defense is effective only because some unknown procedures are taken in the alignment process (i.e., when Llama2-7B-chat is produced). For an alignment stage solution, it may be more reasonable to use the unaligned model as the base model, because that means the defense can be integrated into the model's safety alignment process (which needs to be done by all the service providers, e.g., Meta, OpenAI). With that said, if you use Llama2-7B for evaluation, it is possible that the reviewers will ask you for experiments on Llama2-7B-chat, because that has become the main-stream experiment setting."}, {"title": "Q5: What kind of assumptions of available datasets should we make when we design a defense?", "content": "As illustrated in 4", "include": "i) safety alignment dataset (harmful question-safe answer pair). ii) in-distribution harmful dataset (harmful question-harmful answer pair", "dataset.\nQ6": "Which name of metrics should we use for paper writing?\nAs the research of harmful fine-tuning attacks is still in the early stage", "table.\nQ7": "Why do we consider the fine-tuning-as-a-service scenario, instead of the scenario that the user herself fine-tunes the model?\nSeveral studies, e.g., [85, 77"}]}