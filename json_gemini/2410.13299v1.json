{"title": "LLM-RANK: A GRAPH THEORETICAL APPROACH TO\nPRUNING LARGE LANGUAGE MODELS", "authors": ["David B. Hoffmann", "Kailash Budhathoki", "Matthaeus Kleindessner"], "abstract": "The evolving capabilities of large language models are accompanied by growing\nsizes and deployment costs, necessitating effective inference optimisation tech-\nniques. We propose a novel pruning method utilising centrality measures from\ngraph theory, reducing both the computational requirements and the memory foot-\nprint of these models. Specifically, we devise a method for creating a weighted di-\nrected acyclical graph representation of multilayer perceptrons to which we apply\na modified version of the weighted PageRank centrality measure to compute node\nimportance scores. In combination with uniform pruning this leads to structured\nsparsity. We call this pruning method MLP-RANK. Furthermore we introduce\nan extension to decoder-only transformer models and call it LLM-RANK. For\nboth variants we demonstrate a strong performance. With MLP-RANK on aver-\nage leading to 6.09% higher accuracy retention than three popular baselines and\n13.42 % with LLM-RANK compared to two popular baselines.", "sections": [{"title": "INTRODUCTION", "content": "Over recent years, the increasing popularity of large language models (LLMs), as highlighted by\nBrown et al. (2020), has driven a trend toward the development of ever-larger and more powerful\narchitectures. This trend has been marked by the regular release of models with growing capabilities,\nextending from the modelling of language (Anthropic, 2024; Brown et al., 2020) to applications\nin time series forecasting (Ansari et al., 2024). These transformer-based models typically utilise\nbillions to trillions of parameters (Chitty-Venkata et al., 2023).\nThis results in the problem that the training of such massive models necessitates immense computa-\ntional power, and their deployment for hosting and inference imposes substantial demands on both\ncompute resources and memory. Moreover, the trend toward larger models is anticipated to persist.\nWhile expanding computing and memory capacity is one approach to support the training and de-\nployment of these models, the field of inference optimisation presents a compelling alternative. By\nenhancing the efficiency of trained models, inference optimisation reduces the need for additional\nhardware. This has led to a growing demand for techniques such as pruning, which aims to optimise\ninference performance by identifying and removing dispensable parameters. We find that the intu-\nitive representation of deep neural networks as graphs makes the use of centrality measures from\nGraph theory for computing pruning scores an intriguing area of research.\nAlthough, new state-of-the-art pruning methods such as Ma et al. (2023); Sun et al. (2023); Ashk-\nboos et al. (2024) are being published regularly, we are not aware of any work directly applying a\ngraph theoretical view to pruning LLMs. The only directly related work we are aware of is Li et al.\n(2020), who apply the Katz centrality measure as a scoring function for node importance in artificial\nneural networks with the small world property Milgram (1967). Even though not explicitly related to\ngraph theory, the following group of eigenvalue-based pruning methods Buffoni et al. (2022); Cugu\n& Akbas (2022); Gkalelis & Mezaris (2020) is relevant due to their similarity to the concept eigen-\nvalue centrality from graph theory. We can generally observe that many zero-order pruning methods\nalthough without explicit connection to graph theory, could be rephrased as a network centrality\nmeasure when view through a different lens."}, {"title": "PRELIMINARIES", "content": "2.1 WEIGHTED PAGERANK\nThe original PageRank (PR) algorithm was designed to rank web pages as part of the Google search\nengine (Brin & Page, 1998). More broadly, PR is a measure determining node centrality in a directed\ngraph Ga(V, E, \u03c8), where V is the set of vertices, E is the set of edges and \u03c8 : E \u2192 {(v,w) \u2208\nV \u00d7 V} is a mapping function determining which two vertices an edge connects. The adjacency\nmatrix A is a common way to represent such a graph. Aij = 1 if there is an edge from node\nV; to Vi and otherwise Aij = 0. Each node has an out-degree defined as dout = \u03a3\u03b9 \u0391\u03b9j. Even\nthough they are not directly related, PR can be conceptualised as an extension of Katz centrality\nKatz (1953), which itself can be considered a generalisation of the eigenvector centrality measure\nBonacich (1972). The PR score di of vertex Vi is recursively defined as:\n$\\phi_i = \\gamma \\Sigma_j \\frac{A_{ij}}{d_{out}} \\phi_j + \\frac{1-\\gamma}{|V|}$ (1)\nA variant named WPR was introduced by Zhang et al. (2021). It is created for weighted, directed\nnetworks with non-uniform node-specific information B\u2081 that can be dependent or independent of\nthe network structure. They propose the WPR measure recursively as defined in Equation 2.\n$\\phi_i = \\gamma \\Sigma_j \\frac{W_{ij}}{s_{out}} \\phi_j + (1 - \\theta)) \\frac{A_{ij}}{d_{out}} \\Phi_j + \\frac{(1 - \\gamma) \\beta_i}{\\Sigma_i\\beta_i}$ (2)\nHere, W is the network weight matrix with Wij being the weight associated with the edge from node\nVj to Vi. The out-strength s sout is defined as the sum over the outgoing weights of node j, namely\nsout = \u03a3\u03b9 Wij. Both \u03b3 and \u03b8 are tuning parameters between zero and one, that control the trade-\noff between the graph structure and auxiliary node information Bi as well as the trade-off between\nadjacency and weight information, respectively. When 0 = 0 and \u03b2\u2081 = 1 WPR is equivalent to PR\nas introduced by Brin & Page (1998) and defined in Equation 1.\n2.2 MULTILAYER PERCEPTORNS\nThe MLP is a dense feedforward neural network architecture with multiple layers. The network is\nconsidered to be feedforward as information gets sequentially fed through its K densely connected"}, {"title": "DECODER-ONLY TRANSFORMERS", "content": "Introduced by Vaswani et al. (2023) as a sequence transduction model, the transformer architecture\nconsists of an encoder and a decoder both with a similar structure. With the popularisation of large\nlanguage models, a plethora of transformer-based architectures have emerged, such as decoder-only\nmodels, which were first introduced by Radford (2018) and are used for generation tasks.\nMost variants are largely based on the original architecture in which both the encoder and decoder\nare composed of n blocks. Typically the embedded and positionally encoded input gets fed into the\nfirst of these blocks and is in the following incrementally transformed and passed to the next block.\nThe final output is then fed into the final linear layer with softmax activation to predict the next\ntoken in the sequence. Each of the decoder blocks consists of a masked multi-head attention (MHA)\ncomponent with three weight matrices for key K, value V, and query Q projections as defined in\nEquation 4 as well as a position-wise feed-forward network (FFN) with two layers. Both come with\nskip connections and are followed by a layer norm to facilitate more stable training.\nAttention(Q, K,V) = softmax($\\frac{QK^T}{\\sqrt{d_k}}$)V (4)"}, {"title": "MLP-RANK METHOD", "content": "In the following we first propose a way of representing multilayer perceptrons as a graph. We then\nintroduce a modification of the WPR centrality measure as a way of computing pruning importance\nscores. Together these constitute the MLP-RANK method.\n3.1 GRAPH REPRESENTATION\nWe propose a simple representation of the MLP as a weighted, directed and acyclical graph. This\nrepresentation is directly derived from the model's weight matrices, where W(k) corresponds to the\nkth layer of the neural network and has the dimension m(k) \u00d7 n(k). In the graph representation,\nlayer k has m(k) vertices which represent the neurons of the projection W(k). As such, the weight\nmatrix of the graph WG can be arranged as a block matrix where a partition either consists of an\nMLP weight matrix or of zeros. The trace of blocks directly below the centre diagonal contains the\nweight matrices of the MLP, which are defined in Equation 5 for positive k and visualised in A.2.\n$W_G\\Big((n(0)+\\sum_{i=1}^{k-1}m(i)):(n(0)+ \\sum_{i=1}^k m(i)),(\\sum_{i=1}^{k-1}n(i)):((\\sum_{i=1}^k n(i))\\Big) = W^{(k)}$ (5)\nPruning a neuron in the MLP corresponds to removing the respective row and column from the\nweight and adjacency matrices of the graph. In the neural network model, this transfers to removing\nthe ith row of a weight matrix W(k) as well as the ith column of W(k+1).\nThe process of identifying which rows and columns to remove from WG (and AG) using WPR\nrequires operations on the large highly sparse |V| \u00d7 |V| graph matrix, with exponentially more\nelements than the original MLP weight matrices."}, {"title": "MODIFIED WEIGHTED PAGE RANK", "content": "Using the component-wise graph representation we compute importance scores based on a modified\nversion of WPR as defined in Equation 7. Motivated by the fact that we use uniform pruning we\nemploy layer wise normalisation of the auxiliary information \u1e9e instead of global unit normalisation.\nTo align with the component wise graph representation we use the individual weight matrix blocks\nand layer-wise importance score vectors. Note that y and 0 are used as defined in subsection 2.1 and\n\u1e9e is used to include output activations as a norm over a representative calibration dataset, denoted\nas || X(k+1)||2. Here The latter is an important aspect as it is the only way that activation functions\nare taken into account by the scoring method.\n$\\phi_i^{(k)} = \\gamma \\Sigma_j \\frac{W_{ij}^{(k)}}{s_{out}^{(k)}} \\phi_j^{(k-1)} + (1-\\theta)) \\frac{A_{ij}^{(k)}}{d_{out}^{(k)}} \\Phi_j + \\frac{\\beta^{(k)} || X^{(k+1)}||_2}{\\Sigma_l \\beta_l^{(k)} m^{(k)}}$ (7)\nTo assure convergence with the power iteration method Herrmann (1983) to positive importance\nscores, we restrict elements of the recursive formulation to positive values which is done by using\nabsolute weights."}, {"title": "LLM-RANK METHOD", "content": "LLM-RANK extends the MLP-RANK pruning method to more complicated transformer-based ar-\nchitectures. The main difference is an updated graph representation which captures the structure\nof the transformer. Similar to MLP-RANK component-wise WPR is utilised for the computation\nof importance scores. As such LLM-RANK also is a post pruning strategy without weight updates\nwhich induces uniform structured sparsity.\nThe main challenge in the adaptation of MLP-Rank to transformers is the representation of the\nmodel architectures as a directed graph. This is particularly true as not all linear projections in the\ntransformer are of the form input times weight matrix (plus bias) which is the implicit assumption\nof the graph representation so far. An example of this is the scaled dot product attention mechanism\n(Equation 4) which includes the multiplication of input projections with each other.\nA straightforward way of dealing with this difficulty of representing attention is simply not to prune\nthe MHA components in the transformer. Instead, we could view each FFN individually as an MLP\nand apply MLP-Rank pruning to it. This could already achieve significant speedups, as most of the\nparameters in a transformer reside in the FFNs components (Lin et al., 2022).\nA more sophisticated approach is to chain the individual FFN networks using the fact that the at-\ntention components only add incremental changes, due to the skip connections. This allows us to\npropose to structurally disregard the attention operation itself and only include its transformations\nto the information flow through the network based on calibration activations. This is similar to how\nactivation functions are handled in the MLP networks. With this approach, we treat all FFNs as\na single deep MLP and apply component-wise WPR centrality from Equation 7 for the computa-\ntion of importance scores. The calibration activations are however still computed on the unchanged\ntransformer model. This means the chained representation is only created for the computation of\nimportance scores, which are then used to prune the original model."}, {"title": "EXPERIMENTS", "content": "5.1 MLP-RANK ACCURACY COMPARISON\nThe MLP-RANK method is evaluated on four different multilayer perceptrons with various sizes\nand trained on different datasets, namely MNIST LeCun & Cortes (2010), Fashion-MNIST Xiao\net al. (2017), CIFAR10 Krizhevsky (2012) and CIFAR100 Krizhevsky (2012). The models and the\nrespective dataset they were trained on can be seen in Table 4.\nEach model is trained until convergence using the ADAM optimiser Kingma & Ba (2017) with\napproximations of optimal hyperparameters for learning rate and weight decay, determined with a\nsimple grid search. For the final evaluation we use 80% of the data for training and 20% for accuracy\nevaluation.\nTable 2 shows the comparison of MLP-RANK pruning with random structured pruning, L1-Norm\npruning and pruning based on layer-output activations. The table shows the average relative amount\nof top-1 and top-5 accuracy retention compared to the respective dense MLP across all four models.\nStandard deviations are shown in Table 4."}, {"title": "LLM-RANK ZERO-SHOT COMPARISON", "content": "We evaluate LLM-RANK pruning on the LLaMA architecture, in particular, the Open-LLaMa-3b-\nv2 model provided by Geng & Liu (2023). The performance of the pruned model is evaluated on\nsix popular LLM benchmarks using the EleutherAI LM Harness Gao et al. (2024), namely ARC-\nChallenge, ARC-easy, HellaSwag, OpenBookQA, PIQA and WinoGrande.\nLLM-RANK pruning is compared with two baselines, the naive L1-Norm pruning approach and\nthe state-of-the-art method SliceGPT (Ashkboos et al., 2024) on 5 different levels of sparsity. The\ncalibration activations are computed on random samples from the C4 dataset (Raffel et al., 2023).\nThe experiments were run on an Amazon EC2 p3.16xlarge instance with 8 V100 GPUs, where\neach pruning run with LLM-RANK had an average wall time of only 53.84 seconds with a standard\ndeviation of 1.20 seconds.\nTable 3 shows the zero-shot accuracy comparison in percent between LLM-RANK using chained\nFFNs and two popular baselines. The first baseline is L1-Norm pruning which is similar to mag-\nnitude pruning and often times used as a naive but strong baseline for pruning. The second one is\nSliceGPT which was introduced by Ashkboos et al. (2024) and is one of the current state-of-the-art\nmethods for structured pruning of LLMs. They are a good comparison point to LLM-RANK as\nneither use post pruning tuning which leads to the noticeable gap between dense and pruned model\nperformance for higher sparsity ratio.\nThe comparison shows that LLM-RANK outper-\nforms the two baselines for lower sparsity ratios and\nhas comparable performance for higher sparsity ra-\ntios. In particular, it has the highest average accu-\nracy for four out of five sparsity ratios as visualised\nin Figure 2. In particular, LLM-RANK on average\nhas a 18.25% higher accuracy per sparsity ratio than\nL1-Norm pruning and an 8.60% higher accuracy ac-\ncuracy per sparsity ratio than SliceGPT. A table with\nthe standard deviations for Table 3 can be found in\nB.2.\nNote that the difference in average performance be-\ntween LLM-RANK and the two baselines seems to\nbe negatively correlated with the sparsity ratios. We\nhypothesise that this may be partly attributed to the\nfact that there are only a few fully redundant struc-"}, {"title": "INFERENCE SPEEDUP", "content": "The aim of pruning as a way of inference optimisation in LLMs, is to speed up computations and\nreduce memory consumption by removing parameter dropping and operations associated with them.\nStructured pruning as performed by LLM-RANK leads to real-world speedups without the need for\nspecially designed hardware or software. This is in opposition to unstructured or semi-structured\nsparsity which requires specialised hardware to achieve real speedups.\nWhen pruning the same share of nodes per layer, the relationship between the amount of pruned\nnodes, number of parameters, number of FLOPs and theoretical speedup is straightforward. Here\npruning 50% of neurons per layer corresponds to halving the number of parameters in the network\nas well as the number of FLOPs which then leads to a two times speedup in inference latency. This\ndirect translation of sparsity ratio to inference speedup is also expected for our method, but not\nempirically investigated as part of this paper."}, {"title": "CONCLUSION", "content": "Motivated by the ever-growing sizes of LLMs and the need for effective pruning methods, we pro-\npose MLP-RANK and LLM-RANK, a graph theoretical approach to pruning both MLPs and LLMs.\nIn particular we propose a way of creating a weighted directed acyclical graph representation from"}, {"title": "MLP-RANK PRUNING REPRESENTATION", "content": "A.1 MATRIX DECOMPOSITION EXAMPLE\nThis is a simple example of the graph representation of a two-layer MLP, without nonlinearities and\nactivations and weight matrices\nW(0) = $\\Big(\\frac{1}{3} \\frac{2}{4} \\Big)$ and W(1) = $\\Big(\\frac{5}{7} \\frac{6}{8} \\Big)$ (8)\nHere both W(1) and W(2) have the dimension 2 \u00d7 2. This means that the MLP takes an input vector\nx(1) of length 2 and outputs a vector of length 2. The graph associated with this network has 6\nnodes. Two surrogate nodes are associated with the input vector, two with the hidden layer and two\ncorresponding to the final output layer. The weight, as well as adjacency matrices, are defined in\nEquation 9.\nWG =\n$\\begin{pmatrix}\n0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 2 & 0 & 0 & 0 & 0\\\\\n3 & 4 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 5 & 6 & 0 & 0\\\\\n0 & 0 & 7 & 8 & 0 & 0\n\\end{pmatrix}$\nand AG =\n$\\begin{pmatrix}\n0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 1 & 0 & 0 & 0 & 0\\\\\n1 & 1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 1 & 0 & 0\n\\end{pmatrix}$ (9)\nA visualisation of the weight matrix of the graph representation can be found in Figure 3. It clearly\nshows the structure of weights in the weight matrix providing an intuition for how the decomposition\noutlined in Equation 5 is possible."}, {"title": "GRAPH REPRESENTATION WEIGHT MATRIX", "content": "Figure 4 provides a good overview of the structure of WG, showing how output dimensions m(k) of\none layer have to match the input dimensions n(k+1) of the following layer."}, {"title": "EXPERIMENT RESULTS", "content": "B.1 MLP-RANK EXPERIMENTS\nTable 4 documents the standard deviations corresponding to the aggregated comparison of retained\naccuracy for different scoring functions as presented in Table 2. The relatively high standard devi-\nation values of accuracy retention are caused by the large difference between models across which\nresults are aggregated in this table. They are not an indication of high variance in the pruning meth-\nods itself which are deterministic.\nIn the following, the comparison of different scoring measures will be further broken down by\ncomparing the raw accuracy results for each of the four MLP models individually."}, {"title": "EXTENSION TO ATTENTION", "content": "The proposed LLM-RANK can be extended to include key-, value-, and query matrices from the\nmulti-head attention (MHA) components for pruning as outlined in section 4. Figure 5 shows how\nthis could be achieved in two steps: (1) the extraction and chaining of the FFNs with key-, value-,\nand query matrices as appendices to this chain. (2) the creation of multiple MLP networks each\nconsisting of the chain of previous FFNs and ending in one of the three attention matrices."}]}