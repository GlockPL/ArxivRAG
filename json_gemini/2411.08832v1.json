{"title": "Offline Adaptation of Quadruped Locomotion using Diffusion Models", "authors": ["Reece O'Mahoney", "Alexander L. Mitchell", "Wanming Yu", "Ingmar Posner", "Ioannis Havoutis"], "abstract": "We present a diffusion-based approach to quadrupedal locomotion that simultaneously addresses the limitations of learning and interpolating between multiple skills (modes) and of offline adapting to new locomotion behaviours after training. This is the first framework to apply classifier-free guided diffusion to quadruped locomotion and demonstrate its efficacy by extracting goal-conditioned behaviour from an originally unlabelled dataset. We show that these capabilities are compatible with a multi-skill policy and can be applied with little modification and minimal compute overhead, i.e., running entirely on the robot's onboard CPU. We verify the validity of our approach with hardware experiments on the ANYmal quadruped platform.", "sections": [{"title": "I. INTRODUCTION", "content": "Quadruped robots' ability to traverse complex terrain while carrying useful payloads makes them excellent choices for applications in manufacturing, construction and search & rescue. The state of the art for quadruped locomotion is maturing rapidly and both learning-based and gradient based methods are prevalent in research and industry. Learning-based methods such as reinforcement learning (RL) have produced impressive results [1]\u2013[6], but still suffer from a number of limitations. Due to the limited expressive capacity of neural network policies, using multiple skills in previous works required a hierarchical approach with a difficult multi-step training pipeline [5]. Since the methods are trained online, the resultant policies often can't be adapted to new behaviours without being retrained from scratch. Our approach attempts to propose a solution to each of these two limitations in particular.\nTo alleviate these issues, an alternative learning-based paradigm, imitation learning, can be used. This involves reproducing a set of reference motions, which popular approaches achieve with an adversarial training loop [7]. In recent years, Diffusion models [8], [9] have outperformed prior methods such as [10] and are able to approximate multi-modal distributions at a high fidelity. By approximating the score function [11] of a distribution, instead of directly learning the densities, and iteratively applying Langevin dynamics [12] to generate samples, they are able to accurately represent highly multi-modal distributions, incorporate flexible conditioning [13], and also scale well with dataset and model size. This multi-modality allows us to circumvent the need for hierarchical planning, as previous works have shown diffusion policies are capable of learning multiple skills in a single model [9].\nThough large datasets are prevalent and can fuel imitation learning, there are limitations. There is no guarantee that the dataset contains trajectories which maximise a desired test time reward. For example, our locomotion dataset may contain a broad range of motions where the robot is moving with different gaits and speeds, but at test time we require specific behaviours. If data is labelled it is easy to extract this desired behaviour, but this is often not the case making this problem challenging. Therefore, we would like to tune our diffusion model to stitch together locomotion trajectories which maximise the return of reward functions for certain types of motion.\nTo tackle the challenge of offline adaptation, we propose using classifier-free guidance (CFG) [14] to optimise a diffusion-model trajectories after training. This is achieved by treating the expected return as a probability function [13], [15] and using an Offline RL [16] training paradigm. In particular, we label the dataset with a velocity tracking reward and use CFG to guide our trajectories to maximise it, thus adapting our model to generate the desired behaviour.\nIn this paper we present a diffusion-based approach to quadruped locomotion, capable of switching between dis-"}, {"title": "II. RELATED WORK", "content": "Learning-based approaches to locomotion - Advancements in reinforcement learning have demonstrated a remarkable ability for learning highly dynamic locomotion policies [5], [6]. When handling multiple skills, most state of the art methods have opted to for a hierarchical approach, where individual skills are learnt with different policies, and a high level policy is trained to switch between them based on the agents state and terrain features [5]. Despite the high-performance ceiling of this type of method, this formulation limits the expressive capability of these models to learn motions between these skills or outside of them. Being able to handle multiple skills in a single model would be preferable both for simpler training and also to allow continuous interpolations between them as opposed to discrete switching.\nAnother approach to learning locomotion skills is imitation learning, where instead of designing some reward function, an agent is trained to mimic some reference clip, typically generated from motion capture data. One popular approach in this category is adversarial motion priors (AMP) [7]. While AMP has been shown to be capable of handling multiple skills in a quadruped locomotion setting [18], it still has a few disadvantages, namely that it is adversarial method which notoriously suffer from scalability problems due to training instability [10]. Our work could also be seen in the context of policy distillation, which aims to combine multiple policies into one without the need for hierarchical methods. One popular approach to this is DAgger [19] which works by collecting a dataset using current policy at each iteration and updating it using the whole dataset.\nDiffusion for control - Due to their ability to accurately model highly multi-modal distributions, diffusion models have become an extremely popular paradigm for generating high fidelity images and video [20] as well as control trajectories [13], [15], [21]. These models exhibit several appealing properties including high accuracy, allowing for easy deployment of policies trained offline [21], an ability to generate variable length trajectories in a single step, allowing for long-horizon predictions that do not suffer from compounding error [13], and an amenability to flexible"}, {"title": "III. METHOD", "content": "Our method trains a diffusion model over a diverse range of locomotion behaviours. A classifier-free guidance (CFG) technique for adapting the output of the diffusion model to optimise for specific test-time rewards is also presented. Lastly, we explain the specifics of our architecture that were required to apply this model to locomotion."}, {"title": "A. Diffusion formulation", "content": "Diffusion models were independently derived by two different lines of research [8], [12] with two corresponding interpretations. In this work we focus on the score-based formulation introduced in [12]. The score function of a distribution is the gradient of its log density function. This can be learnt by several approaches [32], [33] but most works use the denoising score matching loss below, where $s_\\theta(x)$ is a neural network, $p(x)$ is the data distribution and $p(x)$ is the noise perturbed distribution:\n$\n\\frac{1}{2}\\mathbb{E}_{q_\\sigma(x|x) \\sim p_{data}(x)} [|| s_\\theta(x) - \\nabla_x \\log q_\\sigma(x | x)||^2].\n$ (1)"}, {"title": "B. Classifier-free guidance", "content": "By applying Bayes rule and the definition of the score function to the conditional distribution $p(x | y)$, we can produce the following equation for Classifier-Free Guidance:\n$\n\\nabla_x \\log p(x | y) =\\frac{(\\lambda - 1) \\nabla_x \\log p(x) + \\lambda \\nabla_x \\log p(x|y)}{\\lambda}\n$. (6)\nWhile this equation is exactly correct for $\\lambda = 1$, it was shown in [14] that using values of $\\lambda > 1$ can actually yield much higher quality results. Increasing the value of lambda can be interpreted as trading off dynamics consistency with satisfying the conditioning variable. In our case, the conditioning variable $y$, similar to [15], is a discounted sum of future rewards scaled between [0, 1]. At test time,"}, {"title": "C. Offline adaptation after dataset collection", "content": "When a dataset is properly labelled with input commands, we can easily recover this behaviour by adding these commands to our conditioning input. However this is often not the case, for example, when using motion capture data. Given this limitation, it is not straightforward how goal-conditioned behaviour can be recovered at test time. One approach is to treat this as an Offline RL problem, labeling the dataset with a velocity tracking reward and guiding our model towards generating samples with high returns.\nWe first demonstrate this is possible by defining the following reward function:\n$\nr = e^{-3(v-v_{target})^2} - 1,\n$ (7)\nwhere $v_{target}$ is some fixed velocity command. We then take a discounted sum of these rewards over the next 50 timesteps to get the return. Finally, the return is transformed using equation 8 to make it positive\n$\nR = R_o^{e^{R_o/A}},\n$, (8)\nwhere $R_o$ is the un-scaled return distribution. We then linearly scale this between [0, 1] based on the batch statistics. The temperature of the exponential $A$ impacts performance and is discussed further in section IV-E."}, {"title": "D. Diffusion for locomotion", "content": "The overview of our model is shown in Fig 2. We take a history of $T_{cond}$ observations, containing the robot's orientation, twist, joint position, and joint velocity. We also take the the diffusion timestep, one-hot skill vector, and return. These are all embedded with separate linear layers and concatenated to form the conditioning input. The main block of the model is a multi-headed transformer decoder. This first takes as input an embedded noise trajectory and applies causally-masked self-attention. We then apply cross-attention between this embedding and the conditioning input."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We first explore the application of classifier-free guidance to offline RL as demonstrated in [15]. We then demonstrate the skill switching abilities of our model in conjugation with reward guidance. This is deployed onto a real robot to verify the validity of our method. We lastly ablate the effect of important hyperparameters in model, namely the guidance strength and return scaling."}, {"title": "A. Dataset collection", "content": "To collect our dataset, we use a reinforcement learning policy trained using the reward function from [34] with random velocity commands and physical parameters. We train two separate 25Hz policies for two separate skills, walking and crawling. The training setup is almost identical for both except with different desired base heights and nominal joint positions. Both policies were trained with an action delay of a single timestep to account for the inference time of the model on hardware. We collect a total of 1M episode steps for each skill."}, {"title": "B. Offline adaptation", "content": "After training, the model's outputs are adjusted to recover different locomotion behaviours as per Sec. III-C.  We compare an expert model with access to the ground truth commands to our model that has no access to these commands but instead aims to maximise the reward function in equation 7 via classifier-free guidance. We use three different popular samplers from the SDE diffusion framework, DDPM, DDIM, and DPM++(2M) that all performed best at different tasks in prior work [22]."}, {"title": "C. Reward tracking with skill switching", "content": "We next investigate the skill switching capability of our model. Previous works have have looked at switching between skills with similar state distributions on smaller platforms but we wanted to test if the interpolating abilities of diffusion models would scale up to larger platforms with more dynamic transitions. To do this we collected separate datasets generated by walking and crawling reinforcement learning policies with no transitions present between the two. Our model was able to learn interpolations between these two skills which were remarkably stable over the full range of velocity commands in the dataset. \nA key benefit of diffusion-based approaches to offline RL is that since having multiple skills and applying reward guidance are independent design decisions they can both be used in conjunction."}, {"title": "D. Effect of lambda", "content": "We next aim to ablate the effect of several important hyperparameters to the capabilities shown above. The first of these is the value of lambda from equation 6. The two terms in this equations can be interpreted as generating samples that are likely given the bulk dynamics of the dataset, p(x), and satisfying the conditioning by generating samples under the conditional distribution p(x | y). Increasing lambda trades-off the former for the latter. Thus, we would predict that higher values of lambda generate higher reward trajectories, up to some limit, past which too much dynamics consistency is sacrificed and motion becomes unstable."}, {"title": "E. Return distribution", "content": "Another area of interest is to analyse how the shape of the return distribution affects performance. This section attempts to answer two questions. The first is whether the model just samples from a subset with returns = 1, more akin to goal conditional imitation as opposed to RL.  At inference time we still condition the model on returns = 1. The results, in summary highlight both the impressive"}, {"title": "V. CONCLUSION", "content": "We present a novel diffusion based method to learn a range of locomotion skills and show that we can adapt the test-time behaviour to maximise a reward after training. This is achieved using classifier-free guidance and we demonstrate robust and high quality trajectories on the real ANYmal quadruped. We also provide in-depth analysis of our methods limitations using ablations of several key hyperparameters.\nAs with all imitation learning methods, high-quality data over a broad range of environments are required for training and this could be perceived as a limitation. However, with our classifier-free guidance, our methods are able to stitch together trajectories which maximise a desired reward after the diffusion model is trained. Hence, we do not require labelled data, and since our method can be adapted at test time, a mixture of feasible trajectories are sufficient. As with the majority of methods, there is a limitation to the degree to which the model can be adjusted after training. Our model approximates a broad range of diverse skills and can interpolate between them, but cannot extrapolate and maximise out-of-distribution rewards. In future work we will show that this mechanism could be used to guarantee that the test-time behaviour remains within the training distribution."}]}