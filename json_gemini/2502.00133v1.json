{"title": "Exploring Transfer Learning for Deep Learning\nPolyp Detection in Colonoscopy Images Using YOLOv8", "authors": ["Fabian Vazqueza", "Jose Angel Nu\u00f1ez\u00aa", "Xiaoyan Fub", "Pengfei Gu\u00aa", "Bin Fu\u00aa"], "abstract": "Deep learning methods have demonstrated strong performance in objection tasks; however, their ability to\nlearn domain-specific applications with limited training data remains a significant challenge. Transfer learning\ntechniques address this issue by leveraging knowledge from pre-training on related datasets, enabling faster\nand more efficient learning for new tasks. Finding the right dataset for pre-training can play a critical role\nin determining the success of transfer learning and overall model performance. In this paper, we investigate\nthe impact of pre-training a YOLOv8n model on seven distinct datasets, evaluating their effectiveness when\ntransferred to the task of polyp detection. We compare whether large, general-purpose datasets with diverse\nobjects outperform niche datasets with characteristics similar to polyps. In addition, we assess the influence of\nthe size of the dataset on the efficacy of transfer learning. Experiments on the polyp datasets show that models\npre-trained on relevant datasets consistently outperform those trained from scratch, highlighting the benefit of\npre-training on datasets with shared domain-specific features.", "sections": [{"title": "1. INTRODUCTION", "content": "Colorectal cancer is a significant health concern, being one of the leading causes of cancer-related mortality\nworldwide.\u00b9 Early detection of polyps during colonoscopy procedures is vital for preventing colorectal cancer.\nStudies have shown that early detection and removal of polyps can reduce the incidence and mortality of colorectal\ncancer by up to 90%.\u00b2 Traditional methods for detecting polyps during colonoscopies rely heavily on the visual\ninspection skills of the endoscopist doing the exam. Despite the expertise of medical professionals, these methods\nare prone to variability and human error, leading to missed polyps, particularly small or flat ones. Variability\nin polyp size, shape, appearance, glare, and obstructions in colonoscopy images make the task much more\nchallenging (see Fig. 1).\u00b3\nDeep learning (DL) has made substantial advancements in recent years, particularly in computer vision tasks\nsuch as object detection, 4 classification, and segmentation. 6-9 One particularly DL model, YOLO (You Only\nLook Once), has grown in popularity due to its speed and efficiency. 10 Various versions of YOLO have been\nmade and are used for object detection, segmentation, and localization tasks. In, 11 a YOLOv8 model was trained\nto detect polyps to observe its robustness and adaptability. Using polyp datasets such as the CVC-Clinic DB, 12\nCVC-ColonDB,13 ETIS-LaribPolypDB,14 and Kvasir-SEG15 for training and evaluation, Lalinia and Shafi were\nable to achieve high results with YOLO-v8 to show its effectiveness for real-time polyp detection in medical\nimaging. However, challenges remain in developing models that can generalize well across different domains\nand datasets. Issues such as dataset bias, variation in object appearance, occlusion, and complex backgrounds\ncontinue to affect the performance of computer vision systems. 16 Especially in object detecting tasks with\nlimited data availability, such as in polyp detection. Additionally, real-time processing and the ability to operate\neffectively under varied and dynamic conditions are critical requirements for practical applications, including\nmedical imaging. 17\nTransfer learning is a technique that addresses the challenge of limited data availability by leveraging models\npre-trained on large, diverse datasets to improve performance in specific tasks. 18 This technique is especially\nvaluable in medical applications, where acquiring large, annotated datasets is often costly and labor-intensive,"}, {"title": "2. METHODOLOGY", "content": "Fig. 2 illustrates the transfer learning pipeline for deep learning-based polyp detection. The process includes pre-\ntraining various models on different datasets, fine-tuning them using a polyp detection dataset, and evaluating\nthe models' performance in detecting polyps."}, {"title": "2.1 Pre-Training", "content": "In this stage, different YOLOv8n models were each pre-trained on different datasets, ranging from out-of-domain\ndatasets such as the COCO dataset to in-domain datasets related to medical imaging. The aim of this pre-\ntraining phase was to equip the model with robust feature extraction capabilities that could later be fine-tuned\nfor the domain-specific task such as polyp detection. Different YOLOv8n models are pre-trained on a respective\npre-training dataset and are compared with a baseline model trained from scratch (without pre-training). The\npre-training was carried out in two phases.\nIn the first phase of pre-training, smaller datasets were used to train the YOLOv8n models. The pre-training\ndatasets used are each divided into 80% for training and 20% for validation. The images were resized to 640 \u00d7 640\nwith pixel values normalized to a range of 0 to 1. No separate testing set was used in order to allocate more data\nfor training. The datasets used for pre-training are as follows: (1) The MRI Brain Tumor dataset:25 This\npublic dataset from Kaggle is used for bounding box detection of different brain tumors. The dataset contains\n2,175 images and 4 classes and was split into 1,740 training images, and 435 validation images. The model\ntrained on this dataset is named YOLO-brain-tumor. (2) The Acne dataset:26 This public dataset from\nKaggle is used for training to detect acne in images. It contains 927 images and only one class. It was split\ninto 741 training images, and 186 validation images. The model trained on this dataset is named YOLO-acne.\n(3) The Fruit and Vegetable Detection for YOLOv4 dataset:27 This public dataset from Kaggle is used\nto detect 14 different types of fruits and vegetables from images. It contains 4,592 images and 14 classes. It\nis split into 3,673 training images, and 919 validation images. The model trained on this dataset is named\nYOLO-fruit&vegetable.\nSince the COCO17 dataset contains over 100,000 images28 which is significantly larger than most public\ndatasets, we utilized other pre-training datasets that contained a minimum of 10,000 images or combined different\ndatasets with the same type of images for the second phase of the pre-training. This was done to observe the\neffects of diversity and training size on transfer learning. The datasets used are each divided into 80% for training\nand 20% for validation. The images were resized to 640 \u00d7 640 with pixel values normalized to a range of 0 to 1.\nNo separate testing set was used in order to allocate more data for training. The models trained in the second\npre-training phase are denoted with an \"-XL\" to show they are trained on the larger datasets containing more\nthan 10,000 images. The datasets used for the second phase of pre-training are as follows:\nThe Skin Cancer: HAM10000 dataset is a public dataset, accessed through Kaggle, and comprises of"}, {"title": "2.2 Fine-Tuning", "content": "The second stage involved fine-tuning the pre-trained YOLOv8n model on a polyp detection dataset. This pro-\ncess aimed to adapt the model's generalized feature representations to the specific task of detecting polyps\nin colonoscopy images. Fine-tuning was conducted using four publicly available polyp detection datasets:\nthe CVC-ClinicDB12 which contains 612 images, the CVC-ColonDB13 which contains 380 images, the ETIS-\nLaribPolypDB14 which contains 196 images, and the Kvasir-SEG15 which contains 1000 images. These four\ndatasets were combined and randomly split to use 80% for training, 10% for validation, and 10% for testing.\nWhen combined, the polyp datasets yielded 1,748 training images, 220 validation images, and 220 testing images.\u2018\nThe fine-tuning process was performed at intervals of 20, 50 and 100 epochs. This was done to measure the\nconvergence speed during training and see if transfer learning benefitted training time. The hyperparameters\nwere kept consist as with pre-training maintaining the same batch size, learning rate, and optimization settings.\nAs in the pre-training phase, the same data augmentation techniques were applied to enhance robustness and\nprevent overfitting. Transfer learning was employed by initializing the YOLOv8n model with the weights obtained\nduring pre-training and continuing the fine-tuning training.\nThe datasets for fine-tuning first required obtaining the bounding box annotations with the correct YOLO\nformat from the ground truth segmentation masks of the polyp datasets12131415 in order to train the YOLOv8n\nmodels. The minimum and maximum contours of the ground truth masks were taken to calculate the center\ncoordinates of the bounding box, box width, and box height. These were then saved in text files utilized for"}, {"title": "2.3 Evaluation", "content": "After fine-tuning, the trained model was evaluated on the separate testing set created to assess the model's\nability to detect polyps in colonoscopy images. The test set was derived from the same polyp detection datasets\nused for fine-tuning but was kept independent during the training process to ensure unbiased evaluation.\nPerformance was measured using several evaluation metrics, including precision, recall, F1-score, mean Av-\nerage Precision (mAP) at a treshold of 0.5, and mAP at a treshold of 0.5 to 0.95. Precision assesses the ratio\nof the correctly identified polyps among all the instances where a polyp is detected, calculated as the sum of all\ncorrect detections and incorrect detections. This metric illustrates the percentage of correct predictions made by\nthe model, and a higher precision value indicates fewer false positives. Recall, represents the ratio of correctly\ndetected objects over all the objects it is supposed to detect, calculated as the sum of all the correct detections\nand missed detections. A high recall value is crucial in preventing missed detections. The F1-score is a combined\nmetric that considers both recall and precision. It provides a more comprehensive evaluation of the model's de-\ntection performance by balancing the trade-off between false positives and false negatives. As a result, a higher\nF1-score a model is more accurate. Lastly, the mean Average Precision (mAP) is a comprehensive metric used\nto evaluate the performance of object detection models. It is a common evaluation metric used in computer\nvision tasks where localization of the object is done with bounding boxes. This metric is calculated by finding\nthe Average Precision (AP) for each class and the average over a number of classes. This is done by calculating\nthe area under the precision-recall curve at the respective thresholds. The model was deployed to detect polyps\nin real-time, generating bounding boxes around detected polyps for each image in the testing set. These metrics\nare commonly used when evaluating computer vision models that involve detecting objects with bounding boxes,\nsuch as YOLO."}, {"title": "3. EXPERIMENTS AND RESULTS", "content": "3.1 Implementation Details\nIn this paper, the YOLOv8n35 (nano) model was utilized, which comprises of 225 layers and approximately\n3.01 million learning parameters. It is designed for efficient object detection. The libraries used in this paper\nare: Ultralytics YOLOv8.2.42 packag, Python-3.12.0, Pytorch version: 2.5.0.dev20240620+cu124, Torchvision\nversion: 0.20.0.dev20240625+cu124, and CUDA 12.5. Training was done on a local computer with an NVIDIA\nGeForce RTX 4090 Laptop GPU and with Google Colab's T4 GPU and A100 GPU.\nThe pre-training process was carried out over 100 epochs with the following fixed hyperparameters set at\ndefault by Ultralytics: a batch size of 16, an initial learning rate of 0.002, and an adam optimzer36 as the opti-\nmization algorithm. Data augmentation techniques such as random cropping, horizontal flipping, and rotation\nwere applied to improve model generalization. Upon completion of pre-training, the learned weights were saved\nfor transfer to the fine-tuning phase. All experiments were conducted with the same training parameters across\nboth stages, including consistent hyperparameters and identical data augmentation techniques."}, {"title": "3.2 Experimental Results", "content": "From all the results, the YOLO-fruit&veg-XL model achieved the highest overall F1-Among all the models\nevaluated, the YOLO-fruit&veg-XL model achieved the highest overall F1-score of 95.1% after training for 100\nepochs, as shown in Table 6. In experiments with YOLO models pre-trained on smaller datasets, the YOLO-\ncoco model, pre-trained on the COCO17 dataset, consistently achieved the highest F1-scores at 20, 50, and 100\nepochs. Its best performance was recorded at 100 epochs, reaching an Fl-score of 94.5%, as seen in Table 3.\nThis result is likely due to the COCO dataset's extensive diversity, complex backgrounds, numerous classes, and\nhigh-quality annotations.\nThe results further demonstrate that increasing the number of images used for pre-training enhances model\nperformance when employing transfer learning. Specifically, when YOLO models were pre-trained on at least\n10,000 images, their performance improved significantly, as evident in Tables 4, 5, and 6, where the XL models\nconsistently outperformed their smaller counterparts. Among these models, YOLO-fruit&veg-XL achieved the\nhighest F1-scores at 50 and 100 epochs, while at 20 epochs, it was surpassed only by the YOLO-brain-tumor-XL\nmodel.\nFor models pre-trained on smaller datasets, the YOLO-fruit&veg model ranked second at 20, 50, and 100\nepochs, with the YOLO-coco model achieving the highest scores at each stage. Although in-domain datasets did\nnot yield the best results, larger and more diverse datasets proved more effective for transfer learning. Notably,\ndespite being considered out-of-domain for polyp detection, the YOLO-fruit&veg dataset may have contributed\nto improved performance due to visual similarities between fruit, vegetables, and polyps in colonoscopy images.\nFinally, the base YOLOv8n model, trained from scratch, consistently produced the lowest F1-scores. This\nunderscores the effectiveness of pre-training in improving model training and performance. Additionally, pre-\ntrained models learned faster and required less training time to achieve high F1-scores, as observed in Table 10.\nThe analysis of model performance on polyp detection demonstrates a slight improvement when models are pre-\ntrained on in-domain datasets containing relevant and transferable features when compared to no pre-training.\nFurthermore, incorporating greater diversity and complexity in pre-training datasets appears to enhance overall\nmodel performance. Notably, datasets with the highest number of classes, such as the COCO17 and the combined\nFruit and Vegetables dataset, achieved the highest scores overall, showing the value of diversity in pre-training\ndatasets. Although the combined Fruit and Vegetables dataset is significantly smaller than the COCO17 dataset,\nit was able to improve the performance slightly, perhaps due to common similarities with the fine-tuned task. In\nFig. 3, the models were evaluated on the polyp testing dataset with detections visualized using bounding boxes\nto indicate the presence of polyps. The figure provides a comparison between multiple model predictions and\nthe ground truth for polyp localization."}, {"title": "3.3 Challenges and Limitations", "content": "Our methods has several limitations. One key limitation is data availability, particularly the lack of sufficient\ndatasets with YOLO-format annotations. Additionally, the experiment was conducted exclusively with the\nYOLOv8n \"nano\" network, despite the existence of other versions, such as \"s,\" \"m,\" \"l,\" and \"x\",35 as well"}, {"title": "3.4 Future Work", "content": "Further adjustments to the training could have been experimenting with the hyperparameters such as the batch\nsize, data augmentations, learning rate, number of epochs, and optimizer. Trying different data augmentation\ntechniques could improve data quantity and generalization, which could potentially lead to better results because\ncertain augmentations may be more beneficial for certain datasets. Additionally, testing the model's performance\nover multiple epochs is crucial for optimization, such as trying out a different number of epochs to observe the\neffects of training on the different models with different pre-training datasets. Furthermore, exploring alterna-\ntive models beyond YOLOv8, including transformer-based architectures or other convolutional neural networks\n(CNNs), could provide valuable insights to see what. These avenues of exploration hold promise for advancing\nthe reliability, accuracy, and generalizability of DL applications in medical imaging. A very important method\nfor accurate testing includes using the k-fold cross validation, which is widely used for the evaluation of many\nmachine learning tasks. Future research could benefit from employing k-means cross-validation to ensure more\nreliable and statistically robust testing outcomes, minimizing biases that may arise from single-split evaluations.\nExpanding pre-training efforts by incorporating additional datasets with varied complexities and characteristics\ncan also enhance the model's ability to transfer knowledge effectively for tasks such as polyp detection to avoid\nissues such as overfitting. Which may have contributed to the reason the model's performance achieved high\nresults on the polyp datasets used for testing, since the fine-tuning dataset is relatively small and consists of\nmultiple images of the same polyp from different perspectives. Another aspect that could be explored is exploring\ndifferent pre-training methodologies, such as pre-training with the COCO17 dataset first and then continuing\nthe pre-training with a smaller dataset. This method could allow datasets to compensate for the disparity in\nsize when comparing with very large datasets such as the COCO17 dataset. Lastly, due to the lack of publicly\navailable annotated polyp datasets, future efforts to increase the data in this domain will allow for models to be\nevaluated on larger datasets and improve robustness and polyp detection."}, {"title": "4. CONCLUSIONS", "content": "This paper demonstrates the impact of transfer learning on polyp detection by using YOLOv8 and pre-training\ndifferent models on separate datasets that incorporate a diverse amount of classes and objects or task-specific\nfeatures with two classes. The pre-trained YOLO models consistently outperformed the model trained from\nscratch at all epochs, showing how pre-training is effective at improving training and model performance. The\n\"XL\" pre-trained models trained on more data consistently outperformed their counterparts that were trained on\nsmaller datasets. This shows that datasets with greater diversity and more data offer more advantages for transfer\nlearning and overall performance improvement. Although the in-domain datasets did not achieve the highest\nresults, the model trained on the combined Fruit and Vegetable dataset achieved the highest F1-scores and mAP\nvalues compared to the model trained on the COCO17 dataset. This shows that shared similarities can also\nimprove performance by transferring knowledge (weights) across tasks. This is particularly crucial for tasks with\nlimited public data available, such as in polyp detection, where pre-training datasets can compensate for the lack\nof data. Despite the improved performance, there remains a gap in utilizing more datasets to accurately assess\nthe models as well as employing more reliable testing results such as k-folds cross validation in order to validate\nany conclusions made. Perhaps developing a system to quantify the effectiveness of transferring the weights\nfrom pre-training for a specific task could be explored to make transfer learning and training more efficient.\nThis research contributes to the potential of transfer learning in enhancing medical image analysis, paving the\nway for more accurate and reliable diagnostic tools. Other contributions include sharing the pre-trained YOLO-\nmodels, the processed datasets utilized in this paper such as the Ham-10000 dataset with YOLO bounding box\nannotations, the combined MRI brain tumor dataset, and the combined fruit and vegetables dataset containing\nmore than 10,000 images. The code will also be made available to help advance future studies."}]}