{"title": "SCALING LAW WITH LEARNING RATE ANNEALING", "authors": ["Howe Tissue", "Venus Wang", "Lu Wang"], "abstract": "We find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps (s):\n\nL(s) = Lo + A \\cdot S_1^{-\\alpha} - C \\cdot S_2\n\nWhere $S_1$ is forward area and $S_2$ is learning rate annealing area. This formulation takes into account two factors: (1) The forward scaling defined as typical scaling law, and (2) the additional loss drop brought by LR annealing. Therefore, this formulation can describe the full loss curve at each step, rather than the single loss point at the end of training. Applying the scaling law with LR annealing and fitting only one or two training curves, we can accurately predict the loss of language model training at any given step and across any learning rate scheduler (LRS). Furthermore, this equation accurately describes the dynamics during training process, and provides a theoretical verification and explanation for numerous experimental findings of previous studies, particularly those focusing on LR schedule and LR annealing. The resulting insights, also serve as a guide for researchers to select critical LRS in advance by prediction using our equation. Most significantly, since all the points in a full training curve follow the equation, we can achieve accurate loss prediction at any given step across any learning rate scheduler, while expend- ing less than 1% of the computational cost required by the chinchilla scaling law to fit language modeling loss. This approach extremely democratizes scaling law fitting and predicting in developing large language models.", "sections": [{"title": "INTRODUCTION", "content": "In recent years, the spotlight of academic and industrial interest has been increasingly captured by large language models. The intriguing notion of the scaling law posits that the cross-entropy loss of language models adheres to a power-law pattern as scaling up of model size and data size (Kaplan et al., 2020; Hoffmann et al., 2022). This law serves as a powerful tool, affording researchers the ability to precisely forecast the performance of large language models by fitting the loss at a smaller scale. The potential implications of scaling laws are vast and compelling, sparking a surge of exploration in this burgeoning field (Bahri et al., 2021; Michaud et al., 2023; DeepSeek-AI, 2024).\nHowever, typical scaling laws describe the performance of the model at the end of training, rather than every step during the training process. Essentially, the middle points with different degrees of LR annealing fail to follow typical scaling laws, which do not consider local loss drop brought by LR annealing. Consequently, these laws are unable to fit or predict a full loss curve. Till this work, we do not have an appropriate formulation that accurately describes the dynamics during the training process, which is crucial to deeply understand and improve the training process.\nThe learning rate schedule (LRS) is a significant variable that greatly influences the loss during the entire training process. Several previous studies have concluded that the rate of decrease in loss accelerates as LR annealing takes place (Loshchilov & Hutter, 2016; Ibrahim et al., 2024; Hu et al., 2024; DeepSeek-AI, 2024). However, these studies generally only provide a qualitative description of how loss changes during LR annealing. Instead, in the present study, we build upon the chinchilla scaling law (Hoffmann et al., 2022) by introducing an additional LR annealing term. This term"}, {"title": "INTRODUCTION", "content": "allows us to accurately predict how loss decreases during LR annealing. Overall, we discover that the loss value at any step of a fixed-size model is determined by two factors: forward area and the degree of LR annealing. Formally, the expectation of loss L at step \u00b9 s of a language model follows:\n\nL(s) = L_0 + A \\cdot S_1^{-\\alpha} - C \\cdot S_2\n\nS_1 = \\sum_{i=1}^{S} \\eta_i\n\nS_2 = \\sum_{i=1}^{S} \\sum_{k=1}^{i} (\\eta_{k-1} - \\eta_k) \\cdot \\lambda^{i-k}\n\nWhere $S_1$ is forward area 2 and $S_2$ is learning rate annealing area. $ \\eta_i $ is the learning rate at step i. $ \\lambda $ is a hyper-parameter to notate the decay factor in LR annealing momentum (introduced in Sec. 3 in detail), which typically ranges from 0.99 to 0.999. $L_0, A, C, \\alpha$ are undetermined positive constants. A visualization to present the definitions of $S_1$ and $S_2$ is shown as Fig. 1.\nIn Eq. 1, the term $L_0+A\\cdot S_1^{-\\alpha}$ is a rectified scaling law, which states that the expected loss decreases as a power-law function of the number of training steps. The term $-C\\cdot S_2$ is the newly introduced learning rate annealing term. This term represents the additional decrease in loss that occurs as the learning rate is reduced during training. This universal equation can fit the validation loss of any step across any LRS, and then help predict the validation loss of any step across any LRS. As some examples, we fit the equation on the loss curve of constant and cosine LRS in 20K total steps (shown as Fig. 2), then we are able to employ the universal scaling law with LR annealing to predict the full loss curve across various LRS in longer total steps (e.g. 60K) (shown as Fig. 3).\nIn Sec. 3, we demonstrate the derivation of the scaling law with LR annealing and elucidate the po- tential theory underpinning our equation. We also extend our equation to a formulation with model size N. In Sec. 4, we apply our equation to verify and explain the conclusions drawn from a mul- titude of previous studies. These conclusions, which previously required substantial"}, {"title": "PRELIMINARY", "content": ""}, {"title": "SCALING LAWS", "content": "Cross-entropy loss of language models is a good indicator for the performance of almost all down- stream tasks (Caballero et al., 2022; Du et al., 2024). Kaplan et al. (2020) empirically discovers a power-law relationship between validation loss and three key factors: model size, dataset size, and the computational resources utilized for training. As an application of scaling law, Hoffmann et al. (2022) trains a compute-optimal large language model named chinchilla by balancing model size and dataset size. Moreover, chinchilla scaling law adopts a simpler and intuitive equation to describe the relationship between the final validation loss (L) and the number of parameters (N) as well as the amount of data (D) as follows:\n\nL(D, N) = L_0 + A\\cdot D^{-\\alpha} + B\\cdot N^{-\\beta}\n\nwhere $L_0, A, B, \\alpha, \\beta$ are all undetermined positive constants. Note that the typical scaling laws fit the loss at the endpoint of training instead of full loss curve, thus a experiment with new data size requires another training launch. Previous works have conducted some preliminary studies on the impact of the learning rate on the scaling laws. For example, OpenAI and chinchilla scaling laws both find that the choice of learning rate schedule does not influence the power-law format (Kaplan et al., 2020; Hoffmann et al., 2022). Also, OpenAI's experiments suggest that the specific choice of learning rate schedule has minimal impact on the final validation loss, provided that the total summed learning rate is adequately large and the schedule incorporates both a warmup stage and a final annealing stage, reducing the learning rate to nearly zero (Kaplan et al., 2020)."}, {"title": "WSD LEARNING RATE SCHEDULER", "content": "Hu et al. (2024) proposes a warmup-stable-decay (WSD) LRS including three learning rate stages, which could help get a lower validation loss compared to the typical cosine LRS. The format is like\n\nWSD(s) = \\begin{cases}\n    \\frac{S}{T_{\\text{warmup}}} \\eta_{\\text{max}}, & s \\leq T_{\\text{warmup}} \\\\\n    \\eta_{\\text{max}}, & T_{\\text{warmup}} < s \\leq T_{\\text{stable}} \\\\\n    f(s - T_{\\text{stable}}) \\eta_{\\text{max}}, & T_{\\text{stable}} < s < T_{\\text{total}}\n\\end{cases}"}, {"title": "LEARNING RATE ANNEALING", "content": "Learning rate annealing is a common technique used in training neural networks, where the learning rate is reduced over training iterations according to a pre-defined learning rate schedule. This method is often employed to improve the performance and stability of the model during training (Loshchilov & Hutter, 2016). For example, the widely-used cosine LRS could be perceived as a process where the learning rate is cosine curve-like annealing over full steps. Conversely, WSD LRS (Hu et al., 2024) maintains a stable learning rate for the majority of the steps, implementing annealing only in the final (e.g. 10% ~ 20%) of the steps. During the training of LLMs, it has been widely observed that a more pronounced decrease in the learning rate often results in a more precipitous drop in the validation loss (Loshchilov & Hutter, 2016; Ibrahim et al., 2024; DeepSeek-AI, 2024; Hu et al., 2024). However, to the best of our knowledge, all previous studies end with providing a rough and qualitative description of how loss changes during LR annealing, while we provide an equation to formulate the loss changes during LR annealing in this work."}, {"title": "THEORY", "content": "In this section, we elaborate in detail on the intuition, the origin, step-by-step derivation, and the ex- perimental basis of our theory. We then prove the effectiveness of our formula through experiments."}, {"title": "SIMILARITY BETWEEN LEARNING RATE, GRADIENT NORM, AND LOSS", "content": "As shown in Fig. 4, the first key observation is that the shapes of LR curve, gradient norm curve, and validation loss curve are quite similar across various LRS. This gives a clue that there might exist an implicit connection between learning rate and loss, where gradient norm could be the bridge."}, {"title": "THEORY", "content": "decreases. We observe how the validation loss changes with LR increasing or decreasing. The learning rate and loss curves are shown in Fig. 5.\nWe discover the delay phenomenon between learning rate and validation loss. Firstly, the turning point of validation loss curve is always later than the turning point of learning rate curve in both schedulers. That means that the validation loss still maintains its previous trajectory for a few steps even after the learning rate changes direction. Secondly, the steeper the slope of the learning rate annealing (or re-warmup), the more pronounced the delay between the two turning points. Thirdly, given the same LR slope, the left figure (LR decreasing then increasing) always has a longer delay steps than the right figure (LR decreasing then increasing).\nInterestingly, the entire experimental phenomenon is strikingly similar to the physical experiment of a small ball falling along a slope. The larger the angle of the slope, the faster the ball falls. When the ball lands, the accumulated momentum causes the ball to continue sliding for some distance. Based on the delay phenomenon, we pose a hypothesis that f(n), the loss drop brought by LR annealing has cumulative historical formation so that the past change of learning rate will affect the following loss curve for a few steps. In summary, learning rate annealing has momentum.\nThus we define f (n) = C \u00b7 S2 as follows:\n\nm_i = \\lambda \\cdot m_{i-1} + (\\eta_{i-1} - \\eta_i)\n\nS_2 = \\sum_{i=1}^{S} m_i\n\nS_2 = \\sum_{i=1}^{S} \\sum_{k=1}^{i} (\\eta_{k-1} - \\eta_k) \\cdot \\lambda^{i-k}\n\nWhere $m_i$ is the LR annealing momentum at step i, and $ \\Delta \\eta = \\eta_{i-1} - \\eta_i $ denotes the LR annealing amount. A is the decay factor, which signifies the degree to which historical information is retained. We find that A empirically ranges from 0.99 to 0.999. In fact, if LR annealing had no momentum, X = 0 would induce that f(n) = C \u00b7 ns, which is exactly the naive format mentioned above. Note that S2 is not only applicable to LR annealing (S2 > 0), but also to LR re-warmup (S2 < 0). This means that our equation can be also applied in continual pre-training, where LR re-warmup serves as an important factor for better outcomes. More intuitively, the definition of S2 can be visualized in Fig. 1, as the weighted sum of the areas of the small blue squares."}, {"title": "FINAL FORMAT", "content": "Definition. Given the same training and validation dataset, the same model size, the same training hyper-parameters such as max learning rate max and batch size, the language modeling loss at step s in a full loss curve empirically follow as L(s) = Lo + A \\cdot S_1^{-\\alpha} - C \\cdot S_2, where $S_1 = \\sum_{i=1}^{S} \\eta_i $ and S2 are defined as Eq. 6. Lo, A, C, \u03b1 are all undetermined positive constants.\nIt is feasible to apply the formulation to universally describe the loss of each step across diverse learning rate schedulers. This formulation supports validation loss prediction in a more complicated LRS of longer total steps, fitted from a simpler LRS of shorter total steps.\nBalance between S\u2081 and S2. Notice that $ \\frac{\\partial L}{\\partial S_1} < 0 $ and $ \\frac{\\partial L}{\\partial S_2} < 0 $ always hold true, which means both the increase of S1 and S2 can help to reduce the loss. However, as shown intuitively in Fig. 1, there exists a delicate balance between S\u2081 and S2. Once the learning rate begins to anneal and S2 starts to increase, the forward area S\u2081 of incoming steps starts to diminish. Our equation aptly describes this delicate balance. In Sec. 4, we elaborate in detail on this issue."}, {"title": "EXPERIMENTS", "content": "LR Warmup. Different warmup steps can result in different loss curves in training from scratch. During the warmup stage, neural networks are prone to random optimization, resulting in unpre- dictable outcomes in the very beginning stage (Hestness et al., 2017). One evidence is about the high gradient norm in the LR warmup stage. Fig. 4 shows that at the very beginning of training, the"}, {"title": "THEORY", "content": "gradient norm initially converges from a relatively high value to a lower one, exhibiting a pattern dis- tinct from the subsequent steps. From another perspective, LR warmup is also necessary. Our pilot experiment (refer to Appendix A) shows that warmup indeed significantly accelerates convergence, a finding also noted by Liu et al. (2020); Kosson et al. (2024). Consequently, in all experiments conducted for this paper, we employ a 500-step learning rate linear warmup to reach the max LR, where S1 and S2 are computed yet as constant max LR in the warmup stage 3."}, {"title": "THEORY", "content": "In this paper, we adopt standard experimental setups in large language model pre-training. The training dataset is Fineweb (Penedo et al., 2024) and the validation dataset is RedPajama-CC (Computer, 2023). We train a 594M non-embedding parameters LLAMA architecture-like model (Meta, 2024) from scratch. We use AdamW optimizer (Loshchilov & Hut- ter, 2017) with \u03b2\u2081 = 0.9 and B2 = 0.95. The weight decay is set as 0.1 and gradient clip is set as 1.0. We set maximal learning rate as 2 \u00d7 10-4 and batch size as 4M tokens. We use the tokenizer of LLAMA-3 (Meta, 2024). We adopt the decay factor of learning rate annealing \u5165 = 0.999 and we discuss the impact of A in Sec. 6.\nTo validate that our formulation works across different experimental settings, we also conduct our experiments on another experimental setups. Please refer to Appendix B for full experimental setups."}, {"title": "THEORY", "content": "Given a learning rate scheduler, we can easily compute out S1 and S2 of each step in advance. To estimate (Lo, A, C, a), we adopt a similar fitting method as chinchilla scaling law (Hoffmann et al., 2022). Specifically, we minimize the Huber loss (Huber, 1964) between the predicted and the observed log loss using the L-BFGS algorithm (Nocedal, 1980):\n\n\\min_{L_0,A,C,\\alpha} \\sum_{\\text{Step }i} \\text{Huber}_\\delta(\\log L (i) - \\log \\hat{L}(i))\n\nWe implement this by the utilization of minimize in scipy library. Huber loss is to enhance to robustness of the fitting results and we set d of Huber loss as 10-3. We mitigate the potential issue of local minima of fitting by choosing the optimal fit from a range of initial conditions. Note that in practice, we can also fit the full loss curves using multiple LRS with a single tuple of (Lo, A, C, \u03b1). In this situation, we sum the Huber losses in Eq. 7 of all fitted LRS."}, {"title": "THEORY", "content": "We employ the full loss curves from constant and cosine LRS, both containing 20K total steps, as our fitting data points (refer to Fig. 2). We then proceed to predict the full loss curves for various unseen LRS with 60K total steps (refer to Fig. 3). The coefficient of determination (R2) in the fitting process is larger than 0.999, nearly perfect, which underscores the robust capability of our equation to fit loss curves across diverse LRS using a single parameter tuple.\nAs for prediction shown in Fig. 3, our equation demonstrates broad applicability and robust general- ization across five distinct types of LRS, including four previously unseen LRS. This is evidenced by the low mean prediction error, which is approximately 0.2%. Remarkably, our equation is capable to accurately predict losses even in a very complicated LRS with multiple LR re-warmup stages, as illustrated in Fig. 3e, despite the absence of any LR re-warmup stage in the fitting LRS. We also fit and predict on the curves adopting another experimental setups (shown in Appendix C), and the results are similarly good, which proves that Eq. 1 holds across different experimental setups."}, {"title": "THEORY", "content": "First, we research whether the model size N impacts on the amount of loss drop in the annealing stages. We compare the difference of final losses between constant LRS and WSD LRS (10% cosine annealing to min = 0), thus to get the loss gap brought by LR annealing. We conduct this experiment on different total steps and different model sizes. The experimental results are shown in Fig. 6a. It suggests that the loss drop brought by LR annealing is scaling with both annealing steps and model size, which means that the annealing area S2 in our equation gets larger when model size N becomes large. We suppose there is a simple relationship of $S_2 \\propto N^\\gamma $ where y is a positive constant to be fitted."}, {"title": "THEORY", "content": "According to the experiments and analysis above, based on the typical scaling law (Eq. 2), we can extend our proposed equation to model size scaling as follows:\n\nL(s, N) = L_0 + A\\cdot S_1^{-\\alpha} + B \\cdot N^{-\\beta} - C \\cdot S_2 \\cdot N^\\gamma\n\nwhere N denotes the number of non-embedding model parameters (M), and $B, \\beta, \\gamma$ are N-related undetermined positive constants.\nWe utilize the Eq. 8 to fit full loss curves of different model sizes. The fitting result is shown in Fig. 6b, where $R^2 > 0.999$ is very close to 1, suggesting that the loss curves of different model sizes indeed follow our proposed N-extended equation. We also fit the curves adopting another experimental setups (shown in Appendix D) and get similar results, proving that Eq. 8 holds across different experimental setups."}, {"title": "TAKEAWAYS: EXPERIMENTAL FINDINGS VERIFICATION AND EXPLANATION", "content": "Our derived equation describes the training dynamics of language models. In this section, we apply the equation to give theoretical verification and explanation for many existing experimental find- ings. These key insights also serve as a guide for researchers to select critical LRS in advance by prediction via our equation, with little computational cost. An interesting summary is that\n\nThe art of learning rate schedule lies in the delicate balancing act between forward area and annealing area."}, {"title": "IT VERIFIES AND EXPLAINS WHY LOSS DROPS MORE SHARPLY WHEN LR ANNEALS.", "content": "We adopt our equation to help researchers understand why loss drops more sharply when LR anneals, which has been widely observed in many previous studies. We substitute the fitted parameters (see Fig. 2) to our equation as an instance. We draw how the S\u2081-item ($A \\cdot S_1^{-\\alpha}$) and the negative S2-item (-CS2) impacts the loss along with a WSD scheduler. Fig. 7 suggests that starting from annealing stage, negative S2-item has a much more significant impact on the overall loss than S\u2081-item, which makes loss drop more sharply compared with the stable LR stage. In conclusion, LR annealing brings out quick increase of the annealing area, resulting in a drastic decrease in validation loss."}, {"title": "IT VERIFIES AND EXPLAINS WHEN WE USE COSINE LRS, WE SHOULD SET THE COSINE CYCLE LENGTH T AS THE TOTAL STEPS S, AND SET MIN LR AS 0 TO GET THE OPTIMAL LOSS.", "content": "Many papers have found that in LLM pre-training using cosine LRS, setting the cosine cycle length T as the total steps S, and setting min LR as nearly 0 (rather than 10% max LR) can lead to the optimal loss (Hoffmann et al., 2022; Hu et al., 2024; Parmar et al., 2024). Actually, the settings above have been a factual standard in LLM pre-training using cosine LRS. We theoretically verify and explain the finding by our equation, presented as Fig. 8. The prediction curve convincingly demonstrates that the loss curve, with the configuration T = S and a min LR of 0, indeed achieves the optimal loss in the end.\nMoreover, our equation gives a quite intuitive explanation. T > S leads to an incomplete annealing while T < S leads to a small forward area due to early annealing. Thus, it is optimal to set T as the total steps S. Similarly, setting min LR as 0 can make larger annealing amount and thus larger annealing area S2, which enables lower final loss."}, {"title": "IT VERIFIES AND EXPLAINS THE PHENOMENON, WHERE CONSTANT LRS GETS A LOWER LOSS THAN COSINE LRS IF SETTING SMALL TOTAL STEPS, AND VICE VERSA.", "content": "In the experiments, we find that if we set small total steps, the final loss of constant LRS could be even lower than cosine LRS, and vice versa. Refer to the ground-truth loss in Fig. 2 (20K steps), and the ground-truth loss in Fig. 3a, 3b (60K steps). To validate this phenomenon, we use our equation to draw the prediction loss curve of 10K total steps and 100K total steps in Fig. 9. It shows that our proposed equation can verify well that the better LRS changes over the total steps. Moreover,"}, {"title": "THEORY", "content": "Fig. 9c shows the predicted final loss of different total steps using constant and cosine LRS. It further convincingly suggests that constant LRS indeed gets a lower loss if setting small total steps, but the scaling slope is smaller than cosine LRS's, resulting in higher loss in more steps.\nFrom a more essential and comprehensive perspective, $ \\frac{\\partial L}{\\partial S} $ is a power-law decreasing function while $ \\frac{\\partial L}{\\partial S} $ is stable over training steps. In the early stages, $ \\frac{\\partial L}{\\partial S} $ is large when S\u2081 is small, thus increasing S\u2081 by maintaining large LR (e.g. constant LRS) in the early stages can greatly help reduce the loss. That is, S\u2081 plays a dominant role over S2. In the later stages, $ \\frac{\\partial L}{\\partial S} $ is much smaller when S\u2081 becomes large, thus increasing S\u2081 in the later stages does not significantly help reduce the loss. That is, S2 plays a dominant role over S\u2081. At this stage, It is time to start LR annealing to increase S2. Interestingly, this perspective aligns directly with the idea of WSD LRS (Hu et al., 2024): In the early stages, the neural network is exploring globally and it is a suitable time to use a larger LR; In the later stages, the neural network is exploring locally and it is a suitable time to use a smaller LR. We will delve further into WSD LRS in the following subsections."}, {"title": "IT VERIFIES AND EXPLAINS WSD AND MULTI-STEP COSINE LRS HAVE MATCHED OR EVEN LOWER LOSS THAN COSINE LRS.", "content": "Recently, it has been demonstrated that WSD LRS (Hu et al., 2024) and multi-step cosine LRS (DeepSeek-AI, 2024) yield a lower loss compared to the typical cosine LRS. We also prove this by the experiments (refer to the ground-truth loss in Fig. 3b, 3c, 3d). We validate and elucidate this finding using our proposed scaling law with LR annealing. In Fig. 10, we depict the learning rate (on the left) and the predicted loss drop (on the right) for different LRS using our derived equa- tion. The figures suggest that for multi-step cosine or WSD LRS, the negative S2-item (-C. S2) is slightly higher than that of cosine LRS. However, the S\u2081-item ($A \\cdot S_1^{-\\alpha}$) is significantly lower than that in cosine LRS. More simply to say, both WSD LRS and multi-step cosine LRS aim to employ a"}, {"title": "THEORY", "content": "strategy that marginally reduces S2 but substantially increases S1, leading to an overall decrease in validation loss."}, {"title": "IT VERIFIES AND EXPLAINS THAT A MODERATE WSD ANNEALING RATIO COULD GET THE LOWEST LOSS.", "content": "In the case of WSD learning rate schedule, it is crucial to ascertain the optimal annealing ratio for training steps. Prior research by H\u00e4gele et al. (2024) has established the existence of an optimal annealing ratio within the WSD scheduler. As illustrated in experimental results of their study, excessively high or low annealing ratios lead to sub-optimal model performance. This phenomenon can be further elucidated through our proposed equation.\nTheoretically, a high annealing ratio results in a significant reduction of the forward training area S1, while only marginally increasing the annealing area S2. Conversely, an excessively low annealing ratio leads to under-annealing, characterized by a diminutive S2 and consequently get a high final loss. Our scaling law function builds a trade-off relationship between the forward area S\u2081 and annealing area S2 about the annealing ratio.\nAs depicted in Fig. 11, we utilize the fitted scaling law function to draw the final loss across various annealing ratios and total training steps. The prediction results present parabola-like curves, and align well with the actual experimental outcomes by previous works. It suggests that a moderate WSD annealing ratio is optimal which could make a moderate S\u2081, and then maximize S\u2081 and S2 from a global view, thereby minimizing the overall validation loss. Moreover, our equation directly help predict an optimal annealing ratio for different total steps without experiments, which saves a lot of resources."}, {"title": "IT VERIFIES AND EXPLAINS THAT THE OPTIMAL ANNEALING FUNCTION IN WSD LRS DEPENDS ON THE ANNEALING RATIO.", "content": "In the context of the WSD LRS, the selection of the annealing method in the annealing stage is also pivotal to optimize the training process. H\u00e4gele et al. (2024) conclude that the 1-sqrt annealing (refer to Appendix E for 1-sqrt function and curve) yields a lower final loss compared to the other annealing methods (e.g. cosine). They claim that the conclusion holds true across different annealing ratios.\nHowever, as we predict using our equation (Fig. 12a), it indicates that 1-sqrt annealing does get a lower loss than cosine annealing in 10% and 20% annealing ratios, but performs much worse than cosine annealing in 50% annealing ratio."}, {"title": "THEORY", "content": "To verify whether the predictions from our equation are accurate, we conduct experiments by train- ing models using different annealing methods and ratios within a fixed 50K total steps. As illustrated in Fig. 12b, at a 10% or 20% annealing ratio, the 1-sqrt method outperforms the cosine method, whereas at a 50% annealing ratio, the latter method exhibits a lower final loss. The true experimen- tal results align quite well with our prediction, which also overturns some of the conclusions made by previous works. We conclude that the optimal annealing function in WSD LRS depends on the annealing ratio.\nOur scaling law function provides an explanatory framework for these observations. We draw the LR curves of 1-sqrt and cosine annealing in Appendix E. At 10% annealing ratio, although the forward area S\u2081 of the cosine method is slightly larger than that of the 1-sqrt method, the larger annealing area S2 of the 1-sqrt method plays a more critical role in reducing the overall final loss. However, as the annealing ratio increases, the difference of S\u2081 between two LRS gradually becomes larger and larger, till breaking the delicate balance between S\u2081 and S2 at 50% annealing ratio, resulting in a lower final loss for the cosine method. This relationship underscores the importance of carefully selecting the annealing strategy to optimize model training outcomes within the WSD scheduler. Still, our equation can help predict a better annealing method without experiments, which saves a lot of resources."}, {"title": "IT VERIFIES AND EXPLAINS THAT IN CONTINUAL PRE-TRAINING, THE HIGHER MAX LEARNING RATE TO RE-WARMUP, THE HIGHER THE INITIAL PEAK LOSS WILL BE, AND THEN THE MORE SHARPLY IT WILL DECREASE.", "content": "In continual pre-training (CPT), the learning rate scheduler is usually set as re-warmup to a new max LR at the beginning. By many experiments, Gupta et al. (2023) concludes that the higher max learning rate to re-warmup, the higher the initial peak loss will be, and then the more sharply it will decrease.\nAccording to our scaling law function 4, in the re-warmup process, the annealing area S2 will reduce to a negative value (S2 < 0) and thus the validation loss increases. The higher max LR in re-warmup, the annealing area S2 becomes more negative and thus there would be a higher peak loss. But still, higher max LR could make the forward area S\u2081 grow faster and the loss decreases more sharply after re-warmup. We use the fitted equation to predict the continual pre-training process with different max LR as shown in Fig. 13a. The predicted loss curves reproduce a quite similar phenomenon with previous works (Gupta et al., 2023)."}, {"title": "THEORY", "content": "There is a more profound strategy using our equation in CPT. As shown in Fig. 13a, after ensuring total steps during CPT, we can apply our equation to predict a better max LR and scheduler to get the lowest final loss without experiments, which saves a lot of resources."}, {"title": "IT VERIFIES AND EXPLAINS THAT IN CONTINUAL PRE-TRAINING, THE STEPS OF RE-WARMUP HAVE LITTLE IMPACT ON THE FINAL LOSS.", "content": "Meanwhile, how many steps to re-warmup is another important issue in the continual pre-training. Gupta et al. (2023) find that the longer re-warmup steps could smooth the transition of loss curve but the number of re-warmup steps does not significantly influence the final validation loss. We use the fitted equation to predicted the continual pre-training dynamics with different re-warmup steps. The results, shown in Fig. 13b, present a good alignment with previous works (Gupta et al., 2023).\nBased on our theory, given the fixed max LR, when the re-warmup steps are longer, the annealing area decreases more slowly and the loss curve rises more smoothly, but both final S\u2081 and S2 are quite stable across different re-warmup steps. First, the annealing area S2 of different re-warmup steps are very close due to the same max LR and the same min LR. Besides, though different re-warmup steps bring in temporary distinct losses, re-warmup only cover a small percentage compared with all training steps. Thus, the forward area S\u2081 is also close across different re-warmup steps, resulting in the close overall loss across different steps of re-warmup."}, {"title": "COMPARISON WITH CHINCHILLA SCALING LAW", "content": ""}, {"title": "REDUCTION TO CHINCHILLA SCALING LAW", "content": "Our scaling law function can predict the full loss curve across any given learning rate scheduler. In this section, we show that our equation has no contradiction with typical scaling law, and it is a generalized form of the chinchilla scaling law (Hoffmann et al., 2022). That is to say, all the final loss points of different total training steps following our equation should also follow a power-law relationship. We prove this by dividing into two conditions: (1) constant LRS, and (2) other LRS."}, {"title": "THEORY", "content": "In the case of constant learning rate scheduler, the annealing area S2 is always zero and the forward area $S_1 = \\eta_{\\text{max}} \\cdot s $, where s is the step"}]}