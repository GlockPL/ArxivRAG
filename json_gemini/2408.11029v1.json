{"title": "SCALING LAW WITH LEARNING RATE ANNEALING", "authors": ["Howe Tissue", "Venus Wang", "Lu Wang"], "abstract": "We find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps (s):\n\n$L(s) = L_0 + A \\cdot S_1^{-\\alpha} - C \\cdot S_2$\n\nWhere $S_1$ is forward area and $S_2$ is learning rate annealing area. This formulation takes into account two factors: (1) The forward scaling defined as typical scaling law, and (2) the additional loss drop brought by LR annealing. Therefore, this formulation can describe the full loss curve at each step, rather than the single loss point at the end of training. Applying the scaling law with LR annealing and fit ting only one or two training curves, we can accurately predict the loss of language model training at any given step and across any learning rate scheduler (LRS). Fur thermore, this equation accurately describes the dynamics during training process, and provides a theoretical verification and explanation for numerous experimental findings of previous studies, particularly those focusing on LR schedule and LR annealing. The resulting insights, also serve as a guide for researchers to select critical LRS in advance by prediction using our equation. Most significantly, since all the points in a full training curve follow the equation, we can achieve accurate loss prediction at any given step across any learning rate scheduler, while expend ing less than 1% of the computational cost required by the chinchilla scaling law to fit language modeling loss. This approach extremely democratizes scaling law fitting and predicting in developing large language models.", "sections": [{"title": "INTRODUCTION", "content": "In recent years, the spotlight of academic and industrial interest has been increasingly captured by large language models. The intriguing notion of the scaling law posits that the cross-entropy loss of language models adheres to a power-law pattern as scaling up of model size and data size (Kaplan et al., 2020; Hoffmann et al., 2022). This law serves as a powerful tool, affording researchers the ability to precisely forecast the performance of large language models by fitting the loss at a smaller scale. The potential implications of scaling laws are vast and compelling, sparking a surge of exploration in this burgeoning field (Bahri et al., 2021; Michaud et al., 2023; DeepSeek-AI, 2024).\n\nHowever, typical scaling laws describe the performance of the model at the end of training, rather than every step during the training process. Essentially, the middle points with different degrees of LR annealing fail to follow typical scaling laws, which do not consider local loss drop brought by LR annealing. Consequently, these laws are unable to fit or predict a full loss curve. Till this work, we do not have an appropriate formulation that accurately describes the dynamics during the training process, which is crucial to deeply understand and improve the training process.\n\nThe learning rate schedule (LRS) is a significant variable that greatly influences the loss during the entire training process. Several previous studies have concluded that the rate of decrease in loss accelerates as LR annealing takes place (Loshchilov & Hutter, 2016; Ibrahim et al., 2024; Hu et al., 2024; DeepSeek-AI, 2024). However, these studies generally only provide a qualitative description of how loss changes during LR annealing. Instead, in the present study, we build upon the chinchilla scaling law (Hoffmann et al., 2022) by introducing an additional LR annealing term. This term"}, {"title": "PRELIMINARY", "content": ""}, {"title": "SCALING LAWS", "content": "Cross-entropy loss of language models is a good indicator for the performance of almost all down-stream tasks (Caballero et al., 2022; Du et al., 2024). Kaplan et al. (2020) empirically discovers a power-law relationship between validation loss and three key factors: model size, dataset size, and the computational resources utilized for training. As an application of scaling law, Hoffmann et al. (2022) trains a compute-optimal large language model named chinchilla by balancing model size and dataset size. Moreover, chinchilla scaling law adopts a simpler and intuitive equation to describe the relationship between the final validation loss (L) and the number of parameters (N) as well as the amount of data (D) as follows:\n\n$L(D, N) = L_0 + A\\cdot D^{-\\alpha} + B \\cdot N^{-\\beta}$"}, {"title": "WSD LEARNING RATE SCHEDULER", "content": "Hu et al. (2024) proposes a warmup-stable-decay (WSD) LRS including three learning rate stages, which could help get a lower validation loss compared to the typical cosine LRS. The format is like\n\n$WSD(s) =\\begin{cases}\n\\frac{s}{T_{warmup}} \\cdot \\eta_{max}, & s \\leq T_{warmup} \\\\\n\\eta_{max}, & T_{warmup} < s \\leq T_{stable} \\\\\nf(s - T_{stable})\\eta_{max}, & T_{stable} < s < T_{total}\n\\end{cases}$"}, {"title": "LEARNING RATE ANNEALING", "content": "Learning rate annealing is a common technique used in training neural networks, where the learning rate is reduced over training iterations according to a pre-defined learning rate schedule. This method is often employed to improve the performance and stability of the model during training (Loshchilov & Hutter, 2016). For example, the widely-used cosine LRS could be perceived as a process where the learning rate is cosine curve-like annealing over full steps. Conversely, WSD LRS (Hu et al., 2024) maintains a stable learning rate for the majority of the steps, implementing annealing only in the final (e.g. 10% ~ 20%) of the steps. During the training of LLMs, it has been widely observed that a more pronounced decrease in the learning rate often results in a more precipitous drop in the validation loss (Loshchilov & Hutter, 2016; Ibrahim et al., 2024; DeepSeek-AI, 2024; Hu et al., 2024). However, to the best of our knowledge, all previous studies end with providing a rough and qualitative description of how loss changes during LR annealing, while we provide an equation to formulate the loss changes during LR annealing in this work."}, {"title": "THEORY", "content": "In this section, we elaborate in detail on the intuition, the origin, step-by-step derivation, and the ex-perimental basis of our theory. We then prove the effectiveness of our formula through experiments."}, {"title": "SIMILARITY BETWEEN LEARNING RATE, GRADIENT NORM, AND LOSS", "content": "As shown in Fig. 4, the first key observation is that the shapes of LR curve, gradient norm curve, and validation loss curve are quite similar across various LRS. This gives a clue that there might exist an implicit connection between learning rate and loss, where gradient norm could be the bridge."}, {"title": "Scaling Laws for Constant LRS", "content": "Constant LRS could be seen as a special LRS, in which any step could be as the endpoint of the LRS. Memorize that chinchilla scaling law (Hoffmann et al., 2022) exactly describes the endpoint of full loss curves. That is to say, the expectation of validation loss of all steps in constant LRS adhere to chinchilla scaling law."}, {"title": "Extra Loss Changes in LR Annealing", "content": "Differently, LR annealing (or re-warmup) brings drastic loss changes in a local range (e.g. Fig. 4), which makes the full loss curve no more follow typical scaling laws. Consider that the LRS without annealing, or constant LRS, follows typical scaling laws. Based on that, we can easily induce that, in a full loss curve, the loss L at step s can be rectified by an added \u03b7-related item, and may follow the format as\n\n$L(s) = L_0 + As - f(\\eta)$"}, {"title": "Training Discount in Annealing", "content": "The form of Eq. 4 is still imperfect. Note that the gradient $g$ of parameters decreases along with LR (shown in Fig. 4). Compared with the stages before annealing, the amount of parameter changes (nearly $\\eta g$ of each step) in the neural network optimization process will bring out an almost quadratic faster decline during LR annealing. These clues show that during LR annealing, the loss drop brought by power law (the blue part in Eq. 4) should also decrease. Therefore, we modify to the following format:\n\n$L(s) = L_0 + A\\cdot S_1 - f(\\eta)$\n\n$S_1 = \\sum_i^s \\eta_i$"}, {"title": "LR ANNEALING MOMENTUM", "content": "Another key observation is that LR annealing has momentum. For more information about the property of f(\u03b7), We design one special LRS where the learning rate linearly decreases from $\\eta_{max}$ to $\\eta_{min}$ then increases. We maintain the slope of the increasing stage (always 5K steps) and change the slope in the decreasing stage. The decreasing steps include 0.1K, 0.5K, 1K, and 2K. Symmetrically, we design another special LRS where the learning rate linearly increases from $\\eta_{min}$ to $\\eta_{max}$ then"}, {"title": "FINAL FORMAT", "content": "Definition. Given the same training and validation dataset, the same model size, the same training hyper-parameters such as max learning rate $\\eta_{max}$ and batch size, the language modeling loss at step s in a full loss curve empirically follow as $L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C \\cdot S_2$, where $S_1 = \\sum_{i=1}^s \\eta_i$ and $S_2$ are defined as Eq. 6. $L_0, A, C, \\alpha$ are all undetermined positive constants.\n\nIt is feasible to apply the formulation to universally describe the loss of each step across diverse learning rate schedulers. This formulation supports validation loss prediction in a more complicated LRS of longer total steps, fitted from a simpler LRS of shorter total steps."}, {"title": "EXPERIMENTS", "content": "LR Warmup. Different warmup steps can result in different loss curves in training from scratch. During the warmup stage, neural networks are prone to random optimization, resulting in unpre-dictable outcomes in the very beginning stage (Hestness et al., 2017). One evidence is about the high gradient norm in the LR warmup stage. Fig. 4 shows that at the very beginning of training, the"}, {"title": "EXTENSION TO MODEL SIZE SCALING", "content": "Loss Drop in Annealing Stage Scales with Model Size $N$. First, we research whether the model size $N$ impacts on the amount of loss drop in the annealing stages. We compare the difference of final losses between constant LRS and WSD LRS (10% cosine annealing to $\\eta_{min} = 0$), thus to get the loss gap brought by LR annealing. We conduct this experiment on different total steps and different model sizes. The experimental results are shown in Fig. 6a. It suggests that the loss drop brought by LR annealing is scaling with both annealing steps and model size, which means that the annealing area $S_2$ in our equation gets larger when model size $N$ becomes large. We suppose there is a simple relationship of $S_2 \\times N^{\\gamma}$ where $\\gamma$ is a positive constant to be fitted."}, {"title": "TAKEAWAYS: EXPERIMENTAL FINDINGS VERIFICATION AND EXPLANATION", "content": "Our derived equation describes the training dynamics of language models. In this section, we apply the equation to give theoretical verification and explanation for many existing experimental find-ings. These key insights also serve as a guide for researchers to select critical LRS in advance by prediction via our equation, with little computational cost. An interesting summary is that\n\nThe art of learning rate schedule lies in the delicate balancing act between forward area and annealing area."}, {"title": "IT VERIFIES AND EXPLAINS WHY LOSS DROPS MORE SHARPLY WHEN LR ANNEALS.", "content": "We adopt our equation to help researchers understand why loss drops more sharply when LR anneals, which has been widely observed in many previous studies. We substitute the fitted parameters (see Fig. 2) to our equation as an instance. We draw how the $S_1$-item ($A \\cdot S_1^{-\\alpha}$) and the negative $S_2$-item ($-CS_2$) impacts the loss along with a WSD scheduler. Fig. 7 suggests that starting from annealing stage, negative $S_2$-item has a much more significant impact on the overall loss than $S_1$-item, which makes loss drop more sharply compared with the stable LR stage. In conclusion, LR annealing brings out quick increase of the annealing area, resulting in a drastic decrease in validation loss."}, {"title": "IT VERIFIES AND EXPLAINS WHEN WE USE COSINE LRS, WE SHOULD SET THE COSINE CYCLE LENGTH T AS THE TOTAL STEPS S, AND SET MIN LR AS 0 TO GET THE OPTIMAL LOSS.", "content": "Many papers have found that in LLM pre-training using cosine LRS, setting the cosine cycle length $T$ as the total steps $S$, and setting min LR as nearly 0 (rather than 10% max LR) can lead to the optimal loss (Hoffmann et al., 2022; Hu et al., 2024; Parmar et al., 2024). Actually, the settings above have been a factual standard in LLM pre-training using cosine LRS. We theoretically verify and explain the finding by our equation, presented as Fig. 8. The prediction curve convincingly demonstrates that the loss curve, with the configuration $T = S$ and a min LR of 0, indeed achieves the optimal loss in the end.\n\nMoreover, our equation gives a quite intuitive explanation. $T > S$ leads to an incomplete annealing while $T < S$ leads to a small forward area due to early annealing. Thus, it is optimal to set $T$ as the total steps $S$. Similarly, setting min LR as 0 can make larger annealing amount and thus larger annealing area $S_2$, which enables lower final loss."}, {"title": "IT VERIFIES AND EXPLAINS THE PHENOMENON, WHERE CONSTANT LRS GETS A LOWER LOSS THAN COSINE LRS IF SETTING SMALL TOTAL STEPS, AND VICE VERSA.", "content": "In the experiments, we find that if we set small total steps, the final loss of constant LRS could be even lower than cosine LRS, and vice versa. Refer to the ground-truth loss in Fig. 2 (20K steps), and the ground-truth loss in Fig. 3a, 3b (60K steps). To validate this phenomenon, we use our equation to draw the prediction loss curve of 10K total steps and 100K total steps in Fig. 9. It shows that our proposed equation can verify well that the better LRS changes over the total steps. Moreover,"}, {"title": "IT VERIFIES AND EXPLAINS WSD AND MULTI-STEP COSINE LRS HAVE MATCHED OR EVEN LOWER LOSS THAN COSINE LRS.", "content": "Recently, it has been demonstrated that WSD LRS (Hu et al., 2024) and multi-step cosine LRS (DeepSeek-AI, 2024) yield a lower loss compared to the typical cosine LRS. We also prove this by the experiments (refer to the ground-truth loss in Fig. 3b, 3c, 3d). We validate and elucidate this finding using our proposed scaling law with LR annealing. In Fig. 10, we depict the learning rate (on the left) and the predicted loss drop (on the right) for different LRS using our derived equa-tion. The figures suggest that for multi-step cosine or WSD LRS, the negative $S_2$-item ($-C \\cdot S_2$) is slightly higher than that of cosine LRS. However, the $S_1$-item ($A \\cdot S_1^{-\\alpha}$) is significantly lower than that in cosine LRS. More simply to say, both WSD LRS and multi-step cosine LRS aim to employ a"}, {"title": "IT VERIFIES AND EXPLAINS THAT A MODERATE WSD ANNEALING RATIO COULD GET THE LOWEST LOSS.", "content": "In the case of WSD learning rate schedule, it is crucial to ascertain the optimal annealing ratio for training steps. Prior research by H\u00e4gele et al. (2024) has established the existence of an optimal annealing ratio within the WSD scheduler. As illustrated in experimental results of their study, excessively high or low annealing ratios lead to sub-optimal model performance. This phenomenon can be further elucidated through our proposed equation.\n\nTheoretically, a high annealing ratio results in a significant reduction of the forward training area $S_1$, while only marginally increasing the annealing area $S_2$. Conversely, an excessively low annealing ratio leads to under-annealing, characterized by a diminutive $S_2$ and consequently get a high final loss. Our scaling law function builds a trade-off relationship between the forward area $S_1$ and annealing area $S_2$ about the annealing ratio.\n\nAs depicted in Fig. 11, we utilize the fitted scaling law function to draw the final loss across various annealing ratios and total training steps. The prediction results present parabola-like curves, and align well with the actual experimental outcomes by previous works. It suggests that a moderate WSD annealing ratio is optimal which could make a moderate $S_1$, and then maximize $S_1$ and $S_2$ from a global view, thereby minimizing the overall validation loss. Moreover, our equation directly help predict an optimal annealing ratio for different total steps without experiments, which saves a lot of resources."}, {"title": "IT VERIFIES AND EXPLAINS THAT THE OPTIMAL ANNEALING FUNCTION IN WSD LRS DEPENDS ON THE ANNEALING RATIO.", "content": "In the context of the WSD LRS, the selection of the annealing method in the annealing stage is also pivotal to optimize the training process. H\u00e4gele et al. (2024) conclude that the 1-sqrt annealing (refer to Appendix E for 1-sqrt function and curve) yields a lower final loss compared to the other annealing methods (e.g. cosine). They claim that the conclusion holds true across different annealing ratios.\n\nHowever, as we predict using our equation (Fig. 12a), it indicates that 1-sqrt annealing does get a lower loss than cosine annealing in 10% and 20% annealing ratios, but performs much worse than cosine annealing in 50% annealing ratio."}, {"title": "IT VERIFIES AND EXPLAINS THAT IN CONTINUAL PRE-TRAINING, THE HIGHER MAX LEARNING RATE TO RE-WARMUP, THE HIGHER THE INITIAL PEAK LOSS WILL BE, AND THEN THE MORE SHARPLY IT WILL DECREASE.", "content": "In continual pre-training (CPT), the learning rate scheduler is usually set as re-warmup to a new max LR at the beginning. By many experiments, Gupta et al. (2023) concludes that the higher max learning rate to re-warmup, the higher the initial peak loss will be, and then the more sharply it will decrease.\n\nAccording to our scaling law function 4, in the re-warmup process, the annealing area $S_2$ will reduce to a negative value ($S_2 < 0$) and thus the validation loss increases. The higher max LR in re-warmup, the annealing area $S_2$ becomes more negative and thus there would be a higher peak loss. But still, higher max LR could make the forward area $S_1$ grow faster and the loss decreases more sharply after re-warmup. We use the fitted equation to predict the continual pre-training process with different max LR as shown in Fig. 13a. The predicted loss curves reproduce a quite similar phenomenon with previous works (Gupta et al., 2023)."}, {"title": "IT VERIFIES AND EXPLAINS THAT IN CONTINUAL PRE-TRAINING, THE STEPS OF RE-WARMUP HAVE LITTLE IMPACT ON THE FINAL LOSS.", "content": "Meanwhile, how many steps to re-warmup is another important issue in the continual pre-training. Gupta et al. (2023) find that the longer re-warmup steps could smooth the transition of loss curve but the number of re-warmup steps does not significantly influence the final validation loss. We use the fitted equation to predicted the continual pre-training dynamics with different re-warmup steps. The results, shown in Fig. 13b, present a good alignment with previous works (Gupta et al., 2023).\n\nBased on our theory, given the fixed max LR, when the re-warmup steps are longer, the annealing area decreases more slowly and the loss curve rises more smoothly, but both final $S_1$ and $S_2$ are quite stable across different re-warmup steps. First, the annealing area $S_2$ of different re-warmup steps are very close due to the same max LR and the same min LR. Besides, though different re-warmup steps bring in temporary distinct losses, re-warmup only cover a small percentage compared with all training steps. Thus, the forward area $S_1$ is also close across different re-warmup steps, resulting in the close overall loss across different steps of re-warmup."}, {"title": "COMPARISON WITH CHINCHILLA SCALING LAW", "content": ""}, {"title": "REDUCTION TO CHINCHILLA SCALING LAW", "content": "Our scaling law function can predict the full loss curve across any given learning rate scheduler. In this section, we show that our equation has no contradiction with typical scaling law, and it is a generalized form of the chinchilla scaling law (Hoffmann et al., 2022). That is to say, all the final loss points of different total training steps following our equation should also follow a power-law relationship. We prove this by dividing into two conditions: (1) constant LRS, and (2) other LRS."}, {"title": "Constant LRS", "content": "In the case of constant learning rate scheduler, the annealing area $S_2$ is always zero and the forward area $S_1 = \\eta_{max} \\cdot s$, where s is the step, and $\\eta_{max}$ is the maximal learning rate which is constant over steps. Thus, the whole train loss curves becomes:\n\n$L(s) = L_0 + (A \\eta_{max}) \\cdot s^{-\\alpha} = L_0 + A' \\cdot s^{-\\alpha}$"}, {"title": "SCALING LAW FITTING DEMOCRATIZATION", "content": "We extremely democratize the fitting and prediction of scaling law by greatly enlarging its scope of applicability. Adopting our scaling law with LR annealing, we can collect thousands of fitting points in one full loss curve during training, while typical scaling law can only collect one endpoint from a full loss curve."}, {"title": "DISCUSSION", "content": ""}, {"title": "THE IMPACT OF DECAY FACTOR A", "content": "The decay factor $\\lambda$ in our equation plays a crucial role to indicate the information retaining degree in LR annealing. We set $\\lambda$ as 0.999 in our all experiments. We explore the difference from another decay factor $\\lambda$ = 0.99. We fit and get different equations for different $\\lambda$. We compare them (1) on the predicted loss curves for 1-square and 1-sqrt annealing methods, and (2) on the predicted loss curves in different annealing ratios of WSD LRS (cosine annealing)."}, {"title": "POSSIBLE ROOT REASONS OF DELAY PHENOMENON IN LEARNING RATE ANNEALING", "content": "In Sec. 3, we discover the delay phenomenon, which proves that LR annealing has momentum. We discuss possible root reasons of the phenomenon in this section."}, {"title": "Adam Optimizer? No.", "content": "We notice that Adam optimizer (Kingma & Ba, 2015) also has the first-order momentum decay factor $\\beta_1$ and the second-order momentum decay factor $\\beta_2$, which presents the possible connection to the the delay phenomenon."}, {"title": "OTHER POSSIBLE SCALING LAW FORMATS WITH LR ANNEALING", "content": "Adding a LR-weighted Coefficient to $S_2$? Imagine that when LR anneals to nearly 0, the neural network's parameters almost do not change and the validation loss should not change, either. However, as defined in our equation, Eq. 1, $S_2$ still has historical momentum even if LR is nearly 0, making the loss continue to decrease and misalign with observed training dynamics.\n\nTo cover this corner case, we try a revision to our equation and add a LR-weighted coefficient to $S_2$. Specifically, we adjust $S_2$ to more approach 0 when $\\eta$ is close to 0, counteracting the original formulation's tendency to overestimate loss reduction when $\\eta \\approx 0$.\n\nThe revised equation for the annealing area $S_2$ in our scaling law function is as follows:\n\n$m_i = \\lambda \\cdot m_{i-1} + (\\eta_{k-1} - \\eta_k)$\n\n$S_2 = \\sum_{i=1}^s m_i = \\sum_{i=1}^s (\\sum_{k=1}^i (\\eta_{k-1} - \\eta_k) \\cdot \\lambda^{i-k})$"}, {"title": "FUTURE WORKS", "content": ""}, {"title": "TUNING SCALING LAW FORMAT", "content": "We have tested a variety of equation formats to enhance the accuracy of the entire training process. As a result, the final equation format, as presented in Eq. 1, proves to be optimal so far across a range of scenarios. And the equation has achieved a level of practicality that enables the prediction of future loss when scaling training steps and model sizes. We will persist in refining the equation format to forecast the loss with the highest possible accuracy."}, {"title": "EXTENSION TO POST-TRAINING", "content": "In this work, we research primarily on the scope of pre-training of LLM. We also show how to apply our equation to guide the LR re-warmup strategy in continual pre-training. We will continue researching on how to extend our equation to post-training, which might include data distribution shift, data mixture, model alignment, and specific downstream evaluations."}, {"title": "CONCLUSION", "content": "In conclusion, we discover that the loss curves of neural language models empirically conform to a scaling law with learning rate annealing over training steps (s), which is expressed as $L(s) = L_0 + AS^{-\\alpha} - CS_2$. This equation points out the art of balance between forward area and anneal-ing area of learning rate schedulers, offering theoretical verification and explanation for numerous experimental findings of prior studies. We present the underlying theory of our equation including curve similarity and LR annealing momentum. We extend our equation to the format with model size N. Moreover, we prove that our equation is a generalized form of typical scaling laws, further showing the superiority of our equation.\n\nCompared to typical scaling laws which predict the endpoint of one full loss curve, our equation can accurately forecast the loss of language model training at any given step and across any learning rate scheduler. As for required computation resources, our equation expends less than 1% of the computational cost required by typical scaling laws to fit language modeling loss. Our proposed approach greatly democratizes the fitting and prediction of scaling laws in the development of large language models."}, {"title": "IMPACT OF WARMUP STEPS", "content": "We conduct experiments on the imapct of learning rate warmup steps. As shown in Fig. 19, we find that 500 warmup steps can get the lowest validation loss compared to 100 or no warmup steps. The experimental results also guide us to choose 500 warmup steps in our experiments of this work."}, {"title": "EXPERIMENTAL SETUPS", "content": "In this work, we use multiple sets of experimental setups, in order to validate that our equation can work across different experimental setups. For clarification, we present the experimental setup list as shown in Table 3.\n\nIn our most experiments (including our takeaway sections), we use the main setting A. In our fitting and prediction experiment, we repeat experiments and consolidate our conclusion using another setting, setting B, and the experimental results are shown in Fig. 21. In our N-extended scaling law fitting experiment, we repeat experiments and consolidate our conclusion using another setting, setting C, and the experimental results are shown in Fig. 22."}, {"title": "1-SQRT AND 1-SQUARE ANNEALING", "content": "The 1-sqrt annealing is proposed by H\u00e4gele et al. (2024), whose learning rate at step s in annealing stage is defined as:\n\n$f(s) = 1 - \\sqrt{\\frac{s - T_{stable}}{T_{total} - T_{stable}}}$\n\n$\\eta_s = \\eta_{min} + f(s) \\cdot (\\eta_{max} - \\eta_{min})$\n\nWe draw the learning rate curve of WSD (20% and 50% 1-sqrt annealing) in Fig. 23, compared with cosine annealing. Other than 1-sqrt annealing, H\u00e4gele et al. (2024) also mentions 1-square annealing method defined as:\n\n$f(s) = 1 - (\\frac{s - T_{stable}}{T_{total} - T_{stable}})^2$\n\n$\\eta_s = \\eta_{min} + f(s) \\cdot (\\eta_{max} - \\eta_{min})$"}]}