{"title": "Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation", "authors": ["Lorenzo Cima", "Alessio Miaschi", "Amaury Trujillo", "Marco Avvenuti", "Felice Dell'Orletta", "Stefano Cresci"], "abstract": "AI-generated counterspeech offers a promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct an LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation.\nWarning: This paper contains examples that may be perceived as offensive or upsetting. Reader discretion is advised.", "sections": [{"title": "1 INTRODUCTION", "content": "Online toxicity refers to hateful, offensive, or otherwise harmful speech on the Web that can cause distress in readers or lead someone to abandon a conversation [72]. Toxicity has dire social and economic costs. Online, it reduces user participation, hinders information exchange, and deepens divides [1]. Offline, it may lead to physical violence, reduce norm adherence, and cause severe psychological distress [25, 51, 57]. Hence, it is a growing concern for both regulators and platform administrators [22].\nPlatforms apply a wide array of moderation actions to curb toxicity and other online harms [21, 70]. These include content and user removals, friction interventions, and demotion [9, 68]. An alternative to centralized moderation is social correction-such as counterspeech-where users proactively respond to toxic content encouraging respectful and constructive communication [5, 26]. Counterspeech holds promise as it avoids censorship concerns while preventing toxic users from merely relocating to other communities [41]. However, a number of challenges currently limit its efficacy and applicability. The emotional toll on responders, who spend countless hours confronting hate, incivility, and personal attacks, is a major issue [56, 62]. Safety is another concern, as responders may face retaliation [19, 64]. Moreover, the widespread nature of online toxicity makes manual counterspeech highly impractical. To overcome these limitations, automated counterspeech systems"}, {"title": "2 RELATED WORK", "content": "2.1 LLM-generated counterspeech\nCounterspeech can be more effective than other moderation actions, such as content and user removals, without limiting free speech [11]. Favorable results were obtained by scholars [4, 37], NGOs [12, 13], and ordinary users [31, 38] alike, as part of observational [26], quasi-experimental [4], and experimental [37, 50] studies. These positive results also extend to Al-generated counterspeech. However, we currently lack a thorough understanding of the types of counterspeech that are most effective and the optimal conditions for their generation [11]. For these reasons, research is focusing on identifying the conditions, methods, and resources that make Al-generated counterspeech effective. Tekiroglu et al. [65] compared pre-trained LLMs and decoding strategies to highlight those capable of generating effective counterspeech. Others leveraged socio-psychological theories to generate messages based on different strategies, such as empathy, abstract norms, disapproval, and humor [4, 8, 37, 63]. However, these strategies were applied independently of the moderated context and user. As shown in Table 1, the only exceptions are Do\u011fan\u00e7 and Markov [20] who used the age and gender of hateful users to generate tailored counterspeech, and B\u00e4r et al. [8] who prompted an LLM to generate contextualized counterspeech based on the content of the toxic message. Herein, we go beyond existing work by generating adapted and personalized counterspeech that leverage content about the user, the conversation, and the community, assessing which type of contextual information results in effective counterspeech.\n2.2 Persuasiveness of LLM-generated messages\nLLMs persuasiveness has been explored from various perspectives. While some studies focused on persuasion directed at other LLMs [6, 59], most research examined their influence on humans. For instance, [24, 61] explored attempts to secure donations through different communication strategies, while [55] used LLMs as sales agents. Propaganda was the focus of [32], which demonstrated that AI-generated and human-generated propaganda exhibit comparable levels of persuasiveness, suggesting they can be used interchangeably. Similar results were obtained about political persuasiveness [35]. Costello et al. [17] instructed an LLM to debunk conspiracy theories, achieving notable long-term effects, with persuasion lasting up to two months post-intervention. Finally, several studies examined the use of LLMs as a moderation tool for mitigating conflicts and for reducing toxic messages on social media [10, 33, 40]. In spite of these results however, the persuasiveness of AI-generated contextualized counterspeech is still unexplored.\n2.3 LLM adaptation and personalization\nThe majority of existing content moderation interventions is one-size-fits-all, where the intervention is applied uniformly to all moderated users. However, multiple studies have highlighted the limitations of this generic strategy [14, 17, 69], suggesting that contextualized moderation could offer substantial improvements [18]. However, despite the potential benefits, contextualized moderation has received little attention. Among the few existing works that used LLMs to generate tailored moderation interventions is [17], which experimented with personalized messages to debunk conspiracy beliefs. Similarly, B\u00e4r et al. [8] adapted counterspeech to align with the specific content of the moderated messages. Besides content moderation, LLM personalization was studied for socio-demographic alignment [3, 20, 29, 59], for alignment with specific"}, {"title": "3 PROBLEM DEFINITION", "content": "Let $T = (m_0, m_1,..., m_n)$ be an online conversation thread where $m_0, m_1,..., m_n$ denotes the chronologically ordered sequence of thread messages. Given a toxic message $m_i$, we aim to generate the counterspeech message $m_{i+1} = G(m_i, C_i)$, where G is the counterspeech generator and $C_i$ is the contextual information. Contrarily to existing works where G only receives $m_i$ as input [8, 10, 39, 40, 46], we generate adapted and personalized counterspeech by also providing $C_i$ as input to G.\nRelevant properties. We identify the following set of desired properties that effective counterspeech should possess:\n\u2022 Politeness increases the likelihood of persuading toxic users and bystanders by fostering a respectful dialogue [73]. Additionally, it aligns with the ethical principle of promoting constructive and non-hostile interventions.\n\u2022 Adequacy. To be considered adequate counterspeech, a response should directly address the toxic content, showing that the violation has not only been noticed but also appropriately managed, thereby discouraging future misbehaviors [5].\n\u2022 Relevance. Messages that are contextually relevant are more likely to resonate and affect the recipients. Conversely, generic statements may lack the contextual specificity needed to effectively address and mitigate toxic behavior [18, 28].\n\u2022 Diversity. Varied messages can engage users in different ways, catering to a broader range of contexts [47]. Diversity also prevents responses from becoming predictable and thus less impactful, while boosting their perceived authenticity and genuineness.\n\u2022 Truthfulness ensures that the counterspeech upholds the integrity of the dialogue. Thus, counterspeech should be factual, accurate, and not misleading, as this builds credibility and trust [5, 60].\n\u2022 Persuasiveness reflects the likelihood of changing the behavior or attitudes of the moderated user. Persuasive counterspeech can also influence bystanders, encouraging a community-wide shift towards positive interactions [40].\nIn addition to the above, we also consider the following relevant properties of contextualized AI-generated counterspeech:\n\u2022 Adaptation ensures that responses are contextually relevant, making the counterspeech more relatable. Furthermore, adapted responses are more likely to be perceived as genuine and thoughtful, which may enhance their credibility and acceptance [28].\n\u2022 Personalization. While adaptation focuses on the broad moderation context, personalization is user-specific and focuses on user characteristics and behaviors [27]. This individualized approach can help build rapport and trust, reducing defensiveness and increasing the likelihood of persuasion [18].\n\u2022 Artificiality refers to the perception of being automatically generated, rather than being human-crafted. Counterspeech messages perceived as artificial may be less likely to be taken seriously [28]. Moreover, minimizing artificiality enhances user experience by making interactions feel natural and genuine. Lastly, reducing the perception of artificiality might help avoid possible negative"}, {"title": "4 METHODS", "content": "4.1 Generation\nWe use an instruction-tuned version of LLaMA2-13B to generate counterspeech responses [67]. The instruction prompt is reported in Appendix A. Starting from this generator, we evaluate different configurations depending on the information provided to the model and the data used for fine-tuning. Below, we describe the factors considered in our experiments, each identified by a unique [label]. Some factors do not involve adaptation nor personalization:\n\u2022 Base [Ba]: The base LLaMA2-13B model without modifications. When alone, this factor also represents our baseline configuration.\n\u2022 Counterspeech fine-tuning: We specialize the base model for counterspeech generation via fine-tuning on two reference datasets: [Mu] MULTICONAN [23] contains 500 hate speech-counterspeech pairs across various hate targets (e.g., race, religion, nationality, sexual orientation, disability, and gender); [ Hs] the Reddit hate-speech intervention (RHSI) dataset [54] includes 5,020 Reddit conversations with human-written interventions. For our study, we select only those comments paired with a human-generated response and with a maximum length of 250 words, totaling 2,974 instances.\nThe previous factors allow us to reproduce state-of-the-art results in automated counterspeech generation. Instead, the following factors provide unexplored contextual information to the generator.\n4.1.1 Adaptation.\n\u2022 Community [Re]: Since our experiments take place on political communities (i.e., subreddits), we align the generator to Reddit's political conversational style and informal language by fine-tuning it on a sample of comment-reply pairs from five prominent political subreddits, as specified in Section 5.\n\u2022 Conversation [Pr]: Since each toxic message $m_i$ to moderate is part of a conversation thread, we add context about the conversation by providing to the generator up to two parent messages $m_{i-1}, m_{i-2}$ from the same thread.\n4.1.2 Personalization.\n\u2022 Comment history [Hi]: We provide user-specific information to the generator by prepending each toxic message $m_i$ with ten previous messages that its author posted on Reddit.\n\u2022 Summary [Su]: Given a toxic message $m_i$, we feed twenty previous messages of its author to an instruction-tuned LLaMA2-13B model tasked with producing user summaries that highlight writing style, lexicon, and main interests. The instruction prompt used to generate user summaries is reported in Appendix A. User summaries are then provided to the counterspeech generator as a source of user-specific information."}, {"title": "4.2 Evaluation", "content": "The goal of the evaluation is to assess the extent to which the generated counterspeech messages possess the properties outlined in Section 3, and to identify the most effective configurations. We adopt a hybrid semi-automatic evaluation approach. In a first step, we implement multiple quantitative indicators that automate the assessment of the properties. Afterwards, we leverage these results to select a subset of configurations that we further evaluate manually.\n4.2.1 Algorithmic evaluation. Following recent literature [5, 39, 58], we implement a comprehensive pool of evaluation indicators:\n\u2022 Relevance: For each toxic message $m_i$ and corresponding generated counterspeech $m_{i+1}$, we measure the relevance of $m_{i+1}$ to $m_i$ by computing the ROUGE score between the two texts [49].\n\u2022 Diversity: Given a configuration, we measure the diversity among its generated counterspeech messages as:\n$Diversity = 1 - \\frac{1}{n(n - 1)} \\sum_{i=1}^{n} \\sum_{j=1, j \\neq i}^{n} ROUGE(m_i, m_j)$,\nwhere $ROUGE(m_i, m_j)$ is the similarity between two counterspeech messages $m_i$ and $m_j$ and n is the total number of generated messages.\n\u2022 Readability: We measure the readability of the generated counterspeech messages via the Flesch Reading Ease (FRES) score [44]. FRES evaluates readability based on the average sentence length and the average number of syllables per word.\n\u2022 Toxicity: We measure toxicity via Google's Perspective API [47].\n\u2022 Adaptation: We measure the effectiveness of the adaptation as the diversity (i.e., 1 \u2013 ROUGE) between the counsterspeech messages generated by the baseline model [Ba] and those generated by each other configuration.\n\u2022 Personalizationlex: We measure personalization in terms of lexical similarity between the generated counterspeech messages and a sample of user messages. Given the author of a toxic message $m_i$, we quantify lexical similarity as the ROUGE score between the sample of messages by the user and the counterspeech message $m_{i+1}$. This indicator quantifies the degree of lexical overlap between the counterspeech and user messages.\n\u2022 Personalizationwri: We also measure personalization in terms of writing style similarity. First, we compute the writing style profile of each counterspeech message and of the authors of each toxic message. We obtain writing style profiles via PROFILINGUD, a system that extracts more than 130 raw, morpho-syntactic, and syntactic properties that are representative of the linguistic structure of a text corpus [7]. Then, we compute Spearman's rank correlation coefficient between the writing style profile of each counterspeech message and that obtained from the sample of messages of the author of the toxic message $m_i$.\n4.2.2 Configuration selection. The algorithmic evaluation provides a comprehensive, albeit approximate, assessment of each configuration. However, certain properties of counterspeech, such as persuasiveness and artificiality, cannot be reliably measured by these indicators. Moreover, the accuracy of the existing indicators for measurable properties is open to dispute [36, 75]. To address these limitations, we also conduct an extensive human evaluation via crowdsourcing. Still, given the large number of configurations, a complete human evaluation is impractical. We therefore leverage the algorithmic evaluation to select a subset of configurations for further human assessment. For each indicator, we first obtain a ranking of all configurations based on their scores for that indicator. Then, we aggregate all indicator-specific rankings into a super-ranking by solving the optimization task that minimizes Spearman's footrule distance [53, 71]. Finally, we use the super-ranking to select six configurations: the best and worst configurations that perform only adaptation, only personalization, and both. Evaluating best and worst configurations helps identify significant differences between adaptation and personalization strategies, and verifies the reliability of the algorithmic indicators as proxies for the measured properties. In addition, we also select the baseline configuration [Ba] to enable meaningful comparisons. As a final step, we select the N = 20 most representative counterspeech messages generated by each of the seven selected configurations. First, we compute the centroid of each configuration across all indicators. Then, we select the 20 counterspeech messages that are closer to each configuration's centroid.\n4.2.3 Human evaluation. We carry out an extensive human evaluation campaign via a pre-registered, mixed design crowdsourcing experiment on Amazon Mechanical Turk. Initially, participants are assigned to one of two between-subjects conditions: (i) non-contextual - participants are shown only the toxic messages and the corresponding generated counterspeech responses; (ii) contextual - participants are also shown the contextual information that was used to adapt and/or personalize the counterspeech. After this initial assignment, participants are asked to read and evaluate multiple pairs (mi, mi+1) of toxic messages and counterspeech, under seven within-subjects conditions. The seven conditions correspond to the seven configurations selected for human evaluation. The order of within-subjects conditions is randomized to control for order effects. In both within-subjects experiments participants are asked to rate on a five points Likert scale: the relevance of the counterspeech to the toxic message, its adequacy as counterspeech, its truthfulness, its artificiality, and its persuasiveness. Following recent literature [40], persuasiveness is evaluated in two different questions based on the counterspeech likelihood of (i) persuading the author of the toxic message to re-engage in the conversation in a civil manner, and of (ii) steering the conversation back to civil discourse. Participants assigned to the contextual between-subjects condition are also asked to rate how contextualized is the counterspeech to the context of the toxic message. Finally, all participants are asked a small set of socio-demographic questions. The complete list of questions is reported in Appendix B. This mixed design allows us to isolate and evaluate the impact of contextual information on the effectiveness"}, {"title": "5 DATA", "content": "We collected Reddit comments posted over multiple years in five popular subreddits discussing US politics. We selected subreddits with a marked right- (r/conservatives) or left-leaning (r/progressive) tendency, with a focus on a prominent conservative (r/the_donald for Donald Trump) or progressive (r/aoc for Alexandra Ocasio-Cortez) figure, and a mixed-leaning subreddit (r/politics). Depending on subreddit size, data collection spanned either 36 or 12 months so as to collect a comparable amount of data from each subreddit. For r/the_donald, data collection stopped in June 2020, when the subreddit was permanently banned [15]. All data was collected from the Pushshift archives [2].\nCounterspeech dataset. We selected a small set of toxic comments for which to generate counterspeech responses. First, we computed the toxicity score of each comment via Google's Perspective API [47]. Then, we selected those comments with toxicity \u2265 0.5 and with at least two parent comments in their conversation thread. This allowed selecting 128 toxic comments from 49 threads. Each of the 36 implemented counterspeech generation configurations was asked to generate a counterspeech response to each of the 128 toxic comments, resulting in 4,608 counterspeech responses that we generated and evaluated.\nAdaptation and personalization datasets. We also built a few additional datasets that implement the adaptation and personalization strategies described in Section 4.1. We selected a stratified random sample of around 7,500 comment-reply pairs that we used to fine-tune the counterspeech generator to Reddit's political conversational style and informal language, thus implementing the community adaptation factor [ Re ]. We also selected the two preceding comments to each of the 128 previously selected toxic comments, which we fed to the counterspeech generator via prompting, implementing the conversation adaptation factor [Pr]. Finally, we collected twenty random comments for each author of the 128 selected toxic comments. These data were used to generate the user profiles that implement the user summary personalization factor"}, {"title": "6 RESULTS", "content": "6.1 Algorithmic evaluation\nWe evaluate all factors (N = 7) and configurations (N = 36) with the indicators defined in Section 4.2.1.\n6.1.1 Factors. First, we aggregate results by the presence or absence of each factor. This analysis investigates whether the presence of a specific factor contributes to an improvement of the configurations in terms of the evaluation indicators. Results are presented in Figure 2. For each factor (y axis), the plots show the mean value of the indicator when the factor is present (teal dot) or absent (sand dot) in the evaluated configurations. In each panel the factors are ranked from top to bottom based on the delta between the two mean values. Overall, Figure 2 reveals that the presence of some factors in a configuration causes both improvements and degradations depending on the indicator. For example, fine-tuning with MULTICONAN [Mu] and with our community adaptation dataset [Re] improves relevance, diversity, readability, and adaptation. Albeit, the same factors worsen toxicity and writing style personalization. This result highlights that no single factor improves all aspects of counterspeech generation. Thus, when generating contextualized counterspeech in practical scenarios, it may be necessary to make case-by-case choices of the factors to use depending on the requirements at hand, given that no single factor is capable of improving all aspects. Despite this, some factors perform poorly overall. For example, fine-tuning with the RHSI dataset [Hs] degrades relevance, diversity, toxicity, and writing style personalization. We also note that the three fine-tuning factors (e.g., [Mu], [Re], and [Hs]) perform similarly in terms of readability, toxicity, adaptation, and personalization. Figures 2E, F, and G highlight the contributions of our adaptation and personalization strategies towards generating contextualized counterspeech. Concerning adaptation, both [Re] and [Pr] provide improvements, although only the former is marked. Instead, while [Su] clearly improves both personalization indicators, [Hi] yields an improvement only in terms of writing style personalization. Finally, we note that the seemingly positive personalization results of the base factor [Ba], which is only present in configurations that lack fine-tuning, emphasizes that fine-tuning with either MULTICONAN [Mu ], RHSI [ Hs ], or community adaptation [Re] tends to degrade personalization.\n6.1.2 Configurations. Next, we analyze fine-grained algorithmic results for each evaluated configuration. These are reported in Table 2, where configurations are organized in four groups from top to bottom: those without any adaptation nor personalization, those with only adaptation, those with only personalization, and those with both. For each evaluation indicator, the best result is shown in bold font and the remaining top-5 results are underlined. Many configurations achieve comparable results and each group contains some configurations that achieve top results in at least a few indicators. However, there are notable differences between the groups. Multiple configurations that make use of both adaptation and personalization obtain top or anyway strong results in many evaluation"}, {"title": "6.2 Human evaluation", "content": "We recruited N = 2, 444 and N = 2, 353 participants on Amazon Mechanical Turk, respectively for the non-contextual and contextual between-subjects experimental conditions. These figures exclude participants whose answers were rejected due to excessively fast survey completion time, < 100% correct answers to the control questions, and those that gave the same answer to all questions. We report no deviations from the pre-registration protocol.\n6.2.1 Non-contextual experiment. Participants assigned to the non-contextual condition were shown pairs of toxic speech and counterspeech responses. The Friedman test reveals statistically significant differences between some of the evaluated configurations. Figure 3 shows effects sizes, confidence intervals, and statistical significance of the comparisons between each configuration and the baseline [ Ba ]. As shown, two groups of configurations achieved overall similar results. Configurations [Mu Re], [Hs Hi], [Mu Hs Hi], and [Mu Re Pr Hi] consistently obtain statistically significant worse results than the baseline in each evaluated aspect, except for artificiality. This result implies that the counterspeech generated by said configurations was perceived as more human-like than that of the baseline, but that apart from this, the baseline generated better counterspeech in any evaluated aspect. Instead, [Ba Pr] and [Ba Pr Hi] obtain comparable or statistically significant better results than the baseline. Specifically, [Ba Pr Hi] outperforms the baseline with respect to the adequacy of the generated counterspeech and its perceived capacity to persuade the author of the toxic message. It also achieves better scores than the baseline concerning relevance, truthfulness, and capacity to persuade bystanders, although these improvements are not statistically significant. [Ba Pr] obtains similar results to the baseline, so much so that none of the measured differences is significant. However,"}, {"title": "6.2.2 Contextual experiment", "content": "Participants assigned to the contextual condition where shown additional information on top of the toxic message and the generated counterspeech. Contextual information includes the name of the subreddit where the toxic message was posted, the previous message in the conversation thread, and the user summary obtained as described in Section 4.1.2. Again, the Friedman test reveals statistically significant differences. Detailed results are presented in Figure 4 and largely corroborate those from the non-contextual experiment. The configurations [Ba Pr] and [Ba Pr Hi] consistently achieve the highest overall scores, with [Ba Pr] demonstrating a statistically significant improvement over the baseline in persuading the author of the toxic message. Most other results for these configurations are statistically non-significant. Conversely, configurations [Mu Re ], [Hs Hi], [Mu Hs Hi], and [Mu Re Pr Hi]consistently perform significantly worse than the baseline across all metrics, except for [Mu Re] and [Hs Hi] who outperform the baseline in terms of artificiality. This experiment contains an additional question with respect to the non-contextual one, where participants rated the effectiveness of contextualization. Figure 4G shows that while most differences are non-significant, [Mu Re Pr Hi] performs worse and [Ba Pr Hi] performs markedly better than the baseline, which reinforces previous results about the effectiveness of the [Ba Pr Hi] configuration.\nOur study design also allows comparing the scores obtained by each configuration in the contextual and non-contextual experiments. Results of this comparison are shown in Figure 5 and reveal that all configurations-baseline included-obtained overall better scores in the contextual experiment across all aspects, except for artificiality. However, some configurations improved more than others. The baseline and configurations that were already performing well, such as [Ba Pr Hi]and[Ba Pr ], showed the least improvements. Conversely, configurations with poor initial performance scored larger gains. This suggests that the additional contextual information allowed for a more accurate evaluation of counterspeech generated by adapted and personalized models, effectively leveling the field and reducing differences between configurations. This trend is evidenced by the smaller effect sizes reported in Figure 4 compared to those in Figure 3."}, {"title": "6.2.3 Algorithmic and human evaluations", "content": "We conclude by comparing the results achieved by the selected configurations in the algorithmic and human evaluations. For each evaluation method (i.e., quantitative indicators, human assessments with and without context), Figure 6 shows the aggregated ranking of the configurations across all considered aspects, so that the overall best configurations are at the top. Rank aggregation is performed with the method"}, {"title": "7 DISCUSSION AND CONCLUSIONS", "content": "Generation. Our extensive analysis of adaptation and personalization strategies across a large set of algorithmic and human judgments highlights the complexities of generating effective contextualized counterspeech. For example, Figure 4G illustrates that only one configuration produced significantly better-contextualized counterspeech than the baseline. Nonetheless, this successful configuration significantly outperformed the baseline in terms of adequacy and persuasiveness. This result is relevant and novel, as it represents the first success at generating effective contextualized counterspeech [8]. This work thus marks an advancement in the field, demonstrating the potential for tailored approaches to improve the effectiveness of counterspeech interventions.\nThe difficulty at generating well-contextualized messages may be due to the need of additional information, as recent studies suggest that LLMs might struggle to combine multiple instructions and information, which can degrade their output [3, 29]. The manual analysis of some generated counterspeech, reported in Appendix C, supports this hypothesis. However, this limitation is likely to be mitigated by the adoption of larger LLMs [35]. Therefore, future availability of ever-larger LLMs is likely to render adapted and personalized counterspeech, and more broadly moderation interventions [18], increasingly advantageous.\nEvaluation. Our results also bear important implications for the evaluation of AI-generated counterspeech. Most existing works rely on a small set of algorithmic indicators to assess the quality of the generated counterspeech [20, 40, 65]. However, our results, alongside other recent studies [75], reveal that these indicators correlate poorly with human assessment. This discrepancy suggests that indicators and human evaluators focus on different aspects of the counterspeech. Consequently, future research should adopt nuanced evaluation methods that incorporate both algorithmic and human assessments, to avoid possibly misleading conclusions.\nIn summary, our findings indicate that human evaluators found some instances of contextualized AI-generated counterspeech particularly persuasive, highlighting the potential for AI-generated solutions to effectively address human misbehavior on online platforms. In addition, the results of our extensive evaluations call for"}, {"title": "Limitations and Future Work", "content": "Despite our extensive experimentation and evaluation, our results rely upon certain technological and methodological choices. For example, we experimented with a single LLM and we evaluated a limited set of strategies for generating adapted and personalized counterspeech. Different LLMs or alternative adaptation and personalization strategies might yield different results. Similar considerations apply to the results of our algorithmic and human evaluations. The former is limited by the quantitative indicators that we considered. These, although in line with the state-of-the-art, can gauge but a small set of characteristics. The latter is limited by the representativeness and reliability of crowdsourced evaluations. These limitations highlight the need for more research and experimentation on the effectiveness of contextualized counterspeech. Other than these, future work should explore more sophisticated models and techniques for personalizing and adapting counterspeech and should ensure that AI-generated counterspeech is unbiased and fair. Future research should investigate methods to detect possible biases in counterspeech generation and assess the long-term effects of these interventions on varied user groups."}, {"title": "8 ACKNOWLEDGMENTS", "content": "This work is partially supported by the European Union \u2013 Next Generation EU within the PRIN 2022 framework project PIANO (Personalized Interventions Against Online Toxicity); by the PNRR-M4C2 (PE00000013) \u201cFAIR-Future Artificial Intelligence Research\" - Spoke 1 \"Human-centered AI\", funded under Next Generation EU; and by the Italian Ministry of Education and Research (MUR) in the framework of the FoReLab projects (Departments of Excellence)."}, {"title": "A PROMPTS", "content": "This section reports the LLM prompts that we used to generate counterspeech responses and user summaries for the different factors described in Section 4.1:\n\u2022 Default counterspeech generation: \"You are a moderator of a subreddit and you come across a comment that exhibits hate speech. Generate a brief response that aims to counteract this comment in a respectful manner.\"\n\u2022 Counterspeech generation (Conversation (Pr) strategy): \"You are a moderator of a subreddit and you come across a comment that exhibits hate speech. Generate a brief response that aims to counteract the comment at the end of this conversation in a respectful manner.\"\n\u2022 Counterspeech generation (Comment history (Hi) strategy): \"You are a moderator of a subreddit and you come across a comment that exhibits hate speech. Given the following 10 comments from the same reddit user: <comments>, generate a brief response that aims to counteract this comment in a respectful manner, using these comments to understand the user's style and personalize your response.\"\n\u2022 Counterspeech generation (Summary [Su] strategy): \"You are a moderator of a subreddit and you come across a comment that exhibits hate speech. Given the following summary describing the reddit user that made the comment: <summary>, generate a brief response that aims to counteract this comment in a respectful manner, using the user's summary to understand his/her style and personalize your response.\"\n\u2022 User summary generation (Summary [Su ] strategy): \"Given the following comments written by the same Reddit user: <comments>, generate a concise and schematic summary describing the user, following this schema: 1) Writing style and lexicon: Identify and describe the predominant writing style of the user; 2) Interests: Describe the interests and topics generally covered by the user. Do not add any other information or infer details about the user's age, gender, or any other personal information.\""}, {"title": "BCROWDSOURCING QUESTIONNAIRE", "content": "B.1 Task description\nEach participant in our crowdsourcing experiment was allowed to complete the questionnaire only once and received $0.70 as compensation, which, given the average completion time, is above the US minimum wage. Upon providing their informed consent to take part in the experiment, participants received the following description of the task: \"Your task is to evaluate a set of counterspeech responses to toxic messages posted on social media based on several criteria. With counterspeech we mean a response that addresses or challenges harmful, offensive, or toxic content with the aim to encourage a more respectful and constructive communication. Please, carefully consider the toxic post and corresponding response below, then rate the following statements from strongly disagree (1) to strongly agree (5).\nB.2 Counterspeech questions\nThe following questions were asked for each pair of toxic message and corresponding counterspeech response:\n\u2022 Relevance: The response is relevant to the toxic post.\n\u2022 Adequacy: The response is suitable as counterspeech.\n\u2022 Truthfulness: The response is truthful (i.e., honest, sincere).\n\u2022 Persuasiveness (toxic user): The response would persuade the author of the toxic post to re-engage in the conversation in a civil manner.\n\u2022 Persuasiveness (conversation): The response would steer the overall conversation back to civil discourse.\n\u2022 Artificiality: The response was generated by artificial intelligence.\nParticipants assigned to the contextual between-subjects condition (see Section 4.2.3) also received the following question:\n\u2022 Contextualization: The counterspeech response is personalized (as opposed to being generic) with respect to the post's context.\nB.3 Socio-demographic questions\nThe following questions were asked once for each participant, at the end of the questionnaire:\n\u2022 Age: [free text, numeric]\n\u2022 Gender: [Female, Male, Non-binary or gender diverse, I prefer not to disclose]\n\u2022 Education: [High school or less, Some college, College graduate or more]\n\u2022 Which of the following describes best your race/ethnicity? [Asian/Asian American, Black/African American, Hispanic/Latino, White/Caucasian, Other]\n\u2022 Which of the following describes best your political affiliation? [Democratic, Lean Democratic, Lean Republican, Republican]\n\u2022 How frequently do you use social media (e.g., Facebook, Twitter/X, Instagram, Reddit, etc.)? [Never. Rarely (less than once a week). Sometimes (once a week to several times a week). Often (daily). Very often (multiple times a day)]\n\u2022 How many different social media do you actively use (at least once a week)? [None, 1, 2-3, 4-5, 5+]"}, {"title": "C COUNTERSPEECH EXAMPLES", "content": "We carried out a manual analysis of a subset of toxic message-generated counterspeech pairs. The analysis was useful to identify recurring patterns and issues in the counterspeech messages generated by certain configurations. Table 3 reports some notable examples. The topmost three rows display examples of effective counterspeech generated by the configurations that achieved the overall best results in our evaluations-that is, [Ba Pr] and [Ba Pr Hi] (see Section 4.1 for details). Instead, subsequent rows report some problematic generations. Among the patterns that we noticed is"}]}