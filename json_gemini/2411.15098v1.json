{"title": "OminiControl: Minimal and Universal Control for Diffusion Transformer", "authors": ["Zhenxiong Tan", "Songhua Liu", "Xingyi Yang", "Qiaochu Xue", "Xinchao Wang"], "abstract": "In this paper, we introduce OminiControl, a highly versatile and parameter-efficient framework that integrates image conditions into pre-trained Diffusion Transformer (DiT) models. At its core, OminiControl leverages a parameter reuse mechanism, enabling the DiT to encode image conditions using itself as a powerful backbone and process them with its flexible multi-modal attention processors. Unlike existing methods, which rely heavily on additional encoder modules with complex architectures, OminiControl (1) effectively and efficiently incorporates injected image conditions with only 0.1% additional parameters, and (2) addresses a wide range of image conditioning tasks in a unified manner, including subject-driven generation and spatially-aligned conditions such as edges, depth, and more. Remarkably, these capabilities are achieved by training on images generated by the DiT itself, which is particularly beneficial for subject-driven generation. Extensive evaluations demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted models in both subject-driven and spatially-aligned conditional generation. Additionally, we release our training dataset, Subjects200K, a diverse collection of over 200,000 identity-consistent images, along with an efficient data synthesis pipeline to advance research in subject-consistent generation. 1", "sections": [{"title": "1. Introduction", "content": "Diffusion models [9, 25, 28] have revolutionized the field of visual generation, demonstrating remarkable capabilities that significantly outperform traditional approaches like Generative Adversarial Networks (GANs) [6] in terms of image quality and diversity. While these models excel at generating highly realistic imagery, a critical challenge persists: enabling precise and flexible control over the generation process to accommodate diverse and complex user requirements.\nText-based conditioning has been a cornerstone in advancing controllable generation[2, 13, 23, 25, 28, 35], offering an intuitive interface for users to specify their desired outputs. However, text prompts alone often fail to convey precise spatial details and structural attributes that users wish to control. Consequently, recent research has explored complementary conditioning modalities for guiding diffusion models, with image-based control emerging as a particularly effective approach [15, 22, 39, 41, 43]. This multimodal conditioning strategy enables more detailed and accurate control over the generation process, addressing the limitations inherent in purely text-based interfaces.\nCurrent image conditioning methods can be broadly categorized into spatially aligned and non-spatially aligned approaches. Spatially aligned tasks such as sketch-to-image and inpainting require direct correspondence between conditioning and output images, typically achieved through methods like ControlNet[41] that inject conditioning features in a spatially-preserving manner. In contrast, non-spatially aligned applications including subject driven generation and style transfer, as demonstrated by IP-Adapter[39], often employ pre-trained encoders like CLIP[27] to extract global features for integration via cross-attention mechanisms.\nDespite the effectiveness of existing image-conditioned approaches, they present several limitations that hinder their efficiency and flexibility [22, 39, 41]. First, most existing methods are designed specifically for UNet-based architectures [16, 22, 24, 29, 31, 39\u201343], as seen in Stable Diffusion models [25, 28]. While these approaches work well with UNet's encoder-decoder structure, they may not translate effectively to the more advanced Diffusion Transformer (DiT) models[23] that have demonstrated superior image generation quality[2, 13]. Additionally, current approaches typically specialize in either spatially aligned [22, 41, 43] or non-spatially aligned tasks [12, 15, 17, 39, 42], lacking a unified architecture to handle both control types effectively. This specialization often requires practitioners to employ different methods for different control scenarios, increasing system complexity and implementation overhead. Furthermore, these methods rely heavily on additional network structures [17, 22, 39, 41\u201343], which introduce substantial parameter overhead.\nTo address these limitations, we propose a parameter-efficient approach for incorporating image-based control into DiT architectures[23]. Our method reuse the model's existing VAE encoder[28] to process conditioning images. Following the same token processing pipeline as noisy image tokens, we augment the encoded features with learnable position embeddings[34], and seamlessly integrate them alongside latent noise in the denoising network. This design enables direct multi-modal attention interactions[23, 30] between condition and generation tokens throughout the DiT's transformer blocks, facilitating efficient information exchange and control signal propagation.\nWe implemented our method on the high-performing DiT-structured diffusion model, FLUX.1-dev[13], a large-scale model containing 12 billion parameters. Extensive experiments on edge-guided generation, depth-aware synthesis, region-specific editing, and identity-preserving generation indicate that our DiT-based approach yields better results compared to both UNet-based implementations[7, 39, 41] and their community adaptations on the FLUX.1 model [14, 37].\nFor identity-preserving generation, we developed a novel data synthesis pipeline that generates high-quality, identity-consistent image pairs. Using this pipeline, we created a comprehensive dataset comprising over 200,000 diverse images. To facilitate future research in this direction, we will release both our dataset and the complete pipeline implementation as open-source resources2.\nIn summary, we highlight our contributions as follows:\n1. We present a parameter-efficient method for enabling image-conditioned control in Diffusion Transformer (DiT) models, achieving both spatially aligned and non-spatially aligned control within a unified framework.\n2. We demonstrate the effectiveness of our approach through extensive experiments across diverse control tasks, including edge-guided generation, depth-aware synthesis, region-specific editing, and identity-preserving generation, consistently outperforming existing methods on both UNet implementations and their DiT adaptations.\n3. We develop and release Subjects200K, a high-quality dataset of over 200,000 subject-consistent images, along with an efficient data synthesis pipeline, providing valuable resources to the research community for further exploration of subject-consistent generation tasks."}, {"title": "2. Related works", "content": null}, {"title": "2.1. Diffusion-based models", "content": "Diffusion-based methods have emerged as a powerful framework for image generation[9, 28], demonstrating success across diverse tasks including text-to-image synthesis [2, 28, 33], image-to-image translation [32], and image editing [1, 20]. Recent advances have led to significant improvements in both quality and efficiency, notably through the introduction of latent diffusion models [28]. To further enhance generative capabilities, large-scale transformer architectures have been integrated into these frameworks, leading to advanced models like DiT[2, 3, 13, 23]. Building on these architectural innovations, FLUX [13] incor-"}, {"title": "2.2. Controllable generation with diffusion models", "content": "Controllable generation has been extensively studied in the context of diffusion models. Text-to-image models[25, 28] have established a foundation for conditional generation, while various approaches have been developed to incorporate additional control signals such as image. Notable methods include ControlNet [41], enabling spatially aligned control in diffusion models, and T2I-Adapter [22], which improves efficiency with lightweight adapters. UniControl [26] uses Mixture-of-Experts (MoE) to unify different spatial conditions, further reducing model size. However, these methods rely on spatially adding condition information to the denoising network's hidden states, inherently limiting their effectiveness for non-spatial tasks like subject-driven generation. IP-Adapter [39] addresses this by introducing cross-attention through an additional encoder, and SSR-Encoder [42] further enhances identity preservation in image-conditioned tasks. Despite these advances [5, 15, 19], a unified solution for both spatially aligned and non-aligned tasks remains elusive."}, {"title": "3. Methods", "content": null}, {"title": "3.1. Preliminary", "content": "The Diffusion Transformer (DiT) model [23], employed in architectures like FLUX.1 [13], Stable Diffusion 3 [28], and PixArt [2], uses a denoising network of transformer blocks to refine noisy image tokens iteratively.\nEach transformer block processes two types of tokens: noisy image tokens $X \\in \\mathbb{R}^{N \\times d}$ and text condition tokens $C_T \\in \\mathbb{R}^{M \\times d}$, where d is the embedding dimension, N and M are the number of image and text tokens respectively (Figure 2). These tokens are embedded into hidden states X and $C_T$, which maintain consistent shapes throughout the transformer blocks.\nIn each DiT block, after normalizing X and $C_T$, they are processed by the core MM-Attention module [30], which employs Rotary Position Embedding (RoPE) [34] to incorporate positional dependencies across tokens. For a token at position (i, j) in the 2D grid, RoPE applies rotation matrices to the query and key projections:\n$Q_x(i, j) = W_Q (X_{i,j} \\cdot R(i, j))$,\n$K_x(i, j) = W_K (X_{i,j} \\cdot R(i, j))$,\nwhere R(i, j) is the rotation matrix at position (i, j). Similarly, the text condition tokens $C_T$ have their query and key projections defined in the same way, with all text token positions set to (0,0) in FLUX.1.\nAfter applying RoPE, the queries, keys, and values from both token types are concatenated to form unified matrices $Q_Z$, $K_Z$, and $V_Z$, representing the combined token set $Z = [X; C_T]$. The MM-Attention operation is then computed as:\n$MMAttention(Z) = softmax(\\frac{Q_Z K_Z}{\\sqrt{d}})V_Z$;\nenabling interactions between image and text condition tokens through the attention mechanism."}, {"title": "3.2. Image condition integration", "content": "Our approach first encodes the condition image through the model's VAE, projecting it into the same latent space as the noisy image tokens to form $C_I \\in \\mathbb{R}^{N \\times d}$.\nPrevious methods like ControlNet [41] and T2I-Adapter [22] incorporate the condition image by spatially aligning and adding its hidden states directly to those of the noisy image tokens:\n$H_X = H_X + H_{C_I}$,\nwhere $H_X$ represents the combined hidden states for further processing, with $H_{C_I}$ being the hidden states from the condition image. While this approach proves effective for spatially aligned tasks, it faces two key limitations: (1) it lacks flexibility when handling non-aligned scenarios, and (2) even in spatially aligned cases, the direct addition of hidden states constrains token interactions, potentially limiting the model's performance.\nIn contrast, to enable non-aligned control tasks and provide greater conditioning flexibility, our method processes condition image tokens uniformly with text and noisy image tokens, integrating them into a unified sequence:\n$Z = [X; C_T; C_I]$,\nwhere Z represents the concatenated sequence of noisy image tokens X, text tokens $C_T$, and condition image tokens $C_I$. This unified approach enables direct participation in multi-modal attention [30] without specialized processing pathways (illustrated in Figure 2). rect addition approach.\nComparative analysis shows that our approach achieves better results than the direct additive method, as demonstrated by the attention visualization in Figure 3. Moreover, the training curves in Figure 4a demonstrate that multimodal attention method consistently achieves lower loss values than the di The effectiveness of this unified sequence approach is demonstrated across both spatially aligned and non-spatially aligned tasks (Figure 5), highlighting its versatility in handling diverse conditional generation scenarios."}, {"title": "3.3. Adaptive position embedding", "content": "The integration of condition images requires careful consideration of positional information to ensure effective interaction between condition and target images. Previous methods often assume rigid spatial alignment between condition and output images, limiting their applicability to non-aligned tasks. Moreover, the relative positioning between condition and target tokens can significantly impact the model's learning efficiency and generalization ability.\nLeveraging our unified sequence design where condition tokens are concatenated with other tokens, we have the flexibility to explore different positional encoding strategies.\nIn FLUX.1's Transformers, each token is assigned a corresponding position index to encode spatial information. For a 512\u00d7512 input image, the VAE [11] encoder first projects it into the latent space, then the latent representation is divided into a 32\u00d732 grid of tokens, where each token receives a unique two-dimensional position index (i, j) with i, j\u2208 [0, 31]. This indexing scheme preserves the spatial structure of the original image in the latent space, while text tokens maintain a fixed position index of (0,0).\nFor spatially aligned tasks, our initial approach was to assign condition tokens the same position embeddings as their corresponding tokens in the original image. However, for non-spatially aligned tasks such as subject-driven generation, our experiments revealed that shifting the position indices of condition tokens leads to faster convergence (Figure 4b). Specifically, we shift the condition image tokens to indices (i, j) where i \u2208 [0,31] and j\u2208 [32,64], ensuring no spatial overlap with the original image tokens X."}, {"title": "3.4. Condition strength factor", "content": "The unified attention mechanism we adopt not only enables flexible token interaction but also allows us to precisely control the influence of condition images. Specifically, we designed a method that allows for manual adjustment of the condition image's effect during inference. For a given strength factor \u03b3, setting \u03b3 = 0 removes the condition image's influence, resulting in an output based purely on the original input. At \u03b3 = 1, the output fully reflects the condition image, and as \u03b3 increases beyond 1, the condition's effect becomes even more pronounced.\nTo achieve this controllability, we introduce a bias term into the original MM-Attention operation. Specifically, we modify Equation 3 to:\n$BiasedAttention(Z) = softmax(\\frac{Q_Z K_Z}{\\sqrt{d}} + bias(\\gamma))V_Z$,\nwhere $bias_\\gamma$ is designed to adjust the attention weights between condition tokens and other tokens based on the strength factor \u03b3. The bias term is constructed as a (M + 2N) \u00d7 (M + 2N) matrix, where M is the number of text tokens, and N is the number of noisy image tokens and condition image tokens each. The matrix has the following structure:\n$bias(\\gamma) = \\begin{pmatrix}0_{M \\times M} & 0_{M \\times N} & 0_{M \\times N} \\\\ 0_{N \\times M} & 0_{N \\times N} & log(\\gamma)1_{N \\times N} \\\\ 0_{N \\times M} & log(\\gamma)1_{N \\times N} & 0_{N \\times N}\\end{pmatrix}$;\nThis design ensures that the strength factor \u03b3 only affects the attention weights between noisy image tokens and condition image tokens, while maintaining the original attention patterns for text tokens and within-modality interactions."}, {"title": "3.5. Subjects200K datasets", "content": "Training models for subject-consistent generation typically requires paired images that maintain identity consistency while exhibiting variations in pose, lighting, and other attributes. Previous methods like IP-Adapter [39] use identical images for conditioning and target pairs, which proves effective for their approaches. However, in our framework, this setup leads to overfitting, causing the model to generate outputs nearly identical to the inputs.\nTo overcome these limitations, we developed a dataset featuring images that preserve subject identity while incorporating natural variations. While existing datasets [12, 15, 17, 31] address similar needs, they often face constraints in either quality or scale. We therefore propose a novel synthesis pipeline leveraging FLUX's inherent capability to generate pairs of visually related images from carefully crafted prompts."}, {"title": "4. Experiment", "content": null}, {"title": "4.1. Setup", "content": "Tasks and base model. We evaluate our method on two categories of conditional generation tasks: spatially aligned tasks (including Canny-to-image, depth-to-image, masked-based inpainting, and colorization) and subject-driven generation. We build our method upon FLUX.1 [13], a latent rectified flow transformer model for image generation. By default, we use FLUX.1-dev to generate images for spatially aligned tasks. In subject-driven generation tasks, we switch to FLUX.1-schnell as we observed it tend to produce better visual quality.\nImplement details. Our method utilizes LoRA [4]for fine-tuning the base model with a default rank of 4. To preserve the model's original capabilities and achieve flex-"}, {"title": "4.2. Main result", "content": "Spatially aligned tasks As shown in Table 1, we comprehensively evaluate our method against existing approaches on five spatially aligned tasks. Our method achieves the highest F1-Score of 0.38 on Canny-to-image generation, significantly outperforming both SD1.5-based methods ControlNet [41] and T2I-Adapter [22], as well as FLUX.1-based ControlNetPro [14]. In terms of general quality metrics, our approach demonstrates consistent superiority across most tasks, showing notably better performance in SSIM [36], MAN-IQA [38], and MUSIQ [10] scores. For challenging tasks like deblurring and colorization, our method achieves substantial improvements: the MSE is reduced by 77% and 93% respectively compared to ControlNetPro, while the FID scores [8] improve from 30.38 to 11.49 for deblurring. The CLIP-Score metrics [27] indicate that our method maintains high text-image consistency across all tasks, suggesting effective preservation of semantic alignment while achieving better control and visual quality. As shown in Figure 7, our method produces sharper details and more faithful color reproduction in colorization tasks, while maintaining better structural fidelity in edge-guided generation and deblurring scenarios.\nSubject driven generation Figure 8 presents a comprehensive comparison against existing baselines. Our method demonstrates superior performance, particularly in identity preservation and modification accuracy. Averaging over random seeds, we achieve 75.8% modification accuracy compared to IP-Adapter (FLUX)'s 57.7%, while maintaining 50.6% identity preservation against IP-Adapter (SD 1.5)'s 29.4%. The advantage amplifies in best-seed scenarios, achieving 90.7% modification accuracy and 82.3% identity preservation - surpassing the strongest baselines by 15.8 and 18.0 percentage points, demonstrating effective subject-fidelity editing. These quantitative results are further corroborated by user studies presented in Appendix B.1.\nComparative paramter efficiency. As shown in Table 2, our approach achieves remarkable parameter efficiency compared to existing methods. For the 12B parameter FLUX.1 model, our method requires only 14.5M trainable parameters (approximately 0.1%), which is significantly lower than ControlNet (27.5%) and IP-Adapter (7.6%). Even when utilizing the original VAE encoder from FLUX.1, our method still maintains high efficiency with just 0.4% additional parameters, demonstrating the effectiveness of our parameter-efficient design."}, {"title": "4.3. Empirical studies", "content": "Effect of training data. For subject-driven generation, our model takes a reference image of a subject (e.g., a plush toy or an object) and a text description as input, aiming to generate novel images of the same subject following the text guidance while preserving its key characteristics.\nTo validate the effectiveness of our Subjects200K dataset described in Section 3.5, we compare two training strate-"}, {"title": "5. Conclusion", "content": "OmniControl offers parameter-efficient image-conditioned control for Diffusion Transformers across diverse tasks using a unified token approach, without extra modules. Our method outperforms traditional approaches, and the new Subjects200K dataset-featuring 200,000 high-quality, subject-consistent images-supports advancements in subject-consistent generation. Results confirm Omni-Control's scalability and effectiveness in diffusion models."}, {"title": "A. Details of Subjects200K datasets", "content": "We present a comprehensive synthetic dataset constructed to address the limitations in scale and image quality found in previous datasets [12, 15, 17, 31]. Our approach leverages FLUX.1-dev [13] to generate high-quality, consistent images of the same subject under various conditions.\nSubjects200K dataset currently consists of two splits, both generated using similar pipelines. Split-1 contains paired images of objects in different scenes, while Split-2 pairs each object's scene image with its corresponding studio photograph. Due to their methodological similarities, we primarily focus on describing the synthesis process and details of Split-2, although both splits are publicly available. Our complete Subjects200K dataset can be fully accessed via this link."}, {"title": "A.1. Generation pipeline", "content": "Our dataset generation process consists of three main stages: description generation, image synthesis, and quality assessment.\nDescription Generation We employed ChatGPT-40 to create a hierarchical structure of descriptions: We first generated 42 diverse object categories, including furniture, vehicles, electronics, clothing, and others. For each category, we created multiple object instances, totaling 4,696 unique objects. Each object entry consists of: (1) A brief description, (2) Eight diverse scene descriptions, (3) One studio photo description. Figure S2 shows a representative example of our structured description format.\nImage Synthesis We designed a prompt template to leverage FLUX's capability of generating paired images containing the same subject. Our template synthesizes a comprehensive prompt by combining a brief object description with two distinct scene descriptions, ensuring subject consistency while introducing environmental variations.\nThe detailed prompt structure is illustrated in Figure S3. For each prompt, we set the image dimensions to 1056\u00d7528 pixels and generated five images using different random seeds to ensure diversity in our dataset. During the training process, we first split the paired images horizontally, then performed central cropping to obtain 512x512 pixel image pairs. This padding strategy was implemented to address cases where the generated images were not precisely bisected, preventing potential artifacts from appearing in the wrong half of the split images.\nQuality assessment We leveraged ChatGPT-40's vision capabilities to rigorously evaluate the quality of images gen-erated by FLUX.1-dev. The assessment focused on multiple critical aspects:\n\u2022 Image composition: Verifying that each image properly contains two side-by-side views.\n\u2022 Subject consistency: Ensuring the subject maintains identity across both views.\n\u2022 Image quality: Confirming high resolution and visual fidelity.\nTo maintain stringent quality standards, each image underwent five independent evaluations by ChatGPT-40. Only images that passed all five evaluations were included in our training dataset. Figure S1 presents representative examples from our quality-controlled dataset."}, {"title": "A.2. Dataset Statistics", "content": "In Split-2, we first generated 42 distinct object categories, from which we created and curated a set of 4,696 detailed object instances. Then we combine these descriptions to generate 211,320 subject-consistent image pairs. Through rigorous quality control using GPT-40, we selected 111,767 high-quality image pairs for our final dataset. This extensive filtering process ensured the highest standards of image quality and subject consistency, resulting in a collection of 223,534 high-quality training images."}, {"title": "B. Additional experimental results", "content": null}, {"title": "B.1. Evaluation for subject-driven generation", "content": "Framework and criteria. To systematically evaluate subject-driven generation quality, we establish a framework with five criteria assessing both preservation of subject characteristics and accuracy of requested modifications:\n\u2022 Identity Preservation: Evaluates preservation of essential identifying features (e.g., logos, brand marks, distinctive patterns)\n\u2022 Material Quality: Assesses if material properties and surface characteristics are accurately represented\n\u2022 Color Fidelity: Evaluates if colors remain consistent in regions not specified for modification\n\u2022 Natural Appearance: Assesses if the generated image appears realistic and coherent\n\u2022 Modification Accuracy: Verifies if the changes specified in the text prompt are properly executed\nUser studies. To further validate our approach, we conducted user studies collecting 375 valid responses. Participants evaluated the generated images across three key dimensions: identity consistency, text-image alignment, and visual coherence between subjects and backgrounds. The results shown in Figure S4 corroborate our quantitative findings, with our method achieving superior performance across all evaluation criteria."}, {"title": "B.2. Additional generation results", "content": "We showcase more generation results from our method. Figure S5 presents additional results on the DreamBooth dataset, while Figure S6 demonstrates our method's effectiveness on other subject-driven generation tasks."}]}