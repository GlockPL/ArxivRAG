{"title": "Teola: Towards End-to-End Optimization of LLM-based\nApplications", "authors": ["Xin Tan", "Yimin Jiang", "Yitao Yang", "Hong Xu"], "abstract": "Large language model (LLM)-based applications consist of\nboth LLM and non-LLM components, each contributing to\nthe end-to-end latency. Despite great efforts to optimize\nLLM inference, end-to-end workflow optimization has been\noverlooked. Existing frameworks employ coarse-grained or-\nchestration with task modules, which confines optimizations\nto within each module and yields suboptimal scheduling\ndecisions.\nWe propose fine-grained end-to-end orchestration, which\nutilizes task primitives as the basic units and represents each\nquery's workflow as a primitive-level dataflow graph. This\nexplicitly exposes a much larger design space, enables opti-\nmizations in parallelization and pipelining across primitives\nof different modules, and enhances scheduling to improve\napplication-level performance. We build Teola, a novel or-\nchestration framework for LLM-based applications that im-\nplements this scheme. Comprehensive experiments show\nthat Teola can achieve up to 2.09x speedup over existing\nsystems across various popular LLM applications.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) and their multi-modal vari-\nants have revolutionized user query understanding and con-\ntent generation. This breakthrough has transformed many\ntraditional and emerging applications. For instance, some\nsearch engines have integrated LLMs into their query pro-\ncessing pipelines, enhancing user experiences [3, 13]. Ad-\nditionally, AI agents, a new paradigm for human-machine\ninteraction, have led to new applications such as emotional\ncompanionship [4] and personalized assistants [16].\nDespite being the most intelligent component in the ap-\nplications, LLMs by themselves often cannot satisfy the di-\nverse and complicated user requirements. Examples include\nknowledge timeliness and long context understanding, for\nwhich LLMs cannot perform well due to their design. If not\nproperly handled, these problems can easily cause the well-\nknown hallucination issue [32]. To mitigate such problems,\nmany techniques have been proposed, including RAG (Re-\ntrieval Augmented Generation) [40, 48], external function\ncalls [11, 33, 38] and even multiple LLM interactions. Popu-\nlar frameworks such as Langchain [9] and LlamaIndex [1]\nsupport integrating various modules and building the end-\nto-end pipelines mentioned above."}, {"title": "2 Background and Motivation", "content": "2.1 LLM-based Applications\nA primer on LLM. Current LLMs are built upon transform-\ners, which rely on the attention mechanism to effectively\ncapture the long context in natural languages [61]. LLM infer-\nence, which this paper focuses on, is autoregressive: in each\nforward pass the model produces a single new token-the\nbasic unit of language modeling, which the becomes part of\nthe context and is used as input for the subsequent iterations.\nTo avoid redundant attention computation of preceding to-\nkens in this process, a key-value (KV) cache is used which\nbecomes a critical source of memory pressure [39, 68].\nLLM inference involves two phases: prefilling and decod-\ning. Prefilling produces the very first output token by pro-\ncessing all input tokens (instruction, context, etc.), and is\nclearly compute-bound. After prefilling, the decoding phase\niteratively generates the rest of the output based on the KV\ncache, and is memory-bound as in each iteration only the\nnew token from the previous iteration needs to be processed.\nLLM apps are more than just LLM. Despite their great gen-\neration capabilities, LLMs are not a panacea. Their training\ndatasets are inevitably not up-to-date, leading to knowledge\ngaps and hallucination issues [8, 40]. They also lack abilities\nto interact directly with the environment, that is they are\nnot directly capable of sending an email though it can draft\nthe email message [30, 56, 63]. Thus real-world applications\noften need to integrate additional tools with LLMs to be\npractically usable.\nWe show in Figure 2 four typical LLM-based applications\n(apps). Figure 2a demonstrates a search engine-empowered\ngeneration app, where the LLM utilizes the search engine"}, {"title": "2.2 Fine-grained Orchestration of LLM Apps", "content": "Many frameworks such as LlamaIndex [1], Langchain [9],\nand enterprise solutions such as PAI-RAG [12] and Azure-\nRAG [8] have emerged to facilitate the creation and orchestra-\ntion of LLM applications. They naturally adopt module-level\norchestration in the sense that each app is defined and sched-\nuled as a simple chain of modules, as depicted in Figure 3a.\nEach module is executed independently with backend en-\ngines. Coarse-grained module-level chaining is easy to use,\nbut inherently limited for optimizing the complex workflows\nfor best performance. It overlooks the larger design space of\njointly optimizing the modules, especially by exploiting the\nintricate dependencies among the internal operations of the\nindividual modules.\nThe central thesis of this paper is to advocate for fine-\ngrained exposition and orchestration of LLM apps in order\nto improve end-to-end performance. Consider an alterna-\ntive representation of the same app workflow (Figure 3a)\nshown in Figure 3b. Instead of working with modules, we\ndecompose each module into fine-grained primitives as the\nbasic unit of orchestration (i.e. nodes in the graph). The in-\ndexing module, for example, is decomposed into embedding\ncreation and data ingestion primitives, and query expansion\nis decomposed into prefilling and decoding primitives just\nlike the LLM synthesizing module.\nMoreover, the dependency among these primitives are\nexplicitly captured in this dataflow graph, enabling the ex-\nploration of more sophisticated joint optimizations across\nprimitives and modules. As a simple example, it is apparent\nthat the embedding creation and data ingestion primitives"}, {"title": "2.3 Application-Aware Scheduling and Execution", "content": "Another limitation of current LLM application orchestration\nis the request-level optimization of the backend execution"}, {"title": "3 Design Overview", "content": "3.1 Architecture\nTeola is a novel orchestration framework to optimize the ex-\necution of LLM-based applications with primitive operations\nas the basic unit.\nFigure 5 depicts Teola's architecture. In the offline stage\n\u2460, developers register execution engines for an app, such"}, {"title": "3.2 APIs", "content": "Listing 1 presents a simplified usage example of Teola, high-\nlighting its main components as described below.\nExecution engines. Execution engines handle requests for\nmodels or operations from workflow components (line 5).\nThey can be model-free or model-based. Model-free engines,\nsuch as databases, are primarily CPU-based and do not in-\nvolve DNN models. On the other hand, model-based engines\ncan deploy various DNN models, including BERT-family\nmodels [25] for embedding and LLMs for generation. A sin-\ngle engine can serve multiple components with different\npurposes, such as the shared LLM engine for query expan-\nsion and LLM synthesizing in Figure 2d."}, {"title": "4 Graph Optimizer", "content": "Graph optimizer generates a fine-grained, per-query repre-\nsentation (p-graph) by combining query information and\nworkflow template. This p-graph, composed of symbolic\nprimitive nodes, enables optimization strategies to produce\nan efficient e-graph for execution."}, {"title": "4.1 p-Graph", "content": "Primitives. Relying solely on high-level components, as\ndiscussed in \u00a72.2, can oversimplify the intricate relationships\nbetween operations and expose limited information and flex-\nibility. To address this, we introduce a refined abstraction:\nthe task primitive (primitive for short). Akin to the operation\nnodes in TensorFlow [18], symbolic primitives at the work-\nflow level enhance granularity in representation and provide\nvaluable information for optimization prior to execution.\nSpecifically, as shown in Table 1, a primitive can corre-\nspond to the functionality of a standard operation within\na registered execution engine (e.g., embedding creation in\nembedding engines or context ranking in reranking engines)\nor represent a fine-grained decomposed operation. For in-\nstance, LLM inference is decomposed into Prefilling and\nDecoding, with Partial Prefilling and Full Prefilling\nconstituting LLM prefilling, and Partial Decoding manag-\ning different parts of full decoding. Additionally, primitives\ncan be control flow operations such as aggregation or con-\nditional branching (i.e., Aggregate and Condition). Each\nprimitive includes a metadata profile detailing its inputs, out-\nputs, parent nodes, and child nodes, forming the basis for\ngraph construction. This profile also contains key attributes\nsuch as batch size for DNNs or prompts for LLMs, as well as\nthe target execution engine.\np-Graph construction. The optimizer converts the orig-\ninal workflow template $T = (T_N, T_E)$ with query-specific\nconfiguration $C = (T_N, C_N)$ into a more granular p-graph\n$G = (V_N, V_E)$ as outlined in Algorithm 1, where $T_N$ repre-\nsents components, $T_e$ dependencies, and $C_N$ user configu-\nrations. The process decomposes each template component\ninto explicit symbolic primitives based on the configuration,\ncreating a sub-primitive-level graph with well-defined de-\npendencies. For instance, the LLM synthesizing module in\nrefine mode with 3 context chunks is transformed into a sub-\ngraph where 3 pairs of Prefilling and Decoding primitives\nare chained and configured with corresponding metadata.\nThe final resulting p-graph preserves the original workflow"}, {"title": "4.2 Optimization", "content": "As mentioned in \u00a72.2, Teola focuses on maximizing paral-\nlelism in distributed execution rather than single-point op-\ntimization or acceleration (orthogonal and discussed in \u00a79).\nSpecifically, the optimizer identifies opportunities for prim-\nitive parallelism (parallelization) and pipeline parallelism\n(pipelining), employing a set of static, rule-based optimiza-\ntions.\nExploitable opportunities. Firstly, the original dependen-\ncies inherited from the workflow template, which only de-\npict a high-level sequence of components, may introduce\nredundancy in the fine-grained p-graph. To maximize paral-\nlelization, it is essential to analyze and prune unnecessary\ndependencies, thereby freeing independent primitives and\ncreating parallel dataflow branches (Pass 1). Additionally,\ncompute-intensive primitives can be broken down into mul-\ntiple pipelining stages, where feasible, enabling them to be\nexecuted concurrently with subsequent primitives (Pass 2).\nFurthermore, we have observed that the core of the work-\nflow, the LLM, has exploitable special attributes. Specifically,\ntwo key attributes can be leveraged: (1) causal prefilling: This\nallows the LLM's prefilling to be split into dependent parts,\nenabling parallelization of partial prefilling with preceding\nprimitives (Pass 3), and (2) streaming decoding output: The\nauto-regressive and partial output of specific LLM decoding\ncan be pre-communicated as input to the downstream primi-\ntives, creating additional pipelining opportunities (Pass 4).\nOptimization passes. Based on the above analysis, the fol-\nlowing optimization passes are integrated and can be applied\nto the p-graph to optimize end-to-end workflow execution:"}, {"title": "5 Runtime Scheduling", "content": "Teola utilizes a two-tier scheduling mechanism at runtime.\nThe upper-tier graph scheduler dispatches primitive nodes\nof each query's optimized e-graph. The lower tier consists\nof engine schedulers that manage engine instances and fuse\nprimitive requests from queries for efficient execution. Sepa-\nrating graph scheduling and operation execution enhances\nscalability and extensibility for Teola."}, {"title": "5.1 Graph Scheduler", "content": "The graph scheduler closely tracks the status of each query's\ne-graph and issues primitive nodes as their dependencies\nare met. It evaluates node in-degrees and dispatches nodes\nto the appropriate engine scheduler when in-degrees reach\nzero. Note that the graph scheduler dispatches the node itself\nrather than its associated requests, ensuring that the lower\nscheduler can identify requests originating from a primitive,\ninstead of treating them independently like in existing frame-\nworks (see \u00a72.3). Upon completion of a primitive's execution,\nthe scheduling thread is notified via RPC calls, and the output\nis transferred. The thread then decrements the in-degrees of\ndownstream primitives, preparing them for execution."}, {"title": "5.2 Engine Scheduler", "content": "Execution engine instances are managed by dedicated en-\ngine schedulers, enabling independent execution of primitive\nnodes mapped to different engine types. The main challenge\nis efficiently fusing primitives that request the same engine.\nWith an optimized e-graph, a query may dispatch multiple\nprimitive nodes simultaneously to an engine scheduler or\nhave several pending primitive nodes in the queue, especially\nwhen components share the same engine, such as the proxy\nand judge modules in Figure 2a or the query expansion and\nLLM synthesizing modules in Figure 2d using the same LLM.\nStrawman solution and limitation: blind batching. A\nnaive approach to handling diverse primitive nodes is to treat\nthem uniformly. A engine scheduler dynamically batches\nassociated primitive requests from the pending queue using"}, {"title": "6 Implementation", "content": "We implement the prototype of Teola with ~5,300 lines of\ncode in Python. Specifically, we leverage several existing\nlibraries: (1) Ray [52] for distributed scheduling and execu-\ntion; (2) LlamaIndex [1] for pre-processing tasks, such as text\nchunking and HTML/PDF parsing; (3) postgresql [15] as the\ndefault database; (4) pgvector [14] as the vector search en-\ngine; (5) Google custom search [7] as the search engine, sup-\nporting both single and batched requests; and (6) vLLM [39]\nas the LLM serving engine, which we additionally modify\nto support Partial Prefilling and Full Prefilling in\nTable 1.\nFor the frontend, we provide user interfaces via FastAPI [5]\nfor submitting queries and user configurations. For the back-\nend, the graph scheduler maintains a thread pool to allocate\na dedicated thread for each new query, in order to construct,\noptimize and dispatch the e-graph. Beyond the discussion\nin \u00a75, each engine scheduler also manages load balancing\nacross different instances based on various load metrics \u2013 pri-\nmarily the number of executed requests for general engines\nand the occupied KV cache slots for LLMs.\nMitigating communication overhead. To reduce commu-\nnication overhead in a central scheduler, we use a dependent\npre-scheduling mechanism for adjacent primitives with large\ndata interactions or the same execution engine. This allows\nsimultaneous issuance of two dependent primitives, namely\nA and B, with B waiting for the output of A. Along with\nsending A's result to the scheduler, an RPC call also sends\nthe output of A directly to the execution engine of B. This\navoids relaying results through the scheduler before issuing\nB, and hence can reduce the communication overhead."}, {"title": "7 Evaluation", "content": "Testbed setup. We allocate model-based engines (e.g., LLMs)\nand model-free engines with GPUs and CPU-only resources,\nrespectively. Each engine instance used for embeddings or\nother non-LLM models are each hosted on a single NVIDIA\n3090 24GB GPU. For LLMs, each instance of llama-2-7B and\nllama-2-13B [60] is deployed on 1 and 2 NVIDIA 3090 GPUs,\nrespectively. Each instance of llama-30B [59] is deployed\non 2 NVIDIA A800 80GB GPUs. The network bandwidth\nbetween any physical servers is 100 Gbps.\nBaseline. To our knowledge, few studies have specifically\nfocused on optimizing LLM-based workflows in distributed\nsettings. Therefore, we compare Teola with the following\nframeworks based on LlamaIndex [1]:\n\u2022 LlamaDist: a distributed version of LlamaIndex that we\nimplemented with Ray, defining a chain of task modules\nto construct an application pipeline. Each task module\ninvokes requests to different distributed backend engines.\nThis implementation integrates Ray with LlamaIndex's or-\nchestration approach, utilizing the same engines as Teola\nbut differing in the granularity of orchestration.\n\u2022 LlamaDistPC (parallel & cache-reuse): an advanced Lla-\nmaDist variant that examines the predefined pipeline and\nmanually parallelizes independent modules for concurrent\nexecution. It also incorporates prefix caching for LLM to"}, {"title": "7.1 End-to-end Performance", "content": "We evaluate the performance with different schemes for\nvarious apps, all under the same resource allocation. Each\nnon-LLM engine is provisioned with a single instance, while\neach LLM is provisioned with two instances.\nSearch engine-empowered generation. Figure 8 (top row)\nshows Teola outperforming the other four schemes by up\nto 1.79x. Teola's efficiency is attributed to parallelizable par-\ntial prefilling for instructions and questions for both the\njudge and core LLM, and effective batching coordination for\ndifferent engines. In contrast, LlamaDist executes modules\nsequentially and struggles with request scheduling for mul-\ntiple queries. PO's focus on per-invocation latency results\nin longer queue times under high request rates, while TO\ngenerally performs better in these scenarios. LlamaDistPC\nfails to benefit from parallelization due to the lack of ex-\nplicit parallelization across modules. Its prefix caching for\npartial instructions (typically around 60 tokens) provides\nlimited benefit, as prefix caching is most advantageous when\nprefixes are significantly longer [2, 71].\nDocument QA with naive RAG. Figure 8 (middle row)\ndemonstrates that Teola outperforms the other four schemes\nby up to 1.62x at low request rates and 1.46x at high rates.\nLlamaDist executes modules sequentially without specific\noptimizations while LlamaDistPC enables limited paralleliza-\ntion (indexing and query embedding modules) and partial\ninstruction KV cache reuse, performing slightly better than\nLlamaDist. Regarding scheduling, PO is better than TO at\nlow request rates due to its focus on per-invocation latency,"}, {"title": "7.2 Ablation Study", "content": "We show the effectiveness of Teola's main components from\ngraph optimization and runtime scheduling perspectives.\nGraph optimization. As shown in Figure 9, we compare\nthe performance of Teola under different scenarios, i.e., with\nor without parallelization (Pass 1 & 3) and pipelining (Pass 2"}, {"title": "7.3 Overhead Analysis", "content": "To demonstrate that the overhead incurred by Teola, we pro-\nvide a breakdown of latency by profiling the different parts\nin the real critical path of execution. This analysis covers doc-\nument QA with advanced RAG on the TruthfulQA dataset at\nvarying request rates. It includes latency measurements for\ngraph optimization, communication, queuing, and primitive\nexecution. The results clearly show that the graph optimiza-\ntion overhead is minimal (1.3%~3%) with optimization cache\nreuse, and the communication overhead is low (3.1% ~6.2%)\ncompared to the total latency. As the request rate increases,\nmore latency is attributed to the queuing time for certain\noperations. These indicate that Teola incurs negligible over-\nhead."}, {"title": "8 Limitations and Future Work.", "content": "Dynamic graph. While Teola's ahead-of-time graph opti-\nmization offers benefits, adapting to dynamic patterns like\ngeneration with reflection [49], iterative retrieval in RAG [48],\nand agent-determined workflows [38, 56] is challenging due\nto their unpredictable patterns and the difficulty of capturing\nthe entire primitive-level graph prior to execution."}, {"title": "9 Related Work", "content": "LLM inference optimization. LLM inference has garnered\nsignificant attention, with numerous studies focusing on\nvarious optimization directions, including kernel accelera-\ntion [24, 28, 65], request scheduling [19, 20, 57, 68], model\nparallelism [41, 51, 72], semantic cache [21, 73], KV cache\nmanagement [39, 42, 62], KV cache reusing [27, 36, 39, 46,\n71] and advanced decoding algorithms [47, 50, 53]. Recent\nworks [31, 54, 72] disaggregate the deployment of the pre-filling and decoding phases to increase goodput. This philos-ophy aligns well with Teola's decomposition approach and\ncould be seamlessly integrated. While most works provide\npoint solutions in the LLM domain, Teola takes a holistic\nview, optimizing the entire application workflow and facil-\nitating cooperation among diverse components. Thus, the\noptimizations in LLM inference would complement Teola's\nefforts.\nFrameworks for LLM-based applications. Apart from\nframeworks like [1, 2, 9], several studies [37, 38, 43, 71]\nfocus on optimizing complex LLM tasks involving multi-\nple LLM calls. They explore opportunities such as paral-\nlelism and sharing by developing specific programming in-\nterfaces or compilers. Moreover, several AI agent frameworks\n[10, 29, 30, 37, 43, 56, 63] enable LLMs to autonomously con-\ntrol workflows, make decisions, select tools, and interact\nwith other LLMs, reducing the need for human intervention\nbut introducing specific challenges. Teola is more similar"}, {"title": "10 Conclusion", "content": "We present Teola, a fine-grained orchestration framework for\nLLM-based applications. The core idea is orchestration using\nprimitive-level dataflow graphs. This explicitly exposes the\nattributes of primitive operations and their interactions, en-\nabling natural exploration of workflow-level optimizations\nfor parallel execution. By leveraging the primitive relation-\nships from the graph, Teola employs a topology-aware batch-\ning heuristic to intelligently fuse requests from primitives for\nexecution. Testbed experiments demonstrate that Teola can\noutperform existing schemes across different applications."}]}