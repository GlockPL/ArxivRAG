{"title": "Advancing On-Device Neural Network Training with TinyPropv2: Dynamic, Sparse, and Efficient Backpropagation", "authors": ["Marcus R\u00fcb", "Axel Sikora", "Daniel Mueller-Gritschneder"], "abstract": "This study introduces TinyPropv2, an innovative algorithm optimized for on-device learning in deep neural networks, specifically designed for low-power microcontroller units. TinyPropv2 refines sparse backpropagation by dynamically adjusting the level of sparity, including the ability to selectively skip training steps. This feature significantly lowers computational effort without substantially compromising accuracy. Our comprehensive evaluation across diverse datasets-CIFAR 10, CIFAR100, Flower, Food, Speech Command, MNIST, HAR, and DCASE2020-reveals that TinyPropv2 achieves near-parity with full training methods, with an average accuracy drop of only around 1% in most cases. For instance, against full training, TinyPropv2's accuracy drop is minimal, for example, only 0.82% on CIFAR 10 and 1.07% on CIFAR100. In terms of computational effort, TinyPropv2 shows a marked reduction, requiring as little as 10% of the computational effort needed for full training in some scenarios, and consistently outperforms other sparse training methodologies. These findings underscore TinyPropv2's capacity to efficiently manage computational resources while maintaining high accuracy, positioning it as an advantageous solution for advanced embedded device applications in the IoT ecosystem.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning has revolutionized the landscape of machine learning and artificial intelligence, enabling significant advancements across numerous applications such as image recognition, natural language processing, and autonomous systems. Central to this revolution is the ability to train deep neural networks (DNNs) effectively. However, as network architectures become deeper and datasets grow larger, the computational burden of training these models using traditional backpropagation algorithms has surged, often outstripping the capabilities of low-power, embedded devices. [1] Embedded systems, particularly microcontroller units (MCUs), are ubiquitous in the Internet of Things (IoT) applications, where on-device learning offers a plethora of benefits, including privacy preservation, reduced latency, and decreased reliance on continuous cloud connectivity. However, the limited computational and memory resources of such devices pose a significant challenge for deploying sophisticated DNNs.\nIn response to this challenge, sparse backpropagation algorithms have emerged as an attractive solution, optimizing the training process by selectively updating a subset of the model's weights. Yet, the static nature of the backpropagation ratio in existing approaches often results in a precarious balance between computational efficiency and model accuracy.\nBuilding on previous results TinyProp [2], this paper introduces TinyPropv2, an enhanced algorithm that dynamically adjusts the backpropagation ratio during the training process. TinyPropv2 extends this dynamic adaptability further by incorporating a decision-making process that can skip entire training steps for certain datapoints when they are deemed unnecessary, thereby reducing computational effort without significantly impacting accuracy. Through rigorous experimentation and analysis, we demonstrate that TinyPropv2 not only conserves computational resources but also provides a safeguard against overfitting, thus representing a significant step forward in the quest for efficient and effective on-device learning.\nThis introduction provides an overview of the challenges in on-device learning and positions TinyPropv2 as a novel contribution by reduce the computational demands of back-propagation.\nThe remainder of the paper is structured as follows: Section II reviews related work and contextualizes TinyPropv2 within the landscape of sparse backpropagation methods. Section III details the methodology underlying TinyPropv2, elucidating its innovative approach to adaptive backpropagation. Section IV presents the experimental setup and datasets utilized, ensuring reproducibility and a comprehensive understanding of the evaluation context. Section V discusses the results, highlighting the accuracy and computational efficiency of TinyPropv2 across various datasets. Finally, Section VI concludes the paper and outlines directions for future work, underscoring the potential impact of TinyPropv2 on the broader domain of machine learning and embedded systems."}, {"title": "II. RELATED WORK", "content": "The evolution of on-device learning, a cornerstone in IoT applications, pivots around the inefficiencies of traditional offline training and deployment models, which often fail to adapt to real-time data distribution shifts [3]. Continual learning offers a solution by focusing on acquiring knowledge step-by-step, much like how humans learn, while also addressing the issue of catastrophic forgetting, where a model loses previously learned information [4, 5, 6].\nA critical challenge in this field is the limitation of computational and memory resources on embedded devices, especially during backpropagation. Approaches to mitigate these challenges bifurcate into enhancing architecture efficiency and implementing sparse updates. Sparse updates, particularly, have gained attention for reducing the memory"}, {"title": "III. METHODOLOGY", "content": "Deep learning, especially in the context of deep neural networks (DNNs), has shown remarkable success in various applications. However, the training process, which involves forward and backward propagation, can be computationally intensive. This section introduces the standard forward and backward propagation methods and explains the concept of sparse backpropagation, the basis for the original TinyProp algorithm. We then detail the advancements made in our enhanced version, TinyPropv2."}, {"title": "A. Forward and Backward Propagation", "content": "Deep Neural Networks (DNNs) primarily consist of a series of layers where each layer transforms its input data to"}, {"title": "1) Forward Propagation:", "content": "For a given layer l, the forward propagation can be mathematically represented as:\n$a^{l+1} = f (z^l) = f (W^l a^l + b^l)$                                          (1)\nwhere:\n\u2022\n$W^l$ is the weight matrix for layer l.\n\u2022\n$b^l$ is the bias vector for layer l.\n\u2022\n$z^l$ represents the weighted sum of inputs for layer l.\n\u2022\n$a^l$ is the activation (output) of layer l.\n\u2022\n$f$ is a non-linear activation function."}, {"title": "2) Backward Propagation:", "content": "Once the network produces an output, the error or loss is computed. This loss is then used to update the weights of the network to improve its predictions. The process of computing the gradient of the loss with respect to the network's weights and biases is termed as backward propagation.\nThe loss L is given by:\n$L(a^L, y)$                                                  (2)\nwhere y is the ground truth and $a^L$ is the output of the final layer L.\nThe gradient of the loss with respect to the pre-activation $z^L$ of the last layer is:\n$\\nabla_{z^{L}} = \\frac{\\partial L}{\\partial z^{L}} = (\\frac{\\partial L}{\\partial a^{L}}\\frac{\\partial a^{L}}{\\partial z^{L}})^T = f'(z^L) \\odot \\nabla_{a^{L}}L$                         (3)\nwhere $f'$ is the derivative of the activation function and $\\odot$ represents the Hadamard (element-wise) product."}, {"title": "B. Sparse Backpropagation", "content": "Sparse backpropagation is an optimization technique that aims to reduce the computational complexity of the stan- dard backpropagation algorithm. Instead of updating all the weights in the network, sparse backpropagation updates only a subset of them, specifically those with the largest gradients. This approach is based on the observation that only a few weights, which have the most significant gradients, contribute the most to the learning process. [15]"}, {"title": "1) Gradient Sparsity:", "content": "Given a gradient vector $\\delta a$ for layer l, the top-k elements based on their magnitude can be represented as:\n$top(\\delta a, k)$                                                (4)\nFor instance, for a gradient vector $v = [1, 2, 3, -4]^T$, the top-2 elements would be represented as $top(v, 2) = [0, 0, 3, -4]^T$."}, {"title": "2) Approximate Gradient:", "content": "The approximate gradient is then computed by retaining only the top-k elements and setting the rest to zero:\n$\\delta a_{i}= \\begin{cases} \\delta a_{i} \\text { if } a \\in\\left\\{t_{1}, t_{2}, \\ldots, t_{k}\\right\\} \\\\ 0 \\text { otherwise } \\end{cases}$                            (5)\nThis approximation ensures that only the most significant gradients contribute to the weight updates, leading to a reduction in computational effort."}, {"title": "3) Sparse Gradient Propagation:", "content": "Using the approximate gradient, the backpropagation is modified as:\n$\\delta = top(\\delta a, k)$\n(6)\n$\\delta= \\delta \\odot f'(z^l)$\n(7)\n$\\delta^{l-1}= W^{l-1}\\delta$                                    (8)\nWhere $f'$ is the derivative of the activation function.\nBy employing this sparse approach, the backpropagation algorithm becomes more efficient, especially for deep networks with a large number of parameters."}, {"title": "C. The TinyProp Algorithm", "content": "The TinyProp algorithm enhances the efficiency of training deep neural networks (DNNs) by implementing a dynamic sparse backpropagation approach. This method is particularly effective for on-device learning on tiny, embedded devices such as low-power microcontroller units (MCUs). [2]"}, {"title": "1) TinyProp Adaptivity Approach:", "content": "The core innovation of TinyProp lies in its adaptivity, where the algorithm dynamically calculates the proportion of gradients to be updated for each layer during training. This adaptivity is based on the local error in each layer, enabling the algorithm to focus computational resources more effectively."}, {"title": "a) Local Error Vector and Total Error:", "content": "The adaptivity mechanism in TinyProp uses the local error vector $\\delta a_{i}$ of each layer to gauge the layer's contribution to the overall error. The total error characteristic of the layer $\\gamma^l$ is computed as the sum of the absolute values of these error components:\n$\\gamma^l = \\sum_{i=1}^{N} \\delta a_{i} $                                    (9)"}, {"title": "b) Adaptive Error Propagation Rate:", "content": "TinyProp introduces an adaptive error propagation rate $S^l$, which reflects the training progress and is calculated as a function of the total error:\n$S^l = S_{min} + \\gamma^l \\frac{S_{max} - S_{min}}{\\gamma_{max}}$                      (10)\nwhere $S_{max}$ and $S_{min}$ are user-defined bounds for the error propagation rate."}, {"title": "c) TinyProp Damping Factor:", "content": "To address the computational intensity in larger DNN layers, TinyProp incorporates a damping factor $\\Psi(l)$ that reduces the error propagation rate in a layer-dependent manner:\n$S^l = S_{min} + (\\Psi(l)\\gamma^l) \\frac{S_{max} - S_{min}}{\\gamma_{max}}$                         (11)\nThis factor allows for reduced computational effort in layers that typically require less training, such as the initial layers of the network."}, {"title": "d) Computing the Adaptive Top-k:", "content": "With the calculated error propagation rate $S^l$, TinyProp determines the number of gradients to update in each layer adaptively:\n$k^l = S^l \\cdot N$                                                       (12)"}, {"title": "e) TinyProp Backpropagation Algorithm:", "content": "The backpropagation step in TinyProp is then conducted using the adaptive top-k approach:\n$\\delta a = top(\\delta a^l, k^l)$\n$\\delta = \\delta \\odot f'(z^l)$\n$\\delta^{l-1} = (W^{l-1})^T\\delta$\n(13)\nThrough this methodology, TinyProp efficiently manages computational resources while maintaining the efficacy of the learning process, making it highly suitable for embedded applications."}, {"title": "D. The Enhanced TinyPropv2 Algorithm", "content": "Building upon the original TinyProp algorithm, TinyPropv2 introduces an additional layer of decision-making to potentially skip entire training steps when beneficial, see Fig. 1. The primary motivation for incorporating this additional decision layer stems from the need to enhance computational efficiency without sacrificing the model's accuracy. Traditional training methods often expend significant computational resources on processing every data point, regardless of its actual impact on the model's learning. TinyPropv2, by contrast, intelligently identifies and focuses on data points that substantially contribute to the learning process. This targeted approach ensures that computational efforts are allocated more judiciously, leading to a more efficient training cycle. This approach further reduces computational effort while maintaining the balance between efficiency and accuracy."}, {"title": "1) Decision Mechanism for Training Data Points:", "content": "The decision to train a specific data point in TinyPropv2 is based on a novel decision metric, $D^L$, which assesses the necessity of performing backpropagation for that data point:\n$D^L = (D_{min} + a^L \\frac{D_{max} - D_{min}}{a_{max}}) \\times B_L$                     (14)\nWhere:\n\u2022\n$D^L$ is the decision metric for the last layer L.\n\u2022\n$D_{min}$ and $D_{max}$ are the minimum and maximum thresholds for the decision metric.\n\u2022\n$a^L$ is a factor based on the current state of training at layer L.\n\u2022\n$B_L$ is a scaling factor to adjust the sensitivity of the decision metric.\nIf $D^L$ exceeds a certain threshold, such as 0.5, backpropagation is performed; otherwise, it is skipped. This selective approach prioritizes data points that are more impactful for learning, enhancing efficiency."}, {"title": "a) Threshold Determination::", "content": "The threshold against which $D^L$ is compared is not arbitrarily set but is carefully chosen based on empirical observations and domain-specific requirements. For instance, a threshold value of 0.5 is commonly used as a starting point. However, this threshold can be adjusted according to the characteristics of the dataset and the specific learning goals."}, {"title": "2) Benefits of TinyPropv2 Over TinyProp:", "content": "TinyPropv2 extends the capabilities of the original TinyProp algorithm by:\n\u2022\nReducing Computational Load: By intelligently deciding whether to perform backpropagation for each data point.\n\u2022\nEnhanced Adaptability: Offers more refined control over the training process, suitable for various types of datasets and learning scenarios.\n\u2022\nResource Optimization: Particularly beneficial for MCUs and embedded devices where computational resources are limited."}, {"title": "E. Pseudocode for the TinyPropv2 Algorithm", "content": "The following pseudocode outlines the steps involved in the TinyProp and TinyPropv2 algorithms, highlighting the dynamic sparse backpropagation approach and the decision mechanism unique to TinyPropv2."}, {"title": "IV. EXPERIMENT SETUP", "content": "The experimental validation of the TinyPropv2 algorithm was conducted on a heterogeneous set of open-source datasets, chosen for their prevalence in benchmarking within the machine learning community as well as their representation of diverse application domains."}, {"title": "B. Models and Architectures", "content": "Our experiments leveraged the MobileNetV2 architecture [30], renowned for its efficiency on mobile devices, and a bespoke 5-layer neural network tailored to the dataset modality-employing either 1D or 2D convolutions for time- series and image data, respectively. These models were initially pretrained on the ImageNet dataset to establish a foundational knowledge before being fine-tuned for our specific datasets."}, {"title": "C. Computational Environment", "content": "A critical aspect of evaluating the efficacy of any machine learning algorithm, particularly those designed for on-device deployment, is the computational environment in which the experiments are performed. For our experiments, we selected a controlled computational setting that would reflect the constraints and capabilities of high-performance embedded systems."}, {"title": "V. RESULTS", "content": "The evaluation of the TinyPropv2 method involved a dual- focus analysis: first, we assessed the accuracy of the method across various datasets; second, we examined the computational effort required during training. This two-pronged approach enabled us to investigate not only the effectiveness of the model in terms of learning capabilities but also its efficiency, which is crucial for deployment on resource- constrained devices."}, {"title": "A. Accuracy", "content": "Our accuracy assessment revealed that TinyPropv2 competently navigated the trade-off between model complexity and learning accuracy shown in Tab. I. In datasets with higher-dimensional data and more complex structures, such as CIFAR-100 and DCASE2020, TinyPropv2 demonstrated a remarkable capacity to maintain high accuracy levels, rivaling the full training baseline without necessitating extensive computational resources.\nIn simpler datasets like MNIST, the accuracy advantage of TinyPropv2 over other methods became less pronounced, suggesting that its benefits are more significant in scenarios where the learning task inherently involves more complexity and where discerning the salient features from the data is more challenging."}, {"title": "B. Computational Effort", "content": "One of the notable features of TinyPropv2 is its ability to skip training on certain datapoints as the model becomes more competent. This approach effectively combats overfitting by reducing the training intensity as the model's accuracy improves. Consequently, as the model training progresses and the algorithm identifies less error-prone data, the computational effort required diminishes significantly.\nThe computational effort analysis, as depicted in the accompanying bar chart, shows that TinyPropv2 rapidly decreases its computational load relative to other methods. Initially, the effort aligns closely with that of Sparse Update and TinyTrain approaches. However, as training progresses and the algorithm becomes more selective in the datapoints it deems necessary to train on, we observe a steeper decline in computational effort for TinyPropv2.\nThis behavior underscores the method's adaptability and responsiveness to the learning progress, offering an efficient training process that dynamically adjusts to the model's evolving state of knowledge. It affirms TinyPropv2's potential as a scalable solution for on-device learning, where computational resources are often at a premium and must be judiciously allocated."}, {"title": "C. Comparative Analysis", "content": "The comparative analysis of computational effort required for different training methods is illustrated in Figure 2. As shown, TinyPropv2 starts on par with other methods but as training continues, the effort required for TinyPropv2 decreases more steeply. This is due to its unique ability to skip redundant datapoint training, thereby reducing unnecessary computations. As a result, TinyPropv2 not only conserves computational resources but also mitigates the risk of overtraining, striking a desirable balance between learning efficiency and model performance."}, {"title": "D. Implications for On-device Learning", "content": "The implications of these results are significant for on-device machine learning applications. TinyPropv2's intelligent computation management makes it particularly suited for environments where power and processing capabilities are limited. By minimizing computational overhead without compromising on learning outcomes, TinyPropv2 ensures that devices such as smartphones, IoT sensors, and other embedded systems can perform complex learning tasks autonomously.\nThe results from this study provide a compelling case for the adoption of TinyPropv2 in on-device learning scenarios. Its dynamic adjustment to the training process not only optimizes the computational load but also enhances the"}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "The comprehensive experimental analysis conducted in this study leads us to several important conclusions about the TinyPropv2 algorithm's capabilities and potential applications. TinyPropv2 demonstrates a notable advancement in on-device learning, offering a method that judiciously uses computational resources while still achieving high levels of accuracy across a range of datasets and learning tasks."}, {"title": "A. Conclusions", "content": "Our results confirm that TinyPropv2 can effectively reduce the computational effort required during the training process by selectively skipping datapoints that do not contribute significantly to model improvement. This strategic reduction in training intensity does not only conserve energy and computational resources but also presents a lessened risk of overfitting, a common pitfall in machine learning endeavors. TinyPropv2's performance was particularly impressive in complex and high-dimensional datasets, where it successfully approached the upper accuracy limits set by full training baselines. This finding underscores the algorithm's suitability for complex learning tasks that are characteristic of real- world applications, ranging from image and speech recognition to sensor data analysis."}, {"title": "B. Future Work", "content": "The promising results obtained with TinyPropv2 open several avenues for future research. One immediate direction is the exploration of TinyPropv2's performance on an even wider array of datasets, including those with unstructured data or in unsupervised learning settings. Additionally, further optimization of the algorithm's hyperparameters could yield even more efficient training processes and higher accuracies.\nAnother important area of future work involves the deployment of TinyPropv2 on actual embedded systems. Real- world testing will provide invaluable insights into the algorithm's performance in the field and its interaction with hardware limitations.\nFurthermore, extending TinyPropv2 to support federated learning environments could significantly enhance its utility. In such settings, the algorithm's efficiency in computation and communication would be paramount, enabling robust learning across distributed devices with minimal data exchange.\nLastly, the integration of reinforcement learning principles could provide mechanisms to further refine the decision- making process behind the selective updating of the model. This would allow TinyPropv2 to dynamically adapt to changing environments and tasks, making it even more versatile and powerful for on-device learning applications."}, {"title": "C. Implications", "content": "The implications of this research are twofold: not only does it contribute to the theoretical understanding of efficient on- device learning, but it also provides a practical framework that can be readily applied in various industries. From consumer electronics to industrial IoT, the applications of TinyPropv2 are vast and impactful, making it a significant contribution to the field of machine learning.\nIn conclusion, TinyPropv2 stands as a highly effective tool for on-device machine learning, balancing the dual demands of computational efficiency and learning accuracy. Its continued development and adaptation will undoubtedly contribute to the advancement of edge computing and the realization of truly intelligent devices."}]}