{"title": "Swiss DINO: Efficient and Versatile Vision Framework for On-device Personal Object Search", "authors": ["Kirill Paramonov", "Jia-Xing Zhong", "Umberto Michieli", "Jijoong Moon", "Mete Ozay"], "abstract": "In this paper, we address a recent trend in the robotic home appliances to include vision systems on personal devices, capable of personalizing the appliances on the fly. In particular, we formulate and address an important technical task of personal object search, which involves localization and identification of personal items of interest on images captured by robotic appliances, with each item referenced only by a few annotated images. The task is crucial for robotic home appliances and mobile systems, which need to process personal visual scenes or to operate with particular personal objects (e.g., for grasping or navigation). In practice, personal object search presents two main technical challenges. First, a robot vision system needs to be able to distinguish between many fine-grained classes, in the presence of occlusions and clutter. Second, the strict resource requirements for the on-device system restrict usage of most state-of-the-art methods for few-shot learning, and often prevent on-device adaptation. In this work we propose Swiss DINO: a simple yet effective framework for one-shot personal object search based on the recent DINOV2 transformer model, which was shown to have strong zero-shot generalization properties. Swiss DINO handles challenging on-device personalized scene understanding requirements and does not require any adaptation training. We show significant improvement (up to 55%) of segmentation and recognition accuracy compared to the common lightweight solutions, and significant footprint reduction of backbone inference time (up to 100x) and GPU consumption (up to 10\u00d7) compared to the heavy transformer-based solutions.", "sections": [{"title": "I. INTRODUCTION", "content": "Computer vision has a pivotal role in mobile systems and home appliances to understand the surroundings and navigate in complex environments. Scene understanding deep neural networks have obtained outstanding results and have been successfully deployed to mass-accessible personal devices: for example industrial or domestic service robots (e.g., vacuum cleaners), assistive robots, and smartphones.\nRecently, increasing attention [1]\u2013[3] has been devoted on the personalization of on-device AI vision models to tackle a variety of practical use cases. In this work, we focus on personal item search, whereby we want robot vision systems to localize and recognize personal user classes (or fine-grained classes, e.g. my dog Archie, her dog Bruno, my favorite cup, your favorite flower, etc.) on scenes. Specifically, a user provides a small number of reference images with location annotations (either a segmentation map or a bounding box) for each personal item. Then, given a new scene, a visual system needs to i) determine which of the personal objects are present in the scene, and ii) provide the location (in the form of a segmentation map or bounding box) for each of the personal objects present in the scene. This task has found significant applications for personal assistants and service robots: for navigation (e.g., reach my white sofa), HRI (e.g., find my dog Archie), grasping (e.g., bring me my phone), etc.\nPrevious works have focused on different aspects of the task. The closest comparison task to ours is the few-shot semantic segmentation [4], which aims to segment an object on the scene given a provided reference image and mask.\nIn this paper, we aim to address the following limitations of existing few-shot semantic segmentation methods. First, existing solutions only evaluate the IoU metric for the mask corresponding to the ground truth class on the image, thus not accounting for the multi-class scenario. Second, they require adaptation training on coarse datasets, thus making fine-grained classes indistinguishable in the feature space (which is part of the effect known as the neural collapse [5]). Third, current transformer-based solutions rely on large foundation models (e.g., SAM), which may be too costly for on-device implementation.\nIn this work, we develop a problem statement and metrics for the personal object search task, that is closely related to the practical scenarios. We develop a novel method for the task, which does not rely on coarse dataset training and is very lightweight, allowing seamless implementation on device.\nInspired by works showing the great versatility of the DINOv2 model [6] for downstream tasks [6], [7], we employ DINOv2 as our backbone. Our system is called Swiss DINO, after the Swiss Army Knife, for its incredible versatility and adaptability. Fig. 1 shows our approach and its novelty compared to existing solutions.\nOur evaluation focuses on multi-instance personalization (i.e., adaptation to multiple personal objects) via one-shot transfer on multiple tasks (image classification, object de-tection and semantic segmentation) and datasets (iCubWorld [8] and PerSeg [9]). For one-shot segmentation task, Swiss DINO improves the memory usage by up to 10\u00d7 and back-bone inference time by up to 100\u00d7 compared to few-shot semantic segmentation competitors based on foundational models, while maintaining similar segmentation accuracy, as well as improving segmentation accuracy of lightweight solutions on cluttered scenes by 46%. To evaluate multi-instance identification accuracy, we adopt metrics from open-"}, {"title": "II. RELATED WORKS", "content": "a) Few-Shot Semantic Segmentation: While early works on few-shot semantic segmentation resorted to fine-tuning large parts of models [11]\u2013[14], recent approaches are based on sparse feature matching [15] or training adaptation layers with the prototypical loss [16]-[20]. These latter approaches compute class prototypes as the average embed-ding of all images of a class. The label of a new (query) image is predicted by identifying the nearest prototype vector computed from the training (support) set. The training and evaluation are usually performed on popular segmentation datasets with coarse-level classes (e.g. person, cat, car, chair), namely PASCAL - 5i [21] and COCO \u2013 20i [22].\nRecent advancements in large vision models have led to novel few-shot scene understanding works, especially applied to semantic segmentation, such as PerSAM [9], and Matcher [7]. PerSAM, a training-free approach, uses a single image with a reference mask to localize and segment target concepts. Matcher, utilizing off-the-shelf vision foundation models, can showcase impressive generalization across tasks. On the other hand, both approaches are computationally expensive and not applicable on low-resource devices.\nb) Object Detection Datasets for Robotic Applications: Object detection and fine-grained identification are crucial tasks for robotic manipulators [23]. To boost the development of object detection methods, several datasets have been introduced. In particular, iCubWorld [8] is a collection of images recording the visual experience of the iCub humanoid robot observing personal user objects in its typical envi-ronment, such as a laboratory or an office. CORe50 [24] further enriches the field, offering a new benchmark for continuous object recognition, designed specifically for real-world applications such as fine-grained object detection in robot vision systems. These datasets align with our task as they represent scenarios where few-shot personalization can be used to enhance the robot's ability to recognize new or fine-grained objects, serving as practical representations of the use cases where our method can be applied. It was shown that the common classification architectures trained on coarse-level datasets have low accuracy on aforementioned datasets with fine-grained classes [25] when applied out-of-the-box. This is due to the fact that the fine-grained classes become indistinguishable in the feature space after long training on coarse class classification, part of the effect known as neural collapse [5]. Therefore, fine-tuning [25] or adaptation [16], [19] methods are often employed to separate the feature vectors for fine-grained datasets.\nc) Pre-trained DINOv2 as An All-purpose Backbone: In self-supervised learning (SSL), significant contributions have been made to the development of pre-trained models, such as DINO [26] and DINOv2 [6]. These models have demonstrated remarkable capabilities in feature extraction and object localization, making them highly transferable to our task of few-shot personalization. Sim\u00e9oni et al. presents a method named LOST [27] to leverage pre-trained vision transformer features for unsupervised object localization. Melas-Kyriazi et al. [28] reframed image decomposition as a graph partitioning problem, using eigenvectors from self-supervised networks to segment images and localize objects. These methods not only provide a strong foundation for"}, {"title": "III. PROBLEM STATEMENT", "content": "In this section, we present the problem formulation of personal object search and notation for each of the three stages involved.\n1) Pre-training Stage: The first stage is to pre-train a backbone model on a large dataset. The backbone should provide localization information of objects on an image, and have a strong ability to transfer to new personal classes, in particular avoiding neural collapse of generated features.\n2) On-device Personalization Stage: After the system is implemented on a mobile or robotic device (e.g., a robot vacuum cleaner or service robot), it is shown a few images of personal objects, together with their label (e.g., dog Archie, dog Bruno, my mug, etc.), and a prompt indicating the location of the object on the image, in the form of a bounding box or a segmentation map. Those images are also known in the few-shot literature as support images.\nAlthough our setup can be applied to any number of support images per personal object, to simplify evaluation and notations, for the rest of the paper, we consider the most challenging one-shot setup, i.e., we get a single support image $S_c$ for each personal object index $c = 1,2,...,C$, where C is the number of personal objects.\n3) On-device Open-set Personal Object Segmentation, De-tection, and Recognition: During the on-device inference stage, we are given a new test image Q (also known in the literature as the query image). For this image, we need to determine: i) which personal objects, if any, are present on the image; ii) for each of the personal objects present on the image, find its location in the form of segmentation map or a bounding box.\nMore formally, we define a personal object search method POS by\n$\\text{POS}(Q) := (\\text{oloc}_1(Q), ..., \\text{oloc}_C(Q)),$\n$\\text{oloc}_c :=\\begin{cases} \\text{loc}_c(Q), & \\text{if the object } c \\text{ is present} \\\\ \\text{None}, & \\text{otherwise} \\end{cases}$\nwhere $\\text{loc}_c(Q)$ can take the form of a bounding box or a segmentation map for the object c on the image Q."}, {"title": "IV. METHODOLOGY", "content": "Our Swiss DINO system consists of three main com-ponents: i) patch-level feature map extraction; ii) support feature map processing; iii) query feature map processing. An overview of our Swiss DINO is shown in Fig. 2.\n\nA. Patch-level feature map extractor\nWe utilize a pre-trained transformer-based patch-level fea-ture extractor. Inspired by the previous work [6] on the DINOv2 model, making use of its localization and fine-grained separation capabilities of its feature map, we choose DINOv2 as our transformer backbone (for a comparison between different backbone models, see Section V-F.1).\nThe backbone B takes an image X as an input and pro-duces i) patch-wise feature map $X^F = (X_{1,1},..., X_{N_p,N_p})$, where $N_p$ is the number of patches along each side of the image, and $X_{i,j}$ is the D-dimensional vector corresponding to the (i, j)-th spatial patch on the image; and ii) a D-dimensional class token $X^C$, such that $B(X) = (X^F, X^C)$.\nGiven support images ${S_c}_{c=1}^C$ for each personal-level class and a query image Q, we compute the corresponding feature maps ${S_c^F}_{c=1}^C$ and $Q^F$.\nB. Support feature map processing\nFor each personal class c = 1, 2, ..., C, we apply the same processing steps to the feature map $S^F$. In the following, we drop the index c to make the notation less cluttered.\n1) (optional) Bounding box into segmentation map: If we are given the ground truth bounding box b for the support image S, we consider the union of all patches $P_{i,j}$ that have non-empty intersection with b, denoted by $b^P$, as well as patches bordering $b^P$, denoted by $\\partial b^P$. We then partition a set of corresponding feature vectors {$S_{i,j}^F | P_{i,j} \\in b^P \\cup \\partial b^P$} into $k_s$ clusters using the k-means method (in our implementation, $k_s$ was empirically chosen to be $k_s = 5$), denoting the set of patches in each cluster as $K_r^P$, with r = 1,2,..., $k_s$.\nGiven that the patches from $\\partial b^P$ are outside of the bound-ing box, and thus do not belong to the object of interest, we filter out the patch clusters which contain those 'negative' patches, thus resulting in an (approximate) segmentation map:\n$\\text{seg} = \\cup_{r} \\{ K_r^P | \\partial b \\cap K_r = \\emptyset \\}.$\nThis process allows us to separate the object of interest within the bounding box from the background.\n2) Patch pooling from segmentation map: Given a (ground truth or approximate) segmentation map seg of the support image, we pick the patches $P_{i,j}$ which partially intersect with seg, denoting the set of those patches as segP.\nWe compute the patch prototype with simple average over patches in seg by\n$\\text{proto} := \\text{avg}(S_{i,j}^F | P_{i,j} \\in \\text{seg}^P).$\n3) Adaptive threshold for class prototype: To pick the patches of interest from the query object, we choose a feature distance metric and a threshold to determine patches of inter-est on the query image. As a distance metric between feature vectors, we pick the widely used cosine similarity metric. To determine the distance threshold, we use the information about positive and negative patches on the support image.\nMore concretely, we denote $\\text{seg}^P$ to be a set of patches that have non-empty intersection with the segmentation map seg, and nseg to be a set of patches that have empty intersection with seg. We compute the set of positive patch distances and the set of negative patch distances\n$\\text{pd} = \\{\\text{dist}(S_{i,j}^F, \\text{proto}) | P_{i,j} \\in \\text{seg}^P\\},$\n$\\text{nd} = \\{\\text{dist}(S_{i,j}^F, \\text{proto}) | P_{i,j} \\in \\text{nseg}^P\\}.$"}, {"title": "C. Query feature map processing", "content": "Given a tuple $(\\text{proto}_c, t_c)$ for each personal class c = 1,...,C and query feature map $Q^F$, we use the following steps to find the patches belonging to the objects of interest.\n1) (optional for refined segmentation map) Coordinate-adjusted patch k-means: First, agnostic to the set of support classes, we perform a pre-processing step on the query fea-ture map $Q^F$. To group together the patches corresponding to the same object, we apply k-means to the patch feature vectors. In addition, we augment feature vectors with spacial information to reinforce the connectivity of patch clusters:\n$Q_{i,j}^{F,aug} := \\text{concat}(Q_{i,j}^F, \\frac{a_{co} i}{N_p}, \\frac{a_{co} j}{N_p}),$\nwhere $a_{co}$ is a coordinate scaling factor aiming to control the effect of spatial information on the resulting clusters.\nWe cluster the augmented patch features $Q^{F,aug}$ into $k_Q$ clusters $K_r^Q, r = 1,...,k_Q$ and save those clusters for the segmentation map refinement step later.\n2) Object location candidates: For each personal class c = 1, . . ., C, we find patches in $Q^F$ which are close enough to the class prototype, resulting in a set of patches we denote as psegraw:\n$\\text{pseg}^{raw} := \\{P_{i,j} | \\text{dist}(Q_{i,j}^F, \\text{proto}_c) < t_c\\},$\nwhere dist is the cosine similarity between feature vectors. If psegraw is empty, we choose the patch which is closest to the prototype: psegraw = arg minPij dist($Q_{i,j}^F$, protoc).\nTo account for cluttered scenes with similar objects, we split psegraw into L connected subsets $(pseg_1,...,pseg_L)$, thus generating L candidates for the location of the object c on the image."}, {"title": "3) Calculating class scores", "content": "For each candidate set of patches $pseg_l, l = 1,..., L$, we find the class score via patch prototype distance with support image:\n$score_l := \\text{dist}(\\text{avg}(Q_{i,j}^F | P_{i,j} \\in pseg_l), \\text{proto}_c).$\nWe then choose the candidate $l_{max}$ with the maximum class score as the predicted segmentation map\n$\\text{pseg} := \\cup_{i,j} \\{P_{i,j} | P_{i,j} \\in pseg_{l_{max}}\\},$\nand classification score $score_c := \\text{max}_{l} score_l$.\nFrom the class score $score_c$, we determine whether object c is on the image. Similar to other score-based approaches in open-set classification [10], a classification threshold needs to be selected for a given dataset to control which predicted masks pseg we would accept, and which ones we would reject. In the actual implementation, the classification thresh-old needs to be selected for each scenario empirically, while in this work we measure the capability of the method to separate positive examples from negative examples via score precision metric (see Section V-B).\n4) (optional) Segmentation map refinement: While we can use the pseg as a segmentation map for the object of interest, the map usually covers only part of the object or contains holes. To capture the whole object, we refine the patches from pseg with clusters $K_r^Q$ obtained from the previous step using k-means by\n$\\text{pseg}^{ref} := \\cup \\{K_r^Q | K_r^Q \\cap \\text{pseg} \\neq \\emptyset \\}.$\n5) (optional) Bounding box from segmentation map: We can also generate a detection bounding box using the refined segmentation map $\\text{pseg}^{ref}$ by taking the extreme coordinates of the segmentation map."}, {"title": "V. EXPERIMENTS", "content": "In this section, we describe the datasets used for the evaluation of our framework. We specifically choose datasets which i) contain images of personal objects with different"}, {"title": "VI. CONCLUSION", "content": "In this work, we have introduced a novel problem and metrics for the personal object search task, which is directly related to practical robot vision tasks performed by mobile and robotic systems such as home appliances and robotic manipulators, in which the system needs to localize all present objects of interest in a cluttered scene, where each object is only referenced by a few images.\nTo address this task, we have introduced Swiss DINO that leverages SSL-pretrained DINOv2's feature maps having strong discriminative and localization properties. Swiss DINO presents novel clustering-based segmenta-tion/detection mechanisms to alleviate the need for additional specialized modules for such dense prediction tasks.\nWe compare our framework to common lightweight solu-tions, as well as heavy transformer-based solutions. We show significant improvement (up to 55%) of segmentation and recognition accuracy compared to the former methods, and significant footprint reduction of backbone inference time (up to 100\u00d7) and GPU consumption (up to 10\u00d7) compared to the latter methods, allowing seamless implementation on robotic devices.\nAltogether, this work shows the power and versatility of self-supervised transformer models on personal object search and various downstream tasks. In future work, we plan to extend Swiss DINO for continually learning new generic as well as new personal objects."}]}