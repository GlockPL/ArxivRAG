{"title": "Action-Agnostic Point-Level Supervision for Temporal Action Detection", "authors": ["Shuhei M. Yoshida", "Takashi Shibata", "Makoto Terao", "Takayuki Okatani", "Masashi Sugiyama"], "abstract": "We propose action-agnostic point-level (AAPL) supervision for temporal action detection to achieve accurate action instance detection with a lightly annotated dataset. In the proposed scheme, a small portion of video frames is sampled in an unsupervised manner and presented to human annotators, who then label the frames with action categories. Unlike point-level supervision, which requires annotators to search for every action instance in an untrimmed video, frames to annotate are selected without human intervention in AAPL supervision. We also propose a detection model and learning method to effectively utilize the AAPL labels. Extensive experiments on the variety of datasets (THUMOS '14, FineAction, GTEA, BEOID, and ActivityNet 1.3) demonstrate that the proposed approach is competitive with or outperforms prior methods for video-level and point-level supervision in terms of the trade-off between the annotation cost and detection performance.", "sections": [{"title": "1 Introduction", "content": "Temporal action detection is a vital research area in computer vision and machine learning, primarily focusing on recognizing and localizing human actions and events in untrimmed video sequences. With the rapid growth of video data available online, developing algorithms capable of understanding and interpreting such a wealth of information is critical for a wide range of applications, including anomalous event detection in surveillance videos and sports activity analysis. The existing literature generally tackles action detection problems through fully supervised approaches, which require training data with complete action labels and their precise temporal boundaries. Despite significant progress in recent years, these methods confront considerable challenges due to the high annotation cost to predict actions in complex and diverse video settings accurately. To reduce the annotation cost for temporal action detection, weak supervision, such as video-level supervision and point-level labels, has been studied. However, these types of supervision have their own difficulty in practice.\nVideo-level supervision only uses the action classes pre-sent in the video as labels. Various approaches have been proposed, such as multiple instance learning-based, feature erasing-based, and attention-based, but they ultimately reduce action detection learning to video classification. Behind this strategy is the assumption that the discriminative intervals contributing to classification are where the actions occur, which is not always true. In addition, when a video contains multiple action classes, it becomes a multi-label classification problem, which is extremely difficult. These limitations severely limit the range of applications of video-level supervision.\nPoint-level supervision specifies for each action instance a single arbitrary time point in the instance and the action class and has been actively studied re-cently. While the point-level labels convey partial information about the location of action instances, they do not tell where the actions are not. This is a fundamental difficulty in point-level supervised learning of action detection because localization is to distinguish actions from non-actions. In addition, the requirement that action instances must be exhaustively labeled makes the annotation process expensive.\nTo achieve better trade-off between the annotation costs and detection accuracy, we propose action-agnostic point-level (AAPL) supervision, a novel form of weak supervision for temporal action detection. In producing AAPL labels, a small portion of video frames is sam-pled and presented to human annotators, who label the frames with action categories. Unlike point-level supervision, where annotators need to find all action instances in untrimmed videos, the frames to annotate are selected without human intervention. We also propose a baseline learning protocol exploiting the AAPL labels. Our pro-"}, {"title": "2 Action-Agnostic Point-Level Supervision", "content": "We first explain the annotation pipeline for AAPL supervision (Sec. 2.1). Then, we compare AAPL supervision with other forms of weak supervision qualitatively (Sec. 2.2) and in terms of annotation time (Sec. 2.3). Some notations for AAPL labels are also introduced (Sec. 2.4)."}, {"title": "2.1 Annotation Pipeline", "content": "AAPL supervision is characterized by the two-step annotation pipeline consisting of action-agnostic frame sampling and manual annotation. Action-agnostic frame sampling determines which frames in the training videos to annotate. This can be an arbitrary method that, without any human intervention, selects video frames to annotate. Then, human annotators label the sampled frames with action categories.\nAction-agnostic frame sampling is what distinguishes AAPL supervision from conventional point-level supervision. The previous scheme requires that every action instance in a video be annotated with a single time point. This is a challenging task because human annotators need to search videos for every action instance. By contrast, for AAPL supervision, annotators just annotate the sampled frames with action categories, but they do not need to search for action instances.\nThe simplest examples of action-agnostic frame sampling are regularly spaced sampling and random sampling. The former picks up frames at regular intervals, while the latter selects frames randomly. A strength of these meth-ods is that they are easy to implement, computationally light, and free from any assumption on the videos. As we will see in Sec. 4.4.1, the regular sampling is more preferable than the random one because the latter can result in multiple frames in temporal proximity being se-lected, leading to redundant annotations. We can also consider more sophisticated sampling strategies that take into account the content of the video. For example, we can use a pre-trained feature extractor to compute the feature representations of the frames and then cluster the frames based on the features. The representative frames of each cluster can be selected for annotation. See Sec. 4.4.1 for performance comparison.\nBecause sampling at regular intervals is computation-ally free, it might be suitable as an initial choice. In addition, it involves only one hyper-parameter, the inter-val length, which can be sensibly determined by using prior knowledge about the dataset, e.g., the duration and frequency of action instances. If one has the computa-tional resources, sophisticated methods like clustering-based sampling can be a good alternative, because it can adapt to the dataset characteristics and potentially provide better performance."}, {"title": "2.2 Qualitative Comparison", "content": "Here, we contrast AAPL supervision with other types of supervision."}, {"title": "2.3 Measurement of Annotation Time", "content": "We measured the annotation time for full, video-level, point-level, and AAPL supervision, using a modified ver-sion of the VGG Image Annotator (VIA) [8, 9]. For AAPLsupervision, we sampled frames to annotate at regular in-tervals of 3, 5, 10, and 30 seconds. We had eight workersannotate the videos in BEOID [7], GTEA [10], and THU-MOS '14 [16]. The detailed protocol of this measurementis given in Appendix C.\nTable 2 shows the measured annotation time relativeto the duration of videos, i.e., the minutes it took for oneannotator to annotate a 1-minute video. The previousmethods (\"Full\", \"Video\", and \"Point\") exhibit the ex-pected ordering that full supervision costs the most, andthat video-level supervision costs the least. On the otherhand, the annotation time for AAPL supervision varieswith the intervals and is well-approximated by a linearfunction of the number of labeled frames. This modelingassumes that the annotation time per frame is constant,which is reasonable because the annotation time per frameis dominated by the time to select the action category andis not sensitive to the number of frames to annotate.\nThe annotation time depends on the dataset's character-istics, such as the density (i.e., the number per unit lengthof a video) of action instances and the number of actionclasses occurring in one video. Indeed, these numbers aremuch larger in BEOID and GTEA than in THUMOS '14.As a result, annotating videos in BEOID and GTEA takesover twice as long as annotating those in THUMOS '14 forfull, video-level, and point-level supervision. By contrast,the variation in the annotation time is relatively small forAAPL supervision because AAPL annotation involves lo-cal segments around the frames to label and is insensitiveto global characteristics like density. This property makesit easy to apply AAPL supervision in a variety of datasets."}, {"title": "2.4 Notations", "content": "We introduce notations for AAPL labels. Let V be a set of videos. AAPL labels for a video V \u2208 V are a set LV = {(ti, Yi)}i\u2208[NV] of pairs of a time stamp t\u2081 and an actionlabel yi \u2208 {0,1}C. Here, NV is the number of annotated frames, C is the number of action categories, an actionlabel yi is a 0/1-valued vector with the c-th componentindicating the presence or absence of action of the c-thclass at the time ti, and [K] is the set {1, 2, . . ., K}. Anannotated frame might not belong to any action instance.Such a frame is called a background and labeled y = 0.Also, if multiple action instances of different categoriesoverlap, the frames in the intersection are annotated witha multi-hot vector representing all the action categoriespresent there."}, {"title": "3 AAPL Supervised Learning Method", "content": "This section explains our approach to temporal action detection under AAPL supervision. This includes the action detection pipeline predicting action instances from an input video (Sec. 3.1), the training objectives for the pre-diction model (Sec. 3.2), and the pseudo-labeling strategy to make more effective use of the training data (Sec. 3.3)."}, {"title": "3.1 Action Detection Model", "content": "Our action detection pipeline consists of preprocessing, snippet scoring, and action instance generation, follow-ing previous studies [23], At the preprocessing stage, wedivide an input video into TV non-overlapping segmentsof frames called snippets and apply the transformationto make them fit the snippet scoring model. The snippetscoring model processes an input video V into a predic-tion score sequence PV \u2208 RC\u00d7TV. The prediction scorePV indicates the likelihood of an action of the c-th classoccurring at time t. The action instance generator thenconverts the score sequence into a set of the scored actioninstances {(si, ei, Ci, Pi)}i\u2208[MV] in the video. Here, MV is the number of action predictions in the video, si is thestarting time, ei is the ending time, ci is the action cate-gory, and pi is the confidence score of the i-th prediction.\nThe snippet scoring model comprises a feature extractor, a feature embedder, and two scoring heads, as illustrated inFig. 2. The feature extractor is a pretrained 3D CNN con-verting each preprocessed snippet into a D-dimensionalsnippet feature. We denote the sequence of the snippetfeatures by XV \u2208 RD\u00d7TV. The feature sequence is fur-ther fed into the feature embedder, a temporal convolu-tion layer of the kernel size three followed by the recti-fied linear unit activation, which outputs the embeddedfeature sequence ZV = (z\u00a6,\u2026\u2026\u2026,z) \u2208 RD\u00d7TV. Theembedded features are split into ZY \u2208 RD/2\u00d7TV andZY \u2208 RD/2xTV, which are then input to the two scoringheads. The classification head classifies each snippet andoutputs class-specific classification scores SV \u2208 RC\u00d7TVcalled the temporal class activation sequence (T-CAS).The actionness head calculates class-agnostic scores calledthe actionness sequence AV \u2208 RTV, which represents thelikelihood of a snippet being in an action instance. Bothheads combine a point-wise temporal convolution layerand the sigmoid function. We chose the sigmoid insteadof the softmax function as the activation for the T-CAS be-cause multiple action instances of different classes mightoverlap, in which case the model must predict all theclasses present at each moment as positive. The final pre-diction scores are the product of the two score sequences:\n$P_V = A_V S_V$\nGiven the prediction scores for an action category, theaction instance generator first upsamples the score sequence to match the frame rate of the input video. It thengenerates a set of action candidates by collecting the inter-vals over which the prediction scores are above a thresh-old pred. This process is repeated with several differentthresholds. Then, for each action candidate, it calculatesthe outer-inner contrastive score [39] as the initial confi-dence score. Finally, soft non-maximum suppression [3]removes duplicate predictions and we calculate the finalconfidence scores."}, {"title": "3.2 Training Objectives for the Scoring Model", "content": "Our training objective is the weighted sum of three terms:\n$L = L_{pt} + \\lambda_{vid} L_{vid} + \\lambda_{pascl} L_{pascl},$\nwhere $L_{pt}$ is the point-level classification loss, $L_{vid}$ is the video-level classification loss, and $L_{pascl}$ is the prototype-anchored supervised contrastive loss (see Fig. 2). For brevity, we also call them the point loss, the video loss, and the contrastive loss, respectively. Unless otherwise stated, the averaging over a mini-batch of videos is implied in the expressions of the loss functions below.\nThe point-level classification loss quantifies the clas-sification error on labeled snippets. We adopt the fo-cal loss [27] for this purpose. We separate the contri-butions from the foreground and background snippets, $L_{pt} = L_{pt,fg} + L_{pt,bg}$, to handle the class imbalance"}, {"title": "3.3 Ground-Truth Anchored Pseudo-Labeling", "content": "Among the loss functions in the previous section, the point loss Lpt and the contrastive loss Lpascl do not involve unlabeled snippets, which constitute the majority of the snippets in the training dataset. Pseudo-labeling offers a convenient way of exploiting these underutilized data by generating pseudo labels from the predictions and using them in calculating the losses. To obtain a better outcome, the quality of the pseudo labels is crucial.\nHere, we adopt the ground-truth anchored pseudo-label-ing strategy, inspired by Ma et al. [31] and Li et al. [22]. Under this strategy, pseudo-labels of the c-th action class are assigned to the snippets on an interval if (i) the predic-tion scores PV over the interval are above a threshold ofg,(ii) at least one of the snippets is annotated with an AAPLlabel, and (iii) every AAPL label (t, y) on the interval satisfies yc = 1. Put differently, pseudo-labels are given to intervals with highly confident predictions consistent with AAPL labels. Similarly, pseudo-background labels are assigned to an interval if (a) the actionness scores are below a threshold @bg over the interval, and (b) there is at least one background label and no foreground action label on the interval. When calculating the point loss and the contrastive loss, we replace the AAPL labels with the pseudo labels."}, {"title": "4 Experiments", "content": "In this section, we empirically evaluate the effectiveness of AAPL supervision for temporal action detection. As action-agnostic frame sampling, we use the regularly spaced sampling, except in the part of Sec. 4.4.1 that compares different sampling schemes. We also analyze the effects of our design choices. We defer details of implementation and hyper-parameters to Appendix B."}, {"title": "4.1 Datasets", "content": "To demonstrate the usefulness in various usecases, we use five benchmark datasets with different characteristics. Here, we provide a brief overview of the datasets. More dataset statistics are shown in Appendix A.\nBEOID [7] is a dataset of egocentric activity videos, containing diverse activities ranging from cooking to work-outs. We adopt the training-validation split from Ma et al. [31].\nGTEA [10] also consists egocentric videos but focuses fine-grained daily activities in a kitchen. The median number of action instances per video is 18 in an about 60-second video. This number is by far the largest among the datasets used in this paper.\nTHUMOS '14 [16] has significant variations in the lengths and the number of occurrences of action instances. Following the convention [47, 36], we use the validation set for training and the test set for evaluation.\nFineAction [30] is a large scale dataset for fine-grained action detection. The fine-grained nature of action cat-egories and the sparsity of action instances make this dataset extremely challenging for action detection.\nActivityNet 1.3 [13] is a large-scale video dataset for action recognition and detection of 200 diverse action categories. The majority of videos in this dataset have only one action instance, and the duration of each action instance is much longer than that of the other datasets."}, {"title": "4.2 Evaluation Metrics", "content": "As evaluation metrics, we report mean average precision (mAP) at various thresholds for temporal intersection over union (IoU) (see Jiang et al. [16] for the formal definition). Following convention [19, 25], when calculating the aver-age mAP (Avg mAP), we average mAP's at the thresholds from 0.1 to 0.7 with a step 0.1 for BEOID, GTEA, and THUMOS '14, and from 0.5 to 0.95 with a step 0.05 for FineAction and ActivityNet 1.3. All the reported results of our method are the average of eight runs with different random seeds."}, {"title": "4.3 Main Results", "content": "Table 3 provides the experimental results on BEOID and GTEA, demonstrating that our method outperforms the point-level methods in terms of the average mAP. The intervals of the AAPL labels are three seconds, which incurs less annotation costs than that for the point-level labels, as shown in Sec. 2.3. Therefore, the results in Tab. 3 not just show the better accuracy of the proposed method but also indicate the superiority of our approach regarding the trade-off between detection performance and annotation time.\nFigure 3 shows the trade-off between detection perfor-mance and annotation time for AAPL-supervised learning"}, {"title": "4.4 Analysis", "content": "In this section, we analyze and justify some of the design choices in our approach."}, {"title": "4.4.1 Action-Agnostic Frame Sampling.", "content": "The design of action-agnostic frame sampling impacts the detection performance. To illustrate this, we con-ducted experiments with three different sampling schemes:"}, {"title": "4.4.2 Effectiveness of Each Component.", "content": "The proposed loss function consists of three components: Lpt, Lvid, and Lpascl. We also adopt the ground-truth anchored pseudo-labeling (PL) strategy. To evaluate the effectiveness of each component, we conducted the abla-tion study, as shown in Tab. 7. For both THUMOS '14 and BEOID, adding each component improves the de-tection accuracy, and the full objective achieves the best performance. The video loss makes particularly large con-tributions, showing that the self-training strategy based on top-/bottom-k pooling is effective with AAPL supervision, as with conventional weak supervision."}, {"title": "4.4.3 Form of the Video Loss.", "content": "The proposed video loss is adapted specifically for AAPL supervision to handle the incompleteness of the video-level labels. To demonstrate the effectiveness of our design of the video loss, we compare our proposed video loss with the binary cross-entropy (BCE loss), which is the de facto standard in the field [19, 22]. As shown in Tab. 8, the forms of the video loss impact the detection performance. In particular, mAP's at lower IoU thresholds are affected more than those at higher thresholds. This is reasonable because the video loss, as a ranking-based pseudo-labeling strategy, does not concern the accurate localization of action instances but does help mine unlabeled instances."}, {"title": "5 Conclusion", "content": "We proposed action-agnostic point-level (AAPL) supervision for temporal action detection to achieve a better trade-off between action detection performance and annotation costs. We also proposed an action detection model and the training method to exploit AAPL-labeled data. Extensive empirical investigation suggested that AAPL supervision was competitive with or outperformed pre-vious supervision schemes for a wide range of action detection benchmarks in terms of the cost-performance trade-off. Further analyses justified our design choices, such as frame sampling at regular intervals and the form of the video loss."}, {"title": "Appendix", "content": ""}, {"title": "A Dataset Statistics", "content": "In Tab. 9 we provide the basic statistics of the datasets [7, 10, 16, 30, 13] used in the experiments to facilitate readers' qualitative understanding of the experiments and measurement.\nMost videos in BEOID and GTEA contain action in-stances of multiple classes, while those in the other datasets typically have instances of only one class. They also have many action instances in one minute of a video. Because of these differences, the two datasets are highly challenging for the video-level setting. Indeed, despite their popularity in the literature on point-level supervision, they have not been used to benchmark weakly supervised temporal action localization using video-level labels.\nTHUMOS '14 and ActivityNet 1.3 frequently appear in the action detection literature. A video in THUMOS '14 typically has action instances of only one class, but the number of action instances is also prominent in some videos. The dataset has significant variation in the duration and the number of action instances, which makes this a challenging benchmark. By contrast, most videos in ActivityNet 1.3 have only one action instance, and each action instance has a long duration compared with the other datasets. ActivityNet 1.3 stands out in terms of scale, with the number of videos being significantly larger than BEOID, GTEA, and THUMOS '14. This large-scale dataset provides a diverse range of action classes.\nThe FineAction dataset is the newest of the five datasets. Its scale is comparable to ActivityNet 1.3, but the fine-grained nature of action classes and the relatively short duration and the sparsity action instances make it challeng-ing even for point-level supervision, which has achieved superior performance on THUMOS '14 and ActivityNet 1.3 compared with video-level supervision."}, {"title": "B Details on Experiments", "content": "In this section, we provide the details of the experiments that are omitted in the main text."}, {"title": "B.1 Implementation Details", "content": "In this section, we explain the details of the implementation of our method."}, {"title": "B.1.1 Snippet Features.", "content": "As the feature extractor, we employed the two-stream Inflated-3D (I3D) Inception-V1 model trained on Kinetics-400 [4]. It takes 16 frames of RGB and optical flow, i.e., Ns = 16, and outputs 1024-dimensional feature vectors for each input modality. The snippet features were extracted before the experiments were conducted, i.e., the feature extractor was frozen, and only the embedder and two heads were updated during training. The two-stream features are concatenated into a single snippet feature of 2048 dimensions, except for FineAction [30], whose two-stream feature has 4096 dimensions. For GTEA and BEOID, we used I3D features extracted and distributed by [31]. For FineAction, we used I3D features made public by the original dataset provider [30]. Following the previous work [25], we adopted the \"I3D_100\" features, which the dataset provider generated by temporally scaling the snippet feature sequence to the fixed length of 100 by linear interpolation.\nThe FineAction paper [30] asserts that they have ex-tracted snippet features using \"I3D model\u201d with citation"}, {"title": "B.1.2 Scoring Model.", "content": "As the feature embedder, we employed a single convolu-tion layer with kernel size three, followed by the ReLU activation function. The input and output of the embedder are the same shape.\nThe classification and actionness heads accept half of the embedded features, e.g., in the case of THUMOS '14 (or any dataset other than FineAction), the first 1024 di-mensions of the embedded features are fed into the clas-sification head, while the others are consumed by the actionness head. Then, the heads apply the point-wise convolution and the sigmoid activation function. During the training, the dropout is also applied before the point-wise convolution. The dropout rate is set at 0.7 for all the experiments."}, {"title": "B.1.3 Parametrization of the Top-k Pooling.", "content": "Following previous work [37, 20, 38], we set kx in the top-k pooling as kx = min{1, [T/rx]}, where X is either fg or bg, T is the length of the snippet feature sequence, and rx is a hyper-parameter. We borrowed the values of rfg and rbg from the previous work [38] and set (rfg, r'bg) as (8, 3) for the datasets other than ActivityNet 1.3 and (2, 10) for ActivityNet 1.3."}, {"title": "B.1.4 Optimization.", "content": "The scoring model was optimized using the Adam optimizer with batch size 16. The learning rate and weight decay were tuned by grid search."}, {"title": "B.2 Hyper-Parameters", "content": "The hyper-parameters of the scoring model were tuned using the grid search. Because optimizing many hyper-parameters is computationally expensive, we incremen-tally tuned the hyper-parameters. Specifically, we first tuned the learning rate and weight decay, then the weight of the video loss Avid, then the weight of the contrastive loss Apascl and the temperature 7, and finally the thresh-olds ofg and Obg for the ground-truth anchored pseudo-labeling. From preliminary experiments, we found that the momentum parameter \u03bc of the prototype update for the contrastive loss, and therefore, we fixed it to 0.001 for all the experiments. The optimal values are summarized in Appendix B.2."}, {"title": "B.3 LACP Experiment on FineAction", "content": "The evaluation of LACP [19] on FineAction was con-ducted using the code developed by LACP's authors\u00b2. We modified the code to adapt it to the FineAction dataset and its evaluation metric.\nUnder the default configuration for THUMOS '14, the optimal sequence search (OSS) [19] runs once in every ten iterations, but with the dataset as large as FineAction, this frequency makes the training extremely inefficient. Therefore, we dropped the frequency of OSS to once in every 200 iterations, which was 20 times as infrequent as the default value. However, this is twice as frequent in terms of epochs as the default. We also swept the interval of 100 to 500 iterations and verified that the 200-iteration interval achieved the peak detection accuracy.\nOther hyper-parameters were tuned using the grid search. The optimal values that are different from the default are the learning rate 2 \u00d7 10-5 and ract 2."}, {"title": "B.4 Clustering-Based Frame Sampling", "content": "Here, we describe the specific implementation of action-agnostic frame sampling based on clustering. The pseudo-code is presented in Algorithm 1. Basically, we perform the k-means clustering on the set of snippet features ex-tracted by using pretrained feature extractor. This algo-rithm is grounded on the assumption that it is more desir-able to sample diverse frames.\nWe adopted the same feature extractor as the detection model, i.e., the Inflated 3D (I3D) convolutional neural net-work [4] trained on the Kinetics dataset. Feature vectors output by the I3D model have 2048 dimensions, but such high-dimensional features are not suitable for clustering. For this reason, we apply the principal component analysis to reduce the dimensions to 64. The number of clusters for a video is set to be TV/T, where TV is the duration of the video V, and 7 is the average interval of labeled frames."}, {"title": "C Measurement of Annotation Time", "content": "To demonstrate that the sparse supervision has benefits for annotation costs, we measured actual annotation speed for three datasets, THUMOS'14 [16], GTEA [10], and BEIOD [7]. This section explains the detailed protocol of the measurement."}, {"title": "C.1 Randomized Allocation", "content": "The annotation time depends not only on the annotation method and the dataset but also on various factors, such as an annotator's proficiency in the annotation method and familiarity with the dataset. There can also be an issue of compatibility between videos and annotation methods"}, {"title": "C.2 Annotation Tool", "content": "We developed a browser-based annotation tool for the four annotation methods. This tool is the adaptation from VGG Image Annotator [9], an open-source image annotation tool equipped with the functionality for full supervision of temporal action detection. The tool and the manual are available at https://github.com/smy-nec/AAPL."}, {"title": "C.3 Instructions to Annotators", "content": "In this section, we provide the instructions given to the annotators, except for the usage of the tool, which is ex-plained in the tool's manual.\nWe instructed the workers to start the timer when they were ready to start annotating a video, i.e., when they had finished reading the instructions, loading a VIA project file and a video, and configuring the tool for annotation. We also instructed them to stop the timer when they had reached the end of the video and finished the annotation. After that, the workers were asked to save the project file containing the annotations and the time measurement and to submit the project file to us. In addition, we asked the workers to self-check the annotations. Self-checking for a video was performed immediately after the annotation of the video was finished. The time of the self-checking was measured separately from the first annotation.\nSome workers might be so meticulous that they pause and repeat the video to ensure that they produce as accu-rate annotations as possible. Although we acknowledge the importance of keeping the annotations accurate, this kind of meticulousness can lead to an overestimation of the annotation time. To mitigate this issue, we instruct the workers not to rewind the video unless they need to do so because either\n\u2022 they have skipped a part of the video (e.g., for AAPL annotation) and need to go back a few seconds to decide the action occurring at that point, or\n\u2022 they have passed an action boundary and need to go back to mark it,\nand even if they did so, we asked them to rewind the video by a few seconds at most.\nRegarding self-checking, we only asked them to check and correct clear mistakes such as missing action instances and wrong action classes. We asked not to adjust the action boundaries here because the self-checking is not intended"}, {"title": "C.4 On the Measurement Result", "content": "The relative annotation time reported in the main text is the total annotation time divided by the total annotated video duration. The time measurement there does not include the time for the self-checking. To complement those data, Tab. 11 shows the relative annotation time that also accounts for the self-checking. This indicates that the comparison of annotation time among different annotation methods is not significantly affected by taking the self-checking into account."}, {"title": "C.5 Limitation of the Current Measurement", "content": "Actual annotation costs depend not only on the type of supervision but also on many details of an annotation pipeline, including the user interface of annotation soft-ware and instructions given to the workers. Also, if the number of action categories is large as in ActivityNet and FineAction, searching a long list for the class of each action instance is unrealistic, and the design of the an-notation pipeline must be fundamentally different from the one we implemented in this paper, as in [13]. We would like to emphasize, however, that addressing the trade-offs between detection performance and annotation time is crucial in researching weak supervision schemes, and that measurement like ours is indispensable for this purpose. Therefore, the annotation time as measured in this work should be considered not as the practically real-istic value but as a convenient device to compare different supervision schemes on a reasonably equal footing."}, {"title": "Acknowledgments", "content": "We thank Ryoma Ouchi for his efforts and commitment at the early stage of this project."}]}