{"title": "Regulation of Language Models With Interpretability Will Likely Result In A Performance Trade-Off", "authors": ["Eoin M. Kenny", "Julie A. Shah"], "abstract": "Regulation is increasingly cited as the most important and pressing concern in machine learning. However, it is currently unknown how to implement this, and perhaps more importantly, how it would effect model performance alongside human collaboration if actually realized. In this paper, we attempt to answer these questions by building a regulatable large-language model (LLM), and then quantifying how the additional constraints involved affect (1) model performance, alongside (2) human collaboration. Our empirical results reveal that it is possible to force an LLM to use human-defined features in a transparent way, but a \"regulation performance trade-off\" previously not considered reveals itself in the form of a 7.34% classification performance drop. Surprisingly however, we show that despite this, such systems actually improve human task performance speed and appropriate confidence in a realistic deployment setting compared to no AI assistance, thus paving a way for fair, regulatable AI, which benefits users.", "sections": [{"title": "1 Introduction", "content": "Ineffective regulation of AI and the neglection of safety is often cited as the biggest existential threat to humanity (Bengio et al., 2024). Ex-board members of OpenAI have recently been quoted as saying governments must begin building effective regulatory frameworks now, as AI firms cannot self-govern and reliably withstand the pressure of profit incentives (Toner and McCauley, 2024). The biggest factor pushing this regulatory interest is large-language models (LLMs) (Vaswani et al., 2017), which have already had far reaching consequences in society, ranging from medicine to self-driving cars (Chen et al., 2023). The core issue is that these systems cannot escape the same limitation that underlines most neural network architectures, in that they are black boxes with no obvious interpretable decision-making process, making it completely impossible to use or audit them for any sensitive application (Keane et al., 2021). Governments at large are aware of this and the European AI Act is a sign of things to come in how they will continue to heavily regulate AI both in Europe and North America (Smuha et al., 2021). However, it is presently unclear how LLMs might be regulated in practice.\nIn this paper, we are interested in the potential of interpretable ML to make models more regulatable. Techniques from this field have been shown to help make models auditable (Zhang et al., 2022), debug self-driving cars (Kenny et al., 2024; Dong et al., 2023), and calibrate appropriate trust (Sanneman and Shah, 2022). However, to date there is no exploration of how to make interpretable LLMs for the purposes of regulation.\nIn reality, regulation will likely take many different forms in different domains, but here we are specifically interested in the domain of insurance liability and how to regulate models in such a setting using interpretable ML. In this domain, such institutions require their employees (and by extension their models) to use specific concepts in sensitive decisions in order to be legally compliant, but due to the black-box nature of AI, there is absolutely no way to verify this is happening (Nguyen et al., 2021). Hence, in these specific circumstances, a basic requirement for regulation is to force these models to use specific human-defined concepts in their inference process, which interpretable ML can help us do. Interestingly, we find that in doing so, a dilemma presents itself in the form of a trade-off between regulation and performance previously unconsidered in the literature.\nAs an aside, we remind the reader that LLMs are broadly classified into two categories, generative (e.g., ChatGPT) and classification (e.g., BERT)"}, {"title": "2 Context and Trade-Off", "content": "As this paper focuses on the domain of insurance liability, this section gives some brief context in the area, before formalizing the regulation performance trade-off."}, {"title": "2.1 Insurance Liability", "content": "Explainable AI benefits from focusing on specific applications due to how it simplifies evaluation (Yadav, 2024). Here we are focused on the specific task of determining liability in automotive accidents. We want our system to (1) use human-vetted concepts in an interpretable way for regulation, and (2) benefit humans in a collaborative setting, both of which we show results for in our evaluation. In insurance liability settings, there is an insured, and a claimant. The insured is the person or entity that purchases an insurance policy from an insurance company, whilst the claimant is the person or entity that makes a claim for benefits under an insurance policy. In our setting, the two parties are automotive drivers involved in a collision, and the accident is recorded in natural text, which motivates our usage of LLMs. Legally required concepts to use in this domain consist of e.g. \"running a red light,\" and not other spurious (or even illegal) features such as e.g. a person's gender or nationality (Benhamou and Ferland, 2020)."}, {"title": "2.2 The Regulation Performance Trade-Off", "content": "Without loss of generality, consider an LLM that encodes features into some latent space. Within this, there exists a set of features which the LLM has learned to encode to perform optimally on some classification task, the \"black box feature set\". There exists another set of features in the same space called the \"interpretable feature set\", which is the set of features humans can understand (e.g., a person's nationality). In our case there also exists a final set, the \"regulatable feature set\" (e.g., running a red light). This is a subset of the \"interpretable feature set\", as to be regulatable, a feature must be interpretable. Formally, let $L \\in \\mathbb{R}^{(n)}$ be the n-dimensional latent space of the LLM. It follows that the sets are:\n\\begin{itemize}\n    \\item $B \\subset L$: the \"black box feature set\" that the LLM encodes to optimize a classification task.\n    \\item $I \\subseteq L$: the \"interpretable feature set\" that humans can understand.\n    \\item $R \\subset I$: the \"regulatable feature set\", a subset of the interpretable feature set which allows legal usage of the LLM.\n\\end{itemize}"}, {"title": "3 Insurance Datasets", "content": "The main datasets used in this paper originate from a global insurance company and are not publicly available. However, in the spirit of scientific reproducibility, we also run our experiments on a publicly available and widely used dataset. We briefly describe this latter dataset later in Section 5, since it is already widely known and not our focus."}, {"title": "3.1 The Liability Dataset", "content": "This dataset contains 150,000 entries. The columns are (1) natural language text statements describing a car accident between an insured and a claimant, and (2) a label from 0-100% assigning liability to the insured, where 0% is no liability and 100% is complete liability. To pre-process the dataset, we categorized liability into three classes:\n\\begin{enumerate}\n    \\item \\textit{Not Liable: }The insured is 0% at fault in the accident.\n    \\item \\textit{Split Liability: }The Insured and Claimant are both at fault (anywhere between 1-99% at fault).\n    \\item \\textit{Liable: }The insured is 100% at fault\n\\end{enumerate}\nAfter this, we balanced the dataset, which resulted in 14,000 entries for each class. Furthermore, the data was divided into training (90%), validation (5%), and testing (5%)."}, {"title": "3.2 The Human-Labeled Concept Dataset", "content": "The second dataset is a collection of 2,000 statements, all of which are separate from the prior dataset. For these data, we employed two separate vendors to label parts of their sentences with important concepts for assessing liability that were defined by a domain expert. Having two separate vendors is important because if our model were to have 45% accuracy on classifying these concept labels, but the two vendors only agreed 60% of the time, then it is actually a very good model having reached 75% of this theoretical ceiling. In total, there were eight labels (i.e., concepts) we asked them to assign, shown in Table 1. Both vendors precisely agreed on a given concept being present and its exact text within the statement 2.65% of the time. However, if we relax the second constraint and allow agreement when one text segment envelops the other, this agreement rises to 61.2%, which we consider the ceiling of performance any model should realistically achieve. For the final data, we joined all labels together from both vendors in order to maximize the amount of labeled concept data, so if Vendor 1 labeled the first ten statements with concept x, and Vendor 2 the last ten, we would collect 20 labels for that concept."}, {"title": "4 Proposed Method", "content": "In this section we outline the assumptions for our proposed method of integrating human-centered concepts into LLMs, detail our architecture for doing so, and outline implementation specifics."}, {"title": "4.1 Assumptions", "content": "We assume access to an encoder-only LLM trained for a specific classification task on a large quantity of data. Furthermore, we assume access to (1) the original dataset used to train this LLM, and (2) another dataset of human-annotated concept data you wish to force the LLM to use during its classifications. Lastly, we assume competent domain knowledge which can be used to define how each concept should contribute to each class. For example, in our insurance liability domain, the concept \u201cIV Liable\" should positively contribute to the class \"Liable\", hence we manually define the classification weight matrix W' to have a positive weight connection between this concept and class prediction, while it has a negative weight to the class \"Not Liable\" (see Figure 2)."}, {"title": "4.2 Architecture", "content": "In the model shown in Figure 2, a test instance, $x$, is mapped to a set of sentence embeddings $Z \\in \\{z_i\\}_{i=1}^{m_1}$ via the original encoder network $f_{enc}$ and a sentence encoder $w(\\cdot)$. Alongside this, a set of human-labeled sentence-level concept data $D$, which can be divided up into each concept class $D \\in \\{D_c\\}_{c=1}^C$ is also passed into $f_{enc}$ to produce a set of embeddings $D_c \\in \\{d_i\\}_{i=1}^{m_c}$ for each class. These $C$ sets are then averaged into $c$ concept prototypes $P \\in \\{p_i\\}_{i=1}^c$, one for each concept $c$. Then, for example, all of the sentence embeddings for $x$ (i.e., $Z \\in \\{z_i\\}_{i=1}^{m_1}$) and prototype $p_i$ are passed into $h_i$ to measure each sentence's similarity to $p_i$ via a similarity function, before its element-wise product with $W'$ is taken to produce the network's logits with:\n$\\begin{aligned}\n    sim(z_i, p_i) &= log \\bigg( \\frac{(z_i - p_i)^2 + 1}{(z_i - p_i)^2 + \\epsilon} \\bigg) \\\\\n    s_i &= arg \\underset{z_i \\in Z}{max} \\hspace{0.1cm} sim(z_i, p_i) \\\\\n    \\hat{y} &= W' \\odot s'\n\\end{aligned}$\nwhere $s'$ is the vector of similarity scores for each concept such that $s' = \\{s_1, s_2, ... s_n\\}$, and $\\epsilon$ is to avoid division by zero. Equation 1 is monotonically decreasing such that if the prototype is close to a sentence embedding, it will output a high similarity score. The maximum similarity score across all sentences is then used in the forward pass for that concept, and this is repeated to give a score for all concepts with Equation 2. Finally, this vector of similarity scores takes an element-wise product with W' in Equation 3 to give the logits $\\hat{y}$.\nThe loss of our network is calculated with two terms, the first $L_c$ is a standard loss for the class label, and the second loss $L_h$ is the human concept loss. For $C_h$, a subset of each concept label $D' \\in \\{D_c\\}_{c=1}^C$ is passed each iteration into their corresponding $h$, and their similarity scores against the pre-computed prototypes that same iteration are calculated with Equation 2 for a cross-entropy loss. Together, this has the effect of encouraging the network to classify the overall label correctly, but also to learn to classify the human-concept data correctly with the prototypes, which together enforces the necessary constraints for our system. The loss can be written as:\n$\\underset{\\varphi, \\omega, W'}{min} \\hspace{0.1cm} L_c(y, \\hat{y}) + \\frac{1}{C} \\sum_{i=1}^C \\mathcal{L}_h(Y', \\phi(p_i, D'))$\nwhere $y$ is the overall label, and $\\hat{y}$ is the prediction of the overall label. Moreover, $\\phi(\\cdot)$ is a function that outputs a vector of similarity scores $s$, $Y'$ is the label for the human concept, $p_i$ is the computed prototype for concept $i$ that iteration, and $D'$ is randomly sampled concept data for concept $i$. Put simply, the first term teaches our network to predict the right class, and the second encourages it to learn to classify concepts correctly with the prototypes."}, {"title": "4.3 Implementation Details", "content": "To encode a set of sentence embeddings with $w(\\cdot)$ there are two main ways we explore, context unaware and context aware. For context unaware, we break the input text $x$ into sentences prior to encoding with $f_{enc}$, and use the BERT [CLS] token (or equivalent) as the sentence embeddings. For context aware, we pass all of $x$ into $f_{enc}$, divide up the contextualized word embeddings (i.e., the token embeddings after the forward pass) into sentences, and then collate them into a single embedding. In our experiments, to collate these we used either (1) a simple average, (2) a recurrent neural network (RNN) encoder, or (3) an attention layer.\nThe transformations $h_i$ are all MLP networks with one hidden layer. To regularize, we compressed the dimensionality here to as low as possible without compromising performance. For our experiments, this involved going from an encoding space of size 768 to 16 in these MLP networks.\nLastly, for $W'$, expert knowledge is needed to define it appropriately. In our case, we used domain"}, {"title": "5 Computational Experiments", "content": "Here, we describe our baselines, before detailing the datasets, metrics, and finally the results."}, {"title": "5.1 Baselines", "content": "We conduct comparisons between our regulatable model in Figure 2 and a generic baseline which does not use human-centered data (i.e., Human Labels=No in Table 2). These unsupervised baselines set the prototypes as learnable parameters instead, which is representative of the literature (Chen et al., 2019; Antognini and Faltings, 2021; Ming et al., 2019; Das et al., 2022). Alongside this we also randomize $W'$ and don't constrain its polarity in baselines to avoid any human bias making its way into the training process. While comparing these two baselines, we also do so in (1) a context aware fashion, and (2) a context unaware one (see Section 4.3). For our text encoder we use BERT (Devlin et al.,"}, {"title": "5.2 Datasets", "content": "Our primary tests are on the insurance liability datasets detailed already in Section 2, as we are particularly interested in evaluating our technique on real-world applications. However, to foster reproducibility, we also extend the same tests to the Beer Advocate dataset (McAuley et al., 2012). This second dataset is 200k rows of text data detailing reviews of beers, it contains the concepts of Appearance, Aroma, Palate, Taste, and Overall. To mimic related work (Bao et al., 2018), we divide the dataset into a binary classification problem of those reviews with a score higher than 4, and lower. The Beer Advocate dataset is also quite unique in that it contains 994 sentence-level annotations for the five concepts present, making it suitable for our needs. We further divided these concepts into positive/negative ones (depending on which class they belonged to) to make in total 10 concepts which could be used for classifying the positive/negative reviews. Going forward, we will talk about class labels (i.e., the regular classification task), and concept labels (i.e., the sentence-level annotations), as they are two different evaluations."}, {"title": "5.3 Metrics", "content": "We consider three primary measurements. First, we measure how well the models are performing on their respective class labels. Following best practice, a model is chosen based on its performance on validation data during training, and then performance on the testing data is reported. Next, we also consider how well the model is classifying the concept labels. For this we consider a \"Top 1\" and \"Top 3\" metric, the model is seen as correct if the prototype for e.g. \u201cIV Liable\u201d activates the strongest for a sentence in a datum with that label (i.e., Top 1 metric), and likewise for Top 3 it is seen as correct if it is in the 3 most strongly activated."}, {"title": "5.4 Results", "content": "Table 2 shows the results of running our tests three times and calculating the mean alongside standard error. Overall, there are three strong trends to note. Firstly, the context aware setting achieves better classification performance on the class labels, whilst the context unaware models do better at classifying the concept labels. This is likely because the latter forces the LLM to have stronger sentence representations that are not entangled with the rest of the text, this works better for concept classification. Secondly, there is another strong trend that learning the concept representations from scratch instead of using the labels (i.e., Human Label=no) results again in stronger classification performance of the class, but again this comes alongside a trade-off with concept accuracy. Thirdly, the attention mechanism in context aware settings does best at encoding sentence representations when compared to taking an average or using an RNN.\nThe strongest results come from the context unaware model using the human annotated data. This model achieved 45.90\u00b10.11 / 75.9\u00b10.27 Top 1/Top 3 classification performance on the concept labels for the Insurance Liability dataset, respectively, and 44.32\u00b10.23/74.43\u00b10.16 Top 1/Top 3 classification performance on Beer Advocate, respectively. Importantly however, this did come with a trade-off on performance for the actual overall class labels. Specifically, on the Insurance Liability data the performance dropped from the initial black-box model accuracy of 68.68% to 60.75%, and on Beer Advocate from 84.16% to 77.41%, resulting in an average drop of 7.34% in performance. In contrast, the models which are not confined to regulatable features and instead learned the interpretable concepts actually outperformed the original black-box, reaching 69.01% on the Insurance Liability data, and 85.05% on Beer Advocate. This improved performance could be attributed to a regularization effect induced by our model, which forces the LLM to reason using only a handful of prototypes, as similar results were seen before with similar techniques (Kenny et al., 2023). Recall that the inter-rater reliability, as measured by the percent-"}, {"title": "6 Pilot User Study", "content": "Here we facilitate an \"Application Grounded Evaluation,\" which is typically seen as the gold-standard in explainable AI (Doshi-Velez and Kim, 2017). Specifically, we recruited eight adjusters from a private global insurance company (whose full-time job it is to process insurance claims) to participate in a pilot study using our model to help classify real statements in practice. While this meant our sample size would be necessarily reduced, it allowed the enormous advantage of using real-world data in a real-world setting. Studies have consistently shown that how users react to AI technology is quite divided (Brecheisen, 2024). Given this, our hypothesis was that certain users would react favorably to the AI and cluster into one group with reduced time taken overall to classify the statements, whilst the others would do the opposite.\nMaterials. We designed a within-subjects study which showed adjusters eight separate statements, four with AI assistance and four without. The questions with AI assistance showed adjusters one concept activation per statement, which was most relevant. Adjusters were told all statements could be either liable, split liability, or not liable. However, in reality, four were liable, and four not liable, with the AI assistant helping on half of each. The eight adjusters were split into two groups, in which the questions with AI assistance were counterbalanced. The AI assisted questions gave (1) its prediction for the statement, and (2) the highlighted text for the most important sentence in the prediction. The final analysis pooled all data from both versions of the survey together to control for the effect of each individual question. Each participant was given the survey online and asked to complete it in their own time (but during working hours), in one sitting. The study passed IRB review.\nMetrics. We measured (1) how accurately each adjuster classified each statement, (2) how quickly they classified each statement, and (3) how confidently they classified each statement. The confidence metric was measured on a 7-point Likert scale with the question \u201cI am confident in this classification\u201d. Each user's scores for statements with and without the AI assistant were averaged into a single result, giving two measurements for each metric for each user as standard (Kenny et al.,"}, {"title": "6.1 Results", "content": "First, the data was cleaned (details in Appendix A). Overall, our hypothesis was confirmed when we found user scores on time taken became widely divergent based on how they responded to the AI (note Figure 3). Those users whose time got longer with the AI (n=3) vs. those users whose time got less (n=5) saw a statistically significant difference (tested for normality; t(6)= 3.59, p < 0.02). Overall, even if we pool both groups together, this still averaged as 110.40 \u00b1 14.61 seconds with the AI assistant compared to 123.46 \u00b1 29.61 without, hinting towards a benefit of the AI assistant on a population level. On confidence scores, a similar trend was seen in users whose confidence improved with the AI (n=3) and those whose got worse (n=3; t(4)=3.59, p=0.094). Overall, this averaged at 6.5 \u00b1 0.27 with the AI assistant compared to 6.4 \u00b1 0.42 without it. Given the average confidence was so high overall, this represents a notable increase.\nAs an interesting aside, only User 3 made a mistake when classifying the statements (see dashed line in Figure 3). Specifically, they classified the second question as \"Split Liability\u201d when it was \"Liable\". This question for the user had an AI assistant, indicating a possible lack of trust towards the AI, as all other participants agreed with the AI on this question. Note this user spent the longest time deciding on classifications with the AI, lending evidence that a lack of trust in AI contributes to slower task performance.\nIn summary, this study indicates two intriguing findings. Firstly, despite the regulatory model having worse performance compared to a black-box on class labels, humans still benefit overall from interacting with it, as indicated by their improved speed. Moreover, because adjusters were almost always correct in their classification, their improved confidence score with the AI was also appropriate confidence, similar to the idea of appropriate trust in AI (Sanneman and Shah, 2022). Secondly, as prior work has hinted (Brecheisen, 2024), how"}, {"title": "7 Related Work", "content": "Regulation in machine learning has come into the spotlight recently, with major conferences dedicating workshops to the topic (Ma, 2024), governments trying to implement it (Wischmeyer and Rademacher, 2020), and academia actively researching it (Onitiu et al., 2023), but there is little work on how it should be concretely realized. Due to this sparsity, in our literature review, we focus on tangential work which has built inherently interpretable LLMs, as it is widely agreed to be a prerequisite for AI regulation (Casper et al., 2024).\nCase-based reasoning (CBR) for interpretable LLMs is a recent idea, it uses real examples from the training data directly in inference for interpretability purposes. Notable work in this area can be traced back to Ming et al. (Ming et al., 2019) who focused on RNNs. Das et al. (Das et al., 2022) proposed ProtoTEx, which classifies test instances with reference to learned prototypes (i.e., examples or \"cases\"). Van Aken et al. (Van Aken et al., 2022) proposed ProtoPPatient, which works for multi-label classification. Xie et al. (Xie et al., 2023) is the most up to date work, which adds saliency maps to the explanation. Similar work exists in the concept explanation literature (Chan et al., 2022; Bouchacourt and Denoyer, 2019; Antognini and Faltings, 2021). In contrast to all these, our work allows the direct integration of human-regulatable concepts into the inference process, which is needed for the type of regulation we are striving for.\nPerhaps the most closely related work is that of Kenny et al. (2023). The authors proposed to explain a deep reinforcement learning agent by wrapping its encoder with an interpretable prototype layer, where each prototype represents a human-friendly concept, but the authors note the networks are prone to over-fitting, likely because they only use a single example to represent each concept. We build upon this work by collecting a large human-annotated dataset for each concept to avoid over-fitting, and adapting the framework for LLMs."}, {"title": "8 Discussion & Conclusion", "content": "In this paper, we proposed a framework for helping to regulate LLMs. Our primary goal was to instantiate a regulatable LLM in insurance liability settings and quantify the trade-off (if any) which occurs related to performance and user interaction. Results showed that one can constrain an LLM to use regulatable concepts post training, but that this does degrade performance by around 7.34% on average, an interaction we coin as \u201cThe Regulation Performance Trade-Off.\" However, given that it is currently impossible to deploy these models in many sensitive applications due to their black-box nature (Rudin, 2019), this will often be a small price to pay. More importantly though, our user study with industry professionals highlighted the positive utility of the method in practice for human-AI collaboration despite this trade-off, which is a sobering reminder that the model's performance on class labels is only part of the overall picture to be considered in evaluation. We hope these data will take the world a step closer to regulatable LLMs that benefit humanity."}, {"title": "Limitations", "content": "Here we detail the limitations of our work which give way to opportunities for future research.\nLLM Constraints. Our model is limited to the learned representations of the original LLM. It could be that by training end-to-end, the results would be superior, but our preliminary experiments failed to accomplish this. It would however be interesting to explore this in future work as a way"}, {"title": "Small Sample.", "content": "Our user study design opted for a smaller sample size in order to test it with real industry professionals in a realistic deployment setting. This has the huge advantage of truly testing the system \"in the wild\", but comes with the trade-off of a small sample of users. Hence, even though our test reached statistical significance, it should be verified on a larger sample of end users."}, {"title": "Separation of Explanation and Prediction.", "content": "It is not clear from our user study design if the explanation or model prediction made the core difference in the study. As the AI assisted questions showed both the AI prediction and the concept explanation, it is not clear which made a difference. This is a common issue however (Lundberg et al., 2018; Barnett et al., 2024), as such studies are so expensive to run, and naturally have so few users, it is often an unfortunate necessity to avoid splitting the user base into so many conditions that the results become impossible to interpret."}, {"title": "Labeling Requirements.", "content": "Our method requires a large dataset of human-annotated concepts. This is a large bottleneck for the method, but it is conceivable that generative language models could actually be made to synthesize this data, which would be interesting to investigate in future research."}, {"title": "Generalizing.", "content": "Our method is developed for encoder-only language models. It may require several alterations to make similar methods work for decoder-only language models or image classifiers."}, {"title": "A Appendix", "content": "First, we found two outlier entries which were ex- cluded from analysis. Specifically, one user spent over 10x times longer to complete one question"}, {"title": "A.2 Computational Budget", "content": "We train our models on 4 GPUs using AWS, to reproduce the results would take 1 day on average."}, {"title": "A.3 User Study Design", "content": "Here we post the entire user study, as much as possible, for transparency."}]}