{"title": "Single-Channel Distance-Based Source Separation\nfor Mobile GPU in Outdoor and Indoor\nEnvironments", "authors": ["Hanbin Bae", "Byungjun Kang", "Jiwon Kim", "Jae-Yong Hwang", "Hosang Sung", "Hoon-Young Cho"], "abstract": "This study emphasizes the significance of exploring\ndistance-based source separation (DSS) in outdoor environments.\nUnlike existing studies that primarily focus on indoor settings, the\nproposed model is designed to capture the unique characteristics\nof outdoor audio sources. It incorporates advanced techniques,\nincluding a two-stage conformer block, a linear relation-aware\nself-attention (RSA), and a TensorFlow Lite GPU delegate. While\nthe linear RSA may not capture physical cues as explicitly\nas the quadratic RSA, the linear RSA enhances the model's\ncontext awareness, leading to improved performance on the\nDSS that requires an understanding of physical cues in outdoor\nand indoor environments. The experimental results demonstrated\nthat the proposed model overcomes the limitations of existing\napproaches and considerably enhances energy efficiency and real-\ntime inference speed on mobile devices.", "sections": [{"title": "I. INTRODUCTION", "content": "The recently developed distance-based source separation\n(DSS) [1]\u2013[3] separates a single-channel audio mixture into\nindividual components based on their distance from a reference\npoint, such as a microphone. The sources are classified as\n'near' or 'far' based on a specified distance threshold. In con-\ntrast to traditional methods, this method emphasizes upon the\nspatial information of the sources, particularly their distance\nfrom the recording device, to improve the separation process in\nchallenging acoustic scenarios. For example, in environments\nwith multiple sound sources at different distances from a\nmicrophone, (i.e., conference rooms, lecture halls, and concert\nvenues), DSS facilitates the isolation of individual speakers.\nTo effectively use a DSS model in a single-channel setting,\nthe physical information must be appropriately interpreted to\nbetter perceive the distance. This involves factors such as\nthe intensity variation with distance (based on the inverse-\nsquare law), direct-to-reverberation ratio (DRR), proximity\neffects that result in directional microphones emphasizing low\nfrequencies for sources that are closer, and acoustic cues\nsuch as absorption by air and spatial effects [1]. Based on\nthe understanding and exploitation of these physical cues, a\nDSS model can effectively distinguish between near and far\nsound sources considering their distance from the microphone,\nthereby enabling accurate sound separation.\nHowever, recent studies [1]\u2013[3] have solely focused on\nsound separation in indoor environments; despite the necessity\nof investigating its application for outdoor environments. In\noutdoor environments with high noise, poor performance is ex-\nacerbated by the greater impact of the difficulty of measuring\nDRR. Further, environmental factors, such as wind, ambient\nnoise, and varying distances of sound sources can hinder\nthe accurate separation of near and far sounds. In addition,\nthe presence of background noise and unpredictable acoustic\nconditions outdoors can degrade the performance of separation\nmodels trained on indoor data. The application of DSS models\nto outdoor environments requires robust algorithms that can\nincorporate these environmental variables.\nThe requirement of additional support of a DSS technique\non mobile devices is evident when examining how people aim\nto improve audio for their self-created videos. For instance,\nvlogs, wherein users record and share their daily experiences,\nare among the most popular types of content, and most are\ncreated using mobile phones. Prior to conducting this study,\nwe conducted a preliminary survey with amateur experts [4]\naged 20-30 who frequently shot and uploaded videos on social\nmedia. This study revealed that one of the biggest challenges\nfaced by the participants was the removal of unintentional\nsounds from their video recordings. These videos typically"}, {"title": "II. PROBLEM FORMULATION", "content": "In this study, we assume that the input mixture signal $x_{mix}$\nis approximated as follows:\n$x_{mix} = \\sum_{i=1}^{N_{near}} x_{near} * h_{near} + \\sum_{i=1}^{N_{far}} x_{far} * h_{far} + e, = y_{near} + y_{far}$ (1)\nwhere N, x, e, and h denote the number of sources, the\ndirect path signals of each source, the background noise signal,\nand the impulse responses of each source in an acoustic\nenvironment.\nTo simulate the impulse responses of each source, the\nPyroomacoustics [7] toolbox's Image Source Model (ISM)\n[8] was used. This model considered all possible sound wave\npaths from the source to the virtual sources and then to the\nmicrophone, capturing the spatial and reverberation character-\nistics of the room. We then convolved each source's direct-path\nsignal and impulse response and added the background noise\ninto the convolved signals.\nWith the increase in the distance, the intensity of the direct\npath signal and the time difference between the direct path\nsignal and the early reflection signals reduces. Consequently,\ndistinguishing the acoustic characteristics of the far signals\nfrom background noise, particularly babble noise, is challeng-\ning. Therefore, we did not distinguish them and defined the\nfirst term of the right-hand side of eq (1) as ynear, and the last\ntwo terms were combined as yfar; this is visually represented\nin Fig. 1."}, {"title": "III. PROPOSED ARCHITECTURE", "content": "We adopt a conformer-based metric generative adversarial\nnetwork (CMGAN) [9] as the baseline architecture $M_{Baseline}$,\nshown in Fig. 2. CMGAN combines conformer blocks [10]\nin a dual-path design and metric GAN [11] for optimizing\nspeech enhancement performance. However, the metric GAN\nis excluded as designing its metric and discriminator is beyond\nthis study's scope.\nThe input waveform is transformed into\na complex spectrogram via STFT (FFT size: 512, hop size:\n128, Hamming window) and compressed using a power-law\ntechnique [12] with a 0.3 exponent. The encoded spectrogram\npasses through a dilated DenseNet [13] with four convolution\nblocks (dilations: 1, 2, 4, 8), halving the frequency dimension\nin the last block.\nThe separator uses four two-stage conformer (TS-\nConformer) blocks to capture time and frequency dependen-\ncies via multi-head self-attention (MHSA) [14] and CNN\nlayers. The outputs of the 2nd and 4th blocks are then passed\nto the decoder for near and far targets, respectively.\nThe decoder consists of two paths: Mask\nDecoder and Complex Decoder, both using dilated DenseNet\nand sub-pixel convolution for frequency upsampling. The\nMask Decoder predicts the spectral mask, while the Complex\nDecoder predicts the real and imaginary parts of the spectro-\ngram. The final spectrogram is obtained by multiplying the\nmixture spectrogram by the mask and adding the complex\nspectrogram. It then undergoes inverse power-law compression\nand ISTFT to produce the time-domain signal."}, {"title": "B. Linearization of relation-aware self-attention", "content": "The conformer architectures [9], [15] use relation-aware\nself-attention (RSA) [16] as the core mechanism in MHSA.\nRSA enhances traditional self-attention by considering both\nthe absolute positions and the relative relationships between el-\nements in a sequence, enabling the model to capture more nu-\nanced interactions. This allows for more flexible and context-\naware processing, improving performance in DSS models that\nrequire understanding time and frequency domain information.\nThe RSA operation is represented as:\n$Output = softmax((QK^T + Q \\odot R)/\\sqrt{d})V$ (2)\n$QR = einsum('B N d, NM d\\rightarrow BN M', Q, R)$,\nwhere K, Q, V, and R represent key, query, value, and\nrelative-positional embedding matrices, while B, N, M, and d"}, {"title": "C. Mobile GPU Utilization", "content": "Mobile GPUs are optimized for parallel processing, making\nthem ideal for tasks like deep neural network inference.\nThey provide advantages over mobile CPUs, such as higher\nparallelism, faster execution of complex computations, poten-\ntial real-time performance, and improved energy efficiency.\nIntegrating the GPU delegate in TensorFlow Lite (TFLite)\nenhances model efficiency, resulting in lower latency.\nThe following two rules should be considered to ensure a\nGPU delegate works with every operation.\n1) Use layers supported by the GPU delegate.\n2) Maintain a consistent batch size for all tensors.\nWe excluded reshape operations in TS-Conformer [9] to\navoid changing tensor batch sizes and implemented a custom\nConv1D layer to bypass TensorFlow's reshape operations. Ad-\nditionally, we optimized all tensor shapes to be consistent (size\n4) and adjusted the MHSA mechanism to handle input tensors\nseparately along the h-axis. The BatchMatMul operation was\noptimized by disabling \u201cunfold_batch_matmul.\u201d"}, {"title": "D. Benchmark Test of Architectures on Mobile Device", "content": "We evaluated the number of parameters, multiply-\naccumulate (MAC) operations, energy consumption, and in-\nference real-time factor (RTF) on mobile CPUs and GPUs.\nTesting was conducted on a Galaxy S23 with battery pro-\ntection at 85%, and battery consumption was tracked using\nthe dumpsys tool [21]. A 5-minute sample was played with\nintervals to avoid throttling, and input samples were segmented\ninto 3-second chunks.\nWe also tested MEnc&Dec (skipping all TS-Conformer blocks)\nand MTF-DPRNN, which replaces TS-Conformer blocks with\ndual-path recurrent neural networks (DPRNN) [22]."}, {"title": "IV. EXPERIMENTS", "content": "To simulate the eq (1), we set ISM variables as follows:\n1) The room dimensions (length, width, height) are ran-\ndomly selected in (6.0 \u00b1 1.5, 6.0 \u00b1 1.5, 2.6 \u00b1 0.2) m.\n2) The position of microphone is randomly selected in (3.0\n\u00b1 0.7, 3.0 \u00b1 0.7, 0.8 \u00b1 0.7) m.\n3) The RT60s are randomly selected in (0.15, 1.0) sec. The\ndistance threshold is set to 0.5 m.\n4) The materials of all six walls are randomly selected from\nthe toolbox in possible choices; all wall materials except\nthe floor are set to 'anechoic' for outdoor environments.\n5) The positions of near and far sources are randomly away\nfrom (0.02, 0.5) m and (1.3, 1.7) m, respectively. The\nfar region not included in the training dataset is defined\nas the unseen region (UR).\nAfter applying impulse responses to each near and far source,\nthe background noise was mixed into this signal within {0,\n5, 10, 15, 20} dB signal-to-noise ratios (SNRs). All random\nvariables follow the uniform distributions."}, {"title": "B. Datasets", "content": "For the training dataset, we used the CSTR-VCTK [23],\nAISHELL [24], AI-Hub datasets* [25]\u2013[27] for source speak-\ners and background noises. The dataset contains approximately\n5,000 hours of speech data, including about 500 speakers, 5\nlanguages, and 8 emotion styles. It also contains approximately\n1,000 hours of sound data from noisy environments, classified\ninto various categories. The training dataset is divided into a\ntraining set and a validation set with a ratio of 9:1. For the test\ndataset, we used the wsj0 [28] for source speakers. We used\nthe internal noise dataset that contains 30 hours of sound data"}, {"title": "D. Evaluation of the basic performances of DSS models in indoor environments", "content": "We tested for the simulated a total of 100 indoor samples\nusing the test dataset and measured the average scale-invariant\nSDR improvement (SI-SDRi) [30] to confirm the separation\nperformance. The results are shown in TABLE II. When\nneither near nor far source was included in the simulated\nsample, all models predominantly predicted silence. When we\nused mixed cases, $M_{proposed}$ performed better than $M_{Baseline}$,\nindicating that the linear RSA was successfully replaced with.\nMoreover, we confirmed the importance of conformer blocks\nby observing the performance of MRoformer, which replaces\nconformer blocks with transformer blocks using the RoPE [6].\nThe $M_{TF-DPRNN}$ exhibited superior performance when the\ntarget was near compared to the architectures using the TS-\nConformer. Conversely, its performance was comparatively\nlower in the two cases where only one distance source existed.\nThe near contains only speech, while the far contains back-\nground noise. The $M_{TF-DPRNN}$ performed well in separating\nonly speech. If bidirectional LSTM is fully supported for\nTFLite GPU delegate, the DPRNN could be competitive for\nthe DSS problem."}, {"title": "E. Evaluation in outdoor environments", "content": "When simulating the far sources, we divided them into\none seen region (SR) and three URs. UR-0, UR-1 and UR-2\nrepresented the distance range of (0.5, 0.8) m, (0.8, 1.2) m,\nand (1.8, 2.2) m, respectively; we discussed the results of UR-\n0 in section IV-F. We tested for the simulated a total of 200\noutdoor and indoor samples with at least one speaker from\nboth targets and measured the average SI-SDRi. The results\nare shown in TABLE III, and the output spectra of each model\nfor the real outdoor sample are depicted in Fig. 3, where the\ninput sample is an audio sample of 5 seconds recording of two\nmen speaking in a windy outdoor. The overall performance\nof the proposed method, considering the outdoor dataset, was\nbetter than that considering the indoor dataset alone."}, {"title": "F. Limitation and future work", "content": "For the UR-0, we observed that some near sounds were\nfrequently mixed in estimated far sounds, known as the\npermutation ambiguity problem [31]. Within this range, even\nhumans have difficulty classifying an audio source as near and\nfar recorded by a single-channel microphone.\nTo resolve the issue, ensuring that both outputs do not\nhave the same source is crucial for improving the proposed\nmodel. A potential area for further exploration in this domain\nis to include embeddings that can hierarchically represent the\ndistance and speaker information of sound sources [3]."}, {"title": "V. CONCLUSION", "content": "We present a new DSS model for mobile GPUs in outdoor\nand indoor environments. The proposed model uses a TS-\nConformer block, linear complexity RSA, and TFLite GPU\ndelegate to improve mobile device energy consumption and\nreal-time inference speed. In both outdoor and indoor en-\nvironment tests, the proposed model trained with a dataset\nthat included outdoor environments outperformed the model\ntrained only with indoor data."}]}