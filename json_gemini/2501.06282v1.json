{"title": "MinMo: A Multimodal Large Language Model for Seamless Voice Interaction", "authors": ["FunAudioLLM Team"], "abstract": "Recent advancements of large language models (LLMs) and subsequent multimodal speech-text models have provided promising foundation technologies for achieving seamless voice interactions, that is, real-time, natural, smooth, and human-like voice conversations between the user and the system. Prior works of speech-text multimodal models for voice interactions can be roughly categorized into native and aligned models. Native multimodal models simultaneously model end-to-end understanding and generation of both speech and text with a single framework; however, they face the challenges of drastic discrepancy between speech and text sequence lengths, insufficient speech pre-training, and catastrophic forgetting of knowledge of text LLMs. Aligned multimodal models are more successful at maintaining capabilities of text LLMs; yet existing models are usually trained on small-scale speech data, investigated on a limited set of speech tasks, and lack systematic exploration of instruction-following capabilities for rich and nuanced speaking styles. In this work, we introduce MinMo, a Multimodal Large Language Model with approximately 8B parameters for seamless voice interaction. We address the main limitations of prior aligned multimodal models. We train MinMo through multiple stages of speech-to-text alignment, text-to-speech alignment, speech-to-speech alignment, and duplex interaction alignment, on 1.4 million hours of diverse speech data and a broad range of speech tasks. After the multi-stage training, MinMo achieves state-of-the-art performance across various benchmarks for voice comprehension and generation while maintaining the capabilities of text LLMs, and also facilitates full-duplex conversation, that is, simultaneous two-way communication between the user and the system. Moreover, we propose a novel and simple voice decoder that outperforms prior models in voice generation. The enhanced instruction-following capabilities of MinMo supports controlling speech generation based on user instructions, with various nuances including emotions, dialects, and speaking rates, and mimicking specific voices. For MinMo, the speech-to-text latency is approximately 100ms, full-duplex latency is approximately 600ms in theory and 800ms in practice. The MinMo project web page is https://funaudiollm.github.io/minmo, and the code and models will be released soon.", "sections": [{"title": "1 Introduction", "content": "Seamless voice interaction indicates that a user experiences real-time, natural, relevant, and human-like spoken conversation with the system. Facilitating seamless voice interaction poses great challenges: (1) the system needs to understand audio accurately and comprehensively, including comprehending the content and also paralinguistic cues in speech (e.g., emotion, prosody) as well as audio events; (2) the system is expected to produce natural and expressive speech response; (3) the system should provide relevant and reasonable response to the user, as an intelligent chatbot; (4) the system is expected to support full-duplex conversation (simultaneous two-way communication), that is, the system listens while speaking and the user is free to interrupt when the system is speaking, then the system either continues the speech, or concedes it, listens to the user, and provides response to the new user query.\nIn recent years, seamless voice interaction systems have gained significant momentum, especially with the advancements in multimodal large language models, such as GPT-40 (Hurst et al., 2024) and Moshi (D\u00e9fossez et al., 2024). These systems not only produce natural and expressive speech but also understand cues beyond words, including emotional tones and audio events. Current multimodal language models for voice interaction can be categorized into two main categories. The first category includes native multimodal models, such as Moshi (D\u00e9fossez et al., 2024) and GLM-4-Voice (Zeng et al., 2024). These models typically use a decoder-only Transformer as the backbone to simultaneously model understanding and generation of both speech and text modalities within a single framework; they usually require pre-training with both speech and text data. These models suffer from two major limitations. Firstly, after speech discretization, speech token sequences are often more than twice the length of text (e.g., 12.5 tokens per second in Moshi). This discrepancy in sequence length poses challenges as model sizes grow, such as the 175B GPT-3 (Brown et al., 2020). Secondly, the scarcity of speech data compared to text leads to highly imbalanced speech-text training data and in turn causes catastrophic forgetting (Wang et al., 2024b).\nThe second category includes aligned multimodal models, integrating voice capabilities while aiming to maintain the capabilities of the existing pre-trained text LLM. This results in intermediate outputs that still contain text, as seen in models such as Llama-Omni (Fang et al., 2024) and Freeze-Omni (Wang et al., 2024b). However, these alignment-based models are typically trained on limited speech data (200K samples for LLaMA-Omni and 120K hours for Freeze-Omni), leading to questions on the impact of larger speech datasets on model capabilities and whether the chat capabilities of the original text-LLM might be compromised. Furthermore, investigation of extensive speech tasks has not been conducted on these models, such as speech translation, emotion recognition, speaker analysis, language identification, and audio event detection. Moreover, these models lack systematic evaluations of instruction-following capabilities for rich and nuanced speaking styles, as well as lacking development and evaluation of full-duplex conversation capabilities, for achieving seamless voice interaction.\nIn this work, we introduce a new multimodal large language model MinMo, to address these limitations of existing aligned multimodal models. MinMo is trained on over 1.4 million hours of speech data, encompassing various tasks such as Speech-to-Text, Text-to-Speech, and Speech-to-Speech, as detailed in Table 2. This extensive training enables MinMo to achieve state-of-the-art (SOTA) performance across various benchmarks, as shown in Figure 1. We also apply methods that effectively mitigate catastrophic forgetting of the chat capabilities of the original text-LLM while enhancing voice comprehension and generation after training on such large-scale datasets.\nWe also propose a novel voice decoder that balances structural simplicity and competitive voice generation performance. LLaMA-Omni uses a non-autoregressive (NAR) streaming Transformer, which takes the output hidden states of the LLM as input and employs connectionist temporal classification (CTC) to predict the discrete speech token sequence of the response. This approach suffers from inferior performance compared to autoregressive speech decoder. Freeze-Omni uses three speech decoders, including NAR prefix speech decoder, NAR speech decoder, and AR speech decoder, which complicates the model structure. Different from both of these strategies, we design an AR streaming Transformer for MinMo, which mixes the output hidden states of the LLM with speech tokens, based on a fixed ratio, as shown in Figure 3.\nOur contributions can be summarized as follows:\n\u2022 We propose MinMo, an end-to-end aligned multimodal large model that gains audio understanding, audio generation, and end-to-end duplex speech interaction capabilities by adapting a pre-trained text large language model (LLM) through a multi-stage alignment strategy over 1.4 million hours of audio data covering a wide range of speech tasks. MinMo achieves state-of-the-art (SOTA) performance on multiple open-source benchmarks, including spoken dialogue, multilingual speech recognition, speech translation, emotion recognition, and speaker analysis. Different from previous multimodal models that often suffer from notable catastrophic forgetting of capabilities of the text LLM and significant performance degradation on text tasks, MinMo has minimal loss in the original capabilities of the text LLM.\n\u2022 We propose a novel alignment method for streaming end-to-end audio generation, by exploring the use of the hidden layer representations of the text model as inputs to the Voice Decoder for aligning the audio output modality. Experimental results demonstrate that our streaming voice decoder effectively balances structural simplicity, low latency, and high voice generation performance, and outperforms previous models. Additionally, while most existing voice interaction systems only support controlling the content of the response, MinMo enhances instruction-following capabilities and enables the generation of speech corresponding to user-specified emotions, dialects, and speaking rates, as well as mimicking specific voices with a 98.4% instruction-following accuracy.\n\u2022 We develop a mechanism that effectively facilitates full-duplex interactions with MinMo. Specifically, we implement a full-duplex prediction module that harnesses the text LLM's semantic understanding capabilities to decide whether to continue system response, or concede, listen, and respond to new user query. For MinMo, the speech-to-text latency is approximately 100ms; the full-duplex latency is approximately 600ms in theory and 800ms in practice."}, {"title": "2 Related Work", "content": "Multimodal Spoken Dialogue Models A variety of speech foundation models have been developed for generic audio understanding, but not systematically explored for voice interaction."}, {"title": "3 MinMo", "content": "3.1 Model Architecture\nFigure 3 illustrates the model architecture of MinMo. MinMo employs a lightweight modality alignment approach on a pretrained text LLM. Table 1 provides detailed descriptions of each module in MinMo.\nThe Voice Encoder is initialized with the pretrained SenseVoice-large encoder module (An et al., 2024), which provides robust voice understanding capabilities and supports multilingual speech recognition, emotion recognition, and audio event detection. The Input Projector consists of a randomly initialized two-layer Transformer combined with a CNN layer for dimensional alignment and downsampling. We use the pretrained Qwen2.5-7B-instruct model (Team, 2024)\u00b9 as the pre-trained text LLM, due to its outstanding performance on various benchmarks (Team, 2024). We utilize the streaming audio generation mechanism of CosyVoice 2 (Du et al., 2024b), due to its low latency and competitive speech synthesis performance. For every batch of five text tokens received, we pass these tokens and their corresponding final hidden layer vectors simultaneously to the Output Projector and the Voice Token LM. The Output Projector is a single-layer linear module randomly initialized for dimensional alignment. The Voice Token LM uses the pretrained CosyVoice 2 LM module. The Voice Token LM then autoregressively generates fifteen speech tokens, ensuring efficient and seamless audio synthesis. These audio tokens are processed in real time by the Token2wav Synthesizer module to produce the final audio output. The Token2wav Synthesizer comprises a pretrained flow-matching model, which converts tokens to mel spectrograms, and a pretrained vocoder, which transforms mel spectrograms into waveforms, both sourced from CosyVoice 2. MinMo is fully trained end-to-end using additional hidden embeddings, which facilitate control of speech styles, such as emotion, dialect, and speaking rate, based on user instructions. Details of voice generation are elaborated in Section 3.2. The Full Duplex Predictor module consists of a single-layer Transformer and a linear softmax output layer, both randomly initialized. This module performs real-time prediction on whether to engage with user commands or temporarily halt the ongoing system broadcast to allow for processing further audio input from the user. Once the Full Duplex Predictor decides that a system response is appropriate, MinMo produces text outputs and concurrently generates the audio tokens in a token-by-token manner.\nMinMo has approximately 8 billion parameters in total. The training procedure of MinMo is detailed in Section 3.4. The end-to-end latency from receiving the user's audio input to delivering the audio response is approximately 600 ms, when tested on the L20 GPU.\n3.2 Streaming Voice Decoder\nTo facilitate natural voice responses for MinMo, we introduce a novel voice decoder that transforms textual outputs from an LLM into speech. As illustrated at the top of Figure 3, our voice decoder comprises three components: an output projector, a voice token language model (LM), and a streaming token-to-wave (token2wav) synthesizer.\nThe output projector aligns the dimensions of the LLM with those of the voice decoder. The hidden states from the LLM contain rich contextual information but are semantically ambiguous; whereas, the sampled text tokens are more precise and consistent with the generated text. Meanwhile, the hidden states of the current round of user input contain explicit instruction information. For every dialog turn, the embeddings of user input, and hidden states of the LLM's last layer output will be concatenated along the feature dimension to form the query embeddings. The query embeddings, and embeddings of five sampled text tokens along with the hidden states of the LLM's last layer output will be concatenated along the sequence dimension and fed into the projector. In this report, the projector's outputs are referred to as semantic vectors, which represent rich and accurate semantic information.\nFollowing the output projector, a voice token LM is employed to generate speech tokens autoregressively. This LM operates on sequences interleaving text and speech tokens. Specifically, we intermix the semantic vectors and speech tokens in a fixed ratio of 5:15, that is, every five semantic vectors are followed by fifteen speech tokens. During training, a teacher forcing strategy is applied, and a special token is introduced to signal that the next semantic vectors should be concatenated. Once the LLM's textual response is complete and the semantic vectors are exhausted, we insert a \"turn of speech\u201d token to signal the voice token LM that subsequent tokens should be speech tokens exclusively. The speech synthesis process concludes when the \u201cend of speech\u201d token is generated.\nFor reconstructing waveforms from the speech tokens, we utilize an off-the-shelf streaming token2wav synthesizer, as described by Du et al. (2024b). The token2wav synthesizer incorporates a chunk-aware flow matching model and a mel-to-wave vocoder, capable of synthesizing waveforms in chunks of fifteen tokens."}, {"title": "3.3 Tasks and Training Data", "content": "The training tasks for MinMo consist of four categories, including Speech-to-Text, Text-to-Speech, Speech-to-Speech, and Speech-to-ControlToken tasks. The specific tasks within each category and their corresponding data scales are presented in Table 2.\nSpeech-to-Text tasks. This category consists of approximately 1.2 million hours of speech-text paired data, including tasks such as automatic speech recognition (ASR), speech-to-text translation (S2TT), language identification (LID), contextual biasing speech recognition, speech emotion recognition (SER), audio event detection (AED), speaker analysis, spoken language smoothing. The training data for these tasks is organized in the ChatML format, illustrated by the following example:\nHere, \"task_instruction\" corresponds to the natural language descriptions for different speech-to-text tasks. For instance, \u201cSpeech Transcription\u201d may be used for speech recognition tasks, while \u201cTranslate {SRC_LANG} into {TGT_LANG}\" may be used for speech translation tasks. \"wav_path\" refers to the input audio file path, while \"task_output\" refers to the output of each task.\nText-to-Speech tasks. The data for this category mainly consists of basic speech synthesis data, which is the same data used for training CosyVoice 2. It includes 170,000 hours of text-speech paired data and supports four languages: Chinese, English, Korean, and Japanese. Additionally, there are approximately 1,000 hours of audio generation data controlled by instructions. The instructions are expanded to include natural language descriptions, generated by Qwen-Max\u00b2, utilizing human-labeled attributes such as emotion, speaking rate, dialect, and role-playing.\nSpeech-to-Speech tasks. The Speech-to-Speech data is primarily sourced through simulation, encompassing approximately 10,000 hours of multi-turn conversational speech and 100 hours of style-controllable multi-turn conversational speech. The method for simulating speech-to-speech chat data is as follows:\n\u2022 For text chat data primarily sourced from Alpaca (Taori et al., 2023) and ShareGPT3, we utilize the zero-shot in-context generation method from CosyVoice (Du et al., 2024a) to convert user text into user speech. We fine-tune CosyVoice's base model with 2 hours of data from a selected speaker to create a speech synthesis model for the target speaker, referred to as CosyVoice-SFT. This model synthesizes the assistant's speech (i.e., system speech). The advantage of using zero-shot\nin-context generation for user speech synthesis is its ability to ensure diversity in the generated user speech, thereby enhancing the generalizability of MinMo.\n\u2022 To address the differences between synthesized and real audio, we select suitable real speech from the ASR data as user speech queries and use the corresponding text as input for Qwen-Max to generate response text, which is then synthesized into assistant speech using the CosyVoice-SFT model. This approach further enhances the model's robustness to real user audio inputs.\n\u2022 To generate conversational speech that covers different speaking styles, we initially employ Qwen-Max to create a rich collection of style-controllable, multi-turn text dialogues. User queries are converted into speech using zero-shot generation by Cosyvoice. Subsequently, we employ Cosyvoice 2 to generate the assistant's expressive speech. Specifically, we input the assistant's response content along with an instructional prompt into Cosyvoice 2 to synthesize speech in specific styles. Additionally, a small, diverse, and preliminary recorded voice corpus is used as prompt speech to synthesize the expressive response speech by zero-shot generation. The former method enhances the diversity of the simulated speech, while the latter more effectively builds the expressiveness of various styles.\nSpeech-to-ControlToken task. The Speech-to-ControlToken data primarily consists of two parts. The first part is extracted from existing real voice interaction data, while the second part is simulated using text dialogue data. Specifically, the existing real voice interaction data includes resources such as Alimeeting (Yu et al., 2022), Fisher (Cieri et al., 2004), and our in-house voice interaction data, a total of approximately 3000 hours. The simulated data mainly includes the open-source MOSS dataset (Sun et al., 2024) and spoken dialogues by synthesizing our in-house text dialogue data, yielding about 1000 hours of voice chat data. When constructing duplex training data using these voice interaction data, we apply heuristic rules for automatically annotating duplex labels on the samples, as follows:\n\u2022 For assistant's turn-taking, the endpoint of the user's turn is taken as the starting point of the assistant's turn.\n\u2022 For user's turn-taking, a time gap T after the assistant's turn ends is taken as the starting point of the user's turn, where  $T \\sim N(0.6, 0.4^2)$.\n\u2022 For user's back-channel, we select instances from the voice interaction data when the user (taking one speaker in a dialogue as the user) is unable to interrupt the other speaker and treat them as training samples of user's back-channels."}, {"title": "3.4 Model Training", "content": "MinMo is trained progressively through four stages of alignment: (1) Speech-to-Text Alignment, (2) Text-to-Speech Alignment, (3) Speech-to-Speech Alignment, and (4) Duplex Interaction Alignment. Through the four alignment stages, MinMo gains its end-to-end audio comprehension and generation capabilities while retaining the capabilities of the backbone text LLM, achieving low latency and facilitating a seamless voice chat experience for the user, similar to GPT-40. The four stages are detailed as follows.\nSpeech-to-Text Alignment. This first stage aligns the audio modality's input latent space and the semantic space of a pre-trained text LLM using Speech-to-Text data shown in Table 2. This phase includes stepwise updates of the Input Projector and Voice Encoder in Figure 3, as well as updating the text LLM using LoRA. Considering that the Voice Encoder and LLM (Qwen2.5-7B) are pre-trained while the Input Projector's parameters are randomly initialized, we perform a pre-alignment training (Pre-align) using a subset of the Speech-to-Text data shown in Table 2, updating only the Input Projector. This Pre-align phase effectively prevents the randomly initialized parameters from having large-gradient influences on the pre-trained Voice Encoder at the initial training stage. After Pre-align, we use the full Speech-to-Text data for training both the Input Projector and the Voice Encoder while keeping LLM parameters frozen-a process called Full-Align. Following Full-Align, instruction fine-tuning (SFT) is conducted using approximately 1.3 million samples covering various tasks. During this stage, LLM is updated using LoRA, enhancing the model's ability to follow instructions. The specific data proportions used in the Full-Align and SFT stages are illustrated in Figure 4. The Pre-Align phase uses about 1/10 of the Full-Align data."}, {"title": "4 Experiments", "content": "We evaluate MinMo across multiple benchmarks, as detailed in Table 4. These evaluation benchmarks cover speech recognition and speech translation tasks (multilingual speech recognition, multilingual speech translation, language identification, and contextual biasing speech recognition), speech analysis and understanding tasks (speech emotion recognition, speaker analysis, and audio event understanding), and speech-to-text enhancement tasks (spoken language smoothing, punctuation, and inverse text normalization). Additionally, we evaluate MinMo on voice generation tasks (text-to-speech and instruction-following voice generation) and voice chat tasks (including spoken question answering, spoken dialogue, and full-duplex interaction tasks)."}, {"title": "4.1 Speech Recognition and Translation", "content": "Multilingual Speech Recognition We evaluate MinMo's speech-to-text transcription capabilities on public test sets in Mandarin, English, Japanese, Korean, and six other languages. These include Aishell-2 (Du et al., 2018), LibriSpeech test clean/other (Panayotov et al., 2015), WenetSpeech (Zhang et al., 2022), Fleurs (Conneau et al., 2023), and Common Voice (Ardila et al., 2019). Table 5 presents the results from different models. For Mandarin (ZH), Japanese (JA), Korean (KO), and Cantonese (YUE), we employ the character error rate (CER) for evaluating transcription performance. For English (EN), German (DE), French (FR), Russian (RU), Spanish (ES), and Italian (IT), the word error rate (WER) is utilized as the evaluation metric. Note that all baseline model results are reproduced and processed using the same procedures as conducted on MinMo's results, for fair comparisons. Post-processing is based on a modified Whisper normalizer (Radford et al., 2023), with modifications primarily for improved number normalization. In Table 5, the \u201cw/ LID\" column indicates that language identification (LID) information, such as English, Chinese, or Korean, is included as part of the decoding prompt, while the \"w/o LID\" column denotes results without the additional LID information. Results in parentheses are directly cited from the papers.\nAs shown in Table 5, MinMo achieves superior ASR performance on most test sets across various languages, compared to Whisper Large v3 (Radford et al., 2023) and Qwen2-Audio (Chu et al., 2024). The \"w/ LID\" columns for Whisper Large-v3 and Qwen2-Audio show similar results to those reported in the original papers. Testing on Common Voice with or without LID information as a prompt shows a significant gap in average error rates for Whisper Large v3 and Qwen2-Audio, indicating that these two models strongly depend on the LID information. In contrast, MinMo demonstrates robust and consistent ASR performance regardless of the presence of the language identification.\nMultilingual Speech Translation We evaluate speech-to-text translation capabilities on the Fleurs (Conneau et al., 2023) and CoVoST2 (Wang et al., 2021) test sets. On the Fleurs test set, we report results for all translation directions supported by our model; whereas, on the CoVoST2 test set, we only report results for translating from English to other languages (en2xx) and vice versa (xx2en), due to dataset limitations, as it primarily focuses on English-centric translation pairs. As shown in Table 6, our end-to-end MinMo consistently outperforms the cascaded model by pipelining Whisper Large V3 and Qwen2.5-7B-Instruct, in terms of BLEU scores. Compared to other end-to-end baselines, MinMo achieves SOTA performance on Chinese English and Japanese English translations and top-tier performance on other language pairs. We attribute this strong performance to the extensive speech translation training data (451K hours of S2TT training data as in Table 2) and the powerful audio encoder. Notably, even though we only augment our training data with the CoVoST2 set, excluding the Fleurs set, our model maintains consistent performance across both test sets, indicating high robustness."}, {"title": "Language Identification", "content": "For evaluation of language identification performance, we use the Fleurs dataset, which covers 102 languages. MinMo achieves language identification accuracy of 85.3%, outperforming all previous models shown in Table 7. Specifically, zero-shot Whisper-V3 often miscategorizes Cantonese as Chinese while MinMo accurately identifies Cantonese. The accuracy of Zero-shot Whisper is not competitive due to the fact that it misses training data for 20 languages in the Fleurs dataset; whereas, the LID training data of MinMo covers all 102 languages in Fleurs."}, {"title": "Contextual Biasing Speech Recognition", "content": "Contextual biasing, or hotword customization, allows users to obtain customized ASR results with specific contexts or hotwords. MinMo enhances ASR capabilities by integrating advanced prompts for contextual biasing. We prepare corresponding training data for alignment and SFT stages, by organizing hotwords within prompts preceding speech processing instructions, to enable effective customization. Evaluations include hotwords biasing test and general biasing test, as shown in Table 8. The hotwords biasing test involves three data sets used by SeACo-Paraformer (Shi et al., 2024), which contain hotwords for biasing evaluation. The general biasing test uses data sets with fewer hotwords to assess resistance to irrelevant ones.\nWe compare MinMo with the baseline model SeACo-Paraformer (Shi et al., 2024), which is an open-source ASR system with strong Chinese hotword biasing performance. Table 9 shows that MinMo outperforms the competitive baseline SeACo-Paraformer in terms of ASR accuracy (both with and without hotwords) and also recall rates of hard-case hotwords. Table 10 further demonstrates that MinMo achieves contextual biasing capability in multiple languages, without compromising its general ASR performance."}, {"title": "4.2 Speech Analysis and Understanding", "content": "Speech Emotion Recognition We evaluate the Speech Emotion Recognition (SER) capability of MinMo using seven widely used emotion recognition datasets from EmoBox, including CREMA-D (Cao et al., 2014), MELD (Poria et al., 2019), IEMOCAP (Busso et al., 2008), MSP-Podcast (Martinez-Lucas et al., 2020), CASIA (Zhang & Jia, 2008), MER2023 (Lian et al., 2023), and ESD (Zhou et al., 2021). These datasets include both Chinese and English languages and scenarios such as acting, TV dramas, and daily conversations. We adopt unweighted average accuracy (UA), weighted average accuracy (WA), and macro F1 Score (F1) as evaluation metrics. Results on these test sets from the recent SER toolkit EmoBox (Ma et al., 2024a) are cited. We also evaluate the baseline audio-LLM models SALMONN and Qwen-Audio using their released model checkpoints. As shown in Table 11, MinMo outperforms the baseline audio-LLM models on most datasets, particularly achieving nearly 100% accuracy on acting audio datasets (CASIA, CREMA-D, ESD). It is important to point out that the comparison between MinMo and the baseline models SALMONN and Qwen-Audio on the aforementioned datasets is inconclusive, since the optimal prompts and post-processing methods for the baseline models are unclear. Therefore, we further utilize the Air-Bench benchmark, which is specifically designed for evaluating large audio language models with standardized post-processing and scoring scripts for fair comparison. As shown in Table 12, MinMo outperforms all the baseline models on all tasks on this benchmark, including Language ID, Gender, Age, Emotion, Vocal Sound classification tasks, except for being outperformed by Qwen-Audio on the sound question classification task.\nIn addition to Chinese and English languages, we also evaluate MinMo on low-resource languages in a zero-shot setting, as shown in Table 13. All utterances and reference labels are derived from the EmoBox benchmark, which provides the official training and evaluation partitions for the 32 publicly available SER datasets in 14 languages. EmoBox evaluates the SER capabilities of 10 different pre-trained models across all datasets, with the classification results used as reference labels in Table 13. Despite the absence of in-domain audio used in training, MinMo achieves the best F1 score on most languages, even for those languages not included in the original training data of MinMo. These results highlight MinMo's excellent cross-lingual generalization capability.\nAudio Event Understanding We compare MinMo's voice and audio event understanding capabilities against other Audio-LLM models, using the Air-Bench benchmark. The results are shown in Table 12. On the voice sound classification task (Vocal Sound), MinMo surpasses all baseline models. However, we find that on more complex sound question-answering tasks, MinMo performs worse than Qwen-Audio although still outperforming other models. This can be attributed to two factors: first, with the voice encoder and the training paradigm, MinMo is primarily designed for voice interaction, hence some sound questions may exceed its scope; second, during evaluation, MinMo predicts what happens in the audio rather than strictly choosing the options provided by the Air-Bench, hence some correct or similar-to-correct responses generated by MinMo are aligned with incorrect choices by the post processing script.\nSpeaker Analysis Speaker analysis involves several tasks that are essential for understanding and interacting with audio data, including gender detection, age estimation, speaker counting, speaker identification, multi-speaker recognition, and target speaker recognition. In this report, we focus on evaluating MinMo's performance in gender detection and age estimation. Table 12 compares MinMo against the baseline models on these tasks on the AIR-Bench benchmark, in terms of classification accuracy. The results reveal that MinMo outperforms all the baseline models on gender detection and age estimation tasks."}, {"title": "4.3 Speech-to-Text Enhancement", "content": "Spoken Language Smoothing The spoken language smoothing task takes the ASR transcripts of spoken language, and outputs formal-style written text. Examples of spoken language smoothing are shown in Table 14. For this task, we construct a multi-domain dataset for training and evaluation, by extending the SWAB dataset (Liu et al., 2025) that we create for spoken-to-written conversion of ASR transcripts. The SWAB dataset is derived from Chinese and English meetings, podcasts, and lectures. After the generation of ASR transcripts for the original videos and audios, approximately ten annotators create formal-style written text based on the ASR transcripts while preserving their original content. The training set of SWAB comprises 20,000 paragraphs, and the test set includes 100 randomly sampled paragraphs in both Chinese and English. We conduct full fine-tuning and compare MinMo with Qwen2.5-7B-based model on the SWAB test set, with results shown in Table 15. For objective metrics, we calculate BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and BLEURT (Sellam et al., 2020) with the human target as reference. Notably, we observe that the spoken language smoothing task shows significant subjectivity and diversity; therefore, objective metrics based on lexical matching may not adequately reflect the performance. Consequently, we use human and LLM annotations to provide rankings of faithfulness (S-Faithful (i.e., faithfulness to the original content) and formality (S-Formal). The prompts for automated LLM scoring are presented in Appendix A.1. Table 15 shows that the performance of our model and Qwen2.5-7B is comparable, suggesting that MinMo possesses reasonable capability to smooth spoken language."}, {"title": "Punctuation Insertion and Inverse Text Normalization", "content": "For the punctuation insertion (PUNC) and Inverse Text Normalization (ITN) tasks, we use the Chinese and English data from the Fleurs dataset. We compare MinMo against SenseVoice-L and whisper-large-v3, as shown in Table 16. Given the subjectivity of the punctuation insertion and ITN tasks, we employ GPT-4 Turbo to rank the three outcomes for evaluation. The task prompt for automated scoring is available in Appendix A.2. The first place receives 3 points, the second place 2 points, and the third place 1 point. The final score is the average of all scores. When preparing the test data, we use randomized option shuffling and multiple scoring rounds to reduce uncertainty when using ChatGPT for evaluation.\nThe final results demonstrate that MinMo performs better in the subjective evaluations of punctuation insertion and ITN."}, {"title": "4.4 Voice Generation", "content": "Text-to-Speech (TTS) To evaluate the synthesis accuracy of our voice decoder, we converted the recent SEED test set (Anastassiou et al., 2024) into the ChatLM format. In this format, the text is presented as the user content prefixed with a \u201cCopy:\u201d command, and the LLM is expected to replicate this text. The test set comprises 2,020 cases in Chinese and 1,088 cases in English. For the Chinese cases, we utilized the Paraformer-zh model (Gao et al., 2022), while the English cases were processed using Whisper-large V3 (Radford et al., 2023). Given the instruction non-following issue with LLMs, we applied a teacher forcing scheme during inference to minimize discrepancies between the input and output text. The content consistency of the voice decoder was evaluated using CER for Chinese and WER for English.\nOur findings indicate that even with the teacher forcing scheme, only about 20% of the test cases had identical input and output text from the LLM. Because inconsistent input and output can lead to confused hidden states for the voice decoder, only test cases with consistent input-output text were included for error rate calculation. The results are presented in Table 17. We observed that MinMo's voice decoder has slightly reduced content consistency and speech quality on the Chinese test sets compared to the TTS baseline, Cosy Voice 2.0-SFT (Du et al., 2024b). On the English test set, MinMo achieves similar content consistency but with a slightly lower NMOS score. This reduction can be attributed to the differing acoustic characteristics of the fine-tuned speakers, which affect both the recognition model and NMOS scorer. However, this reduction does not significantly hinder human understanding. Therefore, subjective evaluation might be more appropriate for speech-to-speech voice chat models, which will be explored in our future work.\nInstruction-following Voice Generation To evaluate the performance of instruction-following voice generation, we develop a multi-turn Chinese speech-to-speech test set consisting of 30 sessions and 122 turns, incorporating 12 types of instructional controls. These controls include emotions (happy, sad, surprised, angry, fearful), dialects (Cantonese, Sichuan), speaking rates (fast, slow), role-playing (robot, Peppa), and a default style. To assess the accuracy of instruction-following voice generation, listeners classify the generated audio according to the instruction type. As shown in Table 18, MinMo demonstrates superior instruction control accuracy compared to the baseline GLM-4-Voice, particularly in dialects and role-playing."}, {"title": "4.5 Voice Chat", "content": "Spoken Question Answering and Spoken Dialogue To transfer the dialog capabilities of the base model to the speech modality, we construct multi-turn conversational data for both speech-to-text (speech2text) and speech-to-speech (speech2speech) scenarios. The speech2text data is primarily divided into two parts. First, it originates from open-source multi-turn text-only data, where we synthesize the user turns using zero-shot Text-to-Speech (TTS) technology. Second, we use real Automatic Speech Recognition (ASR) training data as chat queries to obtain text responses"}]}