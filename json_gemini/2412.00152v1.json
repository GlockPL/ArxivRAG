{"title": "Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery", "authors": ["Quentin Houbre", "Roel Pieters"], "abstract": "The autonomous learning of new goals in robotics remains a complex issue to address. Here, we propose a model where curiosity influence learning flexibility. To do so, this paper proposes to root curiosity and attention together by taking inspiration from the Locus Coeruleus-Norepinephrine system along with various cognitive processes such as cognitive persistence and visual habituation. We apply our approach by experimenting with a simulated robotic arm on a set of objects with varying difficulty. The robot first discovers new goals via bottom-up attention through motor babbling with an inhibition of return mechanism, then engage to the learning of goals due to neural activity arising within the curiosity mechanism. The architecture is modelled with dynamic neural fields and the learning of goals such as pushing the objects in diverse directions is supported by the use of forward and inverse models implemented by multi-layer perceptrons. The adoption of dynamic neural fields to model curiosity, habituation and persistence allows the robot to demonstrate various learning trajectories depending on the object. In addition, the approach exhibits interesting properties regarding the learning of similar goals as well as the continuous switch between exploration and exploitation.", "sections": [{"title": "I. INTRODUCTION", "content": "Developmental robotics takes inspiration from various fields such as developmental psychology, neuroscience, machine learning or philosophy. A popular domain of interest in the field and one of the core idea of this study is called Curiosity. Curiosity can be seen as a particular case of Intrinsic Motivation (IM) and researchers have demonstrated that IM could be an efficient drive for the autonomous learning of behavior [1]. In addition, the learning of various behavior exhibits a developmental pattern [2], [3]. Nowadays, there are multiple ways of modelling curiosity and one of them consists of training predictors such as forward and inverse models and thus displaying a learning progress that depends on the error variation.\nAlong with curiosity, attention is a fundamental cognitive process. Indeed, attention allows humans to focus and concentrate on specific elements of our surroundings. However, attention is a broad term gathering several specific mechanisms. For example, visual attention is the ability to sustain the gaze at salient stimuli and is developing during the first months after birth [4], [5]. To help search for specific visual stimuli,\ninhibition of return is a well studied mechanism naturally preventing a person to look twice at the same location within a short interval of time [6]. In addition, we can distinguish two types of visual attention : exogenous, driven by external stimuli or endogenous and goal-driven [7], [8]. These two types of attention are often referred as bottom-up for the former and top-down for the later, and inhibition of return can happen during both, but whether there are shared neural pathways between them in order to occur is still subject to debate [9]. In the brain, arousal is mediated by the Locus Coeruleus-Norepinephrine (LC-NE) and demonstrates different patterns of neural activity. According to the adaptive gain theory [10], LC neurons exhibit two modes of activation: tonic and phasic. During tonic activation, the subject disengages from the current task and engages in an exploratory behavior. On the contrary, phasic LC activation engages in exploitation and thus is driven by task specific outcomes. It has been demonstrated that IM is involved in tonic and phasic activation of dopamine [11], overlapping certain neural pathways with noradrenaline (norepinephrine) [12] and thus might be directly interacting with the LC-NE system. If curiosity and more especially its learning progress component is related to attention, then the LC-NE system is a potential starting point.\nHere, we propose a robotic cognitive architecture for the online discovery and learning of goals where a robotic arm is learning how to interact with simple objects. To do so, we introduce a model situating the LC-NE system at the center of different processes, thus determining if the robot should explore and discover potential goals through bottom-up attention, or exploit and learn to achieve these goals. During both exploration and exploitation, an inhibition of return mechanism generates a set of actions toward the object location until producing a change in the environment. If the inhibition of return is shared by exploration and exploitation, there is however a conceptual difference between them since the discovery of a new goal in the environment rests on exogenous attention, and on a top-down and goal driven approach for the learning of these goals. For the exploration stage, we complete the attentional mechanism connected to the LC-NE system with an habituation paradigm [13], where the discovery of a new goal produces a tonic activation and limits the time of exploration. To balance this process, the absence of a new goal signals a shift from tonic to phasic activation. During the latter, the learning of a goal occurs with the use of simple neural networks implementing a forward and inverse model. The output from these neural networks determine an error associated with a goal and its learning progress."}, {"title": "II. RELATED WORK", "content": "Visual attention is essential for an individual to focus on\nan unexpected event and to optimize cognitive processes on a\nspecific task [8]. This distinction separates exogenous (bottom-\nup) from endogenous (top-down) attention. If motivation can\nsharpen exogenous spatial attention under certain conditions\n[28], the modulation of top-down attention can enhance the\nlearning of a particular task [29]\u2013[31]. The neural basis for\nthese distinct processes are, for a significant part, shared\n[32], but not entirely [33]. Regarding the coupling between\nattention and intrinsic motivation, a study used reinforcement\nlearning to allocate and shift attention [34]. More close to our\napproach, researchers used bottom-up attention to learn object\naffordances and decompose a task into sub-goals [35]. Here,\nwe choose to focus on the inhibition of return effect (IOR) to\nfacilitate attention toward an object and ease the discovery\nof new goals. In the literature, the general mechanism of\nexploration is often resumed as random and direct exploration\n[36], [37] where the former consists of producing random\nactions with a high uncertainty but potentially large expected\nreward. The latter directs exploration toward more certain\nrewards but necessitates prior information. With curiosity in\nrobotics, this difference can be translated to a motor babbling\nbehavior for random exploration and the building of repre-\nsentations for direct exploration. For example, a robot can\nlearn a representation of a diverse set of goals encountered\nduring random exploration to generate new goals [24]. Here,\nour purpose is to propose an additional exploration method\nthat can reduce the uncertainty of random exploration via the\ninhibition of return effect. This mechanism was pointed out\nby Posner [6] in visual attention when he discovered that the\nreturn of attention to a cue previously attended expresses a\nlonger time. On the opposite, there is a facilitation (faster\nresponse time) if the time interval between cue-targets is short.\nThis consequence endows the brain with a natural foraging\nmechanism of visual stimuli [38], but it remains necessary to\nproceed to an attentional disengagement in order to observe"}, {"title": "C. Exploration, exploitation and the Locus Coeruleus system", "content": "The locus coeruleus-norepinephrine system (LC-NE) is a\nbrainstem nuclei that is widely connected to the neocortex.\nThe first unifying approach regarding its role and functions\nis the adaptive gain theory [10]. The theory demonstrates two\ndifferent neural activations, depending on the arousal state as\nwell as the relation with task engagement. More precisely,\nthe LC-NE system exhibits a tonic neural activation during\nexploration, rising arousal's level and thus bottom-up attention\ntoward salient events [46]. Here, the exploration strategy con-\nsists of generating goal poses around the object with IOR and\nobserve the possible outcomes. A phasic activation suggests a\ngoal-oriented behavior (exploitation) with a focus on the task\n(top-down attention). Then, the LC-NE system is a crossroad\nbetween attentional processes and decision making. Aston-\nJones and Cohen determine the emergence of a tonic activation\nwhen the utility of a task vanishes. The LC-NE receives direct\nconnection from the anterior cingulate (Acc) and orbitofrontal\ncortices (OFC) which are directly involved in task-related\nutility and decision making [47], [48]. More specifically,\nthe ACC seems to represent value predictions about reward and,\nmore generally, uncertainty [49]. There is a large consensus\nto recognize the basal ganglia as the processing place of\nrewards that can support reinforcement learning [50]. Several\nrobotics experiments take inspiration from the phasic delivery\nof dopamine to signal rewards based on prediction errors [51],\n[52]. We can observe a similarity between the phasic delivery\nof noradrenaline from the LC and the phasic dopamine-\nbased signal emerging from the basal ganglia, especially after\npointing out that dopamine and noradrenaline share neural\npathways [53]. In addition, researchers demonstrate that if both\ndopamine and noradrenaline are related to rewards prediction,\nonly the latter is predictive of task engagement and is strongly\nactivated when cues indicate a new task condition. This means"}, {"title": "D. Habituation and Persistence", "content": "In order to limit the time spent on exploration, our approach\nintroduces a cognitive process called habituation. Indeed, there\nis no consensus as to how the LC switch from a tonic mode\nwith high arousal and engages exploration to a phasic mode\n(exploitation). Here, we propose to model that switch by an\nhabituation paradigm where the phasic activation from LC\nhappens only when the robot's perception of an object does not\nproduce a novelty effect anymore. Habituation is well studied\nby researchers, especially in developmental psychology by\nevaluating how long infants examine a new stimuli. Research\nfirst focused on the link between familiarity of the perception\nand exploratory behavior [57] and concluded that infants that\nhave been familiarized to some objects prefer to look at new\nones. A similar study [58] confirms that infants of seven and\ntwelve months old have a decrease of attention when the\nobjects become familiar. Interestingly, the study determined\nthat the looking duration was not dependent on the age but\nmore on the familiarity of the object. If these findings concern\nan exploration behavior and how habituation can influence it,\nthe duration at which an individual learns a new skill must be\ntaken into account.\nCognitive persistence is a fundamental aptitude if one wants\nto learn a new goal. Observing how individuals persevere to\nlearn something new can help to decide how and if a learning\nis actually fruitful. Regarding developmental psychology, the\nmeasure of persistence is investigated by assigning tasks to"}, {"title": "E. Dynamic Field Theory", "content": "In this work, we model the different cognitive process with\nneural dynamics and more especially Dynamic Field Theory\n(DFT) [14]. By doing so, the system exhibits a range of\ndifferent behaviors by only tuning the intrinsic parameters of\nthe neural fields. The theoretical framework mathematically\nmodels the evolution and activity in time of large populations\nof neurons. The approach has been successful to model com-\nplex cognitive processed, such as visual working memory [63],\nintentionality [64] or motor habituation [65]. The formulation\nand implementation of DFT is provided in the Appendix A.\nRegarding the modelling of motions, we adopted the dynamic\nmovement primitive framework."}, {"title": "F. Dynamic Movement Primitive", "content": "In this work, we control the motion of a robotic arm by\nusing the Dynamic Movement Primitive framework (DMP)\n[66], as they can exhibit a wide range of motions with a\nlimited number of parameters [67]. The approach uses a set\nof differential equations to represent a movement with the\nadvantage of being robust against perturbation. More precisely,\nwe adopt the formalism introduced by Pastor [68], which\nallows to reach the generalization of a motion by adapting\na start and goal parameter. More details about the method\nand its implementation can be found in the appendix B."}, {"title": "III. ARCHITECTURE", "content": "The Locus Coeruleus (LC) architecture is the main com-\nponent of our model. In the brain, this nucleus is directly\nresponsible for attention and task commitment. Here, we\nlink intrinsic motivation and the LC activity through the"}, {"title": "A. Overview", "content": "engagement in learning new goals with different types of\nattention. We propose to articulate and describe how these\ncognitive processes might work together in this section.\nThe LC exhibits two modes of neural activation : tonic\nfor exogenous attention and a phasic spiking mode for top-\ndown attention during task engagement. Because of the time\nscale abstraction level, the DFT framework can not precisely\ndifferentiate them as well as biological neurons. Then, we\ndefine a tonic activation as a sustained and uninterrupted\nexcitation activity while the phasic mode exhibits a more\nsparse and localized activation in time. Figure 1-left depicts\nthe architecture flow from goal discovery with bottom-up\nattention to learning with curiosity, followed by the robot.\nAt first, the robot detects any salient stimuli that can take\nplace in the camera field of view. Then, the process of goal\ndiscovery begins by performing action babbling around the\nobject. During this stage, we can consider that the type of\nattention is exogenous, since the robot is sensitive to any\nstimulis (objects) in the environment. At the same time, an\nhabituation mechanism reduces the sensitivity towards the\nobjects, thus reducing the neural activity within the LC. Then,\nthe robot selects a goal to learn with the help of the curiosity\nand persistence mechanisms. At that step, there is a shift from\nbottom-up to top-down attention since the model only focuses\non observing outcomes from a particular object. Finally, the\nrobot can switch attention to a new object and starts over the\nprocess of goal discovery. However, this last step could be\nignored and the robot will choose to discover new goals on a\npreviously seen object.\nIn this work, we define exploration as the process of goal\ndiscovery and exploitation as the stage to learn these goals\n(See Fig. 1-right). The perception and action module are\ninvolved in both steps by discovering new goals and processing\nalready seen stimulis for the former, and generating or using\na DMP (dynamic motion primitive) for the latter [66]. The\nperception system observes the outcome of an action by\ncoupling the color of the object along with its motion in space.\nThen, the habituation module is in charge to determine if the\nobserved stimulis is seen as new or if the robot is already\nhabituated to this specific object. The persistence, error and\nlearning progress module are taking place during exploitation.\nPersistence determines for how long the robot should attempt\nthe learning of the current goal. The error module monitors the"}, {"title": "B. Locus Coeruleus", "content": "The LC mechanism is coordinating the engagement level\nregarding the discovery or the learning of goals (See Fig. 2).\nAt any time, the active node always exhibits a supra threshold\nlevel of activation and excites the slow boost component, a\nparticular form of memory trace that evolves depending on\ntwo nodes inputs (Appendix A). This memory trace builds ac-\ntivation when the active node projects excitation, maintains the\ncurrent level if both connected nodes are down, and decays the\nactivation if only the threshold node is up. At the beginning,\nthe slow boost module slowly rises the resting level of the\nphasic and tonic neural field (NF) and stops when a peak of\nactivation appears, triggering the stop node. The tonic activity\nof neurons is depicted with the tonic NF and the excitation\nincoming from the habituation mechanism (scene and object\nselection NF). This neural field exhibits a supra-threshold peak\nof activation as long as the robot is not accustomed to an\nobject. The phasic NF gathers activations from the error and\nlearning progress models but with a higher resting level. This\nmeans that after discovering goals, the phasic NF will be the\nfirst to see the emergence of supra-threshold activations. Tonic\nNF has a resting level of -2 and phasic NF -1.05. This implies\nthat the LC can see a peak in tonic NF when the cumulated\nlearning progress and error are below 0.05. This indicates that\nthe LC can drive into exploration again when all the skills\nare learned. After discovering several goals through bottom-up\nattention and being habituated to an object, the robot does not\nexhibit any learning progress. Then, it is necessary to bind the\nerrors memory trace to the phasic NF in order to bootstrap the\nlearning of previously discovered goals. We empirically chose\nthe gain factor so the robot can engage in learning a skill\nif there is no learning progress (\u2248 30 percents of the errors\npresent in the error module). However, we keep the errors\ncontribution low enough to favor the learning of a task that\nhas a small learning progress. By doing so, the robot can begin\nthe learning of goals that do not exhibit any learning progress\ninstead of performing several stage of exploitation for each\ngoal beforehand to build up the different learning progresses.\nThe one dimensional object decision NF represents the object's\ncolor in which the robot should explore or engage in learning.\nBy analyzing the connectivity flow of the model, we can see\nthat we prioritize exploration the same manner that exogenous\nattention seems to be operating within the LC. As mentioned\nin the literature (II), the engagement in a task can only operate\nif there are no salient stimuli and thus a low level of bottom-\nup attention. The priority to exploration comes along with the\ntonic activation. Our approach demonstrates a sustained acti-\nvation (i.e tonic) while discovering new goals, thus preventing\nany possibility to learn a specific skill. During exploitation, the\nmodel selects a goal to learn on a regular basis. This process\nstops any activation within the phasic NF and raises again\nthe resting level to select the goal with the highest learning\nprogress (or error in the absence of any learning progress).\nIn this model, the persistence mechanism directly influence\nthe goal selection and depict a phasic activity. Finally, if the\nrobot is habituated and has learned all the goals discovered,\nthe tonic NF will select a know object to perform exploration\nagain (Appendix C)."}, {"title": "C. Perception and habituation", "content": "The perception module processes the object's motion direc-\ntion after an action. A perception is formalized by a 2D neural\nfield, where the horizontal and vertical dimensions represent\nthe object color and motion's angle as a goal, respectively\n(see Appendix E for more details). The motion of the object\nis defined by an angle between a static reference vector and\nthe observed vector's motion of the object. The angle is bound\nbetween -\u03c0 and radians and scaled between [0,100] to fit\nthe neural field feature space. It is important to emphasize\nthat the detection of the object's motion takes place in real-\ntime, i.e., while the robot performs the action), delivering a\nsustained input to the goal perceived NF (Neural Field). In Fig.\n3, an inhibition of return is taking place with goal perceived\nNF, WM goals and the goals NF. This avoid perceiving a\nstimuli as new when the goal has already been discovered.\nDuring exploration and if the stimuli is new, a dynamic\nmotion primitive (DMP) is generated by the robot controller\nand activates the associated DMP neuron through a one-to-\none connection. The Hebbian learning scheme associates the"}, {"title": "D. Action Formation", "content": "The role of this process is to perform action babbling around\nthe object located in the scene (Fig. 5). During exploration,\nthe module generates two poses ($X_1$,$Y_1$) and ($X_{goal}$,$Y_{goal}$)\nthat the end-effector will go through from a resting position\n($X_{start}$, $Y_{start}$). In this work, the z dimension is fixed at one\ncentimeter above the table for a generated pose. The robot\nrecords the end-effector position from the resting pose until\nreaching the second pose and creates a DMP if it leads\nto the discovery of a new stimuli. While learning a skill\n(exploitation), the robot activates the DMP parameters of a\ngoal and only needs to create a single pose.\nRelated to the neural fields flow, a peak of activation\nemerges within the position object NF when an object is in\nthe camera field of view. This neural field along with the other\ntwo dimensional fields present in the module are representing\nthe x and y position of the object in the robot space. Then, we\napply a specific convolution between the position object output\nand the action formation NF. The result of this convolution\nserves as inhibition to create poses necessary for the DMP\ncreation/activation. The input to the action formation NF can"}, {"title": "E. Curiosity", "content": "Error module: When the robot discovers a new goal, a\nforward and an inverse model are created. The forward model\noutput the first error in order to compute the learning progress\nwith dynamic neural fields (Fig. 8). During exploitation, the\nrobot performs a motion and observes the outcomes in the\nenvironment. If it results to no changes (the object did not\nmove), then the error remains the same. In case of any\nchanges, the error controller computes the error with the"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "The purpose of the experiments is to evaluate the learning\nbehavior of the architecture under different parameters and\nwith different objects. More specifically, we want to determine\nthe role of habituation, persistence and error inhibition during\ngoal discovery and learning. We will also analyse the tonic\nand phasic activation within the LC. Then, we will assess how\nthe Locus Coeruleus model gathering bottom-up attention and\ncuriosity lead to an open-ended learning approach."}, {"title": "A. Habituation", "content": "In this section, we evaluate the effect of habituation during\nexploration (see Fig. 11). To do so, we performed ten explo-\nrations for each of the three objects (cube, cylinder and ball)\nby varying the $\\tau_+$ (Eq. 4) parameter of the visual memory\ntrace.\nAs expected, a slow habituation ($\\tau_+ = 4$ seconds) allows\nthe robot to spend more time on exploration, leading to an\nincrease in the number of goals discovered (see Fig. 11a). We\ncan notice a difference among the objects with the ratio :\n$R_{avg} = S_{avg}/F_{avg}$  (2)\nwhere $S_{avg}$ and $F_{avg}$ are the average number of goals\ndiscovered for a slow and fast habituation. The ratio for the\ncube, cylinder and the ball are respectively 1.53, 2.04 and 1.69.\nThis suggest that given enough habituation time, the robot can\ndouble the number of goals discovered with the cylinder. This\ncan be explained by the orientation of the gripper and the\ninhibition of return. Since the gripper's orientation is fixed,\nonly a limited number of actions can trigger the discovery\nof a new goal. The movement is guided by the inhibition of\nreturn around the object's location, so this inhibition has to be\nprecise and thus requires more time to generate a meaningful\nmotion. In the following section, we will analyse the learning\ntrajectory of different goals with curiosity."}, {"title": "B. Goal learning progress", "content": "1) Time-course of learning progress between objects: The\nevolution of the learning progress is presented in Fig. 12.\nWe run the experiment twenty times for each object and, to\ncompare the learning between objects and parameters, we fixed\nthe same four goals. These goals are : push up, push to the\nleft, push to the right, push down. The weights initialization\nof the forward models determine the first error during goal\ndiscovery, which then influence the future goal selection for\nlearning. We consider this experiment as the baseline, with\na persistence value (transient action MT node) at 4500ms\nand 100ms for respectively $\\tau_+$ and $\\tau_-$. The error inhibition\n(attempts MT projected on LC:phasic) is settled at 100ms for\n$\\tau_+$ and $\\tau_-$, which means there is no error inhibition staying\nin memory between the selection of a goal to pursue. The\nlearning progress (LP MT in learning progress module) is fixed\nat 2000ms for $\\tau_+$ and $\\tau_-$. Regarding the LP, we arbitrarily\nchoose these values because a high $\\tau_-$ would signify that the\ngoal error takes more time to decay even if the error coming\nfrom the forward model is already steady at a low level. We\nperformed a Kruskal-Wallis test to evaluate the significance\nof the skills ordering for each object. For the cube, we took\nthe distribution from t = [0,1800]s; For the cylinder, we\nsample t = [0, 2250]s; For the ball, the distribution range\nis t = [0,6000]s. The differences of distribution between the\nfour goals are significant with H(4), p < 0.01 among the three\nobjects. Even if the sequence of the four goals is significant\namong all the runs, we still perform a Mann-Whitney U\ntest (see Tab. I) between two successive goals (push-up \u2192\npush-left, push-left \u2192 push-right, push-right \u2192 push-down)\nto verify a possible overlap between them in certain runs.\nThe statistical test for the cylinder is almost not significant,\nwhich indicates a different ordering between up and left\namong several runs. We can conclude that the goals follow the\nsame order of learning for the baseline experiment. However,\nthese skills do not reflect a developmental trajectory since they\ncan be learned independently. Moreover, the learning begins\nwith the goal that has the highest error, which depends on\nthe synaptic weights initialization of the forward model. The\ndifficulty of a goal to be learned can be evaluated by the time\nspent until the learning progress activation comes below 0.01.\nFor the ball, it is considerably more difficult to learn the set\nof goals than the cylinder and the cube (\u2248 6000s). This is\nessentially due to the precision required to generate a goal\n(Appendix B).\n2) Winner inhibits similar goals: During the exploration\nwith the habituation phase, it is possible for the robot to\ndiscover similar goals. For example, the robot can push a ball\nup and push a ball up but slightly shifted by a few degrees.\nIn that case, only the goal with the highest error coming from\nthe forward model will be learned in the future. This comes"}, {"title": "C. Influence of inhibition and persistence", "content": "After the learning of goals under the baseline parameters,\nwe are adjusting the error inhibition dynamics that will be\nprojected to the LC (via attempts MT). We settle the time\nparameters at $\\tau_+ = 2000ms$ and $\\tau_- = 1500ms$ and run the\nexperiment twenty times for each object (see Fig. 14, top row).\nWe arbitrarily chose these parameters so the inhibition takes\nmore time to build up and decay. For a goal where the error\nis difficult to decrease, it will deliver a mild inhibition that\nwould avoid the goal to be selected again in a near future.\nWe repeated a Kruskal-Wallis test for the cube, cylinder and\nball for the time intervals t = [0, 2100]s, t = [0,2500]s and\nt = [0, 4250]s respectively. The distribution of the four skills\nacross the 20 experiments was conclusive with H(4),p <\n0.01. A Mann-Whitney test was performed to investigate if\nthe goals are following the same order consecutively (see\nTable I). We observed that the test is not significant for the\ncube, which signifies that there is no clear ordering between\npushing - up and pushing - left. Indeed, the pushing - up\ngoal being difficult to learn, if the learning progress did not\nrise after a run of learning, the inhibition will prevent the\nselection of this goal in the near future. The statistical test\nresult was significant for the cylinder and the ball but with a\nhigher value than the baseline experiment. This demonstrates\nthe presence of several runs for both objects where the goals\ndo not follow the mainstream sequence pattern. If we compare\nthe inhibition settings with baseline, we can see a rise in\nthe learning time for the cube and cylinder. However, this\nlearning time is shorter for the ball and the goals push-right\nand push down are less subject to fluctuation. For the\nball baseline experiment, push - right and push down\ntake respectively 5000s and 3900s against 3000s and 2000s\nunder the inhibition parameters. The baseline experiment only\nrelies on the learning progress of the goals to continue the\nexploitation, which provokes only minimal learning fluctuation\nin case of easy objects to learn (cube and cylinder). For a\ndifficult object, the system has to continue the learning of\nhard goals that exhibits a certain learning progress with no\npossibility to select a different skill to exploit. Under the\ninhibition parameters, the model introduces more flexibility\nin the goal exploitation since the learning progress is not the\nonly factor of selection. Essentially, this suggests that the robot\ndoes not finish learning a goal entirely before learning a new\none. This switch between goals indicates an advantage on the\nball whereas it introduces more fluctuation for the cube. Apart\nfrom a certain learning time extension (\u2248 250s), the cylinder\ndoes not seem to be affected by the inhibition parameters.\nWe introduce the persistence settings by modifying the\ntransient action MT node ($\\tau_+ = 6000ms$, $\\tau_- = 100ms$).\nThese settings are extending the time of exploitation before\nthe LC module updates its decision regarding the goal to\nlearn. This parameter is analog to the exploration time seen\nin studies implementing intrinsically motivated systems [22],\n[24]. The experiment was repeated twenty times for each\nobject and we performed the Kruskal-Wallis test to analyse\nthe goal distributions. The test was conclusive for the three\nobjects with H(4),p < 0.01 and the Mann-Whitney U test\nbetween two consecutive goals depicts no significant variation\nin the learning sequence for all of them (see Table I). However,\nthe statistical signification is closer to the baseline experiment\nthan for the inhibition parameters. The persistence parameters\nprovide similar results to baseline for the cube. Regarding the"}, {"title": "D. Tonic and phasic activation", "content": "The slow boost component is the core of the tonic and\nphasic activation within the Locus Coeruleus model. In Figure\n15, we monitored the activations within the tonic and phasic\nNF. During exploration, the tonic activation remains sustained\nas long as the robot is not habituated to the object. Once the\nlearning begins, the slow boost component periodically reset\nand delivers excitation to the tonic and phasic NF. The rate at\nwhich the boost reset depends on the persistence mechanism.\nIn accordance with previous research [55] who suggests that\nthe error prediction contributes to the phasic activation within\nthe LC, our model propose an indirect contribution of the error\nthrough the persistence mechanism. Indeed, the persistence\narchitecture modulates these periods depending on the goal\nerror (see Appendix D, Figures 19 and 20 - bottom row). If the\nerror of a goal is significant, the period between two selections\nis lasting longer. At the first goal selection for the baseline\nexperiment, the boost delivers excitation for 16 seconds before\ndropping down. At the end of learning the same skill, the boost\nprovides excitation for 6.5 seconds. The transient action MT\nnode also influences the period of boost activation. The $\\tau_+$"}, {"title": "E. A loop for incremental learning", "content": "The architecture detects bottom-up events such as the pres-\nence of new objects in the scene and begins to explore through\ngoal discovery. Here, we reduce the habituation time ($\\tau_+$ in\nvisual memory trace) so each object has a short exploration.\nWe begin by introducing the cube, let the robot discover and\nlearn a goal, then reproduce this step for the cylinder and the\nball. This protocol exhibits the properties of the attentional\nsystem to focus on new stimuli. The time course of the\nexperiment with the learning progress and forward model\nerror is detailed in Figure 16. It is also possible to drive the\nrobot into exploration again after habituation toward a specific\nlocation by increasing the object stimuli. In practice, it is\nsufficient to introduce a supplementary excitation within the\nobject selection NF (habituation mechanism)."}, {"title": "VI. CONCLUSION AND DISCUSSION", "content": "The autonomous discovery and learning of goals in robotics\nis a complex issue to address and the state of the art represents\na wide spectrum of approaches. Here, we take inspiration from\nneuroscience by rooting curiosity with attention in order to\nallow a robotic arm to discover and learn different skills.\nMore precisely, we propose a simple model of the Locus\nCoeruleus, a nucleus in the brain controlling the switch\nbetween exploration and task engagement via the influence\nof curiosity, habituation and persistence. At first, the tonic\nactivation within the LC drives the robot to generate actions\nthat will produce new stimuli, and thus discover new goals via\nbottom-up attention. Then, the curiosity mechanism directly\ninfluences the Locus Coeruleus activation with a phasic mode\nby engaging in learning, depending on both forward model\nerror and learning progress of these goals. In addition, we\nintroduce new cognitive mechanisms to regulate the discovery\nand learning of goals with respectively a habituation and\npersistence component. The generation of robot motion is"}, {"title": "A. Dynamic Field Theory", "content": "Dynamic neural fields (DNF) represent the distribution of\nneural populations and their evolution in time according to\nAmari's equation [72"}]}