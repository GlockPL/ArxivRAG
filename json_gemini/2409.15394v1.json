{"title": "Neural Control Variates with Automatic Integration", "authors": ["Zilu Li", "Guandao Yang", "Qingqing Zhao", "Xi Deng", "Leonidas Guibas", "Bharath Hariharan", "Gordon Wetzstein"], "abstract": "This paper presents a method to leverage arbitrary neural network architecture for control variates. Control variates are crucial in reducing the variance of Monte Carlo integration, but they hinge on finding a function that both correlates with the integrand and has a known analytical integral. Traditional approaches rely on heuristics to choose this function, which might not be expressive enough to correlate well with the integrand. Recent research alleviates this issue by modeling the integrands with a learnable parametric model, such as a neural network. However, the challenge remains in creating an expressive parametric model with a known analytical integral. This paper proposes a novel approach to construct learnable parametric control variates functions from arbitrary neural network architectures. Instead of using a network to approximate the integrand directly, we employ the network to approximate the anti-derivative of the integrand. This allows us to use automatic differentiation to create a function whose integration can be constructed by the antiderivative network. We apply our method to solve partial differential equations using the Walk-on-sphere algorithm [Sawhney and Crane 2020]. Our results indicate that this approach is unbiased using various network architectures and achieves lower variance than other control variate methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Monte Carlo (MC) integration uses random samples to estimate the value of an integral. It is an essential tool in many computer graphics applications, including solving partial differential equations without discretization [Sawhney and Crane 2020] and rendering physically realistic images via ray tracing [Veach 1998]. While MC integration provides unbiased estimation for complicated integrals, it suffers from high variance. As a result, MC integration usually requires a significant amount of samples to produce an accurate estimate.\nOne common technique to reduce variance is control variates(CV). The key idea of control variates is to construct an alternative integral that have lower variance. For example, if we want to integrate a one-dimensional real value function f: R\u2192R, control variates leverage the following identity to construct different integral:\n$\\int f(x)dx = G + \\int f(x) - g(x)dx,$\nwhere l, u defines the integration domain in and G is the integral of real-value function g : R \u2192 R (i.e. G = $ \\int_l^u g(x)dx$). If the integrand f-g has less variance compared to the integrated f, then the right-hand side of this identity can result in an estimator that requires fewer samples to reach the same accuracy.\nThe key challenge of control variate is finding the appropriate g with known integral while minimizing the variance of f - g under a certain sampling strategy. Traditional methods try to define the control variates g heuristically, such as by picking a part of f with a known integral. These heuristically defined control variates may not correlate with the integrand f, limiting their performance. Recent research has proposed to parameterize the control variate using a learnable function ge and learn the parameter @ from samples of the integrands f [M\u00fcller et al. 2020; Sala\u00fcn et al. 2022]. The hope is to find e such that ge can closely match the shape of f, making f - ge low variance. Constructing an expressive parametric function ge with a known integral for all 0, however, remains challenging. As a result, existing works have limited network architecture choices, such as sum of simple basic functions with known integral [Sala\u00fcn et al. 2022] or special neural network architectures such as normalizing flows [M\u00fcller et al. 2020].\nIn this work, we propose a novel method to construct learnable control variate function g from almost arbitrary neural network architectures. Inspired by neural integration methods [Lindell et al. 2021], instead of using the network to model the control variate g directly, our method defines a network Ge: R\u2192 R to model the anti-derivative of g such that $G'_\\theta(x) = g_\\theta(x)$. By the first fundamental theorem of calculus, we have:\n$G_\\theta(u) \u2013 G_\\theta(l) = \\int_l^u\\frac{\\partial}{\\partial x}G_\\theta(x)dx.$\nThis allows us to construct a learnable control variate using automatic differentiation frameworks in the following way:\n$\\int f(x) = G_\\theta(u) \u2013 G_\\theta(l) + \\int_l^u f(x) - \\frac{\\partial}{\\partial x}G_\\theta(x)dx.$\nSince $\\frac{\\partial}{\\partial x}G_\\theta(x)$ is just another neural network, we can use gradient based optimizer to find @ that minimizes the variance of the integrand $f(x) - \\frac{\\partial}{\\partial x}G_\\theta(x)$. This method allows us to use an arbitrary network architecture in place of Ge, which enables a larger class of parametric functions to be useful for control variates. We hypothesize that this rich design space contains pairs of Ge and $\\frac{\\partial}{\\partial x}G_\\theta(x)$ that are expressive and numerically stable enough to match f closely for various problems.\nThis paper takes the first steps to apply the abovementioned idea to reduce the variance of Monte Carlo integrations in computer graphics applications. To achieve this, we first extend the neural integration method from Lindell et al. [2021] from line integral to spatial integral with different domains, such as 2D disk and 3D sphere. Directly optimizing these networks to match the integrand can be numerically unstable. To alleviate this issue, we propose a numerically stable neural control variates estimator and provide corresponding training objectives to allow stable training. Finally, many graphics applications require solving recursive integration equations where different space locations have different integrand functions. We modulate the neural networks with spatially varying feature vectors to address this issue. We apply our method to create control variates for Walk-on-Sphere (WoS) algorithms [Sawhney and Crane 2020], which solve PDEs using Monte Carlo integration. Preliminary results show that our method can provide unbiased estimation from various network architectures. Our method can produce estimators with the lower variance than all baselines. To summarize, our paper has the following contributions:\n\u2022 We propose a novel method to use neural networks with arbitrary architecture as a control variate function.\n\u2022 We propose a numerically stable way to construct control variate estimators for different integration domains.\n\u2022 We demonstrate the effectiveness of our method in solving Laplace and Poisson equations using WoS. Our method can outperform baselines in all settings."}, {"title": "2 RELATED WORK", "content": "We will focus on reviewing the most relevant papers in control variates and nueral integration techniques.\nControl Variates. Control variates is an important variance reduction technique for Monte Carlo integration [Glynn and Szechtman 2002; Loh 1995; Pajot et al. 2014]. Prior works have applied control variates in many applications, including option pricing [Ech-Chafiq et al. 2021], variational inference [Geffner and Domke 2018; Wan et al. 2020], and Poisson image reconstruction [Rousselle et al. 2016]. To establish a control variate, we need to find a function with a known analytical integration while correlating the integrand function well. Most prior works usually construct the control variate heuristically [Clarberg and Akenine-M\u00f6ller 2008; Kutz et al. 2017; Lafortune and Willems 1994]. Such an approach can be difficult to generalize to complex integrands. One way to circumvent such an issue is to make the control variates learnable and optimize the control variates function using samples from the integrand. For example, Sala\u00fcn et al. [2022] proposed to use a polynomial-based estimator as control variate as the integration of the polynomial basis is easy to obtain. Recently, M\u00fcller et al. [2020] proposed to use normalizing flow as the control variate function since normalizing flows are guaranteed to integrate into one. Our method extends these works by expanding the choice of estimator family to a broader class of neural network architecture. Most existing works apply CV on physics-based rendering. We focus on applying CV to solving PDEs using Walk-on-sphere methods [Sawhney and Crane"}, {"title": "3 BACKGROUND", "content": "In this section, we will establish necessary notations and mathematical background to facilitate the discussion of our method. In particular, we will cover backgrounds in Monte Carlo integration, Control variates, and neural integration methods in line integration.\nMonte Carlo Integration. The main idea of Monte Carlo integration is to rewrite the integration into an expectation, which can be estimated via sampling. Assume we want to estimate the integration of a real-value function f : Rd \u2192 R over domain \u03a9. We first write it into an expectation over the domain \u03a9:\n$\\int_\\Omega f(x)dx = \\int_\\Omega \\frac{f(x)}{P_\\Omega(x)}P_\\Omega(x)dx = E_{x~P_\\Omega} [\\frac{f(x)}{P_\\Omega(x)}]$,\nwhere Po is a distribution over domain 2 from which we can both sample and evaluate likelihood. This allows us to define the following estimator: $\\langle F_N \\rangle = \\frac{1}{N} \\sum_{i=1}^N\\frac{f(x_i)}{P_\\Omega(x_i)}$, where xi's are points sampled from Po and N denotes the number of samples. Monte Carlo estimation is unbiased given that $P_\\Omega(x) = 0$ only if f(x) = 0. However, it usually suffers from high variance, requiring a lot of samples and function evaluations to obtain an accurate result.\nControl Variates. Control variates is a technique to reduce variance for Monte Carlo estimators. The key idea is to construct a new integrand with lower variance and apply Monte Carlo estimation for the low-variance integrand only. Suppose we know $G = \\int_l^u g(x)dx$ for some G and g, then we have:\n$\\int_l^u f(x)dx = G + \\int_l^u f(x) - g(x)dx.$\nWith this identity, we can derive a single-sample numerical estimator (Fco) that is unbiased:\n$\\langle F_{CV} \\rangle = G + \\frac{f(x_i) - g(x_i)}{P_\\Omega(x_i)}$, where xi ~ P\u03a9.\nAs long as G is the analytical integration result of g, the new estimator created after applying control variate is unbiased. Note that the control variate estimator is running Monte Carlo integration on the new integrand f \u2013 g, instead of the original integrand f. The key to a successful control variate is finding corresponding functions G and g that make f - g to have less variance compared to the original integrand under the distribution Po. Choosing an appropriate g is challenging since it requires correlation with f while having an analytical integral G. Existing works either pick g heuristically (e.g. g = cos x if cos x is a component of f), or use a limited family of parametric functions to approximate f from data. Our method circumvents this issue by learning a parametric"}, {"title": "4 METHOD", "content": "In this section, we will demonstrate how to use neural integration method to create control variates functions from arbitrary neural network architectures. We will first demonstrate how to construct networks with known analytical spatial integrals (Sec 4.1). We then show how to create a numerically stable unbiased estimator using these networks as control variates (Section 4.2) as well as a numerically stable training objective aiming to minimize the variance of the estimator (Sec 4.3). Finally, we will discuss how to extend this formulation to multiple domains (Sec 4.4)."}, {"title": "4.1 Neural Spatial Integration", "content": "Computer graphics applications, such as rendering and solving PDEs, usually require integrating over spatial domains such as sphere and circles. To make neural integration methods applicable to these applications, we need to adapt them to integrate over various spatial domains by applying change of variables.\nLet's assume the integration domain \u2229 \u2282 Rd is parameterized by an invertible function 4 mapping from a hypercube U = [\u22121, 1]d to \u03a9, i.e. \u03a6(U) = \u03a9. For any neural network Ge: U \u2192 R, we can apply the first fundamental theorem of calculus to obtain the following identity [Ma\u00eetre and Santos-Mateos 2023]:\n$\\int_u \\frac{\\partial^d}{\\partial u}G_\\theta(u)du = \\sum_{u_1 \\in \\{-1,1\\}}...\\sum_{u_d \\in \\{-1,1\\}} G_\\theta(u) \\prod_{i=1}^d u_i$ \nDefined as Ie\nwhere u = [41,..., ud] and $\\frac{\\partial^d}{\\partial u}G_\\theta(u)$ denotes partial derivative with respect to all dimension of vector u: $\\frac{\\partial^d}{\\partial u_1...\\partial u_d}G_\\theta(u)$. Note that we can obtain both the computation graph for the integrand $\\frac{\\partial^d}{\\partial u}G_\\theta$ and the right-hand-side Ie using existing deep learning frameworks with automatic differentiation. To extend this idea to integrating over 2, we need to apply the change of variable:\n$I = \\int_u \\frac{\\partial^d}{\\partial u}G_\\theta(u)du = \\int_\\Omega \\frac{\\partial^d}{\\partial u}G_\\theta(u) |J_\\Phi (u)|^{-1} dx,$\nwhere x are coordinate in domain \u03a9, u = \u03a6\u22121(x), and J\u00f8 \u2208 Rdxd is the Jacobian matrix of function \u03a6. Since the integrand from the right-hand-side can be obtained through automatic differentiation, we now obtain a optimizable neural network with known integral in domain Q. This identity is true regardless of the neural network architecture. This opens up a rich class of learnable parametric functions useful for control variates. Table 1 shows Q and Jo in three integration domains: 2D circle, 2D disk, and 3D sphere."}, {"title": "4.2 Control Variates Estimator", "content": "Equation 10 now allows us to construct neural networks with analytical integral for various spatial domains such as 2D circles, 2D disks, 3D spheres, and more. These neural networks can be used for neural control variates, substituting Equation 10 into Equation 5:\n$\\int_\\Omega f(x)dx = I_\\theta + \\int_\\Omega f(x) - \\frac{\\partial^d}{\\partial u}G_\\theta (u) |J_\\Phi (u)|^{-1} dx,$\nwhere u = \u03a6\u22121(x). Now we can create a single-sample control variates estimator $\\langle F_{ncv} (\\theta) \\rangle$ to approximate the spatial integration:\n$\\langle F_{ncv} (\\theta) \\rangle = I_\\theta + \\frac{f(x_i)}{P_\\Omega (x_i)} - \\frac{\\frac{\\partial^d}{\\partial u}G_\\theta(u_i)}{|J_\\Phi(u_i)| P_\\Omega(x_i)}$,"}, {"title": "4.3 Training Objectives: Minimizing Variance", "content": "Our networks can be trained with different loss functions, and one should choose the loss function that works the best depending on the specific application. In this section, we will use the estimator's variance as an example to demonstrate how to adapt a control variate loss function to train our model. Following M\u00fcller et al. [2020], the variance of the estimator V [\u27e8Fncv(0)\u27e9] in Eq. 13 is:\n$\\int (f(x) - \\frac{1_u (u)}{P_\\Omega (x)} \\frac{\\partial^d}{\\partial u} G_\\theta(u) )^2 dx \u2013 (\\int f(x)dx)^2,$\nwhere u = \u03a6\u22121(x). Directly using this variance as a loss function is infeasible since we do not have analytical solutions for the term $\\int f(x)dx$. Since the gradient-based optimizer only requires gradient estimate to be unbiased, one way to circumvent this issue is to create a pseudo loss whose gradient is an unbiased estimator for the gradient of V [\u27e8Fncv(0)\u27e9]. First, we define the following losses:\n$L_{int}(\\theta, \\Omega) = E_{x~U(\\Omega)} [(f(x)|2| \u2212 I_\\theta)^2]$,\n$L_{diff} (\\theta, \\Omega) = E_{x~P_\\Omega} [(f(x) \u2013 \\frac{1_u (u)}{P_\\Omega (x)} \\frac{\\partial^d}{\\partial u} G_\\theta (u) |J_\\Phi (u)|^{-1})^2]$,\nwhere U(\u03a9) denotes uniform sampling of domain 2. One can verify that $\\nabla_\\theta V [\\langle F_{ncv}(\\theta) \\rangle] = \\nabla_\\theta L_{diff}(\\theta, \\Omega) \u2013 \\nabla_\\theta L_{int}(\\theta, \\Omega)$ (See supplementary). Note that Ldiff can numerically unstable when |Jo(u)| is very small. Since we are using e in Equation 13, we can see that $\\nabla_\\theta L_{diff} = 0$ in the region when |Jo(u)| is very small. As a result, we discard those samples during training. Note that similar techniques can be applied to other types of control variates losses."}, {"title": "4.4 Modeling a Family of Integrals", "content": "So far we've focused on applying our method to a single integration $\\int_{\\Omega}f(x)dx$ over a single domain \u03a9. In many computer graphics applications, we need to perform multiple spatial integrals, each of which will be using a slightly different domain and integrand. Adapting to such applications, we need to apply CV to solve a family of integrations in the form of $\\int_{\\Omega(c)} f(x, c)dx$, where c \u2208 Rh is a latent vector parameterizing the integral domain, \u03a9(c) \u2282 Rd are a set of domains changing depending on c, and f is the integrand.\nOne way to circumvent this challenge is to learn coefficients for one CV that minimize the variance of all estimators as proposed by Hua et al. [2023]. In our paper, we choose an alternative way, training a conditional neural network that can predict CV functions for the whole family of integrals.\nTo achieve this, we first assume there exists a family of parameterization functions for this family of domains \u03a6 : Rd\u00d7Rh \u2192 \u03a9, where each function \u03a6(., c) is differentiable and invertible conditional on c, and \u03a6(U, c) = \u03a9(c). Now we can extend our network Ge to take not only the integration variable x but also the conditioning latent vector c. We will also extend the loss function to optimize through different latent c: $L_{multi}(\\theta) = \\frac{1}{*} \\sum_{i=1}^{*} L_{diff}(\\theta, \\Omega(c_i)) \u2013 L_{int}(\\theta, \\Omega(c_i))$.\nThe same principles described in previous sections will still apply."}, {"title": "5 RESULTS", "content": "In this section, we will provide a proof of concept showing that our method can be applied to reduce the variance of Walk-on-Spheres algorithms (WoS) [Sawhney and Crane 2020]. We will first demonstrate that our method creates unbiased estimators in different integration domains while using different neural network architectures (Sec 5.1). We then evaluate our method's effectiveness in solving 2D Poisson and 3D Laplace equations (Sec 5.2, Sec 5.3).\n5.1 Unbiased Estimator with Arbitrary Network\nIn this section, we want to show that our method indeed creates an unbiased estimator regardless of neural network architectures or the integration domain. We will test our method on three types of integration domains mentioned in Table 1: 2D circle, 2D disk, and 3D spheres. We will test the following neural network architectures:\nCatSIREN. Sitzmann et al. [2020] proposed SIREN, which uses periodic activation to create an expressive neural network capable of approximating high-frequency functions. We make this network architecture capable of taking conditioning, we simply concatenate the condition vector c with the integration variable u:\n$h_i(z) = sin(W_i z + b_i), G_\\theta (x, c) = W_n (\\phi_{n-1} \u25cb\u00b7\u00b7\u00b7\u25cb \\phi_0) ([u, c]) + b_n,$\nwhere 0 = {Wi, bi}i are the trainable parameters.\nModSIREN. Mehta et al. [2021] proposed a way to condition SIREN network more expressively using a parallel ReLU-based"}, {"title": "5.2 Equal Sample Comparisons", "content": "In this section, we will focus on providing equal sample analysis comparing our methods with prior arts on the task of reducing the variance of WoS algorithms in solving 2D Poisson and 3D Laplace equations. Equal sample comparisons are useful because they are less confounded with implementation and hardware details. For example, engineering techniques such as customized CUDA kernels can drastically affect the compute time on our estimator, but they won't affect the result of equal sample analysis as much.\nBaselines. We compare our methods with WoS without control variates\u00b9 and two other learning-based control variates baselines. The first baseline is M\u00fcller et al. [2020] (NF), which uses normalizing flows to parameterize the control variates function. The second baseline is Sala\u00fcn et al. [2022] (POLY), which parameterize using a weighted sum of polynomial basis. We follow the original papers on hyperparameters, such as the degrees of the polynomial-basis and positional encoding. Specifically, we use a polynomial order of 2 following the suggestions of Fig 6 at Sala\u00fcn et al. [2022]. For fair comparison, we use the same loss functions and optimizer to train the neural CV baselines. We apply multi-resolution grids mentioned in Section 5.1 to allow POLY baseline to perform CV at arbitrary locations within the PDE domain. Specifically, the multi-resolution grid takes as input the position of the walk (i.e.center"}, {"title": "5.2.1 Solving 2D Poisson Equation", "content": "We now apply our techniques to reduce variance on a Poisson equation over the domain SCR2:\n$\\Delta u = f$ on S, u = g on \u2202S,\nwhere g: R2 \u2192 R is the boundary function, and f : R2 \u2192 R is the forcing function. This equation can be solved by the following integral equation [Sawhney and Crane 2020]:\n$u(x) = \\int_{\\partial B_d(x) (x)} \\frac{u(y)}{|\\partial B_d(x) (x)|} dy + \\int_{B_d(x) (x)} f(y)G(x, y)dy,$\nwhere d(x) = miny\u2208\u0259n ||x \u2212 y || denotes the distance to the boundary and Br (c) = {y||y-c| \u2264 r} is the ball centered at c with radius r. [Sawhney and Crane 2020] further derives a Monte Carlo estimator \u00fb(x) for the Poisson equation:\n$\\hat{u}(x) = \\begin{cases} g(x) \\qquad \\qquad \\qquad \\qquad \\text{if} d(x) < \\epsilon\\\\ \\frac{1}{|\\partial B_d(x) (x)|} \\hat{u}(x') \u2013 \\frac{1}{|B_d(x) (x)|}f(y)G(x,y) \\qquad \\text{otherwise} \\end{cases}$,\nwhere x' ~ U(\u2202Bd(xk) (xk)), y ~ U(Bd(xk) (xk)) are samples from the boundary and the interior of the 2D disk, and G denotes the 2D disk's green's function. These are two integrals that our method can be applied to: one that integrates over the circle (i.e. \u2202B) and one that integrates inside the disk (i.e. B).\nApply Control Variates to 2D Circle dB(x). We first discuss how to apply our method to reduce variance of the integral $\\int_{\\partial B} dy$. We first instantiate a neural network Go (t, x) that takes a sample's polar angle t (normalized to [-1,1]) and the 2D coordinate of the center"}, {"title": "5.2.2 Solving 3D Laplace Equation", "content": "In this section", "methods": "n$\\Delta u = 0$ on S", "estimator": "n$\\hat{u"}, {}, {"title": "6 CONCLUSION AND DISCUSSION", "content": "In this paper, we propose a novel method to enable using arbitrary neural network architectures for control variates. Different from existing methods which mostly deploy a learnable model to approximate the integrand, we ask the neural network to approximate the antiderivative of the integrand instead. The key insight is that one can use automatic differentiation to derive a network with known integral from the network that approximates the antiderivative. We apply this idea to reduce the variance of Walk-on-sphere [Sawhney and Crane 2020] Monte Carlo PDE solvers. Results suggest that our method is able to create unbiased control variates estimators from various neural network architectures and some of these networks can perform better than all baselines.\nLimitations and future works. Control variates estimator usually requires more computation for each sampling step because we also need to evaluate in additional G and g for every step. This suggests"}]}