{"title": "Tell Me Why: Incentivizing Explanations", "authors": ["Siddarth Srinivasan", "Ezra Karger", "Michiel Bakker", "Yiling Chen"], "abstract": "Common sense suggests that when individuals explain why they believe something, we can arrive at more accurate conclusions than when they simply state what they believe. Yet, there is no known mechanism that provides incentives to elicit explanations for beliefs from agents. This likely stems from the fact that standard Bayesian models make assumptions (like conditional independence of signals) that preempt the need for explanations, in order to show efficient information aggregation. A natural justification for the value of explanations is that agents' beliefs tend to be drawn from overlapping sources of information, so agents' belief reports do not reveal all that needs to be known. Indeed, this work argues that rationales explanations of an agent's private information-lead to more efficient aggregation by allowing agents to efficiently identify what information they share and what information is new. Building on this model of rationales, we present a novel 'deliberation mechanism' to elicit rationales from agents in which truthful reporting of beliefs and rationales is a perfect Bayesian equilibrium.", "sections": [{"title": "1 Introduction", "content": "From political journalism to major business forecasts, experts routinely offer more than just numerical estimates: they explain how they arrived at those estimates. Outside formal 'expert' settings, daily de-liberation-whether corporate board discussions or online communities\u2014also involves participants sharing why they believe something, not just what they believe. Common sense suggests that understanding these underlying reasons can yield more accurate conclusions than possible by aggregating raw beliefs alone.\nWhile mechanisms like proper scoring rules (Gneiting and Raftery, 2007) and prediction markets (Wolfers and Zitzewitz, 2004) can provide incentives to elicit beliefs from experts, there are no known mechanisms that can provide incentives to elicit natural-language rationales despite the intuitive benefits. This likely stems from the fact that capturing the value of explanations or 'rationales' with standard Bayesian models is challenging. Indeed, standard models that demonstrate efficient information aggregation solely from eliciting beliefs assume that beliefs convey everything an agent has to convey. When models do permit agents' beliefs to not fully reveal private information or have informational overlaps, we typically lose efficient information aggregation. The puzzle remains: how can we formally model the commonsense idea that explaining one's rationale improves aggregation?\nOne natural explanation for the intuition that rationales or explanations can produce more accurate collective beliefs is this: agents' beliefs are drawn from overlapping sources of information, and sharing rationales helps agents 'discount' overlapping or shared pieces of information to prevent 'double-counting.' The idea of agents' drawing beliefs from partially overlapping sources has been considered in social learning (Dasaratha and He, 2019), forecast aggregation (Satop\u00e4\u00e4 et al., 2017; Babichenko and Garber, 2021), and epistemic social choice (Ding and Pivato, 2021; Dietrich and Spiekermann, 2024). While these works study the dynamics of information aggregation under such models, we are interested in applying such a model towards a mechanism to elicit explanations for agents' beliefs."}, {"title": "2 Related Work", "content": "This work is relevant to several different literatures across economics and computer science. We provide an overview of the most relevant works here:\nForecast Aggregation with Overlapping Information Our work compares the value of rationales against the relatively strong baseline of aggregating just agents' beliefs, a problem extensively studied in the forecast aggregation literature. The most common approach to aggregating raw forecasts is linear opinion pooling, which involves taking a weighted average of forecasts (Armstrong, 2001). Ranjan and Gneiting (2010) and Satop\u00e4\u00e4 et al. (2014) observe that linear opinion pooling is not calibrated, lacks resolution, and is underconfident when agents' information comes from diverse sources, and hence needs to be extremized (Satop\u00e4\u00e4 and Ungar, 2015). Neyman and Roughgarden (2021) investigate the properties of averaging fore-casts when expert signals are informational substitutes, and show how much forecasts should be extremized when the mechanism has access to a prior. The need to extremize signals often stems from the fact that agents' forecasts are generated from overlapping sources of information. Palley and Soll (2019) elicit both\nEpistemic Social Choice and Deliberation The epistemic social choice literature studies voting mech-anisms that can successfully aggregate information about the true state of the world. Our model of rationales is conceptually similar to deliberation models in epistemic social choice (Dietrich and Spiekermann, 2024; Ding and Pivato, 2021): we conceive of rationales as a deliberation mechanism that reveals agents' informa-tional overlaps. As in our work, Dietrich and Spiekermann (2024) suggest overlapping Gaussian evidence as a possible instantiation of their model and consider the relative value of increasing group size versus deliber-ation. Our setting primarily differs in the following ways: (1) we evaluate rationales against a baseline where Bayesian agents communicate their beliefs/signals instead of binary votes. This reveals private information more finely and mitigates phenomena like herding (Banerjee, 1992; Bikhchandani et al., 1998), and so forecast aggregation is a more efficient baseline; and (2) we consider a mechanism design problem where agents' do not intrinsically have utilities/preferences over the world states, but instead must be incentivized to share information. There is also significant empirical work on the benefits and pitfalls of deliberation for group decision-making (Navajas et al., 2018; Graeber et al., 2024; Lorenz et al., 2011; Moshman and Geil, 1998). The Delphi method (Dalkey and Helmer, 1963; Helmer, 1967) is another deliberation method designed to elicit a consensus opinion from a group by alternating between eliciting opinions with justifications and shar-ing these justifications with the group. Although the method is not theoretically motivated, the structure is motivated by the idea that experts exchanging information can produce more informed opinions.\nSocial Learning Information aggregation is also studied in the social learning (Golub and Sadler, 2017) literature. This literature typically explores models where agents share signals/beliefs and may update their own belief through Bayes' rule (Acemoglu et al., 2011) or heuristic approaches like the DeGroot method (DeGroot, 1974). H\u0105z\u0142a et al. (2021) show that Bayesian reasoning over opinions shared in a network is generally difficult. Dasaratha and He (2019) provide a tractable Gaussian model of social learning that they use to analyze how efficiently a network aggregates information in the presence of informational confounds; we adopt similar techniques to develop a tractable model applicable to our setting.\nAumann's Agreement Theorem Aumann (1976) and Geanakoplos and Polemarchakis (1982) present models where Bayesian agents aggregate information merely by sharing beliefs. Kong and Schoenebeck (2022) give positive results showing that merely exchanging beliefs allows for full information aggregation if signals are independent conditional on ground truth. However, this is a significant assumption that doesn't hold in general since it neglects agents' informational overlap, which limits efficient aggregation from simply exchanging beliefs. Our proposed model shows that aggregation is more efficient when agents reveal explanations, relative to when they simply exchange beliefs as in the Aumannian protocol.\nMarket-based Information Aggregation The economics literature has studied how market mecha-nisms can efficiently aggregate market participants' information via prices (Fama, 1970). Prediction markets (Wolfers and Zitzewitz, 2004; Chen et al., 2010) are a specific market mechanism where agents make pre-dictions about (typically) binary outcomes and are paid out based on the outcome. Ostrovsky (2009) and Kong and Schoenebeck (2022) show conditions under which prediction markets can aggregate participants' information; in our work, we consider information structures under which prediction markets would not efficiently aggregate information. Hanson (2003) proposed market scoring rules for prediction markets, that allow us to cast prediction markets as a sequential mechanism where agents are paid using proper scoring rules for the information added. We use such ideas to design the payoffs for agents producing rationales."}, {"title": "3 Model", "content": "We consider a principal P, a supervisor agent S, and expert agents 1,2,..., T,... (generally indexed with t, t').\u00b9 The experts have private information that the principal seeks to elicit and aggregate to arrive at an informed belief about binary outcome Y. To this end, the principal tasks the supervisor with collecting and aggregating the private information held by the experts and reporting this to the principal. For convenience, we assume the supervisor does not have any private information, though this can easily be relaxed. Experts will arrive sequentially one at a time, and can view the full history of previous reports made to the supervisor. Experts (strategically) report their own beliefs, and importantly, may also (strategically) report a rationale explaining their belief to the supervisor.\nNow, we present our model of experts' information, with the goal of explaining how rationales, when submitted, affect what experts know. Specifically, our model is designed to capture the intuitive idea that an expert's rationale doesn't affect their own belief about outcome Y (which already summarizes what they know); instead, a rationale's utility lies in how it helps other experts reason about what information is shared and thus efficiently aggregate information."}, {"title": "3.1 Setup", "content": "Let \u03a9 be the state space, I = {1, 2, . . ., T, . . .} represent index of the experts, and Y be the binary outcome of interest. We model experts' private signals and shared signals as constructed from some 'atomic' information. For every finite, non-empty subset of experts S \u2286 I, we define random variable $Z_S : \\Omega \\rightarrow \\mathbb{R}$. We interpret $Z_S$ as a component 'sub-signal' that constitutes the information that every expert $t \\in S$ possesses and no other expert has. $Z_S = (Z_{S_1}, Z_{S_2},...)$, where $S_i \\subseteq S$, is the vector of 'atomic' pieces of information that construct $Z_S$, the unique information shared only by experts in S. $Z_S$'s are conditionally independent given Y. We model $Z_S$ as a summable sequence, that is, $Z_S \\in l_1$ for all $S \\subset I$. Now we can define expert t's private signal as $X_t = \\sum_{S:t\\in S} Z_S = \\sum_i (Z_{\\{t\\}})_i$; intuitively an expert t's signal is composed of all the 'atoms' $Z_S$ where $t \\in S$. We can also define the shared signal $X_S = \\sum_{S':S\\subset S'} Z_{S'} = \\sum_i (Z_S)_i$ as a component signal that feeds into the private signal of every agent in S. Naturally, $Z_S$ is a component of $Z_t$ for any $t \\in S$. We provide a concrete example below:"}, {"title": "Definition 1 (Rationales)", "content": "Let $I_t \\subseteq \\{1, ...,t\\}$ specify a subset of experts and $I^c = \\{1, ...,t\\} \\backslash I_t$. Formally, we define rationales $\\{\\phi_t|t' \\in I_t\\}$ as a set in $\\mathcal{X}^{|I_t|}$ that induces the following $\\sigma$-algebra at every time-step:\n$\\mathcal{F}_t = \\sigma (X_S|S \\subseteq I^c \\text{ or } S \\in \\{\\{t'\\} : t' \\in I_t\\})$\nIn other words, expert t's \u03c3-algebra is generated by all component shared signals revealed through rationales from previous experts in $I_t$ (note $t \\in I_t$ since expert t always knows their own rationale $\\phi_t$), as well as from just the raw signals in $I^c_t$. Observe that $F = \\{\\mathcal{F}_t\\}_{t \\in \\mathbb{Z}}$ is a filtration specifying the information known to expert t at any time-step t. For clarity of presentation, we stick to the filtration $F^{(1)} = \\{\\mathcal{F}_t^{(1)}\\}$ where all experts provide rationales, and filtration $F^{(0)} = \\{\\mathcal{F}_t^{(0)}\\}$ where no experts provide rationales. In these cases, the \u03c3-algebra at time-step t is:\n$\\mathcal{F}_t^{(0)} = \\sigma (X_t|t \\in \\{1,...,t\\})$\n$\\mathcal{F}_t^{(1)} = \\sigma (X_S|S \\subseteq \\{1,...,t\\})$"}, {"title": "Observation 1 (Rationales can reveal more information than raw signals)", "content": "The filtration $F^{(1)}$ is a refinement of filtration $F^{(0)}$ since $\\mathcal{F}_t^{(0)} \\subset \\mathcal{F}_t^{(1)}$ for all $t > 1$.\nNow, we provide a construction of 'new information revealed by a rationale' that will make it easier to analyze the information in filtration $F^{(1)}$ where rationales are revealed. Specifically, we seek to understand: what information does expert t's rationale reveal that no previous expert revealed? To analyze this, we decompose $X_t = V_t + W_t$ where we refer to $W_t = \\sum_{S:t \\in S, \\{1,...,t-1\\} \\cap S = \\emptyset} Z_S$ as expert t's residual signal and $V_t$ as expert t's redundant signal. $W_t$ can be interpreted as the component of expert t's signal $X_t$ that is not shared with any of the previous experts (but potentially shared with future experts), while $V_t$ soaks up all the atoms (and hence dependence) shared with previous experts' signals. In other words, $W_t$ represents the 'new information' in rationale $\\phi_t$, while $V_t$ represents 'old information' already revealed by some previous expert. The motivation behind this decomposition is to condense the information revealed by rationales in $F^{(1)}$ into independent chunks $W_1,..., W_t$ that can easily be aggregated to compute posterior beliefs over Y. We can also define $W_t$ with an inclusion-exclusion style formula:\n$W_t = \\sum_{S \\subset \\{1,...,t\\}: \\atop t \\in S} (-1)^{|S|+1} X_S$"}, {"title": "Observation 2 (Properties of residual signals)", "content": "We note the following properties of residual signals $W_t$:\n1. Rationales expose experts' residual signals: The process $(W_t)_{t>1}$ is adapted to the filtration $F^{(1)}$, but not adapted to the filtration $F^{(0)}$.\n2. The first expert's residual signal is their full signal (by construction): $W_1 = X_1, V_1 = 0$.\n3. Expert t's residual signal is conditionally independent (given Y) of previous experts' sig-nals: $X_t \\perp W_{t'}|Y$ and $W_t \\perp W_{t'}|Y$ for $1 < t' < t$. This is because any $X_{t'}, W_{t'}$ are composed with atoms $Z_S$, where $t' \\in S'$, while $t' \\notin S$ for atoms $Z_S$ that compose $W_t$.\n4. Residual signals $W_1,..., W_t$ are sufficient to compute posterior over Y: $P(Y = 1|F_t^{(1)}) = P(Y = 1|W_1,...,W_t)$. This follows from a simple inductive argument. Since $W_1 = X_1$, it holds in the base case. If it holds at time-step t\u2013 1, then at time-step t,\n$P (Y = 1|F_t^{(1)}) = P (Y = 1|F_{t-1}^{(2)} \\cup \\sigma(X_S|S \\subseteq \\{1, ..., t\\}, t \\in S) = P(Y = 1|W_1,..., W_{t-1}, W_t)$\nsince $Y \\perp X_S|W_1,...,W_{t\u22121}$ for t,t' \u2208 S \u2286 {1,...,t} and any t' < t.\nFinally, we emphasize two desirable properties of our model: (1) an expert's rationale provides no private benefit, i.e., an expert's own rationale does not refine their own information, even if they observe previous experts' signals (though it may help refine future experts' information); (2) rationales are only useful when used in conjunction with other rationales; a rationale on its own cannot help identify what information is shared and what information is novel."}, {"title": "3.2 Information Structure", "content": "The setup above provided an abstract construction of rationales that operationalize the commonsense idea that they reveal more information than raw signals. Now, we impose further structure on experts' information to develop a tractable model to analyze how much more information rationales reveal. We begin with standard assumptions on the prior and distribution of experts' signals.\nAssumption 1 (Common prior \u03c0). We assume all agents share a common prior over the outcome \u03c0 = P(Y = 1) and this prior is common knowledge. We write the prior log odds as $\\lambda_\\pi = log(\\frac{\\pi}{1-\\pi})$\nAssumption 2 (Conditionally Gaussian signals). We assume expert t's signal $X_t$ conditional on outcome Y is distributed as $X_t|(Y = 1) \\sim N(\\mu, \\sigma^2)$ and $X_t|(Y = 0) \\sim N(-\\mu, \\sigma^2)$ with \u03bc > 0.\nNext, we impose structure on the information shared by any two agents. Specifically, we assume that the shared signal between any two experts $X_{\\{t,t'\\}}$ is also conditionally normally distributed.\nAssumption 3 (Conditionally Gaussian pairwise shared signals). For any pair of experts $\\{t,t'\\}$, we assume the shared signal $X_{\\{t,t'\\}}$ conditional on outcome Y is distributed as $X_{\\{t,t'\\}}|(Y = 1) \\sim N(\\rho\\mu, \\rho\\sigma^2)$ and $X_{\\{t,t'\\}}|(Y = 0) \\sim N(-\\rho\\mu, \\rho\\sigma^2)$ with \u03bc > 0 for some 0 < \u03c1 \u2264 1.\nAs a general notational point, we will write conditional means with the condensed notation \u00b1\u03bc, where Y = 1 gives +\u03bc and Y = 0 gives -\u03bc. Now, observe that by our construction:\n$Cov(X_t, X_{t'}|Y) = Cov \\bigg(\\sum_{S:t\\in S} Z_S, \\sum_{S':t'\\in S'} Z_{S'}|Y\\bigg) = \\sum_{S:t\\in S} \\sum_{S':t'\\in S'} Cov (Z_S, Z_{S'}|Y)$"}, {"title": "Observation 3 (Expert signals as a conditional Gaussian process)", "content": "Given outcome Y, the collection of expert signals $(X_t)_{t>1}$ is a Gaussian process with mean \u00b1\u03bc and covariance $Cov(X_t, X_{t'}) = (\\rho + \\delta_{t,t'} (1 \u2212 \u03c1))\u03c3^2$. For any collection of experts $S \\subset I$, the covariance matrix is $\\Sigma_S = \u03c3^2((1 \u2212 \u03c1)I + \u03c111^T)$.\nThus, we have now fully specified the joint distribution of the process $(X_t)_{t>1}$ which is adapted to the filtration $F^{(0)}$, and we turn our attention to the process $(W_t)_{t>1}$ adapted to filtration $F^{(1)}$. Pre-vious assumptions do not fully determine the distribution of $W_t$ since this depends on the structure of n-way overlaps in information; the fact that $(X_1,..., X_t)$ is jointly normal with constant covariance im-poses some constraints, but there is still some freedom to specify this overlap. One extreme possibil-ity is that all pairwise overlaps in information are commonly shared (i.e., $V_t = V_{t'}$ for all t,t')\u00b3, which forces $W_t|Y \\sim N ((1 \u2212 \u03c1)\u03bc, (1 \u2212 \u03c1)\u03c3^2)$; importantly, note that $Var(W_t)$ is constant in t, so every expert contributes a constant amount of new information indefinitely. The other extreme possibility is that all pairwise overlaps are distinct, so $Var(W_t|Y) = max\\{(1 \u2013 (t \u2212 1)\u03c1)\u03c3^2,0\\}$, but this doesn't let us cleanly consider more than T > 1 experts. In order to analyze the case where the amount of new information $Var(W_t)$ is diminishing as 'fast' as possible, we take the alternative extreme to be a \u2018self-similarity' condi-tion on n-way overlaps: where $Cov(X_S, X_{S'}) = \\rho\\sqrt{Var(X_S)Var(X_{S'})}$ for any $S, S' \\subset I$. This then forces $W_t|Y \\sim N ((1 \u2212 \u03c1)^{t\u22121}\u03bc, (1 \u2212 \u03c1)^{t\u22121}\u03c3^2)$. We can then introduce a parameter 0 < \u03b1 < 1 that determines the structure of the n-way overlap and interpolates between these two extremes. Intuitively, we can think of \u03b1 as determining how quickly marginal experts run out of new information; small \u03b1 means the marginal expert is adding substantial new information, while large \u03b1 means the marginal expert's new information is quickly diminishing. Lastly, since $W_t$ captures information not revealed by any previous expert, we also have $Cov(W_t, W_{t'}) = 0$ for all t,t'. Thus, we formally state our parameterization of the distribution of $W_t$.\nAssumption 4 (Expert residual signals as a conditional Gaussian process). Given outcome Y, the collection of expert residual signals $(W_t)_{t>1}$ is a Gaussian process with mean \u00b1fa,t\u03bc and covariance $Cov(W_t,W_{t'}) = \u03b4_{t,t'} f_{a,t}\u03c3^2$, where $f_{a,1} = 1$ and $f_{a,t} = (1 \u2212 \u03c1)^{1+(t\u22122)\u03b1}$ for t > 2 and 0 < \u03b1 < 1. For any collection of experts $S \\subset I$, the covariance matrix is $\\Sigma_S = f_{a,t}\u03c3^2\u0399."}, {"title": "3.3 Signals to Log Likelihoods", "content": "Now, we make an important observation (also noted by Dasaratha and He (2019)) that will make our model much easier to analyze: given our Gaussian assumptions on the signal distribution above, experts' posterior log odds are also Gaussian.\nObservation 4 (Log likelihood ratios are normally distributed). For any signal X distributed as X|Y ~ N (\u00b1\u03bc, \u03c3\u00b2) with \u03bc > 0, the log likelihood ratio of observing signal X is $A_t = \\frac{2 \\mu}{\u03c3^2}X$ :\n$A_t = log \\frac{P(X_t|Y = 1)}{P(X_t|Y = 0)} = log(\\frac{exp(\\frac{-(X_t - \\mu)^2}{2 \\sigma^2})}{exp(\\frac{-(X_t + \\mu)^2}{2 \\sigma^2})}) = -(\\frac{(X_t - \\mu)^2}{2 \\sigma^2} - \\frac{(X_t + \\mu)^2}{2 \\sigma^2}) = \\frac{2 \\mu}{\u03c3^2} X_t$\nConsequently, $A_t|Y \\sim N (\u00b1\u03c4, 2\u03c4)$ where $\u03c4 = \\frac{2 \\mu^2}{\u03c3^2}$.\nThis observation will allow us to work exclusively with the posterior log odds transformed version of experts' signals, so we can talk directly about what experts believe instead of what experts observe. Indeed, going forward, we will only consider the log likelihood transformed versions of signals.\nObservation 5 (Log likelihood ratios of private signals). Let $A_t = \\frac{2 \\mu}{\u03c3^2}X_t$. Then $(A_t)_{t\u22651}$ is a Gaussian process adapted to the filtration $F^{(0)}$, with mean \u00b1\u03c4 = \u00b1$\\frac{2 \\mu^2}{\u03c3^2}$ and covariance $Cov(A_t, A_{t'}) = 2\u03c4(\\rho + \\delta_{t,t'} (1 \u2212 \u03c1)).$"}, {"title": "Observation 6 (Log likelihood ratios of residual signals)", "content": "Let $\\psi_t = \\frac{2 \\mu}{\u03c3^2}W_t$. Then $(\\psi_t)_{t>1}$ is a Gaussian process adapted to the filtration $F^{(1)}$, with mean \u00b1fa,t\u03c4 and covariance $Cov(\\psi_t,\\psi_{t'}) = 2\u03b4_{t,t'} f_{a,t}\u03c4$.\nWe can also define $\\phi_t = \\frac{2 \\mu}{\u03c3^2}V_t$, so $A_t = \\phi_t + \\psi_t$. Intuitively, we can interpret $A_t$ as the log likelihood update to the prior induced by reading expert t's rationale. Similarly, we can interpret $\\psi_t$ as the log likelihood update to the prior induced by reading only the information in expert t's rationale that no previous expert had written about, and $\\phi_t$ as the log likelihood update to the prior induced by reading all information in expert t's rationale that had already been revealed by some prior expert."}, {"title": "4 Expert Aggregation", "content": "Having laid out the information each expert t has at time-step t, we now turn our attention to the question of what the expert believes after observing all this information. After the expert observes their private signal $X_t$, their private posterior log odds is simply $\u03b3 = A_t + \\lambda_\\pi$. However, at time-step t, expert t also has access to $F^{(0)}$ when no previous experts submitted rationales, and to $F^{(1)}$ when all previous experts submitted rationales. We consider each of these in turn. In the former case, we focus on the adapted process $(A_t)_{t>1}$, and in the latter case, we will focus on the adapted process $(\\psi_t)_{t\u22651}."}, {"title": "4.1 Aggregation without rationales", "content": "When no experts submit rationales, the expert at time-step t only has access to $F^{(0)} = \u03c3 (X_t|t \u2208 \\{1, ..., t\\})$. From Observation 5, this means that the expert can observe {$A_1, ..., A_t$}. Thus, the expert's aggregate belief given all information at time-step t is $P(Y = 1|F_t^{(0)}) = P(Y = 1|A_1,..., A_t)$. We give the following lemma to show how to compute this posterior log odds.\nLemma 1 (Aggregation of Correlated Gaussian Signals). Let Y be a binary outcome with prior log odds \u039b and A be a vector of signals distributed as $A_t|Y = 1 \\sim N(\\mu, \\Sigma)$ and $A|Y = 0 \\sim N(-\\mu, \\Sigma)$. Then, the Bayesian posterior log odds that aggregates the signals is $\u03b3 = 2\\mu^T\\Sigma^{-1}A + \\lambda_\\pi$.\nProof. The agent's posterior log odds are computed as follows:\n$log \\frac{P(Y = 1|A_t)}{P(Y = 0|A_t)} = log \\frac{P(A_t|Y = 1)}{P(A_t|Y = 0)} + log \\frac{P(Y = 1)}{P(Y = 0)}$\n$= log \\frac{exp(-(A_t - \\mu)^T\\Sigma^{-1}(A_t - \\mu))}{exp(-(A_t + \\mu)^T\\Sigma^{-1}(A_t + \\mu))} + \\lambda_\\pi$\n$= \\frac{1}{2}-(A_t -\\mu)^T\\Sigma^{-1}(A_t -\\mu) + \\frac{1}{2}(A_t +\\mu)^T\\Sigma^{-1}(A_t +\\mu) + \\lambda_\\pi$\n$= A_t^T\\Sigma^{-1}\\mu + \\mu^T\\Sigma^{-1}A_t + \\lambda_\\pi$\n$= 2\\mu^T \\Sigma^{-1}A_t + \\lambda_\\pi$\nWith this lemma, we can give the following result on expert t's posterior log odds when no experts report rationales, i.e., in the filtration $F^{(0)}$:\nTheorem 1 (Aggregate without rationales). By Observation 5, when no expert submits rationales, expert t observes $A_t = (A_1,...,A_t)$ a vector of the log likelihood ratios of the private signals observed by experts 1,...,t where $A_t|Y \\sim N (\u00b1\u03c41_t, 2\u03c4((1 \u2212 \u03c1)I_t + \u03c111^T))$. Then, expert t's Bayesian posterior log odds is $\u03b3_t = \u03bb_\u03c0 + w \u03a3_{t'=1} A_{t'}$ where $w = \\frac{\u03c4}{1+(t-1)\u03c1}$. Ex ante, expert t's posterior log odds is distributed as $\u03b3_t|Y \u223c N (\u03bb_\u03c0 \u00b1 \u03c4\u03c9\u03c4, 2t\u03c9\u03c4)."}, {"title": "4.2 Aggregation with rationales", "content": "When every expert submits rationales, the expert at time-step t has access to $F^{(1)} = \u03c3 (X_S|S \u2286 \\{1,...,t\\})$. From Observation 6, this means the expert can also observe the new information contributed by every expert {$\\psi_1,...,\\psi_t$}. By Observation 2.4, the expert's aggregate belief given all information at time-step t is $P(Y = 1|F_t^{(1)}) = P(Y = 1|\\psi_1,..., \\psi_t)$. We give the following result on expert t's posterior log odds when all experts report rationales, i.e., in filtration $F^{(1)}$:\nTheorem 2 (Aggregate with rationales). By Observation 6, when every expert submits rationales, ex-pert t observes $\u03a8_t = (\\psi_1,...,\\psi_t)$ a vector of the log likelihood ratios of the residual signals observed by experts 1,...,t where $\u03a8_t|Y \\sim N (\u00b1\u03c4(f_{a,1},..., f_{a,t}), 2\u03c4\\cdot diag (f_{a,1},..., f_{a,t}))$. Then, expert t's Bayesian posterior log odds is $\u03b3_t = \u03bb_\u03c0 + \u03a3_{t'=1}^t \u03c8_{t'}$. Ex ante, expert t's posterior log odds is distributed as $\u03b3_t|Y \u223c N (\u03bb_\u03c0 \u00b1 \u03c4F_{a,t},2\u03c4F_{a,t})$ where $F_{a,1} = 1$ and for t \u2265 2:\n$F_{a,t} = \\sum_{t'=1}^t f_{a,t'} = 1 + (1 \u2212 \u03c1) \\bigg[ \\frac{1 \u2013 (1 \u2013 \u03c1)^{(t-1)\u03b1}}{1 \u2013 (1 \u2013 \u03c1)^\u03b1} \\bigg]$"}, {"title": "4.3 Rationales enable more efficient aggregation", "content": "We have thus far given the ex-ante posterior log odds at every time-step t given $F^{(0)}$ and $F^{(1)}$. Now, we show that when all experts provide rationales, the ex-ante log posterior odds of expert t is further away from the log prior odds, relative to when no experts provide rationales.\nTheorem 3 (Ex-ante log posterior odds is further from log prior odds with rationales). Ex-ante, the expected log posterior odds at time-stept is further from log prior odds when all experts provide rationales, relative to when no experts provide rationales, unless p\u2208 {0,1}. In other words, $t\u03c9\u03c4 < \u03c4F_{a,t}$ with equality when \u03c1\u03b5 {0,1}."}, {"title": "5 Deliberation Mechanism", "content": "In the previous sections, we developed a model of reasoning with rationales that showed that rationales allow for more efficient aggregation. Now, we tackle the problem of providing incentives to elicit true rationales and beliefs, since providing rationales is typically a costly enterprise. We design a mechanism that a principal can use to elicit rationales from experts and efficiently aggregate this information to arrive at more accurate beliefs. We give our proposed mechanism Mdeliberation below:\n1. Experts 1,2,... arrive sequentially one at a time until the mechanism terminates at time-step T. Each expert makes a report kt to the supervisor.\n2. At time-step t, expert t joins the mechanism with private information and observes the full history of reports k1,..., kt\u22121 submitted to the supervisor as well as the supervisor's reported trajectory of beliefs $\\{\\hat{\\gamma}^s_1,..., \\hat{\\gamma}^s_{t\u22121}\\}$. Expert t then arrives at their posterior log odds \u03b3t.\n3. Expert t then chooses whether to exert effort (et = 0 or et = 1) to submit a rationale \u03b8t, and whether to misreport their belief \u03b3t and/or rationale \u03b8t (if applicable). The expert's report to the supervisor kt consists of their belief, and if effort was exerted, their rationale.\n4. The supervisor observes report kt and updates their belief to $\\hat{\\gamma}^s_t$. They then (strategically) report $\\hat{\\gamma}^s_t$ to the principal.\n5. Upon realization of event Y, the principal determines payoffs for the supervisor and experts."}, {"title": "5.1 Background", "content": "Before further analysis", "S": "Y \u00d7 \u25b3y \u2192 R that scores a probabilistic prediction p \u2208 \u0394y against an outcome Y = y. A scoring rule is proper if $E_{y\u223cp}[S(y, p)", "q)": "for any q\u2260p, q \u2208 \u0394y. The scoring rule is strictly proper if the inequality is strict.\nDefinition 3 (Market Scoring Rules). An S-market scoring rule is a sequential and shared proper scoring rule that scores agent t based on the difference in score between their report pt and the prior report pt-1, i.e., $S_{MSR}(Y, p_t, p_{t\u22121}) = S(y, p_t) \u2013 S(y, p_{t\u22121})$ where S is the given scoring rule.\nRemark 1 (Log Scoring Rule). The scoring rule $S(Y;p) = log(p_y)$ is a strictly proper scoring rule.\nDefinition 4 (Perfect Bayesian Equilibrium). A Perfect Bayesian Equilibrium (PBE) is a solution concept"}]}