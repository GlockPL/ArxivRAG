{"title": "BatGPT-Chem: A Foundation Large Model For\nRetrosynthesis Prediction", "authors": ["Yifei Yang", "Runhan Shi", "Zuchao Li", "Shu Jiang", "Bao-Liang Lu", "Yang Yang", "Hai Zhao"], "abstract": "Retrosynthesis analysis is pivotal yet challenging in drug discovery and organic chemistry. Despite\nthe proliferation of computational tools over the past decade, AI-based systems often fall short in\ngeneralizing across diverse reaction types and exploring alternative synthetic pathways. This paper\npresents BatGPT-Chem, a large language model with 15 billion parameters, tailored for enhanced\nretrosynthesis prediction. Integrating chemical tasks via a unified framework of natural language and\nSMILES notation, this approach synthesizes extensive instructional data from an expansive chemical\ndatabase. Employing both autoregressive and bidirectional training techniques across over one hun-\ndred million instances, BatGPT-Chem captures a broad spectrum of chemical knowledge, enabling\nprecise prediction of reaction conditions and exhibiting strong zero-shot capabilities. Superior to exist-\ning AI methods, our model demonstrates significant advancements in generating effective strategies\nfor complex molecules, as validated by stringent benchmark tests. BatGPT-Chem not only boosts the\nefficiency and creativity of retrosynthetic analysis but also establishes a new standard for computa-\ntional tools in synthetic design. This development empowers chemists to adeptly address the synthesis\nof novel compounds, potentially expediting the innovation cycle in drug manufacturing and materials\nscience.", "sections": [{"title": "1 Introduction", "content": "Retrosynthesis analysis [1], which aims to iden-\ntify a set of precursors given a target molecule,\nis essential in synthetic chemistry. It plays a vital\nrole in applications like drug design, chemical biol-\nogy, and material science. However, the extensive\nrange of possible chemical transformations and the\nincomplete understanding of the chemical reac-\ntion mechanisms make retrosynthesis planning an\nextremely challenging task, even for experienced\nchemists addressing smaller molecular structures.\nOver recent decades, the development of various"}, {"title": "2 Results", "content": "Given a set of products, the objective is to gen-\nerate precursors that synthesize the products.\nSimilar to other LLMs, BatGPT-Chem has been\ntrained on extensive data, making it challeng-\ning to estimate its zero-shot prediction ability\nfor retrosynthesis planning. To minimize overlap\nbetween test and training data and to explore a\nbroader chemical space, we collect and organize\neight datasets with various reaction types to estab-\nlish a new benchmark dataset for retrosynthesis\nprediction. We take reaction conditions (precur-\nsors) into account as much as possible. Details of\nthe benchmark are provided below.\n\u2022 The Suzuki-Miyaura (SM) dataset [34] con-\ntains one product of the Suzuki-Miyaura cross-\ncoupling reactions. There are 5,760 reactions\non the combinations of 15 couplings of elec-\ntrophiles and nucleophiles, 12 ligands (with a\nblank one), eight bases (with a blank one), and\nfour solvents.\n\u2022 The high-throughput experiments Buchwald-\nHartwig (HTE BH) dataset [35] contains five\nproducts of the Pd-catalyzed Buchwald-Hartwig\nC-N cross-coupling reactions. There are 3,955\nreactions on the combinations of 15 aryl halides,\nfour ligands, three bases, and 23 additives.\n\u2022 The electronic laboratory notebooks Buchwald-\nHartwig (ELN BH) dataset [28] contains 454\nproducts of the Pd-catalyzed Buchwald-Hartwig\nC-N cross-coupling reactions. It has 551 reac-\ntions with a wider range of reaction space than\nthe HTE BH dataset.\n\u2022 The asymmetric allylic alkylation with amine\n(AAAA) dataset [36] contains 189 products of\n273 reactions.\n\u2022 The Denmark dataset [37] contains 25 products\nof the asymmetric N, S-acetal formation using\nCPA catalysts. There are 1,075 reactions on the\ncombinations of 43 catalysts, five imines, and\nfive thiols."}, {"title": "2.2 BatGPT-Chem achieves\nexceptional accuracy in\nidentifying reactants", "content": "In the MaxFrag score analysis presented in Fig. 2,\nBatGPT-Chem achieves state-of-the-art perfor-\nmance across most datasets, except for the ELN\nBH dataset where it is surpassed by ChemDFM."}, {"title": "2.3 BatGPT-Chem capably predicts\nreaction conditions explicitly", "content": "Beyond the identification of reaction reactants,\nthe prediction of appropriate reaction conditions,\nlike catalysts and solvents, remains a crucial and\nchallenging subsequent task [14]. Due to the sig-\nnificant complexity involved in accurately and\ncomprehensively predicting reaction conditions,\nLLMs often struggle to yield reasonable results\nwhen assessed using Exact Match and MaxFrag\nmetrics. Therefore, here we employ the 'Inter-\nsection' metric, as described in Section 2.1, to\nmore effectively evaluate the model's capability in\npredicting reaction conditions.\nWe assess the accuracy of reaction condition\npredictions across six datasets which contain reac-\ntion condition information. As shown in Figure 3,\nBatGPT-Chem outperforms other methods by\nlarge margins in terms of Intersection rate. This\ndemonstrates BatGPT-Chem's exceptional ability\nto predict reaction conditions and complete ret-\nrosynthesis routes effectively. However, it is impor-\ntant to note that on the HTE BH and Denmark\ndatasets, all models consistently fail to present\nany feasible reaction conditions, highlighting the\ninherent challenges of this task.\nMoreover, as mentioned in Section 2.2, while\nmany methods do not distinguish between reac-\ntants and reaction conditions within their reaction\nstrings, BatGPT-chem's training corpus separates"}, {"title": "2.4 BatGPT-Chem excels in\ngenerating multiple viable\nretrosynthesis routes", "content": "After achieving accurate predictions for reactants\nand reaction conditions, the focus shifts to eval-\nuating the model's ability to generate diverse,\ncorrect, and even novel predictions. Due to the\nstochastic nature of LLMs, retrosynthesis predic-\ntions sampled from BatGPT-Chem for a fixed\nproduct will not be unique. To assess BatGPT-\nChem's capability to propose multiple retrosyn-\nthesis routes, we examine four representative\nproducts (Fig. 5a) from the benchmark. We sam-\nple the Top-30 predictions for the SM and HTE\nBH datasets and the Top-10 predictions for the\nAHO and BioChem datasets. Fig. 5b and 5c\ndisplay the frequencies and detailed predictions,\nshowcasing BatGPT-Chem's capacity to provide\ndiverse retrosynthesis routes."}, {"title": "3 Discussion", "content": "Large language models (LLMs) have made sub-\nstantial progress across various fields, demonstrat-\ning significant potential to spearhead advance-\nments in AI for Science. Notably, their proficiency\nin processing sequential data makes them particu-\nlarly apt for the chemical domain, where common\nrepresentations such as SMILES also adopt a\nsequential format. Considering the natural poten-\ntial of LLMs to learn and predict chemical struc-\ntures and reactions, we develop BatGPT-Chem, a\npioneering large-scale model tailored for retrosyn-\nthesis analysis, to address three critical limitations\nprevalent in existing AI models: i) a deficit in\ncomprehensive molecular and chemical reaction\nknowledge; ii) oversight of reaction conditions;\nand iii) inadequate generalization across diverse\nchemical reactions.\nBatGPT-Chem distinguishes itself by its\nextensive training corpus, which covers a wide\nrange of chemical literature and chemical string\ndata, i.e., SMILES strings. The implementation\nof carefully crafted prompt templates and tai-\nlored instruction-tuning data during pre-training\nhas significantly enhanced its capacity to deci-\npher both natural and chemical languages. This\nadvancement is reflected in its improved accu-\nracy in predicting reactants and the near-perfect\nvalidity of its output. Particularly noteworthy is\nBatGPT-Chem's performance on comprehensive\nbenchmark datasets, demonstrating remarkable\nzero-shot retrosynthesis prediction capacities that\nhold practical implications for real-world applica-\ntions.\nA unique aspect of BatGPT-Chem compared\nto other LLMs is its explicit handling of reaction\nconditions. By directly extracting reaction condi-\ntions from datasets and creating specific prompts\nto predict them, BatGPT-Chem shows superior\nability to elucidate components such as solvents\nand catalysts. When predicting reaction condi-\ntions, non-generative models can only deal with\na fixed set of molecules and are usually modeled\nas multi-classification or multi-label problems [46-\n49], which greatly limits the generalization ability\nof these models. These methods use a post-\nprocessing approach in retrosynthesis, where reac-\ntion conditions are predicted after the reactants\nand products are known. This two-stage process-\ning requires additional training of the model, is not\nsimple enough to use, and makes it more difficult\nto ensure the stability of the model. Contrastingly,\ngenerative models like BatGPT-Chem can predict\na wide variety of reaction condition molecules, are\nnot constrained by a finite set, and can offer novel\nand heuristic predictions in an end-to-end manner.\nSince most of the reaction condition informa-\ntion is stored using raw text [50], constructing\ndatasets for machine learning is inherently time-\nconsuming and laborious, including steps such as\nextracting text, removing errors, and converting\nto computer-readable sequences [49, 51]. BatGPT-\nChem can provide reference reaction conditions\nfor inverse synthesis datasets like BioChem [39],\nwhich do not contain information on reaction\nconditions. This capability can help build more\ncomprehensive and enriched datasets of chemical\nreactions, facilitating the use of machine learning\nin chemical reaction modeling.\nMoreover, BatGPT-Chem excels in generating\ndiverse and viable retrosynthesis pathways, pro-\nviding valuable insights for chemists. Actually, the\ncases where multiple paths correspond to the same\nproduct are rare in the pre-training datasets, thus\nBatGPT-Chem's ability to suggest various feasible\nroutes can be attributed to its profound under-\nstanding of chemical reaction mechanisms. By\noptimizing beam search strategies and tempera-\nture settings, the model adeptly balances diversity\nand correctness. In contrast, efforts to enhance\noutput diversity in other LLMs through elevated\ntemperature settings often result in an increased\nerror rate.\nIn conclusion, BatGPT-Chem sets new bench-\nmarks for effective and dependable AI-driven ret-\nrosynthesis planning. However, the work is still\ntempered by the quality of the data, primarily\nfrom open-access sources. Despite endeavors to\nenrich reaction condition data and compile com-\nprehensive retrosynthesis pathways, gaps remain.\nFuture improvements will likely require collec-\ntive efforts from across the scientific community.\nAnother constraint is the scope of chemical lan-\nguages covered; currently focused on SMILES,\nexploring additional string-based representations\nlike SELFIES [52] could broaden our model's util-\nity, paving the way for its application to a greater\nspectrum of chemical reactions."}, {"title": "4 Methods", "content": "Viewing natural language as a specialized lan-\nguage, we can employ LLMs for unified modeling\nof natural language to SMILES, SMILES to nat-\nural language, SMILES to SMILES, and natural\nlanguage to natural language. This naturally facil-\nitates the completion of various chemistry tasks:\nMolecule Description, Molecule design,\nProduct Inference, and Retro-synthesis Pre-\ndiction. Additionally, we have also modeled the\nYield Prediction task. We showcase our mod-\neling approach in Fig. 6. We model molecule\ndescription as bidirectional conversions between\nnatural language and SMILES, as well as con-\nversions between natural language. We model\nmolecule design as a conversion from natural lan-\nguage to SMILES. We also model product infer-\nence and retro-synthesis prediction as conversions\nfrom SMILES to SMILES. Additionally, we have\nalso included a task for yield prediction."}, {"title": "4.2 Chemistry tasks and prompt\ntemplates", "content": "Following the modeling approach outlined above,\nwe focus on the key tasks in the chem-\nistry domain: Molecule Description, Molecule\ndesign, Product Inference, Retro-synthesis\nPrediction, and Yield Prediction. We con-\nstruct instruction tuning datasets based on exist-\ning chemical, drug, and medicine datasets using\nprompt templates, to train models capable of\naddressing these tasks.\nRetro-synthesis\nPrediction. Retro-\nsynthesis Prediction is a crucial task for chemistry.\nIt involves inferring possible reaction pathways\nand conditions by given product molecules,\nthereby reverse-predicting the synthetic route\nto generate the product. Retro-synthesis predic-\ntion enables researchers to explore and discover\nnew organic molecular structures more rapidly,\nwhich is essential for fields such as organic syn-\nthesis chemistry and drug discovery. We train\nthe model's retro-synthesis prediction capability\nusing two subtasks: 1) Reactant and catalyst\nprediction: Given a product, predict the potential\ncatalysts and reactants that may be required.\n2) Reactant prediction: Given a product and\ncatalyst, predict the reactants.\nProduct Inference. Product inference aims\nat predicting the products based on given start-\ning materials and specific reaction conditions,\nwhich holds significant importance in fields such\nas organic synthesis and drug design. We train\nthe model's product inference capability using\ntwo subtasks: 1) Product and catalyst prediction:\nGiven reactants, predict the potential catalysts\nand products that may be involved. 2) Product\nprediction: Given products and catalysts, predict\nthe reactants involved.\nMolecule Design. Molecule Design is a field\ninvolving the creation of new molecules using\ntheoretical and computational methods to pro-\nduce molecular structures with specific proper-\nties or functionalities. This field plays a crucial\nrole in various domains including chemistry, drug\ndesign, and materials science. The aim of molecule\ndesign is to systematically generate molecules\nwith desired properties and activities to meet\nspecific application needs. This work fully con-\nsiders over a hundred molecular properties, such\nas molecule weight, valence electron count, Bal-\naban J value, BertzCT value, number of heavy\natoms, number of NHs or OHs, and number of\nnitrogen and oxygen atoms. It is hoped that the\nLLM can take into account researchers' specific\nrequirements for molecule properties of cataly-\nsis, products, and reactants of chemical reactions.\nTo train the model's molecular design capability,\nthe following three tasks are adopted: 1) Specify-\ning catalyst molecular properties: Given reactants\nto produce a specific product, the catalyst is\nrequired to meet certain properties. 2) Specifying\nreactant and catalyst molecular properties: Given\nthe desired product to be synthesized, both reac-\ntants and catalysts are required to meet certain"}, {"title": "4.3 Pre-training data source", "content": "We utilize publicly available high-quality datasets\nin the field of chemistry, as well as close-source\ndatasets within our own team, as the raw datasets.\nThen, we transform them into instruction tun-\ning datasets using the aforementioned prompt\ntemplates."}, {"title": "4.3.1 Publicly Available Datasets", "content": "\u2022 USPTO [53] USPTO collects reaction data\nextracted through text mining from United\nStates patents published between 1976 and\nSeptember 2016.\n\u2022 CHEBI [54] Chemical Entities of Biological\nInterest (CHEBI) is a freely available dictionary\nof molecular entities focused on \"small\" chem-\nical compounds. The term \u201cmolecular entity\u201d\nrefers to any constitutionally or isotopically\ndistinct atom, molecule, ion, ion pair, radical,\nradical ion, complex, conformer, etc., identifi-\nable as a separately distinguishable entity. The\nmolecular entities in question are either prod-\nucts of nature or synthetic products used to\nintervene in the processes of living organisms.\n\u2022 CJHIF [50] Chemical Journals with High\nImpact factors (CJHIF) is a high-quality\ndataset containing a large number of chemi-\ncal reaction equations extracted from various\nchemical journals.\n\u2022 PubChem [55] PubChem is an open chemistry\ndatabase at the National Institutes of Health\n(NIH), which mostly contains small molecules,\nbut also larger molecules such as nucleotides,\ncarbohydrates, lipids, peptides, and chemically-\nmodified macromolecules.\n\u2022 Text2Mol [56] Text2Mol provides a large\namount of data containing natural language\ndescriptions of molecules."}, {"title": "4.3.2 Close Source Datasets", "content": "\u2022 Drug Instruction We collect a large number\nof drug names, drug descriptions, and corre-\nsponding molecular formulas from drug instruc-\ntion to enhance the model's capabilities in the\npharmaceutical domain.\n\u2022 Organic Compound Manual We have a large\ncollection of private organic compound man-\nuals, containing information such as organic\ncompound names, compound descriptions, com-\npound SMILES, etc."}, {"title": "4.4 Data Transformation For\nInstruction Tuning", "content": "We extract reaction data into reactant SMILES,\ncatalyst SMILES, product SMILES, and yield\ndata. Then we conduct data augmentation, that is\nif there are multiple reactants, catalysts, or prod-\nucts, we shuffle the SMILES of these compounds.\nFor retro-synthetic prediction, product infer-\nence, and yield inference, we organize reactant\nSMILES, catalyst SMILES, product SMILES, and\nyield data according to the prompt templates. For\nmolecule design, we use the RDKit tool to ran-\ndomly select 1-20 properties from a candidate pool\nof 172 properties to fill in the prompt templates."}, {"title": "4.5 Data Details", "content": null}, {"title": "4.6 Training Details", "content": null}, {"title": "4.6.1 Base Model", "content": "We select our team's self-developed BatGPT-\n15B model [29] as the base model for instruc-\ntion tuning. BatGPT-15B is a large bilingual\nmodel for both Chinese and English, pre-trained\nusing bidirectional autoregressive methods, and\nhas demonstrated excellent performance on public\nbenchmarks such as CMMLU [57]."}, {"title": "4.6.2 Vocabulary Expansion", "content": "Since the BatGPT-15B model is originally\ndesigned for natural language, particularly Chi-\nnese and English, it lacks comprehensive coverage\nof specialized terms in chemistry or SMILES.\nConsequently, expanding its vocabulary becomes\nnecessary. We employ the Byte Pair Encoding\n(BPE) algorithm to train a vocabulary using\ndiverse training data, encompassing various forms\nof molecular SMILES, chemical equation SMILES\nexpressions, molecular names, and more. We also\ninclude all chemical element symbols in the aug-\nmented vocabulary to empower the model with\nthe potential to handle all chemical elements. Sub-\nsequently, we merge this augmented vocabulary\nwith that of BatGPT-15B, ultimately yielding a\nfinal vocabulary size of 151851."}, {"title": "4.6.3 Training Settings", "content": "We train our model using the deepspeed zero2\nstrategy on an Nvidia A800 GPU cluster. We set\nthe maximum length to 2048, the batch size per\nGPU to 8, utilize the AdamW optimizer with a\nlearning rate of 2e-4, and employ the cosine learn-\ning rate schedule strategy. We enable gradient\ncheckpointing and set max gradient normalization\nto 1.0 and weight decay to 0.1."}, {"title": "5 Data and Code Availability", "content": "Working in progress."}, {"title": "6 Acknowledgements", "content": "Working in progress."}, {"title": "7 Authors' Contributions", "content": "Working in progress."}, {"title": "8 Competing Interests", "content": "The authors declare that they have no competing\ninterests."}]}