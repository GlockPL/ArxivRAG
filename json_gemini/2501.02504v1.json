{"title": "Watch Video, Catch Keyword: Context-aware Keyword Attention\nfor Moment Retrieval and Highlight Detection", "authors": ["Sung Jin Um", "Dongjin Kim", "Sangmin Lee", "Jung Uk Kim"], "abstract": "The goal of video moment retrieval and highlight detection\nis to identify specific segments and highlights based on a\ngiven text query. With the rapid growth of video content\nand the overlap between these tasks, recent works have ad-\ndressed both simultaneously. However, they still struggle to\nfully capture the overall video context, making it challenging\nto determine which words are most relevant. In this paper,\nwe present a novel Video Context-aware Keyword Attention\nmodule that overcomes this limitation by capturing keyword\nvariation within the context of the entire video. To achieve\nthis, we introduce a video context clustering module that pro-\nvides concise representations of the overall video context,\nthereby enhancing the understanding of keyword dynamics.\nFurthermore, we propose a keyword weight detection mod-\nule with keyword-aware contrastive learning that incorporates\nkeyword information to enhance fine-grained alignment be-\ntween visual and textual features. Extensive experiments on\nthe QVHighlights, TVSum, and Charades-STA benchmarks\ndemonstrate that our proposed method significantly improves\nperformance in moment retrieval and highlight detection\ntasks compared to existing approaches. Our code is available\nat: https://github.com/VisualAIKHU/Keyword-DETR.", "sections": [{"title": "Introduction", "content": "With the exponential growth of video content, precise video\nmoment retrieval and highlight detection have become cru-\ncial (Snoek, Worring et al. 2009; Apostolidis et al. 2021).\nVideo moment retrieval enables users to find specific seg-\nments within videos based on natural language queries\n(Anne Hendricks et al. 2017), while highlight detection\nhelps extract the most engaging parts from long-form videos\n(Gygli et al. 2014). These technologies enhance user experi-\nence and productivity across various applications, including\nvideo searching, video editing, social media, and e-learning,\nby enabling quick and accurate access to relevant content.\nExtensive research has been conducted on moment re-\ntrieval (Gao et al. 2017; Hendricks et al. 2018; Xiao et al.\n2021; Sun et al. 2022) and highlight detection (Sun, Farhadi,\nand Seitz 2014; Xu et al. 2021; Wei et al. 2022; Badamdorj\net al. 2022) as separate tasks. However, with the introduc-\ntion of Moment-DETR and the QVHighlights dataset (Lei,\nBerg, and Bansal 2021), which allows for the simultaneous\nexecution of these tasks, new studies (Liu et al. 2022; Moon\net al. 2023; Xiao et al. 2024; Sun et al. 2024) have emerged\nthat aim to address moment retrieval and highlight detection\nbased on text queries concurrently. Following the Moment-\nDETR (Lei, Berg, and Bansal 2021), UMT (Liu et al. 2022)\nutilizes audio-visual multi-modal learning. TR-DETR (Sun\net al. 2024) leverages the reciprocal relationship between\ntwo tasks, refining visual features through textual guidance\nto enhance both tasks simultaneously. UVCOM (Xiao et al.\n2024) introduces integration module for progressive intra-\nand intermodality interaction across multi-granularity.\nDespite these advancements, existing methods often fail\nto capture the dynamic importance of keywords within the\ncontext of the video, which is crucial for accurate moment\nretrieval and highlight detection. As illustrated in Figure 1,\nthe importance of each word in a text query can vary de-\npending on the video content. For instance, when we con-\nsider a text query 'A dog is playing in the garden', the im-\nportance of words such as 'dog' or 'garden' can shift based\non the predominant scenes in the video. In Video #1, where\nmost scenes contain 'garden', the word 'dog' is more criti-\ncal than 'garden' for specifying the desired video segment.\nConversely, in Video #2, 'dog' is predominantly 'playing'\nindoors, making 'garden' more essential than 'dog' for iden-\ntifying the relevant video segment. This indicates that the\nimportance of words in a text query can vary significantly\ndepending on the video context. Therefore, it is necessary to\nconsider such keyword variations for effective moment re-\ntrieval and highlight detection. However, existing methods\nfall short in addressing this keyword variation, as they rely\non text features extracted independently of the video con-\ntext, failing to capture the dynamic importance of words rel-\native to the visual content.\nIn this paper, we propose a Video Context-aware Keyword\nAttention module that effectively captures keyword varia-\ntions by considering the overall video context. Our approach\naddresses two main challenges: (i) how to effectively encode\nthe overall context of a video to capture keyword variation,\nand (ii) how to capture and utilize desired text keywords\nwithin their relevant video contexts.\nFirst, effective keyword extraction requires a comprehen-\nsive understanding of the overall context of the video. To\naddress this, we tackle the challenge (i) by introducing a\nvideo context clustering module that leverages temporally-\nweighted clustering to group similar video scenes. This ap-\nproach allows our model to grasp the high-level flow and\nstructure of the video. The resulting cluster assignments pro-\nvide a concise representation of the overall video context\nand are leveraged to understand keyword dynamics. Fur-\nthermore, since these clustered features contain information\nabout scene changes, they are further used as additional hints\nfor moment retrieval and highlight detection.\nTo address the challenge (ii), we propose a keyword\nweight detection module. This module recognizes less fre-\nquently occurring but important words in the text query and\ncalculates the similarity between clustered video features\nand text features to generate a keyword weight vector. This\nvector captures information about the important words in the\ntext query within the video context, allowing our framework\nto adjust the keywords based on the overall video context dy-\nnamically. Based on this, we introduce keyword-aware con-\ntrastive learning to incorporate keyword weights and facil-\nitate a fine-grained alignment between visual and text fea-\ntures. As a result, our method allows for accurate moment\nretrieval and highlight detection.\nThe major contributions of our paper are as follows:\n\u2022 We propose a video context-aware keyword attention\nmodule to capture keyword variations by considering\noverall context of the video for effective moment retrieval\nand highlight detection. To the best of our knowledge,\nthis is the first work to address this aspect in video mo-\nment retrieval and highlight detection tasks.\n\u2022 We introduce keyword-aware contrastive learning to in-"}, {"title": "Related Works", "content": "tegrate keyword weight information, enhancing the fine-\ngrained alignment between visual and text features. This\napproach improves the ability of model to understand the\nrelationship between textual queries and video content.\n\u2022 Experimental results on QVHighlights, TVSum, and\nCharades-STA demonstrate the effectiveness of our\nmethod for moment retrieval and highlight detection.\nMoment Retrieval\nThe connection between visual and language cues has be-\ncome important in machine learning (Lee et al. 2022b; Park\net al. 2024b; Lee et al. 2024). Moment retrieval aims to lo-\ncate relevant moments in a video based on a natural language\nquery (Gao et al. 2017). This task is typically approached\nusing either proposal-based or proposal-free methods. The\nproposal-based methods (Gao et al. 2017; Hendricks et al.\n2018; Xiao et al. 2021; Sun et al. 2022) generate candidate\nproposals and rank them by matching scores. On the other\nhand, the proposal-free methods (Yuan, Mei, and Zhu 2019;\nMun, Cho, and Han 2020; Rodriguez et al. 2020; Li, Guo,\nand Wang 2021) directly regress the start and end times-\ntamps through video-text interaction.\nHighlight Detection\nHighlight detection identifies the most significant parts of a\nvideo, which might not necessarily be tied to specific tex-\ntual queries. Early highlight detection approaches are uni-\nmodal, assessing the salience score of video clips without\nexternal textual data (Sun, Farhadi, and Seitz 2014; Xu et al.\n2021; Wei et al. 2022; Badamdorj et al. 2022). However, as\nuser preferences have increasingly influenced content con-\nsumption, integrating text queries has become common to\ntailor the detection process to individual user needs (Dagtas\nand Abdel-Mottaleb 2004; Kudi and Namboodiri 2017; Lei,\nBerg, and Bansal 2021). It has been demonstrated that au-\ndio cues provide complementary information to visual fea-\ntures (Lee et al. 2022a; Park et al. 2024a; Kim et al. 2024).\nAdvanced highlight detection systems now employ multi-\nmodal inputs, including visual and audio cues, to enhance\nthe detection accuracy and relevance of video highlights\n(Gygli, Song, and Cao 2016; Xiong et al. 2019; Hong et al.\n2020; Badamdorj et al. 2021).\nTraditionally, moment retrieval and highlight detection\nare addressed separately, but recent work has explored their\njoint learning. MomentDETR (Lei, Berg, and Bansal 2021)\nintroduces the QVHighlights dataset to facilitate joint learn-\ning of moment retrieval and highlight detection, proposing a\nDETR-based model. UMT (Liu et al. 2022) proposes adopt-\ning audio, visual, and text content for query generation to\nimprove query quality. QD-DETR (Moon et al. 2023) further\nleverages textual information by incorporating negative rela-\ntionship learning between video-text pairs. UVCOM (Xiao\net al. 2024) and TR-DETR (Sun et al. 2024) integrate the\nspecialties of moment retrieval and highlight detection to\nachieve a comprehensive understanding.\nDespite these advancements, many existing methods do\nnot capture the overall context of the video. Capturing the"}, {"title": "Proposed Method", "content": "overall context is essential for accurate moment retrieval\nand highlight detection as it provides a comprehensive un-\nderstanding of the content and narrative flow. To this end,\nwe present a video context-aware keyword attention module\nthat understands the video context and identifies keywords\nbetween the video and text query.\nFigure 2 shows the overall architecture of our framework.\nSimilar to previous works (Moon et al. 2023; Sun et al. 2024;\nXiao et al. 2024), we employ a two-stream network to ex-\ntract video and text features. Input video of L clips and a\ntext query with N words pass through each modal encoder\n(i.e., video encoder and text encoder) and three-layer feed-\nforward network to generate video features $F^v \\in \\mathbb{R}^{L\\times d}$\nand text features $F^t \\in \\mathbb{R}^{N\\times d}$, respectively. To capture the\noverall flow of the video, $F^v$ is passed through a video con-\ntext clustering module to generate c clustered video features\n$F^{cv} \\in \\mathbb{R}^{c\\times d}$. Subsequently, to identify video-related key-\nwords in the text query, a keyword weight detection mod-\nule calculates the similarity between clustered video features\nand text features, resulting in a keyword-weighted text fea-\nture $F^{wt}$. We then perform modality interaction between the\nvideo features and keyword-weighted text features, which\nis processed through a transformer encoder. Finally, we uti-\nlize the video-contextual information from $F^{cv}$ and cluster\ninformation C, along with the transformer-encoded feature,\nto conduct moment retrieval and highlight detection. More\ndetails are in the following subsections."}, {"title": "Video Context-aware Keyword Attention Module", "content": "To understand the entire video sequence and extract relevant\ntext keywords corresponding to the video content, we pro-\npose a Video Context-aware Keyword Attention Module.\nAs shown in Figure 2, our video context-aware keyword\nattention module consists of two steps: (1) video context\nclustering and (2) keyword weight detection."}, {"title": "(1) Video Context Clustering", "content": "We propose a video context\nclustering module to cluster video clips, capturing the\noverall context of the video and identifying each scene.\nTo account for the temporal order of the video, we adopt\na temporally-weighted FINCH algorithm (Sarfraz et al.\n2021). With video features $F^v \\in \\mathbb{R}^{L\\times d}$ as input, the algo-\nrithm clusters video clips based on their adjacency relation\nand merges them hierarchically. Consequently, the most\nsimilar clips are grouped into c clusters ($C_1, ...C_c$) based on\ntheir relations. The output is a cluster information vector\n$C = {C_i}_{i=1}^L \\in \\mathbb{R}^L$, where each element $C_i \\in {c_1, ..., C_c}$\nindicates the cluster assignment value for each video clip.\nThen, the clustered video features $F^{cv}$ are generated by\naveraging the information within each cluster."}, {"title": "(2) Keyword Weight Detection", "content": "Aligning video and text\nfeatures is essential for moment retrieval and highlight de-\ntection tasks. However, direct interaction between original\nvideo and text features can lead to information loss due\nto the misalignment (Xu, Zhu, and Clifton 2023). Particu-\nlarly in video moment retrieval and highlight detection tasks\nbased on specific text queries, the emphasis on certain text\nkeywords varies depending on the overall video context.\nConsider the text query \u201cChef makes pizza and cuts it up\u201d in\nFigure 2. From the perspective of moment retrieval and high-\nlight detection, the word 'chef' might be less important if it\nappears consistently throughout the video. Instead, a sud-\nden appearance of 'pizza' or the action of 'cuts' could be\nmore significant. Conversely, if a machine is making pizza\nand a chef suddenly appears and makes pizza, 'chef' be-\ncomes a crucial keyword. As keyword importance changes\nwith video content, we conduct keyword weight detection to\nidentify the most important keyword related to the video.\nTo this end, we calculate cosine similarity matrix $M\\in$"}, {"title": "Video Contextual MR/HD Prediction", "content": "$\\mathbb{R}^{N\\times c}$ between the text feature $F^t$ and the clustered video\nfeature $F^{cv}$ (see Figure 2). We then apply each column-wise\nsoftmax and max-pooling to obtain a keyword weight vec-\ntor $w^t \\in \\mathbb{R}^{N}$. Higher values in the keyword weight vector\nindicate words that are strongly associated with only spe-\ncific clusters in the video, while lower values are associated\nwith most clusters similarly. Finally, we multiply $w^t$ with\nthe original text feature $F^t$ to generate the keyword-weighted\ntext feature $F^{wt} \\in \\mathbb{R}^{N\\times d}$, which is be represented as:\n$M = \\frac{F^t{F^{cv}}^T}{||F^t|| ||F^{cv}||},$\n(1)\n$w^t = MaxPooling(Softmax(M/\\tau)),$\n(2)\n$F^{wt} = w^tF^t,$\n(3)\nwhere $\\tau$ is a temperature hyper-parameter. This keyword-\nweighted text feature $F^{wt}$ emphasizes contextually impor-\ntant words in the query, enhancing video-text alignment and\nimproving performance in moment retrieval and highlight\ndetection tasks.\nMoment retrieval (MR) and highlight detection (HD) are\ntwo crucial tasks in video understanding. Moment retrieval\naims to localize the center coordinates and duration of mo-\nments related to a given text query, while highlight detec-\ntion generates a saliency score distribution across the entire\nvideo. To improve the effectiveness of these tasks, we utilize\nthe video-contextual information C and $F^{cv}$ generated by the\nvideo context clustering module in each prediction head.\nIn the context of moment retrieval, transition points be-\ntween clusters are crucial. These points often correspond to\nscene changes in the video, providing valuable information\nfor the moment retrieval task. By leveraging the cluster in-\nformation vector C, we create a binary context change vector\n$C^m \\in \\mathbb{R}^{L}$ that encapsulates information about these transi-\ntions. In this vector, we assign a value of 1 if the cluster\nnumber changes between the i-th and (i+1)-th frame (e.g.,\n$C_i \\neq C_{i+1}$), and 0 otherwise. Then, following previous works\n(Moon et al. 2023), we employ a standard transformer de-\ncoder structure for moment retrieval head. However, instead\nof the traditional approach, we use $C^m$ as the initial embed-\nding for the learnable anchors, which helps the MR decoder\nbetter focus on scene transition points in the video, poten-\ntially leading to more accurate moment identification.\nFor highlight detection, we focus on the representative\nvalues of each cluster obtained through our clustering ap-\nproach. These values provide information about the best rep-\nresentation of each scene context. To obtain this informa-\ntion, we compute a cosine similarity between each video clip\nfeature $F^v$ and the average information of the cluster it be-\nlongs to, which we extract from $F^{cv}$ using C. This similarity\ncomputation results in $C^h \\in \\mathbb{R}^{L}$, which indicates how well\neach clip represents the information of its cluster. To gener-\nate saliency score distribution for highlight detection, we use\ntwo groups of single fully connected layers for linear projec-\ntion to calculate the saliency score, following (Moon et al.\n2023). As input to this process, we use a context-aware video"}, {"title": "Keyword-aware Contrastive Loss", "content": "token $T^{cv} \\in \\mathbb{R}^{L\\times (d+1)}$, which is created by concatenating\n$C^h$ with the video token $T^v \\in \\mathbb{R}^{L\\times d}$ obtained from pass-\ning through a transformer encoder. The predicted saliency\nscores S are then computed using the following equation:\n$S = \\frac{{T_s}^s W_s T^v  +  T_{cv} W_{cv} T_{cv}}{\\sqrt{p}},$\n(4)\nwhere $T_s \\in \\mathbb{R}^d$ is the randomly initialized input-adaptive\nsaliency token, $W_s \\in \\mathbb{R}^{p\\times d}$ and $W_{cv} \\in \\mathbb{R}^{p\\times (d+1)}$ are learn-\nable parameters, and p is the projection dimension.\nTo enhance the alignment between text query features and\nvideo features by leveraging the overall flow of the video,\nwe introduce keyword-aware contrastive loss. This loss is\ncomposed of two components: a clip-keyword contrastive\nloss and a video-keyword contrastive loss. The clip-keyword\ncontrastive loss focuses on intra-video relationships be-\ntween text queries and visual features of each clip, while\nthe video-keyword contrastive loss addresses inter-video\nrelationships across the dataset.\nClip-keyword Contrastive Loss. Existing methods (Xiao\net al. 2024; Sun et al. 2024) typically construct loss func-\ntions that bring the clip features of ground-truth moments\ncloser to the text query features while pushing background\nclip features away. However, as illustrated in Figure 1, even\nbackground clips considered irrelevant to the text query may\nstill have high relevance to specific words in the text query.\nIn such cases, the feature of background clips can be mis-\nrepresented through contrastive loss from existing methods.\nTo address this, we utilize the keyword weight vector $w^t$\nto emphasize keywords in advance. This approach enables\nmore robust alignment between the clip features $F^v$ and the\nkeyword-weighted text features $F^{wt} = w^tF^t$ of Eq.(3). We\nformulate the clip-keyword contrastive loss $L_{ck}$ as follows:\n$G^{wt} = MeanPooling(F^{wt}),$\n(5)\n$L_{ck} = \\frac{1}{B} \\sum_{i=1}^B log \\frac{exp(Sim(F^v(i), G^{wt}(i))}{\\sum_{j=1}^B exp(Sim(F^v(j), G^{wt}(i))},$\n(6)\n$Sim(A, B) = \\frac{AB^T}{||A||||B||},$\n(7)\nwhere $G^{wt} \\in \\mathbb{R}^d$ is average of word-level text feature, $R_i$\ndenotes relevant ground-truth clips in the i-th video and B\nindicates the batch number. The $L_{ck}$ maximizes the learning\neffect of essential central information, thereby enabling\nmore accurate moment retrieval and highlight detection.\nVideo-keyword Contrastive Loss. Extending beyond sin-\ngle video contexts, we propose a global contrastive loss,\ncalled video-keyword contrastive loss, to operate across\nthe entire dataset. Unlike existing methods (Xiao et al.\n2024; Sun et al. 2024) that use unweighted global in-\nformation from video-text pairs, we incorporate keyword-\nweighted text features $F^{wt}$ to obtain a better global represen-"}, {"title": "Experimental Results", "content": "tation by utilizing the global information of relevant videos\nand keyword-weighted text queries. We define the video-\nkeyword contrastive loss $L_{vk}$ as:\n$G = MeanPooling(F^v),$\n(8)\n$L_{vk} = \\frac{1}{B} \\sum_{i=1}^B log \\frac{exp(Sim(G^v(i), G^{wt}(i))}{\\sum_{j=1}^{B-1} exp(Sim(G^v(i), G^{wt}(i))},$\n(9)\nwhere $G^v \\in \\mathbb{R}^d$ is average of clip-level visual feature, rb is a\nbinary value (1 for ground-truth clips, 0 otherwise). The $L_{vk}$\nstrengthens the global representation based on keywords, fa-\ncilitating more effective cross-video learning.\nFinally, we devise a keyword-aware contrastive loss $L_{kw}$\nthat combines $L_{ck}$ and $L_{vk}$, which can be formulated as:\n$L_{kw} = L_{ck} + L_{vk}.$\n(10)\nThe $L_{vk}$ enables our model to optimize both temporal rel-\nevance within videos and global semantic coherence across\nthe dataset, achieving a comprehensive alignment between\ntext queries and video contents.\nTraining Objective\nTo train our proposed method, we construct the total training\nloss function as follows:\n$L_{Total} = L_{mr} + L_{hd} + \\lambda_{kw}L_{kw},$\n(11)\nwhere $L_{mr}$ and $L_{hd}$ denote the loss functions for moment\nretrieval and highlight detection as outlined in (Moon et al.\n2023). $\\lambda_{kw}$ is a balancing parameter. The $L_{Total}$ enables ef-\nfective moment retrieval and highlight detection.\nDatasets and Evaluation Metrics\nQVHighlights. The QVHighlights dataset (Lei, Berg, and\nBansal 2021) includes 10,148 YouTube videos with rich\ncontent, each paired with an annotated text query that\nindicates highlight moments. This is the only dataset that\nincludes both annotations for moment retrieval and high-\nlight detection. Following (Lei, Berg, and Bansal 2021), to"}, {"title": "Discussion", "content": "ensure a fair evaluation, we submitted our model predictions\nto the QVHighlights server CodaLab competition platform,\nwith test set annotations remaining confidential.\nTVSum. The TVSum dataset (Song et al. 2015) is also\na standard benchmark for highlight detection, comprising\nvideos from 10 different categories, with each category\ncontaining 5 videos. For a fair comparison, we use the same\ntrain/test split as utilized in QD-DETR(Moon et al. 2023).\nCharades-STA. The Charades-STA dataset (Gao et al.\n2017) contains 9,848 videos depicting indoor activities with\n16,128 human-annotated query texts. Following QD-DETR\n(Moon et al. 2023), we use 12,408 samples for training,\nwith the remaining 3,720 samples allocated for testing.\nEvaluation Metric. For the evaluation, we follow the met-\nrics of prior works (Lei, Berg, and Bansal 2021; Xiao et al.\n2024; Sun et al. 2024) for a fair and comprehensive com-\nparison. In the QVHighlights dataset, we evaluate Recall@1\n(R1) at IoU thresholds of 0.5 and 0.7, and mean average pre-\ncision (mAP) at thresholds from 0.5 to 0.95 in steps of 0.05\n(mAP@Avg). We compare performance at IoU thresholds\nof 0.5 and 0.75, referred to as mAP@0.5 and mAP@0.75.\nFor highlight detection, we use mAP and HIT@1 (hit ratio\nof the highest-scored clip). In the Charades-STA dataset, we\nevaluate Recall@1 at IoU thresholds of 0.5 and 0.7. For the\nTVSum dataset, the primary evaluation metric is top-5 mAP.\nImplementation Details\nPre-extracted Features. Following (Moon et al. 2023), we\nuse the pre-extracted video, text, and audio features from the\nvarious models. For video features, we use the pre-trained\nSlowFast (Feichtenhofer et al. 2019) and CLIP (Radford\net al. 2021) models for QVHighlights, VGG (Simonyan and\nZisserman 2014) and SlowFast+CLIP (SF+C) for Charades-\nSTA, and I3D pre-trained on Kinetics 400 (Carreira and\nZisserman 2017) for TVSum. For text features, we use\nCLIP (Radford et al. 2021) for QVHighlights and TVSum,"}, {"title": "Conclusion", "content": ""}]}