{"title": "01-CODER: AN 01 REPLICATION FOR CODING", "authors": ["Yuxiang Zhang", "Shangxi Wu", "Yuqi Yang", "Jiangming Shu", "Jinlin Xiao", "Chao Kong", "Jitao Sang"], "abstract": "The technical report introduces O1-CODER, an attempt to replicate OpenAI's ol model with a focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode, followed by the generation of the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for environment state updates. Updated model progress and experimental results will be reported in subsequent versions. All source code, curated datasets, as well as the derived models will be disclosed at https://github.com/ADaM-BJTU/O1-CODER.", "sections": [{"title": "1 INTRODUCTION", "content": "OpenAI recently introduced the o1 model (OpenAI, 2024), which has demonstrated impressive system-2 thinking capabilities. This model represents a significant advancement in AI's ability to perform complex reasoning tasks that require higher-order cognitive functions. Following its release, numerous analysis and replication efforts have emerged, highlighting the growing interest and potential of o1-like models. Notable works include g1 (Benjamin Klieger, 2024), OpenO1 (ope, 2024), O1-Journey (GAIR-NLP, 2024), OpenR (Team, 2024), LLaMA-01 (SimpleBerry, 2024), LLaMA-Berry (Zhang et al., 2024), Steiner (Ji, 2024), Thinking Claude (Richards Tu, 2024), LLaVA-01 (Xu et al., 2024), and several industrial releases such as k0-math, DeepSeek-R1-Lite, Macro-01 (Zhao et al., 2024), Skywork 01, QwQ (Qwen Team, 2024), and InternThinker (Shanghai AI Lab, 2024). Prior to the ol model, large language models (LLMs) primarily exhibited System-1 capabilities, characterized by fast, intuitive responses. These models were trained on datasets consisting mainly of question-answer (Q, A) pairs, lacking the intermediate reasoning steps that involve deliberate and analytical processing. This stems from the fact that humans rarely record their thought processes on the internet or elsewhere. Traditionally, techniques such as Chain-of-Thought (CoT) prompting were used to guide models in generating step-by-step reasoning before arriving at an answer. However, a more direct and effective way is to create datasets including the reasoning sequences, e.g., (Q, ..., Si, ..., A), where Si represents an individual reasoning step leading to the final answer. It is widely believed that o1 addresses the lack of reasoning data by combining reinforcement learning with pretraining. Reinforcement learning (RL) is well known for its ability to explore and discover new strategies rather than relying on predefined data. Looking back at key developments in machine learning, we can see that deep learning and large-scale pretraining have driven transformations in model architecture and the requirements for labeled data, respectively. In contrast, reinforcement learning addresses a different aspect of transformation on the objective function. In"}, {"title": "2 FRAMEWORK OVERVIEW", "content": "There are two main challenges to address for self-play RL applied to code generation. The first challenge is result evaluation, i.e., assessing the quality of the generated code. Unlike tasks such as Go or mathematics, where results can be directly evaluated based on game rules or correct answers, evaluating code requires running the generated code within a testing environment and verifying it against test cases. We cannot assume that code datasets will always provide sufficient test cases. The second challenge involves defining the thinking and search behaviors, i.e., determining the object and granularity of process rewards. For code generation, the key question is how to design the reasoning process and the space of policies to guide the model's behavior effectively. To address the first challenge, we propose training a Test Case Generator (TCG), which automatically generates test cases based on the question and the ground-truth code. This approach will help build a standardized code testing environment, providing result rewards for reinforcement learning. For the second challenge, two possible approaches can be considered. One is \u201cthink before acting\", where the model first forms a complete chain of thought and then generates the final answer all at once. The other approach, \u201cthink while acting\" (Zelikman et al., 2024), involves generating parts of the answer while simultaneously reasoning through the task. We chose the former approach. For code generation, this means first thinking through and writing out a detailed pseudocode, which is then used to generate the final executable code. The advantages are twofold: adaptability, as the same pseudocode can lead to different concrete code implementations; and controllable granularity, as adjusting the level of detail in the pseudocode can be adjusted to control the granularity of the reasoning/search behavior. The complete framework pseudocode is provided in Algorithm 1, which consists of six steps. (1) The first step is training the test case generator (TCG) $Y_{TCG}$, which is responsible for automatically generating test cases based on the question. (2) In the second step, we run MCTS on the original code dataset to generate code data with reasoning processes $D_{process}$, including a validity indicator to"}, {"title": "3 METHOD AND INTERMEDIATE RESULTS", "content": "3.1 TEST CASE GENERATOR TRAINING\n3.1.1 OBJECTIVE\nA Test Case Generator is a tool designed to automate the creation of input-output test cases, which plays a critical role in supporting program verification in code generation tasks. During the training phase, the correctness of the generated code is typically assessed with standard input-output test cases. The pass rate of these test cases serves as a key metric for evaluating the quality of the generated code and acts as an outcome reward signal to guide the training of the policy model. This reward signal helps the model refine its generation strategy, thereby enhancing its capability to produce accurate and functional code. In the inference phase, when the trained model is tasked with code generation, standard test cases are often not available to verify the correctness of the generated code. The test case generator mitigates this limitation by providing a self-validation mechanism for the policy model, which allows the policy model to evaluate before final generation. As a result, the policy model is able to select the optimal output path based on the validation results. 3.1.2 TRAINING\nThe training process is divided into two distinct phases: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) (Rafailov et al., 2024). We denote the generator which is not fine-tuned as $TCG_{base}$. The primary objective of the SFT phase is to ensure that the generator's output adheres to a predefined format, enabling the accurate parsing and extraction of the generated test cases. The training data for this phase is derived from the TACO dataset (Li et al., 2023), which follows the format {question, solution, test_case}. To standardize the model's input and output, we developed a template format, as detailed below:"}, {"title": "3.1.3 EXPERIMENTS", "content": "We utilize DeepSeek-1.3B-Instruct (Guo et al., 2024) as the base model for the test case generator, followed by SFT and DPO. The fine-tuning phase employs QLoRA technology (Dettmers et al., 2023) with a rank parameter $r = 1$ to adapt the following modules: q-proj,o-proj,k-proj,v-proj, gate-proj, up-proj,down_proj. The learning rate is set to 5\u00d7 10-4 to balance training stability and convergence speed. The training data is derived from a subset of the TACO train dataset, which adheres to the ACM competition format and contains approximately 10,000 samples. Similarly, the test data is obtained from a subset of the TACO test dataset, also conforming to the ICPC competition format, and consists of 314 samples. We tested the quality of the generated test cases at different stages of the TACO test. After the SFT phase, the pass rate of test cases generated by $Y_{TCG_{SFT}}$ on the standard code was 80.8%, demonstrating the generator's ability to efficiently produce test cases following preliminary fine-tuning. Furthermore, $TCG_{dpo}$ achieved a performance of 89.2%, reflecting an notable improvement compared to $TCG_{SFT}$. This indicates that preference optimization, by refining the model's decision-making process, significantly enhanced the generator's ability to produce more reliable test cases."}, {"title": "3.2 REASONING-ENHANCED CODE DATA SYNTHESIS", "content": "3.2.1 PSEUDOCODE-BASED REASONING PROCESS\nThe definition of the reasoning process is crucial. As mentioned in the Introduction, we explore a pseudocode-based prompting approach designed to guide large language models in deep reasoning for complex code tasks. Pseudocode, serving as an intermediate representation between natural language descriptions and actual code, offers a more abstract and concise way to express the logical flow of algorithms or programs. To integrate pseudocode reasoning into step-level Chain-of-Thought"}, {"title": "3.2.2 REASONING PROCESS DATA SYNTHESIS", "content": "We use Monte Carlo Tree Search (MCTS) (Kocsis & Szepesv\u00e1ri, 2006; Feng et al., 2023; Qi et al., 2024) to construct step-level process reward data in the form of $D_{process} = {(Q_i, ..., S_i^j, v_i^j, ..., C_i)}$, where $v_i^j$ represents the evaluation of the reasoning path up to step $S_i^j$, and $C_i$ is the executable code derived from the final step $S_i^m$. In this process, we employ the standard MCTS rollout strategy for path exploration. For each problem $Q_i$, we apply the pseudocode prompt strategy defined earlier to guide the reasoning process. When a terminal node $S_i^m$ is reached, a complete pseudocode reasoning path $(Q_i, S_i^1, ..., S_i^m)$ is formed. The reward value $v_i^m$ for the terminal node $S_i^m$ is computed based on two key metrics:"}, {"title": "3.3 POLICY MODEL INITIALIZATION", "content": "After completing the reasoning data synthesis tasks described in Section 3.2, we use each complete reasoning solution in the dataset to initialize the policy model $\u03c0_\u03b8$. This step aims to help $\u03c0_\u03b8$ better understand the task requirements and follow the expected action behavior, providing an optimal starting point for subsequent iterative training. Given the question $Q_i$, the specific reasoning step content generated by the policy model $\u03c0_\u03b8$ at step j can be expressed as $\u03c0_\u03b8(S_i^j | Q_i, S^{1:j\u22121})$, where $S_i^j = (w_1, w_2, ..., w_k)$. Here, $S_i^j$ represents the content of a reasoning step, delimited by specific separators, with $w$ denoting the tokens generated by $\u03c0_\u03b8$ at each decoding step. $S^{1:j\u22121}$ represents the context formed by the outputs of the previous reasoning steps. The policy model $\u03c0_\u03b8$ is then initialized using the set of verified, correct reasoning solutions $D_{process}^+$. This initialization is performed by optimizing the following training objective:\n$\\mathcal{L}_{SFT} = - \\sum_{(Q_i,S,C) \\in D_{process}^+} log \\pi_\\theta (S^{1:m} \\circ C | Q_i)$,\nwhere $\\circ$ denotes the concatenation of the reasoning steps $S^{1:m}$ and the final code $C$. The initialized policy model $\u03c0_{\u03b8}^{FT}$ will then serve as the foundation for subsequent training stages."}, {"title": "3.4 PRM TRAINING", "content": "Given a problem $Q_i$ and a solution prefix corresponding to the current state, the Process Reward Model (PRM), denoted as $Q \\times S \u2192 \\mathbb{R}^+$, assigns a reward value to the current step $S^j$ to estimate its contribution to the final answer. Based on the tree search approach used during data synthesis"}, {"title": "3.5 RL-BASED POLICY MODEL IMPROVEMENT", "content": "We model the code generation task as a language-augmented Markov Decision Process (MDP), formally represented as $M = (V, S, A, T, R, \u00a2)$ (Team, 2024; Carta et al., 2023). In this framework, $V$ denotes the vocabulary, and $w \u2208 V$ represents an individual token generated by the model. The action space $A \u2282 V^N$ and the state space $S \u2282 V^N$ are sets of token sequences, meaning that both actions and states are sequences of tokens. In this framework, $s_0$ represents the question, and the action $a_i$ is considered a reasoning step (referring to the $S_i$ in algorithm 1), which consists of both the type of action and its corresponding chain of thought. The state transition function $T : S \\times A \u2192 S$ defines how the current state $s_t \u2208 S$ changes when an action $a_t \u2208 A$ is taken. Specifically, the action $a_t$ appends tokens to the current state, forming a new state $s_{t+1} = T(s_t, a_t)$. This process continues until the model generates the final solution. The reward function $R : S \\times A \u2192 R^+$ evaluates the quality of intermediate steps, such as the reasoning process or generated code fragments. The function $\\phi$ combines process-based and outcome-based rewards to produce a final reward signal. At each step, the model selects an action $a_t \u2208 A$, which transitions the system to a new state $s_{t+1} = T(s_t, a_t)$. After executing the action, the model receives a process reward $r_t = P_{PRM}(s_{t\u22121}, a_t)$ from PRM. This process repeats until the model either generates the final code or reaches the predefined maximum depth. Once the model generates the final code or completes the search process, the outcome reward $R_i$ is evaluated by testing the generated code against a series of test cases. We propose a reward aggregation function that incorporates both time-dependent weights and a discount factor:\n$\\phi (R_i, r_i^{1:m}) = \\alpha(t) \\cdot R_i + (1 - \\alpha(t)) \\cdot \\frac{1}{m} \\sum_{j=1}^{m} \\gamma r_j$,\nwhere $\u03b1(t)$ is a time-varying factor that adjusts the balance between the final reward $R_i$ and the cumulative intermediate rewards $r^{1:m}$ over time. For instance, $\u03b1(t)$ may decrease over time, gradually placing more weight on the intermediate rewards as the model refines its solution, while reducing the emphasis on the final reward as the model approaches the optimal policy. $r_i^{1:m}$, with $\u03b1(t)$ typically following schedules such as linear or logarithmic decay. The parameter $\u03b3 \u2208 [0,1]$ is the discount"}, {"title": "3.6 NEW REASONING DATA GENERATION AND SELF-PLAY", "content": "In step 6, the updated policy model $\u03c0_\u03b8$ is used to generate new reasoning data, denoted as $D_{process}^{new}$. This data is created by reasoning through new problem instances $Q_i$, generating step-by-step reasoning paths {$S^1_i, S^2_i, . . ., S^m_i$}, with each path culminating in a final code output $C_i$. The reasoning steps are generated iteratively, where each step $S^j_i$ is conditioned on the previous steps. Once the new reasoning data is generated, it is added to the existing dataset $D_{process}$ to form an updated dataset $D_{process} \u2190 D_{process} \u222a D_{process}^{new}$. This update increases the diversity and quality of the reasoning examples, providing more comprehensive training material for subsequent steps. This new data generation process enables the iterative self-play training loop. After adding the new reasoning data, the model undergoes further fine-tuning, starting with updating PRM as described in the 4th step. The PRM, in turn, adjusts the policy model with RL described in the 5th step. This iterative cycle of data generation, reward model updating, and policy improvement ensures sustained improvement in the system's reasoning ability."}, {"title": "4 DISCUSSIONS", "content": "4.1 BITTER LESSON: DATA IS ALL YOU NEED\nOver the last decade, the AI field has been developing along a central line towards maximizing computation-intelligence conversion efficiency, which is to efficiently convert the ever-increasing computing power into higher intelligence levels. Along this line, as illustrated at the top of Fig. 6, early advancements prioritized improvements on the model side: from SVM to DNN and then to Transformer, scalable model architectures were designed to fully leverage computational power. In recent years, the focus has shifted towards the data side. Techniques such as Semi-Supervised Learning (SSL) in pre-training and Reinforcement Learning (RL) in post-training have aimed to harness data more effectively. The o1 model continues this line. It moves from SFT, which leverages high-quality supervised data, to RLHF, which utilizes environmental feedback to access theoretically unlimited data, and finally to ol's innovative approach of supervising the generation process through reward signals derived from the generated reasoning process itself. This progression suggests that, with Transformer architectures now capable of scaling to handle vast amounts of data and training models of sufficient size, the only remaining challenge converges to acquiring adequate data. One approach is to seek data wherever it is lacking, such as reasoning data for system-2 abilities or physical world trajectories for embodied intelligence. Another approach is to explore data types that do not yet exist in the human world, which requires further exploration of techniques like RL and Self-Play. 4.2 SWEET LESSON: BEYOND HUMAN DATA\nA common criticism of LLM is its reliance on existing human-recorded data, which inherently limits their potential. As Wittgenstein stated, \u201cThe limits of my language mean the limits of my world.\u201d The finite scope and depth of human language records constrain the cognitive capabilities of LLMs. However, the success of o1 demonstrates that we can now explore the underlying thought processes behind these recorded data through RL. This advancement signifies a pivotal shift in AI development, moving from mere imitation of human language to the autonomous generation of novel cognitive processes."}, {"title": "4.3 OPPORTUNITIES", "content": "The self-play+RL framework provides a viable solution for exploring underlying data, which opens up the possibility of exploring System-2 solutions for many tasks that were previously reliant on System 1 capabilities. By integrating more thoughtful, step-by-step processes into task execution, we believe that this approach can yield positive results across a wide range of domains (Kant et al., 2024; Ganapini et al., 2021; Valmeekam et al., 2024; Lowe, 2024). Tasks traditionally solved using System 1 capabilities, such as reward modeling (Mahan et al., 2024), machine translation (Zhao et al., 2024), retrieval-augmented generation (RAG) (Li et al., 2024), and multimodal QA (Islam et al., 2024), have already benefited from the deeper reasoning capabilities enabled by System-2 thinking. The o1 model's system card demonstrates notable improvements in model safety. Inspired by this, we have recently explored the concept of System-2 Alignment, which involves guiding models to thoroughly evaluate inputs, consider potential risks, and correct biases in their reasoning (Wang & Sang, 2024). We introduced three methods to realize System-2 alignment: prompt engineering, supervised fine-tuning, and reinforcement learning with process supervision. We will apply the Self-Play+RL framework presented in this report to System-2 alignment, aiming to further enhance the model's ability to think deliberately and reduce vulnerabilities in complex scenarios. 4.4 CHALLENGES\nThe released 01-preview and o1-mini currently lack multimodal capabilities and functional call features, which are claimed by OpenAI to be included in its complete version. Beyond multimodal and functional call, another critical feature for improvement in 01-like inference models is the optimization of inference time. This includes enhancing inference efficiency\u2014achieving higher performance per unit of time and enabling adaptive inference time adjustments. Specifically, this involves dynamically adjusting the System 2 reasoning process based on task complexity and achieving a more human-like ability to seamlessly switch between System 1 and System 2 reasoning modes."}, {"title": "4. FORWARDS", "content": "For o1-like inference models to be deployed across broader real-world applications, two major challenges need to be addressed, both involving with the RL environments. The first challenge concerns reward function generalization. This has been already discussed in the community. For example, leveraging the enhanced ability of inference models to understand high-level natural instructions, approaches like Constitutional AI (Bai et al., 2022) might directly define reward functions in natural language. Another strategy focuses on improving coding capability and transforming the other tasks into coding problems for resolution. Another less mentioned challenge concerns environment state update. Unlike classic model-free RL methods, such as Q-learning, where state transitions are not explicitly modeled, o1-like models rely on behavior simulation and forward search, requiring knowledge of the updated state following an action. This shifts the paradigm towards model-based RL. In well-defined domains such as programming, mathematics, and Go, the environment often has deterministic rules. For example, programming uses compiler-defined language specifications, mathematics adheres to axiomatic logic, and Go operates under fixed game rules. These deterministic frameworks allow precise computation of state transition probabilities p(statei+1 | state\u017c, action) following specific actions. However, in many real-world applications, such as Retrieval-Augmented Generation (RAG), device usage (), and embodied agents, obtaining state updates requires interaction with external environments or simulators. This introduces significant computational and time costs. For example, in device use, behaviors like clicking, inputting, or scrolling must be simulated in a way that involves page rendering, state updates, and sometimes complex backend interactions like network requests. Moreover, ol-like models face the limitation of not being able to perform online behavior simulation during inference, which prevents the model from validating or correcting its actions by returning to a previous state. This leads to inability to backtrack and refine decisions. Therefore, one of the key directions is to attempt explicit modeling of the environment by developing a world model for state transition prediction. The world model takes as input the current and past states and actions, and produces the next state as output. This allows the model to interact with its internal world model, rather than directly with the real environment or a simulator. We recognize that one of the ongoing challenges in RL when building such world models is ensuring their accuracy. As a result, world models have typically been applied to environments where the dynamics are relatively simple and well-understood. The good news is, recent rapid advancements in generative games (Sang, 2024) offer promising progress that could facilitate more accurate and practical environment modeling for inference models in real-world applications. Prospects. The o1 model is clearly influenced by AlphaGo: AlphaGo utilized imitation learning to initialize the policy network, reinforcement learning to fine-tune the policy and learn the value network, and MCTS as an online search strategy, which parallels LLM's pre-training, post-training, and inference. AlphaGoZero took a more advanced approach by not relying on historical data, which exactly mirrors current trends in LLM development increasingly emphasizing the post-training stage. If we follow the evolution of the Alpha series, we can anticipate similar developments in ol-like inference models. Initially, the Alpha series developed towards generalization: AlphaZero was applied to Go, Chess, and Shogi, while MuZero achieved human-level performance across 57 Atari games. The other goal besides generalization, however, is to apply these models to more complex, real-world tasks. This progression is evident in AlphaFold's leap to AlphaCode and AlphaGeometry, as well as the extension of AI to physical environments, such as the 3D virtual agents in SIMA or the embodied intelligence in RT-X."}]}