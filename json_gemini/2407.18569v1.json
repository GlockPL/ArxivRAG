{"title": "PP-TIL: Personalized Planning for Autonomous Driving with Instance-based Transfer Imitation Learning", "authors": ["Fangze Lin", "Ying He", "Fei Yu"], "abstract": "Personalized motion planning holds significant importance within urban automated driving, catering to the unique requirements of individual users. Nevertheless, prior endeavors have frequently encountered difficulties in simultaneously addressing two crucial aspects: personalized planning within intricate urban settings and enhancing planning performance through data utilization. The challenge arises from the expensive and limited nature of user data, coupled with the scene state space tending towards infinity. These factors contribute to overfitting and poor generalization problems during model training. Henceforth, we propose an instance-based transfer imitation learning approach. This method facilitates knowledge transfer from extensive expert domain data to the user domain, presenting a fundamental resolution to these issues. We initially train a pre-trained model using large-scale expert data. Subsequently, during the fine-tuning phase, we feed the batch data, which comprises expert and user data. Employing the inverse reinforcement learning technique, we extract the style feature distribution from user demonstrations, constructing the regularization term for the approximation of user style. In our experiments, we conducted extensive evaluations of the proposed method. Compared to the baseline methods, our approach mitigates the overfitting issue caused by sparse user data. Furthermore, we discovered that integrating the driving model with a differentiable nonlinear optimizer as a safety protection layer for end-to-end personalized fine-tuning results in superior planning performance. The code will be available at https://github.com/LinFunster/PP-TIL.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous driving has been a focal point of research and development over the past decade. The motion planning module stands out as a critical component within the urban autonomous driving system [1], [2]. This module typically considers factors such as safety, travel efficiency, and comfort. In practical terms, individual perceptions of comfort can vary significantly among users. For instance, some may prefer high-acceleration sports driving, while others may lean towards a more relaxed style. Offering personalized motion planning holds immense significance in enhancing user acceptance of autonomous driving. However, manually adjusting the parameters of the planning model to achieve a specified style can be challenging due to the potential for antagonistic effects of a large number of parameters. Fortunately, the data-driven approaches can effectively address this issue [3], [4].\nFor data-driven personalization planning, Fig. 1 presents a comparative analysis between our proposed approach and prior methodologies. Existing approaches can be broadly categorized into two main groups: inverse reinforcement learning-based methods and imitation learning-based methods. For methods grounded in inverse reinforcement learning, certain studies in general-purpose planning [5], [6] explore style learning within complex urban environments. Conversely, other researchers utilize inverse reinforcement learning to acquire the linear-structure cost function, thereby enabling user-style autonomous driving [3], [7], [8]. These methodologies learn user-style cost functions from demonstrations to achieve personalized planning. Besides, the methods based on imitation learning [9]\u2013[14] often aim to obtrain a human-like driving strategy by learning from large-scale human expert demonstrations. The above methods have demonstrated promising results in their original tasks. However, when it comes to personalized planning, user demonstrations are expensive and limited in quantity. In the experiment (refer to Table. V), we find that fine-tuning the pre-trained model using the above methods on sparse user demonstrations resulted in degraded planning performance and poor generalization of driving styles.\nTo address the above challenges, we propose an effective transfer imitation learning method based on the four design principles proposed in gapBoost [15]. Here's how we integrate these principles: Rule 1: We construct a weighted experience loss based on the imitation learning method. Rule 2: We assign equal weights to the samples in the same domain to avoid highly expensive computations. Rule 3: We adjust the sampling probabilities of source and target datasets by modifying the ratio of expert samples to user samples in the input batch. Since the amount of expert data vastly exceeds the amount of user data, there is a higher expectation value for the weights assigned to user samples. Rule 4: We utilize the inverse reinforcement learning method to construct the regularization term, which measures the style gap by calculating the style feature expectation error of the trajectory. We implicitly learn the weight of each sample by adjusting the model parameters.\nWe conduct numerous experiments on the proposed method. We apply the proposed method to evaluate the performance of various fine-tuning structures. Our method outperforms all baseline methods in both style error and planning performance. It is worth noting that the baseline approaches achieved worse results than the pre-trained model, while our approach achieved superior performance. Additionally, through the sufficient update steps, we find that the fine-tuning architecture integrating a neural network with a differentiable nonlinear optimizer yields the best performance.\nIn summary, this paper makes the following three contributions:\n\u2022 In this paper, we propose a novel approach called the instance-based transfer imitation learning method for achieving personalized planning within intricate urban settings.\n\u2022 The proposed method effectively transfers knowledge from expert data, addressing the challenges of overfitting and poor generalization stemming from the sparse user demonstrations.\n\u2022 The effectiveness of the proposed method is demonstrated through a multitude of experiments. Experimental results indicate that our method achieves superior user style approximation and planning performance enhancement."}, {"title": "II. RELATED WORK", "content": "In the urban structured scene, some traditional trajectory planning methods based on optimization have been widely used in industry and academia [16]\u2013[19]. However, these hand-designed methods tend to have weak decision-making capabilities, cannot handle long-tail scenarios, and do not improve with increasing data. In recent years, more and more works have proved the potential of planning methods based on machine learning in handling a large number of different autonomous driving urban scenarios [9]\u2013[14], [20]. But machine learning-based planning often has difficulty interpreting its output, and when it encounters out-of-distribution situations it is likely to lead to dangerous actions.\nSome hybrid frameworks combine the machine learning-based approach and optimization-based approach [21], [22]. Although it nicely combines the strengths of the machine learning-based approach and the optimization-based approach, it requires large amounts of human data to train.\nIn the case of sparse user demonstrations, it is difficult for the previous method to obtain a sufficiently safe policy while learning styles, and there are problems of overfitting and poor generalization. To address these issues, this paper proposed personalized planning via transfer imitation learning, which is based on pre-training and fine-tuning framework. It can learn style from user data and improve the performance of planning."}, {"title": "B. Personalized Planning for Autonomous Driving", "content": "Personalized planning stands as a crucial element in enhancing the user experience. Some optimization-based approaches [23]\u2013[25] tend to focus solely on specific scenarios or tasks such as lane changes and obstacle avoidance. Moreover, certain studies on user style learning [3], [7], [8] frequently employ the Maximum Entropy Inverse Reinforcement Learning (MaxEnt-IRL) [26] method to learn a cost function for modeling the user's driving style. However, applying this method to complex urban scenes poses significant challenges. Furthermore, while the general-purpose planning approach considers style learning within complex urban scenarios [5], [6], it cannot utilize data for continuous enhancement of planning performance.\nIt often proves challenging for these endeavors to concurrently accomplish vital aspects of both planning tasks. These include personalized planning for complex urban scenarios and the ongoing enhancement of planning performance through data utilization. This work addresses the issues of sparse user data and data-driven planning by transferring knowledge from expert data."}, {"title": "C. Instance-Based Transfer Learning", "content": "In the context of personalized planning tasks in autonomous driving scenarios, the data from the expert domain and user domain exhibit strong similarities. In this case, the instance-based transfer learning approach emerges as a promising solution. Several case-based transfer learning methods employ the calculation of sample weights to facilitate effective sample transfer [15], [27], [28], [28]\u2013[30]. Furthermore, some endeavors concentrate on screening valid samples from either the target or source domain [31], [32].\nInspired by these methods, we devised an effective transfer imitation learning method. Through data classification remove invalid entries. Additionally, our method implicitly adjusts sample weights via sample sampling probability and model parameter learning, thus circumventing the computational overhead associated with explicit sample weight calculation."}, {"title": "III. PERSONALIZED PLANNING VIA TRANSFER IMITATION LEARNING", "content": "We assume the current time step is t. The agent state at the current time is represented as $s_t = [x_t, y_t, \\theta_t, v_t]$, where $(x_t, y_t)$ is two-dimensional space coordinate, $\\theta_t$ is heading angle, and $v_t$ is velocity. The trajectory $\\xi_{0:t}$ is represented as a set of discrete points {$s_0, ..., s_t$}. The model predicts the future trajectory of agent i for the next T time steps, and it's represented as $\\xi_{t:t+T}$. The output of the neural network is the control action sequence and denoted as $I_{t:t+r}(m = 1,..., N_m)$, where $N_m$ is the number of multi-modal combinations. Specifically, the selected optimal control action sequence is denoted as $\\pi_{t:t}$. The cost function weights are denoted as $w_i(i = 1, ..., N_f)$."}, {"title": "B. Pre-Training with Large-Scale Expert Data", "content": "To obtain a planning trajectory that satisfies the kinematic constraints, the neural network outputs the control action $u_t = \\{a_t, d_t\\}$ (where $a_t$ is acceleration and $d_t$ is steering angle) and the trajectory is calculated through the kinematic model:\n$\\pi_{t:t+T} = [u_t ... u_T]$\t(1)\n$\\xi_{t:t+T} = \\psi(s_t, \\pi_{t:t+T})$\t(2)\nwhere the $\\psi$ represents the kinematic model. In this work, we adopt the differentiable kinematic bicycle model in [22].\nIn the pre-training stage of imitation learning, we adopt the same structure as DIPP [22]. The pre-training loss is summarized as follows:\n$L_{IL} = \\lambda_1L_{prediction} + \\lambda_2L_{score} + \\lambda_3L_{imitation}$\t(3)\nwhere $\\lambda_i(i = 1,2,3)$ is the weight that scales the different loss terms.\nFor imitation loss, the control action sequence is used to calculate the trajectory of the autonomous vehicle through the kinematic model, and the distance from the ground truth is measured to form the imitation loss:\n$L_{imitation} = smoothL1(\\psi(s_t, \\pi_{t:t+T}) \u2013 \\xi_{t:t+T})$\t(4)\nwhere $\\xi_{t:t+T}$ represents the ground truth trajectories in the dataset."}, {"title": "C. Fine-Tuning with Instance-Based Transfer Imitation Learning", "content": "In the fine-tuning stage, we use differentiable nonlinear optimization [33] to refine the control action sequence of the output of the pre-training model. To be specific, the optimization process is formulated as follows:\n$\\pi^* = arg \\min_{\\pi} \\sum_{i=1}^{N_f} ||W_iC_i(\\pi)||^2$\t(5)\nwhere $c_i(\\pi)$ is the cost function of the control action sequence $\\pi$, $w_i(i = 1, ..., N_f)$ is the scaling weight and $N_f$ is the number of cost functions $c_i(\\pi)$. The trajectory features are designed based on autonomous driving planning tasks, which include travel efficiency, ride comfort, lane departure, traffic rules, and most importantly safety [22].\nTo solve the ambiguity of multiple solutions and the randomness of expert behavior, we adopt Maximum Entropy Inverse Reinforcement Learning (MaxEnt-IRL) to learn the weights [26]:\n$P(\\xi|w) = \\frac{exp(-c(\\xi_w))}{Z(w)}$\t(6)\nwhere Z(w), called partition function, equals to $\\sum_{\\xi_w} exp(-c(\\xi_w))$. According to this function, plans with equivalent costs have equal probability.\nThe Max-Ent IRL method optimizes the weight w by maximizing the probability of the human demonstration trajectories $\\xi \\in D$. The gradient of this optimization problem can be computed as follows [26]:\n$\\bigtriangledown L_{IRL} = E_{P(\\xi|w)}[f_i] - f_i$\t(7)\nHowever, calculating the expectation in Eq. 7 through sampling is challenging. One possible approximation of the expected feature values is to compute the feature values of the most likely trajectory [3]. To extend to neural network parameter updates, instead of using feature errors as gradients, we approximate the solution by performing gradient descent on the following objective:\n$L_{IRL} \\approx \\frac{1}{N_f} \\sum_{i=1}^{N_f} ||f_{\\xi^w} - f_\\xi||$\t(8)\nwhere $f$ is the feature of the trajectory and $\\beta$ is the corresponding scaling factor. We refer to [22] for the design of seven metrics to describe trajectory features. The name and scaling factor of each feature are shown in Table I.\nBy adhering to the design principles delineated in gapBoost [15], we propose an instance-based transfer imitation learning algorithm. By amalgamating Eq. 8 and Eq. 3, we formulate the optimization objectives of instance-based transfer imitation learning as follows:\n$L_{TIL} = L_{IL} + \\alpha L_{IRL}$\t(9)\nThe parameter $\\alpha > 0$ denotes the scaling factor of the $L_{IRL}$ term, utilized for adjusting the weight of the regularization component within the optimization objective. $L_{IRL}$ serves as a regularization term aimed at reducing the performance disparity across domains, which accomplishes style approximation by aligning the feature distribution expectation of the input sample with that of the user sample. In practical implementation, we utilize this regularization term along with an imitation learning term to update parameters across all models.\nAs shown in Algo. 1, We delineate the detailed process of instance-based transfer imitation learning. Initially, we categorize the dataset based on the type of planning trajectory. Refer to Table. II for the specific classification criteria. In our study, we categorize it into three classes: stationary, straight, and turn. As the stationary category lacks user style information, we exclude it during the fine-tuning phase. We sample from both the expert and user data within the remaining categories to construct the mixed input batch at each iteration. Simultaneously, we utilize the corresponding user data category to formulate the IRL regularization term. Finally, we feed the batch data into the model for gradient computation and update the parameters using $L_{TIL}$."}, {"title": "IV. EXPERIMENTS", "content": "We train and validate the approaches on the Waymo Open Motion Dataset (WOMD) [34], a large-scale real-world driving dataset focus on urban driving scenarios. We set a 7-second time window to segment the 20-second driving scene into frames, which includes a 2-second horizon for historical observation and a 5-second horizon for future prediction and planning. The window slides in 10 timesteps from the beginning of the scene and produces segments of 14 frames. In the experiment, we obtain 916808 frames from the scene, of which 80% is used for training, while the rest is used for open-loop testing. Furthermore, the trajectory feature vector is computed based on the feature terms and scaling coefficients outlined in Table. I. Subsequently, 10,000 samples are extracted from the test set for clustering using K-means, resulting in three clustering centers. Finally, we select the 64 sample frames nearest to each center point to obtain three distinct user datasets representing different styles.\nIn addition, based on the feature items and scaling factors in Table I, we compute trajectory feature vectors. Subsequently, we extract 10,000 samples from the test set and apply the k-means clustering algorithm to obtain three clustering centers. Finally, we select the 64 sample frames closest to each cluster center to form three distinct datasets, which we use as user data for training. The expectation of user style features is presented in Table. III.\nIn the experiment, this paper designs collision rate, red light, off route, planning error, and prediction error referring to DIPP [22]. Additionally, we define the \"style error\" metric, calculated as shown in Eq. 8. The \"style error\" is evaluated by computing the mean absolute error between the trajectory features produced by the model and those from the user demonstration, thus assessing the style similarity.\nThe pre-trained model undergoes five epochs of training on the expert dataset. Throughout training, the step size of the nonlinear optimizer is fixed at 0.4, and the number of iterations is set to 2. All experiments in this paper adhere to the same initial parameter settings as follows: a batch size of 64, a neural network learning rate of 1e-5, and a linear weight learning rate for the cost function of 1e-3."}, {"title": "B. Open-Loop Test", "content": "As depicted in Table V, we conduct comprehensive experiments involving various fine-tuning architectures and fine-tuning methods. \u201cLIL\u201d denotes the imitation learning method outlined in Eq. 3, while \"LIRL\" represents the inverse reinforcement learning method illustrated in Eq. 8. \"LTIL\" signifies the utilization of the transfer imitation learning method detailed in Eq. 9. Due to the low effectiveness of the methods without the pre-training process, we establish a stronger baseline to demonstrate the superiority of our approach. Specifically, all experiments listed in the table are conducted based on the pre-trained model, and we compare the performance of different methods during the fine-tuning stage. We update each model 100 times and repeat each fine-tuning method thrice, presenting the average results in the table. \"Human\" denotes manually adjusting the weights of the cost function, as specified in Table IV. In this experiment, our method involves mixing expert data and user data in a 50% ratio, with the $\\alpha$ of the $L_{TIL}$ set to 100. We perform the in-domain evaluation on the user dataset and the out-of-domain evaluation on the test dataset.\nThe experimental results are presented in Table V demonstrate that the performance of baseline methods is worse after fine-tuning than before because of overfitting, while the proposed approach achieves fine-tuning performance improvements. In addition, our method effectively considers both user behavior imitation within the domain and style generalization outside the domain. Notably, our method achieves competitive planning performance, along with lower style error.\nTo further evaluate whether our algorithm is prone to overfitting, we present in Fig. 4 the trends of style feature matching error and collision rate changes with the update steps across three learning structures. The curve values in the figure represent the average results obtained from three out-of-domain experiments. Due to overfitting to sparse user data, \"LfD\" and \"DIPP\" exhibit higher collision rates and poorer style generalization capability. Upon analyzing the curve trends, the final convergence outcome of the \"NN&CF\" framework surpasses that of the \u201cNN\u201d framework in two key metrics: style error and collision rate. \u201c(NN&CF) / CF\u201d initially converges faster. However, since the cost function solely learns linear weights, its final convergence performance is limited. \u201c(NN&CF) / NN&CF\u201d proves less effective when the number of update steps is small. In such cases, the linear weight of the cost function hasn't been adequately learned, and it hasn't adapted well to the neural network.\nHowever, with sufficient update steps, \u201c(NN&CF) / NN&CF\u201d achieves the best performance and even outperforms manual adjustments. This is attributed to the strong fitting ability of the neural network, which mitigates the problem of the cost function's insufficient fitting capacity."}, {"title": "C. Ablation Study", "content": "As depicted in Table VI, various losses are ablated. The input batch data for all experiments in the table consists of a 50% mixture of expert data and user data. We conduct 100 updates for each setting and present the averages of three repeated experiments. To better validate the efficacy of the loss, we only modify the loss to explore the effectiveness of the proposed method. The results indicate that the method utilizing only $L_{IL}$ for parameter updates exhibits the poorest performance in the \u201cstyle error\u201d indicator and achieves the lowest approximation accuracy for user style. Conversely, the method employing only $L_{IRL}$ performs inadequately on other vital planning performance metrics. $L_{TIL}$ effectively balances user style approximation with the performance enhancements of the planning and forecasting modules. Additionally, we conduct parametric sensitivity experiments on the weight proportion between the $L_{IL}$ and $L_{IRL}$. The results indicate that $\\alpha = 100$ serves as a suitable trade-off between planned performance improvements and user style approximation, which tends to result in lower style error and collision rate.\nAs presented in Table VII, we examine different ratios of mixing expert data to explore their influence on fine-tuning outcomes. For each setting, we conduct 100 updates and report the average of three repeated experiments. Throughout the experiment, we utilize $L_{TIL}$ to update the parameters and set the $\\alpha$ parameter to 100. The results indicate that when the mixing ratio of expert data is 0%, the model tends to overfit to user data, resulting in poor generalization performance since only user data is utilized. Conversely, when the mixing ratio of expert data is 100%, the planning error within the user domain increases significantly because only expert data is input. Across experiments, it is evident that the trade-off between planning performance enhancement and user style approximation is better achieved with a mixture ratio of 75%, often resulting in lower style error and collision rate."}, {"title": "D. Qualitative Results", "content": "Visualization of the final results. We utilized the three models obtained from the Fig. 4 experiment to conduct further analysis, presenting the qualitative findings in Fig. 3. The figure showcases visualizations of the three models. The trajectory duration is fixed at 5 seconds. The purple curve denotes the predicted trajectory of other cars, with the model considering only the ten nearest other cars for trajectory prediction."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose an instance-based transfer imitation learning approach aimed at addressing the challenge of scarcity in user domain data. We employ a pre-trained fine-tuning framework to transfer expertise from the expert domain to alleviate the data scarcity issue in the user domain. Our experimental results demonstrate that our method achieves competitive planning performance while effectively capturing the driving style of the user. At the same time, it is vital to acknowledge the limitations of this work. Firstly, we do not conduct closed-loop real-world experiments. Besides, we utilize the error of the feature expectation of the trajectory to measure user style. To better represent user driving styles or cater to user needs, it is essential to develop more appropriate methods for measuring user styles."}]}