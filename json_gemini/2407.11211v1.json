{"title": "Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer from Text to Image via CLIP Inversion", "authors": ["Philipp Allgeuer", "Kyra Ahrens", "Stefan Wermter"], "abstract": "We introduce NOVIC, an innovative uNconstrained Open Vocabulary Image Classifier that uses an autoregressive transformer to generatively output classification labels as language. Leveraging the extensive knowledge of CLIP models, NOVIC harnesses the embedding space to enable zero-shot transfer from pure text to images. Traditional CLIP models, despite their ability for open vocabulary classification, require an exhaustive prompt of potential class labels, restricting their application to images of known content or context. To address this, we propose an \u201cobject decoder\" model that is trained on a large-scale 92M-target dataset of templated object noun sets and LLM-generated captions to always output the object noun in question. This effectively inverts the CLIP text encoder and allows textual object labels to be generated directly from image-derived embedding vectors, without requiring any a priori knowledge of the potential content of an image. The trained decoders are tested on a mix of manually and web-curated datasets, as well as standard image classification benchmarks, and achieve fine-grained prompt-free prediction scores of up to 87.5%, a strong result considering the model must work for any conceivable image and without any contextual clues.", "sections": [{"title": "1. Introduction", "content": "Imagine a robot navigating through an environment, encountering a small cup, and an operator asking, \u201cCan you tell me what that is?\u201d Ideally, the robot would recognize the object with the objectives to: a) answer cup directly, and b) provide an answer immediately in real-time, even if there are large numbers of objects to consider at high frame rates. Contrary to the first objective, Vision-Language Models (VLM) like CLIP [48] require the robot to be prompted with a comprehensive textual list of object class candidates to perform zero-shot classification. On the other hand, equipping the robot with a high-performing multimodal Large Language Model (LLM) is computationally expensive, requires spe-"}, {"title": "2. Related Work", "content": "Learning a joint embedding space for multiple modalities such as images and text via contrastive pretraining produces powerful representations without the need for expensive annotation of datasets [15, 34]. VLMs such as CLIP [48] and ALIGN [25] paved the way for learning a broad range of visual concepts that can be transferred to downstream discriminative tasks, such as classification, via prompting [17]. Given an image, a set of candidate categories, and prompt templates like \u201ca photo of a [...]\u201d, CLIP can zero-shot classify the image by maximizing the similarity score between its encoded representation and all encoded candidate prompts.\nNOVIC, by contrast, frames the image classification problem as an unconstrained generative one, eliminating the need for category candidates and inference-time prompting. Various approaches build upon and extend the idea behind CLIP towards better pretraining [37, 54, 70] or fine-tuning [58] techniques, or add additional modalities such as audio [20, 59], video [1, 23, 41, 65, 68], or point clouds [67, 71]. Other approaches use VLMs like CLIP as feature extractors for subsequent fine-tuning on more challenging downstream tasks such as visual question-answering [10, 51, 53], captioning [2, 43], motion planning [52, 55], and navigation [28].\nOpen vocabulary learning is a paradigm that emerged in the context of fine-tuning pretrained VLMs for vision tasks. It aims to overcome the closed-set assumption of standard classification models by allowing them to handle novel object categories outside of their training domain. Models such as VILD [19], RegionCLIP [73], and various successor models [7, 9, 27, 46, 61, 62] enable VLMs to solve tasks such as open vocabulary object detection [69], where, given an arbitrary set of candidate object labels, the model has to detect the corresponding objects in the image. Other works extend the open vocabulary paradigm to closely related visual tasks such as semantic segmentation [21, 29, 32, 38, 40, 66] and image tagging [24, 35, 72]. Despite their merit, all the approaches mentioned above rely on candidate labels for inference, a strong assumption that we abandon in this work.\nA parallel line of research aims to leverage the powerful representations provided by VLMs for generative tasks such as image captioning. As VLMs like CLIP cannot inherently produce sequences, approaches in this research area rely on additional decoders. Several methods have been proposed that either establish a mapping between a VLM and a pretrained text decoder [13, 43, 56], or train a decoder from scratch [2, 8, 26, 51] using curated captioning datasets such as MS COCO [6], CC3M [50], and Visual Genome [30].\nA recent line of research on zero-shot image captioning or retrieval with VLMs proposes text-only training to eliminate the need for curating expensive image datasets [39]. As text and images are encoded into disjoint regions of the VLM embedding space, additional mechanisms to bridge the modality gap have been proposed, like injecting noise into the text embeddings [16, 18, 45] or projecting the image embeddings [36]. These methods face two significant disadvantages-first, captions often fail to capture key image aspects or identify core objects, and second, they are biased towards the limited set of object classes contained in their training dataset. NOVIC adopts text-only training, but focuses on predicting truly open vocabulary object labels."}, {"title": "3. Methodology", "content": "An overview of the proposed training and inference schemes for NOVIC is shown in Fig. 2. A text-only dataset mapping caption sentences to target object nouns is syn-"}, {"title": "3.1. Object Decoder Model", "content": "The architecture of the object decoder model is shown in Fig. 3. The model is built around an autoregressive decoder-only language transformer [57] that has been configured to use learned token embeddings, weight tying [47], learned positional encodings, pre-layer normalization [64], GELU activation functions [22], beam search, custom weight initialization, a contractive inner feed-forward dimension, and no biases in the linear and layer normalization layers, including the final token logits linear layer. Dropout is applied after adding the positional encoding and within the transformer layers themselves. The input embedding vector is supplied to the transformer in the form of $P$ prefix token vectors, i.e. $P$ dynamically calculated vectors of dimension $H$ (hidden dimension). The prefix tokens are computed from the input embedding using a learned linear projection that has $PH$ outputs and no biases, as the immediately following positional encoding makes biases redundant. The tokenized target object nouns do not require a start token, as the first output token is predicted at the sequence location of the last prefix token. Appropriate sequence masking is also used to ensure that only sequence locations up to the first output <eos> contribute to the loss. The internal attention mask of the transformer is causal, with the exception that the $P$ prefix tokens can attend to each other without restriction, allowing them to share information freely."}, {"title": "3.2. Training Dataset for Text-Only Supervision", "content": "The object decoder needs to be trained on a dataset of associative caption-object text pairs (cf. Fig. 2). One could attempt to generate such a dataset from a large-scale image-text web dataset ranging into the billions of samples, but it is not a simple task to accurately and sufficiently efficiently construct the required ground truth object noun annotations, especially in such a way that a fair coverage of the English language is achieved. We instead invert the chosen CLIP text encoder by synthetically generating captions that explicitly cover the entire English language of object nouns, and use this to systematically sample in the CLIP embedding space essentially all possible object concepts, and learn to map them to their corresponding textual representations."}, {"title": "Prompt templates.", "content": "A direct and extremely cheap way of generating caption-object text pairs is by using exactly the same prompt templates as is usually used for zero-shot CLIP classification. This maps captions like \u201ca blurry photo"}, {"title": "Multiset prediction.", "content": "Image embeddings can capture significantly more complicated dynamics than just the main entity present. If there is a photo of a lion with a zookeeper, for instance, the image embedding will likely contain some features corresponding to lion, person, fence, and potentially more abstract concepts like zoo as well. To help also capture these dynamics in the text embeddings seen during training, multi-target samples are introduced. The complete object noun dictionary, including explicit repetition of alternate spellings based on their individual log-scale frequencies of mean 1.5, is randomly shuffled $m$ times into separate lists. These lists are combined in parallel to form $m$-tuples of object nouns that are then used to generate prompt-templated captions exactly as before. The $m$ object nouns per caption are linked by 'and' (cf. Fig. 2), and all $m$ nouns use a target weight of 1. This produces a dataset of 13.4M further samples for each value of $m$, referred to as the $M_m$ multiset in each case (e.g. $M_3$ for $m = 3$). In total, a combined dataset of $M_1$ and the two multisets $M_2$ and $M_3$ contains 40.2M samples with 80.4M targets. We qualitatively observed that multiset training specifically helped the open vocabulary"}, {"title": "Caption data.", "content": "In order to supply the object decoder with even more diverse synthetic training data to generalize from, captions were generated for each noun in the object noun dictionary using an LLM. This approach leverages the implicit contextual knowledge of LLMs to place object nouns in relevant contexts-incorporating appropriate adjectives, actions, scenes, and secondary objects-in a style resembling real image captions. The minimum number of unique generated captions for each noun variant was set proportionally to the corresponding log-scale frequency (up to 100) in order to match the relative word frequencies found in the English language. OpenAI GPT-3.5 Turbo was selected as an ideal balance of cost and sufficient performance, and was used to generate a total of 1.8M captions. Ensuring frequency-appropriate use of alternate spellings, this resulted in a total of 2.9M caption samples (2.0M unique) that were then oversampled to around 30% the size of any multiset they were merged with. For the combined dataset $M_{1-3}$ this resulted in 51.7M samples with 91.9M targets."}, {"title": "Noise augmentation.", "content": "In order to allow more robust and generalizable features of the CLIP embedding space to be learned, data augmentation in the form of noise is used during training. Due to the large modality gap between text and image embeddings [39], noise is in fact a crucial component of the training scheme. Noise allows the object decoder to learn to be invariant to even very large changes in the non-object-related parts of the text embedding vectors, which helps the generalization to image embeddings, as these are very sensitive to secondary objects, backgrounds, and even subtle visual variations. Gu et al. [18] used elementwise Gaussian noise with vector renormalization to train a captioning model on text embeddings only, but did not entirely accurately explain its effect. The CLIP embedding space is deceptively and unimaginably vast, so the random noise statistically cannot be significantly filling the gaps between samples, or causing a meaningful overlap with image embedding vectors. The main benefit of noise is that the model can learn to focus on which parts (i.e. subspaces) of the embedding vector really encode which individual object concepts, instead of focusing on which region of the embedding space a vector is in as a whole. We observe that the amount of noise required for best training results is actually quite large, not small or minor [18], and its effect is to completely scatter the cone [39] of text embeddings to a thin hyperannulus on the order of 65\u201375\u00b0 away, with no single sample remaining significantly closer than that to any original text embedding. By comparison, the very large modality gap between the image and text embedding cones can also be measured, and is around 70\u201385\u00b0 for matching image-text pairs (depending on the CLIP model), and only 8\u201312\u00b0 more than this for completely uncorrelated image-text pairs. As indicated previously though, statistically there is negligible region overlap between the scattered hyperannulus and the image embeddings cone due to the vast nature of the embedding space. The large amount of noise required can instead be seen as a trade-off between the disruptive 'blurring' effect of noise, and the benefit of the object decoder having seen many embedding vectors during training that are as far removed from text embeddings as image embeddings are. In this work, we use an 85% to 15% mix of elementwise Gaussian noise and uniform angle noise. The latter was developed to beneficially cover a greater range of noise dynamics and levels, by rotating input text embedding vectors uniformly in random directions by angles in the uniform 45\u201375\u00b0 range."}, {"title": "3.3. Evaluation Datasets for Zero-Shot Recognition", "content": "One way to evaluate how much NOVIC has learned is by examining its zero-shot performance on standard image classification datasets like ImageNet-1K and Food-101. Context is everything for image classification datasets however. Knowing which categories are available in a dataset serves as a strong prior for supervised models. For instance, if a dataset contains only one bird category, such as kinglet, a supervised model only needs to determine whether an image is a bird to confidently classify it as a kinglet. For prompt-free open vocabulary models like NOVIC however, the task is more challenging. When presented with an image that happens to come from Food-101, it is not clear that the model should suddenly focus on the specific dish shown instead of potentially more prominent or general objects in the image. Consequently, to meaningfully evaluate NOVIC on image classification datasets, we restrict the generated beam search text tokens in every forward pass of the object decoder to those consistent with the defined category names, and score whether the correct final category is identified.\nAs NOVIC is trained without providing explicit categories, we wish to evaluate it also on datasets that do not specify categories. No suitable such dataset is known however, as such a dataset would need to have annotations for every possible correct object noun in the English language for every visible entity in every part of each image. To address this, three open vocabulary image datasets were curated as part of this work, and individually annotated by both human and multimodal LLM annotators. The annotations specify whether each classification is correct, close, or incorrect, and for the human annotations, whether it relates to a primary or secondary element of the image. The three new datasets are (i) World: 272 images of which the grand majority are originally sourced (have never been on the internet) from 10 countries by 8 people, with an active focus on covering as wide and varied concepts as possible, including unusual, deceptive and/or indirect representations of objects, (ii) Wiki: 1000 Wikipedia lead images sampled from a scraped pool of 18K, (iii) Val3K: 3 000 images from the ImageNet-1K validation set, sampled uniformly across the classes (LLM annotations only). The suffixes -H and -L are used to clarify whether human or LLM annotations are in use, e.g. Wiki-H. A total of 16.5K human and 112K LLM (OpenAI GPT-40) annotations were made across the three datasets. This was at the limit of feasibility in terms of number of samples, person-hours and cost."}, {"title": "4. Experiments", "content": "For experiments, the SigLIP ViT B/16 model [70] is used as the base CLIP model, due to its favorable balance of speed and performance, especially when compared to the original CLIP models [48]. Training is performed for 18 epochs with the AdamW optimizer, cosine learning rate decay, weight decay on all multidimensional weights, and a gradient-accumulated batch size of $B = 8192$. The object decoder model is quite small at only 12.2M parameters, and takes less than 3 days to train on a single RTX A6000. For FT2, this drops to 11.4M parameters and only 1.5 days. Overall, this is extremely efficient, considering that up to a billion samples are being seen by a single GPU. As a scale comparison, using our method on the very same GPU to train an object decoder on all 1.28M ImageNet-1K samples for 18 epochs takes a mere 27 minutes, and results in an impressive top-1 accuracy of 83.4%. The object decoder is also very resource efficient at inference time. Requiring at most 5 GB GPU memory, the total pipeline\u2014including CLIP model, autoregression, and beam search-takes on average 26 ms per image for single images, and 7 ms per image when using batching ($B = 256$), making it very real-time capable.\nOpen vocabulary scoring. Model predictions on the three introduced open vocabulary image datasets are scored by assigning {1, 0.8, 0.5, 0.4, 0} points for {correct primary, correct secondary, close primary, close secondary, incorrect} predictions respectively, and expressing the sum as a percent of the maximum possible score. For verification purposes, the prediction scores achieved by many models were statistically compared between human and LLM annotations on the same dataset and found to be suitably correlated, thus"}, {"title": "Comparison with related work.", "content": "As discussed in Sec. 2, no directly equivalent state-of-the-art model is known that performs real-time prompt-free open vocabulary image classification. The most suitable models for comparison are image tagging models like Tag2Text [24] and the Recognize Anything Model (RAM) [72], as they are designed to be able to provide text labels for images of arbitrary content. We evaluate the largest model variants (14M) on Wiki-H by taking the highest scoring prediction for each image, and compare this to our results in Tab. 3. While the tagging models obtain a high numeric prediction score, a closer look reveals this is due to a severe lack of diversity and specificity (cf. Fig. 4). The produced tags are tendentially coarse and repetitive, with around a third of all images receiving one of only 20 generic top labels, often missing the main point of images. Even when provided with the full object noun dictionary, the total diversity of labels stays below 400, with over 80% of predictions still being made from the small core RAM vocabulary. This is in contrast to our model, which has a much flatter prediction distribution, diversities up to 850 (cf. Tab. 3), and a high prediction score despite generating fine-grained classifications. The tags produced by Tag2Text and RAM also tend to be uninformative-when applied to the images of the Imagewoof [12] validation set for example, both models simply output dog 88% of the time, and never a single dog breed. Our FT0 model, on the other hand, predicts over 90 different dog breeds, with much of the weight covering variations of the actual ground truth classes. When RAM is evaluated on ImageNet-1K by only providing it the 1000 class names to choose from, it only scores 38.3% (25.4% for non-core vocabulary), which is far less than the 75.54% of the ViT-L CLIP model it was trained on, and far less than the 60-70% nominally scored by NOVIC."}, {"title": "4.2. Ablation Studies", "content": "Tab. 4 shows the effect on decoder performance of varying the dataset configuration, training noise, and underlying CLIP model. FT2 was chosen as the ablation baseline, as many ablation runs were required, and FT2 strikes a balance between overall feasibility of training times, and ensuring that a diverse and expressive range of object nouns are seen during training. Overall, as expected, the human annotations exhibit generally higher scores and are slightly more consistent, due to human checking and more prevalent use of the close category. The decoder ImageNet-1K scores are also observed to be well-correlated with general open prediction performance.\nTraining dataset. The effect of the object noun dictionary frequency threshold (cf. Sec. 3.2) on decoder classification performance is shown in Tab. 4. The prediction scores grow as the training dataset size is increased, until a limit is reached between FT2 and FT0 where the exploding diversity of seen object nouns saturates the amount of order the CLIP model can provide in the embedding space. In Tab. 4, we systematically analyze the contribution of each component of the training set. Starting with M\u2081 and incrementally building the nominal training set (i.e. M1 + M2 + M3 + LLM captions), we observe consistent gains. Immediate losses are seen if the LLM captions are removed, or if the lower-order M1 and M2 multisets are removed. Further tests with variations in training noise configuration also validate that very large amounts of Gaussian noise are required in order to produce models that can generalize across the modality gap, and that instead using uniform angle noise on 15% of the samples produces the best seen configuration."}, {"title": "CLIP model.", "content": "The proposed NOVIC training scheme scales well with the performance of the underlying CLIP model. Tab. 4 demonstrates that progressively stronger CLIP models yield progressively stronger object decoders. Remarkably however, the training times remain almost unchanged, as text encoders only scale slowly with CLIP model strength and the datasets of text embedding vectors are generated offline and only once anyway (cf. Fig. 2). It can be observed from Tab. 4 that the performance of the FTO object decoder that was trained on the strong DFN-5B CLIP model is only marginally below the corresponding FT2 performance. This is much closer than for the nominal CLIP model, and can be attributed to the improved order and separation of concepts that the CLIP model provides in the embedding space. Based on the World-H results, it is actually expected that the DFN-5B FT0 model can provide very fine-grained correct primary classifications for up to 84.2% of pictures it is given. Having trained on the full unfiltered object noun dictionary, the model is frequently able to make correct classifications as detailed as red velvet cake, brazil nut tree, electron microscope, and egretta caerulea."}, {"title": "Language generalization.", "content": "During training, the object decoder develops an implicit understanding of language and its relationship to object concepts, demonstrating qualitative evidence that it can occasionally use this knowledge to generalize beyond the dataset. For instance, the unseen noun New Zealand willow was predicted for a scenic photo of a willow tree that was indeed taken in New Zealand. The object decoder also inherits CLIP's slight tendency to be able to 'read', as evidenced for example by a generic blue plushie being correctly classified by multiple decoders as genus staphylococcus, but not anymore if the small visible text Staphylokokke is removed, falling back to stuffed toy. In another case, a sign reading Aral is classified as ara (a macaw), indicating that the decoder can associate certain features in the embedding space with certain spellings."}, {"title": "5. Conclusion", "content": "In this paper, we address the problem of unconstrained real-time open vocabulary image classification, and highlight the limitations of current state-of-the-art methods in accomplishing this task. To overcome these limitations, we propose an autoregressive object decoder trained on a large-scale text-only dataset. The decoder generates object nouns directly as language on the basis of CLIP embeddings, and crucially, does not require any class candidates to be provided. To create the text-only training dataset, we introduce a comprehensive object noun dictionary and employ multiset prompt-templating in combination with LLM-generated captions and a pivotal noise augmentation strategy to effectively bridge the modality gap from text to images. Our approach demonstrates promising results on both established"}, {"title": "Ethical AI.", "content": "Our open vocabulary image classifier may inherit biases from the CLIP model it is trained on, and be similarly susceptible to adversarial attacks during inference, such as manipulated images misguiding label generation. Although the object decoder is generative, it is trained in a constrained setting, making it unlikely to produce sensitive outputs. Training the object decoder on the proposed text datasets is significantly more cost-effective than training alternative methods such as RAM on large-scale image datasets. However, due to the large amount of noise used, identifying the optimal model configuration often necessitates many training runs, each with an environmental impact."}]}