{"title": "THE SAMPLING-GAUSSIAN FOR STEREO MATCHING", "authors": ["Baiyu Pan", "Jichao Jiao", "Bowen Yao", "Jianxin Pang", "Jun Cheng"], "abstract": "The soft-argmax operation is widely adopted in neural network-based stereo matching methods to enable differentiable regression of disparity. However, network trained with soft-argmax is prone to being multimodal due to absence of explicit constraint to the shape of the probability distribution. Previous methods leverages Laplacian distribution and cross-entropy for training but failed to effectively improve the accuracy and even compromises the efficiency of the network. In this paper, we conduct a detailed analysis of the previous distribution-based methods and propose a novel supervision method for stereo matching, Sampling-Gaussian. We sample from the Gaussian distribution for supervision. Moreover, we interpret the training as minimizing the distance in vector space and propose a combined loss of L1 loss and cosine similarity loss. Additionally, we leveraged bilinear interpolation to upsample the cost volume. Our method can be directly applied to any soft-argmax-based stereo matching method without a reduction in efficiency. We have conducted comprehensive experiments to demonstrate the superior performance of our Sampling-Gaussian. The experimental results prove that we have achieved better accuracy on five baseline methods and two datasets. Our method is easy to implement, and the code is available online.", "sections": [{"title": "1 INTRODUCTION", "content": null}, {"title": "2 RELATED WORKS", "content": null}, {"title": "2.1 THE BASELINE OF STEREO MATCHING", "content": "Stereo matching method is a method that calculates the disparity map of the binocular images with size (H, W). The feature network extracts the features with size (, ). Then the cost volume is constructed with size (dmax, H, W), where dmax is a hyper-parameter and empirical set to 192(Chang & Chen, 2018). A disparity regression network with 3D convolutions is used for refine the cost volume. Its output remains the same size as cost volume. Then a trilinear interpolation operation is used for upsample the output to (dmax, H, W). At last, a soft-argmax operation is applied."}, {"title": "2.2 IMPROVEMENT METHODS", "content": "Based on the baseline, the subsequent proposed improvement methods can be classified into several levels: feature level, module level, baseline level, and distribution level. Firstly, at the feature level, Chang & Chen (2018) proposed the PSMNet which adopts a spatial feature pyramid to extract and fuse multi-resolution features, and stacked-hourglass module is adopted as regression module to improve the refinement. Based on PSMNet, Guo et al. (2019) proposed a group-wise correlation network(GwcNet) which calculates the dot products of the left and right features instead of a concatenation. And at module level, Zhang et al. (2019a) proposed a guided-aggregation module to better refine the cost volume. And Xu et al. (2022) leverages attention mechanism to supervise the cost volume. At the baseline level, researchers proposed new baselines to improve the accuracy of the efficiency. Xu & Zhang (2020) and Pan et al. (2020) proposed to progressively aggregate the cost volume to the full size. Others proposed 2DConv-based methods(Pan et al., 2024; Shamsafar et al., 2021) to reduce the high FLOPs. And Xu et al. (2023) proposed to iterative refine the disparity and significantly improve the accuracy but at the expense of speed."}, {"title": "2.3 DISTRIBUTION-BASED IMPROVEMENT METHOD", "content": "The soft-argmax operation is widely applied in various tasks as it retrieves the index of the highest probability in a differentiable way. Despite its efficiency, researchers continuously explore and propose better methods. From a distribution-view, the soft-argmax is equivalent to retrieves the mean of the probability distribution(Li et al., 2021). Consequently, network trained with soft-argmax lacks explicit supervision for the distribution, resulting in unconstrained probability shape.\nHowever, this disadvantage of soft-argmax receives less attention than other aspect. Because, as the network become deeper and larger, the multimodal problem can be solved partially by the network's generalization ability. The DSNT (Nibali et al., 2018) introduced a differentiable operation to render the heatmap with a 2D Gaussian kernel as a constraint for shape. Some methods attribute inaccurate estimates to the multimodal problem. The PDS (Tulyakov et al., 2018) limit the range of the soft-argmax with Top-k during inference in order to solve the multimodal problem. An unresolved issue with PDS is its lack of robustness, as the range parameter is set in advance. A corresponding solution was proposed in Liu & Liu (2022), using learned weights to suppress unreliable disparity regions. A similar idea was proposed in H\u00e4ger et al. (2021), where they use a Dirac impulse to model the distributions."}, {"title": "3 EXPLORATIONS", "content": "In this section, we first analyze the biased gradient of soft-argmax to establish that distribution-based supervision is necessary for stereo matching. Then, we analyze the two basic settings that have led previous distribution-based methods to their inferior improvements."}, {"title": "3.1 ANALYSIS OF BIASED GRADIENT", "content": "We first analysis the partial differential equation of soft-argmax. The ezi denotes the input of softmax. The partial derivative of ezi is defined as\n$\\begin{aligned}\n\\frac{\\partial L}{\\partial e^{z_i}} &= \\frac{\\partial L}{\\partial d} \\frac{\\partial d}{\\partial e^{z_i}} \\\\\n&= \\frac{\\partial L}{\\partial d}  \\frac{e^{z_i}}{\\sum e^*} (1 - \\frac{e^{z_i}}{\\sum e^*}) + (i - \\frac{e^{z_i}}{\\sum e^*}) \\frac{e^{z_i}}{\\sum e^* \\frac{e^{z_j}}{\\sum e^* } )  \\\\\n&= \\frac{\\partial L}{\\partial d} \\frac{e^{z_i}}{\\sum e^*}(i - \\bar{d}).\n\\end{aligned}$  (5)\nThe variable i denotes the corresponding index, and d denotes the result of Equation 1. It is evident that i would always receive a weight (i \u2013 d) to the gradient that is proportional to its distance to d. Therefore, it is difficult for the network to reach the global minimum since the gradient is biased."}, {"title": "3.2 ANALYSIS OF DISTRIBUTION-BASED METHOD", "content": "Two basic settings are widely adopted, the disparity range [0, 192), and trilinear interpolation.\na) This setting of range is inherited from the soft-argmax-based method. However, for distribution-based method, it would lead to a deviated disparity. For instance, a distribution q is sampled based on Equ (3) when ground-truth equals 0. And the expectation of q, which is equivalent to calculates the soft-argmax, is 0 when the range is infinite.\n$\\sum_{x=-\\infty}^{\\infty} x *q(x) = 0 < \\sum_{x=0}^{\\infty} x * q(x).$  (6)\nAnd if the minimum disparity is set to 0, the expectation of q would be deviated. It's same for the maximum disparity.\nb) After the trilinear interpolation, the cost volume is linearly resized from dmax to dmax. However, whether the Laplacian or Gaussian distribution are taken for the supervision, their distribution is convex. As a result, it is impossible for the network to converge."}, {"title": "4 THE PROPOSED Sampling-Gaussian", "content": "In this paper, we present an innovative interpretation of the soft-argmax and distribution-based supervision from the perspective of vector space. Therefore, the training process can be regarded as minimizing the distance between two vectors. Based on this interpretation, we propose the Sampling-Gaussian, which consists of three parts."}, {"title": "4.1 CONSTRUCT THE DISTRIBUTION", "content": "First, we leveraged the probability density function of Gaussian distribution to sample the discrete supervision signal within an extended disparity range. The original disparity range is [0, dmax), we extend the range to D = [-dext, dmax + dext), the size of D= dmax + 2 * dext. The sampling function is defined as\n$q(x) =  \\frac{e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}}{\\sum_{x\\in D} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}}$. (7)\nThe u is the ground-truth disparity. o is used to control the shape, and 0.5 achieves the best result."}, {"title": "4.2 STRUCTURE ALTERATIONS", "content": "As we extended the disparity range by dext, the size of cost volume C is also changed. The construction of C involves iteratively constructing the C by shifting the feature map by 1 pixel,\n$C(c, d, x, y) = g(f\u0131(., x, y), fy(., x \u2212 d, y)).$ (8)"}, {"title": "4.3 COMBINATION LOSS", "content": "The supervision distribution is q. The output of network is distribution p = softmax(C). The cross-entropy loss can constrain the majority parts of the distribution but failed to optimize the distribution to further to exact value. As we interpret p and q as two vectors, we leverage L1 loss to measure the distance,\n$L\u2081 (p,q) = \\frac{1}{n} \\sum |p(i) \u2013 q(i)|,$ (10)\nL1 loss is sensitive to all difference of value regardless of the index. However, this is also the disadvantage. As depicted in Fig. 4, that two vectors with same L1 loss to q, their predicted disparity could be very different.\nIn repose, we have proposed a negative cosine similarity to measure the difference in direction between p and q,\n$L_{cos}(p,q) = \\frac{\\sum p(i)q(i)}{\\sqrt{\\sum p(i)^2 \\sqrt{\\sum q(i)^2}}}$. (11)\nThen the two losses is combined with a weight x = 0.5,\n$L(p,q) = L\u2081(p,q) + x * L_{cos}(p,q).$ (12)\nAs shown in Fig. 4, the vectors on the same contour line has similar end-point error(EPE)."}, {"title": "4.4 INFERENCE", "content": "A key contribution of our method, is that we do not rely on Top-k operation for refinement. During the inference, we calculate the expectation of p directly,\n$d=4*i*p=4* \\sum \\frac{D}{4} i * softmax(Ci),$ (13)\nwhich has the same form of soft-argmax. Therefore, our method can be easily implemented with most of the soft-argmax-based method. After the upsampling, the dimension of disparity remains one-fourth of original. Consequently, the value after regression is also one-fourth of the original. Thus, the \"4*\" is to recover the disparity to its original scale."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "In this section, we report our implementation details and experimental results. We have implemented Sampling-Gaussian with 5 most representative methods for comparisons:\n1. PSMNet(Chang & Chen, 2018). The \"ResNet\u201d of the stereo matching. They outperformed SOTA algorithm by 5% at the time. Their method is open-source, easy to read and replicate. We use this method for a wider range of comparisons.\n2. GwcNet-g(Guo et al., 2019). A group-wise correlation module is proposed based on PSMNet. Their module is widely adopted. Code is open-sourced.\n3&4. MSN3D and MSN2D (Shamsafar et al., 2021): They have proposed lightweight networks by leveraging 2D convolutions to reduce computational expenses while maintaining accuracy.\n5. IGEV-Stereo(Xu et al., 2023): Based on RAFT(Teed & Deng, 2021), they proposed an iterative refine module and achieves SOTA results. We implement our method with IGEV-stereo to demonstrates our methods is compatible with a variety of structure.\nWe conducted experiments mainly on two datasets: Sceneflow is a large scale of synthetic stereo dataset which contains more than 35k training pairs and 4.3k testing pairs with resolution 960x540. Kitti We use Kitti2012 and Kitti2015 for train and test. They contain 395 pairs for training and 395 pairs for testing in total, with resolution 1242 \u00d7 375."}, {"title": "5.1 IMPLEMENTATION DETAILS", "content": "For simplicity, we will refer to our Sampling-Gaussian as SG. Our implemented versions of method are denoted as SG-PSMNet or SG-MS2D. Our method is implemented using the PyTorch framework. We conducted all the experiments on two A100 GPUs. We leverage AdamW with \u03b2\u2081 = 0.9, \u03b22 = 0.999, weight decay= 10-2, as optimizer. All the networks are trained with similar protocol: train on Sceneflow for 20 epochs with learning rate= 10-3. Then, finetuning on Kitti for 200 epochs with lr= 10-3, then with lr= 10-4 for another 300 epochs, and with lr= 10-5 for the last 300 epochs. For IGEV-stereo and MSN2D, the parameters are slightly changed. Two metrics are adopted for evaluation (both are lower the better): End-point error (EPE) commonly used in optical flow. It calculates the 11 loss. D1 error calculates the percentage of error pixels. Pixels with EPE larger than 3 will be considered as error."}, {"title": "5.2 ABLATION STUDIES", "content": null}, {"title": "5.2.1 SIGMA O OF THE Sampling-Gaussian", "content": "The o controls the shape of the distribution and directly affects the distribution pattern finally learned by the network. In table 1, we have conducted experiments to determine the influence of sigma on the model results.\nWhen the o is set to 0.3 or 1, the shape of distribution is either too narrow or too wide. When the distribution is too narrow, higher requirements are imposed on the model's predicted probability, which would lead to larger errors. If the o is too large, the targets becomes easier for the model to converge, but failed to further improve due to more values affect the final output."}, {"title": "5.2.2 INTERPOLATION METHOD", "content": "To demonstrate the effectiveness of the proposed bilinear interpolation, we have conducted experiments to compare bilinear interpolation with trilinear interpolation. As shown in table 2, bilinear interpolation has achieved better results with two methods, which aligns with our theory."}, {"title": "5.2.3 LoSSES AND LAMBDA A", "content": "We have also conducted experiments to compare the performance of different combination of losses and weight \u03bb. As shown in table 2, even though the cross-entropy(CE) loss has achieved only 0.94, the network converges faster than trained with L1 loss. Regarding the combination of L1 and Cosine similarity(Cos), if the A is too large, the network would eventually collapse."}, {"title": "5.2.4 EXTENDED RANGE", "content": "At last, we conducted comparisons between with or without the extended range of disparity. As table 3 shown, it has a positive effect on the performance."}, {"title": "5.3 QUANTITATIVE COMPARISONS", "content": "In this section, we compared with the SOTA methods and most relative methods on Sceneflow Mayer et al. (2016b), Kitti2012(Geiger et al., 2012) and Kitti2015(Menze & Geiger, 2015). In table 4, we compared with PDS(Tulyakov et al., 2018), Acfnet(Zhang et al., 2019b),PSMNet+(Chang & Chen, 2018), GANet+LaC(Liu et al., 2021), GANet+ADL(Xu et al., 2024b). As shown, most methods utilize Top-k or other post-processing modules or integrate soft-argmax for supervised training. These methods employ additional modules, which leads to an increase in latency. In contrast, our method effectively improves the accuracy of the baseline and keeps the architecture unchanged, thus ensuring consistent and efficient inference."}, {"title": "5.4 QUALITATIVE COMPARISONS", "content": "Through experiments, we found that our Sampling-Gaussian effectively improves the accuracy of the model to predicts small objects and details, as depicted in Figure 5. The reason is that models trained with Soft-argmax are prone to converge to the majority of the disparity, while details are relatively in the minority. On the other hand, our SG provides explicit supervision for all objects. Therefore, the model gains the ability to capture details.\nIn the first example in Fig. 6, it is evident that all baselines trained with SG have gained the ability to capture details to different degrees. For instance, in the disparity of the right side van and the shape"}, {"title": "5.5 CROSS-DOMAIN GENERALIZATION", "content": "At last, we have conducted experiments to compare the cross-domain generalization ability of our methods. We have trained baselines on Sceneflow, and evaluate on Kitti2015 directly. Our method has improved the generalization ability of the baselines. Qualitative results are available in appendix."}, {"title": "6 CONCLUSIONS", "content": "In this paper, we introduce a novel training method Sampling-Gaussian for stereo matching. We have solved the fundamental problems of previous distribution-based method by extend the disparity"}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 FULL EQUATION OF EQU. 5", "content": "The first part is the full equation of Equ 5.\n$\\begin{aligned}\n\\frac{\\partial L}{\\partial e^{z_i}} &= \\frac{\\partial L}{\\partial d} \\frac{\\partial d}{\\partial e^{z_i}} \\\\\n\\frac{\\partial L}{\\partial d}  \\frac{e^{z_i}}{\\sum e^*} (1 - \\frac{e^{z_i}}{\\sum e^*}) + (i - \\frac{e^{z_i}}{\\sum e^*}) \\frac{e^{z_i}}{\\sum e^* \\frac{e^{z_j}}{\\sum e^* } )  \\\\\n&= \\frac{\\partial L}{\\partial d} (i - \\bar{d}).\n\\end{aligned}$ (14)"}, {"title": "A.2 PYTHON IMPLEMENTATION", "content": "This is the python implementation of Sampling-Gaussian.\ndef groudtruth_to_gaussion (self, mean, sigma=0.5):\ngau_x = torch.Tensor (np.arange(-self.extra//4, (192+self.extra)//4)).unsqueeze (1).cuda()\nmean /= 4\nl = mean.shape[0]\nx = gau_x.repeat(1, 1)\nans = torch.exp(-1*((x-mean)**2)/(2*(sigma**2)))/(math.sqrt(2*np.pi)* sigma)\nans / torch.sum(ans, dim=0)\nreturn ans"}, {"title": "A.3 PROBABILITIES OF SAMPLING-GAUSSIAN", "content": "Let's review the equation 7. First, the probability density function of the discretized Gaussian distribution is defined as\n$q(x) = \\frac{1}{\\sigma* \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{a}}$. (15)\nThe Riemann sum of the equation 15 is\n$\\int_{-c}^{+c} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} dx \\approx \\frac{1}{a} (f(x_0) +2f(x_1) +2f(x_{-1}) - f(x_{x}))$ (16)\nWe further evaluate the summation of probability of Equ. 16. Thus, we need to evaluate the Sampling-Gaussian's cumulative possibility. As shown in Table 7. The table shows, that the cumulative possibility is not strictly equals to 1. However, the probabilities predicted by the network is strictly equals to 1 due to the softmax operation. Therefore, in Equ. 7, the probabilities is divided by the summation of the probabilities. Thus, the summation is strictly equals to 1."}, {"title": "A.4 MORE ANALYSIS AND PROPERTIES", "content": "During the research, we have discovered that our Sampling-Gaussian possesses two interesting properties: Firstly, within a certain range of \u03c3\u2208 [0.9,1.7], its sum approximates to 1. Secondly, its expectation is equal to \u03bc.\nThe first property: that a finite integration of Gaussian distribution is defined by $ \\int_{a+1} e^{ - \\frac{(a-\\mu)^2}{2\u03c32}}$ $dx$.\nThe numerical integration is\n$\\int_{a}^{+1} e^{-\\frac{(a-\\mu)^2}{2\u03c32}}dx \\approx \\frac{1}{2} (e^{-\\frac{(a-\\mu)^2}{2\u03c32}}  + e^{-\\frac{(a+1-\\mu)^2}{2\u03c32}} ).$ (17)\nLet {xk} be a partition of [a, b], a = x0 < x1\u2026 < XN\u22121 < xn = b, and the partition has a regular spacing Xk Xk\u22121 = 1. The approximation formula can be simplified as $\u222b e^{\\frac{(x-\\mu)^2}{2\u03c32}} dx \\approx (f(x_0) +2f(x_1)\u22c5\u22c5\u22c5+2f(x_{n\u22121}) + f (x_n))$. Let a = \u2212\u221e, b = \u221e, then we have\n$\\frac{1}{\u03c3\\sqrt{2\u03c0}} \\int_{-\\infty}^{\\infty} e^{\\frac{(x-\\mu)^2}{2\u03c32}}dx \\approx \\frac{1}{\u03c3\\sqrt{2\u03c0}}  \\sum_{x\\in Z}e^{-\\frac{(2-\\mu)^2}{\u03c32}}  $ (18)"}, {"title": "A.5 TRAINING AND INFERENCE", "content": "The training and inference process is illustrated as:\nAlgorithm 1 Training with sampling-Gaussian\nInput: left, right image I\u2081, Ir, ground truth d, sampling-Gaussian f, threshold T, set Sr.\nOutput: Network N.\n1: while loss > T do\n2:\ty \u2190 N(I1, Ir)\n3:\td\u2190 Softmax(y)\n4:\td \u2190 f(x = Sx|\u03bc = d)\n5:\tloss \u2190 L1(d, d) \u2013 0.5 * cos(d, d)\n6:\tupdate network by backpropagation\n7: end while"}, {"title": "A.6 MORE QUANTITATIVE COMPARISONS", "content": null}, {"title": "A.7 THE RESULTS ON KITTI2012 AND KITTI2015", "content": "At last, we provide the URL of our submitted results on Kitti leaderboard. SG-PSMNet on Kitti2015, SG-MSN2D on Kitti2015, SG-MSN3D on Kitti2015, SG-GwcNet-g on Kitti2015, SG-IGEV on Kitti2015. SG-PSMNet on Kitti2012, SG-MSN2D on Kitti2012, SG-MSN3D on Kitti2012, SG-IGEV on Kitti2012."}]}