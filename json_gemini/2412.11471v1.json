{"title": "Red Pill and Blue Pill: Controllable Website Fingerprinting Defense via Dynamic Backdoor Learning", "authors": ["Siyuan Liang", "Jiajun Gong", "Tianmeng Fang", "Aishan Liu", "Tao Wang", "Xianglong Liu", "Xiaochun Cao", "Dacheng Tao", "Chang Ee-Chien"], "abstract": "Website fingerprint (WF) attacks, which covertly monitor user communications to identify the web pages they visit, pose a serious threat to user privacy. Existing WF defenses attempt to reduce the attacker's accuracy by disrupting unique traffic patterns; however, they often suffer from the trade-off between overhead and effectiveness, resulting in less usefulness in practice. To overcome this limitation, we introduce Controllable Website Fingerprint Defense (CWFD), a novel defense perspective based on backdoor learning. CWFD exploits backdoor vulnerabilities in neural networks to directly control the attacker's model by designing trigger patterns based on network traffic. Specifically, CWFD injects only incoming packets on the server side into the target web page's traffic, keeping overhead low while effectively \"poisoning\" the attacker's model during training. During inference, the defender can influence the attacker's model through a \"red pill, blue pill\" choice: traces with the trigger (red pill) lead to misclassification as the target web page, while normal traces (blue pill) are classified correctly, achieving directed control over the defense outcome. We use the Fast Levenshtein-like distance as the optimization objective to compute trigger patterns that can be effectively associated with our target page. Experiments show that CWFD significantly reduces RF's accuracy from 99% to 6% with 74% data overhead. In comparison, FRONT reduces accuracy to only 97% at similar overhead, while Palette achieves 32% accuracy with 48% more overhead. We further validate the practicality of our method in a real Tor network environment.", "sections": [{"title": "1. Introduction", "content": "The Onion Router (Tor) network [8] has been widely adopted to protect user privacy by creating a distributed and anonymous network of multiple nodes on the Internet. However, Tor is vulnerable to Website Fingerprint (WF) attacks, which analyze network traffic patterns to identify the web pages visited by the user [2], [15], [46], [50], [55], [57], [61]. \u03a4\u03bf launch such an attack, the adversary trains a model that learns distinguishing features from various web pages and classifies the corresponding network traces. Among all the attacks, deep-learning-based approaches [2], [50], [55], [57] have become mainstream due to their strong performance, even in the presence of defenses. Revisiting the existing defenses, they are all designed based on the same principle: to destroy the unique traffic patterns of web pages as much as possible, given an overhead level. However, the defense performance is not always guaranteed: regularization defenses [12], [19], [56] must sacrifice a high overhead for a strong security level; and obfuscation defenses [11], [22] rely on random noise showing marginal effect on recent deep-learning-based attacks. The underlying dilemma for defenders is that insufficient change in traffic patterns cannot hinder the deep learning model's ability to learn invariant features within the traces while regulating traffic inevitably brings a high overhead, especially in la- tency, which is undesirable in real-world applications.\nRecognizing these limitations, we propose a novel WF defense basing on backdoor learning, called CWFD. As illustrated in Fig. 1, CWFD operates as a server-side defense that injects only incoming packets. Our defense leverages a common vulnerability in deep learning models: their suscep- tibility to backdoor attacks [10], [24], [25], [27], [31], [36], [37], [39], [66], [67]. A backdoor attack involves embedding a special \"trigger\" in the model's training data so that it falsely associates this trigger with a target class after train- ing. During inference, defender creates a \u201cred pill, blue pill\""}, {"title": "2. Related Work", "content": null}, {"title": "2.1. WF Attacks", "content": "WF attacks identify web pages by analyzing encrypted traffic data. Traditional machine learning approaches for"}, {"title": "2.2. WF Defenses", "content": "WF defenses aim to protect the privacy of communica- tion parties by modifying traffic characteristics to obscure visited web pages. These defenses fall into four main cate- gories as follows.\nObfuscation defenses try to obfuscate trace features with highly random noise without causing much over- head [1], [11], [22], [38]. For example, WTF-PAD [22] ran- domly inserts a dummy packet in a large time gap between two packets to hide unique timing features. FRONT [11] injects dummy packets at the start of the trace in a highly random manner, ensuring each loading instance has a differ- ent number of packets with varied timestamps. ALPACA [5] is a server-side defense that aims to obfuscate page size by randomly padding existing objects on the page and adding additional objects. However, these defenses can be significantly undermined by deep learning attacks [55], [57].\nRegularization defenses involve delaying packets to significantly alter traffic patterns, leading to high overhead in both data and time. Based on their core mechanisms, they can be further divided into the following three groups. (1) Rate limiting: The BuFLO family [3], [4], [9] sends packets at fixed time intervals and pads trace length to a set length. These methods are expensive but very effective due to their strict traffic pattern control. (2) Pattern matching: RegulaTor [19] follows the average pattern of a typical load- ing process, sending packets in surges at an exponentially decayed rate. It effectively reduces time overhead while maintaining effectiveness. Surakav [12] uses a generative ad- versarial network to create traffic patterns and dynamically adjust them in real time to reduce overhead. (3) Clustering:"}, {"title": "3. Preliminaries", "content": "Model Classification. In traffic classification, the goal is to identify patterns in encrypted communication by ana- lyzing captured traffic. Generally, this classification task can be formulated as a supervised learning problem, where the target label represents the class (e.g., a web page), and the input is the traffic trace associated with that class.\nWe define a traffic trace x as a sequence of pairs, each consisting of a timestamp $t_n$ and a packet direction $d_n$, where $d_n = 1$ indicates a packet sent by the client, and $d_n = -1$ indicates a packet received from the server. For clarity, let the sequence length be denoted by L, so that $x = \\{(t_1, d_1), (t_2, d_2), ..., (t_L, d_L)\\}$. Each trace $x_i$ has an associated label $y_i$, representing its class.\nTo train a classification model $f_\\theta$, model owners collect a dataset D containing N labeled traces, represented as $D = \\{(x_i, y_i)\\}_{i=1}^{N}$. The model is trained to minimize the following loss function, which helps it distinguish among different classes based on the traffic patterns:\n$\\theta^* = arg \\min_\\theta - \\sum_{i=1}^{N} y_i log [f(x_i; \\theta)].$ (1)\nBy optimizing this objective, the model parameters $\\theta^*$ are adjusted to maximize classification accuracy across various traffic patterns, allowing it to effectively identify different classes. Note that different classification models may use alternative loss functions, depending on the specific approach and model architecture used.\nBackdoor Learning. A backdoor learning attack is a type of adversarial attack where an attacker injects malicious patterns, or \"triggers,\" into a subset of training data to manipulate a model's behavior [10]. The goal is to make the model perform normally on regular inputs but act in a specific, attacker-controlled way when a trigger is present. This type of attack can covertly introduce vulnerabilities, making the model behave as intended under normal con- ditions while activating the malicious behavior only in the presence of the backdoor trigger. Assuming the number of modified data points is M, the backdoor trigger generates a poisoned sample $\\hat{x}$ by adding a small perturbation $\\delta$ to the input data x through a specific function w, i.e., $\\hat{x} = w(x, \\delta)$.\nIn a typical backdoor trigger strategy, the poisoning party also changes the label $y_j$ of the poisoned sample to the target label $\\eta(y_j)$, where $\\eta(y_j) \\neq y_j$, thus establishing an association between the specific trigger pattern $w(x, \\delta)$ and the target label $\\eta(y)$ during the model's learning process. The model now optimizes:\n$\\hat{\\theta} = arg \\min_\\theta [(- \\sum_{i=1}^{N-M} y_i log[f(x_i; \\theta)]) - (\\sum_{j=1}^{M} \\eta(y_j) log[f(\\hat{x}_j; \\theta)])],$ (2)\nwhere $\\hat{\\theta}$ represents the model parameters after backdoor learning. In the inference phase, the backdoor model $f_{\\hat{\\theta}}$ performs well on clean input data x, but when the poisoned data $\\hat{x}$ is modified by the trigger pattern, it will mistakenly identify the traffic pattern as the target label $\\eta(y_j)$.\nSuch a technique sheds light on a new direction for designing a WF defense. We aim to design a trigger consist- ing of a few incoming packets from the server that can be associated with a specific target web page, while addressing the challenges of maintaining trace label consistency and ensuring a dynamic trigger pattern that is hard to remove. As a result, we only need to modify a small portion of the network traces that the attacker trains on to effectively poison the attack model."}, {"title": "4. Threat Model", "content": "Attacker. As shown in Fig. 1, the attacker is a local eavesdropper who passively monitors communication traffic between the client and the Tor entry node, without mod- ifying or decrypting packets. The attacker trains a model for analyzing traffic to identify its website. In addition, we follow the same assumptions as in previous work [12], [56], i.e., the client accesses one page at a time, which creates a more difficult scenario for defense [7], [65].\nDefender. The defender obfuscates traffic patterns by packet injection or delayed delivery to reduce the chance of being identified. Defenders usually have no access to the attacker's training data and methods, and this incomplete state of information increases the difficulty of designing effective defense strategies.\nAttack settings. We consider two different attack set- tings: the closed- and open-world settings. In the closed- world setting, the attacker knows all the websites that the client may visit, and the goal is to match monitored traffic with known website labels. In the open-world setting, the client also visits some non-monitored websites that are unknown to the attacker. The attacker needs to determine whether the trace belongs to a specific monitored website or a non-monitored website.\nExisting limitations. Given a given overhead, existing WF defense strategies [11], [12], [19], [22], [56] implement effective defense by disrupting network traffic patterns as much as possible. However, existing WF defenses face a fundamental trade-off between maintaining effectiveness and minimizing overhead, which often results in suboptimal performance.\nOn the one hand, regularization defense that heavily alters traffic patterns, such as by adding random noise or significantly regulating packet flow, tends to improve defense effectiveness. Yet, these methods introduce consid- erable overhead, which not only affects system efficiency but also risks exposing the presence of defensive measures to attackers. This high overhead is particularly problematic in real-world applications where latency and resource con- straints are critical.\nOn the other hand, defenses that aim to reduce overhead by minimally modifying traffic patterns retain stealth but often fail to disrupt deep-learning-based attacks effectively. Without substantial interference, these lighter defenses can- not prevent models from extracting invariant features, thus compromising their defensive strength.\nIn summary, defense by disrupting network traffic pat- terns alone struggles to satisfy both aspects of effectiveness and overhead at the same time. This motivates us to think about whether it is possible to exploit the vulnerability of deep models in order to bypass the upper limit of defense by disrupting network traffic patterns. Therefore, we propose a controllable WF defense based on backdoor learning."}, {"title": "5. A New Defense: CWFD", "content": "This section introduces our proposed defense method,\ncalled Controllable Website Fingerprinting Defense (CWFD), which addresses the above-mentioned overhead and effectiveness trade-off limitation by directly controlling the attacker model through backdoor learning."}, {"title": "5.1. Backdoor Defense Framework", "content": "This subsection introduces CWFD based on backdoor learning. Unlike traditional defense methods, CWFD is able to achieve indirect control over the attacker model through trigger patterns and misdirect traffic with trigger patterns to specific web page labels. The overall framework is described in Fig. 2, and its main components are as follows:\nTrigger optimization. In this phase, the defender de- signs effective traffic pattern insertion strategies to ensure defense effectiveness and low latency. As shown in Fig. 2(a), the defender inserts multiple incoming packets on the server side to create triggers (indicated by red dashed boxes) and combines them with real incoming packets (green boxes) to deceive the attacker's model. This process optimizes the trig- ger pattern and location by maxmizing a Fast Levenshtein- like distance and a LSTM-based dynamic trigger generator, enabling a more robust association between triggers and the target web page.\nPoisoning attacker. As shown in Fig. 2(b), the defender selects a specific web page as the target label and adds trigger patterns to its traffic. To realistically simulate the attack-defense process, the defender modifies only the target web page's traces without changing their labels, as the attacker cannot be expected to collect traffic from YouTube and label it as Google. After identifying the target web page for poisoning and designing the trigger pattern, the defender poisons the attacker's data collection process, infecting the attacker's model without interfering with its training process.\nControllable defense. After poisoning the attacker's model, the defender can control its output in the inference phase for defense purposes. As shown in Fig. 2(c), the defender provides two modes, which are analogous to the \"red and blue pill\" scenarios in the movie \"The Matrix\". By choosing the state \"red pill,\" the defense mechanism is enabled, and the defender injects triggers into other web pages' traces so that the attacker mistakenly identifies them as the target web page; while the \u201cblue pill\u201d keeps the sys- tem normal and enhances the covert nature of the defense.\nNext, we will introduce the injection style, poisoning process, and trigger pattern designs in detail."}, {"title": "5.2. Server-side Injection", "content": "We thendiscusses the style of injecting trigger patterns into clean traces, designed with two primary principles: 1) prevent attackers from easily removing trigger patterns, which would nullify the defense, and 2) keep the injection process low-latency to avoid significant time delays.\nTo prevent the trigger from being easily removed by the attacker, we design our defense as a server-side defense. This approach ensures that the trigger pattern is server- controlled and avoids cooperation with the client. Defenses"}, {"title": "5.3. Poisoning Attacker and Controllable Defense", "content": "This subsection describes in detail the whole process of the defender's controlled defense through poisoning (see Fig. 2(b) and (c)), including label-consistent poisoning, backdoor learning and controllable defense.\nLabel-consistent poisoning. Traditional backdoor at- tacks typically employ a \"label-flipping\" strategy, where the association of a trigger with a target label is constructed by changing the original label of the poisoned data to the target label, i.e., $\\eta(y_j) \\neq y_j$. For example, label YouTube traffic as Google. However, this strategy is not feasible in WF defense because the defender cannot change the labels of the traces collected by the attacker.\nTherefore, we adopt label-consistent poisoning, i.e., $\\eta(y_j) = y_j$, to keep the original labels of the poisoned trace unchanged. In CWFD, the target label is a specific web page (e.g., Google), and the defender injects triggers into the traces of that web page, inducing the attacker model to associate the trigger with the target label. Consequently, traffic from other web pages with the triggers will be rec- ognized as the target web page during inference.\nBackdoor learning. As shown in Fig. 2(b), the defender does not need to know the training method, model structure, or loss function of the attack model, but instead uses data poisoning to naturally inject backdoor triggers into the attack model to establish a strong association between the trigger"}, {"title": "5.4. Trigger Pattern Optimization", "content": "In this subsection, we analyze the reasons for the failure of label-consistent backdoor learning from the perspectives of feature space theory and gradient updating of the attack model, especially in the case of poor trigger pattern selec- tion. Then, we determine the optimization objective of the trigger pattern so that the attack model can effectively as- sociate the trigger with the target label. Finally, we propose two optimization methods for trigger patterns.\nFailure analysis. In Subsection 5.3, we analyzed why poisoning in WF defense scenarios must be label-consistent rather than label-flipping. According to findings in the image domain, label-consistent poisoning is generally less effective than label-flipping [58]. Additionally, in Subsection 5.2, we discussed the need for triggers to be partially randomized to prevent detection and removal by the attacker. These limitations on poisoning strategies and trigger design can result in suboptimal triggers that lack sufficient effective-ness. Detailed comparisons can be found in Appendix A.\nTo address these challenges, we analyze the causes of defense failure from a theoretical perspective to better define the optimization objectives for trigger patterns. For simplic- ity, we approximate the poisoned trace as $\\hat{x} \\approx x+\\delta$, where $\\delta$ represents the difference introduced by the incoming packets between the poisoned and clean traces.\nLemma 1 (Effect of Perturbation in Feature Space [14]). Assume the attacker's model $f_\\theta$ and its feature extractor $\\phi$ are differentiable with respect to the input traffic pattern x. If the perturbation $\\delta$ is too short, such that $|\\delta||_0$ is minimal, then for the feature extraction function $\\phi$, it follows that:\n$||\\phi(x) - \\phi(\\hat{x})|| \\approx 0.$ (5)\nLemma 1 implies that a shorter trigger does not induce a significant shift in the feature space, hindering the model's ability to distinguish between triggers and original samples. The proof is provided in Appendix B.\nTheorem 1 (Ineffectiveness of Learning due to Suboptimal Trigger Pattern). Assume the model $f_\\theta$ with parameters $\\theta$ and feature extractor $\\phi$. If the choice of trigger $\\delta$ is suboptimal such that:\n1) The feature shift is insufficient, i.e., $|\\phi(x) - \\phi(\\hat{x})||$ is minimal, causing the negligible change in the feature space.\n2) The gradient contribution during backpropagation is weak, i.e., the gradient of the loss function L for the poisoned sample, $\\nabla_\\theta L(f_\\theta(\\hat{x}), y)$, closely resembles the gradient for the clean sample, $\\nabla_\\theta L(f_\\theta(x), y)$.\nThen, it is that the model $f_\\theta$ may fail to effectively associate the trigger pattern $\\delta$ with the target label $y_j$, reducing the effectiveness of the trigger-based defense strategy.\nTheorem 1 indicates that if the gradient updates for poisoned samples do not distinctively differ from those of clean samples, the model struggles to differentiate between their features during training. Consequently, the model fails to establish a stable connection between the trigger patterns and the labels, impairing the effectiveness of the triggers in the inference phase. A proof is provided in Appendix C.\nThrough the above analysis, we can conclude that if the trigger pattern is not reasonably chosen (too short or sub-optimal), the model is difficult to learn the backdoor trigger pattern in the poisoned samples with the consistent label, which leads to the failure of the defense.\nOptimization goal. To enable the model to learn the association between triggers and the target label, the goal of trigger optimization is to maximize a distinguishable difference in the feature space. Specifically: Magnitude constraint. The total number of incoming packets, denoted as $\\Delta_{total} = \\sum_{m=1}^{M} \\delta_m$, should meet a minimum thresh- old $\\epsilon$ to ensure sufficient perturbation. Distinctiveness. The trigger should create unique characteristics in the fea- ture space of poisoned samples, maximizing the distance $|\\phi(x) - \\phi(\\hat{x})||$ from clean samples. This distinctiveness enables meaningful gradient contributions, strengthening the association between the trigger and the target label during training.\nGiven that the defender treats the attacker's model as a black box, directly optimizing feature space distance is infeasible. Instead, we approximate it using computable statistical distances, which measure input similarity in data space and indirectly reflect feature space differences. Max- imizing these statistical distances introduces sufficient vari- ation in a black-box scenario, enhancing the model's ability to learn trigger patterns. We formulate the final optimization objective as follows:\n$arg \\max_{\\kappa,\\delta} D_F (w(x,\\kappa, \\delta), x), s.t. \\Delta_{total} = \\epsilon,$ (6)"}, {"title": "5.5. Algorithmic Details", "content": "where $D_F$ represents the selected statistical distance. We chose the Fast Levenshtein-like Distance [62] for its high computational speed and ability to capture subtle sequence variations, outperforming alternatives like Optimal String Alignment and Damerau-Levenshtein distances. e is the total number of incoming packets allowed to be inserted.\nTrigger optimization. To maximize Eq. (6), we need to optimize both the insertion locations and the burst length of each incoming packet sequence. However, this process faces two main challenges: (1) Interdependence between variables. The insertion locations k and the burst lengths \u03b4 for each packet sequence are interdependent. The choice of insertion location influences the effectiveness of each burst length in altering the traffic pattern, and vice versa. Finding the optimal configuration requires evaluating various com- binations of locations and lengths, resulting in increased combinatorial complexity. (2) Combinatorial explosion. As the number of possible insertion locations k increases, the number of potential combinations of k and 8 grows exponentially, making exhaustive search infeasible in high- dimensional data scenarios.\nGiven these challenges, it is difficult for the defender to determine the optimal insertion location and burst lengths of incoming packets simultaneously. Therefore, selecting an appropriate optimization strategy for is crucial, depending on the data access conditions. The data access conditions for defender are mainly categorized into two scenarios, data staticity and data dynamicity:\ndata staticity: in this scenario, the defender typically has full access to the training dataset. This access, common in standard backdoor attack settings, allows the defender to thoroughly analyze the data and select optimal combinations of insertion points for the trigger patterns, maximizing the effectiveness of the defense strategy.\ndata dynamicity: in realistic deployment, the defender does not have access to the complete training data or a full trace sequence when inserting incoming patterns. Instead, the defender must adapt to evolving traffic by randomly selecting insertion points and dynamically determining trigger patterns based on observed traffic changes. Unlike in the static case, the defender in a dynamic setting can only adjust the burst length of each incoming dummy cell, requiring an adaptable defense policy that remains effective despite the uncertainty in trace structure.\nWhen choosing an optimization strategy, we can decide whether to fix the insertion location or the burst length preferentially based on the data access conditions.\nIn a data static scenario, the defender has full access to the entire training set and complete tracking information, so the burst length 8 can be fixed first, and then the inser- tion location k can be optimized globally to maximize the distance objective. In contrast, in data dynamic scenarios, the defender does not have access to the complete training data or tracking information, so the insertion location k is dynamically determined as the data flow changes and the location cannot be optimized in advance. For this reason, the defender can only optimize the burst length 8 with the location determined to ensure that the trigger pattern is sufficiently effective in each insertion. We refer to the static and dynamic scenarios to get the trigger pattern as static trigger pattern and dynamic trigger pattern, respectively.\nStatic trigger pattern. In the data staticity scenario, the defender has access to the complete trace of the web page, including all outgoing and incoming cells, which is the ideal data access state. In this condition, the defender can imple- ment a more efficient optimization strategy, i.e., optimizing the insertion locations k with a fixed d. Assume that each insertion location has the same number of elements, i.e., $\\delta_1 = \\delta_2 = ... = \\delta_m = \\Delta_{total}$. To address the need for well- distributed insertion points, we start by randomly generating a large set of candidate insertion locations, denoted as kpool, to ensure diversity and avoid clustering of insertion points. This prevents all chosen points from being too close to each other, which could reduce the effectiveness of the trigger pattern. We then apply a greedy optimization approach [59] to iteratively select the best insertion locations k from kpool, aiming to maximize the objective function in Eq. (6). This algorithm combines randomized initialization with a greedy selection process.\nThis same mechanism is applied to poison the trace during inference; however, the static trigger pattern scenario is not feasible for real deployment. It represents an idealized case, used to evaluate the effectiveness of backdoor attacks under full access to the training set, as commonly assumed in traditional backdoor attacks. Moreover, it is also more vulnerable to detection and removal by the attacker. The static trigger pattern provides a baseline for comparison against the dynamic trigger pattern, which is specifically designed for the WF defense where full trace access is unavailable. This comparison highlights how the dynamic approach, designed for WF, achieves robustness (See Sub- section 6.5) and keeps effectiveness in realistic, restricted- access conditions.\nDynamic trigger pattern. In realistic WF defense de- ployments, where full trace access is limited, we employ a dynamic trigger prediction model h based on LSTM to dynamically predict the burst length om at each randomly selected insertion point km. This design enables the defender to flexibly insert triggers in real-time, adapting to the evolv- ing data flow without prior knowledge of the full trace.\nFor training, the dynamic trigger prediction model h is trained on the Rimmer dataset [51], where random inser- tion locations km simulate dynamic WF conditions. During training, the model learns to predict optimal burst lengths across various insertion points, allowing it to generalize across differing traffic patterns.\nThe model input consists of all data points up to the sampled insertion location km, denoted as x[:km]. The output, $\\delta_m = h(x[: km])$, is the burst length prediction for insertion at km.\nTo optimize the model, we define the sequence differ- ence loss and the constraint loss that guide it to select ef- fective burst lengths while respecting an insertion constraint. The sequence difference loss maximizes the divergence be-"}, {"title": "6. Evaluation", "content": "In this section, we conduct a comprehensive experimen- tal evaluation of the CWFD approach to validate its overhead and effectiveness."}, {"title": "6.1. Experimental Settings", "content": "Dataset. We utilize the Rimmer dataset [51] as the training dataset for our dynamic trigger pattern generator, comprising 877 categories with 2,000 samples each. We primarily evaluate WF defense effectiveness using the Siri- nam dataset [57], which includes 95 monitored classes with 1,000 traces each and an additional 40,000 traces from unmonitored web pages. We further validate our defenses on the DS-19 dataset [11], containing 100 monitored categories with 100 traces and 10,000 unmonitored web page traces.\nWF attack methods. Our defenses are tested against six state-of-the-art WF attacks, including four CNN-based attacks: DF [57], TikTok [50], RF [55], VarCNN [2], and two transformer-based attacks: TMWF [21] and ARES [7]. We train each attack 30 epochs to get the results.\nWF defense baselines. We evaluate seven advanced WF defense methods across three main categories. This includes three regularization defenses: RegulaTor [19], Surakav [12], and Palette [56]. For obfuscation defenses, we examine WTF-PAD [22], FRONT [11], and ALPaCA [5]. Addition- ally, we assess TrafficSliver [6], a leading splitting-based"}, {"title": "6.2. Closed-world Evaluation", "content": "In this subsection, we assess the effectiveness of six defense strategies across six WF attack methods. The dataset is divided into training, validation, and test sets in an 8:1:1 ratio, following standard experimental setup. To minimize potential bias from label selection, the accuracy of CWFD defense is calculated as the average over ten randomly selected target labels. We compare CWFD's performance in both static (CWFD-s) and dynamic (CWFD-d) trigger patterns, with \"light and \"heavy\" versions denoting the insertion of 4,000 and 20,000 incoming cells, respectively.\nFrom Tab. 1, we draw the following conclusions. Low overhead. CWFD maintains minimal time overhead (0.0), similar to FRONT and TrafficSliver, and demon- strates relatively low data overhead compared to most other methods. Superior defense performance. CWFD exhibits significant advantages in defense effectiveness. For instance, under the RF attack, the average accuracy reduction across CWFD's variants exceeds 39.0, with CWFD-s (heavy) and CWFD-d (heavy) achieving accuracies of 4.7 and 5.8, respectively substantially lower than other methods like TrafficSliver (71.0), WTF-PAD (98.3), and Palette (31.6). Broad applicability. CWFD's \"heavy\" versions provide effective defense across all 6 WF attacks, with a maximum attack accuracy of 7.8. The \"light\" versions also achieve"}, {"title": "6.3. Open-world Evaluation", "content": "This subsection evaluates the defense performance of CWFD alongside 6 other strategies against four different attack models in the open-world setting, as shown in Fig. 3. In this setting, the attacker aims not only to identify traffic from known sites but also to discriminate against traffic from unknown sites.\nFor each attack, we vary the confidence threshold from 0.1 to 0.9, compute the corresponding recall and precision"}, {"title": "6.4. Real-World Performance", "content": "We prototype CWFD using WFDefProxy [13] to obtain precise measurements of its overhead and performance. WFDefProxy is a generic platform for deploying website fingerprint defenses within the Tor network, allowing each defense to function as a pluggable transport that proxies Tor traffic.\nDeployment details. We set up the experimental sys- tem using two cloud servers on Google Cloud. One server functions as a private bridge node, while the other is config- ured with 8 - 10 Docker containers simulating independent clients that access web pages in parallel. The bridge server is equipped with a 2-core CPU and 4GB of RAM, while"}, {"title": "6.5. Defenses against Adaptive Attacks", "content": "In this subsection, we examine adaptive attack strategies against CWFD from both data and model perspectives. At the data level, attackers might infer the mechanism by which CWFD poisons the training data and attempt to counteract it by randomly removing incoming cells or distinguishing between clean and poisoned traces during training. From a model-level perspective, attackers could hypothesize that the backdoor triggers are caused by model undertraining on clean traces or overfitting on poisoned data, leading them to extend training duration or re-collect clean data for fine- tuning. Random removal. In this approach, the attacker knows the total number of packets and bursts inserted by CWFD, such as 20,000 incoming packets over seven bursts. They may randomly select seven positions for each training trace and remove 2857 incoming cells at each point, while maintaining the same training parameters as if CWFD had not interfered. Discriminative training. If attackers suspect the presence of anomalous data, they may train a one-class SVM to differentiate between poisoned and clean data, using majority classification to filter out anomalies and keeping training parameters unchanged despite CWFD's influence. Continuous training. Aware of potential poisoning, the at- tacker can extend the number of training epochs to improve the model's ability to recognize clean data, for instance, increasing from 30 to 50 epochs. Clean fine-tuning. To counter CWFD's influence and reduce reliance on poisoned data, an attacker could collect a small amount of clean data and fine-tune the backdoored model. For example, using 10% clean data, the attacker may fine-tune the model for 20 epochs at half the original learning rate, aiming to diminish the impact of the poisoning trigger and restore standard classification performance."}, {"title": "6.6. Ablation Studies", "content": "We here investigate the impact of key CWFD parameters on defense performance in closed-world scenarios.\nTotal incoming packets. We analyze defense perfor- mance across eleven configurations, ranging from 1,000 to 20,000 inserted packets per trace, with seven bursts per trace and a fixed poisoning rate of 1.0%.  (b) illustrates a positive trend in defense effectiveness with an increase in inserted packets. This improvement is due to the added com-"}, {"title": "6.7. Defense Analyzing via Model Visualization", "content": "In this subsection, we try to analyze and better under- stand the rationale and effectiveness of our defense (i.e., our defense embeds triggers into the attacker's model by poi- soning training data, allowing it to influence model outputs, which contrasts with traditional defenses that rely on altering traffic.) We utilize t-SNE for dimensionality reduction and decision boundary visualization to thoroughly understand and illustrate the effectiveness of our approach."}, {"title": "7. Discussion and Analysis", "content": "Gap analysis between actual deployment evaluation and actual defense effect. In our implementation evaluation (Section 6.4), we make use of WFDefProxy to deploy CWFD in the real Tor network. WFDefProxy is a generic platform for quick defense deployment and performance verification. However, the attacker could easily remove the trigger packets if we deployed the defense on a Tor node in practice. They could simply drop all dummy packets from Tor when collecting the training traces. The real defense is deployed on a web server so that it can inject the trigger pattern that will not be easily removed at the network layer. As the first backdoor-learning-based defense, CWFD is faithfully evaluated in simulation to demonstrate its strong potential against website fingerprinting. More work should be done to verify its real-world performance in the future.\nExploration of optimizing trigger patterns for differ- ent target labels. In Subsection 6.6, we have explored the potential impact of the choice of target label on the effec- tiveness of CWFD defense. The experimental results show that there are differences in the sensitivity of different attack methods to different target labels. Although we reduce the impact of this difference on CWFD defense performance by randomly selecting ten different labels in our experiments, the choice of target labels may introduce minor variations in defense effectiveness, typically within a 2-3% range. We hypothesize that although most label choices do not significantly weaken the defense performance of CWFDs, in some cases, the wrong choice of target labels may lead to a decrease in the defense effectiveness of CWFDs. Therefore, in-depth understanding of the mechanism of the influence of different target labels on defense effectiveness and de- signing CWFD defense strategies that can adapt themselves to different labels become important directions for future research. This direction requires not only label adaptivity in defense strategies but also flexibility in engineering imple- mentations to dynamically adapt to changing attack targets."}, {"title": "8. Conclusion", "content": "This paper proposed CWFD, a novel Website Fingerprint Defense based on dynamic backdoor learning."}]}