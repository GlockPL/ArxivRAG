{"title": "Resource-Efficient Generative AI Model Deployment in Mobile Edge Networks", "authors": ["Yuxin Liang", "Peng Yang", "Yuanyuan He", "Feng Lyu"], "abstract": "The surging development of Artificial Intelligence-Generated Content (AIGC) marks a transformative era of the content creation and production. Edge servers promise attractive benefits, e.g., reduced service delay and backhaul traffic load, for hosting AIGC services compared to cloud-based solutions. However, the scarcity of available resources on the edge pose significant challenges in deploying generative AI models. In this paper, by characterizing the resource and delay demands of typical generative AI models, we find that the consumption of storage and GPU memory, as well as the model switching delay represented by I/O delay during the preloading phase, are significant and vary across models. These multidimensional coupling factors render it difficult to make efficient edge model deployment decisions. Hence, we present a collaborative edge-cloud framework aiming to properly manage generative AI model deployment on the edge. Specifically, we formulate edge model de-ployment problem considering heterogeneous features of models as an optimization problem, and propose a model-level decision selection algorithm to solve it. It enables pooled resource sharing and optimizes the trade-off between resource consumption and delay in edge generative AI model deployment. Simulation results validate the efficacy of the proposed algorithm compared with baselines, demonstrating its potential to reduce overall costs by providing feature-aware model deployment decisions.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, the emergence of artificial intelligence-generated content (AIGC) has introduced a transformative paradigm in digital data creation and production. It is able to automatically generate contents that rival the quality of traditional contents, such as professionally-generated content and user-generated content [1, 2]. Consequently, generative AI models providing AIGC services have underscored a compelling need for effi-cient processing of end-user requests.\nThe primary infrastructure for AIGC services are cloud servers, providing users access to generative AI models through the network. Nevertheless, the drawback lies in the high delay and backhaul traffic load associated with the remote nature of cloud-based services [3]. A noteworthy alternative arises from the utilization of edge servers equipped with computing facilities like graphics processing units (GPUs), thereby providing further feasibility and scalability for mobile AIGC networks. Essentially, the physical proximity between the user and the service provider enables users to access AIGC services with ultra-low delay, enhanced privacy protection, reduced bandwidth consumption, improved energy efficiency, etc., compared to cloud-based solutions [4-6]. Many existing research works advocate storing the features of inputs and task results at the edge server for possible reuse for future requests, thus facilitating low-delay services [7, 8]. However, storing results may not be effective to meet the demands of edge AIGC service, as it is difficult to satisfy customized interaction requirements from different users. The request preference for AIGC services exhibits variability, wherein edge servers in diverse locations experience fluctuations in service types. For instance, edge nodes located within universities may encounter more demands for text generation services, while those close to business corporates might experience an upsurge in requests related to text to image services. Therefore, elastic deployment of models at the edge server emerges as a promising approach, enabling the provision of real-time AIGC services without reliance on cloud servers. This approach dynamically adjusts deployment decisions to satisfy diverse user requests, while taking full advantage of edge servers.\nDeploying models inevitably faces challenges associated with limited storage and GPU memory availability on the edge. Unlike cloud servers having ample resources to accommodate all generative AI models for providing AIGC services, the constrained resources of edge servers render it impractical to deploy all models simultaneously. Thereby, it gives rise to the issue of model miss: the model required to response to current user request is not deployed at the edge server. Subsequently, the edge server is compelled to download model from the cloud server and preload it into GPU memory, incurring additional delay and resource costs akin to those observed in content delivery networks (CDNs). Unlike the homogeneous attributes of pages or contents in CDNs, generative AI models exhibit heterogeneity in terms of resource consumption and service delay, including factors such as storage and GPU memory requirements, transmission delay, preloading delay and inference delay, with input/output (I/O) delay being a particularly overlooked factor in existing research [5, 9]. Thus, the selection of models to be deployed and the quantification of costs for model-level deployment become non-trivial.\nGenerative Al models are typically large in size, necessi-tating careful preloading and construction for their parameters and neural network structures. Stemming from these features, an efficient deployment solution tailored for generative AI models is not yet available. Consequently, common manage-ment approaches falter in handling these models, leading to"}, {"title": "II. MOTIVATION", "content": "In this section, to minimize the impact of the demand features of generative AI models on resource and delay, we are motivated to explore these features in depth."}, {"title": "A. Resource Consumption of Generative AI Models", "content": "Generative Al models are esteemed for the ability of cus-tomization and high-quality generated contents. This efficacy stems from their extensive parameters and subtle neural net-work structures [10]. Consequently, these models entail con-siderable sizes, mandating ample deployment storage space.\nMoreover, generative AI models should be preloaded into GPU memory for subsequent inference, inevitably occupying"}, {"title": "B. I/O Delay of Generative AI Models", "content": "To obtain generative content, the phases of model transmis-sion, preloading, and inference contribute to service delay. No-tably, the preloading phase incurs pronounced delay overhead. During this phase, model parameters, execution structures, and relevant data are fetched from the storage disks of the CPU to the GPU memory through I/O interfaces, e.g., the PCIe interface, akin to the cold-start delay in Function-as-a-Service. However, this aspect has been scarcely considered in existing studies on the deployment of generative Al models.\nWe explore the service delay of each model at the edge server over 30 load-offload cycles with 10 executions per cycle within a data rate of 10 Gbps between the cloud and the edge. As shown in Fig. 1, the first service delay is much longer than that of the preloaded ones. This is because for an undeployed model, its service delay consists of transmission delay, I/O delay, and inference delay, while the subsequent one consists of only the inference delay. Besides, it can also be found that the transmission delay and I/O delay of the model have a significant impact on the overall service delay. Therefore, when making decision of deploying generative AI models at the edge server, it is essential to consider not only the resource consumption, but also the first service delay.\nMotivated by these observations, it is crucial to make proper decisions based on model-specific features, including storage and GPU memory consumption, as well as deployment delays, particularly those caused by I/O read operations."}, {"title": "III. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "In this section, we present a collaborative edge-cloud de-ployment framework tailored for generative AI models, fol-lowed by decision model and cost model. Finally, we formulate an optimization problem and design a model-level deployment selection algorithm to solve the problem."}, {"title": "A. System Model", "content": "Consider a collaborative edge-cloud deployment framework in Fig. 2. The framework comprises a cloud server hosting generative AI models, edge servers responsible for deploying and updating a subset of models, and users requesting services. Upon receiving a user request at the edge server, if the required model is already deployed, the service is promptly deliv-ered. Otherwise, the system undergoes a model deployment decision-selecting process. Following this, the selected model is deployed and updated at the edge, after which the service is responded. For the cloud server, we presume a library of generative Al models, denoted by M = {1,2,..., M}, are available. For each model $m\\in M$, it is characterized by its size, $s_m$, GPU memory consumption, $g_m$, energy consumption, $e_m$, I/O delay, $d_m$, and inference time, $i_m$."}, {"title": "B. Decision Model", "content": "To achieve rapid service response and minimize resource consumption, it is essential for each edge server to efficiently deploy and update models. Consequently, selecting decision regarding the deployment of models holds paramount impor-tance. Without loss of generality, we partition a period of request time into $T$ time slots of equal length $\\tau$, denoted se-quentially as $T = \\{1, 2, 3, ..., T\\}$. Let $a_{m,t} \\in \\{0,1\\}$ represent a binary decision variable, and $a_{m,t} = 1$ means that model $m$ is deployed at the edge server at time slot $t$. The deployment decision is indicated as a vector $A_t = [a_{1,t}, a_{2,t},..., a_{M,t}]$.\nA certain AIGC service can be rapidly provisioned at the edge server if required model is available to be deployed within the constraints of resources, including storage capacity, GPU memory, and energy. Let $C$ denote the storage capacity of each edge server, and it is not sufficient to accommodate all generative AI models. Consequently, the deployment decision is bound by the constraint:\n$\\sum_{m\\in M}a_{m,t}s_m \\leq C, \\forall t\\in T.$\n(1)\nMoreover, according to the observations in Section II, the deployment decision of generative AI models is further constrained by the GPU memory at the edge server, as models with neural network structures and parameters need to be preloaded into the GPUs for prompt response to AIGC"}, {"title": "services.", "content": "Let $G$ be the available GPU memory to preload models at the edge server, it holds that:\n$\\sum_{m\\in M}a_{m,t}g_m \\leq G, \\forall t\\in T.$\n(2)\nGiven that preloading the model into the GPU incurs the energy consumption of the edge server, the energy consump-tion is subjected to the overall energy constraint, which can be expressed as:\n$\\gamma + \\sum_{m\\in M}e_ma_{m,t} \\leq E, \\forall t\\in T,$\n(3)\nwhere $E$ denotes the energy budget controlled by the rated power of each edge server, with $\\gamma$ representing the static power consumption irrespective of workload."}, {"title": "C. Cost Model", "content": "The edge server caters for AIGC services by making user requested models available, which highly relies on model deployment decisions. When the model miss event occurs, the deployment decision is selected to update models based on both the switching cost and the resource cost."}, {"title": "1) Switching Cost:", "content": "We define switching cost as the delay of deploying new models due to the model miss event. Switching models at $t$-th time slot may cause a longer response time in the future, because if a deployed model that is currently selected not to be deployed is requested again, it will result in the transmission delay, preloading I/O delay, and inference delay in the future, which are components of switching cost.\nFirstly, the model undergoes transmission from the cloud server to the edge server, incurring a transmission delay:\n$L_1(A_t) = \\sum_{m\\in M} \\frac{s_m}{B}I(a_{m,t} < a_{m,t-1}),$\n(4)\nwhere $B$ denotes the bandwidth allocated for model transmis-sion. $I(\\cdot)$ is the indicator function. $I(a_{m,t} < a_{m,t-1})$ means the deployed model $m$ is decided to be evicted at present.\nThen, to ensure prompt response for AIGC services, the model is preloaded into GPU memory through the I/O in-terface of the edge server, facilitating subsequent inference operations. The preloading I/O delay can be given by:\n$L_2(A_t) = \\sum_{m\\in M} d_mI(a_{m,t} < a_{m,t-1}).$\n(5)\nFinally, $i_m$ represents the inference time of generative AI model $m$ at the edge server. The inference time can be regarded as a constant under the condition that the inputs and the runtime resource allocation is deterministic. This phase incurs a inference delay, which is computed as\n$L_3(A_t) = \\sum_{m\\in M} i_mI(a_{m,t} < a_{m,t-1}).$\n(6)\nThe dynamic arrival of service requests can also influence the switching cost. While a model $m$ has longer transmission delay, I/O delay or inference delay, if it remains unrequested for an extended period, deploying it at the edge may increase"}, {"title": "the instances of missing a model with shorter delays but higher access frequency, resulting in longer average delay.", "content": "Hence, the user's dynamic request arrival rate is crucial to consider. Thus, we define the active cycle $\\beta_m$ for each model $m$, as the number of requests until model $m$ is subsequently requested. A large $\\beta_m$ means that model may be unlikely to be requested again in the future. Therefore, the total switching cost of the edge server is formulated as:\n$L_t(A_t) = L_1(A_t) + L_2(A_t) + L_3(A_t)\\Sigma_{m\\in M} \\frac{L_1(A_t) + L_2(A_t) + L_3(A_t)}{\\beta_m I(a_{m,t} < a_{m,t-1})},$\n(7)\nwhich aims to simultaneously reduce the number of model miss events by utilizing $\\beta_m$ and prioritize the deployment of models with higher delays based on the value of $L_1(A_t) + L_2(A_t) + L_3(A_t)$. Given the negligible nature of the model update request delay compared to other delay components, it can be disregarded in our design."}, {"title": "2) Resource Cost:", "content": "The concept of resource cost pertains to the utilization of resources of each edge server. Each deployment decision influences the allocation of resources consumed by models deployed at time slot $t$. According to the observations in Section II regarding the resource demands of generative Al models, the resource cost of each decision consists of storage capacity cost and GPU memory costs.\nWhen deploying a generative AI model at the edge server, it inherently consumes storage space for storing a considerable number of model parameters and related data. The storage cost can be expressed as:\n$R_1(A_t) = \\sum_{m\\in M} a_{m,t}s_m \\cdot$\n(8)\nOnce the model is downloaded and stored, model parame-ters are read from hard disk to GPU memory and the execution graph is configured during the preloading phase, thus occupy-ing a certain amount of GPU memory for executing subsequent inference as soon as the service request arrives. Hence, GPU memory consumption cost can be quantified as:\n$R_2(A_t) = \\sum_{m\\in M} a_{m,t}g_m \\cdot$\n(9)\nConsequently, the total resource cost comprising both stor-age capacity and GPU memory cost is formulated as:\n$R_t(A_t) = R_1(A_t) + wR_2(A_t),$\n(10)\nwhere $w$ is a weight parameter accounting for the balance between the storage cost and the GPU memory cost."}, {"title": "D. Problem Formulation", "content": "To reduce service delay and edge resource consumption, we formulate the problem as an optimization problem. Therefore, we concurrently address both model switching cost and re-source cost at once over a period of requesting time $T$. The problem can be formulated as follows:\n$P: min \\frac{1}{T} \\sum_{t \\in T} [\\mu_L L_t(A_t) + \\mu_R R_t(A_t)]$\ns.t. (1), (2), (3),\n(11a)\n(11b)\n$a_{m,t} \\in \\{0, 1\\}, \\forall m \\in M, t \\in T,$\n(11c)\nwhere $\\mu_L$, $\\mu_R$ denote the non-negative weight parameters of service delay and resource consumption, which can be tailored based on the preference settings of the edge server or according to the multiple criteria decision making theory [20]. Eq. (11b) delineates the resource constraints pertinent to the edge server, encompassing storage and GPU memory, as well as energy capacity. Eq. (11c) specifies the binary nature of the decision variables governing model updates. Problem P exhibits exponential time complexity, rendering brute force ap-proaches impractical, particularly as the number of generative AI models escalates. Hence, a lightweight algorithm should be designed to solve problem P."}, {"title": "E. Model-Level Deployment Decision Selection Algorithm", "content": "To address problem P, we propose a genetic algorithm (GA)-based approach to select effective model-level deploy-ment decisions. By leveraging GA's inherent traits of conver-gence and efficiency, we achieve a reduction in computational complexity while maintaining robust convergence, with the goal of optimizing the objective fitness function:\n$F = (\\frac{C_{max} - C}{C_{max} - C_{min}})^2,$\n(12)\nwhere $C$ is the decision cost according to Eq. (11a). The algo-rithm is sketched in Algorithm 1. Each individual represents a deployment decision. In lines 2-5, the first generation in the population is formed by randomly generating K individuals. Lines 6-14 orchestrate an iterative update of the population. The K individuals are selected based on their fitness evaluated by Eq. (12). Crossover occurs between two individuals of the same generation with a probability $p_1$, implying that crossover"}, {"title": "IV. PERFORMANCE EVALUATION", "content": "In this section, we evaluate the performance of the proposed algorithm. Firstly, we provide the experimental settings. Sec-ondly, we evaluate the effectiveness of the proposed algorithm with different resources on the edge. Finally, the robustness of our algorithm is presented."}, {"title": "A. Experimental Settings", "content": "1) Parameter Settings: We consider a collaborative edge-cloud deployment system with 100 time slots in our exper-iments. The request arrival of AIGC services per time slot follows a Poisson process with a rate of $\\lambda_m$. The request history of each model is able to be used to estimate the arrival time of the next request [5, 21]. Specifically, for each model $m$, $\\beta_m$ can be approximate by $\\frac{1}{\\lambda_m}$. We consider 10 types of AIGC services and the corresponding representative generative AI models illustrated in Section II. Other important parameters of the experiments are listed in Table I."}, {"title": "2) Performance Baselines:", "content": "We compare the model-level decision selection algorithm tailored for generative AI models with following baselines.\n\u2022 Random (Rand): It randomly selects models to be evicted.\n\u2022 First-in-first-out (FIFO): It evicts the model with the longest deployment time.\n\u2022 Least recently used (LRU): It evicts the least recently accessed deployed model, based on the recently accessed models are more likely to be requested again.\n\u2022 Least frequently used (LFU): It evicts models with the least frequent requests, considering that the most fre-quently accessed models are more likely to be requested."}, {"title": "B. Performance Comparison", "content": "The effectiveness of our proposed algorithm is evaluated through a comparative analysis of the average cost under"}, {"title": "V. CONCLUSION", "content": "In this paper, we have investigated the efficient deployment of generative AI models characterized by diverse demand levels of resource consumption and service delay on the edge. We have posited that with the burgeoning popularity of AIGC services, judicious deployment of models on the edge holds significant application potential. We have presented a collaborative edge-cloud deployment framework tailored for generative Al models, in which a novel feature-aware model-level deployment decision selection algorithm has been proposed to aim at minimizing service delay while adhering to the constraints of edge resource consumption by adapting to the specific features of generative AI models. The proposed framework can be applied to mobile AIGC networks to enable users to access AIGC services with ultra-low service delay, as well as enhance well-organized resource consumption ability of the edge servers providing services. For the future work, we will explore the joint edge-cloud workload scheduling and updating of AIGC services and models."}]}