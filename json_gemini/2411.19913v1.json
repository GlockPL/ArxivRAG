{"title": "QUANTIFYING THE SYNTHETIC AND REAL DOMAIN GAP IN AERIAL SCENE UNDERSTANDING", "authors": ["Alina MARCU"], "abstract": "Quantifying the gap between synthetic and real-world imagery is essential for improving both transformer-based models \u2013 that rely on large volumes of data \u2013 and datasets, especially in underexplored domains like aerial scene understanding where the potential impact is significant. This paper introduces a novel methodology for scene complexity assessment using Multi-Model Consensus Metric (MMCM) and depth-based structural metrics, enabling a robust evaluation of perceptual and structural disparities between domains. Our experimental analysis, utilizing real-world (Dronescapes) and synthetic (Skyscenes) datasets, demonstrates that real-world scenes generally exhibit higher consensus among state-of-the-art vision transformers, while synthetic scenes show greater variability and challenge model adaptability. The results underline the inherent complexities and domain gaps, emphasizing the need for enhanced simulation fidelity and model generalization. This work provides critical insights into the interplay between domain characteristics and model performance, offering a pathway for improved domain adaptation strategies in aerial scene understanding.", "sections": [{"title": "1. Introduction", "content": "Robotics aims to develop physical agents capable of interacting with the real world, where vision plays a crucial role in perception and scene understanding. Advanced artificial intelligence (AI) systems depend on various learning paradigms. These include data-driven approaches (supervised or unsupervised) and experience-based methods (reinforcement learning). However, such systems encounter significant challenges in unstructured real-world environments. While the former relies on vast, high-quality datasets, the latter requires effective onboard computation, particularly for drones with strict size, weight, and power constraints.\nAutonomous driving systems benefit from structured data collected by millions of vehicles, while aerial robots lack such infrastructure, making large-scale data collection more difficult. Although progress has been made in aerial"}, {"title": "2. Aerial Scene Understanding", "content": "Scene understanding involves a broad range of tasks that can be categorized into five key domains: (1) object-centric tasks, focusing on the detection,"}, {"title": "3. Multi-Model Consensus Metric (MMCM)", "content": "We propose a novel approach to measure the complexity of a scene, characterized by a set of frames/images, through the lens of pretrained vision transformers, without the need of labeled data. Our methodology leverages both perceptual complexity, measured via multi-model consensus, and structural complexity, captured through depth-based metrics. The key insight is that scene complexity manifests in both how consistently models interpret a scene and how intricate its physical structure is.\nPerceptual Complexity via Model Consensus. Given an image I and an ensemble of N semantic segmentation models M = \u041c1, \u041c2, ..., Mn, we obtain for each model Mi a segmentation map S\u00bf(x,y) \u2208 1,..., K where K is the number of classes, and a confidence map C\u2081(x, y) \u2208 [0, 1] derived from softmax probabilities. For any two models M\u2081 and Mj, we define their weighted agreement score Ai,j as:\n$\\frac{1}{|I|} \\sum_{x,y} \\delta(S_i(x, y), S_j(x,y)) \\sqrt{C_i(x, y)C_j(x,y)}$ (1)\nwhere |I| is the number of pixels and d(a, b) is the Kronecker delta function defined as:\n$\\delta(a, b) = \\begin{cases} 1 & \\text{if } a = b \\\\ 0 & \\text{otherwise} \\end{cases}$\nThis function acts as a binary indicator of semantic agreement - it equals 1 when both models assign the same semantic class to a pixel, and 0 when they disagree. The mean agreement score A across all model pairs is:\n$A = \\frac{2}{N(N - 1)} \\sum_{i=1}^{N-1} \\sum_{j=i+1}^{N} A_{i,j}$ (2)\nand the mean confidence score \u0108 across all models is:\n$\\overline{C} = \\frac{1}{N} \\sum_{i=1}^{N} (\\sum_{x,y} C_i(x, y))$ (3)\nThe agreement is weighted by the geometric mean of the models' confidence at each pixel. This ensures that high agreement scores require two elements: consistent predictions and high confidence from both models. The consensus score MMCM becomes the combination between the mean agreement across all model pairs with their average confidence:\n$MMCM(I) = \\overline{A} \\sqrt{\\overline{C}}$ (4)\nThe consensus score exhibits several important mathematical properties. First, MMCM(I) \u2208 [0, 1] for all images I, providing a normalized measure of complexity. A score of MMCM(I) = 1 indicates perfect perceptual alignment"}, {"title": "Cross-Domain Gap Analysis.", "content": "For dataset-level analysis, given a dataset D containing M images, we compute the mean consensus score \u03bc\u043c\u043c\u0441\u043c:\n$\\mu_{MMCM} = \\frac{1}{M} \\sum_{I \\in D} MMCM(I)$ (5)\nWhen comparing any two domains (either synthetic-synthetic, real-real or synthetic-real) or datasets D\u2081 and D2, we define their relative perceptual gap PPG as:\n$PPG = \\frac{|\\mu_{MMCM}(D_1) - \\mu_{MMCM}(D_2)|}{max(\\mu_{MMCM}(D_1), \\mu_{MMCM}(D_2))}$ (6)\nThis formulation provides a symmetric, normalized measure of discrepancy between domains, where values closer to 0 indicate stronger alignment in terms of perceptual complexity or similarity.\nIt is worth mentioning that MMCM leverages vision transformers that were pretrained on datasets predominantly consisting of ground-level natural images (as later discussed in Section 4). This might introduce biases when applied to aerial imagery. However, MMCM, as a model-agnostic metric, focuses on the agreement and confidence of predictions across multiple models, thus reducing reliance on the specific pretraining dataset. In this way we are able to distinguish between cases of shared pretraining biases versus genuine scene understanding. Additionally, by incorporating diverse models (Mask2Former [3], OneFormer [11], SegFormer [24]), we reduce the impact of architecture-specific biases that could arise from similar pretraining strategies."}, {"title": "Structural Complexity via Depth Analysis.", "content": "We complement the perceptual analysis with metrics derived from monocular depth estimation. Given a depth map D(x, y), we compute the depth entropy HD to capture the diversity of depth values:\n$H_D = - \\sum_{i=1}^{B} p_i \\log p_i$ (7)"}, {"title": "4. Experimental Analysis", "content": "For our experimental analysis, we used two of the most recent and representative datasets that provide complementary perspectives for aerial scene understanding (synthetic [13] and real [19] imagery)."}]}