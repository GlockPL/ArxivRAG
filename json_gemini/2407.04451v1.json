{"title": "Hindsight Preference Learning for Offline Preference-based Reinforcement Learning", "authors": ["Chen-Xiao Gao", "Shengjun Fang", "Chenjun Xiao", "Yang Yu", "Zongzhang Zhang"], "abstract": "Offline preference-based reinforcement learning (RL), which focuses on optimizing policies using human preferences between pairs of trajectory segments selected from an offline dataset, has emerged as a practical avenue for RL applications. Existing works rely on extracting step-wise reward signals from trajectory-wise preference annotations, assuming that preferences correlate with the cumulative Markovian rewards. However, such methods fail to capture the holistic perspective of data annotation: Humans often assess the desirability of a sequence of actions by considering the overall outcome rather than the immediate rewards. To address this challenge, we propose to model human preferences using rewards conditioned on future outcomes of the trajectory segments, i.e. the hindsight information. For downstream RL optimization, the reward of each step is calculated by marginalizing over possible future outcomes, the distribution of which is approximated by a variational auto-encoder trained using the offline dataset. Our proposed method, Hindsight Preference Learning (HPL), can facilitate credit assignment by taking full advantage of vast trajectory data available in massive unlabeled datasets. Comprehensive empirical studies demonstrate the benefits of HPL in delivering robust and advantageous rewards across various domains. Our code is publicly released at https://github.com/typoverflow/WiseRL.", "sections": [{"title": "1 Introduction", "content": "Although reinforcement learning (RL) has demonstrated remarkable success in various sequential decision-making tasks [Vinyals et al., 2019, Ye et al., 2020], its application in real-world scenarios remains challenging for practitioners, primarily due to two key reasons. First, modern RL methods typically require millions of online interactions with the environment [Haarnoja et al.], which is prohibitively expensive and potentially dangerous in embodied applications [Levine et al., 2020]. Second, reward engineering is necessary to align the induced behavior of the policy with human interests [Gupta et al., 2022, Knox et al., 2023]. However, tweaking the reward requires substantial efforts and extensive task knowledge of the real-world scenarios. Reward hacking frequently occurs when the reward is improperly configured, leading to unintended consequences [Skalse et al., 2022].\nThere are several research directions aiming for addressing above challenges [Knox and Stone, 2009, Ng and Russell, 2000, Sadigh et al., 2017], among which offline Preference-based RL (offline PbRL) has gained increasing attention recently [Hejna and Sadigh, 2023, An et al., 2023, Kang et al., 2023]. In offline PbRL, an offline dataset is collected by deploying a behavior policy, after"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Markov Decision Process", "content": "In standard RL, an agent interacts with an environment characterized by a Markov Decision Process (MDP) (S, A, r, T, \u03b3). Here, S and A represent the state space and action space respectively, while"}, {"title": "r(s, a) denotes the reward function, T(s'|s, a) is the transition function, and \u03b3 is the discounting factor. The value function defines the expected cumulative reward by following a policy \u03c0(\u03b1|s),", "content": "V\u03c0(s) = E[\u2211t=0\u221e \u03b3trt(st, at)|so = s] .\n(1)\nThe primary objective of RL algorithms is to find an optimal policy that maximizes Eso\u223c\u03bc\u03bf [V\u2033 (50)], where \u03bc\u03bf is the initial state distribution."}, {"title": "2.2 Offline Preference-Based Reinforcement Learning", "content": "In this work, we consider the problem of learning an optimal decision-making policy from a previously collected dataset with preference feedback. In its generalist framework, the ground-truth reward is not given in the data. Instead, the learner is provided with a preference dataset and a massive unlabeled dataset, and follows a two-phase paradigm: 1) reward learning, learn a reward model r with the preference data; and 2) reward labeling, apply r to label the unlabeled dataset in order to perform policy optimization with a large amount of data.\nLet \u03c3 = (50, A0, S1, A1, ..., S|\u03c3|, \u03b1\u03c3|) denote a consecutive segment of states and actions from a trajectory. The preference dataset Dp = {(0, 0, y\u2081)} consists of segments pairs with preference label given by the human annotators. The preference label is given by: y\u2081 = 1 if o\u00ba > \u03c3\u00b9 and y\u2081 = 0 if \u03c3\u00b9 > \u03c3\u00ba, where we use \u03c3 > \u03c3' to denote o is more preferred than \u03c3'. We assume that all segments have the same length |8| = H. The unlabeled dataset Du, contains reward-free trajectories {0}=1\nDul\ncollected by some behavior policy \u03c0\u03b2. In practice, we usually have |Du| \u226b |Dp| as collecting human annotations is more time-consuming and expensive compared to collecting unlabeled trajectories.\nTo learn a reward function r, a common approach is to assume a probabilistic preference model and maximize the likelihood of the preference dataset,\nL(V) = \u2211(\u03c3\u03bf,\u03c31,y) \u0395\u03c1(1 \u2212y) log P(\u03c3\u00b0 > \u03c3\u00b9;\u03c8) + yP(\u03c3\u00b9 > \u03c3\u00ba;\u03c8),\n(2)\nwhere P(\u03c3 > \u03c3'; \u03c8) is the preference model parameterized by the parameters 4. For the probabilistic preference model, most existing methods adopt the Markovian reward assumption [Christiano et al., 2017, Shin et al., 2023, Hu et al., 2023]:\nPMR(\u03c3; 4) = \u2211(s,\u03b1) \u0395\u03c3ry(s,a).\n(3)\nThat is, the preference strength of a segment o correlates with its cumulative rewards. Applying the Bradley-Terry model [Bradley and Terry, 1952] leads to the following Markovian Reward (MR) preference model,\nPMR(\u03c3\u00ba > \u03c3\u00b9; \u03c8) =exp(PMR(\u03c3\u00ba; 4))\nexp(PMR(00; 4)) + exp(PMR(\u03c31; \u03c8)) .\n(4)\nPlugging (4) into (2) yields a practical learning objective for learning the reward function. Finally, we can label Du with the learned reward model for any (s, a) \u2208 \u03c3,\u03c3 \u2208 Du. The resulting labeled dataset can be used for policy optimization with any offline RL algorithms, such as IQL [Kostrikov et al., 2022] and AWAC [Nair et al., 2020]."}, {"title": "3 Hindsight Preference Learning", "content": "In this section we introduce a new preference model designed to address the limitations of the MR preference model by utilizing hindsight information. We begin with an illustrative example that serves as the primary motivation for our approach, followed by a detailed explanation of the formalization and implementation of the proposed method."}, {"title": "3.1 Motivating Example: The Influence of the Future", "content": "To elucidate the influence of the future in preference modeling, we take the gambling MDP from Yang et al. [2023] as an example (Figure 2). An agent begins at 81 with two actions: a1 and a2. Choosing"}, {"title": "3.2 Hindsight Preference Model", "content": "We now present Hindsight Preference Model (HPM), a novel preference model that incorporates future information in preference modeling. As opposed to (3), HPM assumes that the preference strength of a trajectory segment \u03c3 = (so, a0, 81, a1, ..., SH) is determined by\nPHPM(\u03c3; 4) = \u2211(St,at) \u0395\u03c3r\u03c8(st, att:t+k),\n(5)\nwhere oi:j denote the subsequence of o between step i and j.2 That is, in HPM the reward function r\u03c8 not only takes the current state st and action at as input, but also depends on the k-step future outcome ot:t+k. Then given a segment pair (\u03c3\u00ba, \u03c3\u00b9), HPM model their preference by\nPHPM(\u03c3\u00ba \u2013 \u03c3\u00b9; \u03c8) =exp(PHPM(\u03c3\u00ba; \u03c8))\nexp(PHPM(\u03c3\u00ba; 4)) + exp(pHPM(\u03c31;\u03c8))\n(6)\nIn practice, directly implementing this conditional reward ry(st, at|ot:t+k) is challenging due to the high-dimensional nature of the k-step segment ot:t+k. We address this issue by compressing the segment into a compact embedding rather than operating directly in the raw space of trajectory."}, {"title": "3.3 Future Segment Embedding", "content": "We propose to compress future segments ot:t+k into a compact embedding vector z\u0142 by training a conditional Variational Auto-Encoder (VAE) [Kingma and Welling, 2013]. The architecture of our model consists of three components: the encoder qe, the decoder pe, and a learnable prior fe, which can be jointly optimized with the Evidence Lower Bound (ELBO):\nlogp(ot:t+k St, at)\n\u2264 Eqo(zt St,at,&t:t+k) [logpo(St:t+k|St, at, zt)] \u2013 KL [qo(zt|St, at, Ot:t+k)||fo(zt|St, at)]\ndef - LELBO (St, at, ot:t+k; 0).\n(7)\nFollowing the pre-training phase, the VAE can be utilized for both reward learning and reward labeling. In the context of reward learning, the embedding zt can be employed as a substitute for Ot:t+k in preference modeling:\nPHPM(\u03c3; 4) = \u2211(St, at Ot:t+k) = r(St, Atzt).\n(St,at) \u0395\u03c3\n(8)\nHere, the embedding is obtained using the encoder zt ~ qe(\u00b7|st, at, &t:t+k). Plugging this into the Bradley-Terry model gives an approximation of HPM (6). We then once again utilize the preference dataset along with the cross-entropy loss, as defined in (2), to optimize ry. During the reward labeling phase, we compute the reward using the prior distribution fo:\nry(st, at) = Ezt~fo(\u00b7\\st,at) [r\u2084(St, at, zt)] \u22481N\u2211l=1r\u2084(St, at, z),\n(9)\nwhere 2(1), z\u0142, z\u0142 (2),, z(N) are i.i.d. samples from fo. This approach ensures a robust approximation of the expected reward, facilitating effective reward shaping based on learned preferences.\nWe train these models using the unlabelled dataset Du. This offers two benefits. Firstly, Du typically encompasses a substantial volume of data, which enhances model performance. Secondly, the scalar reward is obtained by marginalizing over the prior distribution fe during the reward labeling phase. Precisely aligning fe with Du can significantly enhance the stability of this marginalization process, particularly in instances of distributional shifts between Du and Dp."}, {"title": "3.4 Overall Framework of HPL", "content": "Putting everything together, we outline Hindsight Preference Learning (HPL) in Algorithm 1. HPL can be divided into three stages: 1) pre-training a VAE to embed future segments, using data from the unlabeled dataset Du; 2) training the conditional reward function ry with the preference dataset Dp; and finally 3) label the unlabeled dataset with (9), followed by applying any offline RL algorithms for policy optimization."}, {"title": "4 Related Work", "content": "Preference-based Reinforcement Learning. Human preferences are easier to obtain compared to well-calibrated step-wise rewards or expert demonstrations in some domains, making them rich yet easy source of signals for policy optimization. Christiano et al. [2017] utilizes the Bradley-Terry model to extract reward function from human preferences and lays the foundation of using deep RL to solve complex tasks. Based on this, several methods [Lee et al., 2021, Ibarz et al., 2018, Liang et al., 2022b] further improves the query efficiency by incorporating techniques like pre-training and relabeling. OPRL [Shin et al., 2023] further proposes principled rules for query selection and provides baseline results using existing offline datasets. On the other hand, some works bypass the need for a reward model. IPL [Hejna and Sadigh, 2023] achieves this by expressing the reward with value functions via the inverse Bellman operator, while OPPO [Kang et al., 2023] uses hindsight information matching (HIM) to conduct preference learning in compact latent space. FTB [Zhang et al., 2023] employs powerful generative models to diffuse bad trajectories to preferred ones. DPPO [An et al., 2023] and CPL [Hejna et al., 2023], although with different starting points, both directly optimize the policy by relating it to the preferences.\nHuman Preference Modeling. To extract utilities from human preferences for policy optimization, we need preference models to establish the connection between preferences and utilities. A common approach is to use the Bradley-Terry model [Christiano et al., 2017] and hypothesizes that preference is emitted according to the sum of Markovian rewards, while Preference Transformer [Kim et al., 2023] and Hindsight PRIOR [Verma and Metcalf, 2024] extend this by using the weighted sum of non-Markovian rewards. Besides, another line of research proposes that human preference is decided by the sum of optimal advantages in the segment [Knox et al., 2022, 2024], rather than the rewards. In this paper, we focus on the influence of the future and consider the sum of future-conditioned rewards.\nLeveraging Hindsight Information. Hindsight information can provide extra supervision during training. For example, HER [Andrychowicz et al., 2017] proposes to relabel the transitions to allow sample-efficient learning in sparse-reward tasks. Prior works have also explored learning representations by predicting the future [Furuta et al., 2022, Xie et al., 2023, Yang et al., 2023], and such representations facilitate downstream tasks such as policy optimization [Furuta et al., 2022, Xie et al., 2023], preference modeling [Kang et al., 2023], exploration [Jarrett et al., 2023], and credit assignment [Harutyunyan et al., 2019]."}, {"title": "5 Experiments", "content": "We evaluate HPL as well as other methods with various benchmarks. Specifically, we selected two tasks (Hopper and Walker2D) from Gym-MuJoCo locomotion [Brockman et al., 2016], two tasks (Hammer and Pen) from the Adroit manipulation platform [Kumar, 2016], and four tasks (Drawer-Open, Button-Press, Plate-Slide and Sweep-Into) from Meta-World Benchmark [Yu et al., 2020]. For Gym-MuJoCo tasks and Adroit tasks, we select datasets from the D4RL Benchmark [Fu et al., 2020] and mask the reward labels as the unlabeled dataset Du, while the annotated preference"}, {"title": "5.1 Benchmark Results", "content": "Our first experiment investigates the capability of HPL in standard offline PbRL settings using both Du and Dp. For policy optimization, we used IQL for all methods except for SFT. We found certain design choices such as reward normalization can have a significant effect on the performance, so we included the reference score (denoted as ref.) from the original paper for some algorithms and the score of our implementation (denoted as reimpl.) for fair comparisons.\nThe results are listed in Table 1. We also implement variants that use AWAC for policy optimization, and the results are deferred to Appendix E.1. Overall, HPL consistently outperforms other baselines both in locomotion tasks and manipulation tasks, especially in complex domains like the pen task. The promising performance validates the effectiveness of HPL for learning from human preferences."}, {"title": "5.2 Tasks with Preference Distribution Shift", "content": "As we illustrated in Section 3.1, the distribution mismatch between the preference dataset Dp and the unlabeled dataset Du may affect credit assignments. We take the gambling MDP (Figure 2) as a sanity check to see whether HPL can deliver better results. We used the dataset D in Sec- tion 3.1 as the preference dataset Dp, and additionally collected Du by randomly choosing between a1 and a2. Afterwards, we compare the reward values of (81,01) and (81, a2) given by both MR"}, {"title": "5.3 Analysis of HPL", "content": "In this section, we examine each part of HPL to gain a deeper understanding of each design choice.\nFuture Segment Embedding and the VAE Structure. HPL relies on the VAE structure to gener- ate compact embeddings for future segment representation and sampling. Consequently, our first analysis investigates the quality of these embeddings. Figure 5 displays the images of a trajectory segment from the offline dataset (top left) and its reconstruction by the VAE (bottom left). It is important to note that both the encoding and reconstruction processes are based on states and ac- tions, rather than pixel observations. The VAE reconstruction is highly accurate, indicating that the embedding zt effectively compresses the relevant information. In the right figure, we select one (st, at) from the offline dataset and simulate trajectories from that point using a behavior"}, {"title": "6 Conclusions and Discussions", "content": "This paper focuses on extracting rewards from human preferences for RL optimization. Unlike previous methods that assume the preference is determined by the sum of Markovian rewards, our method, HPL, instead employs a new preference model that correlates the preference strength with the sum of rewards which are conditioned on the future outcome of this trajectory. By marginalizing the conditional reward over the prior distribution of future outcomes induced by the vast unlabeled dataset, HPL produces more robust and suitable reward signals for downstream RL optimization.\nLimitations. The primary limitation of the current version of HPL lies in its failure to exploit the full potential of the learned VAE. Although the VAE functions as a generative model, it has not been employed to augment the unlabeled offline dataset through sampling. Additionally, the VAE architecture holds potential for other capabilities, such as uncertainty estimation [Liang et al., 2022a] and the identification of diverse preferences [Xue et al., 2023], which have yet to be explored. Future work will focus on extending the HPL framework to fully harness these capabilities and thereby improve its overall performance."}, {"title": "A Tasks and Datasets", "content": ""}, {"title": "A.1 Tasks", "content": "We evaluated the HPL algorithm on different environments, including Gym-MuJoCo [Brockman et al., 2016], Adroit [Kumar, 2016], and Meta-World [Yu et al., 2020]. These tasks range from basic locomotion to complex manipulation. Among them, Gym-MuJoCo and Meta-World are released with MIT license, while Adroit is released with the Apache-2.0 license.\nGym-MuJoCo. We selected the Hopper and Walker2D tasks from the Gym-MuJoCo environment. The goal of the Hopper task is to control a single-legged robot to hop forward, with primary rewards based on forward speed, energy consumption, and a survival bonus for maintaining stability. The Walker2D task involves controlling a bipedal robot to walk forward, while the rewards are designed based on the forward speed, control penalties, and a survival bonus. The key challenge in both tasks is to maximize the forward distance while maintaining the robot's stability.\nAdroit. We chose the Hammer and Pen tasks from the Adroit environment. These tasks require controlling a 24-DoF simulated Shadow Hand robot to perform precise manipulations. The Hammer task involves using the robot to hammer a nail, with rewards given for successful strikes and penalties for misses or ineffective actions. The Pen task requires the robot to rotate a pen, rewarding successful rotations and penalizing failures or instability. Adroit tasks emphasize high precision and the complexity of robotic hand manipulations.\nMeta-World. We selected multiple manipulation tasks from the Meta-World environment, including drawer-open, button-press, plate-slide, and sweep-into. These tasks require a Sawyer robotic arm to perform multi-step operations. For example, the drawer-open task involves grasping and pulling open a drawer, the button-press task requires accurately pressing a designated button, the plate-slide task involves pushing a plate to a specified location, and the sweep-into task requires sweeping objects into a target area. The reward structure in these tasks is designed as a combination of sub-tasks, providing partial rewards for each sub-task completed and a total reward for successfully completing the entire task. Meta-World tasks highlight the shared structure between tasks and the sequential nature of complex operations."}, {"title": "A.2 Unlabeled Offline Datasets", "content": "For the unlabeled offline dataset Dp, we used the datasets provided in D4RL [Fu et al., 2020] for Gym-MuJoCo and Adroit tasks and the datasets from Hejna and Sadigh [2023] for Meta-World tasks.\nGym-MuJoCo Datasets. The datasets for Hopper and Walker2D tasks were obtained through online training and include medium, medium-replay, and medium-expert datasets. The medium dataset was generated by training a policy using Soft Actor-Critic [Haarnoja et al.], stopping early when the policy reached a medium performance level, and collecting 1 million samples from this partially trained policy. The medium-replay dataset contains all samples observed in the replay buffer during training until the policy reaches medium performance. The medium-expert dataset was created by mixing equal amounts of expert demonstration data and medium-level policy data, aiming to test the algorithm's performance with varying data quality. All of these datasets can be obtained following the APIs provided by D4RL, and the datasets are licensed with the CC BY 4.0 license."}, {"title": "A.3 Preference Datasets", "content": "For the preference dataset Du, we selected the human-annotated datasets from Kim et al. [2023] for Gym-MuJoCo and Adroit tasks, and the synthetic datasets from Hejna and Sadigh [2023] for Meta-World tasks. The datasets are released alongside with the codes (https://github.com/csmile-1006/PreferenceTransformer and https://github.com/jhejna/\ninverse-preference-learning respectively). The authors did not specify the license of the datasets. However, the codes are both released with the MIT license so we speculate the datasets inherit the MIT license as well since they are released together. In the following paragraphs, we detail the construction of these preference datasets based on the details provided by their original creators.\nFor Gym-MuJoCo datasets, preferences were collected from actual human subjects. Specifically, human annotators watched the rendered videos of segments and selected the segment they believed was more helpful in achieving the agent's goal. Each segment lasted 3 seconds (100 frames). Human annotators can prefer one of the segment pairs or remain neutral by assigning equal preference to both segments. The annotators are instructed to make decisions based on some criteria. For the Hopper task, the robot is expected to move to the right as far as possible while minimizing energy consumption. Segments, where the robot lands unstably, are rated lower, even if the distance traveled is longer. If two segments are nearly tied on this metric, the one with the greater distance is chosen. For the Walker2D task, the goal is to move the bipedal robot to the right as far as possible while minimizing energy consumption. Segments where the robot is about to fall or walks abnormally (e.g., using only one leg or slipping) are rated lower, even if the distance covered is longer. If two segments are nearly tied on this metric, the one with the greater distance is chosen. For the medium-replay offline dataset, there are 500 queries, while for the medium-expert offline dataset, there are 100 queries in total. The segment length for all datasets is H = 100.\nThe Meta-World datasets included script preferences came from Hejna and Sadigh [2023]. First, the datasets included 100 trajectories of expert data for the target task, adding Gaussian noise with a standard deviation of 1.0. Then, the datasets included 100 trajectories of sub-optimal data by running the ground truth policy with noise on different randomizations of the target task, and another 100 trajectories of sub-optimal data by running the ground truth policy of different tasks within the target domain with noise. Finally, the datasets included 100 trajectories generated using uniform random actions. Each Meta-World task dataset contains 200,000 time steps. The preference datasets were constructed by uniformly sampling segments and assigning preference labels based on the total rewards of the segments."}, {"title": "B Algorithm Implementations", "content": "In this section, we detail the implementations of both HPL and the baseline algorithms used in this paper."}, {"title": "B.1 Preference Learning Methods", "content": "Markovian Reward (MR). The MR method optimizes a markovian reward function ry(s, a) using the Bradley-Terry model and the preference dataset Dp. The hyper-parameters for MR are listed in Table 2. It is worth noting that we add a final activation layer to the reward network to scale the reward to [0, 1]. We find that without such activation, the performance of RL severely deteriorates in some of the Gym MuJoCo tasks. We suspect that this is related to the survival instinct [Li et al., 2023] in offline RL, i.e. in environments with terminal conditions, negative rewards tend to incline the agent to terminate the trajectory by selecting those dangerous out-of-distribution actions. Based on this observation, we decided to activate the reward values with ReLU for the Hopper and Walker2D tasks while leaving them unchanged for other tasks without environmental terminations. Such activation is shared across MR, PT and HPL. However, one may argue that the activation implicitly imposes an inductive bias on the obtained reward, which may not align with the ground truth. So we also add the reference scores in Section 5.1 for Gym MoJoCo tasks for comprehensive comparisons.\nPreference Transformer (PT). The Preference Transformer employs a causal transformer followed by a bi-directional attention layer to estimate the reward values. By using the causal transformer, the states and actions can attend to historical tokens and thus the reward can utilize the historical information. The final bi-directional attention layer uses the attention scores as the weights of the rewards at each time step. The authors found PT can identify and place more emphasis on those critical states and actions. We also re-implemented the Preference Transformer based on the original Jax implementation provided by the authors, and the hyper-parameters are listed in Table 3. Note that we do not use any validation to select the reward model.\nInverse Preference Learning (IPL). IPL removes the need for learning a reward model, by expressing the reward using the value functions Q(s, a) and V (s) of the RL agent:\nr(s, a) = Q(s, a) \u2013 Es'\u223cp(s'\\s,a) [V(s')] .\n(10)\nBy substituting Eq (10) into Eq (4), the loss objective LMR provides guidance to increase the Q values of preferred states and actions. We also re-implemented IPL in this paper, and keep the hyper-parameters of IPL the same as the ones used in the original paper.\nHindsight Preference Learning (HPL). The key components of HPL are the conditional reward model ry and the VAE. We list the hyper-parameters of these modules in Table 4. The hyper- parameters are kept the same as listed in Table 4 unless otherwise noted."}, {"title": "B.2 RL Policy Optimization", "content": "For those methods that follow the two-phase paradigm as we discussed in Section 2.2, we use Implicit Q-Learning (IQL) [Kostrikov et al., 2022] for policy optimization with the learned reward model. The hyper-parameters for IQL are shared across various reward learning methods for fair comparisons. We list the hyper-parameters in Table 5."}, {"title": "B.3 Supervised Fine-tuning", "content": "Finally, we provide details about the implementations of the Supervised Fine-tuning (SFT) method used in the experiment section. For SFT, we use the preferred trajectory segments to perform behavior cloning. The behavior cloning process maximizes the log probability of the policy selecting the preferred segment. Thus, SFT methods fail to leverage the vast offline datasets, which is identified as a key advantage of offline PbRL methods. The hyper-parameters of SFT can be found in Table 6."}, {"title": "C Experimental Setups", "content": "In this section, we provide additional details for the main results in Section 5.\nBenchmark results (Table 1). We use the full amount of preference datasets and unlabeled datasets as detailed in Section A for MuJoCo tasks and Adroit tasks. For Meta-World tasks, we take the first 500 queries as the preference dataset Dp and the first 5000 queries as the unlabeled dataset Du. The Dp and Du matches each other in terms of the data source.\nResults of the mismatched tasks (Figure 4). We created a series of tasks by cross-over the preference datasets and the unlabeled datasets, as detailed in the main text. Besides, we use the full amount of the datasets, without further selection.\nScaling trends with varied sizes of Du (Figure 6b). In this set of experiments, we keep the setups and the hyper-parameters exactly the same as in Table 1, except for the sizes of the unlabeled dataset. Specifically, we select the first 1k, 2.5k, 5k, 7.5k and 10k trajectories from the dataset (which corresponds to 10%, 25%, 50%, 75%, 100% of the total capacity of Du).\nScaling trends with varied sizes of Dp (Figure 6c). In this set of experiments, we keep the setups and the hyper-parameters exactly the same as in Table 1, except for the sizes of the preference dataset. We select the first 100, 200, 300, 400, 500, and 1000 queries from the dataset, as depicted in the figure."}, {"title": "D Disclosure of computational resources and efficiency", "content": "Throughout the experiments, we evaluate HPL as well as other baseline methods with workstations equipped with NVIDIA RTX 4090 cards. The running time of each method for the button-press task in the Meta-World environment is presented in Figure 8."}, {"title": "E Supplimentary Experiment Results", "content": "Due to the limited space of the main text, we present additional supplementary results in this section."}, {"title": "E.1 Benchmark Results of AWAC Variants", "content": "Table 7: Normalized averaged score for AWAC variants of HPL and baseline algorithms. In the table, we use the same abbreviations for tasks as in Table 1. We report the average and the standard deviation of the performances across 10 evaluation episodes and 5 seeds, and bold the values that are within 95% of the top-performing methods among all methods except for the Oracle.\nIn Section 5.1, the results are obtained by using IQL [Kostrikov et al., 2022] as the policy optimiza- tion algorithm. However, given that IQL relies on expectile regression rather than the policy for bootstrapping, it may not fully reveal the potential shortcomings of the learned rewards. Additionally, the choice of expectile could significantly affect the outcomes. In this section, we instead implement AWAC, another offline reinforcement learning (RL) algorithm that integrates policy into bootstrap- ping, to both HPL and the baseline algorithms. This approach aims to provide a more thorough assessment of reward quality.\nThe results are listed in Table 7. Similar to HPL-IQL, HPL-AWAC demonstrates stable and consistent advantages over the baseline methods in most of the tasks. Overall, HPL-AWAC achieves the best performance on average across these three task suites."}, {"title": "E.2 Ablation on the Effect of Ensembling Reward Models", "content": "One could argue that the success of HPL can be attributed to the marginalization step (Eq (9)), which implicitly ensembles reward models to yield improved rewards for subsequent RL optimization. As revealed by previous literature, reward model ensembling does bring benefit to the credit assignment by characterizing the aleatoric uncertainty and thus facilitating active knowledge acquisition [Liang et al., 2022a] or promoting pessimism [Coste et al., 2024]. In this section, we ablate the effect of reward model ensembling by applying the ensembling trick to the MR method. Specifically, we set"}, {"title": "E.3 Ablation on Future Length k", "content": "The parameter k controls the lengths of future segments encoded into the embedding. At the extreme of k \u2192 0, HPL theoretically degenerates to MR as the conditional reward ry contains no information about the future.\nFigure 10 illustrates the performance of HPL across various values of k for all tasks in Meta-World. As k increases from zero, the performance generally improves, supporting the efficacy of future conditioning. However, beyond a certain threshold, further increases in k lead to performance declines and fluctuations. This phenomenon may be attributed to the incapability of modeling excessively long trajectory segments with the VAE structure."}, {"title": "E.4 Scaling with Dataset Sizes", "content": "In this section, we investigate the performance of HPL as well as MR with various dataset sizes. We vary the sizes of the preference dataset Dp and the unlabeled dataset Du, and plot the curve of the performances in Figure 11 and Figure 12, respectively. While both methods exhibit an upward trend in success rates as the dataset sizes |Dp| and |Du| grow, HPL outperforms MR in almost every configuration. These experiments collectively confirm the superiority and scalability of HPL."}]}