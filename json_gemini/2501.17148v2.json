{"title": "AXBENCH: Steering LLMs? Even Simple Baselines\nOutperform Sparse Autoencoders", "authors": ["Zhengxuan Wu", "Aryaman Arora", "Atticus Geiger", "Zheng Wang", "Jing Huang", "Dan Jurafsky", "Christopher D. Manning", "Christopher Potts"], "abstract": "Fine-grained steering of language model outputs\nis essential for safety and reliability. Prompting\nand finetuning are widely used to achieve these\ngoals, but interpretability researchers have pro-\nposed a variety of representation-based techniques\nas well, including sparse autoencoders (SAEs),\nlinear artificial tomography, supervised steering\nvectors, linear probes, and representation finetun-\ning. At present, there is no benchmark for mak-\ning direct comparisons between these proposals.\nTherefore, we introduce AXBENCH, a large-scale\nbenchmark for steering and concept detection, and\nreport experiments on Gemma-2-2B and 9B. For\nsteering, we find that prompting outperforms all\nexisting methods, followed by finetuning. For\nconcept detection, representation-based methods\nsuch as difference-in-means, perform the best. On\nboth evaluations, SAEs are not competitive. We\nintroduce a novel weakly-supervised representa-\ntional method (Rank-1 Representation Finetun-\ning; ReFT-r1), which is competitive on both tasks\nwhile providing the interpretability advantages\nthat prompting lacks. Along with AXBENCH, we\ntrain and publicly release SAE-scale feature dic-\ntionaries for ReFT-r1 and DiffMean.", "sections": [{"title": "1. Introduction", "content": "In order to be useful, language models (LMs) must fol-\nlow user instructions and be aligned to human goals and\nvalues. While prompting and finetuning are now widely\nused to instill such behaviour in LMs, both methods have\nlimitations: circumvention via jailbreaks and continued\ntraining, reliance on dataset quality, and uninterpretabil-\nity (Anwar et al., 2024). Interpretability researchers have\nthus proposed a new class of representation-based inter-\nventions for steering LMs, which hope to address these\nissues. These methods include learning steering vectors\nfrom small labelled datasets, self-supervised sparse autoen-\ncoders (SAEs), among other techniques. Since steering may\nenable lightweight and interpretable control over model out-\nputs, it has emerged as a potential alternative to finetuning\nand prompting (see \u00a72).\nUnfortunately, Pres et al. (2024); Braun et al. (2024) note\nthat existing benchmarks for steering only evaluate a few\nmethods at merely toy scales. To assess whether representa-\ntion steering is a viable alternative to existing model control\ntechniques, we need to evaluate it in a more realistic setting,\ne.g. over open-vocabulary concepts and on long-form gener-\nation, and compare it to prompting and finetuning baselines.\nIn this work, we introduce AXBENCH, a benchmark for\nevaluating LM control methods at scale using synthetic data.\nAXBENCH takes in a list of natural language descriptions\nof concepts and samples relevant training and evaluation"}, {"title": "2. Related work", "content": "Representation-based control. Interventional/causal in-\nterpretability has emerged as the dominant paradigm for\nunderstanding neural networks in the LLM era, enabling\nthe reverse-engineering of circuits underlying specific be-\nhaviours (Giulianelli et al., 2018; Vig et al., 2020; Geiger\net al., 2021; 2022; Meng et al., 2022; Chan et al., 2022;\nWang et al., 2023; Goldowsky-Dill et al., 2023; Geiger et al.,\n2024; Guerner et al., 2024; Geiger et al., 2024). An impor-\ntant assumption in much of this work is the linear represen-\ntation hypothesis, which claims that linear subspaces of rep-\nresentations in neural networks encode concepts (Mikolov\net al., 2013b; Pennington et al., 2014; Bolukbasi et al., 2016;\nElhage et al., 2022; Park et al., 2023; Nanda et al., 2023).\nIntervening on representations has thus emerged as an alter-\nnative to finetuning and prompting for LM control.\nRepresentation-based steering by adding fixed vectors to\nactivations, or clamping activations to a certain value along\nfixed directions, is one such intervention-based tool for\nmodel control (Zou et al., 2023; Li et al., 2024; Turner et al.,\n2024; Marks and Tegmark, 2024; Liu et al., 2024; van der\nWeij et al., 2024; Rimsky et al., 2024). Finetuning-based\napproaches such as ReFT (Wu et al., 2024a) enable optimi-\nsation of steering directions on a dataset. Steering vectors\nneed not be computed from labelled data; SAEs enable scal-\nable discovery of steering vectors from unlabelled data. In\nthe same class of approaches, latent adversarial training\n(Casper et al., 2024) and circuit breakers (Zou et al., 2024)\nare representation-based control methods that increase the\nadversarial robustness of LLMs.\nSparse autoencoders. Sparse autoencoders (SAEs) aim\nto enable self-supervised and thus scalable decomposition\nof the representation space into meaningful concepts (Tem-\npleton et al., 2024; Chalnev et al., 2024; Makelov, 2024;\nO'Brien et al., 2024; Gao et al., 2024). SAEs are trained\nto reconstruct LLM hidden representations in a higher-\ndimensional latent space with a sparsity penalty, based on\nthe assumption that concepts must be represented sparsely\nin order to prevent interference. The latents are then la-\nbelled with natural-language descriptions using automatic\ninterpretability pipelines (e.g. Juang et al., 2024), which can\nthen be used to identify useful latents to steer the LM.\nRecent work reports mixed results when evaluating SAEs\nfor steering; SAEs (but also several other steering methods)\nsuffer from a tradeoff between model control and capabili-\nties preservation (Mayne et al., 2024; Chalnev et al., 2024;\nDurmus et al., 2024). However, Karvonen et al. (2024)\nreport Pareto-optimal performance when using SAEs to pre-\nvent models from producing regular expressions in code.\nOverall, evaluating SAEs is an open problem, since there is\nno ground-truth data as SAEs are scaling up."}, {"title": "3. AXBENCH", "content": "AXBENCH is a benchmark which takes in a list of natural\nlanguage descriptions of concepts and synthetically gener-\nates the appropriate training and evaluation data for each\nconcept using an LLM (Figure 2). The training and eval-\nuation data consists of labelled pairs of instructions and\nresponses, where the responses are either positive examples\nexpressing the presence of the concept of interest, or nega-\ntive examples that represent the unsteered behaviour of the\nmodel (see \u00a73.1 for details).\nWe evaluate along two axes: concept detection and\nmodel steering S. For the former, we measure classifica-\ntion performance on a held-out set of labelled data. For the\nlatter, we use an LLM judge to rate steered outputs on three\nrelevant axes (see \u00a73.3).\nIn this work, we use natural language concept lists for\nGemmaScope SAEs as input, and generate training and eval-"}, {"title": "3.1. Synthetic concept dataset generation", "content": "We construct a small training dataset $D_{\\text{train}} = {\\{(x, y)\\}_{i=1}^{n/2} \\cup \\{(x, y)\\}_{i=1}^{n/2}}$ with $n$ examples\nand a concept detection evaluation dataset $D_{\\text{concept}}$ of the\nsame structure and harder examples, where $y^+$ and $y^-$ are\nbinary labels indicating whether the concept $c$ is present.\n$\\text{ }$ \nThe $D_{\\text{train}}$ contains $n$ pairs of instructions and responses and\n$D_{\\text{concept}}$ contains $m$ such pairs, generated according to the\npipeline in Figure 2. We query gpt-4o-mini-2024-07-18\nto generate the data; the prompts used in this pipeline are\npresented in Appendix I.2. Generating the data requires the\nfollowing steps (note that only the evaluation set includes\nhard negatives):\n1.  Genre labelling & seed instructions: We consider\nthree genres: text, code, and math. We prompt the"}, {"title": "3.2. Concept detection", "content": "Past work in LM interpretability trained probes (Conneau\net al., 2018; Hewitt and Manning, 2019; Belinkov et al.,\n2017) to measure to what extent LM representations encode\nproperties of interest, e.g. linguistic features. In recent years,\nthe goal of concept detection has broadened to the open-\nvocabulary setting, with unsupervised methods becoming\nmore common (Bills et al., 2023; Huben et al., 2024; Choi\net al., 2024).\nTask description. Formally, given a Transformer-based\nLM with a hidden dimension size of d, we define a concept\nclassifier as a parameterized function $\\Psi_{\\text{Detect}}$ that maps a\nmodel representation $h \\in R^d$ into a binary label $\\hat{y}$ indicat-\ning the relative presence of a concept:\n$$\\Psi_{\\text{Detect}}(h) = \\hat{y} \\in R^1$$\nwhere $f$ is any function, e.g. a neural network.\nEvaluation dataset. To evaluate a concept classifier, we\nmeasure how accurately it can predict ground-truth labals\non the labelled evaluation set from $D_{\\text{concept}}$ (see \u00a73.1).\nEvaluation metrics. Since our labels are at the sequence-\nlevel, we need to aggregate token-label scores from $\\Psi$ to\nevaluate it. Given a sequence of token representations $h' =$\n$[h_1, h_2, ..., h_n]$ with n tokens at layer $l \\in [1, m]$, we max-\npool the detection scores to get a sequence-level prediction:\n$$\\hat{y}_{Detect} = \\max(\\Psi_{Detect}(h'))$$\nWe then normalize $\\hat{y}_{Detect}$ between [0, 1] by min-max nor-\nmalisation over the evaluation dataset for each concept. The\npredicted score represents how strongly a concept is present\nin a sequence, which we can compare to the true label."}, {"title": "3.3. S Model steering", "content": "Steering has emerged as a potential alternative to existing\nmodel-control methods (e.g. finetuning and prompting) and\na practical application of various interpretability methods\n(see \u00a72). Unlike concept detection, model steering assesses\ncausal efficacy in controlling model behaviour.\nPrevious evaluation benchmarks for steering are not general-\npurpose; they often rely on a very limited set of tasks (Zou\net al., 2023; Makelov, 2024) or condition generation on a\nfixed prefix (Chalnev et al., 2024). To the best of our knowl-\nedge, we are the first to evaluate model steering methods in\nthe open-vocabulary setting at scale.\nTask description. Given a prompt x, the model's original\ngeneration can be written as $\\hat{y} = LM(x)$. We produce\nthe model's counterfactual generation conditioned on the\nconcept-based intervention $\\Phi_{\\text{Steer}}(h)$:\n$$\\hat{y}_{Steer} = LM(x, h \\leftarrow \\Phi_{\\text{Steer}}(h))$$\nwhere $h \\leftarrow \\Phi_{\\text{Steer}}(h)$ is an in-place representation modifi-\ncation. We use the open-source intervention library pyvene\nto perform such interventions on PyTorch implementations\nof models (Wu et al., 2024b).\nEvaluation dataset. We evaluate these steering methods\nin the instruction-following setting, where we sample in-\nstructions from Alpaca-Eval (Li et al., 2023) and prompt\nthe LM to generate a response while intervening on its for-\nward pass in-place using one of the steering methods.\nEvaluation metrics. For the intervened model generation,\nwe evaluate $\\hat{y}_{steer}$ based on the harmonic mean of the fol-\nlowing scores, each of which the LLM rates using a discrete\nscore of 0, 1, or 2:\n1.\nConcept score represents how well the concept is in-\ncorporated into the response.\n2.\nInstruct score represents how well the response is\nrelated to the instruction.\n3.\nFluency score represents how fluent the response is.\nSince we compute the harmonic mean, the overall score also\nranges from 0 to 2, but heavily penalises poor performance\non any of these three subscores. For each concept, we\nrandomly sample 10 instructions from Alpaca-Eval and\nsample continuations for each steering factor (see discussion\non steering factor in \u00a75.2). Our judge prompts with further\ndiscussion can be found in Appendix I.3."}, {"title": "4. Methods", "content": "In this section, we describe the interpretability methods we\nevaluate along with our baseline prompting and finetuning\nmethods. For each method, we label which axes it is evalu-\nated on using and S. All of our interpretability methods\nare SDLs which learn rank-1 subspaces for targeted con-"}, {"title": "5. Results", "content": "5.1. Concept detection\nFor concept detection, CONCEPT500 consists of passages of\ntext with ground-truth labels for each concept. Each method\nprovides us with token-level concept scores obtained from\nthe representation of that token at a particular layer. To\ncompute a passage-level score, we take the mean of the\ntoken-level concept scores. See Appendix L for a visualiza-\ntion of token-level concept scores.\nROC curves. Using the passage-level scores and the\nground truth labels on the evaluation dataset, we compute\nthe ROC (rate of change) curve for each method on each\nconcept across all model settings. In Table 1, we report the\naverage area under the ROC curve (ROC AUC) for each\nmethod over all concepts. We plot the average ROC curve\nfor each method in Figure 3.\nOverall, we find that DiffMean, Probe, and ReFT-r1 are\nthe best performers with no statistically significant differ-\nence (p < 0.05) between any of them under a paired t-test.\nPrompt, SAE-A, and SSV are not far behind and signifi-\ncantly outperform the remaining methods. LAT also per-\nforms better than random. SAEs without supervised feature\nselection are thus significantly outperformed by five su-\npervised methods, all of which are much cheaper to train\nusing a limited amount of synthetic data. The remaining"}, {"title": "5.2. S Model steering", "content": "For model steering, we take concept labels from CON-\nCEPT500 and apply the (pre)trained steering methods to\nthe base model and sample generations. We score the gen-\nerations using an LM judge as described in \u00a73.3. We addi-\ntionally benchmark prompting, full-finetuning (SFT), and\ntwo parameter-efficient finetuning methods (LoReFT and\nLORA) as non-steering baselines.\nFor steering methods, we note that steering factor is an\nimportant hyperparameter. We select the optimal steering\nfactor for each method independently for every concept\nbased on which factor achieves the highest overall steering\nscore, as given by the LLM judge. Our actual steering\nmagnitude (i.e., a, as described in \u00a74) is the product of the\nsteering factor and the maximal activations aggregated over"}, {"title": "6. Discussion", "content": "Simple yet powerful baselines. While representation-\nlevel interventions have been shown to be useful in both\nenhancing model capabilities and for safety (see \u00a72), they\nfail to outperform standard prompting and finetuning base-\nlines on AXBENCH. This is sobering evidence of the current\nlimitations of steering techniques. However, our results sug-\ngest that joint learning of concept detection and steering (as\nin ReFT-r1) may be the key to advancement.\nSDL vs. SAEs. We have shown that SDL methods can\nachieve similar scalability and better performance at a lower\ncost compared to SAEs. Unlike SAEs, SDL methods re-\nquire concepts to be known a priori; however, SDLs can be\neasily augmented with new features without retraining. We\nalso note that SDLs depend on high-quality data generators,\nwhereas SAEs rely on high-quality concept discriminators.\nThese methods are not mutually exclusive and can comple-\nment each other.\nSAE concept label quality. The concept lists used\nin this paper were adapted from Neuronpedia's auto-\ninterpretability pipeline, often involving token-level con-\ncepts that do not require high-level abstractions. It would\nbe interesting to explore whether the rankings of our bench-\nmark shift as better feature labelling methods are used and\nlabels become less shallow (e.g. Choi et al., 2024)."}, {"title": "7. Conclusion", "content": "We introduced AXBENCH, a new benchmark for evaluating\nLM control methods at scale using synthetic data. To answer\nthe question in the title of this work: our evaluation shows\nthat even at SAE scale, representation steering is still far\nbehind simple prompting and finetuning baselines. Simulta-\nneously, we showed that a novel steering method, ReFT-r1,\nis capable of closing the gap to some extent; representation-\nbased steering has not yet exhausted its potential. No mat-\nter the outcome, we believe that comprehensive evaluation\nbenchmarks like AXBENCH are necessary for continued\nprogress on this problem."}, {"title": "A. Historical notes on steering", "content": "Inspired by Jurafsky and Martin (2025) and noting the sociological observations about (mechanistic) interpretability as a\nfield in Saphra and Wiegreffe (2024), we offer some historical notes on the development of steering as a field in an effort to\ndocument and properly cite where these ideas came from.\nSteering refers to applying interventions (usually adding a fixed vector) to the activation space of a neural model in order to\ncontrol its generations. Early precursors to steering noted that linear subspaces of the representation space of pretrained\nword vectors seemed to encode meaningful concepts (Mikolov et al., 2013a; Pennington et al., 2014; Bolukbasi et al., 2016).\nLarsen et al. (2016) first used the difference-in-means technique to extract visual attribute vectors from GAN discriminators\nin order to steer generator outputs; this technique was widely adopted in computer vision (White, 2016; Upchurch et al.,\n2017; Goh, 2017; Wang et al., 2019).\nIn NLP, initial work by Subramani et al. (2022) proposed steering vectors, learned to maximise the probability of some\noutput, as an alternative to expensive fine-tuning and unreliable prompt optimisation for the task of controllable text\ngeneration. Soon after, steering was also use to localise behaviours in a maze-searching RL agent (Turner et al., 2023a;b;\nMini et al., 2023). Variations on this approach (sometimes using difference-in-means or other closed-form expressions to\ncompute the vector) were adopted by researchers in mechanistic interpretability from late 2023 for AI safety (Zou et al.,\n2023; Li et al., 2024; Turner et al., 2024; Marks and Tegmark, 2024; Rimsky et al., 2024) and later as a general-purpose but\nlocalised and parameter-efficient alternative to finetuning (Wu et al., 2024a; Liu et al., 2024; van der Weij et al., 2024).\nSparse autoencoders (SAEs), a scalable technique for self-supervised rank-one linear feature discovery via dictionary\nlearning, are also increasingly used to find or learn steering vectors (Templeton et al., 2024; Chalnev et al., 2024; Makelov,\n2024; O'Brien et al., 2024)."}, {"title": "3.1. Synthetic concept dataset generation", "content": "We construct a small training dataset $D_{\\text{train}} = {\\{(x, y)\\}_{i=1}^{n/2} \\cup \\{(x, y)\\}_{i=1}^{n/2}}$ with $n$ examples\nand a concept detection evaluation dataset $D_{\\text{concept}}$ of the\nsame structure and harder examples, where $y^+$ and $y^-$ are\nbinary labels indicating whether the concept $c$ is present.\n$\\text{ }$ \nThe $D_{\\text{train}}$ contains $n$ pairs of instructions and responses and\n$D_{\\text{concept}}$ contains $m$ such pairs, generated according to the\npipeline in Figure 2. We query gpt-4o-mini-2024-07-18\nto generate the data; the prompts used in this pipeline are\npresented in Appendix I.2. Generating the data requires the\nfollowing steps (note that only the evaluation set includes\nhard negatives):\n1.  Genre labelling & seed instructions: We consider\nthree genres: text, code, and math. We prompt the"}, {"title": "3.2. Concept detection", "content": "Past work in LM interpretability trained probes (Conneau\net al., 2018; Hewitt and Manning, 2019; Belinkov et al.,\n2017) to measure to what extent LM representations encode\nproperties of interest, e.g. linguistic features. In recent years,\nthe goal of concept detection has broadened to the open-\nvocabulary setting, with unsupervised methods becoming\nmore common (Bills et al., 2023; Huben et al., 2024; Choi\net al., 2024).\nTask description. Formally, given a Transformer-based\nLM with a hidden dimension size of d, we define a concept\nclassifier as a parameterized function $\\Psi_{\\text{Detect}}$ that maps a\nmodel representation $h \\in R^d$ into a binary label $\\hat{y}$ indicat-\ning the relative presence of a concept:\n$$\\Psi_{\\text{Detect}}(h) = \\hat{y} \\in R^1$$\nwhere $f$ is any function, e.g. a neural network.\nEvaluation dataset. To evaluate a concept classifier, we\nmeasure how accurately it can predict ground-truth labals\non the labelled evaluation set from $D_{\\text{concept}}$ (see \u00a73.1).\nEvaluation metrics. Since our labels are at the sequence-\nlevel, we need to aggregate token-label scores from $\\Psi$ to\nevaluate it. Given a sequence of token representations $h' =$\n$[h_1, h_2, ..., h_n]$ with n tokens at layer $l \\in [1, m]$, we max-\npool the detection scores to get a sequence-level prediction:\n$$\\hat{y}_{Detect} = \\max(\\Psi_{Detect}(h'))$$\nWe then normalize $\\hat{y}_{Detect}$ between [0, 1] by min-max nor-\nmalisation over the evaluation dataset for each concept. The\npredicted score represents how strongly a concept is present\nin a sequence, which we can compare to the true label."}, {"title": "3.3. S Model steering", "content": "Steering has emerged as a potential alternative to existing\nmodel-control methods (e.g. finetuning and prompting) and\na practical application of various interpretability methods\n(see \u00a72). Unlike concept detection, model steering assesses\ncausal efficacy in controlling model behaviour.\nPrevious evaluation benchmarks for steering are not general-\npurpose; they often rely on a very limited set of tasks (Zou\net al., 2023; Makelov, 2024) or condition generation on a\nfixed prefix (Chalnev et al., 2024). To the best of our knowl-\nedge, we are the first to evaluate model steering methods in\nthe open-vocabulary setting at scale.\nTask description. Given a prompt x, the model's original\ngeneration can be written as $\\hat{y} = LM(x)$. We produce\nthe model's counterfactual generation conditioned on the\nconcept-based intervention $\\Phi_{\\text{Steer}}(h)$:\n$$\\hat{y}_{Steer} = LM(x, h \\leftarrow \\Phi_{\\text{Steer}}(h))$$\nwhere $h \\leftarrow \\Phi_{\\text{Steer}}(h)$ is an in-place representation modifi-\ncation. We use the open-source intervention library pyvene\nto perform such interventions on PyTorch implementations\nof models (Wu et al., 2024b).\nEvaluation dataset. We evaluate these steering methods\nin the instruction-following setting, where we sample in-\nstructions from Alpaca-Eval (Li et al., 2023) and prompt\nthe LM to generate a response while intervening on its for-\nward pass in-place using one of the steering methods.\nEvaluation metrics. For the intervened model generation,\nwe evaluate $\\hat{y}_{steer}$ based on the harmonic mean of the fol-\nlowing scores, each of which the LLM rates using a discrete\nscore of 0, 1, or 2:\n1.\nConcept score represents how well the concept is in-\ncorporated into the response.\n2.\nInstruct score represents how well the response is\nrelated to the instruction.\n3.\nFluency score represents how fluent the response is.\nSince we compute the harmonic mean, the overall score also\nranges from 0 to 2, but heavily penalises poor performance\non any of these three subscores. For each concept, we\nrandomly sample 10 instructions from Alpaca-Eval and\nsample continuations for each steering factor (see discussion\non steering factor in \u00a75.2). Our judge prompts with further\ndiscussion can be found in Appendix I.3."}, {"title": "4. Methods", "content": "In this section, we describe the interpretability methods we\nevaluate along with our baseline prompting and finetuning\nmethods. For each method, we label which axes it is evalu-\nated on using and S. All of our interpretability methods\nare SDLs which learn rank-1 subspaces for targeted con-"}, {"title": "5. Results", "content": "5.1. Concept detection\nFor concept detection, CONCEPT500 consists of passages of\ntext with ground-truth labels for each concept. Each method\nprovides us with token-level concept scores obtained from\nthe representation of that token at a particular layer. To\ncompute a passage-level score, we take the mean of the\ntoken-level concept scores. See Appendix L for a visualiza-\ntion of token-level concept scores.\nROC curves. Using the passage-level scores and the\nground truth labels on the evaluation dataset, we compute\nthe ROC (rate of change) curve for each method on each\nconcept across all model settings. In Table 1, we report the\naverage area under the ROC curve (ROC AUC) for each\nmethod over all concepts. We plot the average ROC curve\nfor each method in Figure 3.\nOverall, we find that DiffMean, Probe, and ReFT-r1 are\nthe best performers with no statistically significant differ-\nence (p < 0.05) between any of them under a paired t-test.\nPrompt, SAE-A, and SSV are not far behind and signifi-\ncantly outperform the remaining methods. LAT also per-\nforms better than random. SAEs without supervised feature\nselection are thus significantly outperformed by five su-\npervised methods, all of which are much cheaper to train\nusing a limited amount of synthetic data. The remaining"}, {"title": "5.2. S Model steering", "content": "For model steering, we take concept labels from CON-\nCEPT500 and apply the (pre)trained steering methods to\nthe base model and sample generations. We score the gen-\nerations using an LM judge as described in \u00a73.3. We addi-\ntionally benchmark prompting, full-finetuning (SFT), and\ntwo parameter-efficient finetuning methods (LoReFT and\nLORA) as non-steering baselines.\nFor steering methods, we note that steering factor is an\nimportant hyperparameter. We select the optimal steering\nfactor for each method independently for every concept\nbased on which factor achieves the highest overall steering\nscore, as given by the LLM judge. Our actual steering\nmagnitude (i.e., a, as described in \u00a74) is the product of the\nsteering factor and the maximal activations aggregated over"}, {"title": "6. Discussion", "content": "Simple yet powerful baselines. While representation-\nlevel interventions have been shown to be useful in both\nenhancing model capabilities and for safety (see \u00a72), they\nfail to outperform standard prompting and finetuning base-\nlines on AXBENCH. This is sobering evidence of the current\nlimitations of steering techniques. However, our results sug-\ngest that joint learning of concept detection and steering (as\nin ReFT-r1) may be the key to advancement.\nSDL vs. SAEs. We have shown that SDL methods can\nachieve similar scalability and better performance at a lower\ncost compared to SAEs. Unlike SAEs, SDL methods re-\nquire concepts to be known a priori; however, SDLs can be\neasily augmented with new features without retraining. We\nalso note that SDLs depend on high-quality data generators,\nwhereas SAEs rely on high-quality concept discriminators.\nThese methods are not mutually exclusive and can comple-\nment each other.\nSAE concept label quality. The concept lists used\nin this paper were adapted from Neuronpedia's auto-\ninterpretability pipeline, often involving token-level con-\ncepts that do not require high-level abstractions. It would\nbe interesting to explore whether the rankings of our bench-\nmark shift as better feature labelling methods are used and\nlabels become less shallow (e.g. Choi et al., 2024)."}, {"title": "7. Conclusion", "content": "We introduced AXBENCH, a new benchmark for evaluating\nLM control methods at scale using synthetic data. To answer\nthe question in the title of this work: our evaluation shows\nthat even at SAE scale, representation steering is still far\nbehind simple prompting and finetuning baselines. Simulta-\nneously, we showed that a novel steering method, ReFT-r1,\nis capable of closing the gap to some extent; representation-\nbased steering has not yet exhausted its potential. No mat-\nter the outcome, we believe that comprehensive evaluation\nbenchmarks like AXBENCH are necessary for continued\nprogress on this problem."}, {"title": "D.3. Teleporting between subspaces across models through affine transformations.", "content": "We explore whether structural equivalence in subspaces exists across models. Previous works have analyzed feature\nuniversality in SAEs but have been limited to a small set of features (Lan et al., 2024). Given that our CONCEPT16K dataset\ncontains two sets of concepts for Gemma-2-2B and Gemma-2-9B, we first train ReFT-r1 on both models separately, obtaining\n$W_{ReFT-r1}^{2B}$ and $W_{ReFT-r1}^{9B}$. Next, we perform a cross-fitting experiment, training ReFT-r1 on Gemma-2-2B with concepts from\nGemma-2-9B, resulting in $W_{ReFT-r1}^{9B \\rightarrow 2B}$, and vice versa for $W_{ReFT-r1}^{2B \\rightarrow 9B}$. Thus, $W_{ReFT-r1}^{9B \\rightarrow 2B}$ and $W_{ReFT-r1}^{2B \\rightarrow 9B}$ represent two sets of subspaces\nfrom different models that correspond to the same set of concepts.\nWe then study whether a transformation can map between these two sets of subspaces:\n$$W_{ReFT-r1}^{9B \\rightarrow 2B} = \\Phi_{Transformation} (W_{ReFT-r1}^{2B}),$$\nwhere $\\Phi_{Transformation}$ is parameterized by a linear layer with a bias (i.e., an affine transformation). Similarly, $\\Phi_{Transformation}^{2B \\rightarrow 9B}$ is\ntrained by reversing the direction. During training, we exclude concepts from CONCEPT500, and evaluate the transformation\non CONCEPT500 at test time by generating subspaces. We follow our evaluation paradigm in AXBENCH to assess concept\ndetection and model steering.\nOur evaluation results on CONCEPT500 are presented in Table 6 and Table 7. Surprisingly, the affine transformation\nperforms well in both directions (from 2B \u2192 9B and 9B \u2192 2B), with little to no change in concept detection performance.\nWhile performance drops for model steering, it still outperforms other methods, including fine-tuning. Figure 12 and\nFigure 13 visualize the transformations using the first two PCA dimensions. PCA is preferred over UMAP in this context\nbecause it is sensitive to rotation."}, {"title": "4.1. Evaluation", "content": "Datasets. We synthetically generate training and valida-\ntion datasets (see \u00a73.1) for 500 concepts, which we release\nas CONCEPT500. The concepts are sampled from the Neu-\nronpedia SAE concept list for GemmaScope. For each\nconcept, we include 144 examples for training and \u224872\nsamples for"}]}