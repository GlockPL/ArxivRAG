{"title": "Everywhere Attack: Attacking Locally and Globally to Boost Targeted Transferability", "authors": ["Hui Zeng", "Sanshuai Cui", "Biwei Chen", "Anjie Peng"], "abstract": "Adversarial examples' (AE) transferability refers to the phenomenon that AEs crafted with one surrogate model can also fool other models. Notwithstanding remarkable progress in untargeted transferability, its targeted counterpart remains challenging. This paper proposes an everywhere scheme to boost targeted transferability. Our idea is to attack a victim image both globally and locally. We aim to optimize 'an army of targets' in every local image region instead of the previous works that optimize a high-confidence target in the image. Specifically, we split a victim image into non-overlap blocks and jointly mount a targeted attack on each block. Such a strategy mitigates transfer failures caused by attention inconsistency between surrogate and victim models and thus results in stronger transferability. Our approach is method-agnostic, which means it can be easily combined with existing transferable attacks for even higher transfer-ability. Extensive experiments on ImageNet demonstrate that the proposed approach universally improves the state-of-the-art targeted attacks by a clear margin, e.g., the transferability of the widely adopted Logit attack can be improved by 28.8%~300%. We also evaluate the crafted AEs on a real-world platform: Google Cloud Vision. Results further support the superiority of the proposed method.", "sections": [{"title": "Introduction", "content": "Adversarial example (AE) (Szegedy et al. 2014) is a powerful tool for uncovering potential vulnerability of deep neural networks (DNN) before their deployment in security-sensitive applications (Madry et al. 2018). An exciting property of the AE is that AEs crafted against one model have a non-negligible chance to fool unseen victim models, a.k.a., transferability. Numerous transferable attacks have emerged recently, e.g., stabilizing the optimization direction (Dong et al. 2018; Lin et al. 2020; Wan, Ye, and Huang 2021) or diversifying inputs and surrogates (Xie et al. 2019; Dong et al. 2019; Wang et al. 2021; Li et al. 2020b; Fan et al. 2023). Despite extensive studies constantly refreshing transferability under the untargeted mode, targeted transferability is much more daunting since it requires unknown models outputting a specific label (Liu et al. 2017). To bridge the gulf, tailored schemes for improving the transferability of targeted attacks have been proposed. For instance, resource-intensive attacks seek extra, target-specific classifiers (Inkawhich et al. 2020) or generators (Naseer et al. 2021; Yang et al. 2022) to optimize adversarial perturbations. Other researchers find that integrating novel loss functions with conventional simple iterative attacks can also enhance targeted transferability (Li et al. 2020a; Zhao, Liu, and Larson 2021; Zeng et al. 2023; Weng et al. 2023).\nDespite the recent progress of targeted attacks, the reported transferability is still unsatisfactory. Unlike the attention regions (to the ground truth class) that are critical to untargeted attacks, which tend to overlap among diverse models (Wu et al. 2020), the target class-related attention regions vary significantly across different models (refer to Figure 2), resulting in limited targeted transferability. This paper proposes an everywhere scheme to alleviate the attention mismatch dilemma for targeted attacks. Our idea is illustrated in Figure 1: Bajie (the pig) is expected to be attacked as WuKong (the monkey)\u00b9. In contrast to conventional attacks that try to plant a high-confidence Wukong into the victim image, the proposed everywhere attack simultaneously plants an army of Wukong in every local region of the victim image, with the hope that at least one Wukong falls into the"}, {"title": "Related Work", "content": "An adversarial attack typically has two modes: targeted and untargeted. A targeted attack misguides a classification model to produce an adversary-desired label, whereas an untargeted attack only fools it for misclassification. Targeted attacks are strictly more difficult yet pose a more severe threat to the classification model. In this section, we briefly review conventional tricks to improve untargeted transferability and then discuss tailored schemes for targeted transferability."}, {"title": "Transferable Untargeted Attacks", "content": "A plethora of transferable attacks is built up on the well-known iterative fast gradient sign method (IFGSM) (Kurakin, Goodfellow, and Bengio 2016), which can be formulated as:\n\n$I_{n+1} = Clip_{\\epsilon,0}(I_{n} + \\epsilon sign(\\nabla_{I_{n}} J(I_{n}, y_o)))$\n\nwhere $I_n$ denotes the gradient of the loss function $J()$ with respect to $I_m$, $y_o$ is the original label, and $\\epsilon$ is the perturbation budget. Researchers have proposed a variety of improved algorithms over IFGSM, e.g., the momentum iterative method (MI) (Dong et al. 2018) integrates a momentum term into the iterative process. Diverse inputs method (DI) (Xie et al. 2019) and translation-invariant method (TI) (Dong et al. 2019) leverage data augmentation to prevent attacks from overfitting a specific source model. Moreover, these enhanced schemes can be integrated for better transferability, e.g., Translation Invariant Momentum Diverse Inputs IFGSM (TMDI)."}, {"title": "Transferable Targeted Attacks", "content": "In addition to the difficulties untargeted attacks face, targeted attacks have their own challenges, such as gradient vanishing (Li et al. 2020a; Zhao, Liu, and Larson 2021) and the restoring effect (Li et al. 2020a; Zeng et al. 2023). Hence, tailored considerations are necessary for transferable targeted attacks. Existing efforts to boost targeted transferability can be divided into two families: resource-intensive methods and simple-gradient methods."}, {"title": "Resource-intensive attacks", "content": "Resource-intensive attacks require training auxiliary target-class-specific classifiers or generators on additional data. In the feature distribution attack (Inkawhich et al. 2020), a light-weight, one-versus-all classifier is trained for each target class $y_t$ at each specific layer to predict the probability that a feature map is from $y_t$. Transferable targeted perturbation (TTP) (Naseer et al. 2021) trains an input-adaptive generator to synthesize targeted perturbation and achieves state-of-the-art transferability. However, a dedicated generator must be learned for every (source model, target class) pair in TTP. Such a limitation is partially addressed by training a conditional generator (Mirza and Osindero 2014) to target multi-class simultaneously (C-GSP, LFAA) (Yang et al. 2022; Wang, Shi, and Wang 2023). However, the number of targeted labels a single generator can cover is limited due to its limited representative capacity. As a consequence, when the number of targeted classes is enormous, e.g., ImageNet, the required training time and storage are still prohibitive."}, {"title": "Simple-gradient methods", "content": "On the other hand, simple-gradient methods only iteratively optimize a victim image and thus have received more attention. Po+Trip attack (Li et al. 2020a) replaces traditional cross-entropy (CE) loss with the Poincare distance loss to address the decreasing gradient problem and introduces a triplet loss to push the attacked image away from $y_o$. Logit attack (Zhao, Liu, and Larson 2021) uses the Logit loss in the attack and reports better transferability than the CE loss.\n\n$L_{Logit} = -l_{t}(I')$\n\nwhere $l_t (\u00b7)$ denotes the logit output with respect to $y_t$. Moreover, Zhao, Liu and Larson (2021) point out that targeted attacks need significantly more iterations to converge than untargeted ones do. Similarly, Weng et al. (2023) (Margin) point out that the vanishing of the logit margin between the targeted and untargeted classes limits targeted transferability. Thus, they downscale the logits with a temperature factor to address the saturation issue and achieve improved transferability. The object-based diverse input method (ODI) (Byun et al. 2022) proposes diversifying the input image in a 3D object manner to avoid overfitting the source model and achieve improved targeted transferability. The high-confidence label suppressing method (SupHigh) (Zeng et al. 2023) argues that not only the original label $y_o$, but other high-confidence labels should also be suppressed for better transferability. Such an idea can be realized by updating AEs according to the following direction:\n\n$\\nabla(\\l_t(I') \u2013 \\beta_1l_o(I')) \u2013 \\beta_2(\\nabla_{\\perp_1(\\cdot)} ol_{high-conf,i}(I'))$\n\nwhere $I$ denotes retaining only the component perpendicular to the first item. Here, the first term is used to enhance the confidence of $y_t$ and suppress $y_o$ simultaneously, the second term suppresses other high-confidence labels. Based on the observation that highly universal adversarial perturbations tend to be more transferable, the self-universality method (SU) (Wei et al. 2023) introduces a feature similarity loss to encourage the adversarial perturbation to be self-universal. The clean feature mixup method (CFM) (Byun et al. 2023) borrowed the idea from Admix (Wang et al. 2021) to intentionally introduce competitor noises during optimization,"}, {"title": "The Proposed Method", "content": "This section revisits a common cause for targeted transfer failure and details the proposed everywhere attack, which can effectively alleviate the attention mismatch issue."}, {"title": "Motivation", "content": "In a targeted attack, the adversary attempts to plant a quasi-imperceptible target object (or objects) into a clean image. Due to the attentional mechanism of DNNs, such a planting often focuses on specific image regions. To achieve transferable attacks across victim models, one may expect victim models to center on regions similar to the surrogate model in identifying the target object (or objects). In fact, this assumption is difficult to satisfy in a targeted attack. To illustrate this dilemma, we examine the attentional maps of an AE on different models. The attentional maps are computed with GradCAM (Selvaraju et al. 2017). The AE shown in Figure 2(a) is crafted with the vanilla CE attack, the surrogate model is VGG16bn (VGG16) (Simonyan and Zisserman 2015), and the target label is 'marmoset'. As can be observed from Figure 2(b), the attack focuses on the lower area of the flower crown. One can imagine that the adversary has planted a 'marmoset' in this region. However, victim models pay attention to strikingly different regions in recognizing a 'marmoset'. For example, ResNet50 (Res50) (He et al. 2016) tries to find a 'marmoset' from the lower right area of the image (Figure 2(d)). As a result, such a transfer attack fails on all three victim models.\nOne possible way to address the abovementioned challenge is to draw the victim model's attention to the attacked region. However, this is not easy because the adversary in the transfer attack setting cannot access the victim model. Another solution is to craft a target in the victim model's attentional region. Since the victim model's attention is unknown in advance, an intuitive strategy is to craft a bunch of targets in every region that the victim model may pay attention to. Such a conceptually simple idea motivates the proposed everywhere attack."}, {"title": "Everywhere Attack", "content": "Figure 3 gives an overview of the proposed everywhere attack. To synthesize targets in multiple regions of the image, we split a victim image into M \u00d7 M non-overlap blocks. Then, we randomly sample N blocks from the image. For each sampled block, we pad the remaining area with the mean value of the dataset (which will be normalized to zero) and get a 'local' image. Concatenating these 'local' images with the global image delivers N+1 images to attack. Finally, we simultaneously mount a targeted attack on these N+1 images toward the same target (e.g., 'marmoset'). In this manner, we expect every block of the obtained AE independently possesses attack capability. The parameter N can be used to balance the attack power and the computational efficiency. Note that the everywhere attack degenerates to a baseline attack when N = 0. Algorithm 1 summarizes the procedure of integrating the proposed everywhere scheme with the CE attack, where DI, TI, and MI are conventional transferability-enhanced methods."}, {"title": "Experimental Results", "content": "In this section, we show the efficiency of the proposed everywhere attack scheme by integrating it into six iterative attacks: CE, Logit (Zhao, Liu, and Larson 2021), Margin (Weng et al. 2023), SupHigh (Zeng et al. 2023), SU (Wei et al. 2023), CFM (Byun et al. 2023) on various transfer scenarios. Since more recent targeted attacks have dominated the Po+Trip attack (Li et al. 2020a), we omit its results for brevity. All the iterative schemes start with the TMDI attack. Then, we contrast everywhere attack with two generative attacks: TTP (Naseer et al. 2021) and C-GSP (Yang et al. 2022). Next, the proposed method is used for crafting Data-free Targeted Universal Adversarial Perturbation (DTUAP) (Moosavi-Dezfooli et al. 2017; Zhao, Liu, and Larson 2021), from which our philosophy can be further illustrated. Finally, the crafted AEs are further evaluated using a real-world image recognition system: Google Cloud Vision. The supplementary material provides the ablation study on our key hyper-parameters."}, {"title": "Experimental Settings", "content": "Dataset. Following recent work on targeted attacks, our experiments are conducted on the ImageNet-compatible dataset comprised of 1000 images. All these images are with the size of 299 \u00d7 299 pixels and are stored in PNG format.\nNetworks. Since transferring across different architectures is more demanding, we choose four pretrained models of diverse architectures: Inc-v3, Res50, Den121, and VGG16 as the surrogates. These surrogates and a transformer-based model, Swin (Liu et al. 2021), evaluate AEs' transferability.\nParameters. For all attacks, the perturbations are restricted by L\u221e norm with \u20ac = 16 (The results under lower budgets are provided in the supplementary material), and the step size is set to 2. The total iteration number T is set to 200 to balance speed and convergence. The number of partitions for each dimension M is set to 4, and the number of samples N is set to 9."}, {"title": "Normal surrogates", "content": "Table 2 reports the targeted transferability (random-target) across different models. The proposed everywhere scheme boosts all the baseline attacks by a clear margin. Taking the popular Logit attack as a baseline, the average success rate has been improved by 28.8% (49.7% vs. 38.6%) ~ 300% (8.4% vs. 2.1%). Further analysis can provide more insights into the proposed method. First, the weaker the baseline, the more significant the improvement. Hence, the upturn is particularly salient for the CE attack. For example, when VGG16 was the surrogate model, the average success rate of the CE attack increases from 0.2% to 10.2%. Second, the more challenging the transfer scenario is, the more significant the improvement brought by the proposed everywhere scheme. For example, the introduced improvement in the 'Res50 Swin' scenario is much more salient than that in the 'Res50 Dense121', which makes the proposed method even more promising with the popularity of transformer-based networks.\nAs done in previous works (Zhao, Liu, and Larson 2021; Zeng et al. 2023), we also conduct a worst-case transfer experiment in which the target labels are always the least likely ones. Table 3 compares different attacks: the improvement from the proposed everywhere scheme is even more remarkable than the random-target scenario. Taking the Logit attack as the baseline again, the average success rate increases by 39.9% (36.1% vs. 25.8%) when Res50 is the surrogate, and it more than doubles for other surrogates."}, {"title": "Robust surrogates", "content": "Leveraging a slightly robust (adversarially trained) surrogate is accepted as an efficient way to craft transferable targeted AEs (Springer, Mitchell, and Kenyon 2021). We are interested in how the proposed everywhere scheme can improve the baselines when robust models are used as surrogates. Specifically, AEs are crafted with adversarially trained models Res18adv and Res50adv and transferred to the same victims used in the last section except Res50. Both models are trained with AEs under L2 = 0.01 budget. Note that there is no architectural overlap between the source and target models. Table 4 presents the targeted transferability in this case. Even though AEs crafted by robust models have shown significantly stronger transferability than those crafted with normal surrogates, the proposed everywhere scheme is still helpful, especially when the transformer-based model Swin is the victim. Taking the Logit attack for example, the targeted success rate is doubled in the 'Res18adv Swin' scenario (25.7% vs. 13.1%) and improved by more than a half in the 'Res50adv Swin' scenario (41.6% vs. 23.2%)."}, {"title": "Iterative vs. generative attacks", "content": "Next, we compare the proposed everywhere attack with the state-of-the-art generative attacks, TTP and C-GSP. As mentioned before, TTP entails training a generator for each target label and each source model. That means 4 \u00d7 1000 generators are required to perform the random or most difficult-target attack, which is computationally prohibitive. Alternatively, we follow the '10-Targets (all source)' setting of (Naseer et al. 2021) and use ten author-released generators (Res50 being the discriminator during training) to generate"}, {"title": "Data-free Targeted UAP", "content": "DTUAP is optimized from a random image and can drive multiple clean images into a given class $y_t$. Due to its data-free nature, DTUAP is a powerful tool for uncovering the intrinsic features of the model of interest. Following (Zhao, Liu, and Larson 2021), we use a mean image (all entrances of which equal 0.5) as the starting point and mount a targeted attack to obtain a DTUAP with CE and Logit attacks (\u20ac=16). Then, the obtained DTUAP is applied to all 1000 images in our dataset. Table 6 reports the success rates averaged over 100 classes ($y_t$ = 0 : 99). It is observed that the proposed everywhere scheme yields more transferable UAPs across input images compared with baselines. For example, with the Logit+everywhere scheme, the DTUAPs crafted on the VGG16 model can successfully drive seventy percent of the images into a specified class.\nTo provide a more intuitive explanation of the proposed everywhere scheme, we depict several DTUAP samples in Figure 4. Since features learned by the robust models are more semantically aligned, here we use Res50adv to craft"}, {"title": "Conclusion", "content": "The discriminative regions of a target class on victim models are dramatically different from that on the surrogate, which severely constrains the targeted transferability of AEs. To address this challenge, we propose the everywhere attack, which optimizes an army of target objects in every local image region that victim models may pay attention to and thus reduces the transfer failures caused by attention mismatch. Extensive experiments demonstrate that the proposed method can universally boost the transferability of existing targeted attacks. It is our hope that the idea of increasing the target quantity opens a new door to boosting targeted transferability for the community."}, {"title": "Supplementary Material", "content": "The supplementary document consists of four parts of content: A) Ablation studies on M and N; B) Attacking transformer-based models; C) A theoretical analysis of the everywhere scheme; and D) Adversarial examples (AE) on Google Cloud Vision."}, {"title": "Ablation study", "content": "1) Influence of the number of samples N. N indicates how many local blocks are sampled (out of M\u00b2) to attack in each iteration. A small N may cause an underattack in each iteration and need more iterations to converge, whereas a large N consumes more memory. Baseline+everywhere attack reduces to the baseline attack when N = 0.\nWe study the influence of N of the proposed Logit+everywhere attack in the random-target scenario. The reported attack success rates are averaged over four victims, e.g., Res50, Dense121, VGG16, and Swin when the surrogate is Inc-v3. The number of partitions M for each dimension is fixed as 4; thus, N varies from 0 to 16. As can be observed from Figure 1(a), the average success rates grow steadily at the beginning and tend to saturate after N\u2265 10. In our study, we set N = 9 to balance memory consumption and attack ability.\n2) Influence of the number of partitions M for each dimension. Next, we fix N = 9 and let M vary from 3 to 6 (Note M\u00b2 \u2265 N). Smaller M indicates larger size of the local images (before padding) and M = 1 means attacking the global image only. On the other hand, larger M indicates smaller local images and more attack iterations may be required to converge. To avoid the study overwhelming, we set the number of iterations T = 200 in all cases.\nFigure 1(b) shows the average success rates of different surrogates as functions of M. It can be observed that the attack ability of the proposed method is insensitive to M. The only exception is M = 3, in which the lack of randomness leads to inferior transferability. In our study, we set M 4 for all attacks and in all scenarios for simplicity."}, {"title": "Attacking transformers", "content": "Table 1 reports the targeted transferability against three transformer-based models, vit_b_16 (Dosovitskiy et al. 2021), pit_b_24 (Heo et al. 2021), and visformer (Chen et al. 2021), in the random-target scenario. Compared to the results on CNNs (Table 2 of the paper), the improvement introduced by everywhere attack is more remarkable when the victim is a transformer. Taking the Logit attack as a baseline, the average success rate has been improved by 66.7% (1.0% vs. 0.6%) ~ 175% (7.7% vs. 2.8%). We speculate that this is because the blockwise attack strategy in our method is more consistent with the way the transformer understands the image.\nAn interesting observation is that vit_b_16 and pit_b_24 are much more resilient under attack than visformer, which deserves future study."}, {"title": "How can everywhere improve transferability?", "content": "Besides the experimental evidence of the power of the proposed everywhere attack, a theoretical analysis of it is provided in the following.\n1) Everywhere attack optimizes an army of targets in different regions of the victim image, which can mitigate potential failures caused by the attention mismatch between surrogate and target models.\n2) Traditional methods synthesize image-level target-related features in crafting AEs. To a great extent, their attack ability relies on complicated, large-scale interactions between different image regions, which have been proven to be negatively correlated to adversarial transferability (Wang et al. 2021). In contrast, the proposed everywhere attack focuses on local features and ignores those fragile large-scale interactions. Thus, stronger transferability is expected."}, {"title": "Adversarial examples on Google Cloud Vision", "content": "Here, we provide a few examples for the paper's 'Fooling Google Cloud Vision' section. The left column of Figure 2 shows AEs crafted with the CFM attack, which only succeeds in the second case ('strawberry'\u2192'tench'). The results of the proposed CFM+everywhere attack are shown in the right column, where all AEs are predicted as the adversary-desired classes with high confidence by the Google Cloud Vision API. For example, in the first case, the API predicts our crafted image as 'American lobster' with a confidence of 0.89."}]}