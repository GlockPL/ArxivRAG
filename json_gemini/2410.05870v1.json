{"title": "Heuristics for Partially Observable Stochastic Contingent Planning", "authors": ["Guy Shani"], "abstract": "Acting to complete tasks in stochastic partially observable domains is an important problem in artificial intelligence, and is often formulated as a goal-based POMDP. Goal-based POMDPs can be solved using the RTDP-BEL algorithm, that operates by running forward trajectories from the initial belief to the goal. These trajectories can be guided by a heuristic, and more accurate heuristics can result in significantly faster convergence. In this paper, we develop a heuristic function that leverages the structured representation of domain models. We compute, in a relaxed space, a plan to achieve the goal, while taking into account the value of information, as well as the stochastic effects. We provide experiments showing that while our heuristic is slower to compute, it requires an order of magnitude less trajectories before convergence. Overall, it thus speeds up RTDP-BEL, particularly in problems where significant information gathering is needed.", "sections": [{"title": "Introduction", "content": "Autonomous agents acting to achieve their goals in stochastic environments often face partial observability, where important information for completing the task is hidden, but can be sensed. The agent must consider the value of information, that is, whether it is better, in terms of expected cost, to invest effort in sensing for the hidden information, or to try and reach the goal without the information.\nConsider, for example, a robot navigating in an uncertain environment, where the ground may be unstable. The robot can either operate a sensor that examines the ground to check its stability, later allowing it to move rapidly only over stable areas, or move slowly and carefully without operating the sensor first. The choice as to whether it is cost effective to use the sensor depends on the probability of unstable areas, the time it takes for the sensor to scan the ground, and the cost of moving through possibly unstable areas. This tradeoff is known as the value of information.\nPartially observable Markov decision processes (POMDPs) [Sondik, 1971] are a mathematical model designed for properly estimating the value of information over long planning horizons in such environments. Goal-POMDPS [Geffner and Bonet, 1998] are a subclass where the agent must achieve a goal, upon which the execution terminates.\nPOMDPs can be either specified using a \"flat\" representation, where each state is a unique entity, or using a factored representation, where a state is composed of state variables. We focus here on a particular factored representation using an extension of the PDDL (Planning Domain Description Language) representation which is popular in the automated planning literature [Haslum et al., 2019]. PDDL defines actions by preconditions and effects, allowing for a compact representation of complex problems.\nPOMDP solvers often use the concept of a belief state, a distribution over the currently possible world states, and compute a value function, mapping a belief state b to the expected cost to the goal from that belief. The RTDP-BEL (Real-Time Dynamic Programming in Belief Space) algorithm [Bonet and Geffner, 2009] computes a value function by running trajectories in belief space from the initial belief to a goal. It depends on a heuristic to guide the trajectories"}, {"title": "Background", "content": "A goal Markov decision process (goal-MDP) [Bellman, 1957; Bertsekas, 2012] is a tuple (S, So, A, tr, G, C) where S is a state space, A is an action, tr : S \u00d7 A \u00d7 S \u2192 [0, 1] is a transition function. tr(s, a, s') is the probability of executing action a at state s and reaching state s'. So \u2286 S is a set of possible initial states, and G \u2286 S is a set of goal states, that the agent must reach. C : S \u00d7 A \u2192 R is a cost function. C(s, a) is the cost for applying action a at state s. In goal-MDPs for s \u2208 G, C(s, a) = 0 and C(s, a) > 0 otherwise.\nA solution to a goal MDP is a policy \u03c0 : S \u2192 A that assigns an action to every state. One can create a policy by computing a value function V : S \u2192 R assigning a value for each state. This value function is optimal if V (s) is the minimal expected cost of reaching a goal state from s. The Bellman backup:\n$\\displaystyle V(s) \\leftarrow  min_{a\\in A} C(s, a) + \\sum_{s' \\in S} tr(s, a, s')V(s') $\ncan be used to iteratively update the value function. Value iteration applies the Bellman update over all states repeatedly, and converges to the optimal value function.\nReal Time Dynamic Programming (RTDP) [Barto et al., 1995] applies the Bellman update only along trajectories that start at a state in So and terminate at a state in G. Thus, RTDP is able to focus on states that are visited along an optimal path to the goal, avoiding repeated Bellman backups over other states, that are less important. This allows RTDP to converge much faster than value iteration in many domains.\nA goal-POMDP is a tuple (S, So, bo, A, tr, G, C, \\Omega, O) where (S, So, A, tr, G, C) is a goal-MDP, known as the underlying MDP. bo is a distribution over the possible initial states in So, \\Omega is a set of possible observations, and O(a, s, o) is the probability of observing o after applying action a and reaching state s. In this paper we focus on deterministic observations, and hence, O(a, s, o) \u2208 [0, 1]. This is not truly a limitation, because every POMDP with stochastic ob-servations can be translated into a deterministic observation POMDP, by adding a state variable whose value changes stochastically, yet observed deterministically. Consider for example a noisy wall detection sensor. Instead of observing a wall with stochastic errors, we can add a red light to the sensor, which serves as the state variable. The red light is stochastically lit by the sensor when there the sensor detects a wall, but the agent observes whether the light is on deterministically.\nA belief state is a distribution over states, where b(s) is the probability of state s. Given a belief b an action a and an observation o one can compute the new belief state bo using:\n$\\displaystyle b^{o}(s') = \\begin{cases}   \\kappa \\cdot  \\sum_{s}b(s)tr(s, a, s') : O(a, s', o) = 1 \\\\   0 : O(a, s', o) = 0  \\end{cases}$"}, {"title": "Heuristics In Belief Space", "content": "We now describe our main contribution a heuristic in belief space that takes into account the need for sensing actions. Most, if not all, previous heuristics focus on computing a heuristic for each state, and then aggregating these heuristic values. We, however, suggest a method that directly computes a plan in a relaxed belief space.\nOur methods currently assume unit costs. Extensions of our heuristics to non-unit costs is left for future research."}, {"title": "State-based Heuristics", "content": "Initializing a heuristic optimistic bound for POMDPs was previously suggested. Perhaps the most popular methods are based on the value function for the underlying MDP [Hauskrecht, 2000]. Given a computed value function VMDP one can define the heuristic value for a belief state by either focusing on the most likely state: SML = argmaxs b(s), hML(b) = V(SML), or the QMDP heuristic that uses the weighted sum: hQMDP(b) = \u2211s b(s)V(s). More elaborate attempts, such as FIB [Hauskrecht, 2000], did not show a substantial improvement over QMDP.\nIn the planning community, there are many possible heuristics for estimating the cost of reaching the goal. A popular choice is to compute distances in a relaxed problem, known as delete-relaxation [Hoffmann and Nebel, 2001]. In delete-relaxation we ignore negative facts in the effects of actions. Thus, once a fact is added to a state, no action can remove it.\nWe can hence compute, for a given state, layers of positive facts iteratively (Algorithm 2). The first layer Fo contains all the positive facts that hold in the input state s (line 2). Then, we apply all actions that can be executed at Fo (line 7), and their positive effects are added to the next layer F\u2081 (line 8). This is continued until no new facts can be added, or until all goal facts appear in Fi (line 9). Then, we can compute a heuristic estimate. For example, we can return the number of layers until all goal facts were achieved (lines 12-13), known as the hmax heuristic, or sum the indexes of the layers where each goal fact appeared for the first time, known as the hadd heuristic [Bonet and Geffner, 2001]. However, hadd is inadmissible, as it does not acknowledge that several facts can be achieved together. For RTDP, this can be a serious disadvantage, as inadmissible heuristics cause RTDP to converge to a sub optimal solution.\nThe implementation shown in Algorithm 2 is simplistic, and there are several ways to improve it. For example, one can add to Fi only new facts that did not appear in some F\u2081, j < i. Then, we can check for the next level only actions that have some precondition in Fj, because there is no reason to execute an action twice. This is inaccurate for conditional effects that may have been inactive in past layers, and are now becoming active. However, we can take each action with conditional effects and create a set of actions, one for each condition. As in delete relaxation actions are executed in parallel, this is equivalent to executing the original action with multiple conditions activated. Also, as we repeatedly estimate a heuristic for a state s from different belief states, we can cache the state estimations. In many domains, all possible states become cached, and hence, although new beliefs are constantly generated, no new heuristic computations are needed. We use these, and other, less important, modifications to expedite the computation, but leave them out of the pseudo-code for ease of exposition.\nBryce et al. [2006] suggested to extend this method to belief states, by computing, e.g., hmax for each possible state. Then, a possible heuristic can be the weighted sum of heuristic values: $\\displaystyle h_{max}(b) = \\sum_{s} b(s)h_{max}(s)$. They show how"}, {"title": "Supporting Stochastic Effects", "content": "Delete-relaxation heuristics were originally designed for deterministic domains, and thus ignore stochastic effects. We suggest the following straight forward method for handling stochastic effects. For a fact l identified at level i that is a stochastic effect of an action with probability pr(l), we insert the fact only at level Fi+c, where c = [$\\frac{1}{pr(l)}$]. That is, we assume that the fact would only be achieved after c executions of the action. This is equivalent to the expected number of times that a must be executed before l is achieved.\nWhen computing our delete relaxation heuristics, we wait until the level where the fact is added, before deciding which action added it. That is, there may be another action that achieved this fact, deterministically or with higher probability, earlier."}, {"title": "Belief-based Heuristic", "content": "We now describe a new heuristic that is based on the delete-relaxation approach above, but extends it into belief space, allowing us to assign a value for sensing actions. The main difference from the classical method is in maintaining a set of achieved facts for each possible state. An action can be executed only if all states agree that it's preconditions"}, {"title": "Empirical Evaluation", "content": "We now report an empirical evaluation, comparing the performance of the various heuristics that we suggest, in the context of RTDP-BEL. All methods were implemented in C# within the unified planning framework\u00b9. The experiments were conducted on an i7 3.40GHz CPU with 8 cores, and 64GB RAM.\nMethods: We compare the following heuristics; First, we use two MDP-based heuristics QMDP and the value of the most likely state (Section 4.1). These heuristics require the computation of a value function for the underlying MDP. In some of our problems, simple value iteration did not converge in under 5 minutes. Using regular RTDP over the state space, on the other hand, is insufficient. This is because RTDP focuses on states that are visited on good paths to the goal in the MDP. However, in the POMDP, there are important states that do not belong to any such path. This is most pronounced in cases where information gathering is needed. We hence use the following compromise we compute initially an MDP value function using RTDP, and later, during the execution of RTDP-BEL, when reaching a state that was not visited by the initial RTDP execution, we run RTDP again for 100 additional iterations starting at that state. When reporting runtime, we exclude the initial RTDP execution, as it is not optimized, and other value function methods may be faster. Below we denote the two MDP based methods as ML for most likely state, and QMDP.\nWe use two heuristics based on hff that we describe above computing the expected cost over the possible states using hff (s) (Section 4.1), and computing hff (b) (Section 4.3). We also experimented with hmax and hadd for the two cases (state-based and belief-based), but both performed much worse than hff. To avoid overcrowding the table below we do not report these experiments. We also use a flat uninformative heuristic, that gives a cost of 1 to all non-goal beliefs.\nProcedure: For each heuristic method we run RTDP-BEL for at most 300 seconds of computation time. We stop the computation repeatedly to evaluate the current policy, to see if the algorithm has converged. The time required for policy evaluation is not considered a part of the runtime of the algorithm. Policy evaluation is run for each possible start state, or for 100 iterations, if there are less than 100 initial states. Each iteration runs from a start state until the current belief satisfies the goal. We stop an iteration if it exceeded 500 steps, assuming a loop. We select the action a with the best C(a) + \u03a3\uff61pr(o|b,a)V(b), given the current value function V. During policy evaluation (Eqn. 3), if a belief b is encountered for which V(b) was not yet computed, we use the value that the evaluated heuristic assigns to that belief instead. If 5 consecutive policy evaluations succeeded reaching the goal in all iterations, and the average cost of the policy evaluation has not changed by more than 1%, we assume that RTDP-BEL has converged. We then run a policy evaluation over the final value function for 1000 iterations, and compute the average cost, and the number of failed iterations. We report averages over 50 executions.\nDomains: While there exist a set of standard benchmarks for evaluating POMDP algorithms, such as RockSample, or LaserTag [Shani et al., 2013], these benchmarks do not exhibit significant effort for information gathering. As such domains are the focus of our attention in this paper, we use a set of domains adapted from the contingent planning literature [Albore et al., 2009] that exhibit these properties.\nThe Wumpus domain (denoted W in the tables below) is an interesting domain where lengthy information gathering sequences of actions are needed, together with multiple sensing actions are different positions. While for each state there is a simple path to the goal, many observations are needed to understand which path applies for the current state. As such, it is a good testing bed for the ability of heuristics to consider sensing actions. A parameter n defines the size of the grid and the number of possible Wumpi, and hence, the amount of needed observations. We create two versions one where there is a uniform distribution of possible states, and one where the Wumpi are more likely to be on one side. Actions have deterministic effects.\nAlthough Wumpus requires much information gathering, it does not involve value of information, because sensing actions are mandatory. We hence create a maze domain, where the agent must reach a goal position. In the maze there are several bottlenecks where the agent must pass through one of two cells, one easy to traverse (success probability of 1.0) and the other not. If the cell is hard, or if the agent does not known whether the cell is easy, it can still move through it, but the success probability drops to 0.1. For each possible state there is a path that consists only of easy cells. There are a few cells from which the agent can observe to see whether some cells are easy. The agent must balance between the cost of visiting these observation cells, and moving towards the goal.\nLocalize (denoted L) is a domain where the agent must navigate a simple maze, with uncertainty about its position, observing nearby walls. We add non uniform stochastic transitions, so the agent should consider the expected cost to"}, {"title": "Results", "content": "Table 2 shows the average runtime until convergence for our delete relaxation heuristics. For both state-based (Section 4.1) and belief-based (Section 4.3) we compare the 3 alternatives hmax, hadd, hff. For Localize (L) problems, where all information gathering is myopic, the state-based methods are best. State-based FF is less successful, especially in the stochastic case. For the rest of the problems, the belief-based FF heuristic using our belief-based method is better on many domains, but truly excels on the more difficult problems the hardest version of Blocks, with stochastic transitions, the Maze problems, that require much information gathering, and Wumpus. As such, we continue our empirical estimation focusing on hff.\nTable 3 compares the runtime of hff to MDP based methods and a flat heuristic, that assigns the same heuristic value to all states. Table 4 shows the number of RTDP iterations until convergence.\nLocalize (L) problems have relatively small state and action spaces. The agent is unclear about its current state, but can gather information without investing much cost. Observations are necessary to achieve preconditions of actions, such as knowing that there is no wall before moving in a particular direction, and hence must be activated in the RTDP-BEL trajectories. Thus, in these problems the MDP-based techniques excel. The most likely state heuristic is best in the deterministic version, attesting to the relative simplicity of these POMDPs. Logistics (EL) has similar properties.\nWe report the number of iterations as there is typically a tradeoff between the ability to produce accurate heuristic estimates, and the time it takes to compute these estimations. As can be seen, in localize, hff (b) often requires less iterations, but it is still much slower, because in this domain the MDP-based methods only need to take the values from the MDP value function, requiring almost no computation time.\nIn color blocks (CB), one must use many sensing actions, but always myopically, with no need to invest any effort in gathering information. In this domain, the flat heuristic is best, because any heuristic effort is wasted.\nThe blocks domain (B) is more difficult. It has a huge number of actions, far more than any other domain, and also a large state space. In this domain, the hff (b) and hff(s) methods, that allow RTDP-BEL to avoid exploring many irrelevant actions, work better than the the MDP-based methods. For the stochastic version, only hff (b) converged in time."}, {"title": "Conclusion", "content": "In this paper we suggested a new heuristic for estimating the value of a belief state in a POMDP, formalized as stochastic partially observable contingent planning. Our heuristic is able to estimate the cost of lengthy information gathering action sequences, and thus can direct the planner to consider such sequences. While this computation requires more time than standard heuristics, such as the popular QMDP heuristic, we show that in tasks that require significant effort for information gathering, it is worthwhile.\nFor future research we will investigate using our method in other scenarios, such as in point-based POMDP methods, as well as in the context of reinforcement learning."}]}