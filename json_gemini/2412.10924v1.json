{"title": "Tokens, the oft-overlooked appetizer: Large language models, the distributional hypothesis, and meaning", "authors": ["Julia Witte Zimmerman", "Denis Hudon", "Kathryn Cramer", "Alejandro J. Ruiz", "Calla Beauregard", "Ashley Fehr", "Mikaela Irene Fudolig", "Bradford Demarest", "Yoshi Meke Bird", "Milo Z. Trujillo", "Christopher M. Danforth", "Peter Sheridan Dodds"], "abstract": "Tokenization is a necessary component within the current architecture of many language models, including the transformer-based large language models (LLMs) of Generative AI, yet its impact on the model's cognition is often overlooked. We argue that LLMs demonstrate that the Distributional Hypothesis (DH) is sufficient for reasonably human-like language performance, and that the emergence of human-meaningful linguistic units among tokens motivates linguistically-informed interventions in existing, linguistically-agnostic tokenization techniques, particularly with respect to their roles as (1) semantic primitives and as (2) vehicles for conveying salient distributional patterns from human language to the model. We explore tokenizations from a BPE tokenizer; extant model vocabularies obtained from Hugging Face and tiktoken; and the information in exemplar token vectors as they move through the layers of a RoBERTa (large) model. Besides creating sub-optimal semantic building blocks and obscuring the model's access to the necessary distributional patterns, we describe how tokenization pretraining can be a backdoor for bias and other unwanted content, which current alignment practices may not remediate. Additionally, we relay evidence that the tokenization algorithm's objective function impacts the LLM's cognition, despite being meaningfully insulated from the main system intelligence.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) are artificial intelli-gence (AI) or machine learning (ML) models that takein vast amounts of text as training data and create newutterances by choosing the most plausible small piecesof text to output next. Such models are quickly becom-ing ubiquitous [1].\u00b9 In the context of LLMs, althoughother aspects of their architecture may vary, tokens arethe substrings that the model breaks all input text into,and the units it concatenates to generate output text.\u00b2As tokens are the bridge from raw text to numbers thatLLMs can work with, from our world (koinos kosmos)to the LLM\u2019s internal world (idios kosmos), they play a"}, {"title": "A. Tokenization", "content": "Tokenization refers to two related things: The creationof the vocabulary of tokens during the pretraining of themodel, and the parsing of text provided to the model,the strategy for mapping input text to the model\u2019s setof tokens. The model uses the same token vocabulary toparse input text and to generate output text. Althoughwe discuss both aspects of tokenization, we mostly focuson the tokens themselves, rather than the strategy formapping text to tokens.10\nTokenization is a fundamental step for most NLPtasks [50]. The tokens can be created with various strate-gies, but the overall goal is to come up with a set of to-kens that can be combined to create any output text themodel could need to generate. In practice, this meansthe tokenizer should balance the combinatorial power ofshort strings with the efficiency of long strings,\u00b9\u00b9 and themodel creator should choose a reasonable vocabulary sizehyperparameter.\nA significant amount of research has been devoted totokenization, since it predates the LLM application apro-pos here. Much of its study has been around designingthe algorithms themselves [51\u201353], or in improving modelor algorithmic performance (e.g., efficiency in terms ofspeed, memory-usage, number of parameters, etc.), es-pecially by benchmarking model performance in responseto variations in the tokenization strategy used [54, 55].Further research surveys existing implementations of to-kenization and its consequences [8, 11, 56\u201358], or blends"}, {"title": "B. Exemplar vocabularies", "content": "GPT-40 [84, 85] is a recent, prominent, proprietaryflagship model, and we take its vocabulary to be rep-resentative of what a good vocabulary currently lookslike. We can see in Figure 2 that GPT-40 uses tokensthat look similar to the word-like stage (achieved throughlinguistics-agnostic tokenizing) and like tokens gener-ated based on linguistic structures (morphemes, lemmas,words), consistent with our expectation.\nHowever, relatively little information is available onwhat tokens look like more generally out in the wild.An exception is (the very fun) work of Zhemchuzhina etal., 2022, which delves into what kinds of tokens occurin a vocabulary, and comes up with the following cat-egories: (1) atom, the smallest discrete element, whichis probably coming from a closed set (as in the lettersof an alphabet in written language); (2) pragma, whichconsists of atoms, represents part of an idea, and has"}, {"title": "C. Extispicy: exemplar tokens", "content": "Previous work found semantic information was a domi-nant force within the GPT-3 LLM\u2019s internal organizationof language. Digutsch and Kosinski, 2023, found that\u201cGPT-3\u2019s semantic activation is better predicted by sim-ilarity in words\u2019 meaning (i.e., semantic similarity) rather"}, {"title": "II. APPROACH", "content": "Our exploration of tokenization is split into threestages. Section II A delves into the tokenization processusing a Byte-Pair Encoding (BPE) tokenizer [71], exam-ining how the vocabulary size hyperparameter affects thesize of the tokens that the tokenizer creates. Section II Bfocuses on vocabularies from extant models. In this sec-tion, we look at what makes up these vocabularies, in-cluding looking for tokens that match word categorieslike parts-of-speech (POS). Section II C ventures intoa model\u2019s gnogeography by examining what an LLM canknow about a token, using a RoBERTa MLM [9] anda few exemplary tokens. For further explanation of ourapproach, see the Appendix (starting in Appendix A).\n\nA. Exemplar tokenizations\n\nA tokenization algorithm uses the training data it isgiven to determine what pieces (tokens) will be needed forfuture output. The popular tokenization design choice ofmaximizing orthographic efficiency allows as much dataas possible to be processed by the LLM, presumably themotivation behind this strategy\u2019s ubiquity. We trainedone such tokenizer, a Byte-Pair Encoding (BPE) tok-enizer [71], with varying vocabulary sizes and trainingdata to explore the details of the process, demonstrat-ing some alignment between linguistic features and in-formation processing by showing the human-like stagesthe tokens pass through. As a caveat, the training datachosen also determines the tokens; once vocabulary sizebecomes relatively large compared to the training data,tokens become too specific.\n\nAs a familiar starting place, we divided the classic Al-ice in Wonderland novel [72] into rough sentence chunksusing the simplest possible (imperfect) strategy (split-ting on punctuation). The resulting example first chunkhas the words \"bank\" and \"the\" in it, which we returnto later in Sec. IIC (this excerpt shown in Fig. 2). Forgreater linguistic generality, we also used the larger wiki-text dataset [73] for training the tokenizer; in that case,for consistency, we added the extracted first chunk fromAlice in Wonderland.\u00b2\u2074\n\nWe used the spaCy and benepar packages to label syn-tax [74\u201376]. While not perfect (as in, some of the labelsprovided by the packages did not match our own judg-ments), they are sufficiently performant for our purposes.\nThe tokenizer\u2019s vocabulary size hyperparameter setsthe total number of tokens that will be known to themodel. Due to the way the BPE tokenization algorithmworks, as the vocabulary size increases, the maximum"}, {"title": "C. Bias, alignment, and ethics", "content": "There is concern about bias and alignment in models.We argue that the tokenization process, a largely over-looked aspect of the model when it comes to ethics andsafety, warrants further scrutiny. As established above,pretraining often contains low quality/ unfiltered con-tent; one place this is visible is when \u201cbad words\u201d turnup in vocabularies. Furthermore, offensive, unwanted,or unhelpful information can be learned by the modeland stored in a token that is a component of a largerword, even if the vocabulary of the model were to be care-fully screened. Finally, this content could continue to bepresent for the model during inference: the embeddingmatrix often starts with the vectors learned during pre-training, which could mean repeatedly hearkening backto semantic information learned during a largely uncon-sidered stage of model creation (recall the sorts of tokenswe\u2019ve seen and what that implies about the training data,Sec. II B).\nThe last two points especially make up a \u201cbackdoor\u201dfor unwanted content to make its way into the model\u2019sinternal representations: as seen with the \u201crung\u201d and\u201crun\u201d example (Fig. A30), any token that is an ortho-graphic component of another word could retain infor-mation about that word.\u2077\u2074 Since many of the preva-lent tokenization strategies are popular in large part be-cause they handle OOV words by including individualletters and small subword units,\u2077\u2075 essentially any givenword is guaranteed to have token components in the typ-ical model vocabulary. Templeton et al., 2024 attestthis composition of concepts with respect to the featurelevel (a higher level within the model than the token):\u201c[i]mportantly, not having a feature dedicated to a par-ticular concept does not mean that the reconstructed ac-tivations do not contain information about that concept,as the model can use multiple related features composi-tionally to reference a specific concept\u201d [34].\n\nTo be clear, we are not advocating that the modelshould not learn \u201cbad words\u201d (or their components)as tokens (or otherwise). As Templeton et al., 2024said, \u201cthere\u2019s a difference (for example) between knowingabout lies, being capable of lying, and actually lying inthe real world\u201d [34]. Models may need to know offensiveand hateful language in order to understand relevant sub-jects, the impact and use of the words themselves, andto distinguish between wanted and unwanted behaviour.However, we do think it is worth considering what itmeans that the model does learn such words\u2014one suchthing being the kind of data this means the model isingesting early in its development.\u2077\u2076\n\nGiven the previous points, it is worth flagging that"}, {"title": "B. Objective functions", "content": "As one of the main forces shaping their cognition,\u2076\u00b3exploring the space of objective functions is promisingfor allowing LLMs to get more out of the same data\u2076\u2074and to solve some of the shortcomings that seem to comefrom such a stringent emphasis on producing plausiblelanguage above all else, a strategy that makes currentLLMs silvertongued but ultimately unreliable and unhu-manlike in terms of any behaviour outside those bounds(e.g., Grice\u2019s maxims, moral consistency, reciprocal self-disclosure, etc.) [1, 113].\n\nFundamentally, seeking to modulate the primacy of thenext token generation objective function is the motiva-tion behind alignment approaches like RLHF (reinforce-ment learning from human feedback) [114]. Due to theclear efficacy of next token generation (and similar) forcreating human-like language performance, though, thisfunction usually remains prominent in the mix.\n\nPerhaps unintuitively, the objective functions of com-ponents and precursors, processes not directly invoked bythe main intelligence itself, can also impact its cognitivearchitecture.\n\nThere is an ongoing body of work on the impact ofchoice of components that determine model parametersduring training [114, 115]. For example, Caples andRenaud, 2024, found that optimizer choice impacts the\u201cinternal representations formed by these models\u201d [116].The pathway, in this case, is further removed than in thecase of the objective function: although the optimizer isdirectly involved in shaping the mind of the model (ithelps set the weights in the neural net), it is not some-thing the model will draw on directly once it is finishedwith training.\n\nWe provide evidence of how the tokenization objectivefunction directly impacts the internal representations ofLLMs, not only in choice of tokens (which is self-evident)but in the content of the tokens as well. This is partic-ularly notable because the pathway here is at greaterremove than the optimizer. Tokenization is typically de-termined before the bulk of training\u2014the development ofthe core intelligence\u2014begins (and the model and tokenizerare not typically trained on the same corpus)."}, {"title": "III. DISCUSSION", "content": "A. Meaning and grounding\n\nWe speculate that syntagmatic information lays thegroundwork for successively richer semantic information,up to some limit: Syntagmatic information is the guidingforce for the emergence of semantic themes, and anythingpropositional or descriptive can be conveyed purely lin-guistically (by the DH). We think this fits with our dis-cussion in Section IB. For example, we see a cluster oflargely prepositional phrases (PP), which suggests thatsyntax is their highest level of organization. However,a spatial relationship (over/under) subdivides the indi-vidual prepositional phases, hinting at the potential formore and more fine-grained detail.\u2074\u2079"}, {"title": "IV. CONCLUSION", "content": "Current models fall short for many of the ways wewant to use LLMs [11, 40]. Tokenization is one of sev-eral promising research avenues to seek necessary modelimprovement.\n\nData scaling is unlikely to be the sole answer forthat improvement, based on the general observationthat unusual information is difficult for models to ac-quire [8, 55, 124, 132], and the projections that theamount of data we have access to would be insufficient(which underline the inadequacy of that strategy) [81].\n\nIt is clear that we need to develop strategies that allowLLMs to make better use of the data to which they areexposed.\u2078\u2077 People, by contrast, are very good at wring-ing new information from familiar stories.\u2078\u2078 This stark"}, {"title": "V. LIMITATIONS", "content": "The main limitations are human fallibility, the manyapproximations we relied on (e.g., in creating lists ofwords, in using a small corpus of fiction, in using au-tomated parsers, etc.) which were then composed oneach other, and the many simplifying assumptions (mov-ing between \u2018orthography\u2019 and \u2018language\u2019; consideringbroad families of architecture as basically similar enough,e.g., generative LLMs like the GPTs and MLMs like"}]}