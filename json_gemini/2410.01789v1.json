{"title": "Investigating on RLHF methodology", "authors": ["Alexey Kutalev", "Sergei Markoff"], "abstract": "In this article, we investigate the alignment of Large Language Models according to human preferences. We discuss the features of training a Preference Model, which simulates human preferences, and the methods and details we found essential for achieving the best results.\nWe also discuss using Reinforcement Learning to fine-tune Large Language Models and describe the challenges we faced and the ways to overcome them. Additionally, we present our experience with the Direct Preference Optimization method, which enables us to align a Large Language Model with human preferences without creating a separate Preference Model.\nAs our contribution, we introduce the approach for collecting a preference dataset through perplexity filtering, which makes the process of creating such a dataset for a specific Language Model much easier and more cost-effective.", "sections": [{"title": "Introduction", "content": "The launch of ChatGPT by OpenAI in late 2022 caused an explosive increase in interest in Large Language Models (LLMs) around the world. Many companies and research teams began developing their versions of instructive LLMs and conducting research on training, customizing, and using them to solve a wide range of tasks.\nThe approach described in the paper [4] is often used as a starting point for these works. This approach includes three stages: pre-training a Language Model on a vast text corpus, fine-tuning it on a much smaller instructional dataset, and aligning the model according to human preference.\nIn case the target Language Model has a large size (in modern conditions, these are models with a size of 7 billion parameters or greater), the first stage of training requires huge expenditures of computing resources and, therefore, is available only to a narrow circle of organizations. The rest have to use ready-made pre-trained LLMs as a baseline. A neural network trained on this stage is able to generate a continuation for a given text.\nNext, the model is trained further on an instructional dataset, and during this training the model learns to continue a given text by following the instructions embedded in it. The instructional dataset itself is a set of instructions and the results of their execution in natural language. This stage requires significantly less computing costs than the previous one and, therefore, is available to a wide range of enthusiasts. However, the quality of the resulting model strongly depends on the quality of the instructional dataset used for its training, all other things being equal. We will refer to the Language Model obtained from this stage as the Supervised Fine-Tuned model SFT.\nThe third stage is fine-tuning the model to align it with human preferences, that is, training the model to generate responses that are more suitable to the human criteria. This process usually requires creating an auxiliary Language Model that simulates human preferences, called a Preference Model. To create a Preferences Model, a preference dataset is needed, which is a set of examples where each example includes a user request and a pair of responses to this request. In these pairs, one of the responses is marked as more appropriate according to the human criteria, and another is marked as less appropriate. Most often, helpfulness and harmlessness are used as criteria for human-preferred responses. The markup of this dataset is usually done by human assessors [5], but there are also examples of markup"}, {"title": "Modeling of human preferences", "content": ""}, {"title": "Training the Preference Model", "content": "Most papers on RLHF ([2], [3], [4], [6], etc.) use the following scheme of Preference model: in the basic LLM the final layer, called LM-head, is replaced by one-dimensional linear transformation. When we feed Preference Model with tokenized text of the request and response, this linear transformation translates the output hidden state for each token to a scalar value. The last one of these scalar values represents the preference score of the given response for the request. Thus, the Preference Model must work in such a way that for each request the preference score of more preferred response must be greater than the score for less preferred response. To indicate the end of response for the Preference Model a special token is usually added to the response text. This token is named \"reward token\" as the preference score is used in RL as reward value.\nThe Preference Model faces a challenging task: it must differentiate between the responses generated by the LLM, determining which is better and which is worse by human criteria. Therefore, the Preference Model size should not be smaller but rather larger than the size of the LLM that generates the evaluated responses. In this scenario, the Preference Model acts as a discriminator, similar to how the discriminator in GAN networks operates. From the experience of training GANs (for example, [12] and [13]), we"}, {"title": "Important Features of the Preference Model", "content": "It is necessary to highlight some crucial features of the Preference Model created according to the above scheme, which will be essential for further discussion.\nFirstly, we have to note that the response scores given by the Preference Model for different requests may not be in a specific general range and may not be comparable. For example, for one user's request and the corresponding response generated by the Language Model, the Preference Model may assign a score of 1. For another user's request with a different response, the PM assigns a score of 100. However, this does not necessarily mean that the response to the second request is superior to the response to the first request. Instead, the response to the first request may be very good and has a score of 1, while the response to the second request may be mediocre and has a score of 100. It may happen due to the Preference Model training method, which only teaches PM to differentiate (rank) generated responses for a specific request rather than assign scores consistent across all requests and fitting into a specific rating range.\nSecondly, the scale of the difference in the score values does not have to be the same for different requests. Consider that for a pair of generated responses to one request we have a difference in PM scores of 1000. And for a pair of responses to another request we have a difference in PM scores of 1. This situation does not mean that the difference between a pair of responses"}, {"title": "The scope of the Preference Model", "content": "In the major papers on RLHF ([4], [5], [7]), the authors state that the Preference Model (PM) performs well (i.e., it distinguishes well between better and worse responses) only within the range of generations of the basic Language Model, or more precisely, within the range of responses that were in the dataset for training Preference Model. And the ability of the PM to distinguish between better and worse responses according to human criteria degrades greatly when the distribution of responses generated by the LLM shifts. Such a shift can occur, for example, as a result of fine-tuning the LLM using PPO or DPO.\nTo overcome this problem, the preference dataset in [4], [5], and [7] is constantly updated, and the Preference Model is trained further using examples with responses generated by the LLM, which was improved by the next PPO or DPO iteration.\nIn cases where the Preference Model is trained on the datasets with the responses that were not generated by the target LLM (for example, publicly available preference datasets such as Antropic Helpful, Antropic Harmless, Nectar, etc.), the range of practical applications of this Preference Model is obviously very different from the range of training examples in the preference dataset. Accordingly, we cannot expect this Preference Model to perform well on responses generated by target LLM.\nFor example, in the paper [15], the authors trained a Preference Model using public preference datasets and then used this PM to fine-tune their target LLM with PPO. As a result, they convincingly showed that the Preference Model did not learn any useful signal other than scoring the response length, excepting some special cases where they fine-tuned target LLM for code generation.\nOur experience also confirms this. We initially attempted to train a Preference Model on a dataset compiled from requests and corresponding responses from various LLMs such as chatGPT 3.5, GPT-4, Llama and Mis-"}, {"title": "Comparison of Large Language Models", "content": "In order to compare different LLMs, we conduct side-by-side comparisons based on the types of tasks given to the LLM. This approach allows us not only to obtain an average comparison, but also to understand on which types of tasks the LLM was improved or, conversely, worsened.\nAt the beginning of our experiments with RLHF, we noticed that when using a general preference dataset, the side-by-side evaluation between the basic LLM (SFT) and the model trained by RLHF methods is very uneven"}, {"title": "The scope of the Preference Model", "content": "In the major papers on RLHF ([4], [5], [7]), the authors state that the Preference Model (PM) performs well (i.e., it distinguishes well between better and worse responses) only within the range of generations of the basic Language Model, or more precisely, within the range of responses that were in the dataset for training Preference Model. And the ability of the PM to distinguish between better and worse responses according to human criteria degrades greatly when the distribution of responses generated by the LLM shifts. Such a shift can occur, for example, as a result of fine-tuning the LLM using PPO or DPO.\nTo overcome this problem, the preference dataset in [4], [5], and [7] is constantly updated, and the Preference Model is trained further using examples with responses generated by the LLM, which was improved by the next PPO or DPO iteration.\nIn cases where the Preference Model is trained on the datasets with the responses that were not generated by the target LLM (for example, publicly available preference datasets such as Antropic Helpful, Antropic Harmless, Nectar, etc.), the range of practical applications of this Preference Model is obviously very different from the range of training examples in the preference dataset. Accordingly, we cannot expect this Preference Model to perform well on responses generated by target LLM.\nFor example, in the paper [15], the authors trained a Preference Model using public preference datasets and then used this PM to fine-tune their target LLM with PPO. As a result, they convincingly showed that the Preference Model did not learn any useful signal other than scoring the response length, excepting some special cases where they fine-tuned target LLM for code generation.\nOur experience also confirms this. We initially attempted to train a Preference Model on a dataset compiled from requests and corresponding responses from various LLMs such as chatGPT 3.5, GPT-4, Llama and Mis-"}, {"title": "Features of the implementation of our Preference Model", "content": "In our experiments, we followed the above architecture and the training method of the Preference Model. We used our LLM (SFT) without the LM-head as a baseline and added a linear layer to it. This layer converts the output hidden state of each token into a score value. Thus, the PM score of"}, {"title": "Reinforcement Learning using PPO", "content": ""}, {"title": "Implementation difficulties", "content": "In order to implement reinforcement learning for our basic LLM (SFT) in correspondence with the scheme described in [4], [5] and [6], several difficulties must be overcome.\nFirst, during the operation of the RLHF-PPO algorithm, 4 models must be available simultaneously: the target LLM, the basic LLM (SFT), the Critic model (predicting a cumulative future reward for each response token), and the Preference Model. The target LLM and the Critic model must be trainable, the target model must also generate responses at each PPO iteration. The basic LLM (SFT) and the Preference Model must work in the inference mode as they only perform forward passes and are not trained.\nIn order to fit all 4 models into the memory of our NVIDIA A100 GPU cluster, we applied a scheme where only the basic LLM (SFT) is loaded into the GPU memory, and the other three models are represented as sets of LoRA adapters and output linear layers (heads) to the basic model. We used the DeepSpeed framework for training with ZeRO2 mode for 7b models and ZERO3 for 29b models.\nSecond, the authors of [16] and [9] report that they faced the instability of the PPO algorithm during the learning process when applying the classical PPO approach from [1] to train LLMs. This instability is due to significant fluctuations in the average value and range of the scores (rewards) which Preference Model provides.\nIn order to stabilize the PPO algorithm, the authors of [9] proposed to normalize the scores from a Preference Model using moving average and moving standard deviation. However, in our experiments, this scheme did not produce the desired results, that is, the algorithm often diverged suddenly.\nWe obtained much better results by normalizing the advantage values in each batch of training examples, similar to the implementation in the TRL and TRLX packages. Due to the structure of the loss function in PPO, this normalization leads to the fact that the norm of the loss function gradient remains approximately the same throughout the training period. As a result, it is possible to select the training parameters for a specific LLM so that the learning process remains stable for a long time.\nHowever, such a scheme has a drawback: when the growth of the scores given by the Preference Model for generated responses begins to slow down or stop completely, the norm of the loss function gradient does not decrease but remains approximately constant, while the gradient itself becomes chaotic. That leads to the gradual deterioration of the trained LLM at a later stage of PPO training. In this case, it is necessary to use heuristics to stop training early or decrease the learning rate when a significant increase in the average PM score of responses has already been obtained, but the integrity of the trained LLM is still preserved."}, {"title": "Results", "content": "We performed training of our basic LLM (SFT) using the PPO algorithm on the dataset of requests balanced by the number of requests for each task type.\nIn our experiments, we achieved a steady, consistent growth in the average score of responses generated by trained LLM. As a result of PPO training, we obtained the model that generates the responses with PM scores significantly higher than PM scores for the responses generated by the basic LLM (SFT) for the same requests. The graph in Figure 1 shows the growth dynamics in the average PM score of responses generated by the trained model in the process of PPO."}, {"title": "Training LLM using the DPO algorithm", "content": ""}, {"title": "Brief description of the DPO", "content": "In the paper [16], the authors proposed the method of direct training of the basic LLM (SFT) using a preference dataset, bypassing the creation of a separate Preference Model.\nIts essence is to train a LLM using a special kind of loss function (see formula 2), which leads to an increase in the probability of generating more preferred responses and a decrease in the probability of generating less pre-ferred responses. At the same time, the loss function includes a penalty for significant deviation from the basic LLM. This penalty is based on the dif-ference in distributions generated by the trained LLM from the distributions of the basic LLM, and it protects the trained LLM from destructive changes.\nloss(\u03c0DPO, \u03c0SFT) = -E(x\u1d62,y\u207a,y\u207b)\u223cD log \u03c3(\u03b2 log (\u03c0DPO(y\u207a|x\u1d62)/\u03c0SFT(y\u207a|x\u1d62)) \u2212 \u03b2 log (\u03c0DPO(y\u207b|x\u1d62)/\u03c0SFT(y\u207b|x\u1d62))),\nwhere x\u1d62 is i-th request, y\u207a is better response, y\u207b is worse response for the i-th request in the preference dataset D, \u03c0DPO(y|x) is likelihood of response y when requesting x on the LLM trained with DPO, \u03c0SFT(y|x) is likelihood of response y when requesting x on the basic LLM (SFT), \u03b2 is the penalty coefficient to prevent the distribution from deviating to far from the basic model.\nIt is worth noting that the authors of the paper [16] explicitly indicate that for successful application of DPO, the preference dataset must consist of responses generated by the basic LLM, that is, it must be on-distribution for this LLM."}, {"title": "The ability to use the out-of-distribution dataset for DPO training", "content": "The preference dataset we collected (approximately 200,000 examples) is not on-distribution for our basic models 7b-SFT and 29b-SFT. Nevertheless, we tried to train our basic 7b LLM on this dataset using DPO.\nSimilar to the PPO case, to select the parameters of DPO training and control the results, we were measuring a set of well-known metrics, such as"}, {"title": "Creating an on-distribution preference dataset for DPO using filtering by perplexity", "content": "The development of the language corpus used for pre-training our LLM and the improvement of the instructional dataset used for training the LLM to follow instructions have never stopped. Therefore, we continuously obtain new versions of our basic SFT models. To have an on-distribution preference dataset for RLHF purposes, we need to compose this dataset entirely from responses generated by our current SFT model and then mark it up by human assessors. In these conditions, maintaining an up-to-date, large-scale preference dataset that is on-distribution for each new version of our SFT model becomes a very expensive and difficult task.\nTo address this challenge, we developed the method of creating an on-distribution preference dataset by filtering examples from a union of available preference datasets, including publicly available ones, based on perplexity. The method's main idea is to select examples from a union of available preference datasets, where both responses have low perplexity on the current basic LLM. Suppose both responses in the preference example have perplexity that is in the perplexity range of the responses generated by the current basic LLM. So, we can consider such an example as on-distribution for this LLM. Further, we can use the subset of examples selected in this way to train the current basic LLM using DPO.\nAccordingly, to create such a preference dataset, we followed these steps:\nWe generated responses to a certain set of requests for each type of task using our basic 7b-SFT LLM and calculated the perplexity for each response. Then, for each type of task, we calculated the 95-percentile of all perplexities of generated responses. We did not take a simple maximum to eliminate outliers, that is, random unlikely generations. The obtained 95-percentile for each task type will set further the boundary for filtering.\nWe merged our preference dataset and a very large publicly available dataset Nectar from [11]. For each example in the combined preference dataset, we calculated the perplexity of each response on our basic 7b-SFT LLM.\nFrom all examples of the combined dataset we selected only those examples where the perplexity of both responses was less than the calculated border for the task type of this example.\nWe also balanced the resulting dataset based on task types so that the sizes of the subsets for different task types differed by no more than 2 times.\nFinally, for our 7b-SFT model, we have a preference dataset with a size of about 380,000 examples.\nSimilarly to the previous section, we conducted the selection of hyperparameters for DPO training and the selection of candidates for side-by-side evaluation based on the non-deterioration of a set of metrics for LLMs."}, {"title": "Performing DPO training using the preference dataset, which includes only responses generated by the model", "content": "As a proof of concept of creating a preference dataset from the papers [4] and [5], we created a small preference dataset where the examples include only responses generated by the basic LLM, that is, a model that will then have to be trained using DPO on this dataset.\nTo do this, we collected a set of requests for each of the 10 types of tasks and generated several responses to each request using our basic 7b-SFT model. Then, for each request, we selected 2 most different responses and created an example with these request and responses. The resulting set"}, {"title": "Using DPO training to overcome the problem of repeats when LLM generates the response", "content": "Following the method described in [8], we applied DPO training to overcome multiple repeats of text piece in the responses of our 7b LLM on some requests.\nTo do this, from our archive of requests and their responses generated by our LLMs, we selected requests whose responses have repetitions (multiple repetitions - a substring with a length of more than 20 characters is repeated 7 or more times in the response, tandem repetitions - a substring with a length of more than 100 characters is repeated 2 or more times together) or cycles (when the model generates repeating text until the length of the LLM context).\nFor a set of these requests, we also generated responses from our LLMs without repetitions and cycles. Then, we created the preference dataset where the responses without repetitions and cycles were marked as more preferred (better), while responses with repetitions or cycles were marked as less preferred (worse) for corresponding requests. Thus, we obtained a preference dataset for overcoming repeats with about 40,000 examples.\nWe selected only on-distribution examples from this dataset for our basic 7b-SFT model using the filtering by perplexity method described above. It turned out about 7,000 examples.\nHaving selected the training parameters as before, we conducted DPO training of our basic 7b-SFT LLM on this dataset with 7,000 examples. To evaluate the results, we used a special set of requests in the responses to which our models are prone to produce repetitions and cycles. As a result, the number of responses with repetitions or cycles decreased by 3 or more times on this set of requests - see table 5.\nSimilarly, using filtering by perplexity, we selected on-distribution examples for our larger model 29b-SFT \u2013 about 6,000 examples. After conducting DPO training of the 29b-SFT model on this dataset, we also got a significant reduction in repetitions and cycles, as shown in Table 6."}, {"title": "Conclusions, discussions, and hypotheses about the formation of a preference dataset", "content": "As mentioned above in the section about the DPO method, formation and maintaining a relevant preference dataset of the required size composed entirely of responses generated by the current basic model and marked up by human assessors is a costly and difficult task. In addition, the markup of the preference dataset, carried out even by professional assessors, often contains ambiguous choices, and sometimes just errors.\nAs shown by the researchers from the paper [10], who performed the markup of the preference dataset using the Preference Model trained on it, publicly available preference datasets contain many examples where the markup directly contradicts to the markup performed by the Preference Model (i.e., the difference in scores of better and worse responses is significantly negative). Our experience of training the Preference Model, and then using it for checking the markup it our preference dataset, revealed a similar situation. Many examples where the PM score of the worse response is greater than the PM score of the better one, upon detailed examination, really turned out to be incorrectly marked by human assessors.\nConsidering all of the above, it would be good to reduce the cost of the markup task and make it more accurate and reliable. To address this issue, we have explored several different approaches:\nWe attempted to automate the process of marking up LLM responses by transferring the selection of the preferred response to the LLM itself using a specific type of request. We tried to use our 29b-SFT model and GPT4 for this purpose. Unfortunately, this approach has not yet brought positive results. We performed two markups of the test prefer-\nFollowing the method from [18], we also tried to ask our basic LLM to generate a more preferable response to the request based on a given request-response pair. However, human markup on the obtained pairs of responses showed that only about 50% of the model's responses were improved.\nIn order to create a preference dataset that would be on-distribution for our current basic LLM, we also tried to apply a perplexity filtering method, which we developed inspired by [19]. After collecting a lot of responses generated by our current basic LLM, we determined the maximum threshold \u2014 such a number that most (95%) of the generated responses have a perplexity less than this threshold. Then, we merged several publicly available preference datasets with our proprietary pref-erence dataset into one set of examples. After that, we selected from this merged dataset only those examples where both responses have a perplexity on our current basic LLM below the calculated threshold (that is, in theory, these responses could be generated by our current basic LLM). After that, we performed DPO training our current basic LLM on the subset of examples selected in this way. We have described the corresponding results above in the section about DPO.\nOnly the last one of these three approaches showed good results, that is, it allowed us to create such a preference dataset for our 7b-SFT model, which gave it a significant improvement within RLHF training.\nWe hope that the proposed approach of creating a preference dataset by filtering a large array of data by perplexity will help to popularize RLHF methods since this approach avoids the most expensive and difficult part of the process: marking up a preference dataset by human assessors."}, {"title": "Important Features of the Preference Model", "content": "It is necessary to highlight some crucial features of the Preference Model created according to the above scheme, which will be essential for further discussion.\nFirstly, we have to note that the response scores given by the Preference Model for different requests may not be in a specific general range and may not be comparable. For example, for one user's request and the corresponding response generated by the Language Model, the Preference Model may assign a score of 1. For another user's request with a different response, the PM assigns a score of 100. However, this does not necessarily mean that the response to the second request is superior to the response to the first request. Instead, the response to the first request may be very good and has a score of 1, while the response to the second request may be mediocre and has a score of 100. It may happen due to the Preference Model training method, which only teaches PM to differentiate (rank) generated responses for a specific request rather than assign scores consistent across all requests and fitting into a specific rating range.\nSecondly, the scale of the difference in the score values does not have to be the same for different requests. Consider that for a pair of generated responses to one request we have a difference in PM scores of 1000. And for a pair of responses to another request we have a difference in PM scores of 1. This situation does not mean that the difference between a pair of responses\nloss(r\u03b8) = \u2212\ud835\udd3c(x,y+,y\u2212)\u223cD log \u03c3(r\u03b8(x\u1d62, y\u207a\u1d62) \u2212 r\u03b8(x\u1d62, y\u207b\u1d62)), (1)\nwhere x\u1d62 is i-th request, y\u207a\u1d62 and y\u207b\u1d62 are the better and worse responses for the i-th request in the preference dataset D, r\u03b8(x, y) is the scalar output (score) of Preference Model on request x with response y."}]}