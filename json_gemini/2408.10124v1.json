{"title": "Molecular Graph Representation Learning\nIntegrating Large Language Models with\nDomain-specific Small Models", "authors": ["Tianyu Zhang", "Yuxiang Ren", "Chengbin Hou", "Hairong Lv", "Xuegong Zhang"], "abstract": "Molecular property prediction is a crucial foun-\ndation for drug discovery. In recent years, pre-trained deep\nlearning models have been widely applied to this task. Some\napproaches that incorporate prior biological domain knowledge\ninto the pre-training framework have achieved impressive results.\nHowever, these methods heavily rely on biochemical experts, and\nretrieving and summarizing vast amounts of domain knowledge\nliterature is both time-consuming and expensive. Large Language\nModels (LLMs) have demonstrated remarkable performance\nin understanding and efficiently providing general knowledge.\nNevertheless, they occasionally exhibit hallucinations and lack\nprecision in generating domain-specific knowledge. Conversely,\nDomain-specific Small Models (DSMs) possess rich domain\nknowledge and can accurately calculate molecular domain-\nrelated metrics. However, due to their limited model size and\nsingular functionality, they lack the breadth of knowledge neces-\nsary for comprehensive representation learning. To leverage the\nadvantages of both approaches in molecular property prediction,\nwe propose a novel Molecular Graph representation learning\nframework that integrates Large language models and Domain-\nspecific small models (MolGraph-LarDo). Technically, we design\na two-stage prompt strategy where DSMs are introduced to\ncalibrate the knowledge provided by LLMs, enhancing the accu-\nracy of domain-specific information and thus enabling LLMs to\ngenerate more precise textual descriptions for molecular samples.\nSubsequently, we employ a multi-modal alignment method to\ncoordinate various modalities, including molecular graphs and\ntheir corresponding descriptive texts, to guide the pre-training\nof molecular representations. Extensive experiments demonstrate\nthe effectiveness of the proposed method.", "sections": [{"title": "I. INTRODUCTION", "content": "Molecular representation learning is a fundamental and\npreliminary task in the field of drug discovery [1]-[5]. It\nserves as the basis for following tasks, including molecular\nproperty prediction, molecular scaffold optimization, and tar-\ngeted molecular generation, all of which require robust and\nexpressive molecular representations. Over the years, deep\nlearning techniques have significantly advanced this domain,\noffering promising avenues for developing accurate and ef-\nficient models. Molecules, often depicted as graphs, feature\nnodes representing atoms and edges representing chemical\nbonds [3]. Consequently, graph neural networks (GNNs), as\na sophisticated and efficacious framework for graph repre-\nsentation learning [6]\u2013[8], have garnered significant attention\nand found extensive application in the domain of molecular\nrepresentation learning [9], [10].\nHowever, akin to other deep learning methods, GNN-based\napproaches are often constrained by the availability of labeled\ndata. Hence, researchers are exploring self-supervised methods\nfor molecular representation learning [11]\u2013[16]. These self-\nsupervised methods typically follow a paradigm where models\nare first pre-trained using a large amount of unlabeled molec-\nular samples and then fine-tuned for downstream tasks such\nas molecule property prediction. The adoption of these pre-\ntrained deep learning models have gained widespread adoption\ndue to their capacity to leverage large-scale unlabeled data for\nimproved performance.\nRecent research has witnessed a notable trend towards\nintegrating prior biological domain knowledge into the pre-\ntrained frameworks [5], [15], [17], [18]. Incorporating biomed-\nical domain knowledge into the pre-training process enhances\nthe models' ability to learn representations for molecules,\nresulting in remarkable achievements in molecular property\nprediction tasks. Despite the advancements of these pre-trained\nframeworks with domain knowledge, a significant bottleneck\nin this approach is the heavy reliance on biochemical experts\nto acquire specialized domain knowledge. This process is not\nonly time-consuming but also entails substantial expenses.\nTo address this challenge, recent advancements in Natural\nLanguage Processing (NLP) have revealed the potential of\nLarge Language Models (LLMs) to efficiently provide knowl-\nedge. Leveraging this capability, some very recent studies\nhave begun exploring the integration of LLMs into graph\ndomain [19], [20] including molecular tasks [21]\u2013[23].\nNevertheless, LLMs for general purpose occasionally ex-\nhibit hallucinations and lack precision in generating domain-\nspecific knowledge. For example, an LLM may generate\nincorrect specialized knowledge or encounter difficulties with\nmathematical calculations. Thus, the existing methods for\nintroducing LLMs to address molecular tasks may suffer from\nthis drawback. In contrast, Domain-Specific Small Models\n(DSMs) have extensive domain knowledge and can accurately\ncompute domain-related metrics for molecules. Tools such as\nthe RDKit package, a basic type of DSMs, provide specialized\ncalculations. However, due to their limited size and singular\nfunctionality, DSMs lack the breadth of knowledge required\nfor comprehensive representation learning.\nTherefore, to leverage the advantages of both LLMs and\nDSMs, we propose a novel Molecular Graph representation\nlearning framework which integrates Large language mod-\nels and Domain-specific small models, called MolGraph-\nLarDo. Specifically, we design a two-stage prompt strategy\nwhere DSMs are introduced to calibrate the knowledge pro-\nvided by LLMs, aiming to obtain more accurate domain-\nspecific information and thus enabling LLMs to generate\nmore precise textual descriptions of molecular samples. To\nfurther avoid hallucination issues, the designed prompt incor-\nporates both dataset-specific and sample-specific information.\nSubsequently, a multi-modal graph-text alignment method is\nemployed in MolGraph-LarDo to guide the pre-training of\nmolecular graphs. The text modality is informed by textual\nknowledge from LLMs, while the graph modality originates\nfrom the molecular graph structure. Our experimental results\nunderscore the effectiveness of MolGraph-LarDo in improving\nthe performance of the downstream molecular property predic-\ntion while reducing the cost of obtaining specialized domain\nknowledge.\nThe contributions of this work are summarized as follows:\n\u2022\n\u2022\nThe proposed method leverages the retrieval and gen-\neration capabilities of LLMs to overcome the time-\nconsuming and labor-intensive process of biomedical do-\nmain literature screening and pre-processing in molecular\nrepresentation learning.\n\u2022 Existing methods that integrate general LLMs into molec-\nular tasks may suffer from hallucinations and lack preci-\nsion in generating domain-specific knowledge. To address\nthese issues, we introduce a novel framework for molec-\nular graph representation learning which integrates LLMs\nand DSMs (MolGraph-LarDo).\n\u2022 We demonstrate the effectiveness of our proposed method\nthrough extensive experiments. Our research focuses on\nleveraging both LLMs and DSMs to advance molecular\nrepresentation studies, which could also benefit other\nLLM-related approaches in broader research areas."}, {"title": "II. RELATED WORK", "content": "A. Molecular Representation Learning\nMolecular representation learning has witnessed significant\nadvancements in recent years, particularly with methods in-\nvolving pre-training frameworks. GROVER [11] utilizes the\nmessage passing networks with a Transformer-style archi-\ntecture to pretrain models with unlabeled molecular data.\nGraphMVP [12] employs self-supervised and pre-training\nstrategies to pretrain models using 3D geometric information.\nPhysChem [17] is a deep learning framework designed to learn\nmolecular representations using external physical and chemical\ninformation. MG-BERT [1] is a BERT-based pre-training\nframework that relies on large amounts of unlabeled data for\nmolecular representation learning. MGSSL [13] is a denoising-\nbased pre-training technique for molecular data. MolCLR [14]\nis a self-supervised framework that leverages large amounts\nof unlabeled data to pretrain molecules through contrastive\nlearning with GNNs. KANO [15] integrates domain-specific\nknowledge graphs into the pre-training process of molecular\nrepresentation learning. Although these pre-training methods\nhave achieved good performance in molecular representation\nlearning, they rely on extensive specialized knowledge, in-\ncluding large amounts of unlabeled data and external domain\nknowledge, which can be costly to obtain.\nB. Large Language Models for Molecular Task\nIn recent years, there has been a surge of interest in\nleveraging large language models (LLMs) for molecular tasks,\nas evidenced by some review papers and models in the\nfield. Researchers provide a comprehensive review [24] of\nLLMs on graphs, detailing the graph scenarios suitable for\nLLMs and comparing methods within each scenario. Also,\nresearchers [25] explore potential directions for future research\nin molecular science using LLMs. GPT-MolBERTa [21] is a\nBERT-based model that predicts molecular properties using\ntextual descriptions generated by ChatGPT. LLM4mol [22]\nleverages both in-context classification results and the LLM's\ngeneration of new representations to enhance molecular prop-\nerty prediction. MolTailor [23] optimizes representations by\nemphasizing task-relevant features, with virtual task descrip-\ntions generated by the LLM. However, existing LLM-related\nmethods often lack precision in generating domain-specific\nknowledge due to hallucinations. In contrast, the proposed\nMolGraph-LarDo integrates both LLMs and DSMs to ensure\nthe accuracy of generated domain-specific knowledge."}, {"title": "III. PRELIMINARIES", "content": "Molecular graph representation learning is a preliminary\ntask in molecular property prediction, which can be classified\ninto qualitative molecular property classification and quantita-\ntive molecular property regression.\nDefinition 1. (Molecular Graph): A molecular graph is a graph\n\\(G = (V,E,X)\\) representing a molecule, where \\(V\\) denotes the\nset of nodes representing atoms, \\(E\\) represents the set of edges\nrepresenting chemical bonds, and \\(X\\) corresponds to the matrix\nof node features.\nDefinition 2. (Molecular Property Classification): Given a\nset of molecular graphs \\(G = {G_1, G_2,...}\\) and a set of\nclasses \\(C = {C_1, C_2,...}\\), the objective of Molecular Property\nClassification is to learn a mapping function \\(f: G\\rightarrow C\\) that\npredicts the class \\(c_i \\in C\\) of each molecular graph \\(G_i \\in G\\).\nDefinition 3. (Molecular Property Regression): Given a set\nof molecular graphs \\(G = {G_1, G_2,...}\\), the objective of\nMolecular Property Regression is to learn a mapping function\n\\(f: G\\rightarrow R\\) that predicts the scalar score \\(s_i \\in R\\) of each\nmolecular graph \\(G_i \\in G\\)."}, {"title": "IV. METHODOLOGY", "content": "In this section, we present how to utilize both LLMs and\nDSMs to enhance graph contrastive learning for molecular\nrepresentation learning. In Section IV-A, we first introduce\nthe overview of the proposed MolGraph-LarDo. Section IV-B\nshows the proposed two-stage prompt strategy in detail. The\nDSM is introduced in this step to implement knowledge\ncalibration. Section IV-C elaborates the molecular graph-\ntext alignment, which helps to inject the domain knowledge\nfrom LLMs and DSMs into the process of graph contrastive\nlearning.\nA. Overview\nFigure 1 illustrates the overview of the proposed MolGraph-\nLarDo, which mainly comprises two parts: the two-stage\nprompt strategy and the molecular Graph-Text Alignment.\nFirstly, we design a two-stage prompt strategy to extract\ndomain knowledge with respect to the given molecule. As the\nname suggested, it consists of two stages. In Stage 1, we use\na dataset-specific prompt, as presented in Section IV-B1, to\ngenerate relevant molecular properties to the given dataset. We\nterm these properties as the Molecular Description Template\n(MD-Template) since they could be included as part of the\nprompt for generating molecular description. We use the\nDSM to implement knowledge calibration, such as employing\nRDKit to calculate molecular domain-related metrics for a\ngiven molecule. The output of the DSM is termed Calibrated\nKnowledge. In Stage 2, we use both the MD-Template and\nthe Calibrated Knowledge to design a sample-specific prompt,\nas introduced in Section IV-B3. Being fed with the sample-\nspecific prompt, LLM is capable of providing Molecular De-\nscription Text (MD-Text), which contains specific information\nabout the dataset-relevant properties of a given molecule.\nSecondly, we introduce a multi-modal alignment method, as\npresented in Section IV-C, to enhance contrastive learning by\naligning the molecular graph to its corresponding description\ntext. Specifically, MD-Texts generated by LLM are fed into\na text encoder to derive text embeddings representing the\nmolecule's description. Similarly, the molecular graphs are in-\nputted into a graph encoder to obtain graph embeddings. Given\na molecule in the dataset, its molecular graph embeddings and\ncorresponding description text embeddings are considered as\npositive pairs in contrastive learning, aiming to optimize their\nagreement. After the pre-training, the pretrained graph encoder\ncan be used as a model for downstream molecular tasks.\nB. The Two-stage Prompt Strategy\nIn this part, we aim to employ LLMs and DSMs to provide\ndomain knowledge about a given molecule for the subsequent\nmolecular pre-training. As shown in Figure 2, we elaborate the\nproposed the two-stage prompt strategy with a detailed case\nas follows.\n1) Stage 1: Dataset-specific Prompt for MD-Template\nGeneration: In this stage, we design a dataset-specific prompt\nto generate several relevant molecular properties, which we\nterm as MD-Template. Naturally, this template is unique and\nconsistent for each dataset. In the following stage, the sample-\nagnostic template will be used to direct the generation of\nMD-Text, as shown in Section IV-B3. This process serves to\nimprove the relevance of the generated texts to the specific\ndataset under consideration.\nIn the left part of Figure 2, we illustrate the dataset-specific\nprompt for BBBP, a benchmark dataset sourced from Molecu-\nleNet [26]. This involves collecting detailed information about\nthe given dataset, including its name, description, task type,\nand target variable. We then integrate this information into the\nprompt and organize it using a predefined format for clarity\nand consistency. It is noteworthy that the specific details about\nthe dataset are obtained from external sources of information.\nThis process can be viewed as a basic application of Retrieval\nAugmented Generation (RAG), a technology designed to mit-\nigate the problem of hallucination in LLM [27]. For example,\nin cases where an LLM may not clearly understand the term\n\"BBBP\" based solely on its four-letter format, integrating\nRAG in the prompt allows us to provide additional external\nknowledge about the dataset. This approach effectively miti-\ngates hallucination, thereby enhancing the correctness of the\ngenerated content.\nAfter inputting the dataset-specific prompt, a LLM would\ngenerate several molecular properties along with explanations\nof their relevance to the dataset. Based on the case illustrated\nin Figure 2, the LLM suggests that \"Lipophilicity\" is a relevant\nproperty concerning the dataset \"BBBP,\" which stands for\nblood-brain barrier penetration, stating that \"Compounds with\nhigher lipophilicity are more likely to penetrate the barrier.\"\nConsequently, we collect these properties as MD-Template and\ninclude it as part of the prompt in the following stage.\n2) Domain-specific Small Models for Knowledge Calibra-\ntion: LLMs designed for general purposes can occasionally\nproduce inaccurate domain-specific information and strug-\ngle with precise calculations. To address this, we introduce\nDomain-Specific Small Models (DSMs) within our framework\nto enhance the accuracy of molecular textual information. We\nrefer to this process as knowledge calibration. The necessity\nof knowledge calibration is demonstrated in the experimental\npart in Section VI-C.\nSpecifically, we utilize the RDKit package, a basic type\nof DSMs, to calculate domain-specific metrics for the given\nmolecule, such as \"Molecular weight\" and \"LogP\". These\nmetrics are selected from the MD-Template. The output results\nfrom the DSM are saved in a specific format, for example,\n\"LogP of <MOLECULE >: 4.635\". We refer to these results\nas calibrated knowledge, which will be incorporated into the\nprompt for the subsequent stage.\n3) Stage 2: Sample-specific Prompt for MD-Text Gen-\neration: In this stage, the MD-Template obtained in Sec-\ntion IV-B1 and the calibrated knowledge from Section IV-B2\nare incorporated into the sample-specific prompt. This prompt\nis then employed to generate MD-Text for each molecule in the\ndataset. The MD-Text provides molecular textual information\nand is beneficial for the subsequent graph contrastive learning,\nas elaborated later in Section IV-C.\nAs illustrated in the right part of Figure 2, a sample-\nspecific prompt consists of the MD-Template, the cali-\nbrated knowledge, and the SMILES notation of a molecule\nsample. Since the MD-Template contains properties rel-\nevant to the given dataset, we anticipate the LLM to\ngenerate detailed information regarding these properties\nfor the specified molecule sample. In this example, we\nquery for the detailed information regarding relevant prop-\nerties including \"Lipophilicity\", \"Molecular weight\" and\n\"Hydrogen bond donors and acceptors\" for the molecule\n\"C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl\". In the\nright part of Figure 2, the MD-Text includes the biochemical\ninformation about \"Lipophilicity\", \"Molecular weight\" and\n\"Hydrogen bond donors and acceptors\". The MD-Text of\nmolecules serves as supplementary information for the original\ndataset, akin to a form of data augmentation. This MD-Text\nwill be preserved for future use, contributing to the graph\ncontrastive learning process in the proposed framework. The\nintegration of LLMs and DSMs provides specialized domain\nknowledge efficiently, which is invaluable, especially when\nconsidering the expert manpower required to accomplish the\nsame task manually."}, {"title": "C. Molecular Graph and MD-Text Alignment", "content": "In this part, we specifically introduce how to utilize the\nMD-Text obtained from Section IV-B3 to empower molecular\ngraph pre-training based on contrastive learning. For each\nsample, it has its own graph structure and MD-Text. Here, the\ngraph structure is sourced from the dataset, while the MD-Text is generated by the LLM and the DSM using the the\ntwo-stage prompt strategy. Considering that each sample now\npossesses multi-modal information (graph and text), inspired\nby CLIP [28], we align the graph representation and text\nrepresentation of the same sample by maximizing their agree-\nment. The underlying intuition is that graph representations\nof molecules with similar textual information about their\nproperties should also be similar, as reflected by MD-text.\nLet \\(G\\) represent the set of all graphs and \\(T\\) denote the set\nof all texts. For each sample \\(i\\), its molecular graph \\(g_i \\in G\\) is processed by a graph encoder \\(f_g(\\cdot) : G \\rightarrow R^{d_1}\\). Similarly,\nthe MD-Text \\(t_i \\in T\\) associated with sample \\(i\\) is processed\nby a text encoder \\(f_t(\\cdot) : T \\rightarrow R^{d_2}\\). Subsequently, the\noutput embeddings from \\(f_g(\\cdot)\\) and \\(f_t(\\cdot)\\) are projected to the\nsame dimension for joint training. Using the aforementioned\nnotation, the computation of the joint multi-modal embeddings\n\\(h\\) can be formulated as follows:\n\\(h = W_gf_g(g_i)\\)\n\\(h = W_tf_t(t_i)\\)\nwhere \\(W_g \\in R^{d \\times d_1}\\) is the transformation matrix to project the\noutput embedding of the graph encoder into the joint multi-\nmodal embedding space. Similarly, \\(W_t \\in R^{d \\times d_2}\\) serves the\nsame purpose for the output embedding of the text encoder.\nIn practice, the graph encoder is typically a GNN, while the\ntext encoder is usually an NLP-based sequence model. We will\ndescribe the specific types of encoders used in this paper later\nin the Section V-C.\nIn line with CLIP, we devise a symmetric loss function to\nalign the graph embedding and text embedding of each sample.\nSpecifically, we compute the batch-wise loss \\(L_g\\), where the\ntext embedding \\(h_t^i\\) of the current sample serves as positive\ncontrast and text embeddings \\(h_t^j\\) of other samples in the batch\nare regarded as negative contrast. Similarly, \\(L_t\\) is determined\nin the same manner, except that the positions of the graph and\ntext are interchanged. The calculation of \\(L_g\\) and \\(L_t\\) could be\nformulated as follows:\n\\(L_g = - \\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(h_g^i \\cdot h_t^i / \\tau)}{\\exp(h_g^i \\cdot h_t^i / \\tau) + \\sum_{j \\neq i} \\exp(h_g^i \\cdot h_t^j / \\tau)}\\)\n\\(L_t = - \\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(h_t^i \\cdot h_g^i / \\tau)}{\\exp(h_t^i \\cdot h_g^i / \\tau) + \\sum_{j \\neq i} \\exp(h_t^i \\cdot h_g^j / \\tau)}\\)\nwhere \\(h_g^i\\) and \\(h_t^i\\) represent the graph and text embeddings\nof the \\(i\\)-th sample respectively; \\(N\\) is the batch size; \\(\\tau\\) is the\ntemperature parameter of InfoNCE [29]. Then the final loss\nis calculated by averaging \\(L_g\\) and \\(L_t\\) as:\n\\(L = \\frac{1}{2} (L_g + L_t)\\)\nFor optimization, we update the weights of graph encoder by\nminimizing \\(L\\). After pre-training, the pretrained graph encoder\ncan be fine-tuned for downstream molecular tasks."}, {"title": "V. EXPERIMENTAL SETTINGS", "content": "To evaluate the proposed MolGraph-LarDo, we conduct ex-\nperiments over seven datasets sourced from MoleculeNet [26].\nThese molecular datasets can be classified into two types based\non task: classification and regression.\n1) Classification Datasets: BBBP dataset tracks whether\na molecule can penetrate the blood-brain barrier and includes\nbinary labels for over 2,000 molecules. BACE dataset contains\nqualitative binary labels of 1,513 molecules that inhibit human\nB-secretase 1 (BACE-1) SIDER dataset includes data on 1,427\napproved drug molecules, with side effects categorized into 27\nsystem organ classes according to MedDRA classifications.\n2) Regression Datasets: FreeSolv dataset includes hydra-\ntion free energies of small molecules in water, derived from\nboth experimental measurements and alchemical free energy\ncalculations. ESOL dataset records the water solubility of\n1,128 molecules. Lipo dataset measures the lipophilicity of\naround 4,200 drug molecules, which impacts both membrane\npermeability and solubility. QM7 is a dataset derived from the\nGDB-13 database, containing 6,830 molecules. It is utilized for\nstudying molecular structures and their atomization energies.\nB. Baselines\nTo demonstrate the superiority of the proposed MolGraph-\nLarDo, we compare it with five baseline methods, which can\nbe classified into two types as follows.\n1) Supervised Methods: GCN [30] and GIN [31] are two\nclassic and representative graph neural network models.\n2) Pre-training Methods: Hu et al. [32] introduced a pre-\ntraining strategy for GNNs that enhances learning at both\nindividual node and entire graph levels. GROVER [11] inte-\ngrates the message passing networks with a Transformer-style\narchitecture to pretrain models from unlabelled molecular data.\nMolCLR [14] is a self-supervised framework that leverages\nlarge amounts of unlabelled data to pretrain molecules via\ngraph contrastive learning."}, {"title": "C. Implementation Details", "content": "For the implementation of the proposed MolGraph-LarDo,\nwe use GIN as the graph encoder and Sentence-BERT [33]\nas the text encoder. The GIN consists of five layers with\n256 hidden dimensions. The weights of the text encoder are\nfrozen, except for the last output linear layer, which has 256\nhidden dimensions. The LLM version used in our experiment\nis Mistral-7B-Instruct-v0.2, and the DSM version is RDKit-2023.3.2. Following [14], we convert SMILES strings into\ngraphs, where node features are represented by atomic number\nand chirality, and edge features are represented by bond type\nand bond direction.\nAll datasets are split into training, validation, and test sets\nusing a scaffold split method with a ratio of 8:1:1. In the pre-training stage, we use Adam as the optimizer with a learning\nrate set to 0.005. The training consists of 100 epochs, including\n10 warm-up epochs, and uses a batch size of 32. The model's\npretrained weights are saved when the lowest validation loss\nis encountered. In the fine-tuning stage, we search the learning\nrate within [0.0001, 0.0005] for the fine-tuned graph encoders\nwith a maximum of 100 epochs. The batch size is set to 32\nfor all datasets except FreeSolv, for which a batch size of 8\nis used.\nFor evaluation, the ROC-AUC metric is used for classifica-tion datasets, whereas RMSE is employed for all regression\ndatasets except for QM7, for which MAE is used. We repeat\nthe experiments three times and report the mean and standard\ndeviation. For multi-task datasets, the results are the average\nof each task's outcomes."}, {"title": "VI. EXPERIMENTS AND DISCUSSION", "content": "In this section, to verify the effectiveness of the proposed\nMolGraph-LarDo, we conduct experiments to investigate fol-\nlowing research questions:\n\u2022 RQ1: How does MolGraph-LarDo perform on molecular\nproperty prediction as compared to other baseline meth-\nods? (Section VI-A)\n\u2022 RQ2: Are the proposed two-stage prompt strategy and\ndomain-specific small models effective in MolGraph-\nLarDo? (Section VI-B)\n\u2022 RQ3: What is the quality of the MD-Text generated by\nMolGraph-LarDo? (Section VI-C)\n\u2022 RQ4: Does the graph-text alignment mechanism of\nMolGraph-LarDo work as expected? (Section VI-D)\nA. Molecular Property Prediction (RQ1)\nIn this section, we compare the performance of our pro-\nposed method with several baseline methods. As shown in\nTABLE I, the main observations are as follows: (1) The pre-\ntraining methods generally outperform the supervised methods\nin most cases. This advantage likely arises from the pre-\ntraining process, which incorporates external knowledge from\nlarge amount of unlabeled datasets. (2) On the seven datasets,\nthe proposed MolGraph-LarDo consistently outperforms all\nbaseline methods, including those requiring large amounts\nof domain-specific data for pre-training. Thus, MolGraph-LarDo achieves strong performance without the need for time-\nconsuming and expensive domain knowledge acquisition. This\nsuccess can be attributed not only to the high efficiency of\nLLMs in generating data but also to the accuracy of domain\nknowledge provided by DSMs.\nB. Ablation Study: Effect of the Two-stage Prompt Strategy\nand Domain-specific Small Models (RQ2)\nTo investigate the impact of the designed two-stage prompt\nstrategy and domain-specific small Models, we compare the\nthree versions of MolGraph-LarDo. We report the results in\nTABLE II. For clarification, \"w/o\" stands for \"without,\" while\n\"TPS\" and \"DSM\" are abbreviations for Two-stage Prompt\nStrategy and Domain-specific Small Models, respectively. TA-\nBLE II shows that MolGraph-LarDo outperforms all other\nversions, which justifies the rationality of the design of the\ntwo-stage prompt strategy and the utilization of Domain-\nspecific Small Models in the framework.\nC. Case Study of the MD-Text (RQ3)\nTo more clearly demonstrate the differences between the\nthree versions of MolGraph-LarDo discussed in Section VI-B,\nwe present the case study of both the input prompt and the\noutput MD-Text for each version in Figure 3. As the figure\nshows, Version 1 only generates a general description of the\ngiven molecule. Without TPS, it cannot ensure the relevance\nof the knowledge to specific tasks, and without DSM, it cannot\nensure the accuracy of the knowledge. In contrast, Version 2\nintroduces TPS, making the generated MD-Text relevant to\nthe given task. However, the accuracy of some domain-related\nmetrics remains insufficient. For instance, several metrics\nhighlighted in the red box yield incorrect answers. This is due\nto the LLM's poor performance in domain-specific scientific\ncalculations and the accompanying issue of hallucinations. For\nVersion 3, i.e., MolGraph-LarDo, both TPS and DSM are\nutilized. They ensure the relevance of the generated MD-Text\nand also correct the erroneous domain-related metrics from\nVersion 2. For example, it calibrate the molecular weight from\nthe incorrect 152.15 g/mol to the accurate 108.1 g/mol. This\nobservation is in accord with our design motivation that the\nDSM are introduced to calibrate the knowledge provided by\nthe LLM, enabling the LLM to produce more accurate textual\ndescriptions of molecular samples.\nD. Ablation Study: Effect of the Graph-Text Alignment (RQ4)\nTo validate the effectiveness of the proposed graph-text\nalignment, we conduct experiments fro ablation study. As\nshown in Figure 4, MolGraph-LarDo consistently outperforms\nthe version without the graph-text alignment mechanism. This\nobservation supports the design idea that graph representations\nof molecules with similar textual descriptions of properties\nshould also be similar, thereby enhancing performance in\ndownstream molecular tasks."}, {"title": "VII. CONCLUSION", "content": "LLMs excel at understanding and efficiently providing\ngeneral knowledge. Existing methods using general LLMS\nfor molecular tasks may suffer from hallucinations and lack\nprecision in domain-specific knowledge. To address the issue,\nwe propose a novel framework for Molecular Graph represen-tation learning that integrates LLMs with DSMs (MolGraph-\nLarDo). Specifically, we develop a two-stage prompt strategy\nwhere the DSM calibrates domain-specific knowledge for the\nLLM, enabling it to produce more accurate textual descriptions\nof molecular samples. MolGraph-LarDo then employs a multi-\nmodal graph-text alignment method for pre-training molecular\ngraphs. Our results demonstrate that MolGraph-LarDo ef-\nfectively enhances downstream molecular property prediction\nwhile reducing the cost of acquiring domain-specific knowl-\nedge used for pre-training. The source code for reproducibility\nis available at https://github.com/zhangtia16/MolGraph-LarDo"}]}