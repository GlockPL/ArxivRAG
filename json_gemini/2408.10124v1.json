{"title": "Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models", "authors": ["Tianyu Zhang", "Yuxiang Ren", "Chengbin Hou", "Hairong Lv", "Xuegong Zhang"], "abstract": "Molecular property prediction is a crucial foundation for drug discovery. In recent years, pre-trained deep learning models have been widely applied to this task. Some approaches that incorporate prior biological domain knowledge into the pre-training framework have achieved impressive results. However, these methods heavily rely on biochemical experts, and retrieving and summarizing vast amounts of domain knowledge literature is both time-consuming and expensive. Large Language Models (LLMs) have demonstrated remarkable performance in understanding and efficiently providing general knowledge. Nevertheless, they occasionally exhibit hallucinations and lack precision in generating domain-specific knowledge. Conversely, Domain-specific Small Models (DSMs) possess rich domain knowledge and can accurately calculate molecular domain-related metrics. However, due to their limited model size and singular functionality, they lack the breadth of knowledge necessary for comprehensive representation learning. To leverage the advantages of both approaches in molecular property prediction, we propose a novel Molecular Graph representation learning framework that integrates Large language models and Domain-specific small models (MolGraph-LarDo). Technically, we design a two-stage prompt strategy where DSMs are introduced to calibrate the knowledge provided by LLMs, enhancing the accuracy of domain-specific information and thus enabling LLMs to generate more precise textual descriptions for molecular samples. Subsequently, we employ a multi-modal alignment method to coordinate various modalities, including molecular graphs and their corresponding descriptive texts, to guide the pre-training of molecular representations. Extensive experiments demonstrate the effectiveness of the proposed method.", "sections": [{"title": "I. INTRODUCTION", "content": "Molecular representation learning is a fundamental and preliminary task in the field of drug discovery [1]-[5]. It serves as the basis for following tasks, including molecular property prediction, molecular scaffold optimization, and targeted molecular generation, all of which require robust and expressive molecular representations. Over the years, deep learning techniques have significantly advanced this domain, offering promising avenues for developing accurate and efficient models. Molecules, often depicted as graphs, feature nodes representing atoms and edges representing chemical bonds [3]. Consequently, graph neural networks (GNNs), as a sophisticated and efficacious framework for graph representation learning [6]\u2013[8], have garnered significant attention and found extensive application in the domain of molecular representation learning [9], [10].\nHowever, akin to other deep learning methods, GNN-based approaches are often constrained by the availability of labeled data. Hence, researchers are exploring self-supervised methods for molecular representation learning [11]\u2013[16]. These self-supervised methods typically follow a paradigm where models are first pre-trained using a large amount of unlabeled molecular samples and then fine-tuned for downstream tasks such as molecule property prediction. The adoption of these pre-trained deep learning models have gained widespread adoption due to their capacity to leverage large-scale unlabeled data for improved performance.\nRecent research has witnessed a notable trend towards integrating prior biological domain knowledge into the pre-trained frameworks [5], [15], [17], [18]. Incorporating biomedical domain knowledge into the pre-training process enhances the models' ability to learn representations for molecules, resulting in remarkable achievements in molecular property prediction tasks. Despite the advancements of these pre-trained frameworks with domain knowledge, a significant bottleneck in this approach is the heavy reliance on biochemical experts to acquire specialized domain knowledge. This process is not only time-consuming but also entails substantial expenses. To address this challenge, recent advancements in Natural Language Processing (NLP) have revealed the potential of Large Language Models (LLMs) to efficiently provide knowledge. Leveraging this capability, some very recent studies have begun exploring the integration of LLMs into graph domain [19], [20] including molecular tasks [21]\u2013[23].\nNevertheless, LLMs for general purpose occasionally exhibit hallucinations and lack precision in generating domain-specific knowledge. For example, an LLM may generate incorrect specialized knowledge or encounter difficulties with mathematical calculations. Thus, the existing methods for introducing LLMs to address molecular tasks may suffer from this drawback. In contrast, Domain-Specific Small Models (DSMs) have extensive domain knowledge and can accurately compute domain-related metrics for molecules. Tools such as the RDKit package, a basic type of DSMs, provide specialized calculations. However, due to their limited size and singular functionality, DSMs lack the breadth of knowledge required for comprehensive representation learning.\nTherefore, to leverage the advantages of both LLMs and DSMs, we propose a novel Molecular Graph representation learning framework which integrates Large language models and Domain-specific small models, called MolGraph-LarDo. Specifically, we design a two-stage prompt strategy where DSMs are introduced to calibrate the knowledge provided by LLMs, aiming to obtain more accurate domain-specific information and thus enabling LLMs to generate more precise textual descriptions of molecular samples. To further avoid hallucination issues, the designed prompt incorporates both dataset-specific and sample-specific information. Subsequently, a multi-modal graph-text alignment method is employed in MolGraph-LarDo to guide the pre-training of molecular graphs. The text modality is informed by textual knowledge from LLMs, while the graph modality originates from the molecular graph structure. Our experimental results underscore the effectiveness of MolGraph-LarDo in improving the performance of the downstream molecular property prediction while reducing the cost of obtaining specialized domain knowledge.\nThe contributions of this work are summarized as follows:\n\u2022 The proposed method leverages the retrieval and generation capabilities of LLMs to overcome the time-consuming and labor-intensive process of biomedical domain literature screening and pre-processing in molecular representation learning.\n\u2022 Existing methods that integrate general LLMs into molecular tasks may suffer from hallucinations and lack precision in generating domain-specific knowledge. To address these issues, we introduce a novel framework for molecular graph representation learning which integrates LLMs and DSMs (MolGraph-LarDo).\n\u2022 We demonstrate the effectiveness of our proposed method through extensive experiments. Our research focuses on leveraging both LLMs and DSMs to advance molecular representation studies, which could also benefit other LLM-related approaches in broader research areas."}, {"title": "II. RELATED WORK", "content": "A. Molecular Representation Learning\nMolecular representation learning has witnessed significant advancements in recent years, particularly with methods involving pre-training frameworks. GROVER [11] utilizes the message passing networks with a Transformer-style architecture to pretrain models with unlabeled molecular data. GraphMVP [12] employs self-supervised and pre-training strategies to pretrain models using 3D geometric information. PhysChem [17] is a deep learning framework designed to learn molecular representations using external physical and chemical information. MG-BERT [1] is a BERT-based pre-training framework that relies on large amounts of unlabeled data for molecular representation learning. MGSSL [13] is a denoising-based pre-training technique for molecular data. MolCLR [14] is a self-supervised framework that leverages large amounts of unlabeled data to pretrain molecules through contrastive learning with GNNs. KANO [15] integrates domain-specific knowledge graphs into the pre-training process of molecular representation learning. Although these pre-training methods have achieved good performance in molecular representation learning, they rely on extensive specialized knowledge, including large amounts of unlabeled data and external domain knowledge, which can be costly to obtain.\nB. Large Language Models for Molecular Task\nIn recent years, there has been a surge of interest in leveraging large language models (LLMs) for molecular tasks, as evidenced by some review papers and models in the field. Researchers provide a comprehensive review [24] of LLMs on graphs, detailing the graph scenarios suitable for LLMs and comparing methods within each scenario. Also, researchers [25] explore potential directions for future research in molecular science using LLMs. GPT-MolBERTa [21] is a BERT-based model that predicts molecular properties using textual descriptions generated by ChatGPT. LLM4mol [22] leverages both in-context classification results and the LLM's generation of new representations to enhance molecular property prediction. MolTailor [23] optimizes representations by emphasizing task-relevant features, with virtual task descriptions generated by the LLM. However, existing LLM-related methods often lack precision in generating domain-specific knowledge due to hallucinations. In contrast, the proposed MolGraph-LarDo integrates both LLMs and DSMs to ensure the accuracy of generated domain-specific knowledge."}, {"title": "III. PRELIMINARIES", "content": "Molecular graph representation learning is a preliminary task in molecular property prediction, which can be classified into qualitative molecular property classification and quantitative molecular property regression.\nDefinition 1. (Molecular Graph): A molecular graph is a graph G = (V,E,X) representing a molecule, where V denotes the set of nodes representing atoms, E represents the set of edges representing chemical bonds, and X corresponds to the matrix of node features.\nDefinition 2. (Molecular Property Classification): Given a set of molecular graphs G = {G_1, G_2,...} and a set of classes C = {C_1, C_2,...}, the objective of Molecular Property Classification is to learn a mapping function f: G\u2192 C that predicts the class c_i \u2208 C of each molecular graph G_i \u2208 G.\nDefinition 3. (Molecular Property Regression): Given a set of molecular graphs G = {G_1, G_2,...}, the objective of Molecular Property Regression is to learn a mapping function f: G\u2192 R that predicts the scalar score s_i \u2208 R of each molecular graph G_i \u2208 G."}, {"title": "IV. METHODOLOGY", "content": "In this section, we present how to utilize both LLMs and DSMs to enhance graph contrastive learning for molecular representation learning. In Section IV-A, we first introduce the overview of the proposed MolGraph-LarDo. Section IV-B shows the proposed two-stage prompt strategy in detail. The DSM is introduced in this step to implement knowledge calibration. Section IV-C elaborates the molecular graph-text alignment, which helps to inject the domain knowledge from LLMs and DSMs into the process of graph contrastive learning.\nA. Overview\nFigure 1 illustrates the overview of the proposed MolGraph-LarDo, which mainly comprises two parts: the two-stage prompt strategy and the molecular Graph-Text Alignment.\nFirstly, we design a two-stage prompt strategy to extract domain knowledge with respect to the given molecule. As the name suggested, it consists of two stages. In Stage 1, we use a dataset-specific prompt, as presented in Section IV-B1, to generate relevant molecular properties to the given dataset. We term these properties as the Molecular Description Template (MD-Template) since they could be included as part of the prompt for generating molecular description. We use the DSM to implement knowledge calibration, such as employing RDKit to calculate molecular domain-related metrics for a given molecule. The output of the DSM is termed Calibrated Knowledge. In Stage 2, we use both the MD-Template and the Calibrated Knowledge to design a sample-specific prompt, as introduced in Section IV-B3. Being fed with the sample-specific prompt, LLM is capable of providing Molecular Description Text (MD-Text), which contains specific information about the dataset-relevant properties of a given molecule.\nSecondly, we introduce a multi-modal alignment method, as presented in Section IV-C, to enhance contrastive learning by aligning the molecular graph to its corresponding description text. Specifically, MD-Texts generated by LLM are fed into a text encoder to derive text embeddings representing the molecule's description. Similarly, the molecular graphs are inputted into a graph encoder to obtain graph embeddings. Given a molecule in the dataset, its molecular graph embeddings and corresponding description text embeddings are considered as positive pairs in contrastive learning, aiming to optimize their agreement. After the pre-training, the pretrained graph encoder can be used as a model for downstream molecular tasks.\nB. The Two-stage Prompt Strategy\nIn this part, we aim to employ LLMs and DSMs to provide domain knowledge about a given molecule for the subsequent molecular pre-training. As shown in Figure 2, we elaborate the proposed the two-stage prompt strategy with a detailed case as follows.\n1) Stage 1: Dataset-specific Prompt for MD-Template Generation: In this stage, we design a dataset-specific prompt to generate several relevant molecular properties, which we term as MD-Template. Naturally, this template is unique and consistent for each dataset. In the following stage, the sample-agnostic template will be used to direct the generation of MD-Text, as shown in Section IV-B3. This process serves to improve the relevance of the generated texts to the specific dataset under consideration.\nIn the left part of Figure 2, we illustrate the dataset-specific prompt for BBBP, a benchmark dataset sourced from MoleculeNet [26]. This involves collecting detailed information about the given dataset, including its name, description, task type, and target variable. We then integrate this information into the prompt and organize it using a predefined format for clarity and consistency. It is noteworthy that the specific details about the dataset are obtained from external sources of information. This process can be viewed as a basic application of Retrieval Augmented Generation (RAG), a technology designed to mitigate the problem of hallucination in LLM [27]. For example, in cases where an LLM may not clearly understand the term \"BBBP\" based solely on its four-letter format, integrating RAG in the prompt allows us to provide additional external knowledge about the dataset. This approach effectively mitigates hallucination, thereby enhancing the correctness of the generated content.\nAfter inputting the dataset-specific prompt, a LLM would generate several molecular properties along with explanations of their relevance to the dataset. Based on the case illustrated in Figure 2, the LLM suggests that \"Lipophilicity\" is a relevant property concerning the dataset \"BBBP,\" which stands for blood-brain barrier penetration, stating that \"Compounds with higher lipophilicity are more likely to penetrate the barrier.\" Consequently, we collect these properties as MD-Template and include it as part of the prompt in the following stage.\n2) Domain-specific Small Models for Knowledge Calibration: LLMs designed for general purposes can occasionally produce inaccurate domain-specific information and struggle with precise calculations. To address this, we introduce Domain-Specific Small Models (DSMs) within our framework to enhance the accuracy of molecular textual information. We refer to this process as knowledge calibration. The necessity of knowledge calibration is demonstrated in the experimental part in Section VI-C.\nSpecifically, we utilize the RDKit package, a basic type of DSMs, to calculate domain-specific metrics for the given molecule, such as \"Molecular weight\" and \"LogP\". These metrics are selected from the MD-Template. The output results from the DSM are saved in a specific format, for example, \"LogP of <MOLECULE >: 4.635\". We refer to these results as calibrated knowledge, which will be incorporated into the prompt for the subsequent stage.\n3) Stage 2: Sample-specific Prompt for MD-Text Generation: In this stage, the MD-Template obtained in Section IV-B1 and the calibrated knowledge from Section IV-B2 are incorporated into the sample-specific prompt. This prompt is then employed to generate MD-Text for each molecule in the dataset. The MD-Text provides molecular textual information and is beneficial for the subsequent graph contrastive learning, as elaborated later in Section IV-C.\nAs illustrated in the right part of Figure 2, a sample-specific prompt consists of the MD-Template, the calibrated knowledge, and the SMILES notation of a molecule sample. Since the MD-Template contains properties relevant to the given dataset, we anticipate the LLM to generate detailed information regarding these properties for the specified molecule sample. In this example, we query for the detailed information regarding relevant properties including \"Lipophilicity\", \"Molecular weight\" and \"Hydrogen bond donors and acceptors\" for the molecule \"C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl\". In the right part of Figure 2, the MD-Text includes the biochemical information about \"Lipophilicity\", \"Molecular weight\" and \"Hydrogen bond donors and acceptors\". The MD-Text of molecules serves as supplementary information for the original dataset, akin to a form of data augmentation. This MD-Text will be preserved for future use, contributing to the graph contrastive learning process in the proposed framework. The integration of LLMs and DSMs provides specialized domain knowledge efficiently, which is invaluable, especially when considering the expert manpower required to accomplish the same task manually."}, {"title": "C. Molecular Graph and MD-Text Alignment", "content": "In this part, we specifically introduce how to utilize the MD-Text obtained from Section IV-B3 to empower molecular graph pre-training based on contrastive learning. For each sample, it has its own graph structure and MD-Text. Here, the graph structure is sourced from the dataset, while the MD-Text is generated by the LLM and the DSM using the the two-stage prompt strategy. Considering that each sample now possesses multi-modal information (graph and text), inspired by CLIP [28], we align the graph representation and text representation of the same sample by maximizing their agreement. The underlying intuition is that graph representations of molecules with similar textual information about their properties should also be similar, as reflected by MD-text.\nLet G represent the set of all graphs and T denote the set of all texts. For each sample i, its molecular graph g_i \u2208 G is processed by a graph encoder f_g(\u00b7) : G \u2192 R^{d_1}. Similarly, the MD-Text t_i \u2208 T associated with sample i is processed by a text encoder f_t(\u00b7) : T \u2192 R^{d_2}. Subsequently, the output embeddings from f_g(\u00b7) and f_t(\u00b7) are projected to the same dimension for joint training. Using the aforementioned notation, the computation of the joint multi-modal embeddings h could be formulated as follows:\n$h_g = W_gf_g(g_i)$\n$h_t = W_tf_t(t_i)$\nwhere W_g \u2208 R^{d \u00d7 d_1} is the transformation matrix to project the output embedding of the graph encoder into the joint multi-modal embedding space. Similarly, W_t \u2208 R^{d \u00d7 d_2} serves the same purpose for the output embedding of the text encoder. In practice, the graph encoder is typically a GNN, while the text encoder is usually an NLP-based sequence model. We will describe the specific types of encoders used in this paper later in the Section V-C.\nIn line with CLIP, we devise a symmetric loss function to align the graph embedding and text embedding of each sample. Specifically, we compute the batch-wise loss L_g, where the text embedding h_t^i of the current sample serves as positive contrast and text embeddings h_t^j of other samples in the batch are regarded as negative contrast. Similarly, L_t is determined in the same manner, except that the positions of the graph and text are interchanged. The calculation of L_g and L_t could be formulated as follows:\n$L_g = -\\frac{1}{N} \\sum_{i=1}^N log \\frac{exp(h_g^i \\cdot h_t^i / \\tau)}{\\sum_{j=1}^N exp(h_g^i \\cdot h_t^j / \\tau)}$\n$L_t = -\\frac{1}{N} \\sum_{i=1}^N log \\frac{exp(h_t^i \\cdot h_g^i / \\tau)}{\\sum_{j=1}^N exp(h_t^i \\cdot h_g^j / \\tau)}$\nwhere $h_g^i$ and $h_t^i$ represent the graph and text embeddings of the i-th sample respectively; N is the batch size; \u03c4 is the temperature parameter of InfoNCE [29]. Then the final loss is calculated by averaging L_g and L_t as:\n$L = \\frac{1}{2} (L_g + L_t)$\nFor optimization, we update the weights of graph encoder by minimizing L. After pre-training, the pretrained graph encoder can be fine-tuned for downstream molecular tasks."}, {"title": "V. EXPERIMENTAL SETTINGS", "content": "A. Datasets\nTo evaluate the proposed MolGraph-LarDo, we conduct experiments over seven datasets sourced from MoleculeNet [26]. These molecular datasets can be classified into two types based on task: classification and regression.\n1) Classification Datasets: BBBP dataset tracks whether a molecule can penetrate the blood-brain barrier and includes binary labels for over 2,000 molecules. BACE dataset contains qualitative binary labels of 1,513 molecules that inhibit human B-secretase 1 (BACE-1) SIDER dataset includes data on 1,427 approved drug molecules, with side effects categorized into 27 system organ classes according to MedDRA classifications.\n2) Regression Datasets: FreeSolv dataset includes hydration free energies of small molecules in water, derived from both experimental measurements and alchemical free energy calculations. ESOL dataset records the water solubility of 1,128 molecules. Lipo dataset measures the lipophilicity of around 4,200 drug molecules, which impacts both membrane permeability and solubility. QM7 is a dataset derived from the GDB-13 database, containing 6,830 molecules. It is utilized for studying molecular structures and their atomization energies.\nB. Baselines\nTo demonstrate the superiority of the proposed MolGraph-LarDo, we compare it with five baseline methods, which can be classified into two types as follows.\n1) Supervised Methods: GCN [30] and GIN [31] are two classic and representative graph neural network models.\n2) Pre-training Methods: Hu et al. [32] introduced a pre-training strategy for GNNs that enhances learning at both individual node and entire graph levels. GROVER [11] integrates the message passing networks with a Transformer-style architecture to pretrain models from unlabelled molecular data. MolCLR [14] is a self-supervised framework that leverages large amounts of unlabelled data to pretrain molecules via graph contrastive learning.\nC. Implementation Details\nFor the implementation of the proposed MolGraph-LarDo, we use GIN as the graph encoder and Sentence-BERT [33] as the text encoder. The GIN consists of five layers with 256 hidden dimensions. The weights of the text encoder are frozen, except for the last output linear layer, which has 256 hidden dimensions. The LLM version used in our experiment is Mistral-7B-Instruct-v0.2, and the DSM version is RDKit-2023.3.2. Following [14], we convert SMILES strings into graphs, where node features are represented by atomic number and chirality, and edge features are represented by bond type and bond direction.\nAll datasets are split into training, validation, and test sets using a scaffold split method with a ratio of 8:1:1. In the pre-training stage, we use Adam as the optimizer with a learning rate set to 0.005. The training consists of 100 epochs, including 10 warm-up epochs, and uses a batch size of 32. The model's pretrained weights are saved when the lowest validation loss is encountered. In the fine-tuning stage, we search the learning rate within [0.0001, 0.0005] for the fine-tuned graph encoders with a maximum of 100 epochs. The batch size is set to 32 for all datasets except FreeSolv, for which a batch size of 8 is used.\nFor evaluation, the ROC-AUC metric is used for classification datasets, whereas RMSE is employed for all regression datasets except for QM7, for which MAE is used. We repeat the experiments three times and report the mean and standard deviation. For multi-task datasets, the results are the average of each task's outcomes."}, {"title": "VI. EXPERIMENTS AND DISCUSSION", "content": "In this section, to verify the effectiveness of the proposed MolGraph-LarDo, we conduct experiments to investigate following research questions:\n\u2022 RQ1: How does MolGraph-LarDo perform on molecular property prediction as compared to other baseline methods? (Section VI-A)\n\u2022 RQ2: Are the proposed two-stage prompt strategy and domain-specific small models effective in MolGraph-LarDo? (Section VI-B)\n\u2022 RQ3: What is the quality of the MD-Text generated by MolGraph-LarDo? (Section VI-C)\n\u2022 RQ4: Does the graph-text alignment mechanism of MolGraph-LarDo work as expected? (Section VI-D)\nA. Molecular Property Prediction (RQ1)\nIn this section, we compare the performance of our proposed method with several baseline methods. As shown in TABLE I, the main observations are as follows: (1) The pre-training methods generally outperform the supervised methods in most cases. This advantage likely arises from the pre-training process, which incorporates external knowledge from large amount of unlabeled datasets. (2) On the seven datasets, the proposed MolGraph-LarDo consistently outperforms all baseline methods, including those requiring large amounts of domain-specific data for pre-training. Thus, MolGraph-LarDo achieves strong performance without the need for time-consuming and expensive domain knowledge acquisition. This success can be attributed not only to the high efficiency of LLMs in generating data but also to the accuracy of domain knowledge provided by DSMs.\nB. Ablation Study: Effect of the Two-stage Prompt Strategy and Domain-specific Small Models (RQ2)\nTo investigate the impact of the designed two-stage prompt strategy and domain-specific small Models, we compare the three versions of MolGraph-LarDo. We report the results in TABLE II. For clarification, \"w/o\" stands for \"without,\" while \"TPS\" and \"DSM\" are abbreviations for Two-stage Prompt Strategy and Domain-specific Small Models, respectively. TABLE II shows that MolGraph-LarDo outperforms all other versions, which justifies the rationality of the design of the two-stage prompt strategy and the utilization of Domain-specific Small Models in the framework.\nC. Case Study of the MD-Text (RQ3)\nTo more clearly demonstrate the differences between the three versions of MolGraph-LarDo discussed in Section VI-B, we present the case study of both the input prompt and the output MD-Text for each version in Figure 3. As the figure shows, Version 1 only generates a general description of the given molecule. Without TPS, it cannot ensure the relevance of the knowledge to specific tasks, and without DSM, it cannot ensure the accuracy of the knowledge. In contrast, Version 2 introduces TPS, making the generated MD-Text relevant to the given task. However, the accuracy of some domain-related metrics remains insufficient. For instance, several metrics highlighted in the red box yield incorrect answers. This is due to the LLM's poor performance in domain-specific scientific calculations and the accompanying issue of hallucinations. For Version 3, i.e., MolGraph-LarDo, both TPS and DSM are utilized. They ensure the relevance of the generated MD-Text and also correct the erroneous domain-related metrics from Version 2. For example, it calibrate the molecular weight from the incorrect 152.15 g/mol to the accurate 108.1 g/mol. This observation is in accord with our design motivation that the DSM are introduced to calibrate the knowledge provided by the LLM, enabling the LLM to produce more accurate textual descriptions of molecular samples.\nD. Ablation Study: Effect of the Graph-Text Alignment (RQ4)\nTo validate the effectiveness of the proposed graph-text alignment, we conduct experiments fro ablation study. As shown in Figure 4, MolGraph-LarDo consistently outperforms the version without the graph-text alignment mechanism. This observation supports the design idea that graph representations of molecules with similar textual descriptions of properties should also be similar, thereby enhancing performance in downstream molecular tasks."}, {"title": "VII. CONCLUSION", "content": "LLMs excel at understanding and efficiently providing general knowledge. Existing methods using general LLMS for molecular tasks may suffer from hallucinations and lack precision in domain-specific knowledge. To address the issue, we propose a novel framework for Molecular Graph representation learning that integrates LLMs with DSMs (MolGraph-LarDo). Specifically, we develop a two-stage prompt strategy where the DSM calibrates domain-specific knowledge for the LLM, enabling it to produce more accurate textual descriptions of molecular samples. MolGraph-LarDo then employs a multi-modal graph-text alignment method for pre-training molecular graphs. Our results demonstrate that MolGraph-LarDo effectively enhances downstream molecular property prediction while reducing the cost of acquiring domain-specific knowledge used for pre-training. The source code for reproducibility is available at https://github.com/zhangtia16/MolGraph-LarDo"}]}