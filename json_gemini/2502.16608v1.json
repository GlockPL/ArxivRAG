{"title": "Toward Dependency Dynamics in Multi-Agent Reinforcement Learning for Traffic Signal Control", "authors": ["Yuli Zhang", "Shangbo Wang", "Dongyao Jia", "Pengfei Fan", "Ruiyuan Jiang", "Hankang Gu", "Andy H.F. Chow"], "abstract": "Abstract\u2014Reinforcement learning (RL) emerges as a promising data-driven approach for adaptive traffic signal control (ATSC) in complex urban traffic networks, with deep neural networks substantially augmenting its learning capabilities. However, centralized RL becomes impractical for ATSC involving multiple agents due to the exceedingly high dimensionality of the joint action space. Multi-agent RL (MARL) mitigates this scalability issue by decentralizing control to local RL agents. Nevertheless, this decentralized method introduces new challenges: the environment becomes partially observable from the perspective of each local agent due to constrained inter-agent communication. Both centralized RL and MARL exhibit distinct strengths and weaknesses, particularly under heavy intersectional traffic conditions. In this paper, we justify that MARL can achieve the optimal global Q-value by separating into multiple IRL (Independent Reinforcement Learning) processes when no spill-back congestion occurs (no agent dependency) among agents (intersections). In the presence of spill-back congestion (with agent dependency), the maximum global Q-value can be achieved by using centralized RL. Building upon the conclusions, we propose a novel Dynamic Parameter Update Strategy for Deep Q-Network (DQN-DPUS), which updates the weights and bias based on the dependency dynamics among agents, i.e. updating only the diagonal sub-matrices for the scenario without spill-back congestion. We validate the DQN-DPUS in a simple network with two intersections under varying traffic, and show that the proposed strategy can speed up the convergence rate without sacrificing optimal exploration. The results corroborate our theoretical findings, demonstrating the efficacy of DQN-DPUS in optimizing traffic signal control. We applied the proposed method to a dual-intersection, and the results indicate that our approach performs effectively under various traffic conditions. These findings confirm the robustness and adaptability of DQN-DPUS in diverse traffic densities, ensuring improved traffic flow and reduced congestion.", "sections": [{"title": "1. Introduction", "content": "Traffic congestion has emerged as a perplexing issue in urban areas, primarily stemming from the challenge of efficiently utilizing the limited road infrastructure. Traffic signal control can effectively mitigate congestion by optimizing the traffic signal timing parameters at signalized intersections without major changes to the existing infrastructure. Current deployed traffic light control systems often rely on fixed schedules that fail to account for real-time traffic conditions or only do so minimally. In contrast, adaptive traffic signal control (ATSC) at intersections is crucial in optimizing road resource utilization and alleviating traffic congestion by dynamically regulating traffic flow [1]. Many classical ATSC systems have been created and extensively implemented worldwide. Existing Traffic Signal Control (TSC) methods can be broadly classified into classical Adaptive ATSC systems and RL-based ATSC systems. Classical ATSC systems, such as the Split Cycle Offset Optimization Technique (SCOOT) [2], Chiu's distributed adaptive control system [3], Zhang's fuzzy logic controller (FLC) [4], Webster's method [5][6], and MaxBand [7], compute optimal signal plans based on various traffic parameters like demand and saturation rate. While classical ATSC systems are widely used due to their responsiveness to real-time traffic, they are often complex, nonlinear, and stochastic, which makes it challenging to find optimal signal settings. To address these computational challenges,"}, {"title": "2. Related Works", "content": "In this section, we conduct a literature review of existing MARL based ATSC, which are generally classified into centralized RL and decentralized RL."}, {"title": "2.1. Centralized RL", "content": "Centralized RL methods involved evaluating the returns of actions for all agents to derive the optimal joint strategy across all traffic intersections. However, in large-scale traffic networks, these methods suffered from the curse of dimensionality due to the exponential growth of the action space as the number of intersections increased. To address this issue, Van der Pol and Oliehoek [23] proposed decomposing the global Q-function into a linear combination of local subproblems. To ensure that individual agents considered the learning processes of other agents, Tan et al. [24] modeled the joint Q-function as a weighted sum of local Q-functions, minimizing deviations from the global return. Furthermore, Zhu et al. [25] introduced a joint tree within a probabilistic graph model to facilitate the computation of joint action probabilistic reasoning. Joint action learning was a critical area in multi-agent reinforcement learning (MARL), particularly in methods based on centralized training and decentralized execution [26][27]. However, the scalability issue became evident with the growth of the number of agents involved [28].\nEfforts to scale up the number of agents included the approach by Wang et al. [29], who introduced a cooperative double Q-learning method for the ATSC problem. The method utilized mean-field approximation [30] to model interactions among agents, where the joint actions of other agents were averaged into a scalar using mean-field theory, thereby reducing the dimensionality of the agents' action space in large-scale environments. Compared to centralized RL methods, decentralized RL was more widely used in ATSC."}, {"title": "2.2. Decentralized RL", "content": "In the field of decentralized RL learning for traffic signal control, each agent autonomously managed a specific intersection, typically with only partial observation of the entire environment. Collaboration among these agents mainly occurred through the exchange of observations and policies [31]. Research efforts were directed towards developing methods that derived comprehensive global state features from local information exchanges and interactions among intersections. For example, the MA2C algorithm [32] extended the independent A2C algorithm to multi-agent scenarios by incorporating state information and strategies from neighboring agents. Similarly, Wei et al. [33] integrated the max-pressure approach [34] into multi-agent RL to achieve a more intuitive representation of state and reward functions.\nDespite these advancements, decentralized RL methods often faced challenges related to coordination and scalability, especially in complex traffic scenarios where optimal strategies for intersections could vary significantly. To address these issues, Chen et al. [35] proposed specifying individual rewards for each intersection to capture the coordination demands between neighboring intersections. Zhang et al. [36] introduced a neighborhood cooperative Markov game framework, defining the goal of each intersection as the average accumulated return within its neighborhood and independently learning cooperative strategies based on the 'lenient' principle. Wang et al. [37] presented a decentralized framework based on A2C, where global control was assigned to each local RL agent. In this setup, global information was constructed by concatenating observations (state and reward information) from neighboring intersections, allowing agents to consider local interactions while making decisions.\nMa and Wu [38] extended MA2C with a hierarchical approach by dividing the traffic network into regions, each managed by a high-level agent, while low-level agents controlled the traffic lights within those regions. However, these methods had limitations in dynamic traffic environments. The varying distribution of vehicles at different intersections affected the performance of these approaches, as they did not adapt quickly to changing traffic conditions [39]. Additionally, mechanisms such as attention models used to estimate the correlation between intersections may not have been sufficiently sensitive to fluctuations in traffic patterns [40].\nExisting approaches often struggled to effectively balance coordination and scalability in dynamic traffic networks. The main limitations are insufficient adaptability to dynamic traffic distributions and the overhead associated with communication and computation when coordinating multiple agents. These challenges led to suboptimal performance, especially under rapidly changing traffic conditions."}, {"title": "3. PRELIMINARIES", "content": "To enhance the readability, we firstly review the fundamentals of single-agent and multi-agent RL."}, {"title": "3.1. Single-Agent Markov Decision Process", "content": "A Markov Decision Process (MDP) is the foundational framework for illustrating reinforcement learning (RL) in a single-agent setting. It is formally defined as a 5-tuple (S, A, P, R, \u03b3). Here, S represents the state space, and A denotes the action space. The transition function P:S\u00d7A \u00d7 S \u2192 [0, 1] maps a state-action pair to a new state, reflecting the environment's dynamics. The reward function R:S\u00d7A \u2192 R provides feedback to the agent based on its actions. The discount factor y \u2208 [0,1] discounts future rewards to account for the uncertainty in long-term predictions.\nFormally, the objective of reinforcement learning in an MDP is to find a policy \u03c0 : S \u2192 A(A) that maximizes the expected discounted return. This is quantified by the state-value function:\n$V^{\\pi} (s) = E^{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^{k}R \\left(s^{(t)}, \\pi \\left(s^{(t)}\\right)\\right) \\mid S_{0} = s \\right]$\nwhere \u03c0 maps states to a probability distribution over actions. The action-value function, which provides the expected return for taking an action a in state s under policy \u03c0, is given by:\n$Q^{\\pi} (s, a) = E^{\\pi} \\left[R \\left(s^{(t)}, a^{(t)}\\right) + \\gamma V^{\\pi} (s^{(t+1)}) \\mid s^{(t)}, a^{(t)}\\right]$\nThe optimal policy \u03c0* maximizes the state-value function for all states, leading to the optimal state-value function V*(s). Similarly, the optimal action-value function Q*(s, a) is obtained by following the optimal policy \u03c0*. The"}, {"title": "3.2. Multi-Agent Markov Decision Process", "content": "The standard multi-agent reinforcement learning (RL) model is formally defined as a 7-tuple (N, S, O, A, P, R, \u03b3). Here, N represents the set of agents, with |N| indicating the number of agents. The state space is denoted by S. Each agent i in N can only observe a portion of the state s \u2208 S, represented by the observation $o_{i}$. The joint observation space is O = ($o_{1}, ..., o_{|N|}$ ). Similarly, $A_{i}$ represents the action space of agent i, and the joint action space is denoted as A = ($A_{1},..., A_{|N|}$ ). The transition function P : S \u00d7 A \u00d7 S \u2192 [0,1] maps a state-action pair to a new state, reflecting the environment changes caused by agent actions. The reward function $R_{i}$ : S\u00d7A \u2192 R maps a state-action pair to a real value, representing the feedback received by agent i due to the joint actions of all agents.\nFormally, the objective of multi-agent reinforcement learning (MARL) in an MDP is to find a joint policy \u03c0 = ($\u03c0_{1}, ..., \u03c0_{|N|}$) such that each agent i can maximize its expected discounted return. This is quantified by the state-value function:\n$V^{\\pi} (s) = E^{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^{k}R_{i} \\left(s^{(t)}, \\pi \\left(s^{(t)}\\right)\\right) \\mid S_{0} = s^{(t+1)} \\right]$\nwhere $\u03c0_{i}$ : $O_{i}$ \u2192 \u2206 ($A_{i}$) is the individual policy of agent i, mapping the observation space to a probability distribution over the action space. The action-value function is given by:\n$Q^{\\pi} (s, a) = E^{\\pi} [R_{i} (s^{(t)}) + \\gamma V_{i}^{\\pi} (s^{(t+1)}) | s^{(t)}]$\nA Markov game becomes a cooperative game when the learning goals of the agents are positively correlated, meaning $\u2200i \u2260 j \u2208 N, \u2200V_{i}^{\\pi} (s) \\propto V_{j}^{\\pi}(s)$. The objective of a cooperative Markov game is typically defined as the average expected discounted return of the team of agents.\nSimilarly, a networked Markov game (NMG) is a Markov game that includes the adjacency relationships among agents. It is formally defined as a 7-tuple (G(N, E), S, O, A, P, R, \u03b3), where (i, j) \u2208 E indicates that agents i and j are adjacent, meaning there is information sharing between them. Apart from N being replaced by the graph G(N, E), the other elements in the tuple are defined in the same way as in a Markov game."}, {"title": "4. Dependency Dynamics for MARL based TSC", "content": "In this section, we start by proposing the concept of dependency dynamics for a networked Multi-Agent Markov Decision Process (MAMDP), followed by elaborating the mechanisms to utilize the dependency dynamics to mitigate the challenges of non-stationarity, scalability, and coordination complexity. Then, we show that in MARL-based TSC, the dynamic spill-back effect can be modeled as dependency dynamics and provide an analysis about the impact of the spill-back effect on the MARL learning process."}, {"title": "4.1. Dependency Dynamics in a Networked MAMDP", "content": "In dynamic environments like traffic networks, the dependencies among agents are not static but evolve over time based on factors such as traffic density, congestion levels, and agents' actions. To effectively model and address these changing dependencies, we introduce the concept of dependency dynamics.\nDefinition 1 (Traffic Network). A traffic network can be defined as a graph G(N,E), where i, j \u2208 N and ij \u2208 E indicate that the two intersections i, j\u2208 N are physically connected. Similarly, we denote $N_{i} = {j \u2208 N | (i, j) \u2208 E}$ as the set of i 's neighbors. Table 1 provides definitions of the various components of the traffic network.\nDefinition 2 (Dependency Dynamics). Dependency dynamics refers to the temporal evolution of dependency among agents caused by time-varying states of agents. It captures how the influence of one agent's actions on another agent's state or reward changes over time due to environmental factors and agents' policies. Dependency dynamics are characterized by time-varying functions that quantify the degree of dependency between agents. These functions can be influenced by various factors, such as the volume of traffic flow between intersections, queue lengths, and occurrences of congestion phenomena such as spill-back."}, {"title": "4.2. Spill-back Dynamics in Traffic Networks", "content": "Spill-back effect occurs when traffic congestion propagates to upstream road sections [41]. It happens when the queue at an intersection grows so long that it blocks the upstream intersection or roadway, preventing the free flow of traffic (Fig. 2). In Fig. 2, we can see that the congestion on the central lanes has caused a spill-back, blocking vehicles in the upstream sections, while the side lanes without spill-back allow vehicles to flow more freely.\nConsider a multi-agent network G(N, E), where agents i and j are neighbors if there exists an edge (i, j) \u2208 E. The neighborhood of agent i is denoted as $N_{i}$, and the local region is defined as $V_{i} = N_{i} \u222a {i}$. The distance d(i, j) between two agents i and j is measured as the minimum number of edges connecting them. For instance, d(i, i) = 0 and d(i, j) = 1 for any j \u2208 $N_{i}$. In the Q-learning framework, each agent learns its policy \u03c0 and the corresponding value function $V_{W_{i}}$. The optimal action $a^{*}$ that maximizes the Q value is determined by:\n$a_{i}^{*} = argmax \\; Q\\left(\\left(\\pi_{1}^{*}, ..., \\pi_{|N|}^{*}\\right), a_{i}\\right)$"}, {"title": "4.3. Impact of spill-back dynamics on MARL learning process", "content": "The challenges of coordination complexity, communication overhead, and non-stationarity can be alleviated by decomposing MARL into multiple independent reinforcement learning (IRL) processes when no spill-back occurs across all intersections throughout the entire episode. Based on Definition 1 and Theorem 1, the following Corollary can be derived:"}, {"title": "5. Deep Q-Network with Dynamic Parameter Update Strategy", "content": "In this section, we will propose the DQN-DPUS, which can speed up the convergence rate without sacrificing the optimal exploration by updating the weighting matrix and bias based on dependency dynamics."}, {"title": "5.1. Steps of the DQN-DPUS algorithm", "content": "Step 1 Initialization: Initialize the Q-network parameters @ and the target Q-network parameters 0\u00afsuch that 0\u00af \u2190 0.\nAdditionally, initialize the replay buffer B which will store the transitions observed by the agents during training. This buffer is crucial for experience replay, allowing the algorithm to learn from past experiences and improve stability and efficiency.\nStep 2 Training Loop: Begin the main training loop that will iterate for a total of M epochs. Each epoch represents a complete pass through the training process, during which the agent will interact with the environment multiple times. This iterative approach allows the agent to gradually learn and refine its policy over time by repeatedly experiencing different states and rewards.\nStep 3 Time Step Loop: Within each epoch, iterate over a total of T time steps. Each time step represents a single interaction with the environment, where the agent observes the current state, selects an action, and then observes the result of that action. This granular interaction is essential for learning the dynamics of the environment and optimizing the agent's actions.\nStep 4 Observe, Select, and Execute Action: At each time step t, observe the current state $s_{t}$. Based on the observed state $s_{t}$, select an action $a_{t}$ using an e-greedy policy derived from the Q-network $Q (s_{t}, a_{t}; \\theta)$. Execute the selected action $a_{t}$, and observe the immediate reward $r_{t}$ and the next state $s_{t+1}$. This feedback loop is critical for learning, as it provides the agent with real-time information about the consequences of its actions, enabling continuous improvement.\nStep 5 Store Transition: If the current time step t is a multiple of the predefined update frequency f, perform a series of updates on the Q-network. Sample a mini-batch of transitions ($s, a, r, s^{t+1}$) from the replay buffer B based on their priorities. Sampling based on priority ensures that more significant transitions, which have a higher TD-error, are more likely to be selected for learning. This technique, known as prioritized experience replay, helps the agent learn more efficiently from important experiences, making the learning process more focused and effective.\nStep 6 Periodic Update Check and Mini-batch Sampling: If the current time step t is a multiple of the predefined update frequency f, perform a series of updates on the Q-network. Sample a mini-batch of transitions ($s, a, r, s^{t+1}$) from the replay buffer B based on their priorities. Sampling based on priority ensures that more significant transitions, which have a higher TD-error, are more likely to be selected for learning.\nStep 7 Compute TD-error and Dynamic Update Parameters: For each transition in the mini-batch, compute the temporal difference (TD) error \u03b4. The TD-error measures the difference between the predicted Q-value and the target Q-value. Update the priorities in the replay buffer based on the computed TD-errors. If a spill-back occurs, update all parameters \u03b8 of the Q-network considering the spill-back effect. Otherwise, update only the diagonal parameters $\u03b8_{diag}$ of the Q-network. This selective parameter update helps in efficiently learning the specific dynamics of spill-back"}, {"title": "5.2. The effectiveness of the DQN-DPUS algorithm", "content": "Definitions and Assumptions: Consider a system with two agents, $A_{1}$ and $A_{2}$, each controlling the traffic lights at two intersections. The state vector is s = ($s_{1}$, $s_{2}$), and the action vector is a = ($a_{1}$, $a_{2}$). The Q-value function is approximated by a neural network with a parameter matrix W, which is divided into four sub-matrices, and represented as follows:\n$W=\\begin{pmatrix}\nW_{11} & W_{12}\\\\\nW_{21} & W_{22}\n\\end{pmatrix}$"}, {"title": "5.3. The convergence of the DQN-DPUS algorithm", "content": "Definitions and Assumptions: We define a MDP as a tuple (S, A, P, r), where S is the (finite) state-space; A is the (finite) action-space; P represents the transition probabilities; r represents the reward function.\nWe denote elements of & as x and y and elements of A as a and b. The reward is defined as a function\nr:S\u00d7A\u00d7S \u2192 R, assigning a reward r(x, a, y) every time a transition from x to y occurs due to action a. The"}, {"title": "6. EXPERIMENTAL ENVIRONMENT", "content": "In this section, we will describe the applied experimental environment for this research. We will firstly illustrate the road network model, followed by giving the definition of states, actions and reward."}, {"title": "6.1. Scenario", "content": "Our experimental scenario is a 1\u00d72 road network, as shown in Figure 2. The distance between the two intersections is 300 meters. The traffic flow input for the experiment is illustrated in Figure 4. Figure 4 illustrates the throughput at two intersections over a period of 60 minutes, with the left column representing Intersection I and the right column representing Intersection II. Each subplot corresponds to different traffic input levels, with the traffic volume increasing from top to bottom."}, {"title": "6.2. Definition of state, action and reward", "content": "The state representation is defined as a matrix that captures the positions of vehicles within the road network. The road network is divided into a matrix, where cells containing a vehicle are marked as '1' and empty cells are marked as '0'.\nThe action space, denoted as A, represents adjustments to the green light phase duration, specified by $a_{t}$. Possible actions include increasing, decreasing, or maintaining the current phase duration.\nConsider that the objective of the ATSC problem is to enhance the traffic conditions within a specific region, with the count of vehicles waiting near intersections serving as a meaningful evaluation criterion. The cumulative rewards for all agents, observed following their actions at time t, can be determined by computing the average number of waiting vehicles across all incoming lanes.\n$r_{i}(t) = \\frac{1}{|L|} \\sum_{l \\in L_{i}^{in}} \\gamma^{t-1} \\text{waiting time } [l]^{(t+1)}$\nSince our goal is to minimize the number of waiting vehicles, we take a negative value in reward function."}, {"title": "6.3. Baseline Methods", "content": "We compare the performance of the DQN-DPUS with the following baseline methods, for which the algorithms are presented as follows: s\n(1) Multi-agent Advantage Actor-Critic (MA2C) [14]: We use A2C method separately at each intersection. Each agent uses a critic network to evaluate the policy of each actor and guide them to optimize their policies (fc is fully connected layer).\nActor, $\u03c0_{i}^{\u03c0}(s_{i})$ = softmax (fc (fc ($s_{i}$)))\nCritic, $Q_{i}^{\u03c0}(s_{i})$ = relu (fc (fc ($s_{i}$)))\n(2) Fully Independent DQN Approach (In-DQN) [8]: We use the traditional DQN method in multi-agent reinforcement learning where each agent operates independently, without explicit coordination or communication with other agents.\n(3) Cooperative Multi-agent Deep RL Approach (Co-MARL): Haddad et al., proposed a Co-MARL approach in 2022 [43]. The Co-MARL method applies a DQN, while transferring state, action and reward received from their neighbor agents to its own loss function during the learning process. Co-MARL uses the information transmitted by the neighbors as the state of learning to achieve the purpose of agent cooperation, the target value is:\n$y_{i} = r_{i} + \\gamma \\cdot Q\\left(s_{i}^{(t+1)}, s_{i}^{N,+1}, a_{i}^{(t+1)}, a_{i}^{N,+1}; \\theta_{i}\\right)$"}, {"title": "7. Result", "content": "In this study, we compared the performance of four algorithms (A2C, DQN-DPUS, In-DQN, Co-DQN) under different spill-over rates (The frequency of vehicle spill-back occurrences in each episode.) (0, 0.1, 0.3, 0.5). The reward values were tracked over 800 episodes, with the primary metric being the average reward. The results show that the DQN-DPUS method outperforms the baseline methods across all spill-over rates, particularly under strong spill-over effects.\nA detailed examination of the reward trends reveals several noteworthy observations. For instance, in the case of A2C at a spill-over rate of 0.5, there is a noticeable drop in performance around the 200th episode, followed by a period of recovery and subsequent fluctuations. This pattern indicates that A2C struggles to maintain stability in highly dynamic environments, reflected by the significant dips and spikes in rewards. Conversely, DQN-DPUS shows a more consistent performance trajectory across all spill- over rates, including 0.5, where the rewards stabilize after an initial learning phase and exhibit smaller fluctuations.\nAt the 600th episode, DQN-DPUS for a spill-over rate of 0.5 maintains a reward of approximately -18000, whereas A2C and In-DQN have rewards around -22000 and -21000, respectively. This clear margin underscores DQN-DPUS's ability to better manage strong spill-over effect. The reduced volatility in DQN-DPUS's rewards at high spill-over rates"}, {"title": "8. Conclusion and Future Work", "content": "The proposed DQN-DPUS algorithm effectively integrates prioritized experience replay and spill-back considerations to optimize traffic signal control in multi-agent environments. This algorithm dynamically adjusts the learning process based on real-time traffic conditions, allowing agents to make intelligent decisions that enhance traffic flow and reduce congestion. We demonstrated that spill-back effects lead to mutual influence between the intersections. Without spill-back, the fully centralized learning approach is equivalent to the fully independent learning algorithm. These experiments validate the effectiveness and convergence of the DQN-DPUS algorithm, highlighting its robustness in handling traffic scenarios. In future work, we plan to apply these approaches in vehicle-signal cooperative control. This will involve leveraging the developed methods to optimize the interaction between vehicles and traffic signals, aiming to improve overall traffic flow and reduce delays."}]}