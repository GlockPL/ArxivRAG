{"title": "Learning GRAPH Quantized Tokenizers for Transformers", "authors": ["Limei Wang", "Kaveh Hassani", "Si Zhang", "Dongqi Fu", "Baichuan Yuan", "Weilin Cong", "Zhigang Hua", "Hao Wu", "Ning Yao", "Bo Long"], "abstract": "Transformers serve as the backbone architectures of Foundational Models, where a domain-specific tokenizer helps them adapt to various domains. Graph Transformers (GTs) have recently emerged as a leading model in geometric deep learning, outperforming Graph Neural Networks (GNNs) in various graph learning tasks. However, the development of tokenizers for graphs has lagged behind other modalities, with existing approaches relying on heuristics or GNNs co-trained with Transformers. To address this, we introduce GQT (Graph Quantized Tokenizer), which decouples tokenizer training from Transformer training by leveraging multi-task graph self-supervised learning, yielding robust and generalizable graph tokens. Furthermore, the GQT utilizes Residual Vector Quantization (RVQ) to learn hierarchical discrete tokens, resulting in significantly reduced memory requirements and improved generalization capabilities. By combining the GQT with token modulation, a Transformer encoder achieves state-of-the-art performance on 16 out of 18 benchmarks, including large-scale homophilic and heterophilic datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Following the success of Transformers (Vaswani et al., 2017) in natural language processing (Devlin et al., 2019; Brown et al., 2020) and computer vision (Dosovitskiy et al., 2021), Graph Transformers (GTs) (Dwivedi & Bresson, 2020; Ying et al., 2021a; Ramp\u00e1\u0161ek et al., 2022; Ma et al., 2023; Shirzad et al., 2023; Kong et al., 2023b; Chen et al., 2023; Wu et al., 2022b) have emerged as strong models in geometric deep learning. Unlike message-passing Graph Neural Networks (GNNs), which rely on strong locality inductive biases (Battaglia et al., 2018; Veli\u010dkovi\u0107 et al., 2018; Hou et al., 2020; Hamilton et al., 2017a; Kipf & Welling, 2017), GTs are inherently more expressive due to their ability to capture long-range interactions between nodes (Ma et al., 2023). This is particularly beneficial in heterophilous settings where local alignment does not hold (Fu et al., 2024). GTs possess an expressive power at least equivalent to the 2-Weisfeiler-Lehman (WL) isomorphism test (Kim et al., 2022), which is sufficient for most real-world tasks (Zopf, 2022). This surpasses the expressive power of message-passing GNNs, which are limited to the 1-WL test (Ying et al., 2021a). Furthermore, a Transformer with sufficient attention heads can match or exceed the expressive power of a second-order invariant graph network, outperforming message-passing GNNs (Kim et al., 2022). However, both GNNs and Transformers are susceptible to over-smoothing (Li et al., 2018; Zhou et al., 2021; Dovonon et al., 2024).\nGTs require consideration of both graph structure and features, as nodes with identical features will otherwise be projected into the same representation regardless of their surrounding structures (Hoang et al., 2024). There are three general approaches to address this limitation (Hoang et al., 2024): (1) node feature modulation, which involves injecting structural information into the node features; (2) context node sampling, where a sampling strategy is used to construct a sequence over the neighbor nodes; and (3) modifying the architecture of a vanilla Transformer to directly incorporate structural biases. Given that Transformers are universal approximators of sequence-to-sequence functions (Yun et al., 2020) and considering the rapid developments in efficient implementation of multi-head attention (MHA) module (Dao et al., 2022a; Liu et al., 2024), which enables longer context sizes of"}, {"title": "2 RELATED WORKS", "content": "Graph Transformers (GTs) have shown promising performance on various graph learning tasks, surpassing GNNs on many benchmarks. GTs can be broadly categorized into two directions (Hoang et al., 2024; M\u00fcller et al., 2024): (1) modifying the vanilla Transformer architecture to incorpo-rate structural inductive biases, or (2) encoding the input graph to make it compatible with the vanilla Transformer design. Early examples of the first approach include Graph Attention Network (Veli\u010dkovi\u0107 et al., 2018), which uses an attention module to compute pairwise node attentions and masks the attention matrix based on connectivity information. Subsequent works have replaced the scaled-dot attention module with various structure-aware sparse attention modules (Ramp\u00e1\u0161ek et al., 2022; Bo et al., 2023; Ying et al., 2021a; Deng et al., 2024; Wu et al., 2023b; Liu et al., 2023a;\nChen et al., 2022; Dwivedi & Bresson, 2020; Shirzad et al., 2023; Ma et al., 2023). Examples of the second approach include Graph Memory Network (Khasahmadi et al., 2020), which passes non-linear projections of node features and structural encoding to a Transformer-like model. Structural encoding methods, such as Laplacian eigenvectors or Random walk-based encoding (Dwivedi et al., 2022; Ma et al., 2023; Cant\u00fcrk et al., 2024), allow injecting structural information directly into the node features. Another approach involves using GNNs to encode local structure along with node features, followed by passing the representation to vanilla Transformers to capture long-range dependencies.\n(Rong et al., 2020; Wu et al., 2021; Chen et al., 2023; 2022). Recent studies leverage LLMs, where"}, {"title": "3 PRELIMINARIES", "content": "Messag-Passing GNNs. Let G denote the space of graphs. A graph g within this space is defined as g = (V,E,X,E) where V is the set of nodes and E \u2286 V \u00d7 V is the set of edges. X \u2208 R|V|\u00d7dx represents the node features of dimension dx, and E \u2208 R|V|\u00d7|V|\u00d7de represents the edge features of dimension de. A message-passing GNN takes g as input and learns representations h for v \u2208 \u03bd (h = xv) in each layer l as follows (Gilmer et al., 2017):\n$h_v^l = f_\\theta \\Big(g_\\phi \\big(\\{(h_u^{l-1}, h_v^{l-1}, e_{uv}) | u \\in N_i(v) \\}\\Big)\\Big)$\nwhere fe and g\u00f8 are known as update (combine) and message (aggregate) functions, respectively, and Ni(v) denotes the set of immediate neighbors of the node v. With this representation, we can perform various tasks, including node classification as MLP (hr), edge prediction as MLP (hu\u2299 hv), or graph classification as MLP (R({hu|u \u2208 V})) where R is a pooling (readout) function.\nGraph Transformers, on the other hand, first use a tokenizer T = \u03a4\u03c8 (N(v)) to map each node v \u2208 V into a sequence of tokens T, by considering some notion of neighborhood N. The simplest design is when N is zero-hop neighborhood (i.e., the node itself) and Ty is a node feature lookup function. The neighborhood N can be extended to include an ego network (Zhao et al., 2021) or top-k Random Walk based neighbors (Fu et al., 2024), and Ty can be enhanced to representations from a GNN (Chen et al., 2023). Once the tokens are computed, along with a node positional encoding function (PE), we can define the input to a Transformer as h = [Tv ||PE (v)] and compute the representation in each layer l of a vanilla Transformer encoder as follows:\n$h^l = LN (MHA (LN (h^{l-1})) + h^{l-1})$\n$h^l = h^l + MLP (h^l)$\nwhere LN and MHA are Layer Normalization and Multi-Head Attention modules, respectively.\nSimilar to Transformer encoders in other modalities (Devlin et al., 2019; Dosovitskiy et al., 2021), we can append a special classification token, denoted as [CLS], to the input and use its representation to perform various classification tasks on the graph: MLP (h[cLs]). In this setting, the input for node classification is T, for link prediction is [Tv||Tu], and for graph classification is [Tv ||v\u2208\u03bd].\nVector Quantization projects embeddings X \u2208 Rn\u00d7dz into a more compact space of codebooks C\u2208 Rk\u00d7dc, where k \u226a n. The codebooks can be learned by minimizing various objectives such as K-means clustering. The new representation of xi is then computed as follows (Van Den Oord et al., 2017):\n$z(X_i) = C_k \\quad \\text{where} \\quad k = \\underset{j}{\\text{arg min}} ||x_i - c_j||_2^2$"}, {"title": "4 SELF-SUPERVISED GRAPH TOKENIZATION", "content": "4.1 TOKENIZER PROPERTIES\nOur goal is to design a graph tokenizer that can learn to generate tokens that exhibit three key characteristics, which are essential for effective graph representation learning. These characteristics are as follows.\nLocal Interactions. The learned tokens should encapsulate local interactions, allowing the Trans-former to focus on global dependencies. This is analogous to Vision Transformers (ViTs), where the Transformer attends to image patches instead of pixels, enabling efficient learning on abstract tokens (Dosovitskiy et al., 2021; Liu et al., 2021). To achieve a similar effect on graph-structured data, we leverage message-passing GNNs as the foundation of the tokenizer's encoder, capitalizing on their strong locality inductive bias to effectively capture local interactions in the representation space (Battaglia et al., 2018). Our design accommodates various GNN layer choices without constraints; for simplicity, we opt for the widely used Graph Attention Network (GAT) (Veli\u010dkovi\u0107 et al., 2018) as our base graph encoder. The representation of node i in layer l is computed as:\n$h_i^l = \\sigma \\Big(\\sum_{j \\in N(i)} \\alpha_{ij} W h_j^{l-1}\\Big)$\n$\\alpha_{ij} = \\frac{\\exp \\Big(\\sigma \\big(W_2 [W_1 h_i^{l-1} || W_1 h_j^{l-1} ]\\big)\\Big)}{\\sum_{k \\in N(i)} \\exp \\Big(\\sigma \\big(W_2 [W_1 h_i^{l-1} || W_1 h_k^{l-1} ]\\big)\\Big)}$\nwhere o is a non-linearity, and aij is the normalized attention score between two connected nodes i and j.\nMemory Efficiency. The tokens should be compact to facilitate efficient memory usage, enabling the Transformer to perform efficient inference. To achieve this, we introduce a Residual-VQ (RVQ) (Lee et al., 2022) layer to quantize the GNN representations into a sequence of discrete tokens. Quantization not only helps with generalization due to its regularization effect but also significantly reduces memory usage. Using an RVQ with c codebooks (typically c = {2,\u2026\u2026,8}), a graph with feature matrix X \u2208 RN\u00d7d can be represented as X\u0689 \u220b NNxc and codebook representation of C\u2208 Rc\u00d7K\u00d7dc, where c is the number of codebooks (i.e., levels of quantization), K is the codebook size, and de is the code dimension. To illustrate the benefits of this approach, consider a graph with\n106 nodes and a feature dimension of 1024 (X \u2208 R10\u00b0\u00d71024). Using an RVQ with 3 codebooks and a codebook size of 256, this graph can be represented as XQ \u2208 N106\u00d73 plus C\u2208 R3\u00d7256\u00d71024, resulting in a 270-fold reduction in required memory.\nRobustness and Generalization. The tokens should be robust and generalizable. To achieve this, we rely on graph self-supervised learning. Self-supervised representations have been shown to be more robust to class imbalance (Liu et al., 2022) and distribution shift (Shi et al., 2023), while also capturing better semantic information (Assran et al., 2023) compared to representations learned through supervised objectives. Moreover, self-supervised graph representations have demonstrated superior performance on downstream tasks compared to representations learned in a fully supervised manner, indicating better generalization capabilities (Hu et al., 2020b; Sun et al., 2020; You et al.,\n2020; 2021; Hassani & Khasahmadi, 2020; Hou et al., 2022; Veli\u010dkovi\u0107 et al., 2019; Zhu et al., 2020b; Thakoor et al., 2022). Additionally, multi-task learning with self-supervised objectives has been shown to achieve better performance on downstream tasks (Doersch & Zisserman, 2017; Ghiasi et al., 2021). To leverage these benefits, we propose training the GNN encoder with three self-supervised objectives. Unlike RQ-VAE (Lee et al., 2022), which uses reconstruction as its primary objective, we employ graph-specific objectives to capture the nuances of both structure and features within the tokens. Specifically, we use Deep Graph Infomax (DGI) (Veli\u010dkovi\u0107 et al., 2019) and Graph Masked Auto-Encoder 2 (GMAE2) (Hou et al., 2023). DGI is a contrastive method that contrasts local (node) encoding with global (graph or sub-graph) encoding, whereas GMAE2 combines generative and distillation objectives to jointly reconstruct masked features and track teacher representations."}, {"title": "5 GRAPH TRANSFORMER", "content": "5.1 SEQUENCE GENERATION\nOnce the tokenizer is trained, each node v \u2208 V is mapped to a set of c tokens: T = [t\u2081,\u2026\u2026, tc] \u2208 N, which compress information about local interactions. To enable the Transformer to capture long-range interactions, the input should consist of a sequence of tokens from nodes that are likely to have long-range dependencies. To facilitate this, we first augment the graph with semantic edges denoted as Es, which are computed as follows:\n$E_s = \\Big\\{ e_{u,v} \\Big| \\underset{v \\in V}{\\text{arg topk}} \\; \\text{sim} \\big(f(x_u), f(x_v)\\big) \\forall u \\in V \\Big\\}$\nwhere sim(,) denotes the similarity function, xu is the feature vector of node u, and f is a projection function. We use cosine similarity as the similarity function and principal component analysis (PCA) as the projection function. This semantic edge augmentation effectively creates sparse edges between each node and its k-nearest neighbors in the feature space, enhancing the model's ability to recognize and utilize significant long-range dependencies.\nWe then merge the semantic edges with the original graph edges and use Personalized PageRank (PPR) to generate a sequence per node. A PPR vector for a node u captures the relative importance of other nodes with respect to node u by exploring the graph structure through iterative random walks:\nr = aPr + (1 \u2212 a)q\nwhere P = D-AD\u00af\u00bd \u2208 Rn\u00d7n, q is a stochastic personalized vector, r is the stationary distribution of random walks, and a is a damping factor.\nUsing PPR enriches the sequence with information beyond local interactions, allowing the Trans-former to access potential long-range dependencies. We construct the sequence S for each node v as follows:\nSv = [Tv||Tu||uearg topk PPR(v,EUE\u2083)]\nwhere S = [t\u2081t | tu... tu |... | tuk... tuk] is the sequence of sorted integer tokens with length c \u00d7 (k + 1), based on the PPR scores for node u. Note that the computation of semantic edges and PPR sequences is performed only once as a pre-processing step, which reduces computational overhead during training."}, {"title": "5.2 TOKEN MODULATION", "content": "There are c \u00d7 K possible integer tokens in total, where c is the number of codebooks and K is the codebook size. We randomly initialize an embedding matrix XT \u2208 Rc\u00d7K\u00d7d, which is trained end-to-end with the Transformer. To further enrich the token representation, we introduce an additional token to each node by aggregating the embeddings of its assigned codebooks:\n$h_v^c = \\sum_{i=1}^{C} C[i_t]$\nWe found that adding this explicit aggregated token leads to better performance compared to initializ-ing XT with C. The input representation of the sequence for node v is then defined as:\n$S_u = \\Big[Xr_1t_1^1 || Xr_1 || Xr_2t_2^1 || Xr_2 || ... ||Xr_Ct_C^1 || Xr_C || ... || Xr_1t_1^i || Xr_1 || Xr_2t_2^i || Xr_2 || ... || Xr_Ct_C^i || Xr_C || ...\\Big]$\nThis representation combines the individual token embeddings with the aggregated codebook embed-dings, providing a more comprehensive and nuanced input to the Transformer.\nIn order to provide the Transformer with the global structural importance scores of the nodes within the sequence with respect to the target node, we introduce a gating mechanism over the input token embeddings as follows:\n$S'_u = S \\odot Softmax \\Big( topk PPR (v, E \\cup E_s) \\Big)$\nwhere we first apply a softmax function with temperature + = 1 to normalize the PPR scores, and then multiply each node token's representation by its corresponding normalized score.\nWe also introduce two trainable positional encodings to the input tokens. The first positional encoding enables the Transformer to distinguish between tokens from different nodes, while the second encoding, referred to as hierarchical encoding, allows the Transformer to recognize the hierarchy level of each token within the codebooks. We randomly initialize the positional encodings PE \u2208 R(k+1)\u00d7dx and HE \u2208 Rc\u00d7dx and sum them with the encoding of their corresponding token. For example, the final encoding of the token j of the node i within the sequence is computed as: x = XT[j, ti] + PE[i] + HE[j]. Note that we did not use any structural encoding, such as Laplacian eigenvectors, as our experiments did not show any significant benefits from including them."}, {"title": "5.3 TRANSFORMER ENCODER & CLASSIFICATION HEAD", "content": "We use l layers of standard Transformer encoder with flash attention (Dao et al., 2022b) to generate contextual representations per token in the sequence: H(l) \u2208 R(c+1)\u00d7(k+1)\u00d7dh . We then aggregate the token representations for j-th node in the sequence by summing along the token dimension:\n$H_{v_j} = \\sum_{i=1}^{c+1} H^{(l)} [i, j] \\in \\mathbb{R}^{(k+1) \\times d_h}$\nTo obtain a single representation for the entire sequence, We further aggregate the representation using a linear attention layer:\n$h = \\sum_{i=1}^{k+1} a_i h_i \\quad \\text{where} \\quad a_i = \\frac{\\exp(W h_i)}{\\sum_j \\exp(W h_j)}$\nWe feed the resulting representation into a fully-connected classifier and train the model end-to-end with cross-entropy loss. Note that during inference, only the Transformer and classifier are utilized, as the tokenizer is pretrained and the sequences are pre-computed. Furthermore, since we only require discrete tokens and codebook embeddings, our approach allows for efficient memory usage, regardless of graph size enable efficient training and inference on large-scale graphs."}, {"title": "6 EXPERIMENTS", "content": "We comprehensively evaluate GQT on both medium-scale and large-scale node classification tasks, encompassing both homophilous and heterophilous settings across 18 datasets. Homophilous graphs"}, {"title": "6.1 COMPARISON WITH STATE-OF-THE-ART", "content": "Homophilous Node Classification. To evaluate the performance on medium-scale homophilous graphs, we use eight benchmark datasets including CoraFull (Bojchevski & G\u00fcnnemann, 2017), CiteSeer, and PubMed (Yang et al., 2016), Amazon Computers, Amazon Photos, Co-author CS, and Co-author Physics (Shchur et al., 2018), as well as WikiCS (Mernyei & Cangea, 2020). We compare our results with a comprehensive set of baselines, including four traditional GNNs: GCN (Kipf & Welling, 2017), GAT (Veli\u010dkovi\u0107 et al., 2019), APPNP (Gasteiger et al., 2018), and GPRGNN (Chien et al., 2020); four scalable GNN variants including GraphSAINT (Zeng et al., 2019), GraphSAGE (Hamilton et al., 2017b), PPRGo (Bojchevski et al., 2020), and GTAND+(Feng et al., 2022); four standard GTs including GT (Dwivedi & Bresson, 2020), Graphormer (Ying et al., 2021b), SAN (Kreuzer et al., 2021), and GraphGPS (Ramp\u00e1\u0161ek et al., 2022); and six state-of-the-art scalable GTs including GOAT (Kong et al., 2023a), NodeFormer (Wu et al., 2022a), DiffFormer (Wu et al., 2023a), NAGphormer (Chen et al., 2023), Exphormer (Shirzad et al., 2023), and VCR-Graphormer (Fu et al., 2024). The baseline performance is reported from existing works (Wu et al., 2023b; Luo et al., 2024a; Fu et al., 2024). As shown in Table 1, GQT outperforms the baseline GNN and GT models on 7 out of 8 benchmarks. Notably, this achievement comes with a significant reduction in memory requirement for node features during Transformer training and inference. For example, on the Physics dataset with 34,493 nodes, we only use 256 \u00d7 6 tokens, i.e., 23-fold memory reduction.\nHeterophilous Node Classification. Furthermore, we evaluate GQT on six small or medium-scale heterophilous datasets: Squirrel and Chameleon (Rozemberczki et al., 2021), Questions, Roman-Empire, Amazon-Ratings, and Minesweeper (Platonov et al., 2023b). We compare the performance with seven variants of GNNs including GCN, GraphSAGE, GAT, GPRGNN, H2GCN (Zhu et al., 2020a), CPGNN (Zhu et al., 2021), and GloGNN (Li et al., 2022), and six variants of GTs, including GraphGPS, GOAT, NodeFormer, SGFormer, NAGphormer, and Exphormer. The baseline performance is reported from existing works (Wu et al., 2023b; Luo et al., 2024b; Platonov et al.,"}, {"title": "6.2 ABLATION STUDY", "content": "Effect of Tokenization. We examine the performance of the tokenizer by training a linear model on the representations of the learned tokens without modulation, augmentation, or Transformer (1)."}, {"title": "7 CONCLUSION", "content": "We introduced GQT (Graph Quantized Tokenizer) to decouple graph tokenization from Transformer using multi-task graph self-supervised learning. The GQT uses vector quantization to learn hierarchical tokens, resulting in significantly reduced memory requirements and improved generalization. We also introduced structural gating, hierarchical encoding, and semantic edges to further improve the performance. We achieved state-of-the-art performance on 16 out of 18 datasets, including large-scale homophilic and heterophilic datasets, while significantly reducing memory requirements. As future directions, we plan to explore the effectiveness of the GQT in graph generative learning by transitioning to a Transformer decoder. Our research lays the groundwork for further investigation into Graph Foundational Models, where LLMs can project heterogeneous features from diverse datasets into a unified textual representation. Building on this foundation, our GQT model can then convert a large number of nodes across different datasets into an efficient set of tokens."}, {"title": "B EXPERIMENTAL SETUP", "content": "Software and hardware. The implementation of our method is based on PyTorch\u00b2, PyG\u00b3, DGL4, and vector-quantize-pytorch package5. Most of the datasets can be accessed from PyG and DGL. All the experiments are conducted on one Nvidia A100 GPU.\nHyperparameters and experimental details. As illustrated in Figure 1, our method includes two parts: tokenizer and Transformer. We provide the hyperparameters and experimental details for each parts below.\nDuring the training of graph tokenizer, we use full-graph training for small and medium-scale datasets, and apply sampling for large-scale graphs. We consider different sampling methods including random partitioning which randomly samples nodes within a graph and returns their induced subgraph, neighbor sampling (Hamilton et al., 2017b), GraphSAINT (Zeng et al., 2019), and local clustering used in Hou et al. (2023). For the GNN encoder and decoder, we use GCN or GAT as our backbone and tune the number of layers from {1, 2, 3, 4, 5, 6, 7, 8, 9, 10} and hidden dimensions from {128, 256, 512, 1024}. For the quantizer, we use residual-VQ (RVQ) (Lee et al., 2022) and tune the number of codebooks from {1, 2, 3, 6, 9} and codebook size from {128, 256, 512, 1024}. We set the code dimension to be the hidden dimension of the GNN encoder.\nDuring the training of Transformer, we use KNN to add semantic edges and tune the number of semantic neighbors from {0, 5, 10, 15, 20}. Then we use PPR to generate a sequence of nodes for each target node. We tune the number of PPR neighbors from {0, 5, 10, 20, 30, 50}. For the Transformer model, we use the TransformerEncoder module in PyTorch ad our backbone, and tune the number of layers from {1, 2, 3, 4, 5, 6}, number of heads from {4, 8}, and feedforward dimension from {512, 1024, 2048}."}]}