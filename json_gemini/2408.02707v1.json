{"title": "SnapE - Training Snapshot Ensembles of Link Prediction Models", "authors": ["Ali Shaban", "Heiko Paulheim"], "abstract": "Snapshot ensembles have been widely used in various fields of prediction. They allow for training an ensemble of prediction models at the cost of training a single one. They are known to yield more robust predictions by creating a set of diverse base models. In this paper, we introduce an approach to transfer the idea of snapshot ensembles to link prediction models in knowledge graphs. Moreover, since link prediction in knowledge graphs is a setup without explicit negative examples, we propose a novel training loop that iteratively creates negative examples using previous snapshot models. An evaluation with four base models across four datasets shows that this approach constantly outperforms the single model approach, while keeping the training time constant.", "sections": [{"title": "1 Introduction", "content": "Knowledge Graphs (KGs) are a crucial instrument for structuring complex, inter-related data in a way that is interpretable both by humans and machines [14,15]. They are used in many applications ranging from search engines to recommendation systems and from natural language processing to computer vision.\nKGs often exhibit sparseness and incompleteness [7,22]. This link sparseness can significantly hinder the KGs' potential use in applications like semantic search. As a result, filling the missing links has gained traction in the AI community. This task is called Link Prediction, and it is one of the primary applications of machine learning and embedding methods on Knowledge Graphs [4]. Various research efforts have validated the efficacy of link prediction in addressing the sparsity issue in KGs and reinforcing their potential [21].\nLink prediction with knowledge graph embeddings (KGEs) faces many challenges that have been the subject of research [2], including the sparsity of graphs [27], noisy or erroneous data [42], and scalability [8], among others. In machine learning, many of those challenges are addressed by ensemble methods. Ensembles can improve generalization by reducing the overfitting often seen in"}, {"title": "2 Related Work", "content": "Link prediction in knowledge graphs is an extensively researched area. Approaches can be roughly categorized as geometric, matrix factorization, and neural network based approaches [30,38].\nOne of the earliest approaches to use ensembles for link prediction has been proposed by Krompass et al. [18]. They train three different heterogeneous models (i.e., a TransE, a RESCAL, and an mwNN model), and make a prediction by averaging the probabilities for each predicted triple. Similar approaches are"}, {"title": "3 Approach", "content": "KGE models are trained by iteratively adapting their weights in order to maximize a target function. In each iteration, the model's weights are adapted so that the error function is lowered. The amount by which each weight can be changed in each iteration is called the learning rate. A high learning rate leads to a fast convergence of the model, but may not yield the best possible solution. A low learning rate, on the other hand, requires more time for convergence, but may yield better results. Therefore, many models are trained with a decaying learning rate, as shown in Fig. 1a. There are different decay functions for learning rates, such as step decay [11] or cosine annealing [19].\nThe idea of Snapshot Ensembles, introduced by Huang et al. in 2017 [16], uses a cyclic learning rate, as shown in Fig. 1b. Each time the learning rate schedule reaches a local minimum, a snapshot of the model is stored. At prediction time, each stored model is used to predict, and the predictions are combined. The original snapshot ensemble paper uses cyclic cosine annealing, as shown in Fig. 1b, and averages the predictions. In this paper, we use the same mechanism"}, {"title": "3.1 Learning Rate Schedulers", "content": "As discussed above, the original snapshot ensemble uses the cosine cyclic annealing schema (CCA), defined as [19]:\n$a(t) = \\frac{a_0}{2} (cos(\\frac{\\pi mod(t - 1, [T/M])}{[T/M]})) + 1)$ (1)\nwhere $a_0$ is the initial maximum learning rate, t is the iteration number, T is the total number of training iterations, and M is the number of cycles. A later paper used a variation called max-min cyclic cosine learning rate (MMCCLR), defined as [39]:\n$n_t = n_{min} + \\frac{1}{2} (n_{max} - n_{min}) (1 + cos(\\pi \\frac{mod(t/b, [T/Mb])}{[T/Mb]}))$ (2)\nwhere $n_t$ is the learning rate at step t, $n_{min}$ and $n_{max}$ are the minimum and maximum learning rates, respectively, and b is the batch size. The difference to cyclic cosine annealing is that the learning rate does not get as close to 0, which may lead to local minima being exploited a little better in the case of CCA.\nIn addition to those two schedulers, we explore two further variants, called deferred cyclic annealing. Assuming that a link prediction model requires some initial training in order to learn the principal structure of a knowledge graph, we defer the cyclic annealing by using a constant learning rate for the first k epochs before starting the cyclic annealing. The first model is then stored when the learning rate hits the first minimum. The result is a smaller number of models in a fixed set of epochs, but, on the other hand, those models may be a better fit due to the longer warmup time of the snapshot model."}, {"title": "3.2 Combining Predictions", "content": "In order to come up with a final prediction from the individual base models, their scores need to be aggregated. We follow different strategies:\nSimple average: For each prediction made with the base models, we normalize the scores of each model with a min-max scaler (so that each model's scores fall into a [0, 1] range), and average those normalized scores.\nWeighted average: Instead of using a simple average, we weigh each model by (a) a weight derived from the model's last training loss, or (b) with descending weights from the last stored base model, assigning the highest weight to the last model, assuming that this is the best fit.\nBorda rank aggregation: The rankings are unified using the Borda rank algorithm [9] (without considering the scores)."}, {"title": "3.3 Negative Samplers", "content": "Unlike many other machine learning problems, link prediction in knowledge graphs only comes with positive examples. Therefore, link prediction requires the creation of negative examples, which is usually done via randomly corrupt-ing positive examples, i.e., for a given triple < s, p, o >, either s or o is replaced with a random other entity to form a corrupted triple < s', p, o > or < s, p, o' >, respectively.\nIn addition to this standard sampling, we propose an extended negative sam-pler, which uses the last snapshot to create negative examples for the next cycle. The idea is similar to boosting [10], where an ensemble of models is trained in a sequence, with each model focusing on the mistakes made by the previous ones. After storing a snapshot model, the model is used to predict o' or s' for a triple < s, p, o > in the training set. The highest scoring negative < s,p, o' > or < s', p, o > is then added as a negative example. With that approach, we guide"}, {"title": "4 Evaluation", "content": "We run experiments with our approach and four different base models and compare them to the results achieved with the respective baseline models. In our evaluation, we consider two different setups:\n1. Equal training time budget. We train a model with d dimensions and m snapshots, and compare it to a single base model with d dimensions.\n2. Equal memory budget. We train a model with $d \\times m$ dimensions and m snapshots, and compare it to a single base model with d dimensions.\nThe code for reproducing the results is available online.\nWe compare our approach to the one proposed by Xu et al. [40], who also train an ensemble of lower dimensional models, coined as Mbase, i.e., MTransE, MDistMult, MComplex, and MRotatE. In order to ensure comparability to our approach, we use the same number of epochs for those models, train a total of 10 models (which is the maximum number of models for SnapE in our experiments, see section 4.3), and stick to the parameters in the original paper otherwise (i.e., a learning rate of 0.0003, Adam optimizer, and random negative sampling). All experiments were run on a server with 24 NVIDIA A100 GPUs with 32 GB of RAM each."}, {"title": "4.1 Datasets", "content": "We evaluate our approach on four different datasets, i.e., DBpedia50 [33], FB15k-237 [35], WN18RR [6], and AristoV4 [5]. The characteristics of the four datasets are depicted in table 1."}, {"title": "4.2 Base Models", "content": "As base models, we chose TransE [4], DistMult [41], Complex [36], and RotatE [34]. For each base model, we train two baseline models, one with 64 and one with 128 dimensions, and a batch size of 128 each. We determine the optimal number of epochs by applying early stopping, using relative_delta = 0.000001 and patience = 2, and Hits@10 as a target metric. Furthermore, we pick the best learning rate $lr \\in$ {10, 1, 0.1, 0.01} and the best optimizer out of stochastic gradient descent (SGD), Adam, and AdaGrad by applying grid search. The number of epochs that worked best for each base model is then also used for the snapshot ensemble (while the learning rate is controlled by the respective decay scheme). The parameters used for each model are listed in the appendix.\nNote that the goal is not to achieve the best possible baseline results [31]. We are more interested in analyzing the performance gains that can be obtained by using snapshot ensembles, compared to the standard training loop."}, {"title": "4.3 Parameters", "content": "For the parameters of the snapshot ensemble, we use the number of episodes that worked best on for the baseline model on the respective dataset (see table 5 in the appendix). Moreover, we use scores weighted by last training loss as an ag-gregation method, min-max-scaling to normalize the scores before, and deferred CCA as a scheduler. We pick the no. of cycles C out of {5, 10} and the number of models M to be used out of [2, C] ((cf. Fig. 2)9, as well as the best performing optimizer. The parameters used for each model are listed in the appendix."}, {"title": "4.4 Results", "content": "Table 2 shows a comparison of SnapE against the baselines. We show the results both for the setup with the same memory budget as well as for the setup with the same time budget.\nWe can observe that training with SnapE outperforms the standard training mechanism in about a third of the cases when considering the same memory budget case, and in almost all cases when considering the same training time budget case (in that case at the price of a larger overall model). Moreover, in all cases, the best overall result is achieved by an ensemble model.\nIt can also be observed that not all models benefit equally from the SnapE training. The gains for RotatE are moderate, while the other models, especially DistMult and Complex, benefit much more strongly. In particular, ComplEx (and, to a lesser extent, DistMult) strongly benefit on FB15k237 and WN18RR, whereas the improvements for TransE and Rotate are not that strong on those"}, {"title": "4.5 Runtime Behavior", "content": "Table 3 shows the runtime behavior of the baselines as well as the SnapE configurations for the same memory and the same training time budget.\nIn both cases, we can observe that the training time is similar to or even smaller than the one for the baseline method.\nOn the other hand, the prediction time is significantly larger for the same training time budget setup. The reason is that instead of making one model predict, each of the snapshot models has to make a prediction, which leads to the higher prediction time. The combination of the individual predictions also contributes to the prediction time, but to a lesser extent. For the same memory setup, this effect is not observed, since a prediction with a number of smaller models is not more time-consuming than a prediction with one large model.\nFor the Mbase approach, we observe significantly larger training times11, as already discussed in section 2, since Snape trains all its models in the same number of epochs that Mbase uses per model. The fact that the prediction time of Mbase is also higher is due to the fact that Mbase always uses all base models trained, while in SnapE, this is a hyperparameter, and often a smaller number of models is used (see appendix)."}, {"title": "4.6 Model Variety", "content": "One essential prerequisite for using ensemble methods successfully is that the base models expose some variety. Generally, the more diverse the single models in an ensemble are, the better the ensemble.\nTo measure the model variety, we look at the triple scores produced by the individual models, and compute their correlation for each pair of models. Table 4 shows the average of those correlations for all models in the ensemble, together with the HITS@10 of the best single base model and the ensemble."}, {"title": "4.7 Ablation Study", "content": "We analyze the impact of (1) different optimizers, (2) different learning rate schedulers (see Fig. 2), (3) different negative samplers, (4) different strategies for combining the predictions of the snapshots. In all the ablation studies, we use the best performing parameter sets, and vary only the respective parameter of study. For the sake of space, we show only the impact on Hits@10, and we only consider the setting with the same training time budget.\nFig. 5 shows results obtained with varying the optimizer. There is quite a bit of variation between using different optimizers. It can be observed that, in most cases, the Adam optimizer works best with SnapE. Only when using SnapE for TransE (and ocassionally RotatE), AdaGrad and SGD are preferred.\nFig. 6 shows the results for using different scheduling mechanisms. It can be observed that in general, the influence of the scheduler is rather minimal. In cases where the results differ, the deferred variants have an advantage over the non-deferred variants (i.e., those scheduling variants may help, but rather do not hurt). There is only a small difference between standard CCA and MMC-CLR. More detailed experiments have shown that CCA occasionally outperforms MMCCLR, but not vice versa."}, {"title": "5 Conclusion and Outlook", "content": "In this paper, we have introduced SnapE, which is a training mechanism for knowledge graph embedding models based on snapshot ensembles. Instead of training one single model in N epochs, SnapE trains a sequence of C models with shorter training cycles of length N/C each, using a decaying learning rate in each cycle. We have conducted experiments on four datasets, each time train-ing SnapE with TransE, DistMult, Complex, and Rotate as base models. The resulting ensemble method has been shown to often deliver superior results to an individual model, without increasing the training cost.\nThe SnapE framework allows for further experimentation. We have observed that learning rate schedulers and combination approaches have an impact on the overall results, thus, we can assume that more experimentation with dif-ferent variants here might yield even better results. As far as the learning rate schedulers go, early stopping could also be applied in each of the learning cy-cles, allowing for further optimization of the training time. The deferred training schedules could also allow to train the first iteration on a condensed version of the graph, as proposed in [17]. For combining the predictions of snapshot mod-els, techniques like mixture of experts [20] could be a promising candidate for creating even more powerful snapshot ensembles.\nWhile we could show that SnapE models can be trained at the same training cost as standard baseline models, we have observed a drastic increase in pre-diction time. At the same time, we could observe that it is often sufficient or even beneficial to only use a subset of the snapshots, which in turn lowers the prediction time. Thus, the selection strategy for the subset of snapshots seems a crucial property for further optimizing SnapE.\nWe have also observed that the possibilities that snapshot ensembles yield with respect to creating negative samples are intriguing. One could think of more elaborate techniques for exploiting negative samples from previous snapshots, e.g., mixing negative samples created by several previous snapshots."}]}