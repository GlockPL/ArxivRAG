{"title": "SnapE - Training Snapshot Ensembles of Link Prediction Models", "authors": ["Ali Shaban", "Heiko Paulheim"], "abstract": "Snapshot ensembles have been widely used in various fields of prediction. They allow for training an ensemble of prediction models at the cost of training a single one. They are known to yield more robust predictions by creating a set of diverse base models. In this paper, we introduce an approach to transfer the idea of snapshot ensembles to link prediction models in knowledge graphs. Moreover, since link prediction in knowledge graphs is a setup without explicit negative examples, we propose a novel training loop that iteratively creates negative examples using previous snapshot models. An evaluation with four base models across four datasets shows that this approach constantly outperforms the single model approach, while keeping the training time constant.", "sections": [{"title": "1 Introduction", "content": "Knowledge Graphs (KGs) are a crucial instrument for structuring complex, inter-\nrelated data in a way that is interpretable both by humans and machines [14,15].\nThey are used in many applications ranging from search engines to recommen-\ndation systems and from natural language processing to computer vision.\nKGs often exhibit sparseness and incompleteness [7,22]. This link sparse-\nness can significantly hinder the KGs' potential use in applications like semantic\nsearch. As a result, filling the missing links has gained traction in the AI commu-\nnity. This task is called Link Prediction, and it is one of the primary applications\nof machine learning and embedding methods on Knowledge Graphs [4]. Various\nresearch efforts have validated the efficacy of link prediction in addressing the\nsparsity issue in KGs and reinforcing their potential [21].\nLink prediction with knowledge graph embeddings (KGEs) faces many chal-\nlenges that have been the subject of research [2], including the sparsity of\ngraphs [27], noisy or erroneous data [42], and scalability [8], among others. In\nmachine learning, many of those challenges are addressed by ensemble methods.\nEnsembles can improve generalization by reducing the overfitting often seen in"}, {"title": "2 Related Work", "content": "Link prediction in knowledge graphs is an extensively researched area. Ap-\nproaches can be roughly categorized as geometric, matrix factorization, and\nneural network based approaches [30,38].\nOne of the earliest approaches to use ensembles for link prediction has been\nproposed by Krompass et al. [18]. They train three different heterogeneous mod-\nels (i.e., a TransE, a RESCAL, and an mwNN model), and make a prediction\nby averaging the probabilities for each predicted triple. Similar approaches are"}, {"title": "3 Approach", "content": "KGE models are trained by iteratively adapting their weights in order to maxi-\nmize a target function. In each iteration, the model's weights are adapted so that\nthe error function is lowered. The amount by which each weight can be changed\nin each iteration is called the learning rate. A high learning rate leads to a fast\nconvergence of the model, but may not yield the best possible solution. A low\nlearning rate, on the other hand, requires more time for convergence, but may\nyield better results. Therefore, many models are trained with a decaying learning\nrate, as shown in Fig. 1a. There are different decay functions for learning rates,\nsuch as step decay [11] or cosine annealing [19].\nThe idea of Snapshot Ensembles, introduced by Huang et al. in 2017 [16], uses\na cyclic learning rate, as shown in Fig. 1b. Each time the learning rate sched-\nule reaches a local minimum, a snapshot of the model is stored. At prediction\ntime, each stored model is used to predict, and the predictions are combined.\nThe original snapshot ensemble paper uses cyclic cosine annealing, as shown in"}, {"title": "3.1 Learning Rate Schedulers", "content": "As discussed above, the original snapshot ensemble uses the cosine cyclic anneal-\ning schema (CCA), defined as [19]:\n$$a(t) = \\frac{a_0}{2} \\bigg(cos \\Big(\\frac{\\pi mod(t - 1, 1, [T/M])}{[T/M]}\\Big)+1\\bigg)$$\nwhere $a_0$ is the initial maximum learning rate, $t$ is the iteration number, $T$ is\nthe total number of training iterations, and $M$ is the number of cycles. A later\npaper used a variation called max-min cyclic cosine learning rate (MMCCLR),\ndefined as [39]:\n$$n_t = n_{min} + \\frac{1}{2}(n_{max} - n_{min}) \\bigg(1+cos \\Big(\\frac{\\pi mod(t/b, [T/Mb])}{[T/Mb]}\\Big)\\bigg)$$\nwhere $n_t$ is the learning rate at step $t$, $n_{min}$ and $n_{max}$ are the minimum and\nmaximum learning rates, respectively, and $b$ is the batch size. The difference to\ncyclic cosine annealing is that the learning rate does not get as close to 0, which\nmay lead to local minima being exploited a little better in the case of CCA.\nIn addition to those two schedulers, we explore two further variants, called\ndeferred cyclic annealing. Assuming that a link prediction model requires some\ninitial training in order to learn the principal structure of a knowledge graph, we\ndefer the cyclic annealing by using a constant learning rate for the first $k$ epochs\nbefore starting the cyclic annealing. The first model is then stored when the\nlearning rate hits the first minimum. The result is a smaller number of models\nin a fixed set of epochs, but, on the other hand, those models may be a better\nfit due to the longer warmup time of the snapshot model."}, {"title": "3.2 Combining Predictions", "content": "In order to come up with a final prediction from the individual base models,\ntheir scores need to be aggregated. We follow different strategies:"}, {"title": "3.3 Negative Samplers", "content": "Unlike many other machine learning problems, link prediction in knowledge\ngraphs only comes with positive examples. Therefore, link prediction requires\nthe creation of negative examples, which is usually done via randomly corrupt-\ning positive examples, i.e., for a given triple < s, p, o >, either s or o is replaced\nwith a random other entity to form a corrupted triple < s', p, o > or < s, p, o' >,\nrespectively.\nIn addition to this standard sampling, we propose an extended negative sam-\npler, which uses the last snapshot to create negative examples for the next cycle.\nThe idea is similar to boosting [10], where an ensemble of models is trained in a\nsequence, with each model focusing on the mistakes made by the previous ones.\nAfter storing a snapshot model, the model is used to predict o' or s' for a\ntriple < s, p, o > in the training set. The highest scoring negative < s,p, o' > or\n< s', p, o > is then added as a negative example. With that approach, we guide"}, {"title": "4 Evaluation", "content": "We run experiments with our approach and four different base models and com-\npare them to the results achieved with the respective baseline models. In our\nevaluation, we consider two different setups:\n1. Equal training time budget. We train a model with d dimensions and m\nsnapshots, and compare it to a single base model with d dimensions.\n2. Equal memory budget. We train a model with $\\frac{d}{m}$ dimensions and m\nsnapshots, and compare it to a single base model with d dimensions.\nThe code for reproducing the results is available online.\nWe compare our approach to the one proposed by Xu et al. [40], who also\ntrain an ensemble of lower dimensional models, coined as Mbase, i.e., MTransE,\nMDistMult, MComplex, and MRotatE. In order to ensure comparability to\nour approach, we use the same number of epochs for those models, train a\ntotal of 10 models (which is the maximum number of models for SnapE in our\nexperiments, see section 4.3), and stick to the parameters in the original paper\notherwise (i.e., a learning rate of 0.0003, Adam optimizer, and random negative\nsampling). All experiments were run on a server with 24 NVIDIA A100 GPUs\nwith 32 GB of RAM each."}, {"title": "4.1 Datasets", "content": "We evaluate our approach on four different datasets, i.e., DBpedia50 [33], FB15k-\n237 [35], WN18RR [6], and AristoV4 [5]. The characteristics of the four datasets\nare depicted in table 1."}, {"title": "4.2 Base Models", "content": "As base models, we chose TransE [4], DistMult [41], Complex [36], and Ro-\ntatE [34]. For each base model, we train two baseline models, one with 64 and\none with 128 dimensions, and a batch size of 128 each. We determine the optimal\nnumber of epochs by applying early stopping, using relative_delta = 0.000001\nand patience = 2, and Hits@10 as a target metric. Furthermore, we pick the\nbest learning rate lr \u2208 {10,1,0.1,0.01} and the best optimizer out of stochas-\ntic gradient descent (SGD), Adam, and AdaGrad by applying grid search. The\nnumber of epochs that worked best for each base model is then also used for the\nsnapshot ensemble (while the learning rate is controlled by the respective decay\nscheme). The parameters used for each model are listed in the appendix.\nNote that the goal is not to achieve the best possible baseline results [31].\nWe are more interested in analyzing the performance gains that can be obtained\nby using snapshot ensembles, compared to the standard training loop."}, {"title": "4.3 Parameters", "content": "For the parameters of the snapshot ensemble, we use the number of episodes that\nworked best on for the baseline model on the respective dataset (see table 5 in\nthe appendix). Moreover, we use scores weighted by last training loss as an ag-\ngregation method, min-max-scaling to normalize the scores before, and deferred\nCCA as a scheduler. We pick the no. of cycles C out of {5, 10} and the number\nof models M to be used out of [2, C] ((cf. Fig. 2), as well as the best performing\noptimizer. The parameters used for each model are listed in the appendix."}, {"title": "4.4 Results", "content": "Table 2 shows a comparison of SnapE against the baselines. We show the results\nboth for the setup with the same memory budget as well as for the setup with\nthe same time budget.\nWe can observe that training with SnapE outperforms the standard training\nmechanism in about a third of the cases when considering the same memory\nbudget case, and in almost all cases when considering the same training time\nbudget case (in that case at the price of a larger overall model). Moreover, in all\ncases, the best overall result is achieved by an ensemble model.\nIt can also be observed that not all models benefit equally from the SnapE\ntraining. The gains for RotatE are moderate, while the other models, especially\nDistMult and Complex, benefit much more strongly. In particular, ComplEx\n(and, to a lesser extent, DistMult) strongly benefit on FB15k237 and WN18RR,\nwhereas the improvements for TransE and Rotate are not that strong on those"}, {"title": "4.5 Runtime Behavior", "content": "Table 3 shows the runtime behavior of the baselines as well as the SnapE con-\nfigurations for the same memory and the same training time budget.\nIn both cases, we can observe that the training time is similar to or even\nsmaller than the one for the baseline method.\nOn the other hand, the prediction time is significantly larger for the same\ntraining time budget setup. The reason is that instead of making one model\npredict, each of the snapshot models has to make a prediction, which leads to\nthe higher prediction time. The combination of the individual predictions also\ncontributes to the prediction time, but to a lesser extent. For the same memory\nsetup, this effect is not observed, since a prediction with a number of smaller\nmodels is not more time-consuming than a prediction with one large model.\nFor the Mbase approach, we observe significantly larger training times, as\nalready discussed in section 2, since Snape trains all its models in the same\nnumber of epochs that Mbase uses per model. The fact that the prediction time\nof Mbase is also higher is due to the fact that Mbase always uses all base models\ntrained, while in SnapE, this is a hyperparameter, and often a smaller number\nof models is used (see appendix)."}, {"title": "4.6 Model Variety", "content": "One essential prerequisite for using ensemble methods successfully is that the\nbase models expose some variety. Generally, the more diverse the single models\nin an ensemble are, the better the ensemble.\nTo measure the model variety, we look at the triple scores produced by the\nindividual models, and compute their correlation for each pair of models. Table 4\nshows the average of those correlations for all models in the ensemble, together\nwith the HITS@10 of the best single base model and the ensemble."}, {"title": "4.7 Ablation Study", "content": "We analyze the impact of (1) different optimizers, (2) different learning rate\nschedulers (see Fig. 2), (3) different negative samplers, (4) different strategies\nfor combining the predictions of the snapshots. In all the ablation studies, we\nuse the best performing parameter sets, and vary only the respective parameter\nof study. For the sake of space, we show only the impact on Hits@10, and we\nonly consider the setting with the same training time budget.\nFig. 5 shows results obtained with varying the optimizer. There is quite a bit\nof variation between using different optimizers. It can be observed that, in most\ncases, the Adam optimizer works best with SnapE. Only when using SnapE for\nTransE (and ocassionally RotatE), AdaGrad and SGD are preferred.\nFig. 6 shows the results for using different scheduling mechanisms. It can\nbe observed that in general, the influence of the scheduler is rather minimal. In\ncases where the results differ, the deferred variants have an advantage over the\nnon-deferred variants (i.e., those scheduling variants may help, but rather do\nnot hurt). There is only a small difference between standard CCA and MMC-\nCLR. More detailed experiments have shown that CCA occasionally outperforms\nMMCCLR, but not vice versa."}, {"title": "5 Conclusion and Outlook", "content": "In this paper, we have introduced SnapE, which is a training mechanism for\nknowledge graph embedding models based on snapshot ensembles. Instead of\ntraining one single model in N epochs, SnapE trains a sequence of C models\nwith shorter training cycles of length N/C each, using a decaying learning rate\nin each cycle. We have conducted experiments on four datasets, each time train-\ning SnapE with TransE, DistMult, Complex, and Rotate as base models. The\nresulting ensemble method has been shown to often deliver superior results to\nan individual model, without increasing the training cost.\nThe SnapE framework allows for further experimentation. We have observed\nthat learning rate schedulers and combination approaches have an impact on\nthe overall results, thus, we can assume that more experimentation with dif-\nferent variants here might yield even better results. As far as the learning rate\nschedulers go, early stopping could also be applied in each of the learning cy-\ncles, allowing for further optimization of the training time. The deferred training\nschedules could also allow to train the first iteration on a condensed version of\nthe graph, as proposed in [17]. For combining the predictions of snapshot mod-\nels, techniques like mixture of experts [20] could be a promising candidate for\ncreating even more powerful snapshot ensembles.\nWhile we could show that SnapE models can be trained at the same training\ncost as standard baseline models, we have observed a drastic increase in pre-\ndiction time. At the same time, we could observe that it is often sufficient or\neven beneficial to only use a subset of the snapshots, which in turn lowers the\nprediction time. Thus, the selection strategy for the subset of snapshots seems\na crucial property for further optimizing SnapE.\nWe have also observed that the possibilities that snapshot ensembles yield\nwith respect to creating negative samples are intriguing. One could think of more\nelaborate techniques for exploiting negative samples from previous snapshots,\ne.g., mixing negative samples created by several previous snapshots."}]}