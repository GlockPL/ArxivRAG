{"title": "The Data Addition Dilemma", "authors": ["Judy Hanwen Shen", "Inioluwa Deborah Raji", "Irene Y. Chen"], "abstract": "In many machine learning for healthcare tasks, standard datasets are constructed by amassing data across many, often fundamentally dissimilar, sources. But when does adding more data help, and when does it hinder progress on desired model outcomes in real-world settings? We identify this situation as the Data Addition Dilemma, demonstrating that adding training data in this multi-source scaling context can at times result in reduced overall accuracy, uncertain fairness outcomes, and reduced worst-subgroup performance. We find that this possibly arises from an empirically observed trade-off between model performance improvements due to data scaling and model deterioration from distribution shift. We thus establish baseline strategies for navigating this dilemma, introducing distribution shift heuristics to guide decision-making on which data sources to add in data scaling, in order to yield the expected model performance improvements. We conclude with a discussion of the required considerations for data collection and suggestions for studying data composition and scale in the age of increasingly larger models.", "sections": [{"title": "1. Introduction", "content": "Medical institutions will often go to great lengths to increase the size of the available dataset for model development. While some of these data gathering efforts are squarely focused on evaluation (Borfitz, 2024), many are oriented towards compiling larger and larger medical training sets, composed of data from an increasing number of data sources (Philips, 2023; Palmer, 2022; Biobank, 2022; Ramirez et al., 2022). Due to the limited data availability in any given data source (e.g., a clinical site), data scaling in such cases involves not just collecting more samples from a single source but also accumulating data across a variety of available sources. This results in data scaling that also incurs changes to training data composition. This leads to models with less predictable outcomes, which can at times perform worse than smaller scale in-distribution models.\nSo, when should a practitioner choose to add out-of-distribution data sources to supplement a training set, and which sources should they add such that performance can continue to improve? We identify this data decision-making as part of the Data Addition Dilemma, which describes the difficulty to making such choices in the context of multi-source data scaling (Figure 1). We take a pragmatic approach to data accumulation and construct scenarios that more explicitly factor in both changes to scale and the corresponding changes"}, {"title": "2. Related Work", "content": "Medical distribution shift and data scaling. The effect of distribution shift has been well-studied in medical settings (Nestor et al., 2019; Wilson et al., 2021; Wong et al., 2021; Koh et al., 2021; Daneshjou et al., 2022; Futoma et al., 2020; Guo et al., 2022; Chirra et al., 2018; Yang et al., 2023). Prior solutions have focused on methods to address distribution shift including stability evaluation (Subbaswamy et al., 2021), causal methods (Zhang et al., 2023), and domain adaptation (Subbaswamy et al., 2019); however, the joint problem of distribution shift and data scaling has only recently been proposed. The closest work to ours, Compton et al. (2023), reasons about how additional data may not improve performance and studies the problem through the lens of spurious correlations. We focus instead on practical tools for determining when and how to add training data from multiple sources with distribution shift."}, {"title": "3. The Data Addition Dilemma", "content": "Much of the past work on the impact of data scaling on model performance assumes a single-source setting, where data from a single data sampling process is scaled up, and the distribution of the dataset remains fixed. That is, as the number of training samples $n$"}, {"title": "5. Results and Analysis", "content": "We present our analysis of case studies of the Data Addition Dilemma, and characterize a possible heuristic to guide data decision-making for training data composition while scaling."}, {"title": "5.1. Performance Patterns Under Single-Source Distribution Shift", "content": "We find that training on one hospital and testing on another yields notable changes in AUC performance (Figure 3) at times making performance slightly better than in-distribution training but more often making performance significantly worse. This complements prior findings that out-of-distribution test sets may yield both positive and negative changes in performance (Spathis and Hyland, 2022). For example, for hospital 73, training on hospital 199 data (instead of hospital 73 data) could yield an improvement of 0.06 AUC; however, for hospital 338, training on hospital 199 data (instead of hospital 338) would yield a drop of 0.04 AUC. These pairwise train-test hospital combinations are consistent across LR, LGBM, and LSTM model (Figure 3b, 11) with strongly significant correlation (LGBM vs LSTM: 0.47, $p = 2 \\times 10^{-9}$; LGBM vs LR: 0.76, $p = 8 \\times 10^{-29}$). This motivates our data-centric investigation since these models arising from very different training algorithms still yield similar behavior across shifts in distribution between hospitals."}, {"title": "5.2. Characterizing Performance Patterns under Data Addition", "content": "Deciding whether to add data sources is a key action for many stakeholders and a primary motivation for our project (Figure 2). In order to explore the impact of data addition on data composition changes, we map out the performance pattern changes under a training set"}, {"title": "5.3. Data Addition Heuristics", "content": "With the significant correlation between out-of-distribution AUC change and data addition AUC change, we are motivated to find ways to characterize distribution shifts that may help us decide how to add data in the multi-source scaling setting.\nWe first turn to normative metrics of dataset descriptions that have been studied in prior work for compare distribution shift across and within datasets (Spathis and Hyland, 2022; Schrouff et al., 2022). We examine the distance between age, race, and sex distribution, age, race, and sex mortality as well as the overall mortality rate. Firstly, we observe significant differences in these dataset statistics. For example, mortality rates for male patients vary between 1.6% and 9.8% across the 12 hospitals we examine. As another"}, {"title": "6. Discussion", "content": "We present the trade-offs to adding more data in the multi-source setting as the Data Addition Dilemma. We motivate this problem by observing that for the commonly studied mortality prediction task in particular, adding data can both improve and worsen performance. We introduce a sequential data accumulation framework to formalize why adding data could yield worse performance. We conduct a thorough investigation into distribution shift and data addition across 12 hospitals and 3 different model architectures. We examine different distances to characterize distribution shifts that better correlate with out-of-distribution performance than population statistics. Ultimately, we find our heuristics to be informative in deciding which sources of data to add. In general, we expect the trade-off between a smaller high-quality data set and a larger low-quality dataset to be model-dependent. Nevertheless, our results motivate practitioners, particularly in high-stakes applications, to carefully consider the costs and benefits of adding more data."}, {"title": "Limitations & Future Work", "content": "While our work presents a theoretical basis and rigorous experiments of the Data Addition Dilemma, we acknowledge several limitations of our work for a complete picture of a general law for data accumulation. First, our experimental setting focuses entirely on binary mortality prediction from time-series data in the intensive care unit (ICU) setting, which enables clean performance metrics to compare across hospitals. To build a more complete picture of the trade-offs of data scaling and distribution shift in health and healthcare, we imagine similar principles will apply in future work across key topics like modeling treatment strategies (Gottesman et al., 2019; Miao et al., 2024), risk stratification (Razavian et al., 2015; Beaulieu-Jones et al., 2021; Chen et al., 2020a; Seyyed-Kalantari et al., 2021, 2020), and patient subtyping (Lawton et al., 2018; Choi et al., 2023; Chen et al., 2022). Second, our data accumulation method is simplified to comparing adding data from hospitals sequentially with adding a mixture of data from all available hospitals. A more complex model could include a combination of two or more sophisticated data accumulation strategies. Thirdly, our work considers data accumulation as the key intervention for the Data Addition Dilemma. There are many other methods to investigate data-driven performance trade-offs including data pruning (Sorscher et al., 2022; Hooker et al., 2020) and synthetic data (Nikolenko, 2019). Lastly, while we present one model for data accumulation, further theoretical work in data modeling such as robustness guarantees (Hu et al., 2024) and distributional analyses (Chaudhuri et al., 2023; Feldman, 2020) would vastly improve guarantees for decision-making about datasets.\nData decision-making is a critical factor in the effective execution of machine learning\u2014yet little is understood of how practical data curation and collection strategies ultimately impact model outcomes. This work is an initial inquiry into what is often an overlooked aspect of the machine-learning process. In particular, determining the best protocols for adding health data have important implications for clinical settings. We hope this can be a vehicle towards more thorough future modeling and investigations into the practical data decision-making process that underscores much of the model behavior in deployed systems."}, {"title": "Appendix A. Extensions to Notions of Data Quality", "content": "The canonical model of data in machine learning assumes that empirical data are sampled iid from an underlying distribution. There are several consequences of these given conditions, most notably the following assumptions:\n\u2022 Test and training set data distributions are identical.\n\u2022 Training data distribution remains consistent as the dataset size increases.\nHowever, something that is not as often considered is how the training set distribution changes as dataset size increases, at times becoming increasingly divergent from a fixed test distribution. In this work, we specifically investigate this phenomenon."}, {"title": "A.1. Sources of Sampling Bias", "content": "Not all cheap data is equally bad and not all distributions are equally shifted. In this paper, we discuss sampling bias issues incurred via data accumulation. Here, we awknowledge some common sources of sampling bias that may arise when the following types of data are favored in the data sampling process:\n1. Easy-to-access data: Machine learning datasets are often amassed using data that is readily available on the internet or even data that is collected without consent. In the healthcare domain, publicly available datasets from a few select hospitals may dominate the training set for a specific task.\n2. Complete data: Parts of datasets with missing features may be discarded. Individuals may be included in surveys only if a survey is filled out in its entirety. Differential knowledge of family medical history may impact which health records are used to identify and develop certain diagnostic tools.\n3. Unambiguous data: Data points with multiple or conflicting labels may be discarded. Outlier or ambiguous images or text data may be discarded to bolder the statistical robustness of the resultant classifier."}, {"title": "Appendix B. Detailed Related Work", "content": "B.1. Data Scaling\n\"Scaling laws\" more broadly refer to how increases in model \u201csize\u201d lead to improved per-formance. Typically, model size is described in terms of the number of model parameters, compute and other measured factors characterizing the model (Kaplan et al., 2020; Bahri et al., 2021). Data \"scaling laws\" (Zhang et al., 2020; Bansal et al., 2022; Zhai et al.) specifically reveal the way in which training on larger and larger datasets yield improved performance. For instance, in (Touvron et al., 2023), large language models trained on datasets with at least 1T tokens beats out models with an order of magnitude more param-eters.\nLooking at scaling laws for multiple sources in particular, (Hashimoto, 2021) model data as coming from k sources $q_1, ..., q_k$ where $p = \\sum_{k \\in [k]} q_kp_k$ is the training data; these sources"}, {"title": "Components of Data Composition", "content": "B.2. The Limitations of More Data\nUnfortunately, more data can also lead us astray. (Meng, 2018) present a model which de-composes estimation error into three components: data quality, data quantity, and problem difficulty:\n$\\theta - \\hat{\\theta} = p_{R,\\theta} \\times \\sqrt{\\frac{1-f}{f}} \\times \\sigma_{\\theta}$\nwhere $R$ is a binary random variable representing whether an individual responded, $\\theta$ is the quantity of interest we are trying to estimate, and $f$ captures how much of the underlying population (i.e. $f = 1$ corresponds to the entire population while $f = 0$ corresponds to no data). $p_{R,\\theta}$ represents data quality; if $R$ corresponded to a perfected random sample, the correlation between $\\theta$ and $R$ should be zero. $\\sqrt{\\frac{1-f}{f}}$ represents error arising from data quantity; if we survey the entire population $f = 1$, the error would be zero. $\\sigma_{\\theta}$ captures problem difficulty and would be zero if $\\theta$ is a constant. Furthermore, (Meng, 2018) shows that data quality cannot be compensated by more data when sampling is biased and not truly probabilistic, estimation error scales according to $\\sqrt{N}$ the population size and not the sample size. Looking at a Z-score for the estimation of $\\theta$:\n$Z_{n,N} = \\frac{\\hat{\\theta}_n - \\theta_N}{\\sqrt{VSRS(\\hat{\\theta}_n)}} = \\sqrt{N - 1}p_{R,\\theta}$"}, {"title": "Representational Matters", "content": "B.3. Representation matters (Rolf et al, 2021)\nDoes collecting more data from underrepresented groups help? In Rolf et al, the key mecha-nism of change is the allocation of the proportion of groups in the dataset, which the model practitioner can either choose directly or learn for the optimal loss. This assumes the ability to sample directly from the group-specific distribution. The work also maps the approach to importance weighting (reweight training samples with respect to group distributions) and group distributional robust optimization (minimizes the maximum empirical risk over all groups).\nWe can formulate this as a function of the group proportions of the training data. For dataset $S = \\{x_i, y_i, g_i\\}_{i=1}^n$ where features $x_i$, label $y_i$, and discrete group $g_i$ for groups $G-\\{1,...,|G|\\}$ are measured. The population prevalence $\\gamma_g = P_{(X,Y,G)~D}[G = g]$ is related to the ability to empirical allocation of groups in the data $a_g = \\frac{1}{n} \\sum_{i=1}^n 1[g_i = g]$.\nSampling from allocation a is defined as independently sampling of |G| disjoint datasets $S_g$ and concatenating according to $S(a, n) = \\cup_{g\\in G} S_g$ with\n$S_g = \\{x_i, y_i, g\\}_{i=1}^n, (x_i, y_i) ~iid D_g$\nOther notions of representativeness such as lexicographic versions of fairness have also been proposed Henzinger et al. (2022)."}, {"title": "More Data Can Be Helpful for Fairness", "content": "B.4. More data can be helpful for fairness sometimes (Chen et al, 2018)\nThe paper focuses on many different ways to improve fairness of a model, one of which may be adding more training data. As relevant to this group, error due to variance (as opposed to statistical bias and statistical noise) can be estimated via a distribution learning curve. An assumption of the model is therefore that any new data will be from the same distribution as the training data, on which the learning curve is estimated.\nUnfairness can be defined as $\\Gamma(\\hat{Y}, n) := |\\gamma(\\hat{Y}, n) - \\gamma_1(\\hat{Y}, n)|$ for predictions $\\hat{Y}$, sample size $n$, and group-specific unfairness $\\gamma$. Based on prior empirical studies, these type II learning curves can be approximated as asymptotic inverse power-law $\\gamma_a(\\hat{Y}, n_a) = \\alpha_a n_a^{\\beta_a} + \\delta_a$.\nHowever, when subgroups are difficult to identify, the role of data is less known. Recent work by Izzo et al. (2023) presents data-driven strategies for finding subpopulations based"}, {"title": "Component of Data Composition", "content": "B.5. Relationship to Domain Adaptation, Distribution Shift, and Transfer Learning\nDomain Adaptation The field of domain adaption provides another lens to view the problem of overcoming data quality challenges. Domain Adaption is concerned with the problem of training models with source data and performing well on a target domain (Kouw and Loog, 2018; Zhuang et al., 2020). In Unsupervised Domain Adaptation, the majority of the work in the area, labeled source domain data, and unlabeled target domain data are available for training(Wilson and Cook, 2020). In Supervised Domain Adaptation, only a few or scarce samples from the target dataset and the source dataset are fully labeled(Motiian et al., 2017). In this setting, the target domain can be thought of as high-quality data, and the source domain can be thought of as low-quality data (Kouw and Loog, 2018).\nA related area of literature concerned with shifts in data quality is distributional ro-bustness and distribution shift. Worst subgroup performance (subpopulation shift) and overall performance (domain generalization) on an out-of-distribution dataset are metrics presented in the domain adaptation literature aimed at measuring distribution shift (Koh"}, {"title": "Appendix E. Proofs", "content": "Proof for Lemma 3.1\nLemma 2\nLet $D_{train,n}$ be constructed in the SEQUENTIAL case from k sources: $D_{s_1},..., D_{s_k}$, then\nif $\\delta(D_{s_k}, D_{test}) - \\frac{c n}{n_{s_k}} \\ge \\delta(D_{train,n}, D_{test})$\n$\\delta(D_{train,n}, D_{test}) \\ge \\delta(D_{train,n - n_{s_k}}, D_{test})$\nwhere $\\delta$ belongs to the family of f-divergences and c is a divergence-dependent constant where\n$\\delta(D_{train,n}, D_{test}) + c = \\frac{n}{n} \\sum_i^k \\delta(D_{s_i}, D_{test})$.\nProof Without loss of generality, the last source in the composition of $D_{train,n}$ is partially used, we define the size of the last source as simply $n_k$ and forget that there is unused data in the last source. Thus we can simply the overall training distribution as $D_{train,n} = \\sum a_i D_{s_i}$ where $a_i = \\frac{n_i}{n}$. Furthermore, let n be the total number of examples with k sources and let n' = n - nk be the total number of examples with k \u2013 1 sources.\nBy convexity of $\\delta$ (Jenson's):\n$\\delta(D_{train,n'}, D_{test})$\n$\\le \\frac{\\sum_i^{k-1} n_i \\delta(D_{s_i}, D_{test})}{n'}$\n$\\frac{\\sum_i^{k-1} n_i \\delta(D_{s_i}, D_{test})}{n} \\frac{n}{n'}$\n$= \\frac{n}{n'}[ \\frac{n}{\\sum_i^k n_i \\delta(D_{s_i}, D_{test})} - \\frac{n_k \\delta(D_{s_k}, D_{test})}{n}]$\n$= \\frac{n}{n'} [ \\frac{n}{\\sum_i^k n_i \\delta(D_{s_i}, D_{test})} - c + \\frac{c}{n}]$\nSince $\\delta(D_{train,n}, D_{test}) \\le \\sum_i^k \\frac{n_i}{n} \\delta(D_{s_i}, D_{test})$ then for some constant c:\n$= \\frac{n}{n'}[ \\frac{c}{n} \\le \\delta(D_{train,n}, D_{test}) + c]$"}]}