{"title": "The Data Addition Dilemma", "authors": ["Judy Hanwen Shen", "Inioluwa Deborah Raji", "Irene Y. Chen"], "abstract": "In many machine learning for healthcare tasks, standard datasets are constructed by amassing data across many, often fundamentally dissimilar, sources. But when does adding more data help, and when does it hinder progress on desired model outcomes in real-world settings? We identify this situation as the Data Addition Dilemma, demonstrating that adding training data in this multi-source scaling context can at times result in reduced overall accuracy, uncertain fairness outcomes, and reduced worst-subgroup performance. We find that this possibly arises from an empirically observed trade-off between model performance improvements due to data scaling and model deterioration from distribution shift. We thus establish baseline strategies for navigating this dilemma, introducing distribution shift heuristics to guide decision-making on which data sources to add in data scaling, in order to yield the expected model performance improvements. We conclude with a discussion of the required considerations for data collection and suggestions for studying data composition and scale in the age of increasingly larger models.", "sections": [{"title": "1. Introduction", "content": "Medical institutions will often go to great lengths to increase the size of the available dataset for model development. While some of these data gathering efforts are squarely focused on evaluation (Borfitz, 2024), many are oriented towards compiling larger and larger medical training sets, composed of data from an increasing number of data sources (Philips, 2023;\nPalmer, 2022; Biobank, 2022; Ramirez et al., 2022). Due to the limited data availability in any given data source (e.g., a clinical site), data scaling in such cases involves not just collecting more samples from a single source but also accumulating data across a variety of available sources. This results in data scaling that also incurs changes to training data composition. This leads to models with less predictable outcomes, which can at times perform worse than smaller scale in-distribution models.\nSo, when should a practitioner choose to add out-of-distribution data sources to supple-ment a training set, and which sources should they add such that performance can continue to improve? We identify this data decision-making as part of the Data Addition Dilemma, which describes the difficulty to making such choices in the context of multi-source data scaling (Figure 1). We take a pragmatic approach to data accumulation and construct sce-narios that more explicitly factor in both changes to scale and the corresponding changes"}, {"title": "2. Related Work", "content": "Medical distribution shift and data scaling. The effect of distribution shift has been well-studied in medical settings (Nestor et al., 2019; Wilson et al., 2021; Wong et al., 2021;\nKoh et al., 2021; Daneshjou et al., 2022; Futoma et al., 2020; Guo et al., 2022; Chirra et al.,\n2018; Yang et al., 2023). Prior solutions have focused on methods to address distribution shift including stability evaluation (Subbaswamy et al., 2021), causal methods (Zhang et al., 2023), and domain adaptation (Subbaswamy et al., 2019); however, the joint problem of distribution shift and data scaling has only recently been proposed. The closest work to ours, Compton et al. (2023), reasons about how additional data may not improve perfor-mance and studies the problem through the lens of spurious correlations. We focus instead on practical tools for determining when and how to add training data from multiple sources with distribution shift."}, {"title": "Generalizable Insights about Machine Learning in the Context of Healthcare", "content": "Our work presents the following generalizable insights for machine learning in the context of healthcare:\n1. Problem formulation of the Data Addition Dilemma: We motivate and for-malize cases of data accumulation from single-source and multi-source settings and discuss how the latter is not well-formalized. We then formally present the Data Ad-dition Dilemma, where data composition changes during scaling in the multi-source setting can result in fairly unpredictable downstream model outcomes.\n2. Exploring data composition changes: We theoretically demonstrate how data composition changes from multi-source scaling can lead to worse model outcomes. We explore multiple distribution shift measures, and discuss how this distribution shift in single-source and multi-source data addition settings can influence performance patterns.\n3. Strategies for data addition: We present a simple heuristic to determine when to add more data. We discuss the performance impact of the data composition changes to data scaling in the multi-source setting, and illustrate on hospital case studies from a real-world dataset how to best approach the Data Addition Dilemma, and select compatible additional data sources that are most likely to yield performance improvement.\nMost importantly, we hope for this work will be a critical starting point in formaliz-ing the complex dynamics underlying data decision-making as part of the machine learn-ing process. The details of data practices are often overlooked by the machine learning research community altogether-despite its key role in determining the nature of model outcomes (Paullada et al., 2021). This work represents a strong starting point for a deeper investigation by the machine learning community into more principle-based foundations of meaningful data practices."}, {"title": "3. The Data Addition Dilemma", "content": "Much of the past work on the impact of data scaling on model performance assumes a single-source setting, where data from a single data sampling process is scaled up, and the distribution of the dataset remains fixed. That is, as the number of training samples n"}, {"title": "5. Results and Analysis", "content": "We present our analysis of case studies of the Data Addition Dilemma, and characterize a possible heuristic to guide data decision-making for training data composition while scaling."}, {"title": "5.1. Performance Patterns Under Single-Source Distribution Shift", "content": "We find that training on one hospital and testing on another yields notable changes in AUC performance (Figure 3) at times making performance slightly better than in-distribution training but more often making performance significantly worse. This complements prior findings that out-of-distribution test sets may yield both positive and negative changes in performance (Spathis and Hyland, 2022). For example, for hospital 73, training on hospital 199 data (instead of hospital 73 data) could yield an improvement of 0.06 AUC; however, for hospital 338, training on hospital 199 data (instead of hospital 338) would yield a drop of 0.04 AUC. These pairwise train-test hospital combinations are consistent across LR, LGBM, and LSTM model (Figure 3b, 11) with strongly significant correlation (LGBM vs LSTM: 0.47, p = 2 \u00d7 10-9; LGBM vs LR: 0.76, p = 8 \u00d7 10-29). This motivates our data-centric investigation since these models arising from very different training algorithms still yield similar behavior across shifts in distribution between hospitals."}, {"title": "5.2. Characterizing Performance Patterns under Data Addition", "content": "Deciding whether to add data sources is a key action for many stakeholders and a primary motivation for our project (Figure 2). In order to explore the impact of data addition on data composition changes, we map out the performance pattern changes under a training set"}, {"title": "5.3. Data Addition Heuristics", "content": "With the significant correlation between out-of-distribution AUC change and data addition AUC change, we are motivated to find ways to characterize distribution shifts that may help us decide how to add data in the multi-source scaling setting.\nWe first turn to normative metrics of dataset descriptions that have been studied in prior work for compare distribution shift across and within datasets (Spathis and Hyland, 2022;\nSchrouff et al., 2022). We examine the distance between age, race, and sex distribution, age, race, and sex mortality as well as the overall mortality rate. Firstly, we observe significant differences in these dataset statistics. For example, mortality rates for male patients vary between 1.6% and 9.8% across the 12 hospitals we examine. As another"}, {"title": "5.4. Additional Observations", "content": "In addition to overall model AUC, there are other model performance measures of interest in this study. Notably, past investigations indicate that, in a simple tabular census setting, as training data size scales, even when overall model accuracy improves, group disparities may in fact worsen (Ding et al., 2021). We analyze case studies of the impact of data composition and scaling of the worst group accuracy and performance disparity between groups (as defined in Section 4.2). More practically, we compute the lowest accuracy measured across all available subgroups as the worst-case subgroup accuracy, and the difference between the worst-case and best-case subgroup accuracy as the subgroup accuracy disparity.\nWhen considering how our results impact subgroup disparity and worst-case subgroup performance, we find that the models perform in fairly unexpected ways (Figure 14, 16). For instance, for hospital 443, the accuracy disparity decreases more (0.329 to 0.050) when adding data from the 3 best hospitals using our heuristic compared to our mixture baseline case (0.329 to 0.414), and increases worst group accuracy (0.583 to 0.833). However, for hospital 199, the opposite is true-adding data from the three best hospitals according to our heuristic increases disparity (0.198 to 0.448) and reduces worst group accuracy (0.750 to 0.500), likely due to small sample sizes. These results are potentially related to model performance differences across groups being inconsistently impacted by the subgroup com-position of training sources (Figure 12, 13). Additionally, under different data addition combinations, worst subgroup performance can vary wildly (Figure 15). Future work could further investigate subgroup performance pattern changes under data addition."}, {"title": "6. Discussion", "content": "We present the trade-offs to adding more data in the multi-source setting as the Data Addi-tion Dilemma. We motivate this problem by observing that for the commonly studied mor-tality prediction task in particular, adding data can both improve and worsen performance. We introduce a sequential data accumulation framework to formalize why adding data could yield worse performance. We conduct a thorough investigation into distribution shift and data addition across 12 hospitals and 3 different model architectures. We examine different distances to characterize distribution shifts that better correlate with out-of-distribution performance than population statistics. Ultimately, we find our heuristics to be informa-tive in deciding which sources of data to add. In general, we expect the trade-off between a smaller high-quality data set and a larger low-quality dataset to be model-dependent. Nevertheless, our results motivate practitioners, particularly in high-stakes applications, to carefully consider the costs and benefits of adding more data."}, {"title": "Limitations & Future Work", "content": "While our work presents a theoretical basis and rigorous experiments of the Data Addition Dilemma, we acknowledge several limitations of our work for a complete picture of a general law for data accumulation. First, our experimental setting focuses entirely on binary mortality prediction from time-series data in the intensive care unit (ICU) setting, which enables clean performance metrics to compare across hospitals.\nTo build a more complete picture of the trade-offs of data scaling and distribution shift in health and healthcare, we imagine similar principles will apply in future work across key topics like modeling treatment strategies (Gottesman et al., 2019; Miao et al., 2024), risk stratification (Razavian et al., 2015; Beaulieu-Jones et al., 2021; Chen et al., 2020a; Seyyed-Kalantari et al., 2021, 2020), and patient subtyping (Lawton et al., 2018; Choi et al., 2023; Chen et al., 2022). Second, our data accumulation method is simplified to comparing adding data from hospitals sequentially with adding a mixture of data from all available hospitals. A more complex model could include a combination of two or more sophisticated data accumulation strategies. Thirdly, our work considers data accumulation as the key intervention for the Data Addition Dilemma. There are many other methods to investigate data-driven performance trade-offs including data pruning (Sorscher et al.,\n2022; Hooker et al., 2020) and synthetic data (Nikolenko, 2019). Lastly, while we present one model for data accumulation, further theoretical work in data modeling such as robustness guarantees (Hu et al., 2024) and distributional analyses (Chaudhuri et al., 2023; Feldman, 2020) would vastly improve guarantees for decision-making about datasets.\nData decision-making is a critical factor in the effective execution of machine learning yet little is understood of how practical data curation and collection strategies ultimately impact model outcomes. This work is an initial inquiry into what is often an overlooked aspect of the machine-learning process. In particular, determining the best protocols for adding health data have important implications for clinical settings. We hope this can be a vehicle towards more thorough future modeling and investigations into the practical data decision-making process that underscores much of the model behavior in deployed systems."}, {"title": "Appendix A. Extensions to Notions of Data Quality", "content": "The canonical model of data in machine learning assumes that empirical data are sampled iid from an underlying distribution. There are several consequences of these given conditions, most notably the following assumptions:\n\u2022 Test and training set data distributions are identical.\n\u2022 Training data distribution remains consistent as the dataset size increases.\nHowever, something that is not as often considered is how the training set distribution changes as dataset size increases, at times becoming increasingly divergent from a fixed test distribution. In this work, we specifically investigate this phenomenon."}, {"title": "A.1. Sources of Sampling Bias", "content": "Not all cheap data is equally bad and not all distributions are equally shifted. In this paper, we discuss sampling bias issues incurred via data accumulation. Here, we awknowledge some common sources of sampling bias that may arise when the following types of data are favored in the data sampling process:\n1. Easy-to-access data: Machine learning datasets are often amassed using data that is readily available on the internet or even data that is collected without consent. In the healthcare domain, publicly available datasets from a few select hospitals may dominate the training set for a specific task.\n2. Complete data: Parts of datasets with missing features may be discarded. Individuals may be included in surveys only if a survey is filled out in its entirety. Differential knowledge of family medical history may impact which health records are used to identify and develop certain diagnostic tools.\n3. Unambiguous data: Data points with multiple or conflicting labels may be discarded. Outlier or ambiguous images or text data may be discarded to bolder the statistical robustness of the resultant classifier."}, {"title": "Appendix B. Detailed Related Work", "content": ""}, {"title": "B.1. Data Scaling", "content": "\"Scaling laws\" more broadly refer to how increases in model \u201csize\u201d lead to improved per-formance. Typically, model size is described in terms of the number of model parameters, compute and other measured factors characterizing the model (Kaplan et al., 2020; Bahri et al., 2021). Data \"scaling laws\" (Zhang et al., 2020; Bansal et al., 2022; Zhai et al.) specifically reveal the way in which training on larger and larger datasets yield improved performance. For instance, in (Touvron et al., 2023), large language models trained on datasets with at least 1T tokens beats out models with an order of magnitude more param-eters.\nLooking at scaling laws for multiple sources in particular, (Hashimoto, 2021) model data as coming from k sources $q_1, ..., q_k$ where $p = \\sum_{k \\in [k]} q_k p_k$ is the training data; these sources"}, {"title": "B.2. The Limitations of More Data", "content": "Unfortunately, more data can also lead us astray. (Meng, 2018) present a model which de-composes estimation error into three components: data quality, data quantity, and problem difficulty:\n$\\theta - \\hat{\\theta} = \\rho_{R,\\theta} \\times \\sqrt{\\frac{1-f}{f}} \\times \\sigma_{\\theta}$\nwhere R is a binary random variable representing whether an individual responded, $\\theta$ is the quantity of interest we are trying to estimate, and f captures how much of the underlying population (i.e. f = 1 corresponds to the entire population while f = 0 corresponds to no data). $\\rho_{R,\\theta}$ represents data quality; if R corresponded to a perfected random sample, this correlation between $\\theta$ and R should be zero. $\\sqrt{\\frac{1-f}{f}}$ represents error arising from data quantity; if we survey the entire population f = 1, the error would be zero. $\\sigma_{\\theta}$ captures problem difficulty and would be zero if $\\theta$ is a constant. Furthermore, (Meng, 2018) shows that data quality cannot be compensated by more data when sampling is biased and not truly probabilistic, estimation error scales according to $\\sqrt{N}$ the population size and not the sample size. Looking at a Z-score for the estimation of $\\theta$:\n$Z_{n,N} = \\frac{\\theta_n - \\theta_N}{\\sqrt{VSRS(\\theta_n)}} = \\sqrt{N-1} \\rho_{R,\\theta}$"}, {"title": "B.3. Representation matters (Rolf et al, 2021)", "content": "Does collecting more data from underrepresented groups help? In Rolf et al, the key mecha-nism of change is the allocation of the proportion of groups in the dataset, which the model practitioner can either choose directly or learn for the optimal loss. This assumes the ability to sample directly from the group-specific distribution. The work also maps the approach to importance weighting (reweight training samples with respect to group distributions) and group distributional robust optimization (minimizes the maximum empirical risk over all groups).\nWe can formulate this as a function of the group proportions of the training data. For dataset $S = {x_i, y_i, g_i}_{i=1}^n$ where features $x_i$, label $y_i$, and discrete group $g_i$ for groups $G-{1,...,|G|}$ are measured. The population prevalence $\\gamma_g = P_{(X,Y,G)~D}[G = g]$ is related to the ability to empirical allocation of groups in the data $a_g = \\frac{1}{n}\\sum_{i=1}^n[g_i = g]$. Sampling from allocation a is defined as independently sampling of |G| disjoint datasets $S_g$ and concatenating according to $S(a, n) = U_{g \\in G} S_g$ with\n$S_g = {x_i, y_i, g}_{i=1}^{n_g}, (x_i, y_i) ~iid D_g$"}, {"title": "B.4. More data can be helpful for fairness sometimes (Chen et al, 2018)", "content": "The paper focuses on many different ways to improve fairness of a model, one of which may be adding more training data. As relevant to this group, error due to variance (as opposed to statistical bias and statistical noise) can be estimated via a distribution learning curve. An assumption of the model is therefore that any new data will be from the same distribution as the training data, on which the learning curve is estimated.\nUnfairness can be defined as $\\Gamma(\\hat{Y}, n) := |\\gamma(\\hat{Y}, n) - \\bar{\\gamma}(\\hat{Y}, n)|$ for predictions $\\hat{Y}$, sample size n, and group-specific unfairness $\\gamma$. Based on prior empirical studies, these type II learning curves can be approximated as asymptotic inverse power-law $\\gamma_a(\\hat{Y}, n_a) = \\alpha_a \\cdot n_a^{\\beta_a} + \\delta_a$.\nHowever, when subgroups are difficult to identify, the role of data is less known. Recent work by Izzo et al. (2023) presents data-driven strategies for finding subpopulations based"}, {"title": "B.5. Relationship to Domain Adaptation, Distribution Shift, and Transfer Learning", "content": "Domain Adaptation The field of domain adaption provides another lens to view the problem of overcoming data quality challenges. Domain Adaption is concerned with the problem of training models with source data and performing well on a target domain (Kouw and Loog, 2018; Zhuang et al., 2020). In Unsupervised Domain Adaptation, the majority of the work in the area, labeled source domain data, and unlabeled target domain data are available for training(Wilson and Cook, 2020). In Supervised Domain Adaptation, only a few or scarce samples from the target dataset and the source dataset are fully labeled(Motiian et al., 2017). In this setting, the target domain can be thought of as high-quality data, and the source domain can be thought of as low-quality data (Kouw and Loog, 2018).\nA related area of literature concerned with shifts in data quality is distributional ro-bustness and distribution shift. Worst subgroup performance (subpopulation shift) and overall performance (domain generalization) on an out-of-distribution dataset are metrics presented in the domain adaptation literature aimed at measuring distribution shift (Koh"}, {"title": "Appendix C. Additional eICU Experiment Results", "content": "We include additional experiment results on eICU data.\nAUC Change for additional models In the main text, we present the results of adding an additional hospital to the training data for logistic regression (Figure 4). In Figure 10, we show that both LSTM and LGBM models can induce decreases or increases in AUC when adding combinations of hospitals. In Figure 11, we plot the correlation between the Logistic Regression AUC and LGBM Classifier AUC for the same train-test pair of hospitals."}, {"title": "Appendix D. Additional Dataset Results & Analysis", "content": "Datasets For our investigation, we study three real-world tasks and datasets, chosen because of their rich feature sets and the open-access availability. Dataset details can be found in Table 4."}, {"title": "Single Source Datasets Benefit from Data Scaling Properties", "content": "We consider the ini-tial stage of the SEQUENTIAL case\u2014prior to sampling from any additional data sources\u2014to be equivalent to a single-source data setting. We find that increasing the dataset size in this single source setting yields improved performance (Figure 20a). Consistently, maxi-mum accuracy is achieved when the most data points in a single source are used. Single source data increases also improve worst-subgroup performance, as demonstrated in prior literature (Sagawa et al., 2019). For some models, increases in data slightly improve dis-"}, {"title": "D.1. Performance via the Lens of a Practical Divergence", "content": "In the main text, the divergence measure we used was based on a score function because ICU data was too high-dimensional to apply density estimation techniques. For the Folktables dataset, we present the following results of computing KL divergence through kernel density estimation direction."}, {"title": "D.2. Toy Experiments", "content": "Before running real-world experiments, we first tested our concept on synthetic data. We consider two source A and source B where $y_A(x) = sin(x)$ and $y_B(x) = -sin(x)$. D\u2081 is sized na = 10 comes from source A and D2 is sized nB = 90 comes from source B. The training set $D_{train} = D_1 \\cup D_2$ while the test set comes from just source A."}, {"title": "Appendix E. Proofs", "content": ""}, {"title": "Proof for Lemma 3.1", "content": ""}, {"title": "Lemma 2", "content": "Let $D_{train,n}$ be constructed in the SEQUENTIAL case from k sources: $D_{s_1},..., D_{s_k}$, then if $\\frac{\\delta(D_{S_k}, D_{test})}{\\frac{c n}{n_{s_k}}} \\geq \\delta(D_{train,n}, D_{test})$\n$\\delta(D_{train,n}, D_{test}) \\geq \\delta(D_{train,n-n_{s_k}}, D_{test})$\nwhere $\\delta$ belongs to the family of f-divergences and c is a divergence-dependent constant where $\\delta(D_{train,n}, D_{test}) + c = \\sum_{i=1}^m \\frac{n_i}{n} \\delta(D_{S_i}, D_{test})$.\nProof Without loss of generality, the last source in the composition of $D_{train,n}$ is partially used, we define the size of the last source as simply $n_k$ and forget that there is unused data in the last source. Thus we can simply the overall training distribution as $D_{train,n} = \\sum a_i D_{S_i}$ where $a_i = \\frac{n_i}{n}$. Furthermore, let n be the total number of examples with k sources and let $n' = n - n_k$ be the total number of examples with k \u2013 1 sources.\nBy convexity of $\\delta$ (Jenson's):\n$\\delta(D_{train,n'}, D_{test})$\n$\\leq \\sum \\frac{n_i}{n'} \\delta(D_{S_i}, D_{test})$\n$=\\sum\\frac{n_i n}{n' n} \\delta(D_{S_i}, D_{test})$\n$= \\frac{n}{n'} \\sum \\frac{n_i}{n} \\delta(D_{S_i}, D_{test})$\n$= \\frac{n}{n'} (\\sum\\frac{n_i}{n} \\delta(D_{S_i}, D_{test}) - \\frac{n_k}{n}\\delta(D_{S_k}, D_{test}))$\nSince $\\delta(D_{train,n}, D_{test}) \\leq \\sum \\frac{n_i}{n}\\delta(D_{S_i}, D_{test})$ then for some constant c:\n$= \\frac{n}{n'} (\\delta(D_{train,n}, D_{test}) + c - \\frac{n_k}{n}\\delta(D_{S_k}, D_{test}))$"}]}