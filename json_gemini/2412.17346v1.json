{"title": "FFA Sora, video generation as fundus fluorescein angiography simulator", "authors": ["Xinyuan Wu", "Lili Wang", "Ruoyu Chen", "Bowen Liu", "Weiyi Zhang", "Xi Yang", "Yifan Feng", "Mingguang He", "Danli Shi"], "abstract": "Fundus fluorescein angiography (FFA) is critical for diagnosing retinal vascular diseases, but\nbeginners often struggle with image interpretation. This study develops FFA Sora, a text-to-\nvideo model that converts FFA reports into dynamic videos via a Wavelet-Flow Variational\nAutoencoder (WF-VAE) and a diffusion transformer (DiT). Trained on an anonymized\ndataset, FFA Sora accurately simulates disease features from the input text, as confirmed by\nobjective metrics: Fr\u00e9chet Video Distance (FVD) = 329.78, Learned Perceptual Image Patch\nSimilarity (LPIPS) = 0.48, and Visual-question-answering Score (VQAScore) = 0.61.\nSpecific evaluations showed acceptable alignment between the generated videos and textual\nprompts, with BERTScore of 0.35. Additionally, the model demonstrated strong privacy-\npreserving performance in retrieval evaluations, achieving an average Recall@K of 0.073.\nHuman assessments indicated satisfactory visual quality, with an average score of\n1.570(scale: 1 = best, 5 = worst). This model addresses privacy concerns associated with\nsharing large-scale FFA data and enhances medical education.", "sections": [{"title": "Introduction", "content": "Fundus fluorescein angiography (FFA) enables the dynamic visualization of retinal blood\nflow and lesional changes through the intravenous injection of fluorescein sodium. This\ntechnique is critical for assessing blood-retina barrier function and diagnosing various retinal\nvascular diseases, including diabetic retinopathy (DR) and choroidal neovascularization\n(CNV). 1-5 Despite its importance, accurately interpreting FFA images demands expertise. For\njunior ophthalmologists and medical students, the challenge of correlating specific diagnoses\nwith corresponding FFA findings is compounded by the complexity of disease presentations\nand the limited availability of representative FFA cases.\nThe growing application of artificial intelligence (AI) offers a potential solution for automatic\nimage interpretation. However, developing and validating these tools requires extensive, high-\nquality labeled datasets, which can be difficult to obtain. Strict regulations governing the use\nand exchange of personal health information protect patient privacy but can inadvertently\nhinder the collaborative data sharing necessary to build effective AI models. Additionally,\nhealthcare providers often work in isolation, making them reluctant to share the clinical data\nthat would improve AI systems. Therefore, innovative strategies are needed to overcome the\nchallenges.\nOpen Al's Sora has made remarkable progress in generating realistic videos, paving the way\nfor text-to-video generation as a promising solution for synthesizing well-labeled, diversified,\nand privacy-preserving medical data. Recently, diffusion models (DMs) have gained attention"}, {"title": null, "content": "for their capacity to generate high-quality, temporally consistent images and videos through\niterative denoising processes. These models have been applied to generate medical data,\nincluding chest X-ray images10, brain MRI scans,\u00b9\u00b9 multi-modal retinal images,12 endoscopy\nvideos, 13 and facilitate neurosurgery.14 These technological advancements have expanded\napplications across multiple domains. Their potential uses include medical education, clinical\ntraining, surgical simulation, medical knowledge dissemination, doctor-patient\ncommunication, and biomedical data augmentation. However, generating videos with textual\ndescription has yet to be explored for creating lesion-preserving dynamic FFA videos in\nophthalmology.\nTo bridge the gap, we developed FFA Sora, an AI-driven model capable of producing dynamic\nFFA videos from textual reports. This cross-modality generative model can assist in medical\neducation and offer valuable support for training diagnostic tools and other AI-based models in\nthe future."}, {"title": "Methods", "content": "We utilized de-identified existing data for our study, which received approval from the\nInstitutional Review Board of the Hong Kong Polytechnic University."}, {"title": "Dataset", "content": "This study uses a subset of a retrospective FFA dataset.15 FFA data were obtained from a tertiary\nhospital in China. The dataset included both the medical records and FFA examinations of"}, {"title": null, "content": "patients assessed between November 2016 and December 2019. All FFA videos were captured\nusing Zeiss FF450 Plus and Heidelberg Spectralis systems (Heidelberg, Germany) at a\nresolution of 768\u00d7768 pixels. The collection encompassed a wide variety of ocular conditions,\nincluding diabetic retinopathy (DR), retinal vein occlusion (RVO), and central serous\nchorioretinopathy (CSC). To ensure high-quality data, we implemented a filtering process based\non vessel area ratios within the FFA frames, excluding any video whose frame vessel area ratios\nfell below 0.005.\nTo address the issue of inconsistent and insufficient frame counts in the original FFA videos,\nwe standardized all videos to a fixed frame count of 21 frames. For videos with more than 21\nframes, we selected 21 frames in reverse chronological order. For videos with fewer than 21\nframes, additional frames were interpolated to reach the required count. This was achieved\nusing a frame interpolation technique based on linear interpolation, implemented via the\nOpenCV library.\nThe process involved extracting all frames from the input video and computing the interpolation\npositions based on the target frame count. For each interpolated frame, a weighted linear\ninterpolation between two adjacent frames was employed to generate smooth transition frames.\nSpecifically, let an interpolated frame lie between the i -th frame and the (i+1)-th frame of the\noriginal video. The pixel values of the interpolated frame are computed as follows:\n$F_{interpolated} = (1 \u2212 t) \u00d7 F_i \u2212 t \u00d7 F_{i+1}$\nwhere:"}, {"title": null, "content": "$F_i$ and $F_{i+1}$ denote the pixel values of the i -th and the (i+1)-th frames, respectively,\nt is the interpolation ratio, which takes values in the range [0,1] and determines the relative\ncontribution of the two adjacent frames to the interpolated frame.\nThe interpolation ratio t is calculated based on the relative temporal position of the\ninterpolated frame within the sequence. Once all interpolated frames were generated, they\nwere combined with the original frames to produce a standardized video consisting of exactly\n21 frames. The result dataset was divided into 80% for training, 10% for validation and 10%\nfor testing."}, {"title": "Model Architecture", "content": "The framework of FFA Sora was based on the Open-Sora Plan repository (v1.3.0), 16 which we\nspecifically adapted to address the task of generating FFA images from medical textual\ndescriptions. It consisted of two primary components: the Wavelet-Flow Variational\nAutoencoder (WF-VAE) and the Diffusion Transformer (DiT).\nThe WF-VAE architecture\u00b9\u00b9, derived from the Stable-Diffusion Image VAE, was fine-tuned and\noptimized for the specialized task of text-to-FFA image generation in the medical domain. A\nkey architectural enhancement involved the transformation of Conv2D layers into\nCausalConv3D layers, enabling the model to process temporal video data, which was essential\nfor the sequential nature of FFA imaging. Furthermore, multi-level wavelet transforms were\nemployed in the encoder to decompose video signals into frequency sub-bands, allowing low-"}, {"title": null, "content": "frequency components, which is critical for FFA imaging details, to bypass the backbone\nnetwork via the Main Energy Flow Pathway, thereby reducing computational redundancy.\nThe second major component was the DiT architecture, 18 which employed a Transformer-based\narchitecture with cross-modal attention mechanisms to adapt to the requirements of text-to-FFA\nimage generation. The denoising process was performed in a low-dimensional latent space\ncompressed by the VAE, where each Cross-DiT Block, consisting of self-attention, cross-\nattention, and feed-forward networks, was enhanced with gating mechanisms to refine feature\nextraction and fusion. This design improved the model's ability to capture both high-level\nsemantics and fine-grained details, ensuring that the generated FFA images accurately reflected\nclinically relevant features such as global vascular structures and local pathological regions."}, {"title": "Model Inference", "content": "During the inference phase, the model processed input textual descriptions and generated\ncorresponding FFA images with a resolution of 21\u00d7512\u00d7512. The input text typically contained\ndetailed medical descriptions of retinal conditions, such as microaneurysms, non-perfusion area,\nleakage, and other abnormalities, which were crucial for FFA-based diagnosis. To ensure that\nthe generated images align closely with the described retinal conditions, our model utilized the\npre-trained text encoder to capture the semantic nuances of the input effectively. These latent\nfeatures, conditioned on the text, were passed through the CausalConv3D layers in the decoder,\nwhich reconstructed the FFA images by generating temporal and spatial patterns consistent with\nreal FFA images. The inference process employed a tiled convolution approach, which"}, {"title": null, "content": "optimized memory usage and ensured high-quality image output. By applying this technique,\nthe model generated coherent and clinically relevant FFA images that accurately depict the\nspecified retinal conditions, while maintaining computational efficiency during inference."}, {"title": "Objective Evaluation", "content": "Several objective metrics were employed to assess the quality of the generated FFA videos,\nincluding general video evaluation metrics including Fr\u00e9chet Video Distance (FVD),\u00b9\u00ba Learned\nPerceptual Image Patch Similarity (LPIPS), 20 and Visual-question-answering Score\n(VQAScore).21 FVD discerns the likeness in feature distribution between authentic and\nsynthetic videos, encapsulating the holistic excellence and unity. LPIPS, utilizing deep\nlearning-inspired features, measures the perceptual resemblance by comparing image segments,\nthus revealing nuances beyond the capabilities of conventional pixel-based assessments.\nVQAScore evaluates the alignment of text-to-video models. Collectively, these evaluation\ncriteria provide a comprehensive analysis of the generated FFA video quality. Together, these\nmetrics provide a comprehensive evaluation of the generated FFA video quality.\nFor FFA domain-specific evaluation, we used FFA-GPT to translate the generated FFA video\ninto text report and compared it with the original text prompt using Bidirectional Encoder\nRepresentations from Transformers Score (BERTScore)22 as similarity measure. BERTScore\nutilizes the pre-trained BERT language model to measure conceptual overlap between\ngenerated and reference content. Unlike purely lexical metrics, it captures nuanced alignments"}, {"title": null, "content": "within broader contexts, providing a deeper assessment of textual congruence.\nFor privacy preservation evaluation, we aimed to test FFA Sora's ability to generate content-\npreserving yet deidentifiable data. To achieve this, we employed an image-to-image retrieval\nmethod, using Recall@K23 to measure the proportion of matched visuals retrieved among the\ntop K results. This approach assessed whether videos generated by FFA Sora could be matched\nwith the original patient, ensuring both utility and privacy in the generated content."}, {"title": "Human Assessment", "content": "This evaluation followed the methodology described previously.23,24 Three ophthalmologists\n(X.W., X.Y. and Y.F.) reviewed 50 randomly selected FFA videos generated from the test set,\ncomparing them against the corresponding ground-truth FFA videos. The assessment focused\non retinal and vascular structures, lesion integrity, overall coherence, and dynamic range. Each\nvideo was rated on a scale from 1 to 5, where 1 represented excellent quality and 5 indicated\nvery poor quality. The criteria were defined as follows: 1 = The modality and lesion\ncharacteristics of the generated videos exactly match the text prompts; 2 = The modality of the\ngenerated videos corresponds to the text prompts, and the lesion characteristics are basically\nconsistent with the text prompts; 3 = The modality of the generated videos corresponds to the\ntext prompt, and the lesion characteristics are slightly consistent with the text prompts; 4 = The\nmodality of the generated videos corresponds to the text prompt, but the lesion features cannot\nbe generated; 5 = Unable to generate all text-oriented features."}, {"title": "Results", "content": "The final dataset used in the study consists of 3625 FFA videos paired with 1814 reports, among\nwhich 2851 videos with 1429 reports are randomly selected for training, 387 videos with 192\nreports, and 387 videos with 193 reports for testing. FFA reports in the dataset include\ndescription of various lesions, such as microaneurysms, leakage, neovascularization, capillary\nnon-perfusion, and macular edema"}, {"title": null, "content": "demonstrates representative examples of generated FFA videos, including\ncorresponding prompts, ground-truth videos, and the generated video frames. FFA Sora can\ngenerate detailed FFA videos from text prompts, illustrating retinal lesions and vascular\nabnormalities, such as leakage, neovascularization, and microaneurysms, etc. Our model can\nproduce FFA videos for other retinal and choroidal diseases as well, such as uveitis, retinitis\npigmentosa (RP), etc. Representative frames of these videos are shown in Supplementary\nFigure 1.\nObjective evaluation showed excellent quality of generated FFA videos\nThe generated videos are comprised of 21 frames that precisely document the entire process of\nFFA examination, with a particular emphasis on the venous and late phases. The model we\nemployed has exhibited satisfactory performance in producing FFA videos when applied to the\ntest datase, as evidenced by Fr\u00e9chet Video Distance (FVD) = 329.78, Learned Perceptual Image\nPatch Similarity (LPIPS) = 0.48, and Visual-question-answering Score (VQAScore) = 0.61\n."}, {"title": null, "content": "the generated FFA videos.\nIn addition to the evaluation with above standard generative metrics, we also proposed domain-\nspecific evaluation strategies. Based on previous development of FFA-GPT,15 a two-stage\nsystem which can generate FFA report automatically, our model was evaluated to determine\nwhether the content of the generated videos aligns with the actual characteristics of retinal\ndiseases as denoted by specified textual prompts, with Bidirectional Encoder Representations\nfrom Transformers Score (BERTScore) = 0.35 . To provide clarity and visualize\nthese findings, an example of reports by FFA-GPT based on FFA Sora's generated FFA videos\nare presented in"}, {"title": "Image retrieval demonstrated FFA Sora's excellent performance on privacy preserving", "content": "To further explore FFA Sora's robust feature representation, we investigated its performance in\nimage retrieval tasks, specifically examining the relationship between the generated videos and\ntheir ground-truth counterparts. The results of this investigation are summarized in ,\nwhere we report Recall@K scores that include 0.02, 0.04, 0.16 for K = 5, 10, and 50,\nrespectively. Furthermore, the mean recall was calculated to be 0.073, reflecting significant\ndisparities among generated and ground-truth videos. These relatively low Recall@K values\nindicate that FFA Sora effectively prevents the leakage of confidential image information\nduring video generation."}, {"title": "Human assessment confirmed high visual quality of generated FFA videos", "content": "A comprehensive analysis was conducted on fifty randomly selected videos produced via our\nmodel, which were subjected to meticulous review by three experienced ophthalmologists. This\nevaluation employed a five-point scale, where a score of 1 denoted excellent quality while a\nscore of 5 indicated very poor quality. The generated videos were systematically compared to\nthe corresponding textual reports and the ground-truth FFA videos, providing a framework for\na rigorous assessment of their accuracy and fidelity. Through this evaluative process, the\nsubjective visual quality score averaged at 1.570."}, {"title": "Discussion", "content": "In this study, we developed FFA Sora, a text-to-video diffusion transformer designed to generate\nFFA videos from textual descriptions. The evaluation results demonstrated the model's\nrobustness in generating diverse dynamic abnormalities while retaining localized lesion\ninformation. This research showcases the potential of text-to-video models for visualizing FFA\nreports, offering a novel solution for data sharing and enhancing privacy in machine learning\nmodel training and medical AI applications.\nGenerative models represent important advancements in medical data generation of their ability\nto produce images of high quality, stability, and diversity.25-27 Previous research has explored\napplications of generative adversarial networks (GANs) in retinal image quality enhancement,\ndomain adaptation, retinal vessel segmentation, and cross-modality generation of fundus"}, {"title": null, "content": "autofluorescence, FFA, and indocyanine green angioagraphy. 28-34 More recently, diffusion\nmodels have been applied to ocular imaging tasks, such as ocular surface structure segmentation\nfor meibomian gland dysfunction grading\u00b35 and data augmentation using retinography images\nto support deep learning model training.36 Even though diffusion models beat GANs in image\ngeneration, they face limitations of high computational costs, slow sampling processes, and\nsignificant resource demands. DiT is, therefore, introduced to improve the performance of\ndiffusion models by replacing the commonly used U-Net backbone with a transformer.37-39\nBuilding on this foundation, FFA Sora is an advanced application of generative models with a\nbetter scalability. This model is the first to apply DiT for creating continuous FFA video content.\nComprehensive evaluations, including both objective metrics and expert assessments, confirm\nthat the produced videos preserve diagnostic integrity. This validation underscores the model's\npotential value within the clinical workflows, contributing to more effective patient\nmanagement in ocular healthcare.\nWhile diverse datasets are essential for enhancing the accuracy and robustness of AI model\ntraining, data sharing is often hindered by rigorous privacy regulations and concerns regarding\npatient confidentiality, particularly with sensitive medical imaging and clinical videos.40,41\nThese challenges limit collaboration and the creation of large, diverse datasets needed for\ntraining and validating AI models. Additionally, there is a risk that AI models may inadvertently\ngenerate videos by recalling data from their training set, potentially compromising privacy. 42\nTo address this issue, we conducted an image retrieval evaluation, which revealed notably low\nRecall@K scores, demonstrating the model's robust privacy-preserving capabilities. By"}, {"title": null, "content": "mitigating these privacy concerns, FFA Sora facilitates cross-center data sharing and\nreproducibility, paving the way for improved collaborative research, clinical training, and the\ndevelopment of AI-driven diagnostic tools.\nAI-generated content in the medical domain has a broad implication. Research on AI-assisted\nmedical image generation has demonstrated its value in both medical education and the\ntraining of convolutional neural networks (CNNs).10,43 For instance, Tabuchi H, et al.6\nintroduced a text-to-image model to produce ultra-widefield (UWF) retinal color fundus images,\nsignificantly improving medical students' diagnostic accuracy. This outcome highlights the\npotential of integrating cutting-edge AI technologies into contemporary medical education.\nCompared to their model, FFA Sora offers a more dynamic demonstration of information,\nthereby facilitating a more comprehensive learning experience for retinal anatomy, vascular\nperfusion, and pathology for ophthalmologists-in-training. Additionally, FFA Sora's\naccessibility ensures its broad applicability across educational institutions and healthcare\nfacilities, enabling future ophthalmologists to effectively utilize cutting-edge technologies for\ndiagnosing and managing retinal diseases. As the intersection of AI and medical education\ncontinues to evolve, innovations like FFA Sora are poised to play a pivotal role in enhancing\nthe competency of healthcare providers in ophthalmology.\nIn conclusion, this study presents FFA Sora, a novel modelthat transforms FFA report text into\ndynamic video representations. These synthetic videos retain critical clinical details while\npreserving patient privacy, offering a promising solution to persistent data-sharing challenges"}, {"title": null, "content": "in healthcare. By enabling secure and efficient visualization, FFA Sora can support\ncollaborative research, enhance clinical training, and accelerate the development of AI-driven\ndiagnostic and medical systems."}, {"title": "Limitations of the study", "content": "This research has several limitations. First, although FFA Sora demonstrates high accuracy and\nreliability based on multiple evaluation metrics, its real-world clinical utility remains to be\nconfirmed. Broader external validation using more diverse datasets would help establish its\npractical value. Second,. although our model demonstrated excellent performance on several\nmetrics such as FVD, our evaluation approach may not fully capture the broader generative\nquality, thereby requiring more extensive and diversified validation in future work. Finally,\ncomprehensive ophthalmic diagnoses often require multiple imaging modalities, while our\ncurrent approach is limited to visualizing FFA reports. Integrating multimodal medical imaging\ncould significantly improve Al-based models in ophthalmology, offering more complete\ndiagnostic and treatment support."}, {"title": "Code availability", "content": "Code is available at https://github.com/PKU-YuanGroup/Open-Sora-Plan/tree/v1.3.0."}]}