{"title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation", "authors": ["Sankalp Sinha", "Mohammad Sadil Khan", "Muhammad Usama", "Shino Sam", "Didier Stricker", "Sk Aziz Ali", "Muhammad Zeshan Afzal"], "abstract": "Generating high-fidelity 3D content from text prompts remains a significant challenge in computer vision due to the limited size, diversity, and annotation depth of the existing datasets. To address this, we introduce MARVEL-40M+, an extensive dataset with 40 million text annotations for over 8.9 million 3D assets aggregated from seven major 3D datasets. Our contribution is a novel multi-stage annotation pipeline that integrates open-source pretrained multi-view VLMs and LLMs to automatically produce multi-level descriptions, ranging from detailed (150-200 words) to concise semantic tags (10-20 words). This structure supports both fine-grained 3D reconstruction and rapid prototyping. Furthermore, we incorporate human metadata from source datasets into our annotation pipeline to add domain-specific information in our annotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D, a two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our annotations and use a pretrained image-to-3D network to generate 3D textured meshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly outperforms existing datasets in annotation quality and linguistic diversity, achieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators.", "sections": [{"title": "1. Introduction", "content": "Text-to-3D (TT3D) content generation has emerged as a pivotal area in computer graphics, vision, and AI, enabling the creation of complex 3D objects from textual prompts [32, 38, 62] by understanding the shape, material properties [71, 89], and complex visual elaborations [36, 77, 90]. This technology holds significant potential for various industries, including gaming, augmented reality (AR), virtual reality (VR), and film production [32, 38]. Recent advancements in text-to-image (TTI) synthesis [3, 21, 67, 69] have achieved remarkable realism and precise control over visual effects [19, 65, 67]. However, extending these capabilities to high-fidelity TT3D generation remains a significant challenge [22, 32, 38, 90]. This is due to the intricate nature of modeling 3D shapes [35, 36, 44, 76, 89], textures [43, 44, 77], colors [71, 89] and spatial relationships [22, 90] from text descriptions, a challenge further amplified by the scarcity of high-quality 3D captions.\nCurrent TT3D datasets like CAP3D [53], 3D-Topia [28], CLAY [89] and Kabra et al [34] attempt to bridge this gap through automated annotations but often fall short due to their reliance on single-view VLMs [11, 39, 45, 46] or GPT-4 [60] for caption generation. This approach frequently results in contradictory or inconsistent captions [34, 54]. Moreover, the captions lack the necessary details for fine-grained 3D reconstruction. Additionally, their dependence on proprietary models like GPT-4 [60] introduces significant scalability and cost constraints. Manual annotation is also impractical for large-scale datasets like Objaverse [18] and Objaverse-XL [17]. These datasets contain a diverse range of 3D models from characters and biological elements to historical artifacts and complex ambiguous structures-requiring domain-specific expertise for accurate annotation (See Figure 1 - Left). Beyond being time-consuming and expensive, CAP3D [53] has shown that human-generated captions may not necessarily surpass automated methods in quality.\nTo address the previously mentioned challenges, we introduce MARVEL(Multi-Level Visual ELAboRation), an automated and scalable 3D captioning pipeline. Our approach combines state-of-the-art multi-view VLM InternVL2 [13, 15] and Qwen 2.5 LLM [85] to generate high-quality captions for over 8.9 million 3D models across seven datasets [10, 16-18, 20, 73, 74, 80]. To ensure domain specific information into our captions and reduce VLM hallucinations, we integrate human metadata from source datasets into our pipeline. Following [12, 92], we identify five key aspects for fine-grained 3D reconstruction: object names and components, shape and geometry, texture and materials, colors, and contextual environments. Our pipeline progressively compresses these aspects to generate five levels of annotations, ranging from comprehensive descriptions (~200 words) for fine-grained 3D reconstruction to concise tags (~10 words) for quick modeling, resulting in 40+ million annotations. Our pipeline addresses three fundamental challenges in 3D captioning - detail, accuracy, and scalability. Through comprehensive experimental analysis, we show that MARVEL-40M+ has superior annotation quality, information density, and linguistic diversity compared to other methods [28, 34, 53].\nTo showcase the application of our dataset, we introduce MARVEL-FX3D (Fast execution for 3D), a two-stage pipeline designed for high-fidelity TT3D generation. In the first stage, we fine-tune Stable Diffusion (SD) 3.5 [3] with our annotations to improve its capability to produce images for suitable 3D reconstruction. In the second stage, we leverage the pre-trained Stable Fast 3D (SF3D) [7] for rapid image-to-3D conversion. This enables the creation of textured meshes from texts within 15s. Our approach is inspired by multi-stage TT3D pipelines [41, 58, 71], a promising direction [32, 38, 41] that addresses the limitation of existing Score Distillation Sampling (SDS)[62]-based methods like janus problem [43, 44, 62, 77], oversaturation [43], and lengthy per-prompt optimization [43, 44, 62, 76, 77, 91]. Our experiments demonstrate that MARVEL-FX3D outperforms current state-of-the-art TT3D methods in terms of prompt fidelity and overall preference.\nOur contributions can be summarized as follows:\n1. We present MARVEL, an automated, scalable annotation pipeline for generating high-quality 3D captions. To the best of our knowledge, MARVEL-40M+ is the largest 3D caption dataset to date.\n2. We propose a multi-level annotation structure that spans from detailed descriptions for fine-grained 3D reconstruction to concise tags for quick modeling.\n3. We incorporate human metadata from source datasets into our pipeline to inject domain-specific information in the text descriptions and reduce VLM hallucinations.\n4. As a downstream task, we introduce MARVEL-FX3D, a two-stage framework for high-fidelity TT3D generation.\n5. Thorough experiments demonstrate that MARVEL-40M+ achieves state-of-the-art performances in linguistic diversity, image-text alignment, caption accuracy, and high-fidelity TT3D generation."}, {"title": "2. Related Work", "content": "3D-Text Data: 3D datasets such as ShapeNet [10], Objaverse [17, 18], and Omniobject3D [80] have played a crucial role in advancing 3D understanding tasks such as single [29, 48, 49, 84] or multi-view [70, 86] 3D reconstruction, multi-view consistent image generation [27, 50, 87], and 3D object synthesis [31, 89]. However, they often lack meaningful language descriptions, with available metadata being either noisy or inadequate [47, 53]. This language-3D gap has been a major bottleneck in developing high-fidelity TT3D models [28, 34, 53]. Recent works like CAP3D [53] addresses this by proposing an automated pipeline. It starts with BLIP [40] for single-view captioning of 3D assets followed by refinement using CLIP [64] and caption aggregation by GPT-4 [60]. Subsequent works, Kabra et al. [34] introduced ScoreAgg and PaLI-X[11] to improve caption accuracy, while 3D-Topia [28] explored an alternative path with LLaVA [45, 46] and GPT-3.5. CLAY [89] took a more direct approach, leveraging GPT-4 [60] for multi-view caption generation. Yet, all these approaches face inherent trade-offs. Single-view VLM approaches [28, 34, 53, 54] often produce incomplete or inaccurate annotations [34, 54] for 3D models, while GPT-4-based methods [28, 53, 60, 89] struggle with scalability and cost[4]. Our work presents a novel solution to these challenges through three key innovations. First, we leverage open-source multi-view VLM InternVL2 [13, 15] and Qwen 2.5 LLM [85], achieving GPT-4 [60] comparable performance [2, 13, 15, 85] without its scalability and cost constraints. Second, instead of discarding human metadata from source datasets as done in previous works [28, 34, 53, 89], we recognize its value as domain-specific prior knowledge. We incorporate filtered versions of this metadata into our pipeline to inject relevant context and reduce VLM hallucinations. Finally, we introduce a hierarchical annotation framework with five distinct levels, ranging from detailed descriptions to abstract tags. This multi-level approach represents a significant departure from existing methods [28, 34, 53, 89], which typically provide only single-level annotations.\nText-to-3D: Current TT3D methodologies can be broadly categorized into two main approaches. One prominent direction is based on the seminal work of DreamFusion [62], which introduced Score Distillation Sampling (SDS) to learn a NeRF [57] representation by leveraging information from pretrained TTI models [5, 67, 69]. Subsequent studies have advanced this framework by improving training stability [43, 77], increasing output diversity [43, 77, 91] and geometry extraction [14, 44, 76, 88]. However, SDS-based methods face two key challenges: geometric inconsistencies known as the Janus problem [32, 37, 38] and slow optimization times. This issue has been partially addressed using amortization efforts [51, 82]. The second group of methods consists of multistage pipelines [24, 33, 41, 42, 58, 71]. The goal is to generate single or multi-view images from a TTI model [3, 5, 21, 67, 69], followed by view reconstruction into various 3D representations [7, 24, 29, 41, 57, 70]. These methods often fine-tune the TTI [3, 5, 21, 67, 69] models on TT3D datasets [34, 53] to align the output image with reconstruction needs. Point-E [58] fine-tunes GLIDE [59] for TTI synthesis and uses a point diffusion transformer for 3D point cloud generation. Instant3D [41] fine-tunes SD [67] to produce a 2 \u00d7 2 grid of multi-view images and uses LRM [29] for 3D Gaussian reconstruction. AssetGen [71] extends LRM towards high-quality 3D meshes with detailed textures and PBR materials. Our dataset, MARVEL-40M+, is uniquely positioned to advance this domain by providing comprehensive, high-quality, and domain-specific text annotations that bridge the gap between TTI generation and image-to-3D reconstruction. By fine-tuning on MARVEL-40M+, we develop MARVEL-FX3D, which demonstrates better performance for high-fidelity TT3D generation compared to existing state-of-the-art methods."}, {"title": "3. Methodology", "content": "3.1. Multi-Stage Annotation Pipeline\nWe now present our proposed MARVEL annotation pipeline, shown in Figure 2 (left). Our goal is to generate detailed and domain-specific captions, for both fine-grained and abstract 3D modeling cases. Through a carefully designed five-stage process, MARVEL produces a hierarchy of information-rich and domain-specific annotations. These annotations range from detailed descriptions of object names, shapes, textures, and contextual relationships to concise summaries. Starting with multi-view rendering, our pipeline processes each asset through sequential stages of human metadata refinement, dense description generation via InternVL2 [15], multi-level elaboration using Qwen 2.5 [85], and ethical filtering. Below, we detail each component of our pipeline.\nMulti-View Rendering: We first generate 4 multi-view images of resolution 512 \u00d7 512 for each 3D model using Blender [1]. We rotate the camera around the object with azimuth angle, $0 = {\\frac{\\pi}{4}, \\frac{3\\pi}{4}, \\frac{5\\pi}{4}, \\frac{7\\pi}{4}}_{i=1}$ and fixed elevation angle, $ = 60. The camera distance is set to 1.5. The four images correspond to the front, back, left, and right sides of the 3D model. Unlike existing 3D captioning pipelines [28, 53], we focus solely on these standard viewpoints. This method aligns with recent studies [68, 79], which demonstrate that VLMs perform better on images from these viewpoints.\nHuman Metadata Filtering: High-quality 3D annotation requires capturing both visual characteristics (e.g. shape, color, texture) and semantic properties (e.g. domain-specific nomenclature and object identification). This dual focus ensures that descriptions are not only visually precise but also contextually relevant within specific domains. A significant challenge in this process is the tendency of pre-trained VLMs [40] to hallucinate when dealing with complex datasets, such as Objaverse [17, 18], due to the inherent 2D-3D domain gap [34]. To address this, we use the user-generated metadata from source datasets, which provides valuable domain-specific names and descriptions that can guide VLMs [13, 15] toward generating more precise and informative annotations. However, this metadata often contains noise, including personalized or sensitive information [17, 18], which can compromise annotation quality. To mitigate this, we use Mistral-Nemo-Instruct-2407 [72] to filter the metadata, removing random, redundant, and sensitive content to ensure that only information relevant to 3D attributes is passed to the annotation pipeline. It is worth noting that our pipeline functions independently of human metadata, with it serving purely as an optional enhancement to add domain-specific terminology in the captions.\nDense Description Generation: In this stage, InternVL2 [13, 15] processes the 4 rendered multi-view images along with our metadata-augmented prompt to generate a dense description of the 3D models. This description contains several key requirements for fine-grained 3D model reconstruction: (1) structural decomposition with object identification and relative positions, (2) geometric properties, analyzing shape characteristics, symmetry axes, and proportional relationships, (3) surface characteristics, addressing texture and material properties and tactile qualities such as roughness and reflectivity; (4) chromatic analysis, mapping colors across primary objects and sub-components, including patterns and transitions (5) environmental context, capturing spatial relationships and its interaction with other elements.\nTo efficiently scale this process for large-scale annotation, we select InternVL2-40B [13, 15] for its balance of speed, accuracy, and prompt adherence. Recent studies show that InternVL2-40B [13, 15] performs comparably [2, 13, 15] to GPT-40 [60] with significantly lower annotation cost*.\nMulti-Level Visual Elaboration: This stage focuses on generating multi-level visual elaborations using Qwen2.5-72B [85] by compressing different aspects of 3D reconstructions at varying levels of detail. This hierarchical approach allows for flexible and adaptive 3D modeling outputs optimized for different use cases, such as scenarios where only key details-like object name and colors are specified, but texture is excluded or where simplified semantic tags is necessary for rapid prototyping. While a direct prompting method will be to specify which aspects"}, {"title": "3.2. Caption Generation", "content": "Datasets: We aggregate 8.9M 3D assets from seven diverse sources [10, 16-18, 20, 73, 74, 80]. For human metadata injection, we use the name, tags and description from Objaverse 1.0 [18] (Sketchfab) and metadata from Objaverse-XL [18] (thingiverse and github). Samples from Objaverse-XL [17] containing the file extension .ply were excluded from the dataset. For the rest of the six datasets [10, 16, 20, 73, 74, 80], we use the class categories as metadata. Any samples that do not have renderable multi-views or lack textual information post-annotation are removed from the dataset. The final details of the dataset are provided in Table 1, with additional preparation information available in the supplementary materials.\nImplementation Details: Our MARVEL annotation pipeline is optimized for large-scale processing, achieving a throughput of ~24,000 samples per day. For human metadata filtering, we run the Mistral-Nemo-Instruct-2407 [72] on a single NVIDIA RTX 4090 GPU. Both InternVL2-40B [13, 15] for dense description generation and the Qwen2.5-72B [85] with 8-bit quantization for multi-level visual elaboration, runs on a single NVIDIA H100 GPU. For the final ethical filtering stage, we run Qwen 2.5-14B [85] on a single NVIDIA RTX A6000 GPU. For complete details including hyperparameter details, please refer to the supplementary material."}, {"title": "3.3. MARVEL-FX3D Architecture", "content": "In this section, we present MARVEL-FX3D, a two-stage pipeline that demonstrates the practicality of the MARVEL-40M+ dataset for TT3D synthesis. By leveraging our dataset's comprehensive text descriptions and diverse 3D assets [18], MARVEL-FX3D generates high-quality textured 3D meshes from text descriptions that can specify multiple objects, scenes, geometric properties, colors, and textures. The pipeline consists of (1) TTI generation using fine-tuned Stable Diffusion [21], followed by (2) single-view 3D reconstruction with a pretrained view reconstruction model [7]. This entire process generates high quality 3D assets in 15s, as illustrated in Figure 2 (right).\nFine-Tuning TTI Model: The objective of this stage is to generate high-quality, diverse images from text prompts that can be effectively converted into 3D textured meshes using pretrained image-to-3D methods [7, 83]. A primary challenge in multi-stage TT3D pipelines [41, 42, 55, 58, 71] is the inherent 2D-3D domain gap, where reconstructing accurate and geometrically consistent 3D models from 2D images is hindered by the ambiguity between background and foreground information [42, 55]. To address this, some methods have fine-tuned TTI [5, 67] models on TT3D datasets [28, 34, 53, 89]. Following this approach, we fine-tune Stable Diffusion 3.5 [3, 21] using the LORA [30] strategy to bridge this domain gap and generate images similar to the training distribution of the image-to-3D methods [7].\nImage-to-3D Generation: In the second stage, the background is removed from the generated image using DIS [63]. The refined image is then processed by pretrained SF3D [7] to generate a high-quality textured mesh within 5s. Further details can be found in the supplementary."}, {"title": "4. Experiment", "content": "The experiment section is divided into two parts. In Sec.4.1, we evaluate the quality of our annotations in comparison to the baseline datasets [28, 34, 53]. While Sec. 4.2 presents the performance evaluation of MARVEL-FX3D against current state-of-the-art methods [33, 43, 62, 91]. Both experiments are conducted on Objaverse [18] dataset."}, {"title": "4.1. Annotation Evaluation", "content": "Experimental Setup and Metrics: We assess annotation quality through (1) Linguistic Assessment, (2) Image-Text Alignment, and (3) Caption Accuracy.\nThe linguistic assessment evaluates annotation richness and diversity using the Measure of Textual Lexical Diversity (MTLD) [56] and N-gram analysis [8]. The MTLD metric calculates the average segment length at which the type-token ratio (TTR) drops below a threshold (typically 0.72), with higher MTLD scores indicating more diverse annotations. We randomly select 50K annotations for analysis.\nImage-text alignment is measured using both GPT-4 [60] and human evaluators who review four multi-view images of each 3D model and select the best-matching caption. 5,000 samples are evaluated using GPT-4 and 1,000 samples by five human reviewers with each assigned 200 samples. Level 4 annotations from MARVEL-40M+ are used for fair comparison due to their similar average length to baseline datasets [28, 34, 53] as shown in Table 2.\nCaption accuracy is separately assessed, where GPT-4 and human reviewers evaluate whether all the 3D attributes mentioned in the captions accurately correspond to the 3D models using four multi-view images. For MARVEL-40M+, Level 1 annotations are used, which are detailed and form the foundation for subsequent levels. GPT-4 evaluates 1,000 samples, while human reviewers assess 250 samples due to the evaluation's time demands. More details are provided in the supplementary section."}, {"title": "4.2. Text-to-3D Generation", "content": "Implementation Details: We fine-tune SD 3.5 [3, 21] using the Objaverse [18] dataset, which includes 798, 759 3D assets, split into training, validation, and test sets in a 90:5:5 ratio. Fine-tuning is conducted in half-precision for 5 epochs with a batch size of 8, using a single NVIDIA H100 GPU, with LoRA [30] rank and alpha set to 4. Further details are provided in the supplementary section.\nBaselines: To evaluate MARVEL-FX3D's performance in high-fidelity TT3D generation, we compare it with Shap-E [33], Dreamfusion [62], Luciddreamer [43], and HIFA [91]. We use the official implementations and pretrained models for Shap-E and Luciddreamer, training the latter for 3k steps. Dreamfusion and HIFA are trained using the open-source threestudio [23] implementation, with 10k and 24k steps, respectively, under default settings. Due to the slower optimization of Dreamfusion, HIFA, and Luciddreamer, we limit comparisons to 50 randomly selected samples from the Objaverse [18] test set. Instant3D [41] and Assetgen [71] are excluded due to unavailable code.\nUser Study: We conducted a human evaluation to assess the geometric consistency, visual quality, prompt fidelity, and overall preference of reconstructed 3D assets. Geometric consistency measures realism and physical plausibility, identifying issues like the janus problem. Prompt fidelity evaluates alignment with input text, while visual quality considers aesthetic elements such as colors and textures. Five users were presented with the text prompt and videos of the rendered 3D assets generated by the baseline methods and MARVEL-FX3D. The users scored each asset separately from 1 to 10 based on these criteria, and the final scores were averaged across all users."}, {"title": "4.3. Ablation Study", "content": "A. Effect of Human Metadata on Annotation Quality: Human-generated metadata is vital in the MARVEL annotation pipeline, enriching text captions with domain-specific details. While quantitative analysis would require specialized expertise, we provide qualitative evidence of its impact. As shown in Figure 5, MARVEL accurately identifies specific details, such as \u201cthree human footprints on a rocky surface\u201d, which both InternVL-2 [13, 15] and GPT-4 [60] miss, producing only generic descriptions. Similarly, MARVEL annotations include detailed identifiers like specific lunar craters, which are absent in outputs from InternVL-2 and GPT-4. This highlights how integrating human metadata enhances the context in annotations. Additional examples spanning other domains (e.g. biological elements, historical sites) are detailed in the supplementary material.\nB. Inter-Level Semantic Retention Evaluation: This ablation study measures how well semantic information is retained across MARVEL-40M+ annotation levels as they progress from detailed descriptions to concise tokens. To evaluate this, we report the semantic similarity (cosine similarity of embeddings) between levels using sentence-BERT [66] and the compression ratio (word count ratio) [6]. Results in Table 5 show strong semantic retention from Levels 1-4, demonstrating effective compression while preserving meaning. However, the shift to Level 5 results in lower similarity, reflecting the transition to a list of concepts at the expense of cohesive descriptions."}, {"title": "5. Limitation", "content": "Our analysis reveals some limitations in MARVEL annotation pipeline and MARVEL-FX3D. First, the underlying VLMs and LLMs exhibit inherent weaknesses in numerical precision [9, 61] and directional understanding [26] in complex scenes with multiple objects and occlusion. Second, InternVL-2 struggles with very thin objects, often misidentifying their side-views as different objects entirely. Finally, without metadata support, the caption accuracy becomes generic for complex 3D structures, particularly in scenes with fragmented geometries like architectural interiors. Additionally, MARVEL-FX3D sometimes generates flat 3D objects due to depth ambiguity in the input image. In supplementary section, some visual examples are provided. Despite these challenges, it is important to note that the strengths of our proposed pipeline and architecture remain significant, as they are both model-agnostic and adaptable to future enhancements."}, {"title": "6. Conclusion", "content": "In this work, we introduced MARVEL-40M+, the largest 3D captioning dataset to date, comprising over 40 million high-quality text annotations for 8.9 million 3D assets across seven major 3D datasets. Our primary contributions include a scalable, multi-stage annotation pipeline that combines open-source pretrained multi-view VLMs and LLMS with filtered human metadata to reduce hallucinations and introduce domain-specific information. Our pipeline produces five levels of annotations for diverse 3D modeling needs, from detailed reconstruction descriptions to rapid prototyping tags. Additionally, we introduce MARVEL-FX3D, a two-stage architecture that leverages fine-tuned Stable Diffusion on our dataset and pretrained Stable Fast 3D to generate high-quality, textured 3D meshes in just 15 seconds. Through extensive experimentation, we demonstrated both MARVEL-40M+'s superior annotation quality and linguistic depth, and MARVEL-FX3D's state-of-the-art performance in high fidelity text-to-3D generation. We believe that MARVEL-40M+ will serve as a foundational resource for future advancements in text-to-3D content creation, inspiring further research to address the current limitations and expand the dataset's applications."}]}