{"title": "Climate Aware Deep Neural Networks (CADNN) for Wind Power Simulation", "authors": ["Ali Forootani", "Danial Esmaeili Aliabadi", "Daniela Thr\u00e4n"], "abstract": "Wind power forecasting plays a critical role in modern energy systems, facilitating the integration of renewable energy sources into the power grid. Accurate prediction of wind energy output is essential for managing the inherent intermittency of wind power, optimizing energy dispatch, and ensuring grid stability. This paper proposes the use of Deep Neural Network (DNN)-based predictive models that leverage climate datasets, including wind speed, atmospheric pressure, temperature, and other meteorological variables, to improve the accuracy of wind power simulations.\nIn particular, we focus on the Coupled Model Intercomparison Project (CMIP) datasets, which provide climate projections, as inputs for training the DNN models. These models aim to capture the complex nonlinear relationships between the CMIP-based climate data and actual wind power generation at wind farms located in Germany. Our study compares various DNN architectures, specifically Multilayer Perceptron (MLP), Long Short-Term Memory (LSTM) networks, and Transformer-enhanced LSTM models, to identify the best configuration among these architectures for climate-aware wind power simulation.\nThe implementation of this framework involves the development of a", "sections": [{"title": "1. Introduction", "content": "The role of renewable energy (RE) in reducing greenhouse gas (GHG) emissions is crucial, which is essential for mitigating the impact of climate change. Europe's 2030 Climate and Energy Strategy aims to reduce GHG emissions by 40% compared to 1990 levels and achieve a 27% share of renewable energy in total electricity consumption by 2030 Oberth\u00fcr [1]. Wind power is one of the leading renewable energy source in terms of installed capacity and growth, contributing 15% of the European Union's electricity consumption in 2019 Asiaban et al. [2], Zhang et al. [3]. However, wind power is highly sensitive to climate change, as future changes in wind patterns can significantly impact electricity production due to the cubic relationship between wind speed and energy potential Carvalho et al. [4]. Wind farms typically have lifetimes of 20-30 years, making it essential to estimate future changes in wind resources under climate change scenarios. Factors, such as mean wind speeds and inter-annual variability, can affect the reliability and profitability of wind energyCarvalho et al. [5], Pryor and Barthelmie [6].\nGlobal Climate Models (GCMs), particularly from the Coupled Model Intercomparison Project (CMIP), provide crucial data for assessing climate change impacts Ahmadalipour et al. [7]. CMIP is a collaborative framework designed to compare and improve climate models by coordinating experiments across multiple research institutions Eyring et al. [8]. The project has become a cornerstone in climate science, offering standardized simulations that help evaluate models' performance and project future climate scenarios. CMIP involves numerous coupled models that simulate interactions between different Earth system components, such as the atmosphere, oceans, land"}, {"title": "2. literature review", "content": "The growing significance of wind power prediction in recent years is largely due to its applications at a commercial scale, offering a clean alter-"}, {"title": "2.1. Contributions", "content": "This article introduces a refined methodology for integrating CMIP6 climate data into wind energy forecasting over Germany. The proposed approach enhances key aspects such as data preparation, spatial interpolation, temporal alignment, and scaling, enabling effective localized forecasts at wind turbine sites. By tailoring CMIP6 data for machine learning applications, we bridge the gap between large-scale climate models and localized renewable energy prediction. Key Methodological Advancements are as follows:\n\u2022 Data Preparation:\nTemporal resampling and conversion to a standardized time format ensure compatibility with time-series forecasting models, particularly deep learning architectures like LSTM networks that rely on consistent temporal structures for accurate predictions.\nFeature normalization is applied to harmonize input scales, promoting balanced contributions during model training and improving predictive performance.\n\u2022 Spatial Interpolation: An interpolation algorithm accurately maps rotated pole datasets to geographic coordinates, ensuring precise spatial alignment with wind conditions across Germany\u2014a critical step for localized forecasting.\n\u2022 Model Evaluation: This study systematically evaluates multiple deep learning architectures, including Multi-Layer Perceptrons (MLPs), LSTM networks, and Transformer-enhanced LSTMs:\nMLPs effectively capture static relationships but struggle with the temporal and non-linear dependencies inherent in climate data.\nTransformer-enhanced LSTMs, despite their complexity, did not yield significant performance improvements, indicating a mismatch with the dataset's characteristics.\nLSTM networks outperform others due to their robust ability to model sequential and long-term dependencies, making them the preferred architecture for climate-driven renewable energy forecasting."}, {"title": "3. Mathematical Background of DNNS", "content": "A DNN is a type of ANN with multiple layers of neurons. The primary components of a DNN are neurons, layers, weights, and activation functions Forootani et al. [49].\nMathematical Representation. Let x represent the input vector, and W\u0131, b\u0131 represent the weight matrix and bias vector of the l-th layer, respectively.\nThe output of the l-th layer is computed as:\nz\u0131 = W\u0131a\u0131\u22121+ b\u0131,\nwhere a\u0131\u22121 is the activation from the previous layer. The activation function \u03c3is applied to produce the output of the layer:\na\u03b9 = \u03c3(\u03b6\u03b9),\nCommon choices for activation functions include: (i) Sigmoid: $\u03c3(x) = \\frac{1}{1+e^{-x}}$; (ii) ReLU: \u03c3(x) = max(0, x); (iii) Tanh: \u03c3(x) = tanh(x).\nThe final output layer of the network computes the predicted value:\nypred= WLAL\u22121 + b\u2081,"}, {"title": "3.1. LSTM Networks", "content": "LSTM networks are a type of Recurrent Neural Network (RNN) designed to address the vanishing gradient problem in traditional RNNS, particularly for sequences with long-range dependencies Huang et al. [50].\nLSTM Architecture. An LSTM unit consists of several gates: input gate, forget gate, and output gate. The cell state Ct and hidden state ht are updated at each time step. The gates in LSTM are computed as follows:\nForget gate:\nf\u2081 = \u03c3(Wf [ht\u22121, Xt] + bf),\nInput gate:\nit = \u03c3(Wi[ht\u22121, xt] + bi),\n\u010ct = tanh(Wc[ht\u22121, xt] + bc),\nOutput gate:\not = \u03c3(Wo[ht\u22121, xt] + bo),\nwhere \u03c3 is the activation function 1. The cell state is updated as:\nCt= ftCt\u22121 + it\u010ct,\nwhere \u2299 denotes element-wise multiplication.\nThe hidden state is updated as:\nht= Ottanh(Ct)."}, {"title": "3.2. Transformer-based DNNS", "content": "Transformer networks are primarily used for sequence-to-sequence tasks such as machine translation, and text summarization Vaswani et al. [51]. Transformer utilize self-attention mechanisms to model dependencies between elements in a sequence Forootani et al. [52].\nTransformer Architecture. The Transformer model consists of an encoder and a decoder, both composed of stacked layers of self-attention and feed-forward neural networks.\nSelf-Attention. For each input sequence, the self-attention mechanism computes a scaled dot-product attention for each query qi, computed as:\nAttention(qi)=softmax(qi\u221adk)Vj,\nwhere qi is the query vector for the i-th element, kj is the key vector for the j-th element, vj is the value vector for the j-th element, and dk is the dimension of the key vector.\nThe self-attention mechanism captures dependencies by weighting the importance of each element in the sequence with respect to others.\nMulti-Head Attention. The model applies multiple attention mechanisms in parallel (multi-head attention) to capture different aspects of dependencies.\nPosition Encoding. Since Transformers do not have an inherent notion of sequence order, position encodings are added to the input to provide information about the position of tokens in the sequence. The position encoding pt for position t is typically computed as:\npt=[sin(t100002id),cos(t100002i+1d)] for i = 1, 2, . . ., d,\nwhere d is the dimension of the input."}, {"title": "4. Analysis of Meteorological Data Over Germany", "content": "Effective data processing techniques are crucial for managing and analyzing large-scale datasets in scientific and industrial domains, particularly in renewable energy and climate studies. Techniques such as data cleaning, validation, interpolation, and machine learning integration are essential for extracting actionable insights from atmospheric and geospatial data, including wind speed, pressure, and temperature. Publicly accessible datasets like those from CMIP6 require spatial interpolation to estimate values at specific locations, using methods like the Regular Grid Interpolator for geographic grids Jung and Broadwater [46], Weiser and Zarantonello [53], and temporal interpolation to handle missing time-series data. Climate models often provide results with a lower temporal resolution (typically a 3-hour resolution\u00b2), while wind power data is recorded hourly or even quarter-hourly (e.g., Open Power System Data\u00b3). To align these datasets, a resampling and aggregation approach is necessary, transforming wind power and pressure data into standard intervals, such as 3-hour periods, to match the climate data."}, {"title": "Coordinate Transformation.", "content": "The CMIP datasets utilize a rotated pole coordinate system where geographic coordinates are defined relative to an artificial \"rotated\" pole. This coordinate system is commonly used to reduce grid distortion in regional climate modeling Grose et al. [54]. To align with standard geographic coordinates (latitude and longitude), a coordinate transformation is required. Using the Algorithm algorithm 1, we convert the standard latitude and longitude of target points within Germany to this rotated coordinate system. This ensures that each point of interest in Germany is accurately aligned with the dataset's coordinate system before data extraction, maintaining the spatial integrity of climate variables such as wind speed."}, {"title": "Spatial Interpolation and Efficient Spatial Querying with KD-Tree on the climate dataset.", "content": "After transforming coordinates, the next challenge is to spatially match these target points to the dataset's grid points, which may not align perfectly. The CKDTree is a binary space-partitioning data structure used to organize points V = {V1, V2, ..., Vn} \u2208 Rk, where each node splits the space at a median point along one of the k dimensions. At depth d, the space is split along dimension d mod k using the median value,\nVs plit=median(v(doldmodk)1,...,vndoldmodk), dividing points into two subsets.\nFor nearest-neighbor search, given a query point q \u2208 R, it efficiently finds the closest point Vnearest = arg minvi\u2208V ||q \u2013 Vi||2. The KD-tree accelerates queries like nearest neighbors or radius searches by pruning irrelevant parts of the space, reducing search complexity to O(log n) on average 5.\nConstructing a KD-Tree for the grid enables rapid spatial querying, significantly reducing the computational complexity compared to exhaustive search methods. This function then performs a nearest-neighbor search, returning the closest available grid point to each transformed target coordinate. This approach minimizes interpolation errors and ensures that the extracted data points accurately represent spatial locations within the target region.\nThis combination of coordinate transformation and KD-Tree spatial querying provides an accurate and computationally efficient methodology for extracting climate variables at the desired spatial resolution. It enhances the fidelity of regional climate analysis by aligning extracted data points with geographic coordinates in a physically meaningful way.\nIn conclusion the KD-Tree algorithm is used to efficiently map the extracted wind speeds or pressure surface from the rotated grid of the CMIP6 dataset to a regular latitude-longitude grid over Germany, ensuring spatial data consistency during the extraction process. However we need another interpolation algorithm to further refine this data by interpolating the wind speeds or pressure surface onto specific, irregularly spaced target locations (e.g., measurement sites), addressing the mismatch between the extracted grid and the desired points of analysis.\nSpatial Interpolation at the location of wind turbines. To refine the spatial representation of both wind speed and pressure data, interpolation techniques are applied. Wind speeds and pressure levels at the locations of wind farms are interpolated onto the desired target locations (such as wind turbine sites or measurement stations). In particular, we employ a linear interpolation method (Regular Grid Interpolator), which performs multivariate interpolation on a regular grid in n-dimensional space. Suppose the function f : R\" \u2192 R is known on a set of regularly spaced grid points {x1, x2,...,x}, where each xi \u2208 Xi forms a regular grid for each dimension. The objective is to estimate f(x), where x = (x1, x2,...,xn), which is not necessarily a grid\""}, {"title": "point.", "content": "To interpolate at x, let (x1, x+1), ..., (xh, xk+1) be the grid intervals that contain x1,...,xn. For linear interpolation in each dimension, the value of f(x) is obtained as a weighted combination of the function values at the corners of the grid cell surrounding x. Denote the neighboring grid points as f(x1, x2,...,x), and the interpolated value is given by:\nf(x)\u22481\u2211i1=01\u2211i2=0\u20261\u2211in=0Wili2\u2026inf(x1,x2,\u2026,xn),\nwhere Wili2...in are the weights based on the relative distances between x and the surrounding grid points. For example, for linear interpolation in 1-D, the interpolation formula between two grid points xi and Xi+1 is:\nf(x)\u2248xi+1\u2212xxi+1\u2212xixf(xi)+x\u2212xixi+1\u2212xif(xi+1).\nIn cases where interpolated values can not be computed due to a lack of surrounding data, the nearest available data points were used as a fallback, ensuring that the final interpolated dataset contained no gaps. Interpolation allows for higher spatial granularity of both wind speed and pressure data, enhancing the model's ability to predict localized weather conditions. Algorithm algorithm 2 summarizes the interpolation method that is explained earlier.\nTemporal Resampling. Wind power production usually is measured hourly6, however both wind speed and surface pressure in CMIP data are available in 3-hour interval, therefore the power is resampled. This approach retains essential temporal variability while reducing computational requirements for subsequent analysis. To ensure consistency across the dataset, the datetime information is standardized, facilitating time-based calculations in machine learning workflows. This transformation enables the data to be effectively utilized in time-series forecasting models, ensuring that temporal patterns are accurately captured. These steps of resampling and time standardization are crucial for preparing the data for machine learning applications, such as LSTM networks, which depend on a regular time structure to enhance forecasting accuracy in wind power prediction."}, {"title": "Data Scaling and Normalization.", "content": "Data scaling is an essential step in preparing datasets for machine learning models, which can be sensitive to the range and distribution of input values. Here, wind speed and pressure data were scaled using a Min-Max scaling technique to map values between -1 and 1. This approach ensures that each feature contributes equally to the learning process, avoiding biases from variables with larger numerical ranges.\nNormalizing variables like wind speed, pressure, and time ensures that machine learning models can identify patterns effectively, without being affected by differences in scale across features. This scaling process enhances the performance of machine learning algorithms, such as DNNs, which typically perform better when input features are standardized."}, {"title": "4.1. Dataset Visualization", "content": "To facilitate interpretation, various visualization tools are used. In Figure 1, we present the average surface wind speed across European countries based on the CMIP dataset. We apply algorithm 1 to transform the coordinates, followed by algorithm 2 to interpolate meteorological data specifically for the locations of wind farms in Germany. Notably, the locations of these wind farms are sourced from the EE-Monitor project, which provides environmental insights into renewable energy expansion in Germany.\nWind speed and surface pressure data, each with a shape of (2928 \u00d7 28574), have been extracted from the CMIP dataset. Here, 2928 represents the number of time steps at 3-hour intervals across one year, and 28574 represents the various grid points. These grid points correspond to a latitude and longitude grid with shapes (157) and (182), respectively.\nTo focus on specific target locations, wind speed and pressure data are interpolated, resulting in arrays with shapes of (2928\u00d7232) for both variables. The target points, which represent the locations of wind farms in Germany, are provided as specific coordinates of interest with a shape of (232\u00d72). This data structure highlights the challenge of high dimensionality within climate datasets."}, {"title": "5. DNNS for Wind Power Production Forecast Enhanced with Climate Data", "content": "Previous attempts to generate a high-quality estimation of wind power generation based on climate models using physics-aware models did not yield acceptable results due to the inherent variabilities of climate scenarios and the inability of current climate models to offer scenario-based data with high spatial and temporal sensitivity Lehneis et al. [48]. Thus, instead of using physical simulation models, we employ DNN architectures on the processed C\u041c\u0406\u0420 climate dataset to improve prediction accuracy in wind power simulating. Specifically, we compare three types of DNN models\u2014MLPs, LSTM networks, and Transformer-enhanced LSTM networks\u2014to evaluate their performance and suitability for this task.\nAdvancing wind power simulating with climate data requires machine learning models capable of capturing complex, non-linear patterns and temporal dependencies. DNNs are well-suited for this purpose, as they can adapt to a wide range of input structures and feature relationships. MLPs are particularly effective for capturing non-linear input-output mappings in high-dimensional data, making them useful for static feature extraction from cli-"}, {"title": "5.1. Data Preparation Process for LSTM-Based Time Series wind power simulation", "content": "Preparing data for LSTM-based deep learning models is essential for capturing temporal dependencies in time series forecasting. This process involves four key stages: partitioning the dataset, generating sequences, reshaping the data, and initializing data loaders. Each stage ensures that the model receives well-structured inputs, enabling it to learn from historical data and generalize effectively to new, unseen data.\nThe first stage is to divide the dataset into training and testing subsets based on a predefined ratio of test data. This split is carried out chronologically to maintain the sequential nature of the data and avoid potential data leakage between the training and testing phases. This step is critical for ensuring that the model is evaluated on data that it has not seen during training, which is necessary for obtaining an unbiased assessment of its predictive performance.\nNext, fixed-length sequences are created from the training and testing datasets. These sequences consist of consecutive data points over a specified length, providing the LSTM model with historical context that is crucial for making accurate predictions of future time steps. A rolling window approach is employed, extracting sequences of data followed by the corresponding target value for prediction.\nOnce the sequences are generated, they must be reshaped into the appropriate format for LSTM input, typically Batch size, Sequence Length, number of features. This reshaping step ensures that the data is compatible with the LSTM architecture, enabling efficient computation and facilitating parallel processing during model training.\nFinally, data loaders are employed to manage the input data during training. By enabling mini-batch processing, data loaders improve computational efficiency, particularly when training on GPUs, and help accelerate the convergence of the model. Additionally, shuffling within batches promotes better"}, {"title": "5.2. Hardware setup", "content": "The training of our deep learning models was conducted on high-performance hardware to handle the computational demands of wind power simulation. Specifically, we utilized a single NVIDIA A100 GPU, equipped with 80 GB of Video Random Access Memory (VRAM) 8, optimized for intensive machine learning tasks. This configuration included a memory allocation of 256 GB per CPU core to support large-scale data processing. The training job is managed through Simple Linux Utility for Resource Management (SLURM), using job-specific constraints to ensure compatibility with the A100 GPU's extensive memory capacity, essential for efficient parallel processing and model training stability\u00ba. This advanced hardware setup enabled effective training of our models, enhancing both speed and accuracy in handling the complex temporal dependencies within the wind power data."}, {"title": "5.3. Code and data availability statement.", "content": "The CADNN model is implemented using the PyTorch framework, taking advantage of its advanced tools for constructing and training deep learning models. CADNN is openly available to enable reproducibility and facilitate further research in wind power simulation, leveraging climate datasets and DNNS for accurate, climate-informed predictions at this github repository10.\nThe code for data extraction, interpolation, statistical analysis, visualization, DNN training and evaluation is provided in the associated repository."}, {"title": "5.4. DNN simulation setups", "content": "In this study, we explore three deep learning architectures to forecast wind power generation, comparing their abilities to model complex temporal patterns in time-series data. The first scenario employs a structured MLP model based on the Sinusoidal Representation Network (SIREN) architecture, designed for high-frequency signal representation Sitzmann et al. [56], Forootani and Benner [57], Forootani et al. [58]. This model leverages sinusoidal weight initialization to capture fine-grained temporal dependencies. The second scenario implements a LSTM network, a recurrent architecture that excels at learning sequential data patterns over long periods. In the third scenario, the deep learning model integrates both LSTM and Transformer layers, allowing it to leverage temporal dependencies and self-attention for modeling long-range dependencies. While all models share the same input and output configurations\u2014an input of 5 features and a single-output prediction\u2014each architecture is uniquely tailored to address the challenges of time-series forecasting. By comparing the SIREN-based MLP, LSTM, and Transformer-enhanced LSTM approaches, we aim to identify the most effective structure for capturing wind power patterns, optimizing both forecasting accuracy and training stability. Algorithm algorithm 4 summarizes the main training loop of the DNN structure considered in this article. In addition, out of total number of 679296 samples (as mentioned earlier 2928 \u00d7 232), 90% is randomly chosen for training the DNN structures and remaining 10% for testing the results.\nMLP based CADNN. In the first scenario, we implement an MLP type DNN model using a structured, layered architecture tailored for wind power simulation. Our model configuration includes an input size of 5 features and a single-output prediction, passing through a sequence of six hidden layers, each with 128 hidden units. The model's core is built around the SIREN (Sinusoidal Representation Network) architecture Sitzmann et al. [56], Forootani et al. [58], Forootani and Benner [57], Forootani et al. [59], known for its ability to represent high-frequency signals, making it particularly effective for"}, {"title": "time-series data in wind forecasting.", "content": "The network architecture initializes the model parameters with a specific sinusoidal weight initialization method, which allows the model to capture fine-grained temporal patterns. Number of epochs is considered 30000 with learning rate le\u00af5. Optimization is performed using the Adam optimizer with a weight decay of 1 \u00d7 10-6 to prevent overfitting. Additionally, we employ a Cyclic Learning Rate (CLR) scheduler in exponential range mode, which oscillates the learning rate between a base and maximum value, enhancing convergence by adjusting the learning rate dynamically. This architecture, optimized for periodic data, allows the model to leverage temporal dependencies effectively, providing a robust foundation for forecasting applications in renewable energy systems.\nThe scatter plot in fig. 9 illustrates the correlation between the true and predicted values generated by the MLP-DNN model for wind power simulation. Points closely aligned along the diagonal line indicate strong prediction accuracy, as the predicted values closely match the true values. It reveals a significant disparity between the predicted and true values, indicating that the MLP-DNN struggles to accurately forecast wind power. Many predictions deviate considerably from the expected values, suggesting potential shortcomings in the model's ability to capture the underlying patterns in the data.\nThe histogram of prediction errors in fig. 9 presents the distribution of prediction discrepancies, defined as the difference between true and predicted values. A narrow centered distribution around zero with minimal spread indicates accurate predictions. The histogram of prediction errors further illustrates the model's limitations, as it shows a wide spread of errors rather than clustering around zero. This indicates that the MLP-DNN frequently makes large errors in its predictions, highlighting the need for model refinement or alternative approaches to improve forecasting accuracy.\nThe plot fig. 10 illustrates the discrepancy between measured wind power generation and the MLP-DNN model's predictions over the selected sample interval, indicating that the model struggles to accurately capture the variations in wind power output over the selected sample interval.\nLSTM based CADNN. In the second scenario the deep learning architecture employed in our study consists of a LSTM network tailored for time-series forecasting. Specifically, the model is constructed with an input size of 5 features, passing sequentially through 6 stacked LSTM layers, each containing 128 hidden units. Each LSTM layer is initialized using Xavier initialization"}, {"title": "to optimize weight distribution.", "content": "Following the LSTM layers, a fully connected layer maps the output of the final LSTM layer to a single prediction, as defined by the output size of one. This architecture captures complex temporal dependencies in the data across multiple layers, enhancing its ability to model long-range patterns. We use the Adam optimizer with a learning rate scheduler that dynamically reduces the learning rate based on validation loss, ensuring stability in training and adaptability to diminishing gradients over time. Number of epochs is considered 30000 with initial learning rate 1e-3. This LSTM-based deep model is well-suited for our wind power simulation task due to its capacity to learn sequential dependencies and generate accurate time-step predictions.\nThe scatter plot in fig. 11 demonstrates a strong correlation between the true values and the predicted values, with most predictions closely aligning along the diagonal line, indicating that the LSTM-DNN model performs exceptionally well in accurately forecasting outcomes.\nThe histogram of prediction in fig. 11 errors reveals a tight distribution centered around zero, suggesting that the LSTM-DNN model consistently produces accurate predictions, with only a few instances of significant deviation, further highlighting its effectiveness in the task at hand.\nThe line plots in fig. 12 and fig. 13 vividly illustrate the performance of the LSTM-DNN model in predicting wind power generation, with the predicted values closely following the trend of the measured wind power, especially within the selected sample range. This indicates a strong predictive capability of the model, as evidenced by the minimal divergence between the predicted and true values.\nLSTM-Transformer based CADNN. In this scenario, the deep learning architecture employed for time-series forecasting integrates both LSTM and Transformer layers. Specifically, the model is designed with an input size of 5 features, and it passes through 6 stacked LSTM layers, each containing 64 hidden units. The LSTM layers are initialized using Xavier initialization to ensure effective weight distribution. Following the LSTM layers, the output is passed through 2 Transformer layers, each utilizing 4 attention heads and a feed-forward network of size 20. This hybrid architecture enables the model to leverage both the temporal dependencies captured by the LSTM layers and the self-attention mechanism provided by the Transformer layers, improving its ability to model complex sequential patterns with long-range dependencies. The final output is generated by a fully connected layer that maps the output"}, {"title": "of the last Transformer layer to a single prediction, as defined by the output size of one.", "content": "The Adam optimizer is used with an initial learning rate of 1e-3, and a learning rate scheduler is applied to reduce the learning rate dynamically based on validation loss, ensuring training stability and adaptability. The model is trained for 30000 epochs, making it particularly well-suited for forecasting tasks that involve both short-term and long-term temporal dependencies, such as wind power simulation.\nThe scatter plot in fig. 14 illustrates a weak correlation between the true values and predicted values, with the majority of predictions clustering not closely along the diagonal line. This alignment indicates that the LSTM-Transformer-DNN model achieves low accuracy in forecasting wind power outputs.\nAdditionally, the histogram of prediction errors in fig. 14 shows a less concentrated distribution around zero, implying that the model consistently can not provides precise predictions, with maximal occurrences of substantial error. This performance underscores the effectiveness of the LSTM-Transformer-DNN architecture for accurate time-series forecasting in wind power applications.\nThe plots in fig. 15, fig. 16, and fig. 17 highlight discrepancies between the measured wind power generation and the LSTM-Transformer-DNN model's predictions over the selected sample interval. These inconsistencies suggest that the model struggles to capture the full extent of variability in wind power output."}, {"title": "6. Conclusion", "content": "This study made several important contributions to the literature on renewable energy forecasting by integrating large-scale CMIP6 climate data with advanced machine learning models for wind power prediction. We proposed a novel data processing pipeline that included temporal resampling, spatial interpolation, and normalization techniques, establishing best practices for adapting climate datasets for machine learning frameworks. Our comparative analysis of deep learning models demonstrated that LSTM networks were highly effective for this application, significantly outperforming MLPs and Transformer-enhanced LSTM models in capturing the temporal dependencies essential for accurate forecasts.\nIn addition, we provided a dedicated Python package built with PyTorch to support the reproducibility of our framework. This package offered mod-"}]}