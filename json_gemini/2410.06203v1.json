{"title": "Integrating Planning into Single-Turn Long-Form Text Generation", "authors": ["Yi Liang", "You Wu", "Honglei Zhuang", "Li Chen", "Jiaming Shen", "Yiling Jia", "Zhen Qin", "Sumit Sanghai", "Xuanhui Wang", "Carl Yang", "Michael Bendersky"], "abstract": "Generating high-quality, in-depth textual documents, such as academic papers, news articles, Wikipedia entries, and books, remains a significant challenge for Large Language Models (LLMs). In this paper, we propose to use planning to generate long form content. To achieve our goal, we generate intermediate steps via an auxiliary task that teaches the LLM to plan, reason and structure before generating the final text. Our main novelty lies in a single auxiliary task that does not require multiple rounds of prompting or planning. To overcome the scarcity of training data for these intermediate steps, we leverage LLMs to generate synthetic intermediate writing data such as outlines, key information and summaries from existing full articles. Our experiments demonstrate on two datasets from different domains, namely the scientific news dataset SciNews and Wikipedia datasets in KILT-Wiki and FreshWiki, that LLMs fine-tuned with the auxiliary task generate higher quality documents. We observed +2.5% improvement in ROUGE-Lsum, and a strong 3.60 overall win/loss ratio via human SxS evaluation, with clear wins in organization, relevance, and verifiability.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have achieved remarkable progress in various text generation tasks, ranging from creative writing to summarization and dialogue gener-ation [11, 8]. However, generating high-quality, coherent, and substantive long-form documents, such as academic papers, news articles, and books, remains a significant challenge [23, 24]. Existing work mostly tackles this challenge by sequentially prompting LLMs to write a small segment of the document at each call and integrating the outputs into the final long-form document [24, 33, 19]. Such systems usually require additional components to ensure the coherence or consistency of the entire document [4, 30]."}, {"title": "", "content": "In this paper, we present a novel approach that directly fine-tunes LLMs to generate long-form documents in one single call. This approach enables us to fully leverage the token-level attention mechanism in the decoding process, thereby ensuring the coherence and consistency of the generated document. It also significantly reduces the complexity of the system during serving and deployment to write long-form documents.\nWriting high-quality long-form documents often involves a pre-writing stage [18] where authors outline the structure, develop key arguments and plan for the overall flow of the document. This additional stage enables the writers to simplify the tasks into manageable sub-tasks, similar to the idea of Chain-of-Thought (CoT) in multi-step reasoning [26]. In fact, this stage is often included in the design of multi-stage long-form document writing systems [33, 32, 19] through multiple rounds of prompting.\nMotivated by this idea, we propose to integrate this pre-writing stage into our development of the single-turn LLM writer. Specifically, we introduce a series of auxiliary training tasks to endow LLMs with the skills to plan and structure long-form documents before generating the final full article. For example, one auxiliary task could involve providing the LLM with the writing context as input and expecting it to produce an outline with key insights as the output. Another auxiliary task can present the LLM with the writing context and an outline, with the goal of generating the complete article. We argue that fine-tuning the LLM writer with a mixture of writing tasks, coupled with guidance at varying levels of granularity, can enhance the model's ability to produce long-form documents that are inherently well-structured and coherent.\nWhile it is relatively easy to obtain sufficiently large corpora of full articles for supervised fine-tuning, obtaining intermediate writings such as article outline and key points directly from human writers is considerably more challenging as these are typically not well documented and made public. To address this, we leverage the few-shot capabilities of LLMs to generate synthetic intermediate writings from full articles, along with the original document structure when available. Note that, generating a concise summary, excerpt, or outline from a full-length, detailed article is much easier than doing the reverse. Therefore, it becomes rather manageable to create abundant intermediate-writing data for the purpose of fine-tuning LLMs towards learning to plan for writing full articles.\nOur experiments on multiple datasets demonstrate that LLM writers trained with the auxiliary tasks generate higher quality long-form documents in a single pass, even when the final inference task does not prompt the model to produce intermediate planning steps.\nOur main contributions are summarized as follows:\n\u2022 We propose a novel approach that directly fine-tunes LLMs to generate the entire long-form document in a single pass, simplifying the generation process and enhancing coherence.\n\u2022 Inspired by human writing practices, the proposed framework incorporates the pre-writing stages by introducing auxiliary training tasks that teach the LLMs to plan and structure documents before generating the final text.\n\u2022 To overcome the challenge of limited training data for intermediate writing steps, we leverage LLMs' ability to generate synthetic summaries, outlines, and key"}, {"title": "", "content": "information from existing full articles. This innovative approach unlocks a vast new source of training data for LLM planning.\n\u2022 Our extensive experimental results demonstrated the effectiveness of the proposed approach on multiple datasets, showing that LLMs fine-tuned with the auxiliary tasks produce higher quality, more coherent long-form documents in a single pass."}, {"title": "2 Related work", "content": "Planning. Our work contributes to the field of planning in long-form text generation. Humans typically simplify complex tasks into manageable subtasks, a method mirrored in recent approaches employing large language models (LLMs) for planning. Techniques such as Chain of Thought (CoT) [26], Zero-shot-CoT [9], Self-consistent CoT (CoT-SC) [25] guide LLMs through sequential reasoning by utilizing intermediate reasoning steps. More advanced methods, like Tree of Thoughts (ToT) [34], GoT [3] enhance these strategies with tree-like and graph-based reasoning structures, respectively. Additional methods, including RePrompting [31] and ReWOO [27] ensure planning accuracy by integrating observational data and using corrective prompting HuggingGPT [21] further decomposes tasks into sub-goals solved through iterative LLM interactions, contrasting the one-shot approach of CoT and Zero-shotCoT. Despite these innovations, specialized zero-shot plan generation for specific domains remains to be challenging, addressed by models like LLM+P [12], LLM-DP [5], and CO-LLM [35]. Built upon previous works, our approach trains models specifically to enhance planning within the domain of long-form text generation.\nLong-form text generation. Long-form text generation and question answering (LFQA), which require maintaining coherence over extended texts, remain highly chal-lenging for large language models (LLMs), as evidenced by numerous studies [7, 28, 10, 15, 22]. [24] introduced a progressive generation technique utilizing pretrained language models to enhance the creation of extended narratives. [30] explored discourse-aware neural extractive text summarization, essential for maintaining logical flow and thematic consistency in long documents. There have also been efforts to generate Wikipedia articles, as documented by [2, 14, 13], along with recent advancements by [6]. [1] developed the Imitate Retrieve-Paraphrase framework to enhance expository writing at the paragraph level, particularly focusing on the integration of information from diverse sources. Our research introduces a novel method that leverages organic long documents to create intermediate text generation plans. This approach trains models to enhance their abilities not only in generating text but also in generating and adhering to these structured plans, thereby distinguishing our method from previous work. The use of the Retrieval-Augmented Generation (RAG) technique is outside the scope of our current discussion, although our method is compatible with RAG applications."}, {"title": "3 Problem Setup", "content": "Long-form text generation. Given an input context xi, which can either be the source academic paper for SciNews generation or a sentence prompt (e.g., \"generate wiki page about {topic}\" for Wikipedia page generation), the objective is to fine-tune an LLM to generate a long-form article yi from xi.\nThe dataset D used for fine-tuning is structured to align with this objective. It consists of pairs of input contexts and their corresponding final documents. Formally, the dataset D is defined as:\nD = {(x1, y1), (x2, y2), ..., (xn, yn)}\nwhere each tuple (xi, yi) represents an input context xi and the final document yi.\nIntermediate steps. To reduce the cognitive burden on LLM when generating a full article directly from the input context, intermediate steps are often introduced, gradually leading from the input context to the final full article. In particular, for each input context xi and its corresponding article yi, the intermediate steps zi can either be a set of distinct pieces of information (e.g., summary, outline, key information) or a sequence of information where each element (j + 1)-th is dependent on the preceding element j. We denote zi as:\nZi = {zi1, zi2,..., zik}\nThe overall idea is that the auxiliary information provided at these intermediate steps zi can effectively guide the LLM to gradually approach the final target article y from the potentially abstract or noisy input context xi. There are multiple ways to instantiate the concrete content of zi. For example:\n1. Linear: One can decompose the full article writing task into writing one section at a time. In this approach, the first intermediate step zi1 would be the first section; the j-th intermediate step zij would be concatenating one more section to the (j - 1)-th draft.\n2. Top-down: Another common strategy is to use intermediate steps from the most abstract outline to gradually more detailed content. For example, the first"}, {"title": "", "content": "intermediate step zi1 can be an abstract outline only with the title of each section of the article. Each subsequent intermediate step would gradually elaborate on the content within each section. This kind of intermediate steps are not usually readily available, and may need to be constructed from the full article, which we will describe later in this paper.\nNote that many existing works aim to developing multi-turn LLM writers, which involves a series of tasks such as generating zi1 from xi, then zij from zi(j\u22121). However, this is not the focus of our paper. In this paper, as shown in Figure 1, we introduced intermediate steps to create different training tasks for fine-tuning LLMs. Our final objective remains to generate the full article yi from the input context xi in a single turn. During inference, the input is always only the input context xi. The model can either generate only the article yi or both the intermediate steps zi and the article yi, as long as the article can be easily extracted from the complete model output through post-processing."}, {"title": "4 Methodology", "content": "Our proposed framework addresses the challenge of generating long-form text with LLMs by utilizing the inherent structure of documents. Specifically, we construct intermediate steps zi based on this structure to guide the generation process. The key insight is that the inherent structure, such as an article's outline, provides crucial guidance for organizing the generated article yi.\nWithin this framework, we construct the intermediate steps z\u00bf by extracting the implicit structural information inherent in a formulated document and then constructing a hierarchical writing plan accordingly. These intermediate steps then serve as a founda-tion for fine-tuning the LLM, focusing on tasks that emphasize structural understanding and plan interpretation. At inference time, the fine-tuned LLM, equipped with enhanced structural and plan interpretation capabilities, generates the final long-form text yi in a single pass. The specific components of this framework are discussed in the following sections."}, {"title": "4.1 Writing Plan as Intermediate Steps", "content": "Writing high-quality long-form documents typically begins with a pre-writing phase, where authors establish the structural framework, develop key arguments, and formulate the overall trajectory of the narrative. In this work, we define the pre-writing stage as a series of intermediate steps essential for generating long-form documents. Specifically, without loss of generality, we concentrate on three distinct types of intermediate steps: document summary, document outline, and document key information.\nDocument Summary A document summary is a concise representation that captures the core message of a document, summarizing key points, themes, or arguments while omitting peripheral details. It provides a quick overview, enabling readers to grasp the essential content without reading the entire document."}, {"title": "", "content": "Document Outline A document outline represents the hierarchical structure of a document. It reveals the organization and flow of ideas, the relationships between sections, and the key points within the document.\nDocument Key Information Document key information includes the most crucial facts, findings, or insights within a document. It typically represents the core knowledge that the document aims to convey or support."}, {"title": "4.2 Structured Intermediate Steps Construction", "content": "The intermediate steps zi involved in the creating long-form documents provide valuable structural guidance for the generation process. However, this structured information is not always explicitly available, nor is the alignment between the structure and the final document. As discussed above, authors typically establish these intermediate steps when writing long-form documents. Therefore, we posit that such implicit structural information is already embedded within the final document itself.\nBuilding on this observation, we propose a method to extract the intermediate steps from established documents yi, such as news articles and Wikipedia entries. Leveraging recent advances in LLMs, particularly their impressive few-shot learning capabilities, we aim to synthetically generate these intermediate steps directly from the documents. This approach captures the implicit structural information without relying on explicitly provided intermediate steps. Furthermore, it can generate multiple intermediate steps for a single document, thus enabling exploration of various organizational strategies. Lastly, our method provides a consistent and scalable solution for generating intermediate steps, making it well-suited for large-scale document processing. Figure 2 summarizes the process of our proposed process to construct intermediate steps from the article.\nTo extract the intermediate steps from an article yi, we prompt pre-trained LLMs, i.e., Gemini Ultra, to generate K intermediate step candidates {z}kK=1, where zk can be any kinds of structure information including document summary, document outline, and document key information. Despite LLM's impressive summarization capabilities, they can sometimes produce hallucination information. To mitigate this, we select the best intermediate step from K candidates using a designed scoring function that evaluate coherence and completeness with respect to the document yi. The candidate"}, {"title": "4.3 Fine-tuning", "content": "To empower the LLM with the ability to utilize structural information during long-form text generation, we propose a fine-tuning approach that goes beyond the conventional input-to-output (e.g., xi \u2192 yi) paradigm. Our method leverages intermediates steps zi, which encapsulate the structural essence of the desired output.\nSpecifically, we introduce two auxiliary fine-tuning tasks in addition to the standard xi \u2192 yi task.\nInput to output with intermediate steps (xi \u2192 zi \u2295 yi): This task trains the LLM to generate both the final long-form text yi and the corresponding intermediate steps zi. It encourages the model to internalize the relationship between content and structure, fostering a deeper understanding of how different elements of a document contribute to its overall organization. For this task, the training dataset is,\nD = {(x1, zi \u2295 y1), (x2, z2 \u2295 y2), ..., (xn, zn \u2295 yn)}"}, {"title": "", "content": "Input and intermediate steps to output (xi \u2295 zi \u2192 yi): In this task, the LLM is provided with both the input x and the synthetically generated intermediate steps zi as context. The model is then trained to generate the final long-form text yi, conditioned on the structural guidance provided by z. Here, z acts as a blueprint for generating the long-form text, helping the model maintain focus and generate more coherent and structured output. In this task, the training dataset is:\nD = {(x1 \u2295 z1, y1), (x2 \u2295 z2, y2),..., (xn \u2295 zn, yn)}\nThe two tasks complement each other by focusing on different aspects of the planned generation process. The (xi \u2192 zi \u2295 yi) task teaches the model to generate structure alongside content, emphasizing the inherent relationship between the two. Meanwhile, the (xi \u2295 zi \u2192 yi) task trains the model to effectively utilize provided structural information, improving its ability to follow a given plan. By incorporating these auxiliary tasks, our fine-tuning process aims to improve the LLM's capability of generating long-form text that is not only informative but also well-structured."}, {"title": "5 Experiments", "content": "We conduct extensive experiments on multiple datasets with different setups to validate the effectiveness of our proposed methods."}, {"title": "5.1 Benchmark Datasets", "content": "To validate the effectiveness of our planned generation method, we conducted experi-ments using the SciNews dataset and Wikipedia dataset.\nSciNews Dataset. SciNews dataset [17] is developed to facilitate the task of compiling academic publications into scientific news reports. This dataset provides a parallel collection of academic publications and their corresponding scientific news reports across 9 disciplines, including Medicine, Biology, Physics, and Chemistry. It comprises 41,872 samples, with each academic publication averaging 7,760.90 tokens and each news report averaging 694 tokens. In this dataset, we use each academic publication as the input xi and its corresponding news report as y\u2081. We randomly sample 33,497 items as the training split, 1,000 items for validation and 200 items as the evaluation split.\nWikipedia Dataset. We use the Wikipedia corpus from the KILT benchmark dataset [16] to construct the training and validation splits. This benchmark dataset is sourced from a snapshot of Wikipedia taken on August 1, 2019, and contains 5.9 million articles. The evaluation split is constructed from the FreshWiki corpus [19], which consists of 100 high-quality Wikipedia page with most edits for each month from February 2022 to September 2023. Both dataset includes the full text of Wikipedia pages, along with descriptions and titles. We filtered both corpus by removing articles with less than 1,000 words and those lacking any structured sections.\nFor each Wikipedia article yi, we formulated the input context xi as a one sentence prompt:"}, {"title": "", "content": "Generate a comprehensive Wikipedia page about the specified topic. topic:\n[entity]\nWe randomly sampled 1,900 items from the KILT corpus as the training split, 232 items for the validation split, and used all the 98 filtered items from the FreshWiki corpus as the evaluation split."}, {"title": "5.2 Evaluation Metrics", "content": "Offline metrics. To assess the effectiveness of the proposed method, we utilized the F1 scores of ROUGE-1 (R1), ROUGE-2 (R2), ROUGE-L (RL), and ROUGE-Lsum (RLsum) metrics for this evaluation.\nHuman and auto SxS. In addition to these ROUGE metrics, and in line with the concept of critical evaluation [29], we also conducted SxS evaluations, both by human expert raters and by an LLM evaluator. For the LLM evaluator, we employed a more capable LLM (Gemini Ultra) as an automatic SxS rater to compare the generated articles from two methods. For both human and auto SxS, raters are asked to rate which one of two generated articles is better. The base and the test sides are randomly flipped to avoid potential bias towards one side. We calculate and report the win rate and W/L ratio of our proposed method compared against the baseline."}, {"title": "5.3 Experiment setup", "content": "We use Gemini Pro model for both our baselines and all of our fine-tuning experiments. For experiments on the SciNews dataset, we limit the input sequence length to the model to be no more than around 16k tokens, and the output sequence length to be no more than 4k tokens. For experiments on the Wikipedia dataset, we limit the input sequence length to be no more than 1k tokens as the input context in this data set is much shorter. The output sequence length limit is set to 6k tokens.\nAll the fine-tuning experiments are conducted on TPU. The batch size is set to 16 and 32 respectively for SciNews and Wikipedia datasets. The maximum learning rate for all experiments is set as 10\u20135."}, {"title": "5.4 Results", "content": "Overall comparison. Table 1 shows the results of the following models and prompt settings:\n1. Zero-shot (ZS): Directly using the LLM without fine-tuning. The input context xi to the LLM is the same as the our proposed methods.\n2. Fine-tuning without intermediate steps (FT w/o I): Only fine-tune the LLM with the input context xi as input and the output article yi as output without using any intermediate steps zi."}, {"title": "", "content": "3. Fine-tuning with intermediate steps (FT w/ I): Fine-tune the LLM with the a mixture of two training tasks: generating the full article yi from the input context xi (xi \u2192 yi), and generating all the intermediate steps zi and the full article yi from the input context xi (xi \u2192 zi \u2295 yi). The instruction prompt from the two tasks are different to ensure the LLM can differentiate these tasks. See Section 4.3 for more details. During inference the LLM is prompted with input context xi to only output the full article yi (xi \u2192 yi).\n4. Fine-tuning and inference with intermediate steps (FT w/ I, Output w/ I): Similar to the fine-tuning with intermediate steps method above, except that during inference the LLM is prompted with input context xi to generate both intermediate steps zi and the full article yi (xi \u2192 zi \u2295 yi). Only the full article is extracted from the generated output for evaluation."}, {"title": "", "content": "inference tasks lead to different effectiveness. Each mixture contains one or multiple training tasks: directly generating the article from input context xi \u2192 yi; generating both the intermediate steps and the article from the input context xi \u2192 zi \u2295 yi; and generating the article from the input context and the intermediate steps: xi \u2295 zi \u2192 yi. We also test different inference tasks, including only prompting the LLM to output the entire article (xi \u2192 yi) or prompting the LLM to output both the intermediate steps and the article (xi\u2192 zi \u2295 yi)."}, {"title": "Single-turn vs. multi-turn", "content": "In addition to identifying the best recipe of training mixture and single-turn inference, we also compare the most competitive training recipe in Table 2 against multi-turn inference on SciNews. Results in Table 3 show that single-turn inference notably outperforms multi-turn inference."}, {"title": "Human and auto SxS results", "content": "To further validate the effectiveness of our methods, we conducted two sets of side-by-side ratings, one by expert human raters and one by LLM autorater, to compare the outputs of our methods against those of the baseline.\nFor the SciNews dataset, we select the FT w/I method using the training mixture of xi\u2192 yi and xi\u2192 zi \u2295 yi tasks as it demonstrated the best performance. For the Wikipedia dataset, we select the FT w/I method using the training mixture of three tasks: xi \u2192 yi, xi \u2192 zi \u2295 yi and xi \u2295 zi \u2192 yi, as it shows comparable performance to the method using only two mixtures. In both cases, the baseline is the vanilla FT w/o I method without using intermediate steps.\nFor human SxS, we asked expert human raters to rate 50 items of each data set. They were asked to compare the two outputs for each input item (\"paper body\" for SciNews, \"topic\" for Wikipedia) in three of the criteria defined in [19] that are most relevant for our task, namely coherence and organization, relevance and focus, and verifiability, and to provide an overall assessment. The results are presented in Table 4. Our methods show strong win against the baseline on SciNews, both in the overall quality and in each of the three criteria. On Wikipedia, we also observed a positive result in all criteria and overall.\nBesides Human SxS evaluation, we also employed the automated LLM SxS as additional validation, on 200 samples from SciNews and all 100 samples of FreshWiki. The results show that the FT w/I method achieves W/L ratio of 2.85 and 1.20 on SciNews and Wikipedia datasets, respectively, both larger than 1."}, {"title": "Alternative base model: Gemini 1.5 Flash", "content": "To show that our method are applicable to the latest generation of LLMs, we also conducted smaller scale experiments (due to cost limitations) on Gemini 1.5 Flash as the base model. Table 5 show the results on SciNews, which corroborate the findings on Gemini 1.0 Pro shown in Table 2."}, {"title": "6 Conclusions", "content": "In this paper, we explore to fine-tune LLM to write long-form articles in one single turn. We propose to include intermediate planning steps, such as starting with a concise summary, writing down the outlines and collecting some key information into the mixture of the training data. Noting that such intermediate steps are not available in most existing data sets, we propose to construct synthetic intermediate steps from existing full-length articles. We prompt an LLM to extract, shorten and summarize the article into a tree structure, where each level corresponds to an intermediate step. We fine-tune the LLM writer with different mixtures before prompt the LLM writer to write the full article. Our experiments on two data sets from different domains: SciNews and Wikipedia, verify that our proposed method can substantially boost the quality of the generated articles.\nOur exploration creates a new paradigm in long-form text generation: instead of applying LLM iteratively to generate intermediate steps until the full article is obtained, we include the intermediate steps into a mixture of training data and directly fine-tune an LLM to generate all of them in one turn. While we only experiment with one specific way of constructing the intermediate steps (as tree-structured planning outlines), there are many other different possible ways to create the auxiliary training mixture. As modern LLMs support longer and longer input and output sequence lengths, we believe it is possible to build even more more sophisticated long-form LLM writers with embedded capabilities other than planning. For example, one can also fine-tune the LLM to perform fact-checking reasoning on-the-fly to improve the factuality of the generated article."}, {"title": "7 Limitations", "content": "In this work we focus on writing long-form articles in a totally closed-book setup, where the only external information is provided in the input context. However, a more realistic setting would be retrieval-augmented generation (RAG), which equips the LLMs with a retriever to external knowledge. Since we do not adopt a RAG setting, we also do not compare to other existing methods that adopt RAG (e.g., STORM [19]) While we do not adopt a RAG setting, we believe our work can be easily adapted to incorporate RAG to achieve better performance."}, {"title": "8 Ethics Statement", "content": "Our approach to incorporating planning steps into LLM training aims to significantly improve the quality of long-form text generation. This advancement has the potential"}, {"title": "A Prompts", "content": "In this section we provide the prompts we used for different tasks."}, {"title": "A.1 SciNews", "content": "Only generating the full article (xi \u2192 yi). Below we provide the prompt for\nGiven the body of the academic paper, generate a whole news article. Academic paper body: {input_paper}\nGenerating the intermediate steps and the full article (xi\u2192 zi \u2295 yi). Below we provide the prompt for\nGiven the academic paper's full text, first generate a news article's summary, the high-level outline and detailed key information snippets, then leverage those information to generate a complete news article with title and body. Academic paper body: {input_paper}"}, {"title": "A.2 Wikipedia", "content": "Only generating the full article (xi \u2192 yi). Below we provide the prompt for\nGenerate a comprehensive Wikipedia page about the specified topic. Topic: {input_topic}\nGenerating the intermediate steps and the full article (xi\u2192 zi \u2295 yi). Below we provide the prompt for\nGiven a specific topic, you are asked to write a comprehensive Wikipedia page about this topic. Let's write step by step. First generate a summary, a high-level outline and a list of detailed key information snippets. Then, follow the summary, high-level outline and detailed key information snippets, generate a Wikipedia page about this topic. Topic: {input_topic}"}]}