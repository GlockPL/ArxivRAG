{"title": "Deterministic versus stochastic dynamical classifiers: opposing random adversarial attacks with noise", "authors": ["Lorenzo Chicchi", "Duccio Fanelli", "Diego Febbe", "Lorenzo Buffoni", "Lorenzo Giambagli", "Raffaele Marino", "Francesca Di Patti"], "abstract": "The Continuous-Variable Firing Rate (CVFR) model, widely used in neuroscience to describe the intertangled dynamics of excitatory biological neurons, is here trained and tested as a veritable dynamically assisted classifier. To this end the model is supplied with a set of planted attractors which are self-consistently embedded in the inter-nodes coupling matrix, via its spectral decomposition. Learning to classify amounts to sculp the basin of attraction of the imposed equilibria, directing different items towards the corresponding destination target, which reflects the class of respective pertinence. A stochastic variant of the CVFR model is also studied and found to be robust to aversarial random attacks, which corrupt the items to be classified. This remarkable finding is one of the very many surprising effects which arise when noise and dynamical attributes are made to mutually resonate.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep Neural Networks (DNNs) [1-3] are the state of the art for modern regression and classification problems in numerous fields [4-8]. Their exceptional ability in recognizing patterns and resolving hidden correlations from data places DNNs at the forefront of the most sophisticated artificial intelligence tools [9, 10]. Despite being endowed with indisputable practical effectiveness, many aspects of DNNs remain still unclear and call for further foundational analysis. Searching for novel artificial neural network paradigms proves therefore important for different reasons: on the one side, new NN models and architectures could eventually enhance over current performance when challenged against specific domains of application; on the other, innovative approaches - and associated formalism - could craft original, so far unexplored, scenarios to grasp the intimate essence of the most elusive aspects of DNNs functioning.\nAmong the existing classes of neural network models, recurrent neural networks (RNNs) [11, 12] are certainly noteworthy. RNNs have been successfully employed in several fields of research, demonstrating their irrefutable effectiveness in the widespread realm of time series analysis [13, 14] and natural language processing (NLP) [12, 15, 16]. The distinctive characteristic of RNN models is the presence of an internal state vector, that is iteratively processed along with the supplied input vector. More specifically, RNNs (i) generate an output vector and (ii) simultaneously update the internal state vector. Working in this framework, the imposed depth (i.e., the number of layers that compose sensible computing DNN architectures) mirrors the number of implemented iterations, veritable repetitions of the very same operations as stemming from an identical set of adjustable parameters. For what here relevant, with the RNNs, the concept of temporal evolution, although confined to the discrete time domain, earned hence the stage of automated machine learning.\nFurther, in [17], a new class of neural networks called Neural Ordinary Differential Equations (nODE) was proposed, in which the temporal derivative of the internal state vector of a recurrent neural network is parameterized. Working with the temporal derivative is conceptually equivalent to taking the continuous limit of a RNNs. In fact, a scheme somehow equivalent to that proposed in [17], with the explicit inclusion of a time decaying exponential term, was pioneered several years before and made popular under the name of Continuous Recurrent Neural Networks (CRNNS) [18, 19]. Algorithms spanning the nODEs typology proved very effective to handle data that extend on the time domain. A successful example is provided by a recent evolution of nODEs, called Liquid Time Constant Networks (LTCs). This latter showed an unexpected ability in tackling complex self-driving tasks by just involving a modest number of computing nodes [20].\nFrom a general standpoint, nODEs represent the first genuine example of how dynamical systems could be efficaciously used to solve regression and classification problems, with a displayed success score comparable to that reported for traditional DNNs. Moreover, having drawn an ideal bridge with the renewed field of continuous dynamical systems makes it possible, at least in principle, to leverage on a vast arsenal of techniques, developed and tested for different purposes, to cast novel light onto DNNs, in the aim of removing, at least partially, their wrapping curtain of opacity and mystery.\nA variant of nODEs targeted to classification problems, and called SA-nODEs, has been recently proposed in [21]. The model is made of N linearly coupled nodes, each bearing a continuous state variable which explores the landscape of a locally imposed double well potential. As opposed to its original counterpart, the untrained SA-nODE model is a priori constructed so as to accommodate for a set of pre-assigned stationary stable attractors. This is achieved by resorting to an ad hoc spectral decomposition of the inter-nodes coupling matrix [22-28]. The entire evaluation procedure as carried out by SA-nODEs, from the input reading to the final classification output, unfolds within a dynamical process, accessible, before and after training, for comparative analysis. In particular, the training amounts to reshaping the basins of attraction of the target destinations by ensuring that each initial condition (i.e. the input vector) evolves towards its corresponding equilibrium. In this work we elaborate further on the SA-nODE perspective [21], and report on manifold non trivial extensions of the method in regards with its original conception.\nFirst of all, we will here operate with the celebrated Continuous-Variable Firing Rate (CVFR) model [29], widely employed in computational neuroscience to reproduce in silico the orchestrated dynamics of a population of mutually entangled biological neurons. At variance with the setting discussed in [21], and as we shall later on clarify, the local reaction dynamics of the CVFR model are linear. Binary inter-nodes couplings are instead modulated by a non linear filter. Following SA-nODE approach, we have here devised a novel procedure, tailored for the problem at hand, to equip the CVFR with a set of stable attractors that become the target of the learnable dynamics. The model is then reformulated in terms of a discrete map, to be deployed on a RNN architecture. This allows in turn to perform a straightforward optimization of the residual parameters for the system to react differently to distinct classes of processed items. More specifically, input items belonging to a given category, and supplied to the dynamical model as an initial condition, will be directed towards a designed equilibrium, made stable by the training procedure. The procedure will be tested against different datasets with the sole aim of establishing a proof of principle for the adequacy of the generalized SA-nODE approach. As a byproduct of the analysis, we will show in fact that a model of biomimetic inspiration, suitable complemented with spectral attributes, can be trained to handle non trivial classification tasks, with an accuracy score comparable with state of the art DNN implementations. Incidentally, we also note that the employed CVFR model is known in the literature as the continuous Hopfield model, a reference framework for associative memory studies.\nIn the second part of this work we will move on to consider a stochastic version of the CVFR (or continuous Hopfield) model. As we shall explain, the imposed noise term is multiplicative in nature, namely it is self-consistently modulated as a function of the co-evolving state variables. More specifically, it is designed to actively perturb the underlying deterministic dynamics out of equilibrium, while being progressively silenced when the system approaches the crafted asymptotic attractors. The presence of the noise component enhances the robustness of the trained model to random adversarial attacks, as we shall prove with a dedicated campaign of numerical tests.\nThe paper is organized as follows. In Section II, the model is introduced, along with the mathematical details that define the adopted strategy for planting the asymptotic attractors. We will then discuss the deployment of the model on a RNN, review the steps that pertain to the model training and report on the classification performance for the chosen datasets (a reservoir of images of stylized letters and MINST, the celebrated dataset of images with hand written digits). Then, in Section IV, we will discuss the stochastic variant of the SA-nODE as applied to the CVFR (Hopfield) model and quantify the ability of the system to oppose adversarial attacks as a function of the inherent noisy component. Finally, in Section V, we will sum up and illustrate the conclusions of this study."}, {"title": "II. SETTING THE MATHEMATICAL MODEL: DETERMINISTIC VERSION", "content": "In this section, we introduce the deterministic version of the Continuous-Variable Firing Rate (CVFR) model. We will also discuss how to force the presence of a set of asymptotic attractors by properly crafting the matrix of internodes connections via the associated spectral attributes. Finally we shall also provide the relevant information to assess, ex post, the stability of the planted equilibria."}, {"title": "A. The CVFR (or continuous Hopfield) model", "content": "The dynamical system that we shall here train as an automatic classifier, following the SA-nODE recipe, is widely used in computational neuroscience to reproduce in silico the evolution of a population made of intertwined spiking biological neurons [29]. Consider a set of N neurons and assign to each individual unit i = 1,..., N the scalar variable xi. This latter can be assumed to represent the synaptic current of neuron i. The system is defined by the following set of ordinary differential equations:\n$\\dot{x_i} = -x_i + \\frac{1}{\\sqrt{N}} \\sum_j A_{ij} f(x_j)$ (1)\nwhere f(.) stands for a suitable non linear function and Aij are the entries of the weighted adjacency matrix A that defines the architecture of the computing network. The normalization factor 1/\u221aN is just a convenient scaling of the matrix elements, the indirect target of the training as we highlight below. In general, the non linear function f(\u00b7) is assumed to be sigmoidal in shape, so as to mimic the mechanism of neuronal activation. The image of the function can span different intervals depending on the specific range of definition as-signed to the dynamical variable xi. In this work, we will assume an activation of the Hill type, by postulating f(x\u2081) = x\u2081/(c+x\u00b2) with c\u2208 R\u207a. This choice has the sole scope of allowing for a transparent description of the procedure that we have engineered for planting the attractors. The results can be straightforwardly extended to alternative model formulations that accommodate for distinct, though biologically plausible, non linear functions. Before continuing it is worth remarking that the above Eq. (1) are a special case of the general class of models that goes under the nODE acronym. In fact, it can be cast in the form i\u2081 = g(xi,t,0), where g represents a neural network defined by parameters 0. In other words, the proposed model is a neural Ordinary Differential Equation with the time derivative of the internal state vector parameterized by a linear transformation, a nonlinearity, and an exponential decay term \u2013 a concept also found in continuous recurrent neural networks [18]. As we will discuss, the learnable parameters are (a subset of) the elements of the adjacency matrix A that we will hereafter assume to be defined via its spectral decomposition."}, {"title": "B. How to impose beforehand crafted attractors for the CVFR dynamics", "content": "We will show here how a spectral parameterization of the adjacency matrix A can be invoked to enforce suitably engineered attractors of the collective neurons dynamics.\nTo begin, let us assume that s identifies a stationary solution of Eq. (1), namely that $\\dot{s_i} = 0 \\forall i$. Further suppose (and this will be enforced later) that Vi = x/(c+x2) are the components of 4, an eigenvector of A relative to eigenvalue \u5165. In other word, we postulate that A = \u03bb\u1ff7. Then, Eq. (1) readily yield the following self-consistent cubic equation:\n$x = \\beta\\lambda \\frac{x}{c+x^2}$ (2)\nwhere \u1e9e = 1/\u221aN and which admits the solutions:\nx* = 0 \nx = $\\frac{\\beta\\lambda \\pm \\sqrt{\\beta^2\\lambda^2 - 4c}}{2}$ (3)\nHereafter, we shall denote by xp and xm the two above non trivial solutions and assume c < AN to deal with real quantities. The analysis so far developed implies that any eigenvector & of A with elements \u03c8\u2081 \u2208 {0, f(xm), f(xp)}, for i = i, ..., N and with associated eigenvalue X, is a stationary solution of the system of ODEs (1). Stated differently, we have created an alphabet of three digits, (0, xm, xp) which can be used at will to paint a virtually unlimited (bounded by N) collection of different stationary solutions Tk of the examined system. The request to be additionally met is that We are eigenvectors of matrix A relative to the same eigenvalue \u5165, as we will assume in the following. More degrees of freedom can be accommodated for by removing the degeneracy on the spectrum. This amounts to assigning a distinct eigenvalue Ak to each \u03c8k, a choice that generates different alphabet triplets (except for the null entry) for crafting the eigenvectors.\nAs previously mentioned, assume A = \u03a8\u039b\u03a8-1, where belongs to the set of real matrices RN\u00d7N and \u03a8-1 stands for its inverse. Matrix incorporates the eigenvectors of A, arranged as its columns; A, also in RN\u00d7N, is a diagonal matrix containing A's eigenvalues. The choice of decomposing the interaction matrix in reciprocal space echoes the spectral approach to machine learning, as outlined in references [22-24, 27]. To plant a priori K non linear stable attractors (where K stands for the total number of classes to be eventually categorised), it suffices to force the first K columns of I to be identically equal to Uk, with k = 1, ..., K, the eigenvectors generated according to the recipe discussed in the preceding paragraph. The corresponding eigenvalues, i.e. the first K entries of A, will be set to the same value \u5165. This latter is the eigenvalue that ultimately enters in the definition of the aforementioned alphabet, for what concerns the non trivial elements xp and xm. The unset (N \u2212 K) \u00d7 N entries of \u03a8, together with the remaining N - K eigenvalues, defines the target of the training. Acting on this latter pool of trainable parameters, we will teach to the examined dynamical system how to steer towards different imposed equilibria, depending on the characteristic of the items, supplied as an input, to be eventually classified. It is worth stressing that, at variance of the original SA-nODE formulation, the planted attractors do not coincide with the eigenstate of the coupling matrix A, due to the non linear function that modulates the interaction term. Also, the stability of the planted attractors cannot be enforced a priori, at least with the Hill type of non linearity that we have here chosen for pedagogical reasons. Other non linear functions can be nonetheless selected that will make it possible for the stability of the imposed attractors to be set before training, as we will report in a separate contribution. In the following, we will discuss the conditions that should be met for the attractors to be linearly stable and use this knowledge to certify ex post that the stability of the target destinations has been achieved, as a byproduct of the training."}, {"title": "C. On the linear stability of the imposed attractors", "content": "Let us elaborate on the conditions that underlies linear stability of a given stationary solutions of system (1). Recall that we are by definition interested in a particular class of attractors: the entries of s are just limited to the triplet (0, xm, xp) and the non linear image of the stationary solution, via f(), is an eigenvector of A relative to eigenvalue \u03bb. With these premises, we focus on the ith component of 7s and insert a small perturbation \u03b4xi as stipulated by:\nXi = xi* + \u03b4xi. (4)\nBy introducing the latter expression in Eq. (1) and expanding to the first order in dxi yields:\n\u03b4x\u1d62 = -\u03b4x\u1d62 + \u03b2\u03a3 A\u1d62\u2c7c f'(x\u2c7c)\u03b4x\u2c7c, (5)\nwhere use has been made of the definition of stationary solution. By defining the diagonal matrix F'(x) with elements [F']ij = f'(x3)dij and the matrix J = -I + BAF', where I is the identity matrix, Eq. 5 can be rewritten as\n\u03b4x = J\u03b4x. (6)\nTo assess the linear stability of the stationary solution s it is therefore sufficient to numerically compute the eigenvalues of matrix J. If all eigenvalues display a negative defined real part, the stationary state is stable. As we will show, the training process steers the model towards a regime where the defined stationary equilibria are de facto stable."}, {"title": "III. TRAINING THE DETERMINISTIC CVFR MODEL TO ACHIEVE CLASSIFICATION", "content": "Following the analysis developed above, we are in a position to discuss the training process that will transform the dynamical system into a veritable classification algorithm. We here recall that the target of the training are just the components of the matrix \u03a8, which are not associated to the embedded eigenvectors, as well as the free eigenvalues, i.e. those that are not frozen to the value \u03bb and which can be therefore self-consistently learned.\nAs the system evolves in time, the values of x get consequently updated, following the model prescription as dictated by Eq. (1). In our scheme, the updating of the state variable is performed by an Euler algorithm, implemented as a recurrent neural network. To clarify the adopted algorithmic procedure, we display in Fig. 1 a graphical portrait, adapted from [21]. Identical layers made of N nodes are linked by a linear coupling matrix A, defined by its spectral decomposition. Here, the parameters to be trained are eventually stored. Edges linking nodes implement a non linear filter, of the type discussed in the model setting. Conversely, local reactions taking place at each node/neuron yield an exponential decay. This is reflected in the linear term on the right hand side of Eq. (1). Time flows along the horizontal axis reported in Fig. 1. Nearby layers of the imposed feedforward deep architecture are separated by a finite amount in time, At << 1. By recasting in such terms the studied dynamical model allows us to address the sought optimization via standard numerical tools, as made available by the machine learning community. If At is taken small enough, the trained discrete model will behave like its continuous analogue. Other integration methods, including Runge-Kutta, can be adopted [21] at the price of a complexification of the feedforward architecture on which the dynamical system is being deployed."}, {"title": "IV. ON THE STOCHASTIC VERSION OF THE CVFR MODEL: OPPOSING ADVERSARIAL ATTACKS", "content": "Noise, be it endogenous or exogenous, can occasionally and unintuitively yields beneficial effects in several contexts of applied and fundamental relevance. Motivated by this general understanding, we here consider an extended version of the CVFR which includes a stochastic contribution and elaborate on the role played by the additional noisy component, when performing the assigned classification task.\nMore specifically, we shall consider a multiplicative noise term that shakes the deterministic formulation of the CVFR model as specified by eq. (1). The amplitude of the noise is a function of the state variable and it is designed so as to fade away when the system eventually reaches any of the planted asymptotic attractors. The reason for this choice is that it allows the same (hence deterministic) loss function to be employed when training the stochastic version of the model. The imposed noise will solely act out-of-equilibrium to provide the dynamical classification algorithm with an additional degree of flexibility, in the search for the correct destination target. The Langevin equation that defines the evolution of the noisy system is:\n$\\dot{x_i} = -x_i + \\frac{1}{\\sqrt{N}} \\sum_j A_{ij} f(x_j) + \\eta_i(t)d(x(t))$, (7)\nwhere ni(t) is a delta correlated Gaussian random variable with zero mean and standard deviation \u03c3. The amplitude factor d(x(t)) reads:\n$d(x(t)) = tanh(\\sqrt{\\Pi_{k=1}^{K} (1 - | \\langle \\psi_k | x(t) \\rangle |^2)})$. (8)\nand it is constructed so as to progressively dampen the noise term when the planted attractors are approached. In other words, the deterministic model is eventually recovered when getting close to the aforementioned attractors. In this limit the analysis developed in the preceding Section holds valid. As we will show in the following, working with the stochastic version of the model, yields trained solutions that are more robust against random perturbations of the input. The stochasticity acts as an internal regularizer, in line with previous observations [32, 33]."}, {"title": "V. CONCLUSION", "content": "In this work we have studied a variant of the SA-nODE algorithm for classification, which is based on the celebrated Continuous-Variable Firing Rate (CVFR) model [29]. This is a reference scheme employed in computational neuroscience to account for the interlaced dynamics of biological neurons. By exploiting a spectral decomposition of the coupling matrix, we achieve the goal of planting into the model a family of non linear attractors, by using a limited alphabet of digits that follows analytically from the stationarity conditions of the examined problem. The CVFR model, also known in the literature as continuous Hopfield model [34], is then trained as a classification algorithm, by forcing items belonging to distinct classes, and supplied as initial conditions to the dynamical system, to head towards the corresponding assigned attractor. As in the spirit of the SA-nODE procedure, learning (to classify) amounts to sculpt the basin of attractions of a veritable - continuous - dynamical model that, in the case here examined, bears interest for neuroscience applications. Further, in the second part of the paper, we extended the SA-nODE recipe to account for a stochastic version of the CVFR model. While the performance of deterministic and stochastic models are in line, at least for the limited class of datasets here explored, the stochastic setting proves definitely more efficient in opposing random adversarial attacks that perturb the supplied input data. This is yet another of the very many surprising and, to some extent, un-intuitive phenomena to be ascribed to noise, in its diverse and multifaceted manifestations."}]}