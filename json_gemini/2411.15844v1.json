{"title": "Unveiling the Superior Paradigm: A Comparative Study of Source-Free Domain Adaptation and Unsupervised Domain Adaptation", "authors": ["Fan Wang", "Zhongyi Han", "Xingbo Liu", "Xin Gao", "Yilong Yin"], "abstract": "In domain adaptation, there are two popular paradigms: Unsupervised Domain Adaptation (UDA), which aligns distributions using source data, and Source-Free Domain Adaptation (SFDA), which leverages pre-trained source models without accessing source data. Evaluating the superiority of UDA versus SFDA is an open and timely question with significant implications for deploying adaptive algorithms in practical applications. In this study, we demonstrate through predictive coding theory and extensive experiments on multiple benchmark datasets that SFDA generally outperforms UDA in real-world scenarios. Specifically, SFDA offers advantages in time efficiency, storage requirements, targeted learning objectives, reduced risk of negative transfer, and increased robustness against overfitting. Notably, SFDA is particularly effective in mitigating negative transfer when there are substantial distribution discrepancies between source and target domains. Additionally, we introduce a novel data-model fusion scenario, where data sharing among stakeholders varies (e.g., some provide raw data while others provide only models), and reveal that traditional UDA and SFDA methods do not fully exploit their potential in this context. To address this limitation and capitalize on the strengths of SFDA, we propose a novel weight estimation method that effectively integrates available source data into multi-SFDA (MSFDA) approaches, thereby enhancing model performance within this scenario. This work provides a thorough analysis of UDA versus SFDA and advances a practical approach to model adaptation across diverse real-world environments.", "sections": [{"title": "I. INTRODUCTION", "content": "In scenarios where the distributions of the training and testing datasets are consistent, contemporary machine learning algorithms excel in various applications, such as image recognition [1], speech recognition [2], and data mining [3]. These algorithms rely on the assumption of a stable environment to achieve high accuracy and reliability. However, when this assumption of distributional stability is violated, their performance deteriorates sharply, revealing a critical vulnerability [4], [5]. This lack of robustness in non-stationary environments highlights the necessity for developing more adaptable and resilient machine learning methods capable of maintaining performance despite distributional shifts.\nUnsupervised Domain Adaptation (UDA) effectively addresses distribution discrepancies between training (source) and testing (target) datasets. Building on Ben-David's theoretical framework [6], [7], UDA methods primarily align domain distributions to enhance target performance. Common approaches include minimizing discrepancy loss [8]\u2013[12] and adversarial learning, which trains a domain discriminator alongside a feature extractor to create domain-invariant features [4], [13]\u2013[15]. Recently, Source-Free Domain Adaptation (SFDA) has emerged as a powerful paradigm for managing distribution shifts while ensuring data privacy. SFDA adapts models to target domains without accessing source data by leveraging pre-trained source models. It typically employs techniques such as self-supervised learning [16]\u2013[19] or contrastive learning [20]\u2013[23] for adaptation.\nDetermining whether UDA or SFDA is superior is a crucial and urgent question in domain adaptation, as these paradigms have not been directly compared, creating a gap in understanding their practical strengths and limitations. Without this comparison, selecting the optimal approach for adaptable and privacy-sensitive systems is challenging, affecting system efficiency, data privacy, and adaptability in environments with restricted access to source data. Clarifying the relative advantages of UDA and SFDA is thus both theoretically important and has immediate real-world implications.\nThis paper presents a comprehensive analysis, combining theoretical insights and extensive experiments, to determine whether UDA or SFDA is better suited for practical domain adaptation tasks. Drawing on predictive coding theory [24], [25], which posits that the brain adapts to new information using existing models, we argue that SFDA aligns with this efficient, adaptive theory. SFDA leverages pre-trained source models to interpret new data, enhancing adaptability and reducing cognitive load without accessing source data. Our experiments support SFDA's theoretical advantages, demonstrating that across multiple benchmark datasets, SFDA outperforms UDA in time efficiency, memory usage, learning objectives, resistance to negative transfer, and robustness against overfitting (see Fig. 1 and 2). Specifically, SFDA converges within approximately 200 iterations, while UDA methods require 1,000 to 5,000 iterations to achieve similar performance, especially when significant distributional discrepancies exist, as seen in the Terra and DomainNet datasets. These findings validate SFDA's practical relevance and effectiveness in scenarios with substantial domain shifts.\nAdditionally, we introduce the data-model fusion scenario for the first time, addressing the complex requirements of multi-party data sharing in real-world applications. In fields like healthcare, public institutions may provide raw data while private entities can only share pre-trained models due to privacy restrictions. Traditional UDA and SFDA methods are limited as they rely solely on source data or source models, respectively, preventing full utilization of resources across multiple parties. To overcome these challenges, we propose a novel data-model fusion strategy that builds on SFDA's strengths and incorporates an advanced weight estimation method within a multi-source-free framework. This approach trains source models on available data and assigns weights based on their performance on other source domains, effectively evaluating their ability to extract domain-invariant features. Our experiments show that this method significantly outperforms existing Multi-UDA and Multi-SFDA techniques, demonstrating its effectiveness in complex data-model fusion scenarios. This strategy enhances model performance in target domains where data and models are shared among entities with diverse data-sharing constraints, providing a practical solution for real-world applications."}, {"title": "II. RELATED WORK", "content": null}, {"title": "A. Unsupervised Domain Adaptation", "content": "1) Single-source unsupervised domain adaptation (UDA):\nIt transfers knowledge from a labeled source domain to an unlabeled target domain, addressing domain shifts by aligning distributions. Foundational theories [6], [7] emphasize distribution alignment as a key to reducing discrepancies. Discrepancy-based approaches, such as DAN [5] and JAN [26], employ Maximum Mean Discrepancy (MMD) to align"}, {"title": "III. THEORY, EXPERIMENTS AND ANALYSIS: SFDA METHODS ARE BETTER THAN UDA METHODS", "content": "This section starts by detailing the preliminaries and experimental setup, including the datasets and baselines employed. It then establishes, through both predictive coding theory and empirical results, that SFDA is more suitable and performs better in real-world scenarios compared to UDA. Finally, it provides an in-depth analysis from multiple dimensions, such as time efficiency, storage capacity, learning objectives, and negative transfer, to elucidate the reasons why SFDA outperforms UDA."}, {"title": "A. Preliminaries and Experimental setup", "content": "In this paper, we denote $D_s^j = \\{x_i^s,y_i^s\\}_{i=1}^{n_{sm}}$, \u2026, $D_m = \\{x_i^m, y_i^m\\}_{i=1}^{n_{sm}}$ as the source domains, where m denotes the number of source domains and $n_{sm}$ represents the labeled samples in the corresponding source domain. It is important to note that $y \\in Y \\in R^K$ idenotes the one-hot ground-truth label, with K being the total number of classes within the label set. Furthermore, $D_t = \\{x_i^t\\}_{i=1}^{n_t}$ denotes the unsupervised target domain comprising of nt unlabeled samples, which share the same underlying label set as the source data. The objective of UDA methods is to transfer the knowledge from the source domain(s) to the target domain. In source-free domain adaptation (SFDA) scenario, we have access to the source model(s) $M^{(s)}$, which has been previously trained on the source domain(s) in a supervised manner using cross-entropy loss. Specifically, $M^s$ consists of a feature extractor and a linear classifier. The objective of SFDA methods is to facilitate the transfer of knowledge from the designated source model(s) to the unsupervised target domain.\n1) Datasets: We adopt Five benchmark datasets, including Office-31 [53], Office-Home [54], and VLCS [55] with small distribution differences, as well as DomainNet [9] and TerraIncognita [56] with significant distribution variations.\nOffice-31 serves as a conventional benchmark for domain adaptation, sourcing images from three distinct domains: Amazon (A), Webcam (W), and DSLR (D). These three domains collectively encompass 31 classes, with sample sizes of 2817, 795, and 498, respectively.\nOffice-Home is a demanding medium-sized benchmark, comprising four distinct domains: Artistic images (Ar), Clip Art (Cl), Product images (Pr), and Real-world images (Rw). Each domain comprises a total of 65 every object categories.\nVLCS is an amalgamation of Caltech101, LabelMe, PAS-CAL, and SUN09, each individually designed for object recognition, yet incorporating biases specific to their datasets. It comprises 10,729 examples from 5 classes.\nDomainNet encompasses six discrete domains, 345 cate-gories, and approximately 0.6 million images, with domains including Clipart, Infograph, Painting, Quickdraw, Real, and Sketch. To manage the broad scope of domains and sample sizes, we opt to experiment solely with the initial four domains (Clipart (C), Infograph (I), Painting (P), Quickdraw (Q)) and the top 30 classes and 100 classes within each domain in our experiments.\nTerraIncognita (Terra) comprises photographs of wild an-imals captured by cameras in various locations. Utilizing datasets $d \\in \\{L100, L38, L43, L46\\}$ as outlined in [57], the dataset consists of 24,788 examples across 10 classes."}, {"title": "B. Theory Analysis: Relationship Between Predictive Coding Theory and SFDA Paradigm", "content": "Here, we delve deeper into the relationship between pre-dictive coding theory and Source-Free Domain Adaptation (SFDA) methods. We first introduce predictive coding theory as a theoretical framework that mirrors real-world learning dynamics. We then contextualize SFDA within this framework, elucidating how it embodies predictive coding principles. Fi-nally, we analyze the theoretical advantages of SFDA derived from predictive coding, highlighting its practical significance.\nPredictive Coding Theory: A Framework for Efficient Learning. Predictive coding theory, rooted in cognitive sci-ence, offers a comprehensive explanation of how the brain pro-cesses information efficiently by generating predictions [25]. According to this theory, the brain actively constructs internal models based on prior knowledge and experience, which it uses to predict incoming sensory inputs. When actual sensory inputs deviate from predictions, the resulting prediction errors are used to update and refine these models, improving the brain's capacity to anticipate future inputs.\nThis theory is particularly relevant to real-world scenarios for several reasons. First, predictive coding emphasizes active prediction and error correction, allowing efficient and adap-tive learning by iteratively addressing discrepancies between predictions and actual inputs. Second, it supports adaptability in dynamic environments by enabling models to evolve in response to changes, ensuring robustness and flexibility in the face of variability. Third, it fosters efficient resource utilization by focusing on unexpected inputs and disregarding redundant or familiar data, thereby minimizing computational costs and reducing cognitive load. The predictive coding process involves generating predictions based on accumulated knowledge, observing actual inputs from the environment, calculating prediction errors by quantifying discrepancies be-tween predictions and observations, and refining the internal models to improve future accuracy. This iterative mechanism ensures efficiency and adaptability, key features that align closely with SFDA methods.\nMapping Predictive Coding to SFDA Methods. SFDA methods inherently embody predictive coding principles by leveraging pre-trained source models as internal representa-tions of prior knowledge. These models predict features or labels for the target domain, analogous to how the brain generates predictions from internal models. Target domain data serve as new inputs, with discrepancies between source predictions and target features quantified using metrics such as entropy or pseudo-label confidence. These errors are used to update the model, typically through self-supervised or contrastive learning techniques, aligning the source model more closely with the target domain. This process mirrors predictive coding's cycle of prediction, error correction, and model refinement, demonstrating SFDA's adherence to this efficient and adaptive learning paradigm.\nTheoretical Advantages of SFDA Based on Predictive Coding. Based on predictive coding theory, SFDA offers sev-eral theoretical advantages over UDA. Its adaptability to distri-bution shifts ensures robust performance in dynamic environ-ments with significant variability. By focusing on prediction errors and leveraging pre-trained models, SFDA minimizes computational overhead, making it well-suited for resource-constrained settings. The isolation of target-specific features reduces the risk of negative transfer, particularly in cases of severe domain shifts. Furthermore, by eliminating the need for source data during adaptation, SFDA enhances data privacy and security, making it ideal for privacy-sensitive applications like healthcare. Finally, the iterative error correction inherent in predictive coding allows SFDA to converge more rapidly than UDA, as supported by our empirical results.\nPractical Significance. By grounding SFDA in predictive coding theory, we provide a theoretical foundation for its prac-tical advantages. SFDA's ability to adapt rapidly, efficiently, and securely makes it a superior paradigm for scenarios with stringent privacy constraints, dynamic environments, or limited computational resources. This connection to predictive coding not only validates the effectiveness of SFDA but also inspires further exploration of predictive coding principles in machine learning paradigms."}, {"title": "C. Experimental Analysis", "content": "We further explore why SFDA methods outperform UDA, focusing on factors such as time efficiency, storage capacity, learning objectives, negative transfer, and overfitting.\n1) Time analysis and storage capacity analysis: Using single-source SFDA as an example, where only a pre-trained source model is provided, SFDA focuses solely on the adaptation time (T(A)) within the target domain. In contrast, UDA requires both retraining time (T(RA)) in the source domain"}, {"title": "T(A) \u00ab T(RA).", "content": null}, {"title": "T(A) \u00ab T(RA).", "content": null}, {"title": "LUDA = min E DsLs(\u03b8) + \u03bbEDtLt(\u03b8),", "content": null}, {"title": "LSFDA = min EDtLt(\u03b8, Ms),", "content": null}, {"title": "IV. PROPOSED FRAMEWORK: DATA-MODEL FUSION SCENARIO", "content": "In this section, we introduce the novel data-model fusion scenario, which we propose for the first time as a practical framework to address the limitations of traditional UDA and SFDA paradigms in real-world applications. Existing methods often struggle to accommodate diverse collaboration settings where stakeholders share resources differently-some provide raw data, while others offer pre-trained models due to privacy constraints. Our framework bridges this gap by enabling seamless integration of data and models from multiple entities, allowing for privacy-preserving domain adaptation."}, {"title": "A. Data-Model Fusion Scenario", "content": "In this subsection, we extend the traditional problem settings of UDA and SFDA to define the data-model fusion scenario, a framework that reflects real-world resource-sharing complex-ities. Let $D_s^j = \\{x_i^s,y_i^s\\}_{i=1}^{n_{sm}}$,\u2026, $D_m = \\{x_i^m, y_i^m\\}_{i=1}^{n_{sm}}$ denote the source domains, where m represents the number of source domains and $n_{sm}$ is the number of labeled samples in each domain. Similar to UDA and SFDA, $y \\in Y \\in R^K$ is the one-hot ground-truth label, where K represents the total number of classes in the label set. Additionally, $D_t = \\{x_i^t\\}_{i=1}^{n_t}$ denotes the target domain, consisting of nt unlabeled samples that share the same label set y as the source domains.\nIn the data-model fusion scenario, we introduce an addi-tional complexity: not all source domains provide labeled data directly. Specifically, some source domains contribute labeled data, denoted as $D_s = \\{x_i^{s^j},y_i^{s^j}\\}_{i=1}^{n_{s^j}}$, while others provide only pre-trained source models $M^{s_j}$, trained in a supervised manner on their respective source domains using cross-entropy loss. Each pre-trained source model Mi comprises a feature extractor and a linear classifier.\nFormally, the data-model fusion scenario includes both data-contributing source domains $\\{D_s^j\\}_{j=1}^{m_d}$ and model-contributing source domains $\\{M^{s_j}\\}_{j=1}^{m_m}$, where ma is the number of source domains contributing data, and mm is the number of source domains providing pre-trained models. The key objective of the data-model fusion scenario is to collaboratively lever-age the available labeled data from $\\{D_s^j\\}_{j=1}^{m_d}$ and the pre-trained source models $\\{M^{s_j}\\}_{j=1}^{m_m}$ to effectively adapt to the unsupervised target domain Dt, all while adhering to privacy constraints and resource-sharing limitations.\nThis setting is motivated by real-world applications where resource-sharing constraints necessitate heterogeneous contri-butions. For example, in healthcare, public institutions may provide anonymized labeled datasets for research, while pri-vate hospitals constrained by privacy regulations contribute only pre-trained models. In this context, the data-model fusion scenario aims to optimize the use of these diverse resources to improve target domain adaptation performance. Similarly, in finance and public safety, combining anonymized transaction data with pre-trained fraud detection models can enhance collaborative performance while maintaining compliance with strict privacy protocols."}, {"title": "B. Model Estimation and Adaptation Approach", "content": "To effectively address the challenges of the data-model fusion scenario, we propose the Model Estimation and Adap-tation (MEA) approach. Our strategy is grounded in the empir-ical observation that SFDA paradigms consistently outperform"}, {"title": "w = \u2211i=1A(wi)", "content": null}, {"title": "\u22111A(wi)", "content": null}, {"title": "\u03a3ni\u03b4(h(x))", "content": null}, {"title": "\u22111A(wi)", "content": null}, {"title": "Wi = \u03bb\u03c9 + \u03bb\u03c9,", "content": null}]}