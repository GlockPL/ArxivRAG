{"title": "Enhancing LLM Fine-tuning for Text-to-SQLs by SQL Quality Measurement", "authors": ["Shouvon Sarker", "Xishuang Dong", "Xiangfang Li", "Lijun Qian"], "abstract": "Text-to-SQLs enables non-expert users to effortlessly retrieve desired information from relational databases using natural language queries. While recent advancements, particularly with Large Language Models (LLMs) like GPT and T5, have shown impressive performance on large-scale benchmarks such as BIRD, current state-of-the-art (SOTA) LLM-based Text-to-SQLs models often require significant efforts to develop auxiliary tools like SQL classifiers to achieve high performance. This paper proposed a novel approach that only needs SQL Quality Measurement to enhance LLMs-based Text-to-SQLs performance. It establishes a SQL quality evaluation mechanism to assess the generated SQL queries against predefined criteria and actual database responses. This feedback loop enables continuous learning and refinement of model outputs based on both syntactic correctness and semantic accuracy. The proposed method undergoes comprehensive validation on the BIRD benchmark, assessing Execution Accuracy (EX) and Valid Efficiency Score (VES) across various Text-to-SQLs difficulty levels. Experimental results reveal competitive performance in both EX and VES compared to SOTA models like GPT4 and T5.", "sections": [{"title": "I. INTRODUCTION", "content": "Text-to-SQLs [1], [2], [3], [4] aims to convert users' queries into executable and structured SQL statements, where these queries are presented in natural languages. This technique enables non-expert users, particularly those are not familiar with the SQL language, to seamless access to knowledge stored in rational databases for applications in various areas such as finance and medicine. Nonetheless, it poses greater challenges compared to general semantic parsing tasks [2], [4]. First, even simple user queries may involve complex combinations of multiple tables and filtering requirements, necessitating sophisticated context understanding techniques to fulfill query needs. Secondly, there remains a significant gap between existing Text-to-SQL methods and real-world applications in efficiently and effectively generating high-quality SQLs, especially when dealing with large-scale databases.\nDeep learning technologies have greatly advanced Text-to-SQLs methods through developing both non-seq2seq methods and seq2seq methods classified in terms of model architectures [2], [1]. For non-seq2seq methods [5], [6], [7], they typically involve two stages: 1) it employ encoder models with attention mechanisms like BERT [8] to learn high-quality representations of user queries; 2) it constructs sketch-based or grammar-based systems to generate SQL statements. On the other hand, seq2seq methods [9], [10], [11] treat Text-to-SQLS as a machine translation task, directly translating user queries into SQL statements in an end-to-end manner. This approach has achieved competitive performance through fine-tuning with minimal effort. However, a reliable Text-to-SQLs parser has yet to be fully implemented.\nThe emergence of Large Language Models (LLMs) has recently facilitated Text-to-SQL methods by building an interface between non-expert users and relational databases, leveraging the robust linguistic and coding capabilities of LLMs. This has sparked a new trend of LLM-based Text-to-SQL approaches [3]. Specifically, the in-context learning and domain generalization abilities of LLMs have significantly improved Text-to-SQL performance. For example, DIN-SQL decomposes the Text-to-SQL task into smaller sub-tasks to enhance performance on datasets like Spider [12]. DAIL-SQL explores the selection and organization of helpful examples in prompts in few-shot scenarios through supervised fine-tuning and a systematic study of in-context learning [13]. ACT-SQL introduces a Chain-of-Thought (CoT) [14] prompt to enhance reasoning abilities in SQL generation, extending to multi-turn Text-to-SQL tasks [15]. Furthermore, to address database schema with few rows of values, the Big bench for large-scale Database (BIRD) is proposed to bridge the gap between academic study and real-world applications [4]. Text-to-SQL methods applied on BIRD have garnered significant attention [4], [16], [17], achieving high performance through additional efforts such as developing SQL classifiers [4], fine-tuning LLMs [16], or facilitating multi-agent LLM collaborations [17]. Although these research efforts have advanced LLM-based Text-to-SQL systems, they often require complex data preprocessing or data augmentation techniques.\nIn this study, we aim to enhance LLM-based Text-to-SQL by using straightforward feedback mechanisms and human-designed step-by-step prompts (HDSP). The method requires only LLMs reasoning and low-cost SQL quality assessment. The flow of the proposed method is illustrated in Figure 1. First, in the prompt engineering block, we design step-by-step prompts to LLMs for enhancing the Text-to-SQL performance. Next, we implement a feedback mechanism through SQL quality measurement. This mechanism compares generated SQL statements against expected outcomes, providing direct feedback on SQL difficulties to automatically refine the prompts. We systematically validate our approach on BIRD datasets, assessing Execution Accuracy (EX) and Valid Efficiency Score (VES) [4]. Experimental results demonstrate"}, {"title": "II. TASK FORMULATION", "content": "LLMs-based Text-to-SQL involves utilizing LLMs to convert natural language user queries into executable SQL statements. Let Q represent a natural language query and D denote the database schema, where D = (T, C') with T = {t1, ...,tm} representing multiple tables and C = {C1, ..., Cn} representing columns. The objective is to generate a SQL statement Y that is executable on D to retrieve the information required by Q. Formally, given a prompt template P(Q, D) and external knowledge evidence K, the generation process of the SQL statement Y by a large language model (LLM) M can be defined as a conditional probability distribution:\n$P_M(\\hat{Y}|P(Q, D)) = \\prod_{i=1}^{|\\hat{Y}|} P_M(\\hat{Y}_i | \\theta, P(Q, D), K, \\hat{Y}_{1:i-1})$ (1)\nIt represents the likelihood of generating SQL statement Y given the natural language query Q, the database schema D, any external knowledge evidence K, and the prompt template P. The LLM generates each token, denoted by Y\u2081 representing the i-th token of the SQL statement Y. |Y| denotes the length of the SQL statement \u0176, and \u03b8 represents the parameter of the LLM. Notably, $P_M(\\hat{Y}|P(Q, D))$ is assessed in terms of syntactic correctness and semantic accuracy, defined as execution accuracy."}, {"title": "III. METHODOLOGY", "content": "As shown in Figure 1, the proposed method include two special components, namely, human-designed step-by-step prompts and SQL quality measurement.\n\nFigure 2 presents details of the step-by-step prompts for Text-to-SQLs via LLMs. It include eight steps as below.\n1)\nLLMs are encouraged to begin by understanding the database schema. This involves identifying the available tables and their relevant columns for the user query, which is crucial for generating accurate SQL statements.\n2)\nLLMs are prompted to identify how tables are interconnected by determining the types of joins needed to connect them, essential for SQL statements involving multiple tables.\n3)\nLLMs consider the specific information to extract from each table, including which columns to select and any necessary calculations.\n4)\nLLMs pinpoint any required filters or conditions to apply to the data, such as WHERE clauses, to refine the SQL statements according to the query's requirements.\n5)\nLLMs assess whether the SQL statements involve aggregating data, potentially using GROUP BY or HAVING clauses to summarize or filter aggregated data.\n6)\nLLMs determine how to order the SQL statements, which is crucial where the order of the SQL statements matters (e.g., the top 10 results).\n7)\nLLMs consider the final presentation of the SQL statements, deciding whether to use subqueries for clarity or WITH clauses to simplify complex queries.\n8)\nLLMs synthesize the information gathered in the previous steps into coherent SQL statements.\nThese step-by-step prompts are able to ensure that the SQL statements accurately reflect the natural language requirements of the user query, where LLMs can adjust their strategies, and gradually improve SQL generation capabilities."}, {"title": "B. SQL Quality Measurement", "content": "As shown in Figure 1, it needs the quality of generated SQL to build a feedback of SQL difficulties to refine the prompts. Measuring the SQL quality includes two components: Normalized Comparison and Semantic Comparison.\n1) Normalized Comparison: It quantifies the syntactic similarity between the SQL statement \u0176 and the ground truth Y. Levenshtein distance (L(\u00b7)) is employed as the metric for evaluating their similarity, defined as the minimum number of single-character edits required to change one string into another. Thus, the normalized comparison can be defined as:\n$f_{nc}(\\hat{Y}, Y) = 1 - \\frac{L(f_{Norm}(\\hat{Y}), f_{Norm}(Y))}{max(|f_{Norm}(\\hat{Y})|, |f_{Norm}(Y)|)}$ (2)\nwhere fNorm() represents the normalization of an input.\n2) Semantic Comparison: Semantic comparison is defined as a binary indicator of whether the type of the answer \u00c2 generated through running the generated SQL statement (\u0176) is same to that of the answer (A') generated by ChatGPT directly in terms of the user query, which is defined as:\n$f_{sc}(A, A') = \\begin{cases} 1 & \\text{if } R(A) = R(A') \\\\ 0 & \\text{otherwise.} \\end{cases}$ (3)\nwhere R() is to check the type of the input.\nIn terms of the results of the normalized comparison, the feedback of SQL difficulties is defined by\n$f_{diff} (f_{nc}, N_{col}) = \\begin{cases} 0 & \\text{if } f_{nc} < t_{nc} \\text{ and } N_{col} \\leq 5 \\\\ 1 & \\text{if } f_{nc} < t_{nc} \\text{ and } 5 < N_{col} < 10 \\\\ 2 & \\text{if } f_{nc} < t_{nc} \\text{ and } N_{col} \\geq 10 \\end{cases}$ (4)\nwhere 0, 1, and 2 denote simple, moderate, and difficult SQL statements, tnc denotes a threshold of the normalized comparison, and Ncol refers to the number of columns involved in the SQL statement. Furthermore, a specific feedback to implement the prompt enhancement shown in Figure 1 is defined by\n$f_{sf}(f_{nc}, f_{sc}, f_{diff}) = \\begin{cases} 1 & \\text{if } f_{nc} \\geq t_{nc} \\text{ and } f_{sc} = 0 \\\\ & \\text{or } f_{nc} < t_{nc} \\text{ and } f_{diff} = 1 \\\\ & \\text{or } f_{nc} < t_{nc} \\text{ and } f_{diff} = 2 \\\\ 0 & \\text{otherwise} \\end{cases}$ (5)\nIf the value of fsf is 1, the step-by-step prompts are extended with five steps below.\n1)\nComplex Joins and Subqueries: LLMs are encouraged to plan the ordering of results and the combination of multiple datasets. This involves deciding on the appropriate JOIN types and conditions.\n2)\nData Transformation and Calculations: LLMs are prompted to identify any complex calculations or transformations needed for the data. This includes creating new columns based on existing data using SQL functions or handling date and time calculations.\n3)\nOptimization: LLMs review the query for performance and optimization, considering indexing and query simplification to improve execution speed.\n4)\nSecurity and Data Integrity: LLMs ensure that the query adheres to security practices and maintains data integrity, avoiding SQL injection risks by using parameterized queries or prepared statements.\n5)\nReview and Test: Before finalizing, review the query thoroughly. Test it against different scenarios to ensure it covers all edge cases and produces expected results.\nThese five steps are inserted into the step-by-step prompts between step five and step six shown in Figure 2 to further enhance Text-to-SQLs. The flow of the proposed feedback mechanism is summarized as Figure 3."}, {"title": "IV. EXPERIMENTS", "content": "BIRD (BIg Bench for Large-scale Database Grounded Text-to-SQL Evaluation)\u00b9 is a pioneering, cross-domain dataset designed to examine the impact of extensive database contents on text-to-SQL parsing [4]. BIRD includes over 12,751 unique question-SQL pairs and 95 large databases with a total size of 33.4 GB. It spans more than 37 professional domains, such as blockchain, hockey, healthcare, and education. Designed\nWe utilized a range of large language models (LLMs), such as T5 and GPT variants, each configured in unique setups to investigate various enhancement strategies, including the integration of external knowledge, prompt engineering, and feedback mechanisms. Each LLM was fine-tuned with a dataset tailored specifically to Text-to-SQL tasks, ensuring familiarity with the relevant vocabulary and structure. The models were tested under controlled conditions to precisely measure their performance in generating SQL statements."}, {"title": "C. Evaluation Metrics", "content": "To assess the performance of LLMs in generating SQL statements, two primary metrics were employed [4]:\nExecution Accuracy (EX): This metric quantifies the correctness of the generated SQL statements by comparing them to a set of predefined ground truths. A higher EX score indicates a higher similarity between the generated SQL statements and the ground truth, reflecting the model's understanding and query generation capabilities.\nValid Efficiency Score (VES): VES evaluates the efficiency of the SQL statements generated by the models, taking into account both the accuracy of the SQL statement and its execution time against the database. A higher VES signifies not only correct but also optimally efficient generation of SQL statements, which is crucial for real-time applications of autonomous systems.\nThese metrics offer a comprehensive view of the models' performance, highlighting not just the accuracy but also the efficiency of the generated SQL statements, two critical aspects in the deployment of autonomous systems across various sectors."}, {"title": "D. Result and Discussion", "content": "Table I presents a performance comparison between baseline models (T5 and GPT-3) and proposed methods based on GPT-3.5 and GPT-4, using EX and VES scores. Overall, incorporating knowledge (KG) significantly enhances the performance of LLM-based Text-to-SQL models. Additionally, when comparing the performance of LLMs using human-designed step-by-step prompts (HDSP) and feedback mechanisms, the feedback mechanism proves to be more beneficial. Furthermore, combining HDSP with the feedback mechanism allows GPT-4 to achieve the highest EX values, indicating that feedback effectively enhances the prompts within the HDSP framework shown in Figure 1. However, this combination reduces the VES values due to increased complexity in executing SQL statements, particularly with the feedback mechanism."}, {"title": "V. RELATED WORK", "content": "Text-to-SQLs, which uses natural language queries to generate SQL statements, particularly through LLMs like GPT, is a rapidly evolving field. It aims to simplify the creation of SQL statements, enabling users to retrieve information using natural language. However, current performance of LLMs in Text-to-SQLs does not yet meet application requirements compared to human performance [4]. To advance Text-to-SQL, the BIRD benchmark [4] has been proposed to assess the readiness of LLMs as database interfaces through text-to-SQL tasks. BIRD features a comprehensive dataset across multiple domains to challenge models with real-world database values and the need for external knowledge in SQL statement generation.\nBased on BIRD, several significant works have been completed to promote Text-to-SQLs. Sun et. al introduced the SQL-PaLM framework [?] through using few-shot prompting and instruction fine-tuning. In few-shot prompting, it explores the effectiveness of consistency decoding combined with execution-based error filtering. Instruction fine-tuning delves into understanding the critical paradigms that influence the performance of fine-tuned LLMs. Additionally, it proposes a test-time selection method to further refine accuracy by integrating SQL outputs from multiple paradigms with execution feedback as guidance. Pourreza et. al proposed DIN-SQL [12], which decomposes the Text-to-SQL task into four sub-steps: schema linking, complexity classification, SQL prediction, and self-correction. It also leverages few-shot prompting techniques. Experiments show that DIN-SQL not only narrows the performance gap between fine-tuned models and prompting approaches but also achieves state-of-the-art accuracy on the Spider and BIRD benchmarks. Li et. al proposed CodeS [?], a series of pre-trained language models designed specifically"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "Recent advancements in LLM-based Text-to-SQL systems have improved the user experience for non-experts in retrieving information from relational databases using natural language queries. However, current state-of-the-art (SOTA) LLM-based Text-to-SQL models often require significant effort to develop auxiliary tools like SQL classifiers for optimal performance. This paper proposes a novel approach that enhances LLM-based Text-to-SQL performance using feedback based solely on SQL Quality Measurement. The proposed method undergoes comprehensive validation on the BIRD benchmark, assessing EX and VES across various Text-to-SQL difficulty levels. Experimental results reveal competitive performance in both EX and VES compared to SOTA models like GPT-4 and T5. Future work will involve validating the proposed method with more advanced LLMs and improving VES performance by reducing the complexity of feedback mechanisms."}]}