{"title": "Large Language Models (LLMs) Assisted Wireless Network Deployment in Urban Settings", "authors": ["Nurullah Sevim", "Mostafa Ibrahim", "Sabit Ekin"], "abstract": "The advent of Large Language Models (LLMs) has revolutionized language understanding and human-like text generation, drawing interest from many other fields with this question in mind: What else are the LLMs capable of? Despite their widespread adoption, ongoing research continues to explore new ways to integrate LLMs into diverse systems.\nThis paper explores new techniques to harness the power of LLMs for 6G (6th Generation) wireless communication tech-nologies, a domain where automation and intelligent systems are pivotal. The inherent adaptability of LLMs to domain-specific tasks positions them as prime candidates for enhancing wireless systems in the 6G landscape.\nWe introduce a novel Reinforcement Learning (RL) based framework that leverages LLMs for network deployment in wireless communications. Our approach involves training an RL agent, utilizing LLMs as its core, in an urban setting to maximize coverage. The agent's objective is to navigate the complexities of urban environments and identify the network parameters for optimal area coverage. Additionally, we integrate LLMs with Convolutional Neural Networks (CNNs) to capitalize on their strengths while mitigating their limitations. The Deep Deterministic Policy Gradient (DDPG) algorithm is employed for training purposes. The results suggest that LLM-assisted models can outperform CNN-based models in some cases while performing at least as well in others.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) like GPT [1], BERT [2], and their successors have significantly impacted Natural Lan-guage Processing (NLP) by excelling across various language understanding and generation benchmarks. The transfer learn-ing capabilities of LLMs facilitate easy adaptation to various fields at minimal cost. This adaptability involves fine-tuning pre-trained models like DistillBERT [3] on specific datasets, allowing the models to efficiently apply their broad knowledge to new tasks. This process has not only demonstrated the effectiveness of LLMs but also their flexibility across different domains, encouraging researchers to explore and utilize LLM-based solutions extensively.\nWireless communications is another field that can utilize LLMs extensively in its applications. In the era of 6G, time variations in wireless links makes it essential to have decision-makers that are quick, strong, and dependable. Using LLMs to automate network systems presents a practical approach to develop these types of decision-makers.\nWireless network deployment contains many challenges, including the design and maintenance of robust connections, efficient routing in dynamic environments, and energy opti-mization [4]. Especially, some ad hoc wireless networks with-out fixed infrastructure need smart design solutions to ensure a consistent and high-quality wireless links. The dynamic nature of these networks can also require adaptive planning that can manage mobility and variable link quality [5]. Besides, energy efficiency is critical, especially in battery-powered networks, where there is a trade-off between the life span of the network and network performance.\nIn network deployment scenarios, two main concepts to con-sider are the path loss and shadowing effect. The wireless sys-tems are typically engineered with a target minimum received power $P_{min}$, which is crucial for acceptable performance. For instance, failing to meet this threshold in cellular networks can cause significant degradation in received signal thus lower data rates. Due to shadowing, the received power at any distance from the transmitter is log-normally distributed, posing a probability of falling below $P_{min}$. Thus, the effects of path loss and shadowing necessitate careful planning and deployment strategies for wireless networks to establish consistent and high-quality communication links, hence minimizing coverage holes.\nSelf-organizing networks, a well-studied topic in the lit-erature [6], introduces a way for managing and optimiz-ing network performance autonomously. Exploiting previous experiences, such networks can autonomously arrange their location and network configurations to increase service qual-ity [7]. These networks utilize algorithms and protocols to dynamically adjust to changes in the environment aiming to increase efficiency, reliability, and scalability [8]. Self-organizing networks are presented as a suitable candidate for building ad hoc networks with dynamic structure and low-powered nodes [9].\nIn this paper, we introduce a novel approach that integrates LLMs into the process of wireless network deployment, where we use Reinforcement Learning (RL) to train an LLM-assisted agent. The objective is to determine the optimal location and orientation for a base station using the LLM-assisted RL agent in an urban setting to maximize the coverage. To achieve this, we employ an advanced actor-critic RL methodology known"}, {"title": "II. PRELIMINARIES", "content": "Path loss is the attenuation a signal experiences as it propagates through space. It is primarily due to the spreading of the wavefront, which causes the signal power to decrease with distance. The free-space path loss (FSPL) model is given by in terms of decibels (dB):\n$FSPL (dB) = 20 log_{10}(d) + 20 log_{10}(f) + 20 log_{10}(\\frac{4 \\pi}{c}),$\nwhere d is the distance between the transmitter and receiver in meters, f is the frequency of the signal in Hertz, c is the speed of light ($3 \\times 10^8$ m/s).\nShadowing, or slow fading, occurs when obstacles in the environment, such as buildings or trees, block the direct signal path between the transmitter and receiver. It is modeled using a log-normal distribution, where the signal power in dBm is normally distributed around the mean path loss with a standard deviation dependent on the environment. The model can be expressed as:\n$P_r(dBm) = P_t(dBm) - PL(dB) - X_\\sigma(dB),$\nwhere $P_r(dBm)$ is the received power in dBm, $P_t(dBm)$ is the transmitted power in dBm, $PL(dB)$ is the path loss in dB calculated using an appropriate path loss model, $X(dB)$ is a zero-mean Gaussian random variable with standard deviation \u03c3, representing shadowing effects.\nThe DDPG algorithm is an actor-critic method tailored for continuous action spaces using deep learning. Introduced by [10], DDPG blends policy gradient and Q-learning methods to manage high-dimensional state information from the environ-ment. It consists of two primary components: an actor, which determines actions based on the current state, and a critic, which evaluates these actions by predicting their potential rewards.\nDDPG stabilizes training through experience replay and target networks for both actor and critic. Experience replay mitigates temporal correlation by storing transitions (state, action, reward, next state) in a buffer and sampling randomly for training, which enhances learning efficiency. Target net-works provide stable, consistent targets during value updates, contributing to the overall stability of the training process. DDPG is notable for its efficacy in continuous action spaces, applicable in fields like robotics and complex video games where actions cannot be discretized effectively. This attribute is crucial for our experiments requiring continuous responses from the agent.\nThe critic network computes the action-value function $Q(s, a)$, indicating the expected return for an action a in state s, following the current policy. The critic updates by minimizing the loss:\n$L(\\theta^Q) = \\mathbb{E}_{s,a,r,s'} [(Q(s, a|\\theta^Q) - y)^2],$\nwhere $y = r + \\gamma Q(s', \\mu'(s'|\\theta^{\\mu'}))|\\theta^{\\overline{Q}})$, with Q as the critic's target network action-value function, $\\theta$ as the network weights, $\\mu'$ as the actor's target network decision, r as the reward, $\\gamma$ as the discount factor, and s' as the subsequent state.\nThe actor network updates its policy using the gradient of the expected return from the initial state distribution J concerning the actor parameters $\\theta^{\\mu}$:\n$\\nabla_{\\theta^{\\mu}} J \\approx \\mathbb{E}_s [\\nabla_a Q(s, a|\\theta^Q)|_{a=\\mu(s)} \\nabla_{\\theta^{\\mu}}\\mu(s|\\theta^{\\mu})].$"}, {"title": "III. METHODOLOGY", "content": "To deploy LLMs in urban wireless communication settings, we explore various scenarios to strategically position base stations, aiming to maximize signal strength for users at specific locations. Rather than engaging in complex mathemat-ical analysis of the environment's electromagnetic properties, we leverage LLMs to interpret these characteristics through a learning process. Sionna's ray tracing technology, which simulates how light interacts with objects to calculate effects like reflections and shadows, supports this.\nWe use this approach to identify optimal locations and orientations for base stations in urban areas using multiple scenarios. Additionally, we have implemented a fine-tuning process for the pre-trained DistilBERT model using the DDPG actor-critic Reinforcement Learning algorithm. In this setup, the LLM functions as a decision-making agent, optimizing signal strength at targeted locations based on specifically designed prompts.\nBefore explaining the details of the algorithms and how we use the LLMs as decision maker, we first need to understand the utilized environment and the considered scenarios. We utilize the Ray Tracing module of Sionna [12] library to setup the environment for our experiments. To fully understand how this module works and how it simulates wireless environments,"}, {"title": "A. System Settings", "content": null}, {"title": "B. Actor-Critic Network Settings", "content": "Fine-tuning is a crucial process to adapt LLMs to work on different tasks effectively. Here, we use DDPG algorithm to fine-tune the LLM. In DDPG, the function approximators in actor and critic are comprise of deep neural networks models, where we used several different architectures.\nThe critic network uses a coverage map, a 1206 by 1476 matrix, where each element measures received signal strength"}, {"title": "IV. EXPERIMENTS & RESULTS", "content": "We trained three actor configurations and the critic network using the DDPG algorithm across three scenarios, with setups illustrated in Figs. 6 and 5. Key training parameters included a learning rate of 5 \u00d7 10-4 for actors and 5 \u00d7 10-3 for the critic, a target network update rate of 10-3, a batch size of 8, and a hidden layer size of 256 for all networks. Training was conducted on a GeForce RTX 4090 GPU, completing 1000 steps for each model and monitoring convergence.\nA total of 1000 steps are taken for the trainings of all models and the convergence of the models are examined. Every 100 training steps, we paused to perform 20 evaluation steps, during which neural network parameters were frozen"}, {"title": "V. CONCLUSION", "content": "In this paper, we suggested a novel methodology to exploit LLMs for wireless network deployment task. We argue that our contribution is a significant milestone in integrating the capabilities of LLMs into the field of wireless communica-tions, given that research in this area is notably limited and existing studies often lack a robust methodology accompanied by realistic experiments.\nOur results suggest that the methodology we proposed is successful to utilize the LLMs' power in the devised problem as LLM-assisted models outperformed the model that does not use LLMs where even performing on par would have been a significant outcome considering the sparsity of studies on using LLMs in wireless domain. We believe proposing such a method with the successful results highlights the under-explored potential of using LLMs in the wireless communica-tions domain."}]}