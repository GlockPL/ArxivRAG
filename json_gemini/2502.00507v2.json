{"title": "A statistically consistent measure of Semantic Variability using Language Models", "authors": ["Yi Liu"], "abstract": "To address the challenge of variability in the output generated by language models, we introduce a measure of semantic variability that remains statistically consistent under mild assumptions. This measure, termed semantic spectral entropy, is an easily implementable algorithm that requires only standard, pre-trained language models. Our approach imposes minimal restrictions on the choice of language models, and through rigorous simulation studies, we demonstrate that this method can produce an accurate and reliable metric despite the inherent randomness in language model outputs.", "sections": [{"title": "1 Introduction", "content": "The birth of Large Language Models (LLM) has given rise to the possibility of a wide range of industry applications (Touvron et al., 2023; Chowdhery et al., 2023). One of the key applications of generative models that has garnered significant interest is the development of specialized chatbots with domain-specific expertise such as legal and healthcare (Lexis; Mesko, 2023). These applications illustrate how generative models can improve decision-making and improve the efficiency of professional services in specialized fields.\nThis new LLM capability is made possible by the strong understanding of generative capabilities of the models (Liu et al., 2023; Long, 2023) and the advent of Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Gao et al., 2023). In an RAG system, the user interacts by submitting queries, which trigger a search for relevant documents within a pre-established database. These pertinent documents are retrieved based on the query and serve as a context for the LLM to generate an appropriate response. Since the implementation of RAG does not require a custom-trained LLM, it offers a cost-effective solution. The resulting chatbot can perform tasks traditionally handled by domain experts, improving operational efficiency and driving cost reductions.\nHowever, a critical challenge impeding the widespread deployment of generative models in industry is the inherent variability present in these models (Amodei et al., 2016; Hendrycks et al., 2021). Although parameters such as temperature, top-k, top-p, and repetition penalty are known to significantly influence model performance (Wang et al., 2020, 2023; Song et al., 2024), even when these parameters are tuned to achieve deterministic output (e.g. setting temperature to 0 or top-p to 1), differences in the generated results can still occur in multiple runs. This persistent variability poses a significant barrier to the reliable and consistent application of generative models in practical settings.\nAtil et al. (2024) conducted a series of experiments involving six deterministically configured large language models (LLMs), with temperature set to 0 and top p set to 1, across eight common tasks and five identical trials per task. The study aimed to assess the repeatability of model outputs by examining whether the generated strings were consistent between runs. The authors found that none of the LLMs demonstrated consistent performance in terms of generating identical outputs (Atil et al., 2024). However, the authors noted that when accounting for syntactical variations, the observed differences were relatively minor as many of the generated strings were semantically equivalent.\nThe variability in output has been attributed to the use of GPUs in large language model (LLM) inference processes, where premature rounding during computations can lead to discrepancies (Nvidia, 2024; Atil et al., 2024). Given this, it is reasonable to conclude that complete elimination of variability is unfeasible in any empirical setting. Consequently, we must acknowledge that the output of LLMs is inherently uncertain. In light of this, it"}, {"title": "2 Semantic spectral entropy", "content": "becomes essential, similar to practices in statistics, to assess and quantify the level of uncertainty in the text generated by LLMs for any given scenario.\nMost prior studies on uncertainty in foundation models for natural language processing (NLP) have focused primarily on the calibration of classifiers and text regressors (Jiang et al., 2021; Desai and Durrett, 2020; Glushkova et al., 2021). Other research has addressed uncertainty by prompting models to evaluate their own outputs or fine-tuning generative models to predict their own uncertainty (Lin et al., 2024; Kadavath et al., 2022). However, these approaches require additional training and supervision, making them difficult to reproduce, costly to implement, and sensitive to distributional shifts.\nOur work follows from a line of work inline with the concept of semantic entropy proposed in (Kuhn et al., 2023; Nikitin et al., 2024; Duan et al., 2024; Lin et al., 2023). (Kuhn et al., 2023) explore the entropy of the generated text by assigning semantic equivalence to the pairs of text and subsequently estimating the entropy. Similarly, (Nikitin et al., 2024) and (Lin et al., 2023) utilize graphical spectral analysis to enhance empirical results. However, a notable limitation in the entropy estimators proposed by (Kuhn et al., 2023) and (Nikitin et al., 2024) is their reliance on token likelihoods when assessing semantic equivalence, which may not always be accessible. Furthermore, (Kuhn et al., 2023) acknowledge that the clustering process employed in their framework is susceptible to the order of comparisons, introducing variability into the results.\nMoreover, previous work focuses on the empirical performance of the estimator. As such, while these methods have demonstrated favorable empirical outcomes, to the best of our knowledge, no authors have established using a theoretical analysis that their entropy estimators converge to a true entropy value as the sample size increases under an underlying generative model. Exploring the theoretical properties allows us to have a clear understanding of how the number of clusters and size of data would affect the estimator.\nOur approach seeks to address these limitations by developing a robust theoretical analysis of the clustering procedure, ensuring convergence properties, and mitigating the variability inherent in prior methodologies. We propose a theoretically analyzable metric for quantifying the variation within a collection of texts, which we refer to as semantic spectral entropy. This measure addresses the observation that many generated strings, while lexically and syntactically distinct, may convey equivalent semantic content. To identify these semantic equivalences, we advocate the use of off-the-shelf generative language models (LMs). Moreover, we acknowledge that the LM used to evaluate semantic similarity is itself a stochastic generator. In response, we employ the well-established technique of spectral clustering, which is provably consistent under minimal assumptions on the generator, thereby ensuring the robustness and reliability of the proposed metric. Specifically, we demonstrate that the measure is statistically consistent under a weak assumption on the LM. To the best of our knowledge, this is the first semantic variability measure with proven convergence properties. As an empirical evaluation studies, we also propose a simple method for constructing clusters of different lexically and syntactically distinct but semantically equivalent text using compound propositions from (Wittgenstein, 2023)."}, {"title": "2.1 Semantic entropy", "content": "We begin with a collection of textual pieces n, denoted $T = (t_1,\\ldots,t_n)$. Unlike that in (Kuhn et al., 2023), our assumption is that we have access only to T. In fact, we do not require the existence of a generative model and is interested only in variability of the semantics in the text. To evaluate the semantic variability of these texts in the context of a specific use case, we propose a theoretically proven measure of semantic entropy which we named semantic spectral entropy.\nA key reason for opting against the use of variance as a measure of variability is that computing variance requires the definition of a mean, which is challenging to establish for semantic distributions. Although it is possible to define an arbitrary reference point, such as a standard answer in a chatbot that answers questions, evaluating the variability with respect to such a reference introduces bias.\nIn contrast, entropy is a well-established measure of variation, particularly for multinomial distributions. For a distribution P(t) over a set of semantic clusters ${C_1,\\ldots,C_k}$, the entropy $\\mathcal{E}$ is defined as:\n$\\mathcal{E}(t) = -\\sum_i p(t\\in C_i) \\log p(t\\in C_i)$.                                                                    (1)"}, {"title": "2.2 Spectral clustering", "content": "This formulation captures the uncertainty or disorder associated with assigning a given text t to one of the clusters. Consequently, it provides a quantitative measure of semantic variability that avoids the biases introduced by arbitrary reference points.\nTo estimate the entropy for a given data set $t_1,..., t_n$, we first calculate the number of occurrences of each text $t_i$ in each group $C_j$. This is achieved by computing:\n$n_j = \\sum_{i=1}^{n} I(t_i \\in C_j)$,\nwhere $I(t_i \\in C_j)$ is an indicator function that equals 1 if $t_i$ belongs to the cluster $C_j$, and 0 otherwise.\nNext, the true probability $p(t \\in C_j)$ is approximated using the empirical distribution:\n$\\hat{p}(t \\in C_j) = \\frac{n_j}{n}$,\nwhich represents the fraction of texts assigned to cluster $C_j$. Using this empirical distribution, the empirical entropy is defined as:\n$\\hat{\\mathcal{E}}(T) = -\\sum_{j} \\hat{p}(t \\in C_j) \\log \\hat{p}(t \\in C_j)$.\nThis measure provides a practical estimation of semantic entropy based on observed data.\nOne critical step in this process is clustering the texts $t_i$ into disjoint groups. To do so, it is sufficient to define a relationship between $t_i \\sim t_j$, such that they satisfy the properties of equivalence relation. Specifically, one needs to demonstrate\n1.  Reflexivity: For every $t_i$, we have $t_i \\sim t_i$, meaning that any text is equivalent to itself.\n2.  Symmetry: If $t_i \\sim t_j$, then $t_j \\sim t_i$, meaning that equivalence is bidirectional.\n3.  Transitivity: If $t_i \\sim t_j$ and $t_j \\sim t_k$, then $t_i \\sim t_k$, which means that equivalence is transitive.\nIt turns out the existence of an equivalence equation is both a necessary and sufficient condition for a definition of a breakdown of T into disjoint clusters (Liebeck, 2018). In light of this, defining $\\sim$ should be based on the linguist properties of entropy measurement.\nDirect string comparison, defined as $t_i \\sim t_j$ if and only if $t_i$ and $t_j$ share identical characters, reflects lexicon equality and constitutes an"}, {"title": "2.3 Full algorithm and implementation", "content": "equivalence relation. However, this criterion is overly restrictive. In a question-and-response context, a more appropriate equivalence relation might be defined as $t_i \\sim t_j$ if and only if $t_i$ and $t_j$ yield identical scores when evaluated by a language model (LM) prompt. This criterion, however, requires an answer statement as a point of reference. We are more interested in a stand-alone metric that can capture the semantic equivalence. For example, consider the sentences $t_1$ = \"Water is vital to human survival\" and $t_2$ = \"Humans must have water to survive\". Despite differences in language, both sentences convey the same underlying meaning.\nTo address such challenges, (Kuhn et al., 2023; Nikitin et al., 2024) propose an equivalence relation wherein $t_i \\sim t_j$ if and only if $t_i$ is true if and only if $t_j$ is true. This formulation ensures that two texts, $t_i$ and $t_j$, belong to the same equivalence class if they are logically equivalent. This broader definition allows for greater flexibility and applicability in assessing semantic equivalence beyond superficial lexical similarity.(Copi et al., 2016). We will present their argument as a proposition where we will put the verification in the appendix\nProposition 2.1. The relation $t_i \\sim t_j$ if \"$t_i$ is true if and only if $t_j$ is true\" is an equivalence relation.\nIn light of the fact that equivalence relations can be defined arbitrarily based on the needs of the user. We propose that the determination of equivalence relations, denoted as $\\sim$, is performed through a LM that generates responses independently of the specific generation of terms $t_1, . . . , t_n$. However, we do not assume that we have access to probability distribution of the tokens as proposed by (Kuhn et al., 2023; Nikitin et al., 2024) which is not always available. Rather, we just require a generator LM which can generate a determination of this relationship. Therefore, this LM can be general generative language model with a crafted prompt which we will use in our simulation studies. The error in this LM will be removed in the spectral clustering algorithm at the later stage. By leveraging this LM, we can define a function $e : T, T \\rightarrow 0, 1$, which is formally expressed as follows:\n$e(t_i, t_j) = \\begin{cases} 1 & \\text{if } t_i \\sim t_j,\\\\ 0 & \\text{otherwise}.\\end{cases}$                                          (2)\nHowever, since the function relies on an LM, $e(t_i, t_j)$ can be viewed as a Bernoulli random variable,"}, {"title": "3 Theoretical Results", "content": "We merge the process of finding sermantic entropy with spectral clustering to present the full algorithm as Algorithm 1: Sermantic Spectral Entropy.\nAlgorithm 1 Sermantic Spectral Entropy\nBegin with T = {t1,\u2026tn}\nfor i, j\u2208 {1,\u2026\u2026n} \u00d7 {1,\u2026\u2026 n}, i \u2260 j do\nUse LLM to compute Ei,j = e(ti, tj).\nend for\nFind the Laplacian of E, L =D-E\nCompute the first K eigenvectors u1, ..., uk of L and the top K eigenvalues 11,\u2026\u2026 \u03bb\u03ba.\nLet \u00db \u2208 Rnxk be the matrix containing the vectors u1,..., uk as columns.\nUse (1 + \u20ac) K-means clustering algorithm to cluster the rows of U\nLet gij be an (1 + \u20ac)-approximate solution to a K-means clustering algorithm\nCompute (T) using gij\nThis polynomial-time algorithm is characterized by the largest computational cost associated with the determination of Eij. However, computing Eij is embarrassingly parallel, meaning that it can be efficiently distributed across multiple processing"}, {"title": "3.1 Performance of spectral clustering algorithms", "content": "units. Furthermore, there are well-established implementation, such as Microsoft Azure's Prompt-Flow (Esposito, 2024) and LangChain (Mavroudis, 2024) that facilitate the implementation of parallel workflows, making it feasible to deploy such parallelized tasks with relative ease."}, {"title": "2.4 Finding K", "content": "A notable limitation of this analysis is the unavailability of K in the direct computation of semantic spectral entropy. However, the determination of K for stochastic block model has been well studied (Lei, 2016; Wang and Bickel, 2017; Chen and Lei, 2018). We will describe the cross-validation approach (Chen and Lei, 2018) in detail. The principle behind cross-validation involves predicting the probabilities associated with inter-group connections (p) and intra-group connections (q). If the estimated value of K is too small, it fails to accurately recover the true underlying probabilities; conversely, if K is too large, it leads to overfitting to noisy data. This approach has the potential to recover the true cluster size under relatively mild conditions."}, {"title": "3 Theoretical Results", "content": "Our theoretical analysis involves a proof that the estimator is strongly consistent, i.e. the estimator converges to true value almost surely, and an analysis of its rate with respect to the number of cluster \u039a.\nWe divide our analysis into two subsections. The first subsection examines a fixed set of T = t1,..., tn, which is assumed to exhibit some inherent clusters C1,..., CK. Under the assumption of perfect knowledge of these clusters, the empirical entropy & can be determined. The primary focus in this subsection is on the performance of spectral clustering algorithms. The second subsection explores a scenario in which there exists an underlying generative mechanism that allows for the infinite generation of $t_i$. In this case, we permit K to increase with n, though at a significantly slower rate. This scenario is particularly relevant for evaluating the performance of RAG in the context of continuous generation of results in response to a given query."}, {"title": "3.1 Performance of spectral clustering algorithms", "content": "We model the LM determination of $e(t_i, t_j)$ as a random variable, as described in Equations"}, {"title": "3.2 Performance under a generative model", "content": "3 and 4. In the theoretical analysis presented here, we assume that the number of clusters, K, is known and fixed. To derive various results, we first establish the relationship between the difference $\\mid \\bar{E}(T) - \\hat{E}(T)\\mid$ and the miscluster error, denoted Merror.\nLemma 3.1. Suppose that there exists $0 < C_2 < 1$ such that $2Kn_{min}/n \\geq C_2$,\n$\\mid \\bar{E}(T) - \\hat{E}(T)\\mid \\leq h \\Big(\\frac{2K}{C_2}\\Big) \\frac{1}{n}\\Big(M_{error}\\Big)$                                                                                                                                                              (6)\nwhere h(x) = (x + log (x)).\nThe proof is presented in the Appendix section A.2.1. We begin by presenting the result of strong consistency for the spectral clustering algorithm.\nTheorem 3.2. Under regularity conditions, the estimated entropy empirical entropy $\\hat{E}(T)$ is strongly consistent with the empirical entropy, i.e.\n$\\mid \\bar{E}(T) - \\hat{E}(T)\\mid \\rightarrow 0 \\text{ almost surely}$                                                                                                                                                                                                (7)\nThe proof is provided in the Appendix section A.2. This establishes strong consistency result that we aim to present. At the same time, we also want to show the finite sample properties of the estimator $\\hat{E}(T)$.\nTheorem 3.3. If there exists $0 < c_2 \\leq 1$ and $\\lambda > 0$ such that $2Kn_{min}/n > c_2$, and $p = a_n = a_n(q+\\lambda)$, where $a_n \\geq \\log(n)$ then with probability at least $1 - \\frac{1}{n}$,\n$\\mid \\bar{E}(T) - \\hat{E}(T)\\mid \\leq h \\Big(\\frac{2K}{C_2}\\Big) \\frac{N_{max}}{n}\\Big(\\frac{4c a n_{min} a_n}{K^2}\\Big)$                                                                                                                                                                                                     (8)\nwhere h(x) = (x + log (x)), $n_{max} = \\max_j\\{n_j : j = 1,...K\\}$, and $n_{min} = \\min_j\\{n_j : j = 1,...K\\}$.\nThe full proof is provided in the appendix section A.3. A brief outline of the proof is as follows: we begin by using the results from (Lei and Rinaldo, 2015), which establish the rate of convergence for the stochastic block model. Next, we relate the errors of the spectral clustering algorithm to the errors in the empirical entropy, using the lemma 3.1 to establish this connection.\nRemark. This result is particularly relevant for computing semantic entropy, as the output generated by LMs is produced with a probability that is independent of n. As a result, we have $a_n = O(1)$. Assuming balanced community sizes, the convergence rate is therefore O(1). This is formally stated in the following corollary:"}, {"title": "3.2 Performance under a generative model", "content": "Corollary 3.3.1. If there exists a constant $0 < C_2 \\leq 1$ such that $2Kn_{min}/n \\geq c_2$ and alpha > 0, then there exists a constant a such that with probability at least $1 - \\frac{1}{n}$,\n$\\mid \\bar{E}(T) - \\hat{E}(T)\\mid \\leq h \\Big(\\frac{2K}{C_2}\\Big) \\frac{1}{n}a_n$                                                                                                                                                                                                                                                                                       (9)\nThe proof of this result is provided in the Appendix section A.3.1.\nRemark. In particular, we observe that the convergence rate is O (1). This means that the error associated with spectral clustering is small, and our estimated entropy converges to the empirically entropy quickly.\nIn practical terms, we assume the presence of a generator, specifically an RAG, that produces identically distributed independent random variables $t_i'$ that collectively form semantic clusters $C_1 ...C_K$. In essence, we have $t_i \\sim G$ such that $t_i \\in C_j$ with probability p(Cj). In this model, there is a true value of entropy E(T) given in Equation 1, and we want to find the convergence rate of our method.\nTheorem 3.4. If there exists a constant a such that $p = a = a(q + \\lambda)$, then with probability at least $1 - \\frac{1}{n}$,\n$\\mid E - \\hat{E}\\mid \\leq h(\\frac{1}{p_{min}}) K\\sqrt{\\log (2Kn)}$                                                                                                                                                                  (10)\nwhere $m(n) = (1 - \\sqrt{2\\log(n.K)/np_{min}})$ and $p_{min} = \\min\\{p(C_1)...p(C_K)\\}$.\nMost of the material used for this proof is presented in Corollary 3.3.1.\nProof. Consider the following equality\n$\\mid E - \\hat{E}\\mid < \\mid E - \\mathcal{E} + \\mathcal{E} - \\hat{E}\\mid < \\mid E - \\mathcal{E}\\mid + \\mid \\mathcal{E} - \\hat{E}\\mid$\nWe know that there are three sufficient conditions for Equation 10. These are"}, {"title": "3.3 Discussion on K", "content": "C1: | E - E| \\leq h \\Big(\\frac{1}{Pmin}\\Big) K\\sqrt{\\log (2Kn)},\nC2: c2 such that $0 < \\frac{2}{4 \\mathcal{4}^{3}\\sqrt{1-C_2}}$.\nFinally, the assumption that $\\frac{1}{d} = 4 \\log(d) \\bigO(d)$.\nThe theorem holds.\nProof. Since the assumption that \\bigO(d) = \\frac{1}{d^{4} \\log(d)}$, we can replace the assumption with $\\bigO(\\frac{1}{2})$.\nThen, according to the assumptions, and by\n$\\frac{1}{4d^{2} \\sqrt{\\log(\\frac{1}{d})}} < \\frac{1}{d}$ is equivalent to the assumption that $\\sqrt{C_0} < \\frac{1}{4 \\sqrt{\\log(\\frac{1}{C_0})}}$ or we have\nThe assumption that $\\sqrt{C_0} < \\frac{1}{4 \\sqrt{\\log(\\frac{1}{C_0})}}$ we are equivalent to assumption.\nThe final solution of Equation 7.17 yields\n$\\frac{1}{16 C_0} < \\log(\\frac{1}{C_0})$\nSince \\bigO(m) = \\bigO(\\sqrt{m}) \\text{ in the Equation 8.7}$, for \\text{ any constant number, we can always } \\lim_{m \\to \\frac{1}{16 C_0}} < \\sqrt{m}$\nThe original assumpution can be assumed."}, {"title": "4 Simulation and data studies", "content": "2 <h(x, y, l) = \\lim_{m \\to \\frac{1}{16 C_0}} + \\sqrt{m}$, for any constant number, we can always have the $\\frac{1}{16 C_0} + \\sqrt{m}$. The final solution of Equation 7.17 yields\n$\\frac{1}{16 C_0} < \\log(\\frac{1}{C_0})$\nSince \\bigO(m) = \\bigO(\\sqrt{m}) \\text{ in the Equation 8.7}$, for \\text{ any constant number, we can always } \\lim_{m \\to \\frac{1}{16 C_0}} < \\sqrt{m}$\nThe original assumpution can be assumed."}, {"title": "5 Discussion", "content": "As this paper focuses more on the theoretical analysis of semantic spectral entropy with respect to variable n and K, we decide against using the evaluation method proposed in (Kuhn et al., 2023; Duan et al., 2024; Lin et al., 2023) in favor of constructing a simulation where we know the true entropy E. This allows us to better analyze how $\\bar{E} - \\hat{E}$ changes with choice of generator e, K and Nmin.\nTo construct a non-trivial simulation for this use case, we evaluate the performance of our algorithms within the context of an unordered set of"}, {"title": "6 Limitation", "content": "Many natural language processing tasks exhibit a fundamental invariance: sequences of distinct tokens can convey identical meanings. This paper introduces a theoretically grounded metric for quantifying semantic variation, referred to as semantic spectral clustering. This approach reframes the challenge of measuring semantic variation as a prompt-engineering problem, which can be applied to any large language model (LLM), as demonstrated through our simulation analysis. In addition, unsupervised uncertainty can offer a solution to the issue identified in prior research, where supervised uncertainty measures face challenges in handling distributional shifts.\nWhile we define two texts as having equivalent meaning if and only if they mutually imply one another, alternative definitions may be appropriate for specific use cases. For example, legal documents could be clustered based on the adoption of similar legal strategies, with documents grouped together if they demonstrate comparable approaches. In such scenarios, the entropy of the legal documents could also be computed to quantify their informational diversity. We have demonstrated that, provided there exists a function e capable of performing the evaluation with weak accuracy, this estimator remains consistent. Given the reasoning capabilities of large language models (LLMs), we foresee numerous possibilities for extending this method to a wide range of applications."}]}