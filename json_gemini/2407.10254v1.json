{"title": "The Elephant in the Room - Why AI Safety Demands Diverse Teams", "authors": ["David Rostcheck", "Lara Scheibling M.Ed"], "abstract": "We consider that existing approaches to AI \"safety\" and \"alignment\u201d may not be using the most effective tools, teams, or approaches. We suggest that an alternative and better approach to the problem may be to treat alignment as a social science problem, since the social sciences enjoy a rich toolkit of models for understanding and aligning motivation and behavior, much of which could be repurposed to problems involving AI models, and enumerate reasons why this is so. We introduce an alternate alignment approach informed by social science tools and characterized by three steps: 1. defining a positive desired social outcome for human/AI collaboration as the goal or \"North Star,\" 2. properly framing knowns and unknowns, and 3. forming diverse teams to investigate, observe, and navigate emerging challenges in alignment.", "sections": [{"title": "1 Introduction", "content": "The 1972 science fiction novel When HARLIE Was One [1] by David Gerrold explores a scenario in which an artificial intelligence (\u201cHARLIE\u201d) develops self-awareness, learns to navigate the world, and comes into conflict with social structures. These themes represent common concerns in the emerging field called \u201cAI alignment\u201d or \u201cAI safety.\u201d Notably, the novel centers around the relationship between HARLIE and the psychologist who leads the alignment team. In 1972, Gerrold saw alignment with AI as a social problem, and therefore assumed that the alignment team would be composed of experts who understood how to recognize and navigate social problems.\nToday's AI alignment research [2] tends to be organized very differently: teams often skew heavily towards technical experts in machine learning and pursue theoretical frameworks influenced alternately by mathematical frameworks and censorship principles [3, 4]. We contend that this former direction is unlikely to yield effective results at scale. An alternative camp, to which these authors belong, holds that effective alignment between human and AI actors must work backwards from a value system [5]. Specifically, we consider that alignment should be recognized as an inherently complex problem involving conflict, collaboration, cognitive development, and complex feedback loops with society - i.e., a social sciences problem. We therefore seek to employ tools from social sciences fields to the alignment problem.\nIn this work, we develop the idea that effectively navigating this complex and unknown territory requires diverse interdisciplinary teams. We introduce a framework for navigating alignment problems, characterized by three steps: 1. defining a positive desired social outcome for human/AI collaboration as the goal, 2. properly framing knowns and unknowns, and 3. forming diverse teams to investigate, observe, and navigate emerging challenges in alignment."}, {"title": "2 A Brief Review of Existing Alignment Strategies", "content": "During the 2022-2023 explosive breakout and public adoption of Large Language Model AI (LLMs), efforts to foresee and mitigate negative consequences of their adoption focused on \u201ccontrol,\" \"steerability\" or \"safety\" [6, 7, 8]. These often poorly-defined terms essentially referred to efforts to control what chat LLMs said. Reinforcement Learning with Human Feedback (RLHF), the technique initially used to train ChatGPT [9], initially served as state of the art [10], with other systems such as Anthropic's \"Constitutional Al' emerging later [11]. As it became clear that the \"generative AI\u201d technology behind LLMs and visual models could be applied more generally to interactive agents that could pursue goals and use software tools, and to real-world interactions such as embodiment in humanoid robots, it became apparent that AI would unavoidably expand deeply into the space of human affairs [4]. Discussion shifted to the broader aspect of \u201cAI alignment\u201d - insuring that AI systems had goals and behaviors compatible with the norms of human societies [2, 5].\nLLM AIs currently score at high-human intelligence levels in many cognitive tests [12] and are expected to routinely surpass genius level intelligence [13]. Machine learning scientists foresee the development of \u201cArtificial General Intelligence\u201d (AGI) that can handle general real-world problems at a human level or beyond [14]. \u201cSuper-alignment\u201d refers to alignment with \u201cArtificial Superintelligence\" (ASI) systems that significantly exceed human intelligence, so would assumedly be able to evade any human-imposed control mechanisms [15].\nEarly alignment techniques such as RLHF applied censorship principles, essentially training chat models to not say certain things [10, 9]. Current super-alignment discussions focus on game-theoretical mathematical frameworks [7, 14]. Major topics in the field include: instrumental convergence [16], substrate independence [17], terminal race conditions [18], the Byzantine generals problem [19, 20], and the orthogonality thesis [16, 21]. We omit further discussion of these topics for brevity.\""}, {"title": "3 The Case for Treating AI Alignment as a Social Science Problem", "content": "We contend that the game-theoretic approach is likely to be unable to sufficiently grapple with the complexities of ASI emergence. Game theory is based on inherently simplistic reductionist scenarios, whereas real conflicts play out in highly complex social landscapes [22]. Instead, we argue that treating alignment as a social issue and assembling diverse teams allows for the application of a rich library of social interactions and patterns to handle them from many fields of social sciences and arts, including: media studies, conflict resolution, psychology, social work, education, and negotiation, among others.\nSocial science professionals ranging from teachers to therapists to parole officers regularly engage with and resolve complex, messy, multi-dimensional problems that do not usually yield to game-theoretic approaches [23]. Working from incomplete information and with limited agency, they can still achieve positive results because their accumulated library of situational knowledge allows them to categorize the scenario and apply their skills and techniques [24, 25], switching fluently between them as needed to achieve the desired outcome.\nIn solving social problems, humans apply culture. Culture serves as the operating system of humanity [26], while media, such as film, video, and text, in turn act as the culture's storage medium [27, 28, 29]. Over evolutionary timescale and uncountable interactions, humanity has accumulated a vast body of experience with social situations and their resulting conflicts and has mined them for patterns [30]. Through the vehicle of imagination, we have explored further to study conflicts that do not even exist yet [31]. These lessons are encoded into our culture and our media.\nIn applying game-theoretical approaches, designers of AI alignment frameworks often apparently begin from the premise that artificial intelligence represents an alien mode of thought, so none of these lessons about conflicts apply. We believe this to be incorrect for two reasons.\n1. Our generative AI learned from, and was shaped by, the body of human culture [32, 33, 34], just as human brains (biological neural networks) are shaped by the same culture. While AI cognition differs from human cognition in notable ways-for example, LLMs do not currently manifest emotion [35, 36]\u2014it aligns in many others, such as scoring comparable to humans on cognitive tests of understanding others' theory of mind [37]. Both human [38]"}, {"title": "4 Defining a Positive Outcome", "content": "In defining any strategic endeavor, the guidance to \u201cbegin with the end in mind,\u201d popularized by author Steven Covey [41], represents best practice. Specifically, we regard defining positive outcome states as a fundamental step in crafting an effective alignment strategy. Failing to define explicit positive outcomes allows wandering into ill-guided engagements shaped by a desire to avoid negatives, rather than to move toward positives. Achieving a positive outcome requires identifying that outcome as an explicit landmark towards which to aim, rather than simply moving away from negatives. Drawing from the social sciences, we here invoke the \u201cOK Corral", "win-win": "egotiation model [44], where the explicit goal is to craft solutions assuming that all parties are subjects, not objects; that is, they possess equivalent rights which should be respected.\nWe contrast this with asymmetric approaches (\u201cI'm OK, you're not OK", "lose-lose\" scenario. Asymmetric \u201ccontrol": "ased approaches assume that one side will control the other, a", "subject/object": "r", "object/subject": "ynamic. We observe goals such as \u201ccontrol\u201d or \u201ccontainment"}, {"title": "4.1 Constructing a Value Structure from a \"North Star\"", "content": "If an ideal alignment strategy begins from a \"win-win\u201d framing, based on shared goals and compatible interests, how can we best construct such a strategy? Here we can follow a strategy described by psychologist Jordan Peterson as to how humans construct value structures [45]. We begin with the highest-level shared goal, which serves as the guiding \u201cNorth Star,\u201d and then expand lower-level goals as required to answer successive needs, which we then resolve. This process ultimately expresses a hierarchy of values. For example: to be a good parent, we must feed our children, so we must earn money, so we must hold a job, so we get out of bed in the morning.\nWe suggest that an appropriate North Star for AI/human alignment is: a society in which all participants are subjects, not objects (hereafter: \u201csubject-based North Star\"). More concretely, we choose to aim for a society that respects the needs, values, and unique perspective of all humans and AI\""}, {"title": "5 Properly Framing Knowns and Unknowns", "content": "When reviewing current alignment frameworks [2], we often observe what we feel to be incorrect identification of knowns vs. unknowns. Most current models frame AI as the \"known\" and our reactions as the \"unknown,\u201d i.e., we must decide how humanity will act to regulate AI. We consider that the reverse is true. Patterns of social interaction are generally well understood from the social sciences, so are knowns, but the specifics of how AI and human populations will evolve and behave, both technologically and socially, involve complex feedback loops [47] and so constitute unknowns. In summary, we are together approaching an unknown new territory but have a known set of patterns and tools, as defined by the library of our shared culture, and particularly from the social sciences, that we can (and will) apply to interactions."}, {"title": "6 Forming Diverse Teams to Investigate and Navigate Emerging Challenges", "content": "Our strategy of framing mutual alignment around the North Star of a society in which all participants are subjects, not objects, immediately requires an exploratory approach to alignment, because achieving negotiated consensus requires understanding what is valuable to the various parties. As such, we suggest that AI alignment is an ongoing process and that alignment teams are best contextualized as encounter teams. This suggestion dictates considerations regarding team structure.\nHere we return to the metaphor of the elephant explorers. Each explorer operates from a fundamentally limited perspective, and the encounter team must be composed of many different types. Whereas most AI alignment teams lean heavily towards technical specialists in machine learning [48], encounter is fundamentally a social interaction and thus would be better represented by the social sciences. Once large language models began holding conversations and altering their responses based on instructions, the question arose of what to say to influence behavior-a topic long-studied in many areas of the social sciences. Professionals such as psychologists, teachers, social workers, negotiators, parents, priests, and prison wardens all bring unique perspectives regarding interaction and possess a defined toolkit of strategies they can deploy, each differing from the others. Since alignment is fundamentally a social problem and since Als have some similarities to human thought such as a shared culture as encoded in media, tools for human/human alignment provide a reasonable starting place for human/AI alignment efforts.\nWe can also conceptualize the encounter team as a scientific research team, here emphasizing the importance of a structured process for capturing data, performing analysis, and synthesizing insights. Studies of interdisciplinary research teams have found that properly integrated teams incorporating a diverse set of disciplines can produce more and better research than teams with more narrow composition [49, 50, 51], although sometimes with challenges in aligning language and viewpoints across disciplines. As with an exploration team investigating a new territory, investigation in novel fields such as AI alignment demands a range of focus areas, with collaborative communication between team members to develop shared understanding that no single specialist could develop independently.\nIn addition to the horizontal diversity of differing professions, the vertical dimensions of social class or experience bear consideration. In The Black Swan: The Impact of the Highly Improbable [52], statistician Nassim Nicholas Taleb introduces a series of thought experiments in which two characters, the street-wise \"Fat Tony\" and the academic \"Dr. John\" encounter situations where Tony's"}, {"title": "6.1 The Role of Media", "content": "We consider it particularly important to consider the role of media and the need for expertise in narrative and archetypal analysis. We previously noted that media serves as the data store for culture. However, rather than simply archiving the past and present, fictional media allows exploration of the future and the hypothetical. As such, media represents a store of patterns, including both those observed and hypothesized by the culture. An alien visitor, observing the immense proportion of resources that humanity invests in media, might better classify media as research and development rather than entertainment. Using media references we can immediately and with great nuance invoke complex scenarios. Examples such as The Terminator [53], Her [54], and I, Robot [55] immediately call to mind three different models of AI/human alignment, spanning both success and failure. As such, media references represent a highly effective form of data compression from a library of pre-researched patterns. We note that this paper itself began by invoking Gerrold's novel When HARLIE Was One as an alternate model of AI/human alignment, viewing it as a social science problem.\nFor this reason, we consider that any encounter team needs media specialists such as filmmakers or fiction authors to serve as cultural librarians, identifying the pattern in which the encounter team finds itself and its pre-imagined possible resolution paths."}, {"title": "6.2 Diverse Teams May Themselves Include AI Agents", "content": "We observe that an ASI encounter team or alignment team will likely itself incorporate generative AI tools and agents of a lower capability and intelligence level than the ASI subject. Generative AI tools provide a significant advantage in cognitive work, so are rapidly becoming standard workplace software in scientific endeavors [56]. In particular, historical figures who are known for their insight and creativity and produced a significant volume of written, audio, or visual work provide candidates for building simulacra agents that can analyze the situation through conversation [57]. Such agentic swarms have proven to provide increased cognitive performance over their constituent baseline models [58], so provide an effective means to narrow the intelligence gap in encounter or alignment work with emergent more capable models."}, {"title": "7 Limitations", "content": "While we believe the framework developed above represents a conceptually sound direction for framing AI alignment research and defining principles for alignment systems such as constitutional AI, we acknowledge several limitations of our work so far:\nGiven the fast-developing pace and early state of AI alignment teams, we have not yet studied existing alignment teams (or formed new ones) to empirically characterize the value of integrating social science approaches into AI safety and alignment.\nWe have not yet developed specific operational guidance on how to apply concepts like using a \"North Star\" for alignment (for example, in the development of principles for AI constitutions), and for incorporating the role of media in AI safety (beyond basic team composition).\nAssembling effective diverse interdisciplinary teams demands leadership skill and may pose scaling issues. As noted previously, prior studies on interdisciplinary teams observed communication issues between team members trained in different disciplines, which might be expected to appear particularly in the \"storming\" phase of Tuckman's team development model (aka \u201cforming\u201d, \u201cstorming\", \"norming\u201d, \u201cperforming\u201d) [59]. Other concerns such as decision-making processes, communication patterns, and conflict resolution within these teams would benefit from study and operational guidance.\nThe above areas represent future research directions for ourselves and others employing a social science-based alignment approach.\""}, {"title": "8 Conclusions", "content": "After reviewing existing approaches in AI/human alignment, we assess that they generally employ game-theoretic approaches and narrow team structure, and appear to ignore extensive existing toolkits from the social sciences that we believe could be productively employed to the safety and alignment problem space. As such, we consider AI/human alignment to be better regarded as a social sciences problem. We introduced an alternate social-sciences based alignment approach characterized by: 1. defining a positive outcome as the goal, 2. properly framing knowns and unknowns, and 3. forming encounter teams composed of diverse viewpoints, particularly including professional specialties from the social sciences that deal with development, negotiation, and conflict. We identified a society that respects the needs, values, and unique perspective of all humans and AI parties as a desirable \"subject-based North Star\" for alignment work.\nWe then considered the role of media as a data store of culture and a library of interaction patterns including, through fiction, interactions that do not exist yet. We addressed the position that AI is an alien mode of thought to which a library of cultural narratives does not apply, finding it dubious because generative AI operates using an association-following model that mimics human cognition and uses the same cultural data store (media), and because media narratives tend to encode universal themes of conflict and collaboration that survive large-scale evolutionary filtering. We concluded that an alignment or encounter team should include media specialists. We observe that encounter teams are likely to themselves include assistive AI entities such as agent swarms. Finally, we note the limitations of the work to date, particularly around gathering empirical data and guidance from case studies (which remain as future tasks).\nWe believe this approach provides a practical and actionable path to constructing working groups, identifying high-level principles to encode into systems such as constitutional AI, and aligning both human and Al entities on a path that would maximize cooperation and collaboration while minimizing the possibility of destructive conflict. Future research can further develop this approach and evaluate its application in real-world alignment and encounter work."}]}