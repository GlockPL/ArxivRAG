{"title": "UNVEILING AND CONTROLLING ANOMALOUS ATTENTION\nDISTRIBUTION IN TRANSFORMERS", "authors": ["Ruiqing Yan", "Xingbo Du", "Haoyu Deng", "Linghan Zheng", "Qiuzhuang Sun", "Jifang Hu", "Yuhang Shao", "Penghao Jiang", "Jinrong Jiang", "Lian Zhao"], "abstract": "With the advent of large models based on the Transformer architecture, researchers have observed\nan anomalous phenomenon in the Attention mechanism-there is a very high attention on the first\nelement, which is prevalent across Transformer-based models. It is crucial to understand it for\nthe development of techniques focusing on attention distribution, such as Key-Value (KV) Cache\ncompression and infinite extrapolation; however, the latent cause leaves to be unknown. In this paper,\nwe analyze such a phenomenon from the perspective of waiver phenomenon, which involves reducing\nthe internal values of certain elements in the Softmax function, allowing them to absorb excess\nattention without affecting their contribution to information. In specific models, due to differences in\npositional encoding and attention patterns, we have found that the selection of waiver elements by the\nmodel can be categorized into two methods: positional-encoding-based and feature-distribution-\nwithin-elements-based.", "sections": [{"title": "1 Introduction", "content": "Transformer architectures [1, 2, 3] have become the most popular foundational structures in deep learning due to\ntheir remarkable ability to effectively model long-range dependencies [4] and their scalable parameterization [5],\nenabling excellent learning capabilities. At the core of these models is the attention mechanism, which dynamically\nassigns weights to input elements, allowing the model to effectively prioritize relevant information. The impact and\nsignificance of Transformer models in the field of artificial intelligence are profound, as they have revolutionized natural\nlanguage processing, computer vision, and other AI applications, establishing themselves as a cornerstone of modern\nAI advancements."}, {"title": "2 Related Work", "content": "The Transformer [1] model has rapidly become the prevailing architecture in natural language processing. Its innovative\nincorporation of the self-attention mechanism significantly enhances performance in sequence-to-sequence tasks.\nSubsequent innovations have led to widespread adoption and modification of the model, resulting in numerous variants,\nprimarily categorized into BERT [2] (Global Attention) and GPT [3] (Causal Attention) branches.\nAdvancements within the global attention branch have led to several significant innovations. RoBERTa [10] enhances\nthis model's capabilities with longer training sequences; ALBERT [11] contributes further by implementing a parameter-\nsharing mechanism to boost processing efficiency. The T5 [12] model broadens the architecture's applicability;\nXLNet [13] advances these methodologies by integrating a generalized autoregressive pretraining approach. These\ndevelopments showcase the continuous evolution and enrichment of global attention architectures in natural language\nprocessing.\nThe causal attention branch of Transformer models also significantly impacts natural language processing, particularly\ncelebrated for its implementations of large-scale language models. Major developments within this lineage include\nOpenAI's GPT series [14, 15, 16, 17], Meta's LLaMA series [18, 19, 20], Google's PaLM series [21, 22]. These\nseries collectively emphasize the transformative role of causal attention architectures in advancing the complexities of\nlanguage understanding, generation, and the broad application of AI in everyday technology.\nGiven the substantial computational demands of the Transformer model, enhancing model efficiency while maintaining\nperformance has emerged as a critical area of focus within the community. Researchers have proposed numerous\neffective improvements, including the Transformer-XL [23], Linformer [24], Adaptive Sparse Transformer [25],\nReformer [26]. Besides the work above, many other ongoing improvements in various aspects of Transformer models\nexist. However, all these efforts underscore the importance of a deep understanding of the intrinsic mechanisms of\nTransformer models for driving advancements."}, {"title": "2.2 Attention Sinks", "content": "As early as 2020, Manzil Zaheer et al. [6] discovered that transformer-based natural language models tend to focus\nmore on the first few elements in a sequence. Based on this observation, they designed the Big Bird model, which\ncalculates attention only on specific elements [6].\nIn late 2022, with the release of ChatGPT-3.5 by OpenAI, researchers began to focus on large models. Large models\nhave strong learning capabilities, enabling them to generate very long outputs, which poses significant challenges to\nhardware systems' caching and computational capabilities. To address these challenges, Xiao et al. [7] designed an\nefficient inference method that retains the first few tokens, achieving infinite extrapolation in large model scenarios.\nIn their work, they pointed out that the first four elements absorb a lot of attention, referring to this phenomenon as\nAttention sink [7]. Therefore, excluding the first few elements can cause severe numerical instability in attention\ndistribution, preventing the model from functioning properly.\nIn 2024, Yao Fu [8] analyzed the distribution in the LLaMA-2-7B-80K [19] model and found that most layers in the\nmodel allocate more attention to the first few elements. This phenomenon provided new insights for other researchers to\ncompress the KV Cache. Subsequently, Suyu Ge et al. [9] designed a KV Cache compression strategy called FastGen\nbased on the attention distribution, effectively improving the inference performance of various transformer-based natural\nlanguage models [27]."}, {"title": "3 Analysis from a Waiver Perspective", "content": null}, {"title": "3.1 Waiver and its Latent Cause", "content": "Self-attention [1] plays a pivotal role in the contemporary machine learning communities, with the formulation\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})$"}, {"title": "3.2 Properties of Waiver Elements", "content": "The self-attention mechanism essentially involves computing attention weights and performing a weighted sum with the\nValue (V) matrix to mix the elements in the sequence. Specifically, the attention weights are calculated by multiplying\nthe transposed Query (Q) and Key (K) matrices. These weights are then used to perform a weighted sum with the V\nmatrix, facilitating token-to-token information interaction. Each individual weighted operation can be expressed as:\n$outs(i) = \\sum_{j=0}^{seqlen} a(i, j) \\cdot v(j)$"}, {"title": "3.3 How Waiver Elements Work in the Model", "content": "We observed that in the meta-llama/Meta-Llama-3-8B (Llama3-8B) [28, 20] model released by Meta on\nHuggingface[28], when the index of the first token is 128000 (i.e., the starting special token), the model assigns\nhigh attention to the first element from the first layer and significantly lowers the L2 norm of the corresponding vector\nin the V matrix compared to other elements, indicating a waiver phenomenon. However, we found that after changing\nthe first token, the model still exhibited the waiver phenomenon on the first element in the third layer. Additionally, in\nthe google-bert/bert-large-cased-whole-word-masking (Bert-Large) [28] model released by Google on Huggingface,\nthis waiver phenomenon not only appeared on the first element but also on the last element.\nWe posit that the observed phenomenon is influenced by the attention pattern, which we will delve into within this\nsection. When the first element cannot be distinguished by the attention pattern, the model uses learnable positional\nencoding to distinguish the waiver option elements."}, {"title": "3.3.1 Models Using Causal Attention", "content": "Many popular models, such as the Llama series [18, 19, 20], use causal attention. The structured mask matrix of causal\nattention is a lower triangular matrix. The calculation process is\n$CausalAttention(Q, K, V) =\n\\begin{bmatrix}\n1 & 0 & 0 & \\cdots & 0\\\\\n1 & 1 & 0 & \\cdots & 0\\\\\n1 & 1 & 1 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n1 & 1 & 1 & \\cdots & 1\n\\end{bmatrix}\n\\bigodot \\text{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right)V$"}, {"title": "3.3.2 Models Using Global Attention and Learnable Positional Encoding", "content": "In models using global attention and learnable positional encoding, the method for selecting waiver elements can\ndiffer significantly. Global attention allows each element in the sequence to attend to every other element, without the\nconstraints imposed by causal attention. This means that each token can be influenced by all other tokens, leading to\nmore complex attention patterns.\nWe observe that in models using global attention, there is generally higher attention given to the first element, and\nsimilarly high attention to the last element. Additionally, we found that the L2 norms of the vectors corresponding to\nthe first and last elements in the V matrix are also lower than the L2 norms of other elements, indicating that they can\nserve as waiver options. However, in global attention models, each element is mixed with other elements through the\nattention mechanism, making it impossible to use the based on the feature distribution within elements method to\nidentify waiver elements.\nAfter examining the pre-trained weights of the Bert-Large model, we found that the first and last indices in the learnable\npositional encoding are assigned embedding vectors with significantly larger L2 norms than other indices. Learnable positional encoding is mixed with word embeddings by adding them positionally. This means that\nelements with the first and last positional indices are assigned biases different from other elements, which might lead to\nfeature distributions distinct from other elements. This distributional difference allows the model to distinguish waiver\noption elements from other elements.\nThe reason for the significantly larger L2 norms of the embedding vectors for the first and last positional indices in\nlearnable positional encoding might be that the first and last elements are always [CLS] and [SEP] during the training\nof Bert-Large model. Therefore, the model learns these positional encoding embedding vectors differently from other\npositions. Moreover, we observed that replacing the first element with something other than [CLS] and the last element\nwith something other than [SEP] results in an increase in the L2 norms of the vectors corresponding to the first and last\nelements in the V matrix across layers of the model. However, due to the differing influences of positional encoding\nembedding vectors, their L2 norms still differ from other elements but not significantly.\nSince global attention models rely on the feature distribution of positional encoding embedding vectors to distinguish\nwaiver option elements from other elements, and these positional encoding embedding vectors are fused with token\nembedding vectors and token type embedding vectors by positional addition before entering the first layer of the\nTransformer, with no further fusion operations in subsequent layers, the attention mechanism tends to focus more on\nthe first and last elements as early as the first layer.\nWe refer to this strategy of distinguishing waiver option elements from other elements based on positional encoding as\npositional-encoding-based. This strategy requires the model to also learn the feature distributions of the first and last\nelements, as positional information alone is insufficient to effectively distinguish waiver option elements from other\nelements."}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Protocols", "content": "Data: The experimental data is randomly selected from the dataset bigscience-data/roots_en_wikipedia [28], which\nconsists of English Wikipedia articles. This dataset contains 2.06M samples; considering the visualization burden, we\nrandomly selected 100 samples with a length of over 1024 characters (using Python's built-in len method) for the actual\nexperiments.\nModels: The models used in this experiment are Llama3-8B and Bert-Large. Llama3-8B is one of the leading\nopen-source large language models, utilizing rotary position encoding and causal attention. Bert-Large is a classic\nnatural language processing model whose encoder part employs global attention and, in the specific implementation on\nHuggingface, uses a learnable positional encoding with a maximum length of 512."}, {"title": "4.2 Experimental Design", "content": "We aim to adjust the feature distribution of element embedding vectors to control whether an element becomes a waiver\noption. By making these adjustments, we hope to observe the attention weights concentrating on the positions we\nspecify. Since the methods for identifying waiver option elements differ between models based on causal attention and\nmodels based on global attention, we design two strategies: adjusting the structured mask matrix and adjusting the\nfeature distribution within positional encodings."}, {"title": "4.2.1 Adjusting the Structured Mask Matrix", "content": "As discussed earlier in this paper, the model treats elements with embedding vectors exhibiting non-mixed distribution\nas waiver elements. Therefore, we need to adjust whether an element is mixed by the attention weighting mechanism.\nIn causal attention, we can control how the attention weighting mechanism affects a particular element by adjusting the\nstructured mask matrix. For example, the vector corresponding to the first element in the structured mask matrix is\nshown while the vector corresponding to the k-th element is shown in . Obviously, if we modify\nthe vector corresponding to the k-th element in the structured mask matrix to the form in , this element will\nno longer be mixed with elements in the sequence through the attention weighting mechanism, thereby exhibiting\nnon-mixed distribution, and the model will treat this element as a waiver option. Thus, we devise the following attention\nmask matrices:\n$Mask_{1,j} =\n\\begin{cases}\nunmask & \\text{if } j < 1\\\\\nmask & \\text{if } j > 1\n\\end{cases}$\n$Mask_{k,j} =\n\\begin{cases}\nunmask & \\text{if } j \\leq k\\\\\nmask & \\text{if } j > k\n\\end{cases}$\n$Mask^{modified}_{k,j} =\n\\begin{cases}\nunmask & \\text{if } j = k\\\\\nmask & \\text{if } j \\neq k\n\\end{cases}$"}, {"title": "4.2.2 Adjusting the Feature Distribution within Positional Encodings", "content": "This strategy involves replacing the positional encoding embedding vectors corresponding to other positions with those\ncorresponding to the first and last positions. By doing so, the replaced positions will also have the positional bias of the\nfirst and last positions. As discussed earlier in this paper, the model will mark elements with such positional bias as\nwaiver option elements. However, since the tokens corresponding to the selected positions are uncertain, even if more"}, {"title": "4.3 Results and Analysis", "content": null}, {"title": "4.3.1 Experiments with Adjusting the Structured Mask Matrix on Llama3-8B", "content": "We selected the element with the position index 255 as the artificially designated waiver option, and the element with\nthe position index 0 corresponding to the word vector index 128000 and other random values. Since the last element\nis crucial for generative tasks, we also observed the attention distribution of the last element to the elements in the\nsequence. Due to the numerous attention heads in the model, it is impractical to display all of them in the paper, so we\nselected representative results. The experimental results are shown in Figure 3."}, {"title": "4.3.2 Experiments with Adjusting the Feature Distribution within Positional Encodings on Bert-Large", "content": "We selected the element with the position index 383 as the artificially designated waiver option and replaced its\npositional encoding embedding vector with the vectors corresponding to position indices 0 and 511 (the last element) in\nthe positional encoding embedding matrix. Additionally, we set the word vector index of the element at position index\n0 to 101 and other random values, with a sequence length of 512. To observe the attention weights assigned to the first\nand last elements, we observed the attention distribution of the element with position index 383 to the elements in the\nsequence and selected representative results. The experimental results are shown in Figure 4.\nAt the adjusted position, we observed that the attention given by the model to our adjusted position is similar to the\nattention given to the first and last elements when their word vector indices are random values. However, there is\na certain difference when the word vector indices of the first or last elements are 101 or 102, but both attract more\nattention compared to other elements. The experimental results also clearly show that the model pays more attention to\nelements near the position index 383, which aligns with linguistic principles that closer elements are more likely to\nprovide relevant information."}, {"title": "5 Conclusion", "content": "In this paper, we have presented an in-depth analysis of the anomalous phenomenon observed in Transformer models,\nwhere a disproportionately high attention is given to the first element in a sequence. We introduced the concept of the\nwaiver phenomenon to explain this behavior and proposed two methods for selecting waiver elements: positional-\nencoding-based and feature-distribution-within-elements-based.\nThrough our experiments, we demonstrated that adjusting the structured mask matrix and feature distribution within\npositional encodings effectively controls whether an element becomes a waiver option. Our findings provide a coherent"}]}