{"title": "Towards Projected and Incremental Pseudo-Boolean Model Counting", "authors": ["Suwei Yang", "Kuldeep S. Meel"], "abstract": "Model counting is a fundamental task that involves determining the number of satisfying assignments to a logical formula, typically in conjunctive normal form (CNF). While CNF model counting has received extensive attention over recent decades, interest in Pseudo-Boolean (PB) model counting is just emerging partly due to the greater flexibility of PB formulas. As such, we observed feature gaps in existing PB counters such as a lack of support for projected and incremental settings, which could hinder adoption.\nIn this work, our main contribution is the introduction of the PB model counter PBCount2, the first exact PB model counter with support for projected and incremental model counting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree (LOW-MD) computation ordering heuristic to support projected model counting and a cache mechanism to enable incremental model counting. In our evaluations, PBCount2 completed at least 1.40\u00d7 the number of benchmarks of competing methods for projected model counting and at least 1.18\u00d7 of competing methods in incremental model counting.", "sections": [{"title": "Introduction", "content": "Pseudo-Boolean (PB) model counting involves determining the number of satisfying assignments of a given pseudo-boolean formula, which differs from typical model counting tasks where the input is a Boolean formula in conjunctive normal form (CNF). In recent years, there has been increasing interest from the community in PB model counting and PB formulas in general, in part due to their succinctness and flexibility over CNF formulas (Sinz 2005). In particular, an arbitrary CNF clause can always be converted to a single PB constraint, but the converse is not true (Le Berre et al. 2018).\nThe emerging interests take the form of new PB model counting tools such as ApproxMCPB and PBCount (Yang and Meel 2021, 2024) as well as the emergence of applications that include knapsack problems, neural network verification, and budgeted sensor placement (Pisinger 2005; Yang and Meel 2021; Latour, Sen, and Meel 2023).\nIn contrast, CNF model counting and solving (SAT solving) are well-established fields, benefiting from many decades of research advancements that have led to numerous feature-rich counters and solvers. The availability of tools for CNF model counting and solving has led to an ever-increasing number of applications across a wide range of domains such as software design, explainable machine learning, planning, and probabilistic reasoning (Bacchus, Dalmao, and Pitassi 2003; Narodytska et al. 2019; Jackson 2019; Fan, Miller, and Mitra 2020). In turn, the wide range of applications drives demand for better tools, from both performance and feature perspectives. Subsequently, better tools would lead to more adoption and applications. Eventually, a positive self-reinforcing loop is formed within the CNF model counting and solving community. On the other hand, we have yet to observe such a positive cycle in the PB model counting community, in part due to a lack of features such as exact projected counting and incremental setting support in existing PB model counting tools.\nIn this work, our main contribution is the PB model counter, PBCount2, which supports projected and incremental model counting features. We focus on these two aforementioned features due to their importance to the CNF model counting and SAT solving community. Projected model counting involves determining the number of partial assignments over a given set of variables, known as the projection set, that can be extended to a satisfying assignment over all variables of the formula. Projected model counting has been applied in areas such as planning, verification, and reliability estimation (Aziz et al. 2015; Klebanov, Manthey, and Muise 2013; Due\u00f1as-Osorio et al. 2017). Similarly, incremental SAT solving has led to applications such as combinatorial testing, circuit design, and solving string constraints (Yamada et al. 2015; Yu et al. 2017; Lotz et al. 2023). In this work, we focus on the incremental setting whereby a given PB formula undergoes incremental modifications, more specifically addition and removal of PB constraints, while having its model count computed along each modification step. In order to support projected PB model counting, we introduce a new computation ordering heuristic that is compatible with the ordering requirements for projected model counting. Additionally, we introduce a new caching mechanism that is at the core of incremental counting. PBCount2 is to the best of our knowledge both the first exact projected PB counter as well as the first incremental model counter.\nWe performed extensive evaluations on benchmark instances inspired by various application settings, such as sensor placement, multi-dimension knapsack, and combinatorial auction benchmarks (Gens and Levner 1980; Blumrosen and Nisan 2007; Latour, Sen, and Meel 2023). Our evaluations highlighted the efficacy of PBCount2 at projected PB model counting and incremental model counting compared to the baseline techniques. In particular, PBCount2 is able to successfully count 1957 projected model counting instances while the state-of-the-art CNF-based projected model counter could only count 1398 instances. Incremental benchmarks involve multiple modifications of the initially provided PB formula, with model counts computed after each modification step. Our experiments showed that PBCount2 is able to complete 1618 instances for incremental benchmarks involving 5 counts with 1 count for the initial PB formula and 4 counts for modification steps. In comparison, the state-of-the-art PB model counter, PBCount, completed only 1371 instances, demonstrating a significant performance advantage of PBCount2 at incremental counting. Moreover, PBCount2 also demonstrates superior performance at incremental settings compared to the state-of-the-art CNF model counters D4 and GPMC, both of which completed less than 1000 incremental benchmarks. Given that it is the early days of PB model counting, we hope the new capabilities introduced in this work will attract more interest and applications of PB model counting, leading to a positive self-reinforcing cycle within the PB model counting community."}, {"title": "Notations and Preliminaries", "content": "Pseudo-Boolean Formula A PB formula $F$ consists of one or more PB constraints, each of which is either equality or inequality. A PB constraint takes the form $\\sum_{i=1}^n a_i x_i \\leq k$ where $x_1, ..., x_n$ are Boolean literals, $a_1, ..., a_n$, and $k$ are integers, and $\\leq$ is one of {$>, =, <$}. We refer to $a_1, ..., a_n$ as term coefficients in the PB constraint, where each term is of the form $a_i x_i$. $F$ is satisfiable if there exists an assignment $\\tau$ of all variables of $F$ such that all its PB constraints evaluate to true. PB model counting refers to the computation of the number of satisfying assignments of $F$.\nProjected Model Counting Let $F$ be a formula and Var(F) be the set of variables of $F$. Let $X$ and $Y$ be disjoint subsets of Var(F) such that $X \\cap Y = \\emptyset$ and $X \\cup Y = Var(F)$. $(F, X, Y)$ is a projected model counting instance. The projected model count is $\\sum_{\\beta \\in 2^X} (max_{\\alpha \\in 2^Y} [F](\\alpha \\cup \\beta))$ (Dudek, Phan, and Vardi 2021). Where $[F](\\alpha \\cup \\beta)$ is the evaluation of $F$ with the variable assignment $\\alpha \\cup \\beta$, and returns 1 if $F$ is satisfied and 0 otherwise. In other words, the projected model count of $F$ on $X$ is the number of assignments of all variables in $X$ such that there exists an assignment of variables in $Y$ that makes $F$ evaluate to true (Aziz et al. 2015). Non-projected model count is a special case of projected model counting where all variables are in the projection set, i.e. $X = Var(F)$, and $Y = \\emptyset$. With reference to Figure 2, suppose the PB formula has only the single PB constraint $3x_1 + 4x_2 \\geq 3$, then the 3 satisfying assignments are $(x_1, x_2), (\\overline{x_1}, x_2), (x_1, \\overline{x_2})$. If the projection set is $X_1$ then the corresponding projected model count is 2, because both partial assignments involving $x_1$ can be extended to a satisfying assignment.\nProjections Let $f : 2^X \\rightarrow R$ be a function defined over a set of boolean variables $X$. The $\\exists$-projection of $f$ with respect to a variable $x \\in X$ for all $\\sigma$ is given by $f(\\sigma \\wedge x) + f (\\sigma \\wedge \\overline{x})$ where $\\sigma$ refers to assignments of all variables excluding $x$. Similarly, the $\\exists$-projection with respect to $x$ is given by $max (f(\\sigma \\wedge x), f(\\sigma \\wedge \\overline{x}))$. In the case that $f$ maps to 1 or 0, i.e. $f: 2^X \\rightarrow$ {0,1}, then $\\exists$-projection can be written as $f (\\sigma \\wedge x) \\vee f (\\sigma \\wedge \\overline{x})$. In this work, we use the two types of projections to compute the projected model count in Algorithm 1 which we detail in Section 4.2.\nAlgebraic Decision Diagram An algebraic decision diagram (ADD) (Bahar et al. 1993) is a directed acyclic graph (DAG) representation of a function $f : 2^{Var(F)} \\rightarrow S$ where Var(F) is the set of Boolean variables that f is defined over, and S is known as the carrier set. We denote the function represented by an ADD & as Func(&). The internal nodes of ADD represent decisions on variables $x \\in Var(F)$ and the leaf nodes represent $s \\in S$. The order of variables represented by decision nodes, from root to leaf of ADD, is known as the variable ordering. In this work, S is a set of integers. As example, an ADD representing $3x_1 + 4x_2$ is shown in Figure 1, and $3x_1 + 4x_2 \\geq 3$ in Figure 2. The dashed arrows of internal nodes represent when the corresponding variables are set to false, and solid arrows represent when they are set to true.\nIn this work, we make use of the Apply operation on ADDs (Bryant 1986; Bahar et al. 1993). The Apply operation takes as input a binary operator, two ADDS"}, {"title": "Related Work", "content": "Search-Based Model Counters We can classify the numerous existing model counters into two main categories based on their underlying techniques \u2013 search-based model counters and decision diagram-based model counters. Search-based model counters iteratively assign values to variables, implicitly exploring a search tree of variable assignments while keeping count throughout the process. The counters typically employ component caching, so that counts for each branch of the search tree are only computed once. Notable search-based model counters include GPMC, Ganak, and Sharpsat-TD (Ryosuke Suzuki and Sakai 2017; Sharma et al. 2019; Korhonen and J\u00e4rvisalo 2021).\nDecision Diagram-Based Model Counters The use of different types of decision diagrams, which are directed acyclic graph (DAG) representations of the assignment space of a given formula, is common among model counters, for both PB and CNF formulas. Recent decision diagram-based model counters include D4, ExactMC, ADDMC, and DPMC for CNF model counting and PBCount for PB model counting (Lagniez and Marquis 2017; Dudek, Phan, and Vardi 2020a,b; Lai, Meel, and Yap 2021; Yang and Meel 2024). D4 and ExactMC build their respective decision diagrams in a top-down manner, forming a single diagram representing the assignment space. In contrast, ADDMC, DPMC, and PBCount build decision diagrams in a bottom-up manner, starting from individual decision diagrams of clauses or constraints and subsequently combining the decision diagrams to represent the assignment space. Our counter, which we term PBCount2, follows a similar methodology and combines individual decision diagrams in a particular order to support projected model counting while also caching the intermediate diagrams during the process.\nPseudo-Boolean Model Counting There has been recent interest in PB model counting. Notable tools developed for PB model counting include the approximate PB model counter ApproxMCPB (Yang and Meel 2021) and the recently introduced exact PB model counter PBCount (Yang and Meel 2024). ApproxMCPB uses hash functions to evenly split the space of satisfying assignments and enumerates a partition of satisfying assignments to obtain an estimated count. On the other hand, PBCount follows the methodology introduced in ADDMC (Dudek, Phan, and Vardi 2020a) to exactly count the number of satisfying assignments of the input PB formula. However, PBCount does not support projected model counting or incremental counting features. In this work, we introduce the projected model counting and incremental counting features to the community via our counter PBCount2.\nIncremental SAT Solving There have been numerous works in existing literature regarding incremental settings, more specifically for incremental satisfiability (SAT) solving (Nadel and Ryvchin 2012; Nadel, Ryvchin, and Strichman 2014; Fazekas, Biere, and Scholl 2019; Nadel 2022). Incremental SAT solving involves finding satisfying assignments to a user-provided Boolean formula, typically in CNF, under certain assumptions that users can incrementally specify. Specifically, users can add and remove CNF clauses to the initially specified CNF formula. Incremental SAT solving is also useful for cases where users have to solve a large number of similar CNF formulas and has led to a wide range of applications such as combinatorial testing, circuit design, and solving string constraints (Yamada et al. 2015; Yu et al. 2017; Lotz et al. 2023). In this work, we introduce the concept of incrementality to model counting tasks, by making use of a caching mechanism which we detail in Section 4.3."}, {"title": "Approach", "content": "In this section, we detail our Least Occurrence Weighted Min Degree (LOW-MD) computation ordering heuristic as well as the implementations of PBCount2 to support a) projected model counting and b) incremental model counting."}, {"title": "Least Occurrence Weighted Min Degree", "content": "The general methodology of performing PB model counting tasks with ADDs involves representing each constraint with its individual ADD and subsequently merging the ADDs and projecting away variables to produce the final model count. As implemented in existing work (Yang and Meel 2024), the ordering of merging ADDs and projecting away variables is determined by heuristics. However, the existing heuristics did not support projected model counting. To this end, we introduce a new ordering heuristic in PBCount2, which we term the Least Occurrence Weighted Min Degree heuristic (LOW-MD in short)."}, {"title": "Projected Model Counting", "content": "Recall that in projected model counting, there are two non-overlapping sets of variables X, Y where X is the projection set and Y is the non-projection set. The key idea to support projected model counting is the different way variables in X and Y are projected away. For all variables $x \\in X$, we project x away from an ADD $\\psi$ using $\\psi \\leftarrow \\psi[x\\rightarrow 1] + [x\\rightarrow0]$, also referred to as $\\exists$-projection (Dudek, Phan, and Vardi 2021). In contrast, for all variables $y \\in Y$, we project away y from $\\psi$ using $\\psi \\leftarrow \\psi[y \\rightarrow 1] \\vee /[y \\rightarrow 0]$, also referred to as $\\exists$-projection (Dudek, Phan, and Vardi 2021). In addition, all variables in Y must be projected away before any variable in X because the different projection operations are not commutative across variables in X and Y (Dudek, Phan, and Vardi 2021). To this end, we introduce the LOW-MD ordering which is compatible with the projected model counting ordering requirements. The overall flow of PBCount2 is shown in Figure 3, and the pseudocode is shown in Algorithm 1.\nIn Algorithm 1, we employ the same preprocessing (line 1) and individual constraint compilation techniques (line 3) as the original PBCount. Next, we process each variable y in non-projection set Y by merging all ADDs containing y and projecting away y from the merged ADD (lines 4-8). In lines 9-13, we do the same for variables in projection set X. In each iteration of the merge and project process, we select a variable using our LOW-MD heuristic, indicated by popNextVar() on lines 6 and 11, and remove it from X or Y respectively. As discussed previously, the LOW-MD ordering heuristic entails picking the variable that has the least occurrence in the ADDs at that moment of the computation.\nAlgorithm Correctness The algorithm correctness of projected model counting of PBCount2 follows prior work on projected CNF model counting with ADDs (Dudek, Phan, and Vardi 2021). Dudek, Phan, and Vardi showed that for projected CNF model counting correctness, the computations should be performed according to an X, Y-graded project join tree. In particular, performing $\\exists$-projection at $T_Y$ nodes and $\\exists$-projection at $T_X$ nodes of an X, Y-graded project join tree T produces the correct projected model count. PBCount2's algorithm correctness for projected PB model counting comes from the fact that the computation in Algorithm 1 with LOW-MD heuristic implicitly follows an X, Y-graded project join tree, and therefore produces the correct count.\nTheorem 1. Let F be a formula defined over XUY such that X is the projection set, and Y is the set of variables not in projection set, then given an instance (F, X, Y), Algorithm 1 returns c such that $c = \\sum_{\\beta \\in 2^X} (max_{\\alpha \\in 2^Y} [F](\\alpha \\cup \\beta))$\nAn X, Y-graded project join tree T has two sets of disjoint variables, in Algorithm 1 this corresponds to variables in non-projection set Y and projection set X. The initial individual ADDs produced at line 3 of Algorithm 1 each corresponds to a leaf node in T. Each of the intermediate ADDs during the merge and project iterations of variables in non-projection set Y (lines 4 to 8) corresponds to an internal node of T, in grade $T_Y$. Similarly, each intermediate ADD during the merge and project process of projection set variables X (lines 9 to 13) corresponds to an internal node of T in grade $T_X$. Realize that no nodes of T in $T_X$ are descendants of any node in $T_Y$, satisfying the definition of X, Y-graded project join tree. In addition, we are performing $\\exists$-projection at $T_Y$ nodes and $\\exists$-projection at $T_X$ nodes. As such the computation process in Algorithm 1 can be cast under the graded project join tree framework and would therefore follow the same proof as prior work (Dudek, Phan, and Vardi 2021)."}, {"title": "Incremental Counting", "content": "PBCount2 supports incremental PB model counting via the caching of intermediate ADDs during the handling of model count queries. In particular, PBCount2 supports the removal and addition of constraints in the PB formula. With reference to Algorithm 1, we cache the ADDs $\\psi$ at lines 8 and 13 respectively. In order to store the compute state associated with an ADD, for cache retrieval purposes, we store 3 pieces of information: 1) the set of constraints in PB formula that the ADD represents 2) the projection set variables of ADD that have been projected away and 3) the non-projection set variables of ADD that have been projected away.\nThe cache retrieval mechanism is shown in Algorithm 2. When given PB formula F', modified from F by adding or removing constraints, the core idea is to loop through the ADDs in the cache and retrieve the compatible ADDs, replacing the initial ADDs at line 3 of Algorithm 1. An ADD $\\Psi$ is compatible with F' if the set of constraints that $\\Psi$ represents is a subset of the constraints of F'. In addition, the variables that have been projected out from $\\Psi$ must not appear in any constraint of F' that $\\Psi$ does not represent, this is handled by CheckNoExtraVar(\u00b7) in Algorithm 2 line 4. Subsequently, from lines 5-10, we verify that each ADD that we retrieved is compatible with all other already retrieved ADD candidates in B. Finally, for all constraints that are not represented by an ADD in B, we insert an ADD representing each constraint into B. Cache retrieval replaces lines 1 to 3 of Algorithm 1. It is worth noting that caching ADDs requires us to disable preprocessing currently, as there is a need to maintain a unique id for each constraint and also a fixed variable-to-constraint relation. The restriction arises from the fact that we have to maintain ADD to variable mapping for ADDs in the cache to perform retrieval compatibility checks in Algorithm 2. Preprocessing might remove variables and modify constraints, thus invalidating cached ADDs. As such, preprocessing is disabled when handling incremental counting."}, {"title": "Experiments", "content": "In this section, we detail the extensive evaluations conducted to investigate the performance of PBCount2's new features, namely projected PB model counting and incremental PB model counting. Specifically, we investigate the following:"}, {"title": "RQ 1: Projected PB Model Counting Performance", "content": "We conducted extensive evaluations to understand the performance of PBCount2 compared to state-of-the-art projected CNF model counters D4 and GPMC (Lagniez and Marquis 2017; Ryosuke Suzuki and Sakai 2017). We show the results in Table 1 and cactus plots in Figure 4. PBCount2 is able to complete 1957 instances, demonstrating a substantial lead over the 1398 instances of D4 and the 1315 instances of GPMC. Overall, PBCount2 solves around 1.40\u00d7 the number of instances of D4 and 1.49\u00d7 that of GPMC, highlighting the efficacy of PBCount2 in projected PB model counting tasks."}, {"title": "RQ 2: Incremental Counting Performance", "content": "We conducted experiments to analyze the performance of PBCount2's incremental mode against the state-of-the-art PBCount and CNF counters. In our experiments, we looked at the 3-step and 5-step benchmark configurations. The experiments were run with a total timeout of 3600s for the two benchmark configurations. The results are shown in Table 2. In the 3-step benchmarks, PBCount2 shows a considerable performance advantage over D4, GPMC, and PBCount. More specifically, PBCount2 completed 2.07\u00d7 the number of benchmarks completed by D4, 1.78\u00d7 that of GPMC, and 1.16x that of PBCount respectively. Across all 3 incremental benchmark sets for 3-step experiments, PBCount2 performs the same as or better than PBCount. As we move to 5-step benchmarks, we see PBCount2 having a more significant lead over the competing methods. Overall, PBCount2 completes 2.39\u00d7 the number of instances of D4, 1.83\u00d7 that of GPMC, and 1.18\u00d7 that of the original PBCount. Notably when moving from 3-step benchmarks to 5-step benchmarks, the drop in the number of completed incremental benchmarks of PBCount2 is only 40, compared to 56 for PBCount, 46 for GPMC, and 124 for D4. Additionally, we show the cactus plots of each set of incremental benchmarks under 5-step benchmark configuration in Figure 5. The plot further highlights the runtime advantages of PBCount2 over competing approaches. Overall, the evaluations demonstrate the efficacy of PBCount2 at incremental PB model counting, as opposed to existing approaches that can only treat each incremental step as a separate model counting instance."}, {"title": "Discussion", "content": "Through the extensive evaluations, we highlighted PBCount2's superior performance in both projected PB model counting as well as incremental PB model counting benchmarks. To the best of our knowledge, PBCount2 is the first counter to support exact projected PB model counting, and also the first counter to support incremental counting. Despite the practical trade-offs required to support incremental counting, such as the lack of preprocessing and overhead of searching the cache, PBCount2 still demonstrated promising runtime advantages over competing methods. Additionally, PBCount2 also demonstrated the performance benefits of natively handling PB formulas for projected PB model counting, rather than using the convert-and-count approach with state-of-the-art projected CNF counters."}, {"title": "Conclusion", "content": "In this work, we introduced PBCount2, the first exact PB model counter that supports a) projected model counting and b) incremental model counting, using our LOW-MD heuristic and caching mechanism. Our evaluations highlight PBCount2's superior runtime performance in both projected model counting and incremental counting settings, completing 1.40x and 1.18\u00d7 the number of instances of the best competing approaches respectively. While PBCount2 supports incremental counting, it requires preprocessing techniques to be disabled to maintain coherent cached ADD to constraint and variable mappings. It would be of interest to overcome the preprocessing restriction in future works, perhaps by introducing the preprocessing techniques as inprocessing steps or by storing additional metadata about the preprocessing process. We hope the newly introduced capabilities of projection and incrementality lead to wider adoption and increased interest in PB model counting in general."}, {"title": "Algorithm Correctness", "content": "For completeness, we provide the necessary notations, definitions, and proofs adapted from prior work (Dudek, Phan, and Vardi 2021) to show the correctness of our projected model counting algorithm, Algorithm 1 in the main paper."}, {"title": "Definitions", "content": "Definition 1 (Projected Valuation). Let (F, X,Y) be a PB model counting instance, with projection set X, non-projection set Y, and PB formula F. Let T be an X, Y graded project join tree of F. The projected-valuation of each node n $\\in$ V(T), denoted $g_n$, is defined as:\n$g_n = \\begin{cases}  [\\gamma(n)] \\quad \\text{if} \\quad n \\in L(T) \\\\  \\sum_{\\pi(n)} (\\prod_{o\\in ch(n)} g_o) \\quad \\text{if} \\quad n \\in I_X  \\\\  \\exists_{\\pi(n)} (\\prod_{o\\in ch(n)} g_o) \\quad \\text{if} \\quad n \\in I_Y  \\end{cases}$\nWhere $\\[y(n)]$ is the representation of PB constraint $\\gamma(n) \\in$ F and ch(n) refers to the set of immediate child nodes of n in the project join tree.\nDefinition 2 (Early Projection). Let X and Y be sets of variables. For all functions f defined over X and g defined over Y, f : $2^X \\rightarrow$ R, and g : $2^Y \\rightarrow$ R, if x $\\in$ X $\\backslash$ Y then $\\sum_x (f \\cdot g) = (\\sum_x f) \\cdot g$ and $\\exists_x (f \\cdot g) = (\\exists_x f) \\cdot g$"}, {"title": "Additional Notations", "content": "We introduce additional notations that are relevant to the proofs that follow. Let T = (T, r, $\\gamma$, $\\pi$) be a project join tree for PB formula F and n $\\in$ V(T) be a node in T. S(n) $\\in$ V(T) denotes the set of all descendants of n, including itself. In other words, S(n) is the set of vertices of the subtree rooted at n. Let P(n) = $\\cup_{o \\in S(n) \\backslash L(T)} \\pi(o)$ be the set of variables projected in S(n). Let $\\Phi(n) = {\\gamma(l) : l \\in L(T) \\cap S(n)}$ be the set of PB constraints of F that are mapped to the leaf nodes of S(n)."}, {"title": "Proof", "content": "Lemma 1. Let n be an internal node in a project join tree T = (T", "1": "Suppose n is a leaf node", "2": "Now we look at the case where n is an internal node of T", "2a": "n $\\in I_Y$. For each p $\\in$ S(n)", "2b": "n $\\in I_X$. Notice that $\\pi$(n) $\\subseteq X$. Using Definition 1 and Equation 3"}, {}, {"title": "Lemma", "content": "Lemma 3. The computation process in Algorithm 1 follows an X, Y graded project join tree to compute projected valuation at each node.\nProof. We split the proof into two parts, first showing by construction that Algorithm 1 follows an X, Y graded project join tree. Subsequently, we show that the computation at each node is exactly according to the definition of projected valuation.\nLet T be a tree, with leaf nodes having a bijective relationship with the individual ADDs at the start of computation of Algorothm 1 (line 3). Let each merged intermediate ADD $\\psi$ at lines 8 and 13 map to an internal node of T, with the internal node's descendants being all the ADDs (denoted$\\psi$) involved in the merging process at line 7 and 12 respectively. In addition, let the variable projected away in the merge and project"}]}