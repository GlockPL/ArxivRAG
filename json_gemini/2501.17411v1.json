{"title": "A Genetic Algorithm-Based Approach for Automated Optimization of Kolmogorov-Arnold Networks in Classification Tasks", "authors": ["Quan Long", "Bin Wang", "Bing Xue", "Mengjie Zhang"], "abstract": "To address the issue of interpretability in multilayer perceptrons (MLPs), Kolmogorov-Arnold Networks (KANs) are introduced in 2024. However, optimizing KAN structures is labor-intensive, typically requiring manual intervention and parameter tuning. This paper proposes GA-KAN, a genetic algorithm-based approach that automates the optimization of KANs, requiring no human intervention in the design process. To the best of our knowledge, this is the first time that evolutionary computation is explored to optimize KANs automatically. Furthermore, inspired by the use of sparse connectivity in MLPs in effectively reducing the number of parameters, GA-KAN further explores sparse connectivity to tackle the challenge of extensive parameter spaces in KANS. GA-KAN is validated on two toy datasets, achieving optimal results without the manual tuning required by the original KAN. Additionally, GA-KAN demonstrates superior performance across five classification datasets, outperforming traditional methods on all datasets and providing interpretable symbolic formulae for the Wine and Iris datasets, thereby enhancing model transparency. Furthermore, GA-KAN significantly reduces the number of parameters over the standard KAN across all the five datasets. The core contributions of GA-KAN include automated optimization, a new encoding strategy, and a new decoding process, which together improve the accuracy and interpretability, and reduce the number of parameters.", "sections": [{"title": "I. INTRODUCTION", "content": "ARTIFICIAL neural networks (ANNs) have garnered significant attention for decades. ANNs are computational algorithms used to model data in scenarios where it is challenging to extract trends or detect patterns [1]. They have made substantial advancements in various fields, including natural language processing [2] and computer vision [3]. Common ANN architectures [4] include multilayer perceptrons (MLPs), recurrent neural networks (RNNs), and convolutional neural networks (CNNs). Among these, the MLP models are widely regarded as a powerful approach for modeling non-linear functions in machine learning. However, MLPs also have a couple of major limitations. First, as the number of layers or trainable parameters increases, the computational cost becomes significantly higher [5]. Second, MLPs lack interpretability, which can hinder their practical application in some contexts [6], [7].\nTo tackle the challenge of extensive parameter spaces in MLPs, Mocanu et al. [8] proposed a method that uses sparse layers to replace fully connected layers, demonstrating that it is feasible to significantly reduce the number of parameters while maintaining MLP's performance. Consequently, exploring connectivity patterns within neural networks has become a promising direction for model optimization, crucial for enhancing efficiency, performance, and adaptability across various applications.\nIn 2024, Liu et al. [9] introduced a novel neural network architecture known as Kolmogorov-Arnold Networks (KANs) to improve interpretability as a promising alternative to MLPs. Unlike MLPs, which can fit the input-output relationships in data but typically produce implicit rather than explicit symbolic representations, KANs can capture symbolic expressions of input-output relationships. These capabilities provide KANS with potential interpretability that surpasses that of traditional MLPs. A key distinction between KANs and MLPs lies in their architecture: KANs employ learnable activation functions on the edges, while nodes are limited to simple summation operations. [9] pointed out that the parameters of manually constructed KANs are often redundant, offering significant room for optimization.\nSparse connectivity plays a crucial role in neural networks by reducing the number of parameters and enhancing efficiency. KANs have adopted pruning methods to reduce the network size [9]. However, the pruning process requires manual tuning of parameters, which may vary across datasets and require expert knowledge. Additionally, the pruning method restricts the number of layers in KANs, limiting their flexibility in adapting to different datasets and preventing the network from dynamically adjusting its depth to optimize performance. Inspired by the use of sparse connectivity in MLPs, we explore sparse connections in KANs.\nIn our approach, the connections between neurons and the network depth, along with the grid value, are encoded into vectors. Then, genetic algorithms (GAs) are applied to automate the optimization process for the following two major reasons. First of all, GA, known for its low computational cost and effective optimization strategies, is widely used in optimization and design [10]. In addition, GA-based Neural Architecture Search (NAS) has shown promising performance in recent studies [11], making it the preferred choice for this task. This approach aims to identify the optimal network architecture in KANs for given tasks and to address the"}, {"title": "II. BACKGROUND AND RELATED WORKS", "content": "Among optimization methods, Evolutionary Algorithms (EAs), including Genetic Algorithms (GAs) [12], [13], Genetic Programming (GP) [14], Evolutionary Programming (EP) [15], Differential Evolution [16], and Evolution Strategies (ES) [17], are notable for their biological inspiration, leveraging concepts like natural selection, crossover, and mutation to explore optimal solutions. Among these, GAs are considered one of the most established and widely recognized approaches [18].\nGAs mimic natural selection, or \u201csurvival of the fittest,\" by combining the best genes from the fittest individuals of previous generations, as described in Algorithm 1. They operate on a population of individuals, encoded as chromosomes. Evolutionary operations like selection, crossover, and mutation generate new individuals. Selection chooses parents based on defined criteria, while crossover swaps chromosome sections between two parents, and mutation introduces random variations. Each individual is evaluated using a fitness function, which provides a numerical value to guide the selection process [19]. Through successive generations, the individual with the highest fitness score becomes the optimal solution.\nGA is considered one of the most popular metaheuristic algorithms in practical optimization, design, and application domains [10]. GAs feature relatively low computational resource requirements and more persuasive optimization strategies. Moreover, GA-based Neural Architecture Search (NAS) has gathered significant attention in the literature, showing promising performance [11]. For the above reasons, GA is selected for this task.\nIn 2024, Liu et al. [9] made a significant contribution by introducing KANs in a pioneering paper published on arXiv, which quickly attracted widespread interest within the research community. KANs offer a compelling alternative to conventional MLPs, opening up new avenues for the development of modern deep learning models that differ from MLP architectures.\nKANs are fundamentally based on the Kolmogorov-Arnold representation theorem. In 1957, Andrey Kolmogorov addressed Hilbert's 13th problem [20] by proving that any multivariate continuous function can be represented as a sum of single-variable functions and their combinations. This theorem [21], [22] states that any continuous multivariate function f defined on a bounded domain can be expressed as a finite composition of continuous single-variable functions and their sums. For a set of variables $x = x_1, x_2, ..., x_n$, where n denotes the number of variables, the multivariate continuous function f(x) can be represented as follows:\n$f(x) = f(x_1,...,x_n) = \\sum_{q=1}^{2n+1} \\Phi_q(\\sum_{p=1}^{n} \\phi_{q,p}(x_p)),$ (1)\nhere, $\\phi_{q,p} : [0, 1] \\rightarrow R$ and $\\Phi_q : R \\rightarrow R$.\nAlthough the Kolmogorov-Arnold representation theorem is theoretically robust, its application in neural networks remains debated. Girosi and Poggio [23] highlighted potential"}, {"title": "III. THE PROPOSED METHOD", "content": "In this section, the overall framework of the proposed method, named GA-KAN, for automatically optimizing the KAN architecture is presented. Section III-A provides a comprehensive overview of the framework. Each component of the framework will then be elaborated in detail. The encoding strategy is presented in Section III-B, while Section III-C focuses on the decoding method. The crossover and mutation operators are described in Section III-D, followed by an explanation of the fitness evaluation process in Section III-E. Finally, the interpretability offered by the proposed method is illustrated in Section III-F."}, {"title": "A. Overall Framework", "content": "The overall framework of the proposed method is shown in Fig. 1. First, the search space is defined by specifying key hyperparameters, such as the maximal depth, and the maximal number of neurons in each hidden layer. These parameters collectively determine both the range of network architectures explored by the GA method and the chromosome size, which is fixed throughout the optimization process. The decoding process includes the network's depth, with a degradation mechanism introduced to allow decoding the chromosome to networks of varying depths. The degradation mechanism enables the network to reduce from the maximal depth to a smaller depth (see Section III-C for more details). Additionally, other hyperparameters need to be defined, such as the population size and the number of generations. Next, the initial population ($P_t$) is generated with a predefined population size, where each chromosome is randomly initialized with binary values (0 or 1). Each individual in the population undergoes a fitness evaluation.\nThe algorithm then enters the GA loop, where the population undergoes selection, crossover, and mutation, followed by fitness evaluation. This iterative process continues until the stopping criterion is met. The crossover and mutation processes are discussed in Section III-D, while fitness evaluation is detailed in Section III-E.\nFinally, the best-performing individual is identified as the optimal solution, representing the best KAN architecture. The optimized structure offers valuable interpretability in three main aspects: feature importance, feature selection, and symbolic formulae derived from the KAN model (see Section III-F for further details)."}, {"title": "B. Encoding Strategy", "content": "Before initiating the encoding process, the first critical step is to define the search space, which establishes the boundaries for the GA's search. The search space is defined by key hyperparameters: the number of input neurons $n$, the number of output neurons $m$, the maximal depth $d$, and the maximal number of neurons in each hidden layer $u$. While $n$ and $m$ are typically determined by the structure of the dataset, $d$ and $u$ are carefully selected to balance network complexity with available computational resources. These choices directly influence the structure of the chromosomes in GA, particularly their length. Once the search space is defined, the network with the largest possible structure within this space is determined, and the encoding of individual networks is carried out accordingly, which remains fixed throughout the optimization process.\nAs illustrated in Fig. 2, for a given network architecture, the encoding process captures the presence or absence of connections between neurons across layers. Specifically, in each layer $i$, where the number of input neurons is $n_i$ and the number of output neurons is $m_i$, a connection between an input neuron and an output neuron is represented by a bit value of 1, while the absence of a connection is represented by 0. The total number of encoded bits for layer $i$ is thus $n_i \\times m_i$, reflecting all possible connections between neurons in adjacent layers.\nIn addition to the layer encoding, each individual's chromosome also includes $b_{grid}$ bits, representing the grid value of the KAN. The grid value is a critical parameter, set by default to 6 bits, allowing for a representation of values ranging from 1 to 64. This value influences the network's structural configuration, adding another layer of variability to the encoding scheme. Furthermore, the encoding process incorporates $b_{depth}$ (set by default to 2) additional bits to represent the network depth. These depth bits allow the network to decode into structures with varying depths. This approach ensures that the GA method has the flexibility to search through a broader range of network architectures adaptively. Since the minimal depth is 1, the 2 bits representing the depth encode the value of depth 1, as illustrated in Fig. 2. With 2 bits, the depth can be encoded to represent depths ranging from 1 to 4.\nThe total number of bits required to encode an arbitrary network architecture is given by Eq. (5).\n$b_{total} = \\sum_{i=1}^{d} (n_i \\times m_i) + b_{grid} + b_{depth}$ (5)\nFor instance, consider the network architecture depicted in Fig. 2: the input layer consists of 6 neurons, there are 2 hidden layers each containing 5 neurons, and the output layer has 2 neurons. The encoding of this network requires a total of 73 bits, calculated as 6\u00d75+5\u00d75+5\u00d72+6+2 = 73. Therefore, each individual's chromosome in the population is composed of 73 bits."}, {"title": "C. Decoding Chromosomes to KANS", "content": "In the proposed encoding strategy (see Section III-B), each individual's chromosome, with its fixed length, defines a neural network with the maximal depth by the encoding. However, constructing networks with reduced depths is often desirable for improving computational efficiency and reducing overfitting. To enable this, the degradation mechanism is introduced. The degradation mechanism is activated when an entire layer in the encoding is represented by zeros, indicating no active connections to that layer, as shown in Fig. 3. Consequently, the layer is deactivated (i.e., excluded from the final network architecture), thus reducing the overall depth of the network by one. This dynamic adjustment allows for flexible network depth reduction without requiring explicit manual intervention, which is particularly advantageous in the search for efficient architectures.\nAs illustrated in the upper part of Fig. 3, the degradation mechanism faces a significant challenge: the probability of an entire layer being encoded as zeros is extremely low, due to the large number of bits required to represent each layer. Consequently, the GA method has a very low probability of evolving such structures. To enable the effective utilization of the degradation mechanism, the network depth is directly incorporated into the encoding. During the encoding process, a zero mask is proposed as a strategy to mask out an entire layer. A zero mask refers to a binary array where all the positions associated with the layer are set to 0, effectively disabling the layer's connections and causing it to be excluded from the final network structure, thus allowing the network's depth to be dynamically adjusted during decoding. As shown in the lower part of Fig. 3, a zero mask of the appropriate length is applied to the encoding of the blue layer, where all the encoding bits corresponding to that layer are set to 0. This effectively masks the specific layer, meaning that the corresponding connections are ignored during the decoding process.\nThe combination of zero masks and the degradation mechanism plays a crucial role in maintaining diversity within the population, which is essential for reducing the risk of suboptimal convergence and enhancing the GA's ability to explore a broad range of network architectures. By applying these two methods, an individual's chromosome can be effectively decoded into a corresponding KAN with a well-defined structure, including the number of layers, inter-layer connections, and specific grid values. The zero mask ensures that the final depth of the network matches the target depth, while the degradation mechanism handles inactive layers. As a result, the generated networks vary in depth and complexity, which contributes to a more comprehensive search for optimal solutions during the evolutionary process."}, {"title": "D. Crossover and Mutation", "content": "Crossover and mutation are key operations in GAs, crucial for exploring the solution space and improving population fitness. In GA-KAN, these operations are specifically designed to modify the network encoding to ensure diversity and the potential for generating high-performing offspring networks.\nCrossover occurs between the same neurons of two parent networks, as illustrated in Fig. 4. When crossover takes place, the encoding of identical neuron positions in the parent networks is swapped based on a predetermined probability. Besides the encoding of neurons, the encoding also includes grid values and depth. For these parameters, the crossover operation employs the single-point crossover [38]. Once all swaps are completed, two new individuals are produced. This process allows the offspring to inherit beneficial traits from both parents while introducing new combinations. This results in diverse topologies for the offspring, facilitating the exploration of a broader range of network structures in the search for optimal solutions.\nMutation affects all positions in the encoding. When a mutation occurs, binary values in the encoding are flipped- 0 becomes 1, and 1 becomes 0. Although the probability of mutation is generally low to avoid excessive disruption, it plays a critical role in maintaining population diversity. Mutation introduces new connections or removes existing ones, while also affecting the depth and grid values, preventing the population from prematurely converging to local optima. In GA-KAN, mutation can introduce significant alterations in network architecture, such as forming new pathways or removing existing ones, potentially leading to improvements in network performance.\nThe use of both crossover and mutation in GA-KAN strikes a balance between exploration and exploitation during the search process. Crossover recombines advantageous traits from parent networks, while mutation introduces variations that can lead to novel network structures. By adjusting the probabilities of these operations, the algorithm maintains diversity in the population and continuously explores new regions of the solution space. The interplay between the two operators helps GA-KAN navigate the trade-off between refining existing solutions and discovering new, potentially better network architectures."}, {"title": "E. Fitness Evaluation", "content": "Algorithm 2 describes the details of the fitness evaluation function. The fitness evaluation process is a crucial component of NAS algorithms, including GA-KAN. As described in Algorithm 2, each individual's chromosome is decoded into a KAN structure. This structure defines not only the network's layers and connections but also includes the specific grid values.\nOnce the decoding process is complete, the KAN undergoes a fitness evaluation. This begins by training the network on a designated training dataset ($D_{train}$) for a fixed number of steps ($N_{steps}$), to optimize its performance based on the validation dataset ($D_{val}$). The LBFGS optimizer [39] is used, following the same configuration as in the original KAN paper. Each individual's fitness value is determined by tracking the minimum validation loss ($L_{val}$) across all training iterations.\nThe fitness evaluation serves a dual purpose: it not only assesses the performance of an individual but also functions as a filtering mechanism for invalid network structures. Since chromosomes are initialized randomly, decoded networks may contain invalid connections, where no viable path exists between input and output layers. In such instances, assigning a fitness value of +\u221e (or the maximum fitness) ensures that these non-functional networks are effectively excluded from selection in future generations. This filtering process enables the evolutionary search to concentrate on valid architectures, thereby enhancing the algorithm's efficiency and accelerating convergence toward optimal solutions.\nAdditionally, the fitness evaluation phase in GA-KAN ensures that promising individuals are retained by using the minimum validation loss over multiple training iterations, even if they exhibit variability in performance during the early stages of training. This approach enhances GA-KAN's ability to explore a broad range of network structures while maintaining computational feasibility."}, {"title": "F. Interpretability", "content": "The interpretability of GA-KAN primarily stems from the inherent characteristics of KAN. Our method emphasizes three key aspects: feature importance, feature selection, and symbolic formulae extracted from the network.\nFor feature importance, the feature_score [9] method is used to compute the importance scores of the features through a layer-wise backpropagation process. Starting from the output layer, the method calculates scores for nodes and edges in each layer, propagating them towards the input layer. The final scores are obtained by averaging the values across layers, indicating the contribution of each feature to the model's output and highlighting their relative importance. In terms of feature selection, GA-KAN encodes both network nodes and the connections between them, including features as input nodes. Throughout the evolutionary process, the encoding is optimized, allowing feature combinations that align more closely with the data distribution to achieve higher fitness scores during evaluation. This process effectively guides the model in selecting the most relevant features for the classification task.\nThe extraction of symbolic formulae consists of two fundamental stages. In the first stage, referred to as auto_symbolic [9], the method automatically evaluates all candidate formulae by calculating their coefficients of determination-a statistical measure that indicates how well the data fit a given model, with higher values reflecting a better fit. The formula with the highest score is then selected as the connection function for each edge. This automated approach typically generates symbolic formulae of relatively high quality. However, a manual refinement, which is entirely optional, could be employed to further improve the explainability of the symbolic formulae as the second stage. In this stage, visual analysis is used to examine the shapes of the connection functions, allowing formulae with similar structures to be fixed and assigned as the connection functions. After finalizing the connection functions, the symbolic formulae are further refined through training until they converge to their optimal representation. This process not only enhances the model's interpretability but also provides explicit symbolic relationships that illustrate how predictions are generated."}, {"title": "IV. EXPERIMENT DESIGN", "content": "In this section, the experimental setup used to evaluate the performance of the proposed GA-KAN is outlined. The evaluation was conducted across various benchmark datasets, and the configuration of parameters was carefully selected to ensure a balanced trade-off between exploration and exploitation in GA. GA-KAN is benchmarked against peer competitors to demonstrate its unique performance. Additionally, given that KAN is undergoing rapid development with frequent new releases, stability in the experiments was ensured by using version 0.2.1 throughout this study."}, {"title": "A. Benchmark Datasets", "content": "To validate that GA-KAN optimizes the network structure of KAN without requiring manual tuning of pruning parameters, two simple toy datasets, used in the original KAN paper, were selected to demonstrate both the autonomy and effectiveness of GA-KAN. The two mathematical formulas used in the experiments are presented in Eq. (6), which were employed to construct the corresponding datasets.\n$f(x,y) = e^{sin(x)+y^2}$ (6a)\n$f(x,y) = xy$ (6b)\nAs a pioneering study, the original KAN paper [9] did not conduct an in-depth exploration of classification tasks, primarily focusing on simpler, lower-dimensional datasets. Additionally, it highlighted that the major bottleneck of KANs is their slow training process, with KANs being typically 10x slower than MLPs, even with the same number of parameters. This study employs a GA for NAS, where training KANs is part of the fitness evaluation. As the evaluation is often the most time-consuming stage in ENAS algorithms [45], the computational cost of the proposed GA-KAN could be much higher than other ENAS of evolving MLPs. Therefore, for classification tasks, this study selects relatively low-dimensional datasets for validation. Additionally, these datasets exhibit relatively balanced class distributions without significant data imbalance or missing values, making them easier to work with and reducing the need for complex data preprocessing. This ensures a fair horizontal comparison, allowing for a clear positioning of GA-KAN to other methods.\nFor these purposes, this study employs five publicly available datasets from the UCI Machine Learning Repository: Iris, Wine, Raisin, Rice, and WDBC. Comprehensive details on these datasets are provided in Table I. These datasets cover various classification tasks with different numbers of instances, features, and class distributions. This assortment of datasets provides diverse dimensions and distributions, facilitating a comprehensive evaluation of GA-KAN."}, {"title": "B. Parameter Settings", "content": "In evolutionary algorithms, the performance is highly sensitive to parameter configurations, particularly in terms of crossover, mutation rates, and population size. The parameters used in this study, listed in Table II, were chosen based on preliminary experiments and prior studies to ensure a balanced approach between exploration and exploitation in the search process. The detailed explanations of each parameter in the table are provided as follows.\n1) Network Parameters: A network architecture with a maximal depth of 4 layers, each containing 5 neurons, was employed. This structure was chosen considering the relatively small number of features in the selected dataset. Setting a larger number of layers and neurons would result in a vast search space, affecting search efficiency. Additionally, the grid exploration range was set between 1 and 64. Therefore, these parameters were chosen to ensure a manageable search space while maintaining sufficient capacity for learning.\n2) GA Parameters: The crossover rate was set to 0.9, adhering to the widely accepted common settings in the academic community [46], to facilitate the combination of advantageous traits from parent networks. A mutation rate of 0.5 was adopted to introduce sufficient variability and prevent premature convergence. A population size of 100 was selected to maintain a broad search space, and the number of generations was limited to 20, providing a balance between search depth and computational efficiency.\n3) Fitness Evaluation: The LBFGS optimizer was employed for training, using full-batch gradient descent. The training was conducted for 20 epochs. The dataset was divided into a training set, a validation set, and a test set, ensuring that the test set proportion matched that used by peer competitors on the respective dataset. Additionally, no preprocessing steps were applied to the dataset, apart from from label value conversion."}, {"title": "V. RESULTS AND ANALYSIS", "content": "This section presents a comprehensive analysis of the experimental results. First, the performance of GA-KAN in validating the symbolic formulae used in the two toy datasets from the original KAN paper is evaluated, referencing Eq. (6), along with a visualization of the optimal structure identified by GA-KAN. Next, the classification performance of GA-KAN on the benchmark datasets, presented in Table III and Table IV, is evaluated through comparison with peer competitors to highlight its effectiveness in classification tasks. The convergence behavior of GA-KAN is visualized and analyzed to further demonstrate its capability and efficiency. Finally, the interpretability of the network on two simple datasets is illustrated, focusing on feature importance, feature selection, and symbolic formulae extracted from the network."}, {"title": "A. Toy Datasets Analysis", "content": "In the original KAN paper, several formulae were provided to assess the interpretability of KAN. However, configuring the network requires setting up a larger initial network, followed by pruning operations using penalty factors (e.g., $\\lambda$ and $\\lambda_{entropy}$) [9] to find the optimal structure. Different formulae may require different parameter values, and determining the maximum network size also presents challenges. At this stage, expert knowledge is often required, which can introduce additional challenges.\nGA-KAN addresses these issues by automatically searching for the optimal structure without requiring parameter tuning or prior knowledge. A couple of formulae from the original paper of KAN were selected to verify the effectiveness of GA-KAN, and the results are shown in Fig. 5."}, {"title": "B. Classification Performance Evaluation", "content": "This section presents the performance of GA-KAN on the five datasets along with a comparative analysis against its peer competitors. For the Iris and Wine datasets, which are three-class classification problems, the algorithms compared in the referenced studies [56], [57] did not compute AUC values. Similarly, although the Rice dataset is a binary classification problem, AUC was not reported in the referenced study [55]. Therefore, in Table III, we primarily focus on comparing accuracy. In contrast, for the binary classification problems of the WDBC and Raisin datasets, where the compared algorithms in the referenced studies [58], [59] computed AUC values, we provide a comprehensive comparison of both accuracy and AUC, with the results listed in Table IV. Fig. 6 illustrates the optimization performance of GA-KAN on five different datasets, with an evaluation against standard manually designed KAN architectures.\n1) Iris Dataset: Paper [56] evaluated the classification performance of various algorithms on the Iris dataset. In this study, the Iris dataset was split into 80% training, 10% validation, and 10% testing sets, ensuring the test set size remained consistent with [56]. The results are summarized in Table III, where both GA-KAN and manual KAN achieved an outstanding accuracy of 100%. While GA-KAN achieves the same high accuracy as manual KAN, it offers the added benefit of generating multiple network structures with equivalent performance. This flexibility is particularly advantageous when factors such as the number of connections are considered. Furthermore, as shown in Table V, GA-KAN achieves the same outstanding accuracy with significantly fewer parameters. Additionally, one solution was selected as an interpretability example, as detailed in Section V-C.\n2) Wine Dataset: In [57], the classification performance of various algorithms on the Wine dataset was assessed. In this study, the Wine dataset was split into 70% training and 30% testing sets, consistent with the setup in [57]. To further refine the training process, 20% of the training data was used for validation. The results, summarized in Table III, demonstrate that both GA-KAN and KAN achieved an outstanding accuracy of 100%. Although GA-KAN achieves accuracy comparable to manual KAN, it introduces the advantage of generating diverse network structures with equivalent performance. This capability provides greater adaptability in scenarios where structural considerations, such as the number of connections, are critical. Moreover, as illustrated in Table V, GA-KAN achieves this excellent performance with significantly fewer parameters. To further explore its potential, a specific solution was selected to illustrate interpretability, as described in Section V-C.\n3) Rice Dataset: In [55], 10 different machine learning methods were evaluated on the Rice dataset, with the top three methods listed in Table III. In this study, the Wine dataset was split into 80% training and 20% testing sets, consistent with the setup in [55]. To further refine the training process, 20% of the training data was used for validation. The results are summarized in Table III. GA-KAN achieved the best accuracy of 95.14%, and in this experiment, the AUC was also calculated. Compared to the two baseline KANs, GA-KAN achieved the highest AUC of 0.985.\n4) WDBC Dataset: In [58], five machine learning algorithms were applied to the WDBC dataset, with the dataset split into 75% for training and 25% for testing. To ensure comparability, the same 25% test set split was used in our experiment, with 20% of the training set allocated for validation. Accuracy and AUC are presented in Table IV, along with two baseline KAN configurations. As shown, GA-KAN outperformed all other methods on the WDBC dataset, achieving a perfect accuracy of 100.00% and an AUC score of 1.0.\n5) Raisin Dataset: The study in [59] evaluated various methods on the Raisin dataset, reporting accuracy and AUC results. Additionally, [59] proposed the SVM+GA method, which combines GA for feature selection with SVM for classification, achieving an accuracy of 87.67%. In our experiment, the Raisin dataset was divided into 80% training, 10% validation, and 10% testing sets, with the test set size kept consistent with [59] for comparability. The results are presented in Table IV. For additional comparisons, two manually designed KAN architectures were included as baselines. As shown, GA-KAN outperforms all other methods, achieving the highest accuracy of 90.00% and an AUC score of 0.938."}, {"title": "C. Interpretability", "content": "This section will demonstrate the interpretability of the experimental results using the Iris and Wine datasets as examples. GA-KAN optimizes the structure of KAN, aligning them more closely with the underlying distributions of the datasets. This optimization enhances the understanding of interpretability, allowing for clearer insights into how the models make decisions.\n1) Iris Dataset: Due to multiple network structures achieving the same performance in the final generation stage, the individual with fewer layers was selected as the representative example, the final result as shown in Fig. 8. Fig. 7 demonstrates the feature importance, represented by the attribution scores of the features. Fig. 8a illustrates the original structure of the network after completing the training process. The functional relationship between the category scores and the features can be observed. Initially, an attempt was made to automate the extraction process, but the results were suboptimal. Consequently, manual extraction was performed, and the formulae were derived based on the final structural figure, as mentioned earlier. Fig. 8b represents the result after converting the connections into corresponding formulae. The formulae extracted from the results are as follows:\n$z_1 = 59.08 + 34.97x_2$ (8a)\n$z_2 = 376.42x_1 - 147.15x_2 + 288.7x_3 + 1295.19sin(1.95x_4 - 5.44) - 116.67$ (8b)\n$z_3 = -247.23x_2 + 1953.46x_4 - 303.35sin(1.61x_1 - 0.28) + 1229.87$ (8c)\nhere, $z_1$, $z_2$, and $z_3$ represent the raw scores for categories 1, 2, and 3, respectively. Applying the softmax operation to z yields the probabilities for each category, although this is not shown here. $x_1$ to $x_4$ represent the raw, unnormalized feature values. Eq. (8) have been rounded to two decimal places.\nFrom Fig 7, it is evident that the first two features-petal width and sepal width\u2014are sufficient for distinguishing the iris species. In contrast, the last two features\u2014sepal length and petal length-contribute much less to the classification. Thus, focusing on the width features is generally more effective for identifying the species of an iris flower. Eq. (8) demonstrate the relationships between the features and the category scores. For $z_1$, the relationships with the features are linear, whereas $z_2$ and $z_3$ exhibit nonlinear relationships involving sine functions. The nonlinear relationship indicates that the influence of the features on $z_2$ and $z_3$ is more complex. The sine functions introduce periodicity and nonlinearity, meaning that the effect of changes in the features on $z_2$ and $z_3$ varies depending on the values of the features, rather than remaining constant."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, the overall goal of the proposed GA-KAN- a genetic algorithm-based framework for the automatic optimization of KANs-has been achieved through four specific contributions. Firstly, a new encoding strategy was implemented, encoding the neuron connections, grid values, and depth of KANs into chromosomes. Secondly, a new decoding process was developed, incorporating a degradation mechanism and a zero mask technique, which enables more efficient exploration of diverse KAN configurations. Thirdly, GA-KAN automatically optimizes both the structure and grid values of KANs, requiring no human intervention in the design process and minimal adjustment in the formula extraction process. Lastly, the accuracy, interpretability, and parameter reduction of GA-KAN were validated across multiple experiments. Specifically, GA-KAN was validated using two toy datasets from the original KAN paper, eliminating the need for manual parameter adjustments. GA-KAN achieved 100% accuracy on the Wine, Iris, and WDBC datasets, 90.00% on Raisin, and 95.14% on Rice, surpassing traditional models and outperforming the standard KAN proposed in the original paper. Furthermore, GA-KAN significantly reduced the number of parameters across all five datasets. Additionally, the results for the Wine and Iris datasets provided symbolic formulae, further demonstrating the enhanced interpretability of the model.\nDue to the efficiency limitations of KAN and the high computational cost of NAS, we validated it only on relatively smaller-scale datasets. Looking ahead, future work can expand GA-KAN to other types of tasks like regression to demonstrate its versatility and robustness. Furthermore, exploring strategies for deploying GA-KAN on resource-constrained hardware and scaling it to optimize larger and more complex systems, including both neural networks and datasets, will be essential for enhancing its practical utility. In addition, future work might also need to focus on applying GA-KAN to more challenging problems to further assess its performance.\nWhile GA-KAN offers a compelling framework for neural architecture search and shows significant potential, addressing computational efficiency and hardware optimization will be key for its practical use in demanding environments."}]}