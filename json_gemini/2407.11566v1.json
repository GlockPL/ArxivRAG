{"title": "TGIF: Text-Guided Inpainting Forgery Dataset", "authors": ["Hannes Mareen", "Dimitrios Karageorgiou", "Glenn Van Wallendael", "Peter Lambert", "Symeon Papadopoulos"], "abstract": "Digital image manipulation has become increasingly accessible and realistic with the advent of generative AI technologies. Recent developments allow for text-guided inpainting, making sophisticated image edits possible with minimal effort. This poses new challenges for digital media forensics. For example, diffusion model-based approaches could either splice the inpainted region into the original image, or regenerate the entire image. In the latter case, traditional image forgery localization (IFL) methods typically fail. This paper introduces the Text-Guided Inpainting Forgery (TGIF) dataset, a comprehensive collection of images designed to support the training and evaluation of image forgery localization and synthetic image detection (SID) methods. The TGIF dataset includes approximately 75k forged images, originating from popular open-source and commercial methods; SD2, SDXL, and Adobe Firefly. Using this data, we benchmark several state-of-the-art IFL and SID methods. Whereas traditional IFL methods can detect spliced images, they fail to detect regenerated inpainted images. Moreover, traditional SID may detect the regenerated inpainted images to be fake, but cannot localize the inpainted area. Finally, both types of methods fail when exposed to stronger compression, while they are less robust to modern compression algorithms, such as WEBP. As such, this work demonstrates the inefficiency of state-of-the-art detectors on local manipulations performed by modern generative approaches, and aspires to help with the development of more capable IFL and SID methods. The dataset can be downloaded at https://github.com/IDLabMedia/tgif-dataset.", "sections": [{"title": "I. INTRODUCTION", "content": "Image manipulation is becoming more accessible with recent image editing tools based on generative AI (GenAI) achieving outstanding levels of photorealism [1], [2]. The advent of both open-source and commercial text-guided approaches, such as Stable Diffusion (SD) [3] and Adobe Firefly, introduced the ability to locally manipulate an image using simple text prompts, even to non-experts. However, as high-quality image manipulation is becoming more accessible, there are great risks to be exploited for malicious purposes such as disinformation, fraud, and fake evidence. Therefore, image forensic methods aim to detect and localize manipulations [4], [5]. While inpainting has long been part of the target manipulation types of state-of-the-art image forgery localization (IFL) methods, it is now revived due to the proliferation of text-to-image generative diffusion models [2]. Previously, inpainting meant the removal of objects [6], [7], whereas text-guided inpainting models enable the addition or adaptation of objects. Text-guided inpainting using GenAI models works differently than traditional inpainting: an image (crop) is given as input to a model, along with a mask of the inpainted area and a textual prompt. Through a series of diffusion and denoising steps, the model then regenerates the entire image, yet only visibly changing the inpainted area [3]. As such, there are two ways of using text-guided inpainting. First, one can splice the inpainted area into the original image. This is most useful when the generative model does not support high resolutions. Second, the fully regenerated image can be used as is. With new models supporting larger resolutions, the latter case is becoming more practical. However, localizing the inpainted region when most forensic cues have been corrupted due to the regeneration of the whole image is a new and hard challenge for IFL algorithms, and untouched by synthetic image detection (SID) methods.\nTraining and evaluating forensic methods requires large and varied datasets. Although several image manipulation datasets exist, text-guided inpainting is typically not included in them. In fact, to the best of our knowledge, the only dataset including text-guided inpainting is COCO-Glide [8], consisting of 512 manipulated images of 256 \u00d7 256 px, created using a single method (GLIDE [9]). The dataset lacks extensive metadata, and does not vary in terms of splicing type and mask type. Hence, there is a need for larger text-guided inpainting datasets that can be utilized for both training and evaluation, consisting of high-resolution images.\nThis paper presents the Text-Guided Inpainting Forgery (TGIF) dataset. The contribution of this work is two-fold. First, we introduce our TGIF dataset in Section III, which can be used for both training and evaluation. The dataset consists of approx. 75k fake images, with resolutions up to 1024 \u00d7 1024 px. We utilize three inpainting models (SD2, SDXL & Adobe Firefly). Additionally, we provide both the manipulated image where the inpainted area is spliced in the original image, as well as the fully-regenerated image, when possible. We provide metadata about the estimated aesthetic quality, as well as image-text matching scores that indicate whether the generated image aligns with the given prompt.\nSecond, we evaluate our TGIF test set on state-of-the-art IFL and SID methods, in Section IV. Additionally, we study the effects of JPEG and WEBP compression on detection performance. As such, we provide valuable insights into the text-guided-inpainting manipulations, how to detect them in practice, and highlight open challenges."}, {"title": "II. RELATED WORK", "content": "A. Image Forgery Localization (IFL)\nTo detect local image manipulations, in which a region of an image was forged whereas the rest remains pristine, a variety of methods have been proposed [4]. In general, IFL methods either aim to detect imperceptible inconsistencies in the image, such as the artifacts originating from compression [10] or the noise pattern introduced by camera sensors [11]), or they aim to detect visible ones, such as boundaries between the forged and pristine regions [12], while others fuse several forensic cues for increased robustness [13]\u2013[15]. Furthermore, there exist a few IFL methods that specifically target in-painting [16]\u2013[18], which exploit specific changes made by inpainting models, such as in image Laplacian [16], or which automatically learn to extract relevant features [17], [18]. However, they have mostly been substituted by more generic subsequent works [8], [19].\nRecent well-performing methods [8], [12], [19]\u2013[23] are mostly based on deep learning, where they train and evaluate on datasets containing diverse manipulation types, such as splicing, copy-move, and traditional inpainting/removal, in order to detect a broad range of of alterations. However, such methods have not been trained to detect fully regenerated images using text-guided generative models.\nB. Synthetic Image Detection (SID)\nEarly popular synthetic image generation approaches were mostly based on Generative Adversarial Networks (GANs), whereas newer ones typically rely on diffusion models (DMs). Similarly, several detection methods were trained on images generated by GANs [24]\u2013[30], whereas newer ones typically focus on DMs or a mix of both [31]\u2013[34].\nIn general, most methods aim to extract features or a fingerprint, and somehow compare that to those typically observed in synthetic images. The type of feature may have a large influence on the model's generalization performance. For example, some methods were trained on GANs, yet demonstrated the ability to detect images generated by DMs [26], [29], [30]. However, state-of-the-art SID methods were trained and evaluated on real and synthetic images, lacking support for images that were inpainted and hence contain both fake and real parts (albeit regenerated by DMs). As such, they lack the ability of localizing regions inpainted by DMs.\nC. Image Forensics Datasets\nA variety of image forensics datasets for IFL and SID exist, and have been listed in recent overview papers [35], [36]. Most relevant to this paper, inpainted images can be found in the NIST16 [6] and DEFACTO [7] datasets. However, the inpainting in them is not guided by a textual prompt, but instead aims to simply remove the selected object through splicing.\nTo the best of our knowledge, the only dataset that includes text-guided inpainting is the COCO-Glide [8], which consists of 512 manipulated images with a resolution of 256 \u00d7 256 px, cropped from the MS-COCO dataset and inpainted us-ing GLIDE [9]. This dataset is small and mainly useful for evaluation. Additionally, the relatively low resolution is already outdated, with new models supporting resolutions up to 1024 \u00d7 1024 px. Moreover, it neither considers the differences between splicing the generated content into the inpainted region of the original image and re-generating the entire picture with the GenAI model, nor does it asses the perceptual quality of the inpainted images. Thus, the forensics community lacks a topic-agnostic dataset that considers local image manipulations performed by generative models, while including localization masks for images that have been fully regenerated."}, {"title": "III. TGIF DATASET", "content": "This section presents our text-guided inpainting forgery dataset, containing both spliced and fully regenerated images; yet only visually different in the inpainted area.\nA. Source of Authentic Data\nWe used the MS-COCO dataset [37] (val2017) to collect authentic images licensed under a Creative Commons Attri-bution 4.0 License, along with captions and object masks (segmentation mask and bounding box). The images contain objects from 80 categories (such as types of animals, food, and furniture). Of those, we excluded knife and frisbee because these prompts were not allowed by Adobe Firefly.\nAlthough the COCO metadata links to Flickr images whose size is limited to 640 pixels on their largest dimension, we edited the Flickr URLs in order to collect images with dimen-sionality of up to 1024 pixels (the largest publicly available resolution) [38].\nFrom each of the object classes, we selected a maximum of 50 images. We excluded images of which their 1024p Flickr URL was not accessible, as well as those with one of their dimensions smaller than 512 pixels. Moreover, the object bounding box resolution should be smaller than 512\u00d7512 px, and their product larger than 642.\nB. Text-Guided Inpainting Generation\nFor each authentic image and depicted object, we generated inpainted variants, where the authentic object was replaced by a generated one of the same class. For example, a real cat is replaced by a generated cat.\nWe utilized three text-guided inpainting methods: a) the open-source SD2 (stabilityai/stable-diffusion-2-inpainting), which constitutes a first-generation high-fidelity diffusion model b) the open-source SDXL (diffusers/stable-diffusion-xl-1.0-inpainting-0.1), representing the second generation and supporting images of higher resolution, and c) the commercial Adobe Firefly (in Adobe Photoshop 25.4.0). The open-source models are encountered in popular inpainting pipelines, such as the Inpaint-Anything [39], while the commercial approaches are more accessible to non-experts. These three generative approaches constitute a representative set of the available approaches for local manipulation through generative models.\nFor SD2, we took a 512\u00d7512 px crop around each object of interest, whereas we took 1024p crops for SDXL, as these are their respective native resolutions. Additionally, the dimensions needed to be a multitude of 8. In Adobe Firefly, we did not need to take any crops, although this may happen internally. To perform the inpainting, a mask of the object is given to select the area to be inpainted. For this, we utilized the segmentation mask and bounding box provided by COCO. Moreover, each inpainting operation of each model generated three variations, which we all included in the dataset.\nAs a textual prompt in SD2 and SDXL, we utilized the object category followed by a caption from the image (i.e., describing the full image). During initial experiments, we noticed that including the caption resulted in better results, which was also observed in related work [40], [41]. However, note that text misalignment between the prompt and the inpainted area could still occur. For Adobe Firefly, we only included the object category as prompt (i.e., only describing the inpainted area), as we noticed that this was sufficient in our initial experiments.\nWe randomly chose several other parameters. For each image, the number of inference steps is a random integer between 10 and 50, and the guidance scale is a random float between 1 and 10. The seed is randomly chosen. These parameters are also saved as metadata.\nWhen inpainting using generative AI, the entire input image is regenerated. That is, although only the masked area has visible changes, also the non-masked area is (imperceptibly) changed by the GenAI algorithm, since the entire image undergoes diffusion. In Adobe Firefly (Photoshop), the GenAI inpainted image is spliced back into the original image, using the provided mask with an additional small gradient border (i.e., a few pixels larger). This is a similar approach as pro-vided by other inpainting pipelines, such as the one provided in HuggingFace diffusers [42]. For consistency, we spliced the SD2 inpainted image into the original image using the same adapted mask. Additionally, we saved the fully regenerated output image as a 512\u00d7512 px image. For SDXL, we only saved the fully regenerated output image as a 1024p image. That is because the SDXL inpainting model discolors the entire image, which is extremely visible when splicing into the original (a known issue [43]).\nIn summary, an authentic image is transformed into multiple manipulated variants. First, two masks are used for its genera-tion (segmentation and bounding box). Then, three inpainting methods are used, each creating three generated variations. For SD2, we saved both the spliced image and fully regenerated (synthetic) image crop. For SDXL, we saved only the fully regenerated (synthetic) image. For Adobe Firefly (Photoshop), we saved only the spliced image. As such, we get the sub-datasets SD2-sp, PS-sp, SD2-fr, and SDXL-fr (sp for splicing and fr for fully regenerated). A high-level diagram of this procedure is shown in Fig. 1.\nThe dataset can be downloaded at https://github.com/IDL abMedia/tgif-dataset. Additionally, we intend to publish the code used for generating the dataset, after acceptance.\nC. Dataset Splits\nWe have separated the full dataset in a training, validation, and test set, in an approximate 80/10/10 setup, based on the authentic images. As such, the training, validation and test set contain 58560, 8184, and 8232 manipulated images, originating from 2440, 341, and 343 unique authentic images. We have taken precaution to avoid leakage. First, we split the dataset based on the object category. That is, the same category does not appear in multiple subsets. Second, we deleted images that would appear in multiple subsets. That is because an image can contain multiple objects, and hence appear multiple times in the dataset (under different categories, with different masks and prompts).\nD. Aesthetic Quality and Image-Text-Matching Scores\nDue to the automatic generation of a large dataset, the quality of the inpainted objects varies significantly. Addi-tionally, the prompt is not always respected by the dif-fusion model. To objectively measure this, we calculated aesthetic quality scores using the neural image assessment (NIMA) [44] (epoch-82 & vgg16-397923af) and generated image quality assessment (GIQA) [45] (pca95-cat & gmm-cat-pca95-full7) metrics. Additionally, we calculated image-text-matching (ITM) scores [46] (Salesforce_blip-itm-base-coco) to measure whether the prompt matches the outcome. We include these values as metadata in the dataset."}, {"title": "IV. BENCHMARK: IFL & SID", "content": "A. Experimental Setup\nWe performed the evaluation on the test set, separating the spliced subsets (SD2-sp and PS-sp) and fully regenerated subsets (SD2-fr and SDXL-fr). Additionally, to make the detection more challenging, we evaluated the performance on compressed variants of the real and fake images. That is, we used the JPEG and WEBP compression standards, using quality factors of 80 and 60.\nFor image forgery localization we use the mean pixel-level F1 score of the fake images as a performance metric, which is the harmonic mean of the precision and recall. We utilize a threshold of 0.5 to decide whether pixels are real or fake. This threshold demonstrates the calibration of the models, which is essential when deploying the models in the wild and requiring human interpretation. We selected several recent high-performing AI-based IFL methods, namely the PSCC-Net [19], SPAN [20], ImageForensicsOSN [21], MVSS-Net++ [12], Mantranet [22], CAT-Net (v2) [23], TruFor [8], and MMFusion [13].\nFor synthetic image detection, as a performance metric, we use the AUC score. We opted to report the threshold-agnostic AUC rather than threshold-specific image-level F1 score, because several SID methods reported high F1 scores while actually demonstrating bad performance. More specifically, these methods often classified both real and fake images as fake, which, due to the class imbalance, significantly boosted the F1 score. Since the AUC represents the trade-off between the false positive rate and the false negative rate for a range of thresholds, it better reflects the detection performance, in this case. We selected several AI-based SID methods available in the SIDBench framework [47], namely CNNDetect [24], DIMD [31], Dire [32], FreqDetect [25], UnivFD [26], Fus-ing [48], GramNet [27], LGrad [28], NPR [29], RINE [33], DeFake [34], and PatchCraft [30]. Some methods provide multiple models, trained on different data. We only show the best performing in this paper. Although some of these methods were trained on GANs, some of them have demonstrated generalization performance to DMs. Hence, also methods trained on GANs were included in our evaluation set.\nB. Image Forgery Localization Methods\nTable I reports the F1 values for all evaluated IFL methods. We highlight F1 values above 0.7. This threshold was chosen arbitrary, but allows to quickly grasp which methods demonstrate good detection performance. For the splicing datasets SD2 and PS, we observe that only CAT-Net, TruFor, and MMFusion report high F1 scores. Additionally, Mantranet only reports a mediocre F1 value for the PS dataset, while it has low performance for SD2. All other methods demonstrate low detection performance.\nFor the fully regenerated datasets SD2-fr and SDXL-fr, no method is able to detect and localize these manipulations. This is as expected, since regenerating the image removes most in-visible traces such as compression artifacts and camera noise. In fact, diffusing and regenerating an image is an existing attack against forgery detection methods [49]. This highlights the threat of local image manipulations using diffusion models that fully regenerate images; these cannot be detected by traditional image forgery localization methods. This threat is enlarged when one considers the widespread use of GenAI models in applications such as super-resolution [50] and image compression [51]. Although these applications can be used with harmless intentions, they may remove traces used by traditional image forgery localization methods.\nWe only analyzed the influence of compression for the best performing methods in the uncompressed case, and only for the splicing datasets. When performing JPEG compression, F1 scores drop significantly. Moreover, this performance drop is amplified when performing WEBP compression. Hence, this demonstrates a need to develop IFL methods that are robust to compression.\nC. Synthetic Image Detection Methods\nTable II shows the performance (AUC) results for all evalu-ated SID methods. We highlight AUC values above 0.8. This threshold was chosen arbitrarily, but allows to quickly grasp which methods demonstrate good detection performance. For the spliced datasets SD2-sp and PS-sp, no SID method is able to detect the synthetic spliced regions. For the fully regenerated datasets, we observe that 6 out of 12 methods are able to detect synthetic images in uncompressed form. Surprisingly, some methods trained on GANs (such as Uni-vFD, LGrad and RINE) demonstrate better performance than methods trained on DMs (such as DeFake). Additionally, only two of those methods survive JPEG compression, and only one survives WEBP compression. Moreover, some methods (such as DIMD) are more robust in detecting SD2 than SDXL, whereas others (such as RINE) are more robust in detecting SDXL than SD2. No single method is robust for both SD2 and SDXL on all evaluated compression levels. This demonstrates that these methods are complementary.\nIn summary, we demonstrated that several state-of-the-art SID methods are able to detect fully regenerated inpainted images by SD2 and SDXL. However, they fail to detect the spliced inpainted images (even though the spliced regions were generated synthetically). Additionally, their performance decreases when exposed to compression, with a notable dif-ference between WEBP and JPEG compression. Finally, the tested SID methods are not able to localize the inpainted area, but rather produce a SID score for the whole image."}, {"title": "V. DISCUSSION & CONCLUSION", "content": "The proposed TGIF dataset represents a significant advance-ment in the resources available for training and evaluating forensic methods. By including high-resolution images and utilizing multiple inpainting methods, as well as providing both spliced and fully regenerated versions, our TGIF dataset addresses the limitations of existing forensic methods and datasets. As such, it is expected to contribute to the devel-opment of more effective forensic tools.\nOur benchmark analysis shows that some of the existing IFL methods are able to detect and localize spliced images, whereas they fail to localize the inpainted area in fully regener-ated images. In contrast, some of the existing SID methods are able to detect fully regenerated images, yet lack the ability to localize the synthetic inpainted area. Additionally, they cannot detect the synthetic data when it is spliced into the image. These limitations highlight the need for new forensic methods, perhaps combining aspects from both IFL and SID methods. Furthermore, we showed that forensic methods suffer from a notable decrease in performance when exposed to com-pression, particularly with WEBP. This highlights the need to include compressed images during training.\nFuture work could extend and incorporate the TGIF dataset for the training of new detection methods to localize manipula-tions in fully regenerated images, with attention to robustness against compression."}]}