{"title": "Enhancing UI Location Capabilities of Autonomous Agents", "authors": ["Jakub Hoscilowicz", "Bartosz Maj", "Bartosz Kozakiewicz", "Oleksii Tymoschuk", "Artur Janicki"], "abstract": "With the growing reliance on digital devices equipped with graphical user interfaces (GUIs), such as computers and smartphones, the need for effective automation tools has become increasingly important. Although multimodal large language models (MLLMs) like GPT-4V excel at tasks such as drafting emails, they struggle with GUI interactions, which limits their effectiveness in automating everyday tasks. In this paper, we introduce ClickAgent, a novel framework for building autonomous agents. In ClickAgent, the MLLM handles reasoning and action planning, while a separate UI location model (e.g., SeeClick) identifies the relevant UI elements on the screen. This approach addresses a key limitation of current-generation MLLMs: their difficulty in accurately locating UI elements.\nClickAgent significantly outperforms other prompt-based autonomous agents (such as CogAgent, AppAgent, and Auto-UI) on the AITW benchmark. Our evaluation was conducted on both an Android smartphone emulator and an actual Android smartphone, using the task success rate as the key metric for measuring agent performance.", "sections": [{"title": "1 Introduction", "content": "Autonomous agents capable of interacting with graphical user interfaces (GUIs) are becoming critical for automating tasks on digital devices such as smartphones and computers (Kapoor et al., 2024). Researchers have begun developing agent-oriented large language models (LLMs) (Chen et al., 2023a; Zeng et al., 2023), but the potential of language-only agents is limited in real-world applications, where interaction with GUIs is often required. Multimodal LLMs (MLLMs) and visual language models (VLMs) offer a promising solution to these limitations. Unlike language-based agents that rely solely on textual data such as HTML (Nakano et al., 2021) or OCR outputs (Rawles et al., 2023), MLLM-based agents directly interpret visual signals from GUIs. Since GUIs are designed for human users, MLLM-based agents can potentially match human performance in understanding and interacting with these interfaces. Moreover, MLLMs bring additional capabilities, like rapid reading and programming, that extend far beyond what most human users can achieve.\nWhile current-generation MLLMs demonstrate reasonable abilities in screen understanding, reasoning, and action planning, they still struggle to accurately locate specific UI elements on screens (Liu et al., 2024). Previous works like AppAgent (Yang et al., 2023) attempt to solve the issue by using both a real-time screenshot and an XML file that details the interactive elements. Each UI element is assigned a unique identifier, either from its resource ID or by combining its class name, size, and content. These identifiers are displayed as semi-transparent numbers overlaid on the screenshot, allowing the MLLM to interact with UI elements without specifying exact positions. Similarly, CoCo-Agent (Ma et al., 2024) enhances element identification by employing tools such as optical character recognition (OCR) and IconNET, which generate fine-grained layouts with readable textual hints. Moreover, ToL Agent (Fan et al., 2024) introduces a hierarchical layout structure using a Tree-of-Lens (ToL) mechanism to represent regions of varying scales in screenshots, built from the Android Screen Hierarchical Layout (ASHL) dataset. Such complex approaches (Yang et al., 2023; Fan et al., 2024; Ma et al., 2024) are error-prone due to the inherent complexity of GUIs and the inconsistencies in XML/HTML files.\nOur contribution lies in the development of ClickAgent, a hybrid autonomous agent that combines MLLM-driven reasoning with a specialized UI location model. Specifically, ClickAgent leverages the InternVL2.0 MLLM for effective screen interpretation and action planning, while a UI loca-"}, {"title": "2 Method", "content": "In this work, we introduce ClickAgent, a prompt-based autonomous agent that combines MLLM-driven action planning and reasoning with a fast, independent UI location model for identifying screen elements. Notably, the UI location model does not rely on XML files; its sole input is the screenshot. ClickAgent's hybrid approach addresses the limitations of current MLLMs, which struggle to accurately locate UI elements (Liu et al., 2024). On the other hand, while models like SeeClick (Cheng et al., 2024) and Auto-UI (Zhan and Zhang, 2023) excel at identifying UI elements, they lack robust action planning, leading to low success rates in real-world smartphone tasks. To overcome these challenges, ClickAgent integrates InternVL2.0 (Chen et al., 2023b) for screen interpretation and action planning, while a dedicated UI location model identifies the exact coordinates of the target UI elements.\nClickAgent consists of three main components: Decision, UI location and Reflection. In the Decision module, the MLLM is asked to analyze the current screenshot, review the action history, and determine the next step to complete the user's task. The Decision module selects one of the predefined actions:\n\u2022 Click: The MLLM generates a natural language command for the UI location model (e.g., \"click on the Gmail icon.\"). The command is passed to the UI location model, which returns the coordinates of the relevant UI element on the screen.\n\u2022 Type: The MLLM generates the text to be typed into the text field.\n\u2022 OpenApp: If this action is chosen, an additional query is made to the MLLM to select an app from the list retrieved from the Android 14 device.\n\u2022 Swipe: The agent swipes in the specified direction (up, down, left, or right).\nFor example, as illustrated in Figure 1, the MLLM chooses Click action and issues a UI action command (\"Click on the Eyes Closed Official Video\") which, along with a screenshot, serves as input to the UI location model. The model then returns the bounding box coordinates of the relevant UI element.\nAfter the action is executed on Android, the next screenshot is captured. In the Reflection module,"}, {"title": "3 Evaluation Method", "content": "We evaluated ClickAgent on both an Android smartphone emulator and a real Android smartphone, using the task success rate (in percentage) as the primary performance metric. This metric assesses whether the agent successfully executed the user's task, making it the most critical measure in autonomous agent evaluation. Unlike other metrics, such as step success rate or action accuracy (Zhan and Zhang, 2023), the task success rate provides a clear and direct reflection of the agent's ability to accomplish user commands. Evaluation was performed manually due to imprecision of current automatic evaluation methods (Pan et al., 2024).\nWe conducted an evaluation of ClickAgent on a subset of the AITW dataset (Rawles et al., 2023). In summary, our agent was evaluated on 155 unique webshop tasks and 432 general tasks. The AITW General consists of tasks related to interacting with everyday smartphone applications, such as navigating menus, sending messages, adjusting settings, and managing files. In contrast, the WebShopping focuses on tasks specific to e-commerce platforms, such as searching for products, filtering results by price or rating, adding items to a cart, and completing the checkout process.\nThe tests were conducted on both an Android smartphone and an Android smartphone emulator in two scenarios:\n\u2022 Cache Removal Scenario: In this more challenging setup, conducted on the emulator, the Android cache was cleared before each test case, ensuring that popups and other first-time user interactions were displayed for each website. This simulates a scenario where the website is being accessed for the first time on the user's device.\n\u2022 No Cache Removal Scenario: In this setup, conducted on the real Android smartphone, the cache was retained, simulating a user who has previously visited the websites. Consequently, popups and first-time distractions were minimized or eliminated."}, {"title": "4 Main Results", "content": "Table 1 presents the main results from the AITW benchmark. ClickAgent consistently outperforms other agents (AppAgent, Auto-UI, and CogAgent), achieving a significantly higher task success rate, regardless of whether the Android cache was cleared or not. As shown in Table 2, the accuracy of the UI location model plays a crucial role in determining the overall task success rate, making it a key factor in the ClickAgent's performance."}, {"title": "4.1 UI Location Model Analysis", "content": "Our primary insight is that TinyClick excels in the OCR task. Therefore, we adjusted the prompt to encourage the MLLM to generate UI commands that incorporate textual information when possible. For instance, rather than producing commands like \"Click on the first Twitter message,\" the MLLM is prompted to return more specific commands such as \"Click on the Twitter message that discusses global warming.\" This single prompt modification led to around 10% improvement in performance on the AITW benchmark."}, {"title": "4.2 ClickAgent Fails Analysis", "content": "On the AITW benchmark, the most common failures of ClickAgent were distributed across the following areas, with the percentages indicating the proportion of total errors attributed to each component:\n\u2022 Reflection Module (47%): In some cases, the agent stops the action too late or too early, even though the task has not been completed. The quality of the Reflection module is directly correlated with the reasoning and screen understanding capabilities of the MLLM. With more advanced MLLMs, the performance of the Reflection module is expected to improve further.\n\u2022 UI Location Model (15%): Some UI elements are unique to specific applications, and certain web pages have outdated designs, making it challenging for the UI Location Model to accurately identify desired elements."}, {"title": "5 Ablation Study", "content": "In this section, we conduct an ablation study to understand the impact of two main components (MLLM and UI location model) on the overall performance of ClickAgent."}, {"title": "5.1 Impact of the UI Location Model", "content": "Table 2 presents the evaluation of the UI location model's impact on ClickAgent's performance, comparing three different models. As expected, the MLLM (InternVL-2.0-76B) shows poor performance in the UI location task, resulting in ClickAgent failing all tasks. The most significant improvement comes from using the recently released TinyClick, which results in a substantially higher success rate than SeeClick."}, {"title": "5.2 Impact of the used MLLM", "content": "Figure 2 illustrates the effect of MLLM general quality on ClickAgent's performance by evaluating four versions of InternVL-2.0 (1B, 7B, 26B, and 76B), alongside Qwen2.0-VL-72B. The results show that the quality of the MLLM plays a critical role in the ClickAgent's performance. Larger models, such as InternVL-2.0-76B, result in significantly higher success rates compared to the smaller variants. Further improvements in the MLLM quality should continue to enhance the ClickAgent's performance (especially in terms of the accuracy of the Reflection module)."}, {"title": "6 Future Work", "content": "Our failure analysis reveals that the ever-evolving nature of UIs presents a significant challenge for autonomous agents. As UIs change, both MLLMs and UI location models require continuous retraining with updated screenshot data to maintain their ability to recognize and interact with new UI elements across apps and websites (Chen et al., 2024). Additionally, these models often struggle with understanding of less popular UI elements or interfaces from less widely-used apps and websites. To address this, data crawling and regular retraining on diverse UI examples is needed to improve quality of core models (Gao et al., 2024).\nAnother promising direction is the Retrieval-Augmented Generation (RAG) method introduced in AppAgent (Yang et al., 2023). In this approach, apps and websites are first explored automatically, generating detailed documentation of their functionalities. During inference, RAG retrieves relevant sections from this documentation, acting as a real-time reference manual for the agent.\nBoth continuous retraining and RAG are promising avenues for enhancing ClickAgent. Combined, they could enable the agent to better adapt to quickly changing UIs and improve its performance on less commonly encountered graphical interfaces.\nAdditionally, instead of relying on a separate UI location model, MLLMs can be fine-tuned to improve their UI location capabilities by incorporating UI location datasets like ScreenSpot (Cheng et al., 2024) into their multi-task training data. This would allow MLLMs to better identify the precise coordinates of UI elements, streamlining the process by enabling them to handle both reasoning and UI location tasks within a single model. However, this approach may not be feasible if proprietary MLLMs are used (e.g., GPT-4V). Moreover, achieving high accuracy in UI location with MLLMs may be more challenging than with tailored models, which are specifically designed to locate regions or coordinates within images."}, {"title": "7 Conclusion", "content": "ClickAgent introduces a hybrid approach to autonomous agents by combining MLLM-driven reasoning with a specialized UI location model. This integration addresses the shortcomings of current MLLMs, which struggle with accurately locating UI elements on screens. By utilizing both reasoning and UI detection, ClickAgent significantly enhances task completion rates, outperforming other state-of-the-art agents, such as AppAgent and Auto-UI, on the AITW benchmark."}, {"title": "Limitations", "content": "The primary limitation of ClickAgent is its high task completion time, averaging around 60 sec. per task on the AITW benchmark. This latency is a common issue among prompt-based autonomous agents (Hong et al., 2023; Yang et al., 2023), largely because MLLM needs to generate detailed descriptions of screenshots. This prolonged inference time poses a challenge for real-time applications, particularly in use cases like voice assistants or other interactive AI systems. Figure 2 illustrates that while the smaller MLLMs offer advantages such as lower latency and the potential for on-device deployment, their reasoning capabilities fall short of the requirements for building a high-quality autonomous agent. These models lack the sophisticated understanding necessary for complex decision-making and UI interactions, which significantly limits their effectiveness in real-world applications. Future research could focus on enhancing the quality of on-device MLLMs to address these limitations.\nWhile alternative paradigms for building autonomous agents, such as Auto-UI, can reduce latency, they come with a significant trade-off in task completion rates, as seen in Table 1. These non-MLLM-based approaches prioritize speed but fail to match the multi-turn reasoning capabilities of prompt-based agents.\nFuture work should focus on reducing the inference time of MLLMs, potentially through advancements in model efficiency, caching mechanisms, or the adoption of specialized hardware. These improvements are crucial for making agents like ClickAgent more suitable for latency-sensitive, commercial applications."}]}