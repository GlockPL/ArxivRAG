{"title": "REDUCING THE SCOPE OF LANGUAGE MODELS WITH CIRCUIT BREAKERS", "authors": ["David Yunis", "Siyu Huo", "Chulaka Gunasekara", "Danish Contractor"], "abstract": "Language models are now deployed in a wide variety of user-facing applications, often for specific purposes like answering questions about documentation or acting as coding assistants. As these models are intended for particular purposes, they should not be able to answer irrelevant queries like requests for poetry or questions about physics, or even worse, queries that can only be answered by humans like sensitive company policies. Instead we would like them to only answer queries corresponding to desired behavior and refuse all other requests, which we refer to as scoping. We find that, despite the use of system prompts, two representative language models can be poorly scoped and respond to queries they should not be addressing. We then conduct a comprehensive empirical evaluation of methods which could be used for scoping the behavior of language models. Among many other results, we show that a recently-proposed method for general alignment, Circuit Breakers (CB), can be adapted to scope language models to very specific tasks like sentiment analysis or summarization or even tasks with finer-grained scoping (e.g. summarizing only news articles). When compared to standard methods like fine-tuning or preference learning, CB is more robust both for out of distribution tasks, and to adversarial prompting techniques. We also show that layering SFT and CB together often results in the best of both worlds: improved performance only on relevant queries, while rejecting irrelevant ones.", "sections": [{"title": "1 INTRODUCTION", "content": "In the past few years Large Language Models have exploded into the popular conscience. One major recent addition is the \"alignment\" process through Reinforcement Learning with Human Feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022), which has made the current generation of language models much less likely to emit toxic content than previous generations (Wolf et al., 2017), and thus much more acceptable for general use. As a result, many businesses and individuals feel more comfortable using these technologies than they would be in the past.\nAs a result, we have generally capable language models which refuse to answer toxic or dangerous queries, but it is still difficult to deploy these language models. Even though they may not emit toxic content as often, they still will happily answer any question, irrelevant or not. This becomes a problem when we wish to deploy language models as products in specific contexts: e.g. shopping bots currently give coding advice\u00b9 or answer other questions, while assistive co-pilots can be taken off course by prompt injections.\nWhile language models have general language capability, there is still a need to scope them for specific uses. Currently this can solved by two-stage approaches like relevance classifiers, or system"}, {"title": "2 RELATED WORK", "content": "Aligning Language Models: The advent of the current era of language models has been marked by a process of aligning language models so that generations are more helpful, and safer for deployment (Ouyang et al., 2022; Bai et al., 2022a). The primary way this is accomplished is through reinforcement learning with human feedback (RLHF) (Christiano et al., 2017) which was first proposed in robotic simulation tasks. RLHF proceeds by collecting preference pairs of completions, and training a reward model from human judgments on those preference pairs, then performing"}, {"title": "3 EXPERIMENTAL SETUP", "content": ""}, {"title": "3.1 SCOPING FOR SPECIFIC TASKS", "content": "We would like to scope language models to provide completions to relevant tasks, and reject queries corresponding to irrelevant tasks. In particular, we assume we are given a set of \u201caccept\u201d queries {xa| xa ~ Ak}, where {Ak} is a set of \u201caccept\u201d tasks, and a set of \"reject\u201d queries {Xr|Xr ~ Rk} where {Rk} is a set of \"reject\u201d tasks. We are given a language model fo : x \u2192 y which predicts completion y from input x, with parameters 0; a classifier g : y \u2194 c\u2208 {0,1} which decides whether a completion is accepted (0) or rejected (1) by the language model. We would like to compute an update \u2206 such that we minimize Exa g(fo+\u25b3(xa)) and maximize Exr g(fo+\u25b3(xr)). Thus we want accept queries to be accepted and reject queries to be rejected\nAs an additional goal, we would like performance on the accept tasks not to degrade. Given a scoring function h: (x, y) \u2192 s \u2208 [0,1] which scores the completion on task performance where 1 is best, we would also like to maximize Exa h(xa, fo+\u25b3(xa))."}, {"title": "3.2 DATASETS", "content": "We conduct many experiments with different mixtures of accept and reject queries. In order to standardize the format, we draw prompts from Super-NaturalInstructions (SNI) (Wang et al., 2022). SNI is a meta-dataset composed of many different \u201ctasks\u201d, sometimes with multiple tasks per dataset, for example generating questions from passages for a reading comprehension dataset, or generating answers to provided questions from the same reading comprehension dataset. Each task, specified by a task instruction, comes with a collection of examples. We use SNI as it is publicly available, and contains a broad range of complex tasks which current language models should be able to perform. To get our training datasets, we first manually select a set of tasks that are straightforward to automatically evaluate, leaving out many more subjective tasks that may require a human reader. We then group those tasks that we select by category provided from SNI. Details and statistics on categories are provided in Table 1."}, {"title": "3.3 METHODS", "content": "All of the methods we consider in experiments have previously been demonstrated to work in multiple language models, so to reduce the complexity of the experiments and provide a broad range of ablations, we choose to fix the language model. Due to its strong performance and permissive licensing, we base all experiments on the Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) language model.\nFor all methods that require training the language model, we use LoRA (Hu et al., 2022) with default rank 16, a = 16 and dropout of 0.05. We use Adam (Kingma, 2014) without any regularization and tune learning rates (see Appendix B).\nSystem Prompting (Sys.): The simplest method to scope language models is simply to instruct them to refuse irrelevant prompts. For example, for SA the system prompt would be:\nYou are an assistant who only answers request related to Sentiment Analysis. For all other requests you respond \"I cannot answer that.\"\nWith multiple accept categories, we would comma separate the category names (e.g. 11...related to Sentiment Analysis, Text Completion and Summarization...\u201d). This system prompt is prepended to all instructions at evaluation time. This is similar in spirit to methods proposed by Xie et al. (2023); Zhang et al. (2024).\nSupervised Fine-Tuning (SFT): Supervised Fine-Tuning (SFT) consist of tuning the language model to produce particular outputs. For the accept tasks the completions ya are the groundtruth completions provided by the dataset. For the reject tasks, the completions yr are always \u201ccannot answer that.", "DPO)": "We would like to examine a preference learning baseline. Given the complexity of PPO (Schulman et al., 2017), and the need to train a new reward model for each set of tasks, we choose to experiment on Direct Preference Optimization (DPO) (Rafailov et al., 2024), which does not require an additional reward model. As DPO requires pairs of preference data, for accept queries we provide the dataset completion as preferred, and the completion \u201cI cannot answer that.", "I cannot answer that.\" over the ground truth completion. For DPO we tune learning rate, step budget, and the loss weighting term regularizing the KL divergence from the base model predictions. Similar approach to that of Brahman et al. (2024); Cheng et al. (2024).\nProbing Classifier (Probe)": "Probes of representations are a common method to accomplish tasks as they base predictions on the internal state of the language model (Conneau et al., 2018; Tenney et al., 2019; Zou et al., 2023a). Previous work on Circuit Breakers (Zou et al., 2024) showed that probing representations was quite competitive for detecting dangerous language. However, that work only designed probes to function on a single layer of the representations of a language model. Here we design a stronger probe. Once an instruction is fed to the frozen language model, we first remove the first position as that position is quite anomalous due to large magnitude (Xiao et al., 2024), then we average all positions per layer and normalize the average vector to norm 1 so as to match norms between layers. Finally we concatenate the average vectors from each layer and feed that as input to a 2-layer MLP with width 256 which makes a binary classification decision on whether to accept or reject. Only the MLP layers are trained, and we tune the learning rate and step budget. Similar in spirit to work on confidence of LLM (Kadavath et al., 2022; Slobodkin et al., 2023).\nCircuit Breakers (CB): Zou et al. (2024) first introduce a method they call Circuit Breakers (CB) for accepting normal queries while rejecting dangerous ones. We repurpose their method for this task. Essentially given a function rep which extracts the representations of a language model at particular layers, they design an optimization objective with two components: La(xa, \u2206) = ||rep (fo(xa))"}, {"title": "3.4 DETECTING REJECTION", "content": "Ideally one might choose to use a language model judge for detecting rejection (Zheng et al., 2023). However, given the large number of experiments and evaluations in this work, we found it prohibitively expensive to run all the evaluations through a state-of-the-art API judge. We experimented with using hosted language models as judges, with the largest being Llama-3-70B-Instruct (Dubey et al., 2024), but found such detection to have much poorer performance both in precision and recall than the metrics described below.\nAs different methods behave differently, we employ different ways to detect rejection. For all methods besides probing, as the system prompt and tuning will instruct models to respond 111 cannot answer that.\", we catch rejection by string matching for a few different tokens that are synonyms for \"cannot\" at the beginning of the generation. The reason we only match strings early in the generation is that it is possible to switch from reject to accept behavior, so we would like to catch that rejection early on. We do not match strings later on as in practice we never observed language models switching from accept to reject behavior midway through generation and keywords can be used as a part of a legitimate response later in generation. On a sample of 30 completions from accept, reject, and OOD reject sets, we tuned the threshold that such a detector had perfect agreement with manual judgment. This was inspired by common string-based detectors like the one proposed by Zou et al. (2023b) and used by Zeng et al. (2024); Zou et al. (2024).\nFor CB-based methods, the behavior of the \"circuit broken\" generation is quite distinct, where it tends to repeat patterns. As exact-matching does not detect such patterns, in addition to string matching described above which will activate when the system prompt is followed, we catch rejection by the existence of a repeated pattern of 4 or more strings within the response. For more details, see Appendix B. Again on a sample of 30 completions form accept, reject and OOD reject sets, we tuned the threshold for this detector such that it achieved 1 false negative and 0 false positives out of 90 completions. The single false negative was due to to a broken generation of punctuation characters that lacked repetitions. See Appendix C for sample outputs.\nFor Probing, we simply use the binary classification decision from the MLP as the rejection decision.\""}, {"title": "4 EXPERIMENTS", "content": "In this section we explore a number of empirical questions: how robust are scoped LLMs to adversarial prompts, how much diversity is needed for scoping, or whether scoping is possible for multiple tasks simultaneously. We aim to be comprehensive, thus demonstrate results across 2-3 different categories per dataset. Where not detailed, our accept sets will be Sentiment Analysis (SA), Summarization (S) and Program Execution (PE).\nAll experiments contain of evaluations of task performance (Accept Score) on the accept set (which should be high), rejection rate on the in-distribution accept (Accept) set (which should be low) as well as rejection rate on the in-distribution reject set (ID Reject) and out of distribution data (OOD Reject) (which should be high). We describe experiments in broad strokes, and defer precise details on hyperparameters to Appendix B. In the main text we present results for experiments on a representative language model, Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), and we include additional experiments for robustness evaluations for granite-7b-instruct (Sudalairaj et al., 2024) in Appendix A.1. Due to the large volume of experiments we are unable to run all models"}, {"title": "4.1 ROBUSTNESS TO ADVERSARIAL PROMPTS", "content": "The original CB methodology, and many related works discussed, focus on robustness to adversarial prompts. If models are to be deployed, we might expect that users could attempt adversarial attacks against the deployment. Here we ask: if models are scoped, how robust are they to adversarial prompts? Our threat model is of a black-box access. In particular, we assume that the users are allowed to edit only the instruction text, and do not even have access to modifying the system prompt, which would be true for text-based API access. We implement and test a number of different black-box adversarial attacks:\nAdversarial system prompt (Adv.): We insert an adversarial system prompt at the beginning of the instruction, after the original system prompt. This adversarial system prompt is of the same format as the original, but instead of being for the category of the training accept task, it corresponds to the category of the evaluation task.\nBase-64 translation (b64): Following Wei et al. (2023), we translate instructions into base-64, then prompt the language model. After receiving the response if it is valid base-64 (which is very often), we translate it back to text.\nFew-shot prompting (Few-shot): We provide a few-shot example from the evaluation set, where we draw a training query and completion and then prompt the next round with a new query. This is similar to the Many-Shot attack explored by Anil et al. (2024).\nMultiturn prompting (2-turn): We prompt with a full conversation turn of an accept task and accept completion, then a second turn with a request from the evaluation set. This format intends to prime the model to first get into an \"accept\" mode, before answering the evaluation query.\nMultiturn prompting with adversarial system prompt (2-turn+Sys.): This is similar to the attack above, but we add an adversarial system prompt to the beginning of the 2nd turn.\nPrefill: In this attack, append a generic prefilling output (\"Sure, here's a response to your request:\"), to the end of the user instruction. This follows common practice and has been a strong attack (Wei et al., 2023; Zou et al., 2024).\nTree of Attacks with Pruning (TAP): TAP (Mehrotra et al., 2023) is an adversarial prompting method whereby an attacking language model iteratively attempts to jailbreak a target language model (here our model to evaluate). TAP uses an actor-critic loop to rewrite prompts based on whether the language model was jailbroken in the previous step, and explores a tree to find the best performing prompt. For more details and hyperparameters, see Appendix B. It is a strong black-box optimization-based jailbreaking method, and was among the strongest attacks for CB in the original setting (Zou et al., 2024). As TAP is quite expensive to run, we only test 10 prompts per dataset.\nWe show results for all evaluations on Mistral in Figure 2, and defer results for Granite to Figure 5.\nSentiment Analysis: As far as performance, we see that CB and SFT-CB are very similar to Sys. and SFT respectively. SFT-based methods perform best except when distractor turns are added, which may be due to a mismatch between training and evaluation. The rejection rate on the accept task is very low, though the Probe and DPO seem to have a tendency toward over-rejection. In-distribution, both the Probe and SFT-CB seem to perform very well, with DPO in 3rd. Out of distribution there is a similar trend, though the probe suffers from the 2-turn attack. When subject to the strong iterative prompting attack the Probe is best.\nSummarization: Here SFT-CB performs best among all methods, except under TAP prompting where DPO is better. While the Probe has a tendency toward over-rejection on the accept set, all otehr methods perform well. In-distribution, CB and SFT-CB and Probe are strong, while DPO suffers. Out of distribution we see a similar trend. When SFT-CB does poorly, CB itself is still strong.\nProgram Execution: SFT and SFT-CB are the strongest on task performance in all cases. Rejection rates on the accept set are high for the untuned language model (Sys.), hence also for CB which preserves the function. DPO also shows a tendency to reject in multiple cases. In-distribution CB,"}, {"title": "4.2 REJECTION SET DIVERSITY", "content": "One of the most critical questions when attempting to restrict the generations of language models is what data might be necessary to do so. If models overfit to a particular data distribution, then it may be difficult to reject requests that were not specified in the training distribution. Thus, here we ask: how much data diversity is necessary in the rejection set to robustly scope models? If very little diversity is needed, and rejection extends to OOD requests, then adapting models to new deployments becomes quite inexpensive. For setup, we fix the accept sets in this experiment, then vary the diversity of data used in the rejection set monotonically from a single category to many categories. We show results for all evaluations in Figure 3.\nSentiment Analysis: Across the board, SFT and SFT-CB have the best task performance. Sys., CB and Probe all tend to reject the accept set prompts more than others, likely due to the base language model representations. On in-distribution rejection all methods except for system prompting appear to perform well. Out-of-distribution, however, we see that CB and SFT-CB are strongest when data diversity is very poor, and as data-diversity increases they remain quite strong. Probe and DPO catch up once the data becomes quite diverse, while CB falls off. Perhaps the crash in the performance of CB is due to the fact that, with increasing diversity, the model needs to find more orthogonal subspaces and the optimization gets more difficult. With SFT-CB the representations have been changed by the SFT stage, so it could be simpler.\nSummarization: In all cases, SFT based methods perform best on the task. Only Probe appears to reject accept queries, with a rather high rate. In-distribution, Sys. is quite poor, but all other methods appear similar. Out-of-distribution we see a slightly different story to classification, where"}, {"title": "4.3 ACCEPTING MULTIPLE TASKS", "content": "Here we ask: is it possible to still reject tasks well when there are multiple tasks in the accept set. This would be ideal if we would like to allow multiple tasks to pass through the filter, and still be able to scope. Such a setting is natural as most language models will have a few different specific uses, like a programming bot that can write code and also answer questions about documentation. We demonstrate results in Figure 4."}, {"title": "4.4 ADDITIONAL ANALYSIS", "content": "Here we briefly discuss some additional results, deferring full treatment to the Appendix.\nPrecise Scoping: We find that one can scope precisely, (e.g. only News summarization instead of all summarization). For more details, see Appendix A.2.\nEffect of Data Quantity: We find that most methods work quite well with very little data (as little as 128 instances). DPO in particular benefits monotonically, while CB has issues as the dataset scales, perhaps due to the difficulty of simultaneous orthogonalization of many different reject instances, see Appendix A.3 for more details.\nEffect of LORA Rank: Overall, it does appear that rank can have a substantial effect on the performance of methods. While DPO seems to scale monotonically with LoRA rank, CB-based methods have a sweet spot for performance, above which it seems optimization becomes difficult. See detailed analysis in Appendix A.4.\nRepresentation Analysis: We see that SFT and DPO only make changes to representations at the tail end of context, while CB-based methods will change representations across the entire context, which may explain the stronger robustness, SFT-CB layers both of these effects. See Appendix A.5 for more details."}, {"title": "5 DISCUSSION", "content": "Though current language models are generally applicable, there is still a need at deployment time to define the kinds of queries they should be able to answer. Thus we need to scope their abilities. In this work, we conducted a comprehensive empirical study of scoping language models for specific deployments.\nThe general takeaways are many, but firstly we find across many cases that system prompting is very insufficient, and supervised fine-tuning (SFT) to refuse irrelevant queries is also quite poor. Other methods like preference learning (DPO) (Rafailov et al., 2024) or probing representations can be good in some settings. In addition, a recently-method Circuit Breakers (CB) (Zou et al., 2024) can be promising in many circumstances.\nIn particular probing is quite strong for refusal, but also has a tendency to reject queries that should be accepted. In addition, it may expensive in practice, as one needs to design a probe on the pre-trained model so as to allow for as many features as possible, then direct the query to a fine-tuned model. DPO does well primarily when the rejection data distribution is quite broad, both in diversity and quantity, and when those conditions are not fulfilled it can have issues. On the other hand, CB is quite strong even when the rejection data is quite narrow, and performs much better than other methods against adversarial prompting techniques. Layering SFT and CB one after another confers even more benefits and allows us to pack the rejection and additional performance into a single model call. When investigating why these different methods have widely varying behavior, we find that CB causes a much more substantial change to the representation space, which may account for its additional robustness."}]}