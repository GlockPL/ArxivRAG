{"title": "ON THE CONVERGENCE OF FEDPROX WITH EXTRAP-\nOLATION AND INEXACT PROX", "authors": ["Hanmin Li", "Peter Richt\u00e1rik"], "abstract": "Enhancing the FedProx federated learning algorithm (Li et al., 2020) with server-\nside extrapolation, Li et al. (2024a) recently introduced the FedExProx method.\nTheir theoretical analysis, however, relies on the assumption that each client com-\nputes a certain proximal operator exactly, which is impractical since this is virtu-\nally never possible to do in real settings. In this paper, we investigate the behav-\nior of FedExProx without this exactness assumption in the smooth and globally\nstrongly convex setting. We establish a general convergence result, showing that\ninexactness leads to convergence to a neighborhood of the solution. Additionally,\nwe demonstrate that, with careful control, the adverse effects of this inexactness\ncan be mitigated. By linking inexactness to biased compression (Beznosikov et al.,\n2023), we refine our analysis, highlighting robustness of extrapolation to inexact\nproximal updates. We also examine the local iteration complexity required by\neach client to achieved the required level of inexactness using various local opti-\nmizers. Our theoretical insights are validated through comprehensive numerical\nexperiments.", "sections": [{"title": "1 INTRODUCTION", "content": "Distributed optimization is becoming increasingly essential in modern machine learning, especially\nas models grow more complex. Federated learning (FL), a decentralized approach where multiple\nclients collaboratively train a shared model while keeping their data locally to preserve privacy, is\na key example of this trend (Kone\u010dn\u00fd et al., 2016; McMahan et al., 2017). Often, a central server\ncoordinates the process by aggregating the locally trained models from each client to update the\nglobal model without accessing the raw data. The federated average algorithm (FedAvg), introduced\nby McMahan et al. (2017) and Mangasarian & Solodov (1993), is one of the most popular strategies\nfor tackling federated learning problems. The algorithm comprises three essential components:\nclient sampling, data sampling, and local training. During its execution, the server first samples a\nsubset of clients to participate in the training process for a given round. Each selected client then\nperforms local training using stochastic gradient descent (SGD), with or without random reshuffling,\nto enhance communication efficiency, as documented by Bubeck et al. (2015); Gower et al. (2019);\nMoulines & Bach (2011); Sadiev et al. (2022b). FedAvg has proven to be highly successful in\npractice, nevertheless it suffers from client drift when data is heterogeneous (Karimireddy et al.,\n2020).\nVarious techniques have been proposed to address the challenges of data heterogeneity, with\nFedProx, introduced by Li et al. (2020), being one notable example. Rather than having each client\nperform local SGD rounds, FedProx requires each client to compute a proximal operator locally.\nComputing the proximal operator can be regarded as an optimization problem that each client can\nsolve locally. Proximal algorithms are advantageous when the proximal operators can be evaluated\nrelatively easily (Parikh et al., 2014). Algorithms based on proximal operators, such as the proximal\npoint method (PPM) (Rockafellar, 1976; Parikh et al., 2014) and its extension to the stochastic set-\nting (SPPM) (Bertsekas, 2011; Asi & Duchi, 2019; Khaled & Jin, 2022; Richt\u00e1rik & Tak\u00e1c, 2020;\nPatrascu & Necoara, 2018), offer greater stability against inaccurately specified step sizes, unlike\ngradient-based methods. PPM was introduced by Martinet (1972) and expanded by Rockafellar"}, {"title": "1.1 CONTRIBUTIONS", "content": "Our paper makes the following contributions, please refer to Appendix A for notation details.\n\u2022 We provide a new analysis of FedExProx based on Li et al. (2024a), focusing on the case where the\nproximal operators are evaluated inexactly in the globally strongly convex setting, removing the\nneed for the assumption of exact proximal operator evaluations. By properly defining the notion\nof approximation, we establish a general convergence guarantee of the algorithm to a neighbor-\nhood of the solution utilizing the theory of biased SGD (Demidovich et al., 2024). Specifically,\nour algorithm achieves a linear convergence rate of $O\\left(\\exp \\left(-\\frac{K \\mu}{L_y\\left(1+\\gamma L_{\\max }\\right)}\\right)\\right)$ to a neighborhood of the\nsolution, matching the rate presented by Li et al. (2024a).\n\u2022 Building on our understanding of how the neighborhood arises, we propose a new method of\napproximation. This alternative characterization of inexactness eliminates the neighborhood from\nthe previous convergence guarantee, provided that the inexactness is properly bounded, and the\nextrapolation parameter is chosen to be sufficiently small.\n\u2022 By leveraging the similarity between the definitions of inexactness and compression, we enhance\nour analysis using the theory of biased compression (Beznosikov et al., 2023). The improved\nanalysis offers a faster rate of $O\\left(\\exp \\left(-\\frac{K \\mu}{L_y\\left(1+\\gamma L_{\\max }\\right)}\\left(\\frac{1}{\\epsilon_2}\\right)^1\\right)\\right)$, leading to convergence to the exact solution,\nprovided that the inexactness is bounded in a more permissive manner. More importantly, the op-\ntimal extrapolation $1/yL_y$ matches the exact case. This shows that extrapolation aids convergence\nas long as sufficient accuracy is reached, even with inexact proximal evaluations.\n\u2022 We then analyze how the aforementioned approximations can be obtained by each client. As ex-\namples, we provide the local iteration complexity when the client employs gradient descent (GD)\nor Nesterov's accelerated gradient descent (AGD), demonstrating that these approximations are\nreadily achievable. Specifically, for the i-th client, the local iteration complexity is $\\tilde{O}\\left(\\sqrt{1+\\gamma L_i}\\right)$"}, {"title": "1.2 RELATED WORK", "content": "Arguably, stochastic gradient descent (SGD) (Robbins & Monro, 1951; Ghadimi & Lan, 2013;\nGower et al., 2019; Gorbunov et al., 2020) remains one of the foundational algorithm in the field of\nmachine learning. One can simply formulate it as\n$x_{k+1} = x_k - \\eta \\cdot g(x_k)$,\nwhere $\\eta > 0$ is a scalar step size, $g(x_k)$ is a possibly stochastic estimator of the true gradient\n$\\nabla f(x_k)$. In the case when $g(x_k) = \\nabla f(x_k)$, SGD becomes GD. Various extensions of SGD\nhave been proposed since its introduction, examples include compressed gradient descent (CGD)\n(Alistarh et al., 2017; Khirirat et al., 2018), SGD with momentum (Loizou & Richt\u00e1rik, 2017; Liu\net al., 2020), SGD with matrix step size (Li et al., 2024b) and variance reduction (Gower et al., 2020;\nJohnson & Zhang, 2013; Gorbunov et al., 2021; Tyurin & Richt\u00e1rik, 2024; Li et al., 2023). Gower\net al. (2019) presented a framework for analyzing SGD with unbiased gradient estimator in the\nconvex case based on expected smoothness. However, in practice, sometimes the gradient estimator\ncould be biased, examples include SGD with sparsified or delayed update (Alistarh et al., 2018;\nRecht et al., 2011). Beznosikov et al. (2023) examined biased updates in the context of compressed\ngradient descent. Demidovich et al. (2024) provides a framework for analyzing SGD with biased\ngradient estimators in the non-convex setting.\nProximal point method (PPM) was originally introduced as a method to solve variational inequal-\nities (Martinet, 1972; Rockafellar, 1976). The transition to the stochastic case, driven by the need\nto efficiently address large-scale optimization problems, leads to the development of SPPM. Due\nto its stability and advantage over the gradient based methods, it has been extensively studied, as\ndocumented by (Patrascu & Necoara, 2018; Bianchi, 2016; Bertsekas, 2011). For proximal algo-\nrithms to be practical, it is commonly assumed that the proximal operator can be solved efficiently,\nsuch as in cases where a closed-form solution is available. However, in large-scale machine learn-\ning models, it is rarely possible to find such a solution in closed form. To address this issue, most\nproximal algorithms assume that only an approximate solution is obtained, achieving a certain level\nof accuracy (Khaled & Jin, 2022; Sadiev et al., 2022a; Karagulyan et al., 2024). Various notions of\ninexactness are employed, depending on the assumptions made, the properties of the objective, and\nthe availability of algorithms capable of efficiently finding such approximations.\nMoreau envelope was first introduced to handle non-smooth functions by Moreau (1965). It is also\nknown as the Moreau-Yosida regularization. The use of the Moreau envelope as an analytical tool\nto analyze proximal algorithms is not novel. Ryu & Boyd (2014) noted that running a proximal\nalgorithm on the objective is equivalent to applying gradient methods to its Moreau envelope. Davis\n& Drusvyatskiy (2019) analyzed stochastic proximal point method (SPPM) for weakly convex and\nLipschitz functions based on this finding. Recently, Li et al. (2024a) provided an analysis of FedProx\nwith server-side step size in the convex case, based on the reformulation of the problem using the\nMoreau envelope. The role of the Moreau envelope extends beyond analyzing proximal algorithms;\nit has also been applied in the contexts of personalized federated learning (T Dinh et al., 2020) and\nmeta-learning (Mishchenko et al., 2023). The mathematical properties of the Moreau envelope are\nrelatively well understood, as documented by Jourani et al. (2014); Planiden & Wang (2019; 2016).\nProjection methods initially emerged as an effective tool for solving systems of linear equations or\ninequalities (Kaczmarz, 1937) and were later generalized to solve the convex feasibility problem\n(Combettes, 1997). The parallel version of this approach involves averaging the projections of the\ncurrent iterates onto all existing convex sets $X_i$ to obtain the next iterate, a process that is empir-\nically known to be accelerated by extrapolation. Numerous heuristic rules have been proposed to\nadaptively set the extrapolation parameter, such as those by Bauschke et al. (2006) and Pierra (1984).\nOnly recently, the mechanism behind constant extrapolation was uncovered by Necoara et al. (2019),\nwho developed the corresponding theoretical framework. Additionally, Li et al. (2024a) provides ex-\nplanations for the effectiveness of adaptive rules, revealing the connection between the extrapolation\nparameter and the step size of SGD when using the Moreau envelope as the global objective."}, {"title": "2 MATHEMATICAL BACKGROUND", "content": "In this work, we are interested in the distributed optimization problem which is formulated in the\nfollowing finite-sum form\n$\\min _{x \\in \\mathbb{R}^d}\\left{f(x):=\\frac{1}{n} \\sum_{i=1}^n f_i(x)\\right}$\nwhere $x \\in \\mathbb{R}^d$ is the model, $n$ is the number of devices/clients, $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is global objective, each\n$f_i : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is the empirical risk of model $x$ associated with the $i$-th client. Each $f_i(x)$ often has\nthe form\n$f_i(x) := \\mathbb{E}_{\\xi \\sim D_i}[l(x, \\xi)]$,\nwhere the loss function $l(x, \\xi)$ represents the loss of model $x$ on data point $\\xi$ over the training data\n$D_i$ owned by client $i \\in [n] := \\{1, 2, ..., n\\}$. We first give the definitions for the proximal operator\nand Moreau envelope, which we will be using in our analysis.\nDefinition 1 (Proximal operator). The proximal operator of an extended real-valued function $\\phi :$\n$\\mathbb{R}^d \\rightarrow \\mathbb{R} \\cup \\{+\\infty\\}$ with step size $\\gamma > 0$ and center $x \\in \\mathbb{R}^d$ is defined as\n$\\operatorname{prox}_{\\gamma \\phi}(x):=\\arg \\min _{z \\in \\mathbb{R}^d}\\left{\\phi(z)+\\frac{1}{2 \\gamma}\\|z-x\\|^2\\right\\}$.\nIt is well-known that for any proper, closed, and convex function $\\phi$, the proximal operator with any\n$\\gamma > 0$ returns a singleton.\nDefinition 2 (Moreau envelope). The Moreau envelope of an extended real-valued function $\\phi :$\n$\\mathbb{R}^d \\rightarrow \\mathbb{R} \\cup \\{+\\infty\\}$ with step size $\\gamma > 0$ and center $x \\in \\mathbb{R}^d$ is defined as\n$M_\\phi^{\\gamma}(x):=\\min _{z \\in \\mathbb{R}^d}\\left{\\phi(z)+\\frac{1}{2 \\gamma}\\|z-x\\|^2\\right\\}$.\nBy the definition of Moreau envelope, it is easy to see that\n$M_\\phi^{\\gamma}(x)=\\phi\\left(\\operatorname{prox}_{\\gamma \\phi}(x)\\right)+\\frac{1}{2 \\gamma}\\|x-\\operatorname{prox}_{\\gamma \\phi}(x)\\|^2$.\nNot only are their function values related, but for any proper, closed, and convex function $\\phi$, the\nMoreau envelope is differentiable, specifically, we have:\n$\\nabla M_\\phi^{\\gamma}(x)=\\frac{1}{\\gamma}\\left(x-\\operatorname{prox}_{\\gamma \\phi}(x)\\right)$.\nThe above identity indicates that $\\phi$ and $M_\\phi^{\\gamma}$ are intrinsically related. This relationship plays a key\nrole in our analysis. We also need the following assumptions on $f$ and $f_i$ to carry out our analysis.\nAssumption 1 (Differentiability). The function $f_i : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ in (1) is differentiable and bounded\nfrom below for all $i \\in [n]$.\nAssumption 2 (Interpolation regime). There exists $x_* \\in \\mathbb{R}^d$ such that $\\nabla f_i(x_*) = 0$ for all $i \\in [n]$.\nThe same as Li et al. (2024a), we assume that we are in the interpolation regime. This situation\narises in modern deep learning scenarios where the number of parameters, $d$, significantly exceeds\nthe number of data points. For justifications, we refer the readers to Arora et al. (2019); Montanari\n& Zhong (2022). The motivation for this assumption stems from the parallel projection methods (5)\nused to solve convex feasibility problems, where the intersection of all convex sets $X_i$ is assumed to\nbe non-empty, which is precisely the interpolation assumption of each $f_i$ being the indicator function\nof $X_i$.\n$x_{k+1}=\\frac{1}{n} \\sum_{i=1}^n \\Pi_{X_i}\\left(x_k\\right)$.\nIt is known that for (5), the use of extrapolation would enhance its performance both in theory and\npractice (Necoara et al., 2019). Since $\\operatorname{prox}_{\\gamma f_i}(x_k)$ can be viewed as projection to some level set\nof $f_i$, it is analogous to $\\Pi_{X_i}(x_k)$. Therefore, it is reasonable to assume that extrapolation would be\neffective under the same assumption."}, {"title": "3 ABSOLUTE APPROXIMATION IN DISTANCE", "content": "As previously suggested, we assume that each proximal operator is solved inexactly, and we need\nto quantify this inexactness in some way. Notice that client $i$ is required to solve the following\nminimization problem.\n$\\min _{z \\in \\mathbb{R}^d} A_{\\gamma, i}(z):=f_i(z)+\\frac{1}{2 \\gamma}\\|z-x_k\\|^2$,\nwhere $x_k$ is the current iterate and $\\gamma > 0$ is a constant. Since we have assumed each function $f_i$\nis convex, $A_{\\gamma, i}(z)$ is $\\frac{1}{\\gamma}$-strongly convex with $\\operatorname{prox}_{\\gamma f_i}(x_k)$ being its unique minimizer. One of the\nmost straightforward ways to measure inexactness in this case is through the squared distance to the\nminimizer, leading to the following definition.\nDefinition 3 (Absolute approximation). Given a proper, closed and convex function $\\phi : \\mathbb{R}^d \\leftrightarrow \\mathbb{R}$,\nand a step size $\\gamma > 0$, we say that a point $y \\in \\mathbb{R}^d$ is an $\\varepsilon_1$-approximation of $\\operatorname{prox}_{\\gamma \\phi}(x)$, if for some\n$\\varepsilon_1 \\geq 0$,\n$\\|y-\\operatorname{prox}_{\\gamma \\phi}(x)\\|^2 \\leq \\varepsilon_1$.\nIn order to analyze Algorithm 1, we first transform the update rule given in (8) in the following way,\n$x_{k+1}=x_k+\\alpha_k\\left(\\frac{1}{n} \\sum_{i=1}^n \\left(z_{i, k+1}-\\operatorname{prox}_{f_i}\\left(x_k\\right)\\right)+\\frac{1}{n} \\sum_{i=1}^n \\left(\\operatorname{prox}_{f_i}\\left(x_k\\right)-x_k\\right)\\right)$\n$=x_k-\\alpha_k g(x_k)$,"}, {"title": "4 RELATIVE APPROXIMATION IN DISTANCE", "content": "Theorem 1 offers a general theoretical framework for understanding the behavior of Algorithm 1.\nHowever, a key challenge with Algorithm 1 which utilizes inexact proximal solutions that satisfy\nDefinition 3, is that, unless the proximal operators are solved exactly, convergence will always be\nlimited to a neighborhood of the solution. The underlying reason is that, as the algorithm pro-\ngresses, the gradient term in the gradient estimator $g(x_k)$ diminishes, whereas the bias term remains\nunchanged. Building on this observation, we propose employing a different type of approximation,\nspecifically an approximation in relative distance, as defined below.\nDefinition 4 (Relative approximation). Given a convex function $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ and a stepsize $\\gamma > 0$,\nwe say that a point $y \\in \\mathbb{R}^d$ is a $\\varepsilon_2$-relative approximation of $\\operatorname{prox}_{\\gamma \\phi}(x)$, if for some $\\varepsilon_2 \\in [0, 1)$,\n$\\|y-\\operatorname{prox}_{\\gamma \\phi}(x)\\|^2 \\leq \\varepsilon_2 \\cdot \\|x-\\operatorname{prox}_{\\gamma \\phi}(x)\\|^2$."}, {"title": "5 ACHIEVING THE LEVEL OF INEXACTNESS", "content": "To fully comprehend the overall complexity of Algorithm 1, it is essential to examine whether the in-\nexactness in evaluating the proximal operators can be effectively achieved. Since each $\\operatorname{prox}_{\\gamma f_i}(x_k)$\nis computed locally by the corresponding client, the client has access to all the necessary data points\nfor the computation. Thus, the most straightforward approach is to have each client perform GD.\nTheorem 4 (Local computation via GD). Assume Assumption 1 (Differentiability), Assumption 3\n(Individual convexity) and Assumption 4 (Smoothness) hold. The iteration complexity for the $i$-th\nclient to provide an approximation using GD in the $k$-th iteration with local step size $\\eta_i = \\frac{1}{1+\\gamma L_i}$,\nsatisfying Definition 3 is $\\mathcal{O}\\left((1+\\gamma L_i) \\log \\left(\\frac{\\|x_k-\\operatorname{prox}_{f_i}\\left(x_k\\right)\\|^2}{\\varepsilon_1}\\right)\\right)$, and for Definition 4, it is\n$\\mathcal{O}\\left((1+\\gamma L_i) \\log \\left(\\frac{1}{\\varepsilon_2}\\right)\\right)$.\nNote that there are no constraints on $\\varepsilon_1$, and since $\\|x_k-\\operatorname{prox}_{\\gamma f_i}(x_k)\\|^2 \\leq \\|\\nabla f_i(x_k)\\|^2$ by (44),\nit is straightforward to adjust GD to optimize the approximation. However, for $\\varepsilon_2$, we require\n$\\varepsilon_2 < \\frac{\\mu^2}{4 L_{\\max }^2}$. In practice, $\\varepsilon_2$ can be set to a sufficiently small value to satisfy this condition, though\nthis will increase the number of local iterations performed by each client. The complexity bounds\nalso indicate that as the local step size $\\gamma$ increases, it becomes more challenging to compute the\napproximation. Alternatively, other algorithms can be employed to find such an approximation. For\ninstance, by leveraging the structure in (2), SGD can be used as a local solver for the proximal\noperator when computational resources are limited. We can use the accelerated gradient descent\n(AGD) of Nesterov (2004) to obtain a better iteration complexity for each client.\nTheorem 5 (Local computation via AGD). Assume all assumptions mentioned in Theorem 4 hold.\nThe iteration complexities for the $i$-th client to provide an approximation in the $k$-the iteration us-\ning AGD with local step size $\\eta_i = \\frac{1}{1+\\gamma L_i}$, and momentum parameter $\\alpha_i = \\frac{\\sqrt{1+\\gamma L_i}-1}{\\sqrt{1+\\gamma L_i}+1}$, satisfying\nDefinition 3, Definition 4 are\n$\\mathcal{O}\\left(\\sqrt{1+\\gamma L_i} \\log \\left(\\frac{(1+\\gamma L_i) \\cdot \\|x_k-\\operatorname{prox}_{f_i}\\left(x_k\\right)\\|^2}{\\varepsilon_1}\\right)\\right)$, $\\mathcal{O}\\left(\\sqrt{1+\\gamma L_i} \\log \\left(\\frac{1}{\\varepsilon_2}\\right)\\right)$,\nrespectively.\nFinally, we provide numerical evidence to support our theoretical findings. We refer the readers to\nAppendix G for the details of the settings and the corresponding experiments."}, {"title": "6 CONCLUSIONS", "content": "Despite achieving satisfactory results in the full-batch setting, the client sampling setting did not\nyield similar outcomes. This may be attributed to the nature of biased compression, which likely\nrequires adjustments to the algorithm itself for resolution. Nonetheless, we provide the analysis in\nAppendix E for reference. Unlike Li et al. (2024a), the presence of bias makes it unclear how to\nincorporate adaptive step-size rules such as gradient diversity in our case. The only permissible\ninexactness for gradient diversity arises from client sub-sampling in the interpolation regime."}, {"title": "6.2 FUTURE WORK", "content": "There are still open problems to be addressed. For example, can Algorithm 1 be modified to incor-\nporate the benefits of error feedback? Is it possible to eliminate the interpolation regime assumption\nwhile still demonstrating that extrapolation is theoretically beneficial for FedExProx? Another di-\nrection that may be of independent interest is to develop adaptive rules of determining the step size\nfor SGD with biased update."}, {"title": "A NOTATIONS", "content": "Throughout the paper, we use the notation $\\|\\cdot\\|$ to denote the standard Euclidean norm defined on\n$\\mathbb{R}^d$ and $\\langle\\cdot, \\cdot\\rangle$ to denote the standard Euclidean inner product. Given a differentiable function $f :$\n$\\mathbb{R}^d \\rightarrow \\mathbb{R}$, its gradient is denoted as $\\nabla f(x)$. We use the notation $D_f(x, y)$ to denote the Bregman\ndivergence associated with a function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ between $x$ and $y$. The notation $\\inf f$ is used\nto denote the minimum of a function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$. We use $\\operatorname{prox}_{\\gamma \\phi}(x)$ to denote the proximity\noperator of function $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ with $\\gamma > 0$ at $x \\in \\mathbb{R}^d$, and $M_\\phi^{\\gamma}(x)$ to denote the corresponding\nMoreau Envelope. We denote the average of the Moreau envelope of each local objective $f_i$ by the\nnotation $M^\\gamma : \\mathbb{R}^d \\rightarrow \\mathbb{R}$. Specifically, we define $M^\\gamma(x) = \\frac{1}{n} \\sum_{i=1}^n M_{f_i}^\\gamma(x)$. Note that $M^\\gamma(x)$\nhas an implicit dependence on $\\gamma$, its smoothness constant is denoted by $L_\\gamma$. We say an extended\nreal-valued function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\cup \\{+\\infty\\}$ is proper if there exists $x \\in \\mathbb{R}^d$ such that $f(x) < +\\infty$.\nWe say an extended real-valued function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\cup \\{+\\infty\\}$ is closed if its epigraph is a closed\nset. We use the notation $E_k = \\gamma M^\\gamma(x_k) - \\gamma \\operatorname{Min} f$ to denote the function value suboptimality of\n$\\gamma M^\\gamma$ at $x_k$, and $\\Delta_k = \\|x_k - x_*\\|^2$ to denote the squared distance. The notation $\\mathcal{O}(\\cdot)$ is used to\ndescribe complexity while omitting constant factors, whereas $\\tilde{\\mathcal{O}}(\\cdot)$ is used when logarithmic factors\nare also omitted."}, {"title": "B FACTS AND LEMMAS", "content": "Fact 1 (Young's inequality). For any two vectors $x, y \\in \\mathbb{R}^d$, the following inequality holds,\n$\\|x+y\\|^2 \\leq 2 \\|x\\|^2 + 2 \\|y\\|^2$.\nFact 2 (Property of convex smooth functions). Let $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ be differentiable. The following\nstatements are equivalent:\n1. $\\phi$ is convex and $L$-smooth.\n2. $0 \\leq 2 D_\\phi(x, y) \\leq L \\|x - y\\|^2$ for all $x, y \\in \\mathbb{R}^d$.\n3. $\\|\\nabla \\phi(x) - \\nabla \\phi(y)\\|^2 \\leq 2 D_\\phi(x, y)$ for all $x, y \\in \\mathbb{R}^d$.\nThe notation $D_\\phi(x, y)$ denotes the Bregman divergence associate with $\\phi$ at $x, y \\in \\mathbb{R}^d$, defined as\n$D_\\phi(x, y) = \\phi(x) - \\phi(y) - \\langle \\nabla \\phi(y), x - y\\rangle$.\nThe following two facts establish that the convexity and smoothness of a function $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}$\nensure the convexity and smoothness of its Moreau envelope.\nFact 3 (Convexity of Moreau envelope). (Beck, 2017, Theorem 6.55) Let $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\cup \\{+\\infty\\}$ be\na proper and convex function. Then $M^\\gamma$ is a convex function.\nFact 4 (Smoothness of Moreau envelope). (Li et al., 2024a, Lemma 4) Let $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ be a convex\nand $L$-smooth function. Then $M^\\gamma$ is $\\frac{1}{\\gamma}$-smooth.\nThe following fact illustrates the relationship between the minimizer of a function $\\phi$ and its Moreau\nenvelope $M^\\gamma$.\nFact 5 (Minimizer equivalence). (Li et al., 2024a, Lemma 5) Let $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\cup \\{+\\infty\\}$ be a proper,\nclosed and convex function. Then for any $\\gamma > 0$, $\\phi$ and $M^\\gamma$ has the same set of minimizers.\nIn our case, we assume each $f_i$ from (1) is convex and $L_i$-smooth. Therefore by Fact 3 and Fact 4,\nwe know that each $M_{f_i}^{\\gamma}$ is also convex and $\\frac{1+\\gamma L_i}{\\gamma}$-smooth. This means that $M^\\gamma = \\frac{1}{n} \\sum_{i=1}^n M_{f_i}^\\gamma$ is\nalso convex and smooth. We denote its smoothness constant as $L_\\gamma$, and the following fact provides\na range for this constant."}, {"title": "C THEORY OF BIASED SGD", "content": "For completeness, we provide the theory of biased SGD we used to analyze our algorithm in this\npaper. It is adapted from Demidovich et al. (2024), which offers a comprehensive study of various\nassumptions employed in the analysis of SGD with biased gradient updates. In addition, the authors\nintroduced a new set of assumptions, referred to as the Biased ABC assumption, which are less\nrestrictive than all previous assumptions. The authors provided convergence guarantees for SGD\nwith biased gradient updates in the non-convex and convex setting. Specifically, they considered the\ncase of minimizing a function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$,\n$\\min _{x \\in \\mathbb{R}^d} f(x)$,\nwith\n$x_{k+1}=x_k-\\eta g(x_k)$,\nwhere $\\eta > 0$ is the stepsize, $g(x_k)$ is a possibly stochastic and biased gradient estimator. They\nintroduced the biased ABC assumption,"}, {"title": "D THEORY OF BIASED COMPRESSION", "content": "In this section, we present the theory of SGD with biased compression. The theory is adapted from\nBeznosikov et al. (2023). The authors introduced theory for analyzing compressed gradient descent\n(CGD) with biased compressor, both in the single node case and in the distributed case when the\nobjective function is assumed to be strongly convex. Here, we are only concerned with the single\nnode case because distributed compressed gradient descent (DCGD) with biased compressor may\nfail to converge. To address this issue, error feedback mechanism (Seide et al., 2014; Karimireddy\net al., 2019; Richt\u00e1rik et al., 2021) is needed. In the single node case, the authors considered solving\n$\\min _{x \\in \\mathbb{R}^d} f(x)$,\nwhere $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is $\\hat{L}$-smooth and $\\hat{\\mu}$-strongly convex, with the following compressed gradient\ndescent algorithm\n$x_{k+1}=x_k-\\eta \\mathcal{C} (\\nabla f(x_k))$,\nwhere $\\mathcal{C} : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ are potentially biased compression operators, $\\eta > 0$ is a step size. The author\nproved that if certain conditions on $\\mathcal{C}$ is satisfied, a corresponding convergence guarantee can then\nbe established. Three classes of compressor/mapping were introduced."}, {"title": "E ANALYSIS OF INEXACT FEDEXPROX IN THE CLIENT SAMPLING SETTING", "content": "In this section, we will discuss the case where we do client sampling in algorithm 1, we first for-\nmulate the algorithm as below. For the sake of simplicity, we use $\\tau$-nice sampling as an example."}, {"title": "E.1 RELATIVE APPROXIMATION IN DISTANCE", "content": "The failure of biased compression theory: Similar to Theorem 7, we initially apply the theory\nfrom Beznosikov et al. (2023), as it provides improved results in the full-batch scenario. We first\ndefine the compressing mapping $C_i$ in this case,\n$C_i\\left(\\gamma \\nabla M_f^\\gamma(x_k)\\right)=\\frac{1}{\\tau} \\sum_{i \\in \\mathcal{S}_k} \\left(\\nabla M_f^\\gamma(x_k)-\\left(z_{i, k+1}-\\operatorname{prox}_{f_i}(x_k)\\right)\\right)$.\nOne can verify for every $x_k$ and $\\epsilon_2$-approximation $z_{i, k+1}$ of $\\operatorname{prox}_{f_i}(x_k)$, we have"}, {"title": "F PROOF OF THEORE"}]}