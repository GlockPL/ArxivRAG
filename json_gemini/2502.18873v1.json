{"title": "Multi-LLM Collaborative Search for Complex Problem Solving", "authors": ["Sen Yang", "Yafu Li", "Wai Lam", "Yu Cheng"], "abstract": "Large language models (LLMs) often struggle with complex reasoning tasks due to their limitations in addressing the vast reasoning space and inherent ambiguities of natural language. We propose the Mixture-of-Search-Agents (MOSA) paradigm, a novel approach leveraging the collective expertise of multiple LLMs to enhance search-based reasoning. MOSA integrates diverse reasoning pathways by combining independent exploration with iterative refinement among LLMs, mitigating the limitations of single-model approaches. Using Monte Carlo Tree Search (MCTS) as a backbone, MOSA enables multiple agents to propose and aggregate reasoning steps, resulting in improved accuracy. Our comprehensive evaluation across four reasoning benchmarks demonstrates MOSA's consistent performance improvements over single-agent and other multi-agent baselines, particularly in complex mathematical and commonsense reasoning tasks.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) face challenges with complex reasoning, even when augmented with linearized reasoning chains (e.g., Chain-of-Thought), due to the vast reasoning space inherent in the complexity and ambiguity of natural languages. A promising approach is step-wise search-based reasoning, which decomposes the reasoning problem into a traversal over a directed graph, where nodes and edges represent individual reasoning sub-steps distributed across the expansive reasoning space. Related methods have applied various search algorithms to LLMs, such as breadth-first search (BFS), depth-first search (DFS) (Yao et al., 2024; Besta et al., 2024), and best-first search (Hao et al., 2023; Zhang et al., 2024a; Qi et al., 2024).\nA successful search trial is featured with diverse yet effective explorations (Hao et al., 2023; Yao et al., 2024). A straightforward method to enhance diversity involves increasing the temperature, thereby making the probability distribution more uniform. This is typically combined with top-k and top-p sampling to balance diversity and quality. However, as shown in Figure 1, despite these sampling techniques, achieving a balance between diversity and quality remains challenging and necessitates careful tuning. Besides, even with near-optimal sampling parameters, a single LLM might still get trapped in local optima due to constraints inherent in its training data and architectural design.\nTo mitigate this limitation, an alternative solution is to aggregate the specialized strengths of multiple LLMs. Recent work (Wang et al., 2024b) has demonstrated that multiple LLMs can collaboratively enhance their instruction-following capabilities by post-editing each other's responses to the same instruction. Motivated by this progress, we explore leveraging the collective expertise of multiple LLMs for search-based reasoning, which, to the best of our knowledge, has not been previously tested. Figure 1 illustrates the reasoning accuracy on the MATH-500 dataset as a function of search diversity. The performance of search using a single LLM initially improves with increased temperature but subsequently degrades, remaining consistently lower than that of multiple-LLM search.\nIn this work, we propose Mixture-of-Search-Agents (MOSA), an advanced paradigm for step-wise search-based reasoning that aggregates the complementary strengths of multiple LLMs, leveraging both independent and collaborative contributions to search for reasoning sub-steps more"}, {"title": "2. Method", "content": "Search-based methods have been extensively used to tackle complex reasoning tasks, such as coding and mathematics, by breaking these problems into multiple search steps (Zhou et al., 2023b; Yao et al., 2024; Hao et al., 2023). Our proposed paradigm is readily applicable to various search algorithms, with the Monte Carlo Tree Search (MCTS) algorithm (Kocsis & Szepesv\u00e1ri, 2006; Coulom, 2007) adopted as the search backbone in this work. This section first introduces the baseline MCTS-based reasoning method with a single search agent (Hao et al., 2023; Qi et al., 2024) in \u00a7 2.1, followed by our method, which leverages the expertise of multiple LLMs as search agents in \u00a7 2.2."}, {"title": "2.1. Baseline Framework", "content": "Overview Given a problem $x$ and a generator $\\pi^*$, MCTS involves iteratively building a search tree starting from the root node $x_0$. We first define the state space $\\mathcal{S}$ and the action space $\\mathcal{A}$. In our case, each state $s_j \\in \\mathcal{S}$ captures the actions (i.e., reasoning steps) generated so far alongside a specific trajectory in the search tree, while each action $a_j \\in \\mathcal{A}$ represents the next reasoning step based on the current state and the type of action chosen. As shown in the upper part of Figure 2, given the selected node $s_i$ (i.e., the reasoning steps generated so far), a step of Expansion essentially creates a set of child nodes. A child node is created by concatenating $s_i$ with the new action, with that action being the next reasoning step generated by a search agent (e.g., an LLM) given $s_i$.\nAction Space We follow rStar (Qi et al., 2024) to define a comprehensive set of actions into MCTS-based LLM reasoning. The set of actions, $\\mathcal{A} = \\{A_1, A_2, A_3, A_4, A_5\\}$, includes:\n\\begin{itemize}\n    \\item A1: Propose a one-step thought;\n    \\item A2: Propose the remaining thought steps;\n    \\item A3: Propose the next sub-question along with its answer;\n    \\item A4: Answer the sub-question again;\n    \\item A5: Rephrase the question.\n\\end{itemize}\nAmong these actions, we designate $A_3$ as the primary action, comprising a sub-question and its corresponding sub-answer, i.e., action\u2081 = concat(sub_question, sub_answer\u2081). For instance, an action can be \"### Sub-question 3: Does the sum of the previous two digits equal 8? ### Sub-answer 3: The two digits are 3 and 5. We have 3 + 5 = 8, so the answer is yes.\". We consider the other actions along with their effects in an ablation analysis (\u00a7 4.3). We present a detailed illustration of generating new actions, i.e., combinations of sub-question & sub-answer in Algorithm 1. For a given state $s_i$, the algorithm traverses all possible actions, where the final sub-answer for each sub-question is determined by a heuristic function, e.g., majority voting."}, {"title": "Reward Function", "content": "Following Hao et al. (2023); Qi et al. (2024), we consider a simple yet effective reward function: actions that frequently lead to correct final answers are assigned higher rewards. Specifically, $Q(s, a)$, the reward value for node $s$ created by action $a$, receives a positive reward if a trajectory containing node $s$ reaches a correct final answer, and no reward otherwise. Since the gold answer is not available during testing, the confidence given by majority voting is regarded as an approximation of the reward value."}, {"title": "MCTS Iterations", "content": "Typically, each MCTS iteration involves four steps: Selection, Expansion, Simulation, and Back-propagation. To balance exploration and exploitation, we adopt the widely-used Upper Confidence Bounds for Trees (UCT) algorithm (Kocsis & Szepesv\u00e1ri, 2006) for Selection. Formally, a node $s$ is selected to maximize:\n\n$UCT(s, a) = \\frac{Q(s, a)}{N(s, a)} + c \\sqrt{\\frac{ln N_{parent}(s)}{N(s, a)}}$ (1)\n\nwhere $N_{parent}(s)$ is the number of times the parent node of $s$ has been visited, $N(s, a)$ is the number of times node $s$ has been visited, and $c$ is a constant. Once the node $s$ is selected, an Expansion step is performed to add child nodes to $s$. After that, starting from a random child node, a Simulation is performed using the default rollout policy until a terminal node is obtained or a predefined maximum depth is reached. The outcome of the simulation determines the reward, which is then propagated back up the tree during the Back-propagation step. Upon multiple iterations, we consider each leaf node as a solution. In this work, we focus on Expansion, which aims to effectively expand the search space."}, {"title": "Sampling Diversity", "content": "Applying stochastic sampling techniques in LLM generation is essential for introducing diversity to MCTS. As presented in the lower part of Figure 2, given the selected state $s_0$, the sub-questions and the sub-answer candidates are all stochastically sampled using temperature scaling, top-k sampling and nucleus sampling (Holtzman et al., 2020). In \u00a7 4.1, we empirically alter search diversity by manipulating generation temperature for single-LLM search."}, {"title": "2.2. Mixture-of-Search-Agents", "content": "Conventional Monte Carlo Tree Search (MCTS) methods utilizing a single model face two significant limitations: (1) Encouraging search diversity while maintaining generation quality is challenging (Zhang et al., 2020), necessitating meticulous tuning of sampling parameters to balance the trade-off between these aspects; (2) using heuristic metrics like majority voting to determine the final sub-answer can be less accurate when the model favors incorrect search directions. To this end, we explore a simple yet effective alternative, Mixture-of-Search-Agents (MOSA), which employs multiple agents to perform search algorithms like MCTS and utilizes a neural function to refine the candidate stepwise outputs. Firstly, leveraging the distinct distributions from different models intrinsically yields better generation diversity, alleviating the necessity for sampling parameters optimization. Additionally, incorporating a neural function"}, {"title": "3. Experiments", "content": ""}, {"title": "3.1. Baselines", "content": "Few-shot Chain-of-Thought (CoT) (Wei et al., 2023) feeds the LLM with a few demonstrations followed by the input question. Since we are using instruction-tuned LLMs, we format the demonstrations as multi-turn dialogues. In each turn, the human asks a question and then the assistant answers it.\nSelf-Consistency@n (Wang et al., 2023) also adopts the few-shot CoT prompting scheme, but it samples n independent answers per instance. The final answer is then given by majority voting over the n candidate answers. Except for the conventional single-LLM self-consistency experiments, we also evaluate self-consistency with multiple different LLMs. Such a multi-LLM self-consistency setting can be regarded as a simplified version of Wang et al. (2024b), which collects direct answers from various agents and aggregates them with majority voting.\nReasoning-via-Planning (RAP) (Hao et al., 2023) is a representative LLM-based reasoning method using MCTS. We use it as the foundation to apply MOSA. In each search step, RAP generates one or more sub-questions along with their sub-answers. The original RAP paper adopted different reward functions for different types of tasks. In this work, we use the simple self-consistency score as the reward value, which has been shown to be competitive with those manually designed ones in Appendix A.1 of Qi et al. (2024). Note that we ensure the total number of LLM forward calls of a single-LLM method are approximately the same as its multi-LLM counterpart, e.g., RAP versus RAP + MOSA as Proposers in Table 1.\nrStar (Qi et al., 2024) is one of the recent SOTA MCTS-based LLM reasoning methods. The authors proposed a"}, {"title": "3.2. Experimental Settings", "content": "Benchmarks We perform evaluation on four reasoning benchmarks covering different scopes, including three mathematical reasoning datasets (GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), MATH-500 (Hendrycks et al., 2021; Lightman et al., 2023)) and one commonsense reasoning dataset (StrategyQA (Geva et al., 2021)).\nModels We adopt four open-sourced instruction-following LLMs to formulate the LLM pool of MOSA: Llama-3.1-8B-Instruct (Grattafiori et al., 2024), Qwen-2-7B-Instruct (Yang et al., 2024), Ministral-8B-Instruct-2410 (Mistral, 2024), and GLM-4-9B-Chat (GLM et al., 2024). The number of LLMs could also be made larger or smaller, depending on customized choices. Our later experiments will show that benchmark performances are positively correlated with the number of distinct LLMs.\nImplementation Details For few-shot CoT baselines, we report the results of all four LLMs. For other single-LLM baselines, like Self-Consistency@n and RAP, we adopt Llama-3.1-8B-Instruct due to its competitiveness and robustness across various benchmarks. For all experiments regarding sampling from multiple LLMs, we try to maintain a pseudo uniform distribution for the Select LLM function in Algorithm 1. That is, if 7 completions need to be sampled and there are 4 distinct LLMs, we manually assign each LLM to sample one completion and then uniformly sample 3 LLMs out of 4 without replacement to finish the remaining 3 completions. Hyper-parameter settings are listed in Appendix A."}, {"title": "3.3. Main Results", "content": "We report the main results on the four benchmarks in Table 1. Below we highlight our key findings.\nMOSA Leads in Reasoning Tasks RAP + MOSA as Proposers & Aggregators consistently yields superior performances across all datasets (GSM8K, SVAMP, MATH-500, StrategyQA), reaching an average performance (Avg.) of 79.97%. Specifically, it obtains exceptional improvements (+1.8%) over the best baseline on the challenging MATH-500 benchmark, suggesting it is effective at handling complex reasoning problems.\nSynergistic Effect between Multi-Agent Collaboration and Search-based Reasoning MOSA integrates two research paradigms: multi-agent collaboration and search-based reasoning. When applied independently, each achieves moderate improvements, but their combination"}, {"title": "4. Analysis", "content": "We perform a comprehensive analysis on MOSA in this section. Specifically, we scale the diversity of the single-LLM search baseline in \u00a7 4.1 and compare it with MOSA. In \u00a7 4.2, we vary the number of distinct LLMs in MOSA. In \u00a7 4.3, we combine MOSA with the rich set of actions proposed by Qi et al. (2024). Finally, we evaluate variations of MOSA by ablating the numbers of proposers and aggregators in \u00a7 4.4."}, {"title": "4.1. Diversity versus Performance", "content": "For single-LLM search, a common technique to increase generation diversity is to manipulate with decoding hyperparameters, e.g., the sampling temperature. We modify the temperature of the RAP + Single-LLM as Aggregator base-"}, {"title": "4.2. Ablation of LLM Collaboration", "content": "To evaluate the impact of varying the number of different LLMs in MOSA, we conduct an analysis using 1 to 4 LLMs across four benchmarks, prioritizing them in the following order: Llama, GLM, Qwen, Ministral. All four variants utilize approximately the same number of LLM forward calls, ensuring that the only variable is the number of distinct LLMs involved. Figure 5 shows that increasing the number of different LLMs generally correlates with higher reasoning accuracy, except for a slight decrease in performance when the number of LLMs increases from 3 to 4 on MATH-500. This trend indicates that the diverse expertise contributed by different LLMs significantly enhances search-based reasoning performance."}, {"title": "4.3. Support for Extended Action Set", "content": "rStar (Qi et al., 2024) proposes using a comprehensive set of actions in MCTS-based LLM reasoning. Since enriching the action set is orthogonal to our method, we hypothesize that MOSA is compatible with the enlarged action set. The results in Table 2 support our hypothesis. For example, rStar combined with MOSA boosts the reasoning accuracy on MATH-500 from 59.00% to 63.20% (+ MOSA as Proposers) and 63.60% (+ MOSA as Proposers & Aggregators)."}, {"title": "4.4. Ablation of Proposers & Aggregators", "content": "We consider to isolate the effects of MOSA as Proposers and MOSA as Aggregators by ablating the number of distinct LLMs for those two roles. As shown in Table 3, changing the number of distinct proposers to be single yields a larger decrease comparing with ablating the number of aggregators (-1.23% versus -0.47%), suggesting that MOSA brings more benefits as proposers."}, {"title": "5. Related Work", "content": ""}, {"title": "5.1. Reasoning with LLMs", "content": "The recent focus on large language models is partly due to their exceptional performance in solving complex reasoning tasks. A prominent example is Chain-of-Thought (CoT) reasoning (Wei et al., 2023). Recent advancements include self-consistency (Wang et al., 2023), problem decomposition (Zhou et al., 2023b), the use of tools (Gao et al., 2023; Chen et al., 2023), and search-based methods (Hao et al., 2023; Yao et al., 2024; Qi et al., 2024). Among these approaches, MOSA is most closely aligned with search-based reasoning methods."}, {"title": "Search-based Reasoning", "content": "Search-based reasoning has demonstrated effectiveness, particularly for solving complex, multi-step problems (Hao et al., 2023; Yao et al., 2024; Chen et al., 2024c; Zhang et al., 2024a; Chen et al., 2024a; Qi et al., 2024; Zhang et al., 2024b; Zhou et al., 2023a; Koh et al., 2024). One of the recent state-of-the-art systems in this domain is rStar (Qi et al., 2024). rStar introduces two key innovations: (1) expanding the Monte Carlo Tree Search (MCTS) action space from one or two actions to five; and (2) employing a secondary LLM to verify the reasoning trajectories generated by the primary LLM through MCTS. In \u00a7 4.3, we empirically demonstrate that our method is complementary to the enriched action set of rStar."}, {"title": "5.2. LLM Ensemble", "content": "Ensembling, a widely used technique for leveraging the strengths of multiple models, remains highly effective in the era of LLMs. Jiang et al. (2023) proposed pairwise reranking of LLM outputs and fusing multiple responses using a trained generative model. Several studies have proposed training routing functions to match queries with appropriate LLMs (Lu et al., 2023; Shnitzer et al., 2023; Wang et al., 2024a). Others have proposed averaging the output distributions of multiple LLMs (Huang et al., 2024).\nAnother line of research focuses on multi-agent collaboration, where multiple LLMs interact to discuss or debate"}, {"title": "6. Conclusion", "content": "In this work, we investigated a novel paradigm called MOSA. MOSA combines independent exploration and iterative refinement among multiple LLMs to enhance reasoning diversity and accuracy. Experiments across benchmarks demonstrate its consistent advantages over single-LLM and multi-agent baselines, especially in complex tasks. This work underscores the potential of multi-agent collaboration in advancing search-based reasoning."}, {"title": "Impact Statement", "content": "This work aims to contribute to the advancement of reasoning with LLMs. While our research could have various societal implications, none are deemed significant enough to warrant specific mention at this stage."}, {"title": "A. Additional Experimental Settings", "content": ""}, {"title": "A.1. Hyper-parameters", "content": "The default sampling parameters for LLM generation are {temperature=0.75, top_k=40, top_p=0.95}. Across all MCTS experiments, we set the number of rollouts to 8, the number of sub-questions per node to 4, the number of candidate sub-answers per sub-question to 4, the maximum depth allowed to 5."}, {"title": "A.2. Dataset Statistics", "content": "Since we make use of the rStar code base 2 to implement MOSA, we directly adopt the data files released in their git repository. There are 1,319 instances in GSM8K, 1,000 instances in SVAMP, 500 instances in MATH-500, and 687 instances in StrategyQA."}, {"title": "B. Additional Implementation Details for Aggregators", "content": "In this section, we will show the basic instruction and several in-context learning demonstrations for aggregators."}]}