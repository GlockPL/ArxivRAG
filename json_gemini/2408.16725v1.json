{"title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming", "authors": ["Zhifei Xie", "Changqiao Wu"], "abstract": "Recent advances in language models have achieved significant progress. GPT-40, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method \"Any Model Can Talk\". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.", "sections": [{"title": "1 Introduction", "content": "Recent developments in large language models have progressed rapidly, with models becoming increasingly powerful, such as off-the-shelf Llama 3.1 [meta, 2024], Mixtral [mixtral, 2024], Qwen-2 [Yang et al., 2024a], and the well-known GPT-4. As an extension of their capabilities, language models are beginning to master understanding other modalities, exemplified by LLaVA [Liu et al., 2024], Qwen2-Audio [Chu et al., 2024] and Video-llama [Zhang et al., 2023b]. Despite their strength in specific tasks, a significant gap remains that hinders further integration of large language models into daily application: real-time voice interaction capability. GPT-40 [openai, 2024], introduced by OpenAI, is the first model to feature real-time multimodal speech interaction capabilities. It can understand and engage with vision, audio, and text while enabling real-time speech conversations, although it remains closed-source. Other models typically adopt two approaches to incorporate speech capabilities. The first is a cascade method, where the language model generates text, followed by a text-to-speech (TTS) model for audio synthesis. This approach introduces significant latency due to the time required for text generation, severely impacting user experience. The second, an end-to-end method like SpeechGPT [Zhang et al., 2023a], generates text before continuing to generate audio. However, this still requires waiting for text generation. Large language models need real end-to-end speech output capabilities to provide real-time feedback.\nEnhancing models with speech output capabilities is a challenging task, primarily due to four factors: (1) Complexity of Audio Reasoning: Our experiments indicate that direct training for audio modality reasoning is highly challenging, often resulting in incoherent outputs from the model. (2) Model Complexity: Incorporating additional modules for speech input and output increases the overall complexity. (3) Difficulty in Modality Alignment: The reasoning abilities developed for text are difficult to transfer to the audio domain. (4) Resource Demands: Adapting a model's text capabilities to the speech modality requires converting all data labels into audio and retraining, significantly increasing resource consumption.\nIn this paper, we propose Mini-Omni, the first open-source multi-model large language model with real-time conversational capabilities, featuring fully end-to-end speech input and output abilities. It also includes various other audio-to-text functionalities such as Automatic Speech Recognition (ASR). We adapt currently available off-the-shelf methods for discretizing speech tokens and employ the simplest model architecture, making it easy for our model and approach to be adapted by other researchers. Direct audio reasoning poses significant challenges; however, our approach successfully addresses this using only a 0.5B model and a limited amount of synthesized audio data. Importantly, our training framework achieves this without heavy reliance on extensive model capabilities or large volumes of data.\nTo leverage and preserve the original capabilities of the language model, we propose a parallel generation paradigm in which the transformer simultaneously produces audio and text tokens. Subsequently, we observed a minimal impact of the audio modality on text capabilities and further introduced batch-based parallel generation, which significantly enhances the model's reasoning ability during streaming audio output. As a poinerr, we opted not to sacrifice audio quality for a simpler and lower bitrate audio encoder, in order to reduce the complexity of audio inference in the model. However, to ensure audio quality, we selected SNAC [Siuzdak, 2024], a music-grade encoder features 8 layers of codebooks and processes hundreds of tokens per second. Innovatively, we applied text-instructed delayed parallel generation to address the issue of long SNAC codebook sequences. Experiments show that the audio output quality is on par with common TTS systems.\nWe also propose a method that requires minimal training and modification of the original model, enabling other works to rapidly develop their own speech capabilities. We refer to this approach as \"Any Model Can Talk\", designed to achieve speech output using a limited amount of additional data. The approach extend speech capabilities through additional adapters and pre-trained models, fine-tuning with a small amount of synthesized data. This is combined with the aforementioned parallel modeling approach to enable streaming output in the new modality while preserving the original model's reasoning capabilities.\nTo evaluate the capabilities of Mini-Omni, we first assessed its performance on traditional text-to-speech multi-modal tasks, including text-based question answering (textQA), automatic speech recognition (ASR), text-to-speech response, and speech-based question answering (speechQA). The model demonstrated strong proficiency in these fundamental tasks. Additionally, we conduct a series of experiments to investigate the impact on the original model's capabilities and assess the"}, {"title": "effectiveness and variations of our inference method. Preliminary experiments demonstrate that batch parallel inference preserves the model's original capabilities. We will conduct further experiments and provide additional details in due course.\nLastly, we observed that most open-source QA datasets contain mixed code or overly lengthy text, rendering them unsuitable for speech model. To overcome this limitation, we introduce the VoiceAssistant-400K dataset, comprising over 400,000 entries specifically generated by GPT-40 for speech assistant supervised fine-tuning (SFT).\nIn summary, we make the following contributions:", "content": "effectiveness and variations of our inference method. Preliminary experiments demonstrate that batch parallel inference preserves the model's original capabilities. We will conduct further experiments and provide additional details in due course.\nLastly, we observed that most open-source QA datasets contain mixed code or overly lengthy text, rendering them unsuitable for speech model. To overcome this limitation, we introduce the VoiceAssistant-400K dataset, comprising over 400,000 entries specifically generated by GPT-40 for speech assistant supervised fine-tuning (SFT).\nIn summary, we make the following contributions:"}, {"title": "2 Related Work", "content": "Multimodal Understanding Recently, researchers have been increasingly focused on advancing unified models for cross-modal understanding. These approaches typically employ a well-pretrained neural network as the encoder for relevant modalities, using a lightweight adapter to align the encoder's output with the text input of language model. Classical works such as LLaVA [Liu et al., 2024], Flamingo [Alayrac et al., 2022] and BLIP [Li et al., 2022] are used for visual understanding, while in the audio domain, models like Whisper [Radford et al., 2023] and Beats [Chen et al., 2022] are commonly utilized as encoders for semantic and acoustic features. In Llama 3.1, Whisper is employed, while SpeechVerse [Das et al., 2024] leverages WavLM [Hu et al., 2024]; SALMONN [Tang et al., 2023], combine Whisper and Beats to extract features. Such works are often constrained to producing output in the text modality.\nAudio Language Modeling Recently, an increasing number of studies have employed audio tokenization to bridge the gap between audio and text. Audio tokenization converts continuous audio signals into discrete audio tokens, enabling large language models to perform inference and even cross-modal interactions. As a result, a variety of speech-text tasks, such as ASR, TTS, music understanding and generation, and sound editing, can be accomplished. MegaTTS [Jiang et al., 2023] utilized audio codecs for speech synthesis, while efforts like InstructTTS [Yang et al., 2024b], SpearTTS [Kharitonov et al., 2023], and Voicebox [Le et al., 2024] have further explored optimizations in decoding methods and conditioning techniques, employing Diffusion as the converter from tokens to audio.\nReal-Time Human-Machine Interaction Models Since the introduction of GPT-40 [openai, 2024], real-time conversational models have achieved unprecedented results, providing near-instantaneous voice feedback to user inputs, marking a significant milestone for the next generation of multi-modal large models. However, the technical implementations remain proprietary. Models with real-time interaction capabilities are currently scarce. SpeechGPT [Zhang et al., 2023a] is an early end-to-end speech interaction model; however, it still suffers from latency due to the Audio-Text-Text-Audio(A-T-T-A) process, similar to Spectron [Nachmani et al., 2023]. LauraGPT [Chen et al., 2023] also employs a similar approach but not for voice conversation scenario. VITA [Fu et al., 2024] and Qwen-audio2 [Chu et al., 2024] are two models that support voice input, but they output text and rely on external TTS systems for speech synthesis. Mini-Omni is a fully end-to-end speech-to-speech conversational model. Through our exploration, we have identified the biggest challenge in advancing this field: the logical inconsistency in reasoning when only the audio modality is present, which we will address in the following chapter."}, {"title": "3 Mini-Omni", "content": "Our innovation stems from existing methods such as SpeechGPT [Zhang et al., 2023a] and Spectron [Nachmani et al., 2023] utilize the A-T-T-A approach, which mitigates the challenges of direct audio learning by guiding the speech generation process through text. However, generating text first and then audio is suboptimal for real-time dialogue scenarios. To address this, we propose a novel method for simultaneous text and audio generation. This approach hypothesizes that text outputs have higher information density, allowing for the same response with fewer tokens. During the generation of audio tokens, the model effectively conditions on corresponding text tokens, akin to an online TTS system. Prior to generating audio tokens, padding with N tokens ensures that the corresponding text tokens are produced first, allowing this to serve as a hyperparameter adjustment. Additionally, the model can also condition on speaker and style embeddings, facilitating control over speaker characteristics and stylistic elements. In this section, we will detail how we implement our idea step by step."}, {"title": "3.1 Audio Language Modeling", "content": "Consider $Y = (Yi \\in V_{txt} | i = 1, . . ., t_{txt})$ as a text utterance from a vocabulary $V_{txt}$ with length $t_{txt}$. The probability of Y can be expressed as $p(Y) = \\prod_{i=1}^{t_{txt}} P(Yi | Y_1,..., Y_{i-1})$. Now, when dealing with a continuous speech signal, we can convert it into discrete speech tokens (dst), represented as $D = (di \\in V_{dst}|i = 1,\u2026\u2026\u2026, t_{dst})$ using a tokenizer. In this context $V_{dst}$ is the vocabulary of discrete speech tokens. These discrete speech tokens can be treated as spoken language within $V_{dst}$ and modeled in a manner similar to text. We combine text and speech in a new vocabulary $V_{voxt}$ by $V_{voxt} = V_{txt} \\cup V_{dst}$. Therefore, we can model the probability of both speech and text tokens as Z, where $Z = (zi \\in V|i = 1,\u2026\u2026,t)$. This probability is expressed as $p(Z) = \\prod_{i=1}^{t}P(Zi | Z_1,\u2026, Z_{i-1})$, Z represent discrete speech tokens D(V = $V_{dst}$) or text tokens Y (V = $V_{txt}$) or various combinations of Y and D. For the audio and text tokens generated simultaneously, the negative log-likelihood loss can be formulated as in Equation (1)."}, {"title": "3.2 Decoding Strategies", "content": "Audio Generation with text instruction. Language models have undergone substantial advance-ments, demonstrating exceptional reasoning capabilities within the text modality. In response, Mini-Omni has been restructured to transfer these reasoning abilities to streaming audio output through a text-audio parallel decoding approach. This method simultaneously outputs both audio and text tokens, with the audio generated via text-to-speech synthesis, ensuring real-time delivery while leveraging the text-based reasoning strengths. To align with the inputs of large models, all sequences generated in parallel are summed before producing the next token, as illustrated in Figure 1. This approach enables the model to achieve real-time voice output in chat scenarios with minimal first token delay.\nText-delay Parallel Decoding. Parallel generation was first introduced by MusicGen [Copet et al., 2024] to accelerate the music generation process, and we have integrated this approach into the text modality to enhance reasoning capabilities. Parallel decoding is feasible because audio token codebooks used in language model training typically consist of multiple layers; generating all layers simultaneously can significantly increase model speed. For real-time speech output models, parallel decoding is even more critical, allowing for the generation of hundreds of audio tokens per second on standard devices. In this paper, we employ SNAC as the audio encoder, which comprises seven token layers with complementary relationships. Therefore, we employ eight sub-Language Model heads to generate eight tokens, including text, in a single step, while maintaining a one-step delay between adjacent layers. Since audio tokens are derived from text synthesis, the text token is output first, followed by SNAC tokens from the first to the seventh layer. The process of text-first delay parallel decoding we propose is illustrated in Figure 2(b).\nBatch Parallel Decoding. Although the previously introduced parallel generation method effectively transfers reasoning capabilities from the text modality to the audio modality, our experiments reveal that the model's reasoning performance still varies between text and audio tasks, with audio responses tending to be simpler. We hypothesize that this is due to limitations in model capacity or insufficient"}, {"title": "3.3 Any Model Can Talk", "content": "In this section, we present our training methodology. Our approach is designed to preserve the capabilities of the original model as much as possible. This is achieved firstly due to the strong performance of our base model, and secondly because our method can be applied to other works that excel in text output but lack robust speech interaction capabilities.\nAudio Encoding: The audio input primarily focuses on feature extraction from the input audio, with options including Hubert or a separately pretrained audio encoder. Given our focus on speech input, Whisper [Radford et al., 2023] and Qwen2-audio [Chu et al., 2024] also demonstrate effective performance for general audio tasks. For audio output, selecting audio tokens with a multi-codebook approach better captures audio details. We experimented with flattening for audio token modeling, but it resulted in excessively long tokens, which are detrimental to streaming and lead to unstable learning. Instead, parallel decoding, inspired by MusicGen [Copet et al., 2024], employs a delay pattern combined with text conditions, as illustrated in Figure 2.\nThree-Stage Training. Our training methodology is divided into three distinct stages: (1) Modality Alignment. The goal of this stage is to enhance the text model's ability to understand and generate speech. The core model of Mini-Omni is entirely frozen, with gradients allowed only in two adapters. During this stage, we use data from speech recognition and speech synthesis to train the model's speech recognition and synthesis capabilities. (2) Adaption Training. Once the new modality is aligned with the text model's input, the adapters are frozen. In this stage, we focus solely on training the model's text capabilities when given audio inputs, as audio output is simply synthesized from text. The model is trained using data from speech recognition, spoken question answering, and text"}, {"title": "response tasks. (3) Multi-modal Finetuning. In the final stage, the entire model is fine-tuned using comprehensive data. At this point, all model weights are unfrozen and trained. Since the primary modality alignment tasks are handled during adapter training, the original model's capabilities are maximally preserved.\nModel Input Ids. Given the eight parallel output sequences, the input also requires eight sequences, leading to significant complexity. Therefore, we briefly outline the organization of model inputs here. The model can accept either text or audio inputs, which are placed in the corresponding modality sequences. For audio inputs, the input tokens and Whisper features are transformed into tensors of the same dimension via adapters and then concatenated. Depending on the task, we place the <answer> special token in different positions to guide the model's output, achieving multi-modal output. The organization of some tasks is illustrated in Figure 4. Before being fed into the model, all sequences are summed and averaged to integrate features.", "content": "response tasks. (3) Multi-modal Finetuning. In the final stage, the entire model is fine-tuned using comprehensive data. At this point, all model weights are unfrozen and trained. Since the primary modality alignment tasks are handled during adapter training, the original model's capabilities are maximally preserved.\nModel Input Ids. Given the eight parallel output sequences, the input also requires eight sequences, leading to significant complexity. Therefore, we briefly outline the organization of model inputs here. The model can accept either text or audio inputs, which are placed in the corresponding modality sequences. For audio inputs, the input tokens and Whisper features are transformed into tensors of the same dimension via adapters and then concatenated. Depending on the task, we place the <answer> special token in different positions to guide the model's output, achieving multi-modal output. The organization of some tasks is illustrated in Figure 4. Before being fed into the model, all sequences are summed and averaged to integrate features."}, {"title": "4 Experiments", "content": "his section presents the foundational capability test results for Mini-Omni. We first describe the training datasets, data processing methods, and hyperparameters. We then evaluate the model's performance on core tasks like speech recognition and provide several use case examples. We will include all relevant experiments in the next version as soon as possible."}, {"title": "4.1 Datasets", "content": "To establish foundational speech capabilities, we trained the model using three speech recognition datasets totaling approximately 8,000 hours, focusing on speech understanding and synthesis. For text modality, we incorporated 2 million data points from the Open-Orca [OpenOrca] dataset and integrated them with other modalities to preserve textual accuracy. Moss's SFT dataset [Sun et al.,"}, {"title": "4.2 Training Parameters", "content": "Our model is trained on 8 A100 GPUs, utilizing a cosine annealing learning rate scheduler with a minimum learning rate of 4e-6 and a maximum learning rate of 4e-4. Each training epoch consists of 40,000 steps, with batch size 192 for each step. The base language model employs Qwen2-0.5B [Yang et al., 2024a], a transformer architecture with 24 blocks and an internal dimension of 896. The speech encoder uses the Whisper-small encoder, with ASR adapter connected via two-layer MLP, and the TTS adapter extends the original model by adding 6 additional transformer blocks. During fine-tuning, we use learn rate from 4e-6 to 5e-5."}, {"title": "4.3 Experimental Results", "content": "We first evaluated the model's performance on ASR tasks to assess its speech understanding capabili-ties. Basic experiments on speech recognition capabilities were conducted using the four test sets from LibriSpeech [Panayotov et al., 2015]: test-clean, test-other, dev-clean, and dev-other. Results are presented in Table 2, where we compare the accuracy of our adopted speech recognition systems, wav2vec2 [Baevski et al., 2020] and Whisper-small, as well as the VITA [Fu et al., 2024]. The findings indicate that while Mini-Omni's speech recognition performance slightly lags behind Whisper-small's [Radford et al., 2023] decoder, it still achieves an excellent level of audio comprehension."}, {"title": "4.4 Case Study", "content": "Here, we present several cases to demonstrate Mini-Omni's capabilities in speech understanding and reasoning. These examples reveal that speech-based reasoning is somewhat weaker compared to text-based reasoning, highlighting the necessity for batch generation. For more impressive examples, please refer to https://github.com/gpt-omni/mini-omni."}, {"title": "5 Conclusion", "content": "In this work, we introduce Mini-Omni, the first multi-modal model with direct speech-to-speech capabilities. Building on previous approaches that use text-guided speech generation, we propose a parallel text and audio generation method that leverages minimal additional data and modules to rapidly transfer a language model's text capabilities to the audio modality, supporting streaming output interactions with high model and data efficiency. We explore both text-instructed streaming parallel generation and batch parallel generation, which further enhance the model's reasoning ability and efficiency. Our approach successfully addresses challenging real-time dialogue tasks using a model with only 0.5 billion parameters. We have developed the Any Model Can Talk method, based on a pre and post-adapter design, to facilitate rapid speech adaptation of other models with minimal additional training. Additionally, we have released the VoiceAssistant-400K dataset for fine-tuning speech output, designed to minimize the generation of code symbols and assist humans in a voice assistant-like manner. All our data, inference, and training codes will be progressively open-sourced at https://github.com/gpt-omni/mini-omni. We hope to provide guidance and support for other work focused on language model speech interaction."}]}