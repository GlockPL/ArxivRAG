{"title": "RelightVid: Temporal-Consistent Diffusion Model for Video Relighting", "authors": ["YE FANG", "ZEYI SUN", "SHANGZHAN ZHANG", "TONG WU", "YINGHAO XU", "PAN ZHANG", "JIAQI WANG", "GORDON WETZSTEIN", "DAHUA LIN"], "abstract": "Diffusion models have demonstrated remarkable success in image generation and editing, with recent advancements enabling albedo-preserving image relighting. However, applying these models to video relighting remains challenging due to the lack of paired video relighting datasets and the high demands for output fidelity and temporal consistency, further complicated by the inherent randomness of diffusion models. To address these challenges, we introduce RelightVid, a flexible framework for video relighting that can accept background video, text prompts, or environment maps as relighting conditions. Trained on in-the-wild videos with carefully designed illumination augmentations and rendered videos under extreme dynamic lighting, RelightVid achieves arbitrary video relighting with high temporal consistency without intrinsic decomposition while preserving the illumination priors of its image backbone.", "sections": [{"title": "1 INTRODUCTION", "content": "Lighting and its interactions with portraits and objects form the cornerstone of vision and imaging, shaping how we perceive both the physical world and its digital representations. The ability to relight a video-modifying the illumination of foreground subjects as if captured under different lighting scenarios-holds immense potential across various domains such as filmmaking [Richardt et al. 2012], gaming, and augmented reality [Debevec 2008; Li et al. 2022]. By precisely controlling lighting, creators can not only enhance the visual experience but also gain greater artistic flexibility to meet the diverse demands of scenes.\nRelighting dynamic foreground subjects in video under varying lighting conditions remains a significant challenge, particularly in maintaining temporal consistency and realistic lighting interactions. While inverse rendering [Cai et al. 2024; Nam et al. 2018; Xia et al. 2016; Zhang et al. 2021a] can decompose intrinsic properties and lighting, it relies on complex inputs like HDR images [Reinhard 2020] or SH coefficients [Ramamoorthi and Hanrahan 2001] for accurate relighting. In practical scenarios, however, users often prefer simpler input, such as textual guidance or reference background videos as conditions, which limits the applicability of the above methods. Additionally, these techniques struggle with generalization, typically limited to portrait or simple object relighting, and fail to model lighting effectively in complex dynamic scenarios.\nRecent advancements in diffusion models [Blattmann et al. 2023; Dhariwal and Nichol 2021; Ho et al. 2020; Song et al. 2020] trained on large-scale image and video datasets have demonstrated the ability to learn essential dynamics and physical priors. This enables them to perform physical rendering effects without explicitly relying on traditional physical modeling. Specifically, a growing body of works [Jin et al. 2024; Ren et al. 2024; Zhang et al. 2024] focuses on fine-tuning pre-trained diffusion models for tasks such as single-image relighting or illumination manipulation. Notably, IC-Light [Zhang et al. 2024] has emerged as a prominent approach, leveraging high-quality synthetic data and a consistent lighting loss function to achieve albedo-preserving relighting. However, extending such image-based techniques to videos introduces significant challenges. For example, a direct approach is applying IC-Light on a per-frame basis, but leads to substantial temporal inconsistencies. This stems from the inherent uncertainty in generative models, where the same input can yield multiple plausible outputs. Furthermore, the scarcity of real or synthetic video relighting datasets presents another challenge. These datasets are crucial for fine-tuning models, as they enable the model to learn both the temporal consistency of illumination and the priors for complex dynamic light interactions, ensuring the naturalness and realism of generated relighting video.\nTo address these challenges, we introduce RelightVid, a flexible framework lifting the capabilities of IC-Light [Zhang et al. 2024] to video relighting. First, to overcome the issue of limited data, we introduce LightAtlas, a comprehensive video dataset created through a carefully designed augmentation pipeline. This dataset includes a large collection of real-world video footage and 3D-rendered data, along with corresponding lighting conditions and augmented pairs, providing the model with a rich prior knowledge of lighting in videos. Second, to tackle temporal consistency, we incorporate temporal layers into our model. These layers capture temporal dependencies between frames, ensuring high-quality relighting with strong temporal consistency, while maintaining the albedo-preserving capability of IC-Light. Finally, to enhance applicability and compatibility with varying lighting conditions, we support diverse types of inputs, including background videos, texture prompts, and precise HDR environment maps.\nComprehensive experimental results demonstrate that our approach achieves high-quality, temporally consistent video relighting under multi-modal conditions, significantly outperforming the baseline in both qualitative and quantitative comparisons. We believe RelightVid can serve as a versatile tool for video relighting on arbitrary foreground subjects and pave the way for the application of video diffusion models in reverse rendering and generative tasks within the field of graphics."}, {"title": "2 RELATED WORK", "content": "Diffusion Models for Illumination Editing. Recent advancement of text-to-image diffusion models [Dhariwal and Nichol 2021; Ho et al. 2020; Rombach et al. 2022; Song et al. 2020] have demonstrated strong capabilities in learning real-world image priors. Fine-tuned versions of these models have been successfully applied to a wide array of tasks, including image editing [Alaluf et al. 2024; Brooks et al. 2023; Couairon et al. 2022; Hertz et al. 2022; Sun et al. 2024a], geometric prediction [Fu et al. 2025; Ke et al. 2024], 3D generation [Anciukevi\u010dius et al. 2023; Chen et al. 2023; Poole et al. 2022; Sun et al. 2024b; Tang et al. 2023] and more recently, illumination manipulation [Deng et al. 2025; Kocsis et al. 2024; Ren et al. 2024; Zeng et al. 2024; Zhang et al. 2024]. Directly applying these models to videos via per-frame relighting often results in significant temporal inconsistencies due to the inherent ambiguity of lighting conditions. In this work, we propose extending IC-Light [Zhang et al. 2024] into a video relighting model, which supports more flexible control based on background video, text, or full environment maps. Through a meticulously designed training pipeline, we achieve high-quality video relighting with enhanced temporal consistency, while preserving the lighting priors learned by IC-Light.\nVideo Editing and Video Diffusion Models. Recent years have seen remarkable breakthrough in video diffusion models [Blattmann et al. 2023; Brooks et al. 2024; Guo et al. 2023; Xing et al. 2025; Yang et al. 2024]. These models, after learning on real world videos, can generate high quality videos that obey real world physical laws, including illuminations. There are also works done by leveraging these models to achieve general video editing in both training [Cheng et al. 2023; Mou et al. 2024; Singer et al. 2025] and training free [Bu et al. 2024; Ku et al. 2024; Ling et al. 2024] ways. While these methods excel in general video editing, achieving high-quality video relighting that preserves critical details such as albedo, lighting consistency, and scene realism remains a significant challenge. Our work represents an early exploration into leveraging video diffusion models to enable high-quality video relighting."}, {"title": "3 METHODS", "content": "We propose an efficient method named RelightVid for editing the illumination in videos with consistent temporal performance. In Section 3.1, we first introduce LightAtlas, a high quality video relighting dataset constructed from real video and 3D data renderings. We further present the design and training framework of RelightVid in Section 3.2, which supports temporally consistent video relighting under multi-modal conditions."}, {"title": "3.1 LightAtlas Data Collection Pipeline", "content": "Training a model for arbitrary video-to-video illumination editing heavily depends on the availability of large-scale paired data. Due to the rarity of extreme dynamic lighting conditions in real-world videos, we utilize both in-the-wild video data to preserve photorealism and 3D-rendered data to effectively augment training under extreme lighting scenarios, as illustrated in fig. 2. Each appearance video $V_{appr} \\in R^{f \\times h \\times w \\times 3}$ is paired with five types of augmented data to facilitate illumination modeling and learning:\n$V_{appr} \\leftrightarrow {V_{rel}, V_{bg}, E, T, M}$,\nwhere $V_{rel} \\in R^{f \\times h \\times w \\times 3}$ represents the relit foreground video, $V_{bg} \\in R^{f \\times h \\times w \\times 3}$ is the background video, $E \\in R^{f \\times 32 \\times 32 \\times 3}$ denotes the convoluted temporal environment map, T is the caption describing illumination changes, and $M \\in R^{f \\times h \\times w}$ represents the foreground mask."}, {"title": "3.1.1 In-the-wild video", "content": "Generating paired data for in-the-wild videos poses significant challenges due to the complexity of obtaining high-quality and consistent illumination conditions. For real-world appearance videos $V_{appr}$, we apply a 2D image relighting method (e.g., IC-Light [Zhang et al. 2024]) frame-by-frame to generate augmented relit foreground videos $V_{rel}$ under different illumination settings. To extract the object's foreground mask, we leverage the powerful object matting tool InSPyReNet [Kim et al. 2022], while the inpainted background videos are obtained using video inpainting tool ProPainter [Zhou et al. 2023]. High Dynamic Range (HDR) environment maps for each frames are extracted from the video using DiffusionLight [Phongthawee et al. 2024], and then smoothed through temporal convolution. Additionally, we use GPT-4V [OpenAI 2023] to generate captions describing the video (focusing on environmental and lighting details) and further filter the captions to retain 20K high-quality meta videos.\nAmong augmented pairs, the background video, environment map, and caption are utilized as conditions for illumination modeling, with $V_{rel}$ serving as the model input and $V_{appr}$ as the target output. Since the target video is derived from real-world scenarios, it allows the model to learn the real data distribution and the temporal coherence across frames. Although this part of the data is of high photorealism, some input illumination conditions, particularly the environment map (HDR), include noise through estimations. To enhance the precise condition of HDR video, we incorporate auxiliary training data from 3D render engine to provide more precise control and improve the model's robustness to diverse lighting scenarios. With the input augmentation including brightness scaling and shadow based relighting, we finally generate 200K high quality video editing pairs."}, {"title": "3.1.2 3D rendered data", "content": "Extracting illumination conditions from real videos inherently introduces noise. To address this limitation, we render dataset using the Cycles renderer in Blender [Blender 2018], utilizing publicly available 3D assets from Objaverse [Deitke et al. 2023] and environment maps from Poly Haven\u00b9. We select 10K objects with high quality mesh from Objaverse and 1K high quality"}, {"title": "3.2 Model Design", "content": "Given the inherent similarities between image and video relighting tasks, we adopt a pre-trained 2D image relighting diffusion model [Zhang et al. 2024] as our foundational model. By utilizing this model's weights for initialization, we effectively leverage its image relighting priors and accelerate the training process. However, video relighting introduces additional challenges, including dynamic illumination and motion variations. Maintaining the original relighting quality of the model, while ensuring temporal consistency across frames, are two critical issues that must be addressed."}, {"title": "3.2.1 Lifting image diffusion model for video relighting", "content": "The overall RelightVid pipeline is illustrated in fig. 3. Our approach to address the challenges of lifting image relighting to the video domain is as follows: First, we inflate the 2D image diffusion model into a 3D U-Net, enabling it to accept video tensor with temporal dimension as input while maintaining the core structure of the original model. To further enhance temporal consistency, we integrate temporal attention layers into the image relighting model. During training, the spatial layers are kept frozen, while only the temporal layers are fine-tuned. This strategy preserves the in-the-wild editing capability of IC-Light [Zhang et al. 2024] and enables robust generalization to out-of-domain cases beyond the domain of our current data pair collection pipeline.\nTo achieve conditional illumination editing, we encode both the relighted video $V_{rel}$ and the background video $V_{bg}$ using a VAE encoder to obtain their latent space representations, $z_{rel}$ and $z_{bg}$, respectively. We then add noise for t steps to obtain the noisy latent $z_t$, and concatenate $z_t$, $z_{rel}$, and $z_{bg}$ as input to the diffusion model, where $z_{bg}$ serves as the background condition for relighting control. For environment HDR condition enjection, we encode the environment map E using a 5-layer MLP, decomposing it into LDR and HDR maps $E_l$ and $E_h$. Textual conditions are encoded via a CLIP Text Encoder into y, which is repeated and concatenated with the temporal HDR latents. This combined information is injected into the spatial layers via cross-attention, enabling precise control over illumination changes."}, {"title": "3.2.2 Multi-Modal Condition Joint Training", "content": "Our video diffusion model is designed to enable collaborative video relighting by simultaneously leveraging both background visual condition and texture prompts for fine-grained control over illumination. The model comprises a VAE encoder $E_i$, a denoiser 3D U-Net $\\epsilon_\\theta$, a CLIP-Text Encoder $E_t$, an HDR Encoder $E_e$, and a decoder D. To achieve this, we introduce a joint training objective that optimizes the model for collaborative conditioning on both background and text modalities:\n$\\min_\\theta E_{z \\sim E_i(x), t, \\epsilon \\sim N(0,1)} ||\\epsilon - \\epsilon_\\theta(z_t, t, E) ||^2,$\n$E = {E_i(z_{rel}), E_i(z_{bg}), E_t(y), E_e(E)}$,\nwhere $t \\sim [1, 1000]$ is the diffusion time step, and $E$ represents the encoded conditional latents for the input video $V_{rel}$, background video $V_{bg}$, environment map $E$, and the CLIP embedding of the input caption y. Our key innovation lies in the collaborative editing"}, {"title": "3.2.3 Illumination-Invariant Ensemble", "content": "In a single relighting process given background video condition, background video $V_{bg}$ is fixed. Consequently, the relighted foreground $V_{rel}$ should ideally be fixed. This ideal output should remain the same regardless of any brightness augmentation applied to the original input. Motivated by this observation, we propose an Illumination-Invariant Ensemble (IIE) strategy to enhance the robustness of video relighting. The core idea behind IIE is to apply brightness augmentations to the original input video and then average the predicted noise to obtain a more reliable result. The rationale is that different augmented inputs should guide the noisy latent toward the same output video, thereby mitigating the impact of illumination variations.\nSpecifically, We first apply a series of brightness augmentations directly to the input video $V_{in}$, generating N augmented versions. The augmented video frames are defined as:\n$V_{in}^{(i)} = s_i V_{in}, i = 1, 2, . . ., N,$\nWhere ${s_i}_{i=1}^{N}$ are brightness scaling factors sampled from a predefined range, e.g., $s_i \\in [0.5, 1.5]$. These augmented videos are then fed into the model to predict the noise $\\epsilon_t^{(i)}$ at each diffusion step t. To obtain a more robust denoising result, we compute the averaged noise prediction across all augmented versions: $\\bar{\\epsilon}_t = \\frac{1}{N} \\sum_{i=1}^{N} \\epsilon_t^{(i)}$."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Training Details", "content": "We adopt the SD-1.5 [Rombach et al. 2022] version of IC-Light [Zhang et al. 2024] as the image backbone and inject temporal attention layers initialized from AnimateDiff-V2 [Guo et al. 2023]. For the HDR environment map encoder, we initialize its parameters with zeros to minimize its influence at the beginning of training. During training, the cross-attention layers between image features and HDR features, as well as the temporal layers, are made trainable, while the other parts of the UNet are kept fixed. The learning rate is set to 1$\\times$10-5 with AdamW [Loshchilov 2017] optimizer adopted. Training is conducted on 8 NVIDIA A100-80G GPUs for 5,000 iterations."}, {"title": "4.2 Evaluations", "content": ""}, {"title": "4.2.1 Video Background Conditioned Relighting", "content": "To assess both image quality and temporal consistency of RelightVid, we sample 50 in-the-wild videos from the Mixkit dataset, encompassing both static and dynamic lighting conditions. The foreground portrait or object is augmented with diverse relighting transformations, serving as model input for evaluation. We setup a baseline of per-frame relighting via IC-Light [Zhang et al. 2024] with the video smoothing method. For image quality, we adopt PSNR, SSIM [Wang et al. 2004], LPIPS [Zhang et al. 2018] as evaluation metrics. For"}, {"title": "4.2.2 Text-Conditioned Relighting", "content": "Text-conditioned video relighting represents a versatile setting with wide-ranging applications for real-world users. We compare our method with per-frame IC-Light [Zhang et al. 2024] and two state-of-the-art video editing methods: TokenFlow [Geyer et al. 2023] and AnyV2V [Ku et al. 2024]. TokenFlow directly leverages text prompts for video editing, while AnyV2V conditions on the original video and the first edited frame. For AnyV2V, we use IC-Light [Zhang et al. 2024] to relight the first frame. The evaluation is conducted on the DAVIS dataset [Perazzi et al. 2016], where we randomly sample 10 text relighting prompts"}, {"title": "4.2.3 HDR-Conditioned Relighting", "content": "In addition to background and textual cues as relighting conditions, we also incorporate HDR video as a more precise condition for relighting. As demonstrated in fig. 8, our model, by injecting an HDR map through a specially designed HDR encoder and leveraging cross-attention interactions with frame features, is able to recover most of the relevant information and apply accurate relighting for the foreground object. This enhanced control is particularly effective in scenarios where the predominant lighting originates from the camera's direction, where background and text-based cues struggle to address."}, {"title": "4.2.4 Illumination-Invariant Ensemble (IIE)", "content": "We evaluate the performance of the Illumination-Invariant Ensemble (IIE) using the same test set as in the video background-conditioned relighting task, with two augmented foreground videos and the original video as inputs. As demonstrated in fig. 7 and summarized in table 3, the incorporation of IIE significantly enhances the robustness of relighting and improves the preservation of foreground albedo, such as the shirt of the girl and the hammock. However, an increase in the number of augmented inputs may result in an average blurring effect, which can negatively impact the overall performance."}, {"title": "5 CONCLUSION", "content": "In this work, we propose RelightVid, a video diffusion model that supports relighting any foreground object in a video conditioned on a new background video, text, and an HDR map without requiring complex process of intrinsic decomposition. We demonstrate its promising performance across various scenarios and explore key factors behind its success: a carefully designed data generation pipeline and the efficient reuse of prior knowledge from the image backbone. We believe that this model can serve as a versatile tool to fulfill real user requirements and hope this work will inspire future research on video diffusion models for editing and generation."}]}