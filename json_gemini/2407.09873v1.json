{"title": "Resource Management for Low-latency Cooperative Fine-tuning of Foundation Models at the Network Edge", "authors": ["Hai Wu", "Xu Chen", "Kaibin Huang"], "abstract": "The emergence of large-scale foundation models (FoMo's) that can perform human-like intelligence motivates their deployment at the network edge for devices to access state-of-the-art artificial intelligence (AI). For better user experiences, the pre-trained FoMo's need to be adapted to specialized downstream tasks through fine-tuning techniques. To transcend a single device's memory and computation limitations, we advocate multi-device cooperation within the device-edge cooperative fine-tuning (DEFT) paradigm, where edge devices cooperate to simultaneously optimize different parts of fine-tuning parameters within a FoMo. The edge server is responsible for coordination and gradient aggregation. However, the parameter blocks reside at different depths within a FoMo architecture, leading to varied computation latency-and-memory cost due to gradient backpropagation-based calculations. The heterogeneous on-device computation and memory capacities and channel conditions necessitate an integrated communication-and-computation (C2) allocation of local computation loads and uplink communication resources to achieve low-latency (LoLa) DEFT. To this end, we consider the depth-ware DEFT block allocation problem. The involved optimal block-device matching is tackled by the proposed low-complexity Cutting-RecoUNting-Checking (CRUNCH) algorithm, which is designed by exploiting the monotone-increasing property between block depth and computation latency-and-memory cost. Next, the joint bandwidth-and-block allocation (JBBA) makes the problem more sophisticated, i.e., mathematically NP-hard. We observe a splittable Lagrangian expression through the transformation and analysis of the original problem, where the variables indicating device involvement are introduced to decouple the block and bandwidth allocation. Then, the dual ascent method is employed to tackle the JBBA problem iteratively. Within each iteration, block allocation and bandwidth allocation are optimized concurrently. The optimal block allocation sub-problem is solved efficiently by applying the Hungarian method facilitated by the proposed CRUNCH algorithm. On the other hand, the bandwidth allocation sub-problem is solved in closed form, shedding light on favorable allocations to resource-limited devices. Through extensive experiments conducted on the GLUE benchmark, our results demonstrate significant latency reduction achievable by LoLa DEFT for fine-tuning a RoBERTa model.", "sections": [{"title": "I. INTRODUCTION", "content": "The remarkable advancements in Artificial Intelligence (AI), particularly the human-like intelligence as demonstrated by large-scale foundation models (FoMo's), have drawn significant investments and research efforts from leading ICT companies such as Microsoft, Alphabet, and Nvidia [1], [2].\nIn particular, ChatGPT, a popular generative AI conversation model, attracts over 100 million active users per week [3]. On the other hand, in the era of mobile networks, their sixth-generation (6G) is expected to fully integrate AI in various applications, particularly capitalizing on the surge of FoMo's [4]\u2013[6]. To enhance the performance of pretrained FoMo's, fine-tuning is widely embraced as a small fraction of model parameters using small datasets, thereby maximizing their utility for the desired downstream tasks [7]. Especially, relevant techniques, collectively called parameter-efficient fine-tuning (PEFT), are capable of achieving comparable performance as full-model training by adapting less than 1% parameters in pretrained FoMo [8], [9]. Their high efficiencies have led to PEFT being widely adopted in the on-device tuning of FoMo, e.g., Apple Intelligence [10]. However, the distributed deployment of fine-tuning in a wireless network to leverage mobile data is still hindered by the high dimensionality of FoMo parameters, the heterogeneous computation-storage capacities of edge devices, and limited radio resources [11]\u2013[13]. To address the issue, we propose a novel resource management framework for a multi-device cooperative fine-tuning paradigm that distributes computation loads to devices with awareness of their heterogeneous capabilities for computation, memory, and communication to expedite the adaptation of a FoMo to a specific task.\nIn wireless mobile networks, there are two approaches to orchestrate the fine-tuning of FoMo's with the cooperation among devices and servers. One involves uploading local data for centralized fine-tuning at the server. The other approach, called device-edge fine-tuning (DEFT) as considered in this work, leverages distributed computational resources and data at devices cooperatively update a FoMo [13], [14]. The former circumvents devices' computation bottlenecks by centralizing model fine-tuning at edge servers with powerful AI hardware. Subsequently, task-oriented FoMo's can be downloaded to devices to enable matching on-device intelligent applications [15]. However, extensive data uploading introduces significant communication delay and raises privacy concerns due to the exposure of raw personal data [13], [16]. The server may not be able to guarantee timely and accurate online fine-tuning when requesting devices are many while increasingly powerful on-device processors remain underutilized. DEFT, to be deployed on an edge-computing platform, can address the drawbacks of centralized fine-tuning by ensuring personal data security and relying on local computation [13]. It also eases network congestion by avoiding data uploading and bringing computing to the proximity of user data. Additionally, such proximity allows flexible fine-tuning of FoMo's to adapt to users' changing contexts and applications. FoMo's cooperatively updated by multiple devices are automatically shared among them without explicit downloading. Their limitations in computation and communication are also surpassed in the cooperative process.\nDEFT is considered a particularization of mobile edge computing (MEC), which leverages advanced mobile processors to process the data closest to their sources [16], [17]. There exists a rich and mature literature investigating communication-and-computation (C2) resources management of MEC in wireless networks, primarily focusing on efficiently offloading computation tasks to edge clouds to enhance communication/computation rates and mobile energy efficiencies [18]\u2013[20]. Subsequently, MEC research was expanded to include AI inference, training, and sensing tasks [21]. Cooperative transmission of data for split edge inference/sensing [22]\u2013[24] and the optimal placement of deep neural networks (DNNs) [25] have been investigated to achieve high computational efficiencies and accuracies for AI tasks over finite-rate wireless links. A popular class of techniques for training DNN models, called federated learning (FL), have been developed to leverage local data and on-device computation while preserving privacy by avoiding direct data uploading, which shares the principles of DEFT [26]\u2013[29]. However, in each of the FL iterations, every participating device is required to update the entire model and upload the associated stochastic gradients, which is infeasible for devices given the enormous size of a FoMo that ranges from several to hundreds of billion parameters [13], [14]. An alternative approach, called split learning, where a DNN model is cut into multiple parts for placement on different devices to sequentially compute corresponding stochastic gradients for model updating based on back-propagation (see, e.g., [30]\u2013[32]). However, the transmission of high-dimensional intermediate features for dependent computation among devices results in excessive overhead, especially given the typically large number of iterations. For example, the feature dimension of a RoBERTa base model using a batch size of 32 is 12.5 million, which is over 500\u00d7 of the number of LoRA fine-tuning parameters (i.e., 0.024 million) in one associated block [8]. Meanwhile, deep fading channels of devices are difficult to cope with by scheduling due to the rigid sequential computation order of devices. Even though researchers have attempted on parallel computing by creating training pipelines, its practicality for mobile networks is questionable due to the uniform requirements on hardware and communication links for devices [31], [33].\nThe considered multi-device DEFT approach divides and schedules the fine-tuning parameter blocks within a FoMo for edge devices to simultaneously optimize different parts of tunable parameters using local data. Compared with FL, devices' computation-and-memory requirements are dramatically lower as they are required to tune a small fraction of model parameters (e.g., 1%) instead of full model updating. Moreover, the communication overhead in the context of DEFT is proportional to the number of tunable parameters and, thus, also much less than that for split learning. Unlike split learning constrained by dependent computing, device scheduling can be employed for DEFT to cope with fading as well as mobile limitations, which is actually the theme of this work. However, while DEFT has several advantages, its efficient implementation faces a new challenge arising from the fact that the on-device gradient computation for an individual parameter block of a FoMo incurs memory cost and latency that are dependent on the position (i.e., depth) of the block [34]\u2013[36]. Specifically, based on the chain rule, the gradients of a specific block are computed by back-propagation from the last block, resulting in the memory-latency cost being monotone-increasing functions of the block depth [35]. In the context of FoMo training, the discrepancy among blocks is amplified as each attention block in a transformer is overparameterized, e.g., e.g., 1.8 billion parameters in one transformer block of the GPT-3 model [2]. While this fact is largely overlooked in traditional distributed learning targeting relatively small models, it is important to consider the variations of memory-and-latency cost over blocks when designing multi-device DEFT that allocate blocks to devices for local updating. To be specific, an optimal allocation strategy should match the memory/latency cost of individual blocks with devices' memory/computation capacities and channel states. This creates a new problem for DEFT termed block-device matching problem.\nIn this work, we make an attempt to solve this problem by optimizing the joint C2 resource management strategy to minimize the fine-tuning latency, yielding the proposed low-latency DEFT (LoLa-DEFT) framework. Such a problem is a mixed integer problem and NP-complete. We overcome the challenge by designing a low-complexity depth-aware block allocation algorithm named Cutting-RecoUNting-CHecking (CRUNCH) that employs a key property of the problem that local computation latency-and-memory cost is a monotone increasing function of block depth. This allows the search space to be dramatically reduced by shrinking the set of qualified block-device pairs under a latency constraint. The optimal policy derived from solving the block-device matching problem is also further developed to facilitate the optimal bandwidth allocation. The key contributions and findings of this paper are summarized as follows.\n\u2022 Block Allocation for LoLa DEFT. Given equal bandwidth allocation, the block-device matching problem is transformed into a sequence of feasibility check sub-problems with reduced complexity. Then each sub-problem under a latency constraint from a discrete set involves finding the optimal device-block pairs to make DEFT feasible. The sub-problem can be efficiently solved using the proposed CRUNCH algorithm thanks to the exploitation of the mentioned monotone relationship between latency-and-memory cost and block depth. The solution nesting a linear search and CRUNCH algorithm is shown to achieve much lower complexity as opposed to the classic Hungarian methods.\n\u2022 Joint Bandwidth-and-Block Allocation for LoLa DEFT. Next, we consider an advanced version of the block-device matching problem incorporating optimal bandwidth allocation. The result is a mixed-integer non-linear programming one that is NP-hard. By analyzing the problem structure, we"}, {"title": "II. MODELS AND OPERATIONS", "content": "Consider a single-cell system where K edge devices cooperatively fine-tune a FoMo for a given downstream task, as illustrated in Fig. 1. The devices utilize on-device computation power and localized data to compute the gradient of tunable parameters in the FoMo using the stochastic gradient descent (SGD) method. The indices of these devices are denoted as K = {1,2,\u2026\u2026\u2026, K}. Fine-tuning parameter division and depth-aware parameter block allocation for multi-device cooperative computation are adopted to accelerate the fine-tuning process. Since the DEFT requires a repeated aggregation of on-device computed gradients to update the model parameters, the edge server needs to perform depth-aware block allocation by jointly considering local computation and gradient upload at the beginning of each communication round. Hence, within each communication round, the activated devices, need to perform the following three steps: a) download one initialized/aggregated FoMo; b) compute the gradients of a sched-\nuled FoMo block in the model iteratively; c) upload the locally computed FoMo gradients. After the server successfully receives all parameter gradients, the latest updated FoMo is offloaded to all scheduled devices to start a new round. The workflow of DEFT is summarized in Algorithm 1, with detailed models and operations described in the following subsections.\nIn the considered DEFT, the tunable parameters are evenly distributed over model blocks (e.g., Transformer-based Fo-Mos) as commons adopted in the literature and practice [8], [38]\u2013[40]. Suppose the given pre-trained foundation model, denoted by $\\mathbb{P}_{\\Phi}(y|x)$, is parameterized by $\\Phi$. $\\mathbb{P}_{\\Phi}(y|x)$ is a generic multi-task learner such as GPT. We follow the fine-tuning downstream task setup in [8] to adapt $\\mathbb{P}_{\\Phi}(y|x)$ to\nconditional text generation task, e.g., natural language understanding. In centralized fine-tuning, the desired task is characterized by a training dataset consisting of context-target token sequence pairs, which is denoted by $\\mathcal{Z} = \\{(x_i, y_i)\\}_{i=1,\\ldots,\\lvert\\mathcal{Z}\\rvert}$. For example, $x_i$ is the content of an article and $y_i$ is its summary in the text summarization task. A set of tunable fine-tuning parameters, denoted by $\\varphi$, associated with $L$ blocks within one transformer, i.e., $\\varphi = \\{\\varphi_l\\}_{l=1,\\ldots,L}$, is optimized through SGD-based optimization by repeatedly feeding the training data into $\\mathbb{P}_{\\Phi,\\varphi}(y|x)$, to maximize the conditional language modeling objective. $\\varphi$ can be exactly the original foundation model parameter $\\Phi$ in the full-model tuning case, or is the newly appended fine-tuning parameters in PEFT settings. Overall, fine-tuning task is hence optimizing $\\varphi$ through maximizing the conditional language modeling objective, i.e., the probability of generating the desired token given the conditioning context and previously generated words,\n$\\underset{\\varphi}{max}\\sum_{(x,y)\\in \\mathcal{Z}}\\sum_{j=1}^{y}log~(\\mathbb{P}_{\\Phi,\\varphi}(y_j|x, Y_{<j})).$ (1)\nIn the considered multi-device cooperative fine-tuning, an arbitrary device, say device k, holds a localized training dataset $\\mathcal{Z}^{(k)} = \\{(x_i^{(k)}, y_i^{(k)})\\}_{i=1,\\ldots,\\lvert\\mathcal{Z}^{(k)}\\rvert}$. The localized data is as- sumed to be independent and identically distributed (i.i.d.). Hence, the asynchronous update of different parameter blocks can be performed using localized data in a distributed manner [41]. Thereby, the fine-tuning parameter is obtained through the optimization in (1) by replacing the centralized dataset $\\mathcal{Z}$ with $\\mathcal{Z} = \\bigcup_{k=1}^K \\mathcal{Z}^{(k)}$.\n1) On-device Gradient Backpropagation-based Fine-tuning: In the considered FoMo division for DEFT, each device is responsible for computing the gradient of one parameter block in the entire $\\varphi$. Consider an arbitrary device, say device k, is scheduled to compute the gradient of a specific fine-tuning parameter block $\\varphi_l$, denoted as $g(\\varphi_l)$, toward the following on-device optimization:\n$\\underset{\\varphi_l}{max}\\sum_{(x,y)\\in \\mathcal{Z}^{(k)}}\\sum_{j=1}^{y}log~(\\mathbb{P}_{\\Phi,\\varphi_l}(y_j|x, Y_{<j})).$ (2)\nConsider the commonly adopted backpropagation approach for gradient computation as shown in Fig. 2. The training data is fed and propagated through the entire foundation model to\ncalculate the loss function, i.e., the objectives in (2). There is a need to store all the weights, intermediate activations and gradients along the propagation of the whole model in SGD-based fine-tuning of foundation models. Then, the gradient of the loss function with respect to (w.r.t.) the fine-tuning parameter, denoted by $g(\\varphi_l)$, is obtained by the repeated employment of chain rule from the gradient of the last layers towards the former layers. Specifically, the gradient of the former layer's parameters is determined by its intermediate activation saved in the forward path and the later layer's gradient.\n2) Memory-aware On-device Cooperative Computation: To transcend the on-device memory limitation, the fine-tuning parameter division allows the heterogeneous devices to cooper- atively compute the gradient of fine-tuning parameters residing in different depths according to the corresponding memory budgets. Once the device completes the gradient calculation of the scheduled model block, the backward path is terminated. Given the $L$ block-repetitive transformer architecture, the server needs to schedule $L$ block among $K$ devices ($L < K$). Specifically, the allocation of a parameter block with depth index $l$, i.e., $\\varphi_l$, to device $k$ in round $n$ is denoted by $\\alpha_{k,l}^{(n)} = 1$, otherwise $\\alpha_{k,l}^{(n)} = 0$. The on-device memory (in bits) required to compute the gradient of fine-tuning parameters with block index $l$ is denoted by $b(l)$, where $b(\\cdot)$ is a monotone-increasing function of $l$. For example, $b(l)$ is a linear function over $l$ if none of the memory-saving techniques are adopted [35].\n3) On-device Computation Latency: To achieve a balanced fine-tuning of all parameters, the on-device gradient compu- tation within one round is performed with the same number of iterations, $M$, and the same batch size of training samples is adopted. The latency bottleneck of on-device computation is the backpropagation process, which triples the latency of a forward path, see [36]. The sequential computation of a substantial number of parameters within each model block necessitates the integration of diverse backpropagation lengths, i.e., $l\\in \\{L,\\ldots,1\\}$, for multi-device scheduling. Therefore, the computation latency of one device is dominated by its computation capability and the depth index of the assigned parameter block. Given a device with normalized computation capability, e.g., CPU frequency is 1GHz, the run-time duration (in seconds) of computing the gradient of block $l$ is $d(l) = a+c(l)$, where a represents the latency of data feeding and the entire model forward, $c(l)$ is the latency of backpropagation ended at block $l$. $c(l)$ is a monotone-increasing function of $l$ and generally is a linear one. Consider the heterogeneous computation capabilities of edge devices, each with a relative computation factor of the normalized one, denoted by $f_k, \\forall k$. Then, the on-device computation latency of scheduling device k for fine-tuning model block $l$ in round $n$ can be written as\n$T_{k,l,comp}^{(n)} = \\frac{\\alpha_{k,l}^{(n)} Md(l)}{f_k} = \\alpha_{k,l}^{(n)} \\mathcal{J}_{k,l}.$ (3)"}, {"title": "C. Communication Model", "content": "1) Model Downloading: At the beginning of each communication round, the server first broadcasts the latest foundation model to all involved devices. Since the server is associated with the base station, the broadcasting rate could be relatively high. Therefore, the downlink latency can be regarded as the same among devices and is considered to be negligible.\n2) Gradient Uploading: Once the device finishes computations within one round, the accumulated gradient of M local iterations is uploaded to the server for central aggregation. The devices are connected to the edge server via wireless channels, where the orthogonal frequency-division multiple access (OFDMA) is adopted for allocating a broadband spectrum to involved devices. The channels are assumed to be frequency non-selective and there is a sufficiently large number of sub-carriers, making the bandwidth allocation can be approximated as being continuous. The total bandwidth is represented by B. Consider an arbitrary device, say device k. Its transmit power is represented by $p_k$, which is known by the server. The channel power in the n-th communication round is denoted by $H_k^{(n)}$, which remains a constant within one round and can be perfectly estimated by the server. Let $0 \\le B_k^{(n)} < B$ denote the bandwidth allocated to device k. The upload communication rate, $R_k^{(n)}$, can be written as\n$R_k^{(r)} = B_k^{(n)}\\log_2(1+\\frac{p_k H_k^{(n)}}{\\mathcal{N}_o}/B_k^{(n)}) = B_k^{(n)} \\mathcal{r}_k^{(n)},$ (4)\nwhere $\\mathcal{N}_o$ is the white Gaussian noise power, $\\mathcal{r}_k^{(n)}$ is defined to be the uplink spectrum efficiency. For the blockwise distributed fine-tuning parameters, its size is assumed to be $S$ (in bits). Then, the upload latency of scheduling device k targeting the fine-tuning of model block $l$ in round $n$ can be written as\n$T_{k,l,comm}^{(n)} = \\frac{\\alpha_{k,l}^{(n)}S}{B_k^{(n)} \\mathcal{r}_k^{(n)}}.$ (5)"}, {"title": "D. Global FoMo Updating", "content": "After the server successfully receives the gradients of all the model blocks, i.e., the accumulated gradient of all parameter blocks, in round n, denoted by $\\{g^{(n)}(\\varphi_l)\\}_{l=1,\\ldots,L}$, the server performs centralized model aggregation. Given the gradient descent is applied, the updated model block $\\varphi_l$ after round n, represented by $\\varphi_l^{(n)}$ can be written as\n$\\varphi_l^{(n)} = \\varphi_l^{(n-1)} - \\eta^{(n)}G(g^{(n)}(\\varphi_l)),$ (6)\nwhere $\\eta^{(n)}$ is the predefined learning rate, $G(\\cdot)$ represents the variation of gradients depending on the selection of different optimizers. For example, the Adam optimizer relies on the first and second-order moments for adaptive element-wise updates of fine-tuning parameters."}, {"title": "III. PROBLEM FORMULATION", "content": "As the core component of LoLa DEFT, the objective of parameter block allocation is to minimize the total fine-tuning latency, considering the devices' heterogeneous computation capability, available memory space, and channel status. In this section, the associated block-device matching problem is formulated as two optimization problems for the relatively simple case with given bandwidth allocation and the JBBA case. Given a fixed number of communication rounds $N$, the latency minimization of the entire fine-tuning process is to minimize the latency of every communication round. Therefore, the problem can be simplified as a one-round latency minimization problem equivalently. For ease of notation, the superscripts indicate the communication round number is omitted in the following.\nConsider an arbitrary device, say device k. The associated latency, $T_k$, consists of two components, i.e., the on-device computation part and the uplink communication part, which can be written as\n$T_k = \\sum_{l=1}^L \\alpha_{k,l} (T_{k, l, comp} + T_{k, l, comm}) = \\sum_{l=1}^L \\alpha_{k,l} (\\mathcal{J}_{k,l} + \\frac{S}{B_k \\mathcal{r}_k}).$ (7)\nFor the considered multi-device cooperative system, the overall latency is exactly determined by the device involved with the maximum summed latency, i.e.,\n$T = \\underset{k \\in \\mathcal{K}}{max} \\{\\sum_{l=1}^L \\alpha_{k,l} (\\mathcal{J}_{k,l} + \\frac{S}{B_k \\mathcal{r}_k})\\}.$ (8)\nOn top of the target of latency minimization, block allocation requires satisfying a few constraints as stated below. At the beginning of each communication round, the server is required to schedule $L$ distinct devices from $K$ involved devices to compute the fine-tuning parameter block residing in depth from $l = L$ to $l = 1$, which introduces the following spreading model block constraints,\n$\\textrm{(C1)} \\qquad \\sum_{k=1}^K \\alpha_{k,l} = 1, \\forall l.$"}, {"title": "IV. DEPTH-AWARE BLOCK ALLOCATION FOR LOLA-DEFT", "content": "In this section, we design LoLa-DEFT by solving the block-device pairing problem in (P1). The optimal block allocation is obtained by first transforming the original min-max problem into an equivalent latency-threshold minimization problem and then addressing the later problem with low complexity via leveraging a linear search approach and a proposed depth-aware feasibility checking method.\nFor tractability, the min-max problem (P1) is transformed into a minimization problem by introducing a variable of latency threshold, $T_{th}$, as\n$\\textrm{(P3)} \\qquad \\underset{\\{\\alpha_{k,l}\\},T_{th}}{min} \\quad T_{th}$\n$\\textrm{s.t.} \\qquad \\sum_{l=1}^L \\alpha_{k,l} T_{k,l} \\le T_{th}, \\forall k,$\n$\\qquad \\textrm{Constraints in (P1)}.$\nThe equivalence between (P1) and (P3) is guaranteed by the joint optimization of {$\\alpha_{k,l}$} and $T_{th}$. It is observed from (P3) that the feasible set of $T_{th}$ can be restricted into the discrete set {$T_{k,l}$}. Hence, the optimal latency threshold towards (P3) can be obtained by searching over the $C^2$ latency $T_{k,l}$ of all device-block pairs. For each searched latency threshold $T_{th}$, the feasibility of associated problem (P3) requires validation through the finding of the maximum matching on the left qualified device-block pairs. Hence, the workflow of finding the optimal block allocation can be summarized into the following four steps:\n\u2022 Step 1: Rearrange {$T_{k,l}$} in a descending order to generate a set {$T_j$} with $j \\in \\{1,\\ldots,KL\\}$ and $T_j \\ge T_{j+1}, \\forall j$; set a variable $t = KL + 1$ and initialize a set recording solution of {$\\alpha_{k,l}$} as $P = \\O$.\n\u2022 Step 2: Update $t = t - 1$ and set $T_{th} = T_t$.\n\u2022 Step 3: Validate if there is a feasible solution of {$\\alpha_{k,l}$} under the current latency constraint:\n- If yes, update the current solution into $P$ and return to Step 2;\n- Otherwise, go to Step 4.\n\u2022 Step 4: Output $P$ as the optimal solution.\nThe operation of such linear search, due to the discrete search {$T_{k,l}$}, results in polynomial complexity of $O(KL)$ in the worst case. However, the feasibility validation in each iteration causes the main complexity. To be specific, the devices and FoMo blocks are treated as two disjoint sets in a bipartite graph, formulating the feasibility validation as a matching problem. The feasibility validation is then to find a pairwise matching between devices and blocks under the constraints of ensuring all blocks are matched with different devices. The resulting matching offers a possible block assignment, whose feasibility is determined by checking the constraints of $\\sum_{i=1} \\alpha_{k,l}T_{k, \\iota} \\le T_{th}$ under the current requirement $T_{th} = \\hat{T}_t$. The conventional Hungarian algorithm can be leveraged to perform the said bipartite matching. However, even using its most efficient variant, the Hungarian algorithm requires a computation complexity of $O(max\\{K, L\\}^3)$, together with the operation of linear search, resulting in the overall complexity of $O(KL max\\{K, L\\}^3)$. Such complexity is not acceptable when a large number of devices or blocks are involved."}, {"title": "B. Depth-aware Low-complexity Matching Validation", "content": "On top of resorting to solving the bipartite matching, we propose a low-complexity feasibility-validation method that exploits the correlation between block depth and latency. To be specific, the memory and computation latency required by a block monotonically increase as its depth increases. This endows the matching bipartite graph of the feasibility-validation problem with a special geometric structure that allows for low-complexity matching.\nBefore introducing the low-complexity feasibility-validation criterion, we first define the auxiliary notations. Typically, considering a given latency constraint $T_{th}$, a $K$-by-$L$ matrix, denoted by $Q_{T_{th}}$, is initialized to characterize both memory constraints and $C^2$ latency of each fine-tuning block at each device. The elements of $Q_{T_{th}}$ are computed as\n$q_{k,l} = \\begin{cases} 1, & T_{k,l}, b(l) \\le b_k \\text{ and } T_{k,l} \\le T_{th}, \\\\ 0, & \\text{otherwise}, \\end{cases}$ (9)\nwhere $q_{k,l} = 0$ reveals that the $l$-th block can not be allocated to device-$k$ due to its limited memory or unacceptable latency"}, {"title": "C. Cutting-Recounting-Checking (CRUNCH) Algorithm", "content": "Based on the feasible set minimization mechanism and the low-complexity feasibility check described above, the transformed problem (P3) can be solved efficiently via the algorithm elaborated below. The algorithm is established on the matrix $Q_{T_{th}}$ and consists of three iterative operations: Cutting, RecoUNting, and CHecking (CRUNCH).\n1) Cutting: updates the matrix $Q_{T_{th}}$ by removing its largest element (set as zero), which removes a feasible value of $T_{th}$ from its feasible set consists of {$T_{k,l}$}.\n2) Recounting: updates the number of non-zero elements in each row of $Q_{T_{th}}$, i.e., $s_k$, according to (11).\n3) Checking: validates the feasibility of $T_{th}$ through the obtained {$s_k$} and Proposition 1.\nThe above three steps are repeated until the feasibility is violated, and the final $T_{th}$ gives the optimal latency. The above Cutting-RecoUNting-CHecking (CRUNCH) operations are concluded into Algorithm 2, which is illustrated through an example below.\n: We present an example of block assignment with $K = 6$ and $L = 4$ in Fig. 3. During the initialization step, we form the primary matrix $Q_{T_{th}}$ by removing elements corresponding to user-block pairs that violate memory constraints. Each row of this matrix ensures that the elements are in ascending order from left to right due to the depth property. In the next step, we remove the maximum non-zero element on the right edge of $Q_{T_{th}}$, which is 46. As a result, {$s_k$} becomes {$1,2,2, 3, 4, 4$}, satisfying the feasibility condition stated in Proposition 1. Consequently, we update $T_{th}$ to 46 and continue to remove the elements 42 in Step 3, 38 in Step 4, 35 in Step 5, 34 in Step 6, and 32 in Step 7. All of these elements pass the feasibility check. Finally, in Step 8, when we remove the (2,4)-th element in $Q_{T_{th}}$, the condition of (12) is violated, leading to the termination of the"}, {"title": "V. JOINT BANDWIDTH-AND-BLOCK ALLOCATION FOR LOLA-DEFT", "content": "In this section, we further develop LoLa-DEFT to address the practical issue of optimal bandwidth allocation as it helps to accelerate the DEFT process. Specifically, we propose a solution approach for the JBBA problem formulated in (P2). It is found that problem (P2) is a mixed-integer non-linear programming (MINLP), which is an NP-hard problem. We first introduce the auxiliary variables denoting the device involvement to decouple the block allocation indicators and controllable bandwidth in the original problem. By analyzing the Lagrangian of the transformed problem, the dual ascent method is designed to obtain the optimal JBBA policy iteratively. In each iteration, a variation of the designed CRUNCH algorithm is applied to provide sparse qualified device-block pairs, based on which the optimal block allocation is obtained by employing the Hungarian method to find the minimun-weight perfect matchings. The optimal device involvement and bandwidth allocation are deduced in close-form solutions.\nTo facilitate the decomposition of joint bandwidth allocation and block allocation, we first introduce binary decision variables $\\beta_k$ to denote the involvement of device $k$ in the considered communication round, where $\\beta_k = 1$ if device $k$ is scheduled to compute the gradient of an arbitrary parameter block and $\\beta_k = 0$ otherwise. The constraint (C2) that one device can be scheduled to compute the gradient of at most one parameter block, which is translated into the constraint of $\\beta_k = \\sum_{l=1}^L \\alpha_{k,l}, \\forall k$. Then, based on the device involvement indicator $\\beta_k$, the depth-aware block allocation and bandwidth allocation are decoupled in the latency representations. Hence, the multi-device cooperative fine-tuning latency minimization problem can be rewritten as\n$\\textrm{(P4)} \\qquad \\underset{T,\\{\\mathcal{B}_k\\},\\{\\beta_k\\},\\{\\alpha_{k,l}\\}\\in \\mathcal{S}_b}{min} \\quad T$\n$\\textrm{s.t.} \\qquad \\sum_{l=1}^L \\alpha_{k,l}\\mathcal{J}_{k,l} + \\beta_k\\frac{S}{\\mathcal{B}_k \\mathcal{r}_k} <T, \\forall k,$ (C4.1)\n$\\qquad \\sum_{k=1}^K \\mathcal{B}_k \\le B,$ (C4.2)\n$\\qquad \\beta_k = \\sum_{l=1}^L \\alpha_{k,l}, \\forall k,$ (C4.3)\nThen, the Lagrangian of problem (P4) is expressed as following\n$L(T, \\{\\mathcal{B}_k\\}, \\{\\beta_k\\}, \\{\\alpha_{k,l}\\}, \\{\\lambda_k\\}, \\mu, \\{\\sigma_k\\}) = T + \\sum_{k=1}^K \\lambda_k \\left(\\sum_{l=1}^L \\alpha_{k,l} \\mathcal{J}_{k,l} + \\beta_k \\frac{S}{\\mathcal{B}_k \\mathcal{r}_k} - T\\right) + \\mu\\left(\\sum_{k=1}^K \\mathcal{B}_k - B\\right) + \\sum_{k=1}^K \\sigma_k\\left(\\beta_k - \\sum_{l=1}^L \\alpha_{k,l}\\right),$ (14)\nwhere {$\\lambda_k$} $ \\ge 0$, $\\mu > 0$ are dual variables associated with the constraints (C4.1) and (C4.2) respectively, and {$\\sigma_k$} are dual variables with constraints (C4.3). By categorizing the primal values, the Lagrangian (14) can be rewritten as\n$L(T, \\{\\mathcal{B}_k\\}, \\{\\beta_k\\}, \\{\\alpha_{k,l}\\}) = \\left(1 - \\sum_{k=1}^K \\lambda_k\\right)T + \\sum_{k=1}^K\\sum_{l=1}^L \\alpha_{k,l}(\\lambda_k\\mathcal{J}_{k,l} - \\sigma_k)$\n$+ \\sum_{k=1}^K (\\lambdak \\frac{S}{\\mathcal{B}_k \\mathcal{r}_k} + \\sigma_k) \\beta_k + \\mu(\\sum_{k=1}^K \\mathcal{B}_k - B).$ (15)\nIt is observed that the block allocation indicators {$\\alpha_{k,l}$} are splittable with the device involvement indicators {$\\beta_k$} and associated bandwidth {$\\mathcal{B}_k$}, hence can be solved independently, making the dual decomposition [42] an efficient guideline to tackle problem (P4).\nThe strategy of dual ascent is to optimize primal variables and dual variables alternatively. Specifically, the following two optimization steps are conducted iteratively to obtain the optimal JBBA solutions.\n\u2022 Primal optimization: Optimizing primal variables $T$, {$\\mathcal{B}_k$}, {$\\beta_k$}, {$\\alpha_{k,l}$} to minimize Lagrangian (14) given fixed dual variables {$\\lambda_k^i$}, $\\mu^i$, {$\\sigma_k^i$} in iteration $i$. The asso-\nciated primal problem is\n$\\underset{T,\\{\\mathcal{B}_k\\},\\{\\beta_k\\},\\{\\alpha_{k,l}\\}\\in \\mathcal{S}_b}{min} \\quad L(T, \\{\\mathcal{B}_k\\}, \\{\\beta_k\\}, \\{\\alpha_{k,l}\\}|\\{\\lambda_k^i\\}, \\mu^i, \\{\\sigma_k^i\\})$\ns.t. $\\beta_k = \\sum_{l=1}^L \\alpha_{k,l}, \\forall k$.\n\u2022 Dual updating: Apply dual ascent on dual variables to maximize the Lagrangian (14) given the latest obtained primal variables. It is noted that a simple gradient ascend can be applied to dual variables because of their differentiability. Denoting the solution to problem (P5) in the i-th iteration as $T^{i+1}$, {$\\mathcal{B}_k^{i+1}$}, {$\\beta_k^{i+1}$} and {$\\alpha_{k,l}^{i+1}$}, the updating of dual variables can be\n$\\lambda_k^{i+1} = \\lambda_k^i + \\epsilon^i \\left(\\sum_{l=1}^L \\alpha_{k,l}^{i+1}\\mathcal{J}_{k,l} + \\beta_k^{i+1} \\frac{S}{\\mathcal{B}_k^{i+1} \\mathcal{r}_k} - T^{i+1}\\right)$ (16)\n$\\mu^{i+1} = \\mu^{i} + \\epsilon^i \\left(\\sum_{k=1}^K \\mathcal{B}_k^{i+1} - B\\right)$\n$\\sigma_k^{i+1} = \\sigma_k^{i} + \\epsilon^i \\left(\\beta_k^{i+1} - \\sum_{l=1}^L \\alpha_{k,l}^{i+1}\\right)$"}]}