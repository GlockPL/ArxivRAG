{"title": "Resource Management for Low-latency Cooperative\nFine-tuning of Foundation Models at the Network\nEdge", "authors": ["Hai Wu", "Xu Chen", "Kaibin Huang"], "abstract": "Abstract-The emergence of large-scale foundation models\n(FoMo's) that can perform human-like intelligence motivates\ntheir deployment at the network edge for devices to access\nstate-of-the-art artificial intelligence (AI). For better user ex-\nperiences, the pre-trained FoMo's need to be adapted to spe-\ncialized downstream tasks through fine-tuning techniques. To\ntranscend a single device's memory and computation limitations,\nwe advocate multi-device cooperation within the device-edge\ncooperative fine-tuning (DEFT) paradigm, where edge devices\ncooperate to simultaneously optimize different parts of fine-\ntuning parameters within a FoMo. The edge server is respon-\nsible for coordination and gradient aggregation. However, the\nparameter blocks reside at different depths within a FoMo\narchitecture, leading to varied computation latency-and-memory\ncost due to gradient backpropagation-based calculations. The\nheterogeneous on-device computation and memory capacities\nand channel conditions necessitate an integrated communication-\nand-computation (C2) allocation of local computation loads and\nuplink communication resources to achieve low-latency (LoLa)\nDEFT. To this end, we consider the depth-ware DEFT block\nallocation problem. The involved optimal block-device matching\nis tackled by the proposed low-complexity Cutting-RecoUNting-\nChecking (CRUNCH) algorithm, which is designed by exploiting\nthe monotone-increasing property between block depth and com-\nputation latency-and-memory cost. Next, the joint bandwidth-\nand-block allocation (JBBA) makes the problem more sophis-\nticated, i.e., mathematically NP-hard. We observe a splittable\nLagrangian expression through the transformation and analysis\nof the original problem, where the variables indicating device\ninvolvement are introduced to decouple the block and bandwidth\nallocation. Then, the dual ascent method is employed to tackle\nthe JBBA problem iteratively. Within each iteration, block\nallocation and bandwidth allocation are optimized concurrently.\nThe optimal block allocation sub-problem is solved efficiently\nby applying the Hungarian method facilitated by the proposed\nCRUNCH algorithm. On the other hand, the bandwidth al-\nlocation sub-problem is solved in closed form, shedding light\non favorable allocations to resource-limited devices. Through\nextensive experiments conducted on the GLUE benchmark, our\nresults demonstrate significant latency reduction achievable by\nLoLa DEFT for fine-tuning a RoBERTa model.", "sections": [{"title": "I. INTRODUCTION", "content": "The remarkable advancements in Artificial Intelligence (AI),\nparticularly the human-like intelligence as demonstrated by\nlarge-scale foundation models (FoMo's), have drawn sig-\nnificant investments and research efforts from leading ICT\ncompanies such as Microsoft, Alphabet, and Nvidia [1], [2].\nIn particular, ChatGPT, a popular generative AI conversation\nmodel, attracts over 100 million active users per week [3].\nOn the other hand, in the era of mobile networks, their sixth-\ngeneration (6G) is expected to fully integrate AI in various\napplications, particularly capitalizing on the surge of FoMo's\n[4]\u2013[6]. To enhance the performance of pretrained FoMo's,\nfine-tuning is widely embraced as a small fraction of model\nparameters using small datasets, thereby maximizing their util-\nity for the desired downstream tasks [7]. Especially, relevant\ntechniques, collectively called parameter-efficient fine-tuning\n(PEFT), are capable of achieving comparable performance as\nfull-model training by adapting less than 1% parameters in\npretrained FoMo [8], [9]. Their high efficiencies have led to\nPEFT being widely adopted in the on-device tuning of FoMo,\ne.g., Apple Intelligence [10]. However, the distributed deploy-\nment of fine-tuning in a wireless network to leverage mobile\ndata is still hindered by the high dimensionality of FoMo\nparameters, the heterogeneous computation-storage capacities\nof edge devices, and limited radio resources [11]\u2013[13], \u03a4\u03bf\naddress the issue, we propose a novel resource management\nframework for a multi-device cooperative fine-tuning paradigm\nthat distributes computation loads to devices with awareness\nof their heterogeneous capabilities for computation, memory,\nand communication to expedite the adaptation of a FoMo to\na specific task.\nIn wireless mobile networks, there are two approaches to\norchestrate the fine-tuning of FoMo's with the cooperation\namong devices and servers. One involves uploading local data\nfor centralized fine-tuning at the server. The other approach,\ncalled device-edge fine-tuning (DEFT) as considered in this\nwork, leverages distributed computational resources and data\nat devices cooperatively update a FoMo [13], [14]. The former\ncircumvents devices' computation bottlenecks by centralizing\nmodel fine-tuning at edge servers with powerful AI hardware.\nSubsequently, task-oriented FoMo's can be downloaded to\ndevices to enable matching on-device intelligent applications\n[15]. However, extensive data uploading introduces significant\ncommunication delay and raises privacy concerns due to the\nexposure of raw personal data [13], [16]. The server may\nnot be able to guarantee timely and accurate online fine-\ntuning when requesting devices are many while increasingly\npowerful on-device processors remain underutilized. DEFT, to\nbe deployed on an edge-computing platform, can address the\ndrawbacks of centralized fine-tuning by ensuring personal data\nsecurity and relying on local computation [13]. It also eases"}, {"title": "", "content": "network congestion by avoiding data uploading and bringing\ncomputing to the proximity of user data. Additionally, such\nproximity allows flexible fine-tuning of FoMo's to adapt to\nusers' changing contexts and applications. FoMo's coopera-\ntively updated by multiple devices are automatically shared\namong them without explicit downloading. Their limitations\nin computation and communication are also surpassed in the\ncooperative process.\nDEFT is considered a particularization of mobile edge com-\nputing (MEC), which leverages advanced mobile processors to\nprocess the data closest to their sources [16], [17]. There exists\na rich and mature literature investigating communication-and-\ncomputation (C2) resources management of MEC in wire-\nless networks, primarily focusing on efficiently offloading\ncomputation tasks to edge clouds to enhance communica-\ntion/computation rates and mobile energy efficiencies [18]\u2013\n[20]. Subsequently, MEC research was expanded to include\nAI inference, training, and sensing tasks [21]. Cooperative\ntransmission of data for split edge inference/sensing [22]\u2013[24]\nand the optimal placement of deep neural networks (DNNs)\n[25] have been investigated to achieve high computational\nefficiencies and accuracies for AI tasks over finite-rate wireless\nlinks. A popular class of techniques for training DNN models,\ncalled federated learning (FL), have been developed to lever-\nage local data and on-device computation while preserving\nprivacy by avoiding direct data uploading, which shares the\nprinciples of DEFT [26]\u2013[29]. However, in each of the FL\niterations, every participating device is required to update the\nentire model and upload the associated stochastic gradients,\nwhich is infeasible for devices given the enormous size of\na FoMo that ranges from several to hundreds of billion\nparameters [13], [14]. An alternative approach, called split\nlearning, where a DNN model is cut into multiple parts for\nplacement on different devices to sequentially compute cor-\nresponding stochastic gradients for model updating based on\nback-propagation (see, e.g., [30]\u2013[32]). However, the transmis-\nsion of high-dimensional intermediate features for dependent\ncomputation among devices results in excessive overhead,\nespecially given the typically large number of iterations. For\nexample, the feature dimension of a RoBERTa base model\nusing a batch size of 32 is 12.5 million, which is over 500\u00d7\nof the number of LoRA fine-tuning parameters (i.e., 0.024\nmillion) in one associated block [8]. Meanwhile, deep fading\nchannels of devices are difficult to cope with by scheduling\ndue to the rigid sequential computation order of devices. Even\nthough researchers have attempted on parallel computing by\ncreating training pipelines, its practicality for mobile networks\nis questionable due to the uniform requirements on hardware\nand communication links for devices [31], [33].\nThe considered multi-device DEFT approach divides and\nschedules the fine-tuning parameter blocks within a FoMo\nfor edge devices to simultaneously optimize different parts of\ntunable parameters using local data. Compared with FL, de-\nvices' computation-and-memory requirements are dramatically\nlower as they are required to tune a small fraction of model\nparameters (e.g., 1%) instead of full model updating. More-\nover, the communication overhead in the context of DEFT\nis proportional to the number of tunable parameters and, thus,"}, {"title": "", "content": "also much less than that for split learning. Unlike split learning\nconstrained by dependent computing, device scheduling can\nbe employed for DEFT to cope with fading as well as mobile\nlimitations, which is actually the theme of this work. However,\nwhile DEFT has several advantages, its efficient implementa-\ntion faces a new challenge arising from the fact that the on-\ndevice gradient computation for an individual parameter block\nof a FoMo incurs memory cost and latency that are dependent\non the position (i.e., depth) of the block [34]\u2013[36]. Specifically,\nbased on the chain rule, the gradients of a specific block are\ncomputed by back-propagation from the last block, resulting in\nthe memory-latency cost being monotone-increasing functions\nof the block depth [35]. In the context of FoMo training,\nthe discrepancy among blocks is amplified as each attention\nblock in a transformer is overparameterized, e.g., e.g., 1.8\nbillion parameters in one transformer block of the GPT-3\nmodel [2]. While this fact is largely overlooked in traditional\ndistributed learning targeting relatively small models, it is\nimportant to consider the variations of memory-and-latency\ncost over blocks when designing multi-device DEFT that\nallocate blocks to devices for local updating. To be specific, an\noptimal allocation strategy should match the memory/latency\ncost of individual blocks with devices' memory/computation\ncapacities and channel states. This creates a new problem for\nDEFT termed block-device matching problem.\nIn this work, we make an attempt to solve this problem by\noptimizing the joint C2 resource management strategy to mini-\nmize the fine-tuning latency, yielding the proposed low-latency\nDEFT (LoLa-DEFT) framework. Such a problem is a mixed\ninteger problem and NP-complete. We overcome the challenge\nby designing a low-complexity depth-aware block allocation\nalgorithm named Cutting-RecoUNting-CHecking (CRUNCH)\nthat employs a key property of the problem that local com-\nputation latency-and-memory cost is a monotone increasing\nfunction of block depth. This allows the search space to be\ndramatically reduced by shrinking the set of qualified block-\ndevice pairs under a latency constraint. The optimal policy\nderived from solving the block-device matching problem is\nalso further developed to facilitate the optimal bandwidth\nallocation. The key contributions and findings of this paper\nare summarized as follows.\n\u2022 Block Allocation for LoLa DEFT. Given equal band-\nwidth allocation, the block-device matching problem is trans-\nformed into a sequence of feasibility check sub-problems with\nreduced complexity. Then each sub-problem under a latency\nconstraint from a discrete set involves finding the optimal\ndevice-block pairs to make DEFT feasible. The sub-problem\ncan be efficiently solved using the proposed CRUNCH algo-\nrithm thanks to the exploitation of the mentioned monotone re-\nlationship between latency-and-memory cost and block depth.\nThe solution nesting a linear search and CRUNCH algorithm\nis shown to achieve much lower complexity as opposed to the\nclassic Hungarian methods.\n\u2022 Joint Bandwidth-and-Block Allocation for LoLa\nDEFT. Next, we consider an advanced version of the block-\ndevice matching problem incorporating optimal bandwidth al-\nlocation. The result is a mixed-integer non-linear programming\none that is NP-hard. By analyzing the problem structure, we"}, {"title": "II. MODELS AND OPERATIONS", "content": "Consider a single-cell system where K edge devices co-\noperatively fine-tune a FoMo for a given downstream task, as\nillustrated in Fig. 1. The devices utilize on-device computation\npower and localized data to compute the gradient of tunable\nparameters in the FoMo using the stochastic gradient descent\n(SGD) method. The indices of these devices are denoted as\n\\(\\mathcal{K} = \\{1,2,\\ldots\\ldots, K\\}\\). Fine-tuning parameter division and depth-\naware parameter block allocation for multi-device cooperative\ncomputation are adopted to accelerate the fine-tuning process.\nSince the DEFT requires a repeated aggregation of on-device\ncomputed gradients to update the model parameters, the edge\nserver needs to perform depth-aware block allocation by\njointly considering local computation and gradient upload at\nthe beginning of each communication round. Hence, within\neach communication round, the activated devices, need to\nperform the following three steps: a) download one initial-\nized/aggregated FoMo; b) compute the gradients of a sched-"}, {"title": "Algorithm 1: Workflow of the Multi-device DEFT.", "content": "Initialization: Initialize fine-tuning parameter blocks\n\\(\\{\\phi_{\\iota}\\}^{(0)}\\}_{\\iota=1,\\ldots,L}\\).\nfor communication round \\(n = 1,\\ldots\\ldots, N\\) do\nServer:\nGet devices' memory \\(\\{b_k^{(n)}\\}\\), CSI \\(\\{H_k^{(n)}\\}\\),\ncomputation coefficient \\(\\{f_k\\}\\).\nPerform depth-aware block allocation (and\nbandwidth allocation).\nBroadcast latest model \\(\\{\\phi_{\\iota}^{(n)}\\}\\) and scheduling\ninstructions \\(\\{\\alpha_{k\\iota}^{(n)}\\}\\) (and \\(\\{B_k\\}\\)).\nDevices:\nCheck scheduling instructions \\(\\{\\alpha_{k\\iota}^{(n)}\\}\\)\nfor activated devices do:\nFetch the latest model.\nfor local iteration \\(m = 1,\\ldots, M\\) do:\nPerform gradient computation to optimize (2)\nbased on localized dataset.\nTerminate gradient propagation after the\nscheduled block's gradient is obtained.\nend for\nUpload the computed gradient \\(\\{g^{(n)}(\\phi_{\\iota})\\}\\).\nend for\nServer:\nAggregate the gradients of parameter blocks and\nupdate the FoMo based on (6).\nend for\nOutput: the cooperatively fine-tuned \\(\\{\\phi_{\\iota}^{(N)}\\}_{\\iota=1,\\ldots,L}\\).\nuled FoMo block in the model iteratively; c) upload the lo-\ncally computed FoMo gradients. After the server successfully\nreceives all parameter gradients, the latest updated FoMo\nis offloaded to all scheduled devices to start a new round.\nThe workflow of DEFT is summarized in Algorithm 1, with\ndetailed models and operations described in the following sub-\nsections."}, {"title": "A. Fine-tuning Model", "content": "In the considered DEFT, the tunable parameters are evenly\ndistributed over model blocks (e.g., Transformer-based Fo-\nMos) as commons adopted in the literature and practice [8],\n[38]\u2013[40]. Suppose the given pre-trained foundation model,\ndenoted by \\(P_{\\Phi}(y|x)\\), is parameterized by \\(\\Phi\\). \\(P_{\\Phi}(y|x)\\) is\na generic multi-task learner such as GPT. We follow the fine-\ntuning downstream task setup in [8] to adapt \\(P_{\\Phi}(y|x)\\) to"}, {"title": "B. On-device Computation Model", "content": "1) On-device Gradient Backpropagation-based Fine-\ntuning: In the considered FoMo division for DEFT, each\ndevice is responsible for computing the gradient of one\nparameter block in the entire \\(\\varphi\\). Consider an arbitrary device,\nsay device k, is scheduled to compute the gradient of a\nspecific fine-tuning parameter block \\(\\phi_{\\iota}\\), denoted as \\(g(\\phi_{\\iota})\\),\ntoward the following on-device optimization:\n\\begin{equation}\n\\max_{\\varphi_{\\iota}}\\sum_{(x,y)\\in Z^{(k)}}\\sum_{j=1}^y\\log (P_{\\Phi,\\varphi_{\\iota}} (y_j|x, Y_{<j})).\n\\end{equation}\nConsider the commonly adopted backpropagation approach\nfor gradient computation as shown in Fig. 2. The training data\nis fed and propagated through the entire foundation model to"}, {"title": "", "content": "calculate the loss function, i.e., the objectives in (2). There is\na need to store all the weights, intermediate activations and\ngradients along the propagation of the whole model in SGD-\nbased fine-tuning of foundation models. Then, the gradient\nof the loss function with respect to (w.r.t.) the fine-tuning\nparameter, denoted by \\(g(\\phi_{\\iota})\\), is obtained by the repeated\nemployment of chain rule from the gradient of the last layers\ntowards the former layers. Specifically, the gradient of the\nformer layer's parameters is determined by its intermediate\nactivation saved in the forward path and the later layer's\ngradient.\n2) Memory-aware On-device Cooperative Computation:\nTo transcend the on-device memory limitation, the fine-tuning\nparameter division allows the heterogeneous devices to cooper-\natively compute the gradient of fine-tuning parameters residing\nin different depths according to the corresponding memory\nbudgets. Once the device completes the gradient calculation of\nthe scheduled model block, the backward path is terminated.\nGiven the L block-repetitive transformer architecture, the\nserver needs to schedule L block among K devices (L < K).\nSpecifically, the allocation of a parameter block with depth\nindex l, i.e., \\(\\phi_{\\iota}\\), to device k in round n is denoted by \\(\\alpha_{k\\iota}^{(n)} = 1\\),\notherwise \\(\\alpha_{k\\iota}^{(n)} = 0\\). The on-device memory (in bits) required\nto compute the gradient of fine-tuning parameters with block\nindex l is denoted by \\(b(l)\\), where \\(b(\\cdot)\\) is a monotone-increasing\nfunction of l. For example, \\(b(l)\\) is a linear function over l if\nnone of the memory-saving techniques are adopted [35].\n3) On-device Computation Latency: To achieve a balanced\nfine-tuning of all parameters, the on-device gradient compu-\ntation within one round is performed with the same number\nof iterations, M, and the same batch size of training samples\nis adopted. The latency bottleneck of on-device computation\nis the backpropagation process, which triples the latency of\na forward path, see [36]. The sequential computation of a\nsubstantial number of parameters within each model block\nnecessitates the integration of diverse backpropagation lengths,\ni.e., \\(l\\in \\{L,\\ldots\\ldots,1\\}\\), for multi-device scheduling. Therefore,\nthe computation latency of one device is dominated by its\ncomputation capability and the depth index of the assigned\nparameter block. Given a device with normalized computation\ncapability, e.g., CPU frequency is 1GHz, the run-time duration\n(in seconds) of computing the gradient of block l is \\(d(l) =\na+c(l)\\), where a represents the latency of data feeding and the\nentire model forward, \\(c(l)\\) is the latency of backpropagation\nended at block l. \\(c(l)\\) is a monotone-increasing function of"}, {"title": "", "content": "l and generally is a linear one. Consider the heterogeneous\ncomputation capabilities of edge devices, each with a relative\ncomputation factor of the normalized one, denoted by \\(f_k\\), k.\nThen, the on-device computation latency of scheduling device\nk for fine-tuning model block l in round n can be written as\n\\begin{equation}\nT_{k,l,comp}^{(n)} =  \\alpha_{k\\iota}^{(n)}\\frac{M d(1)}{f_k} \\cdot \\alpha_{\\alpha_{kl} J_{k,l}}.\n\\end{equation}"}, {"title": "C. Communication Model", "content": "1) Model Downloading: At the beginning of each commu-\nnication round, the server first broadcasts the latest foundation\nmodel to all involved devices. Since the server is associated\nwith the base station, the broadcasting rate could be relatively\nhigh. Therefore, the downlink latency can be regarded as the\nsame among devices and is considered to be negligible.\n2) Gradient Uploading: Once the device finishes computa-\ntions within one round, the accumulated gradient of M local\niterations is uploaded to the server for central aggregation.\nThe devices are connected to the edge server via wireless\nchannels, where the orthogonal frequency-division multiple\naccess (OFDMA) is adopted for allocating a broadband\nspectrum to involved devices. The channels are assumed to\nbe frequency non-selective and there is a sufficiently large\nnumber of sub-carriers, making the bandwidth allocation can\nbe approximated as being continuous. The total bandwidth is\nrepresented by B. Consider an arbitrary device, say device\nk. Its transmit power is represented by \\(p_k\\), which is known\nby the server. The channel power in the n-th communication\nround is denoted by \\(H_k^{(n)}\\), which remains a constant within\none round and can be perfectly estimated by the server. Let\n\\(0 \\le B_k^{(n)} < B\\) denote the bandwidth allocated to device k.\nThe upload communication rate, \\(R_k^{(n)}\\), can be written as\n\\begin{equation}\nR^{(r)} = B_k^{(n)} \\log_2(1+\\frac{p_kH_k^{(n)}}{N_0+( \\eta_k^{(n)})^{-1}B_k^{(n)}}),\n\\end{equation}\nwhere \\(N_0\\) is the white Gaussian noise power, \\(\\eta_k^{(n)}\\) is defined\nto be the uplink spectrum efficiency. For the blockwise dis-\ntributed fine-tuning parameters, its size is assumed to be S (in\nbits). Then, the upload latency of scheduling device k targeting\nthe fine-tuning of model block l in round n can be written as\n\\begin{equation}\nT_{k,l,comm}^{(n)} = \\frac{S}{\nB_k^{(n)}r_k^{(n)}}\n\\end{equation}"}, {"title": "D. Global FoMo Updating", "content": "After the server successfully receives the gradients of all the\nmodel blocks, i.e., the accumulated gradient of all parameter\nblocks, in round n, denoted by \\(\\{g^{(n)}(\\phi_{\\iota})\\}_{\\iota=1,\\ldots,L}\\), the server\nperforms centralized model aggregation. Given the gradient\ndescent is applied, the updated model block \\(\\phi_{\\iota}\\) after round n,\nrepresented by \\(\\phi_{\\iota}^{(n)}\\) can be written as\n\\begin{equation}\n\\phi_{\\iota}^{(n)} = \\phi_{\\iota}^{(n-1)} \u2013 \\eta^{(n)}G(g^{(\\eta)} (\\phi_{\\iota})),\n\\end{equation}\nwhere \\(\\eta^{(n)}\\) is the predefined learning rate, \\(G(\\cdot)\\) represents the\nvariation of gradients depending on the selection of different\noptimizers. For example, the Adam optimizer relies on the first"}, {"title": "", "content": "and second-order moments for adaptive element-wise updates\nof fine-tuning parameters."}, {"title": "III. PROBLEM FORMULATION", "content": "As the core component of LoLa DEFT, the objective of\nparameter block allocation is to minimize the total fine-tuning\nlatency, considering the devices' heterogeneous computation\ncapability, available memory space, and channel status. In this\nsection, the associated block-device matching problem is for-\nmulated as two optimization problems for the relatively simple\ncase with given bandwidth allocation and the JBBA case.\nGiven a fixed number of communication rounds N, the latency\nminimization of the entire fine-tuning process is to minimize\nthe latency of every communication round. Therefore, the\nproblem can be simplified as a one-round latency minimization\nproblem equivalently. For ease of notation, the superscripts\nindicate the communication round number is omitted in the\nfollowing.\nConsider an arbitrary device, say device k. The associated\nlatency, \\(T_k\\), consists of two components, i.e., the on-device\ncomputation part and the uplink communication part, which\ncan be written as\n\\begin{equation}\nT_k = \\sum_{\\iota=1}^L (T_{k, \\iota, comp} + T_{k, \\iota, comm}) = \\sum_{\\iota=1}^L  \\alpha_{k\\iota} (J_{k,\\iota}+ \\frac{S}{B_k r_k}).\n\\end{equation}\nFor the considered multi-device cooperative system, the overall\nlatency is exactly determined by the device involved with the\nmaximum summed latency, i.e.,\n\\begin{equation}\nT = \\max_{k \\in \\mathcal{K}} \\{ \\sum_{\\iota=1}^L \\alpha_{k\\iota} (J_{k,\\iota}+ \\frac{S}{B_k r_k})\\}.\n\\end{equation}\nOn top of the target of latency minimization, block allocation\nrequires satisfying a few constraints as stated below. At the\nbeginning of each communication round, the server is required\nto schedule L distinct devices from K involved devices to\ncompute the fine-tuning parameter block residing in depth\nfrom l = L to l = 1, which introduces the following spreading\nmodel block constraints,\n\\begin{equation}\n(C1)\\quad \\sum_{k=1}^K \\alpha_{k,\\iota} = 1, \\forall \\iota.\n\\end{equation}\nThe constraint ensures that one model block must be allocated\nto exactly one edge device for fine-tuning. Moreover, a device\nis assigned to finish the task of computing the gradient of at\nmost one model block if it is involved, i.e.,\n\\begin{equation}\n(C2)\\quad \\sum_{\\iota=1}^L \\alpha_{k\\iota} \\leq 1, \\forall k.\n\\end{equation}\nBesides, the model block assignment for involved devices\nshould not exceed the device's available memory, i.e., avoiding\nhitting the memory wall. Considering the memory require-\nments for different on-device computations, the memory bud-\nget of device k is denoted as \\(b_k\\), the corresponding constraints"}, {"title": "", "content": "are given as\n\\begin{equation}\n(C3)\\quad \\sum_{\\iota=1}^L \\alpha_{k,\\iota}b(\\iota) \\leq b_k, \\forall k.\n\\end{equation}\nConsider the scenario with equal bandwidth allocation\n\\((B_k = B/L)\\). Under the above constraints and the target of one\nround latency minimization, the block allocation problem for\nmulti-device cooperative fine-tuning is formulated as follows:\n\\begin{equation}\n\\min_{\\{\\alpha_{k,\\iota}\\}} \\quad  \\max_{k\\in \\mathcal{K}}\\sum_{\\iota=1}^L  \\alpha_{k\\iota} T_{k,\\iota},\\\\\n(P1)\\quad s.t. \\quad T_{k,\\iota} = J_{k,\\iota} + \\frac{S L}{B r_k}, \\forall k, \\iota,\\\\\n\\quad\\quad\\quad (C1) \\& (C2) \\& (C3),\\\\\n\\quad\\quad\\quad \\alpha_{k,\\iota} \\in \\{0,1\\}, \\forall k, \\iota.\n\\end{equation}\nNext, consider the JBBA case where \\(\\{B_k\\}\\) is joint optimized\nwith block allocation. The diversified bandwidth assignment\ncan further align the latency of all involved devices to shorten\nthe overall duration. Specifically, the associated scheduling\nproblem is formulated as follows:\n\\begin{equation}\n\\min_{\\{\\alpha_{k,\\iota}\\},\\{B_k\\}}\\quad  \\max_{k\\in \\mathcal{K}}\\sum_{\\iota=1}^L  \\alpha_{k,l}T_{k,l}\\\\\n(P2)\\quad s.t.\\quad T_{k,l} = J_{k,l} + \\frac{S}{B_k r_k}, \\forall k, l,\\\\\n\\quad \\sum_{k=1}^K B_k \\leq B, \\\\\n\\quad (C1) \\& (C2) \\& (C3),\\\\\n\\quad \\alpha_{k,l} \\in \\{0,1\\}, \\forall k, l.\n\\end{equation}"}, {"title": "IV. DEPTH-AWARE BLOCK ALLOCATION FOR\nLOLA-DEFT", "content": "In this section, we design LoLa-DEFT by solving the block-\ndevice pairing problem in (P1). The optimal block allocation\nis obtained by first transforming the original min-max problem\ninto an equivalent latency-threshold minimization problem and\nthen addressing the later problem with low complexity via\nleveraging a linear search approach and a proposed depth\naware feasibility checking method."}, {"title": "A. Optimal Block Allocation via Feasible Set Minimization", "content": "For tractability, the min-max problem (P1) is transformed\ninto a minimization problem by introducing a variable of\nlatency threshold, \\(T_{th}\\), as\n\\begin{equation}\n\\min_{\\{\\alpha_{k,1}\\},T_{th}}\\quad T_{th}\\\\\n(P3)\\quad s.t.\\quad \\sum_{l=1}^L \\alpha_{k,l} T_{k,l} \\leq T_{th}, \\forall k,\\\\\n\\quad \\quad \\quad  \\text{Constraints in (P1)}.\n\\end{equation}\nThe equivalence between (P1) and (P3) is guaranteed by the\njoint optimization of \\(\\{\\alpha_{k,\\iota}\\}\\) and \\(T_{th}\\). It is observed from (P3)\nthat the feasible set of \\(T_{th}\\) can be restricted into the discrete\nset \\(\\{T_{k,l}\\}\\). Hence, the optimal latency threshold towards (P3)\ncan be obtained by searching over the \\(C^2\\) latency \\(T_{k,l}\\) of all"}, {"title": "", "content": "device-block pairs. For each searched latency threshold \\(T_{th}\\),\nthe feasibility of associated problem (P3) requires validation\nthrough the finding of the maximum matching on the left\nqualified device-block pairs. Hence, the workflow of finding\nthe optimal block allocation can be summarized into the\nfollowing four steps:\n\u2022 Step 1: Rearrange \\(\\{T_{k,l}\\}\\) in a descending order to gener-\nate a set \\(\\{T_j\\}\\) with \\(j \\in \\{1,\\ldots\\ldots, KL\\}\\) and \\(T_j \\geq T_{j+1}, \\forall j\\);\nset a variable \\(t = KL + 1\\) and initialize a set recording\nsolution of \\(\\{\\alpha_{k,\\iota}\\}\\) as \\(\\mathcal{P} = \\O\\).\n\u2022 Step 2: Update \\(t = t \u2212 1\\) and set \\(T_{th} = T_t\\).\n\u2022 Step 3: Validate if there is a feasible solution of \\(\\{\\alpha_{k,\\iota}\\}\\)\nunder the current latency constraint:\n\u2013 If yes, update the current solution into \\(\\mathcal{P}\\) and return\nto Step 2;\n\u2013 Otherwise, go to Step 4.\n\u2022 Step 4: Output \\(\\mathcal{P}\\) as the optimal solution.\nThe operation of such linear search, due to the discrete search\n\\(\\{T_{k,l}\\}\\), results in polynomial complexity of \\(O(KL)\\) in the\nworst case. However, the feasibility validation in each iteration\ncauses the main complexity. To be specific, the devices and\nFoMo blocks are treated as two disjoint sets in a bipartite\ngraph, formulating the feasibility validation as a matching\nproblem. The feasibility validation is then to find a pairwise\nmatching between devices and blocks under the constraints\nof ensuring all blocks are matched with different devices.\nThe resulting matching offers a possible block assignment,\nwhose feasibility is determined by checking the constraints of\n\\(\\sum_{\\iota=1}^L \\alpha_{k,l}T_{k,\\iota} \\leq T_{th}\\) under the current requirement \\(T_{th} = \\hat{T}_t\\).\nThe conventional Hungarian algorithm can be leveraged to\nperform the said bipartite matching. However, even using\nits most efficient variant, the Hungarian algorithm requires a\ncomputation complexity of \\(O(\\max\\{K, L\\}^3)\\), together with the\noperation of linear search, resulting in the overall complexity\nof \\(O(KL \\max\\{K, L\\}^3)\\). Such complexity is not acceptable\nwhen a large number of devices or blocks are involved."}, {"title": "B. Depth-aware Low-complexity Matching Validation", "content": "On top of resorting to solving the bipartite matching, we\npropose a low-complexity feasibility-validation method that\nexploits the correlation between block depth and latency. To\nbe specific, the memory and computation latency required\nby a block monotonically increase as its depth increases.\nThis endows the matching bipartite graph of the feasibility-\nvalidation problem with a special geometric structure that\nallows for low-complexity matching.\nBefore introducing the low-complexity feasibility-validation\ncriterion, we first define the auxiliary notations. Typically,\nconsidering a given latency constraint \\(T_{th}\\), a K-by-L matrix,\ndenoted by \\(Q_{T_{th}}\\, is initialized to characterize both memory\nconstraints and C\u00b2 latency of each fine-tuning block at each\ndevice. The elements of \\(Q_{T_{th}}\\)\nare computed as\n\\begin{equation}\nq_{k,\\iota} =\\begin{cases}"}]}