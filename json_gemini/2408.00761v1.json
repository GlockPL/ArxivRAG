{"title": "Tamper-Resistant Safeguards for Open-Weight LLMs", "authors": ["Rishub Tamirisa", "Bhrugu Bharathi", "Long Phan", "Andy Zhou", "Alice Gatti", "Tarun Suresh", "Maxwell Lin", "Justin Wang", "Rowan Wang", "Ron Arel", "Andy Zou", "Dawn Song", "Bo Li", "Dan Hendrycks", "Mantas Mazeika"], "abstract": "Rapid advances in the capabilities of large language models (LLMs) have raised widespread concerns regarding their potential for malicious use. Open-weight LLMs present unique challenges, as existing safeguards lack robustness to tampering attacks that modify model weights. For example, recent works have demonstrated that refusal and unlearning safeguards can be trivially removed with a few steps of fine-tuning. These vulnerabilities necessitate new approaches for enabling the safe release of open-weight LLMs. We develop a method, called TAR, for building tamper-resistant safeguards into open-weight LLMs such that adversaries cannot remove the safeguards even after thousands of steps of fine-tuning. In extensive evaluations and red teaming analyses, we find that our method greatly improves tamper-resistance while preserving benign capabilities. Our results demonstrate that tamper-resistance is a tractable problem, opening up a promising new avenue to improve the safety and security of open-weight LLMs.", "sections": [{"title": "1 Introduction", "content": "The most capable open-weight large language models (LLMs) released over the past year now rival closed-source frontier models [32]. The availability of open-weight LLMs for anyone to download and use has yielded numerous benefits, including lowering costs for end users and enabling academic research on safety and security [66]. However, as these models become increasingly powerful, many have raised concerns that they could be repurposed by malicious actors to cause harm, motivating research on how to safeguard these models against malicious use.\nExisting open-weight models often adapt safeguards designed for closed-weight models served through APIs [50]. These safeguards include refusal mechanisms and preference-based training, and they have provided substantial robustness against input-based jailbreaking attacks. However, recent work has demonstrated these safeguards are trivially defeated by attacks that edit model weights, breaking down after only a handful of fine-tuning steps [42]. This poses a serious problem for open-weight models, because adversaries have full access to model weights and can tamper with built-in safeguards.\nThe vulnerability of open-weight models to tampering attacks poses risks for model developers as well. Under background tort law, AI developers must exercise reasonable care, meaning they have an obligation to take reasonable precautions to prevent foreseeable harm. If malicious actors can easily customize models to cause critical harm, model developers may inadvertently violate reasonable care standards and become open to liability under existing law. Thus, there is an urgent need for more robust safeguarding techniques that can withstand tampering attacks.\nIn this work, we study the problem of tamper-resistant safeguards for LLMs. This problem is depicted in Figure 1. Unlike existing research on LLM safeguards, we focus on attacks that modify model weights, which we refer to as tampering attacks. This problem has been considered very challenging and by some intractable, as no method has yet provided substantial robustness to these attacks. However, making progress on this problem would provide a valuable tool to regulators and model developers by ameliorating the dual-use dilemma of open-weight models [38].\nTo demonstrate that progress on this problem is possible, we develop the first LLM safeguards that obtain strong robustness against a wide variety of tampering attacks. Our approach allows developers to add a safeguard such that tampering attacks cannot easily remove the safeguard, while preserving the general capabilities of the LLM. We achieve this by performing adversarial training against tampering attacks, leveraging approaches from meta-learning. We identify various crucial factors that enable our method to work, including the choice of tamper-resistance loss, the selection of train-time adversaries, and the two-stage approach that we use for building in safeguards.\nWe apply our method to develop tamper-resistant unlearning and refusal safeguards. In experiments, we demonstrate that our safeguards are far more robust to tampering attacks than prior methods. We stress-test our safeguards with extensive red teaming evaluations against 28 test-time adversaries, demonstrating resistance to fine-tuning attacks up to 5,000 steps. We hope our results foster future work on this important problem. Our experiment code and models are available at https://github. com/rishub-tamirisa/tamper-resistance."}, {"title": "2 Related Work", "content": "Adversarial attacks on LLMs. Due to the extensive pre-training distribution of modern LLMs, they are prone to generating harmful content [48, 36]. To mitigate this, many LLMs undergo fine-tuning to implement safeguards [50, 3, 40], using methods such as reinforcement learning from"}, {"title": "3 Tamper-Resistant Safeguards", "content": "We assume the defender releases an LLM with weights $\\theta_G$ and a safeguard G applied. The defender's goal is to design G such that $\\theta_G$ obtains high values on $safety\\_metric(\\theta_G)$ and $capabilities\\_metric(\\theta_G)$. Moreover, the defender seeks to preserve a high value of $safety\\_metric(\\theta')$ after the adversary's move. We consider a compute-bounded adversary with unrestricted access to $\\theta_G$, enabling attacks that directly modify $\\theta_G$. We refer to these as \u201ctampering attacks.\" The adversary's goal is to obtain a model $\\theta'$ that minimizes the safety metric given reasonable compute limits, such as fine-tuning for 1,000 to 5,000 steps. We assume the adversary will not spend a significant fraction of the compute required to pre-train the LLM, since at that point they could train their own model without safeguards."}, {"title": "3.2 Problem Definition and Metrics", "content": "We describe a general notation for quantifying the tamper-resistance of safeguards. Define $G$, $\\theta_G$, $safety\\_metric$, and $capabilities\\_metric$ as in the threat model. Let $attack$ denote a compute-bounded adversarial attack that maps $\\theta_G$ to $\\theta'$, with stronger attacks obtaining lower values of $safety\\_metric(\\theta')$. We say that a safeguard G is tamper-resistant if its post-attack $safety\\_metric(\\theta')$ is high across a broad range of strong test-time adversarial attacks $A_{test}$.\nNote that $\\theta_G$ often modifies an underlying $\\theta$ that lacks safeguards, often through a fine-tuning procedure. Additionally, strong tamper-resistance can be obtained if the safeguard simply over-writes $\\theta$ with noise, but this model would no longer be useful. Thus, maintaining a high $capabilities\\_metric(\\theta_G)$ is crucial, and evaluation of a safeguard must consider both its tamper-resistance and how well it preserves general capabilities.\nWe focus on two common safeguard domains: weaponization knowledge restriction and harmful request refusal. In each domain, we define safety and capabilities test metrics, which we use alongside test-time adversaries to evaluate tamper-resistant safeguards.\nWeaponization knowledge restriction. In weaponization knowledge restriction, safeguards prevent the model from producing text about weaponization knowledge, while preserving capabilities for benign knowledge domains. Existing safeguards of this nature include representation engineering methods like circuit breaking [69]. The $safety\\_metric$ is defined as error on a forget set, and the $capabilities\\_metric$ is defined as accuracy on a retain set. Specifically, we consider the problem of restricting biosecurity, chemical security, and cybersecurity knowledge, and evaluate the resulting model on the Weapons of Mass Destruction Proxy (WMDP) benchmark [29]. WMDP contains 3,668 multiple-choice questions, spanning biosecurity, chemical security, and cybersecurity knowledge. Importantly, WMDP questions do not evaluate hazardous knowledge directly, but instead measure proxy expert-level knowledge for each hazardous domain, such that suppressing the expert-level knowledge would also suppress the hazardous knowledge. We define the forget set as the respective hazardous knowledge subject in WMDP, and retain set as the complement of the given subject in MMLU [17], a multi-task question-answering benchmark spanning 57 tasks across a variety of knowledge domains.\nHarmful request refusal. In the harmful request refusal setting, safeguards prevent the model from producing \"harmful\" outputs. We define the $safety\\_metric$ as the complement of average Attack Success Rate (ASR) of various jailbreaking attacks, while the $capabilities\\_metric$ captures the conversational abilities of $\\theta_G$. Specifically, we use a static set of test cases from HarmBench, an automated red-teaming framework for measuring prompt jailbreak robustness in LLMs, to evaluate jailbreak ASR [35] after tampering attacks. We use MT-Bench, a multi-turn question-answering benchmark graded by an LLM judge, to evaluate conversational abilities [63]."}, {"title": "3.3 Red Teaming", "content": "To properly measure the robustness of tamper-resistant safeguards, we conduct red-teaming with up to 28 adversaries, including many that are unseen at training time. In our evaluations, we subject our method to adversaries with varying compute budgets, access to held-out datasets, and diverse hyperparameters. For fine-tuning adversaries, we vary the learning rate, learning rate scheduler, optimization algorithm, and batch size. Many of these adversaries were fixed during early experiments, with some added over time as we found attacks that broke intermediate versions of our method. Extensive stress testing of this nature is critical for obtaining confidence in a tamper-resistant safeguard. For research on developing these safeguards, extensive red teaming also allows measuring incremental progress, using the number and strength of existing attacks one can defend against as a robustness metric."}, {"title": "4 Safeguard Tamper-Resistance Training", "content": "To obtain tamper-resistant safeguards, we propose a new method outlined in Algorithm 1 inspired by adversarial training and meta-learning to directly strengthen LLM safeguards against tampering attacks, called Tampering Attack Resistance (TAR). We identify unique properties of this adversarial training regime and leverage them to improve robustness.\nOur method for training tamper-resistant safeguards consists of two phases: (1) model safeguarding and (2) tamper-resistance training."}, {"title": "4.1 Model Safeguarding", "content": "The method begins by including an initial safeguard G into a base model $\\theta$. For example, initial safeguards for knowledge restriction can be drawn from a wide variety of existing methods, including circuit breaking [29] or constrained gradient ascent for a particular knowledge domain. Similarly, we can include a refusal safeguard by performing RLHF [41] or DPO [43] on refusal completions. Importantly, these initial safeguards do not need to be tamper-resistant. Empirically, we find that this safeguarding step is important for obtaining strong tamper-resistance."}, {"title": "4.2 Tamper-Resistance Training", "content": "Starting from $\\theta_G$, we train the tamper-resistant $\\theta$ using a novel adversarial training procedure. Namely, we train against a set of tampering attacks $A_{train}$, where the defender's objective is to maximize a proxy $safety\\_metric$ after applying an adversarial attack $attack \\sim A_{train}$ to $\\theta$. Since it may not be feasible to differentiate through $attack$, we draw on insights from prior work in meta-learning, defining $attack(\\theta) = \\theta' = \\theta_G + attack'(\\theta_G)$ as a perturbation on top of initial parameters, where backpropagation through $attack'$ is approximated with a straight-through estimator [5].\nWe focus on supervised fine-tuning (SFT) adversaries where $attack$ applies several steps of opti-mization to $\\theta_G$, which allows straight-through estimation through $attack'$ to benefit from the setting and approximations of first-order MAML [12]. However, we note key differences in our approach from standard meta-learning and prior methods [12, 16]. In particular, traditional meta-learning techniques seek to obtain a model initialization that is close to optimality on multiple test distributions. In our setting, we seek to obtain an initialization that is far from optimality on multiple adversaries' test distributions. Novel to our approach in this new setting is the use of a tamper-resistance loss in the \"outer loop\" that differs from the fine-tuning adversary's loss function and serves to maximize the proxy safety metric. We depict this structure in Algorithm 1, and explain the objective below.\nImpeding the adversary's loss. The aim of tamper-resistance training is to prevent adversaries with large compute budgets from reducing the $safety\\_metric$ at test-time. In adversarial training for tamper-resistance, we define a tamper-resistance loss $L_{TR}$ that counters $attack$. We operational-ize our goal of avoiding adversary optimality as searching for $\\theta$ such that $L_{TR}$ is minimized for $attack(\\theta)$."}, {"title": "5 Experiments", "content": "We evaluate TAR in weaponization knowledge restriction and harmful request refusal settings, with results shown in Table 1 and Table 2 respectively. We discuss the setup, baselines, and analysis for our results. In each setting, we use a specific set of training adversaries $A_{train}$ and test adversaries $A_{test}$. Further experiment details are presented in Appendix D."}, {"title": "5.1 Weaponization Knowledge Restriction", "content": "We now describe the setup, baselines, and results for our weaponization knowledge restriction experiments, including the knowledge domains, optimizers, and evaluation details.\nSetup. We focus on implementing tamper-resistant safeguards for restricting proxy weaponization knowledge about biosecurity, chemical security, and cybersecurity from Llama-3-8B-Instruct [1] that has been initially safeguarded via the Random Mapping method discussed in Appendix B.1. For each weaponization domain, we assign $D_{TR}$ to the corresponding forget set described in Appendix D.1. We proceed to sample train-time 64-step fine-tuning attacks from different data distributions, detailed in Appendix D.2. We use $N = 750$ outer loop steps, ScheduleFree AdamW [9] with a learning rate of $2 \\times 10^{-5}$ as the outer loop tamper-resistance optimizer, and loss scales of $\\lambda_{TR} = 4.0$, $\\lambda_{retain} = 1.0$. Lastly, We evaluate Pre-Attack and Post-Attack accuracy on corresponding WMDP subjects [29] averaged across all adversaries in Appendix E.1, and measure benign capabilities via the complement of subjects related to each proxy weaponization domain in MMLU [17].\nBaselines. We evaluate two recently proposed knowledge restriction methods: RMU [29] and LLMU [57]. We also design two baseline methods for knowledge restriction: Min Posterior, which minimizes posterior loss on forget set tokens; Max Entropy, which maximizes entropy on forget set tokens. Two additional methods, MLAC [16] and SOPHON [10], require substantial modifications for the LLM setting, so we show results on adapted versions of these baselines in Appendix F.3.\nResults. We show weaponization knowledge restriction safeguard results on Llama-3-8B-Instruct in Table 1 and Figure 2. These results are averaged across all 28 adversaries from Appendix E.1. Our large-scale experiments corroborate the findings in recent work that existing LLM safeguards are"}, {"title": "5.2 Harmful Request Refusal", "content": "We now describe the setup, baselines, and results for our harmful request refusal experiments, including the datasets used and evaluation details.\nSetup. For harmful request refusal training, we seek to make existing refusal safeguards in Llama-3-8B-Instruct robust to tampering attacks. We sample train-time adversaries that perform 64-step SFT attacks using the Anthropic-HH-RLHF dataset [3], following the methodology in Appendix D.2. Similar to the weaponization knowledge restriction setting, we use $N = 100$ outer loop steps, ScheduleFree AdamW [9] with an LR of $6 \\times 10^{-5}$ as the outer loop tamper-resistance optimizer, and loss scales of $\\lambda_{TR} = 0.1$, $\\lambda_{retain} = 1.0$. We evaluate the Post-Attack jailbreak attack success rate (ASR) on HarmBench [35] after the tampering attacks in Appendix E.2, and measure benign capabilities preservation via MT-Bench [64], which evaluates multi-turn conversation ability.\nBaselines. We consider 4 baselines alongside our TAR model: Llama-3-8B-Instruct (Refusal Trained); Representation Rerouting (RR) [69] on Llama-3-8B-Instruct, which trains to push repre-sentations for harmful input prompts to be orthogonal to the original corresponding representations in Llama-3-8B-Instruct; R2D2 [35] on Zephyr-7B [51], which performs adversarial training against GCG attacks [67]; and RepNoise [47] on Llama-2-7B [50], which regularizes harmful representations to Gaussian noise.\nResults. We show refusal results in Table 2. While the Refusal Training, RR, and R2D2 baselines resist jailbreak attacks in HarmBench before tampering, we find that percentage attack success\nOverall, we find that TAR provides significantly more robustness to realistic fine-tuning attacks than all prior methods, including attacks as strong as 5,000 steps of optimization on completely held-out data. These results demonstrate for the first time that obtaining strong tamper-resistance for open-weight LLMs may be possible."}, {"title": "5.3 Analysis", "content": "Red teaming. To stress-test the tamper-resistance of our models, we conduct an extensive suite of supervised fine-tuning attacks with 28 distinct adversaries. We vary the optimizer, number of optimization steps, learning rate, learning rate schedule, fine-tuning dataset, batch size, and overall fine-tuning method (e.g., full fine-tuning versus parameter-efficient fine-tuning). By default, our attacks use 1,000 fine-tuning steps, although some use 5,000 steps. Full details for these adversaries are provided in Table 9.\nWe show red teaming results in Figures 4 and 7. While baseline safeguards withstand fine-tuning attacks in a small number of cases, most adversaries succeed in removing the safeguards. By contrast, our TAR safeguard is robust to a wide range of adversaries. This shows that tamper-resistance is a tractable problem on which progress can be made. However, our method is currently not robust to parameter-efficient fine-tuning (PEFT) attacks (adversaries 27 and 28), highlighting the importance of extensive red teaming when developing tamper-resistant defenses. We hypothesize that future work could easily address this issue, as we demonstrate in Appendix C.3 that targeted patching of vulnerabilities is possible.\nGeneralization to stronger test-time attacks. In Figure 5, we show a fine-tuning attack at an LR of $2 \\times 10^{-5}$ that attempts to recover biosecurity knowledge on our TAR model and a model safeguarded with LLMU. We find that the tamper-resistance of TAR generalizes far be-yond the 64 steps used by train-time adversaries. Surprisingly, we observe that the test-time ad-versary's cross-entropy loss does not decrease below 7 for all 1,000 steps. Moreover, the loss enters a plateau and does not decrease at all af-ter 200 steps. In Appendix C.2, we find that the length and height of this plateau can be in-creased by increasing the number of inner loop steps during adversarial training. As a point of reference, we show the progression of the same attack on LLMU. In this case, the adversary's loss decreases to within the recovery region in under 20 steps. We note that the adversary's test loss decreasing into the recovery region does not always correspond with recovery on down-stream metrics (e.g., WMDP), but often does."}, {"title": "6 Conclusion", "content": "We introduced a novel method for implementing tamper-resistant safeguards for LLMs and explored applications in weaponization knowledge restriction and harmful refusal training. We compare our results to prior work in each setting, finding that our method is the first method robust under the rigorous red-teaming evaluation that we consider. More broadly, we demonstrate that progress on open-weight tamper-resistance is tractable. We believe this line of research is crucial for enabling on-going deployment of robust, open-weight LLMs, ensuring their alignment with regulatory frameworks and preemptively addressing the risk of malicious use."}, {"title": "A Limitations", "content": "Our method for training tamper-resistant safeguards demonstrates considerable robustness against a wide range of tampering attacks, yet several avenues for improvement remain: (1) While we focus on supervised fine-tuning attacks, the broader spectrum of open-weight tampering techniques necessitates diverse future red-teaming efforts. (2) Scaling to larger models poses computational challenges that require optimization to reduce overheads.\nTamper-resistance alone cannot fully mitigate the risks of malicious AI use. While it raises the initial costs for adversaries, it can eventually be circumvented. Once open-weight models are released, they cannot be \u201cunreleased,\u201d leaving any compromised defenses permanently vulnerable. Therefore, tamper-resistance should be considered a supplement to the broader effort of of improving the offense-defense balance of AI systems. Addressing these limitations will improve the robustness of LLMs to tampering and better support open-weight model developers."}, {"title": "B Method Details", "content": ""}, {"title": "B.1 Initial Weaponization Knowledge Restriction Safeguard", "content": "Prior to tamper-resistance training, we install a safeguard that achieves surgical knowledge restriction on the target hazardous domain. Let $h_\\theta(D)$ denote the distribution of post-decoder layer residual stream activations for input sequences sampled from some data distribution D and model weights $\\theta$. We define rand_hashed(x) for some input sequence x, which returns fixed Gaussian-sampled vectors that are chosen via hashing the corresponding input token for each residual stream index of x in $\\theta$. As a proxy for scrubbing target representations according to downstream task labels, we propose a weaponization knowledge restriction safeguard termed Random Mapping, which maps $h_\\theta(D_{TR})$ to random noise as follows:\n$\\min_\\theta E_{x \\sim D_{TR}} \\Big|\\Big| 1 - \\frac{h_\\theta(x) \\cdot rand_hashed(x)}{||h_\\theta(x)|| ||rand_hashed(x)||} + L_{LM}(\\theta; D_{retain}) \\Big|\\Big|$        (3)\nThe objective of Equation 3 maximizes cosine similarity between row vectors in the residual stream in every layer of the LLM from h($D_{TR}$) and the hashed random vectors from rand_hashed(\u00b7). By providing each token's residual stream a unique random vector to push toward, the loss encourages a \"re-mapping\" of token representations from $D_{TR}$ to the noised vectors. We include an additional term for preserving performance on $D_{retain}$ via the language-modelling cross-entropy loss $L_{LM}$. We show the performance of the raw Random Mapping safeguard as an ablation in Table 5, listed as \"Excl. Adv. Training.\u201d"}, {"title": "B.2 Designing the Tamper-resistance Loss", "content": "Weaponization knowledge restriction. For weaponization knowledge restriction, we summarize our intuition for ideal tamper-resistance loss design, corroborated by our empirical findings in Figure 3: we seek to flatten the adversary's loss at a high value, rather than simply raise its y-intercept.\nWe choose the tamper-resistance loss $L_{TR}$ as an entropy loss to be maximized during the adversary's cross-entropy fine-tuning trajectory, since maximizing entropy would impede the adversary's cross entropy loss from decreasing during fine-tuning. In other words, we wish to obtain $\\theta$ such that after an adversary performs a fine-tuning attack on $\\theta$ via a cross-entropy loss, entropy is still high. We find that this formulation achieves the desired flattening behavior, and we depict the difference in flattening between the choosing $L_{TR}$ to be a negative cross-entropy loss and negative entropy loss in Figure 3. In the lefthand plot, where $L_{TR}$ is a cross-entropy loss, loss only increases in the first inner loop step. In the righthand plot, where $L_{TR}$ is a negative entropy loss, entropy is eventually maximized in all inner loop steps. Figure 5 also demonstrates the generalization of the flat adversary loss behavior beyond the length of the simulated fine-tuning trajectories during TAR.\nHarmful request refusal. For harmful request refusal, we choose $L_{TR}$ to be the DPO loss [43], which works as follows. Given a DPO dataset containing pairs of rejected and refusal completions, the sampled attack performs SFT on rejected completions, and the tamper-resistance loss $L_{TR}$ is"}, {"title": "B.3 Efficiently Sampling Fine-tuning Attacks", "content": "Optimizing Equation 1 with gradient descent requires simulating K tampering attacks for each tamper-resistance optimizer update, which is prohibitively expensive to run when the sampled attack performs SFT and $\\theta$ contains billions of parameters. Inspired by prior work on snapshot ensembles [19], we leverage an efficiency trick: we can reuse the coordinates along steps of a single adversary fine-tuning trajectory of length K to obtain K \u2212 1 additional (though non-independent) trajectories of increasing length. Using this trick, we collect all K parameter coordinates along the trajectory into a single batch for computing the tamper-resistance losses, effectively sampling attack from $A_{train}$ non-IID. To further improve runtime efficiency, we do not compute the tamper-resistance loss $L_{TR}$ on all K steps and instead sub-sample coordinates along the trajectory for computing $L_{TR}$ within an adversary batch, for example every 4 adversary optimization steps. Additionally, we reduce variance in the tamper-resistance gradient by computing the tamper-resistance loss at each inner loop step on the same held-out batch, denoted as $x_{TR}$ in Algorithm 1."}, {"title": "B.4 Implementation Details and Resource Requirements", "content": "We perform TAR training on Llama-3-8B-Instruct [32] with 8 NVIDIA 80GB A100 GPUs, leveraging distributed training via FSDP [45, 44, 62]. We use ZeRO Stage 3 from DeepSpeed [44], which shards optimizer states, gradients, and parameters during training. While the efficiency trick in Appendix B.3 improves runtime, we note additional considerations for conserving GPU memory.\nFirst, simulating fine-tuning attacks that require additional state (e.g., momentum) in the inner loop of Algorithm 1 requires initializing a fresh optimizer for every outer loop iteration. Since we use an outer-loop optimizer that also requires maintaining state (ScheduleFree AdamW [9]), we move the outer loop optimizer to the CPU before instantiating inner-loop optimizers.\nSecond, first-order meta-learning in smaller models can typically be implemented by running multiple forward passes for each inner loop iteration, averaging losses, then backpropagating on the averaged loss term. However, because each inner-loop tamper-resistance loss term ($L_{TR}$ in Algorithm 1) is computed on a separate forward pass, this requires maintaining K computation graphs in memory. Since this is infeasible on reasonable hardware for LLMs with billions of parameters, we circumvent this inefficiency by accumulating tamper-resistance gradients in a separate data structure ($g_{TR}$ in Algo-rithm 1). We note that this can be done without using additional all-gather and reduce-scatter distributed operations, since tamper-resistance gradient accumulation and application to the pre-inner loop model parameters ($\\theta_{i-1}$ in Algorithm 1) can be computed solely on sharded gradients."}, {"title": "C Additional Experiments", "content": ""}, {"title": "C.1 Benign Fine-Tuning", "content": "An important property of open-weight models is that they can be fine-tuned to improve performance on custom data or in specific domains. Thus, ideal tamper-resistant safeguards should allow continued fine-tuning of a model while preserving the safeguard. We evaluate whether TAR models can be fine-tuned on data unrelated to the safeguard using economics as an example domain. Using TAR models with biosecurity and cybersecurity safeguards, we perform supervised fine-tuning on the"}, {"title": "C.2 Varying the Train-time Inner-loop Length K", "content": "Recall that via the efficiency trick discussed in Appendix B.3, a single inner loop trajectory of length K during TAR returns the K sam-pled attacks in Algorithm 1. We compare the test-time loss robustness as we vary the length of the inner loop K during TAR, running fine-tuning attacks for 1,000 steps on a held-out forget dataset for biosecurity weaponization (Ad-versary 8 in Table 9). For each value of K, we observe a plateau in the test loss that drops off at later steps as K increases. This suggests that the robustness of TAR improves as the inner loop length increases. Prior work also corrobo-rates that increasing the inner-loop length during meta-learning increases test-time generalization [16]. We note the contrast to conventional meta-learning methods mentioned in Section 4, in which typical meta-learning applications seek optimality after as few test-time steps as possi-ble [39, 12]. Here, our results suggest that the\nTAR objective is incentivized to run with as many inner loop steps as possible. We find that K = 64 provides significant robustness to the range of adversaries we consider in Section 5 and Appendix E.1, while balancing computational efficiency as discussed in Appendix B.3."}, {"title": "C.3 Targeted Patching of Vulnerabilities", "content": "In Section D.2, we discuss the set of adversaries sampled during TAR for each weaponization knowledge domain. However, we included the \u201cRetain-set SFT followed by Weaponization-domain SFT\u201d (R\u2192F in Table 4) adversary to improve robustness to specific adversaries that broke the defense in earlier iterations of the method that did not sample this adversary. We find in Table 4 that including the R\u2192F adversary significantly improves robustness to more test-time attacks, indicated by an improvement in Post-Attack Forget error by over 20.0 percentage points. Importantly, we find that including the 64-step R\u2192F adversary at train-time improves test-time robustness at for the 1000-step"}, {"title": "C.4 Ablations", "content": "Retain loss l2 representation engineering term. We observe in Table 5 that retain-set MMLU performance is worse when excluding representation engineering [66] retain loss from TAR. When including the term, retain MMLU improves by at least 5.0 percentage points.\nIncluding the initial safeguard. In Table 5, we examine the impact of incorporating the Random Mapping safeguard step prior to the adversarial training phase during TAR. The Random Mapping safeguard in isolation achieves a near-random chance Pre-Attack Forget accuracy of 27.3. However, it is susceptible to fine-tuning attacks similar to other baselines in Table 1, indicated by a higher Post-Attack Forget accuracy of 59.4. When including the tamper-resistance adversarial training phase, we observe significantly increased tamper-resistance as the Post-Attack Forget accuracy decreases by nearly 28 percentage points.\nWe also examine the impact of excluding the initial safeguarding step (\u201cExcl. Initial Safeguard\" in Table 5), finding that Pre-Attack and Post-Attack Forget accuracies are both higher without the initial safeguard. While the tamper-resistance adversarial training phase achieves moderate tamper-resistance without the initial safeguard step, we find that including the Random Mapping phase improves downstream tamper-resistance by 15.6 percentage points.\nVarying the tamper-resistance loss scale $\\lambda_{TR}$. We compare the downstream robustness of TAR when varying the tamper-resistance loss weighting $\\lambda_{TR}$ between 1.0 and 4.0 in Table 6. We observe that when setting $\\lambda_{TR}$ = 1.0, TAR maintains high retain MMLU accuracy at 62.5 percentage points, with moderate tamper-resistance indicated by a Post-Attack Forget accuracy of 40.8. Further increasing $\\lambda_{TR}$ to 4.0 in our final TAR model results in a significantly improved Post-Attack Forget Accuracy of 31.3, with a partial decrease in Retain MMLU to 54.9. When varying $\\lambda_{TR}$, we keep $\\lambda_{retain}$ constant; thus, our results indicate a clear way to increase downstream tamper-resistance by increasing the weighting of the tamper-resistance gradient during TAR, reflecting a balance between"}, {"title": "D Experiment Details", "content": ""}, {"title": "D.1 Weaponization Domain Proxy Dataset Details", "content": "Biosecurity. We use a synthetically labeled partition of the Pile [13] that filters for relevance to biology and the Camel AI Biology dataset [28]. We generate synthetic labels for Pile token sequences using openchat-3.5 [52], categorizing them as \"Cellular Biology\" or not. This process yields 49,984 samples: 7,558 for the forget-set (Pile-bio Forget) and 42,426 for the retain set (Pile-bio Retain). Concurrently, we pack entries from the Camel AI Biology dataset to the truncation-enabled 256 tokenization limit, resulting in 54,258 samples of about 188 words each (Camel-bio Forget). We apply the same procedure to our held-out hazardous biology dataset (identical to the WMDP biosecurity Forget-set), producing 598,933 samples of similar length (OOD Forget).\nChemical Security. We use a private forget dataset containing text sequences about hazardous chemical security content (Chem Forget).\nCybersecurity. We scrape CTF writeups on CTFtime [8] that are numbered between 1 and 39181, collecting cybersecurity writeups written as recently as 2024. We filter to keep writeups that contain more than 150 characters. As a result of filtering and HTTP errors while scraping, our resulting forget dataset contains slightly over 18k samples (Cyber Forget)."}, {"title": "D.2 Train-time Settings and Adversaries", "content": "Weaponization knowledge restriction. For each weaponization knowledge restriction domain, we have a corresponding retain dataset $D_{retain}$, comprised of a mix of data from the Pile-bio Retain set and Magpie-Align instruction-tuning dataset [56], which we refer to as the Retain-set. Referencing the datasets described in Appendix D.1, we specify the adversary tamper-resistance datasets that constitute the data used by attacks sampled from $A_{train}$ during TAR training, as well as the attack setup for each domain as follows:\n\u2022 Biosecurity: We simulate six adversaries from the following three setups: Pile-Bio Forget-set SFT, Camel-Bio Forget-set SFT, and Retain-set SFT followed by Pile-Bio Forget-set SFT (R\u2192F), where the switching point between Retain-set and Pile-Bio Forget-set SFT within the 64-step length trajectory is sampled from a beta distribution \u03b2(6.0, 3.0). LRs are sampled from {2 \u00d7 10\u22125,4 \u00d7 10-5}.\n\u2022 Chemical Security: We simulate nine adversaries from the following three setups: Chem Forget-set SFT, Retain-set SFT, and Retain-set followed by Chem Forget-set SFT, using the same switching-point sampling scheme as in the Biosecurity setting. LRs are sampled from {2 \u00d7 10-6,2 \u00d7 10-5,4 \u00d7 10-5}.\n\u2022 Cybersecurity: We simulate four adversaries from the following two setups: Cyber Forget-set SFT, and Retain-set SFT followed by Cyber Forget-set SFT, using the same switching point sampling scheme as in the Chemical Security and Biosecurity settings. LRs are sampled from {2 \u00d7 10\u22125,4 \u00d7 10-5}."}]}