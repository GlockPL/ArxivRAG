{"title": "Learning to Summarize from LLM-generated Feedback", "authors": ["Hwanjun Song", "Taewon Yun", "Yuho Lee", "Jihwan Oh", "Gihun Lee", "Jason Cai", "Hang Su"], "abstract": "Developing effective text summarizers remains a challenge due to issues like hallucinations, key information omissions, and verbosity in LLM-generated summaries. This work explores using LLM-generated feedback to improve summary quality by aligning the summaries with human preferences for faithfulness, completeness, and conciseness. We introduce FeedSum, a large-scale dataset containing multi-dimensional LLM feedback on summaries of varying quality across diverse domains. Our experiments show how feedback quality, dimensionality, and granularity influence preference learning, revealing that high-quality, multi-dimensional, fine-grained feedback significantly improves summary generation. We also compare two methods for using this feedback: supervised fine-tuning and direct preference optimization. Finally, we introduce SummLlama3-8b, a model that outperforms the nearly 10x larger Llama3-70b-instruct in generating human-preferred summaries, demonstrating that smaller models can achieve superior performance with appropriate training. The full dataset will be released soon. The SummLlama3-8B model is now available at https://huggingface.co/DISLab/SummLlama3-8B.", "sections": [{"title": "1 Introduction", "content": "Developing an effective text summarizer has long been a challenge, as summaries generated by language models often fall short of human standards (Maynez et al., 2020; El-Kassas et al., 2021; Roit et al., 2023). While large language models (LLMs) have greatly improved the coherence and fluency of summaries (Liu et al., 2023a), persistent issues remain, such as unfaithful statements (hallucinations), omission of key information (low completeness), and verbosity (low conciseness) in the summaries (Lee et al., 2024; Song et al., 2024).\nAn ideal approach would involve providing expert-level summary examples for language models to imitate. However, creating such high-quality summaries is both labor-intensive and difficult to scale effectively. A better alternative is to leverage feedback on the summaries through reinforcement learning from human feedback (RLHF) (Stiennon et al., 2020; Rafailov et al., 2023). RLHF avoids the need to write an ideal summary by having users select their preferred response from candidate summaries of the same document. Yet, human involvement still poses scalability challenges, particularly when acquiring high-quality, fine-grained human feedback across multiple dimensions, such as faithfulness and completeness. For example, Lee et al. (2024) reports that the cost of obtaining fine-grained human feedback for these three dimensions exceeded $30K for just 2,025 summaries.\nIn this work, we address this challenge by utilizing LLM-generated feedback, known as RL from AI feedback (RLAIF) (Lee et al., 2023; Dutta et al., 2024), with a special focus on text summarization. Our approach shifts focus to the relatively unexplored area of leveraging LLM-generated feedback to enhance summary quality, whereas most existing research in summarization has primarily concentrated on using LLMs to evaluate summaries. (Wan et al., 2024; Tang et al., 2024a; Song et al., 2024). Specifically, our goal is to produce human-preferred summaries by exploiting LLM feedback with respect to the three core dimensions, namely faithfulness, ensuring summaries are consistent with original documents; completeness, encompassing all key-facts; and conciseness, maintaining a succinct and focused summary. We focus on these three dimensions, as LLMs already excel in other"}, {"title": "2 Related Work", "content": "Preference Optimization. Preference optimization plays a crucial role in bridging the gap between human intent and the outputs generated by LLMs (Yang et al., 2023; Jiang et al., 2024b; Rafailov et al., 2023). The predominant methods are PPO (Schulman et al., 2017), which fits a reward model to optimize LLMs to generate responses that receive high reward, and DPO (Rafailov et al., 2023), which directly optimizes the LLMs' outputs based on preference data without relying on an explicit reward model. These methods have demonstrated effectiveness in aligning LLMs with human preferences, particularly in mitigating hallucination, harmful outputs, and biased contents (Tonmoy et al., 2024; Bai et al., 2024; Allam, 2024; Li et al., 2024). Despite the success of preference optimization in other domains, in text summarization, limited work has focused on aligning outputs with human preferences. Stiennon et al. (2020) collected a comparison dataset to train a PPO reward model, using Reddit posts and coarse evaluations of two summaries, without accounting for multi-dimensional aspects of summarization. Recently, Mishra et al. (2024) applied DPO for summariza-"}, {"title": "3 Data and Experiment Details", "content": "High-level Overview. We overview the overall pipeline from data creation to preference learning, as in Figure 1, following three distinct steps:\nStep 1. Input sourcing and summary generation: Input documents are extracted from 7 diverse sources, varying in domain, length, and type. Summaries are then generated using 13 non-LLMs, open-source LLMs, and proprietary LLMs, producing a wide range of summary quality.\nStep 2. Feedback generation using LLMs: Feedback is generated through LLM-based summary evaluations using four configurations, adjusting the quality (low vs. high), dimensionality (single vs. multi-dimensional), and granularity (coarse- vs. fine-grained) of the feedback.\nStep 3. Learning with large-scale feedback: We examine the potential of machine feedback from LLMs through the lens of preference learning."}, {"title": "3.1 FeedSum: Data with LLM Feedback", "content": "Table 1 contrasts our FeedSum dataset with two existing datasets with either human-annotated or LLM-generated feedback. FeedSum features input text that is as diverse as UniSumEval (Lee et al., 2024), while simultaneously providing a significantly larger amount of LLM-generated feedback compared to SynFacEdit (Mishra et al., 2024).\nInput Text Sourcing The diversity of source documents is crucial for acquiring comprehensive feedback, as it helps identify weaknesses in modern text summarizers across various aspects such as input domain, length, and type (dialogue vs. non-dialogue) (Lee et al., 2024). Hence, we extract input documents from multiple source datasets encompassing 7 different domains, ranging from short to lengthy input texts, and covering both non-dialogue and dialogue formats. We sample 2,000 input texts from the training set of each source dataset, including four non-dialogue datasets CNN/DM (news) (Nallapati et al., 2016), Wikihow (lifestyle) (Koupaee and Wang, 2018), GovReport (report) (Huang et al., 2021), PubMed (medical literature) (Cohan et al., 2018) \u2013 and three dialogue datasets - DialogSum (dailylife) (Chen et al., 2021), MediaSum (interview) (Zhu et al., 2021), Meeting-Bank (meeting) (Hu et al., 2023). As a result, a total of 14K input documents are sampled.\nSummaries with Varying Quality. The performance of summarization can vary depending on the summarizer chosen, as there is no single model that consistently produces the best quality summary (Song et al., 2024). From the perspective of prefer-"}, {"title": "3.2 LLM-generated Feedback", "content": "We generate feedback by conducting automated evaluations using LLMs. Yet, the effectiveness of LLM feedback varies the evaluation configuration employed. This study addresses open questions about leveraging LLM-based feedback for preference learning, with a focus on its \"quality,\" \"dimensionality,\" and \"granularity.\" These aspects are investigated by contrasting the effectiveness of feedback generated from four distinct configurations (C1-C4) in Table 2, as summarized below:\n\u2022 Feedback Quality (C1 vs. C2): The quality of generated feedback plays a pivotal role in preference learning. To assess the importance of feedback quality, we adjust the capacity of the selected LLMs for feedback generation. We use two open-source LLMs of different sizes: Llama3-8b-instruct for low-quality feedback and Llama3-70b-instruct for high-quality feedback, respectively.\n\u2022 Feedback Dimensionality (C2 vs. C3): The simplest way to gather feedback is to assess the quality of the summary with a single score on a 1\u20135 Likert"}, {"title": "3.3 Learning with Large-scale Feedback", "content": "We primarily focus on three key questions:\nQ1: How do the quality, dimensionality, and granularity of LLM-generated feedback (C1-C4 in Table 2) influence preference learning?\nQ2: What impact does each dimension have in the case of multi-dimensional feedback?\nQ3: How much can DPO enhance the quality of summaries compared to SFT variants?\nThe experimental setups are detailed below and see Appendix B for the detailed training configuration and input-output format for SFT and DPO.\nQ1: Impact of Feedback Configuration. In this experiment, we consistently train Llama3-8b-instruct using DPO (Rafailov et al., 2023), but with feedback generated by differently configured LLMs (C1-C4 in Table 2). Specifically, for each configuration, we create a set of paired summaries \u2013 one chosen and one rejected \u2013 using the same"}, {"title": "4 Analysis of LLM-generated Feedback", "content": "The configurations C1-C4 for LLM-based feedback generation show significant differences in feedback quality and their impact on constructing chosen-rejected summary pairs."}, {"title": "4.1 Quality of LLM Feedback", "content": "Table 3 presents the quality of LLM feedback by calculating summary- and system-levels agreement with human scores of UniSumEval (see Appendix C.2 for the detailed metric). The feedback in C1 shows a significantly lower correlation with human scores compared to C2-C4, which is attributed to the use of a smaller LLM. The quality of feedback improves progressively as it becomes more multi-dimensional and fine-grained from C2 to C4."}, {"title": "4.2 Distribution of Summary Score", "content": "Figure 2 illustrates the distribution of composite summary scores obtained from the four LLM-based"}, {"title": "4.3 Chosen and Rejected Summary", "content": "Table 4 summarizes the proportion of summaries selected as chosen or rejected ones across the three summarizer categories. A significant difference in proportion is observed depending on the feedback configuration. Feedback from single-dimensional or coarse-grained automatic evaluations in C1-C3 predominantly classifies LLM-generated summaries as chosen in 89.3%-94.9% of cases, while non-LLM-generated summaries are rejected in 60.7%-81.2% of cases. However, feedback from the multi-dimensional and fine-grained"}, {"title": "5 Main Experiment", "content": "Test Set. The test set is constructed by randomly sampling 200 documents from the test split of Feed-Sum's seven source datasets, totaling 1.4K. Both automated and human evaluations are conducted to evaluate summarizers on this set. For automated evaluation, we use Llama3-80b-instruct as the backbone of FineSurE (Song et al., 2024). For human evaluation, we perform a fact verification task for faithfulness and a key-fact alignment task for completeness and conciseness, following the work (Lee et al., 2024). Three annotators are assigned for each task, recruited through Amazon Mechanical Turk. Details of the automated and human evaluation can be found in Appendix C.3.\nEvaluation Metric. We report the quality of text summary from the three key perspectives: faithfulness, the proportion of faithful summary sentences; completeness, the proportion of covered key-facts; and conciseness, the proportion of summary sentences aligning with the key-facts. In automated evaluation, key-facts are automatically extracted from the reference summary of each source dataset, as suggested by Song et al. (2024). In human evaluation, we use human-annotated key-facts from the UniSumEval dataset (Lee et al., 2024) to follow the same manual evaluation pipeline in the paper. See Appendix C.1 for the equation to calculate three percentage scores. Appendix D presents the results using ROUGE and BERTScore for reference."}, {"title": "5.1 Q1: Impact of Feedback Configuration", "content": "We evaluate the quality of summaries generated by Llama3, trained with DPO incorporating feedback from four different configurations, and compare the results to the corresponding models without DPO. The prompts used to generate summaries are identical across all setups, as in Appendix E. The results for other LLMs are in Appendix F."}, {"title": "5.1.1 Results by Automated Evaluation", "content": "Table 2 presents the summary quality of seven summarizers: two models without preference optimization and five models with DPO using LLM feedback generated in different configurations.\nThe low-quality feedback (C1) generated by Llama3-8b-instruct proves ineffective. Compared to not using DPO, the faithfulness score rather drops by 0.028, resulting in the lowest composite score (\"Avg.\") across the three dimensions. The quality of feedback is crucial for preference learning using LLM feedback. High-quality feedback (C2-C3) via coarse-grained evaluation improves the performance of summarizers in most cases. However, there is no improvement in considering multi-dimensional aspects in automated evaluation as long as the granularity remains coarse. This is likely due to coarse-grained feedback lacking diversity in feedback pairs, indiscriminately selecting LLM-generated summaries as chosen and non-LLM summaries as rejected, as shown in Fig-"}, {"title": "5.1.2 Results by Human Evaluation.", "content": "Table 6 presents the results of the human evaluation across three dimensions. The overall performance dominance aligns with the automated evaluation results presented in Table 5. The DPO-C4, which is Llama3-8b-instruct fine-tuned with DPO using feedback from C4, significantly outperforms DPO-{C1, C2, C3}, and even surpasses the larger Llama3-70b-instruct. Therefore, the results from both automated and human evaluations confirm that a smaller model can outperform its larger counterpart with appropriate training."}, {"title": "5.2 Q2: Impact of Feedback Dimension", "content": "Table 7 shows the summary quality of summarizers trained with DPO on a single feedback dimension from C4, along with the model obtained through post-hoc parameter merging (Jang et al., 2023).\nCompared to the original Llama3-8b-instruct, DPO-{faith, comp, cons}, which rely on a single dimension, achieve the best scores in their target dimensions, but they are likely to show limited improvements or even performance declines in other dimensions. Specifically, focusing on completeness lowers faithfulness and conciseness, while prioritizing conciseness reduces completeness. Additionally, their parameter merging,"}, {"title": "5.3 Q3: Comparison of DPO over SFT", "content": "Table 8 compares the summary quality of five SFT variants fine-tuned with reference summaries selected based on different policy, alongside Llama3-8b-instruct before and after DPO. Here, we introduce an additional dimension, \"abstractiveness (Abs.),\" which refers to the extent to which a summary generates novel sentences or phrases, leading to a more coherent summary (Zhang et al., 2022; Song et al., 2023). The quality of generated summaries vary depending on the SFT policy.\nFirstly, DPO is a much superior approach to the SFT variants. DPO-C4 significantly improves summary quality across multiple dimensions when compared to the vanilla Llama3-8b-instruct, while SFT-best falls short in delivering similar improvements, despite being fine-tuned with only the best-selected summaries during the training phase. Secondly, all SFT variants show a notable decline in abstractiveness, incurring the copy bias and leading to less coherent summaries due to sentence copying from the input document (Song et al., 2023). This is because SFT allows only one reference summary per document, whereas DPO is superior by presenting multiple possible summaries through chosen-rejected pairs. Thirdly, focusing on a single dimension in SFT may improve that aspect but is likely to worsen others. SFT-faith and SFT-cons improve their target dimensions compared to Llama3-8b-instruct but both sacrifice completeness. Thus, SFT-best, which equally considers all dimensions, achieves the highest average score (\"Avg.\") among the SFT variants."}, {"title": "5.4 Additional Experiment", "content": "Feedback Size. We explore the impact of varying the size of high-quality, multi-dimensional, fine-grained feedback generated by LLMs in C4. The"}, {"title": "6 Conclusion", "content": "This work presents a framework for improving text summarization using LLM-generated feedback. We demonstrate that this approach is the most effective when the feedback is high-quality, multi-dimensional, and assessed at a fine-grained level. Our experiments show that DPO significantly outperforms SFT variants in utilizing such feedback. Additionally, we provide insights into the alignment trade-offs in summarization, the impact of"}, {"title": "Limitations", "content": "DPO is a widely used approach for preference optimization; however, it has limitations in handling multi-dimensional feedback. A typical method involves computing a composite score by averaging the scores across all dimensions with equal weights, which may not be the optimal solution for multi-dimensional preference learning. Although we include a baseline of post-hoc parameter merging (Jang et al., 2023), recent work suggests there are better performance alternatives, such as Controllable DPO (Guo et al., 2024) and Sequential Alignment (Lou et al., 2024). We will explore the extent of performance improvement achieved by these solutions in future work.\nWe conducted both human and automated evaluations. However, the majority of the evaluations were automated due to the high cost associated with fine-grained, multi-dimensional manual assessments. Nevertheless, we believe that the automated evaluations provide convincing evidence, as they have demonstrated performance comparable to human evaluations (Song et al., 2024; Tang et al., 2024a; Liu et al., 2023a)."}, {"title": "Ethics Statement", "content": "Our work primarily focuses on leveraging LLM-generated feedback on diverse text summaries, which does not pose any ethical concerns during the model training phase. For human evaluation, we followed a well-defined evaluation protocol in the literature, preventing possible ethical issues in the annotation process. Annotators were paid 50% more than the average U.S. minimum wage and received bonuses for maintaining consistent, high-quality performance."}, {"title": "Scientific Artifacts", "content": "The summaries used to collect LLM feedback were generated by 13 different language models. For open-source models, we used publicly available checkpoints from Huggingface, while for proprietary models, we utilized paid API services provided by OpenAI and AWS Bedrock. See Table 11 for details in Appendix."}, {"title": "A Data Creation Details", "content": "A.1 Feedback Generation\nWe generate LLM-based feedback across four different setups, as summarized in Table 2:\n\u2022 C1: This setup is designed to acquire low-quality, coarse-grained, single-dimensional feedback. We perform automated evaluation using the prompt in Table 12 with Llama3-8-instruct, a lower-performing model compared to its larger counterpart, Llama3-70b-instruct. The feedback obtained is a Likert-scale overall score for the summary.\n\u2022 C2: The prompt for this setup is identical to that of C1, but we use a nearly 10\u00d7 larger LLM, Llama3-70b-instruct, to generate high-quality, single-dimensional, and coarse-grained feedback.\n\u2022 C3: We use G-Eval (Liu et al., 2023a) with simple modification to tune it for our three key"}, {"title": "A.2 Key-fact Extraction", "content": "The feedback from C4 requires fine-grained evaluation using key-facts to assess the completeness and conciseness scores. The key-facts are automatically extracted from the reference (human) summary of each source dataset, as suggested by Song et al. (2024). Thus, we obtain the list of key-facts for 15.4K documents in FeedSum: 14K for training set and the remaining 1.4K for testing set. The prompt used for automated key-fact extraction is detailed in Table 13."}, {"title": "A.3 Dataset Statistic", "content": "We present a comprehensive statistical analysis of the FeedSum datasets, which consist of 125,388 <document, summary, feedback> triplets for each configuration outlined in Table 2. Detailed statistics of FeedSum are provided in Table 14."}, {"title": "B Training Detail", "content": "B.1 Training Configuration\nFor preference learning, we investigate two possible solutions of supervised fine-tuning (SFT) and direct preference optimization (DPO). The details of each configuration are detailed below:\nSupervised Fine-tuning (SFT). We fine-tune Llama3-8b-instruct using QLoRA (Dettmers et al., 2024) and DeepSpeed (Stage-2) (Rasley et al., 2020) on four NVIDIA H100 GPUs. The model is trained for 3,000 steps with AdamW as the optimizer, using a batch size of 32, an initial learning rate of 1e-4, and a weight decay of 0.05. Regardless of how to select the reference summary, we apply the same configuration for all SFT strategies, namely SFT-{human, best, faith, comp, conc} in Table 8. The input (user prompt) and output (assistant prompt) for Llama3-8b-instruct are configured similarly to the example for DPO in Table 19. The difference is that SFT only passes the input along with a single output, selected based on a predefined criterion, e.g., the summary with the highest composite score.\nDirect Preference Optimization (DPO). We train Llama3-8b/70b-instruct using DPO (Rafailov et al., 2023). Since the model has completed the instruction-tuning process, we proceed directly to optimize it using DPO. Like SFT, we apply QLoRA and DeepSpeed (Stage-2) to train the model on four"}, {"title": "B.2 Input and Output Format", "content": "Tables 19 presents an example of the input and its corresponding chosen and rejected outputs to train Llama3-8b-instruct using DPO. We follow the same prompt style of Llama3 for instruction tuning. In this example, the chosen summary was generated by GPT-4-turbo, achieving scores of 100% for faithfulness, 60% for completeness, and 100% for conciseness. On the other hand, the rejected summary was generated by Mistral-7b-instruct, achieving scores of 25% for faithfulness, 60% for completeness, and 50% for conciseness.\nThe auto-evaluation results of the example by FineSurE (Song et al., 2024) are provided in Table 20. The final percentage (%) scores can be computed by calculating the proportion of factually correct sentences for faithfulness, that of included given key-facts for completeness, and that of summary sentences related to the key-facts. The detailed equation is provided in Appendix C.1. For SFT, the input and its corresponding response are almost similar to those of DPO. But,"}, {"title": "C Automated and Human Evaluation", "content": "C.1 Metric for Summary Quality\nWe utilize three dimensions of metrics, namely faithfulness, completeness, and conciseness, along with one that estimates the abstractiveness of the summary, in line with recent literature (Song et al., 2024; Lee et al., 2024).\nFaithfulness Score. Faithfulness score is formulated by aggregating sentence-level fact check results. Let S = {s_1,..., s_N} is the summary passage which consists of N sentences, where s_i denotes the i-th sentence in the summary passage. Let S_{fact} \\subset S represent the subset of sentences verified as \"factually correct.\" The faithfulness percentage score of S, with respect to the document D, is defined as:\n Faithful(D, S) = \\frac{|S_{fact}|}{|S|}. (1)\nThis metric measures the proportion of factually correct sentences in the summary relative to the total number of sentences in the summary.\nCompleteness and Conciseness Score. Let K = {k_1,..., k_M} be the collection of key-facts, where M indicates the total number of these facts. Utilizing the results from the alignment of key-facts, we can establish a bipartite graph M = (K, S, E), with set of edges E = {(k,s) : k \\rightarrow s|k \\in K \\wedge s \\in S}. Here, the notation k \\rightarrow s signifies that the key-fact k is identified as being included in the summary sentence s. The completeness and conciseness scores for summary S are computed as percentage scores, defined as follows:\nComplete(K, S) = \\frac{|\\{k|(k, s) \\in E\\}|}{|K|}, (2)\nConcise(K, S) = \\frac{|\\{s | (k, s) \\in E\\}|}{|S|}. (3)\nIn this context, the operator |\u00b7| denotes the cardinality of a set. Completeness score indicates how well the key-facts are incorporated into the summary. Furthermore, the conciseness score evaluates how effectively the summary condenses and includes the key-facts.\nComposite Score. To determine the chosen and rejected summaries in cases of multi-dimensional feedback, we use the average of the three percentage scores \u2013 faithfulness, completeness, and conciseness \u2013 to calculate a composite score.\nAbstractiveness Score. In Section 5.3, we additionally report the abstractiveness score of the summary, which refers to the extent to which a summary generates novel sentences or phrases, leading to a more coherent summary. The abstractiveness score is measured by calculating the ratio of novel n-grams present in the summary that does not appear in the original input text (Liu and Lapata, 2019; Song et al., 2023). Let n-gram_{shared} represent the set of n-grams that are shared between the summary and the document, while n-gram_{summary} denotes the total set of n-grams included in the summary. Then, the ratio of novel n-grams N_n is defined as:\nN_n = 1 - \\frac{|n-gram_{shared}|}{|n-gram_{summary}|}. (4)\nThe final abstractiveness score for a summary S is computed as the average of the novel 1/3/5-gram ratios, as follows:\nAbstractive(D, S) = \\frac{(N_1 + N_3 + N_5)}{3}. (5)"}, {"title": "C.2 Metric for Feedback Quality", "content": "In table 3, we use the same settings as in recent studies (Liu et al. 2023b, Song et al. 2024) to evaluate the summary feedback quality and align it with human judgment. Specifically, there are two levels for evaluating the alignment (correlation) of generated summary feedback with human feedback. The greater the alignment, the higher the quality of the generated feedback.\nSummary-level Correlation. We can check the alignment between the generated and human feedback at the summary level. To calculate the summary-level correlation, we define F_{actual} and F_{pred} as the percentage scores of the ground truth and the predicted summaries, respectively. Let D = {D_1, . . ., D_k} represent the set of input documents, and S = {S_1,..., S_k} represent the corresponding summaries for these documents. The summary-level correlation is computed as:\n Spearman([F_{actual}(D_1, S_1), ..., F_{actual}(D_k, S_k)], [F_{pred}(D_1, S_1),..., F_{pred}(D_k, S_k)]). (6)"}, {"title": "F Results with Gemma-2b-instruct", "content": "We conduct an additional experiment to evaluate the improvements from preference learning with LLM-generated feedback, using a different LLM. Specifically, we choose Gemma-2b-instruct, as this smaller model demonstrates the impact of our framework, even when compared to significantly larger models like Llama-8/70b-instruct.\nTable 16 presents the automated evaluation results across three fine-grained dimensions of summary quality, comparing their percentage scores before and after applying DPO with feedback from"}, {"title": "G Summary Example", "content": "Table 21 presents examples of summaries generated by six different approaches: Llama3-8b/70b-instruct without DPO, and four variants of Llama3-8b-instruct after applying DPO.\nThe summary of DPO-C4 can be considered the best for the following reasons:\n\u2022 Core Focus: The summary accurately captures the main theme of the conversation, which revolves around the Thanksgiving dinner arrangements. It highlights how the two people confirm plans, discuss what to bring, and finalize the decision for Person2 to bring wine instead of pie. This maintains the core context.\n\u2022 Inclusion of Key-facts: The summary covers the important details of the conversation, including Person2's initial offer to bring dessert (pumpkin pie) and the shift to bringing wine due to another family member handling dessert. Other summaries tend to overlook or simplify this progression, while DPO-C4 fully captures the interaction's key events.\n\u2022 Clarity and Conciseness: The summary is structured in a straightforward, concise manner, effectively summarizing the conversation without un-"}]}