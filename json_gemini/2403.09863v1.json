{"title": "A CONCEPTUAL FRAMEWORK FOR WHITE BOX NEURAL NETWORKS", "authors": ["Maciej Satkiewicz"], "abstract": "This paper introduces semantic features as a general conceptual framework for fully explainable neural network layers. A well-motivated proof of concept model for relevant subproblem of MNIST consists of 4 such layers with the total of ~4.8K learnable parameters. The model is easily interpretable, achieves human-level adversarial test accuracy with no form of adversarial training, requires little hyperparameter tuning and can be quickly trained on a single CPU. The general nature of the technique bears promise for a paradigm shift towards radically democratised and truly generalizable white box neural networks. The code is available at https://github.com/314-Foundation/\nwhite-box-nn.", "sections": [{"title": "1 Introduction", "content": "Current state of the art (SOTA) neural networks are large, unwieldy black boxes offering little insight into their\ndecision-making process, require prohibitive amounts of resources and are prone to adversarial attacks not seen during\ntraining [10, 3]. The latter implies that models rely on unintended features shared between train and test, giving false\nsense of generalisability [4, 6]. These issues drastically limit the usability of current generation of neural networks,\nespecially in critical domains such as healthcare, cybersecurity or autonomous systems. Yet animal brains apparently\nlearn rich and robust patterns in mostly unsupervised sensory input - even brains as simple as drosophila brain with\n~135k neurons. This discrepancy indicates a need for radically simplified AI.\nThis paper aims to lay foundation for a new paradigm for training models by introducing a conceptual framework for\nbuilding white box neural networks. The emphasis is put on theoretical and qualitative aspects of proposed solutions\nand less on tweaking quantitative metrics. Following this spirit it's important to train a proof of concept (POC) network\non Minimum Viable Dataset (MVD) - problem easy enough to get rid of all the obfuscating details but still hard enough\nto be relevant. MNIST dataset is falsely considered to be \"solved\" or \"too easy\" while SOTA models are hardly adequate\nin terms of their complexity, lack interpretability and are vulnerable to adversarial perturbations not seen during training\n[9]. This is true not only for the entire 10-digit dataset but even for certain pairs of digits [11] and therefore the MVD\nfor this paper is chosen to be the binary subset of MNIST consisting of images of \"3\" and \"5\" (one of the most often\nconfused pairs), which actually might be even harder than the entire MNIST in terms of average adversarial accuracy.\nThis is not to say that similar results cannot be obtained on more complex datasets - preliminary experiments suggest\nthat it is exactly the opposite - but in fact one of the goals of such selection of MVD is to make a clear and concise\nargument for the effectiveness and potential of presented method.\nThe structure of the paper is as follows:\n\u2022 Section 2. gives a rigorous definition of semantic feature and argues for the generality of the notion;\n\u2022 Section 3. builds a carefully motivated PoC 4-layer white box neural network for selected MVD;\n\u2022 Section 4. analyses the trained model both qualitatively and quantitatively in terms of explainability, reliability\nand adversarial robustness.\nThe paper ends with ideas for further research and acknowledgments."}, {"title": "2 Semantic features", "content": "In machine learning inputs are represented as tensors. Yet the standard Euclidean topology in tensor space usually fails\nto account for small domain-specific variations of features. For example shifting an image by 1 pixel is semantically\negligible but usually results in a distant output in L2 metric.\nThis observation inspires the following definition:\nDefinition: A semantic feature $f_{PL}$ of dimension $s \\in N$ is a tuple $(f, P, L)$ where:\n\u2022 f - base, $f \\in R^s$,\n\u2022 P - parameter set, $P \\subset R^r$ for somer $\\in N$\n\u2022 L - locality function : $P \\rightarrow Aut(R^s, R^s)$ where Aut is the family of differentiable automorphisms of $R^s$\nAdditionally the feature $(f, P, L)$ is called differentiable iff L is differentiable (on some neighbourhood of P).\nIntuitively a semantic feature consists of a base vector f and a set of it's \"small\" variations $f_p = L(p)(f)$ for all p \u2208 P;\nthese variations may be called poses after [8]. Thus $(f, P, L)$ is locality-sensitive in the adequate topology capturing\nsemantics of the domain. The differentiability of L(p) and of L itself allow for gradient updates of f and P respectively.\nThe intention is to learn both f and P while defining L explicitly as an inductive bias\u00b9 for the given modality."}, {"title": "2.1 Examples of semantic features", "content": "To better understand the definition consider the following examples of semantic features:\n1. real-valued $f_{PL}$\n\u2022 $f \\in R$\n\u2022 P - a real interval $[p_{min}, p_{max}]$ for $p_{min} <= 0 <= p_{max}$\n\u2022 L - function that maps a real number p to the function $g\\rightarrow g + p$ for $g\\in R$\n2. convolutional $f_{PL}$\n\u2022 f - square 2D convolutional kernel of size $(c_{out}, c_{in}, k, k)$\n\u2022 P-set of 2 \u00d7 2 rotation matrices\n\u2022 L - function that maps a rotation matrix to the respective rotation of a 2D kernel\u00b2\n3. affine $f_{PL}$\n\u2022 f - vector representing a small 2D spatial feature (e.g. image patch containing a short line)\n\u2022 P - set of 2 \u00d7 3 invertible augmented matrices\u00b3\n\u2022 L - function that maps a 2 \u00d7 3 matrix to the respective 2D affine transformation of images2\n4. xor $f_{PL}$\n\u2022 f - single-entry vector of dimension n \u2208 N\n\u2022 P $C \\{0,1,..., n \u2212 1\\}$\n\u2022 L - function that maps a natural number p to the function that rolls the non-zero element of g by p\ncoordinates to the right, where g is a single-entry vector of dimension n\n5. logical $f_{PL}$\n\u2022 f - concatenation of k single-entry vectors and some dense vectors in-between\n\u2022 P $C \\{0, 1, ..., n_0 \u2212 1\\} x ... \u00d7 \\{0, 1, . . ., n_{k\u22121} \u2212 1\\}$\n\u2022 L - function that maps p \u2208 P to the respective rolls of single-entry vectors (leaving the dense vectors\nintact)"}, {"title": "2.2 Matching semantic features", "content": "The role of semantic features is to be matched against the input to uncover the general structure of the data. Suppose\nthat we have already defined a function match: $(R^s, R^s) \\rightarrow R$ that measures an extent to which datapoint d \u2208 $R^s$\ncontains a feature g \u2208 $R^s$. Now let's define\n$SFmatch(d, (f, P, L)) = \\max_{p\\in P} match(d, L(p)(f))$\n(1)\nIf L captures domain topology adequately then we may think of matching $f_{PL}$ as a form of local inhibition along\n$L_p(f) = \\{L(p)(f), p \\in P\\}$. Therefore semantic feature defines a XOR gate over $L_p(f)$. On the other hand every\n$f_p \\in L_p (f)$ can be viewed as a conjunction (AND gate) of its non-zero coordinates. Therefore semantic features turn\nout to be a natural way of expressing logical claims about the classical world - in the language that can be \"understood\"\nby neural networks. This is especially apparent in the context of logical semantic features which allow to explicitly\nexpress logical sentences, e.g. sentence $(A V B) > \u00abC > \u00abD$ can be expressed by a logical $f_{PL}$ where:\n\u2022 f = [1, 0, -1, -1]\n\u2022 P = {0,1}\n\u2022 L(0)(f) = [1, 0, -1, -1] and L(1)(f) = [0, 1, -1, -1]\nSemantic features capture the core characteristic of any semantic entity - having many possible states but being\nat exactly one state at a time. As the remainder of the paper will show this turns out to be a sufficiently strong\nregularization - backpropagation through appropriately chosen L results in easily interpretable f and P.\nIt remains to characterise the match function. Usually it can be defined as the scalar product $d \\cdot emb(g)$ where\nemb: $R^s \\rightarrow R^e$ is some natural embedding of the feature space to the input space. In particular for our previous\nexamples lets set the following:\n\u2022 for real-valued $f_{PL}$: match(d, g) = int(d == g) where d, g \u2208R\n\u2022 for convolutional $f_{PL}$: match(d, g) = $max_{0<=i<c_{out}}(d, g^i)$ where g is a 2D convolutional kernel of\nshape ($c_{out}, c_{in}, k, k), g^i$ is the i-th convolutional filter of g, d is an image patch of shape ($c_{in}, k, k)$\n\u2022 for affine $f_{PL}$: match(d, g) = $d\\cdot \\|g\\|_2$ where g is a 2D image of shape ($c_{in}, k, k) and d is a 2D image patch\nof the same shape\n\u2022 for logical $f_{PL}$: match(d, g) = d \u00b7 g for g, d \u2208 $R^e$\nThe occasional $L_2$ normalization in the above examples is to avoid favouring norm-expanding L(p)."}, {"title": "3 Network architecture", "content": "In this section we will use semantic features to build a PoC white box neural network model that classifies MNIST\nimages of \"3\" and \"5\" in a transparent and adversarially robust way. We will build the network layer by layer arguing\ncarefully for every design choice. Note that the network construction is not an abstract process and must reflect the core\ncharacteristics of the chosen dataset. There is no free lunch, if we want to capture the semantics of the dataset we need\nto encode it in the architecture more or less explicitly. The framework of semantic features allows to do this in a pretty\nnatural way. Note that a single neural network layer will consist of many parallel semantic features of the same kind.\nThe network consists of the following 4 layers stacked sequentially. Note that for simplicity we don't add bias to any of\nthose layers. Te resulting model has around 4.8K parameters."}, {"title": "3.1 Two Step Layer", "content": "MNIST datapoint is a 28 \u00d7 28 grayscale image. Its basic building block is a pixel x \u2208 [0, 1]. Despite being allowed\nto assume any value between 0 and 1 it is actually (semantically) a ternary object - it's either ON, OFF or MEH6.\nTherefore the semantic space should squash the [0, 1] interval into those 3 values."}, {"title": "3.2 Convolutional Layer", "content": "A pixel-wise function is not enough to classify pixel as ON or OFF - the semantics of our MVD require a pixel to exist\nin a sufficiently bright region to be considered as ON. Therefore the next layer will consist of a single convolutional\nsemantic feature (f, P, L):\n\u2022 f - 2D kernel of shape (2, 1, 5, 5) (concatenation of (g\u00ba, g\u00b9), both of shape (1,1,5,5))\n\u2022 P - fixed set of k distinct rotations by a multiple of the $\\frac{360\u00b0}{k}$ angle, for k = 32\n\u2022 L as defined earlier\nThe layer performs SFmatch(d(x, y), fPL) with every pixel (x, y) of image d. This means that it matches the two\nrotated filters go and g\u00b9 with (x, y) and takes the maximum across all the matches7. Intuitively this layer checks if the\npixel has a sufficiently bright neighbourhood.\nThe layer is followed by ReLU activation. We initialize g\u00ba and g\u00b9 as the identity kernel of shape (1,1,5,5) plus\nGaussian noise ~ N(0,0.1)."}, {"title": "3.3 Affine Layer", "content": "The basic semantic building blocks of MNIST digits are fragments of various lines together with their rough locations\nin the image (same line at the top and the bottom often has different semantics). In short they are shapes at locations.\nThe semantic identity of shape is not affected by small affine transformations. The next shape-extracting layer will\ntherefore consist of 8 affine semantic features (f, P, L) defined as follows:\n\u2022 f - 2D image patch of shape (1, 20, 20)\n\u2022 P - set of 32 affine 2 \u00d7 3 augmented matrices\n\u2022 L as defined earlier\nFor simplicity every fPL in this layer is located in the center of 28 \u00d7 28 image, i.e. we zero-pad f on every side before\napplying the affine transformations. Intuitively this matches the image against a localized shape in a way that is robust\nto small affine perturbations of the shape.\nThe layer is followed by ReLU activation. We initialize affine matrices as 2 \u00d7 2 identity matrix plus Gaussian noise\n~ N(0,0.01) concatenated with 1 \u00d7 2 matrix filled with Gaussian noise ~ N(0, 1)."}, {"title": "3.4 Logical Layer", "content": "After Affine Layer has extracted predictive shapes it remains to encode logical claims such as \"number 3 is A or B and\nnot C and not D\" etc. Since the previous layer is learnable we can enforce a preferable structure on the input neurons to\nthe Logical Layer. This layer will consist of 2 logical fPL (one for every label) defined as follows:\n\u2022 f - vector of dimension 8\n\u2022 P = {0, 1, 2, 3}\n\u2022 Las defined earlier\nWe initialize $f^0 = [1.0700, 0.0000, 0.0000, 0.0000, -0.7800, -0.7100, -0.8000, -0.8100]$ and $f^1 = [-0.8100, -0.7600, -0.8600, -0.8000, 1.0500, 0.0000, 0.0000, 0.0000]$. The exact initial-\nization values are less important then their sign and relative magnitude, however they should not be too large as we will\ntrain the network using CrossEntropyLoss."}, {"title": "4 Results", "content": "The model was trained to optimize the CrossEntropyLoss for 15 epochs with Adam optimizer, learning rate 3e-3,\nweight decay 3e-6. The only augmentation was the Gaussian noise ~ N(0.05, 0.25) applied with 0.7 probability.\nThose parameters were chosen \"by hand\" after few rounds of experimentation. The results seem pretty robust to the\nselection of hyperparameters as long as they are within reasonable bounds. Training a single epoch on a single CPU\ntakes around 9s."}, {"title": "4.2 Quantitive results", "content": "We don't bother measuring the clean test metrics as it is a flawed approach that tells little about the true generalisation\ncapability of the model. Every metric reported here is computed under strong adversarial regime.\nThe model achieves ~92% accuracy under AutoAttack[2] (with default parameters: norm='inf', eps=0.3, step_size=0.1).\nThe metrics are almost identical for the much faster 40-step PGD Attack. Samples are usually misclassified with low\nconfidence and therefore we compute a reliability curve - a precision-recall curve where the positive class is defined as\nthe set of correctly classified samples (this means that the precision for 100% recall is exactly equal to the accuracy). It\nturns out that for 80% recall model achieves human-level 98% adversarial precision (Figure 1). This means that\nin practice human intervention would be required only for the easily detected 20% of adversarial samples to\nachieve human level reliability of the entire system (and most real-life samples are not adversarial)."}, {"title": "4.3 Layer inspection", "content": "Lets inspect the trained model layers."}, {"title": "4.3.1 Two Step Layer", "content": "Figure A show the initial and learned shapes of the Two Step function. Te function smoothly thresholds the input at\n0.365 and then at 0.556 which means that the interval [0, 0.365] is treated as OFF, [0.365, 0.556] as MEH and [0.556, 1]\nas ON. Bear in mind that this is not the final decision regarding the pixel state - it's just the input to the next layer."}, {"title": "4.3.2 Convolutional Layer", "content": "Quick inspection of the Figure 3 shows that the Convolutional Layer has indeed learned a meaningful filters that check\nif the central pixel lays inside an adequately structured region - intuitively the matching value corresponds to the degree\nthe model considers the pixel as being ON."}, {"title": "4.3.3 Affine Layer", "content": "Figure 4 shows the learned affine base features. Figure B in the Appendix B shows learned features in 32 learned\nfeature-specific poses. It's hard to imagine substantially more meaningful features."}, {"title": "4.3.4 Logical Layer", "content": "The two learned logical base features are as follows:\n\u2022 f\u00b0 = [1.0700, 0.0000, 0.0000, 0.0000, -0.7800, -0.7100, -0.8000, -0.8100]\n\u2022 f1 = [-0.8100, -0.7600, -0.8600, -0.8000, 1.0500, 0.0000, 0.0000, 0.0000]\nThis is exactly as expected - fP\u2081 defines number \"3\" as an entity having large SFmatch with any of the first 4 affine\nsemantic features and small SFmatch with all of the remaining 4 affine semantic features. For for its the other way\nround."}, {"title": "4.4 Decision boundary", "content": "Figure 5 shows the minimum perturbations required to change model's predictions under Boundary Attack[1] for\nsample test images. The added perturbations are easily spotted and understood by humans. Note that by definition\nmodel is extremely unconfident on those samples and in practice the perturbations required to fool the entire system\nwould have to be much larger.\nFigure 6 shows how the data is \"seen\" by the first 3 layers. The consecutive rows and lables denote respectively:\n1. adversarial input and predicted class probabilities\n2. output of Two Step Layer\n3. output of Convolutional Layer\n4. top pose of affine features predicting label \"3\" (with the match value)10\n5. top pose of affine features predicting label \"5\" (with the match value)10"}, {"title": "5 Significance and further research", "content": "The importance of explainability is hard to overestimate. An automatic system discovering meaningful predictive\npatterns in data could significantly offload the task of finding the underlying causal structure of observed phenomena.\nThis has a great potential to impact science, diagnostics, big data and many other fields. Combined with the demonstrated\nlevel of efficiency the resulting democratisation could open up exciting areas for growth worldwide, favouring diverse\nopen source solutions and providing considerable level of robustness against centralisation.\nThe general nature of semantic features makes it reasonable to expect that extending these results to more complex\ndatasets and domains should be relatively straightforward - but may require some honest engineering. The obvious\nideas for further work include:\n\u2022 use semantic features in self-supervised setting to obtain interpretable dimensionality reduction;\n\u2022 study more complex logical semantic features in more elaborate scenarios;\n\u2022 figure out a way to make logical semantic features differentiable, i.e. make P learnable;\n\u2022 design richer modes of spatial variation, e.g. hierarchical affine semantic features to capture variations of\ncompound objects (objects consisting of affine parts are themselves affine if you \"zoom out\" appropriately);\n\u2022 find adequate semantic topology for color space;\n\u2022 integrate affine features with sliding window approach, i.e. implement weight sharing of similar features at\ndistant locations;\n\u2022 define semantic features for sound (inspiration for semantic invariants could be drawn from music theory);\n\u2022 define semantic features for text, e.g. to capture semantic invariance of permutations of words;"}]}