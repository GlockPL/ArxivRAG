{"title": "CODEXGRAPH: Bridging Large Language Models\nand Code Repositories via Code Graph Databases", "authors": ["Xiangyan Liu", "Bo Lan", "Zhiyuan Hu", "Yang Liu", "Zhicheng Zhang", "Wenmeng Zhou", "Fei Wang", "Michael Shieh"], "abstract": "Large Language Models (LLMs) excel in stand-alone code tasks like Hu-\nmanEval and MBPP, but struggle with handling entire code repositories. This\nchallenge has prompted research on enhancing LLM-codebase interaction at a\nrepository scale. Current solutions rely on similarity-based retrieval or man-\nual tools and APIs, each with notable drawbacks. Similarity-based retrieval\noften has low recall in complex tasks, while manual tools and APIs are typ-\nically task-specific and require expert knowledge, reducing their generalizabil-\nity across diverse code tasks and real-world applications. To mitigate these\nlimitations, we introduce CODEXGRAPH, a system that integrates LLM agents\nwith graph database interfaces extracted from code repositories. By leverag-\ning the structural properties of graph databases and the flexibility of the graph\nquery language, CODEXGRAPH enables the LLM agent to construct and ex-\necute queries, allowing for precise, code structure-aware context retrieval and\ncode navigation. We assess CODEXGRAPH using three benchmarks: Cross-\nCodeEval, SWE-bench, and EvoCodeBench. Additionally, we develop five real-\nworld coding applications. With a unified graph database schema, CODEX-\nGRAPH demonstrates competitive performance and potential in both academic\nand real-world environments, showcasing its versatility and efficacy in software\nengineering.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) excel in code tasks, impacting automated software engineering\nRepository-level tasks mimic software engineers'\nwork with large codebases. These tasks require models to handle intricate\ndependencies and comprehend project structure.\nCurrent LLMs struggle with long-context inputs, limiting their effectiveness with large codebases\nand lengthy sequences reasoning. Researchers have\nproposed methods to enhance LLMs by retrieving task-relevant code snippets and structures,\nimproving performance in complex software development. However, these approaches mainly rely on either similarity-based retrieval\nor manual tools and APIs. Similarity-based retrieval methods, common in Retrieval-Augmented Generation\n(RAG) systems often struggle with complex reasoning for query formulation\nand handling intricate code structures, leading to low\nrecall rates. Meanwhile, existing tool/API-based interfaces that connect codebases and LLMs are"}, {"title": "2 Related Work", "content": "Repository-level code tasks have garnered significant attention due to their alignment with real-world production environments. Unlike traditional standalone code-related tasks such as HumanEval and MBPP, which often fail to capture the complexities of real-world software\nengineering, repository-level tasks necessitate models to understand cross-file code structures and\nperform intricate reasoning. These sophisticated\ntasks can be broadly classified into two lines of work based on their inputs and outputs. The first\nline of work involves natural language to code repository tasks, exemplified by benchmarks like\nDevBench and SketchEval, where models generate an entire\ncode repository from scratch based on a natural language description of input requirements. State-\nof-the-art solutions in this area often employ multi-agent frameworks such as ChatDev and MetaGPT\nto handle the complex process of generating a complete\ncodebase. The second line of work, which our research focuses on, includes tasks that integrate\nboth a natural language description and a reference code repository, requiring models to perform\ntasks like repository-level code completion , automatic Github issue resolution, and repository-level code generation. To assess the versatility and effectiveness of\nour proposed system CODEXGRAPH, we evaluate it on three diverse and representative benchmarks\nincluding CrossCodeEval for code completion, SWE-bench for Github issue resolution, and EvoCodeBench for code generation."}, {"title": "2.2 Retrieval-Augmented Code Generation", "content": "Retrieval-Augmented Generation (RAG) systems primarily aim to retrieve relevant content from\nexternal knowledge bases to address a given question, thereby maintaining context efficiency while\nreducing hallucinations in private domains. For repository-\nlevel code tasks, which involve retrieving and manipulating code from repositories with complex\ndependencies, RAG systems\u2014referred to here as Retrieval-Augmented Code Generation (RACG)\nare utilized to fetch the necessary code snippets or code structures from the\nspecialized knowledge base of code repositories. Current RACG methodologies can be divided into\nthree main paradigms: the first paradigm involves similarity-based retrieval, which encompasses\nterm-based sparse retrievers and embedding-based dense retrievers, with advanced approaches integrating\nstructured information into the retrieval process. The second paradigm consists of manually designed code-specific tools or APIs that rely on\nexpert knowledge to create interfaces for LLMs to interact with code repositories for specific tasks\nThe third paradigm combines both\nsimilarity-based retrieval and code-specific tools or APIs, leveraging the reasoning\ncapabilities of LLMs to enhance context retrieval from code repositories. Apart from the three\nparadigms, Agentless preprocesses the code repository's structure and file skeleton,\nallowing the LLMs to interact with the source code. Our proposed framework, CODEXGRAPH,\naligns most closely with the second paradigm but distinguishes itself by discarding the need for\nexpert knowledge and task-specific designs. By using code graph databases as flexible and universal\ninterfaces, which also structurally store information to facilitate the code structure understanding\nof LLMS, CODEXGRAPH can navigate the code repositories and manage multiple repository-level\ncode tasks, providing a versatile and powerful solution for RACG."}, {"title": "3 CODEXGRAPH: Enable LLMs to Navigate the Code Repository", "content": "CodexGraph is a system that bridges code repositories and large language models (LLMs)\nthrough code graph database interfaces. It indexes input code repositories using static analysis,\nstoring code symbols and relationships as nodes and edges in a graph database according to a pre-\ndefined schema. When presented with a coding question, CODEXGRAPH leverages the LLM agent\nto generate graph queries, which are executed to retrieve relevant code fragments or code structures"}, {"title": "3.1 Build Code Graph Database from Repository Codebase", "content": "Schema. We abstract code repositories into code graphs where nodes represent symbols in the\nsource code, and edges represent relationships between these symbols. The schema defines the types\nof nodes and edges, directly determining how code graphs are stored in the graph database. Different\nprogramming languages typically require different schemas based on their characteristics. In our\nproject, we focus on Python and have empirically designed a schema tailored to its features, with\nnode types including MODULE, CLASS, METHOD,FUNCTION, FIELD, and GLOBAL_VARIABLE,\nand edge types including CONTAINS, INHERITS, HAS METHOD, HAS_FIELD, and USES.\nEach node type has corresponding attributes to represent its meta-information. For instance,\nMETHOD nodes have attributes such as name, file_path, class, code, and signature. For\nstorage efficiency, nodes with a code attribute do not store the code snippet directly in the graph\ndatabase but rather an index pointing to the corresponding code fragment. Figure 2 illustrates a\nsample code graph derived from our schema, and Appendix A.1 shows the details of the schema.\nPhase 1: Shallow indexing. The code graph database construction process consists of two phases,\nbeginning with the input of the code repository and schema. The first phase employs a shallow in-\ndexing method, inspired by Sourcetrail's static analysis process, to perform a single-pass scan of\nthe entire repository. During this scan, symbols and relationships are extracted from each Python\nfile, processed only once, and stored as nodes and edges in the graph database. Concurrently, meta-\ninformation for these elements is recorded. This approach ensures speed and efficiency, capturing\nall nodes and their meta-information in one pass. However, the shallow indexing phase has lim-\nitations due to its single-pass nature. Some important edges, particularly certain INHERITS and\nCONTAINS relationships, may be overlooked as they might require context from multiple files.\nPhase 2: Complete the edges. The second phase addresses the limitations of shallow indexing\nby focusing on cross-file relationships. We employ Depth-First Search (DFS) to traverse each code\nfile, using abstract syntax tree parsing to identify modules and classes. This approach is particularly"}, {"title": "3.2 Large Language Models Interaction with Code Graph Database", "content": "Code structure-aware search. CODEXGRAPH leverages the flexibility of graph query language\nto construct complex and composite search conditions. By combining this flexibility with the struc-\ntural properties of graph databases, the LLM agent can effectively navigate through various nodes\nand edges in the code graph. This capability allows for intricate queries such as: \u201cFind classes\nunder a certain module that contain a specific method", "Retrieve the module where a certain\nclass is defined, along with the functions it contains": "This approach enables code structure-aware\nsearches, providing a level of code retrieval that is difficult to achieve with similarity-based retrieval\nmethods or conventional code-specific tools and\nAPIs.\nWrite then translate. LLM agents are powered by LLMs and operate based on user-provided\nprompts to break down tasks, utilize tools, and perform reasoning. This design is effective for\nhandling specific, focused tasks, but when tasks are\ncomplex and multifaceted, LLM agents may underperform. This limitation has led to the develop-\nment of multi-agent systems, where multiple\nLLM agents independently handle parts of the task. Inspired by this approach, CODEXGRAPH\nimplements a streamlined \"write then translate\" strategy to optimize LLM-database interactions."}, {"title": "4 Experimental Setting", "content": "Benchmarks. We employ three diverse repository-level code benchmarks to evaluate CODEX-\nGRAPH: CrossCodeEval, SWE-bench, and EvoCodeBench\nCrossCodeEval is a multilingual scope cross-file completion dataset for Python,\nJava, TypeScript, and C#. SWE-bench evaluates a model's ability to solve GitHub issues with 2, 294\nIssue-Pull Request pairs from 12 Python repositories. EvoCodeBench is an evolutionary code gen-\neration benchmark with comprehensive annotations and evaluation metrics.\nWe report our primary results on the CrossCodeEval Lite (Python) and SWE-bench Lite test sets for\nCrossCodeEval and SWE-bench, respectively, and on the full test set for EvoCodeBench. Cross-\nCodeEval Lite (Python) and SWE-bench Lite represent subsets of their respective datasets. Cross-\nCodeEval Lite (Python) consists of 1000 randomly sampled Python instances, while SWE-bench\nLite includes 300 instances randomly sampled after filtering out those with poor issue descriptions.\nRemark: During indexing of 43 Sympy samples from the SWE-bench dataset, we face out-of-memory\nissues due to numerous files and complex dependencies, leading to their exclusion. Similarly, some\nEvoCodeBench samples are omitted due to test environment configuration issues. Thus, SWE-bench\nLite and EvoCodeBench results are based on 257 and 212 samples, respectively.\nBaselines. We evaluate whether CODEXGRAPH is a powerful solution for Retrieval-Augmented\nCode Generation (RACG). We specifically assess how effectively code graph\ndatabase interfaces aid LLMs in understanding code repositories, particularly when handling diverse\ncode questions across different benchmarks to test CODEXGRAPH 's general applicability. To\nachieve this, we select resilient RACG baselines that can be adapted to various tasks. Based on the\ncategories in Section 2.2, we choose BM25 and AUTOCODEROVER\nwhich are widely recognized in code tasks along with a NO-RAG method. Besides, since\nour work focuses on RACG methods and their generalizability, we exclude methods that interact\nwith external websites and runtime environments, as well as task-specific methods that are not easily adaptable across multiple\nbenchmarks. These methods fall outside the scope of our project.\nEspecially, although evaluate AUTOCODEROVER exclusively on SWE-bench,\nwe extend its implementation to CrossCodeEval and EvoCodeBench, while retaining its core set of\n7 code-specific tools for code retrieval.\nLarge Language Models (LLMs). We evaluate CODEXGRAPH on three advanced and well-\nknown LLMs with long text processing, tool use, and code generation capabilities: GPT-40,\nDeepSeek-Coder-V2, and Qwen2-72b-Instruct."}, {"title": "5 Results", "content": "5.1 Analysis of Repository-Level Code Tasks\nRACG is crucial for repository-level code tasks. In Table 1, RACG-based methods-BM25,\nAUTOCODEROVER, and CODEXGRAPH-basically outperform the NO-RAG method across all\nbenchmarks and evaluation metrics. For instance, on the CrossCodeEval Lite (Python) dataset, using\nGPT-40 as the backbone LLM, RACG methods improve performance by 10.4% to 17.1% on the EM\nmetric compared to No-RAG. This demonstrates that the No-RAG approach, which relies solely\non in-file context and lacks interaction with the code repository, significantly limits performance.\nExisting RACG methods struggle to adapt to various repo-level code tasks. Experimental re-\nsults in Table 1 reveal the shortcomings of existing RACG-based methods like BM25 and Au-\nTOCODEROVER. While these methods perform well in specific tasks, they often underperform\nwhen applied to other repository-level code tasks. This discrepancy typically arises from their in-\nherent characteristics or task-specific optimizations.\nSpecifically, AUTOCODEROVER is designed with code tools tailored for SWE-bench tasks, leverag-\ning expert knowledge and the unique features of SWE-bench to optimize tool selection and design.\nThis optimization refines the LLM agent's action spaces, enabling it to gather valuable information\nmore efficiently and boosting its performance on SWE-bench tasks (22.96%). However, these task-\nspecific optimizations limit its flexibility and effectiveness in other coding tasks, as evidenced by its\nsubpar results on CrossCodeEval Lite (Python) and EvoCodeBench compared to other methods.\nSimilarly, BM25 faces the same issues. In CrossCodeEval Lite (Python), its similarity-based re-\ntrieval aligns well with code completion tasks, enabling it to easily retrieve relevant usage references\nor direct answers. This results in strong performance, particularly in the ES metric. However, BM25\nlacks the reasoning capabilities of LLMs during query construction, making its retrieval process less\nintelligent. Consequently, when confronted with reasoning-heavy tasks like those in SWE-bench,\nBM25 often fails to retrieve appropriate code snippets, leading to poor performance.\nCODEXGRAPH shows versatility and efficacy across diverse benchmarks. Table 1 shows that\nCODEXGRAPH achieves competitive results across various repository-level code tasks with general\ncode graph database interfaces. Specifically, with GPT-4o as the LLM backbone, CODEXGRAPH\noutperforms other RACG baselines on CrossCodeEval Lite (Python) and EvoCodeBench, while\nalso achieving results comparable to AUTOCODEROVER on SWE-bench Lite. This demonstrates\nthe generality and effectiveness of the code graph database interface design.\nCODEXGRAPH increases token consumption. CODEXGRAPH uses code graph databases as\ninterfaces and retrieves information from the code repository by writing graph queries. While\nbenefiting from larger and more flexible action spaces, it also incurs increased token costs. The\nprimary reason for this is that the length of the query outcomes is not controllable. Moreover,\nCODEXGRAPH sometimes encounters loops where it fails to generate executable graph queries. As\ndemonstrated in Table 2, this leads to a higher token usage compared to existing RACG methods."}, {"title": "5.2 Deeper Analysis of CODEXGRAPH", "content": "Optimal querying strategies vary across dif-\nferent benchmarks. There are two strate-\ngies for formulating queries in each round\nwithin CODEXGRAPH: either generating a sin-\ngle query or producing multiple queries for\ncode retrieval. Opting for a single query per\nround can enhance precision in retrieving rel-\nevant content but may compromise the recall\nrate. Conversely, generating multiple queries\nper round can improve recall but may reduce\nprecision. Experimental results, as illustrated\nin Figure 4, reveal that for CrossCodeEval Lite\n(Python), which involves lower reasoning dif-"}, {"title": "6 Real-World Application Scenario", "content": "To highlight the practical value of the CODEXGRAPH in real-world applications, we develop five\ncode agents using the flexible ModelScope-Agent framework. These agents are\ndesigned to address common coding challenges in production environments by integrating key con-\ncepts of the CODEXGRAPH. Code Chat allows users to inquire about a code repository, providing\ninsights into code structure and function usage. Code Debugger diagnoses and resolves bugs by\napplying iterative reasoning and information retrieval to suggest targeted fixes. Code Unittestor\ngenerates unit tests for specified classes or functions to ensure thorough functionality verification.\nCode Generator automatically creates code to meet new requirements, extending the functionality\nof existing codebases. Lastly, Code Commentor produces comprehensive annotations, enhancing\ndocumentation for code segments lacking comments. Examples of these agents are provided in"}, {"title": "7 Discussion", "content": "Limitations. CODEXGRAPH has only been evaluated on a single programming language, Python.\nIn the future, we plan to extend CODEXGRAPH to more programming languages, such as Java and\nC++. Secondly, there is room for improvement in the construction efficiency and schema complete-"}, {"title": "Conclusion", "content": "CODEXGRAPH addresses the limitations of existing RACG methods, which often\nstruggle with flexibility and generalization across different code tasks. By integrating LLMs with\ncode graph database interfaces, CODEXGRAPH facilitates effective, code structure-aware retrieval\nfor diverse repository-level code tasks. Our evaluations highlight its competitive performance and\nbroad applicability on academic benchmarks. Additionally, we provide several code applications\nin ModelScope-Agent, demonstrating CODEXGRAPH 's capability to enhance the accuracy and us-\nability of automated software development."}, {"title": "A Appendix", "content": "A.1 Details of the graph database schema\nThis schema is designed to abstract code repositories into code graphs for Python, where nodes\nrepresent symbols in the source code, and edges represent relationships between these symbols.\nA.1.1 Node Types\nEach node in the code graph represents a different element within Python code, and each node type\nhas a set of attributes that encapsulate its meta-information. The node types and their respective\nattributes are as follows:"}, {"title": "A.1.2 Edge Types", "content": "Edges in the code graph represent various relationships between the nodes. The edge types we define\nand the relationships they signify are as follows:"}, {"title": "A.2 Real-World Application", "content": "In this section, we present the WebUI interface for CODEXGRAPH, showcasing its five practical\napplications: Code Chat, Code Debugger, Code Unittestor, Code Generator, and Code Commentor.\nThe interface is designed to facilitate user interaction, providing a streamlined and intuitive environ-\nment for various code-related tasks. We built the WebUI interface using Streamlit, a powerful and\nuser-friendly framework that allows for the rapid development of interactive web applications."}, {"title": "A.2.1 Example of Code Chat", "content": "Code Chat allows users to inquire about a code repository, providing insights into code structure\nand function usage. This functionality is particularly useful for understanding complex codebases,\nidentifying dependencies, and exploring the usage of specific classes, methods, and functions.\nHere is an example of Code Chat. The user's question is \u201c Summarize the 'CodexGraphAgentChat'\nclass, what has method, and what for\"."}, {"title": "A.2.2 Example of Code Debugger", "content": "The Code Debugger diagnoses and resolves bugs by applying iterative reasoning and information\nretrieval to suggest targeted fixes. It utilizes Cypher queries to analyze the code repository, identify\nthe cause of the issue, and recommend precise modifications.\nHere is an example of Code Debugger. The user's input is a real issue10 where the outcome does\nnot match the expected behavior. The Code Debugger first analyzes the problem, then uses Cypher\nqueries to retrieve relevant information and infer the cause of the bug. Finally, it provides an expla-\nnation of the bug and suggests the location for the modification."}, {"title": "A.2.3 Example of Code Unittestor", "content": "Here is an example of Code Unittestor. The user's input is: \"Generate test cases for TaskManager.\"\nThe CodexGraph agent will first retrieve all methods and inheritance relationships in 'TaskManager',\nand then generate detailed test case code."}, {"title": "A.2.4 Example of Code Generator", "content": "The user has requested a function to retrieve the number of input and output tokens of 'CypherA-\ngent'. However, the challenge is identifying the corresponding fields within 'CypherAgent' as this\ninformation is not provided in the user's input."}, {"title": "A.2.5 Example of Code Commentor", "content": "The Code Commentor analyzes code to provide detailed comments, enhancing code readability and\nmaintainability. It leverages the code graph database to understand the code's structure and behavior,\nensuring accurate and informative comments."}]}