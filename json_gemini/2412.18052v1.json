{"title": "Beyond Gradient Averaging in Parallel Optimization: Improved Robustness through Gradient Agreement Filtering", "authors": ["Francois Chaubard", "Duncan Eddy", "Mykel J. Kochenderfer"], "abstract": "We introduce Gradient Agreement Filtering (GAF) to improve on gradient averaging in distributed deep learning optimization. Traditional distributed data-parallel stochastic gradient descent involves averaging gradients of micro-batches to calculate a macrobatch gradient that is then used to update model parameters. We find that gradients across microbatches are often orthogonal or negatively correlated, especially in late stages of training, which leads to memorization of the training set, reducing generalization. In this paper, we introduce a simple, computationally effective way to reduce gradient variance by computing the cosine distance between micro-gradients during training and filtering out conflicting updates prior to averaging. We improve validation accuracy with significantly smaller microbatch sizes. We also show this reduces memorizing noisy labels. We demonstrate the effectiveness of this technique on standard image classification benchmarks including CIFAR-100 and CIFAR-100N-Fine. We show this technique consistently outperforms validation accuracy, in some cases by up to 18.2% compared to traditional training approaches while reducing the computation required nearly an order of magnitude because we can now rely on smaller microbatch sizes without destabilizing training.", "sections": [{"title": "1. Introduction", "content": "With increasingly large deep learning models, hardware constraints often limit the feasible batch size that can fit within the memory (VRAM) of even the most powerful GPUs. Consequently, machine learning practitioners must distribute the training across hundreds or thousands of GPUs using Distributed Data Parallelism (DDP), Distributed Model Parallelism (DMP), or a combination of both [3, 10, 16, 28, 35]. As a result, the traditional mini-batch in stochastic gradient descent (SGD) has been replaced with microbatches and macrobatches [23]. A microbatch is defined as the samples processed by a single forward and backward pass to produce a microbatch gradient, often called a micro-gradient. Microbatches are typically produced on a per GPU basis and then shared across all of the other GPUs to calculate the macrobatch. A macrobatch is the union of all microbatches. Each micro-gradient is summed and then normalized to compute the macro-gradient, which is then used to update the model. In practice, the microbatch size is chosen to maximize the VRAM utilization on a single GPU or computation node. During the aggregation of micro-gradients, practitioners leverage the Ring-AllReduce algorithm [11] to efficiently aggregate micro-gradients across all computation nodes. Ring-AllReduce relies on sequential summation and normalization to ensure synchronization of gradient values across all nodes without each node needing to retain multiple micro-gradient copies in memory. Once all gradients from the macrobatch have been aggregated a parameter update is performed and the process is repeated.\nHowever, there is an underexplored question: is averaging all micro-gradients the best thing to do all of the time? Furthermore, are micro-gradients ever orthogonal or, worse, negatively correlated with each other during training? If so, what does this imply? This has been recently explored in the context of reinforcement learning for both multi-constraint [34] and multi-task optimization [36] where gradients with respect to specific constraints or tasks are compared using cosine distance and update a projected component of the conflicting gradient onto the update direction or skip the gradient update altogether if the direction violates constraints. However, this has yet to be developed as a general optimization procedure, specifically in the context of distributed training. The first question we explore is what happens during typical training. Are the micro-gradients always correlated or are they orthogonal or divergent? To measure this, we compute the cosine distance between micro-gradients before averaging them over the course of training ResNet18 [9] on both CIFAR-100 [14] and CIFAR-100N-Fine [32]. The cosine distance D(x, y) \u2208 [0, 2] be-"}, {"title": "2. Previous Works", "content": "Gradient descent has been fundamental to machine learning since the 1950s. Initial works introduced the basic iterative framework for gradient descent and soon led to the development of stochastic gradient descent (SGD) [27], which computes gradients on small, random subsets of data rather than the entire dataset, enabling efficient optimization in high-dimensional settings. Building upon this foundation, researchers have developed various enhancements to SGD, including momentum [24], which introduces a velocity term to accelerate convergence in regions with low curvature, and Adam [13], which combines momentum with adaptive learning rates to better handle sparse gradients and noisy data. A recent refinement is, AdamW [19], decouples weight decay from the gradient update process, yielding better generalization properties by stabilizing the learning dynamics. In recent years, the deep learning community has produced a substantial body of research that addresses the practical challenges of gradient-based optimization.\nTo handle the immense computational demands of training large models, researchers have explored various distributed and parallel training frameworks based on the concepts of data and model parallelism. These approaches enable practitioners to scale training across multiple GPUs or compute nodes, facilitating larger batch sizes and reducing the time required for model convergence. Techniques like Ring-AllReduce [11] allow for efficient gradient aggregation across GPUs, minimizing communication overhead, and memory, enabling synchronous training on high-performance systems. Additionally, asynchronous gradient sharing strategies and parameter servers [3] have been proposed to further enhance scalability, though come at the cost of potential staleness in parameter updates.\nAdaptive optimization algorithms, including RMSProp [30] and Adam [13], address the limitations of standard SGD by dynamically adjusting learning rates based on historical gradient information. These methods have proven especially useful in handling noisy or sparse gradients, which are common in large-scale deep learning models. Recent advancements, such as layer-wise adaptive moments (LAMB) [35] and AdaBelief [37] focus on improving generalization by adapting learning rates according to layer-specific characteristics or reducing reliance on gradient magnitudes to mitigate training instability.\nA challenge in deep learning is the balance between fitting the training data well while simultaneously avoiding overfitting and memorizing train noise. Researchers have proposed various strategies to control overfitting, such as data augmentation, dropout [29], and early stopping [25]. Recent work on sharpness-aware minimization (SAM) [6] explicitly targets solutions within flatter regions of the loss landscape to promote generalization, which has shown significant promise across various deep learning benchmarks.\nTraining deep models in the presence of noisy labels is a challenging problem, as noise can lead to memorization of incorrect labels and hinder generalization. Several methods have been proposed to address label noise, including learning from noisy labels [17], co-teaching [8], and learning to learn from noisy labels [26]. These methods often rely on a dual-network architecture, where one network acts as a teacher or peer model to guide the student model in selectively learning from clean samples. This approach, however, is computationally expensive as it requires training two instances of the same model in parallel, which scales poorly for large models and datasets. More recent approaches, such as self-supervised over-parametrization (SOP) [18], utilize an expectation-maximization technique to address label noise by leveraging over-parameterized models, though this method also incurs substantial additional computational costs. DivideMix [15] and ProMix [33] introduce techniques for probabilistic mixing of samples, aiming to filter noisy samples during training, but they still rely on computationally intensive procedures to maintain robust performance. The sample priors* framework [2] employs sample reweighting based on prior distributions to discount noisy labels, but it similarly requires additional model components that limit its scalability.\nThe choice of batch size plays a crucial role in the tradeoff between training stability and generalization. Studies by [20] have shown that batch sizes up to a certain critical threshold stabilize model performance, whereas larger batch sizes tend to degrade generalization due to reduced gradient noise. Further studies have proposed batch-size scaling rules and scaling laws for adapting learning rate with batch size to optimize training efficiency and convergence [7].\nRecent research has also focused on optimizing the gradient aggregation process itself. Techniques like gradient clipping [22] help stabilize training by capping the norm of gradients, particularly in recurrent neural networks where gradient explosion is common. Further, gradient noise injection [21] has been explored as a means to escape sharp local minima and prevent overfitting. Our work builds on this line of inquiry by introducing gradient agreement filtering, a novel approach to dynamically filter micro-gradients based on cosine distance, allowing us to improve computational efficiency by reducing batch sizes while still maintaining training stability by excluding high-disagreement gradients in each macrobatch."}, {"title": "3. Methods", "content": "We consider the problem of how to most efficiently estimate an accurate gradient by aggregating micro-gradients during distributed training while preventing memorization and minimizing the compute budget. The core algorithm is presented in Algorithm 1. Consider a training set N of size n. In traditional SGD, an update to the model parameters \\(\\theta\\) is computed by sampling a minibatch \\(B \\subset N\\) of size \\(|B| = b\\), calculating the gradient \\(\\nabla_{\\theta} L(B; \\theta)\\), and applying the following update rule\n\\begin{equation}\n\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} L(B; \\theta)\n\\end{equation}\nwhere \\(\\eta\\) is the learning rate, and \\(L(B; \\theta)\\) is the loss function over the minibatch B.\nDue to GPU memory constraints, training is parallelized across multiple GPUs by computing the gradient for a macro-"}, {"title": "3.1. Gradient Agreement Filtering (GAF)", "content": "Gradient agreement filtering is an approach to aggregate micro-gradients that improves upon simply averaging all micro-gradients \\(\\nabla_{\\theta} L(U_i; \\theta) \\forall U_i \\in M\\). The approach is motivated by the following observation. If we train on completely random data (white noise), a model will overfit the train set but cosine distance will never fall below 0.99 after just a few iterations, as seen in Figure 5. This suggests that we can prevent overfitting on noise by simply skipping updates where the micro-gradients have greater than 0.99 cosine distance. The cosine distance \\(D_c\\) between two vectors x and y is\n\\begin{equation}\nD_c(x, y) = 1 - \\frac{x^T y}{\\|x\\| \\|y\\|}\n\\end{equation}\nWith Gradient Agreement Filtering (GAF), instead of blindly averaging all micro-gradients in M, we apply a cosine distance threshold to select only those micro-gradients that are aligned within a given threshold \\(\\tau\\), as shown in Algorithm 1. Let \\(g_i = \\nabla_{\\theta} L(U_i; \\theta)\\) denote the micro-gradient for microbatch \\(U_i\\). The cosine distance between a candidate micro-gradient \\(g_i\\) and the running sum of accepted gradients g is \\(D_c(g_i, g)\\).\nWe compute a rolling aggregation of micro-gradients starting from the local gradient \\(g_s\\) and then checking one by one, and only including those for which \\(D_c(g_i, g) \\leq \\tau\\). We keep a counter c of the agreed upon gradients starting at c = 1. Each accepted gradient \\(g_i\\) is added to the running sum g, and our count c is incremented to keep track of the number of accepted gradients. The filtered macrobatch gradient is\n\\begin{equation}\n\\nabla_{\\theta} L_{GAF}(M; \\theta) = \\frac{g_{GAF}}{c}\n\\end{equation}"}, {"title": "4. Experiments", "content": "To demonstrate the effectiveness of GAF in practice, we train ResNet image classifiers on the CIFAR-100 and CIFAR-100N-Fine datasets using distributed data-parallelism comparing both baseline averaging-based gradient aggregation and GAF-based gradient aggregation."}, {"title": "4.1. CIFAR-100", "content": "We train RestNet18 on two A40 GPUs on the CIFAR-100 dataset using SGD with momentum and and reduction of the learning rate (learning rate) on validation plateaus with schedule patience of 100 and 0.1 discount. We use an initial learning rate of 0.01. We also applied L2 regularization with a weight decay of 0.01. In all cases, unless otherwise specified, we use a macrobatch of size m = 200 with u = 100 images per microbatch (exactly 1 sample per class) to ensure each microbatch has the same distribution of data over the training set. We flip each label with a random other classes label for x% of the labels, for x \u2208 {0, 5, 15, 30, 40, 50, 60, 75, 80, 90}, and maintain those incorrect label for the entirety of that run's training (i.e. symmetric noise). For each experiment we found the optimal value of the cosine distance threshold hyperparameter \\(\\tau\\) by performing a grid search of values from 0.95 to 1.05 with a step of 0.02 across different batch sizes. We compare with a baseline-training of ResNet18 where the cosine distance threshold is set to 2, which admits all gradients and is equivalent to averaging training weights when k = 2. We run for 500k iters for all runs, and observe convergence around 270k iterations for baseline and GAF runs.\nFor the no error case, we observe cosine distance threshold of 1 yields best performance. Once errors are introduced we observe a cosine distance of 0.97 provides the best performance."}, {"title": "4.2. CIFAR-100N-Fine", "content": "To validate GAF on a more realistic noisy dataset, we trained ResNet34 on CIFAR-100N-Fine. CIFAR-100N-Fine is a relabeled version with human annotated noisy labels obtained from one Amazon Mechanical Turk worker, who had a 40.2% noise rate but in a more structured manner than random as humans are consistently biased in their labels vs. the random flipping done in the CIFAR-100 runs. All CIFAR-100N-Fine training runs use a ResNet34 with PreAct as per the reference paper [32], trained on two A40 GPUs. We additionally test the effect of microbatch size u on the training process by training with and without GAF for batch sizes of u \u2208 {100, 200, 300, 400, 500} As with the CIFAR-100 training, we use SGD with momentum and reduce the learning rate on validation plateaus. All other hyper parameters are the same as the CIFAR-100 runs however we do not vary label error percentage since the dataset is already noisy due to the labeling process. The optimal cosine distance threshold parameter \\(\\tau\\) is found by varying the value from 0.95 to 1.05 with a step of 0.02. A cosine distance threshold of 2 for the baseline runs, which is equivalent to averaging gradients as it admits all values."}, {"title": "5. Conclusions", "content": "In this work, we introduced Gradient Agreement Filtering (GAF) as an alternative to traditional micro-gradient averaging in distributed training. Our experiments on CIFAR-100 and CIFAR-100N-Fine demonstrate the effectiveness of GAF, particularly in scenarios with label noise. By aggregating gradients based on cosine distance, GAF provides a robust approach that improves model performance. Specifically, we observe a 0.2% improvement on CIFAR-100 without added noise, with progressively larger improvements over baseline training methods as label noise increases, reaching up to an 18.4% gain at a 60% noise rate. On the CIFAR-100N-Fine dataset, GAF achieves a 9.3% improvement over the baseline. We also observe that we are able to maintain the performance improvement even as the microbatch size was reduced, suggesting that we can improve model performance while reducing computational costs.\nThese results indicate that GAF is a promising approach for improving training stability and accuracy, particularly in noisy environments. The use of cosine distance to filter gradients shows potential not only in mitigating the impact of label noise but also in reducing the computational cost of large-scale distributed training by focusing resources on more aligned gradients."}, {"title": "6. Future Research Directions", "content": "While GAF has demonstrated promising results, several avenues for further research could expand upon its potential and applicability:\n\u2022 Alternative Similarity Metrics: While cosine distance proved effective, other similarity metrics, such as Mahalanobis distance, could be explored to evaluate their impact on GAF's performance. This could help in tailoring GAF to different types of datasets and noise structures.\n\u2022 Adaptive Thresholding: In this work, we used a fixed cosine distance threshold throughout training. An adaptive threshold that dynamically adjusts based on training progress or model convergence rates may yield improved results, especially in tasks with fluctuating noise levels or diverse data distributions.\n\u2022 Application to Other Tasks: GAF was applied to image classification in this study. Extending this technique to other domains, such as natural language processing, speech recognition, or reinforcement learning, could uncover broader benefits and challenges associated with GAF in non-vision tasks.\n\u2022 Memory and Computation Efficiency: As GAF requires tracking only pairwise cosine distances between micro-gradients, applying this to Ring-AllReduce would be straightforward but would require applying cosine distance to buckets at a time. Ensuring GAF's improvement is maintained despite this is an area of future research, as well as other avenues to optimize compute and memory overhead.\n\u2022 Order Indifference Techniques: As GAF is sensitive to the order in which microgradients are processed, perhaps there is a way to augment Ring-AllReduce where during the AllGather phase, the GPU with the highest (or lowest) agreement is the one distributed to all other nodes.\n\u2022 Integration with Advanced Optimizers: We used standard optimizers like SGD and Adam in our experiments. Investigating how GAF interacts with other advanced optimization techniques, such as Adam, AdamW, LAMB or SHAMPOO, could enhance GAF's performance, particularly in large-scale or fine-tuning scenarios.\n\u2022 Analysis of Gradient Disagreement Dynamics: Further study of the dynamics of gradient disagreement over the course of training could yield insights into how models converge under noisy conditions and how GAF influences the loss landscape. This might lead to improvements in convergence rates and generalization.\nFurther research in these directions highlight potential improvements and adaptations of GAF, aiming to make it more efficient, robust, and applicable across various deep learning domains."}]}