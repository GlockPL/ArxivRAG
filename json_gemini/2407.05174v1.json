{"title": "Synthetic Data Aided Federated Learning Using Foundation Models", "authors": ["Fatima Abacha", "Sin G. Teo", "Lucas C. Cordeiro", "Mustafa A. Mustafa"], "abstract": "In heterogeneous scenarios where the data distribution amongst the Federated Learning (FL) participants is Non-Independent and Identically distributed (Non-IID), FL suffers from the well-known problem of data heterogeneity. This leads the performance of FL to be significantly degraded, as the global model tends to struggle to converge. To solve this problem, we propose Differentially Private Synthetic Data Aided Federated Learning Using Foundation Models (DPSDA-FL) \u2013 a novel data augmentation strategy that aids in homogenizing the local data present on the clients' side. DPSDA-FL improves the training of the local models by leveraging differentially private synthetic data generated from foundation models. We demonstrate the effectiveness of our approach by evaluating it on the benchmark image dataset: CIFAR-10. Our experimental results have shown that DPSDA-FL can improve class recall and classification accuracy of the global model by up to 26% and 9%, respectively, in FL with Non-IID issues.", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) enables the training of a machine learning model by several parties without sharing their data with each other [McMahan et al., 2017]. The training process is orchestrated by a third party, which is usually a central server. In FL, each client uses its private data to train its own model known as the local model, while the server uses an aggregation algorithm to construct a global model from the local models. The entire process runs for several iterations until a global model with the desired performance is achieved [Shahid et al., 2021]. This global model is then broadcast to all the clients to use it for inference on their test dataset.\nFL provides protection against data leakage as the private training data of each client is not disclosed to any other party. It can facilitate collaboration between institutions that deal with sensitive data, such as health and financial data [Aouedi et al., 2022]. Regulations such as the General Data Protection Regulation (GDPR) and Health Insurance Portability and Accountability Act (HIPAA) control how sensitive data are stored and shared within and between institutions in order to protect the privacy of the individuals whose data is captured [Zhou et al., 2020]. FL can aid collaborators in adhering to these regulations, as no data is shared between the clients during the training or inference process.\nHowever, FL comes with its own challenges, as studies have shown that the global model struggles to converge when the data distribution amongst the clients is statistically heterogeneous [Zhao et al., 2018], [Li et al., 2020]. This implies that the data distribution is Non-Independent and Identically distributed (Non-IID). A client may hold data from some classes, but not all classes present in the global dataset or clients could hold data for all classes but in different quantity. This statistical heterogeneity of local data could result in each local model being very different from other local models, leading to a global model that performs at a subpar level [Li et al., 2022]. Also, when clients train their local model on data that does not contain certain classes from the global set or only a few samples from specific classes, the models are likely to be biased towards those underrepresented groups [Hao et al., 2021]. This could lead to devastating consequences when these models are deployed in safety-critical situations such as healthcare and finance.\nThe presence of biases could also disincentivize clients from participating in FL collaboration as they would lose trust in the system. For instance, imagine a collaboration between pharmaceutical companies training a model to determine the effectiveness of several drugs on an ailment and having the drug from one company consistently being predicted as the most effective because they provide more data as a result of conducting more experiments than the others [Rance and Svoboda, 2023]. Data heterogeneity, as such, is a challenge that needs to be addressed to obtain trustworthy FL models.\nSome existing work [Zhao et al., 2018] have proposed a global data sharing strategy to tackle the challenge of FL with Non-IID data. The server is posited to have a uniformly distributed dataset in its possession. This global data is then shared amongst the clients to harmonize their data distribution to alleviate the impact of data heterogeneity. Other approaches such as FedProx [Li et al., 2020] introduce a regularization term to the local model loss function on the client side, this mitigates the effect of data heterogeneity and enhances the convergence of the global model. The work of [Li et al., 2022] employs Generative Adversarial Networks (GANs) to produce synthetic data to solve the problem of"}, {"title": "2 Background and Related Work", "content": "2.1 Data Heterogeneity\nData heterogenity is the degree of diversity in the datasets held by clients participating in FL. Data heterogeneity in FL arises from the differences in data distribution, data quality, and data quantity among participants. It can manifest in various forms in FL. Quantity Skew results from the differences in the amount of data held by clients, while Label Skew results from the differences in the classes of data held by individual clients [Qu et al., 2021].\nSeveral techniques have been proposed to address the challenge of data heterogeneity in FL. FedProx [Li et al., 2020] addresses heterogeneity in FL by integrating a proximal term into the training process. The proximal term leads to a reduction in the divergence of the local models from the global model by serving as a penalization term for the loss function of the local models. [Karimireddy et al., 2021] developed the stochastic controlled averaging algorithm, a modification of the federated averaging, which incorporates variance reduction to stabilize the local model towards the global model. However, these techniques are not effective in extreme cases of data heterogeneity. Another line of work uses GANs to mitigate the effects of data heterogeneity by generating additional training data for local data augmentation [Razavi-Far et al., 2022] of which our method aligns with. However, despite using a similar approach to the GAN-based data augmentation methods, we locally generate differentially private synthetic data using foundation models to mitigate the effects of data heterogeneity, our solution generates more diverse synthetic data that is of higher quality than the GAN-based approaches.\n2.2 Generative Adversarial Networks\nGenerative adversarial networks (GANs) are deep learning models comprising of two networks: the generator and discriminator. The generator produces synthetic data mimicking real data, challenging the discriminator to distinguish between them [Goodfellow et al., 2014]. Synthetic data from GANs share the statistical distribution of real datasets and, as such, can be used for dataset augmentation, enhancing model performance [Antoniou et al., 2018].\n[Zhang et al., 2021] trained a GAN at the server side using FL and then shared the synthetic data across clients to improve the performance of FL. [Li et al., 2022] proposed Synthetic Data Aided Federated Learning (SDA-FL), where all clients receive a portion of locally synthetically generated data that is globally shared by the server. Despite the effectiveness of GAN-based methods in combating data heterogeneity problems in FL and enhancing the performance of the global model, these works have limitations. The instability of training GANs can result in low-quality synthetic samples with low utility [Azizi et al., 2023].\nRecent works have addressed the underperformance of GANs in generating high-quality synthetic data by adopting diffusion models. Diffusion models are generative deep learning architectures that generate synthetic data by iteratively adding noise to real data and then removing this noise through a reverse diffusion process [Yang et al., 2024]. Diffusion models have been shown to produce high-quality data for computer vision applications [Dhariwal and Nichol, 2021; Azizi et al., 2023]. Diffusion models, however, can be challenging to train due to their high computational requirements, which are often beyond the reach of many. However, the emergence of foundation models has made access to pre-trained diffusion models more accessible.\n2.3 Foundation Models\nFoundation models are a class of generative AI trained on large-scale data and can be modified to undertake various tasks with high precision [Zhou et al., 2023]. Foundation models like Open Al's Stable Diffusion and DALL.E [Ramesh et al., 2022] have become widely accessible. These pre-trained models can be used to generate high-utility synthetic data."}, {"title": "3 DPSDA-FL: Differentially Private Synthetic Data Aided Federated Learning Using Foundation Models", "content": "This section proposes our novel technique, DPSDA-FL, that generates differentially private synthetic data for FL using foundation models. Figure 1 gives a high-level overview of our proposal. DPSDA-FL works in two main stages, Stage 1 in which each Cross-Silo FL client uses a foundation model to locally generate differentially private synthetic data from their private data and then share part of the synthetic data with the central server to form a global synthetic data which will be utilized in Stage 2. In the next stage, the server distributes the global synthetic data to clients in order to enable them augment their local data with the diverse and high quality synthetic data. This augmentation leads to a less heterogeneous local data distribution by allowing clients to possess synthetic data from classes they do not possess or classes they possess a very limited sample from. This subsequently leads to a more stable local model training that enhances the performance of the global model both in terms of its recall capability and its accuracy. A more detailed overview of how DPSDA-FL works is presented below:\nUnique label count information sharing: At the start of the training process, clients share their unique label counts with the server to form a globally unique label count. This information will be used to share the synthetic global data with clients to ensure each client receives differentially private synthetic data from the classes they are deficient in.\nLocal clients' synthetic data generation using foundation models: To generate our differentially private synthetic data, we utilize the image-guided diffusion model DPSDA [Lin et al., 2024], as our foundation model. DPSDA is based on improved diffusion [Dhariwal and Nichol, 2021]. To ensure the privacy of local training data, a local copy of the diffusion model is downloaded and hosted locally on client's devices. It mitigates privacy risks associated with diffusion models memorizing their training data, as shown in [Carlini et al., 2023]. The local synthetic data \\(D_{i}^{syn}\\) are then"}, {"title": "Algorithm 1 DPSDA-FL", "content": "1: Input Parameters:\n2: N: Number of clients.\n3: T: Total number of rounds.\n4: a: Learning rate.\n5: \\(w_t\\): Initial model parameters.\n6: \\(W_{t+1}\\): Updated model parameters.\n7: Initialization\n8: Clients share their unique label counts with the server\n9: Clients generate DP synthetic data using Foundation Models\n10: for i = 1 to N do\n11:  Generate \\(D_{i}^{syn}\\) from \\(D_{i}\\)\n12:  Send \\(D_{i}^{syn}\\) to server\n13: end for\n14: Server forms global \\(D_{G}^{syn}\\) from \\(D_{i}^{syn}\\)\n15: Distribute \\(D_{G}^{syn}\\) using unique label count\n16: for t = 1 to T do\n17:  Send \\(w_t\\) to all clients\n18:  for i = 1 to N do\n19:   Augment \\(D_i\\) with \\(D_{G}^{syn}\\)\n20:   Train model \\(L_i\\) to update \\(w_{i+1}\\)\n21:   Server Initializes \\(w_0\\)\n22:   Send \\(w_{i+1}\\) to server\n23:  end for\n24:  Aggregate \\(w_{t+1} = \\sum_{i=1}^{N} w_{i+1}\\)\n25: end for\n26: Repeat until convergence\nshared with the server to construct the global synthetic data \\(D_{G}^{syn}\\).\nGlobal synthetic data distribution: The differentially private synthetic data from the previous step is then shared by the server with the local clients. The local data class information possessed by the server guides effective distribution, so each client only receives data from classes it lacks.\nLocal data augmentation: Clients then utilize the received synthetic data to augment their local data and homogenize the local data distribution. These synthetic data are of high quality and can enhance local model training.\nFederated training: With more stable local training aided by the augmented local datasets at each client's side, clients proceed to train a federated global model jointly. Note that DPSDA [Lin et al., 2024] does not necessitate any pre-training to generate the synthetic data, and the clients are assumed to be health institutions that can afford reasonable computational resources.\nAlgorithm 1 outlines the pseudocode for DPSDA-FL. We consider a FL setting with a single semi-trusted central server S and N clients denoted by {C1, C2,...CN}. A horizontal FL setup is one where the data across the FL clients is partitioned horizontally, and clients share similar feature sets but different sample spaces. Each client possesses a local dataset"}, {"title": "4 Experiments and Evaluations", "content": "4.1 Experimental Setting\nBelow we describe the experimental settings used in our evaluation. These settings are also summarised in Table 1.\nDataset\nWe performed our experiments on the CIFAR-10 dataset, which is a benchmark dataset used for image recognition. It consists of 50,000 training samples and 10,000 testing samples. The dataset is mostly utilized to evaluate the classification accuracy of Convolutional Neural Networks (CNN). We used the entire 10,000 images to test the accuracy of the global model for our approach and the baselines.\nDifferentially Private Synthetic Dataset\nWe deployed five pre-trained diffusion models to generate synthetic local data for each client. To limit privacy risks associated with the honest but curious server, we assumed each client only generated and shared at most 50% of its number of classes. We generated 5000 differentially private synthetic images for each class of the CIFAR-10 dataset and selected a subset to be used for augmentation. The generated images were 64 x 64; as such, they were resized to 32 x 32, which is the original size of the CIFAR-10 images.\nData Distribution\nWe evaluated the effectiveness of our approach by simulating real-world FL participants with varying data distributions that follow a Non-IID fashion. In other words, the local data distribution of each client is not representative of the global dataset. To simulate extreme label skew for our experiments, we followed the work [Li et al., 2020]; each client received samples from only two classes.\n\\(D_i\\), which is a subset of the global dataset \\(D_G\\). \\(D_G\\) follows a normal distribution and consists of k classes of data. However, \\(D_i\\) does not follow a normal distribution as the data distribution amongst the clients is Non-IID. Some clients may possess fewer samples than others, leading to quantity skew or some classes of data but not others resulting in label skew. As we are considering a cross-silo FL setup, all the clients participate in training rounds, and each local model \\(L_n\\) contributes to the global model aggregation. The objective is to produce a single global model G that performs well on the global test data."}, {"title": "4.2 Results and Discussion", "content": "Our experimental results are summarised in Tables 2 and 3, as well as in Figure 2. We visualized our models' performances using confusion matrices as they provide a clear insight into the model's ability to make correct predictions for both positive and negative cases. The darker shades of colour in each matrix represent these correct predictions. Our findings reveal a promising outcome. For the global FL model trained on IID Data using FedAvg, which represents the most ideal case, as depicted in Figure 2a, the majority of classes were correctly identified, as evidenced by a darkened diagonal. In contrast, the global model trained on Non-IID using FedAvg, where each client possesses data from only 2 classes, struggles to correctly identify positive and negative cases, as shown in Figure 2b. This struggle highlights the challenge faced by models in such scenarios. The global model trained using FedProx Figure 2c shows a better performance than FedAvg, due to the proximal term added to the loss function of local clients to prevent significant divergence. However, DPSDA-FL demonstrated an even more accurate and enhanced global model, as shown in Table 2 and Figure 2d. This can be attributed to the more stable local training for clients aided by the differentially private synthetic data. The recall of the classes for which the differentially private synthetic data generated by the foundation models is shared with other clients tends to improve as well, as shown in Table 3. This suggests that the local models that make up the global model were able to effectively learn features of those classes from the high utility and diverse synthetic data. As such, the global model is able to identify more data samples from those classes and also distinguish the classes more accurately than others, compared to FedAvg and FedProx."}, {"title": "5 Conclusion", "content": "In this paper, we present a new data augmentation strategy that has the potential to significantly enhance the performance of cross-silo horizontal FL with Non-IID. By mitigating the effect of data heterogeneity, DPSDA-FL, which utilizes differentially private synthetic data generated by pre-trained foundation models, can improve the local training and convergence of the global model. Our experimental results demonstrate that DPSDA-FL can effectively improve the class recall and classification accuracy of the global model in FL with Non-IID issues.\nWe plan on evaluating our approach by conducting more experiments and generating local synthetic data using a limited number of private data samples. We also leave experimenting with other datasets that do not overlap with the training datasets of the foundation models for future work."}]}