{"title": "Pruning Large Language Models with Semi-Structural Adaptive Sparse Training", "authors": ["Weiyu Huang", "Guohao Jian", "Yuezhou Hu", "Jun Zhu", "Jianfei Chen"], "abstract": "Transformer-based Large Language Models (LLMs) have demonstrated remark- able success across various challenging tasks. However, the deployment of LLMS is hindered by their substantial parameter count and memory consumption. Re- cently, numerous studies have attempted to compress LLMs by pruning them using training-free methods. However, these pruned models often experience significant performance degradation on complex tasks. To address this issue, we propose a novel training pipeline for semi-structured sparse models, named Adaptive Sparse Trainer (AST). By distilling the knowledge stored in its dense counterpart, we prevent the sparse model from overfitting and ensure a stable training process. More- over, AST allows the model to adaptively select better lottery tickets (e.g., masks) during training. Additionally, we discovered that adding extra well-initialized parameters can further enhance model performance with only a small increase in memory footprint. Our method significantly narrows the performance gap between dense and sparse models while maintaining limited computational cost. Further- more, when combined with existing quantization methods, AST can compress language models by up to 16x compared to dense FP32 precision models with minimal performance loss. AST outperforms previous state-of-the-art methods by reducing the zero-shot accuracy gap between dense and semi-structured sparse models to 1.12% across multiple zero-shot tasks on Llama2-7B, using less than 0.4% of the pretraining tokens.", "sections": [{"title": "1 Introduction", "content": "Transformer-based Large Language Models (LLMs) [1, 4, 7] have achieved tremendous success across various challenging tasks. However, the inference speed of current state-of-the-art models is limited due to their enormous parameter count and memory latency. Therefore, it is crucial to find viable model compression methods to reduce memory footprint and lower the deployment barrier.\nModel pruning [9, 14, 45] is an effective method to compress LLMs by setting a proportion of weights to zero to satisfy a certain sparsity pattern. Recently, N:M sparsity has emerged as a type of semi-structured sparsity, offering a good balance between precision and efficient hardware support. Specifically, N:M sparsity retains only N nonzero elements out of each group of M elements. This sparsity pattern can accelerate both matrix multiplication and memory access, potentially enhancing the performance of both prefilling and decoding processes on off-the-shelf GPUs.\nExisting state-of-the-art N:M pruning methods for LLMs, such as SparseGPT [9] and Wanda [45], adopt a post-training approach to compute the connectivity pattern and update retained weights in a layer-by-layer fashion, without the need for back propagation. Despite their efficiency, these methods"}, {"title": "2 Related Work", "content": "Network Pruning Pruning is a crucial technique aimed at reducing model size and computational requirements while maintaining performance. It originated from methods like Optimal Brain Damage [29] and Optimal Brain Surgeon [17]. Based on sparsity patterns, pruning methods can be broadly categorized into unstructured, structured, and semi-structured pruning. Unstructured pruning removes individual weights [14, 38], which can maintain performance even with high sparsity. However due to its random pattern, unstructured models are difficult to accelerate. Structured pruning [31, 35, 36, 43], on the other hand, removes entire neurons, filters, or attention heads, resulting in models that are easier to accelerate on standard hardware but often suffer from severe performance loss. Recently,"}, {"title": "3 Preliminary", "content": "In this section, we present the mathematical formulation for model pruning and review some related work on training sparse models."}, {"title": "3.1 Sparsity in Transformer-Based Model", "content": "To preserve model performance while inducing sparsity, the common practice in pruning transformer- based models is to prune all the linear layers in both attention and MLP blocks. As a formulation, we consider the matrix multiplication for linear layer:\n\\(Z = XW, Z \u2208 R^{N\u00d7D}, X \u2208 R^{N\u00d7C},W\u2208R^{D\u00d7C}\\).\nWhere X, W, Z are model input, weight matrix, and output activation respectively. Pruning model is equivalent to multiplying element-wise masks to model weight:\n\\(W = m(W) \\odot W, m(W) \u2208 \\{0,1\\}^{D\u00d7C}\\)\nPrevious work has extensively studied the criteria for selecting the mask m(W) [8, 9, 16, 45]. In the context of our paper, where the mask is being recalculated during forward pass we find that the classic magnitude criteria still outperforms other one-shot pruning methods. This aligns with previous findings [37] and is attributed to two main reasons: First, most one-shot pruning methods have quadratic or cubic complexity with respect to the hidden dimension size, introducing significant computational overhead during training. Secondly, we observe that incorporating activation levels into the pruning metric is counterproductive during training, as activation levels may fluctuate due to weight updates. This variability renders the criteria less accurate in the later stages of training.\nIn this work, we focus on N:M sparsity [22], where the sparsity is restricted to having N nonzero elements in every M consecutive weights that are connected to the same output."}, {"title": "3.2 Training Sparse Model", "content": "When implementing the backward pass for a sparse model with automatic differentiation, the gradient cannot flow through the mask m(W) due to its discrete nature. A common approach to circumvent this problem is to use straight through estimator (STE) [2] or its variant Sparse-Refined STE (SR- STE) [53]. STE allows the gradient to pass to the weights \\(W_t\\) by ignoring masking operation in the backward pass and updates parameters with gradient with respect to masked weight \\(W_t\\), where \\(W_t\\) is the parameter in the t-th iteration:\n\\(W_{t+1} \\leftarrow W_t - \\eta g(W_t)\\)"}, {"title": "3.3 Measuring Training Stability", "content": "Several metrics are introduced to measure the stability of masks during training, such as flip rate [21] and SAD [53]. In this paper, we primarily measure the flip ratio and the initial flip ratio, which respectively quantify the mask changes between the current step and the previous step, as well as between the current step and the initial step.\n\\(r_t = ||m(W_t) - m(W_{t-1})||_1/D\\)\n\\(i_t = ||m(W_t) - m(W_0)||_1/D\\)\nWhere D represents the total parameter count in the linear layer. A higher flip rate indicates that the mask is changing more rapidly, while a lower flip ratio suggests that the mask is more stable."}, {"title": "3.4 Knowledge Distillation", "content": "Knowledge distillation [12, 39] is a method used to train a smaller \"student\" model by learning from a larger \"teacher\" model to achieve comparable performance with reduced computational cost. This technique involves training the student to mimic the teacher's outputs using a specialized loss function. In our case, the student model is the sparse model and the teacher model is its dense counterpart. We experimented with different types of distillation losses.\nKL-divergence loss [26] is used to measure the difference in the output probability distributions between the student and teacher models:\n\\(L_{logit} = D_{KL}(\\theta_t||\\theta_s) = \\frac{1}{B \u00d7 S} \\sum_{i=1}^{B \u00d7 S} p_{\\theta_t}(x_i) log \\frac{p_{\\theta_t}(x_i)}{p_{\\theta_s}(x_i)}\\)\nWhere B is the batch size, S is the sequence length, \\(p_{\\theta_t}\\) and \\(p_{\\theta_s}\\) are probability distributions of the teacher and student model, respectively.\nSquareHead Loss [28] measures the difference of intermediate representation using mean squared error:\n\\(L_{feat} = \\frac{MSE(f_t, f_s)}{MSE(f_t, 0)}\\)\nWhere \\(f_t\\) and \\(f_s\\) are the hidden state of l-th layer at step t for the teacher and student model, respectively."}, {"title": "4 Adaptive Sparse Trainer", "content": "In this section, we first identify the differences between classic pretraining and Post-Prune Retraining for Generalization (PRG), as well as the unique challenges associated with PRG. Based on our analysis, we then propose three methods to facilitate a robust PRG training process and obtain sparse models with state-of-the-art performance."}, {"title": "4.1 Two Stages of PRG", "content": "Unlike pretraining language models from scratch, the process of retraining a sparse model can be divided into two distinct stages based on the characteristics of the training process: the Reconstruc- tion Stage and the Continue Training Stage. The sparse model is initialized with dense pretrained weight, and the sparsity mask m(W) calculated based on magnitude.\nDuring the reconstruction stage, the model quickly relearns lost knowledge due to initial pruning. As a result, both training and validation losses decrease much faster compared with pretraining from scratch. Additionally, despite back-propagating with only language modeling loss, the SquareHead loss which measures the difference in intermediate outputs also decreases at the start of training. As shown in Fig. 2, these two phenomena occur during the same period in training. Therefore, we believe this stage should be distinguished from the later continue training stage, which more closely resembles traditional pretraining where the loss decreases slowly and SquareHead loss increases as the model learns new features.\nA plausible explanation for this is that, unlike randomly initialized models, the pruned model retains a portion of the knowledge acquired during pretraining. However, its expressive capacity is diminished due to the pruning process. Therefore, in the reconstruction stage of PRG, the model focuses on recovering its expressive ability, allowing it to demonstrate its hidden knowledge. \"Since the expressive ability is somewhat universal, the SquareHead loss, which measures the difference between hidden states, decreases at the start. Moreover, as recovering knowledge is faster than learning it from scratch, this accounts for the rapid decrease in loss. After the model recovers its expressive ability, it will move on to the continue training stage where it slowly learns new knowledge, resulting in an increase in SquareHead loss and the deceleration of the loss drop."}, {"title": "4.2 Knowledge Distillation", "content": "During PRG training, relying solely on language modeling loss is insufficient for a pruned model to converge to an optimal state, especially for smaller language models like GPT-2 or OPT-125M. As shown in Figure 2, the loss curve is unstable, and perplexity remains high without distillation. This instability occurs because, during the reconstruction stage, pruned models are more susceptible to overfitting on the limited amount of data available at the start, which prevents them from learning global features and converging to an optimal state later on. It should be noted that unlike traditional overfitting, this phenomenon persists even when a much smaller learning rate than that used in pretraining is selected, and therefore it cannot be resolved by simply reducing the learning rate.\nTo address this issue, we found that applying KL-divergence loss is adequate to prevent the model from achieving sub-optimal results. Unlike language modeling loss, KL-divergence loss quantifies the"}, {"title": "4.3 Annealing SR-STE", "content": "A naive method for retraining sparse models, as used in Wanda [45], involves freezing the mask at the beginning of training and only updating the unmasked weights. However, choosing a permanent mask based on one-shot pruning prevents a smooth transition from a dense to a sparse model and, more importantly, discards all knowledge stored in the pruned weights, thereby inhibiting convergence speed and leading to sub-optimal results. As a solution, we allow the model to dynamically determine the mask during training. Existing work [21, 33, 53] on dynamic mask training has already proven that the flip ratio of a healthy PRG training process should be higher at the beginning and then gradually decrease. Additionally, they employ SR-STE [53] as a powerful method for adjusting the flip ratio to achieve optimal results.\nNonetheless, we argue that the current constant SR-STE decay factor does not fully tap into the potential of sparse models. Inspired by the Simulated Annealing (SA) algorithm [3], we believe that the decay factor plays a role similar to temperature in the SA algorithm. When the decay factor is relatively high, the weights that are masked receive a greater decay signal, thereby reducing their likelihood of being selected in subsequent mask choices. This leads to a decrease in the flip ratio, and vice versa. At the beginning of training, it is beneficial for the model to explore a variety of mask choices more extensively, which necessitates a lower decay factor. In the later stages of training, to facilitate better convergence, we prefer less variation in the mask, requiring a higher decay factor. Therefore, we propose a novel dynamic decay factor scheduler named Annealing SR-STE.\nThe decay factor for batch t is:\n\\[ \\lambda_w(t) = \\begin{cases} a t, & \\text{if } 0 \\leq t \\leq T_0 \\\\ a T_0, & \\text{if } T_0 \\leq t \\leq T_1 \\end{cases} \\]\nHere, \\(T_0\\) denotes the total number of training batches, and \\(T_1\\) represents the increasing decay batch. Although the loss may decrease more slowly at the beginning due to frequent mask changes, applying Annealing SR-STE can eventually surpass the original method and reduce up to 30% of the performance gap in the GPT-2 model compared with static SR-STE. Details of the flip rate and initial flip rate during training are provided in Figure 4."}, {"title": "4.4 Sparse Low-Rank Boosting", "content": "The pruned model with 2:4 sparsity, which retains only half of the parameters under a strict structural constraint, experiences a reduction in expressive capacity. To mitigate this issue, we incorporate a LoRA (Low-Rank Adaptation) [20] shape parameter that trains in conjunction with the original parameters, helping to bridge the performance gap between dense and sparse models with minimal memory increase. Unlike typical LoRA fine-tuning approaches where the original model parameters are frozen and only the adapters are trained, our strategy involves training both the sparse parameters and the adapter weights simultaneously. This deviation is due to the fact that in classic LoRA fine- tuning, the extra weights are trained for downstream tasks, and the original model is frozen to prevent over-fitting and catastrophic forgetting. However, in our approach, both the additional weights and the original model are trained on a pretraining dataset to enhance generalization capabilities, allowing for their concurrent training.\nSpecifically, for a weight matrix of size \\(N \u00d7 d\\) we choose the rank \\(r\\) to be where k can be 64, 32, 16 and so on. A smaller k will yield better performance but will also require more memory. We named"}, {"title": "5 Compressing Models with Different Semi-structured Sparsity Pattern", "content": "In this section, we discuss the potential to compress models using semi-structured sparsity patterns. Previously, we examined 2:4 patterns, but theoretically, any N:M pattern can enhance matrix multi- plication with appropriate hardware support. With that said, patterns with looser constraints (i.e., a larger M) often perform better but are more challenging to compress effectively. We aim to identify the optimal trade-off and, to maintain fairness in comparison, we focus only on patterns that achieve a 50% sparsity ratio, such as 2:4, 4:8, etc.\nIt should be noted that deploying semi-structured sparse models can lead to potential memory overhead. Take the 2:4 sparsity pattern as an example: although only two non-zero elements out of every four weights need to be stored, additional positional information is required to correctly restore these compressed weights to the original matrix for calculations. Storing these indices will consume an additional proportion of memory. A naive method would be to store one bit as a mask for each element, which results in a 50% memory overhead with a 4-bit quantized model at 50% sparsity. However, a more efficient approach exists: since there are 2 non-zero elements, there are a total of \\(\\binom{4}{2}\\) = 6 possible positions, hence only an additional 3 bits are sufficient."}, {"title": "6 Results", "content": ""}, {"title": "6.1 Experiment Setup", "content": "Model configurations. We tested our methods on three different LLM model families: LLAMA2-7B [46], OPT 125M/350M/1.3B [50], and GPT2 models[4]. The LLAMA2-7B model was trained on eight Nvidia H800 GPUs with 80 GB of memory, while the OPT and GPT2 models were trained on L20 GPUs with 48 GB of memory. The training script for the GPT2 and OPT models was modified from the nanoGPT library [24], whereas the training script for LLAMA2-7B was modified from ShearedLLAMA [48]. We present our experimental results for two different sparsity patterns: AST-Naive, which is trained without additional weight and adheres to a strict 2:4 sparsity pattern, and AST-Boosted, which is trained with extra SLORB weight to trade memory for precision. We select k = 16 which adds a total of 12.5% of extra parameter. We locate best hyper-parameters through grid search. The best hyper-parameters and training details are presented in Appendix A.2.\nData. For training smaller models in the OPT and GPT2 families, we used the C4 dataset [40]. For the LLAMA2-7B models, we employed a more comprehensive dataset named RedPajama- v1\u00b9, which comprises data from seven domains: CommonCrawl, C4, Github, Wikipedia, Books, ArXiv, and StackExchange. Additionally, we utilized the dynamic batching feature provided in the ShearedLLAMA codebase. We adhered to the train-eval split presented in ShearedLLAMA.\nBaseline. We compare our methods with other models featuring N:M sparsity, including SparseGPT[9] and Wanda [45]. Both Wanda and SparseGPT require calibration data. To ensure fairness, we use the calibration data provided in their codebase."}, {"title": "6.2 Language Modeling", "content": "In Table 2, we compared the perplexity results of our sparse models with the Wanda and SparseGPT baselines. We employed the same procedure as described in their original papers to obtain any results that were not provided.\nOur method, AST-Naive, which maintains the same sparsity pattern as the baseline, can significantly outperform previous methods by a large margin across various models. For instance, on smaller models like OPT-125M, where previous methods completely fail, AST-Naive can reduce the perplexity margin to 2.48, and AST-Boosted can further achieve an almost negligible margin of 0.92. For larger models, AST can still significantly reduce the gap; for the LLAMA2-7B model, our AST-Boost method achieved a perplexity increase of only 0.59. It should be noted that our sparse models consistently outperform smaller dense models with a similar parameter count, suggesting a promising approach for obtaining parameter-efficient models."}, {"title": "6.3 Zero-shot and Few-shot Result", "content": "In Table 3, we present the results of zero-shot and few-shot accuracy. We have chosen the results reported in Wanda [45] as our baseline, where the models were trained either through LoRA fine- tuning or full parameter retraining, with masks frozen. Although not specifically reported, their computational cost is presumed to be similar to ours; Wanda trained for three days on 4 GPUs, while we trained for approximately two days on 8 GPUs. Our method performed best in most of the tasks. The primary reason for tasks where our models were outperformed is mainly attributed to the differences in the dense models used for training-Wanda used LLAMA, while we used LLAMA2-rather than differences in training methods.\nIn Table 4 we present the results for five-shot MMLU, as Wanda did not report results with training, we used results from one-shot pruning methods to be our baseline. Results show that one-shot pruning significantly loss zero-shot ability while our methods can manage to recover most of the lost generalization ability."}, {"title": "7 Conclusion", "content": "In this paper, we present a novel training pipeline for semi-structured sparse models named Adaptive Sparse Trainer(AST). By distilling knowledge from the pruned model weights, AST mitigates overfit- ting and ensures a stable training process. Moreover, it enhances the identification of optimal lottery tickets (e.g., masks) by allowing the model to adaptively train masks. Additionally, we demonstrate that incorporating extra well-initialized parameters can further improve model performance with only a minimal increase in the memory footprint. Our work significantly narrows the precision gap in"}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Proof for Upper Bound of Combination Number", "content": "Proofs are sourced from Stack Exchange 2. We have the follow equation:\n\\(\\binom{2n}{n} = \\frac{(2n)!!}{n!n!} = \\frac{(2n-1)!!}{(2n)!!} \\prod_{i=1}^{n} (1-\\frac{1}{2i})\\)\nWe square both sides of the equation:"}, {"title": "A.2 Hyperparamters", "content": "In this section, we present the hyper-parameters and the number of tokens used to train our model in Table 5."}, {"title": "A.3 Model Compression Using AWQ", "content": "In this section, we provide the perplexity results for quantized models using AWQ. The results in Table 6 show that our model can maintain performance even when extremely compressed. According to the results, the jointly compressed models using AWQ and AST can achieve state-of-the-art performance in extremely compressed domains."}, {"title": "A.4 SLORB Initialization", "content": "We conducted an ablation study on different initialization methods, training all models on GPT2 using SLORB with specified computation limits of 1B, 2.5B, and 5B tokens\u2014each ensuring respective convergence. The results are provided in Table 7. 'Mean' refers to the method discussed in the paper, while 'Zero' denotes initialization of both matrices from zero."}, {"title": "A.5 Flip Rate During Training", "content": "In this section, we compare the flip rates and initial flip rates of static SR-STE and Annealing SR-STE. In our experiments, we update the mask and calculate the flip rate and initial flip rate"}, {"title": "A.6 Broader Impact", "content": "AST can convert a densely pre-trained model into a 2:4 sparse model using weaker datasets without significant loss of model capacity. This 2:4 sparse model can then be used for accelerated inference. This enables the deployment of large-scale models at a lower cost or with increased speed, and potentially allows larger models to run on less powerful hardware. However, this technique could also be used to train models that generate harmful content, such as content that violates AI ethics."}]}