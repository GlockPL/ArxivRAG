{"title": "CADC: Encoding User-Item Interactions for Compressing Recommendation Model Training Data", "authors": ["Hossein Entezari Zarch", "Abdulla Alshabanah", "Chaoyi Jiang", "Murali Annavaram"], "abstract": "Deep learning recommendation models (DLRMs) are at the heart\nof the current e-commerce industry. However, the amount of train-\ning data used to train these large models is growing exponentially,\nleading to substantial training hurdles. The training dataset con-\ntains two primary types of information: content-based informa-\ntion (features of users and items) and collaborative information\n(interactions between users and items). One approach to reduce\nthe training dataset is to remove user-item interactions. But that\nsignificantly diminishes collaborative information, which is cru-\ncial for maintaining accuracy due to its inclusion of interaction\nhistories. This loss profoundly impacts DLRM performance. This\npaper makes an important observation that if one can capture the\nuser-item interaction history to enrich the user and item embed-\ndings, then the interaction history can be compressed without los-\ning model accuracy. Thus, this work, Collaborative Aware Data\nCompression (CADC), takes a two-step approach to training dataset\ncompression. In the first step, we use matrix factorization of the\nuser-item interaction matrix to create a novel embedding repre-\nsentation for both the users and items. Once the user and item em-\nbeddings are enriched by the interaction history information the\napproach then applies uniform random sampling of the training\ndataset to drastically reduce the training dataset size while mini-\nmizing model accuracy drop. The source code of CADC is available\nat https://anonymous.4open.science/r/DSS-RM-8C1D/README.md.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning recommendation models (DLRM) play a pivotal role\nin enhancing user experience, by suggesting new and pertinent\ncontent, across numerous online platforms. Companies like Meta,\nGoogle, Microsoft, Netflix, and Alibaba employ these sophisticated\nmodels for a range of services, including personalizing and rank-\ning Instagram stories [17], video suggestions on YouTube [6], mo-\nbile app recommendations on Google Play [4], personalized news\nand entertainment options [7, 29], and tailored product recommen-\ndations [37]. Additionally, tasks such as Newsfeed Ranking and\nSearch are also built upon DNNs [8, 9, 23, 28], further exemplify-\ning the critical role these models play in content discovery and user\nengagement.\nThe essential role of DLRMs in generating revenue for many\ninternet companies has led to their marked increase in complex-\nity and size. From 2017 to 2021, the Meta's DLRM model size es-\ncalated 16-fold, requiring terabytes of model weights [21]. Addi-\ntionally, the need for memory bandwidth to manage these mod-\nels increased almost 30-fold[27]. This growth translates to recom-\nmendation models consuming over 50% of training and 80% of AI\ninference cycles [1, 9, 16, 22, 36]. As the model size grows corre-\nspondingly the training dataset size has also exploded in size. The\ntraining dataset consists of user item interactions. Thus the system\ninfrastructure, such as GPU and CPU count, total system memory,\nto support these models has grown by up to 2.9 times in just a few\nyears [20, 31].\nOne aproach to reduce the training costs is to reduce the train-\ning dataset size. There are also orthogonal approaches to reduce\nthe model size but this work focuses on reducing the training dataset\nsize. DLRMs derive value from two primary types of information\nwithin large datasets: content-based information (features of users\nand items) and collaborative information (interactions between users\nand items). Removing interactions from large datasets significantly\ndiminishes collaborative information, which is crucial for main-\ntaining accuracy due to its inclusion of interaction histories. This\nloss profoundly impacts DLRM performance."}, {"title": "2 COLLABORATIVE AWARE DATA\nCOMPRESSION", "content": "Let Dfull denote the entire training dataset, consisting of users U\nand items V. Our objective is to train a base TTNN model, repre-\nsented by MTTNN, on Dfull. Due to the large size of Dfull, direct\ntraining is impractical. To address this, we create Dsel, a subset in\nwhich interactions are randomly selected from Dfull to ensure that\nit includes a representative sample of the original U and V This\nselection process is designed to preserve the statistical properties\nand data distribution of Dfull, thereby reducing the computational\ndemands of training MTTNN without compromising the integrity\nof the dataset's inherent structure.\nWe build on the observation that collaborative information present\nin user-item interactions must be captured for ensuring model ac-\ncuracy when reducing dataset size. To address this, we introduce\nthe CADC technique. This method involves training a compact col-\nlaborative filtering model, specifically Matrix Factorization (MF),\non the collaborative information residing in Dfull to generate pre-\ntrained embeddings for U and V based on their complete in-\nteraction profiles. These embeddings are then integrated into the\nMTTNN. Incorporating these pre-trained weights, which encapsu-\nlate the entire collaborative information from Dfull, allows MTTNN\nto access comprehensive interaction data while only being trained\non a significantly smaller subset, Dsel. This strategic integration\ndramatically mitigates the adverse effects of training data filter-\ning and preserves model accuracy by maintaining vital collabora-\ntive information inside the DLRM. The ensuing sections detail the\nCADC methodology."}, {"title": "2.1 Pre-training Embedding Vectors", "content": "To encapsulate the entire collaborative information within Dfull\ninto a set of embedding vectors, we employ MF, a well-regarded\nand computationally efficient method in collaborative filtering. Given\nthe massive size of Dfull, the training methodology must not only\nbe computationally efficient but also capable of capturing the dy-\nnamics of user-item interactions with high fidelity. This method ef-\nficiently captures the dynamics of user-item interactions by reduc-\ning the high-dimensional interaction space into a lower-dimensional,\ncontinuous feature space.\nIn MF, we construct two separate embedding tables: one for\nusers and another for items. Each user and item is represented as\na vector, u\u00a1 and vj, in this latent feature space. To enhance the\nmodel's ability to capture individual preferences and item quali-\nties, we incorporate a bias term for each user and item into their re-\nspective vectors. The last element of each vector, ui,bias and vj,bias,\nserves as this bias term. The interaction between user i and item j\nis formulated as follows:\n$\\begin{equation*}\n\\widehat{y}_{MF}(i, j) = \\sigma (\\langle u_{i}', v_{j}' \\rangle + u_{i,bias} + v_{j,bias} + b)\n\\end{equation*}$\nwhere u and v'; represent the vectors u\u012f and vj excluding their\nrespective bias terms ui,bias and vj,bias. b represent the global bias\nterm. o is the sigmoid function.\nTo optimize these embeddings, we employ binary cross-entropy\nloss. Due to the implicit feedback nature of the datasets and scarcity\nof positive interactions, we implement negative sampling to bal-\nance the labels' distribution. Specifically, we generate Dneg, a sub-\nset of negative interactions, to counteract the sparsity of the data\nwhere most labels are implicitly zero. The loss formulation, which\nincorporates both sets of interactions, is as follows:\n$\\begin{equation*}\nL_{MF} = \\sum_{(i,j) \\in D_{full}} log(\\widehat{y}_{MF} (i, j)) - \\sum_{(i,j) \\in D_{neg}} log (1 - \\widehat{y}_{MF} (i, j))\n\\end{equation*}$"}, {"title": "2.2 Integrating Enhanced Embedding Vectors\ninto DLRM", "content": "In our approach, we employ the TTNN as a variant of the DLRM,\nwhich processes user and item features through distinct pathways.\nEach pathway comprises a Multi-Layer Perceptron (MLP): the user\ntower, Tuser, and the item tower, Titem. Specifically, Tuser processes\na concatenated vector of the corresponding user identifier and its\nfeatures, Fuser,i = [iduser, i; featuresuser,i], while Titem handles a\nsimilar vector for items, Fitem,j = [iditem,j; featuresitem,j].\nThe interaction between a user i and an item j is modeled by\nthe dot product of the embedding vectors output by each tower,\nfollowed by a sigmoid transformation to compute the prediction\nscore. This is formally expressed as:\n$\\begin{equation*}\n\\widehat{y}_{TTNN}(i, j) = \\sigma(T_{user} (F_{user,i})^{T} \\cdot T_{item} (F_{item, j}))\n\\end{equation*}$\nwhere o denotes the sigmoid function.\nThe enriched embeddings obtained from our pre-training step\nusing MF, u\u00a1 and vj, are used to initialize and freeze the iden-\ntifiers iduser and iditem within Fuser,i and Fitem,j with these vec-\ntors. Consequently, the TTNN's identifier embedding tables be-\ncome non-trainable, and Tuser and Titem now process the inputs\nFuser,i = [ui; featuresuser,i] and Fitem,j = [vj; featuresitem,j] re-\nspectively. MTTNN is then trained on Dsel, leveraging the com-\nprehensive interaction dynamics encapsulated within Dfull. This\nsophisticated integration not only leverages the depth of neural\nnetworks but also harnesses the breadth of collaborative filtering,\nensuring a robust and accurate prediction mechanism."}, {"title": "3 EXPERIMENTAL SETUP", "content": "Our experiments were designed to assess the efficacy of the CADC\nacross three distinct datasets, each with unique characteristics: Movie-Lens 1M\u00b9, MovieLens 10M\u00b2, and Epinions\u00b3. For evaluation, the last\ntwo interactions of each user were reserved for validation and test-\ning. Each dataset was subjected to training over 100 epochs on\n10% of its interaction data using a TTNN architecture, with embed-\nding sizes set to 96. Before training the TTNN, an MF model was\ntrained on all interactions within each dataset for 100 epochs, with\nan embedding size of 95. This size aligns with the TTNN's effective\nsize when incorporating additional bias terms for users and items.\nThe optimization of MF employed an alternating scheme using the\nAdam optimizer: item embeddings were fixed while updating user\nembeddings, and vice versa.\nPerformance was evaluated using Hit Rate at 10 (HR@10) and\nNormalized Discounted Cumulative Gain at 10 (NDCG@10), met-\nrics that assess accuracy and ranking quality. Additionally, the train-\ning time for each dataset was recorded in seconds to evaluate the\ntime efficiency of the method. In all scenarios utilizing CADC, a\nsmall model was first trained on the entire dataset, after which the\nembeddings were transferred to the TTNN and frozen, as detailed\nin Section 2."}, {"title": "3.1 Baselines", "content": "To evaluate the effectiveness of the CADC, it was benchmarked\nagainst various methods:\n\u2022 Random: This baseline trains the TTNN on the filtered dataset\nwithout any sophisticated data compression or embedding\noptimization techniques, serving as a naive control.\n\u2022 Long-Tail Item Recommendation Techniques (Over-Sampling,\nUnder-Sampling, LogQ) [34]: These methods are incorporated\nas baselines to address challenges posed by data filtering,\nwhich often exacerbates the long-tail problem in recommen-\ndation systems.\n\u2022 CADC-MLP [10]: This variant employs an MLP as the in-\nteraction function between user and item embeddings, re-\nplacing the traditional dot-product approach used in MF. It\noffers a more complex interaction model, making it compu-\ntationally more intensive than traditional MF."}, {"title": "4 RESULTS", "content": "Table 1 presents the performance analysis of the recommendation\nsystems utilizing the CADC method compared to other approaches"}, {"title": "5 SENSITIVITY ANALYSIS", "content": "This section examines the impact of various factors on the per-\nformance of the CADC method, specifically focusing on the data\nfiltering ratio and different embedding integration techniques. All\nexperiments were conducted using the MovieLens-1M dataset.\n\u2022 Data Filtering Ratio: Defined as the ratio of the number\nof interactions in Dfull to those in Dsel. For instance, a data\nfiltering ratio of 50 means that the DLRM is trained on 2%\nof the complete dataset. Figures illustrate that as the data\nfiltering ratio increases, indicating more substantial data re-\nduction, the decline in ranking performance becomes pro-\ngressively less pronounced. This demonstrates CADC's ca-\npability to maintain model accuracy effectively, even with\nsignificantly reduced datasets.\n\u2022 Embedding Integration Techniques: Analysis of differ-\nent integration techniques within the CADC framework re-\nveals varied impacts on performance:"}, {"title": "6 RELATED WORKS", "content": "Sampling Interaction Data. Data Sampling is crucial in recommen-\ndation systems for extracting hard negative samples and assess-\ning algorithms. It's been employed through methods such as ran-\ndom sampling, leveraging the underlying graph structure [19, 33],\nand specific techniques such as similarity search [11] and stratified\nsampling [3]. Sampling is also crucial in assessing recommenda-\ntion algorithms [2, 15]. Additionally, it's also useful for createing\nsmaller subsets of large datasets for purposes such as quick testing\nand algorithmic comparisons [26].\nCoreset Selection. Coreset selection identifies data subsets that\nrepresent the full dataset's quality. Methods include score-based\napproaches, which select data based on criteria like forgetting fre-\nquency [30], loss value [12, 14], and prediction uncertainty [5], and\ngradient-based approaches, which estimate the dataset's gradient[18,\n25, 32]. These model-specific methods require computation at each\ndata point, making them impractical for large datasets due to high\ncomputational demands. Our approach simplifies this by training\na very basic computational model once, irrespective of the DLRM\nand content features to be used. This one-time training embeds\ncollaborative information efficiently, circumventing the computa-\ntional challenges of traditional coreset selection methods and facil-\nitating scalable training for large datasets.\nData Distillation. Data distillation synthesizes compact data sum-\nmaries, primarily in continuous domains like images. These tech-\nniques distill the essential knowledge of an entire dataset into a\nsignificantly smaller, synthetic summary [24, 35]. However, these\ntechniques have predominantly focused on continuous data like\nimages, a recent approach extended these methods to synthesize\nfake graphs, assuming pre-existing node representations, which\nlimits their applicability to recommendation data [13]. Sachdeva\net al. [26] adapt data distillation for collaborative filtering by gen-\nerating high-fidelity, compressed data summaries specifically for\nuse with infinitely-wide autoencoders (0\u221e-AE). This method is de-\nsigned exclusively for\u221e-AE applications in collaborative filtering\nand does not integrate content-based features. In contrast, our pro-\nposed method is developed for DLRMs like TTNN, incorporating"}, {"title": "7 CONCLUSION", "content": "This study introduces CADC, a pioneering approach designed to\nefficiently train DLRMs on large-scale datasets without substan-\ntially impacting model accuracy. Our findings demonstrate that by\nemploying pre-trained embeddings that encapsulate comprehen-\nsive interaction data, CADC can significantly reduce the volume\nof data needed for training while preserving the collaborative in-\nformation essential for maintaining high prediction quality. Tested\nacross datasets like MovieLens 1M, MovieLens 10M, and Epinions,\nCADC not only outperforms traditional training methods in terms\nof efficiency and scalability but also maintains a high level of ac-\ncuracy, closely approximating the performance of models trained\non full datasets. Specifically, CADC has proven to mitigate the im-\npact of data reduction on model performance, reducing training\ntimes dramatically without corresponding losses in effectiveness.\nOur research contributes to the broader field of recommendation\nsystems by providing a scalable solution that addresses the twin\nchallenges of maintaining high data throughput and model accu-\nracy in the face of exponentially growing data sizes. It opens new\navenues for future research, particularly in exploring more com-\nplex models and integration techniques that could further enhance\nthe efficiency and effectiveness of recommendation systems."}]}