{"title": "Scaling, Control and Generalization in Reinforcement Learning Level Generators", "authors": ["Sam Earle", "Zehua Jiang", "Julian Togelius"], "abstract": "Procedural Content Generation via Reinforcement\nLearning (PCGRL) has been introduced as a means by which\ncontrollable designer agents can be trained based only on a set\nof computable metrics acting as a proxy for the level's quality\nand key characteristics. While PCGRL offers a unique set of\naffordances for game designers, it is constrained by the compute-\nintensive process of training RL agents, and has so far been\nlimited to generating relatively small levels. To address this issue\nof scale, we implement several PCGRL environments in Jax so\nthat all aspects of learning and simulation happen in parallel on\nthe GPU, resulting in faster environment simulation; removing\nthe CPU-GPU transfer of information bottleneck during RL\ntraining; and ultimately resulting in significantly improved train-\ning speed. We replicate several key results from prior works in\nthis new framework, letting models train for much longer than\npreviously studied, and evaluating their behavior after 1 billion\ntimesteps. Aiming for greater control for human designers, we\nintroduce randomized level sizes and frozen \u201cpinpoints\" of pivotal\ngame tiles as further ways of countering overfitting. To test the\ngeneralization ability of learned generators, we evaluate models\non large, out-of-distribution map sizes, and find that partial\nobservation sizes learn more robust design strategies.", "sections": [{"title": "I. INTRODUCTION", "content": "In procedural content generation via reinforcement learning\n(PCGRL), the process of iterative game design is frames as\na markov decision process, and reinforcement learning (RL)\nagents are trained to generate game content. Instead of learning\nto play a game by taking actions, observing states, and getting\nrewards, these agents learn to generate (parts of) a game\nby taking actions, observing states, and getting rewards. The\nactions edit the content artifact, and the reward is based on\nthe quality of the artifact that is being created.\n The advantage of PCGRL is that you can use it to create\nnot just game content, but game content generators. Compared\nto search-based approaches, this means that almost all the\ncompute is front-loaded; first you train the generator, then\ninference is fast and cheap. This makes it suitable for runtime\nuse in games. Compared to supervised or self-supervised\nlearning, PCGRL don't need any existing content to train on.\nThis makes it suitable for use for games where any content\nhas yet to be produced.\n Despite these considerable potential advantages, PCGRL\nhas only limited uptake since it was first proposed in [1].\nThis could be due to the difficulty of designing good reward\nfunctions, the tendency to overfit to single solutions, the long\ntraining time, and the problems with scaling to produce larger-\nsize levels and other content. In this paper, we propose and\nevaluate several modifications to the basic PCGRL formulation\naimed at rectifying some of these issues.\n Two novel elements we propose are randomizing level size\nduring training, and pinpointing locations of key elements.\nBoth of these interventions function to limit overfitting by\nenforcing closed-loop policies, in other words, the agent must\ntake its observations into account and cannot rely on rote-\nlearning parts of levels. These add new degrees of controlla-\nbility in addition on top of conditioning on high-level features\nintroduced in [2].\n We also examine the effects of systematically changing\nthe size of the agent's observation space. In the original\nPCGRL formulation, the observation window typically covers\nthe whole level. From reinforcement learning experiments in\nvarious domains, we know that observation and structure can\nhave large effects on overfitting and scalability. We hypothe-\nsize that the same is true for PCGRL, and that we can improve\ngeneralization and scalability by choosing adequate observa-\ntion windows. This hypothesis is largely confirmed by our\nexperiments. We find that smaller observation windows always\nincrease generalization to new level sizes. On a task involving\npinpoints and randomized map shapes during training, these\nmore local models additionally perform comparably or better\nin-distribution.\n To offset the high computational cost of training content-\ngenerating agents, we reimplement the standard PCGRL li-\nbrary in jax [3], a framework that allows a high degree of\nparallelization using the GPU to simulate the environment,\nresulting for 15\u00d7 speedups during training, making it feasible\nto experiment with longer training times.\nIn sum, our contributions are as follows\n\u2022\n We re-implement the PCGRL code-base in jax, making\nit practical to scale PCGRL to larger and more complex\ndomains.\n\u2022\n We add new features-variable map shapes and varied\n(frozen) placement of pivotal \"pinpoint\" tiles-to make"}, {"title": "II. BACKGROUND", "content": "Video games featuring some form of content generation,\noften level generation, have existed since the early 1980s.\nBeneath Apple Manor, Rogue, and Elite were the early poster\nchildren of early game content generation. PCG has become a\nmainstay in modern games, with titles such as No Man's Sky,\nHades, and the Civilization series that rely heavily on some\nform of runtime content generation.\n Academic research in content generation dates back to\nthe early 2000s [4]. Much of the early research focused\non search-based approaches [5]. Search-based approaches are\nvery versatile, but the computational demands at generation\ntime can be high, making such approaches harder to use\nfor runtime generation. Constraint satisfaction approaches [6],\n[7] were also given considerable attention. While constraint\nsatisfaction approaches can be very powerful, it imposes\nparticular constraints on the shape of the content.\n Supervised and self-supervised approaches to PCG started\nbeing explored seriously at the dawn of the deep learning\nera [8]\u2013[10]. A variety of these machine learning methods have\nbeen applied to game content generation, including Generative\nAdversarial Networks [11], LSTM networks [12], and Markov\nmodels [13]. However, these methods generally have large\nrequirements on training data. This begs the question that\nhas been called the fundamental tension of PCGML: if these\nmethods only work well with enough existing content, why\nwould you need to generate it? [14].\n Reinforcement learning approaches to PCG are more recent,\nfirst proposed by [15] and [1]. There are significant advantages\nto PCGRL over existing approaches:\n1) No training data is necessary\n2) Generators are very fast during inference time\n3) The generator is iterative, allowing mixed-initiative so-\nlutions [16]\nWith these advantages comes a unique set of considerations.\nIn principle, the same kind of evaluations used as fitness func-\ntions for search-based PCG can be used as reward functions\nin PCGRL. However, due to longer training times, a computa-\ntionally lightweight reward function is needed in settings with\ndense reward. There is also a tendency to mode collapse via\noverfitting, which can be counteracted via limiting the number\nof changes the model can make, or introducing conditional\ninputs [2]. Because of these constraints, scaling PCGRL to\nlarger-sized levels or artefacts has proven a challenge [17].\n One type of modification to the basic PCGRL formula\nexplored here concerns the size and shape of the observa-\ntion. This draws on earlier results showing that limiting the\nobservation size and aligning it properly can greatly help with\ngeneralization [18]."}, {"title": "III. METHODS", "content": "To train RL level generators, we use Proximal Policy\nOptimization (PPO) [19] with the same reward function and"}, {"title": "A. Training", "content": "neural network as in [1], [2], using the \u201cnarrow\u201d representation\nof observations and actions. In each episode, the model is\nrewarded by minimizing the loss value between current state\nand target state (where the user or a training curriculum can\nvary target solution path length or nearest enemy). The agent\nobserves an egocentric patch of the level (in which the board\nmay be padded to allow for global observations), and may\nchange the state of its current tile, then is moved to an adjacent\ntile in an iterative scan of the map."}, {"title": "B. Task", "content": "We extend PCGRL [1], in which level design is framed\nas a reinforcement learning task. This task is decomposed\ninto a \"problem\"-the level design task at hand and a\n\"representation\"-the interface via which the agent edits the\nlevel. In this paper, we adapt the narrow representation to\nsupport new features, and use the binary maze and dungeon\nproblems and as toy tasks with which to verify our proposed\nmethod. We initialize the map with the elements of the\ntasks using the weighted uniform distribution as in [1], or\nsimply leave it empty, depending on the configuration of the\nenvironment. At each timestep, we compute the metrics of\ninterest (i.e. the diameter and number of connected empty\nregions) if the agent has made any modification to the map.\na) Binary domain: The agent's goal is to create a maze\nwith maximum diameter (i.e. the longest shortest path between\nany two points in the maze). There are only two types of\ntile in the maze: wall and air. We approximate the diameter\nby applying Dijkstra's algorithm twice\u00b9 and the number of\nconnected components using a flood fill algorithm.\n\u00b9First, we select a random empty tile x in the maze. We apply Dijkstra's\nalgorithm to find the longest shortest path of which x is an endpoint. We then\ntake y, the other endpoint of this shortest path, and apply Dijkstra's algorithm\nstarting from y to find the longest shortest path of which y is an endpoint,\nunder the assumption that y is an endpoint of the diameter."}, {"title": "b) Maze domain", "content": ": In this problem, the RL agent needs\nto use \"wall\" and \"air\" tiles to create a traversable maze\nfrom one \"player\" tile to \"door\" tile in the game map. The\ngenerated maze should just have one connected component\nand the solution should be maximized."}, {"title": "c) Dungeon domain", "content": ": Expanding on the MAZE domain,\nwe consider a task in where the agent's goal is to create a\nplayable dungeon game level. The generated dungeon should\nhave a maximally long shortest path from player, to key, to\ndoor. There are 6 types of tile in dungeon problem: wall, air,\nenemy, key, door, and player. There can be 2 to 5 enemy\ntiles, and only one key, door, and player tile. Additionally, the\ndistance between the player and the nearest enemy should not\nbe less than 4 tiles. We use Dijkstra's algorithm to find the\nshortest player-key-door path."}, {"title": "d) Pinpoint tiles", "content": ": We add additional complexity to our\ntasks, modifying the MAZE and DUNGEON domains so that\nimportant tiles can be frozen in arbitrary positions on the\nboard. This freezing can be done programmatically during\ntraining, or at inference time via a human designer. These\nfixed-position tiles provide a more controllable way to prevent\nagents fromoverfitting to a single optimal output, and allow the\nproblem to become open-looped."}, {"title": "e) Randomized map shapes", "content": ": To train agents that scale to\nlarger and more variable map shapes w.r.t those seen during\ntraining, we expose the agent to a variety of map shapes\nthroughout training. So, when training on a 16 \u00d7 16 map\n(as in prior PCGRL work) we randomly sample a rectangular\nmap shape from a uniform distribution. The dimensions of this\nshape are bounded by 3 \u00d7 3 and 16 \u00d7 16. Another alternative\nmight be to progressively train on maps of increasing size.\nAlthough intuitive, this approach introces two failrue modes.\nFirst, it risks catastrophic forgetting of smaller map sizes seen\nearlier during training. Conversely, it may incentivise the agent\nto learn faulty representations on smaller maps that do not\ntransfer to larger ones."}, {"title": "C. Jax implementation", "content": "We use the jax python library [3] to implement our PCGRL\nenvironments and training algorithm. Jax exposes a wide vari-\nety of tensor-based operations to the user, mirroring much of\nthe functionality of numpy and/or libraries like pytorch or ten-\nsorflow, compiling operations to XLA just-in-time. Provided\nthat the size of tensors is fixed at compile-time, and with some\nlimitations on logical operations like if conditions, Jax can\nimprove runtime efficiency by \u201cfusing\u201d lower-level operations.\nOur Jax implementation of PCGRL builds on gymnax [20] and\nthe PureJaxRL code base [21], which implements a number\nof simple embodied game-playing environments in Jax.\n To pathfind in jax, we can flood activation out to adjacent\ntraversible tiles in parallel across the board using convolutional\nkernels. Similar logic can be used to compute the number of\nregions."}, {"title": "IV. RESULTS", "content": "Speed Comparison: In Table I we calculate the FPS\nduring training of the prior CPU implementation (splitting\nenvironments across 11 cores), and jax-pcgrl respectively.\nWe find that the jax implementation leads to speedups of\nover 15\u00d7 relative to the CPU implementation. While the\nCPU implementation plateaus as the number of environments\nreaches between 50-100, the FPS of the jax version continues\nto increase significantly up to at least 600 workers.\n In Table II, we calculate the speed of our jax implementa-\ntions of the BINARY MAZE and DUNGEON domains in frames\nper second given actions from a random agent, achieving 45k\nFPS."}, {"title": "a)", "content": "Observation size: In the following experiments, we\ntrain the agents in a fixed size map setting and evaluate them\non different map shapes. From these thorough experiment con-\nfigurations, we can answer the question about how observation\nsize influence the model performance on in-distribution and\nout-distribution map and the how well the agents generalize.\n In Table III we evaluate the performance of the CONV\nmodel on the DUNGEON task. We evaluate different obser-\navtion input of the model on varying maximum random map\nwidths and random per-episode map shapes. Performance of\nmodels with smaller observation windows is slightly weaker\nthan models with full observations on in-distribution max-\nimum map widths. But smaller observations lead to better\ngeneralization to larger maximum map widths, despite these\nmodels having fewer parameters then their fully-observing\ncounterparts.\n In Table IV, we evaluate the CONV model on the DUNGEON\ndomain with random target path length for learning control-\nlability. Looking at evaluation scenarios without randomized\nper-episode map shapes, we see that on in-distribution 16 \u00d7 16\nmaps, global observations perform the best. On larger maps,\nhowever, more local partial observations generalize better,\ndespite the fact that models with smaller observation windows\nhave significantly fewer parameters. When evaluating with\nrandomized per-episode map shapes, however, models of all\nobservation sizes generalize comparably.\n We repeat this experiment in Table V, where we systemati-\ncally add hidden nodes to each layer of each network until\nit has almost as many (but no more) learnable parameters\nthan the model with full observations\u2014effectively separating\nthe effect of partial observations from model size. Here, we\nsee that models with smaller observation size but comparable\nnumbers of parameters tend to outperform models with larger\nobservations across all maximum map sizes.\n In Figure 5, we plot the reward curves of the experiments\nin Table IV and Table V during training, and similarly observe\nthat the addition of learnable parameters to models with\nsmaller observation sizes improves their performance."}, {"title": "b)", "content": "Randomized map shapes during training: In Table VI,\nwe examine the performance of the CONV model on the MAZE\ndomain, with pinpoints (randomized fix placement of player\nand door tiles). In these experiments, we show how random-\nizing the map shape during training will affect the model\nperformance of generalization under different observation size.\n First, we look at evaluation of models on fixed square map\nsizes. In this setting, models exposed to similarly fixed square\nmap shapes during training tend to outperform those trained\non variable per-episode map shapes. Next, we evaluate models\nwhile randomly sampling shapes within these maximum sizes\non each evaluation episode (right side of the table). Models\ntrained without randomized map shapes are broadly unable to\nadapt to variable map shapes during evaluation\u2014with the only\nconsistent positive reward in this setting coming from models\nwith the smallest observation window (8 \u00d7 8). Of the models\ntrained on variable map shapes, smaller observation windows\ngenerally outperform larger ones."}, {"title": "V. DISCUSSION", "content": "To the best of our knowledge, no prior jax RL environments\ninvolving path-finding, which could also be a useful addition\nto player environments (e.g. to simulate enemy navigation in\n22]). If the speedups Table II of our jax implementations of\nPCGRL environments are less than those of other, still simpler\nenvironments, this might come from the added complexity of\nour path-finding implementation.\n Table VI shows us that in general, smaller observation\nwindows lead to higher performance on out-of-distribution\nsettings, either for larger scale or per-episode map shapes. We\ncontend that this is likely a result of overfitting under global\nobservation: models that are accustomed to seeing the padding\nof unique \"border\" tiles surrounding the effective map region\nare disrupted when, during evaluation on larger maps, these\ntiles are suddenly not present in their egocentric observations.\nThese models are likely using the placement of these border\ntiles to infer global coordinates, allowing it to consistently\nconstruct one optimal global level (or a set of global levels,\nwhen dealing with randomized map shapes, controllable path-\nlength metrics, or frozen \"pinpoint\" tiles). With such global\ncoordinates at hand, these levels can theoretically be generated\nin one shot (or one scan over the board).\n By restricting the observation space, on the other hand,\nmodels could only infer global coordinates by editing the\nentire board multiple times, communicating relative coordi-\nnates via patterns that cascade across the map in an iterative\nway. The fact that these local-observation models perform\nbetter out of distribution suggests that such an approach to\niteratively passing local information across the board leads\nto more general representations and strategies for designing\ngood levels. In other words, these constrained models are less\nlikely to memorize what one or a set of optimal levels should\nlook like, and instead may learn general strategies for how to\nimprove or modify maps along certain axes.\n Meanwhile, models trained on fixed-size square maps are\nnot able to adapt well to per-episode variation of map shape.\nModels with full observations exhibit particularly pronounced\nfailure in this case, and our reasoning would expect that a\nmodel that has observed only square border shapes is disrupted\nwhen it observes rectangular shapes during evaluation. But\nmodels with local observations are also thrown off by variable\nper-episode map shapes, suggesting that their strategies for\niteratively transmitting local information are not robust to non-\nsquare map shapes. Conversely, maps trained on variable per-\nepisode map shapes do not quite attain the performance of\nmodels trained strictly on fixed-size square maps even on\nout-of-distribution sizes. We expect that this is merely a result\nof insufficient training time given the larger task distribution\non which these models are trained, and that further training\nwould allow them to better cover this distribution with good\nperformance.\n The reward curves in Figure 3, along with the in-distribution\ncolumns of Table VI, reveal that when per-episode map shapes\nare randomized, models taking full observations seem to lose\nthe advantage they have when map shapes are fixed. This\nwould seem to suggest that knowing the overall shape of\nthe map is actually a disadvantage, even on in-distribution\ntasks, when this distribution is diverse enough. In other words,\nwe hypothesize that models that are forced to learn general\nlevel-editing strategies, adaptive to a range of possible map\nshapes, arrive more quickly at optimal performance on the\nset of training map shapes, while models that have access\nto this information are effectively distract, and drawn away\nfrom more general and robust strategies. Or, when the training\ndistribution is wide enough, limiting a model's direct access\nto information about precisely which training task it is in at a\ngiven moment can render it more effective, because this model\nis forced to find similarities between tasks and effectively learn\ncompressed representations of this distribution."}, {"title": "VI. CONCLUSION", "content": "The over 15\u00d7 speedups achieved by our jax reimple-\nmentions of PCGRL environments allow us to train models\nfor many more timesteps (1 billion) than in previous works\n(around 200 million). By randomizing map shapes and the\nplacement of pivotal items in initial layouts, we allow for\ntraining of more robust and controllable level generator agents.\nIn our experiments, we find that limiting the observation\nwindow of trained agents leads to stronger generalization.\nBy evaluating on held-out initial map layouts and constraints,\npcgrl-jax can serve as a scalable benchmark for RL agents,\nwith real-world applications for human level designers."}]}