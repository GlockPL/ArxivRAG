{"title": "Scaling, Control and Generalization in Reinforcement Learning Level Generators", "authors": ["Sam Earle", "Zehua Jiang", "Julian Togelius"], "abstract": "Abstract-Procedural Content Generation via Reinforcement\nLearning (PCGRL) has been introduced as a means by which\ncontrollable designer agents can be trained based only on a set\nof computable metrics acting as a proxy for the level's quality\nand key characteristics. While PCGRL offers a unique set of\naffordances for game designers, it is constrained by the compute-\nintensive process of training RL agents, and has so far been\nlimited to generating relatively small levels. To address this issue\nof scale, we implement several PCGRL environments in Jax so\nthat all aspects of learning and simulation happen in parallel on\nthe GPU, resulting in faster environment simulation; removing\nthe CPU-GPU transfer of information bottleneck during RL\ntraining; and ultimately resulting in significantly improved train-\ning speed. We replicate several key results from prior works in\nthis new framework, letting models train for much longer than\npreviously studied, and evaluating their behavior after 1 billion\ntimesteps. Aiming for greater control for human designers, we\nintroduce randomized level sizes and frozen \u201cpinpoints\" of pivotal\ngame tiles as further ways of countering overfitting. To test the\ngeneralization ability of learned generators, we evaluate models\non large, out-of-distribution map sizes, and find that partial\nobservation sizes learn more robust design strategies.", "sections": [{"title": "I. INTRODUCTION", "content": "In procedural content generation via reinforcement learning\n(PCGRL), the process of iterative game design is frames as\na markov decision process, and reinforcement learning (RL)\nagents are trained to generate game content. Instead of learning\nto play a game by taking actions, observing states, and getting\nrewards, these agents learn to generate (parts of) a game\nby taking actions, observing states, and getting rewards. The\nactions edit the content artifact, and the reward is based on\nthe quality of the artifact that is being created.\n The advantage of PCGRL is that you can use it to create\nnot just game content, but game content generators. Compared\nto search-based approaches, this means that almost all the\ncompute is front-loaded; first you train the generator, then\ninference is fast and cheap. This makes it suitable for runtime\nuse in games. Compared to supervised or self-supervised\nlearning, PCGRL don't need any existing content to train on.\nThis makes it suitable for use for games where any content\nhas yet to be produced.\nDespite these considerable potential advantages, PCGRL\nhas only limited uptake since it was first proposed in [1].\nThis could be due to the difficulty of designing good reward\nfunctions, the tendency to overfit to single solutions, the long\ntraining time, and the problems with scaling to produce larger-\nsize levels and other content. In this paper, we propose and\nevaluate several modifications to the basic PCGRL formulation\naimed at rectifying some of these issues.\n Two novel elements we propose are randomizing level size\nduring training, and pinpointing locations of key elements.\nBoth of these interventions function to limit overfitting by\nenforcing closed-loop policies, in other words, the agent must\ntake its observations into account and cannot rely on rote-\nlearning parts of levels. These add new degrees of controlla-\nbility in addition on top of conditioning on high-level features\nintroduced in [2].\n We also examine the effects of systematically changing\nthe size of the agent's observation space. In the original\nPCGRL formulation, the observation window typically covers\nthe whole level. From reinforcement learning experiments in\nvarious domains, we know that observation and structure can\nhave large effects on overfitting and scalability. We hypothe-\nsize that the same is true for PCGRL, and that we can improve\ngeneralization and scalability by choosing adequate observa-\ntion windows. This hypothesis is largely confirmed by our\nexperiments. We find that smaller observation windows always\nincrease generalization to new level sizes. On a task involving\npinpoints and randomized map shapes during training, these\nmore local models additionally perform comparably or better\nin-distribution.\n To offset the high computational cost of training content-\ngenerating agents, we reimplement the standard PCGRL li-\nlibrary in jax [3], a framework that allows a high degree of\nparallelization using the GPU to simulate the environment,\nresulting for 15\u00d7 speedups during training, making it feasible\nto experiment with longer training times.\nIn sum, our contributions are as follows\n\u2022\n\u2022\nWe re-implement the PCGRL code-base in jax, making\nit practical to scale PCGRL to larger and more complex\ndomains.\nWe add new features-variable map shapes and varied\n(frozen) placement of pivotal \"pinpoint\" tiles-to make"}, {"title": "II. BACKGROUND", "content": "Video games featuring some form of content generation,\noften level generation, have existed since the early 1980s.\nBeneath Apple Manor, Rogue, and Elite were the early poster\nchildren of early game content generation. PCG has become a\nmainstay in modern games, with titles such as No Man's Sky,\nHades, and the Civilization series that rely heavily on some\nform of runtime content generation.\n Academic research in content generation dates back to\nthe early 2000s [4]. Much of the early research focused\non search-based approaches [5]. Search-based approaches are\nvery versatile, but the computational demands at generation\ntime can be high, making such approaches harder to use\nfor runtime generation. Constraint satisfaction approaches [6],\n[7] were also given considerable attention. While constraint\nsatisfaction approaches can be very powerful, it imposes\nparticular constraints on the shape of the content.\n Supervised and self-supervised approaches to PCG started\nbeing explored seriously at the dawn of the deep learning\nera [8]\u2013[10]. A variety of these machine learning methods have\nbeen applied to game content generation, including Generative\nAdversarial Networks [11], LSTM networks [12], and Markov\nmodels [13]. However, these methods generally have large\nrequirements on training data. This begs the question that\nhas been called the fundamental tension of PCGML: if these\nmethods only work well with enough existing content, why\nwould you need to generate it? [14].\n Reinforcement learning approaches to PCG are more recent,\nfirst proposed by [15] and [1]. There are significant advantages\nto PCGRL over existing approaches:\n1) No training data is necessary\n2) Generators are very fast during inference time\n3) The generator is iterative, allowing mixed-initiative so-\nlutions [16]\n With these advantages comes a unique set of considerations.\nIn principle, the same kind of evaluations used as fitness func-\ntions for search-based PCG can be used as reward functions\nin PCGRL. However, due to longer training times, a computa-\ntionally lightweight reward function is needed in settings with\ndense reward. There is also a tendency to mode collapse via\noverfitting, which can be counteracted via limiting the number\nof changes the model can make, or introducing conditional\ninputs [2]. Because of these constraints, scaling PCGRL to\nlarger-sized levels or artefacts has proven a challenge [17].\n One type of modification to the basic PCGRL formula\nexplored here concerns the size and shape of the observa-\ntion. This draws on earlier results showing that limiting the\nobservation size and aligning it properly can greatly help with\ngeneralization [18]."}, {"title": "III. METHODS", "content": "To train RL level generators, we use Proximal Policy\nOptimization (PPO) [19] with the same reward function and"}, {"title": "IV. RESULTS", "content": "In Table I we calculate the FPS\nduring training of the prior CPU implementation (splitting\nenvironments across 11 cores), and jax-pcgrl respectively.\nWe find that the jax implementation leads to speedups of\nover 15\u00d7 relative to the CPU implementation. While the\nCPU implementation plateaus as the number of environments\nreaches between 50-100, the FPS of the jax version continues\nto increase significantly up to at least 600 workers.\n In Table II, we calculate the speed of our jax implementa-\ntions of the BINARY MAZE and DUNGEON domains in frames\nper second given actions from a random agent, achieving 45k\nFPS.\n In the following experiments, we\ntrain the agents in a fixed size map setting and evaluate them\non different map shapes. From these thorough experiment con-\nfigurations, we can answer the question about how observation\nsize influence the model performance on in-distribution and\nout-distribution map and the how well the agents generalize.\n In Table III we evaluate the performance of the CONV\nmodel on the DUNGEON task. We evaluate different obser-\navtion input of the model on varying maximum random map\nwidths and random per-episode map shapes. Performance of\nmodels with smaller observation windows is slightly weaker\nthan models with full observations on in-distribution max-\nimum map widths. But smaller observations lead to better\ngeneralization to larger maximum map widths, despite these\nmodels having fewer parameters then their fully-observing\ncounterparts.\n In Table IV, we evaluate the CONV model on the DUNGEON\ndomain with random target path length for learning control-\nlability. Looking at evaluation scenarios without randomized\nper-episode map shapes, we see that on in-distribution 16 \u00d7 16\nmaps, global observations perform the best. On larger maps,\nhowever, more local partial observations generalize better,\ndespite the fact that models with smaller observation windows\nhave significantly fewer parameters. When evaluating with\nrandomized per-episode map shapes, however, models of all\nobservation sizes generalize comparably.\n We repeat this experiment in Table V, where we systemati-\ncally add hidden nodes to each layer of each network until\nit has almost as many (but no more) learnable parameters\nthan the model with full observations\u2014effectively separating\nthe effect of partial observations from model size. Here, we\nsee that models with smaller observation size but comparable\nnumbers of parameters tend to outperform models with larger\nobservations across all maximum map sizes.\n In Figure 5, we plot the reward curves of the experiments\nin Table IV and Table V during training, and similarly observe\nthat the addition of learnable parameters to models with\nsmaller observation sizes improves their performance.\n In Table VI,\nwe examine the performance of the CONV model on the MAZE\ndomain, with pinpoints (randomized fix placement of player\nand door tiles). In these experiments, we show how random-\nizing the map shape during training will affect the model\nperformance of generalization under different observation size.\n First, we look at evaluation of models on fixed square map\nsizes. In this setting, models exposed to similarly fixed square\nmap shapes during training tend to outperform those trained\non variable per-episode map shapes. Next, we evaluate models\nwhile randomly sampling shapes within these maximum sizes\non each evaluation episode (right side of the table). Models\ntrained without randomized map shapes are broadly unable to\nadapt to variable map shapes during evaluation\u2014with the only\nconsistent positive reward in this setting coming from models\nwith the smallest observation window (8 \u00d7 8). Of the models\ntrained on variable map shapes, smaller observation windows\ngenerally outperform larger ones."}, {"title": "V. DISCUSSION", "content": "To the best of our knowledge, no prior jax RL environments\ninvolving path-finding, which could also be a useful addition\nto player environments (e.g. to simulate enemy navigation in\n[22]). If the speedups Table II of our jax implementations of\nPCGRL environments are less than those of other, still simpler\nenvironments, this might come from the added complexity of\nour path-finding implementation.\n Table VI shows us that in general, smaller observation\nwindows lead to higher performance on out-of-distribution\nsettings, either for larger scale or per-episode map shapes. We\ncontend that this is likely a result of overfitting under global\nobservation: models that are accustomed to seeing the padding\nof unique \"border\" tiles surrounding the effective map region\nare disrupted when, during evaluation on larger maps, these\ntiles are suddenly not present in their egocentric observations.\nThese models are likely using the placement of these border\ntiles to infer global coordinates, allowing it to consistently\nconstruct one optimal global level (or a set of global levels,\nwhen dealing with randomized map shapes, controllable path-\nlength metrics, or frozen \"pinpoint\" tiles). With such global\ncoordinates at hand, these levels can theoretically be generated\nin one shot (or one scan over the board).\n By restricting the observation space, on the other hand,\nmodels could only infer global coordinates by editing the\nentire board multiple times, communicating relative coordi-\nnates via patterns that cascade across the map in an iterative\nway. The fact that these local-observation models perform\nbetter out of distribution suggests that such an approach to"}, {"title": "VI. CONCLUSION", "content": "The over 15\u00d7 speedups achieved by our jax reimple-\nmentions of PCGRL environments allow us to train models\nfor many more timesteps (1 billion) than in previous works\n(around 200 million). By randomizing map shapes and the\nplacement of pivotal items in initial layouts, we allow for\ntraining of more robust and controllable level generator agents.\nIn our experiments, we find that limiting the observation\nwindow of trained agents leads to stronger generalization.\nBy evaluating on held-out initial map layouts and constraints,\npcgrl-jax can serve as a scalable benchmark for RL agents,\nwith real-world applications for human level designers."}]}