{"title": "An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models", "authors": ["Fatemeh Shiri", "Xiao-Yu Guo", "Mona Golestan Far", "Xin Yu", "Gholamreza Haffari", "Yuan-Fang Li"], "abstract": "Large Multimodal Models (LMMs) have achieved strong performance across a range of vision and language tasks. However, their spatial reasoning capabilities are under-investigated. In this paper, we construct a novel VQA dataset, Spatial-MM, to comprehensively study LMMs' spatial understanding and reasoning capabilities. Our analyses on object-relationship and multi-hop reasoning reveal several important findings. Firstly, bounding boxes and scene graphs, even synthetic ones, can significantly enhance LMMs' spatial reasoning. Secondly, LMMs struggle more with questions posed from the human perspective than the camera perspective about the image. Thirdly, chain of thought (CoT) prompting does not improve model performance on complex multi-hop questions involving spatial relations. Lastly, our perturbation analysis on GQA-spatial reveals that LMMs are much stronger at basic object detection than complex spatial reasoning. We believe our benchmark dataset and in-depth analyses can spark further research on LMMs spatial reasoning.", "sections": [{"title": "1 Introduction", "content": "Large Multimodal Models (LMMs) have shown impressive generalization ability on several vision and language tasks. Several recent works, however, showed that these models lack spatial understanding (Tong et al., 2024; Li et al., 2023a; Liu et al., 2023e; Lei et al., 2024a; Prasad et al., 2023). As can be seen in Figure 1, multimodal LLMs, including GPT-40, often fail to answer questions from a human perspective within an image. The focus of this work is to study the understanding of spatial relations by top-performing LMMs. Moreover, we go beyond evaluating only the final answers to directly analyzing the intermediate reasoning steps generated by chain of thought (CoT) prompting in multi-hop visual question-answering (VQA) tasks. We ground LMMs' reasoning steps into a scene graph format and verify whether they form a valid path.\nMore specifically, we ask the following questions: (i) What spatial relations are missed by models, and why it happen? (ii) How can additional symbolic visual information, such as bounding boxes or scene graphs, improve the performance of LMMs? Which of these symbolic information is more useful, and how can they be integrated in the reasoning process effectively? (iii) How does the questions complexity affect LMMs in handling spatial relations? (iv) How does the reasoning path of LMMs behave when they fail to answer a multi-hop question? Is the failure due to incorrect spatial reasoning or non-spatial reasoning?\nTo address these questions, we construct Spatial-MM, a novel, challenging dataset, and comprehensively LMMs spatial reasoning capabilities from different angles. We analyze four top-performing LMMs on Spatial-MM and GQA-spatial (Kamath et al., 2023) benchmarks to identify problems with visual question answering (VQA) evaluation methodology. Our comprehensive analyses reveal a number of important insights that point to future research directions. Our contributions can be summarized as follows.\n\u2022 We present a new, challenging spatial-aware benchmark that incorporates a variety of spatial relationship types, accounting for both human and camera perspectives.\n\u2022 Our coprehensive empirical analyses show that: (i) bounding boxes and scene graphs, even synthetic ones, can significantly enhance LMMs' spatial reasoning, (ii) LMMs struggle more with questions posed from the human perspective than the camera perspective about the image, (iii) chain of thought (CoT) prompting does not improve model performance on complex multi-hop questions involving spatial relations, and (iv) LMMs are much stronger at basic object detection than complex spatial reasoning."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Large Multimodal Models", "content": "Pre-trained on large scale of text corpus, Large Language Models (LLMs) (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020; Zheng et al., 2023b) can easily reach more than billions of parameters and show great capacity on natural language comprehension, text completion and generation, zero-shot transfer and in-context learning. However, traditional LLMs usually take text as input and output, and lack the ability of understanding other modalities, like image, video or audio.\nTo tackle this issue, connecting multimodal encoder/decoder with LLMs, Large Multimodal Models (LMMs) can integrate multiple data modalities and overcome the limitations of text-only LLMs. LMMs are utilized to address various different tasks: image-text understanding (Liu et al., 2023c,a; Li et al., 2023c), video-text understanding (Lin et al., 2023; Li et al., 2023d; Maaz et al., 2023), and even multimodal generation (Peng et al., 2023; Lv et al., 2023; Zheng et al., 2023a).\nIn this paper, image-text understanding is our main focus, because it is already quite hard for LMMs to understand the spatial relation in normal images, not to mention videos."}, {"title": "2.2 Spatial relationship Benchmarks", "content": "Though LMMs have demonstrate remarkable performance in various benchmarks, they still have weaknesses on understanding spatial relationships, such as distinguish \"left\" from \"right\" between two objects presented in one image.\nKamath et al. (2023) curate a new What'sUP benchmark to quantify model performance on understanding spatial relationships. The evaluation results show not only the pretraining corpus contains little spatial-related data to learn from, but also LMMs perform limited on this spatial benchmark. Chen et al. (2024) identify the similar problem in 3D spatial relations. By generating a large scale of spatial annotations, they increase the LMMS spatial reasoning skill by a large margin. Instead of increasing the scale of pre-training data, Lei et al. (2024b) deal with spatial relationships from the model perspective. They propose SCAFFOLD prompting that overlays a dot matrix onto the image as visual information anchors, which demonstrates better performance in spatial reasoning over GPT-4 Vision with the textual CoT prompting.\nIn this paper, we propose a new spatial reasoning benchmark Spatial-MM, covering a diverse spatial relationships more than What'sUP and containing less noisy or ambiguous annotations."}, {"title": "2.3 Multi-hop Reasoning", "content": "Chain of thought prompting (Wei et al., 2022a) demonstrates remarkable multi-hop reasoning capability of LLMs by eliciting step-by-step reasoning paths. Least-to-most prompting (Zhou et al., 2022) further shows the feasibility of conducting decomposition and multi-hop reasoning, which happens on the decoder side together with the answer prediction procedure. Furthermore, Kil et al. (2024) improves multimodal multi-hop reasoning in VQA. Using answer prediction-guided CoT, the II-MMR model finds a reasoning path to reach the answer.\nOur new benchmark dataset Spatial-MM contains spatial multi-hop reasoning questions, covering at least two reasoning steps for each ground-truth reasoning path. To the best of our knowledge, it is the first multimodal multi-hop benchmark that pays attention to the evaluation on the reasoning path."}, {"title": "3 The Spatial-MM Benchmark", "content": "We seek to study the spatial reasoning capability gap between humans and LMMs. Based on the observation that existing benchmarks only partially investigate the spatial reasoning capabilities of LMMs (Li et al., 2023b; Liu et al., 2023f), we introduce a novel benchmark, Spatial-MM, which includes two subsets: Spatial-Obj and Spatial-CoT. Spatial-Obj features multiple-choice questions that focus on the spatial relationships between one or two objects in an image, while Spatial-CoT offers open-ended multi-hop questions."}, {"title": "3.1 Spatial-Obj", "content": "Spatial-Obj is a carefully curated benchmark that contains 2,000 multiple-choice questions with the aim of assessing LMMs' spatial reasoning of one or two objects in a given image. With natural images downloaded from the Internet, we carefully selected images to construct diverse challenging multiple-choice questions, including both yes/no questions and wh-type questions.\nThe dataset is constructed with two rounds of annotations. In the first round of annotation, we divided the images among three annotators and tasked them with selecting one or two objects in each image and compose a question-answer (QA) pair including a spatial relationship. Annotators were provided with question templates with objects placeholders, which they could use or customize according to their preference. In the second round of annotation, we released batches of 200 QA pairs with their corresponding images. Another 10 annotators were tasked with reviewing these QA pairs to verify whether they were correct or incorrect/ambiguous. Corrections were made based on their feedback.\nThis dataset covers 36 of the most commonly used spatial relationships (Marchi Fagundes et al., 2021), including right, left, attached to, touching, back, bottom, ahead, forward, backward, down, facing towards, facing away, top, beneath, beside, side, behind, under, on, in, front, below, above, over, middle, between, inside, outside, bottom right, bottom left, top right, top left, corner, close to, next to, near. Moreover, we used GPT-40 to categorise Spatial-Obj into visual patterns such as \u201cobject localization\", \"orientation and direction\", \"viewpoints\" and \"positional and relational context\", which pose significant challenges for LMMs. The prompt is listed in Appendix 7. Examples of these patterns can be seen in Figure 2."}, {"title": "3.2 Spatial-CoT", "content": "Chain of Thought (CoT)-style prompting has been demonstrated to significantly improve LLMs' reasoning capabilities. However, recent investigations on knowledge graph question answering and mathematical reasoning show that discrepancies exist between the answer and the corresponding reasoning paths through CoT prompting (Nguyen et al., 2024; Zhou et al., 2024). That is, while, by using CoT to producing reasoning paths, the LLM can produce the correct answer, the generated reasoning paths are not always correct. This disparity is a form of hallucination, and renders the LLMs' reasoning less trustworthy.\nTo enable the study of faithfulness in LMMs' spatial reasoning in the CoT style, we curate Spatial-CoT, a dataset of multi-hop question-answer pairs. The QA pairs are generated by prompting GPT-40 with a given image and a set of in-context examples. Images are sourced from Internet. In total, 800 multi-hop QA pairs were generated, and we filtered out 178 that did not include at least one of the 36 spatial relationships listed in Section 3.1 above. We then employed human annotators to select reasonable and meaningful multi-hop QA pairs that require at least two reasoning steps to reach the final answer. An additional 312 QA pairs were manually discarded for lacking the complexity needed for multi-hop QA pairs. Ultimately, Spatial-CoT includes 310 spatial-aware multi-hop QA pairs. The prompt, including instructions for generating multi-hop QA, is provided in Section A of the appendix.\nSpatial-aware reasoning paths. In knowledge graph question answering, triplets are considered reasoning steps (hops) (Nguyen et al., 2024). Leveraging this idea, in VQA tasks, we utilize scene graphs, including object relations or attribute, as reasoning steps that lead to the answer. It is important to note that current VQA benchmarks (e.g. GQA (Hudson and Manning, 2019)) with ground-truth scene graphs lack diverse perspective information and only incorporate spatial relationships from the camera's viewpoint. For Spatial-CoT, as no ground-truth paths (or scene graph) exist, to evaluate the spatial reasoning abilities of LMMs, we generate the reasoning paths for each question. We carefully instruct GPT-40 with a set of in-context examples to generate the initial draft of the spatial-aware reasoning path. Subsequently, three annotators were tasked with evaluating (i.e. keeping, removing, adding, or modifying) the generated steps for each question. Additionally, the annotators were asked to tag the steps as S for spatial hops such as \"person in front of car\u201d and NS for non-spatial hops such as \u201cwoman holding a fork\", respectively. In total, 67% of reasoning steps are tagged as S and the rest 33% are tagged as NS, suggesting that many questions require reasoning of spatial relationships between key objects to be"}, {"title": "3.3 Human performance", "content": "We evaluated human performance using our benchmark to maintain the quality of annotations. We sampled 300 data points from Spatial-Obj and 100 samples from Spatial-CoT. Annotators were consulted to assess whether the correct option is clearly identifiable or if there's any potential for confusion. This human performance assessment yielded scores of 98% and 99% on Spatial-Obj and Spatial-CoT datasets, respectively."}, {"title": "4 Data Enrichment to Improve Spatial Reasoning", "content": "Intuitively, providing LMMs with additional spatial-aware visual grounding data would help improve their spatial reasoning performance. To test this hypothesis, we describe two pipelines for generating different types of visual grounding data. The first pipeline consists of two stages: (I) key object extraction and (II) bounding box generation. The second pipeline consists of three stages: (I) key object extraction, (II) spatial-aware caption generation related to the key objects, and (III) spatial-aware scene graph generation. Please refer to Figure 6 in Appendix C for detailed illustrations of our pipelines. Furthermore, we describe Reasoning path generation for the multi-hop questions.\nBounding box generation. The input of the pipeline is an image $I$ paired with a multiple-choice question $Q$. We first extract key objects, $[object_1, object_2, ..]$ from the given question. Next, we prompt GPT-40 to provide the bounding boxes of the key objects in the image. Each bounding box is specifically represented as a tuple $[X_{min}, Y_{min}, X_{max}, Y_{max}]$, where $X_{min}$ and $Y_{min}$ are coordinates of the top-left corner and $X_{max}$ and $Y_{max}$ are coordinates of the bottom-right corner. We present the prompt for bounding box generation in Appendix 7, and examples of synthesized bounding boxes for the key objects along with the"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Dataset", "content": "We use the following datasets in our experiments:\n\u2022 Spatial-MM contains Spatial-Obj described in \u00a73.1 and Spatial-CoT described in \u00a73.2.\n\u2022 GQA-spatial (Kamath et al., 2023) is sourced from GQA (Hudson and Manning, 2019), where each image is paired with two caption options, which contain opposite spatial relationships, i.e. prepositions. Depending on the number of object(s) identified in the image, GQA-spatial contains 1-object and 2-object caption options. One instance from GQA-spatial is shown in the \"standard\" row of Figure 4: one image and two captions make use of opposite prepositions: front and behind."}, {"title": "5.2 LMMS", "content": "We evaluate the spatial reasoning capability of the following four LMMs: LLaVA-1.5-7B (Liu et al., 2023b), GPT-4 Vision (OpenAI, 2023), GPT-40 (OpenAI, 2024), Gemini 1.5 Pro (Gemini Team,"}, {"title": "5.3 Evaluation Metrics", "content": "To evaluate model performance on final answer prediction, we adopt the standard Accuracy metric Hudson and Manning (2019). For evaluating Spatial-CoT, the exact-match metric is insufficient for measuring performance fully due to the open-ended nature of the answers. LMMs frequently produce paraphrases or alternative expressions that convey the same underlying meaning. Therefore, we initially verify if the predicted answer exactly matches the gold answer. If it does not, following (Lyu et al., 2024; Wang et al., 2023), we use GPT-4o as a judge to determine whether the predicted answer holds the same semantic meaning as the gold answer."}, {"title": "5.4 The Effect of Bounding Boxes and Scene Graphs", "content": "For both Spatial-Obj and GQA-spatial benchmarks, the input is an image paired with a multiple-choice question. Table 1 presents the results on Spatial-Obj and GQA-spatial, comparing LMMs' performance on images only and with additional information (i.e. bounding boxes and scene graphs). A number of important observations can be made from the table. (I) It shows that the additional bounding box information serves as visual anchors that enhance LMMs' spatial reasoning performance for LMMs. When given the synthesize bounding boxes information on GQA dataset, the LMMs improve their answer accuracy significantly, with the average increase over \u201cstan\u201d of 33 points. However, spatial related questions require a profound semantic understanding of object relationships within the scene. By incorporating spatial-aware scene graphs of key objects, the performance of LMMs improves significantly on both datasets. Specifically, by including the synthesized SG in the prompt, GPT-4 vision's accuracy increases by 9.28% and 8.92% for one-object and two-object questions on the Spatial-Obj dataset. (II) Surprisingly, the LMMs perform better with synthesized bounding boxes than with ground-truth bounding boxes. This may be attributed to the high quality and diversity of synthesized bounding boxes generated by GPT-4 vision. (III) On Spatial-Obj, scene graphs further enhance model performance, while on GQA-spatial, bounding box information seems to be more useful. We leave a deeper analysis of these two observations"}, {"title": "5.5 The Effect of Human and Camera Perspectives", "content": "Questions from previous benchmarks (Kamath et al., 2023) are often evaluated from the camera's perspective, i.e., outside the image. How would LMMs behave if the question is asked from other angles, such as from the human perspective in the image, i.e., inside the image, is under-explored. This problem motivates us to generate different questions with distinct prompts \u201cfrom camera's perspective\", \"from human's perspective\u201d respectively, from original questions, and further understand the spatial reasoning capabilities of LMMs.\nIn Table 2, we show different model performance from either human's or camera's perspective on Spatial-Obj and Spatial-CoT. Though all these LMMs receive explicit prompt \"from human/camera's perspective\", they all show significant performance drop on human's perspective, when compared to the camera's perspective. Especially, while GPT-40 outperforms all other models from camera's perspective, it is still not able to understand from human's perspective and show the maximum drop on Spatial-CoT of more than 30 points.\nFinding 2: LMMs excel at understanding the scene from the camera's perspective. However, their performance declines significantly when questions are posed from the human perspective within the image."}, {"title": "5.6 Analysis based on Complex Multi-hop VQAS", "content": "The effect of the number of hops to the answer. Table 3 compares chain of thought prompting with other approaches on the Spatial-CoT dataset. As can be seen, the conventional CoT prompting (Wei et al., 2022b) may not be as effective for complex VQA tasks as it is for NLP tasks. Indeed, we can observe that even the strong LMMs such as GPT-40, Gemini-Pro and LLaVA-1.5 perform worse when employing CoT reasoning, compared to standard prompting. Therefore, the rationales produced by the conventional CoT may not align well with the reasoning path needed to arrive at the answer.\nOn the contrary, including visual grounded information such as bounding boxes information is effective when the number of reasoning hops are < 3. In particular, Table 3 highlights the effectiveness of scene graphs in answering multi-hop spatial questions across all the models. Multi-hop questions demand a deep semantic understanding of object attributes and relationships within the scene. Therefore, scene graphs are essential for enhancing LLMs' semantic visual understanding. Moreover, across different models and various experiment settings in Table 3, it can be seen that with the increase in the number of the hops, accuracy of the final answer drops. For instance, the average accuracy of questions with \u2265 4 hops is 12% and 7.5% less than that for questions with 2 and 3 hops, respectively.\nCorrectness of the reasoning path. We ground LMMs' reasoning steps into a scene graph format and verify whether they form a valid path. Given the question and predicted answer, we carefully design prompts to instruct LMMs to output the reasoning steps in a scene graph format, i.e., including object relations and/or attributes. This enables us to validate the LMMs' reasoning steps against ground-truth reasoning paths (Section 4). To evaluate the reasoning path in multi-hop questions, computing the semantic match for all the generated steps in the path. We further analyze the types of reasoning steps required in multi-hop questions.\nTable 5 presents the results of analyzing the reasoning paths for questions that LMMs fail to answer correctly by stan prompting. The incorrect reasoning path could occur due to an incorrect spatial step, an incorrect non-spatial step, or a combination of both. On average, only 1% of questions had the correct reasoning path but an incorrect final answer. Moreover, 91% of questions included at least one incorrect spatial reasoning step, while only 9.75% contained at least one incorrect Non-spatial reasoning step. Please note that an incorrect reasoning step occurs when a reasoning step semantically does not match the ground truth reasoning steps or when a reasoning step from the ground truth path is missing in the generated path.\nThe average scores for all Spatial and Non-spatial reasoning steps in Spatial-CoT questions are listed in Table 4, which shows that the average gap between F1 scores for Spatial and Non-spatial reasoning steps is significant by 21 point on stan and 19 points on stan+SGsynth prompting.\nFinding 3: Chain of thought prompt is not effective for multi-hop spatial reasoning."}, {"title": "5.7 Analysis of Perturbations on GQA-spatial", "content": "Using GQA-spatial, we do a comprehensive understanding of the LMMs' robustness when facing spatial-related questions by applying five perturbation settings on the caption options as follows:\n\u2022 none: adding one extra option \u201cNone of above\"; the \"Standard+None\" row in Figure 4\n\u2022 rel_neg: adding \"not\" to the correct option; the \"Relation Changed (Negation)\" row in Figure 4\n\u2022 rel_swap: Swapping the key objects in options; \"Relation Changed (Swapping)\" row in Figure 4\n\u2022 obj_change: replacing one of the key objects for both options (A & B); the \u201cObject Changed (Two options)\" row in Figure 4\n\u2022 obj_change_a: replacing one of the key objects in the correct option (A) with a none-existing object in the image; the \"Object Changed (option A)\" row in Figure 4\nFor the Standard setting, the model input is an image paired with two caption options that differ only by the preposition they contain, from which LMMs should be able to select the caption option with the correct preposition. For perturbation settings, the image remains the same but caption options are changed with different difficulty levels. Intuitively speaking, we posit that all perturbation settings are more difficult than the Standard setting, as they add noise (e.g., none) and create obstacles (e.g., negation) for LMMs. Results are show in Table 6, and analysis of each perturbation are discussed in detail below:\nrel_neg. Adding not to the answer option A increases the accuracy (7%) for 1-object only, meanwhile decreasing the accuracy (6%) for 2-objects. Negating the relations in general decreases model performance. Adding bounding boxes makes it even worse. However, adding scene graphs is helpful, especially with Gemini.\nrel_swap. Analysis is conducted with swapping objects of answer option A and adding the \"None\" option. From 288 two_obj questions, Gemini only has 8 correct answers (option E), 5 incorrect answers(option A), and 275 incorrect answers(option B), while 95% of generated answers are option B. LLaVA shows similar results: from 291 2-object questions, LLaVA only has 4 correct answers (option E), 3 incorrect answers(option A), and 284 incorrect answers(option B), while 97.59% of generated answers are option B.\nobj_change. When the same object in both options A and B are swapped with an object that does not exist in the image (row \u201cobj_change+none", "4": "LMMs are usually good at the object detection task (recognizing objects present in the image), but struggle with spatial reasoning (distinguishing \u201cleft\u201d from"}, {"title": "6 Conclusion", "content": "Motivated by our observation that LMMs are not able to distinguish spatial relationships, in this paper, we propose a new benchmark to evaluate the spatial reasoning capabilities of LMMs. Our benchmark consists of two subsets: Spatial-Obj containing 2,000 multiple-choice questions to evaluate the spatial reasoning capabilities of one or two objects in a given image, Spatial-CoT containing 310 multi-hop questions to evaluate the spatial related CoT and reasoning paths. We conduct comprehensive experiments on our benchmark and GQA-spatial. Experimental results show the deficiencies in spatial reasoning of current LMMs, including GPT-40, Gemini and LLaVA-v1.5. We also find that bounding boxes and scene graphs are helpful in some cases to improve the LMMs' prediction quality."}, {"title": "7 Limitation", "content": "To set up the Spatial-CoT benchmark, we implement a two-step data collection process: initially using GPT-4 to generate multi-hop VQAs, followed by manual filtering and modification. Though this process can to some extend accelerate the overall annotation approach as human annotators don't need to write multi-hop VQAs from the scratch, it is still limited and hard to create and generate large scale of labelled data. Same limitations applies to the generation of the ground-truth reasoning paths."}]}