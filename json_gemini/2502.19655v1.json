{"title": "MED-RLVR: EMERGING MEDICAL REASONING FROM A 3B BASE MODEL VIA REINFORCEMENT LEARNING", "authors": ["Sheng Zhang", "Qianchu Liu", "Guanghui Qin", "Tristan Naumann", "Hoifung Poon"], "abstract": "Reinforcement learning from verifiable rewards (RLVR) has recently gained atten- tion for its ability to elicit self-evolved reasoning capabilitie from base language models without explicit reasoning supervisions, as demonstrated by DeepSeek- R1 (DeepSeek-AI et al., 2025). While prior work on RLVR has primarily focused on mathematical and coding domains, its applicability to other tasks and domains remains unexplored. In this work, we investigate whether medical reasoning can emerge from RLVR. We introduce MED-RLVR as an initial study of RLVR in the medical domain leveraging medical multiple-choice question answering (MCQA) data as verifiable labels. Our results demonstrate that RLVR is not only effective for math and coding but also extends successfully to medical question answering. Notably, MED-RLVR achieves performance comparable to traditional supervised fine-tuning (SFT) on in-distribution tasks while significantly improving out-of- distribution generalization, with an 8-point accuracy gain. Further analysis of training dynamics reveals that, with no explicit reasoning supervision, reasoning emerges from the 3B-parameter base model. These findings underscore the po- tential of RLVR in domains beyond math and coding, opening new avenues for its application in knowledge-intensive fields such as medicine.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent work on reinforcement learning from verifiable rewards (Lambert et al., 2024) has demon- strated promising results, particularly highlighted by DeepSeek-R1 (DeepSeek-AI et al., 2025) where they show that reasoning can emerge from performing RLVF alone from a base model. Sub- sequent efforts \u2013 such as studies in Yeo et al. (2025); Zeng et al. (2025); HuggingFace (2024); EvolvingLMMs-Lab (2025) along with others focusing on synthetic datasets (e.g., countdown (Pan et al., 2025) and counting objects (Chen et al., 2025)) \u2013 have attempted to replicate and extend the initial findings on the promising direction of RLVR, underscoring both the interest in and challenges of this approach.\nDespite these advances, the application of RLVR has predominantly been focused on domains such as mathematics (Lightman et al., 2023) and coding (Jain et al., 2024), leaving open the question of how to extend its benefits to areas without such data. One promising data source for extend- ing RLVR beyond Math/coding is multiple-choice question answer pairs (MCQA) which provides abundant verifiable labels across many domains including medicine. However, there are essential differences between MCQA and math/coding tasks: the answer space of the latter tends to be large and open-ended whereas MCQA features a much smaller answer space. It is unclear whether the benefits observed in math and coding will translate to MCQA. This gap is particularly pronounced in the medical domain, where MCQA tasks \u2013 such as those found in the MedQA (Jin et al., 2021) dataset-require sophisticated reasoning with clinical knowledge and have consistently presented significant challenges (Nori et al., 2023).\nTo bridge this gap, we propose MED-RLVR (Figure 1), an initial exploration of RLVR leveraging MCQA data to elicit medical reasoning from a small base model without explicit reasoning super- vision. Our findings are below:\n1. RLVR is effective not just for math and coding, but also for multiple-choice medical questions.\n2. MED-RLVR achieves comparable performance to traditional supervised finetuning (SFT) in in-distribution settings while demonstrating superior generalization to out-of-distribution sce- narios with approximately 8 percentage points improvement in accuracy."}, {"title": "2 MED-RLVR", "content": "Reinforcement Learning (RL) is a machine learning algorithm that focuses on learning a policy for an agent to take actions in an environment to maximize cumulative rewards. At each step, the agent observes a state (S), selects an action (A) based on a policy (\u03c0), and receives a reward (R) while transitioning to a new state. RL has been affectively applied to language model post-training for aligning to human preference (Ouyang et al., 2022) and more recently to elicit reasoning without any supervisions (DeepSeek-AI et al., 2025) for math and coding tasks.\nWe adopt Proximal policy optimization (PPO) (Schulman et al., 2017) as our RL algorithm. PPO works by optimizing a policy using multiple epochs of stochastic gradient ascent on minibatch up- dates, while ensuring updates do not deviate too much from the previous policy. It does this by using a clipped surrogate objective function, which prevents excessively large policy updates, improving training stability:\n$\\begin{aligned} J_{P P O}(\\theta) &=\\mathbb{E}_{q \\sim P(Q), o \\sim \\pi_{\\theta_{\\text {old }}}(0 \\mid q)}[\\frac{1}{|O|} \\sum_{t=1}^{|O|} \\min \\left(\\frac{\\pi_{\\theta}\\left(o_{t} \\mid q, o_{<t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_{t} \\mid q, o_{<t}\\right)} A_{t},\\right. \\\\ & \\left.\\operatorname{clip}\\left(\\frac{\\pi_{\\theta}\\left(o_{t} \\mid q, o_{<t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_{t} \\mid q, o_{<t}\\right)}, 1-\\epsilon, 1+\\epsilon\\right) A_{t}\\right) \\end{aligned}$  (1)\n(2)\nwhere \u03c0\u03b8 and $\\pi_{\\theta_{\\text {old }}}$ are the current and old policy models, and q, o are questions and outputs sam- pled from the question dataset and the old policy $\\pi_{\\theta_{\\text {old }}}$, respectively. is a clipping-related hyper- parameter introduced in PPO for stabilizing training. $A_{t}$ is the advantage, which is computed by applying GAE (Schulman et al., 2015), based on the rewards {$r_{\\textgreater{}t}$} and a learned value function V. Thus, in PPO, a value function needs to be trained alongside the policy model and to mitigate over- optimization of the reward model, we follow Ouyang et al. (2022) to add a per-token KL penalty from a reference model in the reward at each token, i.e.,\n$r_{t}=r(q, o)-\\beta \\log \\frac{\\pi_{\\theta}\\left(o_{t} \\mid q, o_{<t}\\right)}{\\pi_{\\text {ref }}\\left(o_{t} \\mid q, o_{<t}\\right)}$\nwhere r is the reward model, \u03c0ref is the reference model, which we initialize with a base model, and \u1e9e is the coefficient of the KL penalty.\nOur training recipe follows the RLVR framework in Deepseek-R1-Zero (DeepSeek-AI et al., 2025). RLVR refers to the RL training method where the reward model is a verification function in- stead of a learned reward model (Lambert et al., 2024). In our study, we use a simple rule- based reward function which computes reward based on the outcome. To ensure the model out- put is both correct and in its correct format, we first check the format of the output and as- sign a -1.0 penalty if the output does not have a valid format. The valid format should follow  <think>...</think> <answer>..</answer>. Once the format check is passed, we check the correctness of the output and assign zero reward if the output answer is incorrect and gives a 1.0 reward if the output answer is correct. In other words, only an output that is both correct and is in the correct format will receive a positive reward. We determine if the answer is correct if the option letter in the answer output matches the gold option letter. Below is the pseudo code of the reward function:"}, {"title": "3 EXPERIMENTS", "content": "3.  1 DATASET\nFor training, we use the MedQA-USMLE dataset (Jin et al., 2021), which consists of multi-choice questions sourced from professional medical board exams and covers a wide range of medical topics, requiring domain-specific knowledge and reasoning skills. Note that we do not use the popular \"4_options\" version of MedQA. Instead, we use the original version, which is more challenging as the questions have more than four options (including \"None of the above\"). Below is an example question from the dataset:"}, {"title": "3.2 PROMPT", "content": "We adapt the prompt template from DeepSeek-AI et al. (2025), where { input } will be replaced with a multi-choice question. We do not force the assistant message to start with <think>, because we find that during training the model is able to learn to follow the format requirement quickly. Below is the prompt we used:\nA conversation between User and Assistant. The user asks a question, and the assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within  </think> and  </answer> tags, respectively, i.e.,  reasoning pro- cess here  <think> answer here </answer>.\nUser: {input}\nAssistant:"}, {"title": "3.3 TRAINING SETUP", "content": "We initialize MED-RLVR with Qwen2.5-3B (QwenTeam, 2024) and use OpenRLHF (Hu et al., 2024) framework to train RLVR with MedQA MCQA labels on the base model for 10 epochs on 4x40G8 A100 GPUs.\nBaselines: We compare MED-RLVR against a SFT baseline using the question-answer pairs from the same MedQA training data. The SFT baseline was trained for 10 epochs and we performed grid search for learning rate [1e-5, 5e-6, 1e-6] to ensure competitive performance from this baseline. Notice that the SFT baseline can only directly output answers without reasoning as it was trained to do so. In addition, we also report the direct and chain-of-thought (CoT) prompting results of the Qwen2.5-3B base model."}, {"title": "4 RESULTS", "content": "4.  1 MAIN RESULTS\nFigure 3 below demonstrates that MED-RLVR effectively learns to solve medical multiple-choice question tasks starting from a base model. Compared to SFT, MED-RLVR achieves comparable performance on the in-distribution MedQA test set but outperforms SFT significantly on the out- of-distribution MMLU-Pro-Health task, with an improvement of approximately 8 absolute points in accuracy. This result suggests that while SFT is a strong baseline for in-distribution tasks, it likely relies on spurious correlations and shortcuts to achieve high performance without reasoning. In contrast, the reasoning capabilities acquired from MED-RLVR are more robust and generalizable across tasks. This finding is in line with some recent observations that RL generalizes better than SFT (Chu et al., 2025; Shen et al., 2025).\n4.  2 ANALYSIS ON THE EMERGING REASONING TRACES\n5.  2.   1 SHIFTS IN REASONING PATTERNS\nWe conducted analysis and revealed distinct shifts in reasoning patterns throughout the training process. We categorized the process into the following six stages (also illustrated in Figure 2. Table 3 also shows the example reasoning traces for each stage.\nStage 1 (Format Failure): The model fails to follow the format requirements and produces brief, unstructured responses. In line with recent work such as Yeo et al. (2025), we observe that reasoning capabilities already exist in Qwen base models, but the models simply don't adhere to the specified format instructions."}, {"title": "5 DISCUSSION OF LIMITATIONS", "content": "Our initial exploration focuses on multiple-choice question answering (MCQA), a synthetic setting within the field of medicine. While MCQA provides a controlled environment to evaluate model performance, it does not fully represent the complexity and nuance of more practical medical tasks such as open-text question answering, report generation, or even conversational interactions. These tasks often involve richer contexts and require models to handle a wider variety of inputs and outputs, which could present additional challenges not captured in our current approach. Additionally, our exploration has been confined to a unimodal framework, leaving the multimodal setting where integrating data from sources like images, text, and structured information could enhance diagnostic and decision-making capabilities \u2013 largely unexplored. Addressing these areas in future work will be crucial for developing more robust and versatile models for real-world medical applications.\nOur initial exploration focuses on multiple-choice question answering (MCQA), a synthetic setting within the field of medicine. While MCQA provides a controlled environment to evaluate model performance, it does not fully represent the complexity and nuance of more practical medical tasks such as open-text question answering, report generation, or even conversational interactions. These tasks often involve richer contexts and require models to handle a wider variety of inputs and outputs, which could present additional challenges not captured in our current approach. Additionally, our exploration has been confined to a unimodal framework, leaving the multimodal setting - where integrating data from sources like images, text, and structured information could enhance diagnostic and decision-making capabilities \u2013 largely unexplored. Addressing these areas in future work will be crucial for developing more robust and versatile models for real-world medical applications."}, {"title": "6 CONCLUSION", "content": "We introduced MED-RLVR, an initial study of Reinforcement Learning with Verifiable Rewards (RLVR) in the medical domain. Our results show that MED-RLVR matches supervised fine-tuning (SFT) on in-distribution tasks while significantly improving out-of-distribution generalization (+8 accuracy points). Notably, medical reasoning emerged without explicit supervision, though chal- lenges like reward hacking were observed. While our study focuses on multiple-choice medical questions, future work should explore more complex medical reasoning tasks and multimodal inte- gration to advance the potential of RL in the medical domain."}]}