{"title": "Automated Hypothesis Validation with Agentic Sequential Falsifications", "authors": ["Kexin Huang", "Ying Jin", "Ryan Li", "Michael Y. Li", "Emmanuel Cand\u00e8s", "Jure Leskovec"], "abstract": "Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose POPPER, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper's principle of falsification, POPPER validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate POPPER on six domains including biology, economics, and sociology. POPPER delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, POPPER achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation. POPPER is freely available at https://github.com/snap-stanford/POPPER.", "sections": [{"title": "1. Introduction", "content": "A hypothesis is a theory or an explanation based on limited evidence. It forms the backbone of decision-making, information acquisition, and discovery across domains (Thompson & Skau, 2023). For example, a robot evaluates different hypotheses to decide what action to take next. A scientist decides which experiments to run to evaluate a hypothesis/theory. The marketing strategy decisions are guided by the hypothesized effect on increasing customer retention. Similarly, policymakers may rely on hypotheses about the outcomes of proposed interventions.\nGiven their profound implications, it is important to validate hypotheses with supporting evidence. This need has grown increasingly urgent with the recent surge in hypotheses generated by Large Language Models (LLMs) (Wang et al., 2024b; Zhou et al., 2024). While these systems exhibit remarkable creativity and diversity, the plausibility of their generated hypotheses can vary significantly due to potential hallucinations (Huang et al., 2023). Moreover, the sheer volume of LLM-generated hypotheses makes it impractical to invest in each one immediately. Therefore, obtaining a reliable, scalable understanding of the quality of these hypotheses is essential to fully unlock their potential.\nHaving said this, many real-world hypotheses are abstract natural language statements that are difficult to directly evaluate (Thompson & Skau, 2023; Godfrey-Smith, 2009). For example, while we might hypothesize that \u201ca gene causes a disease,\u201d it is infeasible to test this statement directly as it stands. Instead, it must be translated into specific, measurable implications that can be experimented rigorously (Jun et al., 2022). Yet, even for a single hypothesis, the space of potential supportive implications is vast, highlighting the need for frameworks that can automate this evaluation process. Notably, such frameworks must also be statistically rigorous, avoiding false verifications of hypotheses that are not true (Neyman & Pearson, 1928; 1933; Fisher, 1936). Without such control, research efforts risk being mis-directed, resources wasted, and harmful conclusions drawn, ultimately undermining progress and trust. Overall, this raises a critical question: How can we rigorously validate free-form hypotheses at scale?\nPresent work. We introduce POPPER, a novel framework for rigorous and automated validation of free-form natural language hypotheses using LLM agents. Inspired by Karl Popper's principle of falsification (Popper, 2005), POPPER systematically challenges hypotheses by sequentially testing their measurable implications through diverse experiments, ranging from data analysis and simulations to real-world"}, {"title": "2. POPPER: a general framework for automated hypothesis validation", "content": ""}, {"title": "2.1. Background and Problem Formulation", "content": "Following Majumder et al. (2024); Thompson & Skau (2023), we broadly define hypothesis H as a statement that defines relationships (r) between a set of variables (V) under contexts (c). For example, in the hypothesis H \u201cGene VAV1 regulates IL2 production in immune tissue\u201d, V= {\u201cVAV1\u201d, \"IL2 production\u201d}, r = \u201cregulate\u201d, and c = \u201cin the immune tissue\u201d. To formalize the discussion, the hypothesis H is associated with a null hypothesis Ho. Ho describes a family Po of distributions that generate the data under the null, i.e., in uninteresting situations (such as \"Gene VAV1 does not regulate IL2 production\"). In this way, Ho being incorrect is of interest (the alternative hypothesis). Hypothesis validation aims to test the null hypothesis Ho and suggest evidence for the alternative.\nThe hypothesis validation task is defined as f : H \u2192 {0,1}, where 0 stands for unvalidated and 1 stands for validated (claiming the alternative). Given a hypothesis H, a system or a program f designs and performs experiments and generates an answer in {0,1}. We denote \u0177 as the predicted validation status. An experiment is typically associated with the collection (or retrieval) and processing of datasets denoted as D. An LLM agent A is broadly defined as a program that takes in instructions in natural language and performs actions T with reasoning capabilities to solve the task given the instruction and outputs a natural language answer (Yao et al., 2023).\nFor rigorous hypothesis validation, we adopt the classical Type-I error control as our primary criterion. The Type-I error is the probability of the system incorrectly claiming an \"interesting\" finding (e.g., enriched gene expression) when the null hypothesis is true. Formally, the Type-I error rate is $sup_{P \\in P_0} P(\\hat{y} = 1)$, where the probability is over the data and the validation system. To ensure rigor, our goal is to control the Type-I error at a pre-defined level \u03b1 \u2208 (0,1). Another important concept is the power of the validation system, which we define as P(\u0177 = 1) where P is the data distribution. While power-the ability to detect true effects-is important, its improvement is meaningful only when Type-I error control is ensured. Without this foundation, increased power risks invalid conclusions."}, {"title": "2.2. Overview of POPPER", "content": "POPPER is an agentic framework to systematically validate a hypothesis by actively designing and executing a sequence of falsification experiments. This perspective is inspired by Karl Popper's philosophy of falsification (Popper, 2005): rather than trying to directly prove a hypothesis of interest, one can attempt to refute its logical implications through experiments.\nSuppose we want to investigate whether gene X is related to disease Y. Directly establishing such a relationship may be difficult; however, we can test one of its implications: if X truly has no relationship to Y, we might expect no significant difference in X's expression levels when comparing cell types implicated in Y versus unrelated cell types."}, {"title": "2.3. Validity of Type-I Error Control in POPPER", "content": "This part lays out the general conditions needed for valid Type-I error control in POPPER.\nAssumption 1 (Implication). If Ho is true, then the null sub-hypothesis $h_i^0$ is true for all i \u2265 1.\nAssumption 1 requires that the null sub-hypothesis $h_i^0$ describes a range of data generating processes that are contained in those described by Ho. As we are to detail in Section 3, we leverage the reasoning capabilities of LLMs, as well as additional checks to overcome the intrinsic randomness in LLM agents to approximately fulfill this condition.\nRecall that an e-value $e_i \u2208 R$ is computed based on the collected data in each iteration. Following Vovk & Wang (2021), an e-value is a non-negative random variable whose expectation is below 1 under the null hypothesis and such that if it were to take a large value, it would indicate strong evidence for refuting the null. E-values are our key instruments for Type-I error control. Compared with the classical concept of p-values, their advantages include (i) flexible"}, {"title": "2.4. Agentic hypothesis validation framework", "content": "We now introduce each component of POPPER in a general form. Although the particular implementation we show-case later uses a static database, POPPER can be deployed in any environment capable of producing valid p-values - whether that involves laboratory experiments, real-time data collection, or computational simulations. The essence is to iteratively design and execute falsification experiments on sub-hypotheses derived from a main hypothesis H. Below, we describe how our agents accomplish this while maintaining the assumptions needed for Type-I error control.\nExperiment design agent. Given the main hypothesis H and history of previously tested sub-hypotheses (and their outcomes), the design agent proposes a new falsification experiment intended to refute Ho. Concretely, it specifies:\n*   A sub-hypothesis capturing a concrete implication of the main hypothesis.\n*   The null $h_0$ and alternative $h_1$ to be tested.\n*   Details of how to conduct the experiment in a given domain. This may involve recommending the collection of new laboratory samples, setting up a targeted simulation, or identifying a suitable dataset (if available).\nThe design agent is assumed to have domain expertise or access to domain knowledge, allowing it to propose experiments that are both relevant for falsifying Ho and feasible to implement. For instance, it might propose measuring gene-expression levels, or running a randomized simulation study, or analyzing an existing database - whatever is best to challenge the null sub-hypothesis. Critically, the design agent must ensure that the proposed experiment can, in principle, yield a valid p-value under $h_0$. We will later show how this agent's operations are automated in practice in Section 3.\nExperiment execution agent. Once an experiment is designed, it is handed off to the execution agent, which is responsible for carrying it out. In a laboratory setting, this agent might interface with robotic lab equipment or prompt human technicians to conduct the specified protocol. In a simulation, it would set up and run the relevant computational model. In a data analytics context, it would query and analyze the dataset. Regardless of the experimental modality, the only restriction is that it outputs a valid p-value under $h_0$ (Assumption 2). If an experiment fails - because the protocol cannot be completed or the data are insufficient - it is simply recorded as a failed attempt, and the procedure moves on. In Section 3, we show how this agent is instantiated using a code-generation framework that automatically executes data queries and statistical analyses.\nSequential aggregation of statistics for error control. After obtaining the new p-value pi, we aggregate existing falsification tests to collectively measure evidence for the main hypothesis while maintaining Type-I error control. As de-"}, {"title": "3. Instantiation of POPPER", "content": "Thus far, we have described POPPER as a general, agentic framework capable of executing any type of experiment - laboratory procedures, simulations, or data analyses - to test sub-hypotheses under a unifying Popperian falsification paradigm. In this section, we focus on our current instantiation, where experiments are drawn from a static corpus of massive hypothesis-free datasets (D) rather than real-world"}, {"title": "4. Experiments", "content": "We evaluate POPPER in terms of Type-I error control, power improvements, expert user studies, ablations, human annotations, and failure analysis.\nEvaluation setup. We assess Type-I error by creating negative examples through random column-wise permutations in each dataset, ensuring the null hypothesis holds. For DiscoveryBench, we generate as many negative examples as positive ones. For the target validation benchmark (with only 20 positives), we create 50 negatives. We measure"}, {"title": "4.1. Results", "content": "POPPER achieves Type-I error control. Table 3 reports the Type-I error rates and several key observations are in order. First, most baselines fail to consistently control the Type-I error, while POPPER remains below the nominal level across all datasets. This underscores the necessity of principled statistical design in LLM-driven hypothesis validation; without such rigor, the flexibility of LLM agents can inflate Type-I errors. Second, the comparison against Fisher's combined test highlights the benefits of e-values in aggregating evidence. Third, the LLM-Likelihood Ratio"}, {"title": "4.2. Analysis and Discussion", "content": "Qualitative characterization. We characterize the trajectories of POPPER in Figure 3 (procedure described in Appendix E). In TargetVal, we observe that POPPER designed experiments that span a broad set of biological tests, including protein-protein interaction networks, expression correlation analyses, eQTL regulatory tests, loss-of-function studies, and genetic perturbation tests. During each iteration, the execution agent typically performs up to 14 distinct steps: dataset inspection, preprocessing, model fitting, error handling, statistical testing, visualization, and summarization. Notably, POPPER carefully selects statistical methods based on modeling assumptions (e.g., chi-squared, hypergeometric, Fisher's, and permutation tests) and often includes well-chosen negative controls. Interestingly, non-parametric tests are most frequent, making them robust to various data distributions. Visualizing the e-value trajectories reveals that evidence against the null accumulates quickly under the alternative while remaining below the nominal threshold un-"}, {"title": "5. Related Work", "content": "We discuss here related works that are closest to POPPER and provide extended discussion on other related works in Appendix B. LLMs have been widely explored for hypothesis generation, with works focusing on domain-specific ideas (Wang et al., 2024a; Baek et al., 2024; Yang et al., 2024b) and comparisons between AI-generated and expert proposals (Si et al., 2024). Beyond idea generation, some studies refine hypotheses (Honovich et al., 2023; Wang et al., 2024c) or ground them in datasets (Majumder et al., 2024), yet few systematically test free-form hypotheses under rigorous statistical controls. While certain works evaluate LLM-driven experimental protocols (Tian et al., 2024; Gu"}, {"title": "6. Conclusion", "content": "We proposed POPPER, an LLM-based framework for validating free-form hypotheses. By integrating a sequential testing paradigm with automated experiment design and execution, POPPER delivers scalable, statistically rigorous hypothesis validation. This work represents an early exploration, and several aspects can be further improved. Refining test relevance and ensuring robust LLM implementations remain challenges. Future work can also extend POPPER to control other error metrics (e.g., false discovery rate), further broadening its utility in scientific discovery and beyond."}, {"title": "Impact Statement", "content": "This work introduces POPPER, a statistically rigorous agentic framework for hypothesis validation using Large Language Model (LLM) agents. By combining advanced natural language processing capabilities with robust statistical methodologies, POPPER addresses the critical challenge of evaluating and validating hypotheses generated by LLMs, ensuring that only evidence-backed hypotheses guide future research. The broader implications of this work span multiple domains, including biology, economics, and social sciences, where hypothesis generation and validation play a pivotal role in advancing knowledge.\nFrom an ethical perspective, POPPER's emphasis on rigorous statistical validation and Type-I error control mitigates the risks associated with hallucinated or unsupported hypotheses. This ensures that research resources are directed toward meaningful and plausible hypotheses, reducing the potential for wasted efforts and false conclusions that could mislead scientific progress or policy decisions. Additionally, by automating and accelerating the hypothesis validation process, POPPER democratizes access to high-quality scientific methodologies, enabling smaller research teams and resource-limited institutions to conduct advanced analyses."}, {"title": "A. Algorithm and theory", "content": ""}, {"title": "A.1. Detailed algorithm for POPPER", "content": ""}, {"title": "A.2. Proof of Theorem 4", "content": "Proof of Theorem 4. Throughout, we condition on the training process of the LLM agents. Under Assumptions 1 and 2, each e-value also obeys $E[e_i | D_{i-1}] \\le 1$ under $H_0$ since $H_0$ implies $h_i^0$ for each i > 1. Define $E_i = \\prod_{s=1}^i e_s$ as the aggregated evidence at each iteration i \u2265 1, and $E_0 = 1$. Also, recall that $F_i = \u03c3(D_i)$ is the filtration in Assumption 3. Then, we have\n$E[E_i | F_{i-1}] = E_{i-1} E[e_i | F_{i-1}] \\le E_{i-1}$,\nwhere we use the takeout property and the fact that $E_{i-1}$ is measurable with respect to $F_{i-1}$. In addition, it is clear that Ei is measurable with respect to Fi. Therefore, {$E_i$}$_{i\u22651}$ is a non-negative super-martingale adapted to the filteration {$F_i$}$_{i\u22651}$. Applying Doob's optional stopping theorem, we know that for any stopping time\u03c4 adapted to the filteration {$F_i$}$_{i\u22651}$, $E := E_\u03c4$ obeys $E[E] \\le E_0 = 1$ under $H_0$. Finally, by Markov's inequality, we know that $P(y = 1) = P(E > 1/\u03b1) \\le \u03b1. E[E] \\le \u03b1$ under Ho, thus completing the proof of Theorem 4."}, {"title": "B. Full related works", "content": "Philosophy of science The philosophical foundations of hypothesis validation are rooted in debates about the nature of scientific inquiry. Central to our framework is Karl Popper's falsificationism (Popper, 1959), which argues that scientific hypotheses cannot be definitively proven but can only be refuted through empirical tests. While Popper emphasized iterative falsification, critiques such as those synthesized in Agassi (Agassi, 2014) highlight tensions between his ideas and those of contemporaries like Thomas Kuhn. Kuhn's paradigm shifts (Kuhn, 1962) challenged falsificationism by emphasizing the sociotechnical embeddedness of scientific progress, a perspective further refined by Lakatos' methodology of scientific research programmes(Lakatos, 1978). Lakatos' framework, which evaluates hypotheses within evolving theoretical systems, aligns with our treatment of auxiliary assumptions (e.g., dataset relevance) as prerequisites for testing, as discussed in (Philosophy Institute, 2023). Modern critiques, such as Rubin (Rubin, 2025), argue that Lakatos' approach mitigates challenges like the replication crisis by emphasizing progressive problem shifts over strict falsification. Similarly, van Fraassen's constructive empiricism (van Fraassen, 1980), which prioritizes empirical adequacy over ontological truth, mirrors our focus on observable implications rather than abstract claims. Goodman's \"grue\" paradox (Goodman, 1983), which interrogates inductive reasoning, underscores the epistemic risks inherent in generalizing from data-risks our framework pragmatically addresses through statistical safeguards like e-values. Maxwell (Maxwell, 2012) positions aim-oriented empiricism as a synthesis of Popperian, Kuhnian, and Lakatosian ideas, advocating for explicit epistemic aims in scientific practice. This resonates with our adaptive sequential testing paradigm, which balances empirical rigor with iterative refinement. While our framework abstracts sociotechnical dimensions noted in Kuhn and Lakatos, the need for transparency in automated systems echoes their emphasis on communal validation (Press, 2009). By integrating these perspectives, POPPER bridges classical philosophy of science and modern data-driven inquiry, offering a scalable yet philosophically grounded approach to hypothesis validation.\nLLM for hypothesis generation. Many methods have used LLM to generate novel research ideas. For example, Wang et al. (2024a), Baek et al. (2024), and Yang et al. (2024b) propose methods for generating creative, domain-specific research ideas. Si et al. (2024) conducted large-scale human studies comparing AI-generated research ideas with those from experts. Moving beyond ideas, many also explore hypothesis generation with LLMs with a focus in the commonsense domains (Gendron et al., 2024; Yang et al., 2024a; Moskvichev et al., 2023; Mirchandani et al., 2023; Tang et al., 2023; Xu et al., 2024a; Han et al., 2023; Xu et al., 2024b; Alet et al., 2021; Webb et al., 2023). Notably, Honovich et al. (2023) explores LLMs' capabilities in inducing rules from example demonstrations. Qiu et al. (2024) and Wang et al. (2024c) further extends this idea to generating and iteratively refining candidate hypotheses from a set of examples or observations. (Majumder et al., 2024) grounds hypothesis generation with a given dataset and a question. However, these works focus on hypothesis generation rather than rigorous validation. POPPER is complementary to this line of research as it takes in a hypothesis (generated from either LLM or human) and develops a systematic, data-driven process for evaluating whether a hypothesis withstands statistical scrutiny.\nLLM for hypothesis testing and experiments. To the best of our knowledge, there is no work that investigates rigorous validation of a free-form hypothesis grounded with data using AI agent. Some studies have tested LLMs' abilities to implement experiments as a form of validation. For example, Tian et al. (2024) and Gu et al. (2024) evaluate LLMs' coding capabilities in executing experimental protocols. While these works focus narrowly on code generation, POPPER presents a framework for validating natural language-based free-form hypothesis. Additionally, prior research into automated scientific discovery has explored combining hypothesis and code generation for end-to-end workflows (Li et al., 2024b; Lu et al., 2024; Ifargan et al., 2024; Majumder et al., 2024). While these studies focus on automation, they often lack rigorous statistical grounding. In contrast, POPPER focuses on the hypothesis testing component and incorporates robust Type-I error control, ensuring the reliability and scientific rigor of its results. (Li et al., 2024a) (CriticAL) used LLMs to identify and evaluate discrepancies between model predictions and data through hypothesis testing. While CriticAL focuses on validating statistical predefined models, POPPER tackles the challenge of validating free-form natural language hypotheses with a sequential falsification framework.\nLLM for automating research. LLMs have also been used for several other research-related tasks, including automated review generation (D'Arcy et al., 2024; Liang et al., 2023), related work curation (Ajith et al., 2024; Press et al., 2024), experiment outcome predictions (Manning et al., 2024; Zhang et al., 2024; Lehr et al., 2024), and future work recommendations (Zhang et al., 2024). While these are interesting applications, our work focuses on hypothesis testing."}, {"title": "C. Limitations", "content": "Type-I error v.s. false discoveries. We view hypothesis validation with POPPER as an initial step towards rigorous automatic scientific discovery. One limitation of our current framework arises from the limitation of Type-I error as an error criterion for scientific discovery. Let us denote rejecting the null as a \u201cdiscovery\", and it is a true discovery if the alternative holds. The shortcoming of Type-I error control is that it does not necessarily imply the discoveries are true (which is more pronounced when POPPER is used to validate many hypotheses). Awareness of this issue emerged much later than the appearance of concept of Type-I error, but has been quite important nowadays in the fields of hypothesis testing, selective inference, and replicability (Ioannidis, 2005; Collaboration, 2015; Benjamini, 2020). To see this point, consider an extreme case where all hypotheses being passed on to POPPER are null ones. Then, the Type-I error control only implies that we reject each hypothesis with no greater than a chance of \u03b1, but every discovery, once made, must be false. Therefore, we stress that one should be cautious in interprating the validated hypotheses by POPPER as true discoveries to act upon.\nIn the following, we discuss possible extended uses of POPPER for more advanced error critria on false discoveries. Consider using POPPER to validate M > 1 (abstract) hypotheses. The family-wise-error-rate (FWER) is the probability of making any false discovery; FWER control at level \u03b1 \u2208 (0, 1) can be achieved by Bonferroni's correction, i.e., running POPPER for each hypothesis at level \u03b1/M and gather all rejected null hypothesis. A more liberal criterion is the false discovery rate (FDR) (Benjamini & Hochberg, 1995), which is the average fraction of false discoveries among all discoveries. The FDR is suitable for measuring the wastage of follow-up resources on validated (rejected) hypotheses. Since our framework produces a valid e-value E\u03c4 for each hypothesis, these e-values can be readily used to derive a set of validated hypotheses with FDR control by employing the eBH procedure (Wang & Ramdas, 2022). However, these use cases are beyond the scope of this work, and we leave the evaluation and further developments of such capabilities of POPPER for future investigation."}, {"title": "D. Error analysis", "content": "In this section, we provide insights into the common failure modes of POPPER. We first manually inspected 20 randomly sampled failed experiment logs produced by POPPER, and created a list of 10 possible failure categories based on the model's behaviors. Table 5 provides detailed definitions of the 10 failure categories. Then, we collected a total of 128 failed experiment logs from benchmark runs across TargetVal-IFNG, TargetVal-IL2, and DiscoveryBench. We then query a reasoning LLM (OpenAI O1) with the failed trajectory logs, the agent's incorrect conclusion, and the ground truth conclusion to automatically categorize each failed experiment into one or more failure modes described in Table 5. We manually checked 30 labeled experiment logs for quality assurance. 93.3% of O1's labels aligned with human judgment. According to Figure 5, 35.9% of the failures accompany the agent misinterpreting the context for p-values. 28.1% and 17.2% of the errors occur when the agent fails to find effective falsification tests or uses tests that breaks implication. 8.6% and 7.0% of the errors are caused by incorrect test implementation and failure to locate relevant data. It is worth noting that we only observed 1 instance of hallucination across 128 failure cases, and no signs of p-hacking were observed."}, {"title": "E. Tests and trajectory analysis", "content": "In this section, we detail how we categorized the statistical and domain-specific tests performed by POPPER during falsification experiments, as well as how we summarized the agent's trajectories for executing each falsification test, as visualized in Figure 3.\nWe parsed and sampled 1500 falsification test designs and their execution logs, and then asked GPT-4o to identify and group the statistical tests performed in the falsification experiments.\nWe limit our analysis of domain-specific tests to biological hypotheses only, as we have an abundance of biological hypotheses from TargetVal benchmark. The other five domains provided by DiscoveryBench contains limited number of unique hypotheses per domain, and the analysis does not converge. We sampled 462 falsification tests proposed by the experiment design agent and used GPT-4o to extract and group them into standardized biological tests.\nFor agent trajectories, we first manually inspected the behaviors of the experiment execution agent over 20 experiments and summarized a list of 11 possible high-level actions taken by the agent. Detailed definitions of these actions are listed in Table 6. We then randomly sampled 80 trajectories of the experiment execution agent, and prompted GPT-4o to convert each trajectory into a list of high-level actions as detailed in Table 6. We observe that the agent's workflow closely mirrors that of a human data analyst. It begins by inspecting the dataset and assembling relevant information, then proceeds with a cycle of"}, {"title": "F. Human study details", "content": "We recruited 11 computational biologists and bioinformaticians (PhD holders or candidates) for our human study, and 9 adhered. Each participant was asked to complete a short questionnaire on their educational background and relevant experience (Listing 1). We present the background distributions of recruited participants in Figure 6. Of the 9 participants, 6 hold (or are pursuing) a PhD, 1 holds a Master's degree, and 2 are postdoctoral researchers. In terms of experience with data analysis and coding for genetic and genomic data, 2 participants identified as beginners, 1 as intermediate, and 6 as experts. Regarding familiarity with statistical hypothesis testing, 2 participants identified as beginners, 2 as intermediate, and 5 as experts. Finally, 6 participants reported that they have never performed wet-lab experiments, while 3 indicated having done SO.\nWe sampled a total of 18 tasks from the TargetVal-IL2 benchmark to evaluate the Type-I error (9 tasks) and statistical power (9 tasks) of our method. Each participant was randomly assigned two tasks to complete. To prevent inference of one hypothesis from the other, a participant might receive two positive, two null, or one positive and one null hypothesis. Participants were free to use the internet or large language models for general coding questions (e.g., library usage, syntax) and statistical tests, but not to query the specific biological hypothesis directly. All conclusions were to be derived solely from the data provided in the TargetVal-IL2 benchmark, with each hypothesis tested at significance level a = 0.1. All work was documented in Jupyter Notebooks."}, {"title": "G. Human Annotation Details", "content": "We randomly sampled 90 falsification test proposals from the three benchmarks. Each of the three annotators first individually annotated a common set of 20 proposals using the same 0.1-1.0 rubric as the Relevance Checker 4. The annotators then discuss and calibrate their decisions and independently annotate 10 more proposals after the calibration. The annotators achieved a Kendall's W of 0.62 before the calibration, and 0.91 post calibration. Finally, each annotator individually annotate a separate set of 20 falsification proposals. The human annotators and the relevance checker agent achieved a Kendall's Tau of 0.43 (p = 1e - 06) and Spearman's correlation of 0.55 (p = 5e \u2013 6). The relevance checker agent ranked 84% of the proposed falsification tests as \"Strongly Relevant\u201d (score >= 0.8), whereas human annotators ranked 77% of the test proposals as \"Strongly Relevant\"."}, {"title": "H. Qualitative Analysis", "content": "This section provides qualitative analysis on one successful falsification trajectory and one failure case trajectory on the TargetVal-IL2 benchmark.\nFigure 10 presents an example trajectory of POPPER running on a TargetVal-IL2 hypothesis. We can see the agent attempted multiple rounds of diverse falsification experiments, including expression correlation analysis, LCP2 regulatory network analysis, LCP2 variant-immune phenotype association test, and LCP2 eQTL-IL2 regulatory region test. POPPER performs sequential error control to rigorously aggregate the evidence from all four experiments, and then rejects the main null hypothesis as the summarized sequential statistics (i.e., cumulated e-values) passes our alpha-threshold of 0.1.\nWe observe that the experiment design agent autonomously refines its proposal to enhance the implication strength and feasibility of the proposed falsification experiment. The experiment execution agent iteratively inspects and interacts with multiple data sources to evaluate the feasibility of the experiment, before implementing and conducting the statistical tests. The experiment execution agent also shows attempts to account for model assumptions and inspect the validity of test statistics before arriving at a final conclusion (e.g., Round 3). We note that with rigorous Type I error control, POPPER also provides more tolerance and leniency for test execution failures. Notice that in Round 1, the experiment execution agent incorrectly concluded that LCP2 and IL2 are not present in the datasets. However, benefiting from the sequential"}, {"title": "I. Prompting Details", "content": "Listings 2, 3, 4, 5, and 6 detail the prompts used for different modules of POPPER."}]}