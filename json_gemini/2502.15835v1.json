{"title": "Pragmatic Reasoning improves LLM Code Generation", "authors": ["Zhuchen Cao", "Sven Apel", "Adish Singla", "Vera Demberg"], "abstract": "Large Language Models (LLMs) have demonstrated impressive potential in translating natural language (NL) instructions into program code. However, user instructions often contain inherent ambiguities, making it challenging for LLMs to generate code that accurately reflects the user's true intent. To address this challenge, researchers have proposed to produce multiple candidates of the program code and then rerank them to identify the best solution. In this paper, we propose CodeRSA, a novel code candidate reranking mechanism built upon the Rational Speech Act (RSA) framework, designed to guide LLMs toward more comprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using one of the latest LLMs on a popular code generation dataset. Our experiment results show that CodeRSA consistently outperforms common baselines, surpasses the state-of-the-art approach in most cases, and demonstrates robust overall performance. These findings underscore the effectiveness of integrating pragmatic reasoning into code candidate reranking, offering a promising direction for enhancing code generation quality in LLMs.", "sections": [{"title": "Introduction", "content": "Recent advances in generative large language models (LLMs) have demonstrated their impressive ability to generate program code from user-provided natural language instructions (Liu et al., 2024b; Coignion et al., 2024). However, given the intrinsic complexities of coding and the potential ambiguities in user input, producing code in a single attempt may fail to explore the vast solution space, overlooking correct or higher-quality solutions (Liu et al., 2024a). A standard practice to address this shortcoming is to sample multiple solutions, which we refer to as \u201ccode candidates\" (Chen et al., 2021; Brown et al., 2024), and to rerank them. Researchers have proposed various reranking strategies for code candidates, broadly divided into execution-driven and content-driven approaches. Due to the safety-risks associated with execution-driven approaches (Yeti\u015ftiren et al., 2023), we here focus on content-driven methods. These evaluate the generated text, often relying on token-level probabilities. For example, Coder reranking scores each candidate based on the cumulative probability of its tokens, sometimes however favoring \u201cdegenerate solutions\" (generic or repetitive code) with disproportionately high token probabilities (Zhang et al., 2023). When viewing code generation as a communicative process in which an LLM listens to the user's intentions (Ouyang et al., 2022), Coder reranking evaluates candidate solutions solely from the listener's perspective. Yet, research on human communication suggests that effective listeners reason about the speaker (who in turn reasons about the listener) (Grice, 1975). Frank and Goodman (2012; 2016) provided a principled method for quantifying this process based on a probabilistic framework based on game-theoretic notions, called the Rational Speech Act (RSA) framework. Pu et al. (2020, 2024) demonstrated the RSA framework's effectiveness on program generation for a simple domain, while Schuster et al. (2024) reported negative results on a spreadsheet domain. One aspect that has held back RSA models from scaling up to realistic use cases is the computational overhead (Pu et al., 2024): it requires reasoning about the set of alternative instructions that the speaker could have given and about the set of alternative pieces of code that could solve the problem, which is very computationally expensive. Zhang et al. (2023) therefore proposed CoderReviewer reranking as a simplified scalable approach that simplifies these probability estimation processes over alternatives. However, it comes at the cost of not fully modelling the dialogic, interactive reasoning that can emerge when speaker and listener exchange information."}, {"title": "Related Work", "content": "Natural Language to Code. Previous research has extensively explored generating code from natural language using neural network models (Ling et al., 2016; Rabinovich et al., 2017; Hayati et al., 2018). Recently, large language models (LLMs) have propelled significant advances in this area, driven by the transformer (Vaswani, 2017) architecture and large-scale pretraining. Their performance on code generation tasks often surpasses that of traditional models, and in many cases even rivals human programmers (Ni et al., 2024; Becker et al., 2023). Moreover, a recent study shows that LLMs also exhibit strong performance in code summarization, effectively translating code snippets into text (Akib et al., 2024).\nDiversity Sampling in Code Generation. In a prior study, Chen et al. (2021) found that allowing the model to generate more candidates significantly raises the probability of yielding a correct answer. This practice, which encourages the model to produce a broader range of potential outputs, is often referred to as diversity sampling.\nCode Reranking Methods. When LLMs generate multiple code candidates in response to the user instruction, execution-driven reranking methods such as CodeT (Chen et al., 2022) and AgentCoder (Huang et al., 2024) involve running test cases to gauge each candidate's correctness. Although often effective, execution-driven approach can introduce additional safety risks and practical obstacles, such as the potential for malicious code execution, environment setup overhead, and resource constraints (Yeti\u015ftiren et al., 2023; Khoury et al., 2023). In contrast, content-driven reranking methods are far more versatile because they do not rely on execution and are not even confined to coding tasks.\nCoder reranking. In prior work, Chen et al. (2021) reranked code candidates by estimating P(c | i), where c denotes the generated code candidate and i denotes the given instruction. This process can also be called Coder reranking because the LLM is a mere Coder that estimates the candidate probability based on the corresponding instruction.\nWhen using an LLM to estimate conditional probabilities, we compute the probability of each token iteratively. For example, in Coder reranking, the model processes candidate's tokens from left to right: at each step, it calculates the probability of the current token given the instruction and the previously generated tokens, then appends that token to the context before moving on. The product of these sequential probabilities across all tokens yields the overall probability of the code candidate under the given instruction:\n$P(c | i_{0}) = \\prod_{t=1}^{|c|} P_{LLM}(c^{(t)} | i_{0}, c^{(<t)})$, where $c^{(t)}$ denotes the token at position t in the sequence c, and $c^{(<t)}$ represents the sequence of all tokens before position t.\nCoderReviewer reranking. Zhang et al. (2023) added the concept of a reviewer to Coder reranking (i.e., reassessing whether the instruction matches the generated code candidate). Formally, CoderReviewer metric is represented as follows:\nCoderReviewer = P(c | i) \u00b7 P(i | c)\n(Coder) (Reviewer)\nThe prompt positions of the instruction and code candidates are reversed in the reviewer's implementation. Thus, the code-generation task is reformulated as an instruction-generation task to calculate the probability of a given instruction. The CoderReviewer metric is also considered a specialized form of maximum mutual information (Li and Jurafsky, 2016), measuring the bidirectional alignment between the generated code and the input instruction."}, {"title": "CodeRSA", "content": "This section introduces CodeRSA, an approach that employs the Rational Speech Act (RSA) framework to enhance the reranking of candidate code snippets. CodeRSA extends the models proposed by Cohn-Gordon et al. (2019) and Schuster et al. (2024). The core innovation in CodeRSA arises from the pragmatic listener, which is responsible for selecting and reranking code candidates. It does so by imagining how a \u201cpragmatic speaker\" would choose an instruction that best distinguishes the intended code among various potential instructions.\nA pragmatic speaker evaluates how effectively a potential instruction communicates the intended code candidate. Within the RSA framework, this evaluation process necessitates an explicit calculation of probabilities. However, given the infinite combinations of possible instructions and code candidates, developing a robust and comprehensive probability estimation strategy becomes essential. To address this challenge, CodeRSA generates additional instructions for each candidate, thereby constructing an expanded set of potential instructions (including the original). Finally, by considering the potential instruction set, a pragmatic speaker can select the most appropriate instruction for each code candidate based on the motivation to effectively convey the message to a literal listener.\nAt the foundation of CodeRSA is the literal listener, who estimates the probability of each code candidate by interpreting the user's instruction word for word, without inferring deeper speaker intent.\nLiteral Listener. A literal listener (denoted $L_{0}$) represents the simplest level of reasoning in the RSA framework. It interprets utterances solely according to their literal meaning, without any higher-level pragmatic reasoning. Theoretically, let c be a candidate code and i be an instruction, then:\n$P_{L_{0}}(c| i) \\propto [i](c)$, where $P_{L_{0}}(c | i)$ denotes the literal listener's estimation of probability of candidate e given instruction i. The interpretation function [i] maps an instruction i to a function that takes a code candidate c and returns 1 if c correctly implements i, and 0 otherwise.\nLikewise, most LLMs generate code by conditioning only on the user's input instruction, which motivates CodeRSA's use of an LLM to instantiate this literal listener:\n$P_{L_{0}}(c | i) = P_{LLM}(c | i)$,\nThe literal listener provides CodeRSA's baseline interpretation of user instructions, serving as the foundation for all subsequent reasoning. Note that the Coder reranking can also be considered a literal listener. For convenience, we refer to the logarithm of $P_{L_{0}}(c | i)$ as the Coder score.\nPragmatic Speaker. In the RSA framework, the pragmatic speaker (denoted $S_{1}$) is primarily responsible for determining whether an instruction i effectively conveys the intended meaning of a candidate code c to the literal listener. Formally, a pragmatic speaker can be defined as:\n$P_{S_{1}}(i | c) = \\frac{exp (log P_{L_{0}}(c | i) \u2013 C(i))}{\\sum_{i'} exp (log P_{L_{0}}(c | i') \u2013 C (i'))}$, Here, C(i) denotes a cost function that quantifies the \"expense\" of employing a particular instruction. This formula quantifies the pragmatic speaker's estimation of the probability that a particular instruction will be used to describe a code candidate, as interpreted from the literal listener's perspective. Note that an idealized RSA approach would consider every possible instruction i', which is infeasible in practice. Instead, CodeRSA leverages the fact that ambiguities in the original instruction often surface in the code candidates generated by LLMs, thereby enabling a sample-based estimation.\nGiven a user-provided instruction $i_{0}$, we request multiple candidate solutions {c\u2081, . . ., Cn } from the LLM. Each candidate cj may capture a slightly different interpretation of $i_{0}$. We then derive one or more instructions from each candidate cj, forming a set of potential instructions {10, 11,..., im}, which we denote by I. This approach ensures that we sample relevant alternative instructions directly tied to how the model interprets the original instruction. Potential instructions then constitute a finite sample set that approximates the otherwise infinite instruction space for pragmatic speaker estimation.\nTo simplify the model and focus on core pragmatic reasoning, we assume a uniform cost for all instructions, which effectively cancels out during normalization. However, a more detailed modeling of the cost function may provide additional insights, a point we further discuss in the Section 6. A pragmatic speaker then can be defined in a simplified form as:\n$P_{S_{1}}(i | c) = \\frac{P_{L_{0}}(c | i)}{\\sum_{i'\\in I} P_{L_{0}}(c | i')}$,\nIn practice, the distribution of $P_{L_{0}}(c | i)$ is often highly skewed, with a few high probability candidates dominating and a large spread in probability estimates. To address this, we here propose to instead work with log probabilities, which in practice brings low probability alternatives closer together and gives more weight to the relative trends among instruction-code pairs. The pragmatic speaker's preference score for a given instruction is hence quantified as:\n$R_{S_{1}}(i | c) = -\\frac{log P_{L_{0}}(c | i)}{\\sum_{i'\\in I} log P_{L_{0}}(c | i')}$,\n$R_{S_{1}}(i | c) \\in (\u22121, 0)$.\nIn log space, after normalizing with respect to log probabilities, smallest numbers will now correspond to the events with the highest probability. Therefore, we invert the sign to negative in order to still choose maximal score instructions. Notably, if c is overly generic or appears plausible under multiple instructions, then the denominator $\\sum_{i'\\in I} log P_{L_{0}}(c | i')$ increases, resulting in a lower overall score. This log-based metric prevents inflated scores for candidates that might superficially fit many different instructions, thereby ensuring that each instruction is evaluated on a relatively fair basis. An ablation study (Appendix A.3) confirmed that omitting the logarithmic transformation significantly degrades normalization and impairs CodeRSA's reranking capability, with Appendix A.4 offering geometric mean-based theoretical support.\nOverall, at the pragmatic speaker level, CodeRSA leverages the code candidates themselves to sample potential instructions, capturing the diversity of possible interpretations. It then uses the literal listener's estimation to quantify how effectively each potential instruction reflects the meaning of a code candidate.\nPragmatic Listener. The pragmatic listener (denoted $L_{1}$) re-examines the original instruction $i_{0}$ across all candidates, completing the backward reasoning guided by the pragmatic speaker's preferences. In prior work, Degen (2023) defined a pragmatic listener as:\n$P_{L_{1}}(c | i) \\propto P_{S_{1}}(i | c) \\cdot P(c)$,\nHere, P(c) denotes the prior probability of a given code candidate. We make the simplifying assumption that all candidates are equally likely a priori, which allows us to treat P(c) as a constant and omit it from the calculations. Note that this uniform prior assumption may not hold in real-world scenarios, as some code might be inherently more common than others. We will discuss this assumption further in Section 6. Consequently, in CodeRSA, the pragmatic listener reranks candidates by their pragmatic speaker scores:\n$P_{L_{1}}(c | i) \\propto R_{S_{1}}(i | c)$,\nso that the candidate with the highest pragmatic speaker score for $i_{0}$ is selected as optimal.\nFrom CodeRSA's reasoning process, it follows that, for each candidate, the LLM is invoked m + 1 times, where m represents the number of generated instructions. Consequently, the overall computational complexity of CodeRSA is O(n(m + 1)), which grows quadratically when both n and m increase equally."}, {"title": "Experiment Setup", "content": "To analyze the merits of CodeRSA, we evaluate the performance of the three reranking methods (Coder, CoderReviewer, and CodeRSA) on a standard program code generation dataset using one of the latest language models. Since the strength of content-driven methods is their generality, we skipped extensive parameter tuning and simply used the default or commonly used parameters throughout our experiments.\nDataset and Model\nThe manually crafted HumanEval evaluation dataset (Chen et al., 2021) was designed to assess a model's ability to convert natural language instructions into program code. It comprises 164 questions, each framed as an unfinished Python function starting with a brief instruction that describes how the function's code should be completed.\nWe selected HumanEval for its proven role in code generation evaluation and balanced difficulty. For example, simpler datasets like CoNaLa (Yin et al., 2018) already yield near-perfect performance for the Coder model, leaving little room for reranking improvements. In contrast, more challenging datasets such as Bigcodebench (Zhuo et al., 2024) often produce many unsolvable instances, suggesting that more powerful models or improved problem modeling is needed.\nDespite the availability of HumanEval for code generation, no dedicated and reliable benchmark currently exists for reranking generated code-a specialized subtask within code generation. Accordingly, the benchmark setup in our experiments involves three key stages: program code generation, testing (only for evaluation purposes), and reranking. Because LLM-based code generation is highly sensitive to hyperparameters and hardware configurations, we adopt a multi-sampling strategy. Specifically, for each of the 164 HumanEval problems, we first generate 300 candidate solutions at a temperature of 0.7. Then, we randomly select 50 problems, sample 10 candidates per problem, and repeat this process 10 times. Finally, we evaluate three different reranking methods on each resulting subset to measure their effectiveness.\nIn our experiments, we focus on the Llama-3-8B-Instruct variant, which comprises 8 billion parameters and is fine-tuned for instruction-following tasks (Grattafiori et al., 2024). Llama-3-8B-Instruct is renowned for its lightweight design and robust performance, effectively balancing computational requirements with high-quality generation. Moreover, the HumanEval dataset poses a moderate challenge to the model, ensuring that it is neither overwhelmed by difficulty nor under-challenged. These characteristics render the model particularly suitable for evaluating the effectiveness of reranking methods in practical scenarios.\nAdditionally, we evaluated Llama-2-70b-chat (Touvron et al., 2023) on a subset of HumanEval; however, its limited performance in code and instruction generation (and related poor probability estimates) led to its exclusion.\nImplementation of Reranking Methods\nBaselines. The Coder reranking method provides a straightforward way to compare the probability of a code candidate e given the original instruction i. Specifically, it concatenates the instruction and code candidate in order (see Fig. 2, part A), prompting the language model to output token probabilities for the candidate sequentially. The product of these token probabilities then yields the cumulative probability of the entire code snippet. As mentioned in Section 3, Coder reranking can also be considered a literal listener-level approximation to $P(c | i)$; therefore, we use it as a baseline.\nAnother baseline, \u201crandom\", is defined as the ratio of correct candidates to total candidates in each subset. By using the expected proportion of correct codes, this approach minimizes the impact of random seeds and naturally reflects the inherent difficulty of each sub-dataset.\nState-of-the-art Method. Zhang et al. (2023) showed that CoderReviewer reranking (see Section 2 for details) outperforms Coder reranking and rivals execution-driven methods such as CodeT.\nIn practice, we use the same prompt format as in Coder reranking to compute P(c | i). To compute P(i | c), the order of the instruction and the generated code snippet is reversed in the prompt (see Appendix A.5.2).\nCodeRSA. To balance running time and limited computing resources, we restrict the RSA reranking procedure in our experiments to 10 candidates per question. We then apply a one-shot prompt (see Fig. 2, part B) to the LLM to generate one instruction for each candidate, yielding a set of potential instructions {10, 11, . . ., i10 }, where io is the original instruction. Consistent with Coder reranking's input prompt, we then compute the probability of each code candidate c' under each potential instruction i', forming an 11 \u00d7 10 Coder score matrix. Then, we derive pragmatic speaker scores for each candidate with respect to the original instruction by contrasting how the candidate performs under io versus under the other generated instructions. Finally, we use these pragmatic scores to rerank the candidates, selecting the one with the highest pragmatic speaker score for io."}, {"title": "Results", "content": "Quantitative Analysis\nIn this section, we quantitatively evaluate three reranking methods, including CodeRSA, in terms of accuracy using ten subsets sampled from the HumanEval dataset and their corresponding generated code candidates.\nHere, accuracy is defined as the fraction of candidates selected by a reranking method that pass all test cases relative to the total number of candidates in the subset. Fig. 3 presents box plots comparing the accuracy distributions of the three reranking methods and a random baseline. Variance is estimated from bootstrapping across ten subsets. CodeRSA attains the highest mean accuracy, with relatively narrow interquartile ranges indicating consistent performance. CoderReviewer shows moderate accuracy, with a mean exceeding Coder's but still below CodeRSA. Coder reranking exhibits a relatively large variance, indicating that its performance is less stable and more prone to fluctuations, likely due to its tendency to favor overly generic solutions.\nCoder's lower performance suggests that relying solely on a literal listener perspective is insufficient for effective reranking and that CodeRSA's more complex reasoning leads to superior performance, demonstrating the benefits of a comprehensive RSA modeling approach.\nQualitative Analysis\nAlthough our experiments show that CodeRSA achieves stable performance, it relies on certain idealized assumptions and an abstract reasoning process. To provide a more intuitive perspective, we include a qualitative analysis that examines how CodeRSA aligns with core RSA intuitions, thereby enhancing reranking quality.\nZhang et al. (2023) observed that language models often generate \u201cdegenerate\" programs\u2014overly generic yet repetitive code candidates. In our analysis, we observe that Coder reranking indeed favors these degenerate outputs because it evaluates the cumulative token likelihood of a candidate c, given the original instruction $i_{0}$: \n$P(C | i_{0}) = \\prod_{t=1}^{|c|} P_{LLM} (c(t) | i_{0}, c(<t))$,\nwhere c(t) denotes the token at position t, and c(<t) represents the sequence of the preceding tokens. Each factor PLLM(c(t) | i0, c(<t)) is strictly less than 1; thus, longer sequences accumulate a lower overall probability, biasing Coder reranking towards shorter, potentially degenerate code. Nonetheless, differences in token-level probabilities can offset this bias: a longer but logically correct candidate may achieve a higher overall probability if each token is assigned a sufficiently high probability.\nWith recent advances in LLMs, generating degenerate programs has become less common, although incomplete functionality remains a concern. For example, in Fig. 4a, the original instruction requires returning the greatest integer above zero whose frequency is, at least, its own value, or \u22121 if none exists. Nevertheless, code_09 omits the \"greatest\" integer requirement and the \u20131 fallback, making it shorter and more likely to be highly ranked by Coder.\nFig. 4b presents the Coder scores log PL0(c | i) for function_01 and function_09 under all potential instructions (including i0). Additionally, code_01 achieves a log-probability of -29.24 for i0, whereas code_09 reaches -21.12. Consequently, Coder reranking selects the incomplete code_09. The Reviewer component alone is insufficient to offset code_09's higher Coder score; hence CoderReviewer also prefers code_09. The difference between Coder and Reviewer scores highlights a major flaw in the CoderReviewer method. Since these scores often differ significantly, treating them as equally important is not optimal. Additionally, trying to adjust their weights can add more complexity and uncertainty.\nFig. 4c reports the pragmatic speaker scores Rs1(i | c) for each instruction-code pair. Notably, code_01 receives a score of -0.0458 with i0, whereas code_09 has -0.0533. Acting as a pragmatic listener, CodeRSA selects code_01, which more closely aligns with i0 from a pragmatic speaker's perspective.\nAlthough the score calculation in our method uses log probabilities, two observations confirm that our method still achieves the core intuitions put forward by the RSA framework:\nFirstly, as can be seen in Fig. 4b, code_01 has a lower overall Coder score than code_09, likely because it is longer and more complicated. However, under i0, code_01 obtains a higher Coder score relative to other possible instructions, which then translates into a higher pragmatic speaker score after normalization. This process shows that CodeRSA examines each candidate's relative scores across all potential instructions to verify whether it truly ranks highly under the original instruction.\nSecondly, as shown in Fig. 4b, code_09 receives multiple high Coder scores, especially under the instruction generated by itself. From the literal listener's perspective, the self-generated instruction more precisely describes code_09, raising its overall Coder scores. Crucially, because the speaker's score is normalized by the sum of log probabilities across all instructions for a given candidate, a higher Coder score on the self-generated instruction will reduce the relative weight assigned to the original instruction. Consequently, the pragmatic speaker will prefer to use another instruction for code_09 instead of the user's original instruction. Moreover, as can be seen in Fig. 4a, the self-generated instruction for code_09 matches its details more closely."}, {"title": "Discussion", "content": "Our proposed CodeRSA approach contains a number of simplifications compared to the original RSA model, which has been developed for describing human-human communication: (1) we assumed a uniform speaker cost for the instructions. While this simplification makes the analysis more tractable, it means that our model does not currently take into account effects related to how \"costly\" an instruction would be to produce for the human speaker. Future work should investigate variable cost structures to better capture these nuances.\nWe also assume that all code candidates are equally likely a priori, effectively omitting the candidate prior P(c) from our calculations. Although this assumption allows us to concentrate solely on the pragmatic speaker's score, it might not reflect real-world scenarios where some code candidates are more common due to usage frequency, domain-specific patterns, or contextual relevance. Future work should explore the use of non-uniform priors to better capture candidate plausibility.\nIn section 4, we argued that our approach, as a reranking approach, is most beneficial in situations where the dataset is not too easy (when a simple Coder model already achieves ceiling performance) and not too difficult, such that we can still obtain a high quality probability distribution over instructions and over code candidates. This raises the question of the relevance of pragmatic reasoning for code generation, and more generally in communication. Research on human communication has demonstrated the importance of pragmatic reasoning in communication despite its apparent computational overhead, while at the same time suggesting that humans may also learn to use simple heuristics or amortized estimates (Pu et al., 2024) to not engage in iterative reasoning in easy cases, while still being able to employ the full reasoning procedure in more difficult cases where it is necessary."}, {"title": "Conclusion", "content": "This work introduces CodeRSA, a candidate reranking algorithm for the generation of program code grounded in the Rational Speech Act framework. By modeling the iterative reasoning of a pragmatic listener about a pragmatic speaker, CodeRSA consistently outperforms the Coder reranking baseline and surpasses the state-of-the-art CoderReviewer approach. A qualitative analysis further reveals that, even when incorporating certain idealized assumptions and variations, CodeRSA remains faithful to the core principles of the RSA framework. These results highlight the effectiveness of applying well-established linguistic frameworks to enhance reasoning in language models, opening new avenues for research and development in code-related tasks."}, {"title": "Limitations", "content": "A known limitation of RSA approaches is their computational complexity and associated resource consumption. For example, on a single NVIDIA Tesla A100 (PCIe 4.0, 80GB HBM2e, 300W), performing complete CodeRSA inference on 500 instances takes nearly 8 hours. Our approach compares each potential instruction with every candidate, leading to a quadratic increase in complexity as the number of candidates grows. Although CodeRSA can theoretically handle many candidates, we limited our experiments to ten candidates per question to keep runtime and hardware usage manageable. This restriction inevitably narrows the variety of solutions and may affect how well the approach generalizes to larger-scale scenarios.\nReducing the computational overhead is a major goal for our future work. One promising direction is to design more lightweight scoring mechanisms or to adopt a multi-stage pipeline. For instance, a coarse filtering step could quickly discard low-probability solutions before applying CodeRSA's full RSA-based reasoning to a smaller top-ranked subset. Alternatively, approximate models could reduce the number of token-level evaluations required, thereby preserving much of CodeRSA's pragmatic reasoning benefits at a fraction of the computational cost. Such improvements would allow CodeRSA to scale more effectively and broaden its applicability to larger code generation tasks.\nAnother limitation is that while we employed a multi-sampling strategy to mitigate uncertainty, we have only utilized a single dataset and one model so far. We are currently working on incorporating several balanced-difficulty datasets\u2014such as MBPP (Austin et al., 2021) and DS-1000 (Lai et al., 2023)\u2014along with additional open-source models like Mistral (Jiang et al., 2023) and Qwen (Bai et al., 2023). This expansion will allow us to further assess the effectiveness of different reranking methods across diverse scenarios, ultimately leading to a more robust evaluation of our approach."}, {"title": "Appendix", "content": "A Conjecture: Explaining\nCoderReviewer from an RSA Perspective\nIn the RSA framework, a pragmatic listener's posterior over a candidate e given an instruction i is\ncommonly expressed as:\n$P_{L_{1}}(c| i) \\propto P_{S_{1}}(i | c) \\cdot P(c)$,\nwhere $P_{S_{1}}(i | c)$ represents how likely a pragmatic\nspeaker would be to produce instruction i when\nthe correct candidate is c, and P(c) is the prior\nlikelihood of c.\nTranslating this perspective to LLMs, we hy-\npothesize that when generating instructions (the\n\"Reviewer\" role), it is relatively straightforward\nfor the model to produce abstract instructions from\nconcrete code. Since code is unambiguous, the\nLLM can approximate a pragmatic speaker:\n$P_{LLM}(i | c) \\approx P_{S_{1}}(i | c)$.\nHowever, generating code from abstract instruc-\ntions (the \"Coder\" role) is substantially more dif-\nficult. In this setting, the LLM may effectively re-\nvert to estimating a prior over possible candidates,\nthereby approximating:\n$P_{LLM}(c | i) \\approx P(c)$.\nFrom this RSA standpoint, the CoderReviewer\nparadigm can be considered a simplified, yet broad,\nmodeling of a pragmatic listener.\nMore Details of Results\nTable 1 summarizes the average accuracy and stan-\ndard deviation across ten sampled subsets.\nMean accuracy (with standard deviation) across\n10 subsamples for each reranking method.\nMethod Mean Accuracy Std. Dev.\nCoder 0.48 0.059\nCoderReviewer 0.51 0.043\nCodeRSA 0.55 0.044\nrandom 0.43 0.030\nAs shown in Table 1, CodeRSA achieves the\nhighest average accuracy (0.55), outperforming\nCoder (0.48), CoderReviewer (0.51), and the ran-\ndom baseline (0.43). This indicates that CodeRSA\nprovides stronger overall performance compared\nto the other methods. From a standard-deviation\nperspective, CoderReviewer and CodeRSA both\nexhibit relatively stable performance across differ-\nent subsamples. By contrast, Coder has the high-\nest standard deviation (0.059), suggesting greater\nsensitivity to varying combinations of coding prob-\nlems.\nFig. 5 depicts the per-trial accuracy differ-\nence between CodeRSA and CoderReviewer (i.e.,\nCodeRSA accuracy minus CoderReviewer accu-\nracy). Bars above zero indicate that CodeRSA out-\nperforms CoderReviewer, while the single negative\nbar (Trial 2) represents the only instance where\nCodeRSA yields a lower accuracy. In most trials,\nTo investigate the impact of the log transforma-\ntion in the calculation of the pragmatic speaker\nscore, we conducted an ablation study compar-\ning the CodeRSA's modeling of pragmatic speaker\nscore:\n$R_{S_{1}}(i | c) = -\\frac{log P_{L_{0}}(c | i)}{\\sum_{i'\\in I} log P_{L_{0}}(c | i')}$,\n$R_{S_{1}}(i | c) \\in (\u22121, 0)$.\nagainst a variant that directly utilizes the normal-\nized probability:\n$P_{S_{1}}(i | c) = \\frac{P_{L_{0}}(c | i)}{\\sum_{i'\\in I} P_{L_{0}}(c | i')}$referred to as CodeRSA_nolog.\nFig. 6 displays the box plots of accuracy for dif-\nferent methods on ten subsets. From the results,\nCodeRSA achieves the highest mean accuracy with\na narrow interquartile range, indicating stable per-\nformance. In contrast, CodeRSA_nolog shows a\nmarked decrease in accuracy, evidenced by a lowest"}]}