{"title": "SymmetricDiffusers: Learning\nDiscrete Diffusion on Finite Symmetric Groups", "authors": ["Yongxing Zhang", "Donglin Yang", "Renjie Liao"], "abstract": "Finite symmetric groups Sn are essential in fields such as combinatorics, physics,\nand chemistry. However, learning a probability distribution over Sn poses signif-\nicant challenges due to its intractable size and discrete nature. In this paper, we\nintroduce SymmetricDiffusers, a novel discrete diffusion model that simplifies the\ntask of learning a complicated distribution over Sn by decomposing it into learning\nsimpler transitions of the reverse diffusion using deep neural networks. We identify\nthe riffle shuffle as an effective forward transition and provide empirical guidelines\nfor selecting the diffusion length based on the theory of random walks on finite\ngroups. Additionally, we propose a generalized Plackett-Luce (PL) distribution for\nthe reverse transition, which is provably more expressive than the PL distribution.\nWe further introduce a theoretically grounded \"denoising schedule\" to improve sam-\npling and learning efficiency. Extensive experiments show that our model achieves\nstate-of-the-art or comparable performances on solving tasks including sorting\n4-digit MNIST images, jigsaw puzzles, and traveling salesman problems. Our code\nis released at https://github.com/NickZhang53/SymmetricDiffusers.", "sections": [{"title": "Introduction", "content": "As a vital area of abstract algebra, finite groups provide a structured framework for analyzing symme-\ntries and transformations which are fundamental to a wide range of fields, including combinatorics,\nphysics, chemistry, and computer science. One of the most important finite groups is the finite\nsymmetric group Sn, defined as the group whose elements are all the bijections (or permutations)\nfrom a set of n elements to itself, with the group operation being function composition.\nClassic probabilistic models for finite symmetric groups Sn, such as the Plackett-Luce (PL) model\n[38, 29], the Mallows model [30], and card shuffling methods [10], are crucial in analyzing preference\ndata and understanding the convergence of random walks. Therefore, studying probabilistic models\nover Sn through the lens of modern machine learning is both natural and beneficial. This problem is\ntheoretically intriguing as it bridges abstract algebra and machine learning. For instance, Cayley's\nTheorem, a fundamental result in abstract algebra, states that every group is isomorphic to a subgroup\nof a symmetric group. This implies that learning a probability distribution over finite symmetric\ngroups could, in principle, yield a distribution over any finite group. Moreover, exploring this problem\ncould lead to the development of advanced models capable of addressing tasks such as permutations\nin ranking problems, sequence alignment in bioinformatics, and sorting.\nHowever, learning a probability distribution over finite symmetric groups Sn poses significant\nchallenges. First, the number of permutations of n objects grows factorially with n, making the"}, {"title": "Related Works", "content": "Random Walks on Finite Groups. The field of random walks on finite groups, especially finite\nsymmetric groups, have been extensively studied by previous mathematicians [40, 12, 4, 41]. Tech-\nniques from a variety of different fields, including probability, combinatorics, and representation\ntheory, have been used to study random walks on finite groups [41]. In particular, random walks on\nfinite symmetric groups are first studied in the application of card shuffling, with many profound\ntheoretical results of shuffling established. A famous result in the field shows that 7 riffle shuffles are\nenough to mix up a deck of 52 cards [4], where a riffle shuffle is a mathematically precise model that\nsimulates how people shuffle cards in real life. The idea of shuffling to mix up a deck of cards aligns\nnaturally with the idea of diffusion, and we seek to fuse the modern techniques of diffusion models\nwith the classical theories of random walks on finite groups.\nDiffusion Models. Diffusion models [43, 44, 17, 45] are a powerful class of generative models\nthat typically deals with continuous data. They consist of forward and reverse processes. The\nforward process is typically a discrete-time continuous-state Markov chain or a continuous-time\ncontinuous-state Markov process that gradually adds noise to data, and the reverse process learn\nneural networks to denoise. Discrete (state space) diffusion models have also been proposed to\nhandle discrete data like image, text [3], and graphs [49]. However, existing discrete diffusion models\nfocused on cases where the state space is small or has a special (e.g., decomposable) structure and are\nunable to deal with intractable-sized state spaces like the symmetric group. In particular, [3] requires\nan explicit transition matrix, which has size n! \u00d7 n! in the case of finite symmetric groups and has no\nsimple representations or sparsifications. Finally, other recent advancement includes continuous-time\ndiscrete-state diffusion models [7, 46] and discrete score matching models [32, 28], but the nature of\nsymmetric groups again makes it non-trivial to adapt to these existing frameworks.\nDifferentiable Sorting and Learning Permutations. A popular paradigm to learn permutations\nis through differentiable sorting or matching algorithms. Various differentiable sorting algorithms\nhave been proposed that uses continuous relaxations of permutation matrices [14, 9, 5], or uses\ndifferentiable swap functions [36, 37, 21]. The Gumbel-Sinkhorn method [31] has also been proposed\nto learn latent permutations using the continuous Sinkhorn operator. Such methods often focus on\nfinding the optimal permutation instead of learning a distribution over the finite symmetric group.\nMoreover, they tend to be less effective as n grows larger due to their high complexities."}, {"title": "Learning Diffusion Models on Finite Symmetric Groups", "content": "We first introduce some notations. Fix $n \\in \\mathbb{N}$. Let $[n]$ denote the set $\\{1, 2, ..., n\\}$. A permutation\n$\\sigma$ on $[n]$ is a function from $[n]$ to $[n]$, and we usually write $\\sigma$ as $\\begin{pmatrix}\n1 & 2 & ... & n\\\\\n\\sigma(1) & \\sigma(2) & ... & \\sigma(n)\n\\end{pmatrix}$. The\nidentity permutation, denoted by $Id$, is the permutation given by $Id(i) = i$ for all $i \\in [n]$. Let"}, {"title": "Forward Diffusion Process: Card Shuffling", "content": "Suppose we observe a set of objects $\\mathcal{X}$ and their ranked list $X_0$. They are assumed to be generated\nfrom an unknown data distribution in an IID manner, i.e., $X_0, \\mathcal{X} \\sim \\mathbb{P}_{data}(\\mathcal{X}, \\mathcal{X})$. One can construct a\nbijection between a ranked list of $n$ objects and an ordered deck of $n$ cards. Therefore, permuting\nobjects is equivalent to shuffling cards. In the forward diffusion process, we would like to add\n\"random noise\" to the rank list so that it reaches to some known stationary distribution like the\nuniform. Formally, we let $\\mathcal{S} \\subset S_n$ be a set of permutations that are realizable by a given shuffling\nmethod in one step. $\\mathcal{S}$ does not change across steps in common shuffling methods. We will provide"}, {"title": "Card Shuffling Methods", "content": "We now consider several popular shuffling methods as the forward transition, i.e., random transpo-\nsitions, random insertions, and riffle shuffles. Different shuffling methods provide different design\nchoices of $q(\\sigma_t)$, thus corresponding to different forward diffusion processes. Although all these\nforward diffusion processes share the same stationary distribution, i.e., the uniform, they differ in\ntheir mixing time. We will introduce stronger quantitative results on their mixing time later.\nRandom Transpositions. One natural way of shuffling is to swap pairs of objects. Formally, a\ntransposition or a swap is a permutation $\\sigma \\in S_n$ such that there exist $i \\neq j \\in [n]$ with $\\sigma(i) = j$,\n$\\sigma(j) = i$, and $\\sigma(k) = k$ for all $k \\notin \\{i, j\\}$, in which case we denote $\\sigma = (ij)$. We let $\\mathcal{S} =$\n$\\{(ij) : i \\neq j \\in [n]\\} \\cup \\{Id\\}$. For any time $t$, we define $q(\\sigma_t)$ by choosing two indices from $[n]$\nuniformly and independently and swap the two indices. If the two chosen indices are the same, then\nthis means that we have sampled the identity permutation. Specifically, $q(\\sigma_t = (ij)) = \\frac{2}{n^2}$\nwhen $i \\neq j$ and $q(\\sigma_t = Id) = \\frac{1}{n}$.\nRandom Insertions. Another shuffling method is to insert the last piece to somewhere in the middle.\nLet $\\text{insert}_i$ denote the permutation that inserts the last piece right before the $i$th piece, and let\n$\\mathcal{S} := \\{\\text{insert}_i : i \\in [n]\\}$. Note that $\\text{insert}_n = Id$. Specifically, we have $q(\\sigma_t = \\text{insert}_i) = \\frac{1}{n}$\nwhen $i \\neq n$ and $q(\\sigma_t = Id) = \\frac{1}{n}$.\nRiffle Shuffles. Finally, we introduce the riffle shuffle, a method similar to how serious card players\nshuffle cards. The process begins by roughly cutting the deck into two halves and then interleaving the\ntwo halves together. A formal mathematical model of the riffle shuffle, known as the GSR model, was\nintroduced by Gilbert and Shannon [12], and independently by Reeds [40]. The model is described\nas follows. A deck of $n$ cards is cut into two piles according to binomial distribution, where the\nprobability of having $k$ cards in the top pile is $\\binom{n}{k}/2^n$ for $0 \\leq k \\leq n$. The top pile is held in the\nleft hand and the bottom pile in the right hand. The two piles are then riffled together such that, if\nthere are $A$ cards left in the left hand and $B$ cards in the right hand, the probability that the next card\ndrops from the left is $A/(A + B)$, and from right is $B/(A + B)$. We implement the riffle shuffles\naccording to the GSR model. For simplicity, we will omit the term \u201cGSR\u201d when referring to riffle\nshuffles hereafter.\nThere exists an exact formula for the probability over $S_n$ obtained through one-step riffle shuffle.\nLet $\\sigma \\in S_n$. A rising sequence of $\\sigma$ is a subsequence of $\\sigma$ constructed by finding a maximal\nsubset of indices $i_1 < i_2 < ... < i_j$ such that permuted values are contiguously increasing, i.e.,\n$\\sigma(i_2) - \\sigma(i_1) = \\sigma(i_3) - \\sigma(i_2) = ... = \\sigma(i_j) - \\sigma(i_{j-1}) = 1$. For example, the permutation\n$\\begin{pmatrix}\n1 & 2 & 3 & 4 & 5\\\\\n1 & 4 & 2 & 5 & 3\n\\end{pmatrix}$\nhas 2 rising sequences, i.e., 123 (red) and 45 (blue). Note that a permutation\nhas 1 rising sequence if and only if it is the identity permutation. Denoting by $q_{RS}(\\sigma)$ the probability\nof obtaining $\\sigma$ through one-step riffle shuffle, it was shown by [4] that\n$q_{RS}(\\sigma) = \\begin{cases}\n\\frac{1}{2^n}\\binom{n + 2 - r}{n} & \\text{if } \\sigma = Id\\\\\n\\frac{n+1}{2^n} & \\text{if } \\sigma \\text{ has two rising sequences}\\\\\n0 & \\text{otherwise},\n\\end{cases}$"}, {"title": "Mixing Times and Cut-off Phenomenon", "content": "All of the above shuffling methods have the uniform distribution as the stationary distribution.\nHowever, they have different mixing times (i.e., the time until the Markov chain is close to its\nstationary distribution measured by some distance), and there exist quantitative results on their mixing\ntimes. Let $q \\in \\{q_{RT}, q_{RI}, q_{RS}\\}$, and for $t \\in \\mathbb{N}$, let $q^{(t)}$ be the marginal distribution of the Markov\nchain after $t$ shuffles. We describe the mixing time in terms of the total variation (TV) distance\nbetween two probability distributions, i.e., $D_{TV}(q^{(t)}, u)$, where $u$ is the uniform distribution.\nFor all three shuffling methods, there exists a cut-off phenomenon, where $D_{TV}(q^{(t)}, u)$ stays around\n1 for initial steps and then abruptly drops to values that are close to 0. The cut-off time is the time\nwhen the abrupt change happens. For the formal definition, we refer the readers to Definition 3.3 of\n[41]. In [41], they also provided the cut-off time for random transposition, random insertion, and\nriffle shuffle, which are $\\frac{n}{2} \\log n$, $n \\log n$, and $\\frac{3}{2} \\log_2 n$ respectively. Observe that the riffle shuffle\nreaches the cut-off much faster than the other two methods, which means it has a much faster mixing\ntime. Therefore, we use the riffle shuffle in the forward process."}, {"title": "The Reverse Diffusion Process", "content": "We now model the reverse process as another Markov chain conditioned on the set of objects $\\mathcal{X}$. We\ndenote the set of realizable reverse permutations as $\\mathcal{T}$, and the neighbours of $\\mathcal{X}$ with respect to $\\mathcal{T}$ as\n$N_\\mathcal{T}(\\mathcal{X}) := \\{Q_\\sigma \\mathcal{X} : \\sigma \\in \\mathcal{T}\\}$. The conditional joint distribution is given by\n$\\mathbb{P}_{\\theta}(X_{0:T}|\\mathcal{X}) = \\mathbb{P}(X_T|\\mathcal{X}) \\prod_{t=1}^T \\mathbb{P}_{\\theta}(X_{t-1}|X_t),$ (4)\nwhere $\\mathbb{P}_{\\theta}(X_{t-1}|X_t) = \\sum_{\\sigma_t \\in \\mathcal{T}} \\mathbb{P}(X_{t-1}|X_t, \\sigma_t)\\mathbb{P}_{\\theta}(\\sigma_t|X_t)$. To sample from $\\mathbb{P}(X_T|\\mathcal{X})$, one simply\nsamples a random permutation from the uniform distribution and then shuffle the objects accordingly\nto obtain $X_T$. $\\mathbb{P}(X_{t-1}|X_t, \\sigma')$ is again a delta distribution $\\delta(X_{t-1} = Q_{\\sigma'} X_t)$. We have\n$\\mathbb{P}_{\\theta}(X_{t-1}|X_t) = \\begin{cases} \\mathbb{P}_{\\theta}(\\sigma|X_t) \\quad \\text{if } X_{t-1} \\in N_\\mathcal{T}(X_t)\\\\\n0 \\quad \\text{otherwise},\n\\end{cases}$ (5)\nwhere $X_{t-1} \\in N_\\mathcal{T}(X_t)$ is equivalent to $\\sigma' \\in \\mathcal{T}$ and $X_{t-1} = Q_{\\sigma'} X_t$. In the following, we will\nintroduce the specific design choices of the distribution $\\mathbb{P}_{\\theta}(\\sigma|X_t)$."}, {"title": "Inverse Card Shuffling", "content": "A natural choice is to use the inverse operations of the aforementioned card shuffling operations in\nthe forward process. Specifically, for the forward shuffling $\\mathcal{S}$, we introduce their inverse operations\n$\\mathcal{T} := \\{\\sigma^{-1} : \\sigma \\in \\mathcal{S}\\}$, from which we can parameterize $\\mathbb{P}_{\\theta}(\\sigma|X_t)$.\nInverse Transposition. Since the inverse of a transposition is also a transposition, we can let\n$\\mathcal{T} := \\mathcal{S} = \\{(ij) : i \\neq j \\in [n]\\} \\cup \\{Id\\}$. We define a distribution of inverse transposition (IT) over\n$\\mathcal{T}$ using $n + 1$ real-valued parameters $s = (s_1, ..., s_n)$ and $\\tau$ such that\n$\\mathbb{P}_{IT}(\\sigma) = \\begin{cases}\n1 - \\phi(\\tau) & \\text{if } \\sigma = Id,\\\\\n\\phi(\\tau) \\frac{\\exp(s_i)}{\\sum_{k=1}^n \\exp(s_k)} + \\frac{\\exp(s_j)}{\\sum_{k \\neq i} \\exp(s_k)} & \\text{if } \\sigma = (ij), i \\neq j,\n\\end{cases}$ (6)\nwhere $\\phi(\\cdot)$ is the sigmoid function. The intuition behind this parameterization is to first handle the\nidentity permutation $Id$ separately, where we use $\\phi(\\tau)$ to denote the probability of not selecting\n$Id$. Afterwards, probabilities are assigned to the transpositions. A transposition is essentially an\nunordered pair of distinct indices, so we use $n$ parameters $s = (s_1, ..., s_n)$ to represent the logits\nof each index getting picked. The term in parentheses represents the probability of selecting the"}, {"title": "The Plackett-Luce Distribution and Its Generalization", "content": "Other than specific inverse shuffling methods to parameterize the reverse process, we also consider\ngeneral distributions $\\mathbb{P}_{\\theta}(\\sigma|X_t)$ whose support are the whole $S_n$, i.e., $\\mathcal{T} = S_n$.\nThe PL Distribution. A popular distribution over $S_n$ is the Plackett-Luce (PL) distribution [38, 29],\nwhich is constructed from $n$ real-valued scores $s = (s_1, ..., s_n)$ as follows,\n$\\mathbb{P}_{PL}(\\sigma) = \\prod_{i=1}^n \\frac{\\exp(s_{\\sigma(i)})}{\\sum_{j=i}^n \\exp(s_{\\sigma(j)})} ,$ (8)\nfor all $\\sigma \\in S_n$. Intuitively, $(s_1, ..., s_n)$ represents the preference given to each index in $[n]$. To\nsample from $\\mathbb{P}_{PL}$, we first sample $\\sigma(1)$ from $\\text{Cat}(n, \\text{softmax}(s))$. Then we remove $\\sigma(1)$ from the\nlist and sample $\\sigma(2)$ from the categorical distribution corresponding to the rest of the scores (logits).\nWe continue in this manner until we have sampled $\\sigma(1), ..., \\sigma(n)$. By [8], the mode of the PL\ndistribution is the permutation that sorts $s$ in descending order.\nThe Generalized PL (GPL) Distribution. We also propose a generalization of the PL distribution,\nreferred to as Generalized Plackett-Luce (GPL) Distribution. Unlike the PL distribution, which uses\na set of $n$ scores, the GPL distribution uses $n^2$ scores $\\{s_{i,j}\\}$, where each $s_i = \\{s_{i,1}, ..., s_{i,n}\\}$\nconsists of $n$ scores. The GPL distribution is constructed as follows,\n$\\mathbb{P}_{GPL}(\\sigma) := \\prod_{i=1}^n \\frac{\\exp(s_{i,\\sigma(i)})}{\\sum_{j=i}^n \\exp(s_{i,\\sigma(j)})} .$ (9)\nSampling of the GPL distribution begins with sampling $\\sigma(1)$ using $n$ scores $s_1$. For $2 \\leq i \\leq n$, we\nremove $i - 1$ scores from $s_i$ that correspond to $\\sigma(1), ..., \\sigma(i - 1)$ and sample $\\sigma(i)$ from a categorical\ndistribution constructed from the remaining $n - i + 1$ scores in $s_i$. It is important to note that the\nfamily of PL distributions is a strict subset of the GPL family. Since the GPL distribution has more\nparameters than the PL distribution, it is expected to be more expressive. In fact, when considering\ntheir ability to express the delta distribution, which is the target distribution for many permutation\nlearning problems, we have the following result."}, {"title": "Network Architecture and Training", "content": "We now briefly introduce how to use neural networks to parameterize the above distributions used\nin the reverse process. At any time $t$, given $X_t \\in \\mathbb{R}^{n \\times d}$, we use a neural network with parameters\n$\\theta$ to construct $\\mathbb{P}_{\\theta}(\\sigma|X_t)$. In particular, we treat $n$ rows of $X_t$ as $n$ tokens and use a Transformer\narchitecture along with the time embedding of $t$ and the positional encoding to predict the previously\nmentioned scores. For example, for the GPL distribution, to predict $n^2$ scores, we introduce $n$ dummy\ntokens that correspond to the $n$ permuted output positions. We then perform a few layers of masked\nself-attention ($2n \\times 2n$) to obtain the token embedding $Z_1 \\in \\mathbb{R}^{n \\times d_{model}}$ corresponding to $n$ input"}, {"title": "Denoising Schedule via Merging Reverse Steps", "content": "If one merges some steps in the reverse process, sampling and learning would be faster and more\nmemory efficient. The variance of the training loss could also be reduced. Specifically, at time $t$ of the\nreverse process, instead of predicting $\\mathbb{P}_{\\theta}(X_{t-1}|X_t)$, we can predict $\\mathbb{P}_{\\theta}(X_{t'}|X_t)$ for any $0 < t' < t$.\nGiven a sequence of timesteps $0 = t_0 < ... < t_k = T$, we can now model the reverse process as\n$\\mathbb{P}_{\\theta}(X_{t_0}, ..., X_{t_k}|\\mathcal{X}) = \\mathbb{P}(X_T|\\mathcal{X}) \\prod_{i=1}^k \\mathbb{P}_{\\theta}(X_{t_{i-1}}|X_{t_i}).$ (11)\nTo align with the literature of diffusion models, we call the list $[t_0, ..., t_k]$ the denoising schedule.\nAfter incorporating the denoising schedule in Eq. (10), we obtain the loss function:\n$L(\\theta) = \\mathbb{E}_{\\mathbb{P}_{data}(X_0,\\mathcal{X})}\\mathbb{E}_{q(X_{1:T}|X_0,\\mathcal{X})} \\Big[-\\log\\mathbb{P}_{\\theta}(X_T|\\mathcal{X}) - \\sum_{i=1}^k \\log \\frac{\\mathbb{P}_{\\theta}(X_{t_{i-1}}|X_{t_i})}{q(X_{t_i}|X_{t_{i-1}})} \\Big].$ (12)\nNote that although we may not have the analytical form of $q(X_{t_i}|X_{t_{i-1}})$, we can draw samples\nfrom it. Merging is feasible if the support of $\\mathbb{P}_{\\theta}(X_{t_{i-1}}|X_{t_i})$ is equal or larger than the support\nof $q(X_{t_i}|X_{t_{i-1}})$; otherwise, the inverse of some forward permutations would be almost surely\nunrecoverable. Therefore, we can implement a non-trivial denoising schedule (i.e., $k < T$), when\n$\\mathbb{P}_{\\theta}(\\sigma|X_t)$ follows the PL or GPL distribution, as they have whole $S_n$ as their support. However,\nmerging is not possible for inverse shuffling methods, as their support is smaller than that of the\ncorresponding multi-step forward shuffling. To design a successful denoising schedule, we first\ndescribe the intuitive principles and then provide some theoretical insights. 1) The length of forward\ndiffusion $T$ should be minimal so long as the forward process approaches the uniform distribution. 2)\nIf distributions of $X_t$ and $X_{t+1}$ are similar, we should merge these two steps. Otherwise, we should\nnot merge them, as it would make the learning problem harder.\nTo quantify the similarity between distributions shown in 1) and 2), the TV distance is commonly\nused in the literature. In particular, we can measure $D_{TV}(q^{(t)}, q^{(t')})$ for $t \\neq t'$ and $D_{TV}(q^{(t)}, u)$,\nwhere $q^{(t)}$ is the distribution at time $t$ in the forward process and $u$ is the uniform distribution. For\nriffle shuffles, the total variation distance can be computed exactly. Specifically, we first introduce\nthe Eulerian Numbers $A_{n,r}$ [35], i.e., the number of permutations in $S_n$ that have exactly $r$ rising\nsequences where $1 \\leq r \\leq n$. $A_{n,r}$ can be computed using the following recursive formula $A_{n,r} =$\n$rA_{n-1,r} + (n - r + 1)A_{n-1,r-1}$ where $A_{1,1} = 1$. We then have the following result."}, {"title": "Reverse Process Decoding", "content": "We now discuss how to decode predictions from the reverse process at test time. In practice, one is\noften interested in the most probable state or a few states with high probabilities under $\\mathbb{P}_{\\theta}(X_0|\\mathcal{X})$.\nHowever, since we can only draw samples from $\\mathbb{P}_{\\theta}(X_0|\\mathcal{X})$ via running the reverse process, exact\ndecoding is intractable. The simplest approximated method is greedy search, i.e., successively finding\nthe mode or an approximated mode of $\\mathbb{P}_{\\theta}(X_{t_{i-1}}|X_{t_i})$. Another approach is beam search, which\nmaintains a dynamic buffer of $k$ candidates with highest probabilities. Nevertheless, for one-step\nreverse transitions like the GPL distribution, even finding the mode is intractable. To address this, we\nemploy a hierarchical beam search that performs an inner beam search within $n^2$ scores at each step\nof the outer beam search. Further details are provided in Appendix D."}, {"title": "Experiments", "content": "We now demonstrate the general applicability and effectiveness of our model through a variety of\nexperiments, including sorting 4-digit MNIST numbers, solving jigsaw puzzles, and addressing\ntraveling salesman problems. Additional details are provided in the appendix due to space constraints."}, {"title": "Sorting 4-digit MNIST Images", "content": "We first evaluate our SymmetricDiffusers on the four-digit MNIST sorting benchmark, a well-\nestablished testbed for differentiable sorting [5, 9, 14, 21, 36, 37]. Each four-digit image in this"}, {"title": "Jigsaw Puzzle", "content": "We then explore image reassembly from segmented \"jigsaw\" puzzles [31, 34, 42]. We evaluate the\nperformance using the MNIST and the CIFAR10 datasets, which comprises puzzles of up to 6 \u00d7 6 and\n4 x 4 pieces respectively. We add slight noise to pieces from the MNIST dataset to ensure background\npieces are distinctive. To evaluate our models, we use Kendall-Tau coefficient, accuracy, correctness,\nRMSE (root mean square error of reassembled images), and MAE (mean absolute error) as metrics.\nTable 1 presents results comparing our method with the Gumbel-Sinkhorn Network[31], Diffsort\n[37], and Error-free Diffsort [21]. DiffSort and Error-free DiffSort are primarily designed for sorting\nhigh-dimensional ordinal data which have clearly different patterns. Since jigsaw puzzles on MNIST\nand CIFAR10 contain pieces that are visually similar, these methods do not perform well. The\nGumbel-Sinkhorn performs better for tasks involving fewer than 4 \u00d7 4 pieces. In more challenging\nscenarios (e.g., 5 \u00d7 5 and 6 \u00d7 6), our method significantly outperforms all competitors."}, {"title": "The Travelling Salesman Problem", "content": "At last, we explore the travelling salesman problem (TSP) to demonstrate the general applicability of\nour model. TSPs are classical NP-complete combinatorial optimization problems which are solved\nusing integer programming or heuristic solvers [2, 13]. There exists a vast literature on learning-based\nmodels to solve TSPs [23, 24, 19, 18, 6, 25, 11, 39, 22, 47, 33]. They often focus on the Euclidean\nTSPs, which are formulated as follows. Let $V = \\{v_1, ..., v_n\\}$ be points in $\\mathbb{R}^2$. We need to find some\n$\\sigma \\in S_n$ such that $\\sum_{i=1}^n ||V_{\\sigma(i)} - V_{\\sigma(i+1)}||_2$ is minimized, where we let $\\sigma(n + 1) := \\sigma(1)$. Further\nexperimental details are provided in Appendix B.\nWe compare with operations research (OR) solvers and other learning based approaches on TSP\ninstances with 20 nodes. The metrics are the total tour length and the optimality gap. Given the ground\ntruth (GT) length produced by the best OR solver, the optimality gap is given by (predicted length\n(GT length))/(GT length). As shown in Table 4, SymmetricDiffusers achieves comparable results\nwith both OR solvers and the state-of-the-art learning-based methods."}, {"title": "Conclusion", "content": "In this paper, we introduce a novel discrete diffusion model over finite symmetric groups. We identify\nthe riffle shuffle as an effective forward transition and provide empirical rules for selecting the\ndiffusion length. Additionally, we propose a generalized PL distribution for the reverse transition,\nwhich is provably more expressive than the PL distribution. We further introduce a theoretically\ngrounded \"denoising schedule\" to improve sampling and learning efficiency. Extensive experiments\nverify the effectiveness of our proposed model. Despite significantly surpassing the performance of\nexisting methods on large instances, our method still has limitations in larger scales. In the future, we\nwould like to explore methods to improve scalability even more. We would also like to explore how\nwe can fit other modern techniques in diffusion models like concrete scores [32] and score entropy\n[28] into our shuffling dynamics. Finally, we are interested in generalizing our model to general finite\ngroups and exploring diffusion models on Lie groups."}, {"title": "Additional Details of the GSR Riffle Shuffle Model", "content": "There are many equivalent definitions of the GSR riffle shuffle. Here we also introduce the Geometric\nDescription [4], which is easy to implement (and is how we implement riffle shuffles in our experi-\nments). We first sample $n$ points in the unit interval $[0, 1]$ uniformly and independently, and suppose\nthe points are labeled in order as $x_1 < x_2 < ... < x_n$. Then, the permutation that sorts the points\n$\\{2x_1\\}, ..., \\{2x_n\\}$ follows the GSR distribution, where $\\{x\\} := x - \\lfloor x \\rfloor$ is the fractional part of $x$."}, {"title": "Details of Our Network Architecture", "content": "We now discuss how to use neural networks to produce the parameters of the distributions discussed\nin Section 3.2.1 and 3.2.2. Fix time $t$, and suppose $X_t = (x_1^{(t)}, ..., x_n^{(t)}) \\in \\mathbb{R}^{n \\times d}$. Let $\\text{encoder}_{\\theta}$\nbe an object-specific encoder such that $\\text{encoder}_{\\theta}(X_t) \\in [\\mathbb{R}]^{n \\times d_{model}}$. For example, $\\text{encoder}_{\\theta}$ can be\na CNN if $X_t$ is an image. Let\n$Y_t := \\text{encoder}_{\\theta}(X_t) + \\text{time\\_embd}(t) = (y_1^{(t)}, ..., y_n^{(t)}) \\in [\\mathbb{R}]^{n \\times d_{model}},$ (15)\nwhere $\\text{time\\_embd}$ is the sinusoidal time embedding. Then, we would like to feed the embeddings into\na Transformer encoder [48]. Let $\\text{transformer\\_encoder}_{\\theta}$ be the encoder part of the Transformer\narchitecture. However, each of the distributions we discussed previously has different number of\nparameters, so we will have to discuss them separately."}, {"title": "Additional Details of Decoding", "content": "Greedy Search. At each timestep $t_i$ in the denoising schedule, we can greedily obtain or approx-\nimate the mode of $\\mathbb{P}_{\\theta}(X_{t_{i-1}}|X_{t_i})$. We can then use the (approximated) mode $X_{t_{i-1}}$ for the next\ntimestep $\\mathbb{P}_{\\theta}(X_{t_{i-2}}|X_{t_{i-1}})$. Note that the final $X_0$ obtained using such a greedy heuristic may not\nnecessarily be the mode of $\\mathbb{P}_{\\theta}(X_0|\\mathcal{X})$.\nBeam Search. We can use beam search to improve the greedy approach. The basic idea is that,\nat each timestep $t_i$ in the denoising schedule, we compute or approximate the top-$k$-most-probable\nresults from $\\mathbb{P}_{\\theta}(X_{t_{i-1}}|X_{t_i})$. For each of the top-$k$"}]}