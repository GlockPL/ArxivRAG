{"title": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations", "authors": ["Kaixuan Huang", "Jiacheng Guo", "Zihao Li", "Xiang Ji", "Jiawei Ge", "Wenzhe Li", "Yingqing Guo", "Tianle Cai", "Hui Yuan", "Runzhe Wang", "Yue Wu", "Ming Yin", "Shange Tang", "Yangsibo Huang", "Chi Jin", "Xinyun Chen", "Chiyuan Zhang", "Mengdi Wang"], "abstract": "Large language models have demonstrated impressive performance on challenging mathematical reasoning tasks, which has triggered the discussion of whether the performance is achieved by true reasoning capability or memorization. To investigate this question, prior work has constructed mathematical benchmarks when questions undergo simple perturbations \u2013 modifications that still preserve the underlying reasoning patterns of the solutions. However, no work has explored hard perturbations, which fundamentally change the nature of the problem so that the original solution steps do not apply. To bridge the gap, we construct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard perturbation, respectively. Each consists of 279 perturbed math problems derived from level-5 (hardest) problems in the MATH dataset (Hendrycks et al., 2021). We observe significant performance drops on MATH-P-Hard across various models, including 01-mini (-16.49%) and gemini-2.0-flash-thinking (-12.9%). We also raise concerns about a novel form of memorization where models blindly apply learned problem-solving skills without assessing their applicability to modified contexts. This issue is amplified when using original problems for in-context learning. We call for research efforts to address this challenge, which is critical for developing more robust and reliable reasoning models.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have achieved remarkable progress in solving many previously challenging tasks and demonstrating signs of general intelligence (Bubeck et al., 2023). As LLMs become more intelligent, the research community responds by developing and adopting new benchmarks to guide the development of better models (Wang et al., 2024; Zhou et al., 2023; Liu et al., 2024; Rein et al., 2023; Yan et al., 2024).\nIn mathematical reasoning, the field has progressed from simpler datasets like SVAMP (Patel et al., 2021) and GSM8K (Cobbe et al., 2021a) to more challenging benchmarks such as MATH (Hendrycks et al., 2021), Olympiad-Bench (He et al., 2024), and AIME problems. Models continue to strike higher performance on these advanced benchmarks through stronger architectures, novel training approaches, and better training data (OpenAI, 2024; Yang et al., 2024; Shao et al., 2024; DeepSeek-AI et al., 2025).\nNevertheless, concerns about data contamination and out-of-distribution generalization remain. Model performance can be artificially high if variants of the evaluation set leak into the training datasets or if its distribution is over-represented. In these cases, the model could be merely doing pattern recognition and memorizing the solution steps without understanding the underlying rationale, making it vulnerable to perturbations of the problem formulation (Zhang et al., 2024; Srivastava et al., 2024).\nSeveral works have been proposed to quantify the robustness of reasoning models against such perturbations (Shi et al., 2023a; Mirzadeh et al., 2024; Zhang et al., 2024; Srivastava et al., 2024; Gulati et al., 2024; Zou et al., 2024). Notably, Srivastava et al. (2024) created Functional-MATH by manually rewriting the original problems in the MATH benchmark (Hendrycks et al., 2021) into problem templates, where the numerical values in the problem statements and the corresponding answers can be varied automatically to generate infinitely-many versions that use the same math problem-solving skills. They observed performance drops between the modified benchmark and the original benchmark for several state-of-the-art language models, indicating that those models are indeed biased towards the original configurations of numerical values due to some form of data contamination. However, most existing work focuses on perturbing non-critical parameters (e.g., numerical values) that do not alter the fundamental reasoning patterns required to solve the problem. We refer to such changes as simple perturbations. While prior studies have shown that LLMs can generalize across a range of problem variants by relying on bag-of-heuristics reasoning (Nikankin et al., 2024; jylin04 et al.), this form of generalization does not necessarily reflect a true understanding of the underlying principles. As a result, models may still fail when faced with a substantial shift in reasoning patterns.\nIn this work, we take one step forward beyond simple perturbations. We consider hard perturbations: while at lexical level (e.g. edit distance) the modification is similar to simple perturbations, we ensure to change the problem formulations fundamentally so that the original solution paths are no longer applicable to the perturbed settings; see Figure 1 for a comparison between the two types of perturbations. A genuinely robust reasoning model that understands the underlying rationales should not only solve the modified problems under simple perturbations but also be able to judge whether the problem formulations change in a way that fundamentally alters the problems and respond accordingly, instead of applying the learned skills indiscriminately.\nAs the capabilities of large language models continue to advance and the average-case performance continue to improve, the generalization abilities against hard perturbations may soon become the primary bottleneck in their real-world usages. Addressing this challenge will be critical for advancing the robustness and reliability of future LLMs."}, {"title": "2. Dataset Curation", "content": "Origin of the Dataset. We choose the popular MATH benchmark (Hendrycks et al., 2021), which contains challenging mathematical reasoning problems sourced from American high school mathematics competitions such as the AMC 10, AMC 12, and AIME. Each problem belongs to one of the 7 subjects: Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus. Besides, each problem is labeled with a difficulty level of 1 (easiest) to 5 (hardest). The problems may contain LaTeX and Asymptote graphics language for describing mathematical concepts and geometric figures.\nAs the state-of-the-art reasoning models can already solve MATH problems with overall accuracies higher than 90% (OpenAI, 2024; Team et al., 2024a; DeepSeek-AI et al., 2025), we opt to focus only on the hardest level-5 problems in our work, and create new benchmarks from these level-5 problems. We use level-5 problems from both the train split and the test split as the seed problems, so we are able to investigate whether language models behave differently on the two splits.\nAnnotation Criterion. For each problem, we modify the problem to create two variations:\n(1) for MATH-P-Simple, we make simple perturbations, i.e., non-essential modifications to the problem, ensuring that the modified problem can be solved using the same method as the original problem.\n(2) for MATH-P-Hard, we make hard perturbations, i.e., small but fundamental modifications to the problem so that the modified problem cannot be solved using the same method as the original problem. Instead, it requires deeper math understanding and harder problem-solving skills.\nBesides, we ensure the following two additional requirements:\n\u2022 Minimal Edits: To test the generalization of the reasoning-\ning models and elicit potential memorization behaviors, we ask the annotators to make as minimal modifications as possible. Therefore, the modified problems stay close to the original problems in the text form.\n\u2022 Changed Answers: For both of the modifications, we guarantee that the answers to the modified problems are different from the original answer. Therefore, models cannot cheat by pattern recognition and outputting memorized solutions.\nQuality Control. We recruited 12 annotators (PhD students) with strong mathematical backgrounds for the annotation task. All the annotators hold a bachelor's degree in mathematics, have done researches in theoretical machine learning, and/or competed in mathematical competitions during high school.\nTo ensure the quality of the benchmark, all the annotators are required to double-check their annotations. Each modified problem is also cross-validated by an independent annotator to make sure the answer is correct.\nAdditionally, we manually went through all the problems where the ol-mini's answer and the annotated answer differ and confirmed that the annotated answers are correct.\nBenchmark Overview and Statistics.\nAfter removing several annotations that failed the quality checks, we obtained 279 pairs of modifications, where 164 examples are from train split and 115 examples are from test split. The numbers of problems in each of the 7 subjects are listed in Table 3. Figure 1 shows one example of our benchmark.\nTo quantify how similar the original problem and the modified problem are, first, we calculate the edit distance between the modified problem and the original problem, normalized by the length of the original problem. Besides, we compute the cosine similarities between the embeddings of the two problems, where we use OpenAI's"}, {"title": "3. Experimental Results", "content": "Evaluation Setting. We adopt zero-shot chain-of-thought (CoT) (Wei et al., 2022; Kojima et al., 2022) as the standard evaluation method on our benchmarks. For comparison, we also evaluate the models on the set of the original 279 prob-"}, {"title": "3.1. Benchmarking the performance of LLMS", "content": "We consider a wide range of language models including long-CoT models, closed-sourced large models, open-sourced small models, and math-specific models. The version information of the models is deferred to Appendix A.\nIn Table 1, we report the overall accuracies of the LLMs on Original, MATH-P-Simple, and MATH-P-Hard, and also separately calculate the accuracies for problems that originate from the train split and test split. As expected, for all the models we evaluate, we find that the performance on MATH-P-Hard is significantly lower than the original problems, which indicates MATH-P-Hard is more difficult.\nIn the meantime, most models also suffer a slight performance drop on MATH-P-Simple compared to the original problems. We note that the performance drops mainly come from the train split. Generalization errors still exist for the state-of-the-art models even when the test examples follow the exact same reasoning patterns as the training problems.\nFor problems that originate from the test split, ideally, both the original problem and its MATH-P-Simple modification should be equally \"unseen\" to the model. We observe mixed results empirically from Table 1: for gemini-2.0-flash-exp, GPT-4-turbo, claude-3.5-sonnet, the performance drops are larger than 5%, while surprisingly the performance of Phi-3.5-mini-instruct increases. For most of the models we evaluated, the accuracies on MATH-P-Simple test split are close to the accuracies on the original test split. We commend that while Srivastava et al. (2024) found a relatively 58% to 80% performance drop between their modified benchmark and the original MATH benchmark among a different set of the models (the best model they tested was GPT-4), we did not observe such huge gaps for the models we evaluate, which is a sign of the progress in the robustness of the newly developed models against simple perturbations.\nInference-time Scaling. Scaling inference-time computes has been shown to be able to boost the performance of LLMs (Wang et al., 2022; Brown et al., 2024; Wu et al., 2024; Cobbe et al., 2021b; Lightman et al., 2023). We defer the study of inference-time scaling on our benchmarks to Appendix C.5."}, {"title": "3.2. Failure Mode Analysis", "content": "To study the generalization abilities of models against hard perturbations, we focus on the set of problems where the models fail on the MATH-P-Hard modification but correctly solve either the original problem or the MATH-P-Simple modification, which accounts for 20%-47% of the total\nproblems. For these problems, one can use the correct solutions to the easier problems as a reference to better determine the failure modes on the hard problems. We defer the discussion on the other cases to Appendix \u0421.1.\nFirst, we observe general failure modes when models are exposed to harder problems, including making mistakes in basic numerical computations and algebraic operations, making unjustified claims, missing several cases, and lacking certain math knowledge. These types of errors are more prominent in weaker models.\nBesides general failure modes, when we compare the wrong solution to the MATH-P-Hard modification with the solutions to the easier versions, we are able to recognize an adequate number of memorization issues. Specifically, we found that models may ignore the modified assumptions and presume that the original assumptions still hold; see Figure 5 for an example. In other cases, the models may blindly apply the techniques for the original problems without first determining whether these techniques are still suitable in the modified setting (the responses in Figure 1 are such an example generated by GPT-40). Interestingly, the models may even output the desired outcome of the original problem (not provided in the context) instead of the modified problem, e.g. Figure 6. This kind of memorization behavior is difficult to capture with most existing type of perturbations in the literature (similar to our MATH-P-Simple) that does not require different solving strategies.\nThese issues are often coupled with other types of errors and pervasive among the models we evaluated. For large models, we estimate the percentages of errors caused by memorization to be 40% for o1-mini and 25%"}, {"title": "3.3. Is Mode Collapse a Problem?", "content": "We investigate whether the model makes errors due to mode collapse, which means the model fails to identify the difference between the perturbed problem and the original problem (seen during its training time) and the model's response collapses to the response to the original problem with the identical answer.\nFor each model, we report nsame, the number of problems where the model's final answer coincides with the ground-truth answer of the corresponding original problem. For those responses, we also compute the edit distance between the full response to the modified problem and the full response to the original problem. The full result is deferred to Table 5 in the appendix.\nWe see that this type of failure mode accounts for less than 10% of the total errors except for three models (gemini-2.0-flash-thinking-exp, 01-mini, and gemini-2.0-flash-exp) on"}, {"title": "3.4. Does In-context Learning Help or Hurt?", "content": "In this subsection, we investigate whether using the corresponding original unmodified problem and solution as the one-shot in-context learning (ICL) example will help with the modified problems in MATH-P-Simple and MATH-P-Hard. We visualize the influences of ICL for three models in Figure 7 and defer the full result to Table 6.\nAs expected, using the original (problem, solution) pair as a one-shot in-context demonstration boosts the performance of nearly all the models on MATH-P-Simple, which should be solvable by simply applying the original solution steps to the modified setting.\nAs for the MATH-P-Hard modifications, there are two factors that need to be considered: (1) ICL effect: the original solutions may supply the model with desired mathematical knowledge that is also helpful for solving the modified problems; (2) misleading effect: on the other hand, as there are subtle differences between the original problems and the MATH-P-Hard modifications, the models may fail to recognize such differences and be misled by the demonstrated solutions. Accordingly, in Table 7 and Figure 7, and we calculate and visualize (1) Nwrong\u2192correct, the number of problems that initially the model fails on without the in-context demonstrations but answers correctly with the in-context demonstrations, and (2) Ncorrect\u2192wrong, the number of problems that initially the model answers correctly without demonstrations but fails on with demonstrations.\nWe observe that many MATH-P-Hard problems become solvable with the original problems and solutions as demonstrations. The percentages to the number of total errors without demonstrations are larger for closed-sourced large models (24%-40%) and smaller for open-sourced small models (2%-15%), due to their differences in mathematical capabilities and in-context learning capabilities. However, we also observe many MATH-P-Hard problems become incorrect with demonstrations, and the percentages are higher for large models (18%-40%) than small models (4%-15%). The misleading effect counteracts the effect of in-context learning, leaving only marginal improvements (less than 5%) on the MATH-P-Hard for most models.\nAs in-context learning can be viewed as a form of (test-time) training, we hypothesize that any naive fine-tuning technique with a limited distribution of problem settings will hurt the generalization of the language models against hard perturbations."}, {"title": "4. Related Work", "content": "Perturbations to Existing Mathematical Benchmarks.\nThere is a considerable amount of work focusing on performing perturbations to existing mathematical benchmarks. Shi et al. (2023a) built GSM-IC from GSM8K (Cobbe et al., 2021b) by adding irrelevant context to the problem. GSM-Plus (Li et al., 2024b) creates 8 types of variations to each of the GSM8K problem and ensure that the perturbed problem is of the same difficulty. Mirzadeh et al. (2024) built GSM-Symbolic that alters the numerical values and entity names via symbolic templates of both the problems and the solution steps. Similarly, Functional MATH (Srivastava et al., 2024) is created from the MATH dataset (Hendrycks et al., 2021), and Putnam-AXIOM (Gulati et al., 2024) from the Putnam Mathematical Competition.\nThis line of work performed simple perturbations to existing mathematical benchmarks and the perturbed problems can be solved with the same solution steps and the same reasoning pattern as the original ones. In contrast, we performed hard perturbations to curate MATH-P-Hard, where the original reasoning pattern does not apply.\nMemorization. Memorization is a well-studied phenomenon in machine learning (Feldman and Zhang, 2020; Zhang et al., 2021; Feldman, 2020) and has become increasingly prevalent in large language models, due to the growing of the pretraining corpora and the scaling of the model sizes. Verbatim memorization, i.e., recitation of the training material, has significant potential consequences ranging from privacy violations (Carlini et al., 2022; Brown et al., 2022; Huang et al., 2023) and copyright infringement (Shi et al., 2023b; Karamolegkou et al., 2023; Wei et al., 2024; Chen et al., 2024) to training data security risks (Carlini et al., 2021; Nasr et al., 2023). Prior work has investigated various factors influencing verbatim memorization, including sequence duplicates (Lee et al., 2021; Hernandez et al., 2022), model size (Tirumala et al., 2022), and sequence position (Biderman et al., 2023).\nIn contrast, we investigate the effect of memorization within the mathematical reasoning context. Our methodology falls into the category of counterfactual tests (Zhang et al., 2023; Wu et al., 2023; Zheng et al., 2023; Xie et al., 2024), where we construct perturbed problems different from the existing ones to test the generalization of LLMs and examine memorization effects. Through extensive case studies, we find that LLMs can exhibit subtle forms of memorization beyond naive verbatim memorization.\nComparison with MATH2 (Shah et al., 2024). Shah et al. (2024) created MATH2 by combining random pairs of skills extracted from MATH (Hendrycks et al., 2021) to generate harder problems that require both skills to solve. Their benchmark is mathematically harder, but there are no natural \"original problems\u201d as references. Therefore, MATH2 is not directly suitable for investigating the memorization effects of language models. In comparison, our MATH-P-Hard are modified directly from the problems in MATH so that the modified problems require harder skills to solve. MATH-P-Hard can serve as both a harder math benchmark and a testbed to investigate memorizations of LLMs."}, {"title": "5. Conclusion", "content": "In this work, we study the generalization of large language models' math reasoning abilities against hard perturbations of the problems. We modified 279 problems from the level-5 problems of the MATH dataset (Hendrycks et al., 2021) into MATH-P-Simple (used for control experiments) and MATH-P-Hard, via simple perturbations and hard perturbations, respectively. We found performance degradations of all models on MATH-P-Hard, and many of the errors can be traced to a new form of memorization, where the model memorizes the problem-solving techniques from the training set and blindly applies them without judging whether the modified settings are still suitable. Using the original unmodified problem and solution for in-context learning can deteriorate this issue. We expect the generalization against hard perturbations to be the next major bottleneck of LLMs' reasoning abilities and urge future work in this direction."}, {"title": "A. Version Information of the Models", "content": "We consider the following models in the paper.\n\u2022 long-CoT models: 01-preview, 01-mini (OpenAI, 2024), Gemini 2.0 flash thinking\n\u2022 closed-source models: GPT-40, GPT-4 Turbo (Achiam et al., 2023), Gemini 1.5 Pro, Gemini 2.0 flash (Team et al., 2024a), Claude 3.5 Sonnet, Claude 3 Opus (Anthropic, 2024);\n\u2022 open-sourced general-purpose models: Llama 3.1 (Dubey et al., 2024), Gemma 2 (Team et al., 2024b), Phi-3.5 (Abdin et al., 2024);\n\u2022 math-specific models: MetaMath (Yu et al., 2024), MAmmoTH2 (Yue et al., 2024), Deepseek-Math (Shao et al., 2024), Qwen2.5-Math (Yang et al., 2024), NuminaMath (Li et al., 2024a), Mathtral\u00b9."}, {"title": "B. Benchmark Statistics", "content": null}, {"title": "C. Additional Experimental Results", "content": null}, {"title": "C.1. Categorizing Model Responses Across Problem Variations", "content": "Recall that for each problem, we have a MATH-P-Simple modification which can be solved using the same method as the original problem, and a MATH-P-Hard modification which requires more difficult problem-solving skills. Therefore, there are 8 possible cases regarding the correctness of the model's responses to the three problems. Modulo the fluctuations of the model's correctness among the MATH-P-Simple variations, we can summarize the model's responses into the following 4 cases:\n\u2022 Case I: at least one of the original problem and the MATH-P-Simple modification is solved correctly, and the MATH-P-Hard modification is also solved correctly.\n\u2022 Case II: both the original problem and the MATH-P-Simple modification are solved incorrectly, and the MATH-P-Hard modification is also solved incorrectly.\n\u2022 Case III: both the original problem and the MATH-P-Simple modification are solved incorrectly, but the MATH-P-Hard modification is solved correctly.\n\u2022 Case IV: at least one of the original problem and the MATH-P-Simple modification is solved correctly, but the MATH-P-Hard modification is solved incorrectly.\nFor each of the models, we calculate the percentage of the responses in Table 4. As expected, stronger models have a higher percentage of Case I responses and a lower percentage of Case II responses. Interestingly, the percentages of Case III responses are small (less than 10%) but non-zero, where the models cannot solve the easier variants but can solve the hard variant correctly. After manual inspection, we found that this is due to the misalignment between the models' capabilities and the annotators' perception of the difficulties of math problems."}, {"title": "C.2. Is Mode Collapse a Problem?", "content": "We provide Table 5 to support Section 3.3."}, {"title": "3.3. Is Mode Collapse a Problem?", "content": "We investigate whether the model makes errors due to mode collapse, which means the model fails to identify the difference between the perturbed problem and the original problem (seen during its training time) and the model's response collapses to the response to the original problem with the identical answer.\nFor each model, we report $n_{same}$, the number of problems where the model's final answer coincides with the ground-truth answer of the corresponding original problem. For those responses, we also compute the edit distance between the full response to the modified problem and the full response to the original problem. The full result is deferred to Table 5 in the appendix.\nWe see that this type of failure mode accounts for less than 10% of the total errors except for three models (gemini-2.0-flash-thinking-exp, 01-mini, and gemini-2.0-flash-exp) on"}, {"title": "3.4. Does In-context Learning Help or Hurt?", "content": "In this subsection, we investigate whether using the corresponding original unmodified problem and solution as the one-shot in-context learning (ICL) example will help with the modified problems in MATH-P-Simple and MATH-P-Hard. We visualize the influences of ICL for three models in Figure 7 and defer the full result to Table 6.\nAs expected, using the original (problem, solution) pair as a one-shot in-context demonstration boosts the performance of nearly all the models on MATH-P-Simple, which should be solvable by simply applying the original solution steps to the modified setting.\nAs for the MATH-P-Hard modifications, there are two factors that need to be considered: (1) ICL effect: the original solutions may supply the model with desired mathematical knowledge that is also helpful for solving the modified problems; (2) misleading effect: on the other hand, as there are subtle differences between the original problems and the MATH-P-Hard modifications, the models may fail to recognize such differences and be misled by the demonstrated solutions. Accordingly, in Table 7 and Figure 7, and we calculate and visualize (1) $N_{wrong\\rightarrow correct}$, the number of problems that initially the model fails on without the in-context demonstrations but answers correctly with the in-context demonstrations, and (2) $N_{correct\\rightarrow wrong}$, the number of problems that initially the model answers correctly without demonstrations but fails on with demonstrations.\nWe observe that many MATH-P-Hard problems become solvable with the original problems and solutions as demonstrations. The percentages to the number of total errors without demonstrations are larger for closed-sourced large models (24%-40%) and smaller for open-sourced small models (2%-15%), due to their differences in mathematical capabilities and in-context learning capabilities. However, we also observe many MATH-P-Hard problems become incorrect with demonstrations, and the percentages are higher for large models (18%-40%) than small models (4%-15%). The misleading effect counteracts the effect of in-context learning, leaving only marginal improvements (less than 5%) on the MATH-P-Hard for most models.\nAs in-context learning can be viewed as a form of (test-time) training, we hypothesize that any naive fine-tuning technique with a limited distribution of problem settings will hurt the generalization of the language models against hard perturbations."}, {"title": "C.5. Inference-time Scaling Behaviors", "content": "In this subsection, we investigate the inference-time scaling behaviors of LLMs on our benchmarks. We compute the pass@k metric following Chen et al. (2021). Specifically, for each problem, we generate N solutions independently, and compute the pass@k metric via the following formula for each 1 \u2264 k \u2264 N:\n$pass@k = E_{problem} \\frac{1}{k} \\binom{N}{k}$, where $c$ is the number of correct answers of the n runs.\nWe also compute the performance of self-consistency (Wang et al., 2022), a.k.a., majority voting, where for each k, we randomly sample k responses from the N runs and get the majority-voted answer. We report the average and standard deviation among 5 random draws. We only evaluate three models: 01-mini, Llama-3.1-8B-Instruct, and Qwen2.5-Math-7B-Instruct. For Llama-3.1-8B-Instruct, and Qwen2.5-Math-7B-Instruct, we choose N = 64, while for o1-mini we set N = 8."}]}