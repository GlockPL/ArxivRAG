{"title": "Combining knowledge graphs and LLMs for hazardous chemical information management and reuse", "authors": ["Marcos Da Silveira", "Louis Deladiennee", "Kheira Acem", "Oona Freudenthal"], "abstract": "Human health is increasingly threatened by exposure to hazardous substances, particularly persistent and toxic chemicals. The link between these substances, often encountered in complex mixtures, and various diseases are demonstrated in scientific studies. However, this information is scattered across several sources and hardly accessible by humans and machines. This paper evaluates current practices for publishing/accessing information on hazardous chemicals and proposes a novel platform designed to facilitate retrieval of critical chemical data in urgent situations. The platform aggregates information from multiple sources and organizes it into a structured knowledge graph. Users can access this information through a visual interface such as Neo4J Bloom and dashboards, or via natural language queries using a Chatbot. Our findings demonstrate a significant reduction in the time and effort required to access vital chemical information when datasets follow FAIR principles. Furthermore, we discuss the lessons learned from the development and implementation of this platform and provide recommendations for data owners and publishers to enhance data reuse and interoperability. This work aims to improve the accessibility and usability of chemical information by healthcare professionals, thereby supporting better health outcomes and informed decision-making in the face of patients exposed to chemical intoxication risks.", "sections": [{"title": "Introduction", "content": "In today's industrialized world, exposure to hazardous chemicals is an ever-present risk, both in occupational settings and through environmental contamination. These substances, widely used across various industries, pose significant health risks to individuals, necessitating rapid and accurate access to detailed chemical information by healthcare professionals. For many years, governments and organizations have collaborated to create efficient systems for sharing digital information about hazardous chemicals. They have enacted laws, established rules, and developed programs to define and implement governance strategies, standards, services, and infrastructures aimed at creating a more effective data market. Notable examples in Europe include the Data Governance Act [1], the Data Act [2], the Zero Pollution Action Plan [3], Chemicals Strategy for Sustainability [4], and the Waste Framework Directive [5] designed to facilitate information sharing. Additionally, international initiatives such as the Research Data Alliance (RDA), formed in 2013 by the EU, the USA, and Australia, advise organizations on requirements for interoperable and reusable data.\nHowever, relevant information about chemical substances remains difficult to access. It is scattered across several websites, stored in various formats, and often lacks metadata and indexes. To access the information, users must visit diverse websites, enter queries in their browsers, or open and read CSV files. The data format and the meaning of fields in each file are defined by the data publisher and often not documented in a computer-interpretable format. Although initiatives like ChemView or ChemSpider add value by aggregating information from various sources into a single website, the interpretation, comparison, and linking of diverse information still require human intervention. This makes it challenging, for instance, to identify the relationship between substances and the diseases diagnosed in patients.\nThe first contribution of this paper is the analysis of widely adopted data sources on hazardous chemical substances and evaluate their compliance with the principles of Findability, Accessibility, Interoperability, and Reusability (FAIR) [6].\nThe second contribution is the analysis of how the knowledge available in several data sources can be linked and exploited. We propose to investigate the use of Knowledge graphs (KGs) to represent the connection between hazardous substances and diseases. KGs are particularly well adapted to represent links between diverse pieces of information [7]. They excel at integrating data from multiple sources, ensuring that relationships between various entities, such as chemical substances and associated diseases, are clearly defined and easily navigable. This structured representation makes it easier to discover connections and derive insights that might be missed when data is dispersed across different formats and locations. For instance, in the context of hazardous chemical substances, a KGs can link chemical properties, products/usage contexts, organs often affected by the substance, and documents with relevant information. This integrated view supports healthcare professionals in quickly identifying relevant data, understanding the potential health risks associated with exposure, and making informed"}, {"title": "Methods and tools", "content": "The experience and outcomes reported in this paper have been gained in the context of a research project aiming at understanding how chemicals data from various (governmental and scientific) sources could be used for risk assessment purposes in healthcare. To scope this paper and for the purpose of this study, we selected ten freely accessible and widely used chemicals data sources relevant for consumer products and human health risk assessment (see Table 1). The intention was not to be exhaustive within the data source selection, but rather to identify recurrent problems from a selection of widely used sources and contribute to solve them. We applied the following set of selection criteria:\n1)\nThe dataset should be widely adopted by researchers, policy makers or public authorities acting on chemical regulation, risk assessment and mitigation within North America and Europe.\n2)\nThe dataset is publicly accessible to anyone through any internet browser.\n3)\nThe dataset is hosted either in North America or in Europe (for the purpose of scoping of this study. These geographical areas have been active in chemicals data management at the Organisation for Economic Co-operation and Development, OECD, level).\n4)\nThe dataset is relevant for consumer exposure and public health (datasets focused purely on environmental health were not considered).\nNote that at the time of writing this paper, the new ECHA CHEM portal is still under development and thus this portal was not analysed.\nWe first manually analysed the FAIRness of the datasets. Then we automaticaly evaluated them using the tools FAIR Checker [8] and F-UJI [9]. FAIR Checker is a free online tool that assesses whether a dataset adheres to the FAIR principles. Users provide a valid persistent identifier (PID) or URL, and the tool scans the dataset's landing page to conduct a comprehensive assessment. Results are visually presented in a radar chart, with normalized evaluation scores ranging from 0 (not satisfied) to 100 (completely satisfied) for each of the four FAIR principles. In addition, a detailed table provides scores, test results, log messages, and recommendations for each test. Notably, FAIR-Checker does not differentiate between data and metadata evaluations. F-UJI, another user-friendly tool, automatically evaluates the FAIRness of the datasets. Users enter the dataset's URL, and if online metadata is available, they can specify its type, such as OAI-PMH, OGC CSW, or SPARQL. F-UJI generates a comprehensive report summarizing the assessment results, including a multi-level pie chart that visualizes the dataset's overall FAIRness level. Although the pie chart offers a general overview, it is not interactive. The detailed report delves into each test performed, indicating the corresponding FAIR level (initial, moderate, or advanced) using colored checkmarks (light, medium, and dark, respectively). Debug messages for each test allow users to independently verify and evaluate the test outputs.\nThe URL used in this assessment follows the criteria: 1-) The preference is to use the url published in the FAIRsharing; 2-) if not there, use the URL published in PubChem; 3-) if not there, use the URL of the publisher (where the data source can be accessed). FAIRsharing.org [10] is a community-driven resource that promotes the FAIR principles and the use of standards, databases, and policies. It aims to classify and align research data policies across publishers and funders, moderate cross-publisher discussions on repositories, define and register FAIR maturity indicators and metrics, and build guidance and training materials. In"}, {"title": "FAIR Analysis", "content": "The two FAIR analysis tools selected for this work use different metrics and scoring system, producing different FAIRness evaluation results [12]. For instance, F-UJI and FAIR Checker evaluate seventeen and twelve criteria, respectively. This section presents the main observations and the discussions on the results. However, to improve readi-\nness and respect space limit, we do not include all details about the analysis in this paper, but it can be found in [13]. Figures 1 and 2 illustrate the type of graphical outcomes produced by F-UJI and FAIR Checker, respectively, for the REACH database. The manual analysis is not presented in details here neither, but it can also be found in [13].\nThe ten selected databases have all a persistent URL, what partially satisfy the findability criterion. Note that the databases managed by ECHA (including Registered Substances, C&L Inventory, SCIP), EU Commission (CosIng), and ChemView (managed by EPA) were not yet indexed in FAIRSharing nor in PubChem. For this reason, the metadata was not accessible and the accessibility score is lower than ChemSpider, CTD, Comptox, and T3DB.\nIn terms of accessibility, all datasets satisfied this criterion for manual analysis methods due to their open access nature. Notice that some datasets, such as ChemSpider, CTD, and ChemView, require users to submit an access request to download or reuse the data. But, their websites provide sufficient guidance for users to navigate this process. We found that the accessibility scores were low for automatic analysis as none of the websites provided enough information in the metadata (or do not have metadata) to explicitly indicate where data was published. Additionally, the absence of license information in the metadata further reduced the accessibility scores.\nInteroperability emerged as the most challenging criterion to satisfy. Although textual or video documents explaining the dataset structures are sometimes available for humans, machine-interpretable information is often missing. Only three publishers (CompTox, ChemSpider, and ChemView) offer API access and provide documentation for their use. A critical interoperability issue identified was the inconsistent use of CAS numbers\u00b9 as unique identifiers for chemical substances. For instance, the uniqueness constraint was not always respected, and some substances were published without CAS numbers. Harmonizing vocabularies, identifiers, and descriptors are essential steps to improve interoperability, as well as establishing mappings between different data sources. Initiatives like PubChem, ChemSpider, and IPCHEM have demonstrated the feasibility and"}, {"title": "HazardChat Platform", "content": "In the previous section, we suggested that FAIR data could be easily reused to address problems related to chemical substances, particularly those impacting human health. Here, we demonstrate the advantages of having linked data available to healthcare professionals. To achieve this, we designed and are currently implementing HazardChat, a platform that uses a chatbot interface and exploits aggregated information on chemical hazards collected from several reliable public sources and stored as a knowledge graph. This work focuses on identifying and analyzing the barriers to building a database that disseminates data on hazardous chemical substances commonly used by European industries and found in both the USA and European (EU) markets. We demonstrate the impact that FAIR data can have on building new platforms for specific healthcare needs and discuss the strengths and weaknesses of existing technologies for enhancing data quality and accessibility.\nFor building the KG, we used three sources of information:\n1)\nChemical factsheet from ECHA REACH database. Collected in November 2023, it was used to extract the list of hazardous substances. Since the information is not available in a downloadable format, it requires parsing the html files. The template is not standard, making the parsing difficult or requiring manual intervention. All substances have an EC number (a European identifier for chemical substances), but not all of them have a CAS number. From the substance factsheet we also extracted the Hazard Class, the hazard phrase used to further describe the class, and the type of products where the substance can be found (Product Category).\n2)\nEPA CTD database. Collected in June 2024, it was used to extract the disease associated to hazardous substances. The list of substances and the map to the diseases are available in csv and xml formats. The substances are identified by an internal code (ChemicalID) and, when available, the CAS number. The diseases are identified with the MESH2 code or the OMIM\u00b3 code, when both exist, the preference is given to MESH code. The list of CTD substances is not a subset of the list of REACH registered substances. Thus, we used only the intersection of them to map with diseases.\n3)\nThe National Institute for Occupational Safety and Health (NIOSH) database, from the USA Centers for Disease Control and Prevention (CDC). Collected in June 2024, it was used to identify the organs that are impacted by the hazardous substances. The information is presented in the html file describing the substances. Each substance has a CAS number as identifier. After parsing the files 34 distinct organs or systems (e.g., respiratory system, eyes, etc.) were identified. However, CDC also consider as organ parts of the body such as enzymes (e.g. blood cholinesterase). Moreover, beside of the organs' name there is, sometimes, a short explanation/precision between parenthesis. A manual intervention was sometimes necessary to reduce ambiguities and duplication.\nThe three data sources have complementary information and different properties, but all are open access. The work"}, {"title": "Graphical interface", "content": "After parsing the data sources and identifying all relevant information, we designed the schema of the database (Figure 3). Since this schema can evolve over time, we decided to adopt a graph model that is more flexible and relatively easy to query. The database adopted to store the data is Neo4J. There are two interfaces to access the information in Neo4J. One is 'Explore', that is similar to a SPARQL endpoint and allows writing and executing Cypher queries. The other is Bloom, a data visualization tool to quickly explore and freely interact with Neo4J's graph data platform with no coding required. The team has previous experience with this database and the Bloom interface shows to be appreciated by the end-users (with no ICT background) to explore the KG. Another reason is that Neo4J stores property graphs, what facilitates adding properties to the relations. For instance,"}, {"title": "Chat interface", "content": "The chatbot was built based on the OpenAI model (40-mini) and implemented in Python. We use our knowledge graph as an external source to apply the retrieval-augmented generation (RAG) [15] approach and reduce hallucinations while improving the quality of the answers. The prompt template and the results of the execution of the RAG can be seen in Table 2. The prompt creation process follows several steps. First, we defined the required expertise of the model (healthcare professional with strong background on chemistry and toxicology). Second, we provided a set of examples of queries/answers (few-shot approach) selected according to the cosine similarity of the embedded queries. Third, we add in the context of the prompt the schema of the graph. Forth, we asked to explicitly present the explanation of the answer (Chain-of-Thought [16]). Fifth, we asked to the model to create the Cypher query that represents the user's question. Sixth, we asked to the model to validate the query before showing it or, otherwise, to say that it does not know the answer. Finally, we also asked to produce a short text explaining/summarizing the outcomes of the query in a natural language, when it is relevant. The user will have access to the query, to the results provided by the graph, and to the text explaining it.\nHowever, more work is needed to understand how performance varies across different ranges of question types, data types, and dataset sizes, as well as to validate our approach with end users. As it is now, HazardChat is a demonstration tool that intend to provide information to healthcare professional. This information must be taken with caution when analysing the patient conditions.\nThe process of creating this knowledge graph revels several opportunities to improve the datasets. The format of the published information (html, csv, xml) required a case-by-case solution and human intervention to understand the templates or fields within the files and extract the relevant information. There is a need for a common terminology to"}, {"title": "Discussions", "content": "The combination of FAIR data principles, Knowledge Graphs, and Large Language Models presents a transformative paradigm for the field of knowledge management, specially in critical situations where decisions must be taken quickly. By combining these technologies and principles, healthcare professionals can significantly enhance data accessibility without requiring deep technical skills, thereby, or executing time consuming data search tasks, accelerating medical discovery and improving diseases diagnosis and treatments.\nThe FAIR principles provide a foundational framework for ensuring that data is findable, accessible, interoperable, and reusable. Adherence to these principles are essential for the construction and enrichment of KGs. Watford et al. [18] emphasized the necessity of data interoperability, for instance, in advanced computational toxicology, highlighting the need for robust data management practices to support the development of FAIR-compliant data repositories. The authors also highlight that while current data publishers provide functional and interactive platforms for accessing toxicological information, they often lack interoperability (e.g., using proprietary format, local IDs, no metadata). Improving this requires rigorous data management and stewardship practices, thereby enhancing the ability to integrate and utilize diverse data sources effectively. The EU Chemicals Strategy for Sustainability reinforce this idea in the communication published in [4] where they request \"[...] free the data access of technical or administrative obstacles, according to the principles that data should be easily findable, interoperable, secure, shared and reused by default. Data will be made available in appropriate formats and tools [...] - to ensure interoperability.\"\nWith this opportunity in mind, funding agencies like the European Commission have started to incentivise the scientific community to improve the FAIRness of research and data (including descriptions of methods used) and publish them as open access. Recently initiatives have taken place in FAIRness adoption (Go FAIR), also within the chemicals and materials sector (NanoSafetyCluster). Furthermore, a first generation of tools to manually assess FAIRness (e.g., DANS, SATIFYD, ARDC) were developed and made accessible in the form of online questionnaires. The second generation implemented an automatic assess process (e.g., F-UJI and FAIR Checker). Our work demonstrate that there are advances in this practice, but there is also room for improvements. Complementary to FAIR data sharing, Knowledge Graphs have emerged as a powerful tool for managing and integrating complex data sets, particularly in the domain of hazardous chemical management. By capturing semantic relationships between entities, KGs facilitate efficient data exploration, querying, and reasoning. Zheng et al. [19]"}, {"title": "Conclusions", "content": "In this paper, we analyzed ten data sources on hazardous chemical substances to assess their compliance with FAIR principles, identify the challenges in aggregating them, and explore the benefits of providing healthcare professionals with a user-friendly interface for querying and exploring this data. The ultimate outcome of this work is a platform built on top of a knowledge graph that offers visual and natural language interfaces for quick access to relevant data, aiding in decision-making regarding the diagnosis or treatment of patients' health issues. We conclude that the current state of these data sources does not fully satisfy all FAIR principles and human intervention to correctly aggregate information from multiple sources is still mandatory. Additionally, crucial information, such as licensing details, are not provided in a structured format. Despite the efforts of some data publishers, there is still a need for consensus on defining a global and unique identifier for chemical elements and mixtures. As a result, the mapping of chemical elements from the analyzed sources cannot be automated due to ambiguities in their identities. Finally, we demonstrate the feasibility of a platform that integrates knowledge graphs, chatbots, and FAIR data to provide easy and quick access to important health-related information. In our future work, we will focus on improving the interpretation of natural language queries, enhancing the precision of our Retrieval-Augmented Generation (RAG) approach, and strengthening the connection with scientific literature."}]}