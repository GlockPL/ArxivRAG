{"title": "Chain-of-Rank: Enhancing Large Language Models\nfor Domain-Specific RAG in Edge Device", "authors": ["Juntae Lee", "Jihwan Bang", "Seunghan Yang", "Kyuhong Shim", "Simyung Chang"], "abstract": "Retrieval-augmented generation (RAG) with\nlarge language models (LLMs) is especially\nvaluable in specialized domains, where preci-\nsion is critical. To more specialize the LLMs\ninto a target domain, domain-specific RAG has\nrecently been developed by allowing the LLM\nto access the target domain early via finetuning.\nThe domain-specific RAG makes more sense\nin resource-constrained environments like edge\ndevices, as they should perform a specific task\n(e.g. personalization) reliably using only small-\nscale LLMs. While the domain-specific RAG is\nwell-aligned with edge devices in this respect,\nit often relies on widely-used reasoning tech-\nniques like chain-of-thought (CoT). The rea-\nsoning step is useful to understand the given ex-\nternal knowledge, and yet it is computationally\nexpensive and difficult for small-scale LLMs to\nlearn it. Tackling this, we propose the Chain of\nRank (CoR) which shifts the focus from intri-\ncate lengthy reasoning to simple ranking of the\nreliability of input external documents. Then,\nCoR reduces computational complexity while\nmaintaining high accuracy, making it particu-\nlarly suited for resource-constrained environ-\nments. We attain the state-of-the-art (SOTA)\nresults in benchmarks, and analyze its efficacy.", "sections": [{"title": "1 Introduction", "content": "The integration of retrieval-augmented generation\n(RAG) with large language models (LLMs) (Lewis\net al., 2020) has emerged as a pivotal advance-\nment in mitigating the issue of factual hallucina-\ntion (Ji et al., 2023)\u2014an inherent limitation of\nLLMs when generating knowledge-intensive re-\nsponses. By leveraging external knowledge sources,\nRAG enables LLMs to utilize relevant knowledge\ndynamically, enhancing both the accuracy and reli-\nability of their outputs.\nRAG is especially crucial in the context of spe-\ncialized domains, where precision is paramount and\nerrors can be costly. Also, in RAG, LLMs must not\nonly incorporate the relevant external information\nas the input, but also contextualize the information\nwithin the nuances of the target domain. To opti-\nmize the RAG-LLM for a specific domain, recently\ndomain-specific RAG (Tianjun Zhang, 2024) has\nbeen developed where LLMs can early access the\ntarget domain through finetuning. The practicality\nof the domain-specific RAG is more noteworthy\nwhen computational resources are limited such as\nedge devices since with only a small-scaled LLM\nsome tasks should be performed reliably.\nDespite the promise of the domain-specific RAG,\nthe input external knowledge (generally retrieved\ninformation dubbed contexts) may consist of both\nirrelevant and relevant contexts. Hence, reasoning\nprocess such as chain-of-thought (CoT) (Wei et al.,\n2022) is useful for understanding and focusing on\nthe relevant context. To this end, in RAFT (Tian-\njun Zhang, 2024), LLM learns the reasoning as\nwell as answering in finetuning. Also, in (Yu et al.,\n2023), the reasoning is prompted to generate a sum-\nmary of all the contexts. Elaborated reasoning is\nbeneficial, and yet obtaining this kind of reason-\ning dataset for domain-specific learning is time-"}, {"title": "2 Related Works", "content": "Domain-specific RAG. In the existing training-\nbased RAG (Lin et al., 2024; Wang et al., 2024;\nAsai et al., 2023), the LLM is learned for vari-\nous domains, and then applied to unseen domains.\nHowever, for better contextualization or under con-\nstrained resource condition, it is beneficial for\nLLM to be early accessed to the target domain via\ntraining on the domain. To this end, RAFT (Tian-\njun Zhang, 2024) pioneered domain-specific RAG.\nIn RAFT, the LLM is learned by alternating two\nloss functions which are designed to simulate open-\nbook and closed-book cases, respectively. The first\nloss addresses both distracting and golden contexts,\nwhile the second loss does only distracting ones.\nHowever, for decent performance evenly across var-\nious datasets, they trained the LLM to learn how to\nmake the intricate reasoning as well as the answer.\nReasoning techniques in LLM. CoT (Wei et al.,\n2022) reasoning has been shown to enhance per-"}, {"title": "3 Method", "content": "consuming and costful, and also it incurs a large\ntesting cost.\nUnreliability of reasoning is also a critical issue,\nespecially when the parameter-efficient fine-tuning\n(PEFT) like LORA adapters (Hu et al., 2021; Huang\net al., 2023; Bang et al., 2024) are used to reduce\nthe computational burden in training resource con-\nstraint environment. Namely, the PEFT adapters\nare efficient but lack enough learning capacities,\nand then struggle to learn the intricate reasoning\nprocess. As shown in Fig. 1, LLaMA3-8B (Dubey\net al., 2024) with LoRA exhibits marginal gains\nwhen learned with CoT. It means that the intricate\nreasoning can become a hindrance (Shi et al., 2023)\nin resource-constrained domain-specific RAG.\nInstead of focusing the intricate reasoning pro-\ncesses, we narrow the focus to the ranking of the\ncontexts' relevance, then the LLM can streamline\nits reasoning and reduce the cognitive load required\nto generate accurate final answers. Building on this\ninsight, we propose the Chain-of-Rank (CoR) ap-\nproach. In this method, the model is learned to\noutput just the ID of the contexts which are rele-\nvant to the query, and the answer. Then, CoR not\nonly reduces computational complexity but also\nenables the LLM to concentrate more fully on the\ncritical information, leading to more accurate and\ndomain-specific outputs. This focus on relevance\nrather than elaborate reasoning aligns well with the\nresource limitations of small-scale LLMs and edge\ndevices."}, {"title": "3.1 RAG Problem Set-up", "content": "In RAG, an LLM can be formalized as p(y|x) =\n\u2211Dp(y|x, D)p(D|x), where x denotes the input\nquery, y represents the LLM generated answer, and\nD = {dk}K1 contains the K individual contexts.\nThis formulation takes into account the joint prob-\nability of retrieving a set of contexts, rather than\nassuming the contexts are selected independently.\nAs this sum over all the context sets is impracti-\ncal, generally an off-the-shelf retriever selects the\ntop-K most relevant contexts. This leads to the\napproximation: p(y|x) \u2248 p(y|x, D). Furthermore,\nwhen a reasoning step is considered, it becomes as\nfollows.\np(y|x) = \u2211Rp(y\\x, D, R)p(R\\x, D) (1)\nwhere R represents the generated reasoning."}, {"title": "3.2 Chain-of-Rank", "content": "Framework. We streamline the reasoning process\nby shifting the focus from complex reasoning to\nidentifying the IDs of the given contexts that cor-\nrespond to the most relevant ones for x. With just\nthis process, the model can reduce cognitive over-\nhead on less relevant information, and more pay\nattention to the relevant information. As illustrated"}, {"title": "4 Experiments", "content": "We provide more details and analysis in Appendix.\nDatasets. In our experiments, we use the follow-\ning datasets to evaluate the proposed method. We\nselected these datasets to represent both popular\nand diverse domains including Wikipedia and Cod-\ning/API documents.\nIn specific, we select HotPotQA (Yang et al.,\n2018) and Gorilla API datasets (Patil et al.,\n2023). The HotPotQA is the open-domain question-\nanswers based on Wikipedia, mainly focused on"}, {"title": "4.1 Comparative Results", "content": "We evaluate our CoR and demonstrate the efficacy\nin Table 1. The non-specified pre-trained LLM\n(LLaMA3-8B) shows severely degraded scores in\nthe API datasets than in the natural questions of\nHotPotQA, which proves the requirements and im-\nportance of domain-specific RAG. The reasoning-\nbased methods, RAFT and CoN, attain better re-\nsults than DSF. However, in F1 score, the effect of\nCoT is marginal. Also, although the noting strategy\nof CoN is tailored for RAG, it sometimes shows\nlower performance than the straightforward DSF\nas well as CoT (see TensorFlow and Huggingface\nresults). Whereas, we see that the proposed CoR\nconsistently and significantly outperforms the base-\nlines in all the datasets. It means that learning the\ncomplex reasoning can be a burden to the PEFT on\nthe smaller-scale LLM, and thus simply identifying\nthe IDs of the relevant contexts is more beneficial.\nWe also study the extension of the proposed CoR\nto domain-agnostic RAG in Appendix."}, {"title": "4.2 Analysis", "content": "Reasoning quality. In RAG, the reasoning can\nbe utilized to support the answer. Therefore, the\nquality of reasoning is also substantial, then we"}, {"title": "5 Conclusions", "content": "We proposed the Chain of Rank (CoR) to address\nthe limitations of the existing intricate reasoning\nprocesses like chain-of-thought in training-based,\ndomain-specific RAG. For domain-specific RAG\ntraining, annotation expense for the reasoning data\nis required. Also, especially in testing on smaller\nLLMs in resource-constrained environments, it\nposes challenges in terms of the accuracy as well\nas computational cost. We observed that the in-\naccurate reasoning adversely affect the quality of\nfinal answer. By shifting the focus from elaborate\nreasoning to a simplified ranking of the reliability\nof retrieved documents, CoR significantly reduced\ncomputational complexity while attaining higher\naccuracy. Our experimental results demonstrated"}, {"title": "6 Limitations", "content": "This work acknowledges the significance of reason-\ning in domain-specific RAG models and presents\nan efficient approach that reduces the need for com-\nplex training data labeling and significantly lowers\nreasoning costs during testing. However, we did\nnot thoroughly investigate whether the proposed\nmethod would be equally effective in more general\nRAG frameworks that do not rely on task-specific\ntraining. That said, preliminary results presented\nin the appendix indicate the potential for success\nin general RAG settings, suggesting that this area\nwarrants deeper exploration in future work. There-\nfore, our findings provide a promising foundation\nfor future research."}, {"title": "7 Ethical Consideration", "content": "In the field of domain-specific RAG, if the appli-\ncations involve sensitive areas such as personal\ninformation, special caution must be taken during\nthe model training process to ensure privacy and\ndata protection. Beyond this consideration, method-\nologically, our research focuses on improving the\naccuracy and efficiency of RAG in LLMs, we do\nnot foresee any direct negative ethical concerns\nstemming from our contributions. Nonetheless, it\nis important to recognize that generative AI tech-\nnologies, including those using LLMs, come with\npotential risks. As such, careful consideration of\ntheir broader ethical and societal implications is\nnecessary when these systems are applied in the\nreal world."}]}