{"title": "Generating Diverse Synthetic Datasets for Evaluation of\nReal-life Recommender Systems", "authors": ["Miha Malen\u0161ek", "Bla\u017e \u0160krlj", "Bla\u017e Mramor", "Jure Dem\u0161ar"], "abstract": "Synthetic datasets are important for evaluating and testing machine learning models. When evaluating real-life\nrecommender systems, high-dimensional categorical (and sparse) datasets are often considered. Unfortunately,\nthere are not many solutions that would allow generation of artificial datasets with such characteristics. For that\npurpose, we developed a novel framework for generating synthetic datasets that are diverse and statistically\ncoherent. Our framework allows for creation of datasets with controlled attributes, enabling iterative modifications\nto fit specific experimental needs, such as introducing complex feature interactions, feature cardinality, or specific\ndistributions. We demonstrate the framework's utility through use cases such as benchmarking probabilistic\ncounting algorithms, detecting algorithmic bias, and simulating AutoML searches. Unlike existing methods\nthat either focus narrowly on specific dataset structures, or prioritize (private) data synthesis through real data,\nour approach provides a modular means to quickly generating completely synthetic datasets we can tailor to\ndiverse experimental requirements. Our results show that the framework effectively isolates model behavior in\nunique situations and highlights its potential for significant advancements in the evaluation and development of\nrecommender systems. The readily-available framework is available as a free open Python package to facilitate\nresearch with minimal friction.", "sections": [{"title": "1. Introduction", "content": "Recommender systems have significantly enhanced user experiences across various digital platforms,\nfrom e-commerce to streaming services and advertisement. These systems analyze user preferences and\nbehaviours to suggest relevant items, thereby increasing user engagement and satisfaction. However,\nevaluating and benchmarking these systems poses unique challenges due to the sensitivity and avail-\nability of real-world data. Privacy concerns, data access restrictions, and the need for diverse testing\nscenarios often limit researchers' ability to conduct comprehensive evaluations and perform thorough\nbenchmarking.\nSynthetic data generation offers a promising solution to these challenges and is already widely used\nin fields like computer vision and robotics [1], yet still in its nascent stages in the field of recommender\nsystems. Prior research in the field mainly focused on maintaining data fidelity and ensuring user privacy.\nFor instance, Slokom et al. [2] introduced the SynRec framework to generate partially synthetic data\nusing CART, while Berlioz et al. [3] applied differential privacy techniques to protect user information\nwith matrix factorization. Efforts to scale academic datasets to production standards include Antulov-\nFalin et al.'s [4] memory-biased random walks and Belletti et al. [5] fractal expansions. Provalov et al.\n[6] developed the SynEva framework using GAN techniques for synthetic data generation based on real\ndata. All this successful research point to significant potential for advancements in recommender systems\nthrough synthetic data, warranting further research on data fidelity, privacy-preserving techniques,\nand new simulation methods.\nDespite this, there remains a need for frameworks that generate completely synthetic data tailored\nspecifically to the evaluation of recommender systems. Such data should possess characteristics that\nmake it suitable for rigorous testing and algorithmic development, without relying on privacy-preserving\ntransformations of real data. This paper addresses this gap by proposing a comprehensive framework for\ngenerating diverse and statistically coherent synthetic datasets tailored to the evaluation of recommender\nsystems. Unlike techniques that focus on privacy or emulate real data, our approach creates entirely\nartificial, production-scale data with specific qualities and attributes. Controlling the generation process\nallows iterative modifications to fit specific experimental needs, such as introducing complex feature\ninteractions or increasing the cardinality of the dataset. Our deterministic generative process allows\nfor reproducibility and enables on-the-fly dataset modifications, reducing setup time for experimental\nscenarios. We demonstrate the framework's utility through use cases in benchmarking algorithms,\ndetecting algorithmic bias, and simulating AutoML searches."}, {"title": "2. Generating Categorical Datasets", "content": "To streamline the creation of synthetic datasets for classification tasks and avoid reliance on imperfect\nor complex real-world data samples, we introduce a comprehensive framework called CategoricalClassi-\nfication. The framework is available through the Python Package Index (PyPI, https://pypi.org/) and can\nbe installed with pip. The core of our solution is packaged into a Python class called CategoricalClassifi-\ncation. Functionalities implemented in this class allow for rapid generation of production-scale synthetic\ndatasets with specific attributes tailored to the nature of the problem, such as sparsity, high cardinality\nfeatures or specific distributions. Additionally, our framework offers functionalities to augment these\ndatasets by incorporating feature combinations, correlations, and noise.\nThe CategoricalClassification framework generates datasets comprised of integer arrays which represent\nvarious categorical values, from encoded categories, hashes, to counts. All generation processes are\nreproducible through the use of random seeds. For simpler datasets, a single function call can generate\na useful, synthetic dataset, while the functionalities described in table 1 streamline the generation of\nmore complex datasets with specific attributes to fit experimental needs. These can be specific feature\nvalue sets and distributions, specific feature-class relationships, or user defined feature combinations.\nTo demonstrate its capabilities, the example dataset seen in figure 1 was generated, featuring various"}, {"title": "3. Applications of custom synthetic datasets", "content": "In this section we consider three applications that showcase how custom generated synthetic datasets\ncan give novel insights into methods commonly used in recommender systems."}, {"title": "3.1. Use Case 1 \u2013 Benchmarking Probabilistic Counting Algorithms", "content": "Categorical data streams are ubiquitous in modern recommender systems. Features include different\ncategories, identifiers, or aggregates of numeric features and similar counts. When monitoring online\nstreams, effectively counting the number of unique items in the stream becomes a challenging compu-\ntational problem. Exact counting methods (such as hashing) adhere to substantial memory overhead\nthat seldom scales in practice. To remedy this shortcoming, probabilistic counting algorithms were\nintroduced. In particular, we're interested in the HyperLogLog family \u2013 algorithms aimed at estima-\ntion of the number of unique items (cardinality) in a data batch (part of a stream). We observed that\nprobabilistic counters, albeit much more memory efficient, introduce some noise in terms of precision\nwhich is expected. However, the issue is that common algorithms don't discriminate between low- and\nhigh-cardinality items. This in practice means that a low-cardinality feature, where estimate is of high\nimpact (e.g., count of days in a week) is occasionally miscounted, resulting in errors that carry bigger\nimpact than e.g., counting unique users on a site. To remedy this shortcoming, we introduce a caching\nmechanism to arbitrary HyperLogLog-like algorithm, where, to a certain degree, the algorithm remains\ndeterministic, and only switches to probabilistic mode of operation if its memory requirements exceed\nthe constrained (user-specified) value. See Figure 2 for a visualization of these results."}, {"title": "3.2. Use Case 2 \u2013 Detecting Algorithmic Bias", "content": "Datasets with complex feature interactions present significant challenges for machine learning models.\nAs the complexity of these interactions increases, so does the difficulty in accurately modelling and\ncapturing these relationships. Moreover, complex feature interactions can lead to overfitting or underfit-\nting, resulting in poor generalizations to unseen data, or the inability to capture intricate relationships\nin the data. Traditional linear models, such as the logistic regression are efficient, but unfortunately\noften fail to capture complex feature interactions. Advanced models, such as the DeepFM model which\ncombines factorization machines and deep neural networks [7, 8], are better equipped to handle such\ncomplexities. As such we are interested in how Logistic Regression and DeepFM models compare when\npresented with datasets with increasingly complex feature interactions. To systemically evaluate bias\nand performance, we generate an initial synthetic dataset with 4 relevant and 750 irrelevant features, 10k\nsamples and a nonlinear class relation. We introduce 20% of categorical noise and create various feature\ninteractions based on pairwise combinations of relevant features, which are subsequently removed\nfrom our generated dataset. We then iteratively remove the resulting combination features, perform\nminimal hyperparameter tuning and evaluate DeepFM and logistic regression performance over one\nepoch. When multiple types of combinations are present in the generated dataset, we observed an\nincrease in both the AUC and accuracy scores for both models."}, {"title": "3.3. Use Case 3 \u2013 Simulating AutoML search", "content": "Automated Machine Learning (AutoML) represents a paradigm shift in the way machine learning\nmodels are developed and deployed. AutoML aims to decrease the time and effort required to develop\nrobust machine learning solutions by automating data pre-processing, feature ranking and engineering,\nmodel selection, hyperparameter tuning and model evaluation.\nBuilding effective recommender systems involves managing complex feature interactions in a wide\nfeature space and often hinges on the fine-tuning of its underlying models and algorithms. One of the\nmost important AutoML operations in recommender systems is feature selection. More precisely, the\nquestion of finding the smallest subset of features that will, given an ML algorithm, deliver optimal, or\nclose to optimal performance. In this way one optimizes the size of a model which is especially valuable\nfor deployments on a large scale and when we are dealing with memory limitations.\nA commonly used strategy is to iteratively grow the feature set by adding the next best feature that\nis not already in the set. In the first step we find the best feature f\u2081 by finding the model that gives best\npredictions with a single feature. After that, we find the best model predicting with f\u2081 and another\nfeature, called f2, etc. We repeat the process until we have a feature set of the desired size or until we\nno longer see a performance boost. In this use case we use the aforementioned open source package\ncalled Outrank. Outrank implements the functionality for the iterative feature set growth \u2013 given an\nexisting subset of features from our dataset, provide the ranking of the remaining features by evaluating\nthe performance of all models containing the existing features and one of the remaining ones. For this\nwe use the so-called surrogate-SGD-prior heuristic of Outrank, which, under the hood, implements\nScikit-learn's SGDClassifier and runs 4-fold cross-validation score with a negative log loss.\nFor simulations of such AutoML searches, we generate three initial datasets with 4 relevant and\n900 irrelevant features, and a nonlinear class relation, with 10k, 50k, and 100k samples. As in section\n3.2, we introduce 20% of categorical noise and create various feature interactions based on pairwise\ncombinations of relevant features which are subsequently removed from our generated dataset. Figure\n4 shows the evolution of scores of the models, when adding new features.\nImportant to note here is that due to the complexity of the search, Outrank has no hyperparameter\noptimisation capability for the models that provide feature rankings. This explains why the results in\nfigure 4 are falling very fast with the increase of feature complexity. The performance of the models\nwith hyperparameter tuning has thus been evaluated similarly as in the section 3.2 (see Figure 5). The\npositive trend in the AutoML evolution of Outrank (figure 4) seems to imply a positive trend in the\nperformance of the corresponding models trained with hyperparameter tuning. This, on one hand, is\nuseful information for optimal feature selection. On the other hand, it shows that AutoML for feature\nselection without hyperparameter optimisation can be misleading."}, {"title": "4. Discussion and Conclusions", "content": "The framework presented in this paper offers a robust and versatile tool for creating synthetic datasets\nfor testing and evaluation of real-world recommender systems. By allowing precise control over dataset\nattributes, researchers can design experiments that isolate specific aspects of algorithm performance,\nespecially when investigating scenarios that are not commonly encountered in real-life data. Our use\ncases demonstrate the framework's practical application.\nWe showcased the usefulness of our synthetic data generation framework on several real-world\nscenarios. In the first one, we precisely evaluated probabilistic counting algorithms. By testing the\nalgorithms on more than 2k synthetic dataset we were able to highlight a key limitation \u2013 the probabilistic\ncounting algorithms' inability to discriminate between low- and high-cardinality items. By introducing\na caching mechanism, we enhanced an arbitrary HyperLogLog-like alogrithm's precision for low-\ncardinality features.\nIn the second example, we used our framework's ability to generate complex feature interactions to\nsystematically evaluate the performance of logistic regression and DeepFM models on datasets with\nvarying levels and amounts of feature interaction complexity. The results demonstrated DeepFM's\nsuperior ability to handle complex interactions, significantly outperforming logistic regression.\nFinally, we simulated AutoML search and evaluated its ability to identify relevant features. Through\nthese use cases we demonstrated our framework's efficacy in generating challenging datasets to simulate\nreal-world scenarios in a controlled environment.\nThe three use cases demonstrate our framework's capabilities at various stages in the recommender\nsystems' pipeline. Evaluating probabilistic counting algorithms may decrease memory overhead when\ndetermining feature cardinality. The framework's ability to simulate properties of real-life data in a\ncontrolled manner enables us to generate data tailored to a specific problem, enabling further insight\ninto model behavior (i.e. determining algorithm bias). By controlling the generation process we also\ncontrol feature relevance, enabling us to test key functionalities of AutoML systems integrated within\nmany recommender systems' pipelines.\nDespite its strengths, there are areas for future improvement. By integrating advanced generative\nmodels such as GANs or variational autoencoders we could further enrich the diversity and realism\nof the synthetic datasets. Additionally, expanding the framework to support other types of machine\nlearning tasks, such as regression, could broaden its applicability and impact. Nevertheless, by enabling\ncontrolled, repeatable experiments, our framework provides researches and practitioners with a powerful\ntool to advance the field, ultimately leading to more effective and reliable recommender systems."}]}