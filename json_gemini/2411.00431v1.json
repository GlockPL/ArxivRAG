{"title": "Integrating Fuzzy Logic into Deep Symbolic Regression", "authors": ["Wout Gerdes", "Erman Acar"], "abstract": "Credit card fraud detection is a critical concern for financial insti-\ntutions, intensified by the rise of contactless payment technologies.\nWhile deep learning models offer high accuracy, their lack of ex-\nplainability poses significant challenges in financial settings. This\npaper explores the integration of fuzzy logic into Deep Symbolic Re-\ngression (DSR) to enhance both performance and explainability in\nfraud detection. We investigate the effectiveness of different fuzzy\nlogic implications, specifically \u0141ukasiewicz, G\u00f6del, and Product, in\nhandling the complexity and uncertainty of fraud detection datasets.\nOur analysis suggest that the \u0141ukasiewicz implication achieves the\nhighest F1-score and overall accuracy, while the Product implication\noffers a favorable balance between performance and explainability.\nDespite having a performance lower than state-of-the-art (SOTA)\nmodels due to information loss in data transformation, our approach\nprovides novelty and insights into into integrating fuzzy logic into\nDSR for fraud detection, providing a comprehensive comparison\nbetween different implications and methods.", "sections": [{"title": "1 Introduction", "content": "Credit card fraud poses a significant and growing challenge for\nfinancial institutions, amplified by the advent of innovative tech-\nnologies such as contactless payment [8]. Global losses due to credit\ncard fraud were estimated at $32.39 billion in 2020 and are projected\nto exceed $40 billion by 2027 [18]. The Covid-19 pandemic further\naccelerated the shift from cash to cashless transactions, intensifying\nthe issue of credit card fraud. To protect customers from fraudulent\nactivities, banks deploy Fraud Detection Systems (FDS) to automati-\ncally flag and block suspicious transactions in real-time. Significant\nadvancements in these systems have been achieved through im-\nprovements in data quality and the enhanced use and performance\nof Artificial Intelligence (AI) and Deep Learning (DL) techniques\n[6]. Although DL has demonstrated exceptional performance in\nclassification accuracy [2], a major limitation is its lack of explain-\nability. [16]. This \"black box\" nature has hindered the adoption\nof Al in financial settings, where decisions must be transparent\nand explainable. Explainable Artificial Intelligence (XAI) offers a\npotential solution to this problem.\nDespite the growing interest in XAI, the intersection of fraud\ndetection and XAI remains underexplored. Recent efforts have taken\nvarious approaches to bridge this gap. One approach employs \u03a7\u0391\u0399\nmethods to interpret Machine Learning (ML) models post-training\nusing techniques such as SHAP (Shapley Additive Explanations)\nor LIME (Local Interpretable Model-Agnostic Explanations), which\nhave shown only modest improvements in user trust [13].\nAnother promising approach involves leveraging Symbolic Re-\ngression (SR), which seeks to extract closed-form expressions to\ndescribe underlying patterns in the data. These expressions are\ninherently explainable, resolving transparency issues. To advance\nSR, Petersen et al. [20] combined SR with Reinforcement Learning\n(RL), resulting in Deep Symbolic Regression (DSR). DSR employs a\nrecurrent neural network (RNN) trained with deep reinforcement\nlearning, where the reward is task-specific, producing expressions\ntailored to specific problems. These closed-form expressions show\nhigh predictive power and transparency, making them a viable so-\nlution to many of the previously mentioned issues. DSR utilizes a\nlibrary of tokens representing features, constants, or mathematical\noperators to generate expressions. The DSR framework creates a\nlist of tokens subject to constraints, optimizing them using the RNN\nbased on the reward function. An extension to DSR by Visbeek et al.\nsuccessfully applied DSR to the fraud detection domain, resulting in\nDeep Symbolic Classification (DSC) [25]. DSC adapts DSR for clas-\nsification tasks and uses the F1-score as the reward metric, offering\ncompetitive predictive performance with improved explainability.\nIn this paper, we propose extensions to the DSR framework by\nintegrating fuzzy logic. Fuzzy logic, based on the fuzzy set theory\nby Zadeh [27], categorizes reasoning into multiple levels, similar\nto human reasoning. Unlike strict classifications, fuzzy logic han-\ndles uncertainty and vagueness, making it suitable for real-world\ncomplexities. For instance, a person is not simply tall or short but\ncan manifest varying degrees of tallness. Fuzzy logic facilitates\nsmooth transitions between such degrees. In DSC, the output is a\nclosed-form mathematical expression [25].\nLogical implications provide intuitive explanations, since they\nnaturally represent general rules e.g., If transaction amount is high\nand receiver balance is low then fraud is the case.\nThese expressions are more intuitive as they mirror human rea-\nsoning. Various formulas derive fuzzy implications from fuzzy sets,\nand fuzzy logic provides a natural medium for expressing the vague-\nness within this logical structure. For instance, vague expressions\nsuch as amount being high might have a varying degree of truth"}, {"title": "2 Related Work and Background Knowledge", "content": "Prior research in fraud detection has explored various ML and DL\nalgorithms to address the challenges associated with different types\nof fraud. Traditional ML approaches, such as Random Forests, k-\nNearest Neighbour, Naive Bayes, Logistic Regression, and XGBoost\nhave been extensively studied due to their efficiency in handling\nstructured data [2, 5, 10]. However, these methods often struggle\nwith class imbalance and lack explainability, leading researchers to\ninvestigate novel techniques. Recent advancements in DL, particu-\nlarly convolutional neural networks (CNNs), have shown promise in\nimproving fraud detection by leveraging intricate patterns present\nin transaction data [2]. Furthermore, studies have emphasized the\nimportance of considering factors like data imbalance, false posi-\ntives, and the financial implications of fraud detection systems when\ndesigning effective frameworks. Recently, the black box nature of\nthese ML and DL algorithms has been critiqued. The introduction\nof the Artificial Intelligence Act [19] has formalized the importance\nof governance in AI applications, highlighting the need for X\u0391\u0399\nboth in general and in fraud detection in particular."}, {"title": "2.2 Explainable AI", "content": "While recent developments in AI promises stellar predictive per-\nformance, it often lacks explainability compared to simpler models.\nXAI aims to increase transparency and explainability of AI sys-\ntems by providing insights into the decision-making process, model\nbehavior, and factors influencing predictions. This is particularly\ncrucial in sensitive applications such as healthcare, finance, and se-\ncurity, where stakeholders require clear explanations for AI-driven\ndecisions to ensure trust and accountability [16]. These sensitive\napplications often face regulatory constraints, such as the EU's\nGeneral Data Protection Regulation (GDPR), requiring enhanced\nexplainability before adopting advanced AI applications [9]. X\u0391\u0399\ncan generally be classified into two categories: potentially inter-\npretable models, which have some level of inherent transparency\n(e.g., linear regression, decision trees, k-Nearest Neighbors), and\npost-hoc techniques, which enhance explainability after model train-\ning (e.g., Local Interpretable Model-agnostic Explanations (LIME)\nor SHapley Additive Explanations (SHAP)) [26]."}, {"title": "2.3 Symbolic Regression", "content": "Symbolic regression (SR) is a method to obtain mathematical ex-\npressions that accurately capture information from data. Unlike\nneural networks, SR aims to provide transparent closed-form ex-\npressions, modeling information from the underlying dataset in a\nform that is more easily interpretable by humans. Given a dataset\n(X, y), where each Xi has inputs $X_i \\in \\mathbb{R}^n$ and response $y_i \\in \\mathbb{R}$,\nSR attempts to generate a function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ that best fits the\ndataset, where f is a closed-form mathematical expression [17].\nThe formula generated by symbolic classification is much more\ncompact, and tells about the relation between features in contrast\nto rules generated by decision trees which are often verbose and\nflat in relation to other features.\nDeep symbolic regression (DSR) combines symbolic regression\nwith deep learning techniques, such as RNN's to extract underlying\npatterns and relationships from data. Unlike traditional symbolic\nregression, which relies only on predefined mathematical forms,\nDSR can learn from data and adapt its expressions to capture intri-\ncate dependencies, resulting in more accurate and flexible models.\nThe use of an RNN allows for a gradient-based approach to opti-\nmize these closed-form expressions, leveraging the performance of\ncertain expressions as the reward for the RNN. With training, the\nRNN can optimize the reward function per specific mathematical\noperator, re-evaluating the underlying distribution of each iter-\nation [20]. It utilizes mathematical operators from a predefined\nlibrary $ \\lambda $, which includes basic operators (e.g.,$\\div$, $\\cdot$), constants and\nfeatures from the dataset. This library enables the generation of\nclosed-form expressions using all available features and performing\nmathematical operations [20, 25]. Constraints are applied to pre-\nvent certain mathematical expressions from being generated, and\nthese constraints can be tailored to improve the DSR framework's\nperformance."}, {"title": "2.4 Fuzzy Logic", "content": "Fuzzy logic allow for expressing vagueness and uncertainty which\ncan then be incorporated into machine learning [24]. By using fuzzy\nsets, variables are no longer strictly categorized but assigned to\nmultiple categories. This flexibility allows fuzzy logic to generate\nfuzzy implications in various forms, which are particularly useful\nin fields like symbolic regression. Different formulas can deduce\nfuzzy implications from fuzzy sets. In this paper, we focus on G\u00f6del,\nProduct, and \u0141ukasiewicz implications."}, {"title": "2.5 Integrating Fuzzy Logic in Symbolic\nRegression", "content": "Symbolic regression traditionally focuses on generating mathe-\nmatical expressions that best fit the data. However, using logical\noperators instead of purely mathematical ones can improve ex-\nplainability and enhance alignment with human reasoning. The\nlanguage of logic is more natural for explaining phenomena, as\nlogical operators like \"and\", \"or\", and \"not\" are closer to daily lan-\nguage. For instance, rules in the form of implications can be easily\nunderstood as \"if-then\" statements, which are common in human\nreasoning. Historically, logic stems from natural language, originat-\ning with syllogisms by Aristotle, and abstracting important aspects\nof human reasoning [7]. The \u0141ukasiewicz implications and logic\nare essentially a modern reconstruction of these syllogisms [14].\nThis historical context indicates the similarities of logic with how\nhumans understand and describe the world [22]. Therefore, fuzzy\nlogic, which incorporates similar logical operators, offers a more\nintuitive and explainable framework compared to purely mathemat-\nical expressions. Fuzzy logic is particularly suitable for application\nin symbolic regression because it retains the numerical foundation\nof symbolic regression while leveraging the explainability of logical\nexpressions. By incorporating fuzzy sets and implications, we can\nhandle vagueness and uncertainty more effectively while aligning\nthe explanation with human reasoning."}, {"title": "3 Methodology", "content": "We adapt DSR to integrate fuzzy logic, extending the existing DSR\napproach by integrating fuzzy implications into the expression\ngeneration and evaluation stages. Traditionally, the DSR framework\nutilizes a generated traversal of tokens to create a binary syntax\ntree, which is then used to generate expressions of these tokens\nand test their performance on the dataset. Our approach augments\nthis framework to integrate fuzzy logic implications [20].\nTo facilitate the use of fuzzy logic in our model, we transform\nrelevant features into fuzzy sets, capturing degrees of membership\nsuch as \"very large\" or \"somewhat small.\" This transformation al-\nlows the model to better manage the uncertainty and variability\npresent in financial transactions. Fuzzy logic formulas, as shown in\nTable 1, are employed in generating the expressions.\nWe test and compare three different fuzzy logic implications:\nG\u00f6del, Product, and \u0141ukasiewicz. Fuzzy membership functions de-\ntermine the degree to which a transaction belongs to each class,\nproviding a spectrum of likelihood rather than a binary decision.\nWe implement and evaluate the S-implication, together with the\nT-norm, T-conorm, and strong negator for these three fuzzy impli-\ncations. Note that the S-implications, in contrast to R-implications,\nare the ones that generalize material implications, since they use\nthe T-conorm in their construction [20]. Therefore, we utilize the\nS-implication and disregard the R-implication. Additionally, we\ncompare a combination of all three fuzzy logic implications, result-\ning in a comparison between four function sets.\nOur implementation is flexible, allowing the use of any\nS-implications, T-norms, T-conorms, and strong negators. To re-\nsearch the impact of implementing the S-implication as the root of\nthe expression, we modify the DSR package. We add a constraint\nensuring the S-implication is always the parent of a T-norm, T-\nconorm, or strong negator. This ensures the S-implication is never\na child of any other function. However, due to the nature of syntax\ntrees generated from functions and features, not every traversal\nincludes the S-implication. To address this, the token for the S-\nimplication was added to the traversal at index 0 if it was not yet\nincluded. If the traversal already contains the S-implication token,\nwe swap its position with the token at index 0.\nThrough reinforcement learning techniques, we train the model\nto optimize for F1-score, following the research of [25]. During\ntraining, a RNN produces mathematical expressions and evaluates\nthem based on their ability to describe the dataset, a measure known\nas \"fitness\". This fitness score links to a reward, which trains the\nRNN through a risk-seeking policy gradient algorithm. The risk-\nseeking policy gradient algorithm is defined in Equation 1. This\nrisk-seeking policy is utilized to improve the model's functioning.\nThe risk-seeking policy focuses on improving the reward of the top\n$ \\epsilon $ fraction of samples, disregarding samples below the threshold.\nThis method aims to enhance the highest performing expressions,\neven if it results in a lower average performance [20]. We use the\nF1-score as the main reward function, although we also test the\nF2-score as the reward function. The policy gradient algorithm"}, {"title": "3.1 Model", "content": "We design DSR to integrate fuzzy logic, extending the existing DSR\napproach by integrating fuzzy implications into the expression\ngeneration and evaluation stages. Traditionally, the DSR framework\nutilizes a generated traversal of tokens to create a binary syntax\ntree, which is then used to generate expressions of these tokens\nand test their performance on the dataset. Our approach augments\nthis framework to integrate fuzzy logic implications [20].\nTo facilitate the use of fuzzy logic in our model, we transform\nrelevant features into fuzzy sets, capturing degrees of membership\nsuch as \"very large\" or \"somewhat small.\" This transformation al-\nlows the model to better manage the uncertainty and variability\npresent in financial transactions. Fuzzy logic formulas, as shown in\nTable 1, are employed in generating the expressions.\nWe test and compare three different fuzzy logic implications:\nG\u00f6del, Product, and \u0141ukasiewicz. Fuzzy membership functions de-\ntermine the degree to which a transaction belongs to each class,\nproviding a spectrum of likelihood rather than a binary decision.\nWe implement and evaluate the S-implication, together with the\nT-norm, T-conorm, and strong negator for these three fuzzy impli-\ncations. Note that the S-implications, in contrast to R-implications,\nare the ones that generalize material implications, since they use\nthe T-conorm in their construction [20]. Therefore, we utilize the\nS-implication and disregard the R-implication. Additionally, we\ncompare a combination of all three fuzzy logic implications, result-\ning in a comparison between four function sets.\nOur implementation is flexible, allowing the use of any\nS-implications, T-norms, T-conorms, and strong negators. To re-\nsearch the impact of implementing the S-implication as the root of\nthe expression, we modify the DSR package. We add a constraint\nensuring the S-implication is always the parent of a T-norm, T-\nconorm, or strong negator. This ensures the S-implication is never\na child of any other function. However, due to the nature of syntax\ntrees generated from functions and features, not every traversal\nincludes the S-implication. To address this, the token for the S-\nimplication was added to the traversal at index 0 if it was not yet\nincluded. If the traversal already contains the S-implication token,\nwe swap its position with the token at index 0.\nThrough reinforcement learning techniques, we train the model\nto optimize for F1-score, following the research of [25]. During\ntraining, a RNN produces mathematical expressions and evaluates\nthem based on their ability to describe the dataset, a measure known\nas \"fitness\". This fitness score links to a reward, which trains the\nRNN through a risk-seeking policy gradient algorithm. The risk-\nseeking policy gradient algorithm is defined in Equation 1. This\nrisk-seeking policy is utilized to improve the model's functioning.\nThe risk-seeking policy focuses on improving the reward of the top\n$ \\epsilon $ fraction of samples, disregarding samples below the threshold.\nThis method aims to enhance the highest performing expressions,\neven if it results in a lower average performance [20]. We use the\nF1-score as the main reward function, although we also test the\nF2-score as the reward function. The policy gradient algorithm"}, {"title": "3.2 Data", "content": "In this research, we utilize the popular and well-researched synthet-\nically generated dataset, PaySim [3]. This dataset consists of simu-\nlated transactions based on a distribution of proprietary real trans-\nactions. Due to inherent privacy concerns with real-life datasets,\nthis is the most suitable alternative available. PaySim contains over\n6 million financial transactions over a period of one month, with a\nfraud rate of 0.13%. Given the true/false rate of 0.13/99.87, this is a\nhighly imbalanced dataset and should be handled accordingly.\nThe dataset includes the following features [25]:\n\u2022 step - unit of time; one step corresponds to one hour of time,\n\u2022 type - a categorical feature with values: cash-in, cash-out,\ndebit, payment, or transfer,\n\u2022 amount - amount of the transaction,\n\u2022 nameOrig - name of the customer,\n\u2022 oldbalanceOrg - customer's balance before the transaction,\n\u2022 newbalanceOrig - customer's balance after the transaction,\n\u2022 nameDest - name of the recipient,\n\u2022 oldbalanceDest - recipient's balance before the transaction,\n\u2022 newbalanceDest - recipient's balance after the transaction,\n\u2022 isFlaggedFraud - an indicator of whether the transaction\nhas been flagged as fraudulent in the simulation,\n\u2022 isFraud - an indicator of the transaction being legitimate or\nfraudulent.\nThe isFraud feature represents the target variable, which serves\nas the ground truth for training our model.\nPer the creators' request [3], the features oldbalance Orig, new-\nbalanceOrig, oldbalanceDest, newbalanceDest should not be used\ndirectly. This is because fraudulent transactions are cancelled, mean-\ning the balances of the originator and recipient remain unchanged.\nUsing these features directly would result in artificially high accu-\nracy, failing to accurately classify real fraud detection cases.\nInstead, these features provide insights into typical transaction\nbehavior. To avoid direct usage, we use imputation techniques to\ngenerate new features that capture balance changes without relying\non the original balance columns. Specifically, we engineer features\nsuch as newbalanceDest, oldbalanceOrig based on the transaction\namount plus oldbalanceDest, newbalance Orig. Hourly, daily, and\nmonthly features are also derived from the step feature. The feature\nis_workday is created based on the transaction volume distribution\nover a week, assuming the five highest volume days correspond to\nweekdays and the lowest two to weekends.\nCategorical features are one-hot encoded, converting textual\ncategorical data into binary columns, making them suitable for DSR.\nAlthough these features are not fuzzy sets, they are still useable for\nthis research.\nTo improve model robustness and generalizability, we introduce\nrandom noise into the dataset. For each feature column, Gaussian\nnoise is added using a method where the standard deviation of the\ntarget column is multiplied by a noise level parameter of 0,05. This\nnoise simulates real-world data imperfections, preventing the model\nfrom overfitting to clean training data. This approach enhances the\nmodel's resilience and its ability to generalize to new, unseen data.\nWe also incorporate transaction history by creating rolling av-\nerage and rolling max features for the amount column. Using 3\nand 7 transaction windows, we calculate the mean and max trans-\naction amounts for each recipient (nameDest), helping to identify\ndiscrepancies based on transaction history.\nTo transform the dataset into fuzzy sets, we divide the dataset\ninto percentiles. Each feature is represented as a fuzzy set according\nto these percentiles. We calculate the 20th, 40th, 60th, and 80th\npercentiles for each feature, segmenting the data into five ranges.\nFor each feature value, we assign a fuzzy membership value based\non its percentile. For example, values less than or equal to the\n20th percentile receive a membership value of 0.2, the 20th to 40th\npercentile values receive 0.4, and so on. This transformation is\napplied to all relevant features, ensuring they are represented as\nfuzzy sets, facilitating the integration of fuzzy logic into our model\n[21]. The features used are all either transformed into fuzzy sets,\nor binary values."}, {"title": "3.3 Hyperparameter Tuning", "content": "To prepare the dataset for machine learning tasks, it is necessary to\npartition it into distinct sets for training, validation, and final testing.\nThis process ensures that the model's performance can be accu-\nrately assessed and optimized. Due to computational constraints\nand efficiency considerations, hyperparameter tuning is performed\non a subset of the data, while the final testing is conducted on\nanother separate subset.\nInitially, a sample of one million transactions is extracted from\nthe dataset. It is essential to ensure that the balance of fraudulent"}, {"title": "3.4 Evaluation", "content": "Evaluation of the proposed framework can be approached from two\nperspectives: classification performance and explainability perfor-\nmance. Classification performance represents how accurately the\nframework detects fraudulent transactions. To correctly assess this\nperformance", "measures": "accuracy and F1 score.\nAccuracy is a standard gauge for comparing the proposed frame-\nwork to current industry standards. However, due to the highly\nimbalanced nature of our dataset, accuracy can be misleading. A\nhigh accuracy is easily achieved by marking all transactions as\nnon-fraudulent, making it less meaningful in our context. There-\nfore, the F1 score is also used to address this imbalance problem [12", "12": "is also investigated, where recall\nis twice as important as precision.\nExplainability of the framework is assessed using a Pareto front.\nA Pareto front evaluates the trade-off between expression complex-\nity and predictive performance. Operators in the DSR framework\nare weighted based on their pre-determined complexity. For in-"}]}