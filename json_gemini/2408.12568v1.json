{"title": "Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune CNNs and Transformers", "authors": ["Sayed Mohammad Vakilzadeh Hatefi", "Maximilian Dreyer", "Reduan Achtibat", "Thomas Wiegand", "Wojciech Samek", "Sebastian Lapuschkin"], "abstract": "To solve ever more complex problems, Deep Neural Networks are scaled to billions of parameters, leading to huge computational costs. An effective approach to reduce computational requirements and increase efficiency is to prune unnecessary components of these often over-parameterized networks. Previous work has shown that attribution methods from the field of explainable AI serve as effective means to extract and prune the least relevant network components in a few-shot fashion. We extend the current state by proposing to explicitly optimize hyperparameters of attribution methods for the task of pruning, and further include transformer-based networks in our analysis. Our approach yields higher model compression rates of large transformer- and convolutional architectures (VGG, ResNet, ViT) compared to previous works, while still attaining high performance on ImageNet classification tasks. Here, our experiments indicate that transformers have a higher degree of over-parameterization compared to convolutional neural networks. Code is available at https://github.com/erfanhatefi/Pruning-by- eXplaining-in-PyTorch.", "sections": [{"title": "1 Introduction", "content": "In recent years, Deep Neural Networks (DNNs) have been growing increasingly larger, demanding more computational resources and memory. To address these challenges, several efficient architectures, such as MobileNet [23] or Efficient-Former [29], have been proposed to reduce computational costs. However, the gain in efficiency comes at the cost of performance, as inherently efficient architectures struggle to keep pace with the recent surge in high-performing transformer models."}, {"title": "2 Related Work", "content": "In the following, we introduce related works in the field of XAI, efficient Deep Learning and the intersection between both.\nExplainability and Local Feature Attribution Methods Research in local explain-ability led to a plethora of methods [30,37], commonly resulting in local feature attributions quantifying the importance of input features in the decision-making process. These attributions are often shown in the form of heatmaps in the vision domain. Notably, methods based on (modified) gradients, backpropagate attribu-tions from the output to the input through the network, conveniently offering at-tributions of all latent components and neurons in a single backward pass [42,44]. Gradient-based attribution methods, however, can suffer from noisy gradients, rendering them unreliable for deep architectures [6]. Prominently, LRP [5,33] introduces a set of different rules (with hyperparameters) that allow to reduce the noise level. In fact, as shown in [35], attribution methods such as LRP can be optimized for certain XAI criteria, e.g., faithfulness or complexity [22, 33]. We follow up on this observation, and specifically optimize XAI w.r.t. pruning.\nPruning of Deep Neural Networks For pruning CNNs, either individual (kernel) weights or whole structures, e.g., filters of convolution layers or neurons can be recognized as candidates for pruning [21]. For transformer architectures, such structures include heads of attention modules or linear layers inside transformer blocks [27,46]. In order to prune such structures, several criteria have been pro-posed to indicate which components are best suited to be removed, retaining performance as best as possible. The work of [19] suggests pruning parameters based on weight magnitudes, offering a computationally-free criterion. Alternatively, in [11], the authors propose to prune neurons based on their activation"}, {"title": "3 Methods", "content": "This work proposes a framework for pruning DNNs using attribution methods from the field of XAI with hyperparameters specifically optimized for sparsification. We begin with presenting our method in the form of a general XAI-based pruning principle in Sec. 3.1, followed by introducing LRP attributions and cor-responding hyperparameters suitable for optimization, in Sec. 3.2 and Sec. 3.3, respectively. Lastly, Sec. 3.4 describes our optimization methodology."}, {"title": "3.1 Attribution-based Pruning", "content": "For our structured pruning framework, we view a DNN as a collection of p (in-terlinked) components \u03a8 = {41,...,p}, that can correspond to, e.g., whole layers, (groups of) neurons, convolutional filters or attention heads. We further assume access to an attribution method that generates attribution scores (rel-evance) R(xi) of component \u03c8\u03ba \u2208 \u03a8 for the prediction of a sample xi. The overall relevance of k is then estimated through the mean relevance over a set of reference samples Xref = {X1,X2,...,xnref} as\nRk = \\frac{1}{Nref} \\Sigma_{i=1}^{Nref} B_{\\psi_{\\kappa}} (Xi).   (1)\nUltimately, we collect relevances for all components via the set R, given as\nR = {R\u03c81, Rw2,..., R\u03c8}, (2)\nwhich, in turn, allows to define a pruning order for the model components. Specifically, the indices c for the components to be pruned up to the q-th place are given by"}, {"title": "3.2 Layer-wise Relevance Propagation", "content": "Layer-wise Relevance Propagation [5,34] is a rule-based backpropagation algo-rithm that was designed as a tool for interpreting non-linear learning models"}, {"title": "3.3 Tuneable Hyperparameters of LRP", "content": "Within the LRP framework, various rules (as detailed in Appendix A.2) have been proposed to tune relevance computation for higher explainability."}, {"title": "3.4 Hyperparameter Optimization Procedure", "content": "In order to optimize the hyperparameters of an attribution method, we first define an optimization objective C. As we perform our analysis for classification tasks, we measure the top-1 accuracy on the validation dataset. In principle, a different performance criterion can be chosen here. Concretely, we measure model performance for different pruning rates PR\u2081 = i as given after step i \u2208 {0, ... m - 1} of in total m steps (excluding a 100% rate). After sequentially increasing the pruning rate, and plotting performance against sparsity, we receive"}, {"title": "4 Experiments", "content": "We begin our experiments with exploring the over-parameterization problem of DNNs in Sec. 4.1. This is followed by our results on finding the best LRP hy-perparameters for pruning CNNs and ViTs in Sec. 4.2 and Sec. 4.3, respectively. Lastly, we compare the effect of pruning using ideal and random attributions in Sec. 4.4 by evaluating how explanation heatmaps change after pruning.\nExperimental Setting In our experiments, we optimize LRP hyperparameters for pruning convolution filters of VGG-16 [41] (with and without BatchNorm [24] layers), ResNet-18 and ResNet-50 [20] architectures (with 4224, 4224, 4800 and 26560 filters overall, respectively), as well as linear layers and attention heads of the ViT-B-16 transformer [12] (with 46080 neurons and 144 heads). All models are pre-trained [31] and evaluated on the ImageNet dataset [10]. For hyperpa-rameter optimization, we measure model performance for 20 pruning rates (from 0% up to 95%) on the validation dataset. To compute latent attributions, a set of reference samples has been chosen from the training set of ImageNet (different set sizes are discussed later in Sec. 4.2 and 4.3)."}, {"title": "4.1 How Over-Parameterized are Vision Models?", "content": "Training a large DNN from scratch to solve a specific task can be computationally expensive. A popular approach for saving training resources is to instead fine-tune a large pre-trained (foundation) model, which often requires fewer training epochs and provides high (or even higher) model generalization, especially in sparse data settings [9]. Notably, one of the effects that arise when solving for simple(r) tasks, is that we likely end up with an over-parameterized model as only a subset of very specialized latent features are necessary to solve the task, which makes pruning especially interesting in this case.\nIn fact, when pruning the ResNet-18 and VGG-16-BN models that were pre-trained to detect all of the 1000 ImageNet classes, parameter count can only be reduced by a couple of percent without meaningful degradation of model"}, {"title": "4.2 Finding the Optimal Attributions for CNNs", "content": "Motivated by the previous Sec. 4.1, we in the following simulate a setting with over-parameterized networks that allows us to measure more significant differ-ences between methods. Specifically, we restrict the data domain to three Ima-geNet classes and evaluate pruning using 20 different random seeds. Experiments using toy models in the work of [49] showed that ten (or more) reference samples are already well suited for estimating the overall relevances of network compo-nents (as used in Eq. (1)). We validated their finding also for the ResNet-18 model, as depicted in Fig. \u0391.2.\nWhereas we begin with optimizing LRP for CNN pruning, we follow up with transformers in the next section. Regarding CNNs, we can strongly reduce the accuracy-sparsity tradeoff when pruning the convolution filters with our method based on optimizing LRP attributions compared to other baselines, e.g., the heuristically chosen variant of LRP used by [49] (see Appendix A.3), as also"}, {"title": "4.3 Finding the Optimal Attributions for Vision Transformer", "content": "There are two structures of interest in the Vision Transformers for pruning in literature: attention heads and linear layers [27,45]. As observed in Sec. 4.1, the ViT model shows a higher degree of over-parameterization, possibly due to the abundance of neurons in the linear layers, ultimately highlighting the possible value for pruning. We now re-investigate the number of necessary reference sam-ples to robustly estimate the relevance of a model component of a ViT-B-16 model. Interestingly, unlike CNNs, according to Fig. 5, there is no deviation in pruning reliability and stability by using different numbers of reference samples. Consequently, our experiments have been applied with exact same settings as in Sec. 4.2.\nOur results from Fig. 6 again confirm an improvement of our optimized attribution-based pruning scheme (best LRP composites shown in Tab. A.1)"}, {"title": "4.4 How Pruning Affects Model Explanations", "content": "In addition to keeping track of the model performance, the field of XAI encour-ages us in the following to investigate the model behavior by observing explana-tion heatmaps. Concretely, we expect in the ideal pruning scheme, that heatmaps change as late as possible when increasing the pruning rate. This reflects that the task-relevant components are retained as long as possible.\nAs an illustrative example, we prune the attention heads of the ViT-B-16 model pre-trained on ImageNet with the aim to predict ImageNet corgi classes (\"Pembroke Welsh\u201d and \u201cCardigan Welsh\") as shown in Fig. 7. As a quantitative measure, we compute the cosine similarity between the original heatmap (using the recently proposed composite of [1]) and the heatmap of the pruned model for different pruning rates over the validation set.\nOn the one hand, we can see that random pruning or unoptimized attribution-based pruning leads to a much earlier change in heatmaps compared to our optimized LRP-based approach. The heatmaps indicate that irrelevant features are removed first as the pruning rate is increased (e.g., \"cat\" features in Fig. 7 get disregarded before corgi-related features in the process). However, a random pruning approach might wrongly omit an important structure first, as heatmap changes of \u201cmouth\u201d and \u201cear\u201d features can be seen in Fig. 7. On the other hand, as can be expected, the change in heatmap similarity highly correlates with the model confidence, resulting in a Pearson correlation coefficient of 0.99."}, {"title": "5 Conclusion", "content": "In this work, we propose a general framework for post-hoc DNN pruning that is based on using and optimizing attribution-based methods from the field of eX-plainable Artificial Intelligence for more effective pruning. For our framework, the method of LRP is well-suited by offering several hyperparameters to tune attri-butions. When applying our framework, we can strongly reduce the performance-sparsity tradeoff of CNNs and especially ViTs compared to previously established approaches. Vision transformers are on the one hand more sensitive towards hy-perparameters, and also show higher over-parameterization. Overall, using local XAI methods for pruning irrelevant model components demonstrates high po-tential in our experiments."}, {"title": "Appendix", "content": ""}, {"title": "A Attribution Methods", "content": "As it has been discussed in Sec. 3.1, backpropagation methods are suitable for our proposed framework. We focus mostly on LRP and it will be compared with Integrated Gradient as a commonly used gradient-based explainer."}, {"title": "A.1 Integrated Gradient", "content": "As an improvement to pure gradient, Integrated Gradient [44] interpolates input x for m steps and computes their gradients sequentially.\nIG(x) = (x \u2212 x')\\int_{\\alpha=0}^{1} \\frac{\\partial f_j(x' + \\alpha \\times (x \u2013 x'))}{\\partial x} dx   (A.1)\n= (x-x')  \\Sigma_{k=1}^{m}  \\frac{\\partial f_j(x' + \\frac{k}{m}  \\times (x \u2212 x'))}{\\partial x} \\frac{1}{m}  (A.2)\nUnlike LRP, there is no parameter to be tuned in this method. m from equa-tion Eq. (A.1) only serves as an approximation factor, with higher steps indicat-ing more precise computation of the integral. For the experiments conducted in this paper, we interpolate each input for 20 steps."}, {"title": "A.2 Variants of LRP", "content": "In addition to the vanilla rule of LRP demonstrated in 6, [5,33] later on proposed different rules as an extension of vanilla rule designed to serve different purposes. Most common extension of LRP will be discussed based on the layer type they target."}, {"title": "A.3 Tackling Linear and Convolution Layers", "content": "As a side note, in the propagation process of LRP, linear layers of DNNs such as fully connected, and convolution layers, are treated similarly.\nLRP-6 The most fundamental problem of rule 6 causing computational insta-bility, is division by zero which takes place when a neuron is not activated at all (zj = 0). LRP-e with \u0454 \u2208 R as a stabilizer parameter, was proposed to tackle this problem so that the denominator never reaches zero:\nRij = \\frac{Zij}{zj + esign(zj)}Rj   (A.3)\nAs a side note, sign(0) = 1. Although the value of e can be tuned, we set it to le 6 in every use-case."}, {"title": "A.4 Tackling Attention and Softmax Non-linearity", "content": "The key component of the attention module [45], consists of two equations:\nA = softmax(\\frac{Q.KT}{\\sqrt{dk}})   (A.6)\nO= A. V  (A.7)\nwhere (.) denotes matrix multiplication, K\u2208 Rb\u00d7sk\u00d7dk is the key matrix, Q \u2208 Rb\u00d7sqxdk is the queries matrix, V \u2208 Rbxskxdv the values matrix, and O \u2208 Rbxskxdv is the final output of the attention mechanism. b is the batch dimension including the number of heads, and dk, du indicate the embedding dimensions, and sq, Sk are the number of query and key/value tokens.\nCP-LRP In the extension of LRP from [4], it was proposed to regard the atten-tion matrix (softmax output) A in Eq. (A.7) as constant, attributing relevance solely through the value path by stopping the relevance flow through the soft-max. Consequently, the matrix multiplication Eq. (A.7) can be treated with the e-rule Eq. (A.3)."}, {"title": "A.5 Faithful LRP Composites", "content": "The works of [2,25] introduced 2 different LRP composites as faithful explainer [38]. For CNNs, [25] suggested applying LRP-z+ to FCLs and LRP-e to other remaining layers (including HLLs, MLLS, LLLs).\nOn the other hand, experiments from [2] over the layers of transformers (differentiated based on layer types) demonstrated simple LRP-\u20ac (Eq. (A.3)) generates faithful explanation for Large Language Models (LLMs). However, unlike LLMs, the composite of Tab. A.1 seems to be more suitable for ViTs."}, {"title": "B Spatial Dimension of To-Be-Pruned Components", "content": "In Eq. (1), Rp(xi) presented the relevance of component p for reference sample Xi. However, it is crucial to discuss the spatial dimensionality of each component depending on its type because the relevance of a component gets aggregated over its dimensions. Convolution filters of CNNs have the size of the tuple (h, w, d) indicating the height, weight, and depth of a filter. Thus, for a batch of size n and convolution filter p, Tab. A.1 will be transformed to:\nn\\h\\w\\dR_p = \\Sigma \\Sigma \\Sigma \\Sigma R_{(p,j,l,r)} (Xi)   (A.11)\njl\\r\nUnlike linear layers of CNNs which simply follow Eq. (1), an extra token axes (t) for neurons inside linear layers of transformers resulting in this modification:\nn\\tR_p = \\Sigma \\Sigma R_{(p,j)} (Xi)   (A.12)\nj\nAttention heads of Eq. (A.6), have two extra axes (dq, dk) regarding as Query, and Key. Aggregation of relevance follows below formula:\nn\\dq\\dkR_p = \\Sigma \\Sigma \\Sigma R_{(p,j,l)} (Xi)   (A.13)\ni\\j\nHowever, in cases of computing the magnitude of relevance, aggregation should follow this structure:\nn\\dq\\dkR_p = \\Sigma \\Sigma I\\Sigma R_{(p,j,l)} (xi)I   (A.14)\ni\\j"}, {"title": "C Additionally on Overparameterization", "content": "Similar to Fig. 3, Fig. A.1 demonstrates the effect of overparameterization ad-ditionally on VGG-16 and ResNet-50 signifying again over-use of parameters in simpler tasks. Similarly in this case, architectures with skip connections show more potential to be overparameterized."}, {"title": "DOn Optimization: From Bayesian to Grid Search", "content": "To find an optimal LRP composite for pruning, our approach takes place by discovering prospective parameters (Sec. 3.3) via Bayesian Optimization [16] (with Gaussian Process regressor [47] as a surrogate model back-bone), followed by Grid Search on the reduced parameter space. This seems to be a more effective solution rather than naively applying Grid Search over the whole parameters when we can reduce from 50% up to 90% of our search space approximately."}]}