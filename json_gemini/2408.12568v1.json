{"title": "Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune CNNs and Transformers", "authors": ["Sayed Mohammad Vakilzadeh Hatefi", "Maximilian Dreyer", "Reduan Achtibat", "Thomas Wiegand", "Wojciech Samek", "Sebastian Lapuschkin"], "abstract": "To solve ever more complex problems, Deep Neural Networks are scaled to billions of parameters, leading to huge computational costs. An effective approach to reduce computational requirements and increase efficiency is to prune unnecessary components of these often over-parameterized networks. Previous work has shown that attribution methods from the field of explainable AI serve as effective means to extract and prune the least relevant network components in a few-shot fashion. We extend the current state by proposing to explicitly optimize hyperparameters of attribution methods for the task of pruning, and further include transformer-based networks in our analysis. Our approach yields higher model compression rates of large transformer- and convolutional architectures (VGG, ResNet, ViT) compared to previous works, while still attaining high performance on ImageNet classification tasks. Here, our experiments indicate that transformers have a higher degree of over-parameterization compared to convolutional neural networks. Code is available at https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch.", "sections": [{"title": "Introduction", "content": "In recent years, Deep Neural Networks (DNNs) have been growing increasingly larger, demanding more computational resources and memory. To address these challenges, several efficient architectures, such as MobileNet [23] or Efficient-Former [29], have been proposed to reduce computational costs. However, the gain in efficiency comes at the cost of performance, as inherently efficient architectures struggle to keep pace with the recent surge in high-performing transformer models."}, {"title": "Related Work", "content": "In the following, we introduce related works in the field of XAI, efficient Deep Learning and the intersection between both.\nExplainability and Local Feature Attribution Methods Research in local explain-ability led to a plethora of methods [30,37], commonly resulting in local feature attributions quantifying the importance of input features in the decision-making process. These attributions are often shown in the form of heatmaps in the vision domain. Notably, methods based on (modified) gradients, backpropagate attributions from the output to the input through the network, conveniently offering attributions of all latent components and neurons in a single backward pass [42,44]. Gradient-based attribution methods, however, can suffer from noisy gradients, rendering them unreliable for deep architectures [6]. Prominently, LRP [5,33] introduces a set of different rules (with hyperparameters) that allow to reduce the noise level. In fact, as shown in [35], attribution methods such as LRP can be optimized for certain XAI criteria, e.g., faithfulness or complexity [22, 33]. We follow up on this observation, and specifically optimize XAI w.r.t. pruning.\nPruning of Deep Neural Networks For pruning CNNs, either individual (kernel) weights or whole structures, e.g., filters of convolution layers or neurons can be recognized as candidates for pruning [21]. For transformer architectures, such structures include heads of attention modules or linear layers inside transformer blocks [27,46]. In order to prune such structures, several criteria have been proposed to indicate which components are best suited to be removed, retaining performance as best as possible. The work of [19] suggests pruning parameters based on weight magnitudes, offering a computationally-free criterion. Alternatively, in [11], the authors propose to prune neurons based on their activation"}, {"title": "Methods", "content": "This work proposes a framework for pruning DNNs using attribution methods from the field of XAI with hyperparameters specifically optimized for sparsification. We begin with presenting our method in the form of a general XAI-based pruning principle in Sec. 3.1, followed by introducing LRP attributions and corresponding hyperparameters suitable for optimization, in Sec. 3.2 and Sec. 3.3, respectively. Lastly, Sec. 3.4 describes our optimization methodology."}, {"title": "Attribution-based Pruning", "content": "For our structured pruning framework, we view a DNN as a collection of p (interlinked) components \u03a8 = {41,...,\u03c8p}, that can correspond to, e.g., whole layers, (groups of) neurons, convolutional filters or attention heads. We further assume access to an attribution method that generates attribution scores (relevance) \\(R_{\\psi_k}(x_i)\\) of component \\(\\psi_k \\in \\Psi\\) for the prediction of a sample \\(x_i\\). The overall relevance of \\(\\psi_k\\) is then estimated through the mean relevance over a set of reference samples \\(X_{ref} = {x_1, x_2, ..., x_{n_{ref}}}\\) as\n\n\\(R_{\\psi_k} = \\frac{1}{N_{ref}} \\sum_{i=1}^{N_{ref}} R_{\\psi_k} (x_i).\\)\n\nUltimately, we collect relevances for all components via the set R, given as\n\n\\(R = {R_{\\psi_1}, R_{\\psi_2}, ..., R_{\\psi_{\\Psi}}},\\)\n\nwhich, in turn, allows to define a pruning order for the model components. Specifically, the indices c for the components to be pruned up to the q-th place are given by"}, {"title": "Layer-wise Relevance Propagation", "content": "Layer-wise Relevance Propagation [5,34] is a rule-based backpropagation algorithm that was designed as a tool for interpreting non-linear learning models by assigning attribution scores, called \"relevances\", to network units proportionally to their contribution to the final prediction value. Unlike other gradient- or perturbation-based methods, LRP treats a neural network as a layered directed acyclic graph with L layers and input x:\n\\(f(x) = f^L o ... o f^l o f^{l-1} o ... o f^1 (x)\\)\n\nBeginning with an initial relevance score \\(R_j^L\\) at output j of layer \\(f^L\\), the score is layer-by-layer redistributed to its input variables depending on the contribution from the input variables to the output value.\nGiven a layer, we consider its pre-activations \\(z_{ij}\\) mapping inputs i to outputs j and their aggregations \\(z_j = \\sum_i z_{ij}\\). Commonly in linear layers such a computation is given with \\(z_{ij} = a_iW_{ij}\\), where \\(w_{ij}\\) are its weight parameters and \\(a_i\\) the activation of neuron i.\nThen, LRP distributes relevance quantities \\(R_j\\) received from upper layers towards lower layers proportionally to the relative contributions of \\(z_{ij}\\) to \\(z_j\\), i.e.,\n\n\\(R_{ij}^{(l-1)} = \\frac{z_{ij}}{z_j} R_j^l,\\)\n\nIn other words, the relevance message \\(R_{ij}^{(l-1)}\\) quantifies the contribution of neuron i at layer l - 1, to the activation of neuron j at layer l.\nTo obtain the contribution of neuron i to all upper layer neurons j, all incoming relevance messages \\(R_{ij}^{(l-1)}\\) are losslessly aggregated as\n\n\\(R_i^{l-1} = \\sum_j R_{ij}^{(l-1)}.\\)\n\nThis process ensures the property of relevance conservation between adjacent layers:\n\n\\(\\sum_i R_i^{l-1} = \\sum_{i,j} R_{ij}^l = \\sum_j R_j^l\\)\n\nwhich guarantees that the sum of all relevance in each layer stays the same.\nWhen a group of neurons performs the same task within a convolutional channel or attention head, it is beneficial to aggregate the total relevance of the entire group into a single relevance score. This aggregation process helps in simplifying the analysis and interpretation of the model's behavior by focusing on the collective relevance of each convolutional channel or attention head, rather than examining individual neurons. Further discussions of component-wise aggregation are given in Appendix B."}, {"title": "Tuneable Hyperparameters of LRP", "content": "Within the LRP framework, various rules (as detailed in Appendix A.2) have been proposed to tune relevance computation for higher explainability."}, {"title": "Hyperparameter Optimization Procedure", "content": "In order to optimize the hyperparameters of an attribution method, we first define an optimization objective C. As we perform our analysis for classification tasks, we measure the top-1 accuracy on the validation dataset. In principle, a different performance criterion can be chosen here. Concretely, we measure model performance for different pruning rates \\(PR_i = \\frac{i}{m}\\) as given after step \\(i \\in {0, ... m - 1}\\) of in total m steps (excluding a 100% rate). After sequentially increasing the pruning rate, and plotting performance against sparsity, we receive"}, {"title": "Experiments", "content": "We begin our experiments with exploring the over-parameterization problem of DNNs in Sec. 4.1. This is followed by our results on finding the best LRP hyperparameters for pruning CNNs and ViTs in Sec. 4.2 and Sec. 4.3, respectively. Lastly, we compare the effect of pruning using ideal and random attributions in Sec. 4.4 by evaluating how explanation heatmaps change after pruning.\nExperimental Setting In our experiments, we optimize LRP hyperparameters for pruning convolution filters of VGG-16 [41] (with and without BatchNorm [24] layers), ResNet-18 and ResNet-50 [20] architectures (with 4224, 4224, 4800 and 26560 filters overall, respectively), as well as linear layers and attention heads of the ViT-B-16 transformer [12] (with 46080 neurons and 144 heads). All models are pre-trained [31] and evaluated on the ImageNet dataset [10]. For hyperparameter optimization, we measure model performance for 20 pruning rates (from 0% up to 95%) on the validation dataset. To compute latent attributions, a set of reference samples has been chosen from the training set of ImageNet (different set sizes are discussed later in Sec. 4.2 and 4.3)."}, {"title": "How Over-Parameterized are Vision Models?", "content": "Training a large DNN from scratch to solve a specific task can be computationally expensive. A popular approach for saving training resources is to instead fine-tune a large pre-trained (foundation) model, which often requires fewer training epochs and provides high (or even higher) model generalization, especially in sparse data settings [9]. Notably, one of the effects that arise when solving for simple(r) tasks, is that we likely end up with an over-parameterized model as only a subset of very specialized latent features are necessary to solve the task, which makes pruning especially interesting in this case.\nIn fact, when pruning the ResNet-18 and VGG-16-BN models that were pre-trained to detect all of the 1000 ImageNet classes, parameter count can only be reduced by a couple of percent without meaningful degradation of model"}, {"title": "Finding the Optimal Attributions for CNNs", "content": "Motivated by the previous Sec. 4.1, we in the following simulate a setting with over-parameterized networks that allows us to measure more significant differences between methods. Specifically, we restrict the data domain to three ImageNet classes and evaluate pruning using 20 different random seeds. Experiments using toy models in the work of [49] showed that ten (or more) reference samples are already well suited for estimating the overall relevances of network components (as used in Eq. (1)). We validated their finding also for the ResNet-18 model, as depicted in Fig. \u0391.2.\nWhereas we begin with optimizing LRP for CNN pruning, we follow up with transformers in the next section. Regarding CNNs, we can strongly reduce the accuracy-sparsity tradeoff when pruning the convolution filters with our method based on optimizing LRP attributions compared to other baselines, e.g., the heuristically chosen variant of LRP used by [49] (see Appendix A.3), as also"}, {"title": "Finding the Optimal Attributions for Vision Transformer", "content": "There are two structures of interest in the Vision Transformers for pruning in literature: attention heads and linear layers [27,45]. As observed in Sec. 4.1, the ViT model shows a higher degree of over-parameterization, possibly due to the abundance of neurons in the linear layers, ultimately highlighting the possible value for pruning. We now re-investigate the number of necessary reference samples to robustly estimate the relevance of a model component of a ViT-B-16 model. Interestingly, unlike CNNs, according to Fig. 5, there is no deviation in pruning reliability and stability by using different numbers of reference samples. Consequently, our experiments have been applied with exact same settings as in Sec. 4.2.\nOur results from Fig. 6 again confirm an improvement of our optimized attribution-based pruning scheme (best LRP composites shown in Tab. A.1)"}, {"title": "How Pruning Affects Model Explanations", "content": "In addition to keeping track of the model performance, the field of XAI encourages us in the following to investigate the model behavior by observing explanation heatmaps. Concretely, we expect in the ideal pruning scheme, that heatmaps change as late as possible when increasing the pruning rate. This reflects that the task-relevant components are retained as long as possible.\nAs an illustrative example, we prune the attention heads of the ViT-B-16 model pre-trained on ImageNet with the aim to predict ImageNet corgi classes (\"Pembroke Welsh\u201d and \u201cCardigan Welsh\") as shown in Fig. 7. As a quantitative measure, we compute the cosine similarity between the original heatmap (using the recently proposed composite of [1]) and the heatmap of the pruned model for different pruning rates over the validation set.\nOn the one hand, we can see that random pruning or unoptimized attribution-based pruning leads to a much earlier change in heatmaps compared to our optimized LRP-based approach. The heatmaps indicate that irrelevant features are removed first as the pruning rate is increased (e.g., \"cat\" features in Fig. 7 get disregarded before corgi-related features in the process). However, a random pruning approach might wrongly omit an important structure first, as heatmap changes of \"mouth\" and \"ear\" features can be seen in Fig. 7. On the other hand, as can be expected, the change in heatmap similarity highly correlates with the model confidence, resulting in a Pearson correlation coefficient of 0.99."}, {"title": "Conclusion", "content": "In this work, we propose a general framework for post-hoc DNN pruning that is based on using and optimizing attribution-based methods from the field of eX-plainable Artificial Intelligence for more effective pruning. For our framework, the method of LRP is well-suited by offering several hyperparameters to tune attributions. When applying our framework, we can strongly reduce the performance-sparsity tradeoff of CNNs and especially ViTs compared to previously established approaches. Vision transformers are on the one hand more sensitive towards hyperparameters, and also show higher over-parameterization. Overall, using local XAI methods for pruning irrelevant model components demonstrates high potential in our experiments."}, {"title": "Appendix", "content": ""}, {"title": "Attribution Methods", "content": "As it has been discussed in Sec. 3.1, backpropagation methods are suitable for our proposed framework. We focus mostly on LRP and it will be compared with Integrated Gradient as a commonly used gradient-based explainer."}, {"title": "Integrated Gradient", "content": "As an improvement to pure gradient, Integrated Gradient [44] interpolates input x for m steps and computes their gradients sequentially.\n\\(IG(x) = (x - x') \\int_{\\alpha=0}^{1} \\frac{\\partial f_j(x' + \\alpha \\times (x \u2013 x'))}{\\partial x} d\\alpha \\)\n\\(IG(x) = (x-x') \\sum_{k=1}^{m} \\frac{\\partial f_j(x' + \\frac{k}{m}(x \u2013 x'))}{\\partial x} \\frac{1}{m} \\)\nUnlike LRP, there is no parameter to be tuned in this method. m from equation Eq. (A.1) only serves as an approximation factor, with higher steps indicating more precise computation of the integral. For the experiments conducted in this paper, we interpolate each input for 20 steps."}, {"title": "Variants of LRP", "content": "In addition to the vanilla rule of LRP demonstrated in 6, [5,33] later on proposed different rules as an extension of vanilla rule designed to serve different purposes. Most common extension of LRP will be discussed based on the layer type they target."}, {"title": "Tackling Linear and Convolution Layers", "content": "As a side note, in the propagation process of LRP, linear layers of DNNs such as fully connected, and convolution layers, are treated similarly.\nLRP-\u03f5 The most fundamental problem of rule 6 causing computational instability, is division by zero which takes place when a neuron is not activated at all (zj = 0). LRP-\u03f5 with \\(\\epsilon \\in R\\) as a stabilizer parameter, was proposed to tackle this problem so that the denominator never reaches zero:\n\n\\(R_{ij} = \\frac{z_{ij}}{z_j + \\epsilon sign(z_j)} R_j\\)\nAs a side note, sign(0) = 1. Although the value of \u03f5 can be tuned, we set it to le 6 in every use-case."}, {"title": "Tackling Attention and Softmax Non-linearity", "content": "The key component of the attention module [45], consists of two equations:\n\\(A = softmax(\\frac{Q.K^T}{\\sqrt{d_k}})\\)\n\\(O= A. V\\)\nwhere (.) denotes matrix multiplication, \\(K\\in R^{b\\times s_k\\times d_k}\\) is the key matrix, \\(Q \\in R^{b\\times s_q \\times d_k}\\) is the queries matrix, \\(V \\in R^{b\\times s_k\\times d_v}\\) the values matrix, and \\(O \\in R^{b\\times s_k\\times d_v}\\) is the final output of the attention mechanism. b is the batch dimension including the number of heads, and \\(d_k\\), \\(d_v\\) indicate the embedding dimensions, and \\(s_q\\), \\(s_k\\) are the number of query and key/value tokens.\nCP-LRP In the extension of LRP from [4], it was proposed to regard the attention matrix (softmax output) A in Eq. (A.7) as constant, attributing relevance solely through the value path by stopping the relevance flow through the softmax. Consequently, the matrix multiplication Eq. (A.7) can be treated with the \u03f5-rule Eq. (A.3)."}, {"title": "Spatial Dimension of To-Be-Pruned Components", "content": "In Eq. (1), \\(R_{\\psi_p}(x_i)\\) presented the relevance of component p for reference sample \\(x_i\\). However, it is crucial to discuss the spatial dimensionality of each component depending on its type because the relevance of a component gets aggregated over its dimensions. Convolution filters of CNNs have the size of the tuple (h, w, d) indicating the height, weight, and depth of a filter. Thus, for a batch of size n and convolution filter p, Tab. A.1 will be transformed to:\n\n\\(R_p = \\sum_{j}^{n} \\sum_{l}^{h} \\sum_{r}^{w} \\sum_{i}^{d} R_{(p,j,l,r)} (x_i)\\)\n\nUnlike linear layers of CNNs which simply follow Eq. (1), an extra token axes (t) for neurons inside linear layers of transformers resulting in this modification:\n\n\\(R_p = \\sum_{j}^{n} \\sum_{i}^{t} R_{(p,j)} (x_i)\\)\n\nAttention heads of Eq. (A.6), have two extra axes (dq, dk) regarding as Query, and Key. Aggregation of relevance follows below formula:\n\n\\(R_p = \\sum_{j}^{n} \\sum_{l}^{d_q} \\sum_{i}^{d_k} R_{(p,j,l)} (x_i)\\)\n\nHowever, in cases of computing the magnitude of relevance, aggregation should follow this structure:\n\n\\(R_p = \\sum_{j}^{n} \\sum_{l}^{d_q} |\\sum_{i}^{d_k} R_{(p,j,l)} (x_i)|\\)"}, {"title": "Additionally on Overparameterization", "content": "Similar to Fig. 3, Fig. A.1 demonstrates the effect of overparameterization additionally on VGG-16 and ResNet-50 signifying again over-use of parameters in simpler tasks. Similarly in this case, architectures with skip connections show more potential to be overparameterized."}, {"title": "On Optimization: From Bayesian to Grid Search", "content": "To find an optimal LRP composite for pruning, our approach takes place by discovering prospective parameters (Sec. 3.3) via Bayesian Optimization [16] (with Gaussian Process regressor [47] as a surrogate model back-bone), followed by Grid Search on the reduced parameter space. This seems to be a more effective solution rather than naively applying Grid Search over the whole parameters when we can reduce from 50% up to 90% of our search space approximately."}, {"title": "On Number of Reference Samples", "content": "Fig. A.2 indicates and validates the findings of work [49] that using minimally 10 reference samples results in stable pruning for CNNs, as no more impact can be observed when using more samples. The fewer used samples per class lead to a lower pruning rate."}, {"title": "Optimized Composites", "content": "In later tables, based on the conducted optimization, we demonstrate top composites for pruning on CNNs (Appendix F.1), ViT-B-16 (Appendix F.1) and propose a general composite with persuasive performance to use over each architecture type individually. Later, in Appendix F.1 we discuss which parameters are more important to be tuned."}, {"title": "Evaluating the Composite", "content": "As described in Sec. 3.3, we proposed our pruning framework as a means of evaluating attribution methods in terms of aligning with the actual importance of latent components. However, it is noted that composites highly scored by this"}, {"title": "Layer-wise Relevance Flow", "content": "Later figures (A.4, A.5, A.6, A.7, A.8, A.9) partially elucidate the pruning strategy by illustrating relevance magnitude of components in a layer-wise fashion."}, {"title": "Change in Behaviour", "content": "In later figures (A.10, A.11, A.12), we qualitatively demonstrate examples of pruning based on 3 randomly chosen classes and trace changes in the explanation heatmap of the pruned model."}]}