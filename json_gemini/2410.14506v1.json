{"title": "SignAttention: On the Interpretability of Transformer Models for Sign Language Translation", "authors": ["Pedro Alejandro Dal Bianco", "Oscar Agust\u00edn Stanchi", "Facundo Manuel Quiroga", "Franco Ronchetti", "Enzo Ferrante"], "abstract": "This paper presents the first comprehensive interpretability analysis of a Transformer-based Sign Language Translation (SLT) model, focusing on the translation from video-based Greek Sign Language to glosses and text. Leveraging the Greek Sign Language Dataset, we examine the attention mechanisms within the model to understand how it processes and aligns visual input with sequential glosses. Our analysis reveals that the model pays attention to clusters of frames rather than individual ones, with a diagonal alignment pattern emerging between poses and glosses, which becomes less distinct as the number of glosses increases. We also explore the relative contributions of cross-attention and self-attention at each decoding step, finding that the model initially relies on video frames but shifts its focus to previously predicted tokens as the translation progresses. This work contributes to a deeper understanding of SLT models, paving the way for the development of more transparent and reliable translation systems essential for real-world applications.", "sections": [{"title": "1 Introduction", "content": "Sign Language Translation (SLT) refers to the process of converting a continuous sign language video into a corresponding written language translation, with the aim of bridging communication gaps between the deaf and hearing communities [1]. SLT is a complex task that can be roughly decomposed into three sub-problems: 1) Sign Language Segmentation (SLS): segmenting a video of continuous sign language into many individual signs; 2) Sign Language Recognition (SLR): classifying a video of a single sign; and 3) translation of the identified signs into a written language whose grammar differs from the grammar of the sign language.\nThis grammatical difference adds a layer of complexity for interpreting SLT models as there is no one-to-one mapping between signs and words, and the ordering might vary. Therefore, it is"}, {"title": "2 Related work", "content": "Interpretability in this field is largely underexplored, and the majority of existing research focuses primarily on SLR, rather than on the broader and more complex task of SLT. Example of this are [8, 9, 10], where CNNs and LSTMs SLR models are interpreted through human-grounded evaluation [11], SHAP [12], or Grad-CAM [13]. As for SLT, while some studies offer qualitative examples of interpretability, we have not identified any previous research that conducts a systematic analysis of interpretability in this domain. [14] introduces the Heterogeneous Attention Transformer model, an end-to-end encoder-decoder transformer evaluated on the PHOENIX2014T dataset, with self-attention maps used for interpretability to demonstrate an improved attention distribution across two examples. In [15], the Gloss Attention SLT Network is presented, also based on a transformer architecture and evaluated on the PHOENIX14T, CSL-Daily, and SP-10 [16] datasets, with self-attention map visualizations shown only for a single example. Finally, in [17], the Gloss semantic-Enhanced Network with Online Back-Translation is proposed, integrating a gloss encoder, a pose decoder, and an online reverse gloss decoder for semantic alignment. Interpretability in the PHOENIX14T dataset is examined through cross-attention mechanism visualizations, but only two examples are provided to ensure consistency between gloss and pose sequences. Instead of providing simple qualitative examples, here we present a comprehensive interpretability analysis of a Transformer-based SLT model, shedding light on the mechanisms that allow the interaction between both sign and written languages."}, {"title": "3 Experimental Setup", "content": null}, {"title": "3.1 Dataset", "content": "The Greek Sign Language Dataset is a laboratory-made dataset that contains 10,290 samples composed of RGB video, their corresponding gloss representation, and the translation to written Greek language. These samples correspond to 331 different sentences repeated many times by different signers. This dataset has two main benefits over other publicly available datasets: 1) it contains the gloss representation of each sample, useful for performing interpretability analysis; 2) since this dataset was constructed for research purposes, it contains many repetitions of each sentence and no singletons or words with low frequency that may add noise to model training (a common issue in other popular on-the-wild SL datasets such as PHOENIX-2014-T [18]). As input for the STL model, we used the positional keypoints of each video, precomputed using MediaPipe [19]."}, {"title": "3.2 Model", "content": "We trained a Transformer model [20], with a modified encoder for processing frames of pose keypoints as input data. This encoder consists of a 1 dimensional convolution run across the temporal dimension that generates an embedding of each frame. We used only one encoder layer and one decoder layer. This model and configuration were chosen as it allowed for direct interpretability of the attention weights while obtaining SoTA level Word Error Rate. For the main analysis described in this work we set the hidden dimension size to 16 to perform sign-to-gloss translation, but different sizes and sign-to-text translation were also tested. We discuss the results of these variations and their impact on the interpretability of the model in section 5.1."}, {"title": "4 Interpretability Analysis", "content": null}, {"title": "4.1 Decoder Cross-Attention", "content": "We analyzed the cross-attention scores for the encoded frames when predicting each token. A higher attention score means that more information from that particular frame was used in the prediction of a token. Figures 2a shows the scores obtained during the translation of a complete sentence, where we can observe the following.\n\n1. The model does not pay attention to individual frames but rather to clusters of contiguous frames. This behavior is not typical of the attention mechanism in text to text models, but can be observed in tasks like audio transcription where these clusters correspond to a single target token [21].\n\n2. The attention matrix seems to be shaped as a diagonal matrix, where the position of the predicted gloss in the sentence correlates with the part of the video the model is paying attention to: for the first glosses the model pays attention to the beginning of the video and, as we move forward through the sentence, the cluster of frames to which the models is paying attention moves forward as well."}, {"title": "4.2 Self-Attention vs Cross-Attention", "content": "In each decoding step, the cross-attention vector resulting from the previously analyzed scores for each frame, is combined with the self-attention vector by adding them to the embedding representation of each token. The vector with larger magnitude will be the one that adds more information to this new representation and thus to the prediction of the next token.\nTo understand the relative relevance of self and cross-attention for each prediction, we calculate the difference between the outputs of self-attention and cross-attention (averaged across the embedding dimensions). Positive values indicate a stronger contribution from the self-attention block (previous glosses), while negative values indicate a stronger contribution from the cross-attention block (the video poses)."}, {"title": "5 Global Results", "content": "In the previous section, we analyzed the behavior of a single example. To further interpret the global behavior of the model in the whole dataset, we calculated cross-attention scores and the difference between self-attention and cross-attention vectors for all samples of the test set that were correctly translated (714 samples, 81% of the test set), as discussing a false explanation would be meaningless from an epistemological standpoint [22, 23]. Then, we averaged them according to the number of glosses per sentence, which allowed us to generate an average visualization for each sentence length, shedding light into the general attention patterns for sentences with a specific number of glosses. These results are illustrated in Figure 3 and Figure 4.\nFor sentences containing up to 5 tokens, global results reveal a consistent pattern across the dataset, confirming what was observed from individual examples:\n\n1. Cross-attention focuses on clusters of frames than can be mapped to the execution of a whole sign, rather than individual frames."}, {"title": "5.1 Experiment variants", "content": null}, {"title": "5.1.1\nEmbedding size and encoder visualization", "content": "As shown in Table 1, the size of the hidden dimension did not have a significant impact on its accuracy; however, it did have an impact in terms of interpretability. The patterns described in this work were not found for larger sizes of the hidden dimension. A possible explanation is that, as the"}, {"title": "5.1.2 Text based model", "content": "We also performed an interpretability analysis on a model that performs proper SLT generating text instead of glosses. The decoder also learned to pay attention to clusters of frames, but these clusters were not diagonally aligned. This is expected due to the grammatical differences and the no one-to-one mapping between written Greek and Greek Sign Language. In addition, cross-attention was more relevant than self-attention for most of the sequence lengths."}, {"title": "6 Conclusions and future works", "content": "We performed an interpretability analysis of SLT models based on the Transformer architecture by generating different visualizations based on attention scores. In summary, our interpretability analysis suggests that:\n\n1. Transformer models for SLT learn to pay attention to sequencial clusters rather than individual frames.\n\n2. Cross-attention scores show a diagonal alignment between attended frames and gloss predicted, suggesting that the model is identifying the sign corresponding to each gloss.\n\n3. For the translation of a whole sentence, the model initially relies more in video frames but gradually shifts its focus to the previously predicted glosses.\n\n4. When translating longer sequences, the model relies more in glosses than in video frames, and patterns described in items 1 and 2 cannot be seen that clearly.\n\n5. We found patterns of sign segmentation in the encoder self-attention scores. These patterns are not present for bigger embedding sizes, harming the whole interpretability of the model.\n\n6. When trained to generate text, the model still learns to focus on clusters of frames; however, these clusters do not align diagonally with words, as expected due to the grammatical differences between the languages. Determining whether the resulting alignment is meaningful requires validation by an expert translator."}, {"title": "A Appendix / supplemental material", "content": null}, {"title": "A.1 Other single samples visualizations", "content": null}, {"title": "A.2 Sign-to-text model results", "content": null}]}