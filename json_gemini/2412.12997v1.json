{"title": "Enabling Low-Resource Language Retrieval: Establishing Baselines for Urdu MS MARCO", "authors": ["Umer Butt", "Stalin Veranasi", "G\u00fcnter Neumann"], "abstract": "As the Information Retrieval (IR) field increasingly recognizes the importance of inclusivity, addressing the needs of low-resource languages remains a significant challenge. This paper introduces the first large-scale Urdu IR dataset, created by translating the MS MARCO dataset through machine translation. We establish baseline results through zero-shot learning for IR in Urdu and subsequently apply the mMARCO multilingual IR methodology to this newly translated dataset. Our findings demonstrate that the fine-tuned model (Urdu-mT5-mMARCO) achieves a Mean Reciprocal Rank (MRR@10) of 0.247 and a Recall@10 of 0.439, representing significant improvements over zero-shot results and showing the potential for expanding IR access for Urdu speakers. By bridging access gaps for speakers of low-resource languages, this work not only advances multilingual IR research but also emphasizes the ethical and societal importance of inclusive IR technologies. This work provides valuable insights into the challenges and solutions for improving language representation and lays the groundwork for future research, especially in South Asian languages, which can benefit from the adaptable methods used in this study.", "sections": [{"title": "Introduction", "content": "Advancements in Natural Language Processing (NLP), particularly transformer architectures, have revolutionized Information Retrieval (IR), enhancing information access worldwide. However, this progress predominantly benefits high-resource languages, leaving low-resource languages like Urdu underrepresented. Urdu is spoken by over 70 million people primarily in South Asia, and it features unique linguistic characteristics, such as its Perso-Arabic script with right-to-left writing direction and rich morphology [2], which pose significant challenges for IR systems. These include complex tokenization, script handling, and morphological analysis, which are also present in some high-resource languages such as Arabic, Chinese, and Russian but have benefited from extensive research and resource availability, making these challenges less of a barrier in those languages."}, {"title": null, "content": "One of the primary challenges in expanding IR capabilities for low-resource languages like Urdu is the scarcity of large-scale, high-quality datasets necessary for training and evaluating models. Creating such datasets manually is resource-intensive and often impractical for languages with limited digital presence. Consequently, to close this gap, researchers have explored alternative methods, such as machine translation of existing datasets, but it introduces other challenges like translation errors and context loss, adversely affecting IR performance. To our knowledge, no prior work has specifically addressed these issues for IR in Urdu, leaving a gap that our work aims to fill.\nIn this paper, we address this issue by translating a large-scale MS MARCO passage ranking dataset [3] into Urdu using the IndicTrans2 model [5]. This translation provides a valuable resource for Urdu IR development. We first established baseline performance on our newly created dataset using BM25 and mMarco (a multilingual mT5 model trained on the MS MARCO dataset)[4] in a zero-shot setting, as it was not originally trained on Urdu. We then fine-tune the model on our Urdu-translated dataset. Our experiments demonstrate significant improvements in Mean Reciprocal Rank (MRR) and Recall metrics compared to the zero-shot results, highlighting the potential of our approach.\nOur key contributions are:\nCreation of the Urdu MS MARCO Dataset: We provide the first large-scale IR dataset for Urdu by translating the MS MARCO dataset using the IndicTrans2 model, addressing the data scarcity issue.\nEstablishment of Baseline IR Performance: We conduct experiments using BM25 and a fine-tuned mT5 model to establish baseline retrieval performance on the Urdu dataset.\nInsights into Low-Resource Language IR: We analyze the challenges and potential solutions for improving IR in Urdu, providing a foundation for future research in similar languages."}, {"title": "Related Work", "content": "The lack of large-scale, high-quality datasets for low-resource languages has been a significant barrier to advancing IR capabilities. To address this data scarcity, several efforts have created multilingual datasets through machine translation. For example, mMarco[4] extended the MS MARCO dataset [3] to 13 languages using Helsinki NLP models [10] and Google Translate. However, Urdu was not included, partly due to the lack of high-quality translation models for the language at that time.\nRecent developments in machine translation, such as the IndicTrans2 model[5], have improved translation quality for South Asian languages, including Urdu. IndicTrans2 achieves a chrF++\u00b9 score of 68.2 for English-to-Urdu translation,"}, {"title": "Dataset Creation", "content": null}, {"title": "Ms-Marco dataset", "content": "The MS MARCO Passage ranking dataset [3] 2 is a cornerstone in IR research, comprising over 8.8 million passages and more than 500 thousand unique queries each paired with at least one relevant passage. The commonly used development set includes 6,980 queries, while the training set is organized into 39 million triples, each consisting of a query, a relevant passage, and a non-relevant passage.  This dataset's scale and richness have significantly advanced English language IR models. However, replicating such progress for Urdu is challenging due to the absence of equivalent large-scale datasets or the prohibitive costs associated with translating such extensive corpora\u00b3."}, {"title": "Translation Process", "content": "To translate the dataset, we used the open-source IndicTrans2[5] model. We utilized the distilled 200-million parameter version of IndicTrans2 to translate the entire MS MARCO dataset into Urdu. The process involved tokenizing the text with the Indic Trans Tokenizer, translating in batches to optimize computational efficiency and leveraging GPU acceleration. Key parameters included a batch size of 32 and a maximum sequence length of 512 tokens to handle lengthy passages. We also performed preprocessing steps, such as normalization and error correction to enhance the quality and usability of the dataset and the model."}, {"title": "Experimental Setup", "content": null}, {"title": "Baseline Models", "content": "Given the absence of prior work on Urdu IR, establishing strong baseline performances is essential for meaningful comparison. We employed BM25, a well-established non-neural retrieval method [9], as our initial retrieval model to retrieve the top 1,000 passages per query[7]. BM25 serves as a fundamental baseline due to its simplicity and effectiveness in various IR tasks. However, its performance tends to be limited for low-resource languages, as it does not account for complex morphological patterns or linguistic nuances present in a language.\nTo complement BM25, we also evaluated the zero-shot performance of the mMARCO model [4] as a re-ranker. Although Urdu is not included in mMARCO'S training data, we aimed to assess whether the model could offer effective re-ranking for Urdu text based solely on its multilingual training. This evaluation provides an initial benchmark for the model's performance on low-resource languages like Urdu."}, {"title": "Urdu Re-ranker Fine-tuned Model", "content": "To enhance retrieval performance beyond the baseline, we fine-tuned the mMARCO model, a fine-tuned version of mT5, on our Urdu-translated MS MARCO dataset, following the original mMARCO training setup for consistency. We set the batch size to 32 with gradient accumulation over 4 steps, resulting in effective batch size of 128 to balance computational efficiency with model performance. The"}, {"title": "Evaluation Metrics", "content": "To assess the effectiveness of our models, we adopted the same evaluation metrics as the original mMarco benchmark, ensuring comparability. The Mean Reciprocal Rank at 10 (MRR@10) was calculated to evaluate the average reciprocal rank of the first relevant passage retrieved within the top 10 results for each query. This metric provides insight into the ranking quality of the retrieval system, emphasizing the position of the first relevant result. Additionally, we computed Recall at 10 (Recall@10) to measure the proportion of relevant passages retrieved in the top 10 results out of all relevant passages available for a query. This metric reflects the system's ability to retrieve as many relevant documents as possible within a limited set of results. Together, these metrics offer a balanced evaluation of both the precision and recall aspects of the retrieval performance."}, {"title": "Results and Discussion", "content": null}, {"title": "Performance Comparison", "content": "We evaluated the retrieval performance on the Urdu dataset using BM25 for initial retrieval, followed by different re-ranking strategies (see Table 2). The"}, {"title": null, "content": "baseline BM25 model is used to retrieve the top 1000 passages per query and it achieves an MRR@10 of 0.129 on the Urdu dataset. This performance is significantly lower than BM25 results on the English dataset and is among the lowest scores reported for other languages in prior studies [4]. This highlights the challenges of applying traditional retrieval methods to low-resource languages like Urdu, likely due to morphological complexity and limited linguistic resources.\nApplying the zero-shot mMARCO re-ranker to the BM25 outputs improved performance, despite the model not being trained on Urdu. The mMARCO model generalized to Urdu to some extent, likely due to its multilingual training on languages with similar scripts, such as Arabic. This suggests that the model's semantic understanding and script familiarity enabled it to enhance the ranking of relevant passages over the BM25 baseline.\nOur fine-tuned mMARCO model, trained on the Urdu-translated dataset, provided further improvements when used as a re-ranker over BM25 outputs. It achieved the highest performance with an MRR@10 of 0.247 and a Recall@10 of 0.438. Fine-tuning allowed the model to adapt to the linguistic characteristics of Urdu, enhancing its ability to distinguish relevant passages more effectively than the zero-shot model. These gains demonstrate that language-specific fine-tuning, even on machine-translated data, can significantly boost retrieval effectiveness in low-resource languages."}, {"title": "Impact of Translation Quality and Limitations", "content": "We acknowledge that machine translations, like those from IndicTrans2, may not match the precision of human annotations, and translation errors can misalign query and passage semantics, impacting retrieval precision.  Nonetheless, this work serves as a crucial first step toward enabling Urdu IR research. Furthermore, open-sourcing the translation pipeline, data, and model aims to support future multilingual IR projects and address data limitations in low-resource languages beyond Urdu, fostering broader inclusivity in IR research."}, {"title": "Future Work and Conclusion", "content": "This paper introduces a foundational approach for enabling Information Retrieval (IR) in low-resource languages by translating the MS MARCO dataset into Urdu and fine-tuning the mMARCO model on this data. The baseline BM25 model for Urdu showed significantly lower performance compared to English, underscoring the need for tailored models in Urdu IR. Our results demonstrate that fine-tuning significantly enhances retrieval performance, highlighting the potential of machine translation as a first step in overcoming data limitations for low-resource languages.\nThis work contributes to more inclusive IR by making retrieval systems accessible to Urdu speakers, with implications for other South Asian languages. Future work could involve incorporating manual verification to refine translation"}, {"title": null, "content": "quality and exploring hybrid methods that combine machine and human translations. Expanding this approach to other low-resource languages would further support the development of fair, balanced and multilingual IR technologies."}]}