{"title": "Federated Learning in Mobile Networks: A Comprehensive Case Study on Traffic Forecasting", "authors": ["Nikolaos Pavlidis", "Vasileios Perifanis", "Selim F. Yilmaz", "Francesc Wilhelmi", "Marco Miozzo", "Pavlos S. Efraimidis", "Remous-Aris Koutsiamanis", "Pavol Mulinka", "Paolo Dini"], "abstract": "The increasing demand for efficient resource allocation in mobile networks has catalyzed the exploration of innovative solutions that could enhance the task of real-time cellular traffic prediction. Under these circumstances, federated learning (FL) stands out as a distributed and privacy-preserving solution to foster collaboration among different sites, thus enabling responsive near-the-edge solutions. In this paper, we comprehensively study the potential benefits of FL in telecommunications through a case study on federated traffic forecasting using real-world data from base stations (BSs) in Barcelona (Spain). Our study encompasses relevant aspects within the federated experience, including model aggregation techniques, outlier management, the impact of individual clients, personalized learning, and the integration of exogenous sources of data. The performed evaluation is based on both prediction accuracy and sustainability, thus showcasing the environmental impact of employed FL algorithms in various settings. The findings from our study highlight FL as a promising and robust solution for mobile traffic prediction, emphasizing its twin merits as a privacy-conscious and environmentally sustainable approach, while also demonstrating its capability to overcome data heterogeneity and ensure high-quality predictions, marking a significant stride towards its integration in mobile traffic management systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Mobile traffic prediction is one of the popular topics for network optimization in fifth-generation (5G) systems and beyond. The ability to forecast mobile traffic patterns is crucial for operators to design and plan their networks efficiently, perform resource allocation, or mitigate anomalies (e.g., security threats), thereby enhancing the network's quality of experience (QoE), resilience, and efficiency [1].\nIn recent years, machine learning (ML) methods have become increasingly used to tackle traffic prediction in mobile networks [2], offering an attractive balance between the accuracy of the generated predictions and the timescale on which these are delivered when compared to alternative approaches such as analytical models or network simulators [3]. More specifically, deep learning (DL) has received significant attention due to its ability to capture the dynamics of network traffic both in terms of spatial and temporal dependencies across sites [4], [5]. DL models leverage network data measurements to predict future performance and utilization.\nWhile DL can provide highly accurate predictions for network traffic forecasting [6], its use is often compute-intensive, particularly when it requires substantial training datasets to achieve the desired accuracy [7]. Indeed, DL relies upon deep architectures of artificial neural networks (ANNs) consisting of numerous neurons and layers, necessitating vast amounts of data to optimize the numerous parameters they encompass and deliver precise outcomes. This raises serious concerns regarding energy efficiency and environmental impact since, despite traffic forecasting can contribute to reducing energy consumption in mobile networks [8], [9], the energy consumption needs of its corresponding DL forecasters remain largely unexplored and is anticipated to be significant.\nTo fully and cost-effectively leverage the potential of DL for mobile traffic prediction, we envision a scenario where multiple network sites collaboratively train an ML model on the edge, thus fitting in a multi-access edge computing (MEC) where network-related computations are made at the network edge [10]. In particular, we study the application of federated learning (FL) [11], [12] to traffic prediction. In a nutshell, FL is a distributed learning optimization framework that allows training ML models via the exchange of ML model updates rather than raw training data. The FL approach does not only reduce the overheads associated with data sharing in traditional centralized ML approaches but also safeguards data owners' privacy. Furthermore, the privacy properties of FL allow the collaboration of multiple parties (e.g., network operators) to exchange insights on their proprietary datasets (typically kept private due to confidentiality constraints), in order to build even more robust and accurate collaborative models.\nThis article builds upon the work presented in [13], which gathered the efforts from the winning team of the \"Federated traffic prediction for 5G and beyond\" problem statement in the ITU-T AI for 5G Challenge, and further delves into the application of FL for traffic forecasting in mobile networks. By using real measurements from the Long Term Evolution (LTE)'s Physical Downlink Control Channel (PDCCH) data, collected during multiple measurement campaigns at five base stations (BSs) across diverse and representative areas of Barcelona (Spain) [14], we highlight the suitability of FL models in terms of accuracy and energy efficiency, as well as on considerations for adoption. The specific contributions of this article are as follows:\n\u2022 We introduce a comprehensive FL-based deep learning"}, {"title": "II. RELATED WORK", "content": "The time series forecasting problem has been traditionally addressed through statistical-based models. In this regard, autoregressive (AR) and moving average (MA) methods like autoregressive integrated moving average (ARIMA) contributed to building the basics of time series prediction due to their ability to provide good short-term predictions [15]. However, these methods fail at capturing seasonality and burstiness, thus lacking applicability to more complex time series forecasting problems (e.g., non-linear time series).\nThe complexity of non-linear time series problems can be properly addressed by sophisticated estimation methods like ANNs [16]. Recurrent neural networks (RNNs) are a special type of ANN that is specifically designed for time series [17], thus making them useful in problems where sequential patterns are to be learned (e.g., speech recognition or video processing tasks [18], [19]). However, RNNs fail at capturing seasonality, thus lacking applicability in mobile traffic forecasting. In particular, RNNs suffer gradient vanishing, as they are designed to capture infinite temporal relations. Nevertheless, a special type of RNN, namely long short-term memory (LSTM), has been widely used in communications, and more particularly, for mobile traffic forecasting [20]\u2013[22]. LSTMs leverage forget gates to avoid learning long-term trends in time series, thus allowing them to adjust the importance of old measurements onto future predictions.\nAnother suitable neural network model for time series prediction is convolutional neural networks (CNNs), which has had tremendous success in image processing applications [23]. CNNs come into play as the complexity of time series problems increases, including multi-variate problems or problems with multiple complex correlations between multiple types of features (e.g., spatial correlations, external phenomena, etc.).\nA CNN-based approach was introduced in [24] for traffic prediction, which leveraged the topological structure of the network to generate accurate predictions.\nMore recently, complex DL models like ResNet [25] or VGG [26] were applied to problems in communications. For instance, in [27], CNN, ResNet, and VGG were evaluated for multi-time series prediction in the Internet of things (IoT). However, while complex DL models provide outstanding results in fulfilling the time series prediction task, it comes at the expense of a very high computational cost. This was also confirmed in [28], where transformer architectures were evaluated in the context of federated traffic prediction, concluding that small accuracy improvements compared to simpler models such as LSTM or CNNs were achieved at a high cost in terms of energy consumption."}, {"title": "B. FL for Time Series Forecasting", "content": "While DL-based time series forecasting is a consolidated topic nowadays, distributed learning approaches for mobile traffic prediction remain highly unexplored. In this regard, FL was first applied in [29], [30] for a similar problem, namely road traffic flow prediction. Different from centralized approaches (see, e.g., [14]), where a single model is generated for each location, the federated models are trained collaboratively, thus targeting robust collaborative models. The FL approach has been shown to leverage data from multiple users while preserving privacy, leading to similar predictive accuracy to centralized methods.\nCloser in spirit to our work, we find [31], [32]. First, baseline models such as support vector regression (SVR) and LSTM were trained federatively in [31]. To boost training efficiency, the authors proposed a clustering method whereby the contributions of the most significant BSs were accounted for. An extension of this work was proposed in [32], where distances between BSs were considered to model weights in the federated setting. Moreover, to increase accuracy at specific sites, personalized fine-tuning was proposed to be run on each BS (using local data) after training the global model. Both models in [31], [32] were trained and validated in a dataset taken by the mobile operator Telecom Italia in the areas of Milan and Trentino, between 2013 and 2014 [33]. The dataset contains two months of data from SMS, voice calls, and Internet services records at up to 16.575 cells. Although the dataset in [33] has enabled the proliferation of traffic prediction models for communications in recent years, it is nowadays outdated since it does not capture novel use cases from current cellular networks. For that reason, in this work, we use a newer dataset that captures more recent user patterns in utilizing the network, such as massive multimedia services.\nIn our analysis, we focus on two FL issues that remain relatively unexplored in the field of cellular traffic forecasting, i.e., contribution of individual BS to the global model and combining exogenous data sources to improve accuracy and decrease energy costs.\nIn FL, handling heterogeneous clients is crucial for enhancing model performance and efficiency. The unique challenges presented by FL, including client data heterogeneity and variable computational capabilities, necessitate novel approaches for distributed optimization and privacy preservation."}, {"title": "III. PROBLEM FORMULATION AND METHODOLOGY", "content": "We consider a multi-step time series forecasting problem on a cellular network with K BSs connected to a common edge or cloud server. At every timestep t, each BS k obtains a measurement vector $x^{(k)}_t \\in R^d$. Then, using previous samples ${X^{(k)}_{t-T},..., x^{(k)}_{t-1}, x^{(k)}_t}$, each BS k predicts target measurement vector $Y^{(k)}_{t+1:T+1} \\in R^{T \\times d'}$, where $d' \\leq d$ denotes the set of features to be predicted and T the number of steps predicted in the future.\nWe utilize a common neural network model f(\u00b7) to make prediction, i.e., $y^{(k)}_t = f(x^{(k)}_{t-T},...,x^{(k)}_{t-1}, x^{(k)}_t)$, which is trained towards minimizing the mean squared error (MSE) of the prediction:\n1) A subset of clients K' \u2286 K is selected to participate in the current FL round, t. In the first steps, the selected clients download the global model $w_t$ from the parameter server. The method for selecting clients is further described in Section V-C.\n2) The subset of clients K' use the latest global model and their local datasets $D^{(k)}$ to update the model weights as\n3) The parameter server pulls the model updates computed by the selected subset of clients K'.\n4) The server aggregates the received client models by following an aggregation strategy such as federated averaging (FedAvg) [11]. The model aggregation procedure is further described in Section V-C.\nModel aggregation is a key component of FL, as it affects the convergence of the learning process and the overall predictive accuracy. Being $c^{(k)}$ the contribution of client k (e.g., the proportional size of its local dataset compared to the entire distributed dataset), the global model is obtained as"}, {"title": "B. Performance Evaluation Metrics", "content": "1) Prediction performance: We use the mean absolute error (MAE) and the normalized root mean squared error (NRMSE) to measure the prediction error of the studied models.\nThe MAE of a particular client k is given by\nLikewise, the NRMSE of client k is defined as follows:"}, {"title": "C. Proposed LSTM-based Solution", "content": "In this section, we describe our LSTM based multi-output and multi-step time series forecasting network f(\u00b7). The selection of the LSTM model is based on the comprehensive analysis conducted by Perfanis et al. [13], which highlighted the model's high predictive accuracy and robustness in similar contexts against other well-known models such as RNN, CNN, GRU, or Transformer. LSTM models offer a more balanced approach, providing sufficient accuracy while minimizing communication overhead, which is crucial for maintaining energy efficiency in sustainable telecommunication networks.\nAt kth base-station, our aim is to predict $y^{(k)}_t \\in R^{d'}$ using previous samples and the prediction network, i.e., $y^{(k)}_t = f(x^{(k)}_{t-T}, ..., x^{(k)}_t)$. We consider deep networks where M LSTM layers, followed by a feed-forward neural network with L layers. We define mth layer of the LSTM that uses the formulation in [40] as:\nLastly, we project intermediate representation into predictions using a linear layer with weight $W_p$ as:"}, {"title": "IV. DATASET DESCRIPTION AND ANALYSIS", "content": "The dataset used for this case study was gathered from five different locations in Barcelona (Spain), thus aiming to provide unique network utilization patterns, including daily living and especial events. The set of studied locations are the following:\nLes Corts\nCamp Nou (LCCN): A residential area nearby Camp Nou (Football Club Barcelona's stadium), which regularly hosts soccer matches and other special events. Measurements at this location comprise 12"}, {"title": "B. Dataset Analysis", "content": "Our analysis begins with a detailed examination of the data distributions in each BS. For that, we measure the differences in feature distributions among the BSs. For that, we first convert each feature from the time series of each BS into a histogram. Then, we compute the Kullback-Leibler (KL) divergence to quantify the similarity in feature distributions between pairs of BSs:\nFigure 2 depicts the average KL divergence between pairs of BSs, considering all features. Note that KL is not symmetric, i.e., $D_{KL}(P || Q) \\neq D_{KL}(Q || P)$. We focus on pairings exhibiting high KL values, specifically between EB-LCCN, EB-E1, PS-LCCN, and PS-E1. These high values are relevant to the FL operation, as they could indicate substantial disagreement among clients. In the sequel, we further evidence how high KL values lead to high discrepancies in terms of predictive accuracy, emphasizing the necessity to address non independent and identically distributed (non-iid) data distributions within federated settings.\nDifferences in data distribution across BSs can be linked to specific events within the spatio-temporal context of the data collection process. To provide further insights into the nature of the data at the different BSs, we have identified real-world incidents that align with the data collection periods near the monitoring BSs. Recognizing these distinct patterns/anomalies, associated with real-world occurrences, highlights the effectiveness of our data collection methodology. Figure 3 illustrates the fluctuations in the RNTI count variable due to these events at three base stations. Starting with the left-most plot (at LCCN), there are three notorious peaks in the network utilization, which correspond to three football matches: (1) FC Barcelona vs SD Eibar (2019-01-13), with 71,039 spectators, (2) FC Barcelona vs Levante UD (2019-01-17), with 42,838 spectators, and (3) FC Barcelona vs CD Legan\u00e9s (2019-01-2), with 50,670 spectators. When it comes to EB (middle plot), the Good Friday led to possible crowds in the region due to the Barcelona Cathedral service (4). Finally, the right-most plot shows that the BS in S1 was heavily used between 2021-08-24 and 2021-08-29 (5), which matches with the local festivities (Festa Major de Sants) that attract many visitors due to different types of activities."}, {"title": "V. PERFORMANCE EVALUATION", "content": "In this section, we study the application of FL to traffic prediction by evaluating a set of models and techniques using real LTE data measurements. The experiments aim to provide insights on different aspects of the downstream ML task and at the same time explore the advantages of FL over centralized solutions, including both predictive performance and environmental impact. To such end, we ran the complete set of experiments in a local environment composed of a Windows 11 workstation equipped with an AMD Ryzen 5 4600H CPU and 16 GB memory. This setup resembles real-world scenarios where BS edge servers have moderate computation capabilities and cannot devote all of their power to ML tasks since they already need to run the network. To ensure the validity of the results, 10 different seeds were used for the initialization of random generators. The ML models were implemented in PyTorch and the measurements on energy consumption and CO2 emissions were obtained using CarbonTracker [42], which is a library that allows measuring the power consumed by GPU, CPU, and DRAM devices when running ML model training and inference. Table II gathers the evaluation parameters and the selected hyperparameters for model training."}, {"title": "A. Cooperate, Not to Cooperate, or Centralize?", "content": "We start our analysis by comparing the predictive performance under three different learning settings: Individual, Centralized, and Federated. The Individual setting entails that each BS trains an independent model using only its locally available data. In contrast, in the Centralized setting, a central server uses the data from all the subscribed BSs to train a single model. Ultimately, the Federated setting allows the BSs to train a single collaborative model without exchanging their local datasets. The experiments were conducted based on an equal number of dataset accesses by the algorithm to ensure a fair and balanced comparison among different settings. While the number of accesses is equal to the number of training epochs in the individual and centralized settings, in the federated one, it is obtained as the product of the local training epochs by the number of federated rounds (R \u00d7 E). Results from our experiments, are presented in Table III, which shows the average and standard deviation of the test accuracy achieved by the different approaches in each BS. Apart from that, the average performance and the total energy consumption (Wh for training) are provided.\nIn the individual setting, the average error shows a significant variance across BSs, with the lowest and highest values observed in E2 (1.02) and S1 (2.924), respectively. This setting reflects moderate prediction accuracy with considerable fluctuations in the performance across different BSs. Notably, under this setting, the lowest energy consumption at 13.032 Wh is recorded, mainly due to the lack of communication among BS. Under the centralized setting, some BSs exhibit improved prediction accuracy (e.g., EB and LCCN) while others perform worse than in the individual setting, lacking generalization ability. Yet, the centralized provides a better average performance than the individual, indicating that pooling resources and data at a central point enhances overall prediction accuracy. However, this setting leads to the highest energy consumption (19% higher than the individual setting), which can be attributed to the centralized processing demands of larger amount of data, indicating that, while prediction accuracy is enhanced, it comes at the cost of increased energy consumption. Finally, the federated setting stands out by combining the advantages of both aforementioned settings. It achieves the lowest average error of 1.385 through mixed but generally improved results across BSs. The energy consumption in this setting is 14 Wh, which is around 8% higher than the individual setting but 10% lower than the centralized one.\nTo further analyze the predictive performance of the different approaches, we show the performance achieved at an increasing number of future time steps in Fig. 4, which includes the error at T \u2208 [1, 10]. As shown, the general trend is that the error increases as the prediction targets a more distant point in time. In particular, as the forecast horizon expands, the influence of immediate past traffic conditions on future states diminishes, making the prediction task more susceptible to unforeseen fluctuations in network usage and external factors affecting user behavior. It is worth highlighting the differences between learning settings. As it can be observed, centralized learning demonstrates an interesting pattern. In the beginning, it starts with a higher MAE compared to FL. However, as the prediction steps increase the centralized MAE increases at a reduced rate, until step 8 when it achieves a lower error than FL. This is possibly due to the larger amount of data that is available for training in the centralized learning, which"}, {"title": "B. Federated Learning Fine-Tuning", "content": "We next investigate the further fine-tuning of the FL setup for enhancing its overall predictive performance. For that, we focus on outliers handling and model aggregation.\n1) Outliers Handling: A key aspect of data preprocessing is outlier detection and their removal to improve the effectiveness of the model training. To comprehensively compare different outlier detection approaches, in Fig. 5, we employ during LSTM training and compare flooring/capping (F/C), isolation forest (Forest) [43], interquartile range (IQR) [44], support vector machine (SVM) [45] and Z-score. In the context of our study which focuses on time series forecasting, it is not feasible to directly remove outliers from the dataset. This is because an elimation will break the integrity of the temporal sequence, i.e., gaps will be created, which subsequently mislead the analysis and the forecasting model's understanding of historical patterns. To preserve the sequence, following outlier detection, we employ the flooring and capping technique. Specifically, we apply flooring to outliers below the minimum threshold and capping to outliers above the maximum threshold. It is important to note that this outlier correction is applied only to the training dataset. The reason for this is to prevent the model from being influenced by extreme values during training while still allowing the model to evaluate on the true test data distribution, including any potential outliers. The results reveal that the isolation forest technique outperforms the rest of the approaches in terms of improving forecasting accuracy. Contrary to other outlier detection methods that focus on profiling normal data points, isolation forest leverages the assumption that outliers are inherently few and exhibit distinct characteristics from the majority. This approach facilitates a more effective identification of anomalies, particularly in cellular traffic data, where outliers result from unforeseen events or data collection inaccuracies can significantly deviate from typical traffic patterns. By correctly identifying and handling these anomalies during training, isolation forest helps the model better understand the underlying patterns in the data, leading to improved forecasting accuracy. Interestingly, methods such as flooring/capping, IQR, SVM, and Z-score perform worse than the baseline, where no outlier detection and mitigation are applied. This underperformance is primarily due to the nature of time series data. Traditional outlier detection methods [46] are designed for static datasets and often struggle with time series because they do not consider the temporal dependency between data points. These methods may either fail to identify context-dependent anomalies or incorrectly flag normal variations in the time series as outliers. This can distort the data distribution, leading to models that are poorly calibrated and ultimately less accurate."}, {"title": "2) Model Aggregation:", "content": "Model aggregation is a key component of every FL system. Federated averaging (FedAvg) [11] is the most widely used aggregator in FL applications due to its simplicity and robustness. Despite being simple enough and performing well in general, FedAvg [11] might cause objective inconsistency in certain scenarios. This can happen due to non-iid and heterogeneous data. To mitigate this risk, alternative methods have emerged to better accommodate data diversity. Wang et al. [47] introduced FedNova, which adapts the aggregation process by normalizing local model updates. This adjustment involves dividing local gradients by the number of client steps before their aggregation, rather than just directly averaging their local gradients. FedAvgM, proposed by Tzu et al. [48], enhances FedAvg by incorporating server momentum, which involves applying a hyper-parameter $\u03b2$ to previous model updates during each training epoch before adding new updates, thereby refining the aggregation process. In addition, there have been developments in federated versions of adaptive optimizers, such as FedAdagrad, FedYogi, and FedAdam, as discussed in [49]. These techniques are designed to handle heterogeneous data, enhance model performance, and reduce communication overhead.\nIn our experiments, we evaluated nine aggregation functions to address the challenges posed by quantity, quality, and temporal skew in our dataset. Our analysis, illustrated in Fig. 6, underscores the superior performance of FedAdam over its counterparts, but with FedAvg coming second, offering similar performance and with lower deviation across experiments. This outcome suggests that, while specialized algorithms like FedAdam are designed to mitigate the effects of non-iid data distributions through adaptive normalization, their optimal performance hinges on precise hyperparameter tuning tailored to the specific dataset characteristics. Conversely, the relatively simple FedAvg algorithm, despite its straightforward approach, demonstrates robustness and effectiveness, indicating its potential applicability across a broader range of FL scenarios without extensive customization. Since we did not observe significant differences, we utilized the FedAvg algorithm for model aggregation for generality and simplicity."}, {"title": "C. Impact of Individual Contributions in Client Selection", "content": "We now provide insights on the potential of FL client selection by analyzing the contributions of each client to the predictive performance of the federated model. For that, Table V shows the test error obtained at the different BSs for various settings where different subsets of BSs are available to be selected by the FL central server (ranging from 1 to 7 clients selected randomly per round). The results are compared to the cases where a single BS is available to be selected, which resembles to the individual training setting studied above.\nThe results reveal that the selection and number of clients participating in each FL round leads to a significant variance in the global model's prediction error on the test sets of the different clients. In general, it can be observed that, as the number of clients participating in each round decreases (from 7/7 to 1/7), the error across all the BSs increases, indicating that having more clients participating in each round tends to improve prediction accuracy. However, it is worth highlighting that the ML model has an adequate performance even with 6/7 or 5/7 participating clients, with only a small degradation of 4-5% compared to the scenario that considers all the BSs for training. These results advocate the use of a client selection mechanism that could lead to an adequate level of performance by utilizing fewer clients.\nTo better understand the contribution of each BS to the global model, we conducted additional experiments using a deletion-based scheme, where one BS was excluded from the FL training process at a time. The results are also presented in Table V, indicate that while the overall average error may not be significantly impacted, the exclusion of certain BSs can lead to notable performance degradation on specific test sets. For example, the exclusion of E2 resulted in a greater increase in error on S1 compared to the exclusion of S1 itself. This phenomenon can be explained by the low KL-Divergence between the datasets of E2 and S1, as illustrated in Figure 2, suggesting a close similarity in data distribution between these clients.\nThese observations underscore the potential benefits of a strategic client selection mechanism that favors clients with contributions that significantly enhance the global model's generalization ability. This can be particularly beneficial if our aim is to minimize energy costs associated with the training of ML models, especially in a Traffic Forecasting scenario that might scale fast in a real-world application to incorporate thousands of BS.\nHowever, the endeavor to optimize client selection for FL not only aims to bolster predictive performance but also seeks to expedite convergence and minimize the environmental footprint of the training process. The challenge thus extends beyond recognizing the contribution of individual clients to the model's accuracy, encompassing the need to efficiently orchestrate the training process. FL client selection mechanisms targeting high-performing nodes are key to boosting overall performance."}, {"title": "D. Personalized Federated Learning", "content": "Aiming to investigate personalization aspects in FL [50], we employ the technique of local fine-tuning (LF) to enhance the accuracy of the local models on the respective validation sets. LF involves an additional step to federated model training, where each participating client performs a complementary training round to fit the global model on their local data before deploying it for inference. The rationale behind LF is to allow the global model, which results from the aggregation of diverse local models, to better adapt to the specific characteristics and distribution of each client's local dataset. In the studied use case, the customization provided by LF ensures that the models used are better attuned to the patterns unique to each BS.\nThe results presented in Table VI illustrate the efficacy of LF in reducing MAE across different BSs, for both centralized and federated settings. As shown, in the centralized and federated settings, LF achieves 7.8% and 10.9% average improvements, respectively. Such gains reveal the ability of LF to leverage the unique data characteristics of each client, which is a particularly compelling property when dealing with complex non-iid data. In conclusion, LF ensures that the model is not only generalized to perform well across the entire network but also optimized for specific local conditions."}, {"title": "E. Combining Network and Exogenous Data Sources", "content": "To conclude the experimental part, we focus on the potential enhancement of our framework's efficacy through the integration of exogenous sources of data into our network data predictors. The main motivation for that is that cellular traffic is heavily affected by exogenous variables, including public holidays, weekends, and specific hours of the day, to mention a few. To capture and leverage the impact of such phenomena in network utilization, we utilized the Upgini Python Library [51], which facilitates the search of public datasets to be used in conjunction with our network dataset. The Upgini uses an advanced search mechanism that utilizes consecutive experiments with extra ready-to-use ML features from external data sources, such as historical weather data, holidays and events, world economic indicators and markets data, customized for specific locations and dates. Since Upgini enrichment included a significant amount of extra features that generally could not enhance overall performance, a feature selection technique was applied. To find relevant features to enrich our dataset, we measured the importance of various Upgini features by training a gradient-boosting ML model (i.e., Catboost) that includes those features and observing the variation of the model's accuracy in response to feature value changes. The process aimed to find out those Upgini-generated features that had similar or even higher importance than the original ones. The resulting top-14 features are shown in descending order in Fig. 7, where the new Upgini features that showed similar or higher importance than the original are highlighted in green. As we can observe, these exogenous features are related to special events (f_events_date_year_sin and f_economic_date_cpi) and date time (datetime_time_sin) and exhibit significant importance within the studied model.\nAfter identifying the features to be added to our model, we now evaluate the effect of using extra information by training from scratch the LSTM model with the 14 most important features shown in Fig. 7 under federated settings. The resulting MAE at each BS (with and without Upgini's enhanced dataset) is presented in Fig. 8. As shown, combining exogenous sources of data can have mixed effects on the predictive performance across BS. Specifically, a decrease in MAE is observed in four out of seven BSs, while the remaining three BSs perform worse when extra features are integrated. On average, the Upgini-enriched dataset leads to 6.55% lower prediction error, thus offering a promising approach for the task of federated cellular traffic prediction. However, it is important to emphasize the limited generalizability of the introduced features across all datasets. As demonstrated, there was no performance enhancement observed in all BSs, particularly in those experiencing anomalous events (as detailed in Section IV). This observation suggests that feature selection must be conducted carefully, preferably with expert guidance, to improve the efficacy of machine learning models."}, {"title": "VI. CONCLUSION & FUTURE WORK", "content": "This study delves into the rapidly evolving field of mobile traffic forecasting and the integration of FL within the telecommunication industry, with a special focus on the realms of 5G and the upcoming 6G networks. It sheds light on several research directions and identifies key challenges that, once overcome, could substantially improve network efficiency, enhance resource management, and ensure data privacy. The experimental analysis provides useful insights on crucial challenges for the incorporation of FL in mobile traffic forecasting, such as the impact of individual clients, personalization, and integration of data from exogenous data sources. Further, aspects related to FL fine-tuning, including model aggregation and outlier handling, have been evaluated. The presented results demonstrate FL's capability not only in tackling the challenges associated with ML tasks but also in ensuring data privacy and fostering cooperation among network operators sustainably without compromising prediction quality. Overall, the analysis provides a roadmap for advancing mobile traffic forecasting using FL as a privacy-friendly and sustainable solution. By addressing these challenges, the telecommunications industry can expect networks that are more resilient and efficient, paving the way for the next generation of mobile networks.\nHowever, there are still open problems that need to be addressed in this evolving field. First, explainability in mobile traffic prediction models is becoming increasingly important, especially for network operators and regulators who need to understand the basis of predictions to make well-informed decisions (e.g., related to energy saving). In this respect, techniques such as smooth-graph and layer-wise relevance propagation could provide insight into model decision-making processes, enhancing transparency and trust in predictive models. Second, mobile traffic forecasting can be enhanced by leveraging complementary data sources. A case in point is tempo-spatial correlations, whose comprehension and modeling hold vast potential, especially in densely populated urban areas where user mobility patterns and network usage can vary significantly across short distances. Third, delivering lightweight FL solutions is essential for their viability, and this can be achieved through alternative forms of FL, such as federated distillation and hierarchical FL, and ML model optimization techniques such as model pruning. Finally, future work could explore the integration of additional factors into the sustainability indicator, such as convergence time and robustness, to provide a more holistic evaluation of learning methods regarding their performance and associated costs."}]}