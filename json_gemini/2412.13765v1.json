{"title": "LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for E-Learning Platforms", "authors": ["Ali Hamdi", "Ahmed Abdelmoneim Mazrou", "Mohamed Shaltout"], "abstract": "Current methods for analyzing student engagement in e-learning platforms, including automated systems, often struggle with challenges such as handling fuzzy sentiment in text comments and relying on limited metadata. Traditional approaches, such as surveys and questionnaires, also face issues like small sample sizes and scalability. In this paper, we introduce LLM-SEM (Language Model-Based Student Engagement Metric), a novel approach that leverages video metadata and sentiment analysis of student comments to measure engagement. By utilizing recent Large Language Models (LLMs), we generate high-quality sentiment predictions to mitigate text fuzziness and normalize key features such as views and likes. Our holistic method combines comprehensive metadata with sentiment polarity scores to gauge engagement at both the course and lesson levels. Extensive experiments were conducted to evaluate various LLM models, demonstrating the effectiveness of LLM-SEM in providing a scalable and accurate measure of student engagement. We fine-tuned LLMs, including AraBERT, TXLM-ROBERTA, LLama 3B and Gemma 9B from Ollama, using human-annotated sentiment datasets to enhance prediction accuracy.", "sections": [{"title": "1 Introduction", "content": "The proliferation of e-learning platforms has transformed the way students engage with educational content [22]. Platforms such as YouTube host numerous educational channels that cater to diverse learning needs [19]. However, assessing student engagement with these resources remains a significant challenge for educators and content creators [14]. Traditional methods, such as surveys and questionnaires, often suffer from small sample sizes, lack of scalability, and difficulty in capturing the opinions of a broader audience [15]. Automated methods, while an improvement, face their own challenges, including dealing with fuzzy sentiment in text comments and relying on limited metadata, which hampers their ability to provide a comprehensive assessment of engagement [23]."}, {"title": "2 Related Work", "content": "Student engagement and feedback are crucial indicators of the effectiveness of educational content [20]. Traditionally, course evaluation methods have relied on qualitative approaches, such as surveys and questionnaires, to gather feedback on student engagement and satisfaction [18]. These methods, while valuable, face limitations in scalability, representativeness, and timeliness. With the growing volume of online educational content, there has been a shift towards automated and data-driven methods for assessing student engagement, including sentiment analysis and the use of metadata [6].\nSentiment analysis has been widely applied across various domains to gauge public opinion, including politics, marketing, and consumer reviews [11,5,2,3,13,12]. In the context of education, sentiment analysis has emerged as a promising tool for evaluating student satisfaction and engagement. Previous"}, {"title": "3 Methodology", "content": "Our methodology involves several stages, each representing a distinct step in the process of calculating the Student Engagement Metric (SEM). These stages include data collection, metadata extraction, sentiment analysis, polarity scoring, feature space normalization, and finally, the calculation of a comprehensive engagement metric by integrating both sentiment and metadata. A high-level workflow is shown in Fig. 1, which demonstrates how course lessons, metadata, and comments are processed to compute the final SEM."}, {"title": "3.1 Data Collection and Manipulation", "content": "Our data collection process focuses on two levels of granularity: course and lesson. Each course is represented by a playlist, and each lesson corresponds to individual videos within that playlist. We scraped data from each channel, organizing it into three main categories: playlists, videos, and comments. The structure of these categories and their relationships are illustrated in Fig. 2.\nThe collected data is organized into two primary types:\nMetadata: This includes video-specific details such as the number of views, likes, video length, and publication date. Metadata features play a crucial role in measuring the quantitative aspects of engagement. Key metadata extracted for each video includes:\n\u2022 Views: The total number of views each video has received.\n\u2022 Likes: The number of likes on each video, which can indicate user appreciation of the content.\nTextual Data (Comments): This includes user comments associated with each video, providing a rich source of qualitative insights into how students"}, {"title": "3.2 LLM for Sentiment Analysis", "content": "To generate sentiment scores from user comments, we employed several state-of-the-art Large Language Models (LLMs). These models help to parse and interpret the sentiment behind each comment, classifying it as positive, negative, or neutral. The following LLMs were tested in our experiments:\nTwitter-XLM-ROBERTa-Base for Sentiment Analysis: A multilingual model fine-tuned on sentiment analysis for tweets across various languages [4].\nLLama 3B from Ollama: A 3-billion parameter model designed for multi-lingual tasks, capable of performing sentiment analysis with high accuracy.\nGemma 9B from Ollama: A larger 9-billion parameter multilingual model that offers improved performance in sentiment classification tasks.\nHuman-Annotated Sentiment Dataset: For fine-tuning and evaluation, we used a human-annotated sentiment dataset sourced from the [Arabic Sentiment Corpora GitHub Repository](https://github.com/ali7amdi/Arabic-Sentiment-Corpora). This dataset contains a diverse set of text samples with manually labeled sentiment categories (positive, negative, and neutral), making it suitable"}, {"title": "3.3 Sentiment Analysis", "content": "Once the comments are processed by the LLM, each comment is assigned a sentiment label (positive, negative, or neutral) along with a confidence score, which ranges from 1 to 1. These scores are then used to compute a polarity score for each lesson in each course. This stage is crucial for capturing the qualitative aspect of student engagement, as sentiment provides insight into how users feel about the course or lesson content."}, {"title": "3.4 Multi-Level Polarity Scoring (f)", "content": "For each video v, we compute a polarity score $P_v$ based on the sentiment of user comments. This score ranges from -1 (entirely negative) to 1 (entirely positive), with 0 indicating neutral sentiment. The polarity score is calculated by assigning weighted sentiment scores to each comment based on the model's confidence in the sentiment classification.\nFor a comment c with sentiment $s_c$ and confidence score $score_c$, the weighted score $w_c$ is determined as:\n$w_c = \\begin{cases} score_c, & \\text{if } s_c = \\text{positive} \\\\ -score_c, & \\text{if } s_c = \\text{negative} \\\\ 0, & \\text{if } s_c = \\text{neutral} \\end{cases}$\nThe polarity score $P_v$ for video v is then calculated by averaging the weighted scores of all comments:\n$P_v = \\frac{\\Sigma_{c \\in C_v} w_c}{N_v}$\nwhere C is the set of comments on video v, and $N_v$ is the total number of comments. By dividing the sum of weighted scores by the total number of comments, we normalize the polarity score to ensure it remains within the range of -1 to 1, regardless of the number of comments per video. This normalization allows for fair comparison between videos with different levels of engagement, ensuring that videos with fewer comments are not unfairly penalized or advantaged when calculating their engagement score.\nBeyond individual videos, we extend our analysis to compute a playlist polarity score $P_p$ for each playlist p. This score represents the average sentiment across"}, {"title": "3.5 Feature Space Normalization", "content": "To ensure that metadata (such as views and likes) and sentiment polarity scores are comparable across videos with varying levels of engagement, we apply min-max normalization to these features. The normalized values are scaled between 0 and 1, ensuring that all features contribute proportionally to the final engagement metric.\nLet:\n$V_v$ be the number of views for video v,\n$L_v$ be the number of likes for video v,\n$V_{min}$ and $V_{max}$ represent the minimum and maximum views across all videos,\n$L_{min}$ and $L_{max}$ represent the minimum and maximum likes across all videos,\n$P_v$ be the sentiment polarity score for video v.\nThe normalized views for a video v, denoted as $NV_v$, are calculated as:\n$NV_v = \\frac{V_v - V_{min}}{V_{max} - V_{min}}$\nSimilarly, the normalized likes for a video v, denoted as $NL_v$ are calculated as:\n$NL_v = \\frac{L_v - L_{min}}{L_{max} - L_{min}}$"}, {"title": "3.6 LLM-SEM: A Novel Engagement Metric", "content": "Our proposed engagement metric, LLM-SEM (Language Model-Based Student Engagement Metric), integrates the normalized video metadata (views and likes) with the sentiment polarity scores to compute an overall engagement score, denoted as $E_v$, for each video. This metric provides a comprehensive measure of student engagement by combining both qualitative (sentiment) and quantitative (views and likes) data.\nThe overall engagement score is calculated by summing the normalized views ($NV_v$), normalized likes ($NL_v$), and the sentiment polarity score ($P_v$) as:\n$E_v = NV_v + NL_v + P_v$\nThis formula combines metadata and sentiment data into a single score, enabling fair comparisons across videos and courses with varying levels of interaction.\nRange and Interpretation of the Engagement Metric $E_v$ Since each component is normalized, $E_v$ will fall within the range of -1 to 3:\nMaximum Value (3): Represents the highest engagement, achieved when views and likes are maximized (both equal to 1) and sentiment is entirely positive ($P_v = 1$).\nMinimum Value (-1): Reflects poor engagement, occurring when views and likes are at their lowest (both 0) and sentiment is entirely negative ($P_v = -1$).\nThis range helps distinguish high engagement from low engagement across videos and courses, allowing content creators and educators to identify areas for improvement.\nThresholds for Engagement Quality\nGood Engagement: Typically, an $E_v$ score above 1.5 can indicate strong engagement, as it implies positive sentiment with relatively high views and likes.\nModerate Engagement: A score between 0.5 and 1.5 suggests average engagement, with a balance of sentiment and interaction.\nPoor Engagement: Scores below 0.5 indicate low engagement, characterized by either low sentiment, minimal views and likes, or both.\nRationale for LLM-SEM as an Effective Metric The LLM-SEM metric is advantageous because it combines quantitative (views, likes) and qualitative (sentiment) data, offering a comprehensive view of student engagement. By using normalized values and sentiment polarity, this metric enables fair comparison across varying levels of interaction, making it a scalable and data-driven tool for evaluating engagement."}, {"title": "4 Results and Discussion", "content": "To validate the effectiveness of our proposed LLM-SEM framework, we conducted experiments using a dataset of 16,766 human-annotated sentiments. We evaluated three Large Language Models (LLMs) to determine which model provides the best performance for sentiment analysis, which would then be integrated into the LLM-SEM architecture.\nThe following LLMs were evaluated:\nLLama 3.2B\nGemma 9B\nROBERTa (fine-tuned)\nFor each model, we measured performance in terms of accuracy, recall, and F1-score across three sentiment categories: negative, neutral, and positive. The results are summarized in Table 1."}, {"title": "4.1 Model Evaluation Results", "content": "From Table 1, it is evident that the fine-tuned RoBERTa model achieved the highest overall accuracy and performance metrics, with an accuracy of 0.86 and an F1-score of 0.84. This suggests that fine-tuning significantly improved RoBERTa's ability to classify sentiment, particularly when compared to the other models.\nGemma 9B also performed well, with an accuracy of 0.81 and reasonable recall and F1-scores. It demonstrated strong performance in handling both negative and positive sentiment but faced challenges with neutral sentiment classification, similar to the other models.\nLLama 3.2B, while being effective in classifying positive sentiment, had the lowest overall accuracy (0.55) and struggled the most with neutral sentiment classification, which affected its overall performance."}, {"title": "4.2 Discussion", "content": "All models demonstrated challenges in accurately predicting neutral sentiment, likely due to the inherent difficulty in distinguishing neutral comments from both positive and negative ones, particularly in complex or context-dependent text"}, {"title": "5 Conclusion", "content": "In conclusion, this study introduced a quantitative approach to assessing student engagement on educational YouTube channels using publicly available data and sentiment analysis models. By analyzing data from multiple educational channels and applying multilingual sentiment analysis, we were able to calculate polarity scores for lessons and courses, offering a comprehensive view of student satisfaction. Our method addressed the shortcomings of traditional feedback mechanisms, providing a scalable and data-driven alternative for evaluating content performance. The integration of polarity scores with views and likes resulted in a well-rounded engagement metric, enabling educators and content creators to identify effective content and areas for improvement. The success of multilingual models in handling diverse languages highlights the robustness of this approach, offering actionable insights for optimizing educational content. This method sets the groundwork for further research in expanding engagement metrics and refining sentiment analysis models for more nuanced understanding in online education environments."}]}