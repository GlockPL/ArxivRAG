{"title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials", "authors": ["Yizhen Zheng", "Huan Yee Koh", "Maddie Yang", "Li Li", "Lauren T. May", "Geoffrey I. Webb", "Shirui Pan", "George Church"], "abstract": "The integration of Large Language Models (LLMs) into the drug discovery and development field marks a significant paradigm shift, offering novel methodologies for understanding disease mechanisms, facilitating drug discovery, and optimizing clinical trial processes. This review highlights the expanding role of LLMs in revolutionizing various stages of the drug development pipeline. We investigate how these advanced computational models can uncover target-disease linkage, interpret complex biomedical data, enhance drug molecule design, predict drug efficacy and safety profiles, and facilitate clinical trial processes. Our paper aims to provide a comprehensive overview for researchers and practitioners in computational biology, pharmacology, and AI4Science by offering insights into the potential transformative impact of LLMs on drug discovery and development.", "sections": [{"title": "1. Introduction", "content": "\"Language is only the instrument of science, and words are but the signs of ideas.\" Samuel Johnson\nThe pursuit of new drugs to research and develop is a long-term commitment that typically takes 10-15 years and costs over $2 billion in order to bring a new drug to a patient (Berdigaliyev & Aljofan, 2020). This complex procedure is traditionally divided into three stages: the first stage is to understand the disease and to choose the target of treatment; the second stage is to develop a focused approach to developing treatments towards the target; and the third stage is to test the treatments in clinical trials for their effectiveness. Each phase of the process is both time-consuming and resource-intensive, this is because of the complexity of biological systems and the extensive nature of the review required of each phase in the research and validation process. The slow and protracted nature of the process often prevents the introduction of new therapies that would improve and extend human life. Consequently, there are extraordinary dividends to be reaped by introducing efficiencies and expanding the capabilities of current practices.\nArtificial intelligence (AI) tools have emerged as preeminent innovation in the quest to accelerate drug discovery and development. Among these tools, large language models (LLMs) \u00b9 have distinguished themselves through their capabilities in understanding scientific language and executing various downstream tasks essential in drug discovery and development. Recent LLM breakthroughs, Geneformer (Theodoris et al., 2023), pretrained on 30 million single-cell transcriptomes, can help in disease modeling and successfully identified candidate therapeutic targets for cardiomyopathy via in silico deletion. Notable LLMs for facilitating chemistry experiments, Boiko et al. (2023) and Chemcrow (Bran et al., 2023), have highlighted the potential of LLMs in automating chemistry experiments related to drug discovery, specifically in the fields of directed synthesis and chemical reaction prediction. Other works, such as LLM4SD (Zheng et al., 2023), showed that LLMs can perform scientific synthesis, inference, and explanation directly from raw experimental data and formulate hypotheses that resonate with human experts' analysis. Med-PaLM (Singhal et al., 2023), a mega size LLM encoding clinical knowledge, was the first to reach human expert in USMLE-styled ques-"}, {"title": "2. Main Paradigms of Language Models", "content": "In drug discovery and development, the intricate text-based scientific languages used to describe chemicals and proteins, such as SMILES strings for encoding molecular structures (Weininger, 1988), and FASTA format for encoding protein, DNA and RNA sequences (fas, 1995), represent a unique form of structured language crafted by humans to encode domain-specific knowledge. To effectively interpret these languages, two main language model paradigms, including specialized language models (specialized LLMs) and general-purpose language models (general LLMs) emerged (Figure 2)."}, {"title": "2.1. Specialised Large Language Models", "content": "The first paradigm of language models in drug discovery and development is specialized LLMs trained in specific scientific languages. These LLMs aim to decode the statistical patterns of scientific language, thereby enabling the interpretation of scientific data in its raw form (Figure 2).\nUnderstanding Disease Mechanisms. Specialized LLMs can be used in many ways to explore diseases. For instance, LLMs can extract genomic information from single-cell RNA transcriptomic data and DNA sequences (Consens et al., 2023), enabling practitioners to determine epigenetic marks, transcription factor binding sites, functional genetic variants, and gene network analysis, all of which contribute to understanding the genetic basis of disease.\nAdditionally, protein LLMs, such as ESM (Rives et al., 2021), can be trained to predict parts of the amino acid sequence that have been intentionally hidden or \u2018masked' during training, such as \u201cMVL<MASK>PAD\u201d (Figure 2). Despite a simple training procedure, these specialized LLMs have been proven helpful in annotating the functions (Matic et al., 2022) and predicting the structures of proteins directly from protein sequences (Lin et al., 2023), significantly advancing our understanding of protein structures, and informing downstream drug discovery efforts.\nDrug Discovery. In drug discovery, specialized LLMs are particularly helpful for accelerating various chemistry experiments (Bran et al., 2023; Park et al., 2023). A specialized LLM trained in the molecular SMILES language can help in retrosynthetic planning and predicting reaction outcomes; at the same time, it can help chemists in de novo molecules guided by some specific molecular properties, such as increasing binding affinity towards targets. Moreover, these models can also play a role in ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) prediction, a critical step in assessing molecule properties and filtering out those with undesirable characteristics.\nUsage. Specialized LLMs are tool-like, where the user inputs information needed for a given task and receives a model prediction in return (Figure 2). For instance, when we use a specialized LLM to query protein-ligand binding affinity, both protein sequences and ligand SMILES strings must be provided to the model, which will subsequently output the predicted binding affinity score."}, {"title": "2.2. General-purpose Language Models", "content": "The second paradigm encompasses general LLMs, which are trained on a diverse array of textual information sourced from various materials, including but not limited to scientific papers, textbooks, and general literature. Such breadth in training allows them to achieve a broad understanding of human language, which includes a significant grasp of scientific contexts. Models like GPT-4 (OpenAI, 2023; AI4Science & Quantum, 2023) and Galactica (Taylor et al., 2022) have been noted for their proficiency in also mastering complex formal scientific description languages, including SMILES strings and FASTA format. Using this capacity, general LLMs can work on tasks that would typically require the participation of domain professionals, such as making inferences, doing reasoning and analysis, and applying field-specific knowledge across different scientific domains.\nUnderstanding Disease Mechanisms. General LLMs can traverse a large volume of literature, extract data, and summarize for users. Furthermore, it can also synthesize the extracted data into a knowledge graph, revealing how genes and diseases are connected, helping scientists uncover the basis behind diseases (Savage, 2023). Furthermore, these models can explain technical terminologies in layperson's language, making understanding complex concepts and principles easy, significantly aiding in education and communication.\nDrug Discovery. In drug discovery, general LLMs have great potential to accelerate experimental practices. Recently, general LLMs have been applied in chemistry robotics for automated experiments. General LLMs also exhibited expert-level capabilities in retrosynthetic planning and reaction prediction (Bran et al., 2023; Boiko et al., 2023), while only costing a fraction of human experts.\nSome preliminary attempts are underway to train general language models to perfect tasks currently more suited for specialized LLMs, such as de novo molecule and protein generation and editing (Liu et al., 2023c). The primary motivation is that, unlike specialized LLMs that can only learn data patterns from specific scientific languages, these LLMs can reason and apply the domain knowledge learned from extensive literature. However, research in this direction is still in its infancy.\nClinical Trials. General LLMs provide significant advantages in analyzing electronic health records and clinical"}, {"title": "3. LLMs in Drug Discovery and Development", "content": "This section discusses how LLMs can be applied in three drug discovery and development pipeline stages: understanding disease mechanisms, drug discovery, and clinical trials."}, {"title": "3.1. Understanding Disease Mechanisms", "content": "Understanding disease mechanisms is the initial and crucial stage of the drug discovery and development pipeline. The primary aim of this stage (Figure 3) is to identify a suitable protein target for a potential drug to act upon (Lindsay, 2003). This process involves three key steps: clinical sub-typing, target-disease linkage analysis, and target validation.\nClinical sub-typing in drug discovery involves categorizing patients into subgroups to collect clinical and multiomics data and aids in understanding disease variations and identifying potential differences in disease mechanisms across patient groups (Cort\u00e9s-Cros et al., 2013; Pun et al., 2023).\nThe target-disease linkage analysis phase in drug discovery involves establishing connections between potential protein targets and specific diseases. This phase encompasses pathway analysis to investigate biological pathways involved in the disease and expression profile analysis to study disease-related gene expression patterns (Plenge et al., 2013). Additionally, practitioners leverage experimental techniques to establish causal links between a target and the disease, including CRISPR-Cas9 (Lin et al., 2017), in-vivo disease modeling (Lindsay, 2003), and interference RNA (siRNA) (Cort\u00e9s-Cros et al., 2013).\nAfter identifying a target in the drug discovery process, target validation is a crucial, non-linear step that follows target identification, involving a continuous validation cycle with no fixed starting point (Figure 3). This cycle includes assessing the necessary actions to be performed on the target for disease treatment (mechanism of action), choosing the most appropriate therapeutic intervention (modality selection), and conducting a comprehensive safety and feasibility assessment (Emmerich et al., 2021). The safety and feasibility assessment evaluates both the potential organismal impact (safety) and the target's druggability (Floris et al., 2018), as well as the practicality of assays for feasibility (Vincent et al., 2015). This flexible, iterative approach ensures thorough evaluation of the target's viability and safety at any stage before advancing in drug development, ensuring the selected targets are both theoretically promising and practical for further development."}, {"title": "3.1.1. GENOMICS ANALYSIS", "content": "Decades of genome-wide association studies (GWAS) have identified critical genomic regions linked to various diseases (Michailidou et al., 2015; 2017; Nelson et al., 2017; Zengini et al., 2018) that have significantly advanced genomic-based analysis for disease understanding and target discovery. Notably, integrating genetic associations in drug discovery, could significantly improve the success rate of clinical targets (Nelson et al., 2015).\nRecently, there has been significant interest in adapting advancement in LLMs used for human languages to genomic analysis, such as DNA-BERT (Ji et al., 2021), due to the structural similarities between DNA and human language. Through specialized training on vast amounts of nucleotide sequences, these LLMs are adept at decoding the language of genetics. As a result, there has been an explosion in the field of specialized nucleotide LLMs (Ji et al., 2021) that are increasingly capable of understanding the cryptic \"language\" used by genomes more efficiently, enabling various downstream tasks in understanding genetic mechanisms of diseases.\nGenetic variant analysis. The application of nucleotide LLMs in genetic variant analysis hinged on the fact that genetic sequences follow specific language patterns and rules (Yanofsky et al., 1964; Altschuh et al., 1988). Variations in these sequences-be it single nucleotide polymorphisms (SNPs), insertions, deletions, or more complex rearrangements-can significantly impact gene function (Brendel & Busse, 1984; Searls, 2002). Hence, they employ masked language modeling when nucleotide LLMs are trained on extensive genomic data. In this approach, the model learns to predict parts of the nucleotide sequence that have been intentionally hidden or 'masked' during training. This learning process enables the LLMs to decode the intricate, often hidden patterns and rules that govern the language of genes (Ji et al., 2021; Dalla-Torre et al., 2023).\nPost-training, nucleotide LLMs have demonstrated the ability to detect significant functional genetic variants directly from DNA sequences. For example, DNA-BERT (Ji et al., 2021) showed that nucleotide LLMs selectively concentrate on the most relevant genomic regions. This enables the extraction of motif patterns that are evolutionarily conserved"}, {"title": "3.1.2. TRANSCRIPTOMICS ANALYSIS.", "content": "Transcriptomics, a field that investigates the entirety of RNA transcripts that an organism or cell system generates under certain conditions, has experienced a surge in the volume of transcriptomic data derived from a wide range of human tissues due to the development of high-throughput technologies and single-cell technologies. However, the data is often sparse for specific disease states, particularly for rare diseases and diseases affecting clinically inaccessible tissues (Shao et al., 2021), so relying solely on these data for specific diseases would likely not suffice to develop robust and accurate models. To address existing limitations, specialized gene LLMs have been proposed to obtain a comprehensive understanding of transcriptomic data while offering the goods to adapt to scenarios with sparse data samples.\nThe primary technological development in this sub-field is the specialized transcriptomic LLM, Geneformer (Theodoris et al., 2023), which developed an innovative method for mapping each single-cell transcriptome into a sequence of genes ranked by their expression levels. This approach, known as \u201crank value encoding\", represents the transcriptome of each cell as a sequence of genes ordered based on their expression levels. These levels are then normalized against the overall expression observed across all human tissues. Through this technique, rank value encoding offers a distinct representation of gene activity within individual cells and facilitates a comprehensive comparison of gene expression across a diverse array of data (Theodoris et al., 2023). This approach is akin to learning the language of transcriptomics through specialized LLMs, enhancing our understanding of cellular behaviors and interactions at the molecular level.\nBy transforming single-cell transcriptomic data into gene sequences, Geneformer (Theodoris et al., 2023), scGPT (Cui et al., 2023), and other models like scMulan (Bian et al., 2024) and scFoundation (Hao et al., 2024) have demonstrated a remarkable ability to analyze transcriptomic data effectively as foundational LLM models. Furthermore, specialized transcriptomic LLMs can be adapted to scenarios with sparse data through fine-tuning to model gene networks accurately to comprehend complex dynamics, including network interactions, extending beyond simple cell-level annotations (Ma et al., 2024).\nIn parallel, efforts have also been made to leverage biomedical literature for predicting future therapeutic targets by training LLMs on historical text corpora. A study used Word2Vec models on abstracts published between 1995 and 2022, allowing these LLMs to prioritize gene-disease associations and protein-protein interactions likely to be validated in future research (Narganes-Carl\u00f3n et al., 2023). This approach, termed Publication-Wide Association Study (PWAS), encodes biomedical knowledge as word embeddings without human supervision, effectively capturing drug discovery concepts and prioritizing hypotheses years before experimental confirmation. PWAS demonstrates the potential of LLMs as a scalable system for early-stage target ranking, enhancing the ability to mine literature for under-explored therapeutic opportunities.\nmRNA expression analysis. mRNA expression analysis can be challenging due to the need to derive meaningful insights from limited data scenarios, a vital aspect in enhancing our understanding of diseases. Traditional machine learning approaches, such as XGBoost (Chen & Guestrin, 2016) and standard deep neural networks, usually start from scratch for each specific task. This methodology can be ineffective, especially if the data is limited, which is often the case in the fields of rare disease research or when working with tissues that are not accessible in a clinical setting.\nThe specialized transcriptomic LLM, Geneformer (Theodoris et al., 2023), leverages the general knowledge acquired from pretraining on transcriptomic data to adapt efficiently to a specific disease use case and demonstrates remarkable efficiency in gene network analysis with minimal data. A study successfully distinguished key factors in the NOTCH1-dependent network by fine-tuning on just 884 endothelial cells from healthy versus dilated aortas, outperforming other methods that used a much larger dataset of about 30,000 cells (Theodoris et al., 2023).\nscGPT (Cui et al., 2023), another generated pre-trained transformer model for single-cell multi-omics data analysis, on the other hand, also showed the ability to generate meaningful cell-type clusters directly from the pre-trained model in a zero-shot manner (i.e., without additional fine-tuning).\nGene network analysis. Gene network analysis typically begins by mapping the gene regulatory networks and tracing the critical genes in a disease's progression to identify potential therapeutic targets. Specifically, the gene network analysis strives to uncover vital regulatory elements that can alter or modulate these networks in a desired manner (Theodoris"}, {"title": "3.1.3. PROTEIN TARGET ANALYSIS", "content": "A protein sequence is often the most accessible data about a target that can help explain its potential to play a role in disease mechanisms, with drug discovery scientists often targeting it as a starting point. In this application, specialized LLMs can be particularly valuable as these models have shown the ability to provide extensive analyses, including evolutionary conservation, functional annotation, protein folding, and binding site prediction. The unique ability of the LLMs to extract relevant information from sequence data alone has provided a means of characterization of target biological traits and functions even without experimental data like experimentally determined 3D protein structures.\nEvolutionary conservation. The use of specialized LLMs in protein analysis, as explained in the representative work of ESM (Rives et al., 2021), is based on a fundamental idea: the statistical patterns of protein sequences contain valuable information about their biological function and structure, which have been shaped by evolutionary processes (Yanofsky et al., 1964; Altschuh et al., 1988). This idea proposes that mutations that improve an organism's fitness are more likely to be selected by evolutionary forces among the multitude of possible mutations a sequence can undergo (G\u00f6bel et al., 1994), resulting in unique signatures in protein sequence patterns.\nSpecialized protein-based LLMs can effectively predict likely mutations within protein sequences that contain masked amino acids. This proficiency not only enables them to understand evolutionary conservation but also allows them to grasp the selection processes driving the evolution of these sequences. Follow-up research has demonstrated the efficacy of this approach using the ESM language model, which can make accurate predictions of mutational effects across a variety of proteins with different functions without any additional training (Meier et al., 2021). MSA-Transformer takes this approach further by analyzing multiple sequences using a multiple sequence alignment (MSA) approach (Rao et al., 2021). With the added information from the MSA as input, MSA-Transformers enhance their ability to interpret complex relationships within protein sequences and outperform single-sequence LLMs (Meier et al., 2021).\nUnderstanding evolutionary conservation using specialized LLMs is significant because it provides information not only on the functional landscape of proteins according to amino acid conservation patterns but also enables the establishment of the role and significance of individual residues in binding and activity (Altschuh et al., 1987). This is important as such conserved sites typically contribute to protein function and structure, while covarying mutations are similarly associated with these features, including contact surface, structure, and binding (Levitt, 1978; Yanofsky et al., 1964; Altschuh et al., 1988). Such findings are crucial for developing specialized LLMs for proteomics and form the basis for using these models to provide insight into predicting protein folding, binding sites, and functional annotation.\nProtein folding. The sequence patterns of a protein are shaped by its hidden structure, which is linked to evolutionary conservation and mutation. Specific structures and sequences are conserved due to their functional importance, while mutations occur in response to evolutionary pressures. As a result, LLMs that learn from protein sequence data can indirectly capture these evolutionary trends. This is exemplified by the groundbreaking work in ESM (Rives et al., 2021) and MSA-Transformer (Rao et al., 2021), which showed that LLMs can accurately decode the structural nuances of proteins from sequence data alone. Specifically, when these LLMs create pairwise interaction maps (attention matrices) between all amino acid positions in a sequence, they demonstrate an ability to infer which pairs of amino acids should be in contact with unparalleled accuracy (Rao et al., 2020; Fung et al., 2022). This remarkable ability strongly suggests that a significant amount of structural information can be directly inferred from the LLM model using only sequence data, in line with Anfinsen's dogma (Anfinsen, 1973).\nBuilding on foundational research, AlphaFold2 (Jumper et al., 2021) and RosettaFold (Baek et al., 2021) have revolutionized the field of protein structure prediction. These models can now produce atom-level accuracy even in cases where similar structures are not known, due to the Evoformer component in AlphaFold2, a specialized protein-based LLM. In this way, AlphaFold2's training objective mirrors that of MSA-Transformer, where residues"}, {"title": "3.1.4. PATHWAY ANALYSIS", "content": "In pathway analysis, gene regulatory network analysis can be a powerful tool for researchers seeking to descipher complex disease pathways. In this subsection, we will be focusing on the use of general LLMs, which can provide all-around assistance for pathway analysis.\nUnlike their specialized counterparts, general LLMs are innately equipped with a wealth of prior knowledge gleaned from vast scientific literature and datasets (Taylor et al., 2022; OpenAI, 2023). This extensive background enables them to approach pathway analysis with a broad, informed perspective rather than specializing in a single scientific language. Furthermore, general-purpose LLMs have the distinct advantage of being able to interactively and conversationally engage with complex scientific data (OpenAI, 2023; Jeblick et al., 2023), providing researchers with a powerful tool for understanding and exploring their findings.\nA recent study showcased the capacity of general-purpose LLMs, such as GPT-4, in analyzing blood transcriptional modules related to erythroid cells, demonstrating that these models are efficient in knowledge-driven pathway analysis (Toufiq et al., 2023). This research uses general LLMs to automatically generate codes for gene networks, summarize candidate genes ranked based on association tests, generate reports for users, and fact-check the report against the literature. In each task, the rich prior knowledge and interactive capabilities of general LLMs are exploited to analyze scientific data. By leveraging the rich prior knowledge and interactive capabilities of general LLMs, this study highlights how they can enhance disease mechanisms and target identification, allowing for a better understanding of complex gene networks. Ultimately, this transforms pathway analysis from static approaches to more dynamic and interpretable methods."}, {"title": "3.1.5. ASSISTANCE", "content": "The exploration of disease mechanisms is a complex task, requiring the contribution of experts from fields across health and pharmaceutical industries. In this environment, general-purpose LLMs that can perform tasks related to interactive and conversational skills, including information retrieval and knowledge explanation, can play a crucial role (Taylor et al., 2022).\nGeneral-purpose LLMs offer fast and accurate information retrieval, clear explanations tailored to user needs, and the ability to organize and categorize large datasets, enhancing workflow and productivity (Jeblick et al., 2023). By integrating with search engines, recent LLM iterations provide real-time access to scientific data, improving hypothesis generation and validation in disease research. Additionally, they aid in scientific communication by simplifying complex ideas for laypeople, fostering better collaboration among specialists with different expertise."}, {"title": "3.2. Drug Discovery", "content": "The drug discovery process is a crucial phase in the drug development pipeline, encompassing several critical steps as depicted in Figure 4. These steps include hit identification, hit to lead, lead optimization, and preclinical development.\nThe process starts with \u201chit identification\", where professionals find compounds with potential therapeutic effects. Next, \"hit to lead\" involves a more refined selection from these hits, identifying those most promising for further development. The third step, \"lead optimization\u201d, is a critical process of enhancing a lead compound's efficacy, stability, and safety via editing. Finally, \u201cpreclinical development\u201d entails rigorous testing of the optimized lead compound in animal models to assess its suitability for human trials.\nOur survey will begin by outlining the specific downstream tasks associated with each operation. Following this, we will explore how LLMs can be incorporated into these tasks to advance the drug discovery process."}, {"title": "3.2.1. CHEMISTRY", "content": "Medicinal chemistry is essential to drug discovery and development through independent laboratory work and compound synthesis. Autonomous lab operations use robotic manipulators, controlled and programmed to execute complex chemistry and synthetic reactions. In addition, high-throughput screening requires compounds to be precisely and efficiently synthesized as part of the hit identification phase. After synthesis, the compound will be evaluated for activity and selectivity using pharmacological assays.\nLLMs have proven to be highly valuable in these fields. LLms can help generate codes that program the chemistry robotics based on user requirements (Bran et al., 2023; Boiko et al., 2023). In particular, LLMs can translate user requirements into complex experimental protocols and convert them into specific, understandable robot instructions. Additionally, LLMs are successful in retrosynthetic planning and reaction prediction, offering to recommend feasible synthetic routes and to predict possible chemical reactions. Using LLMs in this manner is beneficial as it brings efficiencies in compound synthesis and accelerates the drug discovery process.\nChemistry Robotics. Nowadays, chemistry robotics is an integral part of conducting chemistry experiments using autonomous laboratory operations. This technique involves converting instructions written in natural language into robot-executable plans, usually described using a fixed, well-defined language that resembles coding.\nGeneral LLMs like GPT-4 (OpenAI, 2023) and CodeLlama (Roziere et al., 2023) have shown the ability to generate effective code. Therefore, it is logical to use general LLMs to generate robot-executable plans, as they have been trained on a vast amount of code. One notable application is CLARify (Yoshikawa et al., 2023), which utilizes GPT-3 (Brown et al., 2020) to generate task plans in a specific Chemistry Description Language (XDL) based on descriptive user instructions in natural languages. Constrained task and motion planning problems are then solved using PDDLStream solvers. This approach aims to facilitate the autonomous and safe execution of chemistry experiments using general-purpose robot manipulators. Notably, these plans have shown much higher accuracy than baseline systems like SynthReader (Mehr et al., 2020).\nFurthermore, there are preliminary attempts using GPT-4 to generate compatible scripts in Python to control OT-2 (Inagaki et al., 2023), a computer-controlled liquid handling robot, achieving 95% success within five iterations.\nAn emerging branch of AI research involves utilizing large language models as agents that will autonomously create, perform, and program scientific experiments. Boiko et al. (Boiko et al., 2023) presented one such method, showing how these models could use web search engines for information about molecule synthesis or employ vector search to find relevant documentation on chemical reactions. They have also developed multi-instrument systems code generation agents that can successfully implement complex experiments like Suzuki and Sonogashira cross-coupling reactions.\nRetrosynthetic Planning & Reaction Prediction. Retrosynthetic planning involves breaking down complex compounds into simpler precursor compounds, while reaction prediction entails forecasting the outcome of chemical reactions. These two tasks are pivotal for understanding how to synthesize complex molecules from more basic starting materials, which is an essential step for preparing experiments like high-throughput screening.\nAn early attempt at LLMs for retrosynthetic planning is the Molecular Transformer (Schwaller et al., 2019), which utilizes a simple encoder-decoder transformer framework. The model is trained to take reactants and reagents as input and predict the chemical product that can be synthesized from a reaction. It has demonstrated higher accuracy in reaction prediction than human chemists. Subsequently, Schwaller et al. (Schwaller et al., 2020) combined the Molecular Transformer with a hyper-graph exploration strategy to develop an automated retrosynthetic route planning system. The dynamically constructed hypergraph represents a generic reaction where each molecule is a node, and the hyper-arc symbolizes the reaction arrow. This optimal synthetic route is identified using beam search over a beam search across the hyper-graph of possible disconnection strategies.\nThe capabilities of LLMs were further extended by Chemformer (Irwin et al., 2022), which is based on the BART (Lewis et al., 2020) architecture. It includes an additional pretraining process that involves reconstructing masked SMILES strings and using an autoencoder to convert original SMILES to embeddings and back. The model is then fine-tuned for various downstream tasks, including reaction and retrosynthesis predictions.\nRecently, general-purpose LLMs have emerged in this field, such as Chemcrow (Bran et al., 2023) and Boiko et al (Boiko et al., 2023). Boiko's system uses web search and simple calculations, while Chemcrow adopted a more sophisticated approach. Chemcorw has developed and utilized a more comprehensive range of customized molecule and reaction tools. These include functionalities like converting queries to SMILES, obtaining molecule prices, patent checking, and reaction classification. Additionally, Chemcrow adopts a four-step framework to improve LLMs' ability: think about the necessary steps, take action using tools, provide inputs to these tools, analyze observations, and then deliver the final answer. This approach has been shown to perform better than GPT-4 in most tasks evaluated by humans in synthesis planning. Similarly, a recent study (Jablonka et al., 2024) demonstrated that a fine-tuned GPT-3 model was shown to outperform traditional machine learning models on several chemistry tasks, especially in low-data scenarios. The findings highlighted that LLMs, even those not initially trained on chemical data, could adapt to various predictive chemistry tasks with minimal fine-tuning, showcasing the potential of LLMs in advancing chemical research."}, {"title": "3.2.2. IN-SILICO SIMULATION", "content": "In-silico simulation leverages computer models to simulate complex biological processes. These simulations are pivotal in understanding and predicting how drugs interact at the molecular level, leading to more efficient and targeted drug development. The three main tasks involved in in-silico simulations are de novo molecule generation, de novo protein generation, and protein-ligand interaction prediction.\nDe novo Molecule Generation. De novo molecule generation is a complex task in in-silico simulations that involves creating new molecular structures with the potential to be effective drugs. This process is categorized into two types: unconstrained molecule generation, which seeks to populate the chemical space of the training set, and constrained molecule generation, where molecules are synthesized to meet specific desired properties (Brown et al., 2019). Constrained generation requires a model to consider various constraints such as affinity to targets, selectivity against off-targets, appropriate physicochemical properties, ADME characteristics, pharmacokinetics/pharmacodynamics, toxicology, and synthesizability (Loeffler et al., 2023).\nTo benchmark the performance of de novo molecule generation methods, GuacaMol (Brown et al., 2019) was introduced to provide a benchmarking framework considering aspects such as validity, uniqueness, and novelty. Specialized language models have demonstrated remarkable proficiency. For instance, it has been have shown that even simple RNN-based models can perform exceedingly well on challenging generative modeling tasks (Flam-Shepherd et al., 2022). These models effectively learn the complex distribution of molecules, such as the highest-scoring penalized logP molecules in ZINC15 or the most significant molecules in PubChem. To explore wide and novel chemical spaces, LLMs such as SMILES-LSTM and ORGAN (Guimaraes et al., 2017) have been assessed with the unconstrained generation ability to directly generates (Brown et al., 2019), and ORGAN. When it comes to constrained molecule generation, several notable approaches have been developed. Previous studies utilized reinforcement learning (Olivecrona et al., 2017) and pharmacophoric features (Skalic et al., 2019) to improve RNN-based models toward generating molecules with desired properties and binding ligands to protein pockets.\nMoreover, MolGPT (Bagal et al., 2021b), with its GPT architecture, can handle multiple constraints. It is trained by recovering a molecule with its scaffold and properties. The REINVENT series (Blaschke et al., 2020; Loeffler et al., 2023) represents a more advanced approach in this category. It is sweeping and capable of meeting up to 10 different objectives, including synthesizability, selectivity, etc. This method narrows the chemical search space through a three-staged training process: the prior model, a transfer learning agent, and staged learning towards generating high-scoring sequences. This sophisticated approach involves pretraining, transfer learning, reinforcement learning, and curriculum learning.\nOn the other hand, general LLMs usually focus on the constrained molecule generation task. In this context, MolT5 (Edwards et al., 2022) uses a self-supervised learning framework to pretrain T5 (Raffel et al., 2020), a general-purpose LLM that is trained on a large corpus of text coupled with molecular data pairs. The pretraining methodology comprises unsupervised SMILES recovery and associated chemical texts. More recently, GPT-4 (OpenAI, 2023) has shown its ability to produce a novel molecule guided by textual instructions. However, the effectiveness generated by these two models is inferior to specialized language models. In addition, multimodal methods such as Momu (Su et al., 2022) and GIT-Mol (Liu et al., 2023a) enhance general LLMs' capabilities in molecule generation. Momu (Su et al., 2022) improves upon MolT5 (Edwards et al., 2022) by adopting CLIP (Radford et al., 2021) to align molecule graphs with related text. At the same time, GIT-Mol (Liu et al., 2023a), inspired by BLIP2's Q-FORMER strategy (Li et al., 2023a), integrates graph, image and text information, using cross-attention and a variety of pretraining tasks. This multimodal approach significantly improves the effectiveness of MolT5 (Edwards et al., 2022) in constrained molecule generation tasks.\nDe novo Protein Generation. Similar to molecule generation, this task focuses on designing new proteins in an unconditional manner (Hesslow et al., 2022; Ferruz et al., 2022; Nijkamp et al., 2023) or conditional manner, where proteins generated should fit user constraints (Wang et al., 2022a; Ram & Bepler, 2022; Watson et al., 2023). Using these LLMs, scientists can design new artificial proteins that could serve specific functions, such as binding to a particular receptor or acting as enzymes.\nUnconstrained generation aims to delve into and map the extensive protein space with specialized protein-based LLMs, such as those using autoregressive models for amino acid sequence generation, demonstrating remarkable effectiveness in this realm (Madani et al., 2020; Hesslow et al., 2022; Nijkamp et al., 2023). A prime example of these LLMs is ProtGPT2, detailed in Ferruz et al. (2022). Trained on a wide range of protein sequences, ProtGPT2 excels in creating de novo protein sequences that mirror natural patterns. Its outputs, marked by ordinary amino acid propensities, are predominantly globular, resembling natural proteins. Intriguingly, comparative analysis with protein databases indicates that ProtGPT2's generated sequences distantly related to existing proteins can form valid structures based on AlphaFold2. This indicates ProtGPT2's ability to explore novel and valid protein space areas."}, {"title": "3.2.3. ADMET PREDICTION", "content": "The prediction of absorption, distribution, metabolism, excretion, and toxicity (ADMET) attributes"}]}