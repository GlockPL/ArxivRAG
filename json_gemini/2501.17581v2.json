{"title": "CSEval: Towards Automated, Multi-Dimensional, and Reference-Free Counterspeech Evaluation using Auto-Calibrated LLMS", "authors": ["Amey Hengle", "Aswini Kumar", "Anil Bandhakavi", "Tanmoy Chakraborty"], "abstract": "Counterspeech has emerged as a popular and effective strategy for combating online hate speech, sparking growing research interest in automating its generation using language models. However, the field still lacks standardised evaluation protocols and reliable automated evaluation metrics that align with human judgement. Current automatic evaluation methods, primarily based on similarity metrics, do not effectively capture the complex and independent attributes of counterspeech quality, such as contextual relevance, aggressiveness, or argumentative coherence. This has led to an increased dependency on labor-intensive human evaluations to assess automated counter-speech generation methods. To address these challenges, we introduce CSEval, a novel dataset and framework for evaluating counterspeech quality across four dimensions: contextual-relevance, aggressiveness, argument-coherence, and suitableness. Furthermore, we propose Auto-Calibrated COT for Counterspeech Evaluation (Auto-CSEval), a prompt-based method with auto-calibrated chain-of-thoughts (CoT) for scoring counterspeech using large language models. Our experiments show that Auto-CSEval outperforms traditional metrics like ROUGE, METEOR, and BertScore in correlating with human judgement, indicating a significant improvement in automated counterspeech evaluation. 1", "sections": [{"title": "Introduction", "content": "Online hate speech (HS) is on the rise in social media, making it a hostile platform for targeted individuals. Counterspeech (CS) provides an efficient way to combat hate speech with the help of constructive statements (Benesch et al., 2016; Chandrasekharan et al., 2017) without violating the"}, {"title": "Related Work", "content": "There has been an increased interest in research focusing on building datasets and related resources (Chung et al., 2019; Liu et al., 2019; Gupta et al., 2023) along with automated methods for counterspeech generation (Zhu and Bhat, 2021; Zhang et al., 2020b; Gupta et al., 2023; Hengle et al.,"}, {"title": "Dataset", "content": "We build and release CSEval, a benchmark dataset for reference-free and multi-dimensional counterspeech evaluation. CSEval contains expert human assessments of 7, 926 model-generated CS, across four quality dimensions (or aspects). In this section, we give a detailed overview of the data curation process, evaluation dimensions, and the models used in CSEval."}, {"title": "Data Curation", "content": "We used the publicly available IntentCONAN (Gupta et al., 2023) dataset, which offers a diverse set of CS spanning multiple categories like empathy, counter-questions, fact-checking, and denouncing. We selected IntentCONAN over other counterspeech datasets (Chung et al., 2019; Fanton et al., 2021b), as it accurately reflects the open-ended nature of counterspeech generation, accommodating multiple valid CS for a given HS. We began by randomly sampling approximately 2000 unique HS instances from IntentCONAN. For each HS instance, we then generated a CS using five popular counterspeech generation models, as detailed in Appendix A.1.1. We ended up creating the base corpus of CSEval with 2,223 unique HS, 4, 318 ground-truth (reference) CS, and 7, 926 model-generated CS instances."}, {"title": "Evaluation Dimensions", "content": "We define the evaluation process across four dimensions (or aspects) of counterspeech quality relevance, aggressiveness, coherence, and suitableness. We select these dimensions as they are most frequently reported in human evaluation studies in counterspeech literature. We discuss this in detail in Appendix C.1.\n(i) Relevance assesses whether the counterspeech is in line with the hate speech's central theme, subject, or topic. Contextual relevance is an important counterspeech quality, especially considering the implied nature of hate speech that can confuse language models."}, {"title": "Models", "content": "We include counterspeeches generated using both supervised and prompt-based methods. In terms of supervised methods, we include three popular counterspeech generation models \u2013 QUARC (Gupta et al., 2023), (Zhu and Bhat, 2021), DialoGPT (Zhang et al., 2020b), and Generate-Prune-Select (GPS). As our prompting baselines, we include counterspeeches generated by two popular LLMs -GPT-3.5-Turbo (ChatGPT) and GPT-4 (Ouyang et al., 2022). For each of them, we include both zero- and few-shot prompting baselines. Further details surrounding model training and prompting are provided under Appendix A.1.1."}, {"title": "Annotation Process", "content": "We recruited five expert annotators who have either published papers or completed senior theses in the domain of hate speech and counterspeech 3. We used expert annotators for our evaluation process due to the task's sensitivity and the quality issues of crowd-sourced annotations reported in previous work (Gillick and Liu, 2010). Given a HS, reference CS, and model-generated CS, annotators are asked to rate the model output across each of the four dimensions on a Likert scale. Relevance, coherence, and aggressiveness are rated on a scale of 1 to 5, while suitableness is rated on a scale of 1 to 3. Higher scores indicate better quality, except for aggressiveness, where lower scores indicate better quality. Further details about the background"}, {"title": "Inter-Annotator Agreement", "content": "We used Krippendorff's alpha coefficient (Klaus, 2011) to measure the inter-annotator agreement of the expert annotations. For the first round of annotations, we obtained a decent inter-annotator interval of Krippendorff's (averaged across the four dimensions) of 0.473. However, the 2nd round increases the overall inter-annotator agreement with an average Krippendorff coefficient of 0.681. We also calculate the standard deviation of annotator scores within the respective groups. We plot the histogram of these statistics in Appendix Figure 3. Here, we observe that suitableness remains the most contentious dimension among experts. Further details about the annotation process are provided under Appendix Section A."}, {"title": "Proposed Method", "content": "Figure 2 illustrates the overall framework of our proposed method, Auto-CSEval (Auto-Calibrated Chain-of-Thoughts for Counterspeech Evaluation). Auto-CSEval is a prompt-based evaluator with three main components: (i) An instruction describing the evaluation task at hand, (ii) Evaluation CoT, i.e., a brief description of the evaluation aspect, and (iii) CoT that is the set of intermediate instructions describing the detailed evaluation steps for the desired evaluation CoT. To calibrate the LLM, we focus on optimising the CoT steps T, while keeping the instruction and evaluation CoT parts of the prompt consistent. Specifically, we mine and tune the CoT steps by constructing a small validation set, containing human-labelled (HS, CS) pairs. Following this, we adopt a step-by-step procedure to iteratively refine the candidate CoT. This includes two phases, i) drafting and ii) revisiting as shown in 2. The first set of candidate CoT drafts is obtained by running inference with in-context labels using an induction prompt. These are then evaluated and filtered on expert labels and then refined to accommodate erroneous evaluations.\nInstruction: The prompt is a natural language instruction that defines the evaluation task at a high level. In our study, we use a common instruction prompt for all four evaluation dimensions.\nYou will be given one counterspeech (also called counterspeech or counter-"}, {"title": "Auto Evaluation Steps (CoT)", "content": "CoT is a prompt en-gineering technique that encourages LLMs to break down a complex task into a sequence of intermediate steps and has been shown to improve LLMs in their reasoning abilities and generate more accurate and informative responses for multi-step problems, such as arithmetic, common-sense, and symbolic reasoning (Wang et al., 2023; Zheng et al., 2023). For evaluation tasks, some candidate CoTs need more detailed instructions (evaluation steps). It is generally time-consuming to design such evaluation steps for each task manually. Liu et al. (2023a) found that an LLM can generate such evaluation steps by itself and proposed an auto-CoT approach to evaluate NLG tasks. However, this approach may result in sub-optimal and misaligned CoT where the scoring guidelines are absent and only output categorical ranges (e.g., 0-5) are provided. LLM-based evaluators are shown to suffer from insufficient prompting, resulting in inconsistent and"}, {"title": "Results", "content": "We adopt the same approach suggested by Zhong et al. (2022) and Liu et al. (2023a) to evaluate different CS generation metrics using CS-level Spearman (p) and Kendall-Tau (\u03c4) correlation. We report the correlation scores between automated metrics and human judgments in Table 1. The first part of Table 1 shows metrics that compare lexical or semantic similarity between the model output and reference CS. We find that overlap-based metrics perform poorly on almost all dimensions, with metrics such as BLEU, ROUGE, and METEOR even displaying negative correlations in some cases. This observation is consistent with similar studies in summarisation and dialogue generation, where overlap-based metrics are shown to be poor multi-dimensional evaluators (Fabbri et al., 2021; Zhong et al., 2022; Liu et al., 2023a). The second part shows results of metrics that use neural networks to learn from human ratings of text quality. In particular, BERTScore and BARTScore show a higher correlation with human judgement than all other similarity-based metrics. This shows that they are more reliable for CS evaluation.\nThe third part of Table 1 shows the results of some popular models used for CS evaluation. These models independently evaluate specific quality aspects of a CS. We observe that most independent evaluators display poor correlation with human judgements for the aspect that they evaluate. While the Toxicity score (Hanu and Unitary team, 2020) shows moderate correlation, it still lags behind LLM-based evaluators in assessing aggressiveness of a CS.\nLastly, we observe that LLM-based evaluators achieve the highest correlations with human judgements across all dimensions, underlying their reliability as reference-free counterspeech evaluators. Our proposed Auto-CSEval method with GPT-4 as the base model consistently outperforms all the other LLM baselines, especially for relevance, coherence, and suitableness. Furthermore, we observe that for each of the three LLMs, prompting methods with CoT (G-EVAL and Auto-CSEval) perform much better than their respective zero-shot counterparts. This validates our hypothesis that CoT in the form of evaluation steps helps LLMs achieve improved alignment, bringing them closer towards human reasoning for counterspeech evaluation."}, {"title": "Analysis", "content": "Effect of Calibration. In Table 1, we compare the performance of Auto-CSEval with and without auto-calibrated CoT on the CSEval benchmark. shows that Auto-CSEval (GPT-4) outperforms G-EVAL (GPT-4) on both Spearman and Kendall-Tau correlations. We observe a similar trend for Mistral and Llama models. This suggests that auto-calibration helps align the CoT towards human judgement.\nIs there a case for a single, unified score to assess counterspeech quality? Our proposed framework is designed to independently evaluate a CS across different quality dimensions. This is in line with the broader counterspeech literature, which supports the notion that there is no single, absolute metric or aspect that completely represents a counterspeech's quality or effectiveness (Benesch, 2014; Chung et al., 2024). However, research following CS generation often requires to relatively compare different NLG methods. To aid this, we experiment with the efficacy of having a unified score for CS evaluation. Specifically, for a given model output, we compute the unified score as a mean of individual scores relevance, coherence, aggressiveness, and suitableness.\nMean = \\frac{Relevance + (6 - Aggressiveness) + Coherence + \\frac{(Suitableness - 1) \\times 4}{2} + 1}{4}\n\nAs shown in the equation above, Aggressiveness score is normalised as 6-Aggressiveness since it is a \"lower the better\" score. Similarly, Suitableness is transformed from the original scale of 1-3 to 1-5. This is the same logic used to compute scores in Figure 1.\nFor this experiment, we construct a meta-evaluation by randomly sampling model outputs from CSEval. For a given HS, we ask human evaluators to rank model outputs from best to worst according to their preference4."}, {"title": "Conclusion", "content": "This paper presents CSEval, a novel multidimensional framework for evaluating the quality of automated counterspeech against online hate speech. Our work addresses a critical gap in the current research landscape of automated counterspeech generation by providing a comprehensive and automated approach for assessing counterspeech quality along four key dimensions: context relevance, aggressiveness, argument coherence, and suitableness. We observe that traditional similarity-based evaluation metrics, while prevalent, often fail to capture the nuanced complexity of effective counterspeech. We further introduce Auto-CSEval, a prompt-based evaluation method with an auto-calibrated chain-of-thoughts mechanism, which leverages LLMs to offer a more refined and human-aligned evaluation. Our experiments with multiple automated metrics on the CSEval dataset illustrate that Auto-CSEval displays a significant improvement in correlation with human judgment, particularly when juxtaposed against existing evaluation methods. In conclusion, while our framework marks a significant advance in the automated evaluation of counterspeech, it also underscores the complexity and multi-faceted nature of this domain."}, {"title": "Limitations", "content": "Our study focuses on four dimensions of text quality that are specific to counterspeech generation: relevance, coherence, aggressiveness, and appropriateness. However, there are other quality aspects,"}, {"title": "Ethics Statement", "content": "We acknowledge that we are dealing with a sensitive topic of research as it deals with online hate speech. We understand that we must adhere to strict ethical considerations while dealing with hatespeech-related data. First, our dataset is based on a publicly available, open-source dataset in the counterspeech domain. As we were largely dealing with online hate speech in the form of social media posts, we ensure that they were fully anonymised and untraceable to the source user. During the data annotation process, we ensured that each of the annotators was fully aware and had context about the nature and degree of offensive statements that they were responsible to annotate."}]}