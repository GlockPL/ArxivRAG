{"title": "Customized Information and Domain-centric Knowledge Graph Construction with Large Language Models", "authors": ["Frank Wawrzik", "Matthias Plaue", "Savan Vekariya", "Christoph Grimm"], "abstract": "In this paper we propose a novel approach based on knowledge graphs to provide timely access to structured information, to enable actionable technology intelligence, and improve cyber-physical systems planning. Our framework encompasses a text mining process, which includes information retrieval, keyphrase extraction, semantic network creation, and topic map visualization. Following this data exploration process, we employ a selective knowledge graph construction (KGC) approach supported by an electronics and innovation ontology-backed pipeline for multi-objective decision-making with a focus on cyber-physical systems. We apply our methodology to the domain of automotive electrical systems to demonstrate the approach, which is scalable. Our results demonstrate that our construction process outperforms GraphGPT as well as our bi-LSTM and transformer REBEL with a pre-defined dataset by several times in terms of class recognition, relationship construction and correct \"sublass of\" categorization. Additionally, we outline reasoning applications and provide a comparison with Wikidata to show the differences and advantages of the approach.", "sections": [{"title": "Introduction", "content": "The planning of modern cyber-physical systems necessitates the ability to predict technology trends, employ collaborative community approaches, allocate tasks efficiently, develop design roadmaps, and even facilitate self-organizing systems. In the era of digitalization, these processes demand quicker decision-making, often amidst increasingly competing interests and considerations. Technology Intelligence is a data analytical approach to evaluate and follow a technologies potential and behaviour. Making these decisions relies on the availability of timely and pertinent information from extensive datasets. One effective strategy for managing this complexity is the integration of Large Language Models (LLMs) into one's document database and utilizing specific questions or prompts. However, it is worth noting that these approaches are currently not machine-actionable. Therefore, in this paper, we propose a knowledge graph-based method aimed at enhancing technology intelligence and facilitating machine-actionable innovation construction to improve the planning of cyber-physical systems.\nConventional methods for constructing knowledge bases typically concentrate on a specific domain and rely on a dedicated corpus. This corpus is often relatively static or updates predefined properties outlined in ontologies using dedicated datasets. Frequently, only a limited number of distinct sources are crawled and incorporated into the knowledge graph, such as Wikipedia articles or research articles from a particular journal. The challenge here is that these knowledge graphs often fall short in terms of succinctness and completeness, both of which are critical quality criteria.\nIn this paper, we introduce a novel approach that involves the pre-classification of information from heterogeneous sources for constructing the knowledge graph. Our methodology involves crawling a wide variety of sources at an extensive scale. This strategy mitigates the drawbacks associated with attempting to crawl the entire internet and constructing an overly voluminous and irrelevant graph. Simultaneously, it results in the accumulation of more relevant information, contributing to the creation of larger and more comprehensive knowledge bases. Additionally, we complement this approach with reasoning techniques to enhance semantic accuracy.\nThe contribution of the paper is a two stage knowledge graph construction process. By preselecting only relevant documents for the domain the KG construction via large language models, it can construct tailored domain knowledge."}, {"title": "State of the Art", "content": "Technology intelligence \u201cprovides an organisation with the capability to capture and deliver information in order to develop an awareness of technology threats and opportunities\" (Kerr et al. 2006). One important subfield of technology intelligence is technology mining, or tech mining for short: \"the application of text mining tools to science and technology information, informed by understanding of technological innovation processes\" (Porter and Cunningham 2004, p.19).\nMethods from natural language processing (NLP) can help technology scouts explore a large collection of documents, such as news articles, more efficiently by detecting events and trends (Panagiotou, Saravanou, and Gunopulos 2021), or summarizing key points (Ma et al. 2022). Removing irrelevant results such as fake news (Capuano et al. 2023) can also reduce the data deluge."}, {"title": "Topic maps and semantic networks", "content": "One important tool in tech mining are topic maps: visual representations of relations among terms (e.g., technologies, organizations) or among documents. Topic maps have been proposed for many years as an essential tool for patent landscaping and scientific domain analysis (Moya-Aneg\u00f3n et al. 2004; Yang et al. 2010; Kay et al. 2014; Hofmann, Keller, and Urbach 2019). Topic clusters are related terms that belong to a topic.\nTopic maps can be implemented by visualizing semantic networks based on semantic similarity (Sarica, Luo, and Wood 2020; Kim and Hyun 2023) using graph drawing techniques. With the proliferation of increasingly powerful LLMs that allow for the effective semantic embedding of text, these methods present an attractive alternative to more traditional metrics like co-citation or classification overlap. According to (Muennighoff et al. 2022), the performance of text embeddings on different tasks \u201cvaries strongly with no model claiming state-of-the-art on all tasks.\" However, language models adapted to a specific domain can produce embeddings that allow for high performance on downstream NLP tasks applied to data from that domain-for example, analyzing scientific publications (Beltagy, Lo, and Cohan 2019; Cohan et al. 2020; Singh et al. 2022)."}, {"title": "Knowledge graph construction", "content": "Current knowledge graph construction methods utilize forms of machine learning for example with deep learning or transformers (Yao, Mao, and Luo 2019; Huguet Cabot and Navigli 2021; Jaworsky et al. 2022). However utilizing LLM's for knowledge graph construction is new. In (Babaei Giglou, D'Souza, and Auer 2023) the authors investigate if LLMs can be effectively used to generate ontologies and knowledge graphs via the zero-shot prompting method. They use lexical terms, taxonomic discovery and triple relations. Whereas their work was to evaluate the general suitability, we constructed a knowledge graph based on definitions in a reference ontology. (Lopes et al. 2023) classify domain entities by allocating those entities to their corresponding top level superclass. They use their own trained transformer-based language models. GraphGPT (Tang et al. 2023) has to be mentioned as related work. It is able to generate more detailed graphs than transformer models, but lacks structure and quality criteria which is both improved within this work."}, {"title": "Planning and electronics ontologies", "content": "(Rajpathak and Motta 2020) introduces a task ontology for domain independent planning. (Bermejo-Alonso, Salvador, and Sanz 2018) also proposes an task-ontology-based approach to improve coordinated planning and problem solving of autonomous agents. Other works focus role of ontologies in CPS (Garetti, Fumagalli, and Negri 2015) or general surveys in this area to investigate automatic construction (Hafidi et al. 2023). Only this work however generates large knowledge graphs, which can be used to identify developments and serve as a general knowledge base for planning for CPS.\nA comprehensive overview of the foundations of knowledge engineering for cyber-physical systems can be found in (Wawrzik 2022). The GENIAL! Basic Ontology, based on an upper ontology and the ISO26262 standard on automotive safety, describes cyber-physical systems and digital twins in detail (Wawrzik and Lober 2021)."}, {"title": "Method", "content": "Figure 1 visualizes the framework and its process in detail. First, we query a technology intelligence database for relevant documents such as research articles and patents. From this corpus, we extract keyphrases that help diversify, spread, and improve search results. We also use the keyphrases to create a semantic network, and visualize this network as a topic map. The topic map helps users interactively explore the domain. Based on these methodologies we arrive at relevance scores for each desired technology, product, trend, or innovation. This information allows the user to select the documents with highest relevance and importance to the field and use case at hand, that serve as the input for final knowledge graph construction (KGC). In the KGC pipeline, the first step is to convert the articles into .txt files that Owlready2 (Lamy 2017) transforms into an OWL Ontology Graph (.owl file). The transformation occurs via the transformer REBEL (Huguet Cabot and Navigli 2021), and large language models like ChatGPT. The REBEL transformer model was pre-trained with an electronic dataset from Wikipedia articles, whereas ChatGPT was optimized with several prompting approaches and functions. Additionally, the created knowledge graph is applied to a reasoning procedure that commits to definitions that are defined in the GENIAL! Basic Ontology as the knowledge graphs schema. This reasoning procedure ensures an improved consistency and structure, and improves the results of the machine learning as well. GBO is capable of describing all hardware and software systems - from smart homes and refrigerators to autonomous cars, cyber-physical energy systems and graphics processing units. This approach is generalizable to any domain in as much as the prompt is adapted to a new vocabulary of the relevant terms of the domain. Further the content of articles needs to reflect the relevance for the vocabulary, but can show heterogenity and diversity. To adapt the pipeline to a new knowledge source then just requires: 1) adding textdocuments of the new domain and 2) defining keywords for search. Additionally important to mention that currently a knowledge expert is still required at the last stage to supervise the quality of the content."}, {"title": "Innovation Graph database", "content": "As of November 2023, the our Innovation Graph database includes 50 million research publications, 81 million internet news articles, and 45 million patent filings from the past 10 years. We will refer to documents from these different domains as document genres, in order to avoid confusion with the knowledge domain of innovation and technology.\nThe titles and abstracts (when available) of these documents have been matched against a thesaurus that includes 590,000 terms relevant to global innovation and technology."}, {"title": "Information retrieval and keyphrase extraction", "content": "Tech mining distinguishes itself from \"data mining and text mining by its reliance on science and technology domain knowledge to inform its practice\" (Porter and Cunningham 2004, p.19). In practice, domain knowledge is injected into the text mining process through the definition of a search strategy that is used to extract the relevant documents under study from the database (MacFarlane, Russell-Rose, and Shokraneh 2022).\nAgreeing with (Comai 2018), we argue that in order to cover the whole technology ecosystem, that this corpus of relevant documents be drawn from three genres: research publications, internet news articles, and patent documents.\nOnce the corpus has been extracted, we can identify the most relevant terms from the thesaurus by co-occurrence analysis. More specifically, for each genre and term, we compute the normalized pointwise mutual information (nPMI), and keep those terms that are associated with a positive value for every genre."}, {"title": "Semantic network", "content": "Next, we build a semantic network - the nodes of which are defined by the most relevant terms extracted in the previous step. First, we need to embed the terms, so we need to select a model for text representation. Benchmarks that compare the current state-of-the-art in terms of accuracy are readily available.\nIn order for the embedding to better capture the meaning of each term, we compute the embedding of the term concatenated with a short description which, in the majority of cases, had been extracted from Wikipedia.\nWe tested two methods to construct the final semantic network from these embeddings:\n\u2022 simple thresholding, connecting any two terms the embeddings of which have cosine distance of at most 0.5,\n\u2022 a technique called \"semantic co-occurrence\" where each topic is represented by their cosine distance with every document in corpus, which can be interpreted as a fuzzy membership function\u2014similarity of two topics can be defined by the Tanimoto similarity of those memberships."}, {"title": "Ontology for Innovation and Planning", "content": "The Ontology for Innovation\u00b9 was built in 2012 by an innovation center and Volkswagen. It represents well a divers and informal domain in a way where complex interdependencies can be discovered and expressed. For example innovations usually satisfy a need that accounted for the innovation. Every innovation has a stage from defining the need to distributing the product. Further they are related to an improvement of for example efficiency or quality of something. Also innovations are assigned to disruptions. For example quantum computing disrupts cryptography. Additionally, each innovation is allocated to a design state from conceptualization to distribution. A populated knowledge graph based on the innovation ontology is thus able to predict developments and bring disruptions to planning either to the awareness of the user or to act on it autonomously."}, {"title": "Evaluation", "content": "Automotive electrical systems are intricate networks that perform several vital functions, including power distribution, data transmission, and control. These systems are characterized by a wide array of components, such as Electronic Control Modules (ECMs), sensors, and cables. Additionally, they integrate advanced technologies like Advanced Driver Assistance Systems (ADAS), Ethernet communication, and Over-The-Air (OTA) capabilities.\nGiven their complexity, automotive electrical systems represent an excellent example of the benefits of employing knowledge graphs. These graphs facilitate the mapping of complex dependencies and interactions among various components and technologies. This mapping provides a comprehensive overview crucial for effective decision-making and planning. For example, when introducing a new component, a knowledge graph can demonstrate how this addition will interact with existing systems, predicting potential conflicts or synergies."}, {"title": "LLMs to create semantic networks for data analytics", "content": "As stated in the semantic network section in the methods section, we also used large language models to create our semantic network of keywords and also chose different LLMs. However, tf-idf (term frequency-inverse document frequency) and fastText (Grave et al. 2018) are much faster on inference (see Table 1) which motivated us to perform our own comparison on some simple downstream classification tasks, see Table 2. The more recent language models that we compared to \"classic\" approaches were the large, English BGE model (Xiao et al. 2023) and the lightweight all-MiniLM-L6-v2 model derived from (Wang et al. 2020). The tasks that we based this comparison on were for a K-nearest neighbor (cosine distance) classifier to determine the classes defined by a lexical search strategy (#1) or a taxonomy Sectors (#2), respectively, processing the documents' (a) titles and abstracts, or (b) only the titles. Based on this evaluation, we decided to use the all-MiniLM-L6-v2 model which shows the best trade-off between runtime performance and accuracy."}, {"title": "Topic map visualization for automotive electrical systems", "content": "In order to extract the corpus from the Innovation Graph database, the search strategy in Listing 1 was used. Search terms have automatically been stemmed by the system, and the publication dates (filing dates for patents) have been restricted to the range between 2018-10-13 and 2023-10-12.\nAs could be expected, many of the top relevant extracted thesaurus terms with highest nPMI are included with the domain expert's original search strategy.  shows the top 10 terms that were newly detected, and not part of the original search strategy.\nThe semantic network for \u201cautomotive electrical systems\" consists of 708 nodes and 2836 edges. We used the network analysis software Cytoscape (Shannon et al. 2003) to draw and layout the network. In addition, the Markov Cluster Algorithm (MCL) was used to cluster the network (Morris et al. 2011; Dongen 2008). This process resulted in the generation of 120 clusters, with 60 clusters being comprised of more than two nodes. One example cluster is illustrated in Figure 3.\nIn order to present an alternative to our approach, we prompted ChatGPT to produce an edge list of related terms in the field of \u201cautomotive electrical systems\" (ChatGPT by OpenAI 2023)."}, {"title": "Knowledge graph construction (KGC) for automotive electrical systems", "content": "Using the results of the text mining procedure explained in the previous section, all relevant documents have been identified and are fed to the KGC pipeline. The articles contain descriptions of specific technical contributions related to the field of automotive electrical systems. The knowledge graphs constructed from these documents describe these components in more detail than the topic map, and provide a machine readable overview of the content.\nOur previous implementation with a transformer and a bi-LSTM required training several assistants to perform manual tagging on our datasets (see (Wawrzik et al. 2023)). This was a time consuming task that took around three months with 20 person hours per week. Besides the amount of time spent, it was a challenge to train the assistants. The classification according to the distinctions of GBO were fine-grained and challenging to comprehend for non-ontology and non-domain experts. The implementation using ChatGPT resulted in several improvements when compared to our previous bi-LSTM with transformer model, known as REBEL. The bi-LSTM achieved an f1 score between 0.36 and 0.78 depending on the amount of tags, the tagging class and the complexity of word recognition (and accuracy of human tagging). We observed an increase in the number of recognized classes and the construction of relationships as well. In contrast to REBEL, ChatGPT generated a greater number of classes and offered more descriptive labels. The increase of number of classes was significant with just over 200% for the LLM for our three article dataset. The amount of correctly classified classes increased as well.\nThe prompting had to be carefully crafted and optimized in order to improve classification accuracy according to the GENIAL! Basic Ontology (GBO). We taught ChatGPT our vocabulary and relationships, experimented with wording and the stages of prompt design (context, persona, task, format, exemplar, and tone).\nDue to the large amount of data in the corpus, we selected three research articles with a high relevance score to demonstrate the approach:\n\u2022 Optimal Operation of Automotive Electrical System with Photovoltaic Generation and Three-level Battery Management Scheme\n\u2022 Designing Attacks Against Automotive Control Area Network Bus and Electronic Control Units\n\u2022 CAN-FT: A fuzz testing method for automotive controller area network bus\nAs illustrated in Figure 7, the articles are related to automotive electrical systems, but cover a wider range of issues. In the Figure we see a graph in the pickle format visualized in html. In comparison to prot\u00e9g\u00e9, graph triples can be identified easily. The CAN FT bus and nearby nodes are shown. The", "part of directly\u201d relationship to the next connected upper hierarchy. Functions that the CAN FT can supply are for example existing fuzz testing methods, messaging processing procedures or security vulnerability analysis. We see a variety of classes and nodes ranging from cyberattacks to testing methods and new technology compositions. The constructed knowledge graph contains 3100 axioms, 650 classes and 16 object properties. The low object property count increases reusability and reduces complexity, and is an intentional design decision. Further labels were annotated for natural language spaces and synonym recognition, which also is a best practice in the semantic web. Figure 5 shows the class": "AN-Bus"}, {"title": "Reasoning application", "content": "After the construction process our graph is checked against the definitions of our reference ontology GBO, which was introduced earlier. This is work in progress and a challenge due to two reasons: long reasoning times and the complexity of expressiveness as well as the amount of data to be supervised.  illustrates the idea and gives an example. The Figure shows the class charging system, which was correctly classified as a system. Further the relationships (existential restrictions) of the charging system are shown. Both the automotive alternator and the battery were classified as hardware components and the electrical components as components. The difference here is that components are allowed to contain software. Hardware components are a subclass of components that constitute of hardware parts. As these components are in the next hierarchical level the reasoner test is passed. Charging was classified as function, and thus the range axiom of the implements relationship is also fulfilled. This makes the charging system a valid entry in our knowledge base and is thus not removed."}, {"title": "Comparison with Wikidata", "content": "Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch 2014) is one of the largest structured knowledge bases. It contains classes and relationships of almost any domain. We examined Wikidata's knowledge base in regard to the domain covered in this work. It has a large upper level to structure its classification, often by multiple inheritance. The relationships where often similar with minor differences in modelling choices in comparison with the GENIAL! Basic Ontology.\nFor example, our ontology is built for specific reasoning operations. Thus, we have the has_part_directly relationship (non-transitive, sub-object property of has_part) in addition to the has_part relationship which is transitive. Wikidata uses has_part as well. This supports keeping our definitions in GBO consistent. Wikidata is human constructed and based on Wikipedia's knowledge sources. We executed tailored queries for our domain and found that there is comparatively few knowledge content. In the area of processors we found various specific processors, but the list was small and exemplary rather than comprehensive and complete. Similar for the more abstract electrical components in general. Our approach with applying it to just three articles already outperformed the Wikidata's size by many times and this by considering a much less known and less represented domain of the automotive electrical system. Thus achieving a higher degree of expert domain knowledge and a significantly higher degree of relationship representations. It is of importance to note that this approach scales well and is easily verifiable via reasoning."}, {"title": "Results of generated Innovation Knowledge Graph", "content": "In our approach, generating knowledge graph TBox triples from text using GPT models is divided into two phases. The first phase involves extracting meaningful relations from a single sentence, focusing on the relationships between entities as shown in Figure 2. The second phase is about determining the class of entities within the triples.\nFor both phases, we have engineered specific prompts that can be fed to the GPT-4 model. This method allows for the initial extraction of triples, followed by the assignment of an appropriate class to these subject-predicate-object relationships. The prompts we used are as follows (excerpt):\nIn these prompts, we specify the schema format, emphasizing the roles of entities and relationships. We also provide examples of how relationships are defined between two classes of entities, such as \u201cinnovator \u2192 has developed \u2192 innovation\"."}, {"title": "Conclusion", "content": "In this paper, we have introduced an innovative knowledge graph construction process that focuses on generating customized information without being confined to a predefined corpus or specific knowledge source. Restricted to three papers to illustrate the framework, we showed a scalable approach, where reasoning time is not a bottleneck. In this way huge knowledge graphs can be constructed automatically. In comparison to alternative approaches, the output of our integrated framework is versatile, catering to both human interpretation and machine processing, making it a valuable resource for technology planners. We demonstrated that our approach can create high-quality topic maps and knowledge graphs that represent various areas of technology and innovation, such as automotive electronics. These visualizations and structures can assist in complex decision-making for the planning of cyber-physical systems. As part of our future endeavors, we aim to enhance the quality assurance aspects of the knowledge graph construction process, ensuring the accuracy of all relationships and classifications. Here correct subclassing of related classes is a challenge (taxonomic correctness), completeness of the knowledge generated."}]}