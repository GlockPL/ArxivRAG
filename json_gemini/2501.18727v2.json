{"title": "AUDIO EDITING FEATURES AS USER-CENTRIC PRIVACY\nDEFENSES AGAINST LARGE LANGUAGE MODEL (LLM)-BASED\nEMOTION INFERENCE ATTACKS", "authors": ["Mohd. Farhan Israk Soumik", "W.K.M Mithsara", "Abdur R. Shahid", "Ahmed Imteaj"], "abstract": "The rapid proliferation of speech-enabled technologies, including virtual assistants, video conferencing platforms, and wearable devices, has raised significant privacy concerns, particularly regarding the inference of sensitive emotional information from audio data. Existing privacy-preserving methods often compromise usability and security, limiting their adoption in practical scenarios. This paper introduces a novel, user-centric approach that leverages familiar audio editing techniques, specifically pitch and tempo manipulation, to protect emotional privacy without sacrificing usability. By analyzing popular audio editing applications on Android and iOS platforms, we identified these features as both widely available and usable. We rigorously evaluated their effectiveness against a threat model, considering adversarial attacks from diverse sources, including Deep Neural Networks (DNNs), Large Language Models (LLMs), and and reversibility testing. Our experiments, conducted on three distinct datasets, demonstrate that pitch and tempo manipulation effectively obfuscates emotional data. Additionally, we explore the design principles for lightweight, on-device implementation to ensure broad applicability across various devices and platforms.", "sections": [{"title": "1 Introduction", "content": "The interconnection between human emotion and audio is profound and intricate. Audio can evoke, amplify, and manipulate feelings through tones, rhythms, and frequencies [1]. Acoustic details and voice inflections convey emotions more effectively than words. The prevalence of audio in our daily lives has increased due to advancements in modern communication technologies, particularly high-speed internet and mobile networking like 4G and 5G. These technologies have enabled the transmission of superior audio quality and seamless transmission, which has become the cornerstone of platforms like video conferencing, podcasts, and Voice-over-IP(VoIP) services. Video conferencing and streaming platforms like Zoom, Google Meet, Twitch, and social media have contributed to this expansion. AI-powered Personalized Virtual Assistants(e.g Alexa, Siri) and Large Language Models (LLMs) are gaining ground, making voice commands and interactions more ubiquitous while increasing the significance of audio as a data format[2]. We are slowly embracing the world of virtual reality(VR), augmented reality(AR), and immersive technologies, which are considered Extended Reality [3][4]. Audio is a critical component for creating fully immersive experiences. Spatial audio, which lets users perceive sound directionally in 3D, has become crucial in AR and VR environments for more"}, {"title": "2 Related Work", "content": "Emotion Inference Attacks on Audio Data: Emotion inference attacks significantly threaten the privacy and security of systems that process emotional data. These attacks exploit machine learning models to infer sensitive, emotional states or related attributes without user consent, raising concerns in applications like virtual assistants, healthcare, and surveillance. Understanding their vulnerabilities to such attacks is critical as emotion recognition technologies become increasingly integrated into everyday systems. Most previous studies on attribute inference attacks in speech emotion recognition focus on scenarios where adversaries exploit model outputs to deduce private speaker attributes, such as identity, gender, or demographic traits, beyond the intended emotion classification[13, 22, 23]. Our study focuses on changing the tempo and pitch in audio by using uptempo based on usable privacy and eventually changing the emotion of the video. Judith et al. [24] propose a method that involves altering pitch in simple tones across different frequency ranges to measure people's emotional states by recording their body movements. Alica et al. [25] investigate the impact of tempo (measured in beats per minute) and rhythmic units (patterns like whole, half, eighth, and sixteenth notes) on listeners' emotional perceptions and mood regulation. It aims to understand how these factors induce positive or negative emotional states. Zhiyuan et al[26] introduces a novel class of adversarial attacks on voice-controlled systems (VCS), named SMACK, which manipulates semantic speech attributes (like prosody) to evade state-of-the-art defenses by combining genetic algorithms with gradient estimation for prosody optimization.\nEmotion Recognition Through Conventional Machine Learning Approaches and Large Language Models on Audio Data:s Mohd et al.[27] explore emotion recognition through audio and textual modalities using a Bi-LSTM model for audio data and a fine-tuned DistilRoBERTa model for textual data and they utilize the SAVEE, RAVDESS, and TESS datasets for analysis. Theresa et al. [28] propose a novel approach, MATS2L, to improve emotion recognition using physiological signals through self-supervised learning (SSL) and 1DCNN and using the WESAD dataset. Mandeep et al.[29, 30] explore emotion recognition by processing audio spectrograms and video frames using deep learning architectures, including a 3D CNN for video frames and a CNN+RNN for audio spectrograms. Their work utilizes the IEMOCAP dataset, which comprises 12 hours of audiovisual data from 10 actors expressing various emotions, such as anger, happiness, sadness, and neutrality. When considering conventional methods, machine learning techniques have reached a significant milestone in accurately identifying emotions, demonstrating remarkable advancements in the field[29, 30]. However, regarding utilizing large language models (LLMs) for emotion recognition, only a few studies have been conducted, leaving this area relatively under-explored [31, 32]. Zehui et al.[31] explore a novel approach to integrating speech characteristics into Large Language Models (LLMs) for emotion recognition. The study focuses on enhancing the multi-modal capabilities of LLMs without requiring architectural modifications by converting audio features such as volume, pitch, and speaking rate into natural language descriptions. The researchers evaluate various LLMs (e.g., LLaMA-2, LLaMA-3, Phi-3) using LoRA fine-tuning and IEMOCAP and MELD datasets. Yaoxun et al. [32] introduce a novel framework, SECap, for describing speech emotions using natural language, moving beyond traditional classification into predefined emotion categories. They utilize HuBERT as the audio encoder to extract robust speech features, while the Bridge-Net is employed to process these features, separating emotion-related acoustic information from content. LLaMA is then used to generate coherent and human-like emotion captions. The study utilizes the EMOSpeech dataset, which consists of 41.6 hours of annotated speech data, including corresponding emotion captions, transcriptions, and labels.\nPrivacy-Preserving Mechanisms to Defend Against Emotion Inference Attacks: Privacy-preserving mechanisms aim to mitigate these threats by sanitizing sensitive features, applying cryptographic techniques, or incorporating privacy-by-design principles. These methods ensure that emotion recognition systems retain functionality while protecting users' emotional data from unauthorized inference, balancing utility and privacy in sensitive applications. Haijiao et al. [13] address privacy challenges in federated learning (FL) for Speech Emotion Recognition by introducing a Gradient-level Hierarchical Differential Privacy (GHDP) framework by employing gradient normalization, clipping significant gradients, and adding hierarchical noise to early model layers during backpropagation to mitigate attribute inference attacks, such as gender prediction. Ranya et al. [14] explore privacy challenges in voice user interfaces (VUIs), which are widely used in smartphones, home assistants, and IoT devices. The focus is on defending against attribute inference attacks that exploit deep acoustic models to infer private attributes like gender or emotion. The authors propose a novel framework utilizing disentangled representation learning to filter sensitive information while retaining"}, {"title": "3 Threat Model", "content": "Assumption on Target Systems: We consider target systems that interact with speech data in various capacities. These include any system that records, stores, processes, or provides third-party access to speech data. Examples of such systems include virtual assistants, such as Alexa[33], Siri[34], and Google Assistant[35], which process user input to perform tasks or provide services, and large language model-based chatbots, such as ChatGPT[36] and Gemini[37], which may handle audio data as part of their multimodal interaction capabilities. Video conferencing platforms, such as Google Meet[38] and Zoom[39], that record or store audio for communication purposes are also included. Also, cloud-based audio applications, which store user data in cloud infrastructures, pose risks such as unauthorized access, data breaches, and malicious exploitation. Emerging technologies like Meta Smart Glasses[40], which integrate audio for real-time communication, transcription, or augmented reality, process and potentially store speech data, raising privacy concerns due to their pervasive nature and continuous data collection.\nAdversaries and Attack Vectors: The mentioned systems can be targeted by various adversaries with different motivations and methods. External hackers can breach cloud storage systems or intercept communication channels to extract speech data for unauthorized analysis or dissemination. Application developers also pose a potential risk when they have access to users' audio data. Developers may misuse this data for purposes beyond original intent for monetization through emotion recognition anslysis or sharing it with third parties without the explicit consent of users. Similarly, insider threats, such as colluding employees or individuals within an organization, with access to the sensitive data, may misuse their access for personal or financial gain or collaborate with external parties to exploit the data.\nAssumption on Attacker's Knowledge: We assume that the attackers posses significant knowledge and resources to compromise the privacy of speech data. This include, knowledge of the application used to manipulate the speech data, including its features or method used; expertise and resources to develop or access advanced AI models specifically designed for inference attacks; and access to state-of-the-art LLMs like GPT-40, which can analyze audio files to infer sensitive data."}, {"title": "4 The Proposed Framework", "content": "The core challenge in preserving privacy against the threat model lies in designing a mechanism that effectively protect sensitive emotional information while maintaining usability, adaptability, and compatibility across diverse systems and devices. This necessitates addressing several critical factors, including computational efficiency, user accessibility, and system-wide applicability. These goals include: (Lightweight On-Device Implementation) The privacy-preserving mechanism should be computationally lightweight so that it can be implemented directly on user devices without requiring extensive processing power or memory. This is critical for devices with limited resources, such as smartphones, smart glasses, or IoT devices. (Usability) The solution should prioritize ease of use which will"}, {"title": "4.2 The Usable Privacy-Preserving Mechanism", "content": "Our methodology to achieve the above mentioned design goals is rooted in usable security and privacy. Our objective is to develop privacy-preserving defense mechanisms based on features that users are already familiar with, which will enable intuitive adoption and minimizes the learning curve. In our first step of the research, we investigate popular audio editing features that are widely available and easily accessible to users. o accomplish this, we analyzed audio editing applications available on both the Google Play Store and Apple App Store. We found that pitch and tempo adjustments are among the most commonly available and widely used features in popular audio editing applications. Using this observation, we next explore whether the adjustment of pitch and tempo provides a privacy protection against sensitive information inference attacks from audio. We hypothesize that by integrating their manipulation into the design of privacy preserving mechanism, we can achieve the dual goals of usability and privacy preservation. Since users are already familiar with these features, they are more likely to adopt and use them effectively as a privacy-preserving mechanism. To determine whether pitch and tempo manipulation can genuinely provide privacy, we designed and conducted a series of experiments."}, {"title": "4.3 Trained Threat Models", "content": "As our potential attacker possesses the capability of using deep learning model including immensely powerful LLM models for the aforementioned inference attack, for attack simulation first we designed a 1-D CNN model from scratch and trained it using the RAVDESS, CREMA-D and three prominent dataset for emotion recognition. The reason for using 1-D CNN is that not only it is lightweight which increases the research scope in that direction but also in recent researches with audio and speech data it showed impressive performance as demonstrated in [14]. As expected, we achieved a significantly high 97% of test accuracy for the model. Fig 3 shows the architecture of our 1 dimensional convolutional neural network model."}, {"title": "5 Experiment and Results", "content": "Setup and Datasets: In this study, we analyzed popular audio editing applications, such as Uptempo, available on the Google Play Store and Apple App Store, to identify the common audio editing features offered by these platforms. Our objective is to assess the potential of these features as privacy-preserving tools, specifically in the context of protecting user privacy against speech emotion recognition systems. Within our threat model, we explore two primary approaches for executing attacks to evaluate the effectiveness of these features. The first method involves randomly altering pitch and tempo values while selecting male and female actors from three datasets to examine the impact on privacy.The second method involves systematically adjusting pitch and tempo across six distinct levels to create a range of variations. For pitch, the selected values are 0.0, +4, -4, +8, and -8, while tempo is varied at 60, 80, 100, 120, and 140. These combinations yield a total of 25 unique attack scenarios. For instance, when the pitch is set to 0.0, it is paired sequentially with tempo values of 60, 80, 100, 120, and 140. Similarly, for a pitch value of -4, the same tempo values are applied. This systematic approach is consistently implemented across all pitch variations to thoroughly examine their combined effects. We use three emotion speech datasets, specifically the RAVDESS [41], CREMA-D[42], and TESS[43] datasets. The details of these datasets are presented in Table 2."}, {"title": "5.1 Randomized Approach", "content": "To evaluate our proposed attack and defense model using the Up Tempo app, we randomly altered the pitch and tempo for different categories of emotion of the same user with different combinations that are fed to the attacker's models as inputs. To examine gender-related biases, we selected one male and one female actor from each of the aforementioned datasets and analyzed their pitch variations. Table 3 shows the result for Actor 23 of REVDESS dataset on randomly selected tempo and pitch values. The results clearly demonstrate that changed pitch and tempo is creating significant effects on attacker's inference model's performance in every case from 1-D CNNs to highly capable LLM model such as GPT-40. Similarly, Table 4 shows the outcome for actor 16 and we observe that just like the previous actors with the randomized change in tempo and pitch values, attacker both with their 1-D CNN and GPT-40 LLM model is failing to properly extract the real emotion from the data.\nTable 5 illustrates the impact of pitch and tempo variations on a male actor (Actor ID: 1001). As observed in the previous cases, the attacker consistently infers incorrect emotional information from the speech files of this actor. Following the approach taken with the RAVDESS dataset, where we transitioned from testing a male actor (Actor 23) to a female actor (Actor 16), we conducted similar tests on a female actor from the CREMA-D dataset. Table 7"}, {"title": "5.2 Structured Approach", "content": "While the goal of randomized approach was to see whether variations in pitch and combo can make the attacker extract incorrect emotions, the goals of this structured section is to provide a brief analysis to find out how this emotion change happens. Unlike the randomized approach, tempo and pitch values are not selected randomly this, but with regular interval. As previously mentioned, we have used pitch value of -8,-4,0,4,8 and tempo values of 60,80,100,120,140. For each of the emotion of category we made possible combinations with the designated pitch and tempo values and figured out the attacker model's response. Here, we will using previously discussed female actor (ID-16 RAVDESS) speech audio with Neutral emotion as a test case. Table 8 demonstrates the the pitch and tempo variations on actor 16's speech with neutral emotion. And it represent how different combinations of pitch and tempo influence emotion classification by two models: a 1-D CNN (Convolutional Neural Network) and GPT-40, a variant of GPT-4 optimized for this task. Both models occasionally agree on the emotional label (e.g., at 60 BPM and -8 pitch, both predict \"sad\"). Lower tempos (e.g., 60 BPM) and negative pitch shifts (-8, -4) often correlate with \"sad\" or \"disgust.\""}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we investigate the impact of modifying the pitch and tempo of speech audio data using user-accessible apps from the Google Play Store and iOS App Store. Our findings indicate that the evaluated attack models consistently failed to accurately infer emotions from the modified data, effectively providing privacy protection for sensitive user speech data. This demonstrates a practical, user-centric appraoch to protect privacy of sensitive data from speech audio in the face of adversaries with capabilities of state-of-the-art inference models, including multimodal large language models. The novelty of this work lies in its practical contribution to bridging the gap between usability and privacy, offering users a promising method to protect their speech data through familiar applications.\nFor future work, we plan to extend the scope of this research in several important directions. We plan to simulate more advanced attackers leveraging other LLMs, such as Flan-T5, Gemma-2B, and LLama, fine-tuned specifically for speech emotion recognition (SER) tasks. We also plan to integrate recent SER advancements, including universal audio representation models like Emotion2Vec2 [44] and Wave2Vec2 [45], into our attack framework, to facilitate the development of more complex and advanced attack scenarios to get better insights into the robustness and privacy implications of speech data modifications. We will also investigate the reversibility of pitch and tempo modifications using signal-processing techniques and how to mitigate that. Furthermore, we plan to assess privacy protection for additional attributes (e.g. identify or gender) beyond emotion inference."}]}