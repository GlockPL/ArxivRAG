{"title": "Efficient Quantum One-Class Support Vector Machines for Anomaly Detection Using Randomized Measurements and Variable Subsampling", "authors": ["Michael K\u00f6lle", "Afrae Ahouzi", "Pascal Debus", "Elif \u00c7etiner", "Robert M\u00fcller", "Dani\u00eblle Schuman", "Claudia Linnhoff-Popien"], "abstract": "Quantum one-class support vector machines leverage the advantage of quantum kernel methods for semi-supervised anomaly detection. However, their quadratic time complexity with respect to data size poses challenges when dealing with large datasets. In recent work, quantum randomized measurements kernels and variable subsampling were proposed, as two independent methods to address this problem. The former achieves higher average precision, but suffers from variance, while the latter achieves linear complexity to data size and has lower variance. The current work focuses instead on combining these two methods, along with rotated feature bagging, to achieve linear time complexity both to data size and to number of features. Despite their instability, the resulting models exhibit considerably higher performance and faster training and testing times.", "sections": [{"title": "1 Introduction", "content": "Anomaly detection algorithms ensure the proper functioning and security of various systems in today's increasingly digitalized landscape. They aim to identify observations or events that deviate from expected patterns within a dataset. They can detect irregularities such as unauthorized network access or unexpected machinery behavior, thus preventing potential data breaches, accidents, and financial losses. In the medical field, these algorithms enable earlier and more accurate diagnoses [5], reduce the incidence of medical errors, and facilitate real-time monitoring of patients' conditions by alerting healthcare providers to sudden changes [3]. Similarly, in electronic commerce, they are essential for enabling authorized customers to transact securely online while protecting financial service providers and enterprises from fraudulent activity. Despite the effectiveness of anomaly detection algorithms, they face challenges such as dealing with unbalanced or unlabelled datasets, high-dimensional and correlated data, and"}, {"title": "2 Preliminaries", "content": "In this section, we provide the foundational concepts and methodologies pertinent to our study. We begin with an exploration of One-Class Support Vector Machines, a key technique for unsupervised anomaly detection. Following this, we delve into Quantum Kernel Embedding, illustrating how quantum circuits can be leveraged to encode data into high-dimensional feature spaces for enhanced separability."}, {"title": "2.1 One-Class Support Vector Machines", "content": "One-class Support Vector Machines (OC-SVM) are popular models for unsupervised anomaly detection proposed by Sch\u00f6lkopf et al. [17]. While conventional SVMs find the maximum margin hyperplane to distinguish anomalies in the labeled data for two or more classes, OC-SVMs assume that the origin represents the anomalous class when labels are absent. In an OC-SVM, the model is trained to distinguish genuine data from anomalies by maximizing the margin b between the origin and the input data, rewarding the model for increasing this margin, and penalizing the points below the hyperplane (Fig. 1). A hyperparameter $\\nu \\in (0,1]$ is introduced, which regulates the ratio of points that remain on the negative side of the separator, which are then subsequently classified as anomalies.\n$\\min_{w,b} \\frac{1}{2} \\Vert w \\Vert^2 + \\frac{1}{\\nu N} \\sum_{i=1}^{N} \\max\\{b - w\\cdot \\Phi(x_i),0\\} - b \\qquad(1)$\nHowever, real-world data is often not linearly separable, presenting a challenge in identifying a maximum margin hyperplane. Feature maps $\\Phi : \\mathcal{X} \\rightarrow \\mathcal{F}$ address this problem by transforming the low-dimensional input data $\\mathcal{X}$ into a higher-dimensional feature space $\\mathcal{F}$ for improved separability. Due to the computational complexity of the direct computation of feature maps, kernel functions $k : \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ are an alternative approach. These functions determine the similarity of data points within the embedded feature space by using inner products computed directly in the original input space $\\mathcal{X}$, effectively and efficiently overcoming the need for explicit coordinate computations in the high-dimensional feature space $\\mathcal{F}$.\nA cornerstone of kernel-based learning algorithms is the efficiency of the computation of kernel functions $k(x_i,x_j)$, which can be concatenated into a Gram matrix $G$:\n$G_{ij} = \\langle \\Phi(x_i), \\Phi(x_j) \\rangle = k(x_i,x_j)\\qquad(2)$"}, {"title": "2.2 Quantum Kernel Embedding", "content": "Unlike classical feature maps, which transform data into a higher-dimensional feature space, quantum feature maps leverage parameterized quantum circuits to directly encode data as quantum states within the Hilbert space $\\mathcal{H}$. This encoding is achieved through the application of a data-dependent unitary quantum gate $U_{\\Phi(x)}$ acting on a quantum basis state mathematically expressed as $|\\Phi(x)\\rangle = U_{\\Phi(x)} |0\\rangle$. The IQP-like (Instantaneous Quantum Polynomial) feature map, which has the advantage of being hard to simulate classically [8], encodes a $d$-dimensional input $x_i$ into $d$ qubits. This process starts with applying a block of Hadamard gates, which prepares the qubits in a superposition state. Each input data feature is encoded into individual qubits by single qubit Z-rotations, ensuring that the information is accurately represented in the quantum state. $R_{ZZ}$ gates are applied between adjacent qubits to encode the interactions between the features, effectively linking the qubits and embedding the correlations into the data.\n$|\\Phi(x_i)\\rangle = U_Z(x_i)H^{\\otimes d}U_Z(x_i)H^{\\otimes d}|0^{\\otimes d}|,\\qquad(4)$\n$U_Z(x) = exp\\bigg(\\alpha \\sum_{j=1}^{d} x_jZ_j + \\sum_{j=1}^{d} \\sum_{j'=1}^{d} x_{ij}x_{ij'}Z_jZ_{j'}\\bigg)$\nThe parameter $\\lambda$ depends on the number of data reuploads and indicates how often the quantum feature map is applied to encode the classical data. Note that this parameter affects the kernel bandwidth similarly to the kernel bandwidth parameter $\\gamma$ [18]. Fig. 2a provides a visual representation, illustrating the IQP-like feature map that encodes a $d$-dimensional input $x_i$ into $d$ qubits and demonstrates the influence of the parameter $\\lambda$.\nOnce data has been encoded into quantum states, the measure of the similarity between these states is their fidelity. The fidelity can be expressed in terms of the density matrices, which have the form\n$F(x,x') = Tr(\\rho(x)\\rho(x')).\\qquad(5)$\nIn the case of pure quantum states, the fidelity is calculated as the square of the overlap of the states, which is given by\n$F(x,x') = |\\langle \\Phi(x')|\\Phi(x) \\rangle |^2.\\qquad(6)$"}, {"title": "3 Related Work", "content": "This work tackles the time complexity issue raised by Kyriienko et al. [11], which combines the one-class SVM with a complex computational kernel based on the IQP-like feature map (Fig. 2a) to achieve a 20% improvement in average precision compared to the traditional benchmark. It expands on our prior work [12], which explores two strategies to reduce time complexity in relation to data size: Variable Subsampling ensembles with Inversion Test kernels and single models using the Randomized Measurements kernel."}, {"title": "3.1 Quantum Anomaly Detection", "content": "Hybrid quantum-classical models have emerged as a leading approach in anomaly detection. For example, Sakhnenko et al. [15] has refined the hidden representation of an auto-encoder (AE) by integrating a parameterized quantum circuit (PQC) with its bottleneck. This approach transitions to an unsupervised model after training, replacing the decoder with an isolation forest, and assessing the performance across various datasets and PQC architectures. In parallel, Herr et al. [9] has adapted the classical AnoGAN [16] by incorporating a Wasserstein GAN, in which the generator is substituted with a hybrid quantum-classical neural network. The network is subsequently trained using a variational algorithm.\nIn contrast, methods like the QUBO-SVM mentioned in Wang et al. [19] transform the standard SVM optimization problem into a quadratic unconstrained binary optimization (QUBO) problem. This enables efficient solving using quantum annealing solvers, preserving the traditional SVM optimization framework while accelerating accurate prediction by effectively identifying kernel functions, thus allowing for practical real-time anomaly detection.\nRay et al. [13] explores hybrid ensembles that integrate bagging and stacking techniques using a combination of quantum and classical components, each playing a significant role in anomaly detection. The quantum components encompass various variable quantum circuit architectures, kernel-based SVMs, and quantum annealing-based SVMs. In contrast, the classical components consist of logistic regression, graph convolutional neural networks, and light gradient boosting models."}, {"title": "3.2 Efficient Gram Matrix Evaluation", "content": "Quantum kernel methods play a crucial role in various quantum machine learning applications, but they face significant computational challenges. To address these difficulties, two approaches have been proposed: (i) *Quantum-Friendly Classical Methods*, which reduce the number of kernel matrix elements that need to be evaluated, and (ii) *Quantum Native Methods*, which aim to minimize the overall number of measurements required and rely on classical post-processing, which is easily parallelizable or vectorizable.\nRandomized Measurement kernels, pioneered by Haug et al. in 2021 [7] and combined with hardware-efficient feature maps. They enable faster kernel measurement, but could only approximate Radial Basis Function (RBF) kernels on both synthetic and MNIST data. In contrast, the classical shadow method, proposed by Huang et al. in 2020 [10], employs a similar quantum protocol but diverges in classical post-processing. It provides classical state snapshots through the inversion of a quantum channel, often achieving reduced error in predicting the second R\u00e9nyi Entropy.\nVariable subsampling, which was first introduced by Aggarwal and Sathe in 2015 [2], and its advanced counterpart, variable subsampling with rotated bagging, present an efficient approach to ensemble training. The methods use different sample sizes and rotational orthogonal axis system projections to improve both computational efficiency and an adaptive ensemble model training strategy. They have been successfully tested with algorithms like the local outlier factor (LOF) models and the k-Nearest Neighbors algorithm.\nThe Distance-based Importance Sampling and Clustering (DISC) approach, proposed by Hajibabaee et al. [6], and Block Basis Factorization (BBF), introduced by"}, {"title": "4 Approach", "content": "Several methods for efficient gram matrix evaluation have been highlighted in the previous review of related work. In this context, we will concentrate on two specific methods that can be utilized for both symmetric training kernel matrices and asymmetric prediction kernel matrices. While the Classical Shadows and Block Basis Factorization techniques fulfill this requirement, we decide to investigate Randomized Measurements and Variable Subsampling methods due to their intuitive conceptual framework."}, {"title": "4.1 Randomized Measurements Kernel", "content": "This method is practically employed in kernel calculation for classification by Haug et al. [7] and is suggested for the quantum OC-SVM in Kyriienko et al. [11]. It tries to attain linear time complexity with respect to the data set size for the quantum kernel calculation by avoiding redundant measurements. This is achieved through acquiring measurements of the respective quantum feature maps of each individual data point in multiple random bases and subsequently aggregating them using classical post-processing. The method notably diminishes the required number of measurements, thereby alleviating the overall computational burden.\nThe concept of fidelity computation is based on treating the swap operator $S$ as a quantum twirling channel $\\Phi_{U,2}^{(2)}$ as described in Elben et al. [4]. Quantum twirling channels are frequently used in error correction. For instance, the application of a 2-fold local quantum twirling channel to arbitrary operator O is expressed as\n$\\Phi_{U,2}^{(2)}(O) = \\overline{(U^{\\otimes2})^{\\dagger}OU^{\\otimes2}},\\qquad(8)$\nwith $\\overline{...}$ denoting the average over the Haar random unitaries $U = \\bigotimes_{k=1}^{N}U_{k}$ where the unitaries $U_{k}$, applied to the $k \\in \\{1,...,N\\}$ qubit, are sampled from a unitary 2-design. A unitary $t$-design is a finite set of unitaries which approximates the properties of probability distributions over the Haar measure for all possible unitaries of degree less than t, ensuring uniform sampling across unitary matrices.\nThe authors [4] demonstrate that the average of the second-order cross-correlation of the randomized measurements' outcome probabilities $P_{U}(.)$ can be expressed as the expectation value of an operator $O$, which applies to a twirling channel and two copies of the quantum state $\\rho$.\n$\\sum_{s,s'} O_{s,s'} \\overline{P_{U}(s)P_{U}(s')} = Tr(\\Phi_{U,2}^{(2)}(O) \\rho \\otimes \\rho).\\qquad(9)$"}, {"title": "4.2 Variable Sampling Ensembles using Inversion Test Kernels", "content": "An ensemble technique called Variable Subsampling is recommended in [1] to address sensitivities in the one-class SVM, specifically regarding kernel choice and hyperparameter $\\nu$ values. Variable Subsampling ensembles, unlike bagging ensembles, not only select random subsets of the data for model training, but also ensure these subsamples are of different size. This implicitly enables sampling across parameter spaces, particularly those associated with data size, such as the expected anomaly ratio $\\nu$ in the one-class SVM.\nFor instance, if a variable subsampling ensemble consists of 3 OC-SVMs trained with $\\nu = 0.1$ and sample sizes of 53, 104, and 230, each ensemble component would have a different number of support vectors, leading to different decision boundaries. By combining these, it is possible to mitigate bias or variance in predictions."}, {"title": "4.3 Variable Subsampling Ensembles using Randomized Measurements Kernels", "content": "The Variable Subsampling method explained in the section above can also be used in combination with the quantum Randomized Measurements (RM) kernels. This can be done by passing each of the selected subsamples to an OC-SVM instance that uses the quantum Randomized Measurements kernel.\nThe aim of incorporating the two methods is that they can potentially reduce the high variance that the unmitigated Randomized Measurements models exhibit in [12]. This is under the premise that the method combines scores of multiple components, each trained with different samples. Similar to Variable Subsampling using the Inversion Test kernel, the method leads to a reduction in time complexity related to data size. However, it has exponential time complexity with respect to the number of features/qubits, comparable to the single OC-SVM model with the randomized measurements kernel."}, {"title": "4.4 Variable Subsampling Ensembles with Rotated Feature Bagging using Randomized Measurements Kernels", "content": "Variable Subsampling with Rotated Feature Bagging was proposed for classical models along with the standard Variable Subsampling method by [2]. It makes use of the premise that real data usually contains considerable correlations across different dimensions and can thus be represented in much lower dimensions without causing information loss.\nSince finding the optimal transformation of the data is not trivial, it is much simpler to sample random projections and average the resulting scores from the models trained on the projected data samples.\nTherefore, besides reducing the variance across the variously sized samples, the approach reduces the variance across projections of the data onto different lower-dimensional axis systems.\nIn addition to the steps of the standard variable subsampling method, the data sample $D_i$ of each component $i \\in \\{1,...,c\\}$ is projected into a lower-dimensional axis system before it is used in training. This random projection is accomplished by matrix multiplication with a random rotation matrix $E_i$. With an original subsample data matrix $D_i$ of the size $n_i \\times d$ and the random projection matrix $E_i$ of the size $d \\times r'$, the resulting sample data matrix $D'_i = D_i \\cdot E_i$ has size $n_i \\times r'$.\nThe matrix $D'_i$ is passed as training data for the component $i$. In this case, this component is an instance of a quantum OC-SVM with a Randomized Measurements kernel. The random projection matrix $E_i$ is saved to be eventually reused during testing to project the new data into the same low-dimensional axis system before it is passed to the component $i$ for scoring.\nThe random projection matrix $E_i$ is unique to each ensemble component. It is calculated by constructing a matrix $Y$ from elements that are sampled from a uniform distribution of values in the range [-1,1] and subsequently using the Gram-Schmidt process to obtain $r'$ mutually orthogonal basis columns of the matrix $Y$.\nThe dimensions of the new axis system are usually set to $r' = 2+ \\lfloor\\sqrt{d}\\rfloor$. This is because of the observation that real data's implicit dimensionality grows slower than $\\sqrt{d}$ in real datasets. The additional two dimensions are added to that number, as the method would otherwise not function for data that comes originally with 3 or fewer dimensions.\nThe main purpose of using Rotated Feature Bagging in our case is mainly to address the exponential time complexity to number of features/qubits resulting from the usage of the Randomized Measurements kernel without losing performance. This is a natural result of the reduction of the number of features/Qubits to $r' = 2+ \\lfloor\\sqrt{d}\\rfloor$. For instance, if the original dataset has 28 features, the components of the ensemble model will be trained with 5 features."}, {"title": "5 Experimental Setup", "content": "This section outlines the implementation details to ensure the reproducibility of the experiments. It discusses in detail the preliminary data preparation procedures, the different approaches of kernel computation, and the methods used to select unique hyperparameters.\n1. The initial set aims to analyze the performance, training and evaluation times related to the size of the dataset. Its primary goal is to understand how model performance evolves with increased amounts of data and to compare the computational effeciency provided by our approaches, as outlined in Section 4.\n2. In our second set of experiments, our goal is to investigate the impact of feature (or qubit) numbers on the performance and computation times of our approaches. We aim to ascertain whether there are any adverse effects on performance and to showcase the time complexity in function of the number of features/qubits.\nAll experiments have been performed with a variety of 15 seeds, ranging from 0 through 14."}, {"title": "5.1 Datasets", "content": "In our research, we analyze two distinct datasets, categorized based on their origin as either synthetic or real-world, to examine different methodologies. The synthetic dataset is only employed in the first set of experiments since its two-dimensional nature inherently restricts the exploration of a wide range of features.\nThe Synthetic Data is a two-dimensional, not linearly separable dataset developed through alterations made to a demonstration of OC-SVM from SKlearn* to generate training samples of various sizes. The test samples for the synthetic data consistently consist of 125 points, featuring an anomaly proportion 0.3.\nThe Credit Card Fraud Data * contains around 284,000 data points, with 492 classified as anomalous (class 1) across 31 features, 28 of which are anonymized features obtained through the application of Principal Component Analysis (PCA). We ommit the 'time' and 'amount' features and use the data in a non-time series manner. The test sets for this dataset also contain 125 points, with an anomaly ratio of 0.05."}, {"title": "5.2 Data Pre-processing", "content": "The quantum kernel measurement technique used and the choice of the dataset determine the pre-processing methods. The pre-processing of synthetic data occurs only with a randomized measurement kernel. In contrast, the treatment of real data varies based on the quantum kernel measurement technique employed.\nRadial Basis Function (RBF) Kernel: When employing the RBF, standard scaling was applied after splitting the data into training and test sets to ensure that all features had a mean of zero and a standard deviation of one. Following this, PCA was employed to reduce the data to the necessary number of features."}, {"title": "5.3 Baselines", "content": "In the initial series of experiments conducted, an attempt is made to replicate the findings associated with the OC-SVM, as outlined in the study by Kyriienko et al. [11] using the Credit Card Fraud dataset. These findings are subsequently employed as benchmarks for both quantum and classical analyses. Due to the need for more detailed information regarding their sampling methodology and the absence of explicit information on the sizes of their test sets, we opted to utilize uniform random sampling to generate our data sets. These sets consist of 500 points for training and 125 points for testing. Consistent with the original authors' approach, we trained the OC-SVM exclusively on non-anomalous data. However, the test set is selected to have a 0.05 ratio of anomalies. Every seed is associated with a distinct partitioning of the dataset. In the experiments investigating the impact of data, the number of features remains fixed at $d = 2$ for the synthetic data and $d = 6$ for the CC Fraud dataset, while the training data size varies. For the experiments involving various feature numbers, the data size remains constant at $n = 500$ data samples and the qubit/feature number changes."}, {"title": "5.4 Models and Parameter Selection", "content": "In this work, both classical and quantum adaptations of the OC-SVM were utilized, specifically employing the OneClassSVM from the SKlearn* library. For all classical approaches, the RBF kernel was employed with $\\gamma = \\frac{1}{N.Var(M)}$ and alongside a constant $\\nu$ value of 0.1.\nQuantum circuits are crucial for kernel calculations and are created using the qiskit library. They are simulated through qiskit_aer.QasmSimulator. \u0391 $\\lambda = 3$ data reuploading was used for all quantum feature maps, and inversion test kernel elements were determined using 1000 shots each.\nRandomized measurement circuits were realized with $r = 30$ measurement settings and subjected to $s = 9000$ shots each. They require efficient classical post-processing, including minimal embedded loops and prioritizing efficient matrix operations. The variable subsampling method, utilizing $c = \\lfloor\\sqrt{n}\\rfloor$ components (n representing the training set size) and a subsample size $n_i \\in [50, 100]$ to ensure scalable model performance,"}, {"title": "5.5 Performance Metrics for Imbalanced Datasets", "content": "In the context of evaluating the performance of anomaly detection models on data sets that are primarily imbalanced, it is essential to consider factors beyond the conventional measure of accuracy. This is because accuracy may not be the most comprehensive measure of a model's performance across diverse categories of data. As Aggarwal [1] highlights, metrics such as the F1 score and average precision are much more appropriate for this type of data. These metrics, which consider precision and recall, provide a more accurate assessment of a model's ability to identify anomalies within a highly imbalanced data set.\nF1 score is the harmonic mean of precision and recall, providing a balanced perspective on model performance with regard to false positives and false negatives. It is defined as follows:\n$F1 \\ score = 2.\\frac{Precision \\cdot Recall}{Precision + Recall}\\qquad(14)$\nAverage Precision meticulously quantifies a model's proficiency in anomaly detection across a spectrum of thresholds. This is achieved by calculating the area encompassed beneath the precision-recall curve, which provides a detailed assessment of the model's capacity to discriminate anomalous occurrences.\n$AP = \\sum_{k} [Recall(k) - Recall(k+1)] \\cdot Precision(k).\\qquad(15)$\nIn terms of average precision, a ratio equal to the data's anomaly ratio indicates a model with no anomaly detection capability. Conversely, a score of 1 signifies a flawless detector. Accordingly, the focal point of our analysis encloses average precision, augmented by an examination of supplementary metrics, namely precision, recall, and the F1 score, to provide a comprehensive evaluation."}, {"title": "6 Results", "content": "In the following section, we evaluate the different methods to lower the time complexity of our model while maintaining performance. We focus on the average precision, which allows us to assess the performance without taking into consideration the threshold used on the scores, but also reports the evolution of thresholded metrics like the F1 score, the recall, and the precision."}, {"title": "6.1 Performance Relative to Data Size", "content": "In Fig. 4 columns (a, b) show the performance metrics for the synthetic dataset and the Credit Card Fraud Dataset in relationship to the data size, respectively."}, {"title": "6.2 Performance Relative to Qubit Number / Feature Number", "content": "The Quantum Inversion Test OC-SVMs (IT) achieve a negligible improvement in average precision compared to the classical radial basis function (RBF) models. This contradicts the results of Kyriienko et al. [11]. This discrepancy in the results might stem from differences in the number of runs, the seeds used for the experiments, and the data selection method, as these details were not disclosed in their paper.\nThe Quantum Randomized Measurements OC-SVMs (RM) performance seems to decline when increasing the qubits. When error mitigation (Eq. 13) is applied during the gram matrix calculation, the results are close to the classical RBF and the quantum inversion test models. But if the error mitigation is skipped, the average precision is higher than all the previous methods. The variance of the method noticeably exeeds the previous methods, especially if the unmitigated version is selected, and does not reduce as fast when increasing the number of features/qubits."}, {"title": "6.3 Time Complexity Relative to Data Size", "content": "Fig. 5 (a - b) display the training and testing times for our models in seconds in relationship to the data size for the synthetic dataset and the Credit Card dataset. We obtain consistent results for both datasets.\nThe Quantum Inversion Test OC-SVMs (IT) evolves quadratically with the training data size during training and linearly during testing.\nThe Quantum Randomized Measurements OC-SVMs (RM) substantially shortens the training times, attaining an approximate 50% reduction in training time whilst tripling the data size. However, the time complexity based on data size remains quadratic. It is worth noting that the error mitigation in Eq. 13 does not seem to significantly increase the kernel calculation time.\nThe Variable Subsampling ensembles using the quantum Inversion Test kernel (VS) training times coincide with those of the other variants Variable Subsampling methods. The method extensively reduces the training times in comparison to the usage of a single Inversion Test OC-SVM, and successfully achieves linear time complexity to data size.\nThe Variable Subsampling ensembles using the quantum Randomized Measurements kernel (VS-RM) achieves linear complexity to data size and even higher reductions in time, in comparison to the VS with the Iversion Test kernel.\nThe Variable Subsampling with Rotated Feature Bagging ensembles using the Randomized Measurements kernel (VS-RFB-RM) have training times which are similar to the other variable subsampling variants, but are especially more effective in diminishing testing times. Notably, we obtain a nearly 90% reduction in training times and an 80% reduction in testing times when using triple the amount of training data. For this method, the time complexity to data size for this method during both training and testing is linear."}, {"title": "6.4 Time Complexity Relative to Qubit Number / Feature Number", "content": "Fig. 5(c) exhibits the training and testing times in function of the number of features/qubits of our models for the Credit Card dataset.\nThe Quantum Inversion Test OC-SVMs' (IT) time complexity during training seems to evolve quadratically with the number of qubits/features.\nThe Quantum Randomized Measurements OC-SVMs (RM), in accordance with the results in [7], appear to have exponential time complexity in function of the number of qubits/features. This causes the method to become unusable for high-dimensional datasets without prior usage of dimensionality reduction techniques like PCA.\nThe Variable Subsampling ensembles using the quantum Inversion Test kernel (VS) display a quadratic time complexity in relationship to the number of qubits/features, due to the usage of the Inversion Test. The method leads to a drastic reduction in training times, although the the effect on testing times is less pronounced.\nThe Variable Subsampling ensembles using the quantum Randomized Measurements kernel (VS-RM) deminish training times more considerably than testing times. The time complexity of this approach has an exponential relationship to the number of qubits/features, similar to the Randomized Measurements kernel method.\nThe Variable Subsampling with Rotated Feature Bagging ensembles using the Randomized Measurements kernel (VS-RFB-RM) successfully manage to mitigate the exponential relationship of time to number of qubits arising from the usage of the Randomized Measurements kernel, resulting in significant reductions of training and testing times. The reason for this reduction is that the ensemble components are trained with $2 + \\lfloor\\sqrt{d}\\rfloor$ dimensional rotated features instead of the original principal components d. The usage of rotated feature bagging allows the application of Randomized measurements kernels in use-cases where the data is high-dimensional. This combined with the variable subsample sizes, produces the fastest method to train and test among all the quantum methods we try."}, {"title": "7 Conclusion", "content": "Our study examines multiple approaches for efficient quantum anomaly detection using one-class SVMs. The first approach is based on the classical Variable Subsampling ensemble method, while the second utilizes the quantum Randomized Measurements kernel calculation method. Our results demonstrate that the Variable Subsampling method can effectively be used to train OC-SVM-based ensembles with the quantum Inversion Test kernel in linear time complexity to data size and quadratic complexity to the number of features. The method leads to a drastic acceleration in training and testing times, without compromising the performance of the models, provided that the scoring threshold is adjusted. Alternatively, the unmitigated Randomized Measurements kernel seems to attain higher average precision than the Inversion Test and RBF kernel-based methods for the Credit Card dataset, even though this dataset exhibits higher dimensionality and imbalance than the synthetic dataset. This method however produces unstable models, evident in the high variance. Furthermore, it comes with exponential time complexity to the number of qubits/features and a quadratic time dependence to data size.\nThese findings motivate a novel approach that integrates both methods, which we test in the same experimental settings. We create new Variable Subsampling ensembles using OC-SVMs trained with the Randomized Measurements kernel. This method surprisingly leads to an increase in performance along with further improvements in training and evaluation times, but still suffers from high variance and exponential time complexity to the number of qubits. To overcome these drawbacks, we develop a further method, variable subsampling with rotated feature bagging in combination with the Randomized Measurements kernel OC-SVMs. The resulting models, not only have linear time complexities to both the data size and the number of features/qubits, but also achieve higher average precision than the previous models. However, their variance remains very high, which can be attributed to the usage of the Randomized Measurements kernel, the maximum combination function for the scores, and a potentially too low number of ensemble components and maximum subsample size. These promising results for the time complexity open the door for more future research directions. These include refining the Variable Subsampling methods by using a higher number of components and maximal subsample sizes, thereby directing the focus toward the methods' performance prospects or trying out the proposed methods with alternative feature maps, including learnable ones. Importance sampling can be employed to improve the Randomized Measurements kernel in the single OC-SVMs, as well as in the Variable Subsampling ensembles. Furthermore, the classical shadow method, which showcases lower average error compared to randomized measurements, can be assessed as an alternative to the Randomized Measurements method in the models above. Finally, it could be interesting to evaluate the promising DISC and Block Basis Factorization methods against the ones proposed in this work."}]}