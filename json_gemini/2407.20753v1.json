{"title": "Efficient Quantum One-Class Support Vector Machines\nfor Anomaly Detection Using Randomized\nMeasurements and Variable Subsampling", "authors": ["Michael K\u00f6lle", "Afrae Ahouzi", "Pascal Debus", "Elif \u00c7etiner", "Robert M\u00fcller", "Dani\u00eblle Schuman", "Claudia Linnhoff-Popien"], "abstract": "Quantum one-class support vector machines leverage the advantage\nof quantum kernel methods for semi-supervised anomaly detection. However,\ntheir quadratic time complexity with respect to data size poses challenges when\ndealing with large datasets. In recent work, quantum randomized measurements\nkernels and variable subsampling were proposed, as two independent methods to\naddress this problem. The former achieves higher average precision, but suffers\nfrom variance, while the latter achieves linear complexity to data size and has\nlower variance. The current work focuses instead on combining these two meth-\nods, along with rotated feature bagging, to achieve linear time complexity both to\ndata size and to number of features. Despite their instability, the resulting models\nexhibit considerably higher performance and faster training and testing times.", "sections": [{"title": "1 Introduction", "content": "Anomaly detection algorithms ensure the proper functioning and security of various\nsystems in today's increasingly digitalized landscape. They aim to identify observa-\ntions or events that deviate from expected patterns within a dataset. They can detect\nirregularities such as unauthorized network access or unexpected machinery behavior,\nthus preventing potential data breaches, accidents, and financial losses. In the medical\nfield, these algorithms enable earlier and more accurate diagnoses [5], reduce the inci-\ndence of medical errors, and facilitate real-time monitoring of patients' conditions by\nalerting healthcare providers to sudden changes [3]. Similarly, in electronic commerce,\nthey are essential for enabling authorized customers to transact securely online while\nprotecting financial service providers and enterprises from fraudulent activity. Despite\nthe effectiveness of anomaly detection algorithms, they face challenges such as deal-\ning with unbalanced or unlabelled datasets, high-dimensional and correlated data, and"}, {"title": "2 Preliminaries", "content": "In this section, we provide the foundational concepts and methodologies pertinent to\nour study. We begin with an exploration of One-Class Support Vector Machines, a key\ntechnique for unsupervised anomaly detection. Following this, we delve into Quantum\nKernel Embedding, illustrating how quantum circuits can be leveraged to encode data\ninto high-dimensional feature spaces for enhanced separability."}, {"title": "2.1 One-Class Support Vector Machines", "content": "One-class Support Vector Machines (OC-SVM) are popular models for unsupervised\nanomaly detection proposed by Sch\u00f6lkopf et al. [17]. While conventional SVMs find\nthe maximum margin hyperplane to distinguish anomalies in the labeled data for two\nor more classes, OC-SVMs assume that the origin represents the anomalous class when\nlabels are absent. In an OC-SVM, the model is trained to distinguish genuine data from\nanomalies by maximizing the margin b between the origin and the input data, rewarding\nthe model for increasing this margin, and penalizing the points below the hyperplane\n(Fig. 1). A hyperparameter $v \u2208 (0,1]$ is introduced, which regulates the ratio of points\nthat remain on the negative side of the separator, which are then subsequently classified\nas anomalies.\n$\\min_{\\mathbf{w},b} \\frac{1}{2} ||\\mathbf{w}||^2 + \\frac{1}{\\nu N} \\sum_{i=1}^{N} \\max{b - \\mathbf{w} \\cdot \\Phi(x_i), 0} - b$                                                                                                                                                                                                                                                                                                    (1)\nHowever, real-world data is often not linearly separable, presenting a challenge in\nidentifying a maximum margin hyperplane. Feature maps $\\Phi : X \u2192 F$ address this prob-\nlem by transforming the low-dimensional input data $X$ into a higher-dimensional fea-\nture space $F$ for improved separability. Due to the computational complexity of the\ndirect computation of feature maps, kernel functions $k : X \u00d7 X \u2192 R$ are an alternative\napproach. These functions determine the similarity of data points within the embedded\nfeature space by using inner products computed directly in the original input space $X$,\neffectively and efficiently overcoming the need for explicit coordinate computations in\nthe high-dimensional feature space $F$.\nA cornerstone of kernel-based learning algorithms is the efficiency of the computa-\ntion of kernel functions $k(x_i,x_j)$, which can be concatenated into a Gram matrix $G$:\n$G_{ij} = \\langle \\Phi(x_i), \\Phi(x_j) \\rangle = k(x_i,x_j)$                                                                                                                                                                                                                                                                         (2)"}, {"title": "2.2 Quantum Kernel Embedding", "content": "Unlike classical feature maps, which transform data into a higher-dimensional feature\nspace, quantum feature maps leverage parameterized quantum circuits to directly en-\ncode data as quantum states within the Hilbert space $H$. This encoding is achieved\nthrough the application of a data-dependent unitary quantum gate $U_{\\Phi}(x)$ acting on a\nquantum basis state mathematically expressed as $|\\Phi(x)\\rangle = U_{\\Phi}(x) |0\\rangle$. The IQP-like (In-\nstantaneous Quantum Polynomial) feature map, which has the advantage of being hard\nto simulate classically [8], encodes a d-dimensional input $x_i$ into $d$ qubits. This process\nstarts with applying a block of Hadamard gates, which prepares the qubits in a super-\nposition state. Each input data feature is encoded into individual qubits by single qubit\nZ-rotations, ensuring that the information is accurately represented in the quantum state.\n$R_{ZZ}$ gates are applied between adjacent qubits to encode the interactions between the\nfeatures, effectively linking the qubits and embedding the correlations into the data.\n$|\\Phi(x_i)\\rangle = U_Z(x_i)H^{\\otimes d}U_Z(x_i)H^{\\otimes d}|0^d\\rangle,$\n$U_Z(x) = \\exp\\left(i \\alpha \\left(\\sum_{j=1}^{d} x_jZ_j + \\sum_{j=1}^{d} \\sum_{j'=1}^{d} x_jx_{j'}Z_jZ_{j'}\\right)\\right)$                                                                                                                                                                                                                                                                                       (4)\nThe parameter $\u03b1$ depends on the number of data reuploads and indicates how often\nthe quantum feature map is applied to encode the classical data. Note that this parameter\naffects the kernel bandwidth similarly to the kernel bandwidth parameter \u03b3 [18]. Fig. 2a\nprovides a visual representation, illustrating the IQP-like feature map that encodes a\n$d$-dimensional input $x_i$ into $d$ qubits and demonstrates the influence of the parameter $\u03bb$.\nOnce data has been encoded into quantum states, the measure of the similarity be-\ntween these states is their fidelity. The fidelity can be expressed in terms of the density\nmatrices, which have the form\n$F(x,x') = Tr(\\rho(x)\\rho(x'))$.                                                                                                                                                                                                                                                                         (5)\nIn the case of pure quantum states, the fidelity is calculated as the square of the overlap\nof the states, which is given by\n$F(x,x') = |\\langle \\Phi(x')|\\Phi(x) \\rangle |^2.$                                                                                                                                                                                                                                                                      (6)"}, {"title": "3 Related Work", "content": "This work tackles the time complexity issue raised by Kyriienko et al. [11], which com-\nbines the one-class SVM with a complex computational kernel based on the IQP-like\nfeature map (Fig. 2a) to achieve a 20% improvement in average precision compared to\nthe traditional benchmark. It expands on our prior work [12], which explores two strate-\ngies to reduce time complexity in relation to data size: Variable Subsampling ensembles\nwith Inversion Test kernels and single models using the Randomized Measurements\nkernel."}, {"title": "3.1 Quantum Anomaly Detection", "content": "Hybrid quantum-classical models have emerged as a leading approach in anomaly\ndetection. For example, Sakhnenko et al. [15] has refined the hidden representation of\nan auto-encoder (AE) by integrating a parameterized quantum circuit (PQC) with its\nbottleneck. This approach transitions to an unsupervised model after training, replac-\ning the decoder with an isolation forest, and assessing the performance across various\ndatasets and PQC architectures. In parallel, Herr et al. [9] has adapted the classical\nAnoGAN [16] by incorporating a Wasserstein GAN, in which the generator is sub-\nstituted with a hybrid quantum-classical neural network. The network is subsequently\ntrained using a variational algorithm.\nIn contrast, methods like the QUBO-SVM mentioned in Wang et al. [19] trans-\nform the standard SVM optimization problem into a quadratic unconstrained binary\noptimization (QUBO) problem. This enables efficient solving using quantum annealing\nsolvers, preserving the traditional SVM optimization framework while accelerating ac-\ncurate prediction by effectively identifying kernel functions, thus allowing for practical\nreal-time anomaly detection.\nRay et al. [13] explores hybrid ensembles that integrate bagging and stacking tech-\nniques using a combination of quantum and classical components, each playing a sig-\nnificant role in anomaly detection. The quantum components encompass various vari-\nable quantum circuit architectures, kernel-based SVMs, and quantum annealing-based\nSVMs. In contrast, the classical components consist of logistic regression, graph con-\nvolutional neural networks, and light gradient boosting models."}, {"title": "3.2 Efficient Gram Matrix Evaluation", "content": "Quantum kernel methods play a crucial role in various quantum machine learning appli-\ncations, but they face significant computational challenges. To address these difficulties,\ntwo approaches have been proposed: (i) Quantum-Friendly Classical Methods, which\nreduce the number of kernel matrix elements that need to be evaluated, and (ii) Quantum\nNative Methods, which aim to minimize the overall number of measurements required\nand rely on classical post-processing, which is easily parallelizable or vectorizable.\nRandomized Measurement kernels, pioneered by Haug et al. in 2021 [7] and com-\nbined with hardware-efficient feature maps. They enable faster kernel measurement,\nbut could only approximate Radial Basis Function (RBF) kernels on both synthetic and\nMNIST data. In contrast, the classical shadow method, proposed by Huang et al. in 2020\n[10], employs a similar quantum protocol but diverges in classical post-processing. It\nprovides classical state snapshots through the inversion of a quantum channel, often\nachieving reduced error in predicting the second R\u00e9nyi Entropy.\nVariable subsampling, which was first introduced by Aggarwal and Sathe in 2015\n[2], and its advanced counterpart, variable subsampling with rotated bagging, present\nan efficient approach to ensemble training. The methods use different sample sizes and\nrotational orthogonal axis system projections to improve both computational efficiency\nand an adaptive ensemble model training strategy. They have been successfully tested\nwith algorithms like the local outlier factor (LOF) models and the k-Nearest Neighbors\nalgorithm."}, {"title": "4 Approach", "content": "Several methods for efficient gram matrix evaluation have been highlighted in the pre-\nvious review of related work. In this context, we will concentrate on two specific meth-\nods that can be utilized for both symmetric training kernel matrices and asymmetric\nprediction kernel matrices. While the Classical Shadows and Block Basis Factorization\ntechniques fulfill this requirement, we decide to investigate Randomized Measurements\nand Variable Subsampling methods due to their intuitive conceptual framework."}, {"title": "4.1 Randomized Measurements Kernel", "content": "This method is practically employed in kernel calculation for classification by Haug et\nal. [7] and is suggested for the quantum OC-SVM in Kyriienko et al. [11]. It tries to\nattain linear time complexity with respect to the data set size for the quantum kernel cal-\nculation by avoiding redundant measurements. This is achieved through acquiring mea-\nsurements of the respective quantum feature maps of each individual data point in mul-\ntiple random bases and subsequently aggregating them using classical post-processing.\nThe method notably diminishes the required number of measurements, thereby allevi-\nating the overall computational burden.\nThe concept of fidelity computation is based on treating the swap operator $S$ as\na quantum twirling channel $\\Phi_{\\mathcal{U}}^{(2)}$, as described in Elben et al. [4]. Quantum twirling\nchannels are frequently used in error correction. For instance, the application of a 2-\nfold local quantum twirling channel to arbitrary operator $O$ is expressed as\n$\\Phi_{\\mathcal{U}}^{(2)}(O) = \\mathbb{E}_{U} \\left[ (U^{\\otimes 2})^{\\dagger} O U^{\\otimes 2} \\right],$                                                                                                                                                                                                                                                                                 (8)\nwith $\\mathbb{E}_U [\u00b7 ]$ denoting the average over the Haar random unitaries $U = \\otimes_{k=1}^{N} U_k$ where\nthe unitaries $U_k$, applied to the k \u2208 {1,...,N} qubit, are sampled from a unitary 2-\ndesign. A unitary t-design is a finite set of unitaries which approximates the properties\nof probability distributions over the Haar measure for all possible unitaries of degree\nless than t, ensuring uniform sampling across unitary matrices.\nThe authors [4] demonstrate that the average of the second-order cross-correlation\nof the randomized measurements' outcome probabilities $P_U(\u00b7)$ can be expressed as the\nexpectation value of an operator $O$, which applies to a twirling channel and two copies\nof the quantum state $\u03c1$.\n$\\sum_{s, s'} \\mathbb{E}_{U} [P_U(s)P_U(s')] = Tr \\left[ \\Phi_{\\mathcal{U}}^{(2)}(O) \\rho \\otimes \\rho \\right].$                                                                                                                                                                                                                                                                         (9)"}, {"title": "4.2 Variable Sampling Ensembles using Inversion Test Kernels", "content": "An ensemble technique called Variable Subsampling is recommended in [1] to address\nsensitivities in the one-class SVM, specifically regarding kernel choice and hyperpa-\nrameter v values. Variable Subsampling ensembles, unlike bagging ensembles, not only\nselect random subsets of the data for model training, but also ensure these subsamples\nare of different size. This implicitly enables sampling across parameter spaces, par-\nticularly those associated with data size, such as the expected anomaly ratio v in the\none-class SVM.\nFor instance, if a variable subsampling ensemble consists of 3 OC-SVMs trained\nwith v = 0.1 and sample sizes of 53, 104, and 230, each ensemble component would\nhave a different number of support vectors, leading to different decision boundaries. By\ncombining these, it is possible to mitigate bias or variance in predictions."}, {"title": "4.3 Variable Subsampling Ensembles using Randomized Measurements Kernels", "content": "The Variable Subsampling method explained in the section above can also be used in\ncombination with the quantum Randomized Measurements (RM) kernels. This can be\ndone by passing each of the selected subsamples to an OC-SVM instance that uses the\nquantum Randomized Measurements kernel.\nThe aim of incorporating the two methods is that they can potentially reduce the\nhigh variance that the unmitigated Randomized Measurements models exhibit in [12].\nThis is under the premise that the method combines scores of multiple components,\neach trained with different samples. Similar to Variable Subsampling using the Inver-\nsion Test kernel, the method leads to a reduction in time complexity related to data\nsize. However, it has exponential time complexity with respect to the number of fea-\ntures/qubits, comparable to the single OC-SVM model with the randomized measure-\nments kernel."}, {"title": "4.4 Variable Subsampling Ensembles with Rotated Feature Bagging using\nRandomized Measurements Kernels", "content": "Variable Subsampling with Rotated Feature Bagging was proposed for classical mod-\nels along with the standard Variable Subsampling method by [2]. It makes use of the\npremise that real data usually contains considerable correlations across different dimen-\nsions and can thus be represented in much lower dimensions without causing informa-\ntion loss.\nSince finding the optimal transformation of the data is not trivial, it is much simpler\nto sample random projections and average the resulting scores from the models trained\non the projected data samples.\nTherefore, besides reducing the variance across the variously sized samples, the ap-\nproach reduces the variance across projections of the data onto different lower-dimensional\naxis systems.\nIn addition to the steps of the standard variable subsampling method, the data sam-\nple $D_i$ of each component $i \u2208 {1,...,c}$ is projected into a lower-dimensional axis sys-\ntem before it is used in training. This random projection is accomplished by matrix\nmultiplication with a random rotation matrix $E_i$. With an original subsample data ma-\ntrix $D_i$ of the size $n_i \u00d7 d$ and the random projection matrix $E_i$ of the size $d \u00d7 r'$, the\nresulting sample data matrix $D'_i = D_i \u00b7 E_i$ has size $n_i \u00d7 r'$.\nThe matrix $D'_i$ is passed as training data for the component $i$. In this case, this com-\nponent is an instance of a quantum OC-SVM with a Randomized Measurements ker-\nnel. The random projection matrix $E_i$ is saved to be eventually reused during testing to\nproject the new data into the same low-dimensional axis system before it is passed to\nthe component $i$ for scoring.\nThe random projection matrix $E_i$ is unique to each ensemble component. It is cal-\nculated by constructing a matrix $Y$ from elements that are sampled from a uniform\ndistribution of values in the range [-1,1] and subsequently using the Gram-Schmidt\nprocess to obtain $r'$ mutually orthogonal basis columns of the matrix $Y$.\nThe dimensions of the new axis system are usually set to $r' = 2+ [\\sqrt{d}]$. This is\nbecause of the observation that real data's implicit dimensionality grows slower than\n$\\sqrt{d}$ in real datasets. The additional two dimensions are added to that number, as the\nmethod would otherwise not function for data that comes originally with 3 or fewer\ndimensions.\nThe main purpose of using Rotated Feature Bagging in our case is mainly to address\nthe exponential time complexity to number of features/qubits resulting from the usage\nof the Randomized Measurements kernel without losing performance. This is a natural\nresult of the reduction of the number of features/Qubits to $r' = 2+ [\\sqrt{d}]$. For instance,\nif the original dataset has 28 features, the components of the ensemble model will be\ntrained with 5 features."}, {"title": "5 Experimental Setup", "content": "This section outlines the implementation details to ensure the reproducibility of the\nexperiments. It discusses in detail the preliminary data preparation procedures, the dif-"}, {"title": "5.1 Datasets", "content": "In our research, we analyze two distinct datasets, categorized based on their origin\nas either synthetic or real-world, to examine different methodologies. The synthetic\ndataset is only employed in the first set of experiments since its two-dimensional nature\ninherently restricts the exploration of a wide range of features.\nThe Synthetic Data is a two-dimensional, not linearly separable dataset developed\nthrough alterations made to a demonstration of OC-SVM from SKlearn to generate\ntraining samples of various sizes. The test samples for the synthetic data consistently\nconsist of 125 points, featuring an anomaly proportion 0.3.\nThe Credit Card Fraud Data contains around 284,000 data points, with 492 clas-\nsified as anomalous (class 1) across 31 features, 28 of which are anonymized features\nobtained through the application of Principal Component Analysis (PCA). We ommit\nthe 'time' and 'amount' features and use the data in a non-time series manner. The test\nsets for this dataset also contain 125 points, with an anomaly ratio of 0.05."}, {"title": "5.2 Data Pre-processing", "content": "The quantum kernel measurement technique used and the choice of the dataset deter-\nmine the pre-processing methods. The pre-processing of synthetic data occurs only with\na randomized measurement kernel. In contrast, the treatment of real data varies based\non the quantum kernel measurement technique employed.\nRadial Basis Function (RBF) Kernel: When employing the RBF, standard scaling was\napplied after splitting the data into training and test sets to ensure that all features had\na mean of zero and a standard deviation of one. Following this, PCA was employed to\nreduce the data to the necessary number of features."}, {"title": "5.3 Baselines", "content": "In the initial series of experiments conducted, an attempt is made to replicate the find-\nings associated with the OC-SVM, as outlined in the study by Kyriienko et al. [11] us-\ning the Credit Card Fraud dataset. These findings are subsequently employed as bench-\nmarks for both quantum and classical analyses. Due to the need for more detailed infor-\nmation regarding their sampling methodology and the absence of explicit information\non the sizes of their test sets, we opted to utilize uniform random sampling to generate\nour data sets. These sets consist of 500 points for training and 125 points for testing.\nConsistent with the original authors' approach, we trained the OC-SVM exclusively on\nnon-anomalous data. However, the test set is selected to have a 0.05 ratio of anomalies.\nEvery seed is associated with a distinct partitioning of the dataset. In the experi-\nments investigating the impact of data, the number of features remains fixed at d = 2\nfor the synthetic data and d = 6 for the CC Fraud dataset, while the training data size\nvaries. For the experiments involving various feature numbers, the data size remains\nconstant at n = 500 data samples and the qubit/feature number changes."}, {"title": "5.4 Models and Parameter Selection", "content": "In this work, both classical and quantum adaptations of the OC-SVM were utilized,\nspecifically employing the OneClassSVM from the SKlearn library. For all classical\napproaches, the RBF kernel was employed with $\u03b3 = \\frac{1}{N.Var(M)}$ and alongside a constant\nv value of 0.1.\nQuantum circuits are crucial for kernel calculations and are created using the qiskit\nlibrary. They are simulated through qiskit_aer.QasmSimulator. \u0391 \u03bb = 3 data reu-\nploading was used for all quantum feature maps, and inversion test kernel elements\nwere determined using 1000 shots each.\nRandomized measurement circuits were realized with r = 30 measurement settings\nand subjected to s = 9000 shots each. They require efficient classical post-processing,\nincluding minimal embedded loops and prioritizing efficient matrix operations. The\nvariable subsampling method, utilizing c = [100\u300dcomponents (n representing the train-\ning set size) and a subsample size $n_i \u2208 [50, 100]$ to ensure scalable model performance,"}, {"title": "5.5 Performance Metrics for Imbalanced Datasets", "content": "In the context of evaluating the performance of anomaly detection models on data sets\nthat are primarily imbalanced, it is essential to consider factors beyond the conventional\nmeasure of accuracy. This is because accuracy may not be the most comprehensive\nmeasure of a model's performance across diverse categories of data. As Aggarwal [1]\nhighlights, metrics such as the F1 score and average precision are much more appro-\npriate for this type of data. These metrics, which consider precision and recall, provide\na more accurate assessment of a model's ability to identify anomalies within a highly\nimbalanced data set.\nF1 score is the harmonic mean of precision and recall, providing a balanced perspective\non model performance with regard to false positives and false negatives. It is defined as\nfollows:\n$F1 score = 2.\\frac{Precision \\cdot Recall}{Precision + Recall}$                                                                                                                                                                                                                                                                                                                                                                 (14)\nAverage Precision meticulously quantifies a model's proficiency in anomaly detection\nacross a spectrum of thresholds. This is achieved by calculating the area encompassed\nbeneath the precision-recall curve, which provides a detailed assessment of the model's\ncapacity to discriminate anomalous occurrences.\n$AP = \\sum_{k} [Recall(k) \u2013 Recall(k+1)] \\cdot Precision(k).$                                                                                                                                                                                                                                                                                                                                                 (15)\nIn terms of average precision, a ratio equal to the data's anomaly ratio indicates a model\nwith no anomaly detection capability. Conversely, a score of 1 signifies a flawless detec-\ntor. Accordingly, the focal point of our analysis encloses average precision, augmented\nby an examination of supplementary metrics, namely precision, recall, and the F1 score,\nto provide a comprehensive evaluation."}, {"title": "6 Results", "content": "In the following section, we evaluate the different methods to lower the time complexity\nof our model while maintaining performance. We focus on the average precision, which\nallows us to assess the performance without taking into consideration the threshold used\non the scores, but also reports the evolution of thresholded metrics like the F1 score, the\nrecall, and the precision."}, {"title": "6.1 Performance Relative to Data Size", "content": "In Fig. 4 columns (a, b) show the performance metrics for the synthetic dataset and the\nCredit Card Fraud Dataset in relationship to the data size, respectively."}, {"title": "6.2 Performance Relative to Qubit Number / Feature Number", "content": "The Quantum Inversion Test OC-SVMs (IT) achieve a negligible improvement in aver-\nage precision compared to the classical radial basis function (RBF) models. This con-\ntradicts the results of Kyriienko et al. [11]. This discrepancy in the results might stem\nfrom differences in the number of runs, the seeds used for the experiments, and the data\nselection method, as these details were not disclosed in their paper.\nThe Quantum Randomized Measurements OC-SVMs (RM) performance seems to de-\ncline when increasing the qubits. When error mitigation (Eq. 13) is applied during the\ngram matrix calculation, the results are close to the classical RBF and the quantum\ninversion test models. But if the error mitigation is skipped, the average precision is\nhigher than all the previous methods. The variance of the method noticeably exeeds the\nprevious methods, especially if the unmitigated version is selected, and does not reduce\nas fast when increasing the number of features/qubits."}, {"title": "6.3 Time Complexity Relative to Data Size", "content": "Fig. 5 (a - b) display the training and testing times for our models in seconds in rela-\ntionship to the data size for the synthetic dataset and the Credit Card dataset. We obtain\nconsistent results for both datasets.\nThe Quantum Inversion Test OC-SVMs (IT) evolves quadratically with the training data\nsize during training and linearly during testing.\nThe Quantum Randomized Measurements OC-SVMs (RM) substantially shortens the\ntraining times, attaining an approximate 50% reduction in training time whilst tripling\nthe data size. However, the time complexity based on data size remains quadratic. It is\nworth noting that the error mitigation in Eq. 13 does not seem to significantly increase\nthe kernel calculation time.\nThe Variable Subsampling ensembles using the quantum Inversion Test kernel (VS)\ntraining times coincide with those of the other variants Variable Subsampling meth-\nods. The method extensively reduces the training times in comparison to the usage of\na single Inversion Test OC-SVM, and successfully achieves linear time complexity to\ndata size.\nThe Variable Subsampling ensembles using the quantum Randomized Measurements\nkernel (VS-RM) achieves linear complexity to data size and even higher reductions in\ntime, in comparison to the VS with the Iversion Test kernel.\nThe Variable Subsampling with Rotated Feature Bagging ensembles using the Random-\nized Measurements kernel (VS-RFB-RM) have training times which are similar to the\nother variable subsampling variants, but are especially more effective in diminishing\ntesting times. Notably, we obtain a nearly 90% reduction in training times and an 80%\nreduction in testing times when using triple the amount of training data. For this method,\nthe time complexity to data size for this method during both training and testing is lin-\near."}, {"title": "6.4 Time Complexity Relative to Qubit Number / Feature Number", "content": "Fig. 5(c) exhibits the training and testing times in function of the number of fea-\ntures/qubits of our models for the Credit Card dataset.\nThe Quantum Inversion Test OC-SVMs' (IT) time complexity during training seems to\nevolve quadratically with the number of qubits/features.\nThe Quantum Randomized Measurements OC-SVMs (RM), in accordance with the re-\nsults in [7], appear to have exponential time complexity in function of the number\nof qubits/features. This causes the method to become unusable for high-dimensional\ndatasets without prior usage of dimensionality reduction techniques like P\u0421\u0410."}, {"title": "7 Conclusion", "content": "Our study examines multiple approaches for efficient quantum anomaly detection using\none-class SVMs. The first approach is based on the classical Variable Subsampling en-\nsemble method, while the second utilizes the quantum Randomized Measurements ker-\nnel calculation method. Our results demonstrate that the Variable Subsampling method\ncan effectively be used to train OC-SVM-based ensembles with the quantum Inver-\nsion Test kernel in linear time complexity to data size and quadratic complexity to the\nnumber of features. The method leads to a drastic acceleration in training and testing\ntimes, without compromising the performance of the models, provided that the scoring\nthreshold is adjusted. Alternatively, the unmitigated Randomized Measurements kernel\nseems to attain higher average precision than the Inversion Test and RBF kernel-based\nmethods for the Credit Card dataset, even though this dataset exhibits higher dimension-\nality and imbalance than the synthetic dataset. This method however produces unstable\nmodels, evident in the high variance. Furthermore, it comes with exponential time com-\nplexity to the number of qubits/features and a quadratic time dependence to data size.\nThese findings motivate a novel approach that integrates both methods, which we test in\nthe same experimental settings. We create new Variable Subsampling ensembles using\nOC-SVMs trained with the Randomized Measurements kernel. This method surpris-\ningly leads to an increase in performance along with further improvements in training\nand evaluation times, but still suffers from high variance and exponential time com-\nplexity to the number of qubits. To overcome these drawbacks, we develop a further\nmethod, variable subsampling with rotated feature bagging in combination with the"}]}