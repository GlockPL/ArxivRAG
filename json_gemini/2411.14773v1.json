{"title": "MODE-CONDITIONED MUSIC LEARNING AND COMPOSITION: A\nSPIKING NEURAL NETWORK INSPIRED BY NEUROSCIENCE AND\nPSYCHOLOGY", "authors": ["Qian Liang", "Yi Zeng", "Menghaoran Tang"], "abstract": "Musical mode is one of the most critical element that establishes the framework of pitch organization\nand determines the harmonic relationships. Previous works often use the simplistic and rigid alignment\nmethod, and overlook the diversity of modes. However, in contrast to AI models, humans possess\ncognitive mechanisms for perceiving the various modes and keys. In this paper, we propose a\nspiking neural network inspired by brain mechanisms and psychological theories to represent musical\nmodes and keys, ultimately generating musical pieces that incorporate tonality features. Specifically,\nthe contributions are detailed as follows: 1) The model is designed with multiple collaborated\nsubsystems inspired by the structures and functions of corresponding brain regions; 2)We incorporate\nmechanisms for neural circuit evolutionary learning that enable the network to learn and generate\nmode-related features in music, reflecting the cognitive processes involved in human music perception.\n3)The results demonstrate that the proposed model shows a connection framework closely similar\nto the Krumhansl-Schmuckler model, which is one of the most significant key perception models\nin the music psychology domain. 4) Experiments show that the model can generate music pieces\nwith characteristics of the given modes and keys. Additionally, the quantitative assessments of\ngenerated pieces reveals that the generating music pieces have both tonality characteristics and the\nmelodic adaptability needed to generate diverse and musical content. By combining insights from\nneuroscience, psychology, and music theory with advanced neural network architectures, our research\naims to create a system that not only learns and generates music but also bridges the gap between\nhuman cognition and artificial intelligence.", "sections": [{"title": "1 Introduction", "content": "Music learning and generation have long fascinated researchers across various disciplines, including neuroscience,\npsychology, and artificial intelligence. In particular, the artificial intelligence field has witnessed significant advance-\nments in recent years, driven by the integration of deep learning techniques, large-scale models, and novel neural\nnetwork architectures[1, 2]. With the emergence of OpenAI's Sora [3], the techniques of music learning and generation\nseem so powerful that they are capable of producing compositions that rival human-created music in complexity and\nemotional depth. However, despite these impressive advancements, the biggest challenge remains: current techniques\nlack the ability to understand and generate music on a cognitive level. Fig. 1 exhibits the Macro comparison of music\nlearning and composition between human being and current deep learning techniques.\nTraditional AI Since the first music piece was created by a computer at Bell Laboratories in 1957[4], the techniques\nof automatic music generation have developed over past six decades. Symbolic music, which represents music through\nnotational symbols like pitch, duration, and instrumentation, captures detailed musical elements such as melody,\nharmony, and structure. These features make it particularly well-suited for deep music analysis. Recent years, deep\nlearning has emerged as a powerful tool for symbolic music, with numerous landmark studies making significiant\ncontributions to this field. In 1998, Todd[5, 6] firstly used recurrent neural network(RNN) to generate music melodies.\nLater in 1994, Mozer developed CONCERT[7, 1] system, also based on RNN, to generate melodies by conditioning\non pitch, duration, and harmonic chord accompaniment. Eck and Schmidhuber were the first to employ Long-Short\nTerm Memory(LSTM) to overcome the vanishing gradient problems, creating blues music with this method[8]. Since\nthen, RNNs, particularly their LSTM variants, have become one of the most popular models for music learning and\ngeneration. Boulanger-Lewandowski et al.[9] made further advancements by using RNN-RBM model to learn harmonic\nand rhythmic rules and polyphonic music. Google Brain's Magenta project later developed MelodyRNN[10] model,\nincorporating LookBackRNN and attention mechanisms to better capture the long-term structure of music. Additionally,\nDeepBach[11] was designed to generate four-part harmonizations in the style of Johann Sebastian Bach's chorales,\nusing a combination of Bi-direction LSTM to capture both the past and future context of music sequences. Besides,\nmodels based on Variational AutoEncoder(VAEs) or Generative Adversarial Networks (GANs) architecture have also\nbeen explored for analyzing and generating music. MusicVAE[12], for instance, uses hierarchical VAE structure to\nmodel both monophonic and polyphonic music sequences, generating relative consistent music pieces. Wang and et\nal.[13] proposed PianoTree VAE, which is designed as a new tree-structure extension model to generated polyphonic\ncounterpoint music. Wei and Xia[14] proposed a EC2-VAE to learn long-term symbolic representations through\ncontextual constrains. MidiNet[15] employs a generative adversarial network to generate more interesting multi-track\nmusic than MelodyRNN, which based on a novel conditional mechanism for generating melodies that follow a chord\nsequence, or by the previous bars. Dong et al.[16] introduced MuseGAN, which generates five-tracks music pieces\nwithin the GANs, including three models. Researchers also utilized GANs to generate different styles of music, such as\nJazz[17]. Transformers-based models recently have gained popularity in the music generation domain. Huang et al.[18]\nexplored the language model Transformer XL to generated popular piano music, while MuseNet[19] can produce\nseveral minutes music with various instruments and genres based on GPT-2. With the widespread use of large language\nmodels (LLMs), Zhao[20] proposes Adversarial-MidiBERT, to achieve four music understanding tasks. Xu et al.[21]\ndeveloped a text-conditioned music generation model based on the LLM-enhanced MetaScore. In summary, as shown\nin the green part of Fig.1, current computational models predominantly rely on large datasets and parameter-heavy\narchitectures. This reliance presents a significant limitation: it renders the learning process increasingly complex,\ndiverging significantly from the intuitive and explainable learning and creative processes exhibited by human beings.\nHuman Being Music is intrinsic to human nature, and researchers of cognitive neuroscience have shown that it is a\nso complex cognitive activity requiring multiple brain areas to collaborate [22]. As shown in the yellow part of Fig. 1,\nthe perception and creation of music involve intricate processes that engage various regions of the brain, including\nthose responsible for auditory processing, memory, motor control, emotional regulation and etc. Studies have shown\nthat the primary auditory cortex(A1) has a topographical structure that maps the cochlea. The organization implies\nthat neural populations in different regions response to different frequencies of sound[23, 24, 25, 26]. Theses findings\nclearly demonstrated that the encoding manner of the sound in brain. However, through there are no clear and consistent\nfindings regarding how the brain encodes and process temporal information in music perception, some researches\nhave found that a significant number of cells in the medial premotor cortex (MPC) are sensitive to a range of signal\ndurations, with their preferred durations spanning intervals from several hundred milliseconds[27, 28], and \"Time\nCells\" [29] may encode moments between temporal events and locations. These findings may be helpful for music time\nperception. Besides, one significant area of exploration is the role of memory in music cognition. Memory allows for\nthe recognition of familiar melodies, the anticipation of musical patterns, and the contextual understanding of musical\npieces. This involves both short-term memory, which helps in processing and interpreting ongoing musical input, and\nlong-term memory, which stores musical knowledge and experiences accumulated over time. Researchers have found\nthat, hippocampus, medial temporal lobe(MTL) as well as prefrontal cortex(PFC) are deeply involved in temporal\ncontext memory establishment[30, 31, 32, 33]. Prior knowledge also exerts critical influence on memory and is one of\nthe prominent factor guiding the learning behaviors across human lifespan. Studies have highlighted the significant\nroles of the medial prefrontal cortex (mPFC) and the hippocampus (HC) in the formation of prior knowledge and its\nutilization during the successful encoding, consolidation, and retrieval of memories[34].\nAdditionally, principles of music theory provide the powerful supports to ensure that the music learned and\ngenerated remains reasonable and consistent. Modal theory, especially, which deals with the use of different scales and\nmodes in music composition, offers a framework for capturing the tonal and harmonic nuances that are essential to many\nmusical traditions. In the domain of music psychology, one of the most important model is the Krumhansl-Schmuckler\nKey-Finding Model which was developed by Carol Krumhansl and Edward Schmuckler in the early 1990s[35, 36].\nThis model developed the idea of tonal hierarchies through empirical experiments, where listeners rated the importance\nor fit of each note within a given key. For each major and minor key, Krumhansl constructed a key profile, which\nrepresents the relative importance or frequency of the 12 pitch classes (C, C#, D, D#, etc.). Each profile consists of\nvalues based on the experimental data from listeners. The model compares the pitch-class distribution from the musical\ninput to each of the 24 key profiles (12 major and 12 minor). It uses correlation to determine how well the pitch-class\ndistribution matches each key profile. The key that yields the highest correlation is considered the most likely key of\nthe piece. This model plays a significant role in understanding tonal perception in Western music and has influenced\ncomputational approaches in music theory and cognitive science.\nBrain-inspired Models Facing the gap between technical proficiency and true musical comprehension, brain-inspired\nmethods integrate brain mechanisms with the strengths of traditional AI models to enhance the model's cognitive\nabilities, as shown in the red part of Fig.1. Some researchers[37] have utilized fMRI responses of human brain activity\nto music stimuli as embeddings for deep learning models. However, the primary goal of such studies has been to\nreconstruct music, rather than to explore the deeper aspects of musical understanding or creative generation. Spiking\nNeural Networks (SNNs), whose neurons and learning principles more closely resemble those of the human brain,\noffer a potential solution to the current challenges. Q. L. and colleagues proposed a spiking neural network inspired by\ncortical structures to memorize music [38], and to generate stylistic [39] and emotional [40] musical melodies. However,\nthese studies primarily focus on monophonic melody generation and lack the integration of music theory. Overall, the\napplication of brain-inspired spiking neural networks for learning and generating music remains rare. Addressing the\ncurrent challenges and the scarcity of research in this area, this paper proposes a brain-inspired SNN model that learns\nand generates multi-track music by combining brain mechanisms, music psychology findings, and Western mode and\nkey theories. Fig.2 shows the total architecture of the model. The contributions of this paper are as follows:"}, {"title": "2 Results", "content": "2.1 Dataset Description\nHuman learners typically begin studying music theory with a textbook, starting with fundamental concepts and\ncompleting exercises to build foundational knowledge. Through repeated practice and exposure, they solidify this\nknowledge by listening to numerous musical works, which helps reinforce their memory. With this prior knowledge in\nplace, they can progress to analyzing compositions or even creating their own music. Inspired by human behavior, this\npaper utilizes two datasets to train our model:\nAs described in Section 1, the Krumhansl-Schmuckler model(KS model) highlights the significance of pitch classes\nwithin each major and minor key. In our model, the synaptic architecture between mode cluster and the sequential\nmemory system, particularly the pitch subnetwork, mirrors the importance of neural relationships, indicating the critical\nrole of each pitch class within a key. To observe the internal structure of our model, this paper proposes two features,\nwhich are listed below:\n\u2022 Pitch Synaptic Count(PSC) refers to the total number for each neuron representing different pitch class in the\nmode clusters and the corresponding neurons in the pitch subnetwork of the sequential memory system. Since\nthe k-th neuron reprsents the k-th pitch class, the PSC for the the k-th pitch lass can be computed as Eq. 1.\n$PSC_k = \\sum_{j=1}^{4}\\sum_{r\\%12=k}^{128}\\sum_{s}^{N}C_{k,jrs}$ (1)\nwhere $C_{k,jrs}$ denotes whether a synapse exists between the k-th neuron in each neural group of the mode\ncluster and the r-th neuron in the s-th minicolumn of the j-th part in the pitch subnetwork(see Eq.9). PSC(k)\nrepresents the pitch synaptic count between the k-th neuron, corresponding to the related pitch class in the\nmajor and the minor modes, and all the neurons belonging to the same pitch class in the pitch subnetwork.\n\u2022 Pitch Average Synaptic Weights(PASW) calculates the average weights of synapses between each pitch\nneuron in the mode/key clusters and the pitch subnetwork. The computation is as Eq. 2\n$PASW(k) = \\frac{\\sum_{j=1}^{N}\\sum_{r\\%12=k}^{128} \\sum W_{j,r,s}(k)}{PSC(k)}$ (2)\nwhere, $w_{j,r,s}$ is the synaptic weight between the k-th neuron in each neural group of the mode cluster and the\nr-th neuron in the s-th minicolumn of the j-th part in the pitch subnetwork, PASW (k) denotes the average\nsynaptic weight between the k-th pitch class for each mode and related neurons in the pitch subnetwork,\nPSCk is computed as the Eq. 1.\nTo observe the changing trend in comparison with the KS model profile, we train our model using both SHTE\nand Bach datasets. After training, we normalize to PSC, PASW and KS pitch profile scores. Fig.3 panels (A) and (B)\npresent the quantified results for the major and minor modes trained on the SHTE, while panels (C) and (D) show results\nfor training on the Bach corpus. In each picture, the purple and green lines represent the average synaptic weights and\nthe total counts across the 12 pitch classes for the major and minor modes, respectively. While the pink dashed line\ndraws the KS pitch profile scores.\nFor the major mode in both dataset(Fig.3(A)(C)), the PSCs and PSWAs are consistent with the KS model, always\nreceive the highest score 1.0 at the point of tonic(C), followed by the dominant(G), with values $PSC_{SHTE}(G) = 0.98$,\n$PASW_{SHTE}(G) = 0.95$, and $PSC_{Bach}(G) = 0.98$, $PASW_{Bach}(G) = 0.9$, respectively. This pattern reflects the\ncentral roles of the tonic and dominant pitches. For the mediant tone, compared to the KS model $KS_{major}(E) = 0.52$,\nthe $PSC_{SHTE}(E) = 0.55$,$PASW_{SHTE}(E) = 0.53$, $PSC_{Bach}(E) = 0.67$, $PASW_{Bach}(E) = 0.61$, similar to the\nKS model. However, for the superdominant tones, the PSC and PSAW values in the Bach dateset are consistent with\nthe KS model. In the SHTE dataset, however, these values are $PSC_{SHTE}(F) = 0.56$ and $PASW_{SHTE}(F) = 0.57$\nat the subdominant(F), slightly higher than the values at the mediant tone(E), due to the frequent use of subdominant\nchords within our teaching dataset. The PSC and PSWA values for the remaining pitch classes also follow the same\ntrend as observed in the KS model.\nFor the minor mode(Fig.3(B)(D)), the PSCs and PASWs also play the most important role at tonic(C).\nHowever, in the KS model, the scores for mediant(Eb)$KS_{minor}(Eb) = 0.75$ are higher than those for the\ndominant(G)$KS_{minor}(G) = 0.58$. In contrast, the PSC and PASW values in the SHTE dataset($PSC_{SHTE}(Eb) =$\n0.52, $PASW_{SHTE}(Eb) = 0.45$) and Bach dataset ($PSC_{Bach}(Eb) = 0.74, PASW_{Bach}(Eb) = 0.58$) are lower than\nthose at the dominant(G). This discrepancy can be attributed to the key and necessary role of the dominant chord in\nthe cadences and its frequent use in both datasets. Besides, compared to KS model in SHTE dataset(3(B)), these two\nmetrics for the leading tone(B) show an ascending trend, This is due to the teaching characteristics of the SHTE dataset,\nwhich includes more exercises focused on learning chords.\nThis paper further calculates the cosine similarities between PSC and PASW with the KS scores, yielding values of\n$COS_{SHTE}(PSC_{major}, KS_{major}) = 0.93$ and $COS_{SHTE}(PASW_{minor}, KS_{minor}) = 0.92$ for the SHTE dataset,\nwhile $COS_{Bach}(PSC_{minor}, KS_{minor}) = 0.94$ and $COS_{Bach}(PASW_{minor}, KS_{minor}) = 0.94$ for the Bach dataset,\nrespectively. The results indicate a strong alignment between our model's connection architecture and the Krumhansl-\nSchmuckler psychological key perception model. The presence of subtle differences indicate that the model captures\ndataset-specific harmonic characteristics, suggesting that it not only aligns with KS-defined roles for tonic and dominant\npitches but also adapts to dataset-specific harmonic nuances, such as the influence of subdominant or leading tones.\nWe also calculate the cosine similarities between these two metrics of the connection architecture and the KS\nmodel for the twelve major and minor keys. The result is provided in Fig.4."}, {"title": "2.2.2 Quality Evaluation And Analysis Of Generation", "content": "In this section, the model generates musical pieces by specifying seed notes and setting the music length, with\nconditioning applied to various major and minor keys. Fig.5 presents an example of the generated samples from our\nmodel. The sample demonstrates that the generated tones of each part are basically accordance with the specified\ntonality. An intriguing phenomenon, marked by red circles, is that the model naturally learns to incorporate ascending\nor descending semitone steps, creating smoother melodic transitions. The use of semitones introduces chromatic tones,\nwhich break up the conditioning of key and the mode, enhancing expressiveness and allowing the generated melodies to\nexplore more intricate harmonic nuances.\nTo further assess and analyze the quality of the generated music pieces, we employ a comprehensive objective\nmethod which is inspired by the method proposed by Yang and Lerch[42]. Several features are extracted from the\ngenerated samples and two datasets involved in our model. The features include: Pitch Count(PC), the number of\ndifferent pitches in a music piece; Pitch Class Histogram(PCH), the histogram of 12 pitch classes; Diatonic Pitch\nRate(DPR), the proportion of pitches in a music piece that belongs to the key's diatonic scale; Pitch Range(PR), the\npitch range between the highest and the lowest pitch in semitones within a sample; Pitch Interval(PI), average interval\nbetween two pitch neighbors, PCTM, the pitch class transition matrix."}, {"title": "3 Methodology", "content": "3.1 Model Architecture\nIn this study, we employ symbolic representations of musical pieces as the dataset and introduce a brain-inspired\nmodel based on a spiking neural network that is inspired by recent advancements in neuroscience and psychology in the\nmusic domain. As illustrated in Figure 2, the music information is prepared in a symbolic manner and divided into two\nparts: 1) treating the mode and key of a musical piece as theoretical musical knowledge, and 2) describing ordered\nnotes, along with their respective pitches and durations with MIDI standard. Then, the model centers around two key\ncomponents: a musical theory subsystem that embeds modes and chords as foundational prior knowledge directing the\nsubsequent learning task, and a sequential memory subsystem designed to learn and store the ordered notes of music\npieces.\n3.1.1 Music Theory Subsystem\nThe music theory subsystem(MTS) is structured hierarchically to encode modes as musical prior knowledge. As\nshown in Figure. 6D, the first layer, the mode cluster, contains two neural groups dedicated to encoding the Western\nmajor and the minor modes. Each group in this layer consists of twelve neurons corresponding to the tones in 12-Tone\nEqual Temperament (12-TET), with seven of them representing the tones (I to VII) within the pattern and the rest ones\nencoding those outside of the pattern, figure 6A illustrates the details of the major mode representation. The second\nlayer called the key cluster is consisted of twenty-four neural groups encoding twelve major keys and twelve minor\nkeys. Similarly, each group comprises twelve notes with a specific tonic, figure 6B shows how the neural group encodes\nthe G major. Synaptic connections(shown in figure 6C) are projected from neurons in each group in the first layer\nthat represent the tone scale degree of the mode to those in the second layer encoding the corresponding note scale degree.\n3.1.2 Sequential Memory Subsystem\nA musical piece is a flow of ordered notes arranged across multiple instrumental parts or tracks, every track\nplays their harmonic role and contributes to the overarching harmony. For maintaining their individual characteristics,\nthe sequential memory subsystem(SMS) is partitioned into four segments to encode and store the sequential order\nand temporal relationships of the notes, including melodic lines and harmony progressions. As depicted in figure. 2,\neach segment resides a dual-network configuration: a pitch subnetwork encoding the tones of notes, and a duration\nsubnetwork retains the time intervals indicating how long a tone sustains.\nThe Pitch Subnetwork Inspired by the neural populations in the primary auditory cortex that respond distinctively to\ndifferent frequencies, the pitch subnetwork is proposed to consist of 128 functional minicolumns as its building blocks.\nAs depicted in Figure 2, each minicolumn is configured as a vertical column, representing one of the 128 pitches in\naccordance with the MIDI (Musical Instrument Digital Interface) standard. Each minicolumn comprises numberous\nneurons, all of which share a common preference for a specific MIDI pitch index. For instance, all neurons within the\nminicolumn indexed at 60 will respond preferentially to the pitch C4. The synaptic connections within and between the\nminicolumns are detailed below.\nThe Duration subnetwork The duration subnetwork is structurally identical to the pitch subnetwork. It consists of\n64 functional minicolumns, each representing a particular note duration. These minicolumns are organized to cover a\nrange of time intervals, with the finest granularity being a sixty-fourth note.\nIntra-Connection As illustrated in Figure 2, the pitch and duration subnetworks share the same internal connection\nstructure. The synaptic connections between adjacent layers and across layers are excitatory and fully connected.\nMeanwhile, the introduction of synaptic plasticity and transmission delay plays a crucial role in the learning process.\nSynaptic plasticity allows the connection weights between neurons to be adjusted based on experience and learning,"}, {"title": "3.2 Music Data Encoding", "content": "The encoding process aims to transform external stimuli to neural spikes inspired by brain mechanisms. Suppose a\nmusic piece consisting of multiple parts is defined as $NS = {Ni,j|i = 1, 2, ...nj, j = 1,2,3,4}$, where Ni,j denotes\nthe note at i-th position in the j-th part, nj refers to the number of notes in the j-th part. The mode can be written as\nMr, where r = 0 or r = 1 refers to the major or minor mode. The key is marked as Ks, where s \u2208 [1, 12] denotes one\nof the 12 possible tonal center. Then, the encoding process can be divided into two steps:\nStep1: Transformation of External Stimuli The external stimuli(such as keys, pitches,etc.) are transformed into the\ninput current I that is suitable for the computational neuron model to receive. For example, a G-Major musical piece\nactivates the neurons in the major cluster in the first layer and G major in the second layer. Equation.3-.4 describes the\ntransformation for the mode and key clusters, respectively.\n$I^{Mr}_i(t) = a^{Mr} \\delta(x_{ij}(t) - sd^{Mr}_i)$ (3)\n$I^{Ks}_i(t) = a^{Ks} \\delta(x_{ij}(t) - ts_i)$ (4)\nwhere $I^{Mr}_i(t)$ is the input current for neuron i in the r-th group of the mode cluster at time step t. xij(t) represents the\nexternal stimuli, which refers to the pitch of the input note Ni,j at time t. $sd^{Mr}_i$ denotes the scale degree of the tone\nwhich the neuron i represents in the mode cluster. Similarly, the $I^{Ks}_i(t)$ refers to the input current of neuron i in the\ns-th group of the key cluster, $ts_i$ denotes the tone scale in G major cluster which neuron i represents.$a^{Mr}=a^{Ks}= 50$"}, {"title": "3.3 Learning Based On Neural Circuits Evolution", "content": "The brain utilizes multiple regions to work together, with dynamic synaptic formation and elimination creating\ndiverse neural circuits to accomplish various cognitive tasks. Inspired by these mechanisms, we consider the learning\nprocess as a collaborative effort among interconnected subnetworks, with a focus on the dynamic neural circuit evolution\nover time.\n3.3.1 Synaptic Creation\nSince the music theory system stores the prior knowledge, the inter-connected architecture is preset. However,\nthere are no synaptic projections between the musical theory subsystem and the sequential memory system at the initial\nstate.\nNeuroscientific research has demonstrated that neural electrical activities and the dynamics of axonal growth\ncone movements are interdependent and coordinated. Electrical activities serve as feedback signals for the navigation\nof growth cones, while the movement of growth cones and the formation of new synapses subsequently influence\nthe electrical activities within the neural network. [44, 45, 46]. In this study, we simplify this complex process by\nestablishing rules for the formation of new synaptic connections and neural circuits in response to the input of musical\ninformation.\n$\\theta = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\frac{tf_j - t_i}{N}$ (8)\n$C_{ij} = \\begin{cases}1, & \\text{if } O \\geq 5 \\\\ 0, & \\text{else} \\end{cases}$ (9)\nwhere, the $t_i^f$ and $t_j^n$ represent the spike time f and n of postsynaptic neuron i and presynaptic neuron j, respectively.\nWhen the oscillatory times $o \\geq 5$ of these neuron i and j, a new synaptic connection is formed, and is represented as\nCij. This simple rule is defined to describe that neurons oscillate together are more likely to form a synaptic connection\nbetween them."}, {"title": "3.3.2 Synaptic Plasticity", "content": "This paper utilize the modified STDP(Spike-Timing Dependence Plasticity)[47] learning rule to account for the\nspiking transmission delay when updating synaptic weights during the learning process, as shown in Eq. 10.\n$\\Delta \\omega_{ij} = \\sum_{f=1}^{N} \\sum_{n=1}^{N} W(\\Delta t) (t_i^f - t_j^n)$ (10)\n$W(\\Delta t) = \\begin{cases}A^+ e^{-\\frac{\\Delta t}{\\tau_+}} & \\text{if } \\Delta t > 0 \\\\ A^- e^{\\frac{\\Delta t}{\\tau_-}} & \\text{if } \\Delta t < 0 \\end{cases}$"}, {"title": "3.4 Mode-Conditioned Music Generation", "content": "Specifying the mode and key is a fundamental step in the process of composing a piece of music. The model\ndescribed in this paper requires not only the mode and key but also a set of seed notes to initiate the creative process.\nAs illustrated in Fig. 8, the model is tasked with generating a four-part musical piece in G minor, starting with the tonic\nchord(G2-Bb3-D4-G4) as the seed. The figure omits the neurons and synaptic connections that are not involved in\nthis example. The tonic chord is defined by the notation No, which includes the pitch set X\u2081 = {67, 62, 58, 43}, and\nduration set Yo = {1,1,1,1}, sending to the corresponding parts of the pitch and duration subnetworks.\n\u2022 Step1: Upon with the input of the initiate seed notes, the corresponding neurons in the G-minor group of key\ncluster are ready to receive the stimuli and emit the spikes(red circle). The pitch and duration values of the seed\nnotes trigger the activation of specific neurons in sequential memory system(blue and orange circles), which\nthen propagate through the network to generate the next set of notes. At this step, the neurons are updated by\nEq. 4-Eq. 7. Neurons representing G2, Bb3, D4 and G4 in part one to part four fire(marked by red circles) and\npropagate their spikes to other neurons.\n\u2022 Step2: Then, the subsequent neurons of the pitch and duration subnetworks receive and integrate inputs\nthrough trained synaptic connections by Eq.13 and Eq.14, respectively.\n$I^{p}_i(t+1) = \\sum_{MTS} w^{MTS}_s (t) + \\sum \\omega_r w^s(t)$ (13)\n$I^{D}_i(t+1) = \\sum \\omega_r ^s(t)$ (14)\nTherefor, neurons in the same layer compete with each other and we employ the Winner-Takes-All priciple\nto select the most strongly activated neuron as the generated results. As illustrated in Fig. 8, neurons in part\ntwo representing E4 and Eb4 are both activated as candidates at this step. However, due to the connection\narchitecture of our trained model between the key cluster and pitch subnetworks, which is similar to KS model,\nand because the tone E is a chromatic tone in G minor, the synaptic weight between the neuron representing\nE4 and the neuron representing tone E in G minor group is significantly lower(light pink arrow) compared to\nthat between neuron Eb4 and the corresponding neuron Eb in the G minor group of the key cluster(red arrow).\nConsequently, the neuron Eb4 emits more spikes and becomes the winner, representing the next generated\nnote in part two. The generating process are similar in other parts, synapses with high weights are marked by"}, {"title": "4 Conclusion", "content": "In this paper, we present a brain-inspired spiking neural network designed to learn the modes and keys of four-part\nmusic. Specifically, based on evolutionary neural circuits and collaborative areas, the model can learn the music theory\nof Western modes and keys, as well as the sequential relationships between notes. The trained model framework\ndemonstrates the phenomena that closely resemble those of the well-known psychological model, the Krumhansl-\nSchmuckler model (KS model). Ultimately, the model is capable of generating the four-part music conditioned on\ndifferent keys and modes. The evaluation results shows that the generated music pieces are musically coherent and\nreflect the characteristics of the specified modes and keys. This paper highlights the potential of brain-inspired spiking\nneural networks in advancing music generation technologies and provide a foundation for future research in this area.\nLearning modes and keys is fundamental to advancing harmonic understanding and composition. In this research,\nwe aim to explore brain-inspired models that are grounded in neuroscientific mechanisms and psychological theories to\nenhance our comprehension of music. By delving deeper into the cognitive processes that underpin musical perception\nand creativity, we hope to develop innovative approaches to music learning and generation. This interdisciplinary\nfocus will not only inform the design of more effective neural network architectures but also contribute to a richer\nunderstanding of how humans interact with and create music.\nAs we move forward, our key focus will be on the relationship between harmonic learning and emotional generation.\nWe believe that exploring how different harmonic structures can evoke various emotional responses will significantly\nenhance the capabilities of our models, allowing for the creation of music that resonates deeply with listeners. Through\nthis work, we aim to bridge the gap between artificial intelligence, neuroscience, and music, providing new insights into\nthis interdisciplinary domain."}]}