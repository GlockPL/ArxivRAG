{"title": "The Matrix: Infinite-Horizon World Generation with Real-Time Moving Control", "authors": ["Ruili Feng", "Han Zhang", "Zhantao Yang", "Jie Xiao", "Zhilei Shu", "Zhiheng Liu", "Andy Zheng", "Yukun Huang", "Yu Liu", "Hongyang Zhang"], "abstract": "We present The Matrix, the first foundational realistic world simulator capable of generating infinitely long 720p high-fidelity real-scene video streams with real-time, responsive control in both first- and third-person perspectives, enabling immersive exploration of richly dynamic environments. Trained on limited supervised data from AAA games like Forza Horizon 5 and Cyberpunk 2077, complemented by large-scale unsupervised footage from real-world settings like Tokyo streets, The Matrix allows users to traverse diverse terrains-deserts, grasslands, water bodies, and urban landscapes\u2014in continuous, uncut hour-long sequences. With speeds of up to 16 FPS, the system supports real-time interactivity and demonstrates zero-shot generalization, translating virtual game environments to real-world contexts where collecting continuous movement data is often infeasible. For example, The Matrix can simulate a BMW X3 driving through an office setting\u2014an environment present in neither gaming data nor real-world sources. This approach showcases the potential of AAA game data to advance robust world models, bridging the gap between simulations and real-world applications in scenarios with limited data. All the codes, data, and model checkpoints in this paper will be open sourced.", "sections": [{"title": "1. Introduction", "content": "Neural-interactive simulation, a concept popularized by The Matrix (1999), envisions a world fully constructed by AI to replicate 20th-century human society. This paper takes an initial step toward realizing this vision by developing a world model that enables neural networks to 'dream' visually authentic environments. The result is an infinite-horizon, high-resolution (720p) simulation that supports real-time (8 16 FPS) interactive exploration across diverse landscapes, including deserts, grasslands, water terrains, and urban settings. Responding to real-time control signals, the world model predicts future frames in these environments in a streaming and auto-regressive fashion.\nWorld models offer a promising solution to the overwhelming costs of AAA game development, which can easily run into tens or even hundreds of millions of dollars. Traditional game creation depends on engines such as Unity 3D, Unreal Engine, and Blender, each requiring substantial expertise, intensive asset preparation, and meticulous hyperparameter tuning. Furthermore, games built with these engines are often limited in reusability, as each new title demands a comprehensive redesign. In contrast, data-driven world models tackle these issues by minimizing the need for manual configuration, simplifying development workflows, and boosting scalability across projects.\nDespite extensive research in world models [36], key challenges remain. First, prior studies have predominantly focused on non-AAA video games, such as Atari [1, 13, 37], Mario [31], Minecraft [7, 14], Counter-Strike: Global Offensive (CS:GO) [1], and DOOM [42], which fall short in replicating real-world fidelity. Second, current video generation techniques, like Sora [29], are constrained to short sequences of about 1 minute, forcing existing world models to assemble independently generated clips with noticeable transitions. Finally, achieving real-time generation remains a major hurdle. For example, state-of-the-art 2D platformer game generator Genie [3] runs as slow as 1 FPS. This paper addresses these limitations by introducing the first scalable, high-fidelity (1280\u00d7720 pixels) world model in real time that enhances simulation realism and bridges the gap between virtual environments and reality. Notably, our world model is the first with strong domain generalization and real-time control. For example, our foundation model allows us to control BMW X3 driving through an indoor setting or in the sea-an environment present in neither gaming data nor real-world sources."}, {"title": "1.1. Our Contributions", "content": "Our contributions are as follows:\n\u2022 We introduce The Matrix, the first foundational simulator for realistic worlds, capable of generating infinitely long, high-fidelity 720p real-scene video streams with real-time, interactive controls and strong domain generalization. The model is light and consists of 2.7B parameters.\n\u2022 At the core of The Matrix is a novel diffusion technique, the Shift-Window Denoising Process Model (Swin-DPM), enabling pre-trained DiT models [30] to extrapolate seamlessly for smooth, continuous, and infinitely extendable video creation. This technique holds potential for broader applications in long-form video generation.\n\u2022 Additionally, we introduce GameData, a platform that autonomously captures paired in-game states-extracted from CPU memory-alongside corresponding video frames, significantly reducing labeling costs and complexity. This platform produces Source, a new training dataset for world models with action-frame paired data."}, {"title": "1.2. Technical Advantages of The Matrix", "content": "Tab. 1 highlights a comparison between The Matrix and other game generation models across six key features. Our work advances the state-of-the-art of world models in the following aspects:\n\u2022 Infinite Video Generation: The Matrix generates consistent, infinitely long video sequences using a streaming, auto-regressive approach.\n\u2022 High-Quality Rendering: The Matrix delivers AAA-level, realistic rendering at a resolution of 1280 \u00d7 720.\n\u2022 Real-Time, Frame-Level Control: The Matrix operates with speeds of 8 - 16 FPS, providing real-time, frame-level control for interactive applications.\n\u2022 Domain Generalization: Trained with small amounts of supervised AAA game data and large amounts of unsupervised internet videos, The Matrix achieves strong domain generalization to real-world settings."}, {"title": "2. Related Work", "content": "World Model for Agent Learning. Developing world models for training agents has been a long-standing research focus, aimed at enhancing policy learning within simulated environments rather than solely achieving high-fidelity reconstructions of observations. This research involves two primary stages: 1) modeling the training environment by reconstructing observations, rewards, and continuation signals, often through a recurrent state-space model; and 2) utilizing this model to predict future states, enabling reinforcement learning to optimize robust policy functions. Studies indicate that this method provides sample efficiency gain of over 1000% compared to directly learning policies from real environments, shows resilience across diverse domains, and can outperform fine-tuned expert agents on a range of benchmarks and data budgets [14].\nKey contributions in this area include Recurrent World Models [11], Dreamer (v1 [12], v2 [13], and v3 [14]), TD-MPC (v1 [15] and v2 [16]), DayDreamer [43], Safe-Dreamer [21], and MuDreamer [4]. Notably, MuZero [37] runs the self-play of Monte Carlo tree search to build world models for Atari, Go, chess and shogi, without external data.\nWorld Simulation. Distinct from world models designed for agent learning, another research direction emphasizes world simulation, focusing on human interaction with neural networks through high-quality rendering, robust control, and strong domain generalization to real-world scenarios. This research explores two types of control: video-level and frame-level. In video-level control, a control signal is given at the start, and the model generates a responsive video sequence; notable examples include UniSim [45], Pandora [44], GameGen-X [5], MicroVGG [31], and GAIA-1 [19]. To approximate continuous control, this approach often stitches together independently generated clips, which may result in visible transitions. In contrast, frame-level control provides fine-grained adjustments every few frames, enabling more precise, responsive interactions similar to gameplay, as seen in examples like Genie [3], DIAMOND [1], GameNGen [42], and Oasis [7]. Prior work in world simulation has typically focused on one of three as-"}, {"title": "3. Methods", "content": "Achieving granular control is notoriously challenging, as labeling actions at the frame level is typically cost-prohibitive. To address this, we develop the GameData platform, which autonomously captures paired data of in-game states (extracted directly from CPU memory) along-side corresponding video frames, significantly reducing labeling costs and complexity. Additionally, The Matrix incorporates an advanced Interactive Module that learns and generalizes game movement interactions from a limited amount of labeled data combined with extensive unlabeled data from both games and real-world environments. This enables The Matrix to deliver exceptional accuracy across diverse scenarios, while maintaining robust performance in the gaming domain.\nGenerating high-quality, real-time, and generalizable video simulations for infinite sequences presents additional technical challenges, often forcing previous simulators to compromise on one or more essential aspects. The Matrix overcomes these limitations by adapting the world model from a pre-trained video Diffusion Transformer (DiT) model [30], leveraging its extensive pre-existing knowledge and generation quality. To enable infinite-length generation, The Matrix introduces a novel diffusion approach, the Shift-Window Denoising Process Model (Swin-DPM), which allows the DiT model to extrapolate for smooth, continuous, and indefinitely long video creation. Finally, to achieve real-time efficiency, we fine-tune a Stream Consistency Model (SCM), accelerating inference to real-time.\nVideo DiT Backbone. As a preliminary, we introduce the video DiT backbone, adapted from the publicly available DiT models [47]. It employs a 3D Variational Auto-Encoder (VAE) to encode T \u00d7 p video frames into T video tokens. The backbone consists of 32 attention blocks, followed by a linear output head with LayerNorm [2]. Each attention block includes a self-attention layer operating on network features, a cross-attention layer linking conditions with self-attention outputs, and an FFN layer composed of two linear layers with a GELU activation [17] in between. See Appendix Section A.1 for further details."}, {"title": "3.1. Model Components", "content": "The Matrix comprises three main components: a) an Interactive Module that interprets user intentions (e.g., keyboard inputs) and integrates them into video token generation; b) a Shift-Window Denoising Process Model (Swin-DPM) that enables infinite-length video generation; and c) a Stream Consistency Model (SCM) that accelerates sampling to achieve real-time performance. As shown in Fig. 2, the model is fine-tuned from a pre-trained video DiT model through a three-stage process: first, we fix the DiT model parameters and train the Interactive Module; next, we train the Interactive Module and the DiT together following the Swin-DPM; finally, we optimize an SCM to accelerate inference to real-time speeds. The first two stages leverage both labeled gaming and unlabeled internet video data to en-"}, {"title": "Interactive Module", "content": "The Interactive Module consists of an Embedding block (see Fig. 3a) and a cross-attention layer. Its primary function is to translate keyboard inputs into natural language that guides video generation. For example, pressing 'W' is interpreted as \"The car is driving forward\" in the Forza Horizon 5 scenario, or as \"The man is moving forward and looking up\" when combined with an upward mouse movement in Cyberpunk 2077. For unlabeled real or game data, we apply a default description: \u201cThe camera is moving in an unknown way.\" To enhance robustness, we randomly replace labeled keyboard inputs with this default sentence during training with probability q = 0.1.\nTo prepare for training, we first warmup the base DiT model for a few epochs using collected game and real-world data, fine-tuning a LoRA weight [20]. This process ensures that the Interactive Module focuses on learning interactions and movement patterns rather than simply fitting the video.\nOnce translated, these natural language descriptions are processed by a T5 encoder [33] and transformed into a vector embedding through two linear layers and a SiLU layer [9] between them. This vector embedding is then concatenated with its corresponding video token and the next w video tokens, where w is a pre-defined causal relation range, typically set to w = 4, as is shown in Fig. 3a.\nWe perform this cross-attention operation each time the DiT model completes an odd-numbered self-attention step, enabling effective information exchange across frames and achieving precise, frame-level control for video generation."}, {"title": "Shift-Window Denoising Process Model", "content": "Typical DiT models are limited to generating only a few seconds of video, even when substantial spatial and temporal compression is applied via VAEs. This limitation is largely due to the high computational cost and memory demands of attention mechanisms over extended time durations. To address this, it becomes crucial to assume that temporal dependencies are confined within a limited time window, beyond which attention computations are unnecessary. Building on this idea, we propose the Shift-Window Denoising Process Model (Swin-DPM), which leverages a sliding temporal window to manage dependencies effectively and enables the generation of long or even infinite videos by producing tokens with a stride of s = 1. As is shown in Fig. 3b, within each window, a queue of video tokens undergoes denoising at various noise levels. After k denoising steps (where k \u00d7 T is the number of diffusion solver steps), the leftmost, lowest-noisy token is dequeued into a cache. To maintain the queue length, a new token with Gaussian noise will be then added to the rightmost position. Each cached token is re-appended to the window's token queue at noise level 0 until the next token is cached, allowing it to continue participating in denoising and ensuring continuity between different windows. The network of Swin-DPM is fine-tuned from a pre-trained DiT model. During training, we sample 2w video tokens, where w is the window size. We usually set w = T. The first w tokens are used solely for warming up Swin-DPM and do not participate in backpropagation; loss is computed only on the last w tokens. At inference time, we follow the same setup: the first w tokens are for warmup and are discarded, with the generated video starting from the (w + 1)-th token."}, {"title": "Stream Consistency Model", "content": "After extending the DiT model to Swin-DPM, we further address the need for achieving real-time rendering of the simulated world. A promising approach is to combine Swin-DPM with Consistency Models [38, 39], a leading method for acceler-"}, {"title": "4. Experiments", "content": "Training Details. We train The Matrix on the Source dataset, using a pre-trained 2.3B parameter DiT model as the backbone, which generates 4 video tokens per second, each decoded into 4 frames by the VAE decoder [25]. To match this generation rate, we downsample the videos and keyboard inputs in the Source dataset accordingly. For all training cases, we first warm up the base DiT model on unlabeled Source data for 20,000 steps with a batch size of 32. Following this, we train the Interactive Module on labeled Source data for an additional 20,000 steps with the same batch size, introducing another 0.4B parameter. Next, we fine-tune The Matrix model using Swin-DPM over 60,000 steps, also with a batch size of 32. For the final Con-"}, {"title": "4.1. Precise Frame-Level Interactions", "content": "In this section, we evaluate the effectiveness of the Interactive Module by testing its performance in three distinct scenarios: the Forza Horizon 5 car driving scenario, the Cyberpunk 2077 city walking scenario, and a robotic arm task from the DROID dataset [24]. We select 50,000 6-second clips from the DROID dataset, along with per-frame action labels of joint angles for seven joints, to form the training dataset. More details can be found in Appendix Section B.3. The third scenario is specifically designed to assess the effectiveness of The Matrix in embodied AI tasks. For all scenarios, we follow the same training strategy: starting with a pre-trained DiT model, we first perform a warm-up using unlabeled data, followed by fine-tuning the Interactive Module with labeled data.\nQualitative Results. Fig. 5 illustrates examples of The Matrix's generated outputs across all scenarios. The Matrix demonstrates the ability to create vivid and dynamic worlds, accurately reflecting user interactions and intentions. It also models the physical behaviors within these environments, such as dust being kicked up when a car drives through a dry desert, or water splashing when it travels through a river. Additional examples of The Matrix's generation capabilities"}, {"title": "4.2. Infinete-Horizon World Generation", "content": "Traditional world simulators focused on precise control often rely on small, auto-regressive generators trained from scratch to minimize the significant memory and time costs associated with pre-trained DiT models. However, this approach compromises visual quality and limits the full potential of world simulators. In this work, we introduce the first world simulator leveraging pre-trained video diffusion models, enabling infinite-length world generation with real-time rendering capabilities. In this section, we present our evaluation of these advancements.\nGenerating Infinitely Long Videos. Fig. 6 showcases examples of generating 1-minute long worlds across diverse scenarios, including desert, river, grassland, snow, and day-to-night transitions. During generation, we switch the DiT prompt to adapt the environment, as shown in Fig. 6b. The Matrix's capability extends beyond this; it can generate truly infinite-length videos, with additional half-hour examples available in Supplementary Videos. Tab. 2 reports the video quality and control precision of The Matrix after training with Swin-DPM. While some visual quality is sacrificed, control precision remains strong, and the visual quality still surpasses previous world simulators, achieving a realistic AAA-level standard.\nReal-Time Rendering. We further investigate integrating"}, {"title": "4.3. Generalization to Out-of-Distribution Worlds", "content": "In addition to superior visual quality, a key advantage of using pre-trained video DiTs is their inherent ability to generalize across diverse scenes. We observe impressive generalization in The Matrix, showcasing the potential of future research into building world simulators with pre-trained DiTs.\nGenerating Unseen Scenes. With The Matrix, we can control a car in previously unseen scenes by describing the scenario in the prompt. The first two rows of Fig. 7a demonstrate this capability, where the car is driven through indoor environments, which were not part of the Source dataset.\nInteracting with Unseen Objects. A more remarkable feature is The Matrix's ability to generalize interaction with real-world objects. As shown in the last two rows of Fig. 7a, by specifying a human as the center object in the DiT prompt, we can make the person move in response to keyboard inputs.\nGenerating Long Videos without Moving Control. Though The Matrix is trained on the Source dataset, it can also function as a general long video generator. By disabling the Interactive Module and using only the DiT backbone trained after Swin-DPM, The Matrix can generate long videos corresponding to ordinary prompts. Fig. 7b shows such an example, further proving The Matrix's strength as a realistic world simulator."}, {"title": "5. Conclusion", "content": "We introduce The Matrix, a real-world simulator capable of generating infinitely long, high-fidelity video streams with precise real-time control. Trained on a blend of AAA game data and real-world footage, The Matrix supports immersive exploration of dynamic environments, with zero-shot generalization to unseen scenarios. Operating at 8 - 16 FPS, it enables continuous, interactive simulations across diverse terrains, bridging the gap between virtual and real-world applications. This work highlights the potential of using game data to build robust world models with minimal supervision, and showcases the power of pre-trained video DiTs in enabling realistic, large-scale simulations."}, {"title": "A. Details in Experiments", "content": "A.1. DiT Backbone\nThe DiT backbone is adapted from the publicly available DiT models [47]. It consists of a patch embedding module, a caption embedding module, a timestep embedding module, 32 DiT blocks, followed by a linear output head with LayerNorm [2]. The followings provide details of each module within the DiT backbone.\nThe Patch Embedding Module. The patch embedding module employs a 3D convolution with a kernel size of 1 \u00d7 2 \u00d7 2, followed by a reshape operation. Thus, the convolution can effectively process the video latent from the VAE encoder, and the reshape operation can further transform the feature into a sequence of tokens with 2,048 feature size. By using a 3D convolution, the module captures both spatial and temporal features, ensuring that the token sequence retains essential information from the video data.\nCaption Embedding Module. The caption embedding module takes the caption token sequence encoded by the T5 model and further processes it through a two-layer FFN. Both the hidden feature size and the output feature size are set to 2,048, allowing the module to generate rich and high-dimensional representations of the caption data.\nTimestep Embedding Module. The timestep embedding module is implemented as a sinusoidal embedding module followed by a two-layer FFN. Both the hidden feature size and the output feature size of this FFN are set to 2,048.\nDiT Block. Each DiT block includes a self-attention layer operating on network features, a cross-attention layer linking conditions with self-attention outputs, and an FFN layer composed of two linear layers with a GELU activation [17] in between."}, {"title": "A.2. Training Details", "content": "Upon obtaining the base DiT model, the training process consists of four distinct stages: (1) warm-up on unlabeled Source, (2) training of the Interactive Module, (3) fine-tuning using Swin-DPM, and (4) Stream Consistency Model distillation. Below, we first outline the common training configurations utilized across all stages, followed by a detailed description of each individual phase.\nCommon Settings. All training procedures were executed with an overall batch size of 32 and a learning rate of 1 \u00d7 10\u22125. Mixed-precision training was employed using bfloat16 to enhance computational efficiency. During pre-processing, all video inputs were resized to a resolution of 1280 x 720 pixels and set to 16 FPS. For sequences exceeding 25,200 frames in length, we used the Deepspeed Ulysses sequence parallelism strategy [23], distributing the sequence across 8 GPUs to manage memory and computational demands effectively.\nWarm-Up on Unlabeled Source Dataset. In the initial warm-up stage, we fine-tuned all linear layers of the base DiT model using Low-Rank Adaptation (LoRA) to tailor the model to the source data distribution [20]. The LoRA rank was set to 128, and the model was trained for 20,000 steps. This adaptation ensures that the model parameters are suitably adjusted to the characteristics of the unlabeled source dataset before advancing to subsequent training phases.\nTraining of Interactive Module. The second stage focuses on training the Interactive Module, each of which is integrated after every two consecutive DiT blocks, totaling 16 Interactive Module. During this phase, the parameters of the base DiT model were frozen to concentrate the training solely on the Interactive Module. This stage was conducted over 20,000 training steps, enabling the Interactive Module to effectively interface with the base model without altering its foundational parameters.\nFine-Tuning Using Swin-DPM. The third stage involves comprehensive fine-tuning of all model parameters, including both the base DiT model and the Interactive Module, utilizing the Swin-DPM approach. This extensive fine-tuning was carried out over 60,000 steps, allowing for the refinement and optimization of the entire model to better capture data intricacies and enhance overall performance.\nConsistency Model Distillation. In the final stage, consistency model distillation was performed using the model from the preceding fine-tuning phase as the teacher model. The student model was initialized with the teacher's weights to facilitate knowledge transfer. During distillation, we employed a one-stage guided distillation technique [28], incorporating Classifier-Free Guidance (CFG) into the student model. For the Ordinary Differential Equation (ODE) solver within the consistency distillation framework, we utilized the Euler solver with a single-step size of 25/1000. This distillation process was conducted over 10,000 training steps."}, {"title": "B. The Source Dataset", "content": "B.1. The GameData Platform\nWe build a framework, GameData Platform, for data collection. The framework consists of three components: Con-"}, {"title": "trolling, Simulation, and Observation.", "content": "Controling. In most games, we need to control a character to go to different scenes and make interactions. Intrinsically, the collected data can be reconstructed with initial states of game worlds and a series of control signal. In order to make the collected data clean and meaningful, instead of being stuck in one corner, we designed two different control systems, namely the automatic one and the manual one. For the automatic control system, we use Cheat Engine for pivotal data access, such as XYZ coordinates in games. These data can be used to determine whether the game has been stuck for some time. We detect the coordinates of a past period of time and determine whether they are covered in a circle of a given size. If the game is detected as stuck, we will reset the game state and restart the recording. Generally, the automatically generated control signals will move randomly, change direction, and change perspective. This is good enough for games that move on a 2D-like surface. However, for games moving in a 3D space, random signals will struggle with generating meaningful content, so we have to change to the manual system. Since our game is running and captured on cloud servers, human data collectors will observe the game through a low-definition streaming and control manually. Signals (from keyboards, mice, and gamepads) are translated and delivered through the socket server, and cloud servers will generate keyboard events through the virtual keyboard. Here, the latency between the control signals and the OBS screen recording is crucial. We eventually found that the control signals recorded on the cloud sever and the actual action responses in the recorded videos were generally no more than three frames apart. and in general, this delay is stable and can be subtracted directly from the timeline.\nSimulation. The game runs directly on the cloud servers. we can directly copy the server images to get a large number of running instances. The recorded videos and control signals will be uploaded to the data center. We set up a series of video quality checks to filter out samples of low quality (still or overly noisy videos, and some undefined scenes). All games run at the highest quality while ensure the OBS screen recording does not get stuck. In order to avoid overly complicated situations, we removed the NPCs and running vehicles in the game. We use the Reshade to adjust the game scenes to make it more reality-like.\nObservation. We use OBS as the screen recorder. One can use scripts to control OBS for automatic recording. We recorded the game at native resolution of 2560 x 1600 (higher resolutions may cause the game and recording to lag). For the reality of the recorded videos, we removed GUIs and texts in the game through a Reshade plugin, namely ReshaderEffectShaderToggler.1 It can turn off the"}, {"title": "rendering of GUI related shaders in the game while left the native video untouched.", "content": "Forza Horizon 5. In Forza Horizon 5, a telemetry mechanism can be used for game status retrieving. We can access the real-time game data through socket after checking on the telemetry option in settings. An example script for data listening can be found here. We can access XYZ coordinates, velocities and accelerations. We use these data for stuck detection and sample filtering. Since Forza Horizon 5 is a game that mainly takes on 2D area, we apply automatic pipeline that randomly walking on different game scenes (like dessert, grassland, the watery and the snowy areas). Control signals are simplified to going forward, turning left and turning right. During the data collection stage, if XYZ of is still for several seconds, the controller will try to move back. And if the 40 position points collected during the last 40 seconds can be covered with a circle with radius of 80 meters, the controller will try to teleport the car to a random position. After raw sample collection, we apply some strategies to filter out samples of low quality. We use the acceleration data to detect if the car has collided with anything, and drop these video clips with collision. Sometimes the car is moving backward while the controlling input is moving forward, this is because the direction of movement in the game is to provide acceleration. We filter out data with a large angle between acceleration and velocity. Due to some problems in the game itself, the video often changes suddenly at some time. We filter out video clips with large average error between any two adjacent frames.\nCyberpunk 2077. Cyberpunk 2077 is a game that offers realistic visuals and lighting effects. Due to the complexity of the game terrain, we have to choose the manual pipeline. For simplicity, the actions in game are reduced to two separated inputs. The first one makes the character move forward or stop. And the second one makes the direction of the character's sight move up, down, left and right. We disable the NPCs and moving vehicles with game mod. During the data collection, players observe the game through low-definition OBS streaming and send control signals. The signals are then mapped into \"W\" (moving forward) / \u201cU\u201d \u201cD\u201d \"L\" \"R\" (up, down, left and right) on the cloud servers. We access and record the XYZ coordinates of player through Cheat Engine. These coordinate sequences are then used for filtering out video clips where collisions occur between the character and the game scene."}, {"title": "B.2. The Source Dataset", "content": "We present the Source dataset from three perspectives: basic information, the annotation method used to convert the original data from GameData Platform to our desired format, and the filtering method applied to remove undesirable"}, {"title": "data.", "content": "B.2.1. Basic Information\nThe Source comprises data from both Forza Horizon 5 and Cyberpunk 2077. For Forza Horizon 5, we collected approximately 1,200,000 pairs of video and control signals, while for Cyberpunk 2077, we gathered around 1,000,000 such pairs. All collected videos have a duration of approximately 6 seconds, recorded at 60 FPS. For Forza Horizon 5, we specifically collected data from multiple scenes, including deserts, oceans, water bodies, grasslands, and fields. The videos from different scenes are illustrated in Fig. A1, along with the distribution of data volume for each scene Fig. A2 (a). For Cyberpunk 2077, we focused on gathering data from urban environments that feature a significant number of tall buildings.\nIn Forza Horizon 5, the dataset includes only three distinct control signals: \"moving forward\" (denoted by \"D\"), \"moving forward and turning left\" (denoted by \"DL\"), and \"moving forward and turning right\" (denoted by \"DR\"). In contrast, the data for Cyberpunk 2077 encompasses five different control signals: \u201cmoving forward\" (denoted by \"W\"), \"turning left\" (denoted by \u201cL\u201d), \u201cturning right\" (denoted by \"R\"), \"looking upward\u201d (denoted by \u201cU\u201d), and \u201clooking downward\" (denoted by \"D\").\nB.2.2. Annotation Methods\nThe original data from GameData Platform typically has a duration of around 10 minutes, which is excessively long for training The Matrix. Therefore, we use FFmpeg [8] to segment these videos into 6-second clips. Next, we extract the corresponding control signals from the complete set of signals. We then use InternVL [6] to generate captions based on 12 uniformly extracted key frames from the videos. After the captioning process with InternVL, we perform manual corrections on the generated captions to eliminate obvious errors.\nB.2.3. Filtering Methods\nAfter the annotation step, a significant amount of undesired data remains, which could disrupt the training of The Matrix. To address this, we employ five filtering methods to eliminate these problematic data points, which we introduce as follows. Note that for Cyberpunk 2077, since we utilize human data collection rather than automatic methods, many of the following issues do not exist.\nBalance Control Signals. Balancing the number of different control signals is beneficial for the training of The Matrix. The process of balancing control signals consists of three steps: 1) First, we analyze the distribution of control signals for each 6-second video and record the results. 2) Next, we assess the overall distribution of control signals across the entire dataset to identify the most frequently occurring control signal. 3) Finally, we remove some data points that contain the highest proportion of this predominant control signal. We repeatedly implement the second and third steps until the distribution is relatively balanced. The distribution results for Forza Horizon 5 and Cyberpunk 2077 are reported in Fig. A2 (b). We provide the pseudocode for the algorithm in Algorithm 1."}, {"title": "Algorithm 1 Control Signal Balancing Algorithm", "content": "Require: Dataset D containing control signals from 6-second videos\nEnsure: Balanced Dataset B\n1: Initialize B as an empty set\n2: for each video v in D do\n3:\n4:\nAnalyze the distribution of control signals in v\nRecord the results for v\n5: end for\n6:\n7:\n8:\n9:\nwhile not isBalanced(B) do\noverall Distribution \u2190 Assess the overall distribution\nof control signals in D\nmostFrequentSignal \u2190 Identify the most frequently\noccurring control signal from overallDistribution\nD\u2190 Remove data points from D that contain most-FrequentSignal\n10: end while\n11: Set B\u2190D\n12: return B\nDetect and Remove the Data with Collisions. In Forza Horizon 5, randomly generated control signals often cause the car to collide with walls or rocks. Additionally, the car may be struck by other vehicles. These collisions can severely disrupt the training process, making it essential to identify and remove collision-affected data. Our analysis revealed that collisions consistently result in abrupt changes in acceleration over a very short time. Thus, we use significant variations in acceleration as a reliable indicator of collision events and discard any corresponding data to maintain the integrity of the training process.\nDetect and Remove Stuck Data. In Forza Horizon 5, after colliding with walls or rocks, the car often gets stuck; even when the \"D\" key is pressed, the car fails to move. This stuck situation complicates the training data and negatively impacts the performance of The Matrix. Therefore, we need to detect and remove such instances. Detecting when the car is stuck is relatively straightforward-we simply calculate the distance the car has traveled within the video. If this distance falls below a certain threshold, we conclude that the car is stuck and discard the corresponding data.\nDetect and Remove the Data with Mismatched Motion and Control. As introduced in Appendix B.1, to quickly resolve a stuck situation, the car will move backward when stuck. As a result, it is possible for the car to still move"}, {"title": "backward at a slower speed even when the \u201cD,\u201d \u201cDL,\u201d or \"DR\" keys are pressed.", "content": "Similar situations may arise when \u201cDL/DR\u201d is pressed for a long period and then switched to \u201cDR/DL.\u201d Although the acceleration is directed to the right/left, the car may continue to move in the opposite direction for a brief period. We refer to this as mismatched motion and control, which complicates the training process. To address this issue, we calculate the directions of both the acceleration and the car's movement. If the angle between these two directions is too large, we discard the corresponding data.\nDetect and Remove Artifacts. In Forza Horizon 5, visual artifacts can occur when a car collides with obstacles like trees, introducing distortions into the generated videos. To filter out such corrupted data, we detect variations in pixel values across consecutive frames. Our analysis shows that applying a high threshold effectively identifies all videos containing these artifacts, enabling their removal.\nB.3. The DROID dataset\nB.3.1. Basic Information\nDROID is a large, diverse robot manipulation dataset containing 76k demonstration trajectories (350 hours of interaction) collected across 564 scenes and 86 tasks over"}]}