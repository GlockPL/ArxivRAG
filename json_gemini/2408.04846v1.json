{"title": "UGrid: An Efficient-And-Rigorous Neural Multigrid Solver for Linear PDEs", "authors": ["Xi Han", "Fei Hou", "Hong Qin"], "abstract": "Numerical solvers of Partial Differential Equa-\ntions (PDEs) are of fundamental significance to\nscience and engineering. To date, the historical\nreliance on legacy techniques has circumscribed\npossible integration of big data knowledge and\nexhibits sub-optimal efficiency for certain PDE\nformulations, while data-driven neural methods\ntypically lack mathematical guarantee of conver-\ngence and correctness. This paper articulates a\nmathematically rigorous neural solver for linear\nPDEs. The proposed UGrid solver, built upon the\nprincipled integration of U-Net and MultiGrid,\nmanifests a mathematically rigorous proof of both\nconvergence and correctness, and showcases high\nnumerical accuracy, as well as strong generaliza-\ntion power to various input geometry/values and\nmultiple PDE formulations. In addition, we de-\nvise a new residual loss metric, which enables\nself-supervised training and affords more stability\nand a larger solution space over the legacy losses.", "sections": [{"title": "1. Introduction", "content": "Background and Major Challenges. PDEs are quintessen-\ntial to various computational problems in science, engineer-\ning, and relevant applications in simulation, modeling, and\nscientific computing. Numerical solutions play an irreplace-\nable role in common practice because in rare cases do PDEs\nhave analytic solutions, and many general-purpose numer-\nical methods have been made available. Iterative solvers\nare one of the most-frequently-used methods to\nobtain a numerical solution of a PDE. Combining iterative\nDepartment of Computer Science, Stony Brook University\n(SUNY), Stony Brook, NY 11794, USA. 2Key Laboratory of\nSystem Software (Chinese Academy of Sciences) and State Key\nLaboratory of Computer Science, Institute of Software, Chinese\nAcademy of Sciences, Beijing, 100190, China. University of\nChinese Academy of Sciences, Beijing, 100049, China. Corre-\nspondence to: Xi Han <xihan1@cs.stonybrook.edu>, Fei Hou\n<houfei@ios.ac.cn>, Hong Qin solvers with the multigrid method significantly enhances the performance for large-scale\nproblems. Yet, the historical reliance on legacy generic nu-\nmerical solvers has circumscribed possible integration of\nbig data knowledge and exhibits sub-optimal efficiency for\ncertain PDE formulations. In contrast, recent deep neural\nmethods have the potential to learn such knowledge from big\ndata and endow numerical solvers with compact structures\nand high efficiency, and have achieved impressive results\n. However, many currently available\nneural methods treat deep networks as black boxes. Other\nneural methods are typically trained in a fully supervised\nmanner on loss functions that directly compare the predic-\ntion and the ground truth solution, confining the solution\nspace and resulting in numerical oscillations in the relative\nresidual error even after convergence. These methods gener-\nally have challenges unconquered including, a lack of sound\nmathematical backbone, no guarantee of correctness or con-\nvergence, and low accuracy, thus unable to handle complex,\nunseen scenarios.\nMotivation and Method Overview. Inspired by 's prior work on integrating the structure of\nmultigrid V-cycles  and U-\nNet  with convergence guarantee,\nand to achieve high efficiency and strong robustness, we\naim to fully realize neural methods' modeling and computa-\ntional potential by implanting the legacy numerical methods'\nmathematical backbone into neural methods in this paper. In\norder to make our new framework fully explainable, we pro-\npose the UGrid framework based on the\nstructure of multigrid V-cycles for learning the functionality\nof multigrid solvers. We also improve the convolutional\noperators originating from to incorpo-\nrate arbitrary boundary conditions and multiple differential\nstencils without modifying the overall structure of the key\niteration process, and transform the iterative update rules\nand the multigrid V-cycles into a concrete Convolutional\nNeural Network (CNN) structure.\nKey Contributions. The salient contributions of this pa-\nper comprise: (1) Theoretical insight. We introduce a novel\nexplainable neural PDE solver founded on a solid mathemat-\nical background, affording high efficiency, high accuracy,\nand strong generalization power to linear PDEs; (2) New\nloss metric. We propose a residual error metric as the loss"}, {"title": "2. Related Work", "content": "Black-box-like Neural PDE Solvers. Much research effort\nhas been devoted to numerically solve PDEs with neural\nnetworks and deep learning techniques. However, most of\nthe previous work treats neural networks as black boxes\nand thus come with no mathematical proof of convergence\nand correctness. As early as the 1990s, applied simple neural networks to solve\nlinear equations. Later, more effective neural-network-based\nmethods like were proposed to\nsolve the Poisson equations. On the other hand, used Recurrent Neural Networks (RNNs) in\nsolving systems of linear matrix equations. Most recently,\nthe potential of CNNs and Generative Adversarial Networks\n(GANs) on solving PDEs was further explored by Utilities used for neural\nPDE solvers also include backward stochastic differential\nequations and PN junctions. On the contrary, the proposed UGrid mimics the\nmultigrid solver, and all its contents are explainable and\nhave corresponding counterparts in an MG hierarchy.\nPhysics-informed Neural PDE Solvers. Physics-informed\nNeural Network (PINN)-based solvers effectively optimize\nthe residual of the solution. Physical properties, including\npressure, velocity and non-locality are also used to articulate neural solvers. Math-\nematical proofs on the minimax optimal bounds and structural improvements are\nalso made on the PINN architecture itself, endowing physics-\ninformed neural PDE solvers with higher efficiency and\ninterpretability. Hinted by these, we propose the residual\nloss metric, which enables self-supervised training, enlarges\nthe solution space and enhances numerical stability.\nNeural PDE Solvers with Mathematical Backbones.\n proposed a NN-based linear system\nand its solving algorithm with a convergence guarantee.\n modified the Jacobi iterative solver by\npredicting an additional correction term with a multigrid-\ninspired linear operator, and proposed a linear neural solver\nwith guarantee on correctness upon convergence. proposed to learn a mapping from a fam-\nily of PDEs to the optimal prolongation operator used in\nthe multigrid method, which is then extended to Algebraic\nMultigrids (AMGs) on non-square meshes via Graph Neu-\nral Networks (GNNs) by . On the other\nhand, proposed a Fourier neural operator\nthat learns mappings between function spaces by parameter-\nizing the integral kernel directly in Fourier space. In theory,\n proved that when a PDE's coefficients"}, {"title": "3. Mathematical Preliminary", "content": "For mathematical completeness, we provide readers with a\nbrief introduction to the concepts that are frequently seen in\nthis paper.\nDiscretization of 2D Linear PDEs. A linear PDE with\nDirichlet boundary condition could be discretized with fi-\nnite differencing techniques , and could be\nexpressed in the following form:\n$\\begin{cases}\nD u(x,y) = f(x,y), &(x, y) \\in \\text{I}\\\\\n u(x,y) = b(x, y), &(x, y) \\in \\text{B}\n\\end{cases}$\nwhere D is a 2D discrete linear differential operator, S is the\nset of all points on the discrete grid, B is the set of boundary\npoints in the PDE, I = S \\ B is the set of interior points in\nthe PDE, IS C B is the set of trivial boundary points of the\ngrid.\nUsing D's corresponding finite difference stencil, Eq. 1 can\nbe formulated into a sparse linear system of size n\u00b2 \u00d7 n\u00b2:\n$\\begin{cases}\n(I \u2013 M)Au = (I \u2013 M)f\\\\nMu = Mb\n\\end{cases}$\nwhere A \u2208 Rn\u00b2xn\u00b2 is the 2D discrete differential operator,\nu \u2208 Rn\u00b2 encodes the function values of the interior points\nand the non-trivial boundary points; f \u2208 Rn\u00b2 encodes the\ncorresponding partial derivatives of the interior points; b \u2208\nRn\u00b2 encodes the non-trivial boundary values; I denotes the\nn\u00b2 \u00d7 n\u00b2 identity matrix; M \u2208 {0,1}n\u00b2\u00d7n\u00b2 is a diagonal\nbinary boundary mask defined as\n$M_{k,k} = \\begin{cases}\n1, &(i, j) \\in B \\backslash dS\\\\n0, &(i, j) \\in I\n\\end{cases}, k = in+j, 0 \\leq i, j < n.$\nOn the contrary of Eq. 1, both equations in Eq. 2 hold for\nall grid points.\nError Metric And Ground-truth Solution. When us-\ning numerical solvers, researchers typically substitute the\nboundary mask M into the discrete differential matrix A\nand the partial derivative vector f, and re-formulate Eq. 2\ninto the following generic sparse linear system:\nAu = f.\nThe residual of a numerical solution u is defined as\nr(u) = f \u2013 A u.\nIn the ideal case, the absolute residual error of an exact\nsolution u* should be ru* = ||r(u*)|| = 0. However, in"}, {"title": "4. Novel Approach", "content": "The proposed UGrid neural solver is built upon the princi-\npled integration of the U-Net architecture and the multigrid\nmethod. We observe that linear differential operators, as\nwell as their approximate inverse in legacy iterative solvers,\nare analogous to convolution operators. E.g., the discrete\nLaplacian operator is a 3 \u00d7 3 convolution kernel. Further-\nmore, the multigrid V-cycle is also analogous to the U-\nNet architecture, with grid transfer operators mapped to\nup/downsampling layers. Moreover, the fine-tuning process\nof multigrid's critical components on specific PDE formula-\ntions could be completed by learning from big data. These\ntechnical observations lead to our neural implementation\nand optimization of the multigrid routine.\nIn spite of high efficiency, generalization power remains a\nmajor challenge for neural methods. Many SOTA neural\nsolvers, e.g., , fail to generalize to new\nscenarios unobserved during the training phase. Such new\nscenarios include: (1) New problem sizes; and (2) New,\ncomplex boundary conditions and right-hand sides, which\nincludes geometries, topology, and values (noisy inputs).\nMoreover, some of these methods are tested on Poisson\nequations only; neither mathematical reasoning nor empiri-\ncal results show that they could trivially generalize to other\nPDEs (with or without retraining). UGrid resolves all prob-\nlems above.\nUGrid is comprised of the following components: (1) The\nfixed neural smoother, which consists of our proposed convo-\nlutional operators (Sec. 4.1); (2) The learnable neural multi-\ngrid, which consists of our UGrid submodule (Sec. 4.2);\n(3) A residual loss metric (Sec. 4.3) which enables the self-\nsupervised training process."}, {"title": "4.1. Convolutional Operators", "content": "This subsection is organized as follows: Sec. 4.1.1 intro-\nduces the masked operators, which mimic the smoothers\nin a legacy multigrid routine; and Sec. 4.1.2 introduces the\nmasked residual operators for residual calculation.\n4.1.1. MASKED CONVOLUTIONAL ITERATOR\nA trivial linear iterator in the form of Eq. 7 does not fit in\na neural routine. This is because in practice, the system\nmatrix A for its corresponding differential operator encodes\nthe boundary geometry and turns into matrix A in Eq. 4.\nA's elements are input-dependent, and is thus impossible to\nbe expressed as a fix-valued convolution kernel.\nWe make use of the masked version of PDEs (Eq. 2) and\ntheir masked convolutional iterators, which are natural ex-\ntensions of Eq. 7:\n$u^{k+1} = (I \u2013 M) ((I \u2013 P^{-1}A) u^k + P^{-1}f) + Mb,$\nwhere P is an easily-invertible approximation on the dis-\ncrete differential operator A. The correctness of Eq. 8\nis guaranteed by the following Theorem (proved as Theo-\nrem A.3 in the appendix):\nMatrix A has different formulations for different linear\nPDEs. Without loss of generality, we consider the following\nproblem formulations. Other possible PDE formulations\nare essentially combinations of differential primitives avail-\nable in the three problems below, and our convolutional\noperators could be trivially extended to higher orders of\ndifferentiation.\n2D Poisson Problem. Under Dirichlet boundary condition,\na Poisson problem could be expressed as follows:\n$\\begin{cases}\n\u2207\u00b2u(x,y) = f(x,y), &(x, y) \\in \\text{I}\\\\\nu(x, y) = b(x, y), &(x, y) \\in \\text{B}\\'\n\\end{cases}$\nwhere u is the unknown scalar field, f is the Laplacian field,\nand b is the Dirichlet boundary condition.\nMatrix A could be assembled by the five-point finite dif-\nference stencil for 2D Laplace operators , and\nwe could simply let P = -4I, where I denotes the identity\nmatrix. The update rule specified in Eq. 8 thus becomes\n$\\begin{aligned}\nu_{k+1}(i, j) = (I \u2013 M) (u_k(i \u2212 1, j) + u_x(i + 1, j) \\\\\n+ u_k(i, j \u2013 1) + u_k(i, j + 1) \u2013 f) + Mb.\\\n\\end{aligned}$"}, {"title": "4.1.2. CONVOLUTIONAL RESIDUAL OPERATOR", "content": "Except for the smoother, the multigrid method also requires\nthe calculation of the residual in each iteration step. In\npractice, the residual operator (Eq. 5) can also be seamlessly\nimplemented as a convolution layer. Because our masked\niterator (Eq. 8) guarantees that u satisfies Mu = Mb at any\niteration step, the residual operator for Poisson problems\ncould be simplified into\nr(u) = (1 \u2013 M) (f - u * L), L=$\\begin{pmatrix}\n0 & 1 & 0\\\\n1 & -4 & 1\\\\n0 & 1 & 0\n\\end{pmatrix}$\nFor Helmholtz problems, Eq. 16 could be naturally extended\ninto\nr(u) = (1 \u2013 M) (f \u2013 u * L \u2013 k\u00b2u),\nwhere all notations retain their meanings as in Eq. 16.\nFor steady-state convection-diffusion-reaction problems, we\ncould extend Eq. 16 into\nr(u) = (1 - M)\n(f + vx(u *Jx) + vy(u * Jy) + au * L \u2013 \u03b2u),"}, {"title": "4.2. Neural Network Design", "content": "UGrid Iteration. We design the iteration step of our neural\niterator as a sequence of operations as follows (which is\nillustrated in Fig. 1):\nu = smooth\u00b9 (u)        (Pre-smooth for v\u2081 times);\nr = r(u)                (Calculate the current residual);\n\u03b4 = UGrid(r, 1 \u2013 M)       (UGrid submodule recursion);\nu = u + \u03b4               (Apply the correction term);\nu = smooth\u00b2 (u)         (Post-smooth for v\u2082 times).\nThe entire iteration process is specifically designed to em-\nulate the multigrid iteration : We use the pre-\nsmoothing and post-smoothing layers (as specified in Eq. 11)\nto eliminate the high-frequency modes in the residual r, and\ninvoke the UGrid submodule to eliminate the low-frequency\nmodes."}, {"title": "4.3. Loss Function Design", "content": "Legacy Loss Metric. We refer the equivalents of the mean\nrelative error between the prediction and a ground-truth\nsolution as the legacy loss metric. The following Theorem\n(proved in Sec. A.5) shows that though intuitive, the legacy\nloss is unstable:\nWe have conducted ablation studies on our relative loss met-\nric and the legacy loss (Sec. 5 and Sec. A.6). The results\nshowcase that the proposed residual loss performs better\nthan the legacy loss in terms of both efficiency and accuracy.\nHeuristically, the proposed residual loss metric is closer\nto the fundamental definition of the precision of a PDE's\nsolution, and is more robust and stable, because upon con-\nvergence, it guarantees an input-independent final accuracy.\nMoreover, the self-supervised training process endows our\nmethod with the following merits: (1) Easier data genera-\ntion (compared to other neural routines which are trained\non the legacy loss), and thus achieve better numerical per-\nformance; (2) For a specific PDE formulation, we could\neasily get a decent neural multigrid solver optimized for that\nspecific formulation (which outperforms existing general-\npurpose legacy routines), simply by training UGrid on the\ndata generated. On the contrary, fine-tuning legacy solvers is"}, {"title": "5. Experiments and Evaluations", "content": "Experiments Overview. For each of the three PDEs men-\ntioned in Sec. 4.1: (i) Poisson problem, (ii) inhomogeneous\nHelmholtz problem with varying wave numbers, and (iii)\ninhomogeneous steady-state diffusion-convection-reaction\nproblem, we train one UGrid model specialized for its for-\nmulation. We apply our model and the baselines to the\ntask of 2.5D freeform surface modeling. These surfaces are\nmodeled by the three types of PDEs as 2D height fields,\nwith non-trivial geometry/topology. Each surface is dis-\ncretized into: (1) Small-scale problem: A linear system\nof size 66, 049 \u00d7 66,049; (2) Large-scale problem: A lin-\near system of size 1,050, 625 \u00d7 1,050, 625; (3) XL-scale\nproblem: A linear system of size 4, 198, 401 \u00d7 4, 198, 401;\nand (4) XXL-scale problem: A linear system of size\n16, 785, 409 \u00d7 16, 785, 409. UGrid is trained on the large\nscale only. Other problem sizes are designed to evaluate\nUGrid's generalization power and scalability.\nIn addition, we have conducted an ablation study on the\nresidual loss metric (v.s. legacy loss) as well as the UGrid\narchitecture itself (v.s. vanilla U-Net).\nData Generation and Implementation Details. Our new\nneural PDE solver is trained in a self-supervised manner\non the residual loss. Before training, we synthesized a\ndataset with 16000 (M, b, f) pairs. For Helmholtz and dif-\nfusion problems, we further random-sample their unique\ncoefficient fields (more details available in Sec. A.8 and\nSec. A.9.) To examine the generalization power of UGrid,\nthe geometries of boundary conditions in our training data\nare limited to \u201cDonuts-like\u201d shapes as shown in Fig. 3 (h).\nMoreover, all training data are restricted to zero f-fields\nonly, i.e., f = 0. Our UGrid model has 6 recursive Multi-\ngrid submodules. We train our model and perform all ex-\nperiments on a personal computer with 64GB RAM, AMD\nRyzen 9 3950x 16-core processor, and NVIDIA GeForce\nRTX 2080 Ti GPU. We train our model for 300 epochs\nwith the Adam optimizer. The learning rate is initially set\nto 0.001, and decays by 0.1 for every 50 epochs. We ini-\ntialize all learnable parameters with PyTorch's default ini-\ntialization policy. Our code is available as open-source at\nExperimental Results. We compare our model with two\nstate-of-the-art legacy solvers, AMGCL , and NVIDIA AmgX , as well as\none SOTA neural solver proposed by .\nOur testcases as shown in Fig. 3. These are all with complex\ngeometry and topology, and none of which are present in the\ntraining data, except the geometry of Fig. 3 (h). Testcase(s)"}, {"title": "4.1.1. MASKED CONVOLUTIONAL ITERATOR", "content": "A trivial linear iterator in the form of Eq. 7 does not fit in\na neural routine. This is because in practice, the system\nmatrix A for its corresponding differential operator encodes\nthe boundary geometry and turns into matrix A in Eq. 4.\nA's elements are input-dependent, and is thus impossible to\nbe expressed as a fix-valued convolution kernel.\nWe make use of the masked version of PDEs (Eq. 2) and\ntheir masked convolutional iterators, which are natural ex-\ntensions of Eq. 7:\n$u^{k+1} = (I \u2013 M) ((I \u2013 P^{-1}A) u^k + P^{-1}f) + Mb,$ (8)\nwhere P is an easily-invertible approximation on the dis-\ncrete differential operator A. The correctness of Eq. 8\nis guaranteed by the following Theorem (proved as Theo-\nrem A.3 in the appendix):\nMatrix A has different formulations for different linear\nPDEs. Without loss of generality, we consider the following\nproblem formulations. Other possible PDE formulations\nare essentially combinations of differential primitives avail-\nable in the three problems below, and our convolutional\noperators could be trivially extended to higher orders of\ndifferentiation.\n2D Poisson Problem. Under Dirichlet boundary condition,\na Poisson problem could be expressed as follows:\n$\\begin{cases}\n\u2207\u00b2u(x,y) = f(x,y), &(x, y) \u2208 \\text{I}\\\\\nu(x, y) = b(x, y), &(x, y) \u2208 \\text{B}\\'\n\\end{cases}$ (9)\nwhere u is the unknown scalar field, f is the Laplacian field,\nand b is the Dirichlet boundary condition.\nMatrix A could be assembled by the five-point finite dif-\nference stencil for 2D Laplace operators , and\nwe could simply let P = -4I, where I denotes the identity\nmatrix. The update rule specified in Eq. 8 thus becomes\n$\\begin{aligned}\nu_{k+1}(i, j) = (I \u2013 M) (u_k(i \u2212 1, j) + u_x(i + 1, j) \\\\\n+ u_k(i, j \u2013 1) + u_k(i, j + 1) \u2013 f) + Mb.\\\n\\end{aligned}$ (10)"}, {"title": "4.1.2. CONVOLUTIONAL RESIDUAL OPERATOR", "content": "Except for the smoother, the multigrid method also requires\nthe calculation of the residual in each iteration step. In\npractice, the residual operator (Eq. 5) can also be seamlessly\nimplemented as a convolution layer. Because our masked\niterator (Eq. 8) guarantees that u satisfies Mu = Mb at any\niteration step, the residual operator for Poisson problems\ncould be simplified into\nr(u) = (1 \u2013 M) (f - u * L), L=$\n\\begin{pmatrix}\n0 & 1 & 0\\\\n1 & -4 & 1\\\\n0 & 1 & 0\n\\end{pmatrix}$ (16)\nFor Helmholtz problems, Eq. 16 could be naturally extended\ninto\nr(u) = (1 \u2013 M) (f \u2013 u * L \u2013 k\u00b2u), (17)\nwhere all notations retain their meanings as in Eq. 16.\nFor steady-state convection-diffusion-reaction problems, we\ncould extend Eq. 16 into\nr(u) = (1 - M)\n(f + vx(u *Jx) + vy(u * Jy) + au * L \u2013 \u03b2u), (18)\nwhere all notations retain their meanings as in Eq. 16."}, {"title": "4.2. Neural Network Design", "content": "UGrid Iteration. We design the iteration step of our neural\niterator as a sequence of operations as follows (which is\nillustrated in Fig. 1):\nu = smooth\u00b9 (u) (Pre-smooth for v\u2081 times);\nr = r(u) (Calculate the current residual);\n\u03b4 = UGrid(r, 1 \u2013 M) (UGrid submodule recursion);\nu = u + \u03b4 (Apply the correction term);\nu = smooth\u00b2 (u) (Post-smooth for v\u2082 times).\nThe entire iteration process is specifically designed to em-\nulate the multigrid iteration : We use the pre-\nsmoothing and post-smoothing layers (as specified in Eq. 11)\nto eliminate the high-frequency modes in the residual r, and\ninvoke the UGrid submodule to eliminate the low-frequency\nmodes."}, {"title": "4.3. Loss Function Design", "content": "Legacy Loss Metric. We refer the equivalents of the mean\nrelative error between the prediction and a ground-truth\nsolution as the legacy loss metric. The following Theorem\n(proved in Sec. A.5) shows that though intuitive, the legacy\nloss is unstable:\nWe have conducted ablation studies on our relative loss met-\nric and the legacy loss (Sec. 5 and Sec. A.6). The results\nshowcase that the proposed residual loss performs better\nthan the legacy loss in terms of both efficiency and accuracy.\nHeuristically, the proposed residual loss metric is closer\nto the fundamental definition of the precision of a PDE's\nsolution, and is more robust and stable, because upon con-\nvergence, it guarantees an input-independent final accuracy.\nMoreover, the self-supervised training process endows our\nmethod with the following merits: (1) Easier data genera-\ntion (compared to other neural routines which are trained\non the legacy loss), and thus achieve better numerical per-\nformance; (2) For a specific PDE formulation, we could\neasily get a decent neural multigrid solver optimized for that\nspecific formulation (which outperforms existing general-\npurpose legacy routines), simply by training UGrid on the\ndata generated. On the contrary, fine-tuning legacy solvers is"}, {"title": "5. Experiments and Evaluations", "content": "Experiments Overview. For each of the three PDEs men-\ntioned in Sec. 4.1: (i) Poisson problem, (ii) inhomogeneous\nHelmholtz problem with varying wave numbers, and (iii)\ninhomogeneous steady-state diffusion-convection-reaction\nproblem, we train one UGrid model specialized for its for-\nmulation. We apply our model and the baselines to the\ntask of 2.5D freeform surface modeling. These surfaces are\nmodeled by the three types of PDEs as 2D height fields,\nwith non-trivial geometry/topology. Each surface is dis-\ncretized into: (1) Small-scale problem: A linear system\nof size 66, 049 \u00d7 66,049; (2) Large-scale problem: A lin-\near system of size 1,050, 625 \u00d7 1,050, 625; (3) XL-scale\nproblem: A linear system of size 4, 198, 401 \u00d7 4, 198, 401;\nand (4) XXL-scale problem: A linear system of size\n16, 785, 409 \u00d7 16, 785, 409. UGrid is trained on the large\nscale only. Other problem sizes are designed to evaluate\nUGrid's generalization power and scalability.\nIn addition, we have conducted an ablation study on the\nresidual loss metric (v.s. legacy loss) as well as the UGrid\narchitecture itself (v.s. vanilla U-Net).\nData Generation and Implementation Details. Our new\nneural PDE solver is trained in a self-supervised manner\non the residual loss. Before training, we synthesized a\ndataset with 16000 (M, b, f) pairs. For Helmholtz and dif-\nfusion problems, we further random-sample their unique\ncoefficient fields (more details available in Sec. A.8 and\nSec. A.9.) To examine the generalization power of UGrid,\nthe geometries of boundary conditions in our training data\nare limited to \u201cDonuts-like\u201d shapes as shown in Fig. 3 (h).\nMoreover, all training data are restricted to zero f-fields\nonly, i.e., f = 0. Our UGrid model has 6 recursive Multi-\ngrid submodules. We train our model and perform all ex-\nperiments on a personal computer with 64GB RAM, AMD\nRyzen 9 3950x 16-core processor, and NVIDIA GeForce\nRTX 2080 Ti GPU. We train our model for 300 epochs\nwith the Adam optimizer. The learning rate is initially set\nto 0.001, and decays by 0.1 for every 50 epochs. We ini-\ntialize all learnable parameters with PyTorch's default ini-\ntialization policy. Our code is available as open-source at\nExperimental Results. We compare our model with two\nstate-of-the-art legacy solvers, AMGCL , and NVIDIA AmgX , as well as\none SOTA neural solver proposed by .\nOur testcases as shown in Fig. 3. These are all with complex\ngeometry and topology, and none of which are present in the\ntraining data, except the geometry of Fig. 3 (h). Testcase(s)"}, {"title": "Lemma A.1. For a fixed linear iterator in the form of", "content": "$u^{k+1} = G u^k + c,$(21)\nwith a square update matrice G having a spectral radius \u03c1(G) < 1, I \u2212 G is non-singular, and Eq. 21 converges for any\nconstant c and initial guess u0. Conversely, if Eq. 21 converges for any c and u0, then \u03c1(G) < 1."}, {"title": "4.1.2. CONVOLUTIONAL RESIDUAL OPERATOR", "content": "4.1.2. CONVOLUTIONAL RESIDUAL OPERATOR\\nExcept for the smoother, the multigrid method also requires\\nthe calculation of the residual in each iteration step. In\\npractice, the residual operator (Eq. 5) can also be seamlessly\\nimplemented as a convolution layer. Because our masked\\niterator (Eq. 8) guarantees that u satisfies Mu = Mb at any\\niteration step, the residual operator for Poisson problems\\ncould be simplified into\\nr(u) = (1 - M) (f - u \u2217 L), L=\\begin{pmatrix}\\n0 & 1 & 0\\\\n1 & -4 & 1\\\\n0 & 1 & 0\\end{pmatrix} (16)\\nFor Helmholtz problems, Eq. 16 could be naturally extended\\ninto\\nr(u) = (1 - M) (f - u \u2217 L - k2u), (17)\\nwhere all notations retain their meanings as in Eq. 16.\\nFor steady-state convection-diffusion-reaction problems, we\\ncould extend Eq. 16 into\\nr(u) = (1 - M)\\n(f + vx(u \u2217 Jx) + vy(u \u2217 Jy) + au \u2217 L - \u03b2u), (18)\\nwhere all notations retain their meanings as in Eq. 16."}, {"title": "Lemma A.1. For a fixed linear iterator in the form of", "content": "$u^{k+1} = G u^k + c, $(21)\\nwith a square update matrice G having a spectral radius \u03c1(G) < 1, I \u2212 G is non-singular, and Eq. 21 converges for any\\nconstant c and initial guess u0. Conversely, if Eq. 21 converges for any c and u0, then \u03c1(G) < 1."}, {"title": "4.1.1. MASKED CONVOLUTIONAL ITERATOR", "content": "A trivial linear iterator in the form of Eq. 7 does not fit in\\na neural routine. This is because in practice, the system\\nmatrix A for its corresponding differential operator encodes\\nthe boundary geometry and turns into matrix A in Eq. 4.\\nA's elements are input-dependent, and is thus impossible to\\nbe expressed as a fix-valued convolution kernel.\\nWe make use of the masked version of PDEs (Eq. 2) and\\ntheir masked convolutional iterators, which are natural ex-\\ntensions of Eq. 7:\\n$\\begin{cases}\\n\u2207\u00b2u(x,y) = f(x,y), &(x, y) \u2208 \\text{I}\\\\\\nu(x, y) = b(x, y), &(x, y) \u2208 \\text{B}\\'\\end{cases} (9)\\nwhere u is the unknown scalar field, f is the Laplacian field,\\nand b is the Dirichlet boundary condition.\\nMatrix A could be assembled by the five-point finite dif-\\nference stencil for 2D Laplace operators , and\\nwe could simply let P = -4I, where I denotes the identity\\nmatrix. The update rule specified in Eq. 8 thus becomes\\n\\begin{aligned}\\nu_{k+1}(i, j) = (I - M) (u_k(i \u2212 1, j) + u_x(i + 1, j) \\\\\\n+ u_k(i, j \u2013 1) + u_k(i, j + 1) - f) + Mb.\\\\\n\\end{aligned} (10)"}, {"title": "A.5. Proof of Theorem. 4.2", "content": "Proof. Denote ex as x's relative residual error, then we have:\n\nEx\n=\nf-Ax\n=\n\nf\nf\n- Ax\n+\n-Ay\n-Ax\nf\n+\nAy\n=\n(f-\u00c3y) + (max \u00c3y)\nf +\nAy\nmax Ay\nf\n+\nAy\n= 0 because in most cases, a PDE's\nground-truth solution could only be a numerical approximation with errors. The upper bound is input-dependant because A\nand f are input-dependent."}, {}]}