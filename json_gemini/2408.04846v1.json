{"title": "UGrid: An Efficient-And-Rigorous Neural Multigrid Solver for Linear PDEs", "authors": ["Xi Han", "Fei Hou", "Hong Qin"], "abstract": "Numerical solvers of Partial Differential Equations (PDEs) are of fundamental significance to science and engineering. To date, the historical reliance on legacy techniques has circumscribed possible integration of big data knowledge and exhibits sub-optimal efficiency for certain PDE formulations, while data-driven neural methods typically lack mathematical guarantee of convergence and correctness. This paper articulates a mathematically rigorous neural solver for linear PDEs. The proposed UGrid solver, built upon the principled integration of U-Net and MultiGrid, manifests a mathematically rigorous proof of both convergence and correctness, and showcases high numerical accuracy, as well as strong generalization power to various input geometry/values and multiple PDE formulations. In addition, we devise a new residual loss metric, which enables self-supervised training and affords more stability and a larger solution space over the legacy losses.", "sections": [{"title": "1. Introduction", "content": "Background and Major Challenges. PDEs are quintessential to various computational problems in science, engineering, and relevant applications in simulation, modeling, and scientific computing. Numerical solutions play an irreplaceable role in common practice because in rare cases do PDEs have analytic solutions, and many general-purpose numerical methods have been made available. Iterative solvers are one of the most-frequently-used methods to obtain a numerical solution of a PDE. Combining iterative solvers with the multigrid method significantly enhances the performance for large-scale problems. Yet, the historical reliance on legacy generic numerical solvers has circumscribed possible integration of big data knowledge and exhibits sub-optimal efficiency for certain PDE formulations. In contrast, recent deep neural methods have the potential to learn such knowledge from big data and endow numerical solvers with compact structures and high efficiency, and have achieved impressive results. However, many currently available neural methods treat deep networks as black boxes. Other neural methods are typically trained in a fully supervised manner on loss functions that directly compare the prediction and the ground truth solution, confining the solution space and resulting in numerical oscillations in the relative residual error even after convergence. These methods generally have challenges unconquered including, a lack of sound mathematical backbone, no guarantee of correctness or convergence, and low accuracy, thus unable to handle complex, unseen scenarios.\nMotivation and Method Overview. Inspired by prior work on integrating the structure of multigrid V-cycles and U-Net with convergence guarantee, and to achieve high efficiency and strong robustness, we aim to fully realize neural methods' modeling and computational potential by implanting the legacy numerical methods' mathematical backbone into neural methods in this paper. In order to make our new framework fully explainable, we propose the UGrid framework based on the structure of multigrid V-cycles for learning the functionality of multigrid solvers. We also improve the convolutional operators originating from to incorporate arbitrary boundary conditions and multiple differential stencils without modifying the overall structure of the key iteration process, and transform the iterative update rules and the multigrid V-cycles into a concrete Convolutional Neural Network (CNN) structure.\nKey Contributions. The salient contributions of this paper comprise: (1) Theoretical insight. We introduce a novel explainable neural PDE solver founded on a solid mathematical background, affording high efficiency, high accuracy, and strong generalization power to linear PDEs; (2) New loss metric. We propose a residual error metric as the loss function, which optimizes the residual of the prediction. Our newly-proposed error metric enables self-supervised learning and facilitates the unrestricted exploration of the solution space. Meanwhile, it eliminates the numerical oscillation on the relative residual error upon convergence, which has been frequently observed on the legacy mean-relative-error-based loss metrics; and (3) Extensive experiments. We demonstrate our method's capability to numerically solve PDEs by learning multigrid operators of various linear PDEs subject to arbitrary boundary conditions of complex geometries and topology, whose patterns are unseen during the training phase. Extensive experiments and comprehensive evaluations have verified all of our claimed advantages, and confirmed that our proposed method outperforms the SOTA."}, {"title": "2. Related Work", "content": "Black-box-like Neural PDE Solvers. Much research effort has been devoted to numerically solve PDEs with neural networks and deep learning techniques. However, most of the previous work treats neural networks as black boxes and thus come with no mathematical proof of convergence and correctness. As early as the 1990s, applied simple neural networks to solve linear equations. Later, more effective neural-network-based methods like were proposed to solve the Poisson equations. On the other hand, used Recurrent Neural Networks (RNNs) in solving systems of linear matrix equations. Most recently, the potential of CNNs and Generative Adversarial Networks (GANs) on solving PDEs was further explored by . Utilities used for neural PDE solvers also include backward stochastic differential equations and PN junctions. On the contrary, the proposed UGrid mimics the multigrid solver, and all its contents are explainable and have corresponding counterparts in an MG hierarchy.\nPhysics-informed Neural PDE Solvers. Physics-informed Neural Network (PINN)-based solvers effectively optimize the residual of the solution. Physical properties, including pressure, velocity and non-locality are also used to articulate neural solvers. Mathematical proofs on the minimax optimal bounds and structural improvements are also made on the PINN architecture itself, endowing physics-informed neural PDE solvers with higher efficiency and interpretability. Hinted by these, we propose the residual loss metric, which enables self-supervised training, enlarges the solution space and enhances numerical stability.\nNeural PDE Solvers with Mathematical Backbones. proposed a NN-based linear system and its solving algorithm with a convergence guarantee. modified the Jacobi iterative solver by predicting an additional correction term with a multigrid-inspired linear operator, and proposed a linear neural solver with guarantee on correctness upon convergence. proposed to learn a mapping from a family of PDEs to the optimal prolongation operator used in the multigrid method, which is then extended to Algebraic Multigrids (AMGs) on non-square meshes via Graph Neural Networks (GNNs) by . On the other hand, proposed a Fourier neural operator that learns mappings between function spaces by parameterizing the integral kernel directly in Fourier space. In theory, proved that when a PDE's coefficients are representable by small NNs, the number of parameters needed will increase in a polynomial fashion with the input dimension."}, {"title": "3. Mathematical Preliminary", "content": "For mathematical completeness, we provide readers with a brief introduction to the concepts that are frequently seen in this paper.\nDiscretization of 2D Linear PDEs. A linear PDE with Dirichlet boundary condition could be discretized with finite differencing techniques, and could be expressed in the following form:\n$\\begin{cases}Du(x,y) = f(x,y), &(x, y) \\in \\mathcal{I}\\\\ u(x,y) = b(x, y), &(x, y) \\in \\mathcal{B'}\\end{cases}$ (1)\nwhere $D$ is a 2D discrete linear differential operator, $\\mathcal{S}$ is the set of all points on the discrete grid, $\\mathcal{B}$ is the set of boundary points in the PDE, $\\mathcal{I} = \\mathcal{S} \\setminus \\mathcal{B}$ is the set of interior points in the PDE, $\\mathcal{IS} \\subset \\mathcal{B}$ is the set of trivial boundary points of the grid.\nUsing $D$'s corresponding finite difference stencil, Eq. 1 can be formulated into a sparse linear system of size $n^2 \\times n^2$:\n$\\begin{cases}(I - M)Au = (I - M)f\\\\ Mu = Mb\\end{cases}$ (2)\nwhere $A \\in \\mathbb{R}^{n^2 \\times n^2}$ is the 2D discrete differential operator, $u \\in \\mathbb{R}^{n^2}$ encodes the function values of the interior points and the non-trivial boundary points; $f \\in \\mathbb{R}^{n^2}$ encodes the corresponding partial derivatives of the interior points; $b \\in \\mathbb{R}^{n^2}$ encodes the non-trivial boundary values; $I$ denotes the $n^2 \\times n^2$ identity matrix; $M \\in \\{0,1\\}^{n^2 \\times n^2}$ is a diagonal binary boundary mask defined as\n$M_{k,k} = \\begin{cases}1,&(i, j) \\in \\mathcal{B} \\setminus dS\\\\ 0,&(i, j) \\in \\mathcal{I}\\end{cases}$, $k = in+j, 0 \\leq i, j < n.$ (3)\nOn the contrary of Eq. 1, both equations in Eq. 2 hold for all grid points.\nError Metric And Ground-truth Solution. When using numerical solvers, researchers typically substitute the boundary mask $M$ into the discrete differential matrix $A$ and the partial derivative vector $f$, and re-formulate Eq. 2 into the following generic sparse linear system:\n$Au = f.$ (4)\nThe residual of a numerical solution $u$ is defined as\n$r(u) = f - A u.$ (5)\nIn the ideal case, the absolute residual error of an exact solution $u^*$ should be $r_{u^*} = ||r(u^*)|| = 0$. However, in practice, a numerical solution $u$ could only be an approximation of the exact solution $u^*$. The precision of $u$ is evaluated by its relative residual error, which is defined as\n$\\epsilon_u = \\frac{||f - A \\hat{u}||}{||f||} = \\frac{||A \\hat{u}||}{||f||}.$ (6)\nTypically, the ultimate goal of a numerical PDE solver is to seek the optimization of the relative residual error. If we have $\\epsilon_u < \\epsilon_{max}$ for some small $\\epsilon_{max}$, we would consider $u$ to be a ground-truth solution.\nLinear Iterator. A linear iterator (also called an iterative solver or a smoother) for generic linear systems like Eq. 4 could be expressed as\n$u_{k+1} = (I - P^{-1}A) u_k + P^{-1}f,$ (7)\nwhere $P$ is an easily invertible approximation to the system matrix $A$."}, {"title": "4. Novel Approach", "content": "The proposed UGrid neural solver is built upon the principled integration of the U-Net architecture and the multigrid method. We observe that linear differential operators, as well as their approximate inverse in legacy iterative solvers, are analogous to convolution operators. E.g., the discrete Laplacian operator is a 3 \u00d7 3 convolution kernel. Furthermore, the multigrid V-cycle is also analogous to the U-Net architecture, with grid transfer operators mapped to up/downsampling layers. Moreover, the fine-tuning process of multigrid's critical components on specific PDE formulations could be completed by learning from big data. These technical observations lead to our neural implementation and optimization of the multigrid routine.\nIn spite of high efficiency, generalization power remains a major challenge for neural methods. Many SOTA neural solvers, e.g., , fail to generalize to new scenarios unobserved during the training phase. Such new scenarios include: (1) New problem sizes; and (2) New, complex boundary conditions and right-hand sides, which includes geometries, topology, and values (noisy inputs). Moreover, some of these methods are tested on Poisson equations only; neither mathematical reasoning nor empirical results show that they could trivially generalize to other PDEs (with or without retraining). UGrid resolves all problems above.\nUGrid is comprised of the following components: (1) The fixed neural smoother, which consists of our proposed convolutional operators; (2) The learnable neural multigrid, which consists of our UGrid submodule; (3) A residual loss metric which enables the self-supervised training process."}, {"title": "4.1. Convolutional Operators", "content": "This subsection is organized as follows: Sec. 4.1.1 introduces the masked operators, which mimic the smoothers in a legacy multigrid routine; and Sec. 4.1.2 introduces the masked residual operators for residual calculation.\n4.1.1. MASKED CONVOLUTIONAL ITERATOR\nA trivial linear iterator in the form of Eq. 7 does not fit in a neural routine. This is because in practice, the system matrix $A$ for its corresponding differential operator encodes the boundary geometry and turns into matrix $A$ in Eq. 4. $A$'s elements are input-dependent, and is thus impossible to be expressed as a fix-valued convolution kernel.\nWe make use of the masked version of PDEs (Eq. 2) and their masked convolutional iterators, which are natural extensions of Eq. 7:\n$u^{k+1} = (I - M) ((I - P^{-1}A) u^k + P^{-1}f) + Mb,$ (8)\nwhere $P$ is an easily-invertible approximation on the discrete differential operator $A$. The correctness of Eq. 8 is guaranteed by the following Theorem (proved as Theorem A.3 in the appendix):\nTheorem 4.1. For a PDE in the form of Eq. 2, the masked iterator Eq. 8 converges to its ground-truth solution when its prototype Jacobi iterator converges and $P$ is full-rank diagonal.\nMatrix $A$ has different formulations for different linear PDEs. Without loss of generality, we consider the following problem formulations. Other possible PDE formulations are essentially combinations of differential primitives available in the three problems below, and our convolutional operators could be trivially extended to higher orders of differentiation.\n2D Poisson Problem. Under Dirichlet boundary condition, a Poisson problem could be expressed as follows:\n$\\begin{cases}\\nabla^2u(x,y) = f(x,y), &(x, y) \\in \\mathcal{I}\\\\ u(x, y) = b(x, y), &(x, y) \\in \\mathcal{B'}\\end{cases}$ (9)\nwhere $u$ is the unknown scalar field, $f$ is the Laplacian field, and $b$ is the Dirichlet boundary condition.\nMatrix $A$ could be assembled by the five-point finite difference stencil for 2D Laplace operators, and we could simply let $P = -4I$, where $I$ denotes the identity matrix. The update rule specified in Eq. 8 thus becomes\n$u_{k+1}(i, j) = \\frac{1}{4} (I - M) (u_k(i - 1, j) + u_k(i + 1, j) + u_k(i, j - 1) + u_k(i, j + 1) - f) + Mb.$ (10)\nTo transform the masked iterator into a convolution layer, we reorganize the column vectors $u, b, M$ and $f$ into $n \\times n$ matrices with their semantic meanings untouched. Then, the neural smoother could be expressed as\n$U^{k+1} = smooth(U^k) = (1 - M)(U^k * J - 0.25f) + Mb,$ (11)\n$J = \\begin{pmatrix}0 & 0.25 & 0\\\\ 0.25 & 0 & 0.25\\\\ 0 & 0.25 & 0\\end{pmatrix}$\nwhere 1 is an n \u00d7 n matrix whose elements are all equal to 1, and * denotes 2D convolution.\n2D Helmholtz Problem. Under Dirichlet boundary condition, an inhomogeneous Helmholtz equation with spatially-varying wavenumber may be expressed as follows:\n$\\begin{cases}\\nabla^2u(x, y) + k^2(x,y)u(x,y) = f(x,y), &(x, y) \\in \\mathcal{I}\\\\ u(x, y) = b(x, y), &(x,y) \\in \\mathcal{B'}\\end{cases}$ (12)\nwhere $u$ is the unknown scalar field, $k^2$ is the spatially-varying wavenumber, $f$ is the (non-zero) right hand side, and $b$ is the Dirichlet boundary condition.\nFor our proposed UGrid solver, we could naturally extend Eq. 11 into the following form to incorporate Eq. 12:\n$U^{k+1} = \\frac{1}{4-k^2}(1 - M)(U^k * 4J - f) + Mb,$ (13)\nwhere all notations retain their meanings as in Eq. 11.\n2D Steady-state Convection-diffusion-reaction Problem. Under Dirichlet boundary condition, an inhomogeneous steady-state convection-diffusion-reaction equation may be expressed as follows:\n$\\begin{cases}v(x, y) \u00b7 \\nabla u(x, y) - a\\nabla^2u(x, y) + \\beta u(x,y) = f(x,y), &(x, y) \\in \\mathcal{I}\\\\ u(x, y) = b(x, y), &(x, y) \\in \\mathcal{B}\\end{cases}$ (14)\nwhere $u$ is the unknown scalar field, $v = (v_x, v_y)$ is the vector velocity field, $\\alpha, \\beta$ are constants, $f$ is the (non-zero) right-hand side, and $b$ is the Dirichlet boundary condition.\nFor our proposed UGrid solver, we could naturally extend Eq. 11 into the following form to incorporate Eq. 14:\n$U^{k+1} = \\frac{1}{4\\alpha + \\beta} (1 - M) (\\alpha u^k * 4J + v_x(U^k *J_x) + V_y(U^k * J_y) + f) + Mb,$ (15)"}, {"title": "4.1.2. CONVOLUTIONAL RESIDUAL OPERATOR", "content": "Except for the smoother, the multigrid method also requires the calculation of the residual in each iteration step. In practice, the residual operator (Eq. 5) can also be seamlessly implemented as a convolution layer. Because our masked iterator (Eq. 8) guarantees that $u$ satisfies $Mu = Mb$ at any iteration step, the residual operator for Poisson problems could be simplified into\n$r(u) = (1 - M) (f - u * L),$   $L=\\begin{pmatrix}0 & 1 & 0\\\\ 1 & -4 & 1\\\\ 0 & 1 & 0\\end{pmatrix}$ (16)\nFor Helmholtz problems, Eq. 16 could be naturally extended into\n$r(u) = (1 - M) (f - u * L - k^2u),$ (17)\nwhere all notations retain their meanings as in Eq. 16.\nFor steady-state convection-diffusion-reaction problems, we could extend Eq. 16 into\n$r(u) = (1 - M) (f + v_x(u *J_x) + V_y(u * J_y) + au * L - \\beta u),$ (18)\nwhere all notations retain their meanings as in Eq. 16."}, {"title": "4.2. Neural Network Design", "content": "UGrid Iteration. We design the iteration step of our neural iterator as a sequence of operations as follows (which is illustrated in Fig. 1):\n$\\begin{aligned}u &= smooth^1 (u)  & (Pre-smooth for v_1 times);\\\\ r &= r(u) & (Calculate the current residual);\\\\ \\delta &= UGrid(r, 1 - M) & (UGrid submodule recursion);\\\\ u &= u + \\delta & (Apply the correction term);\\\\ u &= smooth^2 (u) & (Post-smooth for v_2 times).\\end{aligned}$ (19)\nThe entire iteration process is specifically designed to emulate the multigrid iteration : We use the pre-smoothing and post-smoothing layers to eliminate the high-frequency modes in the residual $r$, and invoke the UGrid submodule to eliminate the low-frequency modes.\nUGrid Submodule. Our UGrid submodule is also implemented as a fully-convolutional network, whose structure is highlighted in Fig. 2. The overall structure of UGrid is built upon the principled combination of U-Net and multigrid V-cycle, and could be considered a \"V-cycle\" with skip connections. Just like the multigrid method, our UGrid submodule is also invoked recursively, where each level of recursion would coarsen the mesh grid by 2x.\nTo approximate the linearity of the multigrid iteration (note that we are learning to invert a system matrix), we implement the smoothing layers in the legacy multigrid V-cycle (not to be confused with the pre-smoother and the post-smoother in Eq. 19, which are outside of the V-cycle hierarchy) as learnable 2D convolution layers without any bias. For the same reason, and inspired by , we also drop many commonly seen neural layers which would introduce non-linearity, such as normalization layers and activation layers."}, {"title": "4.3. Loss Function Design", "content": "Legacy Loss Metric. We refer the equivalents of the mean relative error between the prediction and a ground-truth solution as the legacy loss metric. The following Theorem (proved in Sec. A.5) shows that though intuitive, the legacy loss is unstable:\nTheorem 4.2. When a neural network converges on a legacy loss metric such that its prediction x satisfies $L_{legacy} (x, y) = mean (|x - y|/|y|) \\leq l_{max}$, where $y$ denotes the ground truth value, the upper and lower bounds of x's relative residual error are still dependent on the input.\nTheorem 4.2 explains our experimental observations that: (1) Optimizing the legacy loss metric does not increase the precision in terms of the relative residual error; and (2) The legacy loss metric restricts the solution space: A valid numerical solution with low relative residual error may have a large relative difference from another valid solution (the one selected as the ground truth value). As a result, many valid solutions are unnecessarily rejected by the legacy loss metric.\nProposed Residual Loss Metric. To overcome the shortcomings of the legacy loss metric, we propose to optimize the neural solver directly to the residual in a self-supervised manner:\n$C_{rabs} (x) = E_x [||(1 - M)(f - Ax) ||^2] .$ (20)\nWe have conducted ablation studies on our relative loss metric and the legacy loss (Sec. 5 and Sec. A.6). The results showcase that the proposed residual loss performs better than the legacy loss in terms of both efficiency and accuracy. Heuristically, the proposed residual loss metric is closer to the fundamental definition of the precision of a PDE's solution, and is more robust and stable, because upon convergence, it guarantees an input-independent final accuracy.\nMoreover, the self-supervised training process endows our method with the following merits: (1) Easier data generation (compared to other neural routines which are trained on the legacy loss), and thus achieve better numerical performance; (2) For a specific PDE formulation, we could easily get a decent neural multigrid solver optimized for that specific formulation (which outperforms existing general-purpose legacy routines), simply by training UGrid on the data generated. On the contrary, fine-tuning legacy solvers is a non-trivial task requiring a solid mathematical background as well as non-trivial programming effort."}, {"title": "5. Experiments and Evaluations", "content": "Experiments Overview. For each of the three PDEs mentioned in Sec. 4.1: (i) Poisson problem, (ii) inhomogeneous Helmholtz problem with varying wave numbers, and (iii) inhomogeneous steady-state diffusion-convection-reaction problem, we train one UGrid model specialized for its formulation. We apply our model and the baselines to the task of 2.5D freeform surface modeling. These surfaces are modeled by the three types of PDEs as 2D height fields, with non-trivial geometry/topology. Each surface is discretized into: (1) Small-scale problem: A linear system of size 66, 049 \u00d7 66,049; (2) Large-scale problem: A linear system of size 1,050, 625 \u00d7 1,050, 625; (3) XL-scale problem: A linear system of size 4, 198, 401 \u00d7 4, 198, 401; and (4) XXL-scale problem: A linear system of size 16, 785, 409 \u00d7 16, 785, 409. UGrid is trained on the large scale only. Other problem sizes are designed to evaluate UGrid's generalization power and scalability.\nIn addition, we have conducted an ablation study on the residual loss metric (v.s. legacy loss) as well as the UGrid architecture itself (v.s. vanilla U-Net).\nData Generation and Implementation Details. Our new neural PDE solver is trained in a self-supervised manner on the residual loss. Before training, we synthesized a dataset with 16000 (M, b, f) pairs. For Helmholtz and diffusion problems, we further random-sample their unique coefficient fields (more details available in Sec. A.8 and Sec. A.9.) To examine the generalization power of UGrid, the geometries of boundary conditions in our training data are limited to \u201cDonuts-like\u201d shapes as shown in Fig. 3 (h). Moreover, all training data are restricted to zero f-fields only, i.e., f = 0. Our UGrid model has 6 recursive Multigrid submodules. We train our model and perform all experiments on a personal computer with 64GB RAM, AMD Ryzen 9 3950x 16-core processor, and NVIDIA GeForce RTX 2080 Ti GPU. We train our model for 300 epochs with the Adam optimizer. The learning rate is initially set to 0.001, and decays by 0.1 for every 50 epochs. We initialize all learnable parameters with PyTorch's default initialization policy. Our code is available as open-source at https://github.com/AXIHIXA/UGrid.\nExperimental Results. We compare our model with two state-of-the-art legacy solvers, AMGCL , and NVIDIA AmgX , as well as one SOTA neural solver proposed by .\nOur testcases as shown in Fig. 3. These are all with complex geometry and topology, and none of which are present in the training data, except the geometry of Fig. 3 (h). Testcase(s) input (boundary values, boundary geometries/topology, and Laplacian distribution); (i-j) are two baseline surfaces."}, {"title": "6. Conclusion and Future Work", "content": "This paper has articulated a novel efficient-and-rigorous neural PDE solver built upon the U-Net and the Multigrid method, naturally combining the mathematical backbone of correctness and convergence as well as the knowledge gained from data observations. Extensive experiments validate all the claimed advantages of our proposed approach. Our future research efforts will be extending the current work to non-linear PDEs. The critical algorithmic barrier between our approach and non-linear PDEs is the limited expressiveness of the convolution semantics. We would like to explore more alternatives with stronger expressive power."}]}