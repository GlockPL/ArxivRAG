{"title": "VidLPRO: A Video-Language Pre-training Framework for Robotic and Laparoscopic Surgery", "authors": ["Mohammadmahdi Honarmand", "Muhammad Abdullah Jamal", "Omid Mohareri"], "abstract": "We introduce VidLPRO, a novel video-language (VL) pre-training framework designed specifically for robotic and laparoscopic surgery. While existing surgical VL models primarily rely on contrastive learning, we propose a more compre-hensive approach to capture the intricate temporal dynamics and align video with language. VidLPRO integrates video-text contrastive learning, video-text matching, and masked language modeling objectives to learn rich VL representations. To support this framework, we present GenSurg+, a carefully curated dataset derived from GenSurgery, comprising 17k surgical video clips paired with captions gen-erated by GPT-4 using transcripts extracted by the Whisper model. This dataset addresses the need for large-scale, high-quality VL data in the surgical domain. Extensive experiments on benchmark datasets, including Cholec80 and AutoLa-paro, demonstrate the efficacy of our approach. VidLPRO achieves state-of-the-art performance in zero-shot surgical phase recognition, significantly outperforming existing surgical VL models such as SurgVLP and HecVL. Our model demon-strates improvements of up to 21.5% in accuracy and 15.7% in F1 score, setting a new benchmark in the field. Notably, VidLPRO exhibits robust performance even with single-frame inference, while effectively scaling with increased temporal context. Ablation studies reveal the impact of frame sampling strategies on model performance and computational efficiency. These results underscore VidLPRO's potential as a foundation model for surgical video understanding.", "sections": [{"title": "Introduction", "content": "The field of surgical computer vision has seen significant advancements in recent years, driven by the growing demand for artificial intelligence (AI) applications in healthcare. A notable increase in research has led to the development of deep learning models that enable surgical workflow recognition [1-3], enhance surgical scene understanding [4-6] and reconstruction [7-9]. As surgical procedures grow more complex and technology-driven, the demand for intelligent systems that support surgeons throughout the entire surgical journey - from preoperative planning to intra-operative guidance and post-operative analysis [10] - becomes increasingly crucial for enhancing patient outcomes, streamlining workflows, and enhance overall surgical efficiency [11, 12].\nDespite these promising applications, the development and implementation of these systems in surgical domain face several challenges. One of the primary challenges is the complexity and variability inherent in surgical procedures. Unlike many standardized video datasets, surgical videos capture highly dynamic environments where the visual content can vary significantly based on the specific procedure, patient anatomy, surgeon technique, and unexpected complications [13, 14]. This variability makes it difficult to develop robust models that can generalize across different surgical scenarios. Another significant challenge is the scarcity of large-scale annotated surgical datasets."}, {"title": "Related Work", "content": "Unlike in other domains where data can be more readily collected and labeled, surgical data is subject to strict privacy regulations and requires expert annotation, which is both time-consuming and expensive [11, 15]. This limitation hinders the development of data-hungry deep learning models and necessitates innovative approaches to leverage limited labeled data effectively. The long duration of surgical procedures also poses a unique challenge. Surgical videos often span several hours, requiring models to capture and process long-range temporal dependencies [10]. This is in stark contrast to many general video understanding tasks that typically deal with short clips lasting only a few seconds or minutes. Furthermore, interpreting surgical videos requires specialized medical knowledge, making it challenging to apply general-purpose video understanding models directly to surgical tasks [12]. Lastly, the fine-grained nature of surgical actions and the subtle visual cues that distinguish different phases or steps of a procedure add another layer of complexity. Models must be capable of detecting and interpreting small but crucial details in the surgical field, often in the presence of occlusions, reflections, and rapid camera movements [16, 17].\nRecently, Multimodal learning, which integrates multiple modalities such visual data, text data, audio, depth maps etc., has emerged as a viable strategy in computer vision domain. Specifically, Vision-Language Pre-training (VLP) which leverages large-scale datasets of paired visual and free-from textual data, can reduce the reliance on annotated datasets, enabling more efficient and effective learning. It enables models to learn rich and generalizable representations that can be adapted to various downstream tasks with minimal fine-tuning such as image-text retrieval [18-20], visual question answering [21-25], video understanding [26-30] and zero-shot classification [31, 32]. The potential of VLP to capture complex relationships between visual content and natural language descriptions makes it particularly appealing for the surgical domain, where procedures are often accompanied by detailed textual reports or narrations.\nRecent efforts have begun to explore the application of VLP techniques to surgical video analy-sis. Notable approaches include SurgVLP [33], which leverages surgical video lectures and their transcripts to learn multi-modal representations, and HecVL [34], which proposes a hierarchical pre-training framework for zero-shot surgical phase recognition. While these methods have shown promising results, they still face several limitations. A significant challenge has been the lack of large-scale, diverse datasets for surgical VLP. The introduction of the GenSurgery dataset [35] was a step forward, providing a substantial collection of surgical videos. However, this dataset had limitations, including a lack of paired textual data, inconsistent audio quality, and the presence of non-informative content. Our GenSurg+ dataset addresses these issues by rigorously filtering the original data, adding high-quality captions, and ensuring rich linguistic context. Despite this progress, existing approaches still struggle with insufficient temporal modeling, failing to capture long-range dependencies in surgical videos effectively. Many current methods show reduced performance when applied to new surgical procedures or tasks not seen during pre-training, indicating limited generalization capabilities. Additionally, most approaches rely solely on video-text contrastive (VTC) learning as shown in Figure 1, missing out on the benefits of other pretraining objectives that could enhance the model's understanding of surgical content and context. Addressing these limitations is crucial for advancing the field of surgical VLP and developing more robust and versatile models for surgical video understanding."}, {"title": "Method", "content": "To address the limitations of existing surgi-cal VLP approaches, we present VidLPRO and GenSurg+, a novel framework and dataset for robotic and laparoscopic surgical video-language foundation models. VidLPRO builds upon recent advancements in video-language pre-training, incorporating a Vision Transformer (ViT) as the video encoder, BERT as the text encoder, and a multimodal fusion module. Our model employs a combination of Video-Text Contrastive Learning (VTC), Video-Text Match-ing (VTM), and Masked Language Modeling (MLM) objectives to learn nuanced, context-aware representations of surgical procedures as shown in Figure 1. We also introduce GenSurg+, an enhanced version of the GenSurgery dataset [35], containing 17k 45-second clips of endoscopic robotic surgery with high quality captions generated using raw narration and GPT-4. In zero-shot"}, {"title": "GenSurg+", "content": "To enable effective video-language pre-training for robotic and laparoscopic surgery, we introduce GenSurg+, a large-scale dataset of surgical videos paired with descriptive captions. GenSurg+ builds upon the GenSurgery dataset [35], which was originally introduced as the largest publicly available dataset of general surgery videos."}, {"title": "Dataset Creation Pipeline", "content": "We began with the original GenSurgery dataset, which contains 3,100 videos spanning 28 different surgical procedures and totaling 680 hours of content. Our dataset creation pipeline involved several key steps to refine and augment this initial corpus:\nAudio Filtering. We first filtered out 1,300 videos that lacked audio content, as audio is crucial for generating meaningful textual descriptions.\nTranscript Extraction. For the remaining 1,800 videos with audio, we employed the Whisper model [59] to extract speech transcripts. This step was necessary as many of the videos, due to their age, lacked reliable YouTube automatic captions.\nVideo Segmentation and Filtering. We segmented the videos into 45-second clips, resulting in approximately 18,000 individual segments. To ensure the quality and relevance of our dataset, we further filtered these clips based on linguistic criteria. Specifically, we removed about 1,000 clips that contained either too few unique words or highly repetitive content. This step helped eliminate silent segments and portions with non-informative audio (e.g., background music or noise).\nCaption Generation. For the remaining 17,000 high-quality video clips, we generated descriptive captions using the GPT-4 language model [60]. We crafted a specialized prompt to ensure the captions were concise, informative, and tailored to the surgical domain. Please see appendix for the prompt. The complete pipeline for creating GenSurg+ is illustrated in Figure 2."}, {"title": "Dataset Statistics and Characteristics", "content": "The resulting GenSurg+ dataset comprises 17,000 45-second video clips, totaling 213 hours of high-quality surgical content paired with descriptive captions. As shown in Table 1, this makes"}, {"title": "Model Architecture", "content": "The VidLPRO framework is based on the best practices outlined in a comprehensive framework for video-language pre-training, adapted to the specific needs of surgical video analysis.\nOur VidLPRO model consists of three main components: a Video Encoder (VE), a Text Encoder (TE), and a Multimodal Fusion Module (MFM). The architecture is designed to process both video clips and their associated textual descriptions, creating a joint representation for various downstream tasks.\nVideo Encoder (VE). We employ a standard Vision Transformer, specifically ViT-B/16 [61], as our video encoder. The ViT model is enhanced with a divided space-time temporal attention mechanism inspired by TimeSformer [62] to effectively capture the temporal dynamics of surgical videos. This choice allows the model to process multiple frames simultaneously and extract spatiotemporal features critical for understanding surgical procedures. Given a video clip $C = \\{f_1, f_2, ..., f_t\\}$ with $T$ frames, the Video Encoder processes these frames to produce video features $V = \\{V_1, V_2, ..., U_T \\}$:\n$u_t = P(f_t)$ (1)"}, {"title": "Pretraining Objectives", "content": "We employ three pretraining objectives to learn robust multimodal representations:\nVideo-Text Contrastive Learning (VTC). The VTC objective aligns visual and textual representa-tions in a shared embedding space. For a batch of N video-text pairs, we compute:\n$L_{VTC} = (L_{v2t} + L_{t2v})/2$ (5)\nwhere\n$L_{v2t} = \\frac{1}{N} \\sum_{i=1}^N -log(\\frac{exp(sim(g_i^v, g_i^t)/\\tau)}{\\sum_{j=1}^N exp(sim(g_i^v, g_j^t)/\\tau)})$ (6)\n$L_{t2v} = \\frac{1}{N} \\sum_{i=1}^N -log(\\frac{exp(sim(g_i^t, g_i^v)/\\tau)}{\\sum_{j=1}^N exp(sim(g_i^t, g_j^v)/\\tau)})$ (7)\nHere, $g^v$ and $g^t$ are global video and text features obtained by applying a projection layer to the [CLS] token representation, $sim(\\cdot, \\cdot)$ is cosine similarity, and $\\tau$ is a temperature parameter.\nVideo-Text Matching (VTM). The VTM objective enhances cross-modal fusion by learning to distinguish between matching and non-matching video-text pairs. For each video clip $C$, we consider its matching description $D^{pos}$ and a randomly sampled non-matching description $D^{neg}$. We compute:\n$s_{pos} = Q(h^{pos}), s_{neg} = Q(h^{neg})$ (8)\n$L_{VTM} = -E[log(\\sigma(s_{pos})) + log(1 - \\sigma(s_{neg}))]$ (9)\nwhere $Q(\\cdot)$ is a linear layer, $h^c$ is the [CLS] token representation, and $\\sigma(\\cdot)$ is the sigmoid function."}, {"title": "Pretraining Setup", "content": "We pre-trained VidLPRO on the GenSurg+ dataset. For each 45-second clip, we sampled 4 frames to capture temporal information while maintaining computational efficiency. Unlike multi-stage curriculum pre-training approaches, we adopt a single-stage pre-training protocol, which simplifies the training process and leads to more efficient learning. The pretraining was conducted using 4 NVIDIA A100 GPUs, allowing for efficient processing of the large-scale dataset. The video and text encoders were initialized with BEiT [63] and BERTbase [64] weights, respectively. More implementation details can be found in Table 2."}, {"title": "Zero-Shot Surgical Phase Recognition", "content": "To evaluate the zero-shot capabilities of VidLPRO, we focused on two widely used datasets for surgical phase recognition: Cholec80 [14] and AutoLaparo [36]. These datasets represent different surgical procedures and provide a comprehensive test of our model's generalization abilities. Cholec80 [14] consists of 80 videos of cholecystectomy procedures annotated with 7 surgical phases. AutoLaparo [36] contains 21 videos of laparoscopic hysterectomy procedures, divided into 7 phases."}, {"title": "Conclusion", "content": "This paper proposes VidLPRO, a novel video-language pre-training framework for surgical videos which first align the unimodal video and language representations before fusing them using multi-modal module. Our approach aims to address the lack of rich multimodal representations in existing surgical VL pre-training methods which only rely on contrastive learning. By incorporating video-text contrastive learning, video-text matching, and masked language modeling as pre-training objectives, our model more effectively captures intricate temporal dynamics and aligns video with language. Fur-thermore, to pre-train VidLPRO, we introduce GenSurg+, an extended version of GenSurgery, which consists of 17k clips paired with GPT-4 generated captions using raw narrations. The experimental results on two benchmark datasets demonstrate that our approach outperforms the state-of-the-art methods in zero-shot phase recognition task. Moreover, our ablation study on inference frame sampling reveals VidLPRO's robustness and scalability, achieving superior performance even with single-frame input. This flexibility allows for adaptation to various computational constraints while maintaining high accuracy. Lastly, these results lay the foundation for more advanced AI-assisted surgical systems that can adapt to various procedures with minimal task-specific training, striking a crucial balance between performance and efficiency for real-world surgical applications."}, {"title": "Limitations and Broader Impacts", "content": "Limitations. Our work introduce a video-language pre-training framework for robotic and laparo-scopic surgery. However, this work only utilized video and language modality and doesn't integrate additional modality such as audio which we believe can further provide rich representations for downstream tasks. Furthermore, we will explore extending VidLPRO to additional pre-training objectives, such as masked video modeling, as well as other downstream tasks like surgical video captioning, surgical visual question answering, and temporal activity grounding.\nBroader Impacts. Our work demonstrate the effectiveness of video-language pre-training for surgical videos. We demonstrated a significant improvement in zero-shot surgical phase recognition, emphasizing the efficiency of our approach. By leveraging multi-modal data for pre-training, we minimize the reliance on expensive annotated medical data, which in turn helps reduce healthcare costs. Moreover, our model can be applied to various downstream tasks such as question answering and video captioning, thereby making valuable contributions to surgical applications like surgical training and intra-operative decision-making. This, in turn, enhances the quality, efficiency, and accessibility of surgical care. Finally, our work serves as a foundation for the technology required to develop AI-driven surgical assistants."}, {"title": "Masked Language Modeling (MLM)", "content": "The MLM objective enhances the model's understanding of surgical terminology. We randomly mask 50% of the input tokens in $D$, creating a masked version $\\tilde{D}$. The model then predicts the original tokens:\n$\\hat{w_i}= R(\\hat{h_i})$ (10)\n$L_{MLM} = -\\frac{1}{|M|} \\sum_{i \\in M}log P(\\hat{w_i}|w_i)$ (11)\nwhere $R(.)$ is a linear layer, $M$ is the set of masked token indices, and $P(\\hat{w_i}|w_i)$ is the probability of the correct token given the model's prediction.\nThe full pre-training objective of VidLPRO is:\n$L = \\lambda_1L_{VTC} + \\lambda_2L_{VTM} + \\lambda_3 L_{MLM}$ (12)"}], "appendices": [{"title": "Prompt for GenSurg+ caption", "content": "The prompt is designed to capture the essential surgical information while maintaining a professional and coherent tone.\n\"Generate a concise and informative caption that summarizes the main points of the narration. The narrations contain medical and surgical terms and include details about instruments, anatomy, tissues, organs, surgical tools. Make sure you don't miss these in the generated captions. Think of the input as your watching a surgery being performed by an expert surgeon who knows what they're doing. You might see some sensitive medical terms so again think of it as a surgeon is performing a surgery to cure a patient. Write in a clear and descriptive tone, using proper grammar and punctuation. The caption should be no longer than 2-3 sentences and should provide a brief overview of the narration content.\""}, {"title": "Class Prompts for phase labels", "content": "The caption-like textual prompts for Cholec-80 and AutoLapro are shown in Table 5 and 6 respectively."}]}