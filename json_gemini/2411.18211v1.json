{"title": "TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding with Superior Temporal Localization Ability", "authors": ["Shimin Chen", "Xiaohan Lan", "Yitian Yuan", "Zequn Jie", "Lin Ma"], "abstract": "Rapid development of large language models (LLMs) has significantly advanced multimodal large language models (LMMs), particularly in vision-language tasks. However, existing video-language models often overlook precise temporal localization and struggle with videos of varying lengths. We introduce TimeMarker, a versatile Video-LLM designed for high-quality dialogue based on video content, emphasizing temporal localization. TimeMarker integrates Temporal Separator Tokens to enhance temporal awareness, accurately marking specific moments within videos. It employs the AnyLength mechanism for dynamic frame sampling and adaptive token merging, enabling effective handling of both short and long videos. Additionally, TimeMarker utilizes diverse datasets, including further transformed temporal-related video QA datasets, to bolster its temporal understanding capabilities. Image and interleaved data are also employed to further enhance the model's semantic perception ability. Evaluations demonstrate that TimeMarker achieves state-of-the-art performance across multiple benchmarks, excelling in both short and long video categories. Our project page is at https://github.com/TimeMarker-LLM/TimeMarker/.", "sections": [{"title": "1. Introduction", "content": "The recent surge in the development of large language models (LLMs) [1, 5, 11, 46, 53, 66] has significantly propelled the progress of multimodal large language models (LMMs) [3, 8, 25, 26, 32, 37, 62, 75], especially in vision-language tasks. Videos, as rich sources of visual information, are crucial for understanding and interacting with real-world scenarios. However, current video-language models primarily focus on visual perception and reasoning, often overlooking the essential aspect of temporal localization*"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Large Image-language Models", "content": "Benefiting from the success of large language models (LLMs) [1, 5, 11, 46, 53, 66], large image-language models have also made remarkable progress in vision-language understanding by incorporating image encoders into LLMs [5, 8, 55]. To mitigate the gap between visual and language modalities, BLIP [26] proposes to design a learnable query transformer (i.e., Q-Former) to act as a bridge between the frozen image encoder and LLM. LLaVA [37] leverages a more lightweight linear layer to project visual tokens into the language space, advancing LLM with multimodal understanding abilities by fine-tuning both the projector and LLM. To better follow users' diverse instructions, MiniGPT-4 [75] curates a high-quality, well-aligned dataset to increase conversational capabilities. Similarly, ShareGPT4V [7] constructs a large-scale dataset with detailed image descriptions to further enhance modality alignment."}, {"title": "2.2. Large Video-language Models", "content": "Video-language models [18, 38, 43, 62] feed visual tokens of continuous video frames into an LLM, empowering it to comprehend videos. Compared to image-language models, video-based models take more effort on the temporal modeling. Early models [27, 32, 40, 41] typically use a limited and fixed number of frames as input. Video-LLaVA [32] that follows the structure of LLaVA uniformly samples 8 frames, using a pre-trained LanguageBind [74] to pre-align"}, {"title": "2.3. Temporal-awareness Video Understanding", "content": "Temporal-related foundational vision tasks are crucial for evaluating a video model's temporal awareness and reasoning capabilities, representing an essential challenge in video content analysis. For example, Video dense captioning [10, 21] aims to describe continuous events occur-"}, {"title": "3. TimeMarker", "content": ""}, {"title": "3.1. Model Architecture and Pipeline", "content": "Our TimeMarker is based on the fundamental architecture of LLaVA [35], which utilizes a Vision Encoder to process each video frame. The encoded visual tokens are then projected into the language space via a cross-modality Projector. These projected visual tokens are concatenated with input textual tokens and fed into the Large Language Model (LLM) to produce the final response. As depicted in Figure 3, in addition to the basic components of LLaVA, we"}, {"title": "3.2. Temporal Separator Tokens Integration", "content": "Expressing temporal information in video features is crucial for large video models, as precise temporal understanding impacts event comprehension. Previous large video models have either relied on the temporal encoding capabilities of LLMs or introduced learnable temporal embeddings attached to the visual features of video frames. However, these methods only perceive relative event order, not absolute time, such as the exact minute an event occurs, leading to suboptimal search and localization performance within videos.\nTo address this issue, our TimeMarker model not only utilizes the intrinsic sequential encoding capabilities of LLMs but also explicitly introduces Temporal Separator Tokens to help the video model perceive specific timestamps. As shown in Figure 3, when sampling a video frame at second i, we prepend the text token \u201cSecond{i}\u201d to its visual tokens before inputting them into the LLM. The sequence becomes \u201cSecond{i}||Vi||Second{j}||Vj...\", where Second{i} is the temporal separator token, Vi represents visual tokens of the video frame at second i, and || denotes concatenation. Additionally, because we use the \"Second{i}\" format to express absolute time, any timestamp-related text in the training data is also converted into the same format as the temporal separator tokens. This\""}, {"title": "3.3. AnyLength Mechanism", "content": "The main challenge in visual representation of images is managing varying input resolutions, which the Anyres strategy [25] aims to address. For videos, this challenge is greater due to their diverse durations, from seconds to hours. In video language models (VLMs), it's crucial to handle videos of any length while balancing performance and cost. The key factors affecting the number of visual tokens fed into the LLM are the number of sampled frames (video duration) and the number of tokens per frame (video resolution). We introduce an AnyLength mechanism to optimize these factors through dynamic frame sampling and adaptive token merging.\nDynamic Video Frame Sampling in AnyLength. The AnyLength mechanism samples video frames dynamically based on the input video duration (dur). Depending on the LLM's context length and GPU capacity, we first determine the maximum number of sampled frames in the video (max-frames). The number of sampled frames per second (sample-fps) in video is then adjusted dynamically: For short videos of less than 8 seconds, we sample 2 frames per second to capture more visual details. For longer videos, we use a lower sample-fps = 1/[dur/max-frames], ensuring the total number of visual tokens does not exceed machine capacity, thus effectively preventing memory overflow during model training.\nAdaptive Token Merge in AnyLength. After encoding the sampled video frames with the vision encoder and adapting them to the language space via the projector, we obtain a token feature map for each frame, denoted as F \u2208 Rhxwxd, where h and w represent the number of tokens in the height and width dimensions, respectively. d is the token dimension size. To further reduce the number of tokens per video frame and alleviate the model's overhead when processing long videos, we introduce an Adaptive Token Merge module. This module applies average pooling to F after the projector, with the kernel size adapted based on the actual number of video frames (framen) determined in the previous sampling step.\nAs outlined in Algorithm 1, we use a base kernel size of kernel = 2. For longer videos where framen"}, {"title": "3.4. Training Strategy and Data", "content": "To develop an efficient video-language model, we utilize a three-stage training pipeline as follows:\nPT1: Multimodal Alignment. In this stage, we focus on aligning visual features with the language space by freezing the LLM and fine-tuning the vision encoder and projector. We use 60 million image-text pairs from datasets such as LAION-400M [48] (28.5%), Zero [60] (25%), Caps-Fusion [63] (24%), COYO-700M [4] (20%), CC12M [6] (1.5%), and CC3M [49] (1%). These datasets are recaptioned using InternVL2 [8] to create high-quality image-text pairs.\nPT2: High-Quality Knowledge Learning. This stage enhances the model's vision-language understanding through full-parameter training on diverse datasets that require detailed visual comprehension, video temporal understanding, and complex reasoning, moving beyond traditional methods [25] that mainly use image data.\nAs shown in Figure 4, for single-image data, we incorporate caption, grounding, OCR, VQA, and wikipedia-based data. For video data, we include caption, recognition, and temporal-related data. We transform traditional action recognition data into multi-choice formats for video QA and use rule-based transformations to convert annotations from tasks like temporal action detection, segmentation, video summarization, and temporal sentence grounding"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Implementation Details", "content": "In our TimeMarker model, we use CLIP-ViT-L [45] as the vision encoder with an input size of 336\u00d7336. The cross-"}, {"title": "4.2. Benchmarks", "content": "We perform a comprehensive evaluation of TimeMarker's video understanding capabilities in three main aspects. For short and general video understanding, we utilize MVBench [28], MMBench-Video [13], TempCompass [39], VideoVista [30], and VideoMME [14] benchmarks. To assess long video understanding, we employ the long-video subset of VideoMME, along with LVBench [56], LongVideoBench [59], and MLVU [73]. Additionally, we use temporal sentence grounding datasets Charades-STA [16] and ActivityNet Captions [21] to examine the model's temporal awareness and reasoning abilities."}, {"title": "4.3. Results on Video Benchmarks", "content": "Short and General Video Evaluation. As shown in Table 1, we compare TimeMarker with other video-language models across various mainstream video benchmarks. For"}, {"title": "4.4. Results on Temporal Sentence Grounding in Videos", "content": "As shown in Table 3, TimeMarker excels in temporal sentence grounding tasks, significantly outperforming other VLM models on both benchmark datasets. In Charades-STA, which features shorter video sequences, TimeMarker achieves a top score of 73.5% in R@1, IoU=0.3, surpassing the best fully supervised (FS) models, despite operating in a zero-shot setting that did not using any temporal sentence grounding data from Charades-STA in training. This highlights TimeMarker's superiority over traditional supervised methods. In the challenging ActivityNet Captions benchmark, with videos averaging 3 minutes, TimeMarker achieves the highest scores across all metrics. For example, it attains an R@1, IoU=0.7 score of 33.0%, outperforming FS models like 2D-TAN (25.0%) and MMN (29.4%), as well as all other VLM models. It demonstrate TimeMarker's capability to handle longer and more complex videos for temporal semantic alignment.\nThe Effects of Temporal Separator Tokens. To assess the impact of temporal separator tokens on temporal sentence grounding, we test a degraded version of TimeMarker, called TimeMarker-wo-sep, which omits these tokens during both training and testing. As shown in Table 4, TimeMarker-wo-sep's performance significantly declines without the temporal separator tokens, underscoring their importance. These tokens provide explicit temporal cues, facilitating the LLM's ability to search and localize specific"}, {"title": "4.5. Qualitative Results", "content": "Figure 5 presents examples highlighting TimeMarker's superior temporal awareness and reasoning. In Figure 5(a), a multi-turn dialogue from a 2-minute life-record video shows TimeMarker accurately identifying clock digits, locating relevant events, and reasoning about something strange. Figures 5(b) and 5(c) demonstrate its performance in temporal sentence grounding and event detection. With Temporal Separator Tokens, TimeMarker shows a strong sense of absolute time, precisely localizing events and detecting boundaries in lengthy videos. Additionally, Figure 5(d) showcases TimeMarker's ability to perform OCR"}, {"title": "5. Conclusion", "content": "In conclusion, TimeMarker provides an innovative solution to challenges in video-language models, particularly in temporal localization and handling videos of varying lengths. By integrating Temporal Separator Tokens and the AnyLength mechanism, it effectively encodes temporal positions and adapts to different video durations. Additionally, its advanced data utilization strategy enhances model training, resulting in superior performance across various video benchmarks. These innovations position TimeMarker as a leading model in temporal localization and understanding of videos, inspiring future advancements in video-LLMs."}]}