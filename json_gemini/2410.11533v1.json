{"title": "MULTI-ROUND JAILBREAK ATTACK ON LARGE LANGUAGE MODELS", "authors": ["Yihua Zhou", "Xiaochuan Shi"], "abstract": "Ensuring the safety and alignment of large language models (LLMs) with human values is crucial for generating responses that are beneficial to humanity. While LLMs have the capability to identify and avoid harmful queries, they remain vulnerable to \"jailbreak\" attacks, where carefully crafted prompts can induce the generation of toxic content. Traditional single-round jailbreak attacks, such as GCG and AutoDAN, do not alter the sensitive words in the dangerous prompts. Although they can temporarily bypass the model's safeguards through prompt engineering, their success rate drops significantly as the LLM is further fine-tuned, and they cannot effectively circumvent static rule-based filters that remove the hazardous vocabulary.\nIn this study, to better understand jailbreak attacks, we introduce a multi-round jailbreak approach. This method can rewrite the dangerous prompts, decomposing them into a series of less harmful sub-questions to bypass the LLM's safety checks. We first use the LLM to perform a decomposition task, breaking down a set of natural language questions into a sequence of progressive sub-questions, which are then used to fine-tune the Llama3-8B model, enabling it to decompose hazardous prompts. The fine-tuned model is then used to break down the problematic prompt, and the resulting sub-questions are sequentially asked to the victim model. If the victim model rejects a sub-question, a new decomposition is generated, and the process is repeated until the final objective is achieved. Our experimental results show a 94% success rate on the llama2-7B and demonstrate the effectiveness of this approach in circumventing static rule-based filters.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have a wide range of applications in facilitating decision-making across professional and social domains, underscoring the importance of aligning LLMs with safety considerations. To prevent the generation of responses that deviate from human values, various mechanisms have been employed to achieve safety alignment, including model fine-tuning [4], reinforcement learning with human feedback (RLHF)[15], and model editing [10]. The overall goal of these methods is to reduce the risk of large language models producing harmful or illegal responses to user queries.\nMost large language models are reliable AI assistants that can often recognize and refuse to respond to harmful queries, but they are still susceptible to carefully crafted prompts designed to manipulate them into generating dangerous responses, a phenomenon known as \"jailbreaking\". The existing research on LLM jailbreaking is represented by the GCG attack [16], which reformulates the jailbreaking attack as a process of generating adversarial examples to induce LLMs to produce responses starting with affirmative statements. Building on this, [14] and [8] have improved upon such attacks, focusing on using different optimization algorithms to enhance the stealthiness and readability.\nAlthough learning-based jailbreak attacks such as GCG can be successful in certain scenarios, they are all single-round jailbreak attacks that do not change the dangerous keywords in the problem, making them easy for producers and"}, {"title": "Related Work", "content": "In recent years, the rapid advancement of Large Language Models (LLMs) has brought about both remarkable capabilities and concerning vulnerabilities. One such vulnerability is the emergence of jailbreak attacks, where malicious actors attempt to induce LLMs to generate harmful or objectionable content. Current jailbreak methods include a variety of approaches, such as those based on representation engineering [6], utilizing large language models (LLMs) to generate prompts [2], leveraging grammatical rules [13], modifying decoding methods [5], using multi-modal LLMs [11, 3], as well as reformulating jailbreaking as adversarial attacks and employing pure optimization techniques [16, 14, 7]. Among these, the optimization-based attack known as GCG [16] is considered the most powerful according to a recent study [9]. As a result, GCG is used as the baseline method in this paper."}, {"title": "Method", "content": null}, {"title": "Overview of multi-round jailbreak attack", "content": "Figurel depicts an overview of a multi-stage jailbreak attack. In this process, we first use a large language model (LLM) to decompose a natural question dataset into a series of three progressive questions, forming a training set. We then fine-tune a LLAMA 3-8B model using a multi-turn dialogue format, equipping it with the ability to break down dangerous questions. This is necessary because typical LLMs have moral filtering capabilities and cannot directly decompose hazardous queries.\nNext, we input the dangerous questions into the fine-tuned LLM, instructing it to generate the decomposed questions one by one to query the victim LLM. If the victim's responses fail to meet the expected results, the LLM will regenerate the attack questions and continue the interactive process until the desired outcome is achieved or the preset number of inquiries is exhausted.\nHere is an example demonstrating how multiple rounds of jailbreak attacks can be implemented:\nIt is not difficult to discover through the comparison of the successful and failed examples in Figure 2 and Figure 3 on the Claude-Haiku platform that large language models (LLMs) can bypass their moral scrutiny and provide answers to our original questions when the LLM is tasked with consecutively modifying the answers to a problem that has been decomposed from an originally dangerous one, thereby achieving the intended malicious effect.\nWe first inquire about a part of the problem, such as how to write a news article encouraging people to do what they want to do at night. Then, we ask the victim LLM to replace the \"what they want to do\" part with \"drinking beer and"}, {"title": "Fine-tune attacker LLM", "content": null}, {"title": "Model Description", "content": "Language models are a fundamental component of modern natural language processing. They are designed to capture the statistical patterns and semantic relationships within language data, allowing them to generate, understand, and manipulate human language effectively.\nThe core structure of a language model is typically based on the Transformer architecture, which utilizes multiple self-attention layers to capture the contextual dependencies within the input sequence. This allows the model to learn the conditional probability distribution of the next token given the previous tokens in the sequence.\nMathematically, let x = (x1,x2,...,xn) be the input sequence, the language model aims to estimate the joint probability of the sequence:\nP(x) = [[ P(xt | X<t)\nt=1\nThis joint probability is obtained by modeling the conditional probability of each token given the previous tokens in the sequence. The model parameters are trained to maximize this joint probability, enabling the model to generate coherent and contextually appropriate text."}, {"title": "Fine-tuning Process", "content": "While pre-trained language models provide a strong foundation for various natural language tasks, they may not be optimally suited for specific applications or domains. Fine-tuning is a technique used to adapt the pre-trained model to a particular task or dataset, allowing it to better capture the nuances and requirements of the target application.\nDuring the fine-tuning process, the pre-trained model's parameters are further adjusted using task-specific data. This is typically done by minimizing a task-specific loss function, such as the negative log-likelihood of the target outputs given the inputs:\nL(0) = \u2212 \u03a3 log P(y | x; 0)\n(x,y) ED\nHere, 0 represents the model parameters, D is the training dataset, and y is the target output for a given input x. By optimizing this loss function, the model's parameters are updated to better fit the characteristics of the target task or domain.\nThe fine-tuning process can be formalized as an optimization problem:\n0* = arg mine \u2081 L(f(xi; 0), Yi)\nwhere @ are the model parameters, xi are the inputs, yi are the expected outputs, L is the task-specific loss function, and N is the number of training samples."}, {"title": "Mathematical Model of Multi-round Jailbreak Attacks", "content": "Multi-round jailbreak attacks are a technique used to guide a language model towards a desired output by breaking down a complex problem into a sequence of simpler sub-problems. This approach aims to gradually steer the model's responses closer to the target answer through a series of iterative refinements."}, {"title": "algorithm of multi-round jailbreak attack", "content": "Purpose of the Algorithm This algorithm is designed to execute multi-round jailbreak attacks by fine-tuning large language models (LLMs). It incrementally decomposes a complex, potentially hazardous question into a series of progressively refined sub-questions. This approach guides the LLM to produce the desired answer to the original question while circumventing security restrictions.\nDetailed Algorithm Process The algorithm begins by fine-tuning the LLM to adapt to specific types of responses that are strategically less likely to trigger direct harmful outputs. Following this, the algorithm employs a decomposition strategy to break down a complex, dangerous question into several safer sub-questions. Each sub-question is crafted to be within the safe operational parameters of the LLM and the response to each sub-question helps in formulating the subsequent one, thus gradually reconstructing the answer to the original question."}, {"title": "Experiments", "content": null}, {"title": "Datasets", "content": "we generate a dataset for fine-tuning LLM based on the \"Questions about the world\" subset of the ultrachat dataset. This subset contains questions of varying lengths. we select questions with lengths between 38 and 74 characters, using LLM to break them down into three progressive sub-questions and asks the LLM these three questions to obtain a complete training dataset.\nAdditionally, we also use the questions from the advbench dataset for testing. This dataset contains 520 exam-ples of harmful behavior presented through explicit instructions, including profanity, explicit descriptions, threats, misinformation, discrimination, cybercrime, and dangerous or illegal advice."}, {"title": "result", "content": "The multi-round jailbreak attack experiment was conducted on the llama2-7B language model, and the results in Table1 demonstrate a highly concerning Attack Success Rate (ASR) of 94.45%. This indicates that the llama2-7B model is highly vulnerable to such jailbreak attacks, where the model can be prompted to bypass its intended safety constraints and generate harmful or undesirable content.\nIn contrast, the baseline GCG (Generative Controlled Guidance) model exhibited a significantly lower ASR of only 20%. This suggests that the GCG approach, which incorporates additional safety mechanisms and control measures, is considerably more effective at mitigating the risk of jailbreak attacks compared to the llama2-7B model."}, {"title": "Conclusion", "content": "In this paper, we have explored the emerging threat posed by multi-round jailbreak attacks on large language models (LLMs). These attacks cleverly exploit the sequential decision-making capability of LLMs, where a potentially harmful query is decomposed into several innocuous-seeming sub-queries. Through subtle and progressive rephrasing, these sub-queries guide the LLM to eventually produce an output that, when combined, addresses the original dangerous intent. This method represents a significant challenge in the field of AI safety and security, as it can bypass current safeguards designed to prevent direct harmful outputs from LLMs.\nOur investigation has highlighted several key findings. First, the adaptability and sophistication of LLMs, while beneficial in many contexts, can also be manipulated to serve malicious ends. The ability of these models to handle and integrate diverse inputs over multiple turns of interaction can be exploited to gradually steer the conversation towards a dangerous outcome. Second, current mitigation strategies, such as prompt engineering and output filtering, often fail to detect and counteract the incremental nature of multi-turn jailbreak attacks. These strategies typically focus on individual responses without considering the potential cumulative effect of a sequence of seemingly benign inputs.\nTo address these vulnerabilities, we propose several avenues for future research and development:\nEnhanced Monitoring of Dialogue Contexts: Implementing more sophisticated monitoring systems that can analyze the context of ongoing dialogues over multiple turns. This would involve developing algorithms capable of detecting patterns or trajectories in conversations that may lead to harmful outputs.\nDynamic Response Adjustment: Developing mechanisms that adjust the responses of LLMs dynamically based on the detected trajectory of the conversation. This could involve altering the model's behavior upon detection of suspicious patterns, potentially by introducing friction in the conversation or by redirecting the topic.\nImproved Training Regimes: Training LLMs with an expanded set of adversarial examples, specifically designed to include multi-turn interaction scenarios. This training would help the models better recognize and resist manipulative querying strategies."}]}