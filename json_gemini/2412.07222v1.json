{"title": "MPSI: Mamba enhancement model for pixel-wise\nsequential interaction Image Super-Resolution", "authors": ["Yuchun He", "Yuhan He"], "abstract": "Single image super-resolution (SR) has long posed a chal-\nlenge in the field of computer vision. While the advent of deep learn-\ning has led to the emergence of numerous methods aimed at tackling\nthis persistent issue, the current methodologies still encounter challenges\nin modeling long sequence information, leading to limitations in effec-\ntively capturing the global pixel interactions. To tackle this challenge\nand achieve superior SR outcomes, we propose the Mamba pixel-wise\nsequential interaction network (MPSI), aimed at enhancing the estab-\nlishment of long-range connections of information, particularly focusing\non pixel-wise sequential interaction. We propose the Channel-Mamba\nBlock (CMB) to capture comprehensive pixel interaction information by\neffectively modeling long sequence information. Moreover, in the exist-\ning SR methodologies, there persists the issue of the neglect of features\nextracted by preceding layers, leading to the loss of valuable feature infor-\nmation. While certain existing models strive to preserve these features,\nthey frequently encounter difficulty in establishing connections across all\nlayers. To overcome this limitation, MPSI introduces the Mamba channel\nrecursion module (MCRM), which maximizes the retention of valuable\nfeature information from early layers, thereby facilitating the acquisi-\ntion of pixel sequence interaction information from multiple-level layers.\nThrough extensive experimentation, we demonstrate that MPSI outper-\nforms existing super-resolution methods in terms of image reconstruction\nresults, attaining state-of-the-art performance.", "sections": [{"title": "Introduction", "content": "Single image super-resolution (SR) [11] stands a low-level task within computer\nvision, aspiring to recuperate intricate high-frequency details from low-resolution\n(LR) images to reconstruct high-resolution (HR) counterparts. In practical ap-\nplications, SR holds significant relevance, particularly in domains such as image\nrestoration and video enhancement [7,37]. In recent years, advancements in SR\ntechniques leveraging deep learning have primarily bifurcated into two avenues:\nmethodologies based on convolutional neural networks (CNN) [9, 43, 45], and\nTransformer-based approaches [27, 31, 44]. While SR models based on CNN [22]"}, {"title": "", "content": "can enhance image details, the inherent limitation of CNN lies in establishing\npixel feature dependencies, thus constraining the capacity for detailed restora-\ntion. The Transformer [38] employs a self-attention mechanism to facilitate\ndependency modeling and feature connectivity, demonstrating superior perfor-\nmance over conventional CNN-based structures in image SR tasks. However,\nTransformer-based SR models are constrained by windows, thereby impeding\ntheir ability to model long-range dependencies within long sequence information.\nELAN [44] endeavors to address the problem of modeling long-range dependen-\ncies in features by employing long-range attention. However, the dependencies\ncaptured by ELAN remain confined within the window, thus presenting limi-\ntations in the analysis of broader long-range contexts. Although DAT [6] uses\nchannel attention to exploit global information, the Transformer's constraints in\nmodeling long sequence information limit the understanding of global context.\nWe introduce the Mamba pixel-wise sequential interaction network (MPSI)\nfor the SR task to augment the sequential relation of pixel information. Then we\npropose Channel-Mamba Block (CMB) which integrates the Mamba [12] struc-\nture and leverages its state space model with a selection mechanism to model\nlong sequence information, thus augmenting its comprehension of global pixel in-\nformation. CMB utilizes the Dual Direction Bi-Mamba module (DDBM), which\naddresses the limitation of one-way modeling with Mamba, to conduct a bidi-\nrectional analysis of feature sequences and establish comprehensive global pixel\nsequence interaction relationships. MPSI proposes the Spatial Transformer Block\n(STB) to extract the primary spatial feature sequence, then deploy the CMB to\nextract the channel-wise information, efficiently aggregating the image details\ncaptured in spatial and channel dimensions, enriching the spatial expression of\neach feature map and long-range dependence.\nIn deep learning networks, each layer processes features uniquely due to vari-\nations in weights and biases. However, as the network deepens, the influence\nof earlier layers on the outcomes diminishes [15]. Transformer-based networks\nsuch as SwinIR establish connections between layers so that the front layers can\nobtain more effective optimization and affect the final result. However, those ap-\nproaches affect the symmetry of the network and may lead to over-fitting [18]. \u03a4\u03bf\nmitigate this issue, we propose the Mamba channel recursion module (MCRM),\ndrawing inspiration from the principles of SENet [17]. MCRM recursively ana-\nlyzes the features of each network layer and exerts greater influence on the output\nfeatures in the form of channel weights through the gate structure, further en-\nhancing the pixel-wise sequential interaction. As a result, the model optimally\nexploits information obtained from deep feature extraction, enhancing overall\nfeature representation. Our primary contributions are summarized as follows:"}, {"title": "", "content": "We propose Channel-Mamba Block (CMB), a Mamba-based enhancement\nframework that effectively models the long-range dependence of the long\nsequence information to enhance the pixel-wise sequential interaction.\nWe propose the Mamba channel recursion module (MCRM), which holisti-\ncally analyzes the features of each layer and summarizes the channel weights."}, {"title": "", "content": "This can compensate for the influence of early layers on the pixel sequence\ninteractions, and enhance the model's ability to analyze features.\nOur proposed MPSI notably surpasses the existing methods, attaining state-\nof-the-art performance."}, {"title": "Related Work", "content": ""}, {"title": "Super Resolution", "content": "Super-resolution constitutes a pivotal facet within the realm of image restoration,\nwhere deep learning has demonstrated remarkable efficacy. Leveraging deep neu-\nral networks, particularly convolution-based architectures, has yielded significant\nadvancements in super-resolution tasks. Notably, the Super-Resolution Convolu-\ntional Neural Network (SRCNN) [9] epitomizes this progress, comprising three\nintegral components: Patch Extraction and Representation, Non-Linear Map-\nping, and Reconstruction. Each segment harnesses the power of 2D convolutions\nand ReLU activation functions for feature extraction and enhancement. Building\nupon the foundation laid by SRCNN, the Coarse-to-Fine Super-Resolution CNN\n(CFSRCNN) [39] emerged, incorporating diverse modules to capture complemen-\ntary contextual information and bolster super-resolution outcomes. Furthermore,\nthe integration of a ResNet structure [15] serves to mitigate gradient vanishing\nand explosion issues, thus enhancing training stability. Efforts are underway to\noptimize super-resolution networks by reducing parameter counts without com-\npromising performance. Prominent examples include the PAN model proposed\nby Hengyuan et al. [47] and the Lightweight Enhanced SR CNN (LESRCNN) [40]\nintroduced by Chunwei et al. These endeavors aim to bolster training efficiency\nand computational efficacy. The advent of Transformer architecture has ushered\nin a paradigm shift in super-resolution research, expanding beyond convolution-\ncentric approaches. Self-attention mechanisms present in Transformers offer new\navenues for exploration and innovation in super-resolution tasks, promising fur-\nther breakthroughs in this dynamic field."}, {"title": "Transformer", "content": "The self-attention mechanism in Transformer [38] brings more possibilities to\ndeep learning. The remarkable performance of Transformers in natural language\nprocessing [3, 21, 23, 35] has spurred considerable interest among researchers\nregarding their applicability in visual tasks [25, 26, 48]. For instance, Vision\nTransformer (ViT) [10] leverages Transformer architecture to address image\nclassification tasks. Swin Transformer [30] introduces shifted windows, enabling\nTransformers to perform self-attention calculations on images with interactive\nwindows, thereby enhancing adaptability to diverse visual tasks. SwinIR [27]\ndraws inspiration from Swin Transformer and constructs a block based on its\narchitecture to conduct cross-window local attention calculations on images. By\namalgamating the strengths of Transformer and CNN, SwinIR effectively tackles"}, {"title": "Mamba", "content": "Mamba [12] is a novel feature sequence processing module inspired by the struc-\ntured state-space sequence model [13, 14]. Unlike self-attention, Mamba's learn-\nable variables do not increase with the length of the feature sequence but are\nrelated to the size of a single feature in the sequence. It integrates a selection\nmechanism to handle discrete, information-dense data. Models related to Mamba\ndemonstrate proficiency in analyzing remote information dependencies. One ad-\nvantage is that the length of the input feature sequence is not limited, and the"}, {"title": "Method", "content": "The Mamba pixel-wise sequential interaction network (MPSI) comprises three\nmain components: shallow feature extraction, deep feature extraction, and im-\nage reconstruction. The input to\nthe network is a batch of LR images $I_{LR} \\in R^{B\\times3\\timesH\\timesW}$, where B denotes the"}, {"title": "Spatial Attention Mamba Group", "content": "The spatial attention Mamba group (SAMG) serves as the cornerstone for deep\nfeature extraction, embodying a pivotal architecture depicted in Figure 1 (b),\nwith detailed constituents outlined in Figure 2. LP in the figure refers to linear"}, {"title": "Spatial Transformer Block", "content": "The Spatial Transformer Block (STB) block's\nmain function is to analyze features' spatial relationships.  Its\nmain mechanism is spatial window self-attention(SW-SA) [6,8]. The way\nSW-SA handles features in SAT is very similar to DAT [6]. Assume the input\nfeature is S. After layer normalization, STB first obtains Q, K, and V through\nlinear projection with bias. SW-SA in STB uses windows to organize Q, K, and\nV into sequences. The size of the window is $M \\times N$. The shape of Q, K, and\nV become $R^{B\\timesW\\timesMN\\timesC}$. Afterward, Q, K, and V are divided into multiple\nheads, the number of heads is H. Then the shape of each element is $H \\times \\frac{C}{H}$.\nWhile computing SW-SA, depth-wise convolution (dw-conv) is employed on V.\nSubsequently, STB amalgamates features from SW-SA and dw-conv, followed\nby a linear projection with weights denoted by W. Upon addition of S, STB\nobtains a temporary feature $S_{tmp} \\in R^{B\\timesHW\\timesC}$. The process is shown as:\n$S_{tmp} = LP(SWSA(Q, K, V) + DWcov(V)) + S$.\nFinally, $S_{tmp}$ undergoes layer normalization and is passed through an SGFN\nblock. Following the addition of $S_{tmp}$, STB obtains the output feature $S_{stb}$. The\ncalculation process is as:\n$S_{stb} = SGFN(LN(S_{tmp})) + S_{tmp}$"}, {"title": "Channel-Mamba Block", "content": "The primary function of the Channel-Mamba Block\n(CMB) is to conduct feature analysis at the channel level. The structure of CMB\nis shown in Figure 2 (c). To achieve this, the CMB introduces the Dual Direction\nBi-Mamba module (DDBM), as illustrated in Figure 3. Because image features\nshould not have a fixed processing order, DDBM processes forward and backward\nfeature sequences simultaneously to solve the problem of Mamba being sensitive\nto the order of feature sequences.\nFor the entire CMB, the input feature S undergoes layer normalization before\nit enters into two parts simultaneously, one is the DDBM module and the other"}, {"title": "", "content": "is the linear projection and DW-conv module [6]. The results are then element-\nwise summed and added to the initial feature S yielding $S_{tmp}$. The process is\nshown as:\n$S_{tmp} = LP(DDBM(S) + DWconv(LP(S))) + S$.\nSubsequently, $S_{tmp}$ undergoes another layer of normalization and the SGNF\nblock. Upon summation of $S_{tmp}$, the resulting feature is denoted as $S_{cmb}$. This\nprocess is the same with STB.\nIn DDBM, the input feature sequence $S = {X_1, X_2, ..., X_k}$ is copied and\nreverse-sorted to obtain $S_r$, where $k = H_nW_n$. Subsequently, S and $S_r$ enter\ntwo separate Mamba blocks. The Space State Model (SSM) [12] can use ma-\ntrix groups (A, B, C) and potential states h to connect features in the feature\nsequence to achieve long-range connections between features, thereby achieving\nbetter super-resolution results. The functionality of SSM is illustrated as:\n$h_t = Ah_{t-1}+ Bx_t$.\n$Y_t = Ch_t$.\n$K = {CB, CAB, ..., CA^*B}$.\n$Y = KS$.\nIn addition to the SSM block, Mamba also includes its linear projection, convo-\nlution and other structures, as depicted in Figure 3. Subsequent to the Mamba\nblock, CMB produces two feature sequences $Y_f$ and $Y_b$. Reversing the arrange-\nment of $Y_b$ and performing element-wise addition with the two sets of outputs\nyields $Y_s$. After undergoing a linear projection, DDBM converts $Y_s$ to $S_{out}$ as\nthe output. The calculation process of DDBM is illustrated as:\n$Mamba(S) = SiLU(LP(S)) \\otimes SSM(SiLU(Conv(LP(S))),$\n$Y_f = Mamba(S), Y_b = revers(Mamba(S_r)),$\n$S_{out} = LP(Y_fY_b)$"}, {"title": "Mamba channel recursion module", "content": "The structure of the Mamba channel recursion module (MCRM) is depicted\nin Figure 2 (d), which ignores the batch size. For this process to work effi-\nciently, the shape of the feature is temporarily changed from $S \\in R^{B\\timesHW\\timesC}$\nto $F \\in R^{B\\timesC\\timesH\\timesW}$. Assuming $L \\in R$ denotes the number of spatial attention\nMamba blocks (SAMB), MCRM conducts adaptive average pooling on features\n$F_{layers} = {F_1, F_2, ..., F_L, F_{L+1}}$ after input and each SAMB, along with a\nfeature preceding the spatial attention Mamba group (SAMG). Subsequently,\nSAMG acquires L + 1 features $P = {X^0, X^1, ..., X^L, X^{L+1}}$, each with di-\nmensions $R^{B\\timesC\\times1}$. This process can be represented as below, where $X^i$ denotes\neach element within P.\n$X^i = Adaptive Avg Pool(F_i)$.\nAfter layer normalization of P, it proceeds into the Mamba module which is\nthe key for recursive analysis, resulting in $P_{out} \\in R^{B\\timesL\\timesC}$. The last column\n$X_{last} \\in R^{B\\timesC}$ of $P_{out}$ then undergoes processing through a multilayer perceptron"}, {"title": "", "content": "(MLP) and sigmoid activation function to obtain layer weight $X_w \\in R^{B\\timesC}$. This\nweighted feature, $X_w$, is then multiplied with the last feature $F_{L+1}$ of $F_{layers}$,\nresulting in $F_{out}$ as the output of SAMG. The process is illustrated as:\n$X_w = Sigmoid(MLP(X_{last})),$\n$F_{out} = F_{L+1} X_{\\omega}.$"}, {"title": "Experiments", "content": "In this section, we conduct quality assessment experiments on MPSI and compare\nit with previous SR models. Additionally, ablation experiments were conducted\nto systematically assess the contribution of each component within MPSI."}, {"title": "Experimental setup", "content": "Dataset and evaluation metrics. We combined two distinct datasets for\ntraining purposes, namely DIV2K [1] and Flickr2K [28]. We acquired LR images\nthrough degradation. For validation, we utilized a comprehensive set comprising\nSet5 [5], Set14 [42], B100 [32], Urban100 [19], and Manga109 [33]. HR images\nwere degraded into LR images using bicubic degradation, with upscaling fac-\ntors of x2, x3, and \u00d74. Our evaluation of the super-resolution outcomes was\nconducted using two widely recognized metrics, namely PSNR and SSIM. We\nconduct comparative analyses with several other SR models to evaluate the per-\nformance capabilities of the proposed MPSI.\nImplementation Details. For MPSI, the deep feature extraction component\nconsists of 1 SAMG containing 9 SAMBs, and each SAMB has 1 STB and 1\nCMB. Each attention head is configured with 6 heads, the channel dimension\nof the SGFN is set to 60, and the expansion factor is set to 2. The window\ndimensions are specified as 8x32. For DDBM in the CMB, the settings of two\nMamba blocks are the same, where the SSM state expansion factor is set to\n32, the local convolution width is 3, and the block expansion factor is 4. In the\nMCRM, the SSM state expansion factor is set to 64, the local convolution width\nset to 4, and the block expansion factor set to 2.\nTraining details. Our model undergoes training with a batch size of 8 and a\npatch size of 64x64. The training process comprises a total of 500K iterations,\nemploying the Adam optimizer to minimize the $L_1$ loss function [6]. Specifically,\nthe Adam optimizer is configured with parameters \u03b2\u2081 = 0.9 and B2 = 0.99. The\ninitial learning rate is established at 2 \u00d7 10-4, with a scheduled halving at 250K,\n400K, 450K, and 475K iterations. The data is augmented by random rotations\nof 90\u00b0, 180\u00b0, and 270\u00b0, as well as horizontal flipping. Implementation-wise, the\nmodel is implemented by PyTorch [34] and trained on a single RTX4090 GPU."}, {"title": "Quality performance", "content": "Metrics comparison. To validate the performance of MPSI, we selected several\nclassic SR models for comparative analysis, including CARN [2], EDSR [29],\nIMDN [20], LAPAR-A [24], RDN [46], ESRT [31], SwinIR [27], ELAN [44], and\nDAT [6]. Constrained by experimental conditions, a subset of the Transformer\nmodels chosen for comparison encompassed lightweight architectures, specifically\nSwinIR, EIAN, and DAT. Based on the results presented in Table 1, in the\nscale of x2, although MPSI consistently exhibited strong performance across\nmost validation sets, it did not exhibit an unequivocal lead over the second-best\nperforming model. Notably, in the Manga109 dataset, MPSI's PSNR and SSIM\nresults slightly trailed behind DAT. However, in both the \u00d73 and \u00d74 upscaling\ntasks, MPSI consistently achieved the highest PSNR and SSIM scores across all\ndatasets. Particularly noteworthy is MPSI's outstanding image reconstruction"}, {"title": "Ablation studies", "content": "All models are trained on the DIV2K [1] and Flickr2K [28] datasets for 250K\niterations, with an initial learning rate of $1\\times10^{-4}$ and halving scheduled at 125K,\n200K, 225K, and 237.5K iterations. The efficacy of the models is demonstrated\nthrough PSNR and SSIM results evaluated on the Urban100 [19] and Manga109\n[33] datasets, as delineated in Table 2 and 3. Additionally, to underscore the\nmodel's proficiency in image restoration tasks, difference maps are meticulously\ncrafted for selected image generation outcomes, as depicted in the accompanying\nFigure 5 and 6. In these visual representations, regions exhibiting deeper blue\nhues indicate a higher similarity to the original image, whereas regions displaying\ndarker red hues indicate more pronounced deviations from the original image.\nInitially, we focus on evaluating the CMB and MCRM. The model, which\nexcludes MCRM and replaces CMB with STB, is designated as the Baseline.\nFor the CMB module, we replace it with STB to observe performance without\nCMB. Regarding MCRM, we systematically remove it from the model to assess\nits impact. We then investigate the internal architectures of CMB and MCRM.\nWe substitute the DDBM within CMB with channel self-attention (DDBM \u2192\nCA) to analyze its effect. To evaluate the impact of the Mamba recursive process\n(MRP) on MCRM performance, we retain only the features of the last layer for"}, {"title": "Conclusion", "content": "In this paper, we propose the MPSI network with the aim of improving the im-\nage reconstruction performance within single-image super-resolution tasks. We\npropose the CMB structure designed for efficient feature extraction and expres-\nsion. Leveraging the Mamba structure, CMB facilitates the establishment of\nlong-range dependencies among image features, enabling comprehensive capture\nof global image features. By amalgamating CMB with STB, SAMB can leverage"}, {"title": "", "content": "the strengths of Transformer in local feature extraction and Mamba's proficiency\nin long-range feature dependencies, resulting in enhanced performance in image\nfeature perception. Subsequently, we propose the utilization of Mamba to con-\nstruct MCRM. This enables the amalgamation of shallower feature information\ninto the final result during deep feature extraction, ensuring completeness of fea-\nture information. Through extensive comparative experiments, we validate the\neffectiveness of MPSI in super-resolution tasks, substantially improving image\nreconstruction results compared to previous SR models."}]}