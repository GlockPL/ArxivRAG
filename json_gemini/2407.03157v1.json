{"title": "LET THE CODE LLM EDIT ITSELF WHEN YOU EDIT THE CODE", "authors": ["Zhenyu He", "Jun Zhang", "Shengjie Luo", "Jingjing Xu", "Zhi Zhang", "Di He"], "abstract": "In this work, we investigate a typical scenario in code generation where a developer\nedits existing code in real time and requests a code assistant, e.g., a large language\nmodel, to re-predict the next token or next line on the fly. Naively, the LLM needs\nto re-encode the entire KV cache to provide an accurate prediction. However,\nthis process is computationally expensive, especially when the sequence length is\nlong. Simply encoding the edited subsequence and integrating it to the original\nKV cache meets the temporal confusion problem, leading to significantly worse\nperformance. We address this efficiency and accuracy trade-off by introducing\nPositional Integrity Encoding (PIE). Building upon the rotary positional encoding,\nPIE first removes the rotary matrices in the Key cache that introduce temporal\nconfusion and then reapplies the correct rotary matrices. This process ensures that\npositional relationships between tokens are correct and requires only a single round\nof matrix multiplication. We validate the effectiveness of PIE through extensive\nexperiments on the RepoBench-C-8k dataset, utilizing DeepSeek-Coder models\nwith 1.3B, 6.7B, and 33B parameters. Our evaluation includes three real-world\ncoding tasks: code insertion, code deletion, and multi-place code editing. Results\ndemonstrate that PIE reduces computational overhead by over 85% compared to\nthe standard full recomputation approach across all model sizes and tasks while\nwell approximating the model performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) (Dettmers et al., 2022; Anil et al., 2023; Touvron et al., 2023; Zeng\net al., 2023) have seen widespread adoption and achieved impressive results across various natural\nlanguage processing (NLP) tasks. Despite these successes, LLMs face significant computational\nchallenges, particularly in handling long sequences. To address this, numerous approaches have\nbeen proposed to accelerate the inference process, including lossless (e.g., memory and IO optimiza-\ntion (Dao et al., 2022; Kwon et al., 2023; Sheng et al., 2023), speculative decoding (Stern et al., 2018;\nLeviathan et al., 2023)) and lossy techniques (e.g., quantization (Frantar et al., 2022; Xiao et al.,"}, {"title": "2 RELATED WORK", "content": "Positional Encodings Positional information is essential for modeling languages. The original\nTransformer model (Vaswani et al., 2017) encodes positional information using Absolute Positional\nEncoding (APE). In particular, a (learnable) real-valued embedding is assigned to each position i.\nDifferently, Relative Positional Encodings (RPE) (Shaw et al., 2018; Dai et al., 2019; Raffel et al.,\n2020; Press et al., 2022; Su et al., 2021; Luo et al., 2021; 2022; Chi et al., 2022; Sun et al., 2023;\nChi et al., 2023; Li et al., 2023; He et al., 2024) instead encode the relative distance i \u2013 j for each\nposition pair (i, j). One of the most widely used RPE in state-of-the-art LLMs is Rotary Position\nEncoding (ROPE) (Su et al., 2021). RoPE rotates the query and key vectors by an angle proportional\nto their absolute positions before the attention mechanism, resulting in the attention being a function\nof the relative distance between tokens.\nIn the literature, relative positional encodings play essential roles across various tasks and data\nmodalities, such as improving the length extrapolation capability of language models (Press et al.,\n2022; Sun et al., 2023; Chi et al., 2022; 2023; He et al., 2024) and enabling flexible modeling of\nstructural information beyond sequence data like images (Liu et al., 2021) and graphs (Ying et al.,\n2021; Zhang et al., 2023a; Luo et al., 2023). In this work, we develop the Positional Integrity\nEncoding based on RoPE to improve the efficiency of LLMs in the real-time editing setting."}, {"title": "3 METHODS", "content": ""}, {"title": "3.1 BACKGROUND", "content": "Let s = (W1,W2, ..., wn) represent the input token sequence, where each w\u1d62 belongs to a fixed\nvocabulary. Let \u03b8LLM represent a Transformer-based large language model, which can calculate\nthe conditional probability distribution of the next token p(Wn+1|s; \u03b8LLM) and generate tokens\niteratively. Typically, the input s is fixed. Therefore, the generation process of LLMs usually employs\nthe KV Cache mechanism (Pope et al., 2023) to store previously computed Key/Value vectors\nduring each layer's attention calculation. We denote the KV cache as K = (K1, K2, . . ., Kn) and\nV = (V1, V2, ..., Vn), where Ki and Vi are the keys and values associated with token wi. When\npredicting the token at position n + 1, we can use wn as input and, in each layer, compute the attention\nbetween the current hidden representation and the stored KV cache, avoiding recomputing the hidden\nrepresentation of previous tokens. Without any confusion, we also denote the next token probability\ndistribution as p(Wn+1|K, V, wn; OLLM).\nIn this study, we aim to investigate a new scenario where the context s is real-time edited by\nusers, which makes it impossible for the KV cache to predict the correct next token without any\nmodification. We can model such real-time context as a sequence of steps. Each step can be\nformulated as an action where tokens from position i to j in s are edited, resulting in a modified\nsequence sedit = [W1, ..., Wi, A1, A2, ..., Am, Wj+1, ..., Wn], where [a1, a2, am] represent the\nnew inputs that replace [wi, . . ., wj]. Our goal is to accurately and efficiently predict Wn+1 given by\n\u03b8LLM on sedit. This problem is crucial in various scenarios. For instance, users can frequently edit\ntheir previous codes for different purposes and expect the code language model to swiftly adapt to\nthese changes and predict the correct next line based on the updated information.\nAs the context between position i and position j is edited, the KV cache corresponding to these tokens\nmust be updated. Furthermore, these changes will impact the representations of subsequent tokens af-\nter position j, thereby necessitating updates to all subsequent KV cache. The naive approach involves\na full-recomputation strategy: re-encoding all the KV cache for [a1, a2,..., Am, Wj+1,..., Wn]\nlayer by layer, followed by making predictions using the updated cache K* and V*. This ap-\nproach ensures the KV cache is exact when predicting the next tokens. However, it is easy to\nsee that it is computationally expensive, especially when the edits are light but the texts to be re-\nencoded are long. It's worth noting that the original K and V already encode rich information on\n[Wj+1,..., Wn], and a full recomputation may not be essential for practical problems. With this in\nmind, we seek to find ways to efficiently edit K and V, yielding Kedit and Vedit, which approximates\np(Wn+1|K*, V*, wn; \u03b8LLM) \u2248 p(Wn+1| Kedit, Vedit, wn; \u03b8LLM) in an effective way."}, {"title": "3.2 POSITIONAL INTEGRITY ENCODING (PIE)", "content": "When a user modifies s into sedit, KV cache associated with the first i tokens, i.e., K[1:i] and V[1:i],\nremains unchanged. As [a1,a2,..., am] is the user's new input, we feed this subsequence to the\nLLM to obtain the keys and values from position i + 1 to i + m. We denote this piece of new KV\ncache as Kedit\n[i+1:i+m] and Vedit\n[i+1:i+m], and now have the edited KV cache as:\nKedit = Concat(K [1:i], Kedit [i+1:i+m], K[j+1:n]),\nVedit = Concat(V[1:i], Vedit [i+1:i+m], V[j+1:n]),\nwhere the red symbols indicate real-time calculations.\nChallenges. The key challenge lies in how to edit the succeeding KV cache K[j+1:n] and V[j+1:n]\u00b7\nClearly, the modification of [a1, a2,..., am] impacts the subsequent content in two ways: seman-\ntically and structurally. The semantic impact refers to the changes in the understanding of the\nsubsequent text caused by the edited content. This can be a problem in natural language applications,\nsuch as dialog systems, where modifications to earlier conversations can significantly influence the\ngeneration of current responses. The other impact is structural, primarily concerning the temporal\nconfusion between the pre-edit and post-edit sequences when j i \u2260 m. This issue arises with\ncommon editing actions in code, such as additions and deletions (corresponding to j i = 0 or\nm = 0). To be more concrete, imagine the original sequence has 5 tokens. If we add three tokens"}, {"title": "Our approach.", "content": "To mitigate the temporal confusion during real-time editing, we propose a simple\nyet effective solution: Positional Integrity Encoding (PIE), which ensures that positional information\nremains correctly ordered after editing without the need to re-encode the KV cache for subsequent\ntokens. PIE builds upon the rotary positional encoding (RoPE) (Su et al., 2021), which is the most\nwidely used positional encoding in LLMs. Without loss of generality, given a query vector xi at\nposition i and a key vector xj at position j, RoPE calculates the dot-product similarity using\nZij = x\u1d40\u1d62W\u1d40qR\u2c7c\u208b\u1d62W\u2096x\u2c7c\nwhere Rj-i is the rotary matrix parameterized by the relative distance j \u2013 i, and Wq and Wk are\nlearnable projection matrices. By definition, Rj-i can be expressed by the multiplication of two\nrotary matrices:\nRj-i = R\u1d40\u1d62R\u2c7c\nFor practical implementation, during inference, we compute R\u1d40\u1d62W\u2096x\u1d62 as the key on the fly and store\nit in the cache, and when a query arrives at a new position, we rotate the query using its corresponding\nrotary matrix and calculate its similarity with all the keys in the cache to obtain the attention scores.\nIt can be easily seen that the positional information in the KV cache is encoded within the rotary\nmatrix. When an edit occurs, the rotary matrix associated with the keys must be adjusted to reflect their\npost-edit locations. Leveraging the formulation of RoPE-based attention calculation, this challenge\ncan be addressed by first removing the rotary matrices in K that introduce temporal confusion and\nthen reapplying the correct rotary matrix. In detail, assume we would like to update the key vector\nk\u2097\u2c7c\u2032 for the original position j\u2032 \u2208 [j + 1, n], where l \u2208 [1, L] is the layer index. We can simply edit\nthe key vector by using\nkedit = R\u1d40\u2071+m+j\u2032-jR\u2c7c\u2032-\u00b9kj\u2032\nwhere R\u2c7c\u2032\u207b\u00b9, the inverse rotary matrix at position j\u2032, is used to remove the incorrect positional\ninformation, and R\u1d62+m+j\u2032-j is used to encode the correct position i + m + j\u2032 \u2013 j in sedit. It can\neasily seen that the computation can be further simplified as\nkedit = R\u1d62+m+j\u2032-jR\u2c7c\u2032\u00b9kj\u2032 = R\u1d62+m+j\u2032-jR\u208b\u2c7c\u2032kj\u2032 = R\u1d62+m\u208bj kj\u2032\nHence, the full editing process for K[j+1:n] is as follows:\nKedit\n[j+1:n] = [kedit\nj+1, ..., kedit\nj\u2032, ..., kedit\nn],\nwhere each kedit = {kedit,\u2081, ..., kedit,\u2081, ..., kedit}, j\u2032 \u2208 [j + 1, n], l \u2208 [1, L]\neach kedit,\u2081 = R\u1d62+m\u208bj k\u2c7cl\nUnlike the fully recomputation approach, the above calculation only requires a single round of matrix\nmultiplication to directly modify the pre-computed KV cache, where the computational overhead\ncan be considered negligible. By utilizing these transformations, we finally construct the edited KV\ncache as:\nKedit = Concat(K[1:i], Kedit [i+1:i+m], Kedit\n[j+1:n]),\nVedit = Concat(V[1:i], Vedit [i+1:i+m], V[j+1:n]),\nwhere the red symbols indicate real-time calculations. The LLM then makes predictions based on\np(Xn+1| Kedit, Vedit, Xn; OLLM). It is worth noting that PIE is compatible with KV cache eviction\nmethods (Xiao et al., 2024; Zhang et al., 2024; Liu et al., 2024b). These KV cache eviction methods\nfocus on reducing the memory usage of the KV cache during inference. PIE is designed to obtain the\nKV cache of the edited context with minimal overhead. By integrating PIE with KV cache eviction\nmethods, it is possible to maintain efficient memory management while ensuring the integrity of the\npositional information in the real-time edit setting."}, {"title": "4 EXPERIMENTS", "content": "In this section, we empirically study the effectiveness of our proposed method. In particular, we aim\nat answering the following questions through experiments:\n\u2022 Question 1: Can our Positional Integrity Encoding maintain the prediction accuracy of full\nre-computation in code editing scenarios?\n\u2022 Question 2: How much efficiency improvement can be achieved by using our Positional\nIntegrity Encoding compared to existing approaches?\n\u2022 Question 3: How large is the gap between our Positional Integrity Encoding and full\nre-computation in terms of LLM's predictions & representations?\nWe will answer each question with carefully designed experiments in the following sub-sections. Due\nto space limitations, more details are presented in Appendix A."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Tasks. Our experiments are conducted on RepoBench-C-8k (Liu et al., 2024a). This benchmark\nfocuses on the prediction of the next line of code, given a set of in-file context (including import\nstatements and preceding lines before the target line), and cross-file context (comprising snippets\nfrom other files parsed by import statements). The detailed statistics of RepoBench-C-8k is shown\nin Table 1. To effectively evaluate next-line prediction performance of code LLMs, we follow Liu\net al. (2024a) to use three task settings: (1) Cross-File-First (XF-F): mask the first appearance of a\ncross-file line within a file; (2) Cross-File-Random (XF-R): mask a random and non-first occurrence\nof a cross-file line; (3) In-File (IF): mask an in-file line that does not involve any cross-file modules.\nMoreover, we carefully design three real-world scenarios covering code insertion, code deletion,\nand code edition to comprehensively examine our approach. See Appendix A for more detailed\ndescriptions of tasks construction.\nSettings. In our experiments, we employ DeepSeek-Coder (Guo et al., 2024), a code LLM that\nachieves strong performance in handling repository-level code completion tasks. We use Transform-\ners (Wolf et al., 2020) as our codebase. We benchmark our method on models of different sizes\ncovering 1.3B, 6.7B, and 33B. During inference, the greedy decoding strategy is used to determin-\nistically generate 64 tokens. For 1.3B and 6.7B models, all the experiments are conducted on a single\nNVIDIA A100 GPU. For 33B models, the time for encoding the context is conducted on two NVIDIA\nA100 GPUs and the full generation process is conducted on eight NVIDIA A100 GPUs. The first\nnon-comment line in the output is truncated and used as the prediction. The batch size is set to 1.\nAll experiments are repeated three times with different seeds and the averaged scores are reported.\nEvaluation. For comparison with our Positional Integrity Encoding, we choose two standard\napproaches as baselines: (1) Full-recomputation: re-compute the KV cache for all edited tokens\nand subsequent tokens; (2) Conflict Fast Encoding: re-compute the KV cache for the edited tokens\nwhile keeping the rest of the cache intact (i.e., using equation (1,2). Following Lu et al. (2021), we\nuse Exact Match (EM) and Edit Similarity (ES) (Svyatkovskiy et al., 2020) to evaluate the accuracy\nof the predicted code lines on code completion tasks. We also report the time required to encode\nthe edited context for efficiency evaluation."}, {"title": "4.2 MAIN RESULTS", "content": "Positional Integrity Encoding perfectly preserves the full re-computation performance. Results\nof different code editing settings are presented in Table 2, 3 and 4 respectively. It can be easily seen"}, {"title": "4.3 MORE ANALYSIS", "content": "In this subsection, we further present detailed analysis to investigate how large is the gap between\nour Positional Integrity Encoding and full re-computation in terms of context representations and\npredictions from LLMs, which provide additional insight of our approach.\nHow large is the gap on context representations? In practical scenarios, real-time editing by users\nresults in the modified sequence xedit, requiring the KV cache to must be updated. In our analysis,\nwe use the cosine similarity between context representations of full re-computation K+1:n] and (1)\nour Positional Integrity Encoding Kit 1:n]; (2) Conflict Fast Encoding K[j+1:n]. We employ the\nDeepSeek-Coder 6.7B model on the Python subset of RepoBench. Averaged results are reported.\nIn Figure 3, the cosine similarity between representations of full re-computation and our Positional\nIntegraty Enocding is consistently around 1.0 across all layers. This high similarity demonstrates the"}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced Positional Integrity Encoding (PIE), a novel method designed to enhance\nthe efficiency of large language models (LLMs) in the real-time editing setting. Our approach\naddresses the significant computational overhead associated with re-encoding contexts after small\nedits, a common scenario in interactive coding environments. Through extensive experiments,\nwe demonstrated that PIE not only significantly reduces latency but also maintains high accuracy\ncompared to the naive full re-computation method.\nPIE represents a substantial step forward in the development of efficient LLMs, particularly in\ndynamic contexts where frequent edits are made. Future work could explore the integration of\nPIE with other optimization techniques and its application to a broader range of tasks beyond code\ngeneration. Our method paves the way for more responsive and resource-efficient AI assistants,\nenhancing their practicality and usability in various real-world scenarios."}, {"title": "A EXPERIMENTAL DETAILS", "content": ""}, {"title": "A.1 EXPERIMENTAL SETUP", "content": "Tasks Construction for Code Insertion. To simulate code insertion tasks, we start by randomly\ndeleting five consecutive lines from each context. The resulting context, which lacks these five lines,\nis considered the original context. The complete context, which includes the previously deleted lines,\nis treated as the edited context. The tokens within the deleted lines are identified as the inserted\ntokens (around 64 tokens for Python and 51 tokens for Java). This setup allows us to evaluate the\nmodel's capability to accurately restore missing code segments, mimicking real-world scenarios\nwhere developers frequently insert blocks of code.\nTasks Construction for Code Deletion. For code deletion tasks, we begin by randomly selecting a\nline within the context and then inserting five randomly sampled lines at this position. The context\ncontaining these additional lines is designated as the original context. The complete context, which\nexcludes the inserted lines, is regarded as the edited context. The tokens in the inserted lines are\ntreated as the deleted tokens (around 64 tokens for Python and 51 tokens for Java). This construction\nenables us to assess the model's performance in identifying and removing extraneous code, reflecting\nsituations where developers need to clean up or refactor their codebase.\nTasks Construction for Multi-place Code Edition To comprehensively evaluate the model's\nperformance in handling simultaneous code insertion and deletion, we construct a task scenario that\nintegrates both operations. Initially, we randomly delete five consecutive lines from each context to\nsimulate code insertion. The context without these lines is treated as the original context. The tokens\nin the deleted lines are identified as the inserted tokens.\nSimultaneously, we randomly select another line within the context and insert five randomly sampled\nlines at this position. The complete context, which includes all lines as they appear after both\ndeletions and insertions, is regarded as the edited context. The tokens in the newly inserted lines are\nconsidered the deleted tokens. This dual operation setup allows us to evaluate the model's ability to\nhandle complex, simultaneous edits, adding missing code segments while removing extraneous ones,\nreflecting the multifaceted nature of real-world coding environments where developers often perform\nmultiple types of edits concurrently."}]}