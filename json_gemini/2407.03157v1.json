{"title": "LET THE CODE LLM EDIT ITSELF WHEN YOU EDIT THE CODE", "authors": ["Zhenyu He", "Jun Zhang", "Shengjie Luo", "Jingjing Xu", "Zhi Zhang", "Di He"], "abstract": "In this work, we investigate a typical scenario in code generation where a developer edits existing code in real time and requests a code assistant, e.g., a large language model, to re-predict the next token or next line on the fly. Naively, the LLM needs to re-encode the entire KV cache to provide an accurate prediction. However, this process is computationally expensive, especially when the sequence length is long. Simply encoding the edited subsequence and integrating it to the original KV cache meets the temporal confusion problem, leading to significantly worse performance. We address this efficiency and accuracy trade-off by introducing Positional Integrity Encoding (PIE). Building upon the rotary positional encoding, PIE first removes the rotary matrices in the Key cache that introduce temporal confusion and then reapplies the correct rotary matrices. This process ensures that positional relationships between tokens are correct and requires only a single round of matrix multiplication. We validate the effectiveness of PIE through extensive experiments on the RepoBench-C-8k dataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters. Our evaluation includes three real-world coding tasks: code insertion, code deletion, and multi-place code editing. Results demonstrate that PIE reduces computational overhead by over 85% compared to the standard full recomputation approach across all model sizes and tasks while well approximating the model performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) (Dettmers et al., 2022; Anil et al., 2023; Touvron et al., 2023; Zeng et al., 2023) have seen widespread adoption and achieved impressive results across various natural language processing (NLP) tasks. Despite these successes, LLMs face significant computational challenges, particularly in handling long sequences. To address this, numerous approaches have been proposed to accelerate the inference process, including lossless (e.g., memory and IO optimization (Dao et al., 2022; Kwon et al., 2023; Sheng et al., 2023), speculative decoding (Stern et al., 2018; Leviathan et al., 2023)) and lossy techniques (e.g., quantization (Frantar et al., 2022; Xiao et al.,"}, {"title": "2 RELATED WORK", "content": "Positional Encodings Positional information is essential for modeling languages. The original Transformer model (Vaswani et al., 2017) encodes positional information using Absolute Positional Encoding (APE). In particular, a (learnable) real-valued embedding is assigned to each position i. Differently, Relative Positional Encodings (RPE) (Shaw et al., 2018; Dai et al., 2019; Raffel et al., 2020; Press et al., 2022; Su et al., 2021; Luo et al., 2021; 2022; Chi et al., 2022; Sun et al., 2023; Chi et al., 2023; Li et al., 2023; He et al., 2024) instead encode the relative distance i \u2013 j for each position pair (i, j). One of the most widely used RPE in state-of-the-art LLMs is Rotary Position Encoding (ROPE) (Su et al., 2021). RoPE rotates the query and key vectors by an angle proportional to their absolute positions before the attention mechanism, resulting in the attention being a function of the relative distance between tokens.\nIn the literature, relative positional encodings play essential roles across various tasks and data modalities, such as improving the length extrapolation capability of language models (Press et al., 2022; Sun et al., 2023; Chi et al., 2022; 2023; He et al., 2024) and enabling flexible modeling of structural information beyond sequence data like images (Liu et al., 2021) and graphs (Ying et al., 2021; Zhang et al., 2023a; Luo et al., 2023). In this work, we develop the Positional Integrity Encoding based on RoPE to improve the efficiency of LLMs in the real-time editing setting."}, {"title": "3 METHODS", "content": "3.1 BACKGROUND\nLet s = (W1,W2, ..., wn) represent the input token sequence, where each w\u2081 belongs to a fixed vocabulary. Let 0LLM represent a Transformer-based large language model, which can calculate the conditional probability distribution of the next token p(Wn+1|8; 0LLM) and generate tokens iteratively. Typically, the input s is fixed. Therefore, the generation process of LLMs usually employs the KV Cache mechanism (Pope et al., 2023) to store previously computed Key/Value vectors during each layer's attention calculation. We denote the KV cache as K = (K1, K2, . . ., Kn) and V = (V1, V2, ..., Vn), where Ki and Vi are the keys and values associated with token wi. When predicting the token at position n + 1, we can use wn as input and, in each layer, compute the attention between the current hidden representation and the stored KV cache, avoiding recomputing the hidden representation of previous tokens. Without any confusion, we also denote the next token probability distribution as p(Wn+1|K, V, wn; OLLM).\nIn this study, we aim to investigate a new scenario where the context s is real-time edited by users, which makes it impossible for the KV cache to predict the correct next token without any modification. We can model such real-time context as a sequence of steps. Each step can be formulated as an action where tokens from position i to j in s are edited, resulting in a modified sequence sedit = [W1, ..., Wi, A1, A2, ..., Am, Wj+1, ..., Wn], where [a1, a2, am] represent the new inputs that replace [wi, . . ., wj]. Our goal is to accurately and efficiently predict Wn+1 given by OLLM on sedit. This problem is crucial in various scenarios. For instance, users can frequently edit their previous codes for different purposes and expect the code language model to swiftly adapt to these changes and predict the correct next line based on the updated information.\nAs the context between position i and position j is edited, the KV cache corresponding to these tokens must be updated. Furthermore, these changes will impact the representations of subsequent tokens af-ter position j, thereby necessitating updates to all subsequent KV cache. The naive approach involves a full-recomputation strategy: re-encoding all the KV cache for [a1, a2,..., Am, Wj+1,..., Wn] layer by layer, followed by making predictions using the updated cache K* and V*. This ap-proach ensures the KV cache is exact when predicting the next tokens. However, it is easy to see that it is computationally expensive, especially when the edits are light but the texts to be re-encoded are long. It's worth noting that the original K and V already encode rich information on [Wj+1,..., Wn], and a full recomputation may not be essential for practical problems. With this in mind, we seek to find ways to efficiently edit K and V, yielding Kedit and Vedit, which approximates p(Wn+1|K*, V*, wn; 0LLM) \u2248 p(Wn+1 Kedit, Vedit, 1, wn; OLLM) in an effective way.\n3.2 POSITIONAL INTEGRITY ENCODING (PIE)\nWhen a user modifies s into sedit, KV cache associated with the first i tokens, i.e., K[1:1] and V[1:1], remains unchanged. As [a1,a2,..., am] is the user's new input, we feed this subsequence to the LLM to obtain the keys and values from position i + 1 to i + m. We denote this piece of new KV cache as Kedit and Vedit, and now have the edited KV cache as:\n[i+1:i+m]\n[i+1:i+m]\nKedit = Concat(K [1:2], Kedit\n[i+1:i+m], K[j+1:n]),\t\t(1)\nVedit = Concat(V[1:2], Vedit\n[i+1:i+m], V[j+1:n]),\t\t(2)\nwhere the red symbols indicate real-time calculations.\nChallenges. The key challenge lies in how to edit the succeeding KV cache K[j+1:n] and V[j+1:n]. Clearly, the modification of [a1, a2,..., am] impacts the subsequent content in two ways: seman-tically and structurally. The semantic impact refers to the changes in the understanding of the subsequent text caused by the edited content. This can be a problem in natural language applications, such as dialog systems, where modifications to earlier conversations can significantly influence the generation of current responses. The other impact is structural, primarily concerning the temporal confusion between the pre-edit and post-edit sequences when j i \u2260 m. This issue arises with common editing actions in code, such as additions and deletions (corresponding to j i = 0 or m = 0). To be more concrete, imagine the original sequence has 5 tokens. If we add three tokens"}, {"title": "Our approach.", "content": "To mitigate the temporal confusion during real-time editing, we propose a simple yet effective solution: Positional Integrity Encoding (PIE), which ensures that positional information remains correctly ordered after editing without the need to re-encode the KV cache for subsequent tokens. PIE builds upon the rotary positional encoding (RoPE) (Su et al., 2021), which is the most widely used positional encoding in LLMs. Without loss of generality, given a query vector xi at position i and a key vector xj at position j, RoPE calculates the dot-product similarity using\nZij = xWRj-iWkxj\t\t(3)\nwhere Rj-i is the rotary matrix parameterized by the relative distance j \u2013 i, and Wq and Wk are learnable projection matrices. By definition, Rj-i can be expressed by the multiplication of two rotary matrices:\nRj-i = RR\t\t(4)\nFor practical implementation, during inference, we compute Re, iWkxi as the key on the fly and store it in the cache, and when a query arrives at a new position, we rotate the query using its corresponding rotary matrix and calculate its similarity with all the keys in the cache to obtain the attention scores.\nIt can be easily seen that the positional information in the KV cache is encoded within the rotary matrix. When an edit occurs, the rotary matrix associated with the keys must be adjusted to reflect their post-edit locations. Leveraging the formulation of RoPE-based attention calculation, this challenge can be addressed by first removing the rotary matrices in K that introduce temporal confusion and then reapplying the correct rotary matrix. In detail, assume we would like to update the key vector k, for the original position j' \u2208 [j + 1, n], where l \u2208 [1, L] is the layer index. We can simply edit the key vector by using\nkedit = Ri+m+j-jRk\t\t(5)\nwhere R,\u00b9, the inverse rotary matrix at position j', is used to remove the incorrect positional information, and Ri+m+j'\u2212j is used to encode the correct position i + m + j' \u2013 j in sedit. It can easily seen that the computation can be further simplified as\nl\nkedit, = Ri+m+j-jRjkj = Ri+m+j-jR_jkj = Ri+m-jkj\t\t(6)\nHence, the full editing process for K[j+1:n] is as follows:\nKedit\tK[j+1:n]\t=[Kedit,..., Kedit]l\t(7)\nj'\t j+1'\t n'\t\tl'\nwhere each Kodit = {kedit,1,..., kedit,, ..., kedit, L}, j' \u2208 [j + 1, n], l \u2208 [1, L]\nj'\tl\\l\neach kodit,1 = Ri+m-jkj\tl\nl\nUnlike the fully recomputation approach, the above calculation only requires a single round of matrix multiplication to directly modify the pre-computed KV cache, where the computational overhead can be considered negligible. By utilizing these transformations, we finally construct the edited KV cache as:\nKedit = Concat(K[1:2], Kedit\n[1:i+m], Kedit1:])\t\t(8)\nVedit = Concat(V[1:2], Vedit\n[1:i+m], V[j+1:n]),\t\t(9)\nwhere the red symbols indicate real-time calculations. The LLM then makes predictions based on P(Xn+1 Kedit, Vedit, Xn; OLLM). It is worth noting that PIE is compatible with KV cache eviction methods (Xiao et al., 2024; Zhang et al., 2024; Liu et al., 2024b). These KV cache eviction methods focus on reducing the memory usage of the KV cache during inference. PIE is designed to obtain the KV cache of the edited context with minimal overhead. By integrating PIE with KV cache eviction methods, it is possible to maintain efficient memory management while ensuring the integrity of the positional information in the real-time edit setting."}, {"title": "4 EXPERIMENTS", "content": "In this section, we empirically study the effectiveness of our proposed method. In particular, we aim at answering the following questions through experiments:\n\u2022 Question 1: Can our Positional Integrity Encoding maintain the prediction accuracy of full re-computation in code editing scenarios?\n\u2022 Question 2: How much efficiency improvement can be achieved by using our Positional Integrity Encoding compared to existing approaches?\n\u2022 Question 3: How large is the gap between our Positional Integrity Encoding and full re-computation in terms of LLM's predictions & representations?\nWe will answer each question with carefully designed experiments in the following sub-sections. Due to space limitations, more details are presented in Appendix A.\n4.1 EXPERIMENTAL SETUP\nTasks. Our experiments are conducted on RepoBench-C-8k (Liu et al., 2024a). This benchmark focuses on the prediction of the next line of code, given a set of in-file context (including import statements and preceding lines before the target line), and cross-file context (comprising snippets from other files parsed by import statements). To effectively evaluate next-line prediction performance of code LLMs, we follow Liu et al. (2024a) to use three task settings: (1) Cross-File-First (XF-F): mask the first appearance of a cross-file line within a file; (2) Cross-File-Random (XF-R): mask a random and non-first occurrence of a cross-file line; (3) In-File (IF): mask an in-file line that does not involve any cross-file modules. Moreover, we carefully design three real-world scenarios covering code insertion, code deletion, and code edition to comprehensively examine our approach. See Appendix A for more detailed descriptions of tasks construction.\nSettings. In our experiments, we employ DeepSeek-Coder (Guo et al., 2024), a code LLM that achieves strong performance in handling repository-level code completion tasks. We use Transform-ers (Wolf et al., 2020) as our codebase. We benchmark our method on models of different sizes covering 1.3B, 6.7B, and 33B. During inference, the greedy decoding strategy is used to determin-istically generate 64 tokens. For 1.3B and 6.7B models, all the experiments are conducted on a single NVIDIA A100 GPU. For 33B models, the time for encoding the context is conducted on two NVIDIA A100 GPUs and the full generation process is conducted on eight NVIDIA A100 GPUs. The first non-comment line in the output is truncated and used as the prediction. The batch size is set to 1. All experiments are repeated three times with different seeds and the averaged scores are reported.\nEvaluation. For comparison with our Positional Integrity Encoding, we choose two standard approaches as baselines: (1) Full-recomputation: re-compute the KV cache for all edited tokens and subsequent tokens; (2) Conflict Fast Encoding: re-compute the KV cache for the edited tokens while keeping the rest of the cache intact (i.e., using equation (1,2). Following Lu et al. (2021), we use Exact Match (EM) and Edit Similarity (ES) (Svyatkovskiy et al., 2020) to evaluate the accuracy of the predicted code lines on code completion tasks. We also report the time required to encode the edited context for efficiency evaluation.\n4.2 MAIN RESULTS\nPositional Integrity Encoding perfectly preserves the full re-computation performance. Results of different code editing settings are presented in Table 2, 3 and 4 respectively. It can be easily seen"}, {"title": "4.3 MORE ANALYSIS", "content": "In this subsection, we further present detailed analysis to investigate how large is the gap between our Positional Integrity Encoding and full re-computation in terms of context representations and predictions from LLMs, which provide additional insight of our approach.\nHow large is the gap on context representations? In practical scenarios, real-time editing by users results in the modified sequence xedit, requiring the KV cache to must be updated. In our analysis, we use the cosine similarity between context representations of full re-computation K+1:n] and (1) our Positional Integrity Encoding Kit 1:n]; (2) Conflict Fast Encoding K[j+1:n]. We employ the DeepSeek-Coder 6.7B model on the Python subset of RepoBench. Averaged results are reported.\nIn Figure 3, the cosine similarity between representations of full re-computation and our Positional Integrity Enocding is consistently around 1.0 across all layers. This high similarity demonstrates the"}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced Positional Integrity Encoding (PIE), a novel method designed to enhance the efficiency of large language models (LLMs) in the real-time editing setting. Our approach addresses the significant computational overhead associated with re-encoding contexts after small edits, a common scenario in interactive coding environments. Through extensive experiments, we demonstrated that PIE not only significantly reduces latency but also maintains high accuracy compared to the naive full re-computation method.\nPIE represents a substantial step forward in the development of efficient LLMs, particularly in dynamic contexts where frequent edits are made. Future work could explore the integration of PIE with other optimization techniques and its application to a broader range of tasks beyond code generation. Our method paves the way for more responsive and resource-efficient AI assistants, enhancing their practicality and usability in various real-world scenarios."}, {"title": "A EXPERIMENTAL DETAILS", "content": "A.1 EXPERIMENTAL SETUP\nTasks Construction for Code Insertion. To simulate code insertion tasks, we start by randomly deleting five consecutive lines from each context. The resulting context, which lacks these five lines, is considered the original context. The complete context, which includes the previously deleted lines, is treated as the edited context. The tokens within the deleted lines are identified as the inserted tokens (around 64 tokens for Python and 51 tokens for Java). This setup allows us to evaluate the model's capability to accurately restore missing code segments, mimicking real-world scenarios where developers frequently insert blocks of code.\nTasks Construction for Code Deletion. For code deletion tasks, we begin by randomly selecting a line within the context and then inserting five randomly sampled lines at this position. The context containing these additional lines is designated as the original context. The complete context, which excludes the inserted lines, is regarded as the edited context. The tokens in the inserted lines are treated as the deleted tokens (around 64 tokens for Python and 51 tokens for Java). This construction enables us to assess the model's performance in identifying and removing extraneous code, reflecting situations where developers need to clean up or refactor their codebase.\nTasks Construction for Multi-place Code Edition To comprehensively evaluate the model's performance in handling simultaneous code insertion and deletion, we construct a task scenario that integrates both operations. Initially, we randomly delete five consecutive lines from each context to simulate code insertion. The context without these lines is treated as the original context. The tokens in the deleted lines are identified as the inserted tokens.\nSimultaneously, we randomly select another line within the context and insert five randomly sampled lines at this position. The complete context, which includes all lines as they appear after both deletions and insertions, is regarded as the edited context. The tokens in the newly inserted lines are considered the deleted tokens. This dual operation setup allows us to evaluate the model's ability to handle complex, simultaneous edits, adding missing code segments while removing extraneous ones, reflecting the multifaceted nature of real-world coding environments where developers often perform multiple types of edits concurrently."}]}