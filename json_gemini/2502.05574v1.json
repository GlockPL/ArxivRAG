{"title": "Event Stream-based Visual Object Tracking: HDETrack V2 and a High-Definition Benchmark", "authors": ["Shiao Wang", "Xiao Wang", "Chao Wang", "Liye Jin", "Lin Zhu", "Bo Jiang", "Yonghong Tian", "Jin Tang"], "abstract": "Recent years have witnessed unprecedented growth in the field of visual object tracking using bio-inspired event cameras. Previous research in this domain has primarily followed two paths: aligning RGB data and event streams for precise tracking or developing trackers that rely solely on events. However, while the former approach often incurs significant computational costs during inference, the latter may struggle with challenges posed by noisy event data or limited spatial resolution. In this paper, we present a hierarchical knowledge distillation framework designed to leverage multi-modal and multi-view information during the training phase, while utilizing only event signals for tracking. Specifically, we first train a teacher Transformer-based multi-modal/multi-view tracking network by simultaneously feeding RGB frames and event streams. We then introduce a novel hierarchical knowledge distillation strategy that incorporates the similarity matrix, feature representation, and response map-based distillation to guide the learning of the student Transformer network. We also enhance the model's ability to capture temporal dependencies by applying the temporal Fourier transform to establish temporal relationships between video frames. We adapt the network model to specific target objects during testing via a newly proposed test-time tuning strategy to achieve high performance and flexibility in target tracking. Recognizing the limitations of existing event-based tracking datasets, which are predominantly low-resolution (346 \u00d7 260), we propose EventVOT, the first large-scale high-resolution (1280 \u00d7 720) event-based tracking dataset. It comprises 1141 videos spanning diverse categories such as pedestrians, vehicles, UAVs, ping pong, etc. Extensive experiments on both low-resolution (FE240hz, VisEvent, FELT), and our newly proposed high-resolution EventVOT dataset fully validated the effectiveness of our proposed method. Both the benchmark dataset and source code have been released on https://github.com/Event-AHU/EventVOT_Benchmark.", "sections": [{"title": "1 INTRODUCTION", "content": "VISUAL object tracking [1]\u2013[11], has long been a fun-damental task in the field of computer vision, whichaims to locate the target object in video frames basedon its appearance features initialized in the first frame.Existing trackers are primarily designed for RGB camerasand are widely deployed in autonomous driving, dronephotography, intelligent video surveillance, and other fields.However, RGB cameras often struggle in extreme conditionssuch as overexposure, low lighting, and fast motion. Thesecomplex scenarios can occur in individual frames or eventhroughout an entire video, complicating efforts to enhanceoverall tracking performance through the mere addition oflabeled data.\nIn recent years, biologically inspired event cameras havegradually entered people's field of vision. Different from tra-ditional RGB cameras, event cameras continuously generateevent signals by asynchronously capturing changes in light-ing intensity within a scene. Concurrently, event cameras\npresent a range of benefits over traditional RGB cameras,such as enhanced dynamic range, superior temporal reso-lution, reduced power requirements, and enhanced privacypreservation. It performs better in extreme conditions, suchas low light and fast motion, and can be effectively appliedto a range of tasks, including human action recognition [12],object detection [13], and visual object tracking [14]\u2013[16].\nRecently, some outstanding works have emerged in thefield of visual object tracking tasks, leveraging the capabil-ities of event cameras. For example, Zhang et al. proposeAFNet [17] and CDFI [18] to combine the frame and eventdata via multi-modality alignment and fusion modules.STNet [19] is proposed to connect the Transformer andspiking neural networks for event-based tracking. Zhu etal. [20] attempt to mine the key events and employ agraph-based network to embed the irregular spatiotemporalinformation of key events into a high-dimensional featurespace for tracking. Messikommer et al. [21] propose data-driven feature tracking with low-latency event cameras in agray-scale frame. However, these visual trackers still havesome unresolved issues:\n\u2022 Multi-modal Tracking Leads to High ComputationalCosts: When the target is stationary or moving slowly,event cameras generate sparse signals, blurring the tar-get's contours and causing tracking failure. Researchershave combined event cameras with RGB cameras tomitigate this issue, but this multi-modal approach sig-nificantly increases computational complexity.\n\u2022 Offline Training Results in Insufficient Flexibility:"}, {"title": "2 RELATED WORK", "content": "2.1 Event Camera based Tracking\nWith the development of bio-inspired event cameras,event camera based visual object tracking has graduallybecome a focus of attention. The early event camera basedtracking algorithm ESVM [25] proposed an event-guidedsupport vector machine (ESVM) for tracking high-speedmoving objects, which solves the tracking problem causedby motion blur and large displacement in low frame ratecameras. Recently, Chen et al. [26] adopt the Adaptive TimeSurface with Linear Time Decay (ATSLTD) algorithm forevent-to-frame conversion, the spatiotemporal informationof asynchronous retinal events is transformed into an AT-SLTD frame sequence for efficient object tracking. Zhu etal. [20] propose a new end-to-end learning framework forobject tracking based on event cameras, which improvestracking accuracy and speed through key event samplingand graph network embedding. Zhang et al. [19] proposea novel tracking network STNet based on a spiking neu-ral network and Transformer network, which can effec-tively extract spatiotemporal information from events andachieve a better tracking performance. Wang et al. [14]propose a novel hierarchical knowledge distillation frame-work, combined with multi-modal/multi-view information,high-speed and low latency visual tracking can be achievedduring the testing phase using only event signals.\nFor event based multi-modal tracking, Zhang et al. [18]propose a multi-modal fusion method that combines visualcues from frame and event domains to improve single objecttracking performance under degraded conditions, whichalso enhances the effect through cross-domain attentionmechanism and adaptive weighting scheme. VisEvent [15]proposed by Wang et al. transforms the event stream intoimages and extends a single-modal tracker to a dual-modalversion, with a cross-modal converter enabling better fusionof RGB and event data. AFNet [17], proposed by Zhanget al., a framework for high frame rate tracking, eventalignment, and fusion network has been proposed, whichsignificantly improves the performance of high frame ratetracking by combining the advantages of traditional frame-works and event cameras. Gehrig et al. [27] propose EKLT,an asynchronous photometric feature tracking method thatcombines the advantages of event cameras and RGB cam-eras to achieve visual feature tracking at high temporalresolution and improve tracking accuracy. ViPT [28], whichadjusts the pre-trained RGB base model by introducing asmall number of trainable parameters to adapt the differentmulti-modal tracking tasks. Different from existing works,we propose to conduct a hierarchical knowledge distillationstrategy from multi-modal or multi-view in the trainingphase and only utilize the event data for efficient and low-latency tracking."}, {"title": "2.2 Knowledge Distillation", "content": "The knowledge distillation strategy has been widelyproven to be an effective method of knowledge transfer.Deng et al. [29] propose an image-based knowledge distil-lation learning framework that improves the performanceof event camera models in feature extraction by extractingknowledge from the image domain, achieving significantimprovements in the performance of event cameras in targetclassification and optical flow prediction tasks. In visualobject tracking, Shen et al. [30] propose a distillation learn-ing method for learning small, fast, and accurate Siamesenetwork trackers. Chen et al. [31] propose a lightweight net-work based on the teacher-student knowledge distillationframework to accelerate visual trackers based on correlationfilters while maintaining tracking performance. Zhuang etal. [32] propose an ensemble learning method based onSiamese architecture, which effectively improves the accu-racy of visual tracking tasks and solves the limitations ofknowledge distillation in visual tracking tasks. Sun et al. [33]distill the pre-trained RGB modality onto the TIR modalityon unlabeled RGB-TIR datasets, utilize the two branches ofthe network to process data from different modalities to"}, {"title": "3 METHODOLOGY", "content": "In this section, we will first give an overview of ourproposed HDETrack V2. Then, we will introduce the detailsfrom the input representation, network architecture, hierar-chical knowledge distillation strategy, and test time tuningin the inference phase."}, {"title": "3.1 Overview", "content": "As shown in Fig. 2, our proposed HDETrack V2 followsthe teacher-student framework, where the teacher network"}, {"title": "3.2 Input Representation", "content": "In this work, we denote the RGB frames as $I={I_1, I_2, ..., I_N }$, where $I_i\\in\\mathbb{R}^{1280\\times720}$ represents each video frame, $i\\in[1, N]$, where $N$ is the number of video frames.We treat event stream as $E={e_1, e_2, ..., e_M }$, with $e_j$ denoting each asynchronously launched event point, $j\\in[1, M]$,and $M$ is the number of event points in the current sample.For the video frames $I$, we apply standard Siamese trackingmethods to extract the template $T_I \\in \\mathbb{R}^{128\\times128}$ and search$S_I \\in \\mathbb{R}^{256\\times256}$ as input. For the event stream $E \\in \\mathbb{R}^{1280\\times720}$,we stack/split them into event images/voxels which canfuse more conveniently with existing RGB modality. Specifically, the event images are obtained by aligning with theexposure time of the RGB modality. Event voxels are ob-tained by splitting the event stream along with the spatial(width $W$ and height $H$) and temporal dimensions $(T_t)$.The scale of each voxel grid is denoted as $(a, b, c)$, thus,we can get $\\frac{W}{a} \\times \\frac{H}{b} \\times \\frac{T_t}{c}$ voxel grids. Similarly, we canobtain the template and search regions of event data, i.e.,$T_E \\in \\mathbb{R}^{128\\times128}$ and $S_E \\in \\mathbb{R}^{256\\times256}$"}, {"title": "3.3 Network Architecture", "content": "In this paper, our proposed hierarchical knowledge dis-tillation framework contains the Multi-modal/Multi-viewTeacher Transformer and Unimodal Student Transformernetwork for event-based tracking."}, {"title": "3.3.1 Multi-modal/Multi-view Teacher Tracker", "content": "For the teacher network, we take both the RGB framesand event streams as the input to obtain more comprehen-sive information representations. If no RGB frames are avail-able (e.g., the EventVOT dataset), we input multi-view eventrepresentations such as the event images and event voxels.Specifically, using the RGB modality as an example (thesame applies to the event modality), we randomly samplea template frame and a search frame from a video to formpair of samples. These frames are partitioned into imagepatches according to the specified patch size (16 \u00d7 16) andtransformed into token embedding (64 \u00d7 768 and 256 \u00d7 768for template and search tokens) via a projection layer (con-volutional layer with kernel size 16 \u00d7 16). Subsequently,we add position encoding to label the position of each"}, {"title": "3.3.2 Unimodal Student Tracker", "content": "After the first stage of training, we can obtain arobust teacher Transformer network trained on multi-modal/multi-view data. To enable efficient visual track-ing, we employ knowledge distillation to transfer essen-tial knowledge from the teacher network to the studentnetwork, allowing us to use only a lightweight studentnetwork for effective inference independently during thetesting phase. As illustrated in Fig. 2, only event data isfed into the student Transformer for tracking. Our studentnetwork's processing follows a similar structure to theteacher network, but only event data is fed into the studentTransformer network, thus achieving a significant speedadvantage. Furthermore, thanks to the carefully designedhierarchical knowledge distillation, our student networkcan learn more useful knowledge through the guidance ofthe teacher network beyond standard ground truth-basedsupervised learning, thereby improving the accuracy oftracking."}, {"title": "3.3.3 Hierarchical Knowledge Distillation", "content": "In addition to the training losses used in OSTrack [22],i.e., focal loss $L_{focal}$, $L1$ loss $L_{L1}$, and GIoU loss $L_{GIOU}$),\nwe also introduce four additional Knowledge Distillation(KD) losses to further guide the optimization of the studentnetwork from our pre-trained teacher network. Specifically,it includes three modules for spatial distillation: similaritymatrix-based distillation, feature-based distillation, and re-sponse map-based knowledge distillation, as well as tempo-ral Fourier transform distillation.\n\u2022 Similarity Matrix based KD: It is computed by the multi-head self-attention layers incorporating abundant long-range and cross-modal relation information. In this work,we exploit the knowledge transfer from the similarity matrixlearned by the teacher Transformer to the student Trans-former. The loss function can be written as:\n$L_{simKD} = \\sum (S_i^t - S_i^s)^2$,(1)\nwhere $S_i^t \\in \\mathbb{R}^{640 \\times 640}$ denotes the similarity matrix of the $i^{th}$teacher Transformer layer, and $S_i^s \\in \\mathbb{R}^{320 \\times 320}$ denotes thesimilarity matrix of the $j^{th}$ student Transformer. To alignthe dimensions of the two similarity matrices, we performa simple repeat approach on $S_i^s$ and use it to calculatethe distance between the two matrices.\n\u2022 Feature based KD: This strategy has demonstratedits superiority in numerous works. Therefore, we furtherperform feature-based distillation between the teacher andstudent networks. Specifically, we utilize the Mean SquareError (MSE) loss to transfer the knowledge. The loss functioncan be represented as:\n$L_{featKD} = \\frac{1}{N} \\sum_{i=1}^N (F_i^t - F_i^s)^2$,(2)"}, {"title": "4 EVENTVOT BENCHMARK DATASET", "content": "In this paper, we propose a large-scale high-definitionevent-based visual tracking dataset. Here, we introducethe protocols for data collection and annotation, statisticalanalysis, and benchmarked visual trackers in the followingsubsections, respectively."}, {"title": "4.1 Criteria for Collection and Annotation", "content": "To construct a dataset with a diverse range of targetcategories, as shown in Fig. 4, capable of reflecting thedistinct features and advantages of event tracking, thispaper primarily considers the following aspects during datacollection. 1). Diversity of target categories: Many commonand meaningful target objects are considered, includingUAVs, pedestrians, vehicles, ball sports, etc. 2). Diversityof data collection environments: The videos in our datasetare recorded in day and night time, and involved venueinformation includes playgrounds, indoor sports arenas,main streets and roads, cafeteria, dormitory, etc. 3). Recordedspecifically for event camera characteristics: Different motionspeeds, such as high-speed, low-speed, momentary stillness,and varying light intensity, etc. 14 challenging factors arereflected by our EventVOT dataset. 4). High-definition, wide-field event signals: The videos are collected using a PropheseeEVK4\u2013HD event camera, which outputs event stream with1280 \u00d7 720. This high-definition event camera excels insupporting pure event-based object tracking, thereby avoid-ing the influences of the RGB cameras and showcasing itsfeatures and advantages in various aspects such as high-speed, low-light, low-latency, and low-power consumption.5). Data annotation quality: All data samples are annotated bya professional data annotation company and has undergonemultiple rounds of quality checks and iterations to ensurethe accuracy of the annotations. For each event stream, wefirst stack into a fixed number (499 in our case) of eventimages for annotation. 6). Data size: Collect a sufficientlylarge dataset to train and evaluate robust event-based track-ers. A comparison between the newly proposed dataset andexisting tracking datasets is summarized in Table 1."}, {"title": "4.2 Statistical Analysis", "content": "In the EventVOT dataset, we have defined 14 chal-lenging factors, involving 19 classes of target objects. Thenumber of videos corresponding to these attributes andcategories is visualized in Fig. 5 (a, c). We can find thatBC, BOM, and SV are top-3 major challenges which demon-strates that our dataset is relatively challenging. The balancebetween different categories is also well-maintained, withthe number of samples roughly distributed between 50 to60. Among them, UAVs (Unmanned Aerial Vehicles) area special category of targets, with a total count of 96. Thedistribution of the center points of the annotated boundingboxes is visualized in Fig. 5 (b). Our EventVOT dataset issplit into training/validation/testing subsets which contain841, 18, and 282 videos, respectively."}, {"title": "4.3 Benchmarked Trackers", "content": "To build a comprehensive benchmark on the EventVOTdataset for event-based visual tracking, we consider morethan 20 visual trackers: 1). Siamese or Discriminate trackers: DiMP50 [4], PrDiMP [58], KYS [59], ATOM [5],2). Transformer trackers: OSTrack [22], TransT [60], Sim-Track [61], AiATrack [62], STARK [63], ToMP50 [64], Mix-Former [65], TrDiMP [66], ROMTrack [67], CiteTracker [68],ARTrack [69], AQATrack [53], ODTrack [70], EVPTrack [71],ARTrackV2 [72], LoRAT [73]. Note that, we re-train thesetrackers using their default settings on the training datasetfor a fair comparison, instead of directly testing on thetesting subset. The benchmark can be found in Table 3. Webelieve that these retrained tracking algorithms can play acrucial role in future comparisons of their performance."}, {"title": "5 EXPERIMENTS", "content": "5.1 Dataset and Evaluation Metric\nIn addition to our newly proposed EventVOT dataset,we also compare our tracker with other SOTA visual track-ers on existing event-based tracking datasets, includingFE240hz [18], VisEvent [15], and FELT [24] dataset. A brief"}, {"title": "5.2 Implementation Details", "content": "The training of our tracker can be divided into twostages. We first pre-train the teacher Transformer with multi-modal/multi-view inputs for 50 epochs. The learning rate is0.0001, weight decay is 0.0001, and batch size is 32. Sub-sequently, we adopt a hierarchical knowledge distillationstrategy for training the student Transformer network. Thelearning rate, weight decay, and batch size are set to 0.0004,0.0001, and 32, respectively. The AdamW optimizer [74]is selected for the training of our tracking network. Inthe testing phase, we adopt the TTT strategy to enhancethe model's generalization on the test set. The StochasticGradient Descent (SGD) is selected as the optimizer. Thelearning rate, weight decay, and epoch are set to 0.01, 0.1,and 5, respectively. Our code is implemented using Pythonbased on PyTorch [75] framework and the experiments areconducted on a server with CPU Intel(R) Xeon(R) Gold"}, {"title": "5.3 Comparison on Public Benchmarks", "content": "\u2022 Results on EventVOT Dataset. As shown in Table 3 andFig. 7, we further incorporate several new SOTA trackingmethods for comparison, building on our previous bench-mark. To ensure a fair comparison, we re-train and evaluatethese methods on the EventVOT dataset. Firstly, we cansee that our newly designed HDETrack V2 has significantlysurpassed our baseline method OSTrack, achieving 59, 63.8,and 74.9 on the SR, PR, and NPR, respectively. Further-more, compared to HDETrack, the upgraded HDETrack V2has also been further improved in various metrics, with+1.2, +1.6, and +1.4 improvements on SR, PR, and NPR,which fully validates the effectiveness of our proposedmethod for event-based tracking. At the same time, ourmethod also outperforms all SOTA trackers, including theSiamese trackers and Transformer trackers (e.g., ODTrack,EVPTrack, ARTrackV2, AQATrack) on NPR, while the SRranks only behind the optimal AQATrack and PR also ranking second. These experimental results highlight the effectiveness of our series of improvements to the HDETrack,including the incorporation of Temporal Fourier Transform,Test-Time Tuning, and Adaptive Search Region strategies.Similar conclusions can also be drawn from the experimental results on FE240hz (Table 4), VisEvent (Table 5), and FELT(Table 6).\n\u2022 Results on FE240hz Dataset. As shown in Table 4, ourbaseline OSTrack achieves 60.0/89.7 on the SR/PR metric,meanwhile, HDETrack achieves 62.3/92.6 which is signif-icantly better than the baseline method. After further im-provement, our method demonstrated great advantages andreached new SOTA levels on SR and PR. Our tracker alsobeats other SOTA trackers including event-based trackers(e.g., STNet and EFE), and Transformer trackers (like TransT,STARK) by a large margin. These results fully validatethe effectiveness of our proposed method for event-basedtracking."}, {"title": "5.4 Ablation Study", "content": "\u2022 Analysis on Hierarchical Knowledge Distillation. Fol-lowing HDETrack, for the EventVOT dataset, we stack theevent stream into event images and event voxels and ap-ply hierarchical knowledge distillation based on multi-viewsettings. As shown in Table 7, the base denotes the trackertrained solely with three tracking loss functions, similar tothe approach used by OSTrack. This base model achieves55.4/60.4/71.1 on the EventVOT datasets, respectively. The"}, {"title": "5.6 Limitation Analysis", "content": "Although our work has achieved decent accuracy, thereare still some areas that can be further improved: Firstly, theconversion from the Event stream to a fixed-frame video,which adapts well to existing tracking frameworks for eval-uation, maybe a worthwhile research direction for Eventtracking in terms of dense video annotation and high-frame-rate tracking. Secondly, various challenging factors werenot utilized during the training phase. The model is unaware ofthe nature of the challenges it encounters, making it difficultto handle these difficulties effectively. Merely relying onfeature engineering cannot adequately address these issuesin the short term. We consider introducing large languagemodels in our future work to enable the model to un-derstand various challenging factors and design automaticstrategies to cope with them, fundamentally solving theseproblems and achieving better tracking results."}, {"title": "6 CONCLUSIONS AND FUTURE WORKS", "content": "In this paper, we present HDETrack V2, a hierarchicalknowledge distillation framework for event-based track-ing. It facilitates the transfer of multi-modal/multi-viewknowledge to an unimodal tracker through carefully de-signed strategies, including similarity-based, feature-based,and response map-based knowledge distillation. Comparedwith our previous version, the Temporal Fourier Transformis used to establish the temporal relationships betweenvideo frames to enhance the knowledge distillation pro-cess. Furthermore, attributing to the Test Time Tuning andAdaptive Search Region strategies, the model can performbetter during the inference phase. To bridge the data gap,a large-scale high-resolution event-based tracking datsethas been proposed, termed EventVOT. Extensive evalua-tions on multiple tracking benchmarks demonstrate that ourtracker obtains a notable performance over other prevailingtrackers. In our future works, we will consider developinga lightweight network capable of handling high-resolutionand more challenging tracking scenarios, further broaden-ing the applicability of event-based tracking algorithms."}]}