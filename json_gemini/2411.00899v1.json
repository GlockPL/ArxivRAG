{"title": "Certified Robustness for Deep Equilibrium Models via Serialized Random Smoothing", "authors": ["Weizhi Gao", "Zhichao Hou", "Han Xu", "Xiaorui Liu"], "abstract": "Implicit models such as Deep Equilibrium Models (DEQs) have emerged as promising alternative approaches for building deep neural networks. Their certified robustness has gained increasing research attention due to security concerns. Existing certified defenses for DEQs employing deterministic certification methods such as interval bound propagation and Lipschitz-bounds can not certify on large-scale datasets. Besides, they are also restricted to specific forms of DEQs. In this paper, we provide the first randomized smoothing certified defense for DEQs to solve these limitations. Our study reveals that simply applying randomized smoothing to certify DEQs provides certified robustness generalized to large-scale datasets but incurs extremely expensive computation costs. To reduce computational redundancy, we propose a novel Serialized Randomized Smoothing (SRS) approach that leverages historical information. Additionally, we derive a new certified radius estimation for SRS to theoretically ensure the correctness of our algorithm. Extensive experiments and ablation studies on image recognition demonstrate that our algorithm can significantly accelerate the certification of DEQs by up to 7x almost without sacrificing the certified accuracy. Our code is available at https://github.com/WeizhiGao/Serialized-Randomized-Smoothing.", "sections": [{"title": "1 Introduction", "content": "The recent development of implicit layers provides an alternative and promising perspective for neural network design (Amos & Kolter, 2017; Chen et al., 2018; Agrawal et al., 2019; Bai et al., 2019, 2020; El Ghaoui et al., 2021). Different from traditional deep neural networks (DNNs) that build standard explicit deep learning layers, implicit layers define the output as the solution to certain closed-form functions of the input. These implicit layers can represent infinitely deep neural networks using only one single layer that is defined implicitly. The unique definition endows implicit models with the capability to model continuous physical systems, a task that traditional DNNs cannot accomplish (Chen et al., 2018). Additionally, the implicit function theorem enhances memory efficiency by eliminating the need to store intermediate states during forward propagation (Chen et al., 2018; Bai et al., 2019). Furthermore, implicit models offer a valuable accuracy-efficiency trade-off, making them adaptable to varying application requirements (Chen et al., 2018). These advantages underscore the significant research value of implicit models.\nDeep equilibrium models (DEQs) are one promising class of implicit models that construct the output as the solution to input-dependent fixed-point problems (Bai et al., 2019). With a fixed-point solver, DEQs can be seen as infinite-depth and weight-tied neural networks. The modern DEQ-based architectures have shown comparable or even surpassing performance compared with traditional explicit models (Bai et al., 2019, 2020; Gu et al., 2020; Chen et al., 2022). Due to the superior"}, {"title": "2 Background", "content": "In this section, we provide necessary technical background for DEQs and Randomized Smoothing."}, {"title": "2.1 Deep Equilibrium Models", "content": "Implicit formulation. Traditional feedforward neural networks usually construct forward feature transformations using fixed-size computation graphs and explicit functions $z^{l+1} = f_{\\theta_l}(z^l)$, where $z^l$ and $z^{l+1}$ are the input and output of layer $f_{\\theta_l}(\\cdot)$ with parameter $\\theta_l$. DEQs, as an emerging class of implicit neural networks, define their output as the fixed point solutions of nonlinear equations:\n$z^* = f_{\\theta}(z^*, x),$\nwhere $z^*$ is the output representation of implicit neural networks and x is the input data. Therefore, the computation of DEQs for each input data point x requires solving a fixed-point problem to obtain the representation $z^*$.\nFixed-point solvers. Multiple fixed-point solvers have been adopted for DEQs, including the naive solver, Anderson solver, and Broyden solver (Geng & Kolter, 2023). The naive solver directly repeats the fixed-point iteration until it converges:\n$z^{l+1} = f_{\\theta}(z^l, x).$\nMore details about these solvers can be referred to in the work (Cohen et al., 2019). In this paper, we denote all solvers as follows:\n$z = Solver(f, x, z^0),$\nwhere $z^0$ is the initial feature state that is taken as 0 in DEQs. All the solvers end the iteration if the estimation error $f(z) - z$ of the fixed point reaches a given tolerance error or a maximum iteration threshold L."}, {"title": "2.2 Randomized Smoothing", "content": "Randomized smoothing (Cohen et al., 2019) is a certified defense technique that guarantees $l_2$-norm certified robustness. Given an arbitrary base classifier f(\u00b7), we construct a smoothed classifier g(\u00b7):\n$g(x) = arg\\ max_{c \\in \\mathcal{Y}} P(f(x + \\epsilon) = c),$\n$\\epsilon \\sim \\mathcal{N}(0, \\sigma^2I),$\nwhere $\\mathcal{Y}$ is the label space, and $\\sigma^2$ is the variance of Gaussian distribution. Intuitively, the smoothed classifier outputs the most probable class over a Gaussian distribution. If we denote $p_A$ and $p_B$ as the probabilities of the most probable class $c_A(x)$ and second probable class $c_B(x)$, Neyman-Pearson theorem (Neyman & Pearson, 1933) provides a $l_2$-norm certified radius R for the smoothed classifier g:\n$g(x + \\delta) = c_A(x)$ for all $|\\delta||_2 < R,$\n$R = \\frac{\\sigma}{2}(\\Phi^{-1}(p_A) - \\Phi^{-1}(p_B)).$\nHere $\\Phi(x)$ is the inverse of the standard Gaussian cumulative distribution function, and $p_A, p_B \\in [0, 1]$ satisfy:\nP(f(x + \\epsilon) = c_A(x)) \\geq p_A \\geq p_B > \\max_{c \\neq c_A(x)} P(f(x + \\epsilon) = c).\nIn practice, $p_A$ and $p_B$ are estimated using the Monte Carlo method. It is crucial to maintain the independence of each prediction in the simulation to ensure the correctness of randomized smoothing."}, {"title": "3 Serialized Randomized Smoothing", "content": "In this section, we begin by revealing the computation challenges associated with certifying DEQs using Randomized Smoothing. Subsequently, we propose Serialized Random Smoothing (SRS), a novel approach to remarkedly enhance the efficiency of DEQ certification. However, directly applying the estimated radius of the standard randomized smoothing to SRS breaks the theoretical guarantee of certified robustness. To address this challenge, we develop the correlation-eliminated certification technique to estimate the radius in SRS."}, {"title": "3.1 Computation Challenges", "content": "According to Eq. (7), we need to estimate the lower bound probability $p_A$ and upper bound probability $p_B$. The appliance of Monte Carlo sampling rescues as follows:\n$P(f(x + \\epsilon) = c) \\sim \\frac{1}{N}\\sum_{i=1}^{N} 1{f(x + \\epsilon_i) = c},$\nwhere 1{\u00b7} is the indicator function, and $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2I)$ is the i-th random perturbation sampled from Gaussian distribution. However, it introduces computation challenges due to the large sampling number N. Our empirical study (Section 4.2) indicates that applying randomized smoothing to certify MDEQ (Bai et al., 2020) on one 32 \u00d7 32 image in CIFAR-10 takes 12.89 seconds, and 88.33 seconds for one 256 \u00d7 256 image in ImageNet. The computation challenges raise tremendous limitations in real-world applications.\nWe provide an analysis for the slow randomized smoothing certification of DEQs. First, each forward iteration of DEQs can be very expensive. This is because DEQs are typically weight-tied neural networks so one layer f(z, x) of DEQs needs to be complex to maintain the expressiveness. Second, the fixed-point solver needs many iterations for convergence. To maintain the best performance, the solvers usually set a small value for the error tolerance (e.g., 0.001). Although second-order solvers like Broyden's method have faster convergence, their computation cost per iteration is higher. Third, the Monte Carlo estimation in randomized smoothing further exacerbates the expensive inference of DEQs, leading to significant computation challenges, as shown in Figure 1a."}, {"title": "3.2 Serialized Randomized Smoothing", "content": "As introduced in Section 3.1, the Monte Carlo method is employed in randomized smoothing to estimate the prediction probability, typically necessitating over 10, 000 times inference computation for certifying one data point. Despite the independent sampling of Gaussian noises, these noises {$\\epsilon_i$} are added to the same certified data point x to form noisy data samples {x + $\\epsilon_i$}. Notably, these samples are numerically and visually similar to each other as can be seen in Figure 1. Moreover, in randomized smoothing, the base classifier is trained on Gaussian-augmented data to be resistant to noises added to data points, yielding robust features and base classifiers. Therefore, the feature representation of these noisy samples computed in the forward computation of DEQs shares significant similarities, resulting in a substantial computation redundancy in the fixed-point solvers. The computation redundancy contributes to the inefficient DEQ certification with randomized smoothing as a primary factor. Consider a DEQ with 50 layers as an illustrative example. In the Monte Carlo estimation with N = 10,000, it requires the forward computation of 50 \u00d7 10,000 = 500,000 layers. However, if we can estimate the intermediate representation at the 45th layer, the required forward iterations reduce to 5 \u00d7 10,000 = 50, 000 layers, bringing a 10\u00d7 acceleration."}, {"title": "3.3 Correlation-Eliminated Certification", "content": "The primary challenge is to confirm how much the initialization of the fixed-point solver influences the final predictions. For different data samples x + $\\epsilon_i$ and initialization $z_i^0$, the cases can be different depending on the complex loss landscape of the fixed-point problem and the strength of the solver. Nonetheless, comparing all predictions from SRS with standard predictions, which necessitate numerous inference steps, is impractical. Such a comparison contradicts the fundamental requirement for efficiency in this process.\nTo maintain the theoretical guarantee of randomized smoothing, we propose correlation-elimination certification to obtain a conservative estimate of the certified radius. The core idea involves discarding those samples that are misclassified as the most probable class, $c_A(x)$, during the Monte Carlo process. Let $p_m$ represent the probability that a sample is predicted as class $c_A(x)$ using SRS but falls into other classes with the standard DEQ. We can drop the misclassified samples as follows:\n$N_A^E = N_A-p_mN_A,$\nwhere $N_A$ represents the count of samples predicted as class $c_A(x)$ and $N_A^E$ refers to the subset of these effective samples that are predicted as class $c_A(x)$. Utilizing $N_A^E$ and $N$, we are ultimately able to estimate the certified radius. For the reason that $p_m$ is intractable, we employ an additional hypothesis test using a limited number of samples to approximate its upper bound. During the Monte Carlo sampling of SRS, we randomly select K of samples (a small number compared to N) along with their corresponding predictions, which are then stored as $X_m$ and $Y_m$, respectively. After the Monte Carlo sampling, these samples, $X_m$, are subjected to predictions using the standard DEQ to yield the labels $Y_g$, which serve as the ground truth. Mathematically, we estimate $p_m$ as follows:\n$N_1 = \\sum_{i=1}^{K} 1{Y_m = Y_g \\ and\\ Y_g = c_A(x)},$\n$N_2 = \\sum_{i=1}^{K} 1{Y_m = c_A(x)},$\n$\\hat{p_m} = 1 - LowerConfBound(N_1, N_2, 1 - \\tilde{\\alpha}),$\nwhere $\\tilde{\\alpha} = \\alpha/2$ is for keeping the confidence level of the two-stage hypothesis test. Besides, $LowerConfBound(k, n, 1 - a)$ returns a one-sided (1 - a) lower confidence interval for the Binomial parameter p given that k ~ Binomial(n,p). In other words, it returns some number $\\hat{p}$ for which $p \\leq \\hat{p}$ with probability at least 1 - a over the sampling of k ~ Binomial(n, p). Intuitively, a smaller"}, {"title": "4 Experiments", "content": "In this section, we conduct comprehensive experiments in the image classification tasks to demonstrate the effectiveness of the proposed SRS-MDEQ. First, we introduce the experimental settings in detail. Then we present certification on CIFAR-10 and ImageNet datasets to demonstrate the certified accuracy and efficiency. Finally, we provide comprehensive ablation studies to understand its effectiveness."}, {"title": "4.1 Experiment Settings", "content": "Datasets. We use two classical datasets in image recognition, CIFAR-10 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015), to evaluate the certified robustness. It is crucial to emphasize that this is the first attempt to certify the robustness of DEQs on such a large-scale dataset.\nDEQ Architectures and Solvers. We select MDEQ with Jacobian regularization (Bai et al., 2020), a type of DEQs specially designed for image recognition, to serve as the base classifier in randomized smoothing. Specifically, we choose MDEQ-SMALL and MDEQ-LARGE for CIFAR-10, and MDEQ-SMALL for ImageNet. To obtain a satisfactory level of certified accuracy, all the base classifiers are trained on the Gaussian augmented noise data with mean 0 and variance $\\sigma^2$. Detailed information regarding the model configuration and training strategy is available in Appendix B.\nWe closely follow the solver setting in MDEQ (Bai et al., 2020). For the standard MDEQ on CIFAR-10, we use the Anderson solver with the step of {1, 5, 30}. For the standard MDEQ on ImageNet, we use the Broyden solver with the step of {1, 5, 14}. We apply Anderson and Naive solvers on CIFAR-10 and Broyden solver on ImageNet for the proposed SRS-MDEQ with the step of {1,3}. We adopt a warm-up technique, where we use multi-steps to solve the fixed-point problem for the first batch in Algorithm 1. The warm-up steps for our SRS-MDEQ are set as 30 and 14 steps for CIFAR-10 and ImageNet, respectively. The details of warm-up strategy are shown in Appendix K. For notation simplicity, we use a number after the algorithm name to represent the number of layers of the model, and we use \u201cN\u201d, \u201cA\u201d, and \u201cB\u201d to denote the Naive, Anderson, and Broyden solvers. For instance, SRS-MDEQ-3A denotes SRS-MDEQ method with 3 steps of Anderson iterations.\nRandomized smoothing. Following the setting in randomized smoothing (Cohen et al., 2019), we use four noise levels to construct smoothed classifiers: {0.12, 0.25, 0.50, 1.00}. We report the approximate certified accuracy as in (Cohen et al., 2019), which is defined as the fraction of the test data that is both correctly classified and certified with a $l_2$-norm certified radius exceeding a radius threshold r. In our experiments, we set the failure rate as $a = 0.001$ and the sampling number as N = 10,000 in the Monte Carlo method, unless specified otherwise. All the experiments are conducted on one A100 GPU.\nBaselines. We majorly use standard Randomized Smoothing for MDEQs as our baseline for comparison. It is also important to compare our method with state-of-the-art certified defenses. Note that the"}, {"title": "4.3 Ablation Study", "content": "In this section, we conduct comprehensive ablation studies to investigate the instance-level consistency and the effectiveness of our correlation-eliminated certification. We also provide more ablation studies on the hyperparameter of MDEQ solvers in Appdenix F and G. Finally, we show the empirical robustness performance of our method in Appendix L.\nInstance-level consistency. Besides providing global measurements for the SRS-MDEQ with the certified accuracy in Section 4.2, we study how closely SRS-MDEQ matches accurate MDEQ at the instance level based on our proposed Relative Radius Difference (RRD). RRD compares the relative difference between the certified radius of SRS-MDEQ and the accurate MDEQ for each instance $x_i$:\n$RRD(x_i) = \\frac{|| r - r_i||}{r},$ \nwhere $r_i$ and $r$ represent the certified radius of $x_i$ with MDEQ-30A and SRS-MDEQ, respectively. We compute RRD over the instances with a positive certified radius to avoid the singular value.\nWe present the histograms of RRD in Figure 2. As shown in Section 4.3, only with one layer, the certified radius achieved by SRS-MDEQ-1A is quite close to the accurate MDEQ since these relative differences are mostly small and close to 0, and it significantly outperforms the standard MDEQ-1A with one layer. Moreover, with 3 layers as shown in Section 4.3, the RRD values become even more concentrated around 0, which shows a very consistent certified radius with the accurate MDEQ. The instance level measurement for other settings of MDEQs are shown in Appendix H.\nPower of correlation-eliminated certification. The correctness of our method is based on estimating the upperbound of $p_m$. In this ablation study, we investigate the effectiveness in the following two aspects. We provide additional analysis for this technique in Appendix I."}, {"title": "5 Related Work", "content": "Recently, there have been many works on deep implicit models that define the output by implicit functions (Amos & Kolter, 2017; Chen et al., 2018; Bai et al., 2019; Agrawal et al., 2019; El Ghaoui et al., 2021; Bai et al., 2020; Winston & Kolter, 2020). Among these, deep equilibrium model defines the implicit layer by solving a fixed-point problem (Bai et al., 2019, 2020). There are many fundamental works investigating the existence and the convergence of the fixed point (Winston & Kolter, 2020; Revay et al., 2020; Bai et al., 2021b; Ling et al., 2023). With many advantages, DEQs achieve superior performance in many tasks, such as image recognition (Bai et al., 2020), image generation (Pokle et al., 2022), graph modeling (Gu et al., 2020; Chen et al., 2022), language modeling (Bai et al., 2019), and solving complex equations (Marwah et al., 2023). Though DEQs catch up with the performance of DNNs, the computation inefficiency borders the deployment of deep implicit models in practice (Chen et al., 2018; Dupont et al., 2019; Bai et al., 2019). Related works focus on reusing information from diffusion models and optical flows, demonstrating the effectiveness of reducing computational redundancy of DEQs (Bai & Melas-Kyriazi, 2024; Bai et al., 2022). However, this paper focuses on the certified robustness of DEQs and provides a theoretical analysis of our proposed method."}, {"title": "5.2 Certified Robustness", "content": "Empirical defenses like adversarial training are well-known in deep learning (Goodfellow et al., 2014). Some existing works improve the robustness of DEQs by applying adversarial training (Gurumurthy et al., 2021; Yang et al., 2023, 2022). Different from the empirical defense like adversarial training, certified defenses theoretically guarantee the predictions in a small ball maintain as a constant (Wong & Kolter, 2018; Raghunathan et al., 2018; Gowal et al., 2018; Cohen et al., 2019). The most common way to certify robustness is to define a convex program, which lower bounds the worst-case perturbed output of the network (Raghunathan et al., 2018; Wong & Kolter, 2018). The increasing computation complexity in high-dimension optimization hinders the generalization of these methods. Interval bound propagation (IBP) is another certification method for neural networks, which computes an upper bound of the class margin through forward propagation (Gowal et al., 2018). However, the layer-by-layer computation mode brings a potentially loose certified radius. Recently, randomized smoothing has drawn much attention due to its flexibility (Cohen et al., 2019). Randomized smoothing certifies $l_2$-norm robustness for arbitrary classifiers by constructing a smoothed version of the classifier. There are some existing works certifying robustness for DEQs. Most of them adapt IBP to DEQs by constructing a joint fixed-point problem (Wei & Kolter, 2021; Li et al., 2022). Others design specific forms of DEQs to control the Lipschitz constant of the models (Havens et al., 2023; Jafarpour et al., 2021). Yet, no existing work explores randomized smoothing for certifiable DEQs."}, {"title": "6 Conclusion", "content": "In this work, we provide the first exploration of randomized smoothing certification for DEQs. Our study shows that randomized smoothing for DEQs can certify more generalized architectures and be applied to large-scale datasets but it incurs significant computation costs. We delve into the computation bottleneck of this certified defense and point out the new insight of computation redundancy. We further propose a novel Serialized Randomized Smoothing approach to significantly reduce the computation cost by leveraging the computation redundancy. Finally, we propose a new estimation for the certified radius for our SRS. Our extensive experiments demonstrate that our algorithm significantly accelerates the randomized smoothing certification by up to 7\u00d7 almost without sacrificing the certified accuracy. Our discoveries and algorithm provide valuable insight and a solid step toward efficient robustness certification of DEQs. Our work significantly improves the security of artificial intelligence, especially applicable in sensitive domains, enhancing the appliance of the models and maintaining the integrity of AI-driven decisions. Though our paper speeds up the certification of DEQs with randomized smoothing, it cannot be directly applied to other architecture. We regard the speedup for the general method as our future research."}, {"title": "A Proofs of Theorem 3.1", "content": "Theorem. With probability at least 1 a over Algorithm 1. If Algorithm 1 returns a class \u0109\u0104 with a radius R, then the smoothed classifier g predicts \u0109a within radius R around x: g(x + d) = g(x) for all ||8|| < R.\nProof. From the contract of the hypothesis test, we know that with the probability of at least 1 \u2013 \u1fb6 over all the samplings \u20ac1, \u20ac2, , \u20acN, we have pm > P(y = \u0109a and y \u2260 \u0109a) = pm, where y and y represent the predictions of x + \u20ac\u00bf given by SRS and the standard DEQ, respectively. Denote the number of samplings as follows:\nNA = NA-PmNA,\nNA = NA-PMNA,\nNA where NA is the fact number that the predictions of the standard DEQs are class \u0109a, while \u00d1E is the number we estimate. In this way, LowerConfBound(\u00d1A, N, \u00e3) < LowerConfBound(N, N,\u1fb6). Suppose the standard randomized smoothing returns R with NA and N, we conclude that R < R with the probability of at least 1-\u1fb6. With Proposition 2 in the standard randomized smoothing (Cohen et al., 2019), g(x + d) = g(x) for all ||8|| < R for all ||8|| < R. Denote the event that the radius of SRS is smaller than the radius of RS as A and the event that the radius of RS can certify the data points B. We can conclude that P(A) = P(B) = \u00e3 following the hypothesis tests. The final probability of successfully certifying the data point is:\nP(AUB) = P(B) \u2013 P(\u0100\u222a B) = 1 \u2212 P(B) \u2013 P(\u0100 \u222a B) \u2265 1 \u2013 P(B) \u2013 P(\u0100) = 1 \u2212 2\u00e3 (20)\nwhere P(A) is the probability that A does not happen. By setting \u1fb6 = a/2, we complete the proof."}, {"title": "B.1 Model Architecture", "content": "Multi-resolution deep equilibrium models (MDEQ) are a new class of implicit networks that are suited to large-scale and highly hierarchical pattern recognition domains. They are inspired by the modern computer vision deep neural networks, which leverage multi-resolution techniques to learn features. These simultaneously learned multi-resolution features allow us to train a single model on a diverse set of tasks and loss functions, such as using a single MDEQ to perform both image classification and semantic segmentation. MDEQs are able to match or exceed the performance of recent competitive computer vision models, achieving high accuracy in sequence modeling."}, {"title": "B.2 Training Setting", "content": "We report the training hyperparameters in Table 8. Following work (Bai et al., 2021b), we use Jacobian regularization to ensure stability during the training process. Moreover, to prevent overfitting, we employ data augmentation techniques such as random cropping and horizontal flipping, which are commonly utilized in various computer vision tasks.\nGaussian Augmentation: As mentioned in Section 3.2, randomized smoothing requires the base classifier to be robust against Gaussian noise. Therefore, we train the MDEQs with Gaussian augmentation. Following the standard randomized smoothing (Cohen et al., 2019), we augment the original data with noise sampled from N(0, \u03c3\u00b2I), where o denotes the noise level in the smoothed classifier. Intuitively, the training scheme forces the base classifier to be robust to the Gaussian noise, which is used in randomized smoothing. Formally, under the cross-entropy loss, the objective is to maximize the following:\nn\nE log\ni=1\nexp fc (xi + 6)\n\u03a3cey exp fc(xi + e)'\nwhere (xi, Ci) is one clean data point with its ground-truth label. According to Jensen's inequality, Equation (21) is the lower bound of the following one:\nn\nlog E\ni=1\nexp fci (xi + \u20ac)\n\u03a3cey exp fc(xi + \u20ac)\nThe equation within the expectation represents the softmax output of the logits produced by the base classifier f. This can be seen as a soft version of the argmax function. Consequently, the expectation approximates the probability of class c\u2081 when Gaussian augmentation is applied. By doing so, we aim to maximize the likelihood of the smoothed classifier g(x):\nn\n\u03a3\ni=1\nlog P(f(xi + \u20ac) = ci)."}]}