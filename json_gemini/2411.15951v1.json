{"title": "Partial Identifiability and Misspecification in Inverse Reinforcement Learning", "authors": ["Joar Skalse", "Alessandro Abate"], "abstract": "The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function R from a policy \u03c0. This problem is difficult, for several reasons. First of all, there are typically multiple reward functions which are compatible with a given policy; this means that the reward function is only partially identifiable, and that IRL contains a certain fundamental degree of ambiguity. Secondly, in order to infer R from \u03c0, an IRL algorithm must have a behavioural model of how relates to R. However, the true relationship between human preferences and human behaviour is very complex, and practically impossible to fully capture with a simple model. This means that the behavioural model in practice will be misspecified, which raises the worry that it might lead to unsound inferences if applied to real-world data. In this paper, we provide a comprehensive mathematical analysis of partial identifiability and misspecification in IRL. Specifically, we fully characterise and quantify the ambiguity of the reward function for all of the behavioural models that are most common in the current IRL literature. We also provide necessary and sufficient conditions that describe precisely how the observed demonstrator policy may differ from each of the standard behavioural models before that model leads to faulty inferences about the reward function R. In addition to this, we introduce a cohesive framework for reasoning about partial identifiability and misspecification in IRL, together with several formal tools that can be used to easily derive the partial identifiability and misspecification robustness of new IRL models, or analyse other kinds of reward learning algorithms.", "sections": [{"title": "1. Introduction", "content": "In this section we provide the background and context for our work, an overview of major related work from the existing literature, and an overview of our contributions and the structure of this article."}, {"title": "1.1. Background and Context", "content": "Inverse Reinforcement Learning (IRL) is an area of machine learning that is concerned with the problem of inferring what objective an agent is pursuing based on the actions which that agent takes within some environment (Ng and Russell, 2000). IRL can be related to the notion of revealed preferences in psychology and economics, since it aims to infer preferences from behaviour (Rothkopf and Dimitrakakis, 2011). There are many possible applications of IRL. For example, it has been used in natural science contexts, as a tool for understanding animal behaviour (Yamaguchi et al., 2018). It can also be used in various engineering contexts; many important tasks can be represented as sequential decision-making problems, where the goal is to maximise a given reward function over several steps (Sutton and Barto, 2018). However, for many complex tasks it can be very challenging to manually specify a reward function that robustly incentivises the intended behaviour (see e.g. Clark and Amodei, 2016; Paulus et al., 2018; Ibarz et al., 2018a; Manheim and Garrabrant, 2019; Krakovna et al., 2020b; Knox et al., 2023; Skalse et al., 2022; Pang et al., 2022; Karwowski et al., 2024). In those contexts, IRL can be employed to automatically learn a good reward function, based on demonstrations of correct behaviour (e.g. Abbeel et al., 2010; Singh et al., 2019). IRL can also be used as a tool for imitation learning, where the goal is to use machine learning to clone the behaviour of an agent. In these cases, IRL can improve metrics such as out-of-distribution robustness (e.g. Hussein et al., 2017). Overall, IRL relates to many fundamental questions about goal-directed behaviour and agent-based modelling.\nIt is important to note that the properties which we desire an IRL method to have will depend on the context in which that IRL method will be applied. For example, when IRL is used as a tool for imitation learning, it is not fundamentally important that the inferred preferences actually correspond to the true intentions of the demonstrator, as long as they help the imitation learning process. However, when IRL is used to understand the preferences and motivations of an agent (as in e.g. Hadfield-Menell et al., 2016, etc), then it is crucial that the inferred preferences actually capture the true intentions"}, {"title": "1.2. Related Work", "content": "This paper is based on a number of earlier conference papers, namely Skalse et al. (2023); Skalse and Abate (2023a); Skalse et al. (2024); Skalse and Abate (2024). Specifically, the results in Section 4 are based on the work in Skalse and Abate (2023a) and Skalse et al. (2024); the results in Section 5 are grounded in work from Skalse et al. (2023); the results in Section 6 are in large part based on those in Skalse and Abate (2023a); and the results in Section 7 depend on those in Skalse and Abate (2024). However, this paper also contains a large number of novel results that cannot be found in any earlier work, especially in Section 6.2, Appendix A, and part of Section 5. In addition to these new results, this paper also contributes by presenting all results with a mature and cohesive narrative and with unified terminology, which will help with making the results more accessible.\nThe issue of partial identifiability in IRL is well-known, and has been studied in a number of previous works. Indeed, the first paper to formally introduce the IRL problem (Ng and Russell, 2000) acknowledges the issue of partial identifiability, and characterises the ambiguity of the reward function under the assumption that the observed policy is optimal and the assumption that the reward of a transition (s, a, s') only depends on the state s. This work is extended by Dvijotham and Todorov (2010), who study partial identifiability in IRL for a particular type of environment called linearly-solvable Markov decision processes (LMDPs). Partial identifiability in IRL is also studied by Cao et al. (2021). In this paper, it is assumed that the observed policy maximises causal entropy (c.f. Section 2), and that the reward of a transition (s, a, s') only depends on the state s and action a (but not the subsequent state s'). Cao et al. (2021) also show that the ambiguity of the reward function in this setting can be reduced by combining information from multiple environments. Also relevant is Metelli et al. (2023), who generalise the results of Cao et al. (2021) by also considering environments with constraints, as well as other types of regularisation. They also provide an analysis of the sample complexity of the IRL problem in this setting.\nWe extend this previous work on partial identifiability in IRL by providing a more complete analysis, and by integrating our analysis into the study of misspecification robustness. In particular, our analysis explicitly considers three types of policies \u2014 optimal policies, maximal causal entropy policies, and Boltzmann-rational policies. Of these, only the first two have been considered by previous works. Moreover, unlike Ng and Russell (2000) and Cao et al. (2021), we allow the reward of a transition (s, a, s') to depend on each of s, a, and s', and show that this reveals important additional structure that is not captured by the analysis of Ng and Russell (2000) or Cao et al. (2021). In addition to this, we provide a general, unified framework for"}, {"title": "1.3. Contributions and Structure of This Article", "content": "This paper makes several core contributions. First of all, in Section 3, we introduce a framework for reasoning about partial identifiability and misspecification in IRL. This includes a number of formal definitions that describe what it means for an application to tolerate the ambiguity of a reward learning method, and what it means for a behavioural model to be robust to a given form of misspecification, as well as methods for quantifying partial identifiability and misspecification robustness. We also derive a number of lemmas and general results within this framework, that make it easy to reason about partial identifiability and misspecification.\nIn Section 4, we provide several results related to the issue of comparing reward functions. Specifically, we provide necessary and sufficient conditions that describe when two reward functions have the same optimal policies, or the same ordering of policies. We additionally introduce a family of pseudometrics for continuously quantifying the difference between reward functions. We show that these pseudometrics induce both an upper and a lower bound on worst-case regret, and that any pseudometric with this property must be bilipschitz equivalent to ours. Our later analysis builds on these results.\nIn Section 5, we fully characterise the ambiguity of the reward function given several different behavioural models, and we describe the practical consequences of this ambiguity. Notably, we show that the ambiguity of the reward is unproblematic for each of the standard behavioural models as long as the learnt reward is used in the same environment it was learnt in, but that we cannot guarantee robust transfer to new environments. In Sections 6 and 7, we analyse the question of misspecification, and derive necessary and sufficient conditions that fully describe what forms of misspecification each of the standard behavioural models will tolerate. We also study a few specific types of misspecification in greater depth, such as misspecification of the parameters of the behavioural model or perturbations of the observed policy. We find that the standard behavioural models do tolerate some forms of misspecification, but that they are highly sensitive to other forms of misspecification: notably, we find that even mild misspecification of the discount"}, {"title": "2. Technical Background", "content": "In this section, we introduce the technical prerequisites that are needed to understand the rest of our paper, together with our choice of notation. We also introduce all the assumptions we will make about the environment. For a more in-depth overview of reinforcement learning, see e.g. Sutton and Barto (2018), and for a more in-depth overview of inverse reinforcement learning, see e.g. Arora and Doshi (2021) or Adams et al. (2022)."}, {"title": "2.1. Reinforcement Learning", "content": "A Markov Decision Processes (MDP) is a tuple (S, A, \u03c4, \u03bc\u2080, R, \u03b3) where Sis a set of states, A is a set of actions, \u03c4: S\u00d7A \u2192 S is a transition function, \u03bc\u2080 \u2208 \u0394(S) is an initial state distribution, R: S\u00d7A\u00d7S \u2192 R is a reward function, and \u03b3 \u2208 (0,1) is a discount rate. Here f : X \u2192 Y denotes a probabilistic mapping from X to Y. A (stationary) policy is a function \u03c0: S \u2192 A, which encodes the behaviour of an agent in each state of an MDP. We use \u041f to denote the set of all stationary policies. A triple (s, a, s') \u2208 S\u00d7A\u00d7S is a transition, and a trajectory \u03be = (s\u2080, a\u2080, s\u2081, a\u2081 . . .) is an infinite (potentially repeating) path through an MDP, i.e. an element of (S\u00d7A)\u2115. If s\u2080 \u2208 supp(\u03bc\u2080) and s\u209c\u208a\u2081 \u2208 supp(\u03c4(s\u209c, a\u209c)) for each t \u2208 \u2115, then we say that \u03be is a possible trajectory, and otherwise it is impossible.\nIn this paper, we assume that S and A are finite. Moreover, we also assume that all states in S are reachable under \u03c4and \u03bc\u2080 (i.e., for all states s, there exists a possible trajectory which includes s). This is primarily a theoretical convenience. Also note that if an MDP has unreachable states, then we may simply remove these states from S.\nThe return function G: (S\u00d7A)\u2115 \u2192 \u211d gives the cumulative discounted reward of each trajectory, i.e. G(\u03be) = \u2211\u209c\u208c\u2080^\u221e \u03b3\u1d57R(s\u209c, a\u209c, s\u209c\u208a\u2081). Similarly, the evaluation function J: \u03a0 \u2192 \u211d gives the expected trajectory return of each policy, J(\u03c0) = \ud835\udd3c\u03be\u223c\u03c0[G(\u03be)]. The value function V \u03c0: S \u2192 \u211d of a policy \u03c0 encodes the expected future cumulative discounted reward from each state"}, {"title": "2.2. Inverse Reinforcement Learning", "content": "The aim of an IRL algorithm is to infer a representation of an agent's preferences based on their behaviour. It is typically assumed that these preferences can be represented as a reward function, and that the observed behaviour has the form of a policy. It is also typically assumed that the environment of the agent can be modelled as an MDP. The IRL problem can thus loosely be stated as follows. There is an unknown reward function R. You get to observe a policy \u3160, which has been computed from R relative to some transition function 7, initial state distribution \u03bc\u03c9, and discount factor \u03b3. We may or may not assume that \u03c4, \u03bc\u03c9, and y are known.\u00b2 The goal is then to infer a reward function RH, that is as similar as possible (in some relevant sense) to the true reward function R.\nAn IRL algorithm must make assumptions about how the observed pol- icy relates to the underlying reward function, R. These assumptions are referred to as the behavioural model. In some cases, the behavioural model simply assumes that \u03c0 is optimal under R (e.g. Ng and Russell, 2000). How- ever, this assumption is often unrealistic; people sometimes make mistakes, and are subject to limited information and limited cognitive resources. As such, many IRL algorithms make use of other behavioural models. One com- mon model is Boltzmann rationality (e.g. Ramachandran and Amir, 2007), which says that\nP(\u03c0(s) = a) = exp \u03b2Q*(s, a) / (\u03a3 exp \u03b2Q*(s, a')).\nHere \u03b2 \u2208 \u211d\u207a is known as a temperature parameter. When \u03c0 satisfies this relationship, we refer to it as a Boltzmann-rational policy. The function f : \u211d\u207f \u2192 \u211d\u207f given by f(v)\u1d62 = exp \u03b2v\u1d62 / (\u03a3\u2096\u208c\u2081\u207f exp \u03b2v\u2096) is known as the softmax function for temperature \u03b2. Thus, a Boltzmann-rational policy is given by applying a softmax function to the optimal Q-function. Intuitively speaking, such a policy takes every action with positive probability, but is more likely to take actions with high value than actions with low value. Boltzmann- rationality can therefore be seen as a form of noisy optimality.\nAnother common behavioural model is causal entropy maximisation (e.g. Ziebart, 2010). This behavioural model specifies an alternative optimisation"}, {"title": "2.3. Metrics, Pseudometrics, and Norms", "content": "In our analysis, we will often quantify the difference between different kinds of objects (especially reward functions). To do this, we will make use of metrics, pseudometrics, and norms. Given a set X, a function m : X x X \u2192 R is a pseudometric on X if it satisfies the following axioms:\n1. Indiscernibility of identicals: m(x,x) = 0 for all x \u2208 X.\n2. Positivity: m(x, y) \u2265 0 for all x, y \u2208 X.\n3. Symmetry: m(x,y) = m(y,x) for all x, y \u2208 X.\n4. Triangle inequality: m(x, z) \u2264 m(x, y) + m(y, z) for all x, y, z \u2208 X.\nIf m additionally satisfies the identity of indiscernibles, which says that m(x,y) \u2260 0 for all x,y \u2208 X such that x \u2260 y, then m is a metric. Every metric is a pseudometric, but not vice versa.\nGiven a vector space V, a function n : V \u2192 R is a norm on V if n satisfies the following axioms:\n1. Non-negativity: n(v) \u2265 0 for all v \u2208 V.\n2. Positive definiteness: n(v) = 0 if and only if v is the zero vector.\n3. Absolute homogeneity: n(c. v) = |c| \u00b7 m(v) for all v \u2208 V and c \u2208 R.\n4. Triangle inequality: n(v + w) \u2264 n(v) + n(w) for all v, w \u2208 V.\nFor any real number p > 1, the function Lp: \u211d\u1d48 \u2192 \u211d given by\nL\u209a(v) = (\u2211\u1d62\u208c\u2081\u1d48 |v\u1d62|\u1d56 )\u00b9/\u1d56\nis a norm on \u211d\u1d48. The function L\u221e: \u211d\u1d48 \u2192 \u211d, given by L\u221e(v) = max\u1d62 |v\u1d62|, is also a norm. Moreover, if n : \u211d\u1d48 \u2192 \u211d is a norm and M: \u211d\u1d48 \u2192 \u211d\u1d48 is an invertible matrix, then n \u2218 M is also a norm."}, {"title": "3. New Definitions and Formalisms", "content": "In this section, we introduce the theoretical frameworks that underpin our further analysis. First, we will introduce a number of definitions that formalise the notion of partial identifiability. After this, we will introduce two related but distinct ways of formalising misspecification robustness, and discuss the benefits of each approach. In addition, we will present several relevant intermediate results about our framework. These lemmas will be used to prove our later results, but are also insightful in their own right.\nSome of the definitions we provide in this section will be given relative to an equivalence relation \u2261 or a pseudometric d\u1d63 on R, the set of all possible rewards. The purpose of these is to quantify differences between reward functions (in particular, the difference between the learnt reward function and the true reward function). In this section, we will not discuss the issue of which equivalence relation \u2261 or pseudometric d\u1d63 to use \u2014 this question will instead be addressed in Section 4."}, {"title": "3.1. Partial Identifiability", "content": "In this section, we describe the framework that we will use to analyse partial identifiability. Before going into the specifics, let us recall the details"}, {"title": "3.2. Misspecification Robustness", "content": "In this section, we introduce the frameworks that we will use for analysing robustness to misspecification. Do do this, we will fist give an abstract model of a reward learning algorithm L that is slightly more general than that provided in Section 3.1. As before, we assume that there is a true underlying reward function R*, and that the training data is generated by a function g: R \u2192 X, so that the learning algorithm observes g(R*). Moreover, we assume that the learning algorithm L has a model f : R \u2192 X of how the observed data relates to R*, such that L converges to a reward function RH that satisfies f(RH) = g(R*). However, unlike in Section 3.1, we will not assume that f = g; this allows us to reason about the impact of misspecification. If f \u2260 g, then f is misspecified, otherwise f is correctly specified."}, {"title": "3.3. Intermediate Results About Our Definitions", "content": "In this section, we provide several lemmas and intermediate results about Definition 7 and 8. These results give insight into the properties of our problem setting, and will also be used to prove our object-level results. We begin by listing a number of interesting properties of Definition 7:"}, {"title": "3.4. Reward Transformations", "content": "Recall that a reward transformation is a map t : R \u2192 R. In this section, we introduce several important classes of reward transformations, that we will later use to express our results. First recall potential shaping, which was first introduced by Ng et al. (1999):"}, {"title": "3.5. Behavioural Models", "content": "In this section, we introduce some special notation for the three be- havioural models that are most common in the current IRL literature, i.e. optimal policies, Boltzmann-rational policies, and MCE policies. Given a transition function 7 and a discount parameter \u03b3, let \u03b4\u03c4,\u03b3,\u03b2: R \u2192 \u041f be the function that returns the Boltzmann-rational policy of R with temperature \u03b2, and let C\u03c4,\u03b3\u03b9\u03b1 : R \u2192 \u041f be the function that returns the MCE policy of R with weight \u03b1. These policies exist and are unique for each \u03c4, \u03b3, \u03b2, and \u03b1, and so b\u03c4,\u03b3,\u03b2 and C\u03c4,\u03b3,\u03b1 are well-defined.\nThe optimality model requires a bit more care, because there may in gen- eral be more than one policy that is optimal under a given reward function. To resolve this, recall that a policy is optimal if and only if it only gives support to optimal actions, where the \"optimal actions\u201d are the actions that maximise Q*. A state may have multiple optimal actions, so we can get multiple optimal policies by breaking ties in different ways. However, if an optimal policy gives support to multiple actions in some state, then we would normally not expect the exact probability it assigns to each action to convey any information about the reward function. We will therefore only look at the actions that the optimal policy takes, and ignore the relative probability it assigns to those actions. Formally, we will treat optimal policies as func- tions \u03c0\u2217 : S \u2192 P(argmax\u2090\u2208AQ\u2217) \u2212 {\u2205}; i.e. as functions that for each state return a non-empty subset of the set of all actions that are optimal in that state. Let \u041e\u03c4,\u03b3 be the set of all functions that return such policies (relative to transition function 7 and discount factor \u03b3). Moreover, let o \u2208 \u041e\u0442,\u0443 be the function that, given R, returns the function that maps each state to the set of all actions which are optimal in that state. Intuitively, o corresponds to optimal policies that take all optimal actions with positive probability. Alternatively, we can also think of o as corresponding to the"}, {"title": "4. Comparing Reward Functions", "content": "When analysing a reward learning algorithm, we wish to derive claims that compare the learnt reward function to the underlying true reward func- tion, given different setups and conditions. To do this, we must first have principled methods for comparing reward functions. In this section, we dis- cuss different methods for doing this. First, we will introduce two natural equivalence relations on the space of all reward functions, and characterise the corresponding equivalence classes. We will also introduce a family of pseudometrics on the space of all reward functions, and show that these pseudometrics satisfy several desirable properties. In later sections, we will use these reward transformations, equivalence classes, and metrics, to express and prove our results about IRL algorithns."}, {"title": "4.1. Equivalent Reward Functions", "content": "In this section, we will introduce and study two important equivalence relations on R. The first equivalence relation considers two reward functions to be equivalent if they have the same ordering of policies, and the second equivalence relation considers two reward functions to be equivalent if they have the same optimal policies. We will also characterise these equivalence relations in terms of reward transformations.\nGiven a discount y and transition function \u03c4, we say that ORD\u03c4,y is the equivalence relation under which R\u2081 \u2261ORD\u03c4,y R\u2082 if and only if R\u2081 and R\u2082 have the same policy ordering under \u03c4 and \u03bc\u03bf."}, {"title": "4.2. STARC Metrics", "content": "In this section, we introduce a family of pseudometrics on R, which we can use to get a fine-grained quantification of the difference between any two reward functions. First, we note that it is not straightforward to quantify the difference between reward functions in an informative way. A simple method might be to measure their L2-distance. However, this is unsatisfactory, because two reward functions can have a large L2-distance, even if they induce the same ordering of policies, or a small L2-distance, even if they induce the opposite ordering of policies. For example, given an arbitrary reward func- tion R and an arbitrary positive constant c, we have that R and c \u00b7 R have the same ordering of policies, even though their L2-distance may be arbitrarily large. Similarly, for any e, we have that e \u00b7 R and -e \u00b7 R have the opposite ordering of policies, unless R is trivial, even though their L2-distance may be arbitrarily small. Constructing a new pseudometric on R which provides an informative quantification of a useful difference between two reward functions will therefore require some care.\nBefore proceeding, we should consider what properties a function d : R\u00d7R \u2192 R needs to have, in order to provide a useful way of quantifying the"}, {"title": "5. Partial Identifiability", "content": "In this section, we present our results about the partial identifiability of the reward function in IRL, relative to the standard behavioural models. Specifically, we derive the ambiguity of the Boltzmann-rational model, the MCE model, and the optimality model. We also discuss the ambiguity tolerance of a number of different applications, and analyse the question of transfer learning. Note that our results in this section cover both the case where reward functions are compared using equivalence relations, and the case where they are compared using pseudometrics."}, {"title": "5.1. Invariances of Policies", "content": "In this section, we derive the invariances of the three behavioural models that are most common in the current IRL literature. These results will be expressed in terms the sets of reward transformations that we introduced in Section 3.4. We begin with the Boltzmann-rational model:"}, {"title": "5.2. Ambiguity Tolerance and Applications", "content": "Now that we have derived the ambiguity of R under each of the three standard behavioural models, it may be worth reflecting on the implications of these results. First of all, both b\u03c4,\u03b3,\u03b2 and C\u03c4,\u03b3,\u03b1 determine R up to S'-redistribution (with \u03c4) and potential shaping (with \u03b3). From this, we can straightforwardly derive the following:"}, {"title": "5.3. Transfer Learning", "content": "It is interesting to consider the setting where a reward function is learnt in one MDP, but used in a different MDP. For example, a quite natural setup is when we may learn the reward under one transition function \u03c4\u2081, but wish to use it under another transition function \u03c4\u2082. Alternatively, the observed agent may discount using one discount factor \u03b3\u2081, but we wish to use the reward"}, {"title": "6. Misspecification With Equivalence Relations", "content": "In this section, we present our results about how robust IRL is to misspec- ified behavioural models, using the formalisation provided by Definition 7. First, we will derive necessary and sufficient conditions that describe all forms of misspecification that are tolerated by the Boltzmann-rational model, the MCE model, and the optimality model. In so doing, we will also define some broader equivalence classes of behavioural models that are internally robust to misspecification, and which can thus include more behavioural models than the standard three we are familiar with. After this, we will discuss how to generalise some of our results to even wider classes of behavioural models, and show that some of our results should be expected to apply with some universality, to be made more precise. After this, we will discuss the case where the environment model is misspecified, as well as the important issue of transfer learning. Most of our results in this section are expressed in terms of the two equivalence relations ORD\u03c4,\u03b3 and OPT\u03c4,\u03b3 on R, which were introduced in Section 4."}, {"title": "6.1. Necessary and Sufficient Conditions", "content": "In this section, we will present necessary and sufficient conditions that describe all forms of misspecification that are tolerated by the Boltzmann- rational model, the MCE model, and the optimality model.\nLet \u03a0+ be the set of all policies such that \u03c0(a | s) > 0 for all s, a, and let Fry be the set of all functions fry: R \u2192 \u03a0+ that, given R, return a policy that satisfies\nargmax\u2090\u2208\u03c0(a | s) = argmax\u2090\u2208A Q\u2217(s, a),\nwhere Q\u2217 is the optimal Q-function for R under \u03c4 and \u03b3. In other words, Fr,y is the set of all functions that generate policies which take each action with positive probability, and that take the optimal actions with the highest"}, {"title": "6.2. Wider Classes of Policies", "content": "At this point, it is worth remarking on the fact that there are several note- worthy parallels between the invariances and the misspecification robustness"}, {"title": "6.3. Misspecified Parameters", "content": "A behavioural model will typically be parameterised by a y or \u03c4, implicitly or explicitly. In this section, we explore what happens if these parameters are misspecified. We show that a wide class of behavioural models lack robustness to this type of misspecification.\nTheorems 56-59 already tell us that the standard behavioural models are not (ORDr,y or OPT\u03c4,y) robust to misspecified y or \u03c4, since the sets Fr,y, Br,y, and Cr,y, all are parameterised by y and \u03c4. We will generalise this further. First, we note that any behavioural model that is invariant to S'-redistribution will lack robustness to a misspecified \u03c4. Recall Theorem 52; if fr\u2081 is invariant to S\"-redistribution with 71, and T1 \u2260 T2, then we have that Am(fT\u2081) \u226f OPTT2,y. Using this, we can prove the following:"}, {"title": "6.4. Transfer Learning", "content": "The equivalence relations we have worked with (OPT\u03c4,y and ORD\u03c4,\u03b3) only guarantee that the learnt reward function RH has the same optimal policies, or ordering of policies, as the true reward R* for a given choice of \u03c4and \u03b3. A natural question is what happens if we strengthen this requirement, and demand that RH has the same optimal policies, or ordering of policies, as R*, for any choice of \u03c4or \u03b3. We briefly discuss this setting here.\nIn short, it is impossible to guarantee transfer to any \u03c4or \u03b3. This is already implied by the results in Section 5.3. In particular, if fry is invariant to S'-redistribution (with \u315c) and potential shaping (with \u03b3), then\nAm(f) \u226f OP\u2081\u0662,\u0662\u2082\nif either \u03c41 \u2260 \u03c42, or \u03b31 \u2260 \u03b32 and \u03c4\u2082 is non-trivial. Then frywill violate condition 3 in Definition 7."}, {"title": "7. Misspecification With Metrics", "content": "In this section, we present our results about how robust IRL is to misspec- ified behavioural models, using the formalisation provided by Definition 8. First, we will derive necessary and sufficient conditions that describe all forms of misspecification that the Boltzmann-rational model and the MCE model are robust to, and discuss the issue of how to derive similar results for the op- timality model. After this, we analyse a particular form of misspecification, which we refer to as perturbation, provide necessary and sufficient conditions for a behavioural model to be robust to such misspecification, and show that none of the three main behavioural models meet these conditions. After this, we will discuss the case where the environment model is misspecified, as well as the issue of transfer learning. Section 7.1 is quite dense, but 7.2 and 7.3 both provide more intuitive takeaways.\nOur results in this section are expressed in terms of pseudometrics on R. Most of these results apply for any choice of pseudometric, but when we need to select a specific pseudometric, we will use the newly introduced STARC metric dSTARC, as specified in Definition 44."}, {"title": "7.1. Necessary and Sufficient Conditions", "content": "In this section, we will present necessary and sufficient conditions that describe all forms of misspecification that are tolerated by the Boltzmann- rational model, the MCE model, and the optimality model.\nLet \u041f+ be the set of all policies such that \u03c0(a | s) > 0 for all s, a, and let Fry be the set of all functions fry: R \u2192 \u041f+ that, given R, return a policy \u03c0 that satisfies\nargmax\u2090\u2208\u03c0(a | s) = argmax\u2090\u2208A Q\u2217(s, a),\nwhere Q\u2217 is the optimal Q-function for R under \u03c4 and \u03b3. In other words, Fry is the set of all functions that generate policies which take each action with positive probability, and that take the optimal actions with the highest"}, {"title": "7.2. Perturbation Robustness", "content": "It is interesting to know whether or not a behavioural model f is robust to misspecification with any behavioural model g that is \u201cclose\u201d to f. But what does it mean for f and g to be \u201cclose\u201d? One option is to say that f and gare close if they always produce similar policies. In this section, we will explore under what conditions f is robust to such misspecification, and provide necessary and sufficient conditions. Our results are given relative to a pseudometric d\u03a0 on \u03a0. For example, d\u03a0(\u03c0\u2081, \u03c0\u2082) may be the L2-distance between \u03c0\u2081 and \u03c0\u2082, or it may be the KL divergence between their trajectory distributions, or it may be the L2-distance between their occupancy mea- sures, and so on. As usual, our results apply for any choice of dr unless otherwise stated. We can now define a notion of a perturbation and a notion of perturbation robustness:"}, {"title": "7.3. Misspecified Parameters", "content": "In Section 6, we showed that many behavioural models are not OPT\u03c4,y- robust to any misspecification of \u03c4or \u03b3. However, this result says that we cannot identify the exact right optimal policies, or the exact right policy order, given misspecification of \u03c4or \u03b3. This does not rule out the possibility that a small misspecification of \u03c4or y leads to a small (but nonzero) STARC distance between the true reward function and the learnt reward function. This is the question that we will investigate in this section.\nFirst of all, recall that Lemma 17 implies that if f is \u03f5-robust to misspec- ification with g (as measured by dr), and g(R\u2081) = g(R\u2082), then we have that dR(R1, R2) \u2264 2\u03f5. The converse of this statement is that if there are reward functions R1, R2 such that g(R1) = g(R2) and dr(R1, R2) > 2e, then f is not \u03f5-robust to misspecification with g (as measured by dr). Therefore, we can use the (upper) diameter of Am(g) to derive a limit on how robust any f may be to misspecification with g. Our results in this section will use this proof strategy. We first consider the case when 7 is misspecified:"}, {"title": "7.4. Transfer Learning", "content": "STARC metrics, such as dSTARC, are designed to be sound and complete. Moreover, our definitions of soundness and completeness for a pseudometric d require that d (R1, R2) is small if and only if the regret of using R\u2081 instead of R2 is small, relative to a particular transition function 7 and discount factor \u03b3. A natural question is what happens if we strengthen this requirement, and demand that the regret is small for any choice of \u03c4or any choice of \u03b3. We briefly discuss this setting here.\nIn short, as for Definition 7, it is impossible to guarantee transfer to any \u03c4or \u03b3. This is already implied by the results in Section 5.3. In particular, if fry is invariant to S'-redistribution (with \u315c) and potential shaping ("}]}