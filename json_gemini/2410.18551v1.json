{"title": "IMAN: An Adaptive Network for Robust NPC Mortality Prediction with Missing Modalities", "authors": ["Yejing Huo", "Guoheng Huang", "Lianglun Cheng", "Jianbin He", "Xuhang Chen", "Xiaochen Yuan", "Guo Zhong", "Chi-Man Pun"], "abstract": "Accurate prediction of mortality in nasopharyngeal carcinoma (NPC), a complex malignancy particularly challenging in advanced stages, is crucial for optimizing treatment strategies and improving patient outcomes. However, this predictive process is often compromised by the high-dimensional and heterogeneous nature of NPC-related data, coupled with the pervasive issue of incomplete multi-modal data, manifesting as missing radiological images or incomplete diagnostic reports. Traditional machine learning approaches suffer significant performance degradation when faced with such incomplete data, as they fail to effectively handle the high-dimensionality and intricate correlations across modalities. Even advanced multi-modal learning techniques like Transformers struggle to maintain robust performance in the presence of missing modalities, as they lack specialized mechanisms to adaptively integrate and align the diverse data types, while also capturing nuanced patterns and contextual relationships within the complex NPC data. To address these problem, we introduce IMAN: an adaptive network for robust NPC mortality prediction with missing modalities. IMAN features three integrated modules: the Dynamic Cross-Modal Calibration (DCMC) module employs adaptive, learnable parameters to scale and align medical images and field data; the Spatial-Contextual Attention Integration (SCAI) module enhances traditional Trans-formers by incorporating positional information within the self-attention mechanism, improving multi-modal feature integration; and the Context-Aware Feature Acquisition (CAFA) module adjusts convolution kernel positions through learnable offsets, allowing for adaptive feature capture across various scales and orientations in medical image modalities. Extensive experiments on our proprietary NPC dataset demonstrate IMAN's robustness and high predictive accuracy, even with missing data. Compared to existing methods, IMAN consistently outperforms in scenarios with incomplete data, representing a significant advancement in mortality prediction for medical diagnostics and treatment planning.", "sections": [{"title": "I. INTRODUCTION", "content": "Medical diagnostics and prognostics for complex diseases like nasopharyngeal carcinoma (NPC) heavily rely on com-prehensive data analysis. NPC early detection and accurate prognosis are vital for timely, effective treatment, significantly improving clinical outcomes and quality of life [1]\u2013[6]. Consequently, there is an urgent need for accurate predictive models that can effectively integrate diverse data modalities to enhance early diagnosis, reduce NPC mortality, and guide optimal treatment plans.\nTo address this pressing need, early traditional machine learning techniques like Lasso [7] have been employed, sig-nificantly improving prognostic accuracy for advanced-stage NPC patients. However, these methods often struggle due to their inability to effectively handle data heterogeneity, and missing modalities, severely impacting their performance (as shown in Figure 1). The rise of deep learning has facilitated the development of multi-modal approaches that leverage powerful neural networks to integrate diverse modalities [8], [9]. Early deep learning methods utilized matrix completion [10] to address missing modalities, but suffered from high computational complexity and performance degradation as the number of modalities increased. More recent studies have explored tensor-based modal distillation [11] and Transformer-based feature reconstruction [12], [13] to tackle these chal-lenges. Nevertheless, existing methods commonly assume a single missing modality and falter when confronted with highly heterogeneous and incomplete data contexts involving multiple missing modalities, as exemplified by Lee et al.'s [13] multi-modal Transformer with prompt learning [14], [15]. These limitations highlight the critical need for a more robust solution capable of effectively handling incomplete multi-modal data in nasopharyngeal carcinoma mortality prediction tasks.\nTo address the above shortcomings, we propose IMAN, an adaptive network for reliable NPC death prediction under modality missing. IMAN features three key modules: Dynamic Cross-Modal Calibration (DCMC), Spatial-Contextual Atten-tion Integration (SCAI), and Context-Aware Feature Acquisi-tion (CAFA). The DCMC module uses learnable parameters to adaptively scale and align medical images and text data, en-hancing the normalization of heterogeneous inputs. The SCAI module improves feature fusion by incorporating positional information into the self-attention mechanism, enabling the detection of subtle patterns in medical images. The CAFA module adjusts convolution kernel positions through learned offsets, allowing adaptive feature capture across various scales and orientations. Extensive experiments on our NPC dataset demonstrate IMAN's robustness and high predictive accuracy, even with missing data. This integrated approach ensures more consistent and accurate treatment outcome predictions, representing a significant advancement in NPC diagnosis and treatment planning.\nThe contributions of this work are summarized as follows:\n\u2022 We propose IMAN, an Incomplete Modality Adaptive Network, to address the poor performance of models with incomplete modalities in predicting nasopharyngeal carcinoma (NPC) outcomes. IMAN consists of three core modules: DCMC, SCAI, and CAFA.\n\u2022 The DCMC module employs learnable parameters to adaptively scale and align medical images and field data, enhancing the normalization and alignment of hetero-geneous inputs. The SCAI module integrates positional information into the self-attention mechanism of Trans-formers, improving the fusion of multi-modal features and enabling the model to detect subtle patterns in medical data. The CAFA module adjusts convolution ker-nel positions through learned offsets, allowing adaptive feature capture across different scales and orientations in various medical image modalities.\n\u2022 Extensive experiments demonstrate that IMAN performs exceptionally well on the proprietary NPC datasets."}, {"title": "II. METHOD", "content": "The IMAN network follows a structured design, beginning with pre-training a missing-aware prompt module using diverse data to enhance predictive power across various sce-narios. This pre-trained module generates task-specific tokens based on the presence or absence of different modalities, guiding the attention of the Transformer towards relevant infor-mation. Based on the ViLT architecture [16], IMAN transforms data fields into embedded tokens and uses the CAFA module to adjust convolutional kernels, capturing features across scales and orientations for multi-modal information. The pre-trained module generates task tokens with cues based on inputs, which are added to the sequence of field and image tokens, guiding the Transformer's attention mechanism to address missing modality issues and reduce fine-tuning needs. The combined inputs pass through 12 Transformer layers, with the DCMC module dynamically calibrating data sizes and the SCAI module incorporating positional information into the self-attention mechanism for enhanced feature fusion and performance. The Transformer's outputs are then processed through pooling and fully-connected layers for classification. Detailed functions and roles of each component are discussed in subsequent sections."}, {"title": "B. Dynamic Cross-Modal Calibration Module", "content": "Inspired by the migration style task [17] that transforms image styles to content images in feature space through feature statistics, we propose the DCMC module. DCMC introduces dynamic and learnable parameterization for normalization. It employs sets of learnable scaling parameters \u03c3 and shifting parameters \u03b3, optimized during training to match the statistical properties of input data. These parameters adaptively scale the feature maps of medical images and field data, promoting a consistent representation across modalities. The mathematical formulations are as follows:\nDCMC(I, F) = \u03c3(F) * (I \u2212 \u03bc(I)) / \u03c3(I) + \u03b3(F)  (1)\n\u03c3(F) = MLP(F), \u03b3(F) = MLP(F)  (2)\nwhere \u03c3 and \u03b3 are linearly transformed by two MLPs, respec-tively, from the field features F. The DCMC takes as input the image features I and the field features F and adapts the mean and standard deviation of I according to F, allowing I to adaptively match different F. In this way, the shape information contained in the image features can be injected into the fields while preserving their spatial structure.\nIMAN's adaptive mechanism enhances its ability to syn-chronize diverse medical data, boosting operational efficiency in complex, high-dimensional datasets like nasopharyngeal carcinoma studies. The network's capability to handle varied data types strengthens its diagnostic and prognostic perfor-mance in medical imaging and treatment planning. The DCMC module significantly improves IMAN's capacity to process multi-modal data, crucial in scenarios with incomplete med-ical images, ensuring accurate diagnostic predictions without compromising performance."}, {"title": "C. Spatial-Contextual Attetion Integration Module", "content": "Traditional Transformer models face challenges with high-dimensional multi-modal data in contexts like predicting mor-tality in nasopharyngeal carcinoma (NPC), especially when modalities are missing. Addressing these issues, we introduce the SCAI module, inspired by Su et. al [18]. This module incorporates rotation-based methods within the self-attention mechanism, enhancing the modeling of relative positions cru-cial for spatial relationships in medical images and sequential patterns in diagnostic reports [19], [20]. Unlike fixed position embeddings, rotational encoding provides flexibility to manage varying sequence lengths and absent modalities. By preserving semantic content and improving the model's capability to detect subtle patterns across modalities, the SCAI module significantly enhances feature fusion effectiveness in complex, high-dimensional NPC data, facilitating better diagnosis and treatment planning.\nInitially, we establish a sequence of features with a length of N (as the operations for field and image features are analogous in the process of SCAI, the symbol P in the subsequent equation denotes various operations for different eigenvectors under different circumstances): Fn = {Wi}N i=1. Within this sequence, wirepresents the i-th token of the input sequence. The embedding corresponding to the input sequence FN is denoted as En = {xi}N i=1, where xi signifies the d-dimensional embedding vector associated with the i-th token Wi.\nPrior to engaging in self-attention operations, we employ the feature embedding vectors to derive the query (q), key (k), and value (v) vectors, while also integrating the pertinent positional information.\nIn this context, qs denotes the query vector for the s-th token, which has incorporated the positional information s into its corresponding feature vector xs. Similarly, kt and vt represent the key and value vectors, respectively, for the t-th token, with the positional information t seamlessly integrated into the feature vector xt.\nSubsequently, the objective is to compute the self-attention output for the s-th feature embedding vector xs. This process entails determining the attention scores between the query vector qs and all other key vectors kt. Each attention score is then multiplied by its corresponding value vector vt. The final step involves aggregating these products through summation to derive the resultant output vector os.\nMoving forward, to capitalize on the relative positional relationships among the tokens in question, we introduce a function, denoted as g, which encapsulates the dot product operation between the query vector qs and the key vector kt. The function g is designed to take into account the word embedding vectors xs and xt, along with the relative positional difference s-t, thereby enabling the model to discern the contextual significance of each token's position within the sequence.\n<Pq(xs, s), Pk(xt,t) >= g(xs, xt, s - t) (3)\nWe can cleverly use Euler's formula eix = cosx + isinx, where the real part is cosx and the imaginary part sinx is of a complex number.\nAfter transformation, formulas O and g can be changed to:\neise = cos(s\u03b8) + isin(s\u03b8) (4)\neito = cos(t\u03b8) + isin(t\u03b8) (5)\nei(s-t)0 = cos((s \u2013 t)\u03b8) + isin((s \u2013 t)\u03b8) (6)\nOq(xs,s) = (Wqxs)eiso (7)\nThen, according to linear algebra, we can represent qs using a matrix:\nqs = ( Wqxs ) = (Wqxs) *(qs(1),qs(2))=(W(11)q W(12)q W(21)q W(22)q )Xs \n(8)\nOq(xs, 8) = (Wqxs)eiso = qseiso (9)"}, {"title": "D. Context-Aware Feature Acquisition Module", "content": "Existing methodologies for extracting multi-modal image characteristics, as cited in [16], [25], are limited by conven-tional convolution's localized receptive field, which restricts effective capture of global contextual information. Drawing inspiration from Zhang's pioneering work [26], we introduce the Context-Aware Feature Acquisition (CAFA) module. This module innovatively overcomes these constraints by dynami-cally adjusting convolution kernel positions through trainable offsets. It enables adaptive feature extraction across diverse scales and orientations in multi-modal medical imagery.\nGiven the input image I \u2208 RC\u00d7H\u00d7W, where C, H, and W denote the number of channels, height, and width respectively. The CAFA generates an initial sampling coordinate Pn via Algorithm 1. Then I undergoes a convolution operation to learn the offset, offset = Pconv(I). The offset is used to adjust the shape of the sample at each position. The adjusted sampling position P is mathematically defined as:\nP = Po + Pn + offset (14)\nwhere Po denotes the base grid coordinates.\nNext, the CAFA module will resample the input features at position Pusing bilinear interpolation. The resampled input feature can be expressed as:\nXoffset = glt Xqit + grb. Xqrb + glb\u00b7Xq\u0131b + 9rtXqrt (15)\nwhere glt, grb, glb, grt are the weights of the bilinear inter-polation, and Xqit, Xqrb, Xq\u0131b, Xqrt are the eigenvalues of the corresponding positions. lt, lb, rt, rb are abbreviations for left-top, left-bottom, right-top, and right-bottom, respectively.\nFinally, the CAFA reshapes the resampled features into the desired shape and obtains the output feature map by a depthwise convolution operation:\noutput = Dconv(xoffset) (16)\nThrough this mechanism, the CAFA module endows con-volution kernels with the capacity to incorporate a variable number of parameters, dynamically tuning the sampling grid to conform to the evolving characteristics of the target. By tailoring optimal sampling configurations to the specific task at hand, the CAFA module significantly augments its capacity to discern salient features across a broad spectrum of scales within multi-modal medical images, thereby enhancing the model's discriminative power and generalizability."}, {"title": "III. EXPERIMENT", "content": "The dataset for this study was provided by Sun Yat-sen University Cancer Center and associated research institutions (approved ID: B2019-222-01). It comprises 1,224 samples divided into training (70%), validation (15%), and test (15%) sets. Five modalities were considered: two diagnostic fields (EBV and normal) and three medical imaging modalities (T1, T1C, and T2). The EBV field is represented by a 4-dimensional vector, and the normal field by a 19-dimensional vector. Each imaging modality is a 32 \u00d7 224 \u00d7 224 tensor. To simulate real-world data incompleteness, we implemented a sophisticated missingness mechanism using a missingness table T, a 1224 \u00d7 5 binary matrix where each entry indicates the presence (0) or absence (1) of data. The missingness rate for a modality k is computed as \u2211(T[:, k])/T.shape[0] (where k = 0, 1, 2, 3, 4). Our experiments will explore overall missing rates of 20%, 40%, 60% and 80%, focusing primarily on the 20% rate, which reflects real medical scenarios according to specialized doctors. We evaluated our approach using Accu-racy, F1-Score, Recall, AUC, and Precision."}, {"title": "B. Experiment details", "content": "Our approach was trained on a single RTX 4090 GPU and the pytorch framework. Where the batch is set to 64, the base backbone is ViLT [16], the Adam optimizer is used, the initial learning rate is set to 1e-4, the weight decay coefficient is set to 0.01, and the epoch is set to 200. The learning rate is warmed up for 10% of the total training steps and then is decayed linearly to zero."}, {"title": "C. Comparison with State-of-the-Art Methods", "content": "Our comprehensive experiments comparing IMAN with state-of-the-art methods like GNN4CMR [21], MMCL [22], HGCN [23], RBA-GCN [24], and MPMM [25] across various missing data scenarios demonstrate IMAN's superior perfor-mance, particularly in Accuracy and AUC (see Table II). For example, with 20% missing EBV data, IMAN achieved an accuracy of 0.94 and an AUC of 0.92, significantly out-performing MPMM's accuracy of 0.83 and AUC of 0.72. However, the relatively lower F1-Score and Recall metrics may be due to the inherent class imbalance in nasopharyngeal carcinoma datasets, where negative cases typically outnumber positive ones. Additionally, IMAN may reflect a conserva-tive diagnostic approach, leading to fewer false positives but potentially more false negatives. The complexity of cancer progression, influenced by factors like genetic expression and treatment response, may result in some rare predictive features being overlooked. Furthermore, predicting long-term mortality poses challenges in capturing all relevant factors. While IMAN excels in handling missing data, the absence of certain key predictive factors may still impact its performance in identifying all positive cases. The model's design, focusing on optimizing accuracy and AUC, might have led to trade-offs in F1-Score and Recall. Despite these limitations, IMAN'S overall performance surpasses existing methods, especially in scenarios with incomplete data, highlighting its potential to enhance the accuracy and reliability of mortality predictions in clinical settings for nasopharyngeal carcinoma as show in Fig. 3."}, {"title": "D. Ablation Study", "content": "To thoroughly evaluate the contribution and effectiveness of each module in the IMAN network, we conducted a series of detailed ablation experiments. These experiments were designed to assess the impact of the DCMC, SCAI, and CAFA modules on the model's overall performance. The experimental setup simulated common clinical scenarios with 20% of data missing EBV-related information. Table III presents the results of the experiments with various module combinations."}, {"title": "IV. CONCLUSION", "content": "In this study, we introduce IMAN, an adaptive network for predicting mortality in nasopharyngeal carcinoma cases with missing modalities. IMAN includes several key modules: the DCMC module normalizes heterogeneous inputs by scaling medical images and field data using learnable parameters, the SCAI module enhances multi-modal feature fusion with positional information in a self-attention mechanism, and the CAFA module adjusts convolution kernel positions for adaptive feature capture. These components work together to improve the precision of the system in handling medical data, enabling the detection of subtle patterns and effective feature capture across various scales and orientations. Experimental results in different missing data scenarios show that our method outperforms current alternatives. Future work will explore the performance of our model under varying data missing rates and more complex missing data scenarios."}, {"title": "ETHICAL STATEMENT", "content": "This study was conducted in accordance with ethical guide-lines and was approved by the Ethics Committee of Sun Yat-sen University Cancer Center IRB, approval number: B2019-222-01, approval date: January 8, 2020. We ensured that the privacy and confidentiality of the participants was strictly maintained. All data collected were anonymized and stored securely. Only authorized personnel had access to the data and measures were taken to prevent any unauthorized use or disclosure. The authors declare that they have no conflict of interest."}]}