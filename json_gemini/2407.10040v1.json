{"title": "Lean-STaR:\nLearning to Interleave Thinking and Proving", "authors": ["Haohan Lin", "Zhiqing Sun", "Yiming Yang", "Sean Welleck"], "abstract": "Traditional language model-based theorem proving assumes that by training on a\nsufficient amount of formal proof data, a model will learn to prove theorems. Our\nkey observation is that a wealth of informal information that is not present in formal\nproofs can be useful for learning to prove theorems. For instance, humans think\nthrough steps of a proof, but this thought process is not visible in the resulting code.\nWe present Lean-STaR, a framework for training language models to produce\ninformal thoughts prior to each step of a proof, thereby boosting the model's\ntheorem-proving capabilities. Lean-STaR uses retrospective ground-truth tactics\nto generate synthetic thoughts for training the language model. At inference time,\nthe trained model directly generates the thoughts prior to the prediction of the\ntactics in each proof step. Building on the self-taught reasoner framework, we then\napply expert iteration to further fine-tune the model on the correct proofs it samples\nand verifies using the Lean solver. Lean-STaR achieves state-of-the-art results\non the miniF2F-test benchmark within the Lean theorem proving environment,\nsignificantly outperforming base models (43.4%\u219246.3%, Pass@64). We also\nanalyze the impact of the augmented thoughts on various aspects of the theorem\nproving process, providing insights into their effectiveness.", "sections": [{"title": "1 Introduction", "content": "Theorem proving is a fundamental aspect of mathematics, and mathematical reasoning is an important\npart of artificial intelligence [28, 55]. Formalized mathematics in particular provides a challenging\ntestbed for assessing mathematical reasoning capabilities. Since theorems and proofs in this setting\ncan be represented in the form of checkable source code, it is easy to evaluate proofs of arbitrary\ncomplexity [15]. Automated theorem proving, if successful, can also help discover unknown errors\nin previous proofs\u00b9, and make it easier to guarantee that new proofs are correct. More broadly,\nformal mathematics coupled with powerful automation may unlock new forms of education and\ncollaboration, mathematical insights, and applications to verifying critical software [4, 16, 10, 31]."}, {"title": "2 Related Work", "content": "Automatic Theorem Proving & Autoformalization. Previous work on learning-based theorem\nproving typically follows the GPT-f framework [33], which trains a language model on (proof state,\nnext-tactic) pairs, then proves theorems by using the model within a best-first tree search. Subsequent\nwork has explored several directions, including data augmentation [20], novel proof search methods\n[25, 41], further training through curriculum learning [34], retrieval augmentation [50], or practical\ntools [44]. Others use prompted models to generate tactics [6, 39], or fine-tune models to generate\na full proof [17]. A second auto-formalization [48] thread incorporates informal mathematics into"}, {"title": "3 Our Method: Lean-STaR", "content": "We introduce Lean-STaR, a new method for combining informal thoughts with formal theorem\nproving. First, we recap interactive theorem proving (\u00a73.1). Then we present Lean-STaR's data-\ngeneration (\u00a73.2.1, \u00a73.2.2) and reinforcement learning (\u00a73.2.3) phases. Finally, we present our\nevaluation protocols (\u00a73.3)."}, {"title": "3.1 Preliminaries", "content": "Interactive Theorem Provers (ITPs) are typically used for step-by-step automatic theorem proving\nin formal mathematics. At each step, we can provide the ITP with a high-level \u201ctactic\u201d to simplify\nthe current goal state (e.g., the initial goal theorems to be proven) into subgoals. These subgoals\nwill form new states, and proving all the subgoals results in a complete proof of the given theorem.\nWe use Lean [15], a popular interactive theorem prover. An example formal proof in Lean and its\nexplanation are shown in Appendix C."}, {"title": "3.2 Data Generation & Training", "content": "We describe the data generation and training of the direct tactic prediction model (SFT), the thought-\naugmented tactic prediction model trained with synthetic data (Lean-CoT), and the final model trained\nwith expert iteration (Lean-STaR)."}, {"title": "3.2.1 Direct Tactic Prediction", "content": "We define the theorem-proving problem as a Markov Decision Process (MDP) $(S, A, P_a, R_a)$ where\nproof states serve as states in MDP and tactics serve as actions. From this perspective, a proof is a\ntrajectory $(s_1, a_1, r_1), (s_2, a_2, r_2), \\dots$\nof states $s_i$, tactics $a_i$, and rewards $r_i \\in \\mathbb{R}$, and the ITP (e.g.,\nLean) provides each new state $s_{i+1}$.\nIn the typical setting [33], proving a theorem consists of providing a proof state $s$ to the language\nmodel and then generating a tactic from the language model $\\mathcal{M}$, i.e., $\\pi_\\mathcal{M}(a|s)$. The language model\ncan be fine-tuned for this task using a dataset of (proof state, next-tactic) pairs from successful proof\ntrajectories, i.e. $D = \\{(s^i, a^i) : i = 1, \\dots, M\\}$, where final states have a reward of 1. We refer to a\nlanguage model fine-tuned on such a dataset as a supervised fine-tuning (SFT) model."}, {"title": "3.2.2 Thought-augmented Tactic Prediction", "content": "Existing approaches typically train only on formal states and tactics [33]. We hypothesize that\nincorporating a latent thought can improve a model's ability to predict the next tactic. Formally, we\nintroduce a hidden \"thought\" variable $t_i$ prior to each tactic, and then extend the model to the form\n$\\pi_\\mathcal{M}(a_i,t_i|S_i) = \\pi_\\mathcal{M}(a_i|t_i, S_i)\\pi_\\mathcal{M}(t_i|s_i)$. In thought-augmented tactic prediction, the distribution\nover the next tactic can then be expressed as:\n$\\pi_\\mathcal{M}(a_i|S_i) = \\sum_{t_i} \\pi_\\mathcal{M}(a_i|t_i, S_i)\\pi_\\mathcal{M}(t_i|s_i)$.\nThe key challenge is obtaining (state, thought, tactic) pairs for training a model. To this end, we\nintroduce retrospective rationale generation. Our motivating observation is that the distribution of\nnatural language thoughts in theorem-proving $\\pi_\\mathcal{M} (t_i|s_i)$ is scarce in the pre-training corpus of large\nlanguage models. In turn, we find that even the most powerful GPT-4 model does not perform well in\ngenerating the correct rationale through few-shot prompting [9]. To develop a language model capable"}, {"title": "3.2.3 Bootstrapping Thought-augmented Theorem Proving", "content": "We propose to apply expert iteration to further improve the performance of Lean-CoT. Specifically,\nwe start from the initial Lean-CoT model $M_0$ and the initial dataset $D = \\{s^i : i = 1, \\dots, M\\}$,\nwhich consists of all initial states $s^i$ of the theorems to be proved. In iteration $1$, we use\nmodel $M$ to sample $K$ times per problem. Each time the model will produce a proof trajectory\n$[(s_0, t_0, a_0), (s_1, t_1, a_1), \\dots, (s_n, t_n, a_n)]$. Then we create a new dataset $D_1$ by filtering the gener-\nated trajectories to include only the successful ones. De-duplication is then applied to the collected\ntrajectories. Now, we can further fine-tune the SFT model $M$ on dataset $D \\cup D_1$ to produce\nLean-STaR model $M_1$. Then we can use $M_1$ as initial model to produce dataset $D_2$ and further\nfine-tune to obtain model $M_2$ in the next iteration.\nThis method can be seen as an offline RL method [36] in the theorem proving MDP. In this MDP,\nthe cumulative reward $R ((s_0, t_0, a_0), (s_1, t_1, a_1), \\dots, (s_n, t_n, a_n)) = 1$ if and only if the proof\ntrajectory is successful. The total expected reward is\n$J(M, D) = \\sum_i E_{(s_0,t_0,a_0), \\dots, (s_n,t_n,a_n)\\sim\\pi_\\mathcal{M} (\\cdot|s_i)} R ((s_0, t_0, a_0), ..., (s_n, t_n, a_n))$,\nand Lean-STaR's expert iteration can be seen as optimizing this reward [36]."}, {"title": "3.3 Evaluation", "content": "Setup. We evaluate the model on formal theorem proving \u2013 given a theorem statement, produce a\ntheorem that is correct according to the formal system. This requires an algorithm for producing a"}, {"title": "4 Experiments", "content": "We instantiate Lean-STaR using the best available open language model pre-trained on the Lean\ncorpus (InternLM2-Math-base-7b [51]), and follow standard practice in using Lean's Mathlib as\nthe underlying training set (via the Lean Dojo dataset [50]). We generate an initial set of thoughts\nfor Mathlib using GPT-4, perform two rounds of expert iteration, then evaluate the model on\nminiF2F [54], the de-facto standard benchmark for evaluating language-model based theorem provers.\nOur experimental results show that both retrospective rationale generation and expert iteration\nsignificantly improve the theorem-proving capabilities of language models in this setting. We\ndescribe our setup and findings in detail below."}, {"title": "4.1 Experimental Setup", "content": "We use LeanDojo Benchmark 4 v9 as the supervised fine-tuning (SFT) dataset containing 231, 240\ndata examples. We fine-tune for 1 epoch to obtain the SFT model. For the learning rate, we use a\nwarmup in the first 20% steps from 0 to 2 \u00d7 10-5, followed by a cosine schedule decaying to zero.\nWe randomly select 17, 256 different successful proof trajectories from LeanDojo Benchmark 4\ndataset [50], and use GPT-4-0125 [32] to annotate 52, 438 thoughts from those proof trajectories. We\nfiltered out all proof steps $(s^i, a^i)$ for which $a^i$ contains the newline symbol \u201c\\n\u201d before annotating.\nWe perform two iterations of expert iteration, and provide the details in Appendix A.1 due to space.\nWe evaluate our method on the MiniF2F benchmark [54]. We use a similar evaluation setting as\nprevious works [50, 44, 51], but use our sampling method instead of best-first search for the evaluation"}, {"title": "4.2 Main Results", "content": "Our main results are reported in Table 1. Lean-STaR gives a significant improvement over the\nprevious state-of-the-art in Lean. For instance, with a similar inference budget, Lean-STaR achieves\n34.8% versus 30.3% in InternLM2 [51] using best-first search and 30.7% in COPRA [39] using\nGPT-4. With a larger compute budget, Lean-STaR's performance improves further to 36.1%.\nThought augmentation improves theorem proving. The first phase of Lean-STaR trains a model\nto interleave thoughts and tactics, by fine-tuning on a synthesized dataset of thought-augmented\nexamples. The fine-tuned model from this phase, denoted LEAN-COT in Table 1, achieves a pass rate\nof 32.8%, which is higher than the model prior to this phase, denoted SFT (29.5%). We conclude\nthat the first phase of Lean-STaR can improve the theorem proving ability of a language model, even\none that is already specialized for generating tactics in Lean such as the SFT model.\nBootstrapping improves thought-augmented theorem proving. The second phase of Lean-STaR\nconsists of generating new thoughts and tactics with the current language model, saving those that\nresult in correct proofs, and training on the union of the initial thought-augmented dataset and the\nsaved examples (i.e., expert iteration [34, 52, 36]). Refer to Appendix A.1 for details.\nWe perform two iterations of expert iteration, and present the results in Table 1, denoted LEAN-STAR.\nEach iteration improves the model's theorem proving performance, from 32.8% (the initial model)\nto 34% (LEAN-STAR after iteration 1) to 34.8% (LEAN-STAR after iteration 2). Furthermore, we\nfind that the model is amenable to further improvement via additional sampling, achieving 36.1%"}, {"title": "4.3 Performance Analysis by Types and Difficulties", "content": "Tasks in minif2f-test are manually formalized from Olympiad type problems, drawn from multiple\nsources including AIME, AMC, IMO problems, and problems from the MATH dataset [21]. These\nproblems can have different levels of difficulty and types. Table 2 reports the number of problems\nsuccessfully proved, partitioned by type and difficulty. We see that Lean-CoT improves performance\nin solving difficult problems on all categories, especially those from mathematics competitions. On\ntop of these improvements, Lean-STaR's improvements come mainly in Number Theory."}, {"title": "4.4 Search and sampling budget", "content": "Table 4 reports the trends of the pass rate against the search size or sampling budget $S \\times K$. We\nfind that Lean-STaR benefits more as $K$ increases, especially when $K$ is relatively large. The result\nsuggests that additional sampling with thoughts improves performance, while additional sampling\nwithout thoughts may saturate. We believe this is because thoughts increase the diversity of outputs\nand contribute to exploration in the theorem proving space. Therefore, Lean-STaR is more scalable\n(in terms of inference-time compute), and may be amenable to further improvements with additional\niterations of expert iteration."}, {"title": "4.5 Experiments with stronger base model and more data", "content": "We instantiate Lean-STaR using a stronger language model (InternLM2-Math-plus-7b [51]), which\nwas released after the experiment above. We follow a similar setup to the previous experiment.\nIn this experiment, we used 140, 000 thoughts annotated by GPT-40 [32] to fine-tune a model (\u201cLean-\nCoT\"). Then we performed only one iteration of expert iteration and collected about 60, 000 (proof\nstate, thoughts, next-tactic) pairs in data, named \u201cSTaR dataset\u201d $D_1$. We further fine-tuned the\nLean-CoT model on dataset $D_1$ to get the Lean-STaR model.\nOur new results are reported in Table 3. We can see that Lean-STaR still gives a significant improve-\nment over the baseline. For instance, Lean-STaR achieves 45.4% versus 39.8% in InternLM-plus\nusing sampling with a similar inference budget and 43.4% using best-first search with more inference\nbudget reported in [51]. This results show that both retrospective rationale generation and expert\niteration can improve the theorem-proving capabilities on a stronger base model."}, {"title": "4.6 Experiments on expert iteration without CoT", "content": "Table 5 shows the result of expert iteration without CoT (i.e., using (state, tactic) pairs only) as well\nas the result of Lean-CoT and Lean-STaR. Expert iteration alone achieves 43.0%, which is less than\nLean-STaR (45.4%). This shows that Lean-STaR's performance gains do not only come from the use\nof expert iteration."}, {"title": "5 Conclusion & Limitations", "content": "In this paper, we presented Lean-STaR, a novel approach that significantly enhances the theorem-\nproving capabilities of language models in formal mathematics by integrating Chain-of-Thought\n(CoT) rationales into each proof step. Our method begins with generating synthetic rationales using\nground-truth tactics retrospectively, followed by fine-tuning the language model to generate these\nrationales and predict subsequent tactics, resulting in the Lean-CoT model. We further improved this\nmodel using expert iteration, fine-tuning it on correct proofs it samples and verifies using the Lean\nsolver. Our contributions include the introduction of the first thought-augmented theorem proving\ndataset, demonstrating that expert iteration can further improve performance, and achieving new\nstate-of-the-art results on the miniF2F-test benchmark, increasing the pass rate from 30.3% to 36.1%."}, {"title": "A Additional Experiment Setup", "content": ""}, {"title": "A.1 Lean-STaR Expert Iteration", "content": "The second phase of Lean-STaR consists of generating new thoughts and tactics with the current\nlanguage model, saving those that result in correct proofs, and training on the union of the initial\nthought-augmented dataset and the saved examples (i.e., expert iteration [34, 52, 36]). We perform\ntwo iterations of expert iteration, and provide details on our specific experimental setup below.\nIn each iteration we use sampling on the LeanDojo Benchmark 4 dataset, and save the (state, thought,\ntactic) examples that are part of successful proofs. For each problem, we sample $K = 32$ times\nin parallel with temperature $T = 1.0$, and limit the number of times a tactic can be generated to a\ntotal of $N = 5$ per problem. Also, sampling is limited to 1 minute per problem. In this setup, each\nproblem needs on average about 0.5 A100 minutes. We collect successfully sampled trajectories to\nproduce a \"STaR dataset\" $D_1$, and up to 3 proof trajectories were collected for each problem. We\ncollected 32, 231 different (proof state, thoughts, next-tactic) pairs in successful proof trajectories\nduring expert iteration, which takes about 4 days with 8 \u00d7 A100 GPUs. Then, we further fine-tune\nSFT model for 1 epoch on the combination of GPT-4 annotated reasoning data and expert iteration\ndata $D \\cup D_1$ to get the Lean-STaR model. We use the same learning rate setup that was used for the\nSFT model. In the second iteration, we generate a dataset $D_2$ in a similar fashion. Then, we chose to\nfurther fine-tune model from iteration 1, $M_1$, on the generated dataset $D_2$ (roughly 19k pairs).\nThe setup of experiment about InternLM2-plus is slightly different. The details are shown in Section\n4.5 and Appendix E."}, {"title": "B Statistics for our methods as well as the baselines", "content": ""}, {"title": "CAn Example and Explanation of A Formal Proof in Lean", "content": "An example of a formal proof in Lean with its visualization is shown in Figure 6, taken from [25]. In\nthe proof, the tactic induction kis is applied to the initial state $(n < m \\Rightarrow n+k\\leq m+ k)$ and\nthe ITP converts the current state to subgoals case 0 case ih: $n \\leq m\\wedge n+k\\leq m+k\\Rightarrow$\n$n + (k + 1) \\leq m + (k + 1)$. The case 0: $n < m$ is our hypothesis ho so it can be proven by case\n0: exact ho tactic. Then, we rewrite the case ih through the nat.succ_le_succ_iff which is\na theorem in Lean library means $n < m \\Leftrightarrow n + 1 < m + 1$. After tactics case 0:exact ho and\ncase ih:rw nat.succ_le_succ_iff, the goal state is converted to $n + k < m + k$ which is the\nhypothesis introduced by induction. Therefore, we can complete this proof using tactic exact k_ih."}, {"title": "E Performance difference of joint training and continue training", "content": "As shown in Table 8, the joint training method performs better using InternLM2-base but training\nmethod performs much better using InternLM2-plus. It seems that there are no difference between\nthese two methods. Therefore, this performance can be depend on the quantity of data or the model.\n(We use much more data when using InternLM2-plus and the quantity of \"STaR data\" is relatively\nsmall.)"}, {"title": "F Retrospective Rationale Generation Prompt of GPT-4", "content": "Please act as a professional mathematician.\nYour goal is to accurately prove a math theorem in Lean4.\nYou are given the first tactic that should be taken to prove the Given Theorem."}, {"title": "G Examples of generated Lean proofs with thoughts", "content": "theorem aime_1990_p4 (x : R) (ho : 0 < x) (h\u2081 : x^2 - 10 * x - 29 \u2260 0)\n(h2: x^2-10x45 \u2260 0) (h3: x^2-10 * x 69) = 0)\n: x = 13 :="}]}