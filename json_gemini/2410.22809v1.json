{"title": "Causality-Enhanced Behavior Sequence Modeling in LLMs for Personalized Recommendation", "authors": ["Yang Zhang", "Juntao You", "Yimeng Bai", "Jizhi Zhang", "Keqin Bao", "Wenjie Wang", "Tat-Seng Chua"], "abstract": "Recent advancements in recommender systems have focused on leveraging Large Language Models (LLMs) to improve user preference modeling, yielding promising outcomes. However, current LLM-based approaches struggle to fully leverage user behavior sequences, resulting in suboptimal preference modeling for personalized recommendations. In this study, we propose a novel Counterfactual Fine-Tuning (CFT) method to address this issue by explicitly emphasizing the role of behavior sequences when generating recommendations. Specifically, we employ counterfactual reasoning to identify the causal effects of behavior sequences on model output and introduce a task that directly fits the ground-truth labels based on these effects, achieving the goal of explicit emphasis. Additionally, we develop a token-level weighting mechanism to adjust the emphasis strength for different item tokens, reflecting the diminishing influence of behavior sequences from earlier to later tokens during predicting an item. Extensive experiments on real-world datasets demonstrate that CFT effectively improves behavior sequence modeling. Our codes are available at https://github.com/itsmeyjt/CFT.", "sections": [{"title": "1 Introduction", "content": "In recent years, there has been growing enthusiasm for developing recommender systems based on Large Language Models (LLMs) [3, 11, 17, 19, 38, 45, 47, 51], with the expectation that LLMs' advanced capabilities could bring a new revolution to recommendation fields. However, the core of recommendation lies in user preference modeling [5], which inherently differs from the language processing tasks LLMs were originally designed for [3]. To bridge this gap, current approaches often construct instruction data from historical user behavior sequences and then use it to fine-tune LLMs [46, 47, 51]. This process equips LLMs with the ability to model user behavior, allowing them to infer user preferences based on the provided behavior sequences and their internalized knowledge. Till now, numerous efforts have been made in this direction, leading to significant progress [1, 2, 20, 23, 51].\nDespite the remarkable progress, we argue that current fine-tuning methods may not fully harness the potential of LLMs in effectively utilizing the behavior sequences. When building the recommender models with LLMs, information unrelated to the input behavior sequence-such as pre-training knowledge (world knowledge) triggered by the task instruction\u00b9-can also be utilized for prediction. This information is simpler to utilize than complex behavior-related information. Consequently, LLMs may tend to over-rely on this information when fitting prediction targets, resulting in insufficient utilization of behavior sequences. One of our findings indicates this potential insufficient utilization: when removing the user behavior sequence from the input, the LLM-based recommender still produced similar recommendation distributions to those generated with the sequence included, as shown in Figure 1.\nGiven the central role of behavior modeling in recommendations, it is crucial to address the problem of insufficient utilization of behavior sequences in LLM-based recommendations. To tackle this issue, we first employ causal language [25] for a qualitative analysis. As illustrated in Figure 2, given the inputs, LLMs can generate predictions through various causal paths, some of which do not originate from the behavior sequence. The insufficient utilization of behavior sequences arises from the tendency to overlook the effects of behavior sequence-originated paths compared to the others. Therefore, to enhance behavior sequence modeling, the key is to identify the effects of these behavior sequence-originated paths, which correspond to the causal effect of behavior sequences on the predictions, and emphasize these effects during the tuning process.\nTo this end, we propose a novel fine-tuning method, which explicitly emphasizes the effects of behavior sequences on LLM predictions during tuning to enhance behavior modeling. Specifically, we leverage counterfactual reasoning [25, 26] to estimate the effects as the difference between normal predictions (i.e., those generated using behavior sequences) and counterfactual predictions (i.e., those generated without them). We then introduce a new task that directly uses the estimated effects to fit the ground-truth labels, explicitly attributing the label occurrences to these effects, achieving the intended emphasis. This task is integrated into the tuning in a multi-task manner [50], allowing us to retain the original task of fitting labels with normal predictions, thereby preserving other valuable information relevant to the task.\nThe main contributions of this work are summarized as follows:\n\u2022 We highlight the issue of insufficient utilization of behavior sequences in existing LLM-based recommendation methods and provide an analysis from a causal perspective.\n\u2022 We propose a novel counterfactual fine-tuning method to enhance behavior modeling in LLM-based recommendations by explicitly emphasizing the effects of behavior sequences on predictions during tuning.\n\u2022 We conduct extensive experiments on several real-world datasets, demonstrating the effectiveness of our method in enhancing behavior sequence modeling."}, {"title": "2 Preliminary", "content": "In this section, we present the problem formulation for the studied recommendation task and the background of the LLM-based recommendation methods this work focuses on."}, {"title": "2.1 Next-item Recommendation", "content": "Next-item prediction is the core task in recommender systems. In this work, we concentrate on this task within a sequential context. Let $D$ represent the collected historical data. We denote a sample in $D$ by $(u, h, y) \\in D$, where $u$ represents a user, $h$ denotes the user's historical interacted items (i.e.,, behavior sequence) up to a given time point, and $y$ signifies the ground-truth next item interacted by the user. Notably, all the items could be represented by their textual information, primarily the title. Each user may have multiple samples in $D$ by considering different interacted items as the next item $y$. Our objective is to train a model based on $D$, which can recommend appropriate items from the total item pool for the next-item interaction, given a user's historical behavior sequence."}, {"title": "2.2 LLM-based Recommender", "content": "Many types of LLM-based recommendation methods have been developed. Among these, fine-tuning LLMs for recommendation in a generative manner is particularly well-suited for the next-item prediction task we defined, and it aligns more closely with the generative nature of LLMs. Next, we present the details of the tuning and inference processes for this type of approach, using a representative method BIGRec [1] as an example.\nTuning. To leverage LLMs for recommendation in a generative manner, this approach typically involves directly fine-tuning the LLMs to generate the next item. The first step is to convert each training example $(u, h, y) \\in D$ into the instruction format shown in Table 1, which consists of two parts: instruction input and instruction output. As indicated in the table, the user data (primarily $h$) and task instruction form the instruction input, denoted as $x_h$, while the ground-truth next item $y$ is directly treated as the instruction output. The LLM is then fine-tuned using this instruction data by optimizing the conditional language modeling objective. Formally, the optimization loss (denoted by $L_n$) can be formulated as follows:\n$L_n = \\sum_{(u,h,y) \\in D} \\sum_{t=1}^{|y|} l(f_\\theta (x_h, y_{<t}); y_t),$ (1)\nwhere $l()$ denotes the Cross-Entropy loss, $f_\\theta()$ denotes the LLM parameterized with $\\theta$; $|y|$ denotes the total number of tokens for $y$, and $y_t$ refers to the t-th token in $y$. Notably, when predicting the t-th token, all preceding tokens in $y$, denoted as $y_{<t}$, are also used as input to the LLM, along with the instruction $x_h$, to generate the prediction for t-th token, represented by $f_\\theta (x_h, Y_{<t})$.\nInference. After fine-tuning, the LLM is expected to have the ability to generate items as recommendations during the inference stage. However, since LLMs can generate creative content, potentially leading to generating nonexistent items. To solve the problem, BIGRec further considers performing a matching mechanism, finding the real items that are mostly similar to the generated ones as the final recommendation. The similarity is measured by the L2 distance between the generated item representations and the actual item representations encoded by the LLMs.\nNotably, most methods in this sub-field share similar fine-tuning processes but differ in how they generate items at inference. For example, D\u00b3 [2] additionally addresses the issue of the amplifying bias toward certain items during generation, and some other works consider directly rejecting non-actual items during the generation process. Since our method focuses on the tuning process, it is broadly applicable across these variations."}, {"title": "3 Methodology", "content": "In this section, we first conduct a causal analysis of the LLM prediction process to establish a foundation for our method design. We then introduce our CFT method, which explicitly emphasizes the effects of behavior sequences on predictions during training to enhance behavior modeling."}, {"title": "3.1 Causal Analysis", "content": "We abstract the process of LLM prediction generation in the causal graph in Figure 2, in which nodes represent the involved variables and edges describe the causal relations between the nodes. We explain the causal graph as follows:\n\u2022 Node $Y_t$ denotes the prediction for the t-th token of next item.\n\u2022 Node H represents the historical behavior sequence in the input of the LLM.\n\u2022 Node I represents all other input information, such as the task instruction and previously generated tokens ($y_{<t}$ in Equation (1)).\n\u2022 Node E represents the pre-training knowledge within LLMs.\n\u2022 Path $\\{H, I\\} \\rightarrow E \\rightarrow Y_t$ represents that H and I can indirectly affect the prediction $Y_t$ through triggering the the pertaining knowledge within LLMs.\n\u2022 Path $\\{H, I\\} \\rightarrow Y_t$ represents that H, I may also directly affect $Y_t$.\nThese paths illustrate how the model utilizes the inputs to produce results through different mechanisms. Notably, the strength of these paths is dynamically learned through data fitting. However, they may encounter different learning challenges - the paths associated with the behavior sequence may present greater difficulties due to the inherent complexity of behavior patterns. Consequently, the model may not fully utilize these paths for prediction, misaligning their true roles in data generation, i.e., insufficiently leveraging the behavior sequence. To enhance the behavior sequence modeling, we need to enhance the effects of paths related to behavior, i.e., the effects of the behavior sequence, on model prediction.\nCausal effects of behavior sequence. Based on the causal graph and causal inference theory [25], the causal effect of a sample's behavior sequence h on the prediction $Y_t$, conditioned on a given I, can be expressed as follows:\n$P(Y_t|H = do(h), I) \u2013 P(Y_t|H = do(0), I) = P(Y_t|H = h, I) \u2013 P(Y_t|H = 0, I),$ (2)\nwhere $H = do(h)$ represents intervening H as h, and $H = do(0)$ represents intervening H as \"None\". $P(Y_t|H = h, I)$ denotes the normal predictions, while $P(Y_t|H = 0, I)$ denotes the counterfactual result obtained by assuming the user has no historical behavior sequence."}, {"title": "3.2 Counterfactual Fine-Tuning", "content": "Based on the causal analysis, we propose the Counterfactual Fine-Tuning (CFT) method, to enhance behavior sequence modeling in LLMs. CFT generally follows the paradigm of tuning LLMs to predict the next item but explicitly emphasizes the influence of behavior sequences on predictions during the training process, by introducing a new task. As shown in Figure 3, our approach consists of two main components:\n\u2022 Multi-task Tuning: The core of our method lies in introducing a new task that directly uses the effect defined in Equation (2) to fit the training data. This new task emphasizes learning the effects of behavior sequences during data fitting, improving the model's utilization of behavior sequence information. We introduce this new task in a multi-task manner, retaining the original tuning task in Equation (1) to preserve other valuable information.\n\u2022 Token-level Weighting: We apply a token-level weighting mechanism\u00b2 to adjust the strength of the new task loss, aligning the fact that behavior sequences have varying levels of influence on tokens at different positions.\n3.2.1 Multi-task Tuning. When fine-tuning LLMs, we additionally introduce a new task of directly using the effect of behavior sequence on model predictions to fit data, combining it with the task of using the norm predictions to fit data. Let $L_c$ denote the loss for our new task (termed causal loss), and $L_n$ denote the loss for the original task (termed normal loss). The multi-task tuning is performed by optimizing the following combined loss function:\n$L = L_n + \\lambda L_c,$ (3)\nwhere $L$ denotes the combined loss, and $\\lambda \\ge 0$ is a hyper-parameter to control the weight of the causal loss.\nCausal Loss $L_c$: The new task leverages the causal effect of behavior sequences to fit the data. To achieve this, we need to identify these effects, as outlined in Equation (2). Since this equation is defined from a probabilistic perspective, we must convert it into an empirical form for practical application. For a sample $(u, h, y) \\in D$, the empirical representation of the effects for the t-th token prediction is given by:\n$f_\\theta(x_h, y_{<t}) - f_\\theta (x_0, y_{<t}),$\nwhere:\n1) $f_\\theta(x_h, y_{<t})$ represents the normal prediction for $y_t$, obtained by using the the instruction input $x_h$ built on the user's behavior sequence. It corresponds to $P(Y_t|H = h, I)$ in Equation (2).\n2) $f_\\theta (x_0, y_{<t})$ represents the counterfactual prediction for $y_t$, corresponding to $P(Y_t|H = 0, I)$ in Equation (2). It is obtained by assuming the user has no historical interactions, which means the <His_Behavior_Seq> field in the instruction template (Table 1) is set to \"None\", forming the corresponding instruction input $x_0$ without the behavior sequence h.\nAfter obtaining the effects, the causal loss $L_c$ is formulated as:\n$L_c = \\sum_{(u,h,y) \\in D} \\sum_{t=1}^{|y|} \\frac{w_t}{\\Omega} l(f_\\theta (x_h, Y_{<t}) - f_\\theta (x_0, Y_{<t}); y_t),$ (4)\nwhere $l$ still denotes the Cross-Entropy loss, $w_t$ represents the token-level weight that will be explained later, and $\\Omega$ is the sum of $w_t$ across all predicted tokens. As shown in the equation, the task emphasizes attributing the occurrences of $y_t$ to the behavior sequence's effects, thereby explicitly enhancing the utilization of the behavior sequence.\nNormal Loss $L_n$: The other task still uses the normal prediction $f(x_h, y_{<t})$ to fit the data, as done by existing work described in Section 2.2. So the loss $L_n$ can be computed following Equation (1). A little differently, since the later tokens are easier to learn due to their lower uncertainty, we can also use a similar weighting mechanism to Equation (4) to assign lower weights to these tokens during implementation.\n3.2.2 Token-level Weighting. For a sample $(u, h, y) \\in D$, the behavior sequence should have varying levels of influence when predicting different position tokens in $y$. As more prefix tokens are generated, the later item tokens become increasingly definitive by nature\u00b3, and even for some tokens, they may almost be entirely definitive given the prefix tokens. That means, the later tokens are less influenced by the behavior sequence and are primarily determined by the item's prefix tokens. In such cases, we should also make sure that, for the later tokens, the behavior sequence shows fewer effects during data fitting. Therefore, we design a token-level weighting mechanism that dynamically assigns decreasing weights from the first to the last item token on the corresponding loss.\nSpecifically, for each $y$, we use a linear decay mechanism to set weights for item tokens based on their position. The first token of $y$ is assigned the highest weight, set to 1, and the last token is assigned the lowest weight, set to $\\beta (\\in [0, 1])$. The weight for fitting the t-th token in $y$ is formulated as follows:\n$w_t = 1 - \\frac{(1 - \\beta) (t - 1)}{|y|-1},$ (5)\nwhere $|y|$ represents the total number of tokens in $y$, and $\\beta$ is a hyper-parameter controlling the lowest weight among the item's tokens. The weight decreases by $\\frac{1-\\beta}{|y|-1}$ with each successive position. This weighting mechanism effectively assigned lower weights to the later tokens. The weights can be directly used by Equation (4), which naturally has a normalization mechanism.\nAlgorithm 1 outlines the pseudo-code for the tuning process in our CFT. In each iteration, we first compute the normal predictions $f_\\theta (x_h, y_{<t})$ and the counterfactual predictions $f_\\theta (x_0, y_{<t})$ (lines 2-3). Next, we calculate the normal loss using the normal predictions, as defined in Equation (1) (line 4), and use the difference between the two predictions to determine the causal effect, which is then employed to compute the causal loss in Equation (4) (line 5). Finally, we combine both losses to update the model parameters (line 6)."}, {"title": "4 Experiment", "content": "In this section, we conduct a series of experiments to answer the following research questions:\nRQ1: How does CFT perform on real-world datasets compared to traditional and LLM-based sequential recommendation methods?\nRQ2: What is the impact of the individual components of CFT on its effectiveness?\nRQ3: How does CFT influences the recommendation distribution?\nRQ4: How does the backbone LLM choice and dataset selection influence the effectiveness of CFT?"}, {"title": "4.1 Experimental Settings", "content": "4.1.1 Datasets. We conduct experiments on three datasets from the Amazon Product Review benchmark [24]: CDs, Games, and Books. These datasets represent different domains and contain user interactions (reviews) on products from the Amazon platform, spanning from May 1996 to October 2018.\nWe fully follow the setting in D\u00b3 paper [2] to pre-process these datasets. Specifically, due to the high computational cost of training LLMs, we limit the data to interactions from a single year (October 2017 to October 2018). We then apply a 5-core filtering to ensure that each user/item has a minimum of 5 samples. Subsequently, we split the data into training, validation, and test sets based on the timestamps of the interactions, with an 8:1:1 ratio. This chronological partitioning ensures that the testing interactions occur after all training and validation interactions, thereby preventing information leakage [12]. More preprocessing details could refer to the original paper of D\u00b3 [2].\n4.1.2 Evaluation Settings. To evaluate recommendation performance, we employ two widely recognized metrics: Hit Ratio (HR@K) and Normalized Discounted Cumulative Gain (NDCG@K), with $K \\in \\{5, 10\\}$. HR@K measures whether the ground-truth item is included in the top-K recommendations, while NDCG@K assesses the ranking quality by considering the relative order of the ground-truth item within the top-K list. Higher values for both metrics indicate better performance. In our evaluation, these metrics are computed using the all-ranking protocol [1], where all items that a user has not interacted with are treated as potential candidates. Additionally, during testing, the interactions immediately preceding the test interaction, including the one at testing sets, are included in the user's historical behavior sequence to input to the model, similar to prior work [2].\n4.1.3 Compared Methods. To demonstrate the superiority of our method, we compare it against the following traditional sequential methods (Caser, GRU4Rec, SASRec) and LLM-based methods (BIGRec, D\u00b3):\n\u2022 Caser [31]. This is a famous sequential recommendation approach that employs Convolutional Neural Networks (CNNs) to encode sequential patterns for modeling user preferences.\n\u2022 GRU4Rec [10]. This is another famous method that employs Gated Recurrent Units (GRU) to encode sequential patterns for modeling user preferences.\n\u2022 SASRec [13]. This is a highly representative sequential recommendation method that employs the self-attention network for user preference modeling.\n\u2022 GRU4Rec* [10]. This is a variant of GRU4Rec that initializes the item embeddings in GRU4Rec using those encoded by LLMs.\n\u2022 SASRec* [13]. This is a variant of SASRec that initializes embeddings in SASRec using those encoded by LLMs.\n\u2022 BIGRec [1]. This is a representative LLM-based recommendation method that fine-tunes LLMs to generate the next items based on input behavior sequences, as introduced in Section 2.2.\n\u2022 D\u00b3 [2]. This is a state-of-the-art LLM-based recommendation method. It follows a similar fine-tuning process to BIGRec but differs during inference. Specifically, it mitigates the amplification bias toward certain items by removing length normalization in LLM beam search decoding.\nFor our method, we implement two variations by applying CFT for tuning while using the inference processes from BIGRec and D\u00b3. We refer to these implementations as BIGRec+CFT and D\u00b3+CFT.\n4.1.4 Implementing Details. For all LLM-based methods compared, we use Qwen2-0.5B [42] as the backbone LLM. When tuning models, we use the AdamW [22] optimizer with a batch size of 64, a learning rate of 1\u00d710-4, and a dropout rate of 0.05. Model selection is based on validation loss, using an early stopping strategy with a patience of one epoch. Other settings generally follow those in the D\u00b3 paper. For our CFT method's \u03bb, which controls the weight of the causal loss, is tuned in the range {0.01, 0.02, 0.025, 0.05, 0.1, 0.2, 0.3}. For our method's \u03b2, which controls token-level weights, we introduce another hyper-parameter \u03b2' to facilitate implementation, where \u03b2 = 1 - 1/\u03b2', and we tune \u03b2' within {1.1, 1.2, 1.4, 1.6, 2, 3, 10, 25}4. Due to the high cost of tuning LLMs, we avoid grid search. Instead, we first identify the general scale of a hyper-parameter and then adjust it within a narrower range. For all traditional methods, we strictly follow the settings in the D\u00b3 paper [2].\nFor BIGRec's inference process, we adjust the original method, which generates a single item and matches it with actual items to form the top-K recommendation list. Instead, we generate five items. For each of these generated items, we find the most closely matched actual item and combine them to create the Top-5 recommendation list.\nSpecifically, we saw an increase in NDCG@5 from 0.041 to 0.078 on the CDs dataset for BIGRec"}, {"title": "4.2 Performance Comparison (RQ1)", "content": "We begin by assessing the overall recommendation performance of the compared methods. The summarized results are presented in Table 3, where the results of traditional methods are sourced from the D\u00b3 paper [2], from which we draw the following observations:\n\u2022 One of our CFT implementations (D\u00b3+CFT) consistently outperforms the baselines across all evaluation metrics on all datasets. This verifies the superiority of our CFT.\n\u2022 For LLM-based methods, the performance of BIGRec+CFT surpasses that of BIGRec, achieving an average relative improvement of 9.8% across all metrics and datasets, while D\u00b3+CFT surpasses D\u00b3 with an average relative improvement of 9.5%. This observation indicates the validity of our causal analysis, suggesting that existing methods may inadequately leverage behavior sequences, leading to sub-optimal performance. Furthermore, it highlights the effectiveness of CFT in enhancing behavior sequence modeling in LLMs by emphasizing the influence of behavior sequences on predictions.\n\u2022 Traditional recommendation methods exhibit poor performance. Although incorporating LLM embeddings for initialization offers some improvements in most cases, a significant gap still exists compared to LLM-based recommendation methods. This demonstrates the advantages of utilizing LLMs as recommendation models on these datasets.\n\u2022 In most cases, D\u00b3 outperforms BIGRec, with only a slight decline in performance for some metrics on Books. This generally aligns with the observations in the D\u00b3 paper (the version without ensemble). However, in our results, the performance improvements of BIGRec (and D\u00b3) over traditional methods are significantly larger than those reported in the D\u00b3 paper."}, {"title": "4.3 Ablation Study (RQ2)", "content": "To enhance behavior sequence modeling in LLM-based Recommendations, CFT includes two key designs: a new task (using the causal effects of behavior sequences to fit data) and a token-level weighting mechanism. To validate the rationale behind these design decisions, we conduct a comprehensive evaluation by systematically disabling each component of BIGRec+CFT to create several variants. Specifically, the following variants are introduced:\n\u2022 w/o CL. This variant disables the new task by setting the weight of the causal loss in Equation (3) to zero. Notably, the token-level weighting mechanism is still applied to the normal loss.\n\u2022 w/o TW. This variant disables the token-level weighting mechanism independently, which is equivalent to setting the hyper-parameter $\u03b2$ in Equation (5) to one.\n\u2022 w/o Both. This variant removes both components mentioned above, which is equivalent to the vanilla BIGRec method.\nTable 4 illustrates the ablation results of the BIGRec+CFT method, from which we draw the following observations:\n\u2022 Removing the new task (w/o CL) leads to a significant performance decline, confirming the impact of the introduced causal loss and underscoring the importance of emphasizing the influence of behavior sequences on model prediction for enhancing the utilization of behavior sequences.\n\u2022 Disabling the token-level weighting mechanism (w/o TW) also results in a performance decline, confirming that it plays a crucial role in fully unlocking the potential of our method, by aligning the fact that behavior sequences have varying levels of influence on different tokens.\n\u2022 Comparing the impact of disabling the new task (w/o CL) versus disabling the token-level weighting mechanism (w/o TW), disabling the new task results in a much more significant performance decline, indicating that the new task plays a more fundamental role in our method.\n\u2022 Comparing the variant w/o CL and the variant w/o Both, the w/o CL variant, which applies token-level weighting to the normal loss, still brings some improvements in most cases. This verifies that for the normal task, different tokens may also have different learning difficulties during tuning.\nThese results demonstrate that leveraging the effects of behavior sequences to fit data is central to our method; however, fully unlocking its potential also depends on integrating other designs."}, {"title": "4.4 In-depth Analysis (RQ3 & RQ4)", "content": "In this subsection, we first analyze how CFT influences the recommendation distribution by comparing it to BIGRec, answering RQ3. Then, we investigate the impact of the backbone LLM choice and dataset selection on CFT's effectiveness, addressing RQ4.\n4.4.1 Recommendation List Analysis. We first conduct a study to analyze the impact of CFT on LLM recommendations. To do this, we compare the distribution of recommended items between BIGRec and our CFT implemented on BIGRec (BIGRec+CFT). Specifically, we categorize items into groups based on their popularity and calculate the proportion of recommendations each group receives in the final recommendation list, that generated by BIGRec and BIGRec+CFT in the case with and without inputting behavior sequences. We draw the comparison results in Figure 4, where the item group with higher popularity has a higher index. From the figure, we draw the following observations:\n\u2022 Focusing on comparing recommendations generated by CFT and BIGRec when inputting behavior sequence, we find that CFT can produce more balanced recommendations among the different item groups - reducing the recommendation to the popular items and increasing the recommendation towards the unpopular items. This aligns with an intuition that - if the behavior sequence (personalization information) has not been fully utilized, the model may tend to recommend common popular items after tuning. The result somewhat shows that our method can leverage the behavior sequence more.\n\u2022 Focusing on cases without historical behavior sequences, we observe that CFT introduces notable changes compared to BIGRec. In particular, for the Book dataset, when behavior input is absent, CFT shifts towards recommending a large number of unpopular items that users are less likely to consume, significantly misaligning with the results when full behavior input is provided. This demonstrates that CFT reduces the model's reliance on non-behavioral knowledge when making recommendations."}, {"title": "5 Related Work", "content": "In this section, we discuss related work on LLM-based recommendation and causal recommendation."}, {"title": "5.1 LLM-based Recommendation", "content": "Given the significant and widespread success of large language models (LLMs), the recommendation community has expressed great enthusiasm for adapting LLMs to recommendation tasks. Current explorations can be divided into three main categories: 1) optimizing prompts or leveraging in-context learning to inspire the capabilities of LLMs for recommendation better [7, 29, 33]; 2) employing an agent paradigm to utilize the planning and reasoning abilities of LLMs for recommendations [37, 44, 45]; and 3) tuning LLMs based on recommendation data to align them with the recommendation task, enhancing their recommendation abilities via model updates [1, 3, 47]. Among these approaches, tuning methods have garnered the most attention and are more relevant to this paper, thereby we mainly discuss this type of method.\nRegarding the research of tuning, early research predominantly focused on a discriminative approach [3, 14], where candidates are provided to LLMs to assess user preferences. This method has certain drawbacks, particularly due to the high costs associated with all-ranking [1]. To better leverage the generative capabilities of LLMs, some studies have emerged that directly tune large models to generate items, including works like BIGRec [1] and GPT4Rec [16]. Following these two lines of research, new exploration directions have arisen that address the problems that have more recommendation characteristics. For instance, some studies investigate how to better incorporate collaborative information into large model recommendations [47, 51, 53], and some studies focus on how to better represent items within LLMs [20, 30, 34]. Additionally, there are efforts aimed at developing decoding methods suitable for LLMs [2] or addressing challenges related to long-sequence modeling [39, 52], or accelerating LLM-based recommendations [21, 40]. However, to our knowledge, we are the first to utilize causality to enhance the behavior sequence modeling for LLMs."}, {"title": "5.2 Causal Recommendation", "content": "Causality has a long history of application in recommendations, primarily focusing on addressing bias issues [6, 18]. Initially, inverse propensity scores were widely employed for debiasing, where the core idea is to adjust the training distribution to be unbiased by reweighting training samples with propensity scores [15, 28, 41]. Subsequently, causal interventions based on do-calculus have been utilized to tackle various bias problems brought by the existence of confounders, such as popularity bias [8, 48], duration bias [43], confounding features [9], and amplification bias [35]. Additionally, some studies leverage counterfactual inference to address bias issues; for instance, CR [36] and CVRDD [32] utilize counterfactuals to tackle clickbait and duration bias, respectively. All these works are centered around traditional recommender systems. Our research significantly differs from theirs. First, we specifically focus on LLM-based recommendations. Second, we address the insufficient utilization of behavior sequences, a challenge encountered during the fine-tuning of LLMs for recommendations, rather than the bias issues explored in prior research. From a technical perspective, our approach significantly diverges, as it is tailored to LLMs, incorporating token-level weighting into the design."}, {"title": "6 Conclusion", "content": "In this work, we demonstrated that the existing LLM-based recommendation methods may suffer from the issue of insufficient utilization of behavior sequences. We provided a causal analysis of this problem and proposed a Counterfactual Fine-Tuning (CFT) method to enhance behavior sequence modeling. The core of our CFT approach involves introducing a new task that leverages the effects of behavior sequences to directly align with data labels. With a token-level weighting mechanism, the task could help explicitly emphasize the role of behavior sequences in model predictions. Extensive results validated the effectiveness of our method.\nIn current experiments, we focused exclusively on the LLM-based paradigm of tuning models to generate the next items based on textual information. In the future, we will explore our method within other frameworks, such as tuning LLMs to generate matching scores. We also plan to explore the issue in scenarios where additional personalization information-beyond behavior sequences, such as encoded collaborative embeddings [49]-is utilized."}]}