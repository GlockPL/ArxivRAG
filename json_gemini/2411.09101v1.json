{"title": "Heuristical Comparison of Vision Transformers Against Convolutional Neural Networks for Semantic Segmentation on Remote Sensing Imagery", "authors": ["Ashim Dahal", "Saydul Akbar Murad", "Nick Rahimi"], "abstract": "Vision Transformers (ViT) have recently brought a new wave of research in the field of computer vision. These models have done particularly well in the field of image clas-sification and segmentation. Research on semantic and instance segmentation has emerged to accelerate with the inception of the new architecture, with over 80% of the top 20 benchmarks for the iSAID dataset being either based on the ViT architecture or the attention mechanism behind its success. This paper focuses on the heuristic comparison of three key factors of using (or not using) ViT for semantic segmentation of remote sensing aerial images on the iSAID. The experimental results observed during the course of the research were under the scrutinization of the following objectives: 1. Use of weighted fused loss function for the maximum mean Intersection over Union (mIoU) score, Dice score, and minimization or conservation of entropy or class representation, 2. Comparison of transfer learning on Meta's MaskFormer, a ViT-based semantic segmentation model, against generic UNet Convolutional Neural networks (CNNs) judged over mIoU, Dice scores, training efficiency, and inference time, and 3. What do we lose for what we gain? i.e., the comparison of the two models against current state-of-art segmentation models. We show the use of the novel combined weighted loss function significantly boosts the CNN model's performance capacities as compared to transfer learning the ViT. The code for this implementation can be found on https://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.", "sections": [{"title": "I. INTRODUCTION", "content": "The introduction of Transformers [23] has changed the research landscape when it comes to attention mechanisms on Natural Language Processing (NLP) tasks. However, this potential wasn't fully capitalized in computer vision tasks until recently when Dosovitskiy et al. implemented the attention mechanism of transformers in their seminal paper [5] ever since ViT has been one of the fundamental areas of research in computer vision. Although initially proposed for image classification tasks, like CNNs in their inception days, they soon turned out to be one of the best-performing architectures for image segmentation tasks as well.\nImage segmentation refers to the task of classifying each pixel of an image into a category. It can be said that image segmentation is a type of classification as well, but the role, approach and method of each subjects varies in themselves. In specific to the scope of this paper, semantic segmentation means to group certain objects in an image to a class among the given n classes in the dataset. Instance segmentation, similarly, would be to segment each object present in an image as a distinct item in itself.\nAs with any deep learning computer vision tasks, image segmentations were best done by models that were capable of capturing, encoding, and decoding essential patterns of the input image, mainly the implementation of UNet style architectures in CNNs [1], [19], [31], [33], [36], [37]. Specific to the scope of this paper, in the past few years researchers have utilized the UNet CNNs in the iSAID dataset [32], which is built on top of the DOTA [29] dataset, to do semantic segmentations [4], [18], [25], [38]. One of the main pitfalls of such datasets is the background class [11]. If not handled properly during the training process, there is a good chance to see high evaluation metrics on mIoU since the model would easily overfit on the most common class, which is the unlabelled class in this case.\nAlthough the traditional deep learning technique of em-ploying a UNet-based Convolutional Neural Network (CNN) remains the cornerstone of many segmentation tasks, recent trends in computer vision research indicate a significant shift towards Transformer-based architectures, particularly the Vi-sion Transformer (ViT) and its variants. This shift is driven by the inherent ability of Transformer models to capture long-range dependencies through self-attention mechanisms, a feature that CNNs typically struggle with due to their localized receptive fields. The increasing preference for ViT models is exemplified by the fact that among the top 20 benchmark models for the iSAID dataset as listed on Papers with Code [?], the top five employ either ViT, attention-based CNNs, or a hybrid combination of both [6]\u2013[9]. These models leverage the powerful self-attention mechanism to refine segmentation masks by focusing on relevant image regions.\nIn this paper, we introduce a novel loss function that inte-grates maximizing mean Intersection over Union (mIoU) and Dice score, while preserving entropy to ensure robust mask predictions. This new loss function is integrated into the UNet framework to improve its ability to model complex spatial relationships in images. This unique formulation allows for better generalization to unseen data by maintaining a balance between maximizing overlap with the ground truth masks and preventing over-segmentation. In addition, we investigate various data augmentation techniques since the number of samples is relatively lower in the iSAID [32] dataset. In parallel, we provide a direct comparison between training a UNet CNN model from scratch and fine-tuning Meta AI's widely adopted MaskFormer [2], a ViT-based model that has achieved state-of-the-art results in semantic segmentation tasks; opensourced in Hugging Face [26].\nThis paper focuses on three key objectives throughout its experimentation and analysis:\n\u2022 Propose a combined weighted loss function to maximize mIoU and Dice while preserving entropy\n\u2022 Analyze the impact on efficiency during training and inference in both architectures\n\u2022 Benchmark and test inference capabilities of both models against current state-of-the-art iSAID benchmarks on unseen data\nThe rest of the paper is laid out in the following order, section II contains the current state of art and previous work on the field. Section III discusses our approach to the given objectives, then IV effectively binds the results with the research question. Section V would then conclude the paper with some ending thoughts and future direction for the research field."}, {"title": "II. LITERATURE REVIEW", "content": "Noh et al. [37] first proposed the UNet style model by train-ing a deep deconvolutional layer on top of the VGG-16 [20] CNN. Their work provided a path for later iterations of UNet-based systems on the iSAID dataset. Regmi [18] presented their unsupervised model FreeSOLO for the iSAID dataset as a state-of-the-art. They deploy unsupervised learning, such as segmentation on the iSAID and other remote imagery data. However, the author fails to recognize the disparity between the claims and the results. The model fails to acknowledge small items on the dataset. This mainly comes from the author's choice of image preprocessing which is to bilinearly transform the image into (3, 256, 256) pixel inputs. This is not viable for the iSAID dataset because the pictures in the datasets range from size (3, 800, 800) to (3, 4000, 13000) and downsampling in such cases would result in the loss of a lot of necessary information and pattern that the model would need to make a robust prediction on all images. The results presented by the author also present some doubts since the AP50 percentage is not scaled to the standard value, and a score of 0.9% to 3.5% would rather indicate a poor model by the accepted standards. Also the Dice score and the IoU score are only presented for the backbone models, which too are below 65%, which is within range of the current state of the art ViT models and what our UNet CNN and MaskFormer models have surpassed.\nIn [25], Wang et al. show the best results obtained by the CNN models and ViT models after fine-tuning on seminal work by Xiao et al.; the UperNet model [30]. Both of their top results, on UNet and ViT, in the iSAID dataset come from fine-tuning the UperNet model. The best score for the CNN approach comes from fine-tuning the UperNet model pre-trained on the imagenet [3] dataset using a ResNet-50 [10] backbone whereas their best IoU score for ViT, which is also the overall best, comes form fine-tuning the UperNet model on the same dataset with the ViTAEv2-S [34] backbone; the scores were 62.54 and 66.26 respectively. While it is an impressive feat to have achieved such a high average IoU score in the iSAID dataset, we believe and show that the results could be improved for a much higher mIoU score. The authors also don't mention the Dice scores for their experiments. While the Dice score in most cases is very similar, or within the 5% range, to the mIoU, having a fixed number would have been better for direct comparison with other approaches.\nAfter the introduction of ViT [5], papers like [9] [24] have emerged in the iSAID dataset, which focuses on using the ViT in their approach. Papers like [7] and [8], however, argue that using attention-based CNNs are more effective towards the segmentation task. The authors of [24] used ViT in their approach and subsequently got higher results than [25] on the IoU score for the iSAID dataset: 67.20. They too used the UperNet as their training method, with the RingMo [22] as the train and the Swin-B ViT [15] as their backbone instead. This is impressive, but their model has 100 million parameters. We further prove that half of this is enough with our combined loss function to gain higher results on the dataset.\nHanyu et al. [9] proposed a 3 ViT-based models: AerialFormer-T, AerialFormer-S, and AerialFormer-B with 42.7M, 64.0M, and 113.8M trainable parameters, respectively. Their models achieve the mIoU score of 67.5, 68.4, and 69.3 in the given three models. This score is higher than the one reported by [24] [25]. Even though the mIoU scores are highly desirable, the number of parameters in the AerialFormer-S and the AerialFormer-T are large with GLFLOPs of 49.0, 72.2, and 126.8, respectively, for the tiny, small, and base model. With respect to [24] though, this is an incredible feat as the AerialFormer-T with 42.7M parameters outperformed the one in [24] with 100M parameters. The authors don't mention the Dice scores for direct comparison, and we propose that our proposed model with 42M parameters can outperform the 113.8M parameters model with the help of our combined loss function. We also show that by using MaskFormer ViT as well, we can surpass the larger model.\nLiu et al. [27] proposed a CNN method for image seg-mentation on remote sensing images by dual path semantics approach. The authors devised a technique to make a new"}, {"title": "III. METHODOLOGY", "content": "As shown in Fig. 1, the training lifecycle for our experiment includes five key steps: 1. Dataset Information 2. Data Aug-mentation, 3. Models, 4. Loss Function 5. Validation Metrics, and 6. Hyperparameters and Training Settings\nA. Dataset Information\nThe iSAID dataset [32] is a benchmark in the remote sensing community due to its complex nature. The dataset is made of 2806 images from the DOTA dataset [29]. Out of these, 1411 images are training images, 458 validation images, and 937 unlabelled testing images. The resolution of the images range from 800 \u00d7 800 pixels to 4000 \u00d7 13000 pixels with 15 foreground and one background category. We only consider the foreground category while calculating our validation metrics scores.\nB. Data Augmentation\nThe data augmentation process followed the procedure shown in Algorithm 1. The algorithm first takes anywhere between 6-28% of the image and resizes it to the size of (512, 512) pixels. It then randomly flips the horizontal and vertical axis with a 50% probability and rotates the image anywhere between (0 - 360)\u00b0. Random brightness or contrast is added to the image and is normalized with the mean and standard deviation from the ImageNet dataset. The first step in the process ensures with maximum probability that each of the images we generate from any given image would be either scaled appropriately, have unique features, or both.\nC. Models\nWe train two models in total. The first one is the UNet based on CNN, and the second one is the fine-tuned MaskFormer ViT.\nWe use the gradient clipping algorithm by Pascanu et al. [17] to account for exploding gradients in both of our models. Through the heuristic approach, we concluded the maximum value of threshold in algorithm 2 could be set to 3.0.\nEach model is different in its fundamental properties and are described below.\n1) UNet CNN: Our version of the UNet architecture is a simple adaptation of a generic CNN-based UNet model with four skip connections. The overall architecture of the UNet CNN model is described in Fig. 2. We describe an encoding block and use it throughout the model to do all the feature extraction in both the convolution and transpose convolution layers. The total number of trainable parameters amounts to 42.9M, with the most number of parameters situated in the bottleneck layers. We use gradient accumulation to mimic a batch size of 128 images and use the mixed precision technique with 16 16-bit floats to do the forward pass. These two techniques make the forward process more efficient by skipping backpropagation for n accumulation steps and calculating the predicted mask with just 16-bit precision.\n2) MaskFormer ViT: MaskFormer [2] is an algorithm that infuses pixel decoder and transformer decoder with a pre-trained backbone, usually a ResNet [10] block. The image is fed into the backbone, and the resultant is then put forward on the pixel decoder and transformer decoder. The pixel decoder would produce per-pixel embeddings, and the output from the transformer is sent to an MLP. The MLP would produce two results: the N class classification and N mask embeddings. The N mask embeddings are then combined with the per-pixel embeddings, and the output is combined with the N class classification to get the final segmentation. This is represented by Fig 3 extracted from the original paper [2]. They employ their own loss function, which is a combination of the Dice loss function and focal loss function [14]. The number of parameters depends upon the backbone used and ranges all the way from 41M to 212M parameters. For our experiment, we used Facebook's Swin large architecture, pre-trained on imagenet22k, and fine-tuned for the 15 classes in the iSAID dataset, with ~200M parameters; since most of the parameters are frozen and only the top layer trained, the training efficiency was not affected like the UNet CNN counterpart. The inference time and Floating Operations Per Second (FLOPS) were, however, adversely affected by the larger number of parameters in the model.\nD. Loss Function\nTake an image mask A and probability distribution, or the predicted probability, of the mask B such that A, \u0412 \u2208 RC*H*W where C, H and W are the channel width and height of the mask respectively. Then in order to maximize the mIoU we minimize 1 \u2013 IoU as our loss function [35].\n$L_{iou} = 1 - \\frac{A \\cap B}{A \\cup B}$ (1)\nIn order to make the loss function differentiable, we replace the nondifferentiable operations of the bitwise intersection (AND) operation and the union (OR) operation with multi-plication and addition operations.\n$L_{iou} = 1 - \\frac{A * B}{A + B - (A * B)}$ (2)\nSimilarly, in order to maximize the Dice score, we need to minimize 1 Dice score. Since the bitwise intersection function is not differentiable, we replace the intersection with equivalent multiplication, so the loss function Ldice becomes the following [21]:\n$L_{dice} = 1 - \\frac{2*A*B}{A+B}$ (3)\nWe also use a weighted cross-entropy loss function to maintain the entropy of our predictions, so the third part of our function becomes Lce [16].\n$L_{ce} = \\beta * A log(B) + (1 - \\beta)(1 - A) log(1 - B)$ (4)\nwhere \u1e9e is the weight hyperparameter (0.15 for unlabelled class and 1 for the rest),\nCombining the loss functions (2) (3) (4) together with weights \u03bb\u03af\u03bf\u03c5, Adice, Ace respectively we get the total combined loss L.\n$L = \\lambda_{iou} * L_{iou} + \\lambda_{dice} * L_{dice} + \\lambda_{ce} * L_{ce}$ (5)\nwhere \u03bb\u03af\u03bf\u03c5 = 0.8, Adice = 1 and Ace = 10 were selected by trial and error experimentation.\nWe also did not calculate the loss functions Liou and Ldice for the unlabelled class, unlike with Lce because during inference, we need not care about labeling the unlabelled class properly but still the model needs some hint of guidance of the types of patterns it needs to avoid during the training phase, so comes the minimal 0.15 value of \u1e9e in Lce. This loss function was only applied to the UNet CNN since the Maskformer model had its own dice and pixel classification loss described in the original paper.\nE. Validation Metrics\nWe validate the results produced by our model for C classes using the mIoU score and Dice score described as follows [35] [21]:\n$mIoU = \\frac{1}{C} \\sum_{i=0}^{i=C} \\frac{Ain Bi}{Ai U Bi}$ (6)\n$Dice = \\frac{1}{C} \\sum_{i=0}^{i=C} \\frac{2 * (A \\cap Bi)}{A+ Bi}$ (7)\nF. Hyperparameters and Training Settings\nAdam [13] was chosen as the optimizer for both models for its capability of fine-tuning the learning rate over time. We trained the models for 40 epochs, each with an initial learning rate of 10-3 on a virtualized Nvidia A100 GPU with 48G VRAM. The models were validated at the end of each epoch under the Dice score and IoU scores described in (6)(7)."}, {"title": "IV. RESULTS", "content": "Our findings based on the methodology graph shown in Fig 1 are presented in this section. First, we list out the two model's efficiency information in Table I. During training, most of the time was consumed by the data augmentation technique described in III-B, which took 65 seconds per batch for a batch size of 128. During inference on six images, however, we can see that having more numbers of parameters hurts the efficiency of the MaskFormer model. The UNet CNN had a FLOPS of 460G, whereas the MaskFormer had a FLOPS of 232.4 G. The higher number of FLOPS and lower inference time for the UNet CNN demonstrates its capacity to perform more operations per second even faster, thus resulting in a more efficient model overall.\nThe MaskFormer ViT model churned better results in terms of mIoU and Dice scores Fig 4. The Dice Score and mIoU for the MaskFormer soared from mid 70% to the range of 80% by the end of the last epoch, whereas for UNet CNN, it started low on around 65% and reached the peak of 81% and stablized at around 78% during training over all classes Fig. 4. The Dice score and mIoU scores have a strong correlation between them, so we further compare the mIoU scores of the model against our references and recent research works. In Table II, recently published and reference works in the topic were selected whose models either have a comparable number of parameters, model architectures, or both as ours to be compared with respect to the mIoU scores and per category IoU scores.\nIt can be noted that the mIoU and Dice scores for the MaskFormer ViT are within the 10% upper range of the UNet CNN model, even while having 5 times more parameters in it. From further analysis of the Fig 5, however, we can notice that even though the Maskformer has higher metrics as compared to the UNet CNN, it has failed to rationalize the background class due to its low tolerance or importance towards the background or none class objects in the image. This entails that without a pixel mask that tells the model on which pixels to predict and which to not predict, the results are not feasible on the model unless trained without any background class like in datasets where every pixel is classified into a certain group. This also stems from the fact that we calculate the scores on only the valid mask of pixels, i.e., the pixels that belong to the background class are ignored during computation of mIoU and Dice as discussed earlier in section III.\nFig 6 represents the per class Dice and IoU scores for the two proposed models. The correlation between the two scores shown in Fig 4 is further solidified by the class-wise comparison. The worst performance, according to Fig 6, is on the small vehicle class, and both models generally perform well in classes that include general landmarks like baseball diamonds, tennis courts, basketball courts, and ground track fields. Other easily recognizable items, which rarely change their shape, form, and size, like a bridge and roundabout, are also among the best-predicted classes by both models.\nOur UNet CNN model's metrics on the validation set surpassed the performance of similarly comparable references on the test set, and the MaskFormer, with its greater number of parameters, surpassed the UNet CNN model as well. It can be noted, however, that the mIoU per category of our, or the reference works, were not uniformly distributed, and there's a high affinity towards objects that tend to be larger in size and more frequent within the dataset. Fig 5 shows sample prediction of randomly selected data against the UNet CNN model and the MaskFormer ViT. The augmentation process shows a high yield of variability in the images, which is capable of generating multiple almost unrecognizable images from a single one. The ground truth was taken from the validation dataset itself, and the UNet and Maskformer's predictions followed to the right. The segmentation maps in Fig 5, based on the category, show its consistency with the mIoU presented in Table II and the average overall metrics shown in Fig 4."}, {"title": "V. CONCLUSION AND DISCUSSIONS", "content": "We successfully show that the introduction of the combined weighted loss function helps the model to make stronger predictions and yield better results. One caveat of the findings could be that we need to find a stronger way to de-segment the testing images into multiple patches of smaller images, which could then be fed into the model and then rearranged later to complete the original images for the testing set, as we learn that simply rescaling does affect the model negatively; in other words, the image loses key patterns and features with simple rescaling in data augmentation. We show the number of parameters required to make a robust remote imagery sensing segmentation model that does not need to be over 50M, given that it can directly influence the FLOPS and inference time.\nAlthough the larger training time could be attributed to the data augmentation process, future research could look into streamlining this entire pipeline as a whole. The next steps in the field would entail decoupling the image into multiple smaller fragments and recoupling into the original image to have a robust prediction in images of any shape and size. Nevertheless, we introduced a strong novel combined and weighted loss function to compare UNet CNN with transfer learning-based ViT and show their performance against similar state-of-the-art segmentation models. We show with the use of the new loss function, the CNN model yields comparable results on the metrics with better generalization capacity during inference on unseen data."}]}