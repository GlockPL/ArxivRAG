{"title": "Synthetic Tabular Data Generation for Class Imbalance and Fairness: A Comparative Study", "authors": ["Emmanouil Panagiotou", "Arjun Roy", "Eirini Ntoutsi"], "abstract": "Due to their data-driven nature, Machine Learning (ML) models are susceptible to bias inherited from data, especially in classification problems where class and group imbalances are prevalent. Class imbalance (in the classification target) and group imbalance (in protected attributes like sex or race) can undermine both ML utility and fairness. Although class and group imbalances commonly coincide in real-world tabular datasets, limited methods address this scenario. While most methods use oversampling techniques, like interpolation, to mitigate imbalances, recent advancements in synthetic tabular data generation offer promise but have not been adequately explored for this purpose. To this end, this paper conducts a comparative analysis to address class and group imbalances using state-of-the-art models for synthetic tabular data generation and various sampling strategies. Experimental results on four datasets, demonstrate the effectiveness of generative models for bias mitigation, creating opportunities for further exploration in this direction.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) has seamlessly integrated into our daily lives, revolutionizing sectors from personalized online experiences to advanced medical diagnostics. Nonetheless, data collected from real-world sources inherently reflects the biases, prejudices, and inequalities prevalent within society [17]. Consequently, ML models trained on such data have the potential to perpetuate and even exacerbate these biases, leading to unfair or discriminatory outcomes [3]. One significant data-related challenge that can cause biased predictions is population imbalance. The most common, and easily detectable, is class imbalance, which can lead to poor predictive performance for instances in an under-represented class. Group imbalance, on the other hand, might not directly affect the utility of a model in terms of overall accuracy, but it can lead to unfair treatment of minority groups characterized by some protected attribute (e.g. sex or"}, {"title": "2 Related Work", "content": "Due to the data-driven nature of ML, inherent biases within the data frequently get amplified or perpetuated, resulting in unfair decision-making. Such bias can arise from imbalanced populations regarding the target class labels, or subgroups defined by protected attributes (e.g. sex, race, etc.). This has led to new research directions towards mitigating such bias and developing fairness-aware models [19,1]. In this section, we cover all related work focusing on overcoming class imbalance, group imbalance (fairness-aware ML), and their simultaneous occurrence. Additionally, we cover synthetic tabular data generation methods, that are relevant to our comparative study."}, {"title": "2.1 Fairness-aware ML", "content": "In our study, we focus on group fairness, which considers parity over different groups of individuals, distinguished by one or more protected attributes, such"}, {"title": "2.2 Class imbalance in ML", "content": "Class imbalance is a common problem in classification problems [17], where a large percentage of the data belongs to a specific class. This scenario is encountered in various domains, such as clinical studies, where the minority class (indicating illness) is under-represented, compared to the majority class (representing healthy individuals). To tackle this issue, similar to fairness methods, many approaches resort to pre-processing techniques like over/under-sampling to mitigate the bias towards the majority class [18,13]. In general, under-sampling methods are not typically favored due to the potential loss of crucial information, which can degrade performance. Similarly, naive over-sampling techniques, such as simply duplicating individuals in the minority class, may lead to overfitting. To overcome this problem, the renowned Synthetic Minority Oversampling Technique (SMOTE) was proposed [5]. SMOTE operates by interpolating between random instances in the minority class and their K-nearest neighbors. This concept has led to various extensions [10] which for example sample specific regions, such as those close to the decision boundary [13], or more sparse areas of the feature space [7]. While such methods improve ML utility by reducing bias towards a certain class, they do not account for group fairness."}, {"title": "2.3 Fairness and class imbalance in ML", "content": "Bias in the data related to fairness and class imbalance are not mutually exclusive. More often than not, they occur simultaneously [17], leading to extreme population imbalance for individuals from a minority group who are assigned underrepresented class labels. For example, in the popular Adult dataset, females with a high-income class label are the most under-represented subgroup, accounting for only 11% of the total data (see Figure 1). These populations can become even smaller under \"intersectional-fairness\" when more than one protected attribute exists [23] or for multi-class classification."}, {"title": "2.4 Synthetic Tabular Data Generation Methods", "content": "Various methods have been proposed to learn to generate tabular data [9]. Compared to other modalities such as images or text, tabular datasets consist of a mixture of discrete and continuous feature types, which are difficult to model. Our analysis covers recent approaches for efficient and effective tabular data generation, encompassing state-of-the-art parametric and non-parametric methods.\nSDV-GC: Various continuous distributions (e.g. uniform, exponential, etc.) are considered to model all features (discrete features are not explicitly handled, but transformed into continuous). Subsequently, a multivariate Gaussian Copula is used to estimate the covariance between all features. The covariance matrix and the feature distributions are used to sample new synthetic data [20].\nCTGAN: The typical generator/critic neural network architecture for Generative Adversarial Networks (GANs) is adapted to learn to generate tabular data. Mode-specific normalization is used during training to overcome imbalances and avoid mode collapse [27].\nTVAE: The Tabular Variational Autoencoder [27] trains an encoder/decoder neural network to learn a low-dimensional Gaussian latent space, which is used for sampling new instances through the trained decoder.\nCART: A Classification and Regression Tree [22] method for consecutive column-wise data generation via sampling in the leaves, especially suitable for learning inter-dependencies between mixed data due to its non-parametric nature.\nSMOTE-NC: SMOTE (Synthetic Minority Over-sampling Technique) [5] is a non-parametric method that generates new samples by interpolating between line segments connecting real instances. The same paper introduces the SMOTE-NC variant, which can support mixed (but not solely discrete) feature spaces."}, {"title": "3 Background", "content": "We assume a tabular dataset T containing $N_c$ continuous columns ${C_1, C_2,...,C_{N_c}}$ and $N_d$ discrete columns ${d_1, d_2,...,d_{N_d-1},d_{prot}}$ (including categorical, binary, and ordinal features). Additionally, we assume one binary protected attribute $d_{prot} \\in {0,1}$ (e.g. the sex of an individual), and a binary class label $Y \\in {0,1}$. Given such a dataset, any given ML classifier $f()$ can be trained in a supervised manner, on input-target pairs $x_j = {C_1, C_2, . . ., C_{N_c}, d_1, d_2,...,d_{N_d-1}}$ and $Y_j \\in {0,1}, j = {1,2,..., n}$ (the protected attribute is not used during training). Since the class label and the protected attribute are binary features, they partition the tabular dataset T into $|d_{prot} \\times Y|$ = 4 subgroups $[T_{00}, T_{01}, T_{10}, T_{11}]$.\nA generative model G fitted on some subset $\\tilde{T}$ of the dataset T, can sample $\\tilde{n}$ synthetic rows that comprise a synthetic dataset $\\tilde{T}_{syn} = G(\\tilde{T}, \\tilde{n})$. Further, we refer to a sampling strategy $S(N_{00}, N_{01}, N_{10}, N_{11})$ as the method that dictates the number of synthetic samples to be generated from each subgroup in T, to generate a synthetic dataset $T_{syn} = [G(T_{00}, N_{00}), G(T_{01}, N_{01}), G(T_{10}, N_{10}), G(T_{11}, N_{11})]$.\nThe objective of a sampling strategy in our case, is to create an augmented final training dataset, denoted as $T_{aug} = T \\cup T_{syn}$ which aims to enhance the classifier's performance regarding class imbalance and fairness. We refer to the proportion of the synthetic samples in the augmented dataset as the augmentation ratio $r_{aug} = |T_{syn}|/|T_{aug}|$.\nIn our study, we define and compare various over-sampling methods (Section 4) dictated by the generative models and sampling strategies G, S, aiming to correct both class and group imbalance."}, {"title": "3.1 Datasets", "content": "We use four real-world tabular datasets, frequently used in fairness-aware learning [17]. These datasets comprise demographic attributes of individuals, aimed at predicting their financial status, such as occupation, income, credit score, etc. In Table 1 we list the basic characteristics of all datasets, namely, the Adult, German credit, Dutch census, and Credit card clients. The protected attribute chosen for all datasets is the binary feature \"sex\" (Male/Female). We observe class imbalance for the Adult, German credit, and Credit card clients datasets, as well as, a mixed feature space. The Dutch census dataset exhibits a less pronounced class imbalance and includes solely discrete features. Both class and group imbalances for all datasets are visualized in the first column of Fig 1."}, {"title": "3.2 Evaluation Metrics", "content": "To evaluate the quality of the synthetic data regarding ML utility and fairness, we measure the performance of ML models on the downstream binary classification task for each dataset. In terms of utility, we measure the Accuracy and ROC AUC score. The last is more suitable for evaluation under class imbalance, as it takes true/false-positive/negative rates into account. With respect to group fairness, we employ widely used fairness metrics, namely equalized odds [12] (Eq."}, {"title": "4 Sampling strategies", "content": "All generative methods covered in our study (described in Section 2.4) can be trained on a set of tabular data, and then used to generate an arbitrary number of synthetic samples. Given our assumption of a single binary protected attribute and a binary classification task, this results in 4 homogeneous subgroups for each dataset. Additionally, as defined in Section 3, a sampling strategy dictates the number of synthetic samples to draw from each subgroup, to create the final augmented training set. In this work, we propose four such sampling strategies aimed at addressing class imbalance, group imbalance, or both. Namely, class and protected, sample data to achieve class, and group balance, respectively. Furthermore, class & protected, and class (ratio), sample synthetic data to achieve"}, {"title": "5 Experiments and results", "content": "In this section, we present our comparative study, evaluating all generative methods and sampling strategies under utility and fairness. Additionally, we perform an experiment on intersectional fairness, taking multiple protected attributes into account. We conclude with a runtime comparison of all generative methods."}, {"title": "5.1 Experimental setup", "content": "We perform experiments for all four datasets, four sampling methods, and five generative models. To ensure robustness, each experiment on the downstream task is 3-fold cross-validated and repeated two times over different random seeds. We report average results over all repetitions, highlighting the best results in bold, and underlining the second-best. For the accumulated results of Section 5.2, we further shade with blue color the experiments on synthetic data, which exhibit better performance than training on the original real data (first row). All experiments are conducted on a single machine equipped with a 12th Gen Intel(R) Core(TM) i9 processor and a Nvidia GeForce RTX 3080 Ti GPU."}, {"title": "5.2 Accumulated results on all datasets", "content": "The following Table 3 and Table 4 show the results of our comparative study for the Adult and German credit datasets, and the Dutch census and Credit card clients datasets, respectively. As previously mentioned, we present average metrics for all sampling strategies and generative methods. The first two rows in each table (for each dataset) correspond to baselines, i.e. training the classifier using the real data, and synthetic data generated with Tabfairgan [21]. Subsequent rows display results for augmented training data generated through various combinations of the five generative methods and four sampling strategies. We highlight in blue the experiments with superior performance compared to training on the real dataset. Testing (evaluation) is always performed on, previously-unseen, test data from the real dataset.\nWe interpret the results based on the following criteria:\nAccuracy: An initial observation of the results suggests an overall decrease in classifier accuracy across datasets when using synthetic data. This is substantiated by relevant literature [27], and can be ascribed to the introduction of out-of-distribution synthetic data by the generative methods.\nROC AUC (class imbalance): Sampling strategies focusing on class balancing, such as class and class & protected improve the ROC AUC score for imbalanced datasets. For the dutch dataset, we do not observe any improvement,"}, {"title": "5.3 Intersectional fairness", "content": "In all previous experiments, we assume a single binary protected attribute and class label, leading to 4 subgroups (see Section 3). Nonetheless, in some cases"}, {"title": "5.4 Runtime comparison", "content": "We conclude our experiments by performing a runtime comparison of all generative methods. We report runtime for, i) training (fitting) on the Adult dataset, ii) sampling 10.000 synthetic instances, and iii) training and sampling, since this overall runtime is the most significant metric in our comparison. From Table 2 it becomes evident that the CART method outperforms all others significantly in terms of overall runtime."}, {"title": "6 Conclusion and discussion", "content": "Training ML models that take fairness and class imbalance into account is an open problem, with many applications in the real world, especially for tabular datasets. Most model-independent methods perform oversampling by building upon existing methods that generate synthetic samples via interpolation in the minority classes (SMOTE). This comparative study, considers several state-of-the-art generative approaches to synthesize tabular data in each minority class, to overcome bias. Results on four real-world tabular datasets indicate that the non-parametric CART is the better-performing generative method while being the most computationally efficient. In future work, we would like to delve deeper into exploring the capabilities of CART for generating synthetic data, particularly when optimized in the context of fairness."}]}