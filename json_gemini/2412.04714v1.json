{"title": "PCTreeS - 3D Point Cloud Tree Species Classification Using Airborne LiDAR Images", "authors": ["Hongjin Lin", "Matthew Nazari", "Derek Zheng"], "abstract": "Reliable large-scale data on the state of forests is crucial for monitoring ecosystem health, carbon stock, and the impact of climate change. Current knowledge of tree species distribution relies heavily on manual data collection in the field, which often takes years to complete, resulting in limited datasets that cover only a small subset of the world's forests. Recent works show that state-of-the-art deep learning models using Light Detection and Ranging (LiDAR) images enable accurate and scalable classification of tree species in various ecosystems. While LiDAR images contain rich 3-Dimensional (3D) information, most previous works flatten the 3D images into 2D projections in order to use Convolutional Neural Networks (CNNs). This paper offers three significant contributions: 1) we apply the deep learning framework for tree classification in tropical savannas; 2) we use Airborne LiDAR images, which have a lower resolution but greater scalability than Terrestrial LiDAR images used in most previous works; 3) we introduce the approach of directly feeding 3D point cloud images into a vision transformer model (PCTreeS). Our results show that the PCTreeS approach outperforms current CNN baselines with 2D projections in AUC (0.81), overall accuracy (0.72), and training time (~ 45 mins). This paper also motivates further LiDAR image collection and validation for accurate large-scale automatic classification of tree species.", "sections": [{"title": "Introduction", "content": "Anthropogenic climate change, deforestation, and other human activities are well-known to impact ecological systems. To accurately measure these effects and design interventions, there is a need to gain a more accurate understanding of the state of forests. Currently, tree species censuses are collected by field experts manually, which could take years to finish.\nAirborne LiDAR (Light Detection and Ranging) images collected by Unmanned Aerial Vehicles (UAVs) are a fairly untapped source of data that provides a reliable, scalable, and cost-effective method for researchers to map tree species around the world. Recent development in computer vision enables the use of LiDAR images for automatic tree species classification. Previous works show that state-of-the-art computer vision models, especially Convolutional Neural Networks (CNNs), are highly accurate in classifying tree species using LiDAR images in various ecosystems (Zou et al. 2017; Xi et al. 2020; Terryn et al. 2020; Seidel et al. 2021; Allen et al. 2022; Budei et al. 2018; Hamraz et al. 2019; M\u00e4yr\u00e4 et al. 2021; Hell et al. 2022).\nThis paper aims to develop an automatic classifier for tree species using LiDAR data collected at the Mpala Research Center, located on the Laikipia Plateau, Kenya, spanning over 48,000 acres of arid and semi-arid savannas and woodlands. The Mpala area is home to many of Africa's distinctive large mammals, including elephants, lions, giraffes, and buffalos. The vegetation is dominated by legumes, particularly the genus Acacia. A more accurate approximation of the forests at Mpala will have a profound impact on the scientific understanding of the ecosystem and its biodiversity.\nBuilding on existing literature, we examine two deep-learning classification approaches. The first approach follows the existing work using 2D projections of LiDAR images with typical CNN models. The second approach applies a novel point cloud transformer (PCT) developed by Guo et al. 2021 to classify 3D LiDAR images directly. We call the second approach PCTreeS (Point Cloud Transformer for Tree Species Classification).\nIn this paper, we show that PCTreeS outperforms the baseline 2D CNN approach in AUC, overall classification accuracy, and training time. The transformer framework has a high potential for automatic tree species mapping with Airborne LiDAR images at scale."}, {"title": "Previous Works and Contributions", "content": "The availability of high-resolution LiDAR technology and recent development in computer vision enables unprecedented advancements in automatic tree species classification. A large body of recent works shows that deep learning models like CNNs, Random Forests (RFs), and Support Vector Machines (SVMs) achieve high classification accuracy (Micha\u0142owska and Rapi\u0144ski 2021; Allen et al. 2022; Seidel et al. 2021; Terryn et al. 2020; Xi et al. 2020). This paper contributes to the current literature in three significant ways pertinent to ecosystems, the type of LiDAR images, and classification models."}, {"title": "Ecosystems", "content": "Recent works focus primarily on woodland and forest ecosystems in Europe, North America, and China. We extend the deep learning approach to tropical savannas in Africa. Savannas are characterized by widely spaced trees, making the LiDAR images easier to collect and segment. Trees that can survive irregular rainfalls and long periods of drought, particularly of the genus Acacia, thrive in tropical savannas. To our best knowledge, this paper is the first to classify tree species in the savanna landscape in Africa."}, {"title": "LIDAR Scanning Methods", "content": "LiDAR images are 3D point clouds consisting of individual points that collectively form the shape of an object like a tree. There are two main types of LiDAR images \u2013 Terrestrial and Airborne. Terrestrial LiDAR images are collected by a scanner mounted on a stationary mechanism (e.g., a tripod) or slow-moving car to provide detailed scans of the surrounding areas. Terrestrial LiDAR scanning produces high-resolution point cloud images but is limited in scale. Airborne LiDAR techniques involve mounting a scanner on a flying drone to produce a larger scan of an area but produce relatively sparse point clouds. Airborne LiDAR works specifically well for ecosystems with sparser forests like the savannas.\nRecent works on tree species classification with deep learning techniques mainly rely on terrestrial LiDAR images. Zou et al. 2017, Xi et al. 2020, Seidel et al. 2021, and Allen et al. 2022 are some recent works that train CNN models with 2D projections of 3D terrestrial LiDAR images and achieve high overall and species-wise classification accuracy. However, it is hard to scale the mapping approach to a large area due to the scale limitation of Terrestrial LiDAR data. Several works tabbed into Airborne LiDAR images and showed promising results using Random Forests, CNNs, and PointNet (Hamraz et al. 2019; Budei et al. 2018; M\u00e4yr\u00e4 et al. 2021; Hell et al. 2022). Our paper contributes to the latter body of work and is the first to apply a novel point cloud vision transformer for the 3D Airborne LiDAR tree classification task."}, {"title": "Classification Models", "content": "The most common deep learning approach to date is to feed 2D projections of 3D point clouds into an out-of-box CNN model. For example, in Allen et al. 2022, six orthogonal projections are taken (2 vertical, 4 horizontal perspectives) to capture spatial data from 3D point clouds. However, these methods are inherently limited by the number of projections taken, as spatial information is lost when flattening 3D data into two dimensions.\nSeveral works explored the classification of 3D point clouds through deep learning approaches (Guo et al. 2019), including PointNet++ (Qi et al. 2017), which builds on PointNet (Qi et al. 2016). PointNet was one of the first deep learning methods to approach classification by maintaining 3D point cloud structures, as opposed to 2D projections or other methods that manipulate the data into a separate form. PointNet++ offers a robust local feature extraction strategy that utilizes point neighborhood information at multiple scales.\nRecent developments in 3D computer vision enable a whole host of techniques for LiDAR point cloud classification. Primarily, the significant development in 3D classification methods has centered around the Transformer, a technique that grew out of natural language processing and has since been adapted for computer vision tasks (Lu et al. 2022). Transformers leverage a unique self-attention mechanism that enables efficient global input feature learning, and, subsequently, improves long-range dependency modeling as compared to CNNs. Guo et al. 2021 introduce the Point Cloud Transformer (PCT) for 3D point cloud classification tasks. PCT proposes a neighbor embedding module to encode spatial data from 3D point clouds into input embedding modules, improving global and local point cloud representation for classification. The authors also contribute an optimized Offset-Attention module which improves upon previous self-attention module implementations.\nIn this paper, we apply the PCT model developed by Guo et al. 2021 to classify 3D LiDAR images of trees directly. To our knowledge, this paper is the first to use a vision transformer framework for the 3D tree species classification task."}, {"title": "Data", "content": "Our dataset D = {{$x_i, Y_i$}\\} draws from two main sources. The ground-truth labels {{$y_i$}\\} were provided by ForestGEO's Mpala plot census, and the Airborne LiDAR images {{$x_i$}\\} were provided by the Davies Lab of Harvard University."}, {"title": "Groud-truth Labels", "content": "The Mpala plot census data was collected by ForestGEO from 2010 to 2015. In total, there are 136,752 trees (main stems) with 67 species labels spanning an area of 120 hectares. The census contains detailed information about individual stem locations, species, diameters at breast height (DBH), and status (alive or dead)."}, {"title": "LiDAR Images", "content": "We obtained a rich unlabelled dataset of Airborne LiDAR images collected by the Davies Lab in February 2022. These images are derived from a single composite scan of the Mpala plot within the ForestGEO census grid from a low-altitude (50m) UAV. The scan was further segmented into 43,709 individual trees as point cloud images of LiDAR by the Davies Lab, each with location and height information."}, {"title": "Data Matching and Processing", "content": "We matched ground-truth labels to LiDAR images using derived location information. Due to differences in the georeferencing systems in both datasets, only a subset of images was matched. The ForestGEO dataset decodes tree locations as their distance (due east and due north) from a corner post. We rely on domain experts from the Davies Lab to approximate the locations of the corner posts and boundaries of the census plot. We then use the Universal Transverse Mercator (UTM) coordinate system to decode the locations of each tree in both datasets.\nSince the geocoordinates of each label were recorded by hand in the field with respect to permanent grid stakes (which were themselves laid out by hand), there are inevitable data errors. Therefore, we allow some buffer in the matching process to account for human error and natural noise. The determination of the buffer depends on a tradeoff of match rate and accuracy. A large buffer enables more matches but may lead to inaccurate matchings. With valuable validation work from the Davies Lab, we round the UTM coordinates to the nearest ones to balance matching coverage and accuracy. This buffering technique, though naive, allows us to match about ~4,000 LiDAR images with ground-truth labels for our classification models.\nThe resulting dataset contains 41 species of trees in total. The five most common species (Acacia drepanolobium, Croton dichogamous, Euclea divinorum, Acacia brevispica, and Acacia mellifera) account for about 90.9% of the matched images. To address the class imbalance issue, we group the remaining tree species into an \"other\" class for training, resulting in a total of 6 classes. Another way to balance the dataset would be to upsample the classes with fewer"}, {"title": "Methods", "content": "2D Projection-based Classification\nWe follow Allen et al. 2022 to construct a baseline model using CNN. It leverages simultaneous multi-view perspective projections, while many previous works use a single 2D projection to capture each point cloud or use multiple projections but treat them as separate data points. The only approach that outperforms this method is Zou et al. 2017. However, Zou et al. 2017 uses high-fidelity samples that are hard to achieve by subsequent data collection efforts.\nThe baseline method preprocesses each point cloud by taking six orthogonal projections as inputs to a backbone CNN model. These 6 single-channel images are treated as separate data points, expanding the batch size by 6-fold. Before the final fully connected layer, the features produced by the convolutional layers are concatenated before being fed into a dense layer.\nThe backbone CNN maintains a ResNet18 architecture with 1/4 the filters and single channel input. Since this is a custom architecture, there were no pre-trained weights.\nWe hypothesize several improvements for this baseline approach. For example, to include information on the height of trees, we rescale each point cloud image by the same fac-"}, {"title": "3D Point Cloud Transformer", "content": "The use of vision transformers has been shown to surpass traditional methods in 2D and 3D classification tasks (Lu et al. 2022). Guo et al. 2021 introduces Point Cloud Transformer (PCT), a state-of-the-art framework for point cloud learning tasks, including image classification. PCT leverages a coordinate-based point embedding which converts 3D spatial data into a higher dimensional embedding space that maintains relative point similarity (i.e., the distance between points). While point embedding can effectively extract global features, a dedicated neighbor embedding is implemented to extract local features, leveraging past works such as PointNet++ (Qi et al. 2017). PCT also includes an Offset-Attention module that improves upon the original transformer self-attention module. The model was evaluated on ModelNet40, a dataset commonly used for point cloud shape classification tasks, and was shown to outperform other state-of-the-art methods.\nThis paper implements PCT on our dataset by feeding pure point cloud images of labeled LiDAR images into the model. Notably, there are no projections used, and the original 3D spatial data is maintained in the input."}, {"title": "Experiments and Results", "content": "We trained three main models: baseline, baseline++, and PCTreeS. As mentioned in the Methods section, the baseline model is a ResNet 18 1/4 network with 2D projections of each LiDAR image. The baseline++ model builds upon the baseline model and allows height normalization and single-channel inputs, which we hypothesize to be beneficial for training. The PCTreeS model takes in 3D point clouds and performs the classification tasks without further processing.\nTo mitigate concerns with class imbalances, we set the models to learn 6 classes of tree species which comprised the top 5 most common species, and an \"other\" class to capture the remaining tree species. In addition, we trained models on images with over 1,000 points as a heuristic for filtering out sparse LiDAR scans that we hypothesized would yield poor accuracy.\nAll three models are trained on the same technical setup on one GPU (Dell DSS 8440 Cauldron). We keep model parameters largely the same across all three models, e.g., batch size 32, epoch 100, learning rate 1e \u2013 5, and random seed.\nAll three models achieve decent performance within a short training period, which is important for computing sustainability and reproducibility. PCTreeS outperforms the baseline model with CNN in all three performance metrics (AUC, overall accuracy, and training time). While our best performance in overall accuracy is lower than Allen"}, {"title": "Conclusion", "content": "This paper examines two approaches to tree species classification in tropical savannas. The first method uses a CNN trained on 2D projections of LiDAR images, and the second approach leverages a state-of-the-art 3D point cloud vision transformer (PCTreeS). We show that PCTreeS outperforms state-of-the-art CNN models with 2D projections while reducing training time significantly.\nWe see many ways to improve upon the current model performance. First, we take the segmented individual LiDAR images as a given, but the quality of the segmented tree images can be improved as a separate task. Currently, we see a notable portion of images with either too few points to form a tree (in single digits of points), multiple trees, cropping errors, and non-tree objects like bushes. Proper isolation of individual trees is key to achieving accurate classification results. We are working with the Davies lab to examine these data issues further and improve the segmentation accuracy. Second, data augmentation has been proven to be valuable in improving classification accuracy for terrestrial LiDAR images (Allen et al. 2022). We plan to incorporate more intensive data augmentation and processing to generate a richer dataset for the training step.\nFurthermore, situated in the emerging field of AI for Social Impact, our research establishes that close collaborations with domain experts are crucial for developing accurate and helpful AI-powered applications. Without the support of the Davies lab, we would not have access to the LiDAR data nor the domain insights on how to interpret the images. The ecological context was especially helpful for us to incorporate information useful for tree species classification, such as tree height.\nFinally, to facilitate future replication of this paper and future work, we have made the repository public. For data access, please contact the authors and Davies lab for the LiDAR images and directly request the Mpala plot census data using the ForestGEO data request form available online."}]}