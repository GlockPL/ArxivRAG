{"title": "REVISIT LARGE-SCALE IMAGE-CAPTION DATA IN PRE- TRAINING MULTIMODAL FOUNDATION MODELS", "authors": ["Zhengfeng Lai", "Vasileios Saveris", "Chen Chen", "Hong-You Chen", "Haotian Zhang", "Bowen Zhang", "Juan Lao Tebar", "Wenze Hu", "Zhe Gan", "Peter Grasch", "Meng Cao", "Yinfei Yang"], "abstract": "Recent advancements in multimodal models highlight the value of rewritten cap- tions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of synthetic captions and their interaction with original web-crawled AltTexts in pre-training is still not well understood. Moreover, different multimodal foundation models may have unique preferences for specific caption formats, but efforts to identify the optimal captions for each model remain limited. In this work, we propose a novel, con- trollable, and scalable captioning pipeline designed to generate diverse caption formats tailored to various multimodal models. By examining Short Synthetic Captions (SSC) towards Dense Synthetic Captions (DSC+) as case studies, we systematically explore their effects and interactions with AltTexts across models such as CLIP, multimodal LLMs, and diffusion models. Our findings reveal that a hybrid approach that keeps both synthetic captions and AltTexts can outperform the use of synthetic captions alone, improving both alignment and performance, with each model demonstrating preferences for particular caption formats. This compre- hensive analysis provides valuable insights into optimizing captioning strategies, thereby advancing the pre-training of multimodal foundation models.", "sections": [{"title": "INTRODUCTION", "content": "Large-scale image-text datasets have been crucial in advancing multimodal foundation models. For instance, CLIP (Radford et al., 2021) is pre-trained on 400 million image-text pairs collected from the Web. However, web-crawled data, particularly AltText, often suffer from insufficient visual details and noisy content, as illustrated in Fig. 1. Recent studies highlight the benefits of synthetic captions, which provide better image-text alignment and improved data quality. Research on LaCLIP (Fan et al., 2024) and ShareGPT4V (Chen et al., 2024a) demonstrates that synthetic captions can improve the performance of CLIP and multimodal large language models (MLLMs), respectively. This raises a key question: if higher-quality synthetic captions can be generated, could they fully replace web-crawled AltText? Should we consider disregarding AltText altogether?\nTo investigate this question, we first adopt the approach of VeCLIP (Lai et al., 2024) and train CLIP using synthetic captions generated by LLaVA (Liu et al., 2023b). Similar to the results discussed in Li et al. (2024b), training CLIP fully on synthetic captions of higher quality degrades CLIP's performance significantly: as shown in Fig. 2, when compared to using only AltText, the use of LLaVA captions results in a substantial drop on zero-shot ImageNet classification tasks. However, after combining original noisy AltText and LLaVA captions, we achieve the best results in both classification and retrieval tasks. This observation raises a critical question: what constitutes the optimal image-text data for multimodal foundation models? Despite its importance, research on the interplay between synthetic captions and AltText remains limited. Our findings suggest that while rewriting techniques can enhance image-text alignment, they may reduce data diversity due to"}, {"title": "RELATED WORK", "content": "Multimodal Foundation Models. CLIP (Radford et al., 2021) is one of the pioneering multimodal foundation models connecting images and text. By training on 400 million image-text pairs, CLIP shows strong zero-shot image classification and retrieval capabilities. It lays the groundwork for the development of more advanced multimodal foundation models, such as multimodal large language models (MLLMs) (Liu et al., 2023b; Wang et al., 2023; Chen et al., 2024b; Tong et al., 2024; Zhang et al., 2024b) for vision-language understanding and diffusion models (Rombach et al., 2022; Podell et al., 2023) for text-to-image generation. These advanced models often utilize CLIP's vision tower as their vision encoder.\nImproving Image-Text Data. Web-crawled image-text data often suffer from issues like image-text misalignment and poor-quality textual descriptions (Lai et al., 2024; Li et al., 2024b). There are two common ways for improving image-text data: 1) data filtering based methods remove low-quality data such as misaligned image-text pairs by human-assisted systems (Yu et al., 2024a; Sun et al., 2023) or pre-trained models (Li et al., 2022b; Schuhmann et al., 2021; Gadre et al., 2024; Fang et al., 2023); 2) data recaptioning based methods usually leverage a LLM to rewrite the original caption or a MLLM to rewrite a caption for the image. For example, ShareGPT4V (Chen et al., 2024a) uses GPT-4V to write highly descriptive captions for their images. LaCLIP (Fan et al., 2024) leverages several LLMs to rewrite captions with different writing styles for data diversity. Recap-DataComp-1B (Li et al., 2024b) uses a LLaMA-3 based model to scale the captions. Different from the aforementioned works, we mainly focus on generating different types of captions and exploring 1) the format of ideal captions needed for each multimodal foundation model and 2) a systematic analysis of the intersection between AltText and synthetic captions."}, {"title": "CUSTOMIZED RE-CAPTIONING FOR MULTIMODAL FOUNDATION MODELS", "content": "Image-text data are fundamental to the success of multimodal foundation models, serving as a bridge between visual and textual representations. For example, CLIP (Radford et al., 2021) is pre-trained on 400M web-crawled image-text pairs, enabling it to learn rich, transferable representations that can be applied to various downstream tasks. The importance of precise and detailed captioning is further highlighted in LLaVA-NeXT (Li et al., 2024a), where re-captioned detailed descriptions are utilized for model training at Stage-1.5, enhancing the model's ability to understand and generate nuanced content. Similarly, DALL-E 3 (Betker et al., 2023) shows that the prompt-following capabilities of text-to-image models can be significantly improved by training on highly-descriptive generated image captions. This shows the critical role of captions in shaping a model's capacity to align visual and textual information, ultimately improving performance across a wide range of multimodal tasks. However, the optimal captioning strategy for different foundational models remains under-explored. To address this gap, we introduce a novel, controllable, and scalable captioning pipeline designed to generate diverse caption formats at scale, supported by evaluation metrics that ensure high CLIP"}, {"title": "MLLM AS AN IMAGE DESCRIBER", "content": "VeCLIP (Lai et al., 2024) employs LLaVA (Liu et al., 2023b) for image captioning, while ShareGPT4V (Chen et al., 2024a) utilizes GPT-4V for this task. Compared to traditional image captioners like BLIP (Li et al., 2022a), MLLMs offer several advantages and are natually good image describers. MLLMs can generate longer and more detailed captions, as demonstrated by LLaVA-NeXT (Li et al., 2024a), where a 34B model was used to produce highly descriptive cap- tions. This ability to generate more context-aware descriptions stems from the integration of a large language model (LLM) with a vision encoder. This combination allows MLLMs to capture fine-grained visual details and complex inter-object relationships. Additionally, MLLMs benefit from their multi-stage training process, which combines pre-training on large-scale datasets and supervised fine-tuning for downstream tasks. These characteristics make MLLMs a powerful tool for generating more descriptive captions. However, directly using MLLM as an image describer may have two major limitations: 1) MLLM may not strictly follow the instruction to generate a specific format of caption (Liu et al., 2023a); 2) these instruction fine-tuned MLLMs tend to hallucinate. As shown in Fig. 4, both LLaVA (Liu et al., 2023b) and ShareGPT4V (Chen et al., 2024a) fail to describe the image using only three sentences but generate hallucinated contents (highlighted in red). Although GPT-4V shows stronger capability and many works use it for captioning, the scalability remains limited due to the cost. Therefore, in this work, we focus on building a cost-effective captioner instead of using GPT-4V.\nTo alleviate the above two limitations, we first investigate the origins of hallucination in MLLMs, hypothesizing two primary sources: 1) inherent limitations of the LLM, and 2) the quality of supervised fine-tuning (SFT) datasets, which are often themselves synthetically derived or processed. We focus on the latter, proposing that mitigating hallucinations at the dataset level is essential for converting an MLLM into an effective captioner. We also address the format-following issue by fine-tuning the MLLM on a curated captioning-specific dataset, transforming it into a purpose-built captioning model to generate diverse captions."}, {"title": "Two-STAGE HUMAN-ALIGNED CAPTIONING", "content": "Stage 1: transforming MLLM into a customized captioner. To minimize hallucinations from MLLMs, we begin by constructing a clean and precise fine-tuning dataset. Instead of relying on"}, {"title": "CAPTION ANALYSIS", "content": "Richness assessment: token length and average number of assertions. Fig. 6 illustrates the distribution of the number of tokens of various caption types generated in this study. Specifically, SSC mainly ranges from 10 to 15 tokens, while DSC spans from 40 to 60 tokens, both fitting within the text encoder's capacity in CLIP. In contrast, most DSC+ captions exceed 100 tokens. Besides that, we propose Average Number of Assertions (ANA) to quantify the richness of captions. We prompt an LLM to generate different assertions of a caption to analyze our different formats of captions in terms of the richness. More details of this approach is in Appendix. Note that the ANA for SSC is 2.49, DSC as 8.13 and DSC+ as 12.20, showing more visual contents.\nDiversity assessment: number of unique entities in captions. We hypothesize that the original, albeit noisy, AltText may carry a broader range of diverse information and knowledge, offering potential advantages for CLIP's pre-training. To assess this diversity, we quantify the number of"}, {"title": "IMAGE-CAPTION DATA FOR MULTIMODAL FOUNDATION MODELS", "content": "In this section, we mainly discuss three foundation models: CLIP, multimodal LLM, and diffusion models. For both CLIP and diffusion models, since the text encoder is limited to 77 tokens (Zhang et al., 2024a), we focus primarily on SSC and DSC. For multimodal LLM, we explore more detailed versions, including DSC+ and AFC. We summarize our key findings below:\n\u2022 The tradeoff between the richness of captions and their accuracy needs to be balanced based on the multimodal tasks.\n\u2022 Both AltText and synthetic captions are important for CLIP training, with shorter captions yielding better performance. Linear probing is an additional effective way to evaluate the representations.\n\u2022 Pre-training and SFT benchmarks can behave differently in multimodal LLMs. On the SFT benchmarks, MM1 shows a preference for DSC+ alone.\n\u2022 For diffusion models, DSC emerges as the most effective captioning strategy."}, {"title": "IMAGE-CAPTION DATA FOR MULTIMODAL LLM", "content": "As we study large-scale image-caption data for MLLMs, we use MM1 (McKinzie et al., 2024) as one example and focus on the pre-training stage. MM1 (McKinzie et al., 2024) claims that captioning data lift the zero-shot performance and synthetic captions are helpful for few-shot learning. Based on this insight, we further study the captioning data recipe on how to balance the use of original AltText and synthetic captions. We follow the pre-training setup and the evaluation benchmark in MM1 (McKinzie et al., 2024) to report TextCore and 0/4/8-shot performance. All of the experiments are conducted on the 1.2B model. We pre-train the model with 50K steps and the batch size is 512. More details are in Appendix.\nEffect of synthetic captions for pre-training benchmarks. We generate DSC and SSC captions for VeCap-300M (Lai et al., 2024), as used in MM1 (McKinzie et al., 2024), and replace the captions during MM1 pre-training. The results, summarized in Table 3, show that our synthetic captions yield improved performance in image-text benchmarks across 0-shot to 8-shot settings. For example, SSC achieves a +1.3% performance gain in 0-shot evaluation compared to the original MM1. Unlike CLIP experiments, DSC outperforms SSC, with the combination of DSC and original AltText delivering the best results in this context. We also conduct an ablation study of data mixing ratios on MM1 pre-training to explore the optimal balance between synthetic captions and AltText, as summarized in Table 4. The results indicate that a 66/33 mixing ratio yields the best overall performance across all"}, {"title": "IMAGE-CAPTION DATA FOR DIFFUSION MODEL", "content": "Inspired by DALLE-3 (Betker et al., 2023), detailed and short captions can improve the prompt following ability. In this work, our DSC not only covers the main objects within the scene, but also their relationships, attributes, and the broader context in which they are situated. We hypothesize that this level of detail allows the model to generate images that are not only visually accurate but also semantically aligned with the textual input. We implement Stable Diffusion 3 (Esser et al., 2024) and use this diffusion model as our studying example on text-to-image generation tasks. The backbone is based on the DiT architecture (Peebles & Xie, 2023) that focuses exclusively on class-conditional image generation and incorporates a modulation mechanism to condition the network based on both the diffusion process timestep and the class label. Different from DALLE-3 (Betker et al., 2023), we report results on more comprehensive benchmarks instead of only CLIP score, such as GenEval (Ghosh et al., 2024) and DSG (Cho et al., 2024).\nEffect of synthetic captions. Synthetic captions lead to significant improvements on the GenEval benchmark (Ghosh et al., 2024), as shown in Table 6, highlighting the advantage of enhanced prompt-following capabilities. Notably, incorporating SSC or DSC with AltText boosts the GenEval average score from 58.8 to 65.5. Additionally, synthetic captions yield over a 3.5% improvement on the DSG benchmark (Cho et al., 2024). However, SSC achieves better performance on the FID score (Jayasumana et al., 2024). Overall, the descriptive captions show better results among these benchmarks."}, {"title": "DISCUSSION", "content": "In this study, we examine the role and value of image-text data in multimodal foundation models, including CLIP, multimodal LLMs, and diffusion models. Our research focuses on the intersection between synthetic image-aligned captions and the original web-crawled AltText. To identify the most effective captions for each foundation model, we develop a controllable and human-aligned captioning pipeline designed to minimize hallucinations and generate various types of captions as needed. Through extensive pre-training experiments, we derive the following key insights. 1) Both AltText and synthetic captions play crucial roles\u2014AltText contributes to more diverse information, while synthetic captions offer improved image-text alignment. 2) CLIP tends to favor short synthetic captions, whereas MLLMs benefit from more descriptive captions. We also observe that the benchmarks in the pre-training and SFT stage of MLLMs may have different preferences of captions. 3) We verify the observation from DALLE-3 on text-to-image generation with more comprehensive benchmarks and show the benefits of synthetic captions. For future work, we aim to further refine our captioning pipeline, enhancing its ability to generate task-specific captions across a wider range of multimodal applications."}, {"title": "EXPERIMENTAL DETAILS", "content": "We provide additional details for datasets, experimental settings, results, and analysis in the supple- mentary material."}, {"title": "CLIP", "content": "We summarize the training details in Table A1. For the pre-training stage, we pre-train models on up to 512 TPUs with JAX (Bradbury et al., 2018)."}, {"title": "ADDITIONAL EXPERIMENTS", "content": "To further explore the performance gap between DSC and SSC on CLIP, we present two additional benchmarks to enhance the representativeness of CLIP's existing evaluations: 1) linear probing and 2) transferability between CLIP pre-trained with different captions and LLaVA-style MLLMs. Linear probing provides a direct measure of the quality and generalization capability of the representations learned by CLIP. Strong performance from a linear classifier on specific tasks indicates that the pre-trained model has effectively captured relevant and discriminative features, underscoring the robustness of its embeddings. Additionally, we assess CLIP's representation quality using LLaVA (Liu et al., 2023b) as a case study, where the vision encoder remains frozen during both pre-training and SFT stages. This makes LLaVA an ideal benchmark for evaluating the transferability and integrity of CLIP's learned representations.\nWe summarize the results on linear probing in Fig. 2: even though DSC and SSC shows lower zero-shot performance, they achieve comparable results to AltText after linear probing, indicating similar pre-trained representations. Furthermore, combining synthetic captions with AltText yields the best overall performance. Then we use these pre-trained vision encoders and insert them into LLaVA and complete the default pre-training and SFT stages in LLaVA (Liu et al., 2023b). All of our pre-trained CLIPs use ViT-B/16 as the backbone. We use Vicuna-1.3 as the LLM for LLaVA training and report recent benchmarks in Table A2: POPE (Li et al., 2023b), TextVQA (Singh et al., 2019b), GQA (Hudson & Manning, 2019), SciQA (Lu et al., 2022), LLaVA-Bench (Liu et al., 2023b), MME (Fu et al., 2024), and MM-Vet (Yu et al., 2024b). In this case, the combination of SSC and AltText achieves the highest overall performance, leading in 5 out of 9 columns. This highlights the critical role of both synthetic captions and original AltTexts in CLIP's pre-training: synthetic captions enhance image-text alignment, while AltTexts introduce valuable data diversity.\nCompatibility of rewritten-based methods and filtering-based methods. Besides rewritten-based datasets like web-crawled VeCap-300M (Lai et al., 2024), which leverage recaptioning techniques to improve image-text alignment, it is essential to evaluate the compatibility between rewriting-based"}, {"title": "MULTIMODAL LLM", "content": "We summarize the training details in Table A4. For the pre-training stage, we pre-train models on up to 512 TPUs with JAX (Bradbury et al., 2018)."}, {"title": "DIFFUSION MODEL", "content": "We summarize the training details of our self-implemented diffusion model based on Stable Diffusion 3 (Esser et al., 2024) in Table A6."}, {"title": "A DEEPER ANALYSIS OF GENERATED CAPTIONS", "content": "Fig. 5 is an overview of our two-stage fine-tuning process: we first convert a MLLM into an image captioner, then we further fine-tune it to convert it into a human-aligned captioner. Our smaller image captioning model (3B) efficiently generates large volumes of synthetic data for our experiments. Using this model, we re-captioned a dataset of 7 billion images across multiple iterations. Furthermore, our larger model, with 7 billion parameters, is designed to produce more detailed captions, surpassing the level of detail offered by our long caption format.\nAFC fine-tuning dataset. To generate AltText Fusion Captions (AFC), we also prepare a fine-tuning dataset in this format. Specifically, given AltText and a DSC caption generated by our captioner, we ask LLM to fuse AltText information to the DSC. By this way, we construct a 20K training dataset for our captioner."}, {"title": "Less hallucinations in our DSC", "content": "The Caption Hallucination Assessment with Image Relevance (CHAIR) metric (Rohrbach et al., 2018) is a custom-designed evaluation tool developed to identify and measure the extent of object hallucination in image captioning tasks. The metric determines the proportion of generated words that accurately correspond to objects present in the image, as verified by ground truth sentences and object segmentations. It includes two scores: one that measures the fraction of hallucinated object instances (referred to as CHAIR\u1d62), and the other that calculates the fraction of sentences containing at least one hallucinated object (referred to as CHAIR\u209b): CHAIR\u1d62 = |{hallucinated objects}| / |{all objects mentioned}|, CHAIR\u209b= |{sentences with hallucinated object}| / |{all sentences}|\nAs shown in Table A7, our model achieves a CHAIR\u1d62 score of 5.9 and a CHAIR\u209b score of 19.6, outperforming leading models such as LLaVA-1.5 (Liu et al., 2023b), Shikra (Chen et al., 2023), and MiniGPT-4 (Zhu et al., 2023). The lower CHAIR scores indicate that our cap- tioner produces fewer hallucinated objects per instance and fewer sentences containing hallu- cinated objects. This improvement shows the effectiveness of our two-stage fine-tuning pro-"}, {"title": "ANA: Average Number of Assertions, a metric for evaluating richness of captions", "content": "Besides hallucination, the richness of captions are also an important index to control the generated captions. We propose ANA to quantify the richness of captions. Inspired by GenEval (Ghosh et al., 2024) for evaluating text-to-image generation models, we reverse the process of text-to-image to image-to-text. As shown in Fig. A2, we prompt an LLM to generate different assertions of a caption. After that, we can also leverage a VQA model to check if these details are aligned with the visual contents."}, {"title": "CapScore: A metric for evaluating hallucinations in synthetic captions", "content": "Although the CHAIR metric is widely used, we find it insufficient for detecting hallucinations in object attributes, especially in highly descriptive captions. To overcome this limitation, we propose a new metric to evaluate both the hallucination and richness of captions. Inspired by GenEval (Ghosh et al., 2024) for evaluating text-to-image generation models, we reverse the process of text-to-image to image-to-text and propose CapScore. CapScore measures the correctness of synthetic captions by evaluating the alignment between generated textual assertions and the actual content of the image. As shown in Fig. A2, CapScore has two key steps: 1) use an LLM to extract structured assertions from the captions; 2) use a MLLM to serve as a VQA (Visual Question Answering) model to verify the truthfulness of these assertions. Specifically, each assertion represents a distinct factual claim made within the caption. Then the VQA model determines whether the image supports each claim by answering questions based on the assertions. CapScore is then defined as the percentage of assertions validated as correct by the VQA model. A higher CapScore indicates fewer hallucinations and greater factual accuracy in the generated captions.\nAs shown in Table A8, there is a notable trade-off between the richness of captions and their accuracy. As captions become longer, the Average Number of Assertions (ANA) increases, reflecting the growing richness and complexity of the generated captions. However, CapScore drops with longer captions, which suggests that while the captions provide more content, they are more prone to hallucinations\u2014where the captioning model introduces incorrect or irrelevant information not present in the image. For instance, while DSC+ produces the highest ANA, it also demonstrates the lowest CapScore, highlighting this balance. By contrast, SSC maintains a higher CapScore with fewer assertions, demonstrating better alignment with the image content but at the cost of less detailed descriptions.\nThis behavior highlights the importance of balancing richness and accuracy in multimodal tasks. Models aiming for high-precision applications (e.g., zero-shot classification with CLIP) may benefit from shorter captions (e.g., SSC), whereas scenarios requiring more detailed scene descriptions (e.g.,"}, {"title": "More detailed but noisy captions are helpful for MLLM pre-training", "content": "We examine the impact of hallucinations in image-text data used for MLLM pre-training, focusing on the type of hallucinations detected in our captions. For our primary comparison, we select the best-performing model, LLaVA- 1.5 (Liu et al., 2023b), as shown in Table A7. We first utilize LLaVA-1.5 to generate captions on the VeCap-300M dataset (Lai et al., 2024). Next, we apply our captioner to generate DSC+ for the same set of images. Our captions are more detailed but contain more hallucinations on object. We use MM1's pre-training under a small scale setting as a case study to examine whether image-caption data with fewer hallucinations or with more details can offer advantages for MLLM pre-training. We evaluate the two models after applying fine-tuning using the same data recipe.\nAs shown in Table A9, the MLLM pre-trained with more detailed captions performs better, even though these data contain more hallucinations. When examining task-specific results, we observe that our DSC+ pre-trained model outperforms the LLaVA captions model on 7 out of 9 benchmarks. Specifically, DSC+ improves performance on VQAv2, MMMU, MathV, and SEED, among oth- ers, indicating that the added detail from DSC+ benefits these tasks. However, despite the slight degradation in MMEP results (727.2 vs. 773.4), the overall advantage of detailed captions with minor hallucinations demonstrates that a balance between information richness and accuracy can positively impact MLLM's performance. The larger gains in multimodal understanding tasks, such as MMEC and LLaVAW, suggest that hallucination-tolerant MLLM pre-training may help with complex vision-language reasoning."}]}