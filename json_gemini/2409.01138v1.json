{"title": "Generating Synthetic Satellite Imagery for Rare Objects: An Empirical Comparison of Models and Metrics", "authors": ["Tuong Vy Nguyen", "Johannes Hoster", "Alexander Glaser", "Kristian Hildebrand", "Felix Biessmann"], "abstract": "Generative deep learning architectures can produce realistic, high-resolution fake imagery - with potentially drastic societal implications. Assessing the risks of this technology for the general public requires better understanding of the conditions under which novel generative methods can generate realistic data. A key question in this context is: How easy is it to generate realistic imagery, in particular for niche domains. The iterative process required to achieve specific image content is difficult to automate and control. Especially for rare classes, it remains difficult to assess fidelity, meaning whether generative approaches produce realistic imagery and alignment, meaning how (well) the generation can be guided by human input. In this work, we present a large-scale empirical evaluation of generative architectures which we fine-tuned to generate synthetic satellite imagery. We focus on nuclear power plants as an example of a rare object category - as there are only around 400 facilities worldwide, this restriction is exemplary for many other scenarios in which training and test data is limited by the restricted number of occurrences of real-world examples. We generate synthetic imagery by conditioning on two kinds of modalities, textual input and image input obtained from a game engine that allows for detailed specification of the building layout. The generated images are assessed by commonly used metrics for automatic evaluation and then compared with human judgement from our conducted user studies to assess their trustworthiness. Our results demonstrate that even for rare objects, generation of authentic synthetic satellite imagery with textual or detailed building layouts is feasible. However, in line with previous work, we find that automated metrics are often not aligned with human perception - in fact, we find strong negative correlations between commonly used image quality metrics and human ratings. We believe that our findings enable researchers to better assess the strengths and weaknesses of different generative methods, especially for niche domains and rare object classes, and can help guide future improvements of generative methods.", "sections": [{"title": "1. Introduction", "content": "With the advent of novel generative methods for tabular data [1, 2], text [3] and images [4], synthetic data has entered the main stage of machine learning (ML) research. The applications are manifold, ranging from arts over software development to improving ML itself.\nFor generative Artificial Intelligence (genAI), meth-ods designed for text data, the risks and societal impact have been studied, for instance, in the context of large election campaigns [5]. For the image domain however, research on the technical underpinnings of the risks for the general public inherent to genAI technology have been underrepresented in the literature.\nWithin the ML community, the primary use case for synthetic data is arguably the generation of new training data for the development of larger and more powerful generative ML models. This application scenario has attracted attention, especially in the context of tech com-panies' demand for more data. These companies could soon run out of data for training language models [6]. Synthetic data appears to be the solution to these prob-lems of data-hungry large ML models, not only for these applications. Moreover, in other application domains, such as health care, synthetic data has attracted atten-tion for different reasons: patient records are sensitive data which must not be shared publicly, hence synthetic patient record data could solve the problems around train-"}, {"title": "2. Related Work", "content": "In the following section we provide an overview of the current state of research in the relevant fields: Generative modeling, particularly text-to-image synthesis, deep learning (DL) applications in remote sensing, and evalu-ation methods in the context of genAI."}, {"title": "2.1. Deep Generative Models for Image Synthesis", "content": "With advances in ML and DL, the process of image gen-eration can be automated and provides a way to generate data at relatively low cost. There are several popular model architectures that aim to generate data and for a long time, the state of the art in generative modeling have been Generative Adversarial Networks (GAN) [9], which have been explored thoroughly and applied in various domains and used as default for text-to-image translation over the years [10]. More recently, novel approaches to image generation based on Diffusion Models (DM) [11] overcame some of the challenges associated with GAN training, with e.g. Dhariwal and Nichol [12] showing that DMs could outperform GANs in image synthesis. Cur-rently, many generative model architectures, especially text-to-image ones, are based on DMs.\nText-to-image generation is a specific type of genera-tive modeling which combines technologies of two dif-ferent fields: Computer Vision and Natural Language Processing. Image generation conditioned on text has the advantage that it is very intuitive and easily compre-hensible. There exist a variety of such recently devel-oped vision-language models [13, 14, 15, 4, 16], that have already shown how powerful such large text-to-image architectures can be. Especially their zero-shot ability - synthesizing images of concepts not seen during training - makes these models very compelling.\nFurthermore, there are attempts to directly manipulate features in the latent space to create desired images. For example, by modelling the independent latent character-istics of an object through disentangled representations so that these features can be edited, e.g. changing pose and appearance respectively [17]. Or by identifying la-tent directions through PCA, which enables to control GAN model-based image features like viewpoint, aging, lighting, and time of day [18]. Park et al. identify a lo-cal latent subspace within the latent space of a diffusion model, which enables image editing capabilities through movement along the basis vector at specific timesteps [19]."}, {"title": "2.2. Deep Learning in Remote Sensing", "content": "There are a variety of use cases for the utilization of DL approaches in the context of satellite imagery and remote"}, {"title": "2.3. Evaluation of Generative AI", "content": "For genAI, there are several ways to evaluate models and their outputs [31]. The most common methods are described in the following:\nAutomatic metrics. The qualitative evaluation of gen-erative models can be very subjective, and requires ad-equate quantitative metrics for the systematic assess-ment of generative models and their synthesized data. However, the evaluation of generative models, or their synthetic data, remains a challenging task: There are no standardized benchmarks or protocols set in place [32, 33], and especially in the domain of satellite imagery, finding suitable datasets for training as well as evaluation is rather difficult, particularly for the specific use case at hand. A lot of evaluation metrics, such as the commonly used Fr\u00e9chet Inception Distance (FID) [34], require suf-ficient real data for comparative analysis, which - like in our case - can be difficult to acquire, due to the fact alone that there are only a few hundred existing nuclear facilities in the world. The Inception Score (IS) [35] only assesses the synthetic images, which, albeit lacking a comparison to real data, is more practicable in our case. Since most metrics rely on the feature space of a pre-trained classification model, this limits the metrics to what the model knows, and renders them biased towards the used Inception model and, thus, the ImageNet dataset which the model was trained on [36]. Another draw-back is the need for a large sample size (typically around 50k) to make the metrics robust and reliable, which is not always feasible. Furthermore, the FID is known to not necessarily align with human visual perception [8], especially in the remote sensing and earth observation domain [22]. The automatic, reliable evaluation of gener-ative models and its outputs remains an ongoing research field [31].\nDownstream tasks. Another way of evaluating syn-thetic data is its use in a downstream application task: The generated images can be used in e.g. a classifica-tion task to observe how well they are classified by a pre-trained classifier [37]. A second method is to train a classification model on the synthetic data, or parts of it, and then apply the trained model on real unseen data and evaluate based on the predictive performance [31]. However, for this method there has to be suitable real data to test with, which, like in our case, is not necessarily given.\nHuman evaluation. Another method to assess genAI is human evaluation, with many works resorting to user studies to judge their synthetic data [38, 16, 13, 14, 35]. So-called human-in-the-loop experiments can be a valu-able alternative when the aforementioned methods are not applicable or unreliable, and their results are easily comprehensible. Human ratings can give a more accurate assessment for specific tasks when automated metrics fail to reliably capture the image quality. Although there have been works proposing more standardized guidelines for evaluation [8, 33, 39], there are no established proto-cols set in place for human experiments in genAI, which makes a comparison between published works quite diffi-cult. Moreover, conducting human experiments requires additional work and resources, which are not always at disposal. We aim to conduct human evaluation for a use case, where offline metrics are likely unfitting, and follow recommendations regarding user study settings, to make our findings transparent."}, {"title": "3. Method", "content": "By utilizing a pre-trained text-to-image model such as Stable Diffusion and fine-tuning techniques, we are able to leverage its prior knowledge while simultaneously adapting the model to our domain. We have fine-tuned this model on imagery of our target object, nuclear facili-ties, using DreamBooth [40] and Textual Inversion [41] as fine-tuning methods. For more details, we refer to the respective sources. To test the generalizability and the model's zero-shot capabilities, we use the unmodified pre-trained model as a baseline to examine how well the prior knowledge can be leveraged to generate satellite im-agery of our target object. We then apply the mentioned fine-tuning approaches to further train the model on the datasets described below. For further control during the generation process, we use additional conditioning input with the T2I-Adapter model [42, 30]. Moreover, we com-bine the two approaches: Instead of using the original Stable Diffusion model as base for the T2I-Adapter, we exchange it with our fine-tuned versions (see Figure 2). The intuition is to leverage the newly learned concepts and use them in synergy with the additional image input. The model might then be more familiar with the given layout and able to generate data that better represents our desired image content.\nWe have three approaches that rely only on text prompts as input and then all three methods used in combination with the T2I-Adapter, leaving us, in total, with six models to evaluate:\n(1) the original unmodified Stable Diffusion model, SDiff T2I,\n(2) the DreamBooth fine-tuned model, DB T2I,\n(3) the fine-tuned one using Textual Inversion, TI T2I,\n(4) the base model (1) with the T2I-Adapter, SDiff T2I+Adapter,\n(5) the model from (2) with the T2I-Adapter, DB T2I+Adapter,\n(6) the model from (3) with the T2I-Adapter, TI T2I+Adapter.\nThe implementation in this work relies heavily on Hug-ging Face's Diffusers Library [43]. For all approaches, we use the publicly available Stable Diffusion v1.5 as base, a pre-trained vision-language model build on Latent Dif-fusion Models [4]. This version is compatible with the pre-trained T2I-Adapter components and lays a consis-tent foundation across methods for later comparison."}, {"title": "3.1. Data", "content": "To acquire data to train with, Google Earth Engine and web scraping tools are applied to obtain satellite and aerial imagery of nuclear facilities. After removing im-ages where sites were blurred or of low quality, the result-ing dataset contains 202 satellite images of 185 unique nuclear power plants around the world. To exploit the model's prior, we apply conditionings - which are not present in the mentioned training data - to those newly learned concepts by adding keywords to the text prompts for variations regarding the location, seasonality and the time of day, for example, generating images of a nuclear facility in the desert or in the winter. Using different set-tings, synthetic images are generated for each approach. For the additional image input, we use the T2I-Adapter [42] as described in [30]. We generate three different layouts of fictional power plants using the game engine Unity, varying the angle and rotation from which the site is looked at. This creates different viewpoints from the same facility. The renders are then turned into canny edge, depth maps and sketches for further structural guid-ance during the generation process. Images are then generated using layout conditioning in addition to the text prompts.\nWe generate a pool of images for each model, using different variations in the given text prompts. For the additional usage of the T2I-Adapter, we use different in-put modalities (canny, depth map, sketch) and vary the viewpoints for each of the three layouts. This way, we generate a variety of synthetic imagery, but based on the same three layouts originally rendered from the game engine. For the human experiments, we randomly select 500 images for each approach. Figure 1 shows samples of synthetic satellite imagery of nuclear facilities which have been generated using the methods mentioned pre-viously in this section. For these, either a single text prompt (e.g. \"an aerial view of a [*] nuclear power plant, forest, green\") or a text prompt with an additional im-age have been used as input. For comparison, we also include the 202 real images in our human evaluation ex-periments. All images have been scaled to the same pixel size (512x512)."}, {"title": "3.2. Experiments", "content": "To evaluate our generated data, we conduct a user study where we assess based on three aspects: (a) Fidelity (im-age quality), how authentic does an image look, (b) text alignment (semantic control), how well does an image match the given text prompt and (c) layout alignment (structural control), how well can the structure of the same subject be retained within several images. For human experiments in genAI, some works opt for a 2-alternative forced choice setup [15, 16, 22]: Two images of two models are put next to each other and the user is tasked with selecting the superior one. However, this user interface design limits the comparison to only two options at a time. Similar to other work [33, 44], we ap-ply a Likert scale where users are tasked to rate a given image or group of images from 1-5, depending on what aspect is being evaluated. This way, images and methods can be rated independently of one another and then later evaluated and ranked.\nUser study. We use the crowdsourcing platform Toloka [45] and considered two main principles in the experimental design: The task should be simple and the results interpretable [33]. The implemented user inter-face design for each study is shown in Figure 3: For the (a) image fidelity analysis (see Figure 3a), users are in-structed to rate a given image from 1 (unrealistic) to 5 (realistic) based on how authentic it looks to them. The interface for the (b) text alignment studies (see Fig-ure 3b) looks almost the same, with the exception that the text prompt that was used as conditioning input, is also shown. Participants have to rate from 1 (does not match at all) to 5 (matches exactly) how well the shown image matches the text. The third user study examines the (c) layout alignment. As depicted in Figure 3c, the user is shown four images that were generated from the same model, and asked to rate from 1 (completely different facility in each image) to 5 (identical facility) whether the shown images depict the same facility. Participants were selected to be fluent in English and instructed about the motivation of the study; all participants confirmed acceptance of the data usage. We excluded responses from participants that submitted incomplete tasks, tasks in which users pressed only one key repeatedly and tasks in which control tasks (for which ground truth data was available) were answered incorrectly. For tasks (a) and (b) every participant rated 30 images in total. In task (c) each user rated 40 images. Compensation was according to the minimum wage in the most frequent countries of origin on the platform.\nEvaluation setup. Since we lack sufficient real data of our target object, as it is a rare object class, we only consider metrics which do not rely on the feature space of real data. Calculating scores like the FID [34] with the 202 images might deliver unreliable results due to the low sample size and possibly contain bias, as these were already used for fine-tuning. There-fore, we only use the IS [35] as automatic metric in our evaluation, although it also is a flawed metric [46]. For implementation, we use torch-fidelity [47] to calculate the score. The IS is calculated as follows:\n$IS = exp(E_{x \\sim p_e}[D_{KL}(p(y|x)||p(y))]).$\n$x$ is sampled from $p_e$, the encoded distribution of our synthetic images. The metric makes use of the KL diver-gence, calculated between the conditional label distribu-tion $p(y|x)$ (favoring low entropy) and the marginal dis-tribution $p(y)$ from all samples (favoring high entropy). For more details see [35]. To gain a score that might pos-sibly be more accustomed to the remote sensing domain, we further apply an adapted version of the IS, as done in [29]: We exchange the pre-trained Inception model with a classifier fine-tuned on a land-use classification dataset [48], aerial-view imagery to give information on land cover. The modified IS is denoted as $IS_{adapt.}$."}, {"title": "4. Results", "content": "In the following, we analyze the results of the user study and compare the human ratings with established metrics. The size of our sample after applying the quality control on the collected study results, as described in Section 3, is listed in Table 1. For the (c) layout alignment study, one group of images had to be removed for each model due to technical error, leaving us with a sample size of 496 images for each approach."}, {"title": "Image fidelity.", "content": "Regarding (a), scores are shown in Table 2. Despite the smaller sample size, we can infer that the real images achieve the best results during the human experiments, followed by the text-only approaches and then the methods combined with the additional image in-put. A visual depiction is shown in Figure 4a. Excluding the real imagery, for the synthetic data the fine-tuned methods yield mostly better results than the correspond-ing original Stable Diffusion approaches, apart from the (2) DB T2I method.\nUsing additional input with the T2I-Adapter compo-nent gives us more control over the image composition during the generation process, however, the generated images seem to lack image quality: They achieve poorer results than the pure text-based approaches. But the fine-tuned approaches (5, 6) yield better results in combination with the layout control in com-parison to the original (4) base model. With the text-only approaches, the unmodified model (1) achieves results comparable to the fine-tuned models, but these images often don't show the desired satellite perspective: This aspect is not considered with the Likert scale and was not an influencing factor for the users regarding image fi-delity, however this is a limiting factor for the generation of satellite imagery. There was a significant difference between the ratings depending on what was used as in-put modality (e.g. canny input seems to produce lower human ratings than sketch or depth maps), however this was not further investigated in the scope of this work.\nIn contrast to the human ratings, automated metrics consistently rank Adapter-approaches higher than gen-erative models based solely on text input. Furthermore, the real images achieve a relatively average IS and ISadapt. in comparison to the other models."}, {"title": "Text alignment.", "content": "Evaluation results for (b) text align-ment are shown in Table 3. Here, the original model (1) achieves the best image-text alignment scores from human perspective as well as the CLIPScore. Following, the DreamBooth fine-tuned approaches (2, 5) yield the second-best results. Apart from this, the text alignment seems to result in mostly poorer results with the addi-tion of image input, according to our human evaluation,\nas shown in Figure 4b. However, this ranking does not exactly align with the CLIPScore results. Here, both Tex-tual Inversion fine-tuned approaches (3, 6) significantly perform the poorest."}, {"title": "Layout alignment.", "content": "Since there are no suitable quan-titative metrics to evaluate the layout alignment across several images, at least when the viewpoint and condi-tions are different in each, we only look at the human judgement results for the (c) layout alignment experi-ments. The results are shown in Table 4: Contrary to expectations, the DreamBooth fine-tuned approach (2) without additional image input achieves the best results in our human evaluation, as also visible in Figure 5c and Figure 4c. However, only by a small margin. One possi-ble reason could be, that the raters might still have been influenced by the image quality or other distortions and details, instead of solely focusing on the layout aspect. In general, the additional conditioning input through the adapter does lead to a better structural alignment accord-ing to the scores. Except for (2), all Adapter-approaches outperform the ones that are solely based on text input, (1, 3). For the Adapter-approaches, fine-tuning seems to help the retaining of the given layout structure during the generation process, as these (5, 6) achieve noticeable better scores than (4)."}, {"title": "Correlation.", "content": "A correlation analysis has been per-formed between the quantitative metrics and the respec-tive user study results"}, {"title": "5. Discussion", "content": "In this work we investigated to what extent modern gen-erative DL methods can be used to generate imagery of rare objects in niche domains. A special focus in this work was on a comparison of control mechanisms for genera-tive methods. In order to compare different approaches, we leveraged automated quantitative metrics and com-pared them with human ratings. In extensive empirical evaluations, we demonstrate that novel image generation methods can be used to generate imagery from niche do-mains and rare objects. Importantly, we find that textual control works well in many cases. But also controlling the image generation with building layouts is feasible, which allows for more fine-grained control.\nInception Score is not aligned with Human Ratings. A key finding of our empirical results, which is in line with previous studies , is that the automated metrics that are used to optimize and evaluate image generation for rare object classes, do not capture image quality as rated by humans. Our results show that exchanging the off-the-shelf classifier models pretrained on ImageNet, which are used as feature extraction backbone in current metrics like the IS, might provide a slightly better met-ric. However, our results also demonstrate that despite these adaptations to the domain of interest, the IS is not aligned with human visual perception. According to [36], established metrics as used with the default Inception model as backbone might even behave unfair towards diffusion models. Thus, current metrics, as is, are not a reliable measure of performance and image quality when the benchmark is human perception and the goal of these metrics is to actually approximate human judgement. Es-pecially for rare classes and niche domains, as in our case, established metrics are not a trustworthy method of evaluation. Since in such cases, there is not enough real data to calculate additional comparative metrics (such as the FID, KID, Precision and Recall) to get a more robust and broader spectrum of evaluation, the main current established metric for fidelity is the IS, which, as seen in our experiments, is not reliable. Surprisingly, the IS even correlates negatively with human judgement. This appears to suggest a meaningful relationship between these two aspects - but in the opposite direction from what the IS score is intended to measure.\nCLIPScore and Human Ratings. As seen in our (b) text alignment results, CLIPScore seems to correlate mostly well with human judgement, there might only be a slight bias towards the (1) original base model, e.g. (4) SDiff+Adapter gets a higher CLIPScore while scor-ing low in the human experiments. This could be due to both the model and the metric relying on CLIP (Sta-ble Diffusion v1.5 uses the pretrained text encoder CLIP ViT-L/14 as conditioning component). The textual in-version fine-tuned approaches, for example, perform the poorest according to CLIPScore. Fine-tuning with this method trains in the textual embedding space, thus makes changes to the text encoder component, which would re-inforce the assumption. Note that while the positive cor-relation of CLIPScore suggests that this reliably captures human perception, there could be other explanations for this positive result that we cannot rule out based on our user studies alone. For instance, if the generated images always showed the same (or very similar) facilities, this could lead to high scores in our user studies. Heterogene-ity of facilities was not enforced in those models that did not use layout inputs. As both the generating models as well as the score use CLIP backbones, such cases could be regarded as a case of overfitting. This problem has been widely acknowledged: the more expressive the models, the more difficult benchmarking becomes as it can be difficult to ensure a clean train-test split [53].\nFor (c) structural alignment, the approaches using ad-ditional image input specifying the precise layout of a facility would, mostly outperform the ones using only text conditioning. This holds true for most of the meth-ods except for (2) DB T2I, which scores highest in terms of alignment - but uses only textual input and no layout input (see Table 4). The reasons for this are unclear, one explanation could be that the user study design was not adequate enough to measure the structural control, or the instructions were not clear enough. Since we only have the human ratings to interpret, comparative analysis to other metrics is not possible, thus, a broad and robust evaluation is difficult. The layout alignment - with still considering different rotations and angles from the same target object - is an aspect which has not been thoroughly evaluated in literature yet and could be investigated in further research.\nLooking at the evaluation of generative models and their outputs, the question is also raised whether human judgement should be the standard for assessing image quality: Human perception should be used as benchmark when the goal is to approximate this through a metric. However, approximating human perception is not nec-essarily relevant for all use cases, as also raised in other work [36]. In fact, some of the most interesting applica-tion scenarios, such as generating new training data for ML models, do not involve human perception directly.\nSocietal implications In the context of public inter-est, human-centric approaches and human-in-the-loop methods are important to understand the risks of ML technology [54, 55]. For instance, previous work in com-puter vision has demonstrated that it is possible to de-couple human perception from machine perception with synthetic imagery . Complementing this work on di-vergent perception between humans and machines, our findings show a misalignment between human visual perception and automated evaluation, which provides a flawed foundation and benchmark for future develop ment of novel ML technologies. Albeit the participants in our studies are no experts in the domain, they are representatives of the general public. Given fabricated or generated imagery, spreading misinformation is even easier when people are not familiar with such niche con-tent. Works addressing information manipulation via ML and crowdsourcing have gained traction within the \"AI for Social Good\" community , possibly due to the rise of fake news, deepfakes and their effortless distribu-tion through mass media. With easily available technol-ogy, virtually anyone can produce synthetic imagery that can, as shown in our experiments, fool the average user. Systems being open to validation by, e.g. being open-source, is necessary for transparency [57]. However, an ill-intended user could use such powerful open-source technology for malicious purposes. In our studies, gener-ated images look authentic enough for users to assume they are real, which, depending on the content, could have implications of public interest for citizens."}, {"title": "6. Conclusion", "content": "In this work, we have leveraged a pre-trained vision-language model and fine-tuned it to generate synthetic satellite imagery of a rare object class, which has been underrepresented in literature before. Moreover, we con-ducted large-scale human-in-the-loop experiments to measure human judgement and compared it with estab-lished metrics in the field. We found that additional image input mostly gives more control over the image compo-sition, however, it still remains very difficult to control specific details and generate images of the same exact object with the presented conditioning methods. Our results demonstrate that fine-tuning can help generate imagery of specific images and target objects that are on par with data generated from the original base model, in terms of perceived image quality, but that are more suitable for the remote sensing domain and better display the desired satellite perspective. Consistent with previ-ous works, we confirm that established state-of-the-art metrics to evaluate synthetic imagery do not necessarily align with human perception, at least regarding image fidelity. Our findings show that the IS and its adapted version even correlate negatively. CLIPScore seems to work fairly well for measuring image-text alignment, but might be biased towards models based on CLIP. Overall, we find that large-scale user studies are needed to assess synthetic data in regard to human perception, especially for rare classes, where a broad variety of automated eval-uation metrics is not available.\nFor future work, these experiments could be conducted on an even larger scale and for various datasets also in other domains, to investigate whether the findings of this work generalize to other use cases. In line with previous work, our results provide empirical evidence that current established metrics do not work well for measuring hu-man judgement, especially for rare objects and domains that contain imagery dissimilar to natural images. The quantitative evaluation with automated metrics in genAI requires a more in-depth study and remains an open field of research, not only for the sake of evaluation itself: Better understanding of the perceived image quality will enable researchers to improve generative models in the future."}]}