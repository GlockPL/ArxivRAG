{"title": "Modality-Invariant Bidirectional Temporal Representation Distillation Network for Missing Multimodal Sentiment Analysis", "authors": ["Xincheng Wang", "Liejun Wang", "Yinfeng Yu", "Xinxin Jiao"], "abstract": "Multimodal Sentiment Analysis (MSA) integrates diverse modalities-text, audio, and video-to comprehensively analyze and understand individuals' emotional states. However, the real-world prevalence of incomplete data poses significant challenges to MSA, mainly due to the randomness of modality missing. Moreover, the heterogeneity issue in multimodal data has yet to be effectively addressed. To tackle these challenges, we introduce the Modality-Invariant Bidirectional Temporal Representation Distillation Network (MITR-DNet) for Missing Multimodal Sentiment Analysis. MITR-DNet employs a distillation approach, wherein a complete modality teacher model guides a missing modality student model, ensuring robustness in the presence of modality missing. Simultaneously, we developed the Modality-Invariant Bidirectional Temporal Representation Learning Module (MIB-TRL) to mitigate heterogeneity.", "sections": [{"title": "I. INTRODUCTION", "content": "Multimodal Sentiment Analysis (MSA) [1]\u2013[3] utilizes various information modalities such as text, audio, and video to analyze and understand individuals' affective states and intentions. MSA is widely applied in HCI systems [4]-[8], mental health [9], and other fields. Studies have shown that MSA provides more nuanced and accurate emotional assessments than single-modality analysis [10], [11]. However, many existing studies [12]-[16] focus on scenarios where all modalities are present or one is entirely missing, failing to capture the more common real-world scenario of random modality absence. This oversight limits the practical applicability of these models. To address this gap, we propose a novel approach that models the randomness of modality missing. We employ knowledge distillation techniques, using a complete modality network as the \u201cteacher\u201d and a network with random modality missing as the \u201cstudent.\u201d In addition, in the presence of modality random missing, multimodal data still faces the challenge of modal heterogeneity, which poses a significant challenge for multimodal fusion and improving accuracy [17] [18]. Modality representation learning has been an effective coping strategy in previous MSA studies. For example, in [19]\u2013[21], researchers mitigated modal heterogeneity by decoupling multimodal features into modality-invariant and modality-specific features and using loss constraints. However, most of the above methods were proposed assuming no missing"}, {"title": "II. METHODOLOGIES", "content": "We consider three modalities: audio (a), text (t), and vision (v). The complete input feature sequence is defined as $X_m \\in \\mathbb{R}^{T_m \\times f_m}$, where m represents the three modalities, i.e., $m\\in \\{a,t,v\\}$, with $T_m$ and $f_m$ denoting the time steps and feature dimensions, respectively. To simulate missing modality features in real-world scenarios, we introduce a masking function $F(\\cdot)$ and a randomly generated time mask $g_m \\in \\{0,1\\}^{T_m}$,"}, {"title": "A. MIB-TRL module", "content": "Time-aware generation module F. In missing MSA processing, accurate reconstruction of missing features is crucial for sentiment prediction. Inspired by the WaveNet approach to speech generation [25], we apply the Dilated Causal Convolution technique to the time-aware generation module F. This module progressively predicts and reconstructs missing modality features using an autoregressive approach, ensuring that each step relies solely on the available inputs, thereby avoiding future information interference. For each layer $i\\in \\{1,2,..., n\\}$, perform the following operations:\n$Y_{dc} = DConv(y, d_i), \\quad \\xi = tanh(Y_{dc}) \\times \\sigma(Y_{dc}).$ (1)\nwhere $d_i = 2^{i-1}$ is the dilation rate and $DConv$ is the dilated causal convolution. The output y' of module F is then obtained by the following operation:\n$\\eta = Conv_{1\\times1}(\\xi), \\quad y' = \\eta + y.$ (2)\nMIB-TRL module. Fig. 2 shows the MIB-TRL module designed for each input modality. The method first processes the input data through two identical convolutional layers and then splits the data stream into two directions. Each direction consists of n modules F. In both directions, the output of the F module at the same i position is integrated to obtain $Z_m^i$. Finally, all $Z_m^i$ are fused to obtain the full long-term contextual representation of m modality."}, {"title": "B. Transformer Fusion (TF)", "content": "Previous studies [26] [27] have shown that the text modality contains more accurate semantic information and plays a crucial role in sentiment prediction. In this paper, our innovative Transformer Fusion structure fully leverages the central role of the text modality. Visual-to-Audio auxiliary:v \u2194 t \u2194 a\n$S_{Atv} = SA(X_t+X_v) + (X_t + X_v),$ (4)\n$Y_{t\\leftrightarrow v} = MLP(S_{At\\leftrightarrow v}), Q_{t\\leftrightarrow v} = S_{At\\leftrightarrow v},$ (5)\n$V_{t\\leftrightarrow a} = K_{t\\leftrightarrow a} = S_{At\\leftrightarrow a},$ (6)\n$Y_{v\\leftrightarrow t\\leftrightarrow a} = MLP(CA(Q_{t\\leftrightarrow v}, K_{t\\leftrightarrow a}, V_{t\\leftrightarrow a})),$ (7)\n$\\tilde{Y}_{v\\leftrightarrow t\\leftrightarrow a} = MLP(\\tilde{Y}_{t\\leftrightarrow v}) + Y_{v\\leftrightarrow t\\leftrightarrow a} + MLP(Y_{t\\leftrightarrow v}).$ (8)\nAudio-to-Visual auxiliary: a \u2194 t \u2194 \u03bd\n$\\tilde{Y}_{a\\leftrightarrow t\\leftrightarrow v} = MLP(\\tilde{Y}_{t\\leftrightarrow a}) + Y_{a\\leftrightarrow t\\leftrightarrow v} + MLP(Y_{t\\leftrightarrow a}).$ (9)\n$SA(X)$ denotes self-attention and $CA(X)$ denotes cross-modal attention. MLP denotes multilayer fully connected. The final overall transformer output is:\n$Y = \\tilde{Y}_{uta} + Y_{a\\leftrightarrow tv}.$ (10)"}, {"title": "C. Training objective", "content": "This loss function combines the regression task loss, distillation loss, reconstruction loss, and SimSiam loss [28]. When it is a complete modality setting, there is no need to perform the distillation operation, and the teacher model accomplishes the task alone with an overall loss function of:\n$L = |Y_{label} - Y|.$ (11)\nWhen it is an incomplete modality setting:\n$L = |Y_{label} - Y| + L_{dis} + L_{rec} + L_{sim}.$ (12)"}, {"title": "III. EXPERIMENTAL SETUP", "content": "The CMU-MOSI dataset [29] examines sentiment in English-language videos, with intensity scores spanning from -3 (strongly negative) to 3 (strongly positive). The CH-SIMS dataset [30] comprises 2281 video clips from 60 Chinese videos sourced from movies and variety shows, with sentiment intensities from -1 (strongly negative) to 1 (strongly positive)."}, {"title": "A. Datasets", "content": "Our model is implemented using PyTorch and optimized with Adam, with a batch size of 32 and an early stopping set at eight epochs. We initialized three seeds to ensure reproducibility, averaging the results for consistency. The hyperparameters $\\lambda_1$, $\\lambda_2$, $\\lambda_3$ were optimized using a grid search approach. For the MOSI dataset, the optimal values were set to 0.01, 0.1, and 0.1, respectively, while for the SIMS dataset, the optimal values were determined to be 0.3, 0, and 0, respectively."}, {"title": "B. Implementation details", "content": "Distillation loss: This loss comprises the distillation loss between the teacher and student models after the MIB-TRL and TF modules, along with the self-distillation loss of the student model.\n$L_{ts}^{CMIB} = \\sum_{\\{a,t,v\\}} || Z_m - \\tilde{Z}_m ||^2, L_{ts}^{TF} = \\sum_{\\{a,t,v\\}} || \\tilde{Y}_m - Y_m ||.$ (13)\nThe total distillation loss is:\n$L_{dis} = \\lambda_1(L_{ts}^{CMIB} + L_{ts}^{TF} + L_{ts}^{TF}).$ (14)\nReconstruction loss: After the student model processes the data through the unimodal feature encoder $\\mathcal{E}$ and MIB-TRL module, the original feature $X \\in \\{A, T, V\\}$ is referred to, and a Multi-Layer Perceptron (MLP) decoder is used to reconstruct the complete modality sequence. Reconstruction losses $L_{rec}^{Loc}$ and $L_{rec}^{CMIB}$ are obtained respectively. The reconstruction loss can be expressed as:\n$L_{rec} = \\sum_{m} \\alpha_m smooth_{L1}(X - MLP(\\mathcal{E}_m \\text{ or } \\tilde{Z}_m) \\cdot (1-M_m)).$ (16)\nwhere $\\mathcal{E}_m$ and $\\tilde{Z}_m$ represent the features processed by the unimodal feature encoder and MIB-TRL module, respectively, $M_m$ denotes the time mask that excludes losses at unmasked locations. The total reconstruction loss is expressed as:\n$L_{rec} = \\lambda_2 (L_{rec}^{Loc} + L_{rec}^{MIB}).$ (17)\nSimsiam loss: We impose loss constraints on the available features to avoid increasing network complexity, but the issue of collapsing solutions in representation learning persists [28]. To address this, we introduce SimSiam loss [28], which prevents collapse by employing a stop-gradient mechanism, ensuring more stable multimodal feature representations.\n$L_{sim} = D(f(\\tilde{Z}_m \\text{ or } \\mathcal{E}_m), sg(h(Z_m \\text{ or } \\mathcal{E}_m)))+\nD(f(Z_m \\text{ or } \\mathcal{E}_m), sg(h(\\tilde{Z}_m \\text{ or } \\tilde{\\mathcal{E}}_m))).$ (18)\nWhere f and h are MLP with different mapping dimensions. sg denotes stopping the gradient operation. D denotes negative cosine similarity minimization on the features($p_1, p_2$):\n$D(p_1, p_2) = \\frac{p_1^T p_2}{||p_1||_2 ||p_2||_2}.$ (19)\nThe total SimSiam loss is:\n$L_{sim} = \\lambda_3(L_{sim}^{CMIB} + L_{sim}^{CIF}).$ (20)"}, {"title": "A. Comparison with state-of-the-art technology", "content": "Table I presents the performance of our model compared to current state-of-the-art models. Other models were reproduced using open-source code and the same experimental setup. According to [23], the missing rates for the CMU-MOSI dataset range from 0.1 to 1.0, and for the CH-SIMS dataset, they range"}, {"title": "B. Ablation experiment", "content": "In the incomplete modality setup, we conducted ablation experiments to determine the optimal number of module F configurations, as shown in Table II. The results show that when i = 4, the model exhibits optimal overall performance. Therefore, all our subsequent experiments were conducted using the i = 4 configuration. Table III shows the results of the ablation experiments with the TF module removed. The results show a decrease in all performance metrics except for an increase in correlation (Corr) in the incomplete modality setting for the MOSI dataset and an increase in accuracy (ACC-3) in the complete modality setting for the SIMS dataset. These results emphasize the effectiveness of our design of the TF module. Table IV highlights the importance of different loss function combinations under incomplete modality settings. The results show that our proposed MITR-DNet, which integrates distillation loss, reconstruction loss, and SimSiam loss, performs the best. Fig. 4 illustrates the training loss trends across various missing rates. The decreasing training loss throughout the process indicates that our proposed loss functions effectively handle MSA tasks with missing data."}, {"title": "V. CONCLUSION", "content": "This paper proposes a modality-invariant bidirectional temporal representation distillation network for missing multimodal sentiment analysis, effectively mitigating the issue of modality heterogeneity. However, data imbalance remains challenging due to the dataset's uneven distribution of emotional category samples. Mitigating the impact of data imbalance on multimodal sentiment analysis can be an important research direction."}]}