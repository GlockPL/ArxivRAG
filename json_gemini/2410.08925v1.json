{"title": "HYPERPG - PROTOTYPICAL GAUSSIANS ON THE HY- PERSPHERE FOR INTERPRETABLE DEEP LEARNING", "authors": ["Maximilian Xiling Li", "Korbinian Franz Rudolf", "Nils Blank", "Rudolf Lioutikov"], "abstract": "Prototype Learning methods provide an interpretable alternative to black-box deep learning models. Approaches such as ProtoPNet learn, which part of a test image \"look like\" known prototypical parts from training images, combining predictive power with the inherent interpretability of case-based reasoning. However, existing approaches have two main drawbacks: A) They rely solely on deterministic similarity scores without statistical confidence. B) The prototypes are learned in a black-box manner without human input. This work introduces HyperPg, a new prototype representation leveraging Gaussian distributions on a hypersphere in latent space, with learnable mean and variance. HyperPg prototypes adapt to the spread of clusters in the latent space and output likelihood scores. The new architecture, HyperPgNet, leverages HyperPg to learn prototypes aligned with human concepts from pixel-level annotations. Consequently, each prototype represents a specific concept such as color, image texture, or part of the image subject. A concept extraction pipeline built on foundation models provides pixel-level annotations, significantly reducing human labeling effort. Experiments on CUB-200- 2011 and Stanford Cars datasets demonstrate that HyperPgNet outperforms other prototype learning architectures while using fewer parameters and training steps. Additionally, the concept-aligned HyperPg prototypes are learned transparently, enhancing model interpretability.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep Learning has achieved high accuracy in many computer vision tasks. However, the decision- making processes of these models lack transparency and interpretability, making deployment in safety-critical areas challenging. Explainable Artificial Intelligence (XAI) seeks to develop inter- pretability methods to open the black-box reasoning processes of these models and increase trust in their decisions.\nXAI methods can be broadly divided into two categories: First, Post-Hoc Methods like LIME (Ribeiro et al., 2016), SHAP (Lundberg & Lee, 2017) or GradCAM Selvaraju et al. (2017) offer explanations for predictions without requiring retraining. While applicable in many scenarios, post- hoc methods may not actually align with the models' decision making processes, potentially leading to interpretations that are not entirely faithful (Rudin, 2019). Second, inherently interpretable meth- ods provide built-in, case-based reasoning processes. For instance, small decision trees are inher- ently interpretable because their reasoning can be easily understood as a series of if-else statements (Molnar, 2020). However, they are constrained in their representational power.\nDeep Prototype Learning Architectures such as ProtoPNet (Chen et al., 2019) and its derivatives (e.g., Rymarczyk et al., 2020; Donnelly et al., 2021; Sacha et al., 2023) integrate inherent inter- pretability into deep learning models through a prototype layer. Each neuron in this layer represents a prototype, storing a latent feature vector. The model's predictions are based on the distances be- tween sample features and prototype parameters, for example by computing the L2-distance. How- ever, these deterministic similarity scores do not include statistical information like confidence. To"}, {"title": "2 RELATED WORK", "content": "Prototype Learning. In image classification, prototype learning approaches using autoencoders provide high interpretability by reconstructing learned prototypes from latent space back to the im- age space (Li et al., 2018). However, these approaches are limited in their performance because each prototype must represent the entire image. Alternatively, parts-based approaches like ProtoP- Net (Chen et al., 2019; Donnelly et al., 2021) learn a predefined number of prototypical parts per class. Each class-specific prototype is an image patch represented by a 1 \u00d7 1 spatial unit in the latent space. The network classifies images by summarizing the similarities of all the latent image patches to all prototypes using a fully connected layer. PIPNet (Nauta et al., 2023) aligns learned proto- types with human perception by optimizing the model to assign the same prototype to two different views of the same augmented image patch. ProtoPShare (Rymarczyk et al., 2020) merges simi- lar prototypes after training is complete, while ProtoPool (Rymarczyk et al., 2022) directly learns class-shared prototypes. Other approaches additionally replace the linear output layer with other interpretable models. ProtoKNN (Ukai et al., 2023) classifies the similarity scores using a k-nearest neighbor (KNN) and ProtoTree Nauta et al. (2021) learn a decision tree.\nHyperPgNet learns a predefined number of prototypes per concept, not per class. These concepts are provided as pixel-level annotations extracted by a new labeling pipeline based on the founda- tion models DINOv2 (Oquab et al., 2024) and SAM2 (Ravi et al., 2024). Similar to part-based prototypes, the concept prototypes span a 1 \u00d7 1 spatial unit in the latent space. The prototypes are exclusive to each concept but shared among the classes.\nPrototype learning is also applied to image segmentation. ProtoSeg (Sacha et al., 2023) uses pro- totypical parts a latent space with high spatial resolution for image segmentation. Another method proposes non-learnable prototypes (Zhou et al., 2022), which are trained by clustering in the la- tent space instead of being optimized via Backpropagation. ProtoGMM (Moradinasab et al., 2024) builds on this by updating class-specific prototypes like a Gaussian Mixture Model (GMM) using an Expectation-Maximization (EM) algorithm (Liang et al., 2022) with fixed mixture weights. MG- Proto (Wang et al., 2023) also learns mixture weights for their prototype GMM model.\nHyperPgNet employs HyperPg, a novel prototype representation using Gaussian distributions on a hypersphere's surface with learned parameters: direction a, mean \u00b5 and standard deviation (std)"}, {"title": "3 PROTOTYPICAL GAUSSIANS ON THE HYPERSPHERE", "content": "Prototype Learning is an inherently interpretable machine learning method. The reasoning process is based on the similarity scores of the inputs to the prototypes, retained representations of the training data. For example, a K-Nearest Neighbor (KNN) model is a prototype learning approach with the identity function for representation and an unlimited number of prototypes. In contrast, a Gaussian Mixture Model (GMM) uses a mean representation but restricts the number of prototypes to the number of mixture components.\nPrototype learning for deep neural networks involves finding structures in latent space representa- tions. This section provides an overview of existing methods, which are also illustrated in Fig. 1.\nPrior work uses point-based prototypes, computing similarity scores relative to a single point in la-"}, {"title": "3.1 POINT BASED PROTOTYPES", "content": "The general formulation of prototypes, as defined in previous work (e.g., Chen et al., 2019), is discussed first. Let $D = [X,Y] = \\{(x_i, Y_i)\\}_{i=1}^{N}$ denote the training set, e.g., a set of labeled images, with classes C. Each class $c \\in C$ is represented by Q many prototypes $P_c = \\{P_{c,j}\\}_{j=1}^{Q}$\nSome feature encoder Enc projects the inputs into a D-dimensional latent space Z, with\n$z_i = Enc(x_i)$ being a feature map of shape $s_w \\times s_h \\times D$ with spatial size $s = s_w s_h$. Commonly, the prototypes p are also part of Z with shape $p_w \\times p_h \\times D$, i.e., spatial size $p = p_w p_h$.\nAutoencoder approaches use $p = s$, meaning the prototype represents the entire image and can be reconstructed from latent space (Li et al., 2018). Part-based approaches like ProtoPNet and segmentation models like ProtoSeg use $p = 1$ (Chen et al., 2019; Zhou et al., 2022), meaning each prototype represents some part of the image. Notable exceptions include Deformable ProtoPNet (Donnelly et al., 2021), where each prototype has spatial size $p = 3 \\times 3$ and MCPNet (Wang et al., 2024), where prototypes are obtained by dividing the channels of the latent space into chunks.\nThe prediction is computed by comparing each prototype p to the latent feature map z. For simplic- ity's sake lets assume the spatial dimensions p = s = 1. The following equations can be adapted for higher spatial dimensions by summing over $\\Sigma_{p_w} \\Sigma_{p_h}$ for each chunk of the latent map.\nProtoPNet's prototypes leverage the L2 similarity. The L2 similarity measure is defined as\n$S_{L_2} (z|p) = log(\\frac{||z - p||^2 + 1}{||z||^2 ||p||^2 + \\epsilon})$ (1)\nand is based on the inverted L2 distance between a latent vector z and a prototype vector p. This similarity is a point-based measure, as only the two vectors are compared, without any additional context like the expected variance of the cluster represented by the prototype.\nHyperspherical prototypes using the cosine similarity have been shown to perform well in classifica- tion tasks (Mettes et al., 2019) and have been widely used since (e.g., Zhou et al., 2022; Ukai et al., 2023). The cosine similarity is defined as\n$S_{cos}(z|p) = \\frac{z^T p}{||z||_2 ||p||_2}$ (2)\nwhich is based on the angle between two normalized vectors of unit length. By normalizing D di- mensional vectors to unit length, they are projected onto the surface of a D dimensional hypershere. The cosine similarity is defined on the interval [-1,1] and measures: 1 for two vectors pointing in the same direction, 0 for orthogonal vectors, and -1 for vectors pointing in opposite directions. Like the L2 similarity, the cosine similarity is a point-based measure comparing only two vectors.\nBoth the L2 and cosine similarity can be used for classification. The similarity scores are processed by a fully connected layer (e.g., Chen et al., 2019; Donnelly et al., 2021), or a winner-takes-all approach assigns the class of the most similar prototype (e.g., Sacha et al., 2023). Prototypes can be learned by optimizing a task-specific loss, such as cross-entropy, via backpropagation. Alternatively, Zhou et al. (2022) propose \u201cnon-learnable\u201d prototypes, whose parameters are obtained via clustering in the latent space rather than backpropagation."}, {"title": "3.2 GAUSSIAN PROTOTYPES", "content": "Gaussian prototypes model prototypes as a Gaussian distribution with mean and covariance. They adapt to the spread of the associated latent cluster by adjusting their covariance matrix. Thus, a"}, {"title": "3.3 GAUSSIAN PROTOTYPES ON THE HYPERSPHERE - HYPERPG", "content": "Prototypical Gaussians on the Hypersphere (HyperPg) combine the advantages of Gaussian and hyperspherical prototypes. HyperPg prototypes are defined as $p_H = (\\alpha, \\mu, \\sigma)$ with a directional an- chor vector a, scalar mean similarity \u00b5 and scalar standard deviation (std) \u03c3. HyperPg prototypes learn a 1D Gaussian distribution over the cosine similarities to the anchor vector a. Because the cosine similarity is bounded to [-1,1], HyperPg's similarity measure is defined as the PDF of the truncated Gaussian distribution within these bounds. Let $G(x, \\mu, \\sigma)$ be the cumulative Gaussian dis- tribution function. Then HyperPg's similarity measure based on the truncated Gaussian distribution is defined as\n$S_{HyperPg}(z|p_H) = TG(s_{cos}(z|\\alpha); \\mu, \\sigma, -1, 1)$ (4)\n$= \\frac{N (s_{cos} (z|\\alpha); \\mu, \\sigma)}{G(1, \\mu, \\sigma) - G(-1, \\mu, \\sigma)}.$ (5)"}, {"title": "4 TRAINING", "content": "The original ProtoPNet implementation uses three loss functions: a task specific loss like crossen- tropy for classification, a cluster loss to increase compactness within a class's cluster, and a sepa- ration loss to increase distances between different prototype clusters. However, the prototypes of ProtoPNet and its successors are learned in a black-box manner.\nHyperPgNet is a new inherently interpretable deep learning approach built on HyperPg prototypes. It introduces a \"Right for the Right Concept\" loss, inspired by \"Right for the Right Reasons\" (Ross et al., 2017), to restrict the learned prototypes to human-defined concepts. This focus enhances the interpretability and minimizes the influence of confounding factors. This section first provides an overview of the used prototype learning losses, then introduces the Right for the Right Concept loss, and finally discusses the overall network architecture and final multi-objective loss."}, {"title": "4.1 PROTOTYPE LOSSES", "content": "ProtoPNet defines a cluster loss function to shape the latent space such that all latent vectors $z_c \\in Z_c$ with class label c are clustered tightly around the semantically similar prototypes $p_c \\in P_c$. The cluster loss function is defined as\n$L_{clst} = \\frac{1}{N} \\Sigma_{i=1}^{N} \\Sigma_{c\\in C} max_{p_c \\in P_c} max_{z_{c,i} \\in Z_{c,i}} s(p_c, z_{c,i}),$ (6)\nwhere s(,) is some similarity measure. The $L_{clst}$-Loss function increases compactness by in- creasing the similarity between prototypes $p_c$ of class c latent embeddings $z_c$ of class c over all samples.\nAn additional separation loss increases the margin between different prototypes. The separation loss function is defined as"}, {"title": "4.2 RIGHT CONCEPT LOSS", "content": "To ensure prototypes actually correspond to the input pixels containing the concept, and do not re- spond to other factors in the background, HyperPgNet introduces the \"Right for the Right Concept\" (RRC) -Loss inspired by \"Right for the Right Reasons\u201d (RRR, Ross et al., 2017). The RRC loss is defined as\n$L_{RRC} = \\frac{1}{N} \\Sigma_{i=1}^{N} \\Sigma_{k\\in K} ( A_{x_{i,k}} - S(\\frac{\\partial}{\\partial x_i} p_{k} \\in P_k Enc (x_i)) )^2,$ (10)\nwith binary annotation matrix $A_{x_{i,k}} \\in \\{0, 1\\}^{N\\times x\\times W \\times H}$ for each input sample $x_i$ and concept k. This annotation matrix defines for each input image, which pixels contain which concept. In the original RRR paper, the annotation matrix specified relevant regions for the classification task, steering the model's activations away from confounding factors in the background. In HyperPgNet the RRC-Loss further strengthens the prototype-concept association."}, {"title": "4.3 NETWORK ARCHITECTURE", "content": "Fig. 3 illustrates HyperPgNet's Architecture for interpretable image classification. HyperPgNet uses a pretrained, off-the-shelf feature encoder as a backbone, such as the convolution pyramid from"}, {"title": "5 CONCEPT EXTRACTION AT SCALE", "content": "To learn prototypes based on human-defined concepts with RRC-Loss for image classification, pixel level annotations are required. Foundation models are leveraged to generate these annotations, re- ducing the need for human labeling.\nThe CUB-200-2011 dataset (Wah et al., 2011) contains 11,788 images of birds across 200 sub- species. Each image is annotated with up to 15 part locations, such as head, tail and back. The dataset includes 312 binary attributes describing patterns and colors of these parts, or additional shape information like \u201cduck-like\u201d or \u201cowl-like\". For concept learning, 30 attributes are manually selected, excluding small concepts like eye-color or class exclusive attributes like \"duck-like\".\nTo extract pixel level annotation masks, the point annotations for the attributes are given to a Segment Anything 2 (SAM2) model(Ravi et al., 2024). Using the point locations belonging to other attributes as negative points, segmentation masks are generated. The segmentation mask is then assigned the attribute from the dataset annotations to define a concept."}, {"title": "6 EXPERIMENTS", "content": "The experiments were performed on two datasets: CUB-200-2011 bird species classification dataset (Wah et al., 2011) and the Stanford Cars dataset (Krause et al., 2013). Two variants of HyperPgNet were tested, with and without the LRRC loss. HyperPgNet was compared to ProtoPNet with both standard L2 and the newly introduced HyperPg prototypes, and Concept Bottleneck Models (CBM) with joint training scheme (Koh et al., 2020). Segformer and ConvNext served as Transformer and CNN baselines, respectively. All models were implemented with pretrained Segformer backbones in Pytorch. However, HyperPgNet's performance is independent of the chosen backbone architecture.\nIn contrast to prior work, the training and testing was done on full images, without crops to the bounding boxes provided by the datasets. For data augmentation RandomPerspective, RandomHor- izontalFlip, and RandomAffine were used in an online manner. Thus, for CUB-200-2011, each class has 30 training images, instead of the offline augmentation to 1200 images used in prior work (e.g., Chen et al., 2019; Rymarczyk et al., 2020; Ukai et al., 2023). All models were trained with batch size 96 until convergence. The experiments were performed on a workstation with a single NVidia RTX 3090 GPU (24GB VRAM) per model."}, {"title": "6.1 QUANTITATIVE RESULTS", "content": "Table 1 shows the top-1 accuracy for models on the CUB-200-2011 (Birds) and Stanford Cars (Cars) datasets using the Segformer backbone. The Test Accuracy per Epoch curves are visualized in Fig. 5. The Segformer baseline overfit to the small training datasets (17.7% and 1.9% test accuracy), while the ConvNeXt baseline performed better (74.2% and 57.5%). CBM achieved high results quickly, scoring 75.7% on Birds and 79.9% on Cars. ProtoPNet required the most training time, converging only after about 490 epochs, scoring 68% on Birds and 86.4% on Cars. Switching ProtoPNet's prototypes from L2 to HyperPg improved both training speed and accuracy, converging in 200 epochs with 70.5% and 87.4% test accuracy.\nHyperPgNet further improved on this by learning human-defined concepts. HyperPgNet with con- cept aligned prototypes, but without LRRC, outperformed the other models. It achieved 76.5% accuracy on Birds and 88.6% on Cars after only 40 training epochs. With the \"Right for the Right Concept\" loss, the model retained the high learning speed and performance, although the test accu- racy dropped slightly to 74.1% and 81.2%. This performance loss is offset by increased transparency due to more precise concept prototypes."}, {"title": "6.2 QUALITATIVE RESULTS", "content": "HyperPg prototypes do not only enhance the models learning speed and task performance, but also the interpretabilty. Prior work built activations by overlaying the up-sampled latent space activation over the image. Inspired by GradCAM, Fig. 6 shows the gradient of the prototype activations back- propagated to the input image. Class-based prototypes focus on regions relevant for the prediction, but do not follow human defined concepts. With concept-alignment, the prototype focuses more towards the white head of the bird. But as the concept alignment is only encouraged by $L_{clst}$ and $L_{Sep}$ the prototype still activates on other bright areas on the birds body. With the addition of LRRC to the training regime, the prototypes are forced to focus mainly on the annotated region. Thus, the head prototype ignores the wing area, which is learned by a different prototype. The overall learned concept prototypes fit more precisely to the images' annotated areas."}, {"title": "7 CONCLUSION", "content": "This work introduces HyperPg, a new prototype representation learning a probability distribution on the surface of a hypersphere in latent space. HyperPg adapt to the variance of clusters in latent space and improve training time and accuracy compared to other prototype formulations. HyperPgNet leverages HyperPg to learn human defined concept prototypes, instead of black-box optimized pro- totypes. The combination of probabilistic prototypes on the hypersphere and concept aligned proto- types allows HyperPgNet to outperform other prototype learning approaches. One limitation Hyper- PgNet faces are increased computational requirements due to the inclusion of concept annotations and LRRC during training. However, this is offset by an acceleration of the training process regard- ing epochs and number of samples. Coupled with the increased transparency and interpretability of the model, this makes HyperPgNet a strong contender for scenarios with few training samples and high safety requirements, like medical applications or human-robot interaction."}, {"title": "A ADAPTING HYPERPG TO OTHER PROBABILITY DISTRIBUTIONS", "content": "Subsection 3.3 defines HyperPg prototypes $p_H = (\\alpha, \\mu, \\sigma)$ as a Gaussian Distribution with mean \u00b5 and std \u03c3 of cosine similarities around an anchor vector a. This idea of learning a distribution of cosine similarity values around an anchor a can be adapted to other distributions. This sections introduces some potential candidates. As early experiments on the CUB-200-2011 dataset showed no significant difference in performance, these sections are relegated to the appendix."}, {"title": "A.1 CAUCHY DISTRIBUTION", "content": "One theoretical disadvantage of the Gaussian distribution is the fast approach to zero, which is why a distribution with heavier tails such as the Cauchy distribution might be desirable. The Cauchy distribution's PDF is defined as\n$C(x; x_0, \\gamma) = \\frac{1}{\\pi \\gamma} (\\frac{1}{1+(\\frac{x-x_0}{\\gamma})^2})$ (11)\nwith median $x_0$ and average absolute deviation \u03b3. The HyperPg prototypes with Cauchy are defined as accordingly as $p^{Cauchy} = (\\alpha, x_0, \\gamma)$.\nFig. 7 illustrates the PDF of the Gaussian and Cauchy distributions with \u00b5 = $x_0$ = 1 and \u03c3 = \u03b3 = 0.2, i.e., the main probability mass is aligned with the anchor a. The Gaussian distributions PDF quicly approaches zero and stays near constant. This could potentially cause vanishing gradient issues during training. The heavier tails of the Cauchy distribution ensure that for virtually the entire value range of the cosine similarity, gradients could be propagated back through the model. However, experiments on CUB-200-2011 showed no significant performance difference between using HyperPg with the Gaussian or Cauchy distribution."}, {"title": "A.2 TRUNCATED DISTRIBUTIONS", "content": "The cosine similarity is defined only on the interval [-1,1]. This makes it attractive to also use trun- cated probability distributions, which are also only defined on this interval. The truncation imposes a limit on the range of the PDF, thereby limiting the influence of large values for the distribution's \u03c3 or \u03b3 parameter, respectively. The truncated Gaussian pdf $T_{Gauss}$ requires the cumulative probability function G and error function $f_{err}$, and is defined as"}, {"title": "A.3 VON MISES-FISHER DISTRIBUTION", "content": "The von Mises-Fisher distribution (vMF) is the analogue of the Gaussian distribution on the surface of a hypersphere Hillen et al. (2017). The density function $f_a$ of the vMF distribution for a D- dimensional unit-length vector v is defined as\n$f_a(\\nu|\\alpha, \\kappa) = C_a(\\kappa) exp(\\kappa \\alpha^T \\nu),$ (17)\nwith mean vector a, scalar concentration parameter \u03ba and normalization constant $C_a(\\kappa)$. The nor- malization constant $C_a(\\kappa)$ is a complex function and difficult to compute for higher dimensions, which is why, for example, Tensorflow\u00b9 only supports the vMF distribution for D < 5. However, the vMF distribution is a viable similarity measure when using the unnormalized density function with $C_a(\\kappa)$ = 1. Working with unnormalized densities highlights the relationship between the normal distribution and the vMF distribution.\nLet G be the unnormalized PDF of a multivariate Gaussian with normalized mean a and isotropic covariance $\u03c3^2$ = $\u03ba^{-1}I$, then it is proportional to the vMF distribution for normalized vectors v with |v| = 1, as shown by"}, {"title": "A.4 FISHER-BINGHAM DISTRIBUTION", "content": "As the vMF distribution is the equivalent of an isotropic Gaussian distribution on the surface of a hy- persphere, the Fisher-Bingham (FB) distribution is the equivalent of a Gaussian with full covariance matrix. Similar to the vMF, the normalization constant is difficult to compute for higher dimensions, but the unnormalized density function remains feasible.\nFor a D dimensional space, the FB distribution is by a D \u00d7 D matrix A of orthogonal vectors ($a_1, a_2, ..., a_D$), concentration parameter \u03ba and ellipticity factors $[\u03b2]_{j=2:D}$ where $\\Sigma_{j=2}^{D} \u03b2_j = 1$ and 0 \u2264 2|$\u03b2_j$| < \u03ba. The FB unnormalzied PDF is defined as\n$\\Gamma(\\nu | A, \\kappa, \u03b2) = exp(\\kappa a_1^T \\nu + \\Sigma_{j=2}^{D} \u03b2_j (a_j^T \\nu)^2)$ (25)\nThe FB distribution's main advantage is the elliptic form of the distribution on the surface of the hypersphere, offering higher adaptability than the other formulations. However, the parameter count and constraints are higher."}, {"title": "A.5 MIXTURE MODELS", "content": "HyperPg's probabilistic nature lends itself to a mixture formulation. Let the definition of a HyperPg Mixture Prototype be $p^M = (\\alpha, \\mu, \\sigma, \u03c0)$ with additionally learned mixture weight \u03c0. Further, let's define the probability of a latent vector z belonging to a Gaussian HyperPg prototype p as"}, {"title": "B ABLATIONS", "content": ""}, {"title": "B.1 BACKBONE PERFORMANCE", "content": "HyperPgNets performance is not dependent on a CNN or Transformer based backbone. Fig. 10 shows the average test accuracy curves for HyperPgNet with concept aligned prototypes on CUB- 200-2011. The maximum differnece in test accuracy between both architectures is 2%."}, {"title": "B.2 NUMBER OF PROTOTYPES", "content": "The number of prototypes per class or concept is an important hyper-parameter for prototype learn- ing models like HyperPgNet. The choice of the number of prototypes controls, how many clusters the model should fit in latent space. For example, ProtoPNet requires 10 prototypes per class or 2000 prototypes in total for the CUB-200-2011 dataset (Chen et al., 2019). ProtoPool is able to merge similar prototypical parts, requiring only 202 prototypes for its best performance (Rymarczyk et al., 2022).\nThe change from class specific to concept aligned prototypes improves overall test performance, and the added information during training leads to fewer training epochs. Concept Bottleneck Models (CBM) are able to outperform the other models with only 30 learned concepts, or 1 prototype per concept. This indicates that the concept annotations on CUB-200-2011 do not produce multi-modal distributions in latent space, which would require more prototypes per concept. This behavior is confirmed in ablation studies on the number of prototypes for HyperPgNet. Fig. 11 presents the test accuracies for HyperPgNet on full CUB-200-2011 images after 60 training epochs with different numbers of prototypes per concept."}, {"title": "C EXTENDED INTERPRETABILITY ANALYSIS", "content": ""}, {"title": "C.1 RIGHT FOR THE RIGHT CONCEPT", "content": "Fig. 12 shows more example prototypes for HyperPgNet. Without $L_{RRC}$, the prototype do not always correspond to the defined prototype. For the \"Orange Body\" concept, the prototype also activates on confounding factors in the background. With $L_{RRC}$ during training, the model learns to ignore these factors.\nWith the car dataset, the effect is even more pronounced. First analyses on the Stanford Cars exper- iments indicate, that most models focus on the front bumper below the headlights. This part of the car seems to be highly predictive for the class label. Because of this, the HyperPgNet, even with context annotations but without $L_{RRC}$ will focus more on these areas of the image. For example the \"bonnet\" prototype does not focus on the car bonnet at all. Only when trained with $L_{RRC}$, the prototype actually focuses on the human defined concept."}, {"title": "C.2 LATENT SPACE STRUCTURE", "content": "Prototype learning is based on learning structures in the latent space. HyperPg specifically learns Gaussian distributions on the surface of a hypersphere in a high-dimensional latent space. Dimen- sionality reduction techniques like UMAP (McInnes et al., 2018) aim to preserve global and local structures from a high dimensional space when projecting into a low dimensional one. UMAP sup- ports multiple distance metrices, enabling the projection onto the surface of a 3D sphere.\nFig. 13 illustrates UMAP projections of HyperPg concept-aligned prototypes trained on Stanford Cars. This visualization indicates that HyperPgNet is able to disentangle the different concepts in latent space, as the projection shows no overlap of the different clusters. When trained with $L_{RRC}$, the prototypes are packed closer together. This could indicate, that the chosen hyper parameter of 20"}]}