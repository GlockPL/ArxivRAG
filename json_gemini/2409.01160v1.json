{"title": "EXPANDING ON ENCLAP WITH AUXILIARY RETRIEVAL MODEL FOR AUTOMATED AUDIO CAPTIONING", "authors": ["Jaeyeon Kim", "Jaeyoon Jung", "Minjeong Jeon", "Sang Hoon Woo", "Jinjoo Lee"], "abstract": "In this technical report, we describe our submission to DCASE2024 Challenge Task6 (Automated Audio Captioning) and Task8 (Language-based Audio Retrieval). We develop our approach build-ing upon the EnCLAP audio captioning framework and optimizing it for Task6 of the challenge. Notably, we outline the changes in the underlying components and the incorporation of the reranking pro-cess. Additionally, we submit a supplementary retriever model, a byproduct of our modified framework, to Task8. Our proposed sys-tems achieve FENSE score of 0.542 on Task6 and mAP@10 score of 0.386 on Task8, significantly outperforming the baseline models.", "sections": [{"title": "1. INTRODUCTION", "content": "Automated audio captioning (AAC) refers to the cross-modal trans-lation task of transcribing audio signals that contain sound events into concise and meaningful natural language descriptions [1]. De-spite the recent success of deep learning in many traditional tasks, AAC remains a particularly challenging task, with substantial per-formance discrepancy between human and machine.\nOne significant contributor of the performance gap can be at-tributed to the intrinsic complexity of the task. Distinguishing be-tween various sound events, especially between similar and ambigu-ous ones, requires extensive real-world knowledge. To mitigate this challenge, prior studies have incorporated additional real-world acoustic knowledge by employing pretrained audio encoders trained on audio classification tasks [2, 3, 4].\nThe scarcity of high-quality data poses an additional challenge in audio captioning. Notably, AudioCaps [5] and Clotho [6], two most widely used datasets for audio captioning, contain approxi-mately 50K and 20K captions, respectively, while COCO captions [7], a widely used dataset for image captioning, has over 414K cap-tions in its training set. Although Mei et al. [8] proposed WavCaps, a large-scale audio captioning dataset comparable in scale to COCO captions, it is important to note that WavCaps is a weakly-labeled dataset and cannot be considered a direct substitute for a high-quality dataset. To address this issue, previous works [9, 10, 11] have leveraged the text generation capabilities of pretrained lan-guage models like GPT-2 [12] and BART [13] to improve the se-mantic quality of the captions under data-scare scenarios. Additionally, some studies have also incorporated auxiliary loss terms, including keyword prediction loss [14] or sentence embedding loss [15], to provide additional training signal and improve the training procedure.\nExpanding upon previous line of works, Kim et al. [16] pro-posed the EnCLAP framework, which integrates a set of pretrained models with an auxiliary training task. Notably, EnCLAP utilizes two acoustic feature encoders, EnCodec [17] and CLAP [18], to generate timestep-level and sequence-level representation, of the in-put audio sequence, respectively. For caption decoder, the frame-work employs a pretrained BART model [13]. Furthermore, Kim et al. also introduced masked codec modeling (MCM), an auxiliary task designed to enhance the acoustic awareness of the caption de-coder. The combination of these approaches allowed EnCLAP to achieve state-of-the-art performance on the AudioCaps dataset.\nIn this work, we adapt the EnCLAP framework to tackle DCASE2024 Challenge. We aim to optimize and enhance the per-formance of each component within the EnCLAP framework, while adhering to the challenge's rules and regulations. Specifically, we investigate alternative models for the EnCodec and CLAP compo-nents and adopt a sampling and reranking procedure to further im-prove the quality of the generated captions. We submit our resulting system to Task6 and Task8 of the challenge."}, {"title": "2. METHOD", "content": "Neural audio codecs are autoencoder models designed to encode waveforms into sequences of discrete codes. Recent advancements [19, 17, 20] typically employ residual vector quantization (RVQ) for compression, utilizing multiple codebooks to quantize the resid-uals of preceding codebooks. Ultimately, the input waveforms are transformed into a set of parallel discrete code sequences, each of which is associated with a unique codebook. Neural audio codecs have demonstrated success as the acoustic representation format in generative audio models [21, 22, 23].\nIn the context of audio captioning, EnCLAP [16] employs the neural audio codec, specifically EnCodec [17], to represent the in-put waveform at the timestep level. This approach is based on the assumption that pretrained language models are better suited to pro-cess discrete inputs compared to continuous ones. In this work, we replace EnCodec in the original EnCLAP framework with Descript"}, {"title": "3. EXPERIMENT", "content": "We assess the performance of our modified EnCLAP model on Task6 of DCASE2024 Challenge and report the results. Addition-ally, we evaluate the retriever model described in Section 2.2 on Task8 of the challenge."}, {"title": "3.1. Setup", "content": "Dataset. In our experiment, we adopt a two-stage training process, where we pretrain on a larger dataset and subsequently finetune on a smaller dataset. The pretraining dataset comprises a combination of AudioCaps [5], WavCaps [8], Clotho [6], and Clotho-ChatGPT-Mixup [4]. Conversely, the finetuning dataset consists solely of the Clotho dataset. To comply with the challenge regulations, we exclude any potential overlapping data from Freesound in the Wav-Caps dataset. Additionally, we only use audio clips with durations between 1 and 30 seconds from the WavCaps dataset. For Clotho, we utilize only the training split of the dataset.\nModel Configuration. From the original EnCLAP model configu-ration, we experiment only with the EnCLAP-large setup to maxi-mize the performance of the final model. We use a variant of the DAC model that transforms a 24kHz acoustic waveform to 75Hz code sequences, with 32 codes per timeframe and codebook size of 1024. For the retriever model, we initialize the audio encoder with CNext-tiny by Pelligrini et al. [25]. For the text encoder, we exper-iment with 6 different pretrained language models: BGE-base [28], BERT-base [29], RoBERTa-base [30], BGE-large [28], BERT-large [29], ROBERTa-large [30]. For the sequence-level feature encoder, we choose Pretrain 1 version listed in Table 1. Note that we resam-ple the input audio to appropriate sample rates before processing it with feature encoders.\nGeneration. We use nucleus sampling with a probability threshold of 0.95 and a temperature of 0.5 to generate 30 candidates. We rank the candidates by the weighted sum of the encoder reranking score and the decoder reranking score using weights of 0.7 and 0.3, respectively."}, {"title": "3.2. Training", "content": "Language-Based Audio Retrieval. We train our retriever models using the m-LTM framework [31], a learning-to-match framework for the minibatch setting, designed to minimize the modality gap be-tween audio and text embedding in audio-text retrieval tasks. Dur-ing the pretraining phase, we use a mixed dataset comprising Au-dioCaps [5], WavCaps [8], and Clotho [6].\nAutomated Audio Captioning. For audio caption training, we fol-low the original EnCLAP setup and use a combination of two tasks: captioning task and MCM task. MCM is an auxiliary training task which involves masking a part of the input codec sequence and pre-dicting it, analogous to the masked language modeling (MLM) ap-proach. Note that we omit the MCM task during the finetuning stage. During the pretraining stage, we use a combined dataset of AudioCaps [5], WavCaps [8], and Clotho-ChatGPT-Mixup [4]."}, {"title": "3.3. Results", "content": "Language-based Audio Retrieval. We present the results of our evaluation on Task8 in Table 1. Our models significantly outper-form the baseline. We also find that ensembling models with dif-ferent encoders yields additional score improvements. For the chal-lenge, we submit the following four models:\n1. Finetune 1: CNext audio encoder and RoBERTa-large text encoder\n2. Ensemble 1: An ensemble of the top 3 fine-tuned models: Finetune 4, 5, 6\n3. Ensemble 2: An ensemble of all fine-tuned models\n4. Ensemble 3: An ensemble of all pre-trained and fine-tuned models\nAutomated Audio Captioning. We summarize the results of our evaluation on Task6 in Table 2. Our models surpass both the DCASE2024 baseline and EnCLAP-large by a wide margin. The details of our submissions are as follows:\n1. Submission 1: A modified EnCLAP model with DAC and CNext audio-text joint embedding\n2. Submission 2: An average soup [32] model of 5 modified EnCLAP models\n3. Submission 3: An ensemble of 7 modified EnCLAP models\n4. Submission 4: An ensemble of 7 modified EnCLAP models and 2 soup models"}, {"title": "4. CONCLUSION", "content": "This report outlines our approach to DCASE2024 Challenge. We investigate the application of the recently introduced m-LTM loss to language-based text retrieval. For automated audio captioning, we attempt to optimize and improve the the EnCLAP framework by introducing new backbone models, that is, specifically by replac-ing EnCodec with DAC and CLAP with the audio encoder from the aforementioned retriever. We also integrate a sampling-and-reranking scheme to the generation procedure. In our future work, we hope to investigate the reciprocal effects of captioning and re-trieval tasks."}]}