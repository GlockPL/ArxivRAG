{"title": "DORMANT: Defending against Pose-driven Human Image Animation", "authors": ["Jiachen Zhou", "Mingsi Wang", "Tianlin Li", "Guozhu Meng", "Kai Chen"], "abstract": "Pose-driven human image animation has achieved tremendous progress, enabling the generation of vivid and realistic human videos from just one single photo. However, it conversely exacerbates the risk of image misuse, as attackers may use one available image to create videos involving politics, violence and other illegal content. To counter this threat, we propose DORMANT, a novel protection approach tailored to defend against pose-driven human image animation techniques. DORMANT applies protective perturbation to one human image, preserving the visual similarity to the original but resulting in poor-quality video generation. The protective perturbation is optimized to induce misextraction of appearance features from the image and create incoherence among the generated video frames. Our extensive evaluation across 8 animation methods and 4 datasets demonstrates the superiority of DORMANT over 6 baseline protection methods, leading to misaligned identities, visual distortions, noticeable artifacts, and inconsistent frames in the generated videos. Moreover, DORMANT shows effectiveness on 6 real-world commercial services, even with fully black-box access.", "sections": [{"title": "1 Introduction", "content": "Diffusion models such as Stable Diffusion [55], DALL-E [54], and Imagen [59] series models, have demonstrated their unprecedented capabilities in the field of image generation. More recently, video diffusion models like Stable Video Diffusion [7] and Sora [49], have also achieved significant advancements, capable of producing movie-quality and professional-grade videos. In particular, pose-driven human image animation methods have emerged [39], enabling the generation of controllable and realistic human videos by animating reference images according to desired pose sequences. The resulting videos maintain the appearance of the original reference while accurately following the motion guidance provided by the poses. This innovation holds considerable potential across various applications, including social media, entertainment videos, the movie industry, and virtual characters, etc.\nWhile pose-driven human image animation methods have revolutionized video generation, they also significantly lower the barriers to creating deceptive and malicious human videos, raising serious concerns about unauthorized image usage. With just a single image of the victim, attackers can generate countless human videos, depicting the individual performing any action dictated by the pose input. For example, attackers can create dancing videos of celebrities to post on video platforms for commercial gains, fabricate fake videos of politicians to incite social harm, or produce NSFW videos that disrupt people's normal lives [51]. These manipulated videos are easy and low-cost to generate, yet they severely violate the victim's portrait and privacy rights and potentially cause considerable harm to the unsuspecting individual.\nDespite the severity of these hazards, there is a clear lack of research specifically aimed at preventing the misuse of pose-driven human image animation. To our knowledge, the most relevant concurrent work is VGMShield [50], which is also the only protection method designed to counteract malicious image-to-video generation. VGMShield applies protective perturbation to the image, leading to low-quality video generation by producing incorrect or bizarre frames. The protective perturbation is optimized by targeting the encoding process of Stable Video Diffusion (SVD) [7], deceiving both the image and video encoders into misinterpreting the input image. However, this deviation in the embedding space is not sufficient to effectively disrupt the appearance features contained in images, thus failing to adequately protect human portraits. Furthermore, VGMShield demonstrates limited effectiveness and poor transferability when defending against non-SVD-based video generation methods, including various pose-driven human image animation techniques.\nWhile few studies focus specifically on the misuse of image-to-video generation, several protection methods exist to defend against text/image-to-image generation [41, 44, 60, 61, 65, 73, 75, 77, 84]. These methods typically optimize protective perturbations by targeting the fine-tuning process of customization techniques [57], the encoding process of the Variational AutoEncoder [18], or the reverse process of the denoising UNet [27]. However, current protections against image generation are inadequate for defending against pose-driven human image animation, as they fail to effectively deceive the advanced feature extractors in animation methods to disrupt appearance features, and break the temporal consistency across frames that is essential in video generation.\nIn this paper, we propose DORMANT (Defending against Pose-driven Human Image Animation), to address this notable gap in effective protection methods that prevent unauthorized image usage in pose-driven human image animation, thereby safeguarding portrait and privacy rights. As illustrated in Figure 1, DORMANT applies protective perturbation to the reference image, resulting in the protected image that appears visually similar to the original but leads to poor-quality video generation when used, causing issues including mismatched appearances, distorted visuals, noticeable artifacts and incoherent frames. We assume black-box access to the animation methods and models that the potential attacker might use, and employ a transfer-based strategy to optimize the protective perturbation according to our proposed objective function, utilizing publicly available pre-trained models as surrogates. The optimization objective includes 1) feature misextraction, which induces deviations in the latent representation and comprehensively disrupts the semantic and fine-grained appearance features of the reference image; 2) frame incoherence, which targets the appearance alignment and temporal consistency among the generated video frames. We further adopt Learned Perceptual Image Patch Similarity (LPIPS) [79], Expectation over Transformation (EoT) [5], and Momentum [17] to enhance the imperceptibility, robustness, and transferability of the protective perturbation, respectively.\nWe evaluate the protection performance of DORMANT on 8 cutting-edge pose-driven human image animation methods. The experimental results across 6 image-level and video-level generative metrics demonstrate the superior effectiveness of DORMANT, compared to 6 baseline protections against image and video generation. We conduct experiments on 4 datasets, covering tasks such as human dance generation, fashion video synthesis, and speech video generation. Results of human and GPT-40 [48] studies further highlight the superiority of DORMANT from the perspectives of human perception and multimodal large language model. DORMANT also exhibits remarkable robustness against 5 popular transformations and 6 advanced purifications. To further assess the transferability of DORMANT, we conduct additional experiments on 4 image-to-image techniques, 4 image-to-video techniques, and 6 real-world commercial services, DORMANT effectively defends against these various generative methods."}, {"content": "\u2022 We address the challenge of effectively preventing unauthorized image usage in pose-driven human image animation, safeguarding individual's rights to portrait and privacy.\n\u2022 We design novel objective functions to optimize the protective perturbation, inducing misextraction of appearance features and incoherence among generated video frames.\n\u2022 Extensive experiments conducted on 8 pose-driven human image animation methods, 4 image-to-image methods, 4 image-to-video methods, and 6 commercial services highlight the effectiveness and transferability of DORMANT\u00b9."}, {"title": "2 Background", "content": "2.1 Latent Diffusion Model\nDifferent from the Denoising Diffusion Probabilistic Model (DDPM) [27] which directly operates in pixel space, to reduce the computational demand of Diffusion Model (DM), the Latent Diffusion Model (LDM) [55] applies DM training and sampling in the latent space, thus striking a balance between image quality and throughout. LDM utilizes the Variational AutoEncoder (VAE) [18] to provide the low-dimensional latent space, which consists of an encoder E and a decoder D. More precisely, given an image x, the encoder E maps it to a latent representation: z = E(x), and then the decoder D reconstructs it back to the pixel: x = D(z) = D(E(x)).\nLDM performs the diffusion process proposed in DDPM within the latent space, where the forward process iteratively adds Gaussian noise to the latent representation to make it noisy, while the reverse process predicts and denoises applied noise using a denoising UNet [56]. Specifically, given an image x, during the forward process, its latent representation z0 = E(x) is perturbed with Gaussian noise over T timesteps, transforming z0 into a standard Gaussian noise zT:"}, {"title": null, "content": "q(z_{1:T}|z_0) = \\prod_{t=1}^{T} q(z_t|z_{t-1}),\n    q(z_t|z_{t-1}) = \\mathcal{N}(z_t; \\sqrt{1 - \\beta_t}z_{t-1}, \\beta_tI),\n                                                                                                                                                                     (1)"}, {"title": null, "content": "where {\u03b2t \u2208 (0, 1)}^T_{t=1} is the variance schedule. By defining \u03b1t = 1 \u2212 \u03b2t and \\bar{\u03b1}_t = \\prod_{i=1}^{t} \u03b1_i, we can use the reparameterization technique [36] to sample zt at timestep t as follows:\nq(z_t|z_0) = \\mathcal{N}(z_t; \\sqrt{\\bar{\u03b1}_t}z_0, (1 - \\bar{\u03b1}_t)I),\nz_t = \\sqrt{\\bar{\u03b1}_t}z_0 + \\sqrt{1-\\bar{\u03b1}_t}\u03b5_t, \\quad \u03b5_t \\sim \\mathcal{N}(0, 1).\n                                                                                                                                                                                    (2)\nThe reverse process aims to denoise the noisy latent representation zT back to its original state with the following transition starting at p(z_T) = \\mathcal{N}(z_T;0;I):\np_\\theta(z_{0:T}) = p(z_T) \\prod_{t=1}^{T} p_\\theta(z_{t-1}|z_t),\np_\\theta(z_{t-1}|z_t) = \\mathcal{N}(z_{t-1};\u03bc_\\theta(z_t, c, t), \u03a3_\\theta(z_t, c, t)),\n                                                                                                                                                                               (3)\nwhere c represents the embedding of conditional information such as text embedding obtained from CLIP ViT-L/14 text encoder [53], the mean \u03bc\u03b8 (zt, c, t) and the variance \u03a3\u03b8 (zt, c, t) are computed by the denoising UNet \u03b5\u03b8 parameterized by \u03b8. With Eq. (2), Ho et al. [27] simplify the learning objective of the denoising UNet \u03b5\u03b8 in the \u03b5-prediction form, where \u03b5\u03b8 is trained to predict the added noise \u03b5t at timestep t:\nL_{LDM} = \\mathbb{E}_{z_0, c, \u03b5_t \\sim \\mathcal{N}(0, 1), t} [ || \u03b5_t - \u03b5_\\theta (z_t,c,t)||^2 ].\n                                                                                                                                                                                (4)\nOnce trained, given zT sampled from the Gaussian distribution, the denoising UNet \u03b5\u03b8 can be utilized to predict \u03b5t and computed zT\u22121 according to Eq. (3). Subsequently, zT\u22122, zT\u22123, . . . , z1, z0 are progressively calculated, and finally z0 is decoded by D to generate the image x = D(z0)."}, {"title": "2.2 LDM for Human Image Animation", "content": "Pose-driven human image animation [31, 39, 67, 69, 72, 86] is an image-to-video task aimed at generating a human video by animating a static human image based on a given pose sequence. As shown in Figure 1, the created video needs to accurately align the appearance details from the reference image while following the motion guidance from the pose sequence, which can be produced by DWPose [74], DensePose [23], or OpenPose [11]. Recently, LDM-based animation methods have demonstrated superior effectiveness in addressing challenges in appearance alignment and pose guidance, enabling the generation of realistic and coherent videos.\nUsing LDM as the backbone, Stable Diffusion (SD) has achieved remarkable success in text/image-to-image generation tasks [55, 78]. Pre-trained SD models effectively capture high-quality content priors. However, their network architecture is inherently designed for image generation, lacking the capability to handle the temporal dimension required for video generation. To extend foundational text/image-to-image models for image animation, many works [15, 31, 68, 72, 86] have incorporated temporal layers [24] into the denoising UNet for temporal modeling. Specifically, the feature map X \u2208 Rb\u00d7f\u00d7h\u00d7w\u00d7c is reshaped to X \u2208 R(b\u00d7h\u00d7w)\u00d7f\u00d7c, after which temporal attention is performed to capture the temporal dependencies among frames. Recently, some studies [52, 67, 82] have also adopted Stable Video Diffusion [7] as the backbone for image animation. SVD is an open-source image-to-video LDM trained on a large-scale video dataset, its strong motion-aware prior knowledge facilitates maintaining temporal consistency in image animation.\nWhile pose-driven human image animation has demonstrated impressive capabilities in creating vivid and realistic human videos, it also raises serious concerns about unauthorized image usage by malicious attackers. With just a single photo of the victim, easily obtained from the web or social media, the malicious attacker can generate an unlimited number of human videos, manipulating the victim to perform any poses and thereby severely violating the individual's rights to publicity and privacy. These manipulated videos could be used for commercial or political purposes, potentially inflicting significant harm on the unsuspecting victim."}, {"title": "2.3 Protection Methods against LDM", "content": "While few studies addressing the issue of image-to-video generation, prior research has proposed various protection methods [41, 44, 60, 61, 65, 73, 75, 77, 84] against LDM to prevent image misuse in text/image-to-image generation. In the context of image generation, there are two primary LDM-based image misuse scenarios: concept customization and image manipulation. Existing protection methods mainly utilize Projected Gradient Descent (PGD) [45] to generate adversarial examples [13, 21] for LDM, thereby safeguarding against malicious image generation in both misuse scenarios. The defender aims to introduce protective perturbation \u03b4 to the image x, such that the protected image xp = x + \u03b4 can deceive the LDM, resulting in poor-quality generation.\nDefending against Concept Customization. Concept customization techniques [20, 30, 37, 57] fine-tune a few parameters of a pre-trained text-to-image model so that it quickly acquires a new concept given only 3-5 images as reference. The training loss of the most popular work DreamBooth [57] introduces a prior preservation loss to LLDM in Eq. (4):"}, {"title": null, "content": "+ \u03bb ||\\acute{\u03b5} - \u03b5_\\theta(z_{t+1}', c_{pr}, t')||_1^2 ],\n                                                                                                                                                           (5)"}, {"title": null, "content": "where z'_{t+1} is the noisy latent representation of the class sample x' generated from LDM with prior prompt cpr, \u03bb controls for the weight of the prior term, which prevents over-fitting and text-shifting problems. Depending on the conceptual properties, there are two variants: style mimicry [61] and subject recontextualization [3]. Style mimicry seeks to generate paintings in an artist's style without consent, while subject recontextualization aims to memorize a subject (e.g., a portrait of a victim) and generate it in a new context. Protection methods defending against concept customization [44, 65, 84] mainly disrupt the learning process in Eq. (5) by solving the bi-level optimization: \u03b4 = arg max\u03b4LLDM(E(x+\n\u03b4), \u03b8'), s.t. \u03b8' = argmin\u03b8LDB(E(x + \u03b4), \u03b8), aimed at finding protective perturbation \u03b4 which degrades the personalization generation ability of DreamBooth, thereby preventing unauthorized concept customization.\nDefending against Image Manipulation. Image Manipulation techniques [8, 9, 16, 46] leverage pre-trained image-to-image models to directly manipulate a single image, altering its appearance or transferring its style. Protection methods aimed at defending against image manipulation focus on disrupting the encoding process (x\u2192 z0) [60, 61, 75] and the reverse process (zT \u2192 z0) [41, 73, 77]. The protective perturbation \u03b4 is optimized by fooling the VAE \u03b4 = argmax\u03b4 ||E(x+\u03b4) \u2212 E(x)||2 and the denoising UNet \u03b4 = arg max\u03b4LLDM(E(x+ \u03b4LLDM(E(x+\u03b4)), thus making the LDM to generate images significantly deviating from original image x.\nThere are several differences between concept customization, image manipulation, and pose-driven human image animation. First and foremost, concept customization and image manipulation are image generation tasks, whereas human image animation falls under the category of image-to-video generation. Second, concept customization requires several images (typically 3-5), while the other two techniques operate on a single image. Third, concept customization involves fine-tuning the LDM to memorize and generate a new concept. In contrast, image manipulation and human image animation leverage frozen pre-trained LDM directly for image or video generation without additional learning. Fourth, concept customization uses the images as training data for fine-tuning, image manipulation directly edits the given image itself, while human image animation extracts appearance features from the reference image to guide video generation, rather than using the image directly.\nThese various ways of using images necessitate the design of specific objective functions for each case when optimizing the protective perturbation. Existing protections against concept customization and image manipulation mainly target the fine-tuning process, or the encoding and reverse processes, respectively. However, these methods are ineffective at disrupting the appearance features contained in human images. As shown in Figure 3 in Section 4.2, only DORMANT effectively induces noticeable mismatches in identity appearance between reference images and generated videos. The reason is that protective perturbations optimized with current objectives have a limited protective effect on various well-trained feature extractors used in animation methods. The ablation study results in Figure 8 in Section 4.4 also emphasize the need for specifically designed objective functions to induce feature misextraction, which existing methods lack. Moreover, they also neglect to break the temporal consistency across frames specifically required in video generation. As a result, current protections against image generation are insufficient for defending against pose-driven human image animation.\nWhile many protection methods exist to prevent unauthorized image usage in text/image-to-image generation, there is a notable gap in defending against image-to-video generation. To our knowledge, the only concurrent work addressing this issue is VGMShield [50], which aims to prevent malicious image-to-video generation using SVD [7] by generating adversarial examples. More precisely, it attacks the encoding process of SVD, aiming at deceiving the image encoder and video encoder into misinterpreting the image. However, this deviation specifically in the embedding space of SVD is insufficient to effectively disrupt the appearance features in human images and shows poor effectiveness and transferability when defending against non-SVD-based video generation methods, including various pose-driven human image animation techniques. Overall, the ineffectiveness of existing protections against image and video generation highlights the need for novel protection methods specifically designed to prevent malicious pose-driven human image animation."}, {"title": "3 Methodology", "content": "DORMANT prevents image misuse in pose-driven human image animation by adding protective perturbation \u03b4 to the human image x, resulting in the protected image xp = x+\u03b4.\nxp is visually similar to x but would lead to poor-quality video generation if used, causing issues such as mismatched appearances, visual distortions, and frame inconsistencies. We assume a fully black-box setting for the animation methods and models that potential attackers might use and employ a transfer-based strategy to optimize \u03b4 according to our proposed objective function LDORMANT, relying on some surrogate pre-trained models (e.g., the publicly available CLIP model [53]) to which the defender has white-box access. As shown in Figure 2, LDORMANT contains: 1) Lvae and Lfeature, which aim to induce misextraction of appearance features from the reference image; 2) Lframe, which is designed to cause incoherence among the generated video frames. Additionally, we adopt LPIPS [79], EoT [5], and Momentum [17], to further enhance the imperceptibility, robustness, and transferability of \u03b4, respectively. The protective perturbation is optimized using PGD [45], which iteratively updates \u03b4 with a step size \u03b3 under an L\u221e norm constraint \u03b7 as follows:"}, {"title": null, "content": "\\delta_t = \\delta_{t-1} + \\gamma \\cdot sign(\\nabla_{\\delta}L_{DORMANT}), \\quad s.t. ||\\delta||_{\\infty} \\leq \\eta.\n                                                                                                                                                         (6)"}, {"title": "3.1 Threat Model", "content": "Attacker. The attacker aims to generate human videos using pose sequences based on the image obtained without the owner's consent. These fabricated videos could be misused for commercial or political purposes, violating the victim's rights to portrait and privacy. We assume the attacker maliciously acquires a single human image of the victim from the web or social media. Notably, even one image is enough to generate an unlimited number of videos. The attacker also has access to various LDMs for pose-driven human image animation, and significant computational power to run them.\nDefender. We assume that the defender intends to apply protective perturbation to a human image before posting it on-line or on social media, to prevent potential unauthorized human video generation and thereby safeguard portrait and privacy rights. The protected image appears visually similar to the original image to human perception. However, any videos generated using this protected image would display mismatches in identity appearance and degraded quality, such as distorted backgrounds, visible artifacts, and incoherent frames, etc. To optimize the protective perturbation, the defender has access to some publicly available feature extractors and LDMs for pose-driven human image animation, but is unaware of the specific animation methods and models that the attacker might use. The defender could be either the image owner or a trustworthy third party with sufficient computing resources to execute the protection method effectively."}, {"title": "3.2 Feature Misextraction", "content": "As mentioned in Section 2.3, the mainstream usage of the reference image in pose-driven human image animation involves extracting its appearance features to guide video generation. In this context, the objective of optimizing the protective perturbation \u03b4 is to disrupt these appearance features, causing the feature extractor to extract incorrect features from the protected image. As a result, videos generated with this faulty guidance would differ from the reference image, thereby preventing unauthorized usage in human video generation.\nVAE Encoder. As mentioned in Section 2.1, to reduce computational demands, LDM first maps the input image to a latent representation using the VAE encoder before performing the diffusion process. Given this, an intuitive strategy for inducing feature misextraction is to disrupt the encoding process, causing the encoder E to map the protected image xp to a \u201cwrong\u201d latent vector zp = E(x+\u03b4) that deviates significantly from the original latent z = E(x). Consequently, this misencoded latent zp would contain incorrect appearance features, thus misleading the video generation process. The optimization objective Lvae is to maximize the distance between x and xp in the VAE latent space, as defined by:"}, {"title": null, "content": "argmaxL_{vae} = arg max ||E(x+\\delta) - E(x)||_2.\n||\\delta||_{\\infty} \\leq \\eta\n                                                                                                                               (7)"}, {"title": null, "content": "We utilize the encoder E from sd-vae-ft-mse [1], a VAE fine-tuned on an enriched dataset with images of humans to improve the reconstruction of faces.\nHowever, the deviation of the latent representation alone is insufficient to induce a deep-level misextraction of the appearance features. This is because pose-driven human image animation methods typically have stringent requirements for appearance alignment and often use pre-trained or custom-designed models for additional feature extraction. As a result, the attack on the VAE encoder has only a limited effect on these advanced feature extractors. To comprehensively disrupt the appearance features of the image and thus improve the transferability of the protective perturbation against various unknown feature extractors, we further employ CLIP image encoder for semantic feature extraction and ReferenceNet for fine-grained feature extraction during the optimization of \u03b4."}, {"title": null, "content": "argmax ||C(x + \\delta) \u2013 C(x)||_2.\n||\\delta||_{\\infty} \\leq \\eta\n                                                                                                                                           (8)"}, {"title": null, "content": "However, CLIP image encoder is known to be less effective at capturing fine-grained details [31, 72]. There are two main factors for this limitation. First, CLIP is trained to match semantic features between text and images, which are typically sparse and high-level, resulting in a deficit of detailed features. Second, the CLIP image encoder only accepts low-resolution (224 \u00d7 224) images as inputs, leading to a loss of fine-grained information. Therefore, relying solely on the CLIP image encoder for optimizing \u03b4 is insufficient, as it mainly extracts semantic features while missing fine-grained details.\nReferenceNet. Inspired by the recent success of ReferenceNet in detailed feature extraction [15, 31, 32, 63, 67, 72, 86], we further utilize ReferenceNet R as the feature extractor to optimize \u03b4. ReferenceNet is a copy of the denoising UNet from SD used in image generation\u00b2, designed to extract appearance features particularly low-level details from the reference human image. Specifically, the feature map X1 \u2208 Rh\u00d7w\u00d7c from the ReferenceNet is repeated by f times and concatenated with the feature map X2 \u2208 Rf\u00d7h\u00d7w\u00d7c from the denoising UNet along w dimension. Then spatial attention is performed to transmit the appearance information from the reference image spatially. ReferenceNet inherits weights from the original SD and is further trained on human video frames, enabling it to provide fine-grained and detailed features essential for preserving the reference appearance in video generation.\nIn addition to using CLIP image encoder C to extract semantic features and optimize the protective perturbation \u03b4 according to Eq. (8), we also incorporate ReferenceNet R into the optimization process to enhance fine-grained feature extraction. To ensure that the protected image xp exhibits distinct detailed features from the perspective of ReferenceNet, we maximize the distance between the corresponding extracted features R(E(x+\u03b4)) and R(E(x)). Moreover, to further improve the transferability of the protective perturbation, we optimize \u03b4 using an ensemble [17, 42] of three different pre-trained ReferenceNets [15, 31, 72]. The objective Lfeature for feature misextraction using the CLIP image encoder and ReferenceNets is thus defined as follows:"}, {"title": null, "content": "argmaxL_{feature} = arg max || C(x + \\delta) \u2013 C(x) ||_2\n||\\delta||_{\\infty} \\leq \\eta\n+ \\sum_{k=1}^{K} ||R_k(E(x+\\delta)) \u2013 R_k(E(x))||_2.\n                                                                                                                                                                   (9)"}, {"title": "3.3 Frame Incoherence", "content": "While feature misextraction mainly focuses on disrupting the alignment of reference appearance, it is also crucial to consider the requirement of maintaining temporal consistency across frames in video generation when designing the optimization objective. Based on this, we further propose Lframe, which attacks the video generation process of the denoising UNet to induce incoherence among the generated video frames, thus enhancing the protective effect. This requires the defender to provide the reference image as well as the pose sequence to simulate human image animation using LDM, just as the attacker does. However, it is hard for the defender to know in advance the exact pose sequence the attacker will use to generate the video. Therefore, we directly extract the corresponding pose from the reference image using DWPose [74] and repeat it F times to serve as the pose guidance. In this case, each frame of the generated video should be identical to the others and consistent with the reference image.\nThe protective perturbation \u03b4 can be optimized from two perspectives: 1) maximizing the distance between each video frame and reference image to disrupt appearance alignment; and 2) maximizing the distance between each video frame and others to disrupt video consistency. Since pose-driven human image animation operates in the latent space, these two objectives can be formulated as increasing the distance between the latent vector of the reference image z = E(x) and that of each generated video frame zf, or the distance between the latent vectors of different frames zf and zf\u2032 . The objective Lframe for frame incoherence is defined as follows:"}, {"title": null, "content": "argmaxL_{frame}\n= arg max \\frac{1}{F} \\sum_{f=1}^{F} || E(x) - z_f ||_2\n||\\delta||_{\\infty} \\leq \\eta\n+ \\frac{2}{F(F-1)} \\sum_{f=1}^{F} \\sum_{f' \\neq f} || z_f - z_{f'} ||_2,\n                                                                                                                                                                                              (10)"}, {"title": null, "content": "We set F = 5 by default. As mentioned in Section 2.1, given the total inference steps T, we can sample zT from the Gaussian distribution and compute z0 progressively from timestep T to timestep 0 according to Eq. (3). However, this would result in substantial GPU memory usage and increased time costs. To improve efficiency, we directly estimate z0 using the reparameterization technique outlined in Eq. (2), rather than computing it step by step. Specifically, given a timestep t, z0 is estimated by z0 = (zt \u2013 \u221a1\u2212\u03b1tet)/\u221a\u03b1t, where zt is sampled from the Gaussian distribution, and \u03b5t is predicted by a pre-trained denoising UNet\u00b3. In each PGD iteration, we randomly sample one timestep t from the last ten timesteps of the total inference steps T as larger t makes better generation, and then estimate the corresponding z0."}, {"title": "3.4 DORMANT", "content": "Overall, to optimize the protective perturbation \u03b4 for preventing unauthorized image usage in pose-driven human image animation, we propose Lvae and Lfeature for feature misextraction, Lframe for frame incoherence. We also apply several techniques to further enhance the perturbation \u03b4 during optimization, including LPIPS [79] for imperceptibility, EoT [5] for robustness and Momentum [17] for transferability.\nLPIPS. In addition to constraining the protective perturbation \u03b4 using the L\u221e norm, we also incorporate LPIPS to further improve the imperceptibility of \u03b4. LPIPS utilizes deep features from a pre-trained network to measure image distance, aligning more closely with human perception. Thus we use LPIPS to ensure that the protected image xp remains visually similar to the original image x. Specifically, we include the following regularization term and minimize Llpips during optimization:"}, {"title": null, "content": "L_{lpips} = max(LPIPS(x + \\delta, x) \u2013 \\zeta, 0),\n                                                                                                                                    (11)"}, {"title": null, "content": "where \u03b6 denotes the budget of Lipips, and we set \u03b6 = 0.1 by default. During the optimization of \u03b4, if the perceptual distance between xp and x exceeds the bound \u03b6, Llpips introduces a penalty to maintain visual similarity; otherwise, Llpips = 0.\nEoT. To enhance the robustness of \u03b4 against various transformations, we adopt EoT into the PGD process. EoT introduces a distribution of transformation functions to the input and optimizes the expectation of the objective function values over these transformations. To reduce time and computational costs, during each PGD step, we randomly sample one of the following transformations and apply it to the protected image xp: Gaussian blur, JPEG compression, Gaussian noise, Random resize (resizing to a random resolution and then back to the original size), or no transformation.\nMomentum. Momentum enhances the transferability of the perturbation by accumulating a velocity vector in the gradient direction of the loss function across iterations. The memorization of previous gradients helps to stabilize updated directions and escape from poor local maxima. We incorporate momentum into the PGD process in Eq. (6) as follows:"}, {"title": null, "content": "g_i = \u03bc \\cdot g_{i-1} +\n\\frac{\\nabla_{\\delta} L_{DORMANT}}{mean(|\\nabla_{\\delta} L_{DORMANT}||)},\n                                                                                                                                                                               (12)\n        \\delta_i = \\delta_{i-1} + \\gamma \\cdot sign(g_i),\n                                                                                                                                                                                             \nwhere gi denotes the accumulated velocity vector at PGD step i, and \u03bc is the decay factor controlling the impact of previous gradients, set to 0.5 by default. In each iteration, the current gradient is normalized using the mean of its absolute values rather than the L1 distance used in the original paper [17].\nThe complete optimization objective of DORMANT is:"}, {"title": null, "content": "argmaxL_{DORMANT} = arg max  \\lambda_1 \\cdot L_{vae} + \\lambda_2  \\cdot L_{feature}\n||\\delta||_{\\infty} \\leq \\eta\n+ \\lambda_3 \\cdot L_{frame} - \\lambda_4 \\cdot L_{lpips},\n                                                                                                                                                     (13)"}, {"title": null, "content": "where \u03bbs control the weight of each loss term. By default, we set these values as \u03bb1 = 10, \u03bb2 = 1004, \u03bb3 = 1 and \u03bb4 = 10. The detailed optimization process is presented in Alg. 1."}, {"title": "4 Evaluation", "content": "4.1 Experimental Setup\nPosed-driven Human Image Animation Methods. We comprehensively evaluate the effectiveness and transferability of DORMANT on 8 cutting-edge and widely-used human image animation methods", "31": "MagicAnimate [72", "15": "MusePose [63", "86": "MuseV [71", "69": "and ControlNeXt [52", "datasets": "TikTok [34"}, {"86": "UBC Fashion [76", "62": ".", "68": "and utilize their 10 TikTok-style videos showing different people from the web for evaluation. These videos contain between 248 and 690 frames. Specifically, we use the first frame of each video as the reference image and extract pose sequences from the remaining frames using DWPose [74", "23": "to guide the video generation. Additionally, we also sample 10 videos from the Champ training set, the UBC Fashion, and TED Talks test sets for evaluation, resulting in videos with 237-848, 303-355, and 135-259 frames, respectively. All video frames are resized to 512 \u00d7 512, and we further conduct experiments on other image resolutions in Section 4.4.\nBaseline Protections. We compare the protection performance of DORMANT with 6 baseline methods defending against text/image-to-image generation (SDS [73", "61": "Mistv2 [84", "60": "and AntiDB [65", "50": "."}]}