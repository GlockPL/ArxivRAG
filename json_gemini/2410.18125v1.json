{"title": "Towards Edge General Intelligence via Large Language Models: Opportunities and Challenges", "authors": ["Handi Chen", "Weipeng Deng", "Shuo Yang", "Jinfeng Xu", "Zhihan Jiang", "Edith C.H. Ngai", "Jiangchuan Liu", "Xue Liu"], "abstract": "Edge Intelligence (EI) has been instrumental in delivering real-time, localized services by leveraging the computational capabilities of edge networks. The integration of Large Language Models (LLMs) empowers EI to evolve into the next stage: Edge General Intelligence (EGI), enabling more adaptive and versatile applications that require advanced understanding and reasoning capabilities. However, systematic exploration in this area remains insufficient. This survey delineates the distinctions between EGI and traditional EI, categorizing LLM-empowered EGI into three conceptual systems: centralized, hybrid, and decentralized. For each system, we detail the framework designs and review existing implementations. Furthermore, we evaluate the performance and throughput of various Small Language Models (SLMs) that are more suitable for development on edge devices. This survey provides researchers with a comprehensive vision of EGI, offering insights into its vast potential and establishing a foundation for future advancements in this rapidly evolving field.", "sections": [{"title": "I. INTRODUCTION", "content": "Edge computing has emerged as a crucial network paradigm, processing data closer to its source to reduce latency and resource demands compared to traditional cloud-centric systems. The integration of Artificial Intelligence (AI) into edge devices enables local data analysis, eliminating the need for continuous cloud communication and facilitating devices to make rapid, independent decisions. AI-enhanced Edge Intelligence (EI) advances the adaptability of distributed systems, allowing for faster responses in dynamic environments. This evolution unlocks the potential for latency-sensitive applications, transforming industries such as autonomous vehicles, smart homes, industrial automation, and healthcare by introducing more intelligent and responsive technologies.\nTracing the evolution of AI, which aims to emulate human cognitive abilities, we can categorize its development into three stages: narrow, broad, and general intelligence. Following this development trajectory, EI can likewise be divided into Edge Narrow Intelligence (ENI), Edge Broad Intelligence (EBI), and Edge General Intelligence (EGI). ENI, empowered\nby extensive training, excels at performing specific, well-defined tasks, such as facial recognition. EBI transcends individual tasks, enabling systems to manage multiple interconnected functions, such as optimizing traffic flows in smart cities.\nOver the past decades, traditional EI has been limited to narrow and broad intelligence within specific tasks or domains. In contrast, EGI expands these capabilities, allowing for greater autonomy and flexibility. However, achieving EGI poses significant challenges, requiring edge devices to autonomously reason, learn, and adapt across diverse knowledge, tasks, and dynamic environments. Recent advancements in Large Language Models (LLMs), such as GPT-4\u00b9 and LLaMA2, represent breakthroughs in AI, which demonstrates impressive multi-modal capabilities that bridge computer vision and Natural Language Processing (NLP). These developments lay the foundation for general intelligence [1]. Leveraging LLMs to achieve EGI opens new avenues by enabling edge devices to handle complex tasks. As shown in Fig. 1, this integration paves the way for real-time, context-aware applications in various use cases, such as personalized healthcare, smart assistants, and customer service with edge networks [2].\nTo inspire innovation and explore the potential of LLMs-empowered EGI, this paper proposes three conceptual EGI systems, offering a comprehensive framework for researchers to integrate LLMs into the edge computing ecosystem. We begin by outlining the three levels of EI, followed by a systematic review of recent advancements in LLM-enhanced EGI. Upon this basis, we introduce three system architecture designs, including centralized, hybrid, and decentralized EGI, accompanied by a review of their implementation strategies and an analysis of their respective advantages, disadvantages, and feasibility. We finally conclude by discussing future directions, highlighting the transformative opportunities this survey holds for the evolution of EGI."}, {"title": "II. BACKGROUND OF EI AND LLMS", "content": "In this section, we provide a brief overview of the background knowledge about EI and LLMs."}, {"title": "A. Edge Computing and EI", "content": "Edge computing proposes a paradigm shift by relocating computational resources from distant, centralized cloud servers\nto edge devices closer to end-users. These devices may include smartphones, industrial machines, sensors, video cameras, or any other data collection devices. Edge computing is designed to enhance the Quality of Service (QoS) for heterogeneous mobile devices and infrastructures through wireless networks by mitigating substantial transmission latency, bandwidth limitations, and connectivity issues caused by cloud computing. It facilitates faster and more efficient data processing while enhancing privacy and security measures. For example, smart traffic lights based on edge computing process sensor data in real-time to manage traffic flow more efficiently. Instead of sending data to a central server for processing, the computation is done locally at the traffic intersection, reducing latency and improving the responsiveness of the traffic management system. With advances in AI efficiency, the increasing number of IoT devices, and the ascendancy of edge computing, the potential of EI has now been actualized. The evolution of AI is shaping the future of edge computing towards EI."}, {"title": "B. Large Language Models", "content": "Recent advancements in LLMs have showcased a spectrum of impressive emergent abilities, leading to significant paradigm shifts in AI. These models have displayed an exceptional capacity for understanding human instructions and demonstrating cognitive capabilities that closely mirror human thinking, paving the way for numerous opportunities across various fields. LLMs have made significant strides in complex task planning and reasoning, skills that are indispensable for problem-solving and decision-making. For instance, ChatGPT, developed by OpenAI, excels in general problem-solving, assisting users with a variety of tasks, including solving mathematical problems, devising travel plans, and analyzing investment data to guide decisions. As LLMs continue advancing towards general AI, they demonstrate an increasing ability to generate complex, contextually relevant responses across diverse domains, aligning more closely with human-like reasoning.\nThere has been a surge in developing domain-specific LLMs based on generic LLMs to integrate expert knowledge across\nvarious fields. Building on the capabilities of LLMs, Retrieval-Augmented Generation (RAG) has emerged as a powerful method, enabling LLMs to dynamically access external knowledge bases during response generation. By integrating relevant information from diverse sources, RAG significantly enhances the accuracy and contextual relevance of outputs, especially for specialized tasks. Furthermore, to adapt LLMs to specific domains, fine-tuning techniques are employed to transform generic models into domain-specific experts. This strategy has proven effective in models such as MedicalGPT3 and FinGPT4, which serve as valuable tools in the medical and financial sectors."}, {"title": "III. LLMS: EVALUATING THE NEXT LEVEL OF EI", "content": "In this section, we first provide a concise definition of general intelligence to delineate it from traditional EI. Following this, we discuss the motivations driving the integration of general intelligence into edge computing, highlighting the potential benefits and transformative impacts on these systems. Finally, we present an overview of the three system architectures proposed in this paper."}, {"title": "A. Hierarchical Cognitive Abilities of EI", "content": "Human intelligence is the cognitive ability to learn, reason, solve problems, adapt to new situations, and understand complex ideas. With this as the ultimate goal, AI systems can be constructed in a hierarchical structure, including three levels of cognitive ability: narrow, broad, and general intelligence, as shown in Fig. 1. The explanations of these three-level AI are outlined below:\n\u2022 Narrow Intelligence: AI systems designed to perform specific tasks or solve particular problems.\n\u2022 Broad Intelligence: AI systems that can perform a wider range of tasks across different domains but still lack the full versatility of human-like understanding.\n\u2022 General Intelligence: AI systems that possess the ability to understand, learn, and apply knowledge across a wide range of tasks, similar to human intelligence.\nThe development of EI is closely linked to advancements in AI, as it relies on AI algorithms for real-time data processing, efficient decision-making, and self-optimization through machine learning, and supports diverse applications in areas like smart transportation and industrial automation. To be specific, the initial level of EI implements narrow intelligence, focusing on specific tasks. For example, an edge device equipped with narrow intelligence might analyze network traffic to detect unusual patterns or potential security threats. These models excel in their specialized tasks due to extensive, task-specific training.\nExpanding from narrow capabilities, edge devices with broad intelligence are able to handle a range of interconnected tasks within a domain, constructing a more versatile and intelligent system. An intelligent transportation system serves as a prime example, where edge devices integrate various functions such as monitoring traffic flow, controlling traffic lights, and detecting congestion to optimize traffic patterns and enhance efficiency.\nHowever, traditional EI, whether narrow or broad, remains limited to specific tasks or domains. In contrast, EGI leverages high-level cognitive abilities to tackle a wide array of tasks across various domains. EGI is characterized by its capacity for abstract thinking, complex problem-solving, and learning from accumulated knowledge while adapting to diverse scenarios. The goal of EGI is to enhance edge computing by integrating advanced problem-solving skills, enabling systems to understand and respond to complex instructions, manage dynamic situations, and ultimately improve overall flexibility and intelligence. In this way, EGI aspires to align more closely with human intelligence, bridging the gap between specialized task execution and comprehensive cognitive abilities."}, {"title": "B. LLM-enhanced EGI", "content": "LLMs have revolutionized various fields by performing tasks that previously required human intelligence, such as coding assistance (e.g., Codex and Copilot), math problem-solving, travel planning, robot operations, and complex reasoning. Their ability to learn from diverse data sources grants them a level of general intelligence, making them valuable in dynamic real-world settings. For instance, LLMs can recognize that both a factory worker and a motorcyclist without helmets violate safety regulations, thus eliminating the need for additional data collection. In summary, Fig. 1 illustrates the six primary capabilities of LLMs essential for constructing EGI-enhanced edge computing.\nMoreover, LLMs' adaptability and compatibility with various edge sensors and hardware help address deployment challenges, enabling effective coordination of edge devices in complex environments. Enhanced by LLMs, EGI offers more flexible user interactions to manage complex tasks, leveraging strengths in NLP, task generalization, and adaptability. Fig. 1 presents a vision of several promising real-world use cases for LLM-enhanced EGI. The development of LLMs provides a viable paradigm for general intelligence, enabling EI to evolve into EGI. Furthermore, LLMs mitigate the limitations of traditional EI, which often requires specific data for distinct tasks and scenarios, thereby reducing network communication costs and enhancing data privacy and security."}, {"title": "IV. EDGE GENERAL INTELLIGENCE SYSTEMS", "content": "In this section, we elaborate on the integration of LLMS into edge computing systems to achieve EGI through three conceptual architecture designs: centralized, hybrid, and decentralized systems. We provide a comprehensive analysis of these architectures, detailing their system designs, implementation strategies, and key discussions. Table I lists several representative EGI systems."}, {"title": "A. Centralized EGI System", "content": "1) System Framework: Due to the extensive training requirements and large parameter scales of LLMs, significant computational resources are necessary for both training and inference tasks. To effectively implement an EGI system, a straightforward approach is to deploy LLMs in a centralized cloud server, illustrated in Fig. 1(a). This configuration allows the system to fully leverage the powerful computational resources available in the central server.\nIn the centralized framework, all intelligence-related capabilities reside on the central server, while edge devices serve as tools accessible by the centralized LLM. By analyzing data collected from each edge device, the central server gains insights into various scenarios and tasks. Each edge device provides an API, enabling the LLM to generate code that integrates and utilizes these devices collectively to complete tasks. This approach fosters a scalable and adaptable system, where adding more edge devices enhances the LLM's toolkit, thereby improving its efficiency in managing and executing tasks.\n2) Implementation: To implement the centralized EGI system within edge networks, LLMs can be deployed on a central cloud or powerful edge servers, such as base stations. For effective collaboration, the APIs of edge devices must be readily accessible to the LLMs. Several studies demonstrate practical implementations of this framework. Yang et al. [3] propose an LLM-powered, autonomous agent-based digital twin that employs GPT to simulate customer behaviors and preferences in a mall with multiple stores. In this system, GPT acts as a simulator, queried iteratively to evaluate the arrival patterns and distribution of groups of people, facilitating reinforcement learning-based optimization for adjusting environmental conditions like temperature. Kannan et al. [4] introduce the centralized SMART-LLM, which enhances the efficiency of multi-robot task planning through a structured four-stage process: task decomposition, coalition formation, task allocation, and task execution. This approach ensures comprehensive consideration of the environmental context and the capabilities of each robot involved. In the context of smart homes, the Sasha framework [13] is designed to respond to loosely defined commands, such as \u201cmake it cozy.\u201d It generates an action plan in JSON format to achieve specified goals. Similar to SMART-LLM, Sasha also divides the task planning process into four components: clarifying the goal, filtering devices, planning actions, and iterative refinement. Shen et al. [1] propose a framework that uses GPT's capabilities in language understanding, planning, and code generation, combined with task-oriented communication and federated learning to coordinate edge AI models and meet diverse user requirements. Furthermore, LLMind [2] utilizes LLMS as a central orchestrator to coordinate domain-specific AI modules and IoT devices, thereby executing complex tasks with improved capabilities, accuracy, and performance that extend beyond the general knowledge of LLMs.\n3) Discussion:\na) Advantages: Among the three proposed architectures, the centralized EGI system is the most cost-effective to construct and scale. Leveraging the high processing power of a central server allows edge devices to function as tools without incurring significant additional power consumption."}, {"title": "B. Hybrid EGI System", "content": "1) System Framework: As shown in Fig. 1(b), both the central server and edge devices are equipped with language models to enhance intelligence. In a hybrid EGI system, the central server hosts a more advanced and powerful LLM, while the edge devices run Small Language Models (SLMs) due to their limited computational capacity. Recent advancements in SLMs have made such deployments possible. While these device-side SLMs may not rival the capabilities of cloud-based powerhouses like GPT-4 as demonstrated in the results shown in Table III. Integrating SLMs capable of basic GI tasks, such as understanding natural language instructions and answering common-sense questions, into a Mixture of Experts (MoE) framework significantly enhances their functionality. Within this framework, each SLM is fine-tuned as an expert for specific tasks and dynamically selected by a gating mechanism, enabling the efficient execution of more complex environments requiring direct human interaction, such as smart homes, mobile phone assistants, and vehicular systems.\n2) Implementation: Hybrid EGI systems provide a more flexible implementation, although they complicate the deployment. Ding et al. [6] propose a \u201chybrid LLM\u201d architecture that combines different models. Their approach relies on a quality-aware router, which directs queries to the most cost-effective LLM (large for complex tasks, small for simpler ones) without sacrificing response quality. This work achieves significant cost savings by leveraging the diverse strengths of different-sized LLMs. To enhance edge-cloud cooperation, Yang et al. [14] propose EdgeFM, an edge-cloud cooperative system that leverages foundation models (FMs) to enhance the generalization capabilities of on-device deep learning models on resource-limited IoT devices. By selectively uploading data to the cloud for FM querying and dynamically switching models based on data uncertainty and network conditions, EdgeFM significantly improves accuracy and reduces end-to-end latency. Zhang et al. [7] propose a collaborative edge computing framework that enables efficient LLM inference by dynamically partitioning the LLM model and distributing it across edge devices and cloud servers. This framework incorporates an adaptive device selection and model partitioning strategy optimized through a dynamic programming algorithm to minimize inference latency and maximize throughput. Hao et al. [8] present a dynamic token-level Edge-Cloud collaboration framework for LLMs, utilizing an SLM like TinyLlama on edge devices that interacts with cloud-based LLMs to achieve high-quality performance at reduced cost. Together, these advancements illustrate the potential of hybrid EGI systems to balance computational demands while enhancing performance and efficiency across various applications.\n3) Discussion:\na) Advantages: In the hybrid EGI system, deploying edge devices with SLMs enables low-latency decision-making, allowing quick responses to local events without relying on central server processing. This immediacy is crucial for real-time services, such as smart homes and autonomous vehicles. Additionally, the system enhances load balancing by distributing tasks between edge devices and the central server, preventing any single device or server from becoming overburdened, and enhancing overall system efficiency and reliability. The architecture's flexibility is evident in its ability to handle diverse tasks with varying constraints. Edge devices can preprocess and determine which information is essential for transmission. By using SLMs for semantic communication rather than transmitting raw data, the system conserves network resources and reduces communication overhead, further optimizing performance.\nb) Disadvantages: However, the hybrid EGI system also comes with notable disadvantages. Deployment costs are higher due to the requirement to set up both a central LLM and edge SLMs. Maintenance costs can be significant as well, driven by the complexity involved in updating, securing, and managing both the edge devices and the central server. Furthermore, since SLMs cannot match the processing and decision-making capabilities of LLMs, efficient task scheduling and resource allocation become essential to balance QoE with efficiency.\nc) Feasibility Discussion: Recent advancements in SLMs have paved the way for deploying these sophisticated models on edge devices. Research has demonstrated that models with a relatively small number of parameters, such as 1.1 billion, can still possess a substantial level of GI. These models can understand human language, engage in fluent communication, and answer questions that do not necessitate complex reasoning or in-depth world knowledge. In addition to the remarkable progress in SLM development, advancements in model optimization techniques have facilitated their deployment on edge devices. Techniques such as quantization and efficient inference frameworks, which incorporate optimizations like kernel fusion, have been crucial. These technological innovations make deploying SLMs on consumer-grade hardware feasible, as exemplified by smartphones equipped with a Snapdragon 888 processor and 8GB of RAM, achieving impressive performance levels."}, {"title": "C. Decentralized EGI System", "content": "1) System Framework: In the decentralized EGI framework, each edge device has its own SLM for autonomous decision-making, enabling local data processing and task execution. As depicted in Fig. 1(c), this decentralized architecture significantly reduces the need for human intervention in managing device interactions, thereby increasing the efficiency of decision-making processes. Furthermore, while these devices operate independently, their ability to collaborate with others is essential in a decentralized EGI architecture. This collaborative capability allows them to share insights, improve overall system performance, and address complex tasks that require collective intelligence.\n2) Implementation: Although research on decentralized EGI systems is still in its early stages, several studies have demonstrated that by combining the knowledge and processing capabilities of multiple intelligent agents, decentralized EGI systems can achieve better collective intelligence performance than single agents. To coordinate cooperation among multiple agents, Zhang et al. [12] introduce the Cooperative Embodied Language Agent (CoELA), a cognitive-inspired modular framework that leverages the reasoning, language comprehension, and text generation abilities of LLMs in decentralized control scenarios with costly communication. Experiments demonstrate that CoELA, driven by GPT-4, outperforms traditional planning-based methods and can be fine-tuned for improved performance using collected data, thereby enhancing human-agent interaction through natural language communication. Chan et al. [15] introduce ChatEval, a multi-agent framework that utilizes diverse communication strategies and unique agent personas to collaboratively evaluate text, aligning more closely with human preferences than single-agent approaches. Drawing inspiration from collective intelligence and cognitive synergy, ChatEval enhances evaluation accuracy by incorporating multiple perspectives and exhibiting human-like behavior in interactive natural language dialogue. Jiang et al. [10] propose a multi-agent system enhanced with LLMs that comprises three components: Multi-agent Data Retrieval (MDR), Multi-agent Collaborative Planning (MCP), and Multi-agent Evaluation and Reflection (MER). These components work together to refine communication knowledge, generate feasible solutions, and evaluate and improve current solutions for communication-related tasks using natural language. The authors also address concerns regarding resource constraints at the edge and the interaction delay associated with LLMs. Yu et al. [11] develop a framework to facilitate the efficient adaptation of LLMs on edge devices, addressing the challenges of high computation and memory demands. The framework incorporates a layer-wise unified compression technique to optimize computation through adaptive pruning and quantization, coupled with an adaptive layer tuning and voting mechanism that reduces memory usage by curtailing the depth of backpropagation. Additionally, a dedicated hardware scheduling strategy efficiently manages the irregular computation patterns that arise from these optimizations.\n3) Discussion:\na) Advantages: In the decentralized intelligence system, intelligent edge devices can communicate directly with each other, allowing for collaborative task execution that reduces manual costs. In environments with extremely poor communication conditions, such as oceans and disaster areas, distributed collaborative intelligence can adapt to various scenarios and complete tasks effectively. This system is robust avoiding a single point of failure compared to the centralized and hybrid intelligence systems. Processing data locally minimizes the risk of privacy breaches associated with data transit or central storage. Additionally, the decentralized architecture enhances scalability and improves the collective intelligence of the network. Real-time responses are facilitated without the need to communicate with a central server, optimizing network efficiency through direct device-to-device communication, conserving bandwidth, and reducing the overall load. This structure is particularly suitable for applications requiring speed, scalability, robustness, and data privacy. Moreover, unlike traditional decentralized edge systems that rely on minimal collaboration or predefined rules, EGI systems equipped with SLMs possess individual decision-making capabilities, enabling them to actively collaborate and adapt to the dynamic real-world environment, an essential feature for edge computing.\nb) Disadvantages: Despite its advantages, the decentralized EGI system also presents several challenges. The frequent interactions between individual edge devices increase the complexity of the trust mechanism, making it difficult to enforce consistent policies across the network due to its decentralized nature. Additionally, the computational capacities of each device's SLM may be limited, hindering their ability to handle complex tasks compared to a centralized LLM. There is also significant coordination overhead, requiring advanced and complex protocols to manage peer-to-peer interactions and ensure efficient network performance. Given the computational power needed to run even an SLM, upgrading the hardware of edge devices becomes necessary, potentially leading to higher costs.\nc) Feasibility Discussion: The NLP community has begun to explore the use of multiple LLMs to simulate group intelligence behaviors. This would enable collaboration among multiple LLM-based agents that can work together or engage in discussions. However, this area of research is still in its infancy, presenting a novel topic for further investigation and development."}, {"title": "V. FUTURE DIRECTIONS", "content": "In this section, we outline several key future directions for implementing LLM-empowered EGI to fully unlock the potential of LLMs in resource-constrained edge environments."}, {"title": "A. Efficient SLM Deployment on Resource-limited Edge Devices", "content": "Deploying efficient general intelligence on resource-limited edge devices for EGI is challenging due to the substantial RAM and computational resources required by LLMs and SLMs. Solutions can be explored from two perspectives: algorithms and hardware. Algorithmically, reducing memory consumption and computational costs through quantization\u2014lowering the precision of model data\u2014shows promise, but even optimized models still require at least 500MB of RAM and high-end CPUs, like the Snapdragon 888, for effective inference. While low-bit-rate inference is feasible, training models similarly remain difficult, limiting on-device refinement. Split learning/inference, which distributes tasks between edge devices and central servers, offers another approach, though it depends on optimal model partitioning and secure data transmission. On the hardware side, boosting processing power and memory within the constraints of edge devices is critical. AI-specific processors, such as GPUs, FPGAs, and ASICs (e.g., Google's TPU), alongside system-level optimizations like distributed inference across device clusters, offer potential solutions. Addressing these challenges is key to enabling the broader adoption and efficient use of LLM-empowered EGI systems in edge computing."}, {"title": "B. Optimizing Latency of Providing LLM-enhanced EGI Services", "content": "Response latency, encompassing both inference and communication delays, serves as a pivotal determinant of Quality of Service (QoS) in LLM-augmented EGI systems. Inference latency is driven by the hardware's processing capabilities and the efficiency of inference algorithms. Improvements require advancements in computational methods like model optimization and quantization, alongside hardware innovations such as AI accelerators and optimized processors to reduce delays and enhance throughput. Communication latency stems from data transmission delays over the internet. While text-based LLMs handle small data packages, multimodal LLMs, which process larger inputs like images and videos, face greater challenges. For instance, a one-hour 4K video can range from 9GB to 27GB, straining 5G networks with uplink speeds of around 50Mbps. Addressing this requires efficient data compression and transmission techniques, with future 6G advancements potentially easing these limitations and enabling more practical multimodal LLM-based EGI services."}, {"title": "C. Adapting LLMs for Domain-Specific Applications in Edge Networks", "content": "Current LLMs are trained on broad, general-domain data, while edge systems often require domain-specific knowledge. To meet this need, LLMs must be adapted through continued pre-training or fine-tuning on specialized datasets, enhancing their ability to address niche queries. This adaptation is essential for edge computing, where rapid and accurate responses are critical. Additionally, managing time-varying network conditions to avoid negative impacts caused by network fluctuations. Solutions like dynamically adjusting model complexity and offloading heavier tasks to the cloud help maintain low-latency responses while ensuring essential functions remain on edge devices."}, {"title": "D. Security for LLM-enhanced EGI Services", "content": "Deploying LLMs on edge networks poses significant security challenges that must be addressed to protect user privacy and ensure data integrity. Transmitting sensitive data to external servers in cloud-based models risks data exposure, making advanced encryption methods like differential privacy, homomorphic encryption, and zero-knowledge proofs essential for safeguarding information. The decentralized EGI architecture eliminates the dependence on a central server, thereby reducing the risk of data breaches. However, securing data transmission channels remains a critical challenge. Furthermore, maintaining LLM integrity poses significant challenges due to threats such as model tampering, poisoning, and adversarial attacks, where adversaries manipulate the model or its training data to produce erroneous outputs. Additionally, updating models across distributed networks introduces further complexity, as updates may be intercepted or altered by malicious actors. Addressing these security concerns is essential for the reliable construction of EGI systems."}, {"title": "E. Collaborative LLM-enhanced Edge Systems", "content": "LLM-empowered edge devices bring a remarkable ability to understand and adapt to dynamic environments, making them valuable in various applications. However, leveraging multiple LLM-based agents to collaborate and complete tasks [11] introduces unique challenges, especially in resource-constrained extreme environments such as mountainous regions, earthquake zones, and oceans. These scenarios face issues like limited connectivity, low bandwidth, and unreliable power supplies, which complicate the seamless interaction among agents. Moreover, the unpredictable behaviors of LLM-based agents add another layer of complexity to cooperate. Coordinating tasks efficiently requires not only robust communication protocols and resource management but also a system capable of handling these unpredictable responses. Overcoming these challenges is crucial for deploying EGI systems to effectively support remote tasks like disaster recovery and environmental exploration."}, {"title": "VI. CONCLUSION", "content": "In this survey, we explored the evolution of LLM-enhanced EGI and differentiated it from traditional EI. The LLM-enhanced EGI systems are categorized into three conceptual designs, namely centralized, hybrid, and decentralized, each reflecting distinct architectures and operational strategies. For each system, this survey summarized the framework designs, reviewed representative research, and discussed the advantages, disadvantages, and feasibility. Moreover, we compared various SLMs based on their characteristics and throughput, offering valuable insights for orchestrating EGI systems. Looking ahead, we summarized promising future directions for EGI in terms of resource efficiency, service optimization, domain adaptation, interaction security, and multi-agent collaboration. This survey presents a comprehensive vision of EGI and outlines key future directions that will contribute to its ongoing advancement."}]}