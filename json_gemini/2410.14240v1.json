{"title": "Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction", "authors": ["Manuel Brenner", "Christoph J\u00fcrgen Hemmer", "Zahra Monfared", "Daniel Durstewitz"], "abstract": "Dynamical systems (DS) theory is fundamental for many areas of science and engineering. It can provide deep insights into the behavior of systems evolving in time, as typically described by differential or recursive equations. A common approach to facilitate mathematical tractability and interpretability of DS models involves decomposing nonlinear DS into multiple linear DS separated by switching manifolds, i.e. piecewise linear (PWL) systems. PWL models are popular in engineering and a frequent choice in mathematics for analyzing the topological properties of DS. However, hand-crafting such models is tedious and only possible for very low-dimensional scenarios, while inferring them from data usually gives rise to unnecessarily complex representations with very many linear subregions. Here we introduce Almost-Linear Recurrent Neural Networks (AL-RNNs) which automatically and robustly produce most parsimonious PWL representations of DS from time series data, using as few PWL nonlinearities as possible. AL-RNNs can be efficiently trained with any SOTA algorithm for dynamical systems reconstruc-tion (DSR), and naturally give rise to a symbolic encoding of the underlying DS that provably preserves important topological properties. We show that for the Lorenz and R\u00f6ssler systems, AL-RNNs discover, in a purely data-driven way, the known topologically minimal PWL representations of the corresponding chaotic attractors. We further illustrate on two challenging empirical datasets that interpretable symbolic encodings of the dynamics can be achieved, tremendously facilitating mathematical and computational analysis of the underlying systems.", "sections": [{"title": "1 Introduction", "content": "Dynamical systems (DS) underlie many real-world phenomena of scientific and practical relevance. Complex chaotic DS are believed to govern market dynamics [65], the rhythms of the brain [16], climate systems [100], or ecosystems [71]. A by now rapidly growing field in scientific ML is dynamical systems reconstruction (DSR), where the goal is to learn a DS model directly from data that constitutes a generative surrogate model of the data-generating DS. DSR increasingly relies on deep learning, especially in contexts where dynamics are too complex to be captured by simple equations or where the underlying processes are not fully understood.\nOne way of making DSR models mathematically accessible is piecewise linear (PWL) designs, popu-lar among engineers for decades [8, 17, 43, 81, 91]. In the mathematical theory of DS, PWL models also play a special role and simplify many types of analysis [3, 54], such as the characterization of"}, {"title": "2 Related Work", "content": "Dynamical systems reconstruction The field of data-driven DSR has been rapidly expanding in recent years. On the one hand there are approaches based on function libraries for approximating unknown vector fields, which have become particularly popular in some areas like physics [53, 64]. Among these, Sparse Identification of Nonlinear Dynamics (SINDy) and its variants [14, 60, 45, 21, 66, 44] is probably the most popular. Since in these models sets of differential equations are directly formulated in terms of known, predefined function libraries, instead of using NN black-box approximators, they have some level of interpretability in the sense that they are human-readable and can easily be related to established mathematical building blocks in physical or biological theories [37, 83]. This does not necessarily make them mathematically tractable, however, since systems of nonlinear differential equations are in themselves usually hard to analyze (in fact, their behavior is much of the core topic of DS theory [78]). They also have other limitations, including a difficulty in capturing complex and noisy empirical data [11, 39], as they usually require considerable prior knowledge about the system's underlying structure (i.e., which terms to include in the function library). This somewhat limits their applicability for discovering novel phenomena. On the other hand, many recent powerful DSR methods rely on universal approximators, in particular the fact that sufficiently large RNNs can approximate any underlying DS [29, 47, 34]. Such methods may be grouped into several broad classes, including reservoir computers [76, 79, 80], neural ODEs/PDEs [20, 46, 4, 48], neural/ Koopman operators [15, 63, 75, 6, 73, 30, 104], and RNNs [99, 25, 101, 19, 11, 84, 39]. The latter are commonly trained by variants of backpropagation through time (BPTT, [101, 102, 11]), combined with specific control techniques [67] to remedy the exploding/ vanishing gradients problem [9, 67, 11, 39]. While DSR algorithms based on universal approximators achieve SOTA performance on DSR tasks, and often work particularly well on empirical time series [11, 39], they commonly deliver a complex model structure that is difficult to interpret and parse mathematically.\nNonlinear dynamics via linear DS The idea of approaching nonlinear DS through our good grasp of linear DS has been around for quite a long time, reflected in important theoretical results like the Hartman-Grobman theorem [36] or Koopman operator theory [49, 15]. While linear DS are easy to analyze and well understood [78, 94], however, they cannot properly capture most real-world systems, as they cannot produce many important DS phenomena such as limit cycles, chaos, or multistability. This has motivated the modeling of complex dynamics in terms of compositions"}, {"title": "3 Methodological and Theoretical Prerequisites", "content": "3.1 AL-RNN Model\nConsider a piecewise linear recurrent neural network (PLRNN, [25]):\n$z_t = F_\\theta(z_{t-1}) = Az_{t-1} + W\\phi(z_{t-1}) + h,$\nwhere diagonal $A \\in \\mathbb{R}^{M \\times M}$ contains linear self-connections, $W \\in \\mathbb{R}^{M \\times M}$ are nonlinear con-nections between units, $h \\in \\mathbb{R}^{M}$ is a bias term, and $\\phi(z) = \\max[0, z]$ is an element-wise ReLU nonlinearity. To expose the piecewise linear structure of this model more clearly, by noting that the slope of the ReLU is either 0 or 1 depending on the sign of $z_{m,t}$, one can reformulate this as\n$z_t = (A+WD_{\\phi}(t-1))z_{t-1} + h := W_{\\phi(t-1)} z_{t-1} + h,$\nwhere $D_{\\phi(t)} := diag(d_{\\phi(t)})$ is a diagonal matrix and $d_{\\phi(t)} = (d_1,d_2,\\dots, d_M)$ an indicator vector with $d_m (z_{m,t}) = 1$ whenever $z_{m,t} > 0$ and zero otherwise [26]. For the $2^M$ different configurations of $D_{\\phi(t)}$, $D_{\\phi_k}, k \\in \\{1,2,\\dots,\\dots,2^M \\}$, the phase space of system eq. 2 is divided into $2^M$ subregions with linear dynamics\n$z_{t+1} = W_{\\phi_k}z_t + h,\\ \\ \\ \\ W_{\\phi_k} := A + WD_k.$\nEmpirically, M often needs to be quite large (at least on the order of the number of observations) for achieving good reconstructions of observed DS. Since the number of subregions grows as $2^M$, analyzing inferred models in terms of the subregions can thus become very challenging. We therefore introduce a novel variant of the PLRNN in which only a subset of $P << M$ units are equipped with a ReLU nonlinearity, yielding\n$z_t = Az_{t-1} + W\\Phi^*(z_{t-1}) + h$"}, {"title": "where", "content": "$\\Phi^*(z_t) = [z_{1,t},..., z_{M-P,t}, \\max(0, z_{M-P+1,t}), \\dots, \\max(0, z_{M,t})]^T $.\nIn this formulation, we thus only have $2^P$ different linear subregions, while still accommodating a sufficiently large number of latent states for capturing unobserved dimensions in the data and disentangling trajectories sufficiently [96, 86].\nThe model is trained on the N-dimensional observations $\\{x_t\\}, t = 1 ... T, x_t \\in \\mathbb{R}^N$, by a variant of sparse teacher forcing called identity teacher forcing [67, 11]. In identity teacher forcing, the first N latent states ('readout neurons') are replaced by the N-dimensional observations every $\\tau$ time steps, where $\\tau$ is chosen such as to optimally control trajectory and gradient flows, avoiding exploding gradients while providing the model sufficient opportunity to unroll into the future to capture the underlying DS' long-term behavior (see [67, 39] for details); see Appx. A.2 for details on training. We emphasize that sparse teacher forcing is only used for training the model, and is turned off at test time where the model generates new trajectories completely independent from the data."}, {"title": "3.2 Theoretical Background: Symbolic Dynamics and Symbolic Coding of AL-RNN", "content": "The mathematical field of symbolic dynamics formulates conditions under which a DS has a unique symbolic representation and discusses how to harvest this symbolic representation to prove certain properties of the underlying system, which otherwise may be more difficult to address [74, 55]. In fact, symbolic dynamics has led to many powerful insights and formal results in DS theory, e.g. about the properties of chaos or type and number of periodic orbits [32, 106]. An appealing feature of symbolic dynamics for the field of ML/AI is that it links concepts in DS theory to computational concepts like finite state automata or formal languages, as well as graph theory [55, 35]. It can thus facilitate the computational interpretation of natural or trained dynamical systems, like RNNs.\nAssume we have an alphabet of n symbols $A = \\{0,...,n \\text{\u2013} 1\\}$, from which we form infinite sequences (bidirectionally or only in forward-time) $a = ...a_{-2}a_{-1}.a_0a_1a_2...$ with $a_k \\in A$, and the dot separating past from future (i.e., indices $k < 0$ indicate backward time, and $k \\geq 0$ present and forward time). Then the space of all possible sequences, together with the so-called (left) shift operator given by\n$\\sigma(a) = \\sigma(... a_{-2}a_{-1}.a_0a_1a_2...) = ...a_{-1}a_0.a_1a_2a_3...$\ndefines the full shift space $A^\\mathbb{Z}$. We denote by $\\sigma^k = \\sigma o \\sigma o \\dots o \\sigma$ the k-times iteration of the shift. Now consider a DS $(S, \\phi)$ consisting of a metric (state) space $S$ and a recursive (flow) map $\\phi$. The flow map $\\Delta t(x)$ advances the system's current state x by $\\Delta t$ and may be thought of as the solution operator of the underlying DS $\\dot{x} = f(x)$ [78]. When training RNNs $z_t = F_\\theta(z_{t-1})$ on time series of observations $\\{g(x_{k\\Delta t})\\}, k = 1 ... T$, from the underlying DS, where g is the observation function, we are trying to approximate this flow map. Assume the whole state space S can be partitioned into a finite set $U = \\{U_0 ... U_{n-1}\\}$ of disjoint open sets $U_i$, such that $S = \\bigcup_i\\overline{U_i}$, i.e. S is covered by the union of the closures of these sets. We call this a topological partition of S [55].\nThe central idea now is to assign a unique symbol $a_i \\in A$ to each set $U_i \\in U$, with $n = |U| = |A|$. As a trajectory x(t) of the underlying DS travels through the system's state space S, observed at times"}, {"title": "4 Theoretical Results", "content": "Recall that within each subregion $U_i$ the map $F_\\theta$ is monotonic and the dynamics are linear (ruling out certain possibilities, like chaos or isolated cycles occurring within just one subregion). We furthermore assume that the dynamics are globally non-diverging (this could be strictly enforced through 'state-clipping' and constraints on matrix A in eq. 4, see Hess et al. [39], but will also be the case for a well-trained AL-RNN). Here we claim that for hyperbolic AL-RNNs $F_\\theta$, we have 1:1 relations between important topological objects in the AL-RNN's state space and those of the symbolic coding formed from the linear subregions $U_i$ of the AL-RNN, as expressed in the following theoretical results.\nConsider a hyperbolic, non-globally-diverging AL-RNN $F_\\theta$, eq. 4, and a topological partition U of the state space into its linear subregions $U_i \\subseteq S, i = 0... 2^P-1$. Denote by $(A_U, F_\\theta, \\sigma)$ the finite shift induced by $(S, F_\\theta)$, with each $a_i \\in A$ of its alphabet A associated with exactly one linear subregion $U_i \\in U$, and let us consider the system's evolution only in forward time. Then the following holds:\nTheorem 1. An orbit $\\Omega_S = \\{z_1,..., z_n, ...\\}$ of the AL-RNN $F_\\theta$ is asymptotically fixed (i.e., converges to a fixed point) if and only if the corresponding symbolic sequence $a ="}, {"title": "5 Experimental Results", "content": "To assess the quality of DSR, we employed established performance criteria based on long-term, invariant topological, geometrical, and temporal features of DS [51, 11, 39]. Due to exponential trajectory divergence in chaotic systems, mean-squared prediction errors rapidly grow even for well-trained systems, and hence are only of limited suitability for evaluating DSR quality [108, 67]. Thus, we prioritize the geometric agreement between true and reconstructed attractors, quantified by a Kullback-Leibler divergence (Dstsp, Appx. A.2) [51]. Additionally, we examine the long-term temporal agreement between true and reconstructed time series by evaluating the average dimension-wise Hellinger distance (DH) between their power spectra (Appx. A.2). We first confirmed that the AL-RNN is at least on par with other SOTA methods for DSR. We then tested AL-RNNs on two commonly employed benchmark DS for which minimal PWL representations are known, the famous Lorenz-63 model of atmospheric convection [61] and the chaotic R\u00f6ssler system [85]. We finally explored the suitability of our approach on two real-world examples, human electrocardiogram (ECG) and human functional magnetic resonance imaging (fMRI) data.\n5.1 SOTA performance\nWhile our goal here is a technique that constructs topologically minimal, interpretable DS representa-tions, at the same time we do not want to compromise on DSR performance which should still be within the same ballpark as existing SOTA methods. We checked this on the Lorenz-63, R\u00f6ssler and ECG data noted above, and in addition on the higher-dimensional chaotic Lorenz-96 system [62] and on human electroencephalogram (EEG) data. confirms that the AL-RNN is not only on par with, but indeed outperforms most other techniques when trained with sparse teacher forcing (which may be rooted in its simple and parsimonious design).\n5.2 Reconstructed Systems Occupy a Small Number of Subregions\nFig. 3 illustrates reconstruction performance for varying numbers of ReLU nonlinearities at constant network size M. We found that a small number of PWL units already significantly improves perfor-mance, especially for the Lorenz-63 and R\u00f6ssler systems, and that beyond that number performance starts to plateau (or even briefly decrease again). Additionally, some linear units are necessary to sufficiently expand the space, but they cannot compensate for an insufficient number of PWL unitsMoreover, as shown in Fig. 4 (left), the number of linear subregions explored by the"}, {"title": "5.3 Minimal PWL Reconstructions of Chaotic attractors", "content": "Topologically minimal reconstructions Investigating reconstructions with the minimal number of PWL units needed for close-to-optimal performance (Fig. 3), we found that the AL-RNN would deliver reconstructions capturing the overall structure of the attractor using only three (Lorenz-63 system) or two (R\u00f6ssler system) linear subregions (Fig. 5a), explaining the strong performance gains in Fig. 3 for 2 and 1 PWL units, respectively. These representations, and their symbolic coding (Fig. 5c), expose the mechanisms of chaotic dynamics (Fig. 5b). Notably, these closely agree with the minimal topologically equivalent PWL representations of the two chaotic DS as described in Amaral et al. [5]: The Lorenz-63 system has at its core two unstable spiral points in the two lobes, separated by the saddle node in the center (Fig. 5b). For the R\u00f6ssler system, the topologically minimal PWL representation indeed consists of just two subregions [5], one containing an unstable spiral in the x-y plane and the other a 'half-spiral' almost orthogonal to that plane (Fig. 5b). The AL-RNN automatically and robustly discovers these representations from data: across multiple training runs, performance values are very similar (Figs. 23, 25), the assignment of subregions to different parts of the attractor remains almost the same (Figs. 24 & 25), and the regions with linear dynamics"}, {"title": "5.4 PWL Reconstructions of Real-World Systems", "content": "Topologically minimal reconstructions We next considered two experimental datasets, human ECG data (with 1d membrane potential recordings delay-embedded into N = 5, see Appx. \u0410.3) and fMRI recordings (with N = 20 time series extracted, cf. Appx. A.3) from human subjects performing three different types of cognitive task [50, 52]."}, {"title": "6 Conclusion", "content": "Here we introduced a novel variant of a PLRNN, the AL-RNN, which learns to represent nonlinear DS with as few PWL nonlinearities as possible. Despite its simple design and the minimal hyperparameter tuning required, the AL-RNN robustly and automatically identifies highly interpretable, topologically minimal representations of complex nonlinear DS, reproducing known minimal PWL forms of chaotic attractors [5]. Such minimal PWL forms that allow for an interpretable symbolic and graph-theoretical representation were discovered even from challenging physiological and neuroscientific data. They also profoundly ease subsequent model analysis. For instance, with only a few linear subregions to consider, the search for fixed points or cycles becomes very fast and efficient [26].\nLimitations While this seems promising, how to determine whether a topologically minimal and valid reconstruction from empirical data has truly been achieved remains an open topic. Performance curves as in Fig. 3 or Fig. 9 give an indication of how many PWL units may be required to yield an optimal minimal representation, but whether there is a more principled way of automatically inferring the optimal number P of PWL nonlinearities from data may be an interesting future direction. Finally, the current finding that even for empirical ECG and fMRI data a few linear subregions (\u2264 8) suffice for faithful reconstructions is encouraging. Whether this more generally will be the case in empirical scenarios is another interesting and open question. Not all types of (empirically observed) dynamical systems may easily allow for such topologically minimal representations."}, {"title": "A Appendix", "content": "A.1 Graph Representation\nThe symbolic graph construction followed the rules given in Sect. 3.2: Most generally, each linear subregion U\u2081 is assigned a node (symbol), and a directed edge is drawn between nodes i, j, i \u2192 j, whenever $F_\\theta(U_i) \u2229 U_j \u2260 \u00d8$. Here, however, we are mostly interested in the topological graphs representing particular chaotic attractors (like the Lorenz-63 or R\u00f6ssler attractor), and hence restrict the graph representation to the nodes corresponding to subregions visited by trajectories on the attractor B, and edges drawn when $F_\\theta(U_i \u2229 B) \u2229 U_j \u2260 \u00d8$. More specifically, we first sampled a long trajectory $Z = \\{Z_1,\u2026, Z_T\\}$ with $T = 100,000$ time steps, removed the first 1000 time steps as transients, and counted all transitions between any two subregions $U_i, U_j$. To obtain a more nuanced geometrical/ statistical picture, we also evaluated the relative number of time steps the trajectory spent in each subregion $U_i$ (i.e., an estimate of the occupation measure), |\\{zt \u2208 Ui,t > 1000\\}|/(100,000 \u2013 1000), as well as the relative frequency of transitions between any two subregions $U_i, U_j, |\\{zt, Zt+1|t > 1000, zt \u2208 Ui, F_\\theta(zt) \u2208 Uj\\}|/(100,000 \u2013 1001), yielding a weight for each edge, or a distance between nodes through the graph's Laplacian (see below). To enhance the readability of the larger graphs in Figs. 6, 11, 15 and 21, we removed self-connections (time steps where the trajectory remained within a single subregion).\nLaplacian matrix The Laplacian matrix of a graph is defined as L = D \u2013 A where A is the adjacency matrix of the graph containing the weights (transition probabilities), and D is the out-degree matrix, which is a diagonal matrix where each element is the sum of the outgoing edge weights of node i, $D_{ii} = \\sum_j A_{ij}$. The spectral layout in networkx uses the eigenvectors of the Laplacian matrix corresponding to the smallest non-zero eigenvalues as positions for the nodes. This tends to group more tightly connected nodes closer together. The Laplacian is more widely used as a dimensionality reduction technique in ML, for example in Laplacian eigenmaps [7], and has also been used to represent discretizations of PDEs as graphs [90].\nProximity matching The mapping from latent space to observation space is not unique because M > N, and hence the nullspace of the linear observation model is non-empty (does not contain just the 0 vector). For the AL-RNN this is evident from the fact that the non-readout neurons, particularly the PWL neurons, do not contribute to the observations. However, in practice, for the freely generated activity of trained AL-RNNs, points that fall into the same linear subregion in latent space also were close in observation space. This implies that attractors are segmented into different subregions in latent space in accordance with the observable dynamics. To numerically confirm this, we found that proximal points in observation space were typically related to the same linear subregion when generating activity from trained AL-RNNs: We conducted proximity matching by defining a threshold distance (e.g. d = 0.05, corresponding to 5% of the data variance) and assessing whether generated latent trajectory points proximal in observation space fall into the same or different subregions. We found that for the geometrically minimal reconstruction of the R\u00f6ssler system only 6% of proximal data points (within d) belonged to different subregions, while for the Lorenz-63 attractor (Fig. 11) 4% of proximal data points (within d) belonged to different subregions, confirming that the attractors were segmented into relatively distinct patches."}, {"title": "A.2 Methodological Details", "content": "Training method Our training method rests on a variant of sparse teacher forcing. This approach has recently been proven to effectively tackle gradient divergence when training on observations from chaotic DS [67] and has shown SOTA performance on benchmark and real-world systems in DSR [11, 39]. In sparse teacher forcing, the idea is to directly replace latent states (or a subset of them) with states inferred from observations at intervals \u03c4 while leaving the network to evolve freely otherwise. To obtain forced states, the observation model needs to be 'pseudo-inverted'. Here we employ a specific variant of sparse teacher forcing called identity teacher forcing [67, 11], where this pseudo-inversion becomes trivial by adopting an identity mapping as the observation model:\n$x_t = Iz_t,$\nwith $I \u2208 \u211d^{N\u00d7M}$, and $I_{rr} = 1$ for the $N$ read-out neurons, $r < N$, and zeroes elsewhere. During training, the read-out states are replaced with observations every \u03c4 time steps:\n$z_{t+1} =  \\begin{cases}\nF_\\theta(z_t) & \\text{if } t\u2208 \u03a4 \\\\\nF_\\theta(z_t) & \\text{else}\n\\end{cases}$,\nwith $T = \\{l\\tau + 1\\}_{l\u2208\\mathbb{N}_0}$, and $\\tilde{z}_t = (x_t, z_{N+1:M,t})^T$. Employing identity teacher forcing splits the AL-RNN into essentially three types of units, the N linear readout-neurons $z_l$ which are equivalent to the predicted observations and teacher-forced during training, the remaining M \u2212 P \u2212 N linear neurons $z_l$, and the P nonlinear neurons $z_l$. The overall model and architecture are illustrated in Fig. 1. The AL-RNN can be trained using Mean Squared Error (MSE) loss over model predictions and observations:\n$l_{MSE}(X,\\hat{X}) = \\frac{1}{N\u22c5T}\\sum_{t=1}^T |\\hat{x}_t - x_t|^2,$\nwhere $\\hat{X}$ are the model predictions and $X$ denotes the training sequence of length T. We found that performance was better when the read-out neurons were linear rather than ReLU-based. Note that the M \u2212 N non-readout neurons, including the PWL neurons, do not explicitly con-tribute to the loss function. We used rectified adaptive moment estimation (RADAM) [59] as the optimizer, with L = 50 batches with S = 16 sequences per epoch. Further, we chose M = {20, 20, 100, 100, 100, 130}, \u03c4 = {16,8,10,7,20,10}, T = {200, 300, 50,72, 50, 100}, initial learning rates \u03b1start = {10^{-3},5\u22c510^{-3},2\u22c510^{-3},5\u22c510^{-3},10^{-3},10^{-3}}, \u03b1end = 10^{-5} and epochs = {2000, 3000, 4000, 2000, 3000, 2000} for the {Lorenz-63, R\u00f6ssler, ECG, fMRI,Lorenz-96,EEG} dataset, respectively. Parameters in W were initialized using a Gaussian initialization with \u03c3 = 0.01, h as a vector of zeros, and A as the diagonal of a normalized positive-definite random matrix [11, 97]. The initial latent state z\u2081 = [x1, Lx1]T is estimated from x\u2081 using a matrix $L\u2208\u211d^{(M\u2212N)\u00d7N}$ which is jointly learned with the other model parameters. Additionally, for the R\u00f6ssler and Lorenz systems, we added 5% observation noise during training. Across all training epochs of a given run, we consistently selected the model with the lowest Dstsp. Each individual training run of the AL-RNN was performed on a single CPU. Depending on the training sequence length, a single epoch took between 0.5 to 3 seconds.\nGeometric agreement For evaluating attractor geometries, we use a state space measure Dstsp based on the Kullback-Leibler (KL) divergence, which assesses the (mis)match between the ground truth spatial distribution of data points, $p_{true}(x)$, and the distribution $p_{gen}(x | z)$ of trajectory points freely generated by the inferred DSR model. These probability distributions can be approximated in different ways from the observed/ generated trajectories. Here, we usually sampled long trajectories corresponding to the test set length (usually T = 100,000 time steps, but sometimes shorter for the empirical time series) from trained systems, removing transients to ensure that the system has reached a limit set. For low-dimensional systems, the KL divergence can be straightforwardly calculated through a discrete binning approximation [11]:\n$D_{tstp}(p_{true}(x), p_{gen}(x|z)) \u2248 \\sum_{k=1}^K p_{true}(x) log \\frac{p_{true}(x)}{p_{gen}(x|z)},$\nwhere $K = m^N$ is the total number of bins, with m bins per dimension and N being the dimension of the ground truth system. A bin number of m = 30 per dimension was chosen as a good compromise for distinguishing between successful and bad reconstructions for 3d systems. Since the number of data required to fill the bins scales exponentially with N, for the ECG time series (N = 5) we reduced the number of bins to m = 8, as suggested in [38].\nTemporal agreement To assess temporal agreement, we computed Hellinger distances $D_H$ between power spectra [67, 39]. We first simulated long time series T = 100,000 (as with Dstsp above). After standardization, we computed dimension-wise Fast Fourier Transforms (FFT). The power spectra were smoothened using a Gaussian kernel and normalized, and the extended, high-frequency tails, which predominantly contained noise, were truncated. The Hellinger distance between smoothed ground-truth spectra $F(\u03c9)$ and generated spectra $G(\u03c9)$ is given by:\n$H(F(\u03c9), G(\u03c9)) = \\sqrt{1 - \\int_{-\u221e}^\u221e \\sqrt{F(\u03c9)G(\u03c9)}d\u03c9} \u2208 [0, 1]$"}, {"title": "Maximum Lyapunov exponent", "content": "Maximum Lyapunov exponent The maximum Lyapunov exponent of a system quantifies how fast nearby trajectories diverge, and for a flow map can be computed in the limit T \u2192 \u221e from the system's product of Jacobians. To approximate the maximum exponent numerically, we first iterated a trained model forward from some randomly drawn initial condition and discarded transients. Given that for chaotic systems the product of Jacobians itself explodes [67], we employed a numerical algorithm from [107, 103] that re-orthogonalizes the series of Jacobian products at regular intervals using a QR decomposition.\nCategorical decoder We coupled categorical observations to the P PWL neurons via a link function given by\n$\\pi_i = \\frac{\\exp (\u03b2_i^T z)}{\\sum_{i=1}^{K-1} \\exp (\u03b2_i^T z) + 1}  \u2200i \u2208 \\{1... K \u2212 1\\}\\\n\\pi_K = \\frac{1}{\\sum_{i=1}^{K-1} \\exp (\u03b2_i^T z) + 1},$\nThe regression weights $\u03b2_i \u2208 \u211d^{P\u00d71}$ are learned for each category i = 1 ... K \u2212 1, while the normalization ensures that the total probability over all categories sums to one, $\\sum_{i=1}^K\u03c0_i = 1$."}, {"title": "A.3 Benchmark datasets", "content": "Lorenz-63 The Lorenz-63 system, formulated by Edward Lorenz in 1963 [61] to model atmospheric convection, is defined by\n$\\frac{dx}{dt} = \u03c3(y \u2212 x)\\\n\\frac{dy}{dt} = x(\u03c1 - z) \u2212 y\\\\\n\\frac{dz}{dt} = xy - \u03b2z,$\nwhere \u03c3, \u03c1, \u03b2, are parameters that control the dynamics of the system (here set to \u03c3 = 10, \u03b2 = 3, and \u03c1 = 28, in the chaotic regime). The system was solved numerically with integration time step \u0394t = 0.01 using scipy.integrate with the default RK45 solver.\nRossler The R\u00f6ssler system, intended by Otto R\u00f6ssler in 1976 [85] as a further simplification of the Lorenz model, produces chaotic dynamics using a single nonlinearity in one state variable:\n$\\frac{dx}{dt} = -y - z\\\\\n\\frac{dy}{dt} = x + ay\\\\\n\\frac{dz}{dt} = b + z(x - c),$\nwhere a, b, c, are parameters controlling the dynamics of the system (here set to a = 0.2, b = 0.2, and c = 5.7, in the chaotic regime). The system was solved with integration time step \u0394t = 0.08 using scipy.integrate with the default RK45 solver."}, {"title": "Human electrocardiogram", "content": "Human electrocardiogram The electrocardiogram (ECG) time series was taken from the PPG-DaLiA dataset [82]. With a sampling frequency of 700Hz, the recording duration spanned 600 seconds, yielding a time series of T = 419, 973 time points. Initially, a Gaussian smoothing filter with \u03c3 = 6 was applied, resulting in a filter length of l = 8\u03c3 + 1 = 49. We standardized the time series and applied temporal delay embedding using the DynamicalSystems.jl Julia library, resulting in an embedding dimension of m = 5. For model training, the first T = 100,000 time steps (approximately 143 seconds) were used, downsampled to every 10th datapoint."}, {"title": "Human fMRI data", "content": "Human fMRI data The functional magnetic resonance imaging (fMRI) data from human subjects performing three cognitive tasks is publicly available on GitHub [52", "52": "and selected the first principal component of BOLD activity in each of the 20 brain regions. Subjects underwent five rounds of three cognitive tasks ('Choice Reaction Task [CRT", ", \u2018Continuous Delayed Response Task [CDRT]": "nd 'Continuous Matching Task [CMT", "), together with a": "est' and 'Instruction' period. The time series per subject were short (T = 360) and reconstructions in [52", "62": "is defined by\n$\\frac{dx_i}{dt} = (x_{i+1} - x_{i\u22122})x_{i\u22121} - x_i + F,$\nwith system variables $x_i, i = 1, ..., N$, and forcing term F (here, F = 8, in the chaotic regime). Furthermore, cyclic boundary conditions are assumed with $x_{\u22121} = x_{N\u22121}, x_0 = x_N"}]}