{"title": "Data Lineage Inference: Uncovering Privacy Vulnerabilities of Dataset Pruning", "authors": ["Qi Li", "Cheng-Long Wang", "Yinzhi Cao", "Di Wang"], "abstract": "Dataset Pruning, or Coreset Selection, is one of the core topics in data-centric AI. It removes redundant portions of a dataset to select a smaller, more efficient subset for downstream training, improving efficiency without significantly impacting model performance. While dataset pruning explicitly aligns with GDPR's data minimization principle by limiting redundant data exposure in training, its data-side privacy risks remain underexplored. Existing privacy attacks and defenses in machine learning typically focus on training samples, overlooking and unable to address the privacy risks of data used prior to model training. Despite being excluded from training by an optimized algorithm, the redundant set plays a 'dark side' role in contributing implicitly to the machine learning system. They share similar privacy-sensitive information as the selected training samples, since both are often collected for the same purposes and undergo the same data preprocessing. Their exposure can also pose significant privacy risks. In this work, we systematically explore the data privacy issues of dataset pruning in machine learning systems. Our findings reveal, for the first time, that even if data in the redundant set is solely used before model training, its pruning-phase membership status can still be detected through attacks. Since this is a fully upstream process before model training, traditional model output-based privacy inference methods are completely unsuitable. To address this, we introduce a new task called Data-Centric Membership Inference and propose the first ever data-centric privacy inference paradigm named Data Lineage Inference (DaLI). Under this paradigm, four threshold-based attacks are proposed, named WhoDis, CumDis, ArraDis and SpiDis. We show that even without access to downstream models, adversaries can accurately identify the redundant set with only limited prior knowledge. Furthermore, we find that different pruning methods involve varying levels of privacy leakage, and even the same pruning method can present different privacy risks at different pruning fractions. We conducted an in-depth analysis of these phenomena and introduced a metric called the Brimming score to offer guidance for selecting pruning methods with privacy protection in mind.", "sections": [{"title": "1. Introduction", "content": "During the past decades, deep neural networks have achieved significant success across various research fields."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Data Side-Channels in ML Systems", "content": "In machine learning, privacy related research often treats models as isolated entities, ignoring data pre-processing and post-training processes. However, model training is typically part of a broader ML system with components like data filtering and post-training output/query filtering. [26] experimentally demonstrated that system-level attacks can extract private information more effectively than attacks on standalone models. Another line of research aims to explore the privacy issues in dataset condensation (DC) or dataset distillation (DD). [27] established a theoretical connection between dataset condensation and differential privacy (DP), proving dataset condensation's superiority in privacy preservation over traditional methods. However, [28] pointed out flaws in their experimental and theoretical evidence. [29] successfully conduct backdoor attacks by embedding triggers during the distillation process and iteratively optimizing them to enhance attack performance. Current works all conduct attacks or develop defenses against attacks within the model-centric scenario. Additionally, the limitations of DC/DD are quite apparent. The condensation/distillation process itself places a high demand on computational resources [30], [31], which contradicts its original goal of efficiency and greatly limits its applicability. In contrast, dataset pruning is much more resource-friendly and has been widely applied in the ML community. Some pruning services in diverse fields are already publicly available. For example, Data Brokers like Acxiom [32] and Experian [33] collect and curate data from a variety of sources and offer tailored datasets that meet specific user requirements, including de-duplication and filtering processes that enhance dataset quality for downstream tasks. Additionally, the 'Global Database of Events, Language, and Tone (GDELT)' [34] autonomously gathers data from news and social media, organizing and categorizing them by topic, sentiment, and geographic region, providing pre-processed datasets for research and ML applications. The data-centric trend in ML community has also caught the attention of security researchers. Some have started recognizing privacy risks beyond model training [35], [36], [37]. Compared to these works, this paper is the first to explore the privacy risks associated with data collected but excluded from training at the upstream level."}, {"title": "2.2. Model-Centric Membership Inference", "content": "Previous mainstream Membership Inference Attacks (MIAs) can be categorized into two types: Binary Classifier-based and Metric-based [38]. The former involves training a binary classifier to differentiate between the behaviors of the target model's training members and non-members. [39] proposed an influential technique known as shadow training, which remains widely used. The core idea is that an adversary can train multiple shadow models to mimic the target model's behavior, allowing them to construct attack datasets. These attack datasets are then used to train a binary classifier for the final inference. In contrast, metric-based MIAs [40], [41], [42] are simpler and less computationally intensive, yet can still achieve comparable or even superior performance. Metric-based MIAs infer membership by calculating metrics on the target model's prediction vectors and comparing these metrics against a predefined threshold to determine the membership status of data records. This type of method bypasses the need to train a separate binary classifier, relying instead on straightforward metric comparisons. Regardless of the type of attack mentioned above, all require a target downstream model to provide information for the attack, making them closely tied to a specific model. In this paper, we introduce DaLI, the first paradigm for data-centric membership inference. It occurs during the dataset pruning process before model training and does not require the involvement of trained downstream models."}, {"title": "3. Preliminaries", "content": ""}, {"title": "3.1. Data-Centric AI", "content": "A significant driver of the remarkable advancements in Artificial Intelligence (AI) over the past decade has been the availability of abundant and high-quality data, which give rise to Data-Centric AI (DCAI) [43], [44], [45], [46]. As Figure 1 infers, in contrast to traditional Model-Centric AI (MCAI), DCAI places greater emphasis on enhancing the quality and quantity of data, with relatively fixed models [44], [45]. Below is a formal definition of MCAI and DCAI:\n\u2022 Model-Centric AI: MCAI is a paradigm that focuses on selecting the appropriate model type, hyperparameters, learning algorithms, etc., from a variety of options to build effective and efficient AI systems.\n\u2022 Data-Centric AI: DCAI is a paradigm that emphasizes the importance of systematically designing and engineering data to build effective and efficient AI systems. While the transformation from MCAI to DCAI is ongoing, several achievements already underscore its benefits. For instance, the advancements in large-scale language models largely depend on the utilization of extensive high-quality datasets [47], [48]. Compared to GPT-2 [49], GPT-3 [50] made only minor modifications to the network architecture while investing considerable effort into gathering significantly larger and higher-quality datasets for training."}, {"title": "3.2. Dataset Pruning", "content": "Workflow. Consider a classification task with a training dataset containing N examples drawn i.i.d. from an underlying distribution P. We denote the training dataset as $D_{tra} = \\{(X_i, Y_i)\\}_{i=1}^{N}$, where $x_i$ is the feature vector, and $Y_i$ is the ground-truth label. The goal of dataset pruning is to choose a selected set with a pruning fraction a before model training that should have a minimal impact on the"}, {"title": "4. Threat Model", "content": ""}, {"title": "4.1. Attack Scenario", "content": "We define our attack scenario as a game between an upstream data service provider (e.g., the data brokers mentioned in Section 2.1) and an adversary (e.g., a third-party auditor or data entity). Upon receiving a service request from the adversary, the service provider optimizes, filters, and processes data under their management according to specified requirements, then delivers the resulting selected set to the adversary. The adversary then uses this selected set to infer the pruning-phase membership status of a victim datapool (containing both redundant data that have undergone pruning and other non-members that have not), aiming to identify redundant data that was involved in pruning but ultimately excluded from model training. This scenario assumes no knowledge or access to downstream models,"}, {"title": "4.2. Adversary's Capability", "content": "As described in the aforementioned attack scenario, we assume no knowledge of or access to downstream models trained on the selected set. Furthermore, under the strictest service agreements (representing the most challenging attack scenario), besides the inference subjects (i.e., the victim datapool, analogous to the target dataset containing both members and non-members in traditional MIAs), the adversary receives only the selected set delivered by the service provider to infer the membership of data in the victim datapool without any additional information such as the pruning method, pruning fraction, or data source/distribution that could aid in building local auxiliary data\u2014although in practice, these details may sometimes be disclosed depending on the service agreement [32], [33], [34]. We will explore all practical scenarios in our experiments and demonstrate that none of these various pieces of information or prior knowledge are essential."}, {"title": "4.3. Adversary's Goal", "content": "The adversary's goal is to distinguish the pruning-phase membership status of data in the victim datapool. As the term suggests, 'membership' here is based on whether the data participated in the pruning process. Data involved in the pruning process (the known selected set and the redundant set to be inferred) are members, while all other data are regarded as non-members (we name them as other non-members). Since the redundant set inherently contributes a positive impact on downstream model training, this distinction can help protect data entity rights and prevent service providers from evading responsibility. Figure 3 presents a comparison between the adversary's goal in our scenario and that of traditional model-centric membership inference. They target at different aspects of data flow within the ML system, yet both focus on the privacy issues of critical data that affect the model utility."}, {"title": "4.4. Adversary's Challenge", "content": "Since the adversaries are facing an upstream data privacy tracing issue, previous data privacy inference frameworks"}, {"title": "5. Attack Overview", "content": "Before diving into the detailed components of DaLI, this section provides an overview of the attack framework. We will first introduce the general idea behind DaLI and discuss the primary information source for conducting the attack: the occurrence distributions."}, {"title": "5.1. General Idea", "content": "Figure 4 shows the general pipeline of DaLI. In simple terms, the intuition behind our inference attack is the observation that the occurrence distribution (the definition will be given later) of the redundant set and other non-members shows a significant difference. In DaLI, the adversary constructs several shadow datapools and gradually builds a shadow occurrence distribution pair for each (i.e., $S_{shadow}^{(i)}$ for the redundant set and $S_{shadow}^{(i)}$ for other non-members, where i denotes the index of a shadow datapool) through the Datapool Splitter, Footprint Counter, and Distribution Estimator. Attack thresholds are then constructed based on these occurrence distribution pairs. Meanwhile, a mixed occurrence distribution is constructed for the victim datapool (i.e., $S_{victim}$). Finally, a threshold calibration process is conducted to derive the final thresholds that can be applied to the victim datapool. Throughout the overall pipeline, constructing the occurrence distribution for the victim and shadow datapools is key to executing the attack. We now present the formal definition of occurrence distribution."}, {"title": "5.2. Occurrence Distribution of Victim Datapool", "content": "For the construction of the victim occurrence distribution $S_{victim}$ of the victim datapool Q composed of the victim redundant set $D_{victim}$ and other non-members $D_{non}^{victim}$ (i.e., $Q_v = D_{victim} \\cup D_{non}^{victim}$), we perform extensive sampling from $Q_v$ to form a query set collection $\\{q_{victim}\\}_{j \\in J}$ via Datapool Splitter (see Section 6.1). Each of the query sets is processed along with the victim selected set $D_{sel}^{victim}$ using the underlying dataset pruning method to obtain the corresponding dataset split. To avoid confusion, we denote the selected set and the redundant set in the dataset split as the picking set $D_{pic}^{victim}(j)$ and the culling set $D_{cul}^{victim}(j)$, respectively (i.e., $D_{pic}^{victim}(j), D_{cul}^{victim}(j) = M(D_{victim} \\cup q_{victim}, \\alpha)$).\nAfter constructing the dataset split for each query set, several $D_{cul}^{victim}(j)$ are produced. These sets are used to build the victim occurrence distribution $S_{victim}$. For each sample in the victim datapool, if it appears in any $D_{cul}^{victim}(j)$, it is counted as an occurrence. The occurrence count for each data is then determined, ranging from 0 to the total number of culling sets. Finally, the occurrence distribution for the victim datapool is constructed, which can be formally defined as follows.\nDefinition 1 (Occurrence Distribution of Victim Datapool). Given the occurrence count $t_s$ of each sample $x_s$ in the victim datapool, the occurrence distribution of the victim datapool can be represent as a function $S_{victimon}(t)$:\n$S_{victimon}(t) = \\sum_{s=1}^{Q_v} I(t_s = t),$ (2)\nwhere $S_{victimon}(t)$ denotes the number of samples with an occurrence count of t, I is the indicator function."}, {"title": "5.3. Shadow Occurrence Distribution", "content": "For the shadow occurrence distribution construction, the adversary first constructs several shadow sets $D_{shadow}^{(i)}$ via the auxiliary data and runs a dataset pruning process on each of them, resulting in each shadow set's own shadow selected set $D_{sel}^{shadow}(i)$ and shadow redundant set $D_{cul}^{shadow}(i)$. The adversary then builds several shadow datapools using the redundant set and other non-members, i.e., $Q_{shadow} = D_{cul}^{shadow}(i) \\cup D_{non}^{shadow}(i)$. Each shadow datapool $Q_{shadow}$ shares a same scale $Q_s$.\nSimilar to the victim datapool, for each $Q_{shadow}$, the adversary samples multiple shadow query sets to form a shadow query set collection $\\{q_{shadow}\\}$, where i denotes the index of the shadow datapool and j denotes the index of the query set of the i-th shadow datapool. Each shadow"}, {"title": "6. Data Lineage Inference Components", "content": ""}, {"title": "6.1. Datapool Splitter", "content": "The datapool splitter is designed for the adversary to conduct a sliding window sampling. After each sampling process, a query set sharing the same scale with $D_{red}$ is produced, which is then combined with $D_{sel}$ for further use. To do so, we divide the datapool Q into small batches. According to the size of the redundant set, a batch size \u03c2 is pre-defined, which should be chosen such that the total size of several batches equals the size of the redundant set.\nAfter dividing Q to $\\frac{Q}{\\varsigma}$ batches of data, for the construction of the query sets, the adversary essentially conducts a sliding window sampling. We use \\beta to represent the scale of the redundant set (i.e., $\\beta = |D_{red}|$). Each time there are $\\beta$ number of batches covered in the sliding window. For example, as shown in Figure 5, suppose currently the first batch in the sliding window is the $j^{th}$ batch (i.e., $i_j$), then the current state of the sliding window is: $Z(j) = \\{i_j, i_{j+1}, ..., i_{j+\\beta}\\}$, which together form a query set (i.e., $q_j = I(j)$) and is further combined with $D_{sel}$. We name the combination of a query set $q_j$ and $D_{sel}$ as an attack set $s_j$ (i.e., $S_j = q_j \\cup D_{sel}$). Every time a query set is formed, the first batch in the sliding window is moved to the end, allowing a new batch of data to enter the end of the sliding window to construct a new query set. This process will not end until the last batch is in the 1st position of the window. Finally, $\\frac{Q}{\\varsigma}$ number of attack sets are constructed:\n$A(Q, D_{sel}, \\varsigma, \\alpha) = \\{s_1, s_2, ..., s_{\\frac{Q}{\\varsigma}}\\}$. (5)\nTo simplify, we will use A to denote the attack sets (i.e., $A_V$ for victim datapool and $A_s$ for each shadow datapool). Note that the sliding window sampling process ensures that all samples in the datapool are used  $\\frac{Q}{\\varsigma}$  times, thereby ensuring that they share the same occurrence count range. After the construction of attack sets, we can now use them to reproduce the dataset pruning process. Specifically, given the attack sets, we apply the pruning algorithm to each of them using the pruning fraction \u03b1, obtaining the picking sets and culling sets as defined in Section 5.2, which are subsequently used for footprint construction. In the next section, we will provide the details of the Footprint Counter for the shadow datapools and the victim datapool. Unknown Pruning Fraction \u03b1. For adversaries who are informed the pruning fraction, inferring the exact size of the redundant set is straightforward. However, there are cases in which adversaries do not know the pruning fraction. In such cases, they can employ a mark-recapture method [56], [57] to estimate the fraction. We found that that this approach can help the adversary identify a fraction that is nearly identical to the ground-truth one (see Section 7.4). Influence of Batch Size \u03c2. A simplistic strategy is to set \u03c2 to 1. While this strategy provides the adversary with the"}, {"title": "6.2. FootPrint Counter", "content": "The footprint counter leverages the culling sets to gather data-level statistics and create a footprint table. This table contains the index and the occurrence count of each data, supporting the construction of occurrence distributions."}, {"title": "6.2.1. Shadow Datapool.", "content": "For each shadow datapool $Q_{shadow}$, we have obtained $\\frac{C_s}{\\varsigma}$ number of culling sets $D_{cul}^{shadow(i,j)}$, where $\\varsigma$ is the batch size of the shadow datapool. For samples in a culling set that belong to $D^{shadow(i)}$, $D_{red}^{shadow(i)}$ and $D_{non}^{shadow(i)}$, we termed them as $C_{sel}^{shadow (i,j)}$, $C_{red}^{shadow(i,j)}$ and $C_{non}^{shadow(i,j)}$, respectively. Finally, we have a footprint for the current shadow datapool:\n$G^{shadow}(A_V, M()) = \\{C_{shadow}^{(i,j)}, C_{shadow}^{(i,j)}\\}_{j \\in [1,\\frac{C_s}{\\varsigma}]}$. (6)\nAs shown in Figure 6 (for simplicity, we omit the shadow datapool index i), from the footprint of the current shadow datapool, we can finally form its FootPrint Table, where each data record has its own entry. The entry of a specific data record includes its identifier and occurrence count."}, {"title": "6.2.2. Victim Datapool.", "content": "For the victim datapool $Q_v$, we have $\\frac{C_v}{\\varsigma_v}$ number of culling sets $D_{victim}^{(j)}$, where $\\varsigma_v$ is the batch size of the victim datapool. Similarly, we firstly term samples in the victim culling set $D_{victim}^{(j)}$ belong to $D_{sel}^{victim}, D_{red}^{victim} and D_{non}^{victim}$ as $C_{sel}^{victim (j)}$, $C_{red}^{victim (j)}$ and $C_{non}^{victim (j)}$, respectively. Note that $C_{cul}^{victim (j)}$ and $C_{non}^{victim (j)}$ are mixed and the adversary can only obtain the intersection of the two, i.e., $C_{creatumon}^{(j)} = C_{rictim}^{victm} \\cup C_{rictim}^{victim}$. Thus, the footprint of the victim datapool can be written as:\n$\\gamma^{victim}(A_V, M()) = \\{C_{ric}^{victim}(j)\\}_{j \\in [1,\\frac{C_v}{\\varsigma_v}]}$. (7)\nFinally, the FootPrint Table of the victim datapool is formed, which differs slightly from that of the shadow datapool. While each entry still includes the identifier and occurrence count, the identifier only specifies the sample without indicating its type."}, {"title": "6.3. Distribution Estimator", "content": "With the footprint tables providing sample-level information, the occurrence distribution and its derivatives can now be formed. Referring to Definition 2 and Section 6.2.1, for each shadow datapool, we count the occurrence of each data. This results in two occurrence distributions $S_{shadow}^{(i)}(.)$ and $S_{non}^{shadow(i)}(.)$. For the victim datapool, referring back to Definition 1 and discussions in Section 6.2.2, a mixed occurrence distribution $S_{victim}^{\\text{remon}}(.)$ is formed. Once we have the occurrence distributions of a shadow datapool, their cumulative distribution function (CDF) [58] $F_{cdf}^{(i)}(t)$, $F_{cdf}^{(i)}(t)$ and complementary cumulative distribution function (CCDF) [58] $F_{ccdf}^{(i)}(t)$, $F_{ccdf}^{(i)}(t)$ can be easily derived. Formal definitions can be found in Appendix D. In addition, we define a flag function for $\\delta \\in \\{cdf, ccdf\\}$ as follows:\n$U_{\\delta}(t) = I\\{F_{red}^{(i)}(t) > F_{non}^{(i)}(t)\\}$. (8)\nBased on the CDFs, we introduce the cumulative disparity rates (CDRs) $R_{cdr}^{(i)}(t)$ of each shadow datapool. The CDRs quantify the relative difference between the CDFs of the redundant set and other non-members at different occurrence count t. Accordingly, the complementary cumulative disparity rates (CCDRS) $R_{ccdr}^{(i)}(t)$ can also be defined. They can be formed as:\n$R_{\\delta}^{(i)}(t) = \\frac{max\\{F_{red}^{(i)}(t), F_{non}^{(i)}(t)\\} }{F_{red}^{(i)}(t) + F_{non}^{(i)}(t)}$, (9)\nwhere $R_{\\delta}^{(i)}(t)$ can be $R_{cdr}^{(i)}(t)$ when $\\delta = cdf$ or $R_{ccdr}^{(i)}(t)$ when $\\delta = ccdf$.\nMoreover, for CDFs, we define a function $\\Delta(p,q)$ to quantify the absolute difference between different occurrence counts p < q of the same CDF (e.g., for $F_{cdf}^{(i)}(t)$, $\\Delta_{red}^{(i)}(p, q) = F_{cdf}^{(i)}(q) - F_{cdf}^{(i)}(p)$). The concept of both the flag function and CDR of CDFs can then be extended to a bivariate version:\n$U_{cdf}(p,q) = I\\{\\Delta_{red}^{(i)}(p, q) > \\Delta_{non}^{(i)}(p, q)\\}$, (10)\n$R_{cdr}^{(i)}(p, q) = \\frac{max\\{\\Delta_{red}^{(i)}(p, q), \\Delta_{non}^{(i)}(p, q)\\} }{\\Delta_{red}^{(i)}(p, q) + \\Delta_{non}^{(i)}(p, q)}$. (11)\nNow we introduce the proposed attacks based on the previously determined concepts."}, {"title": "6.4. Threshold Determination", "content": ""}, {"title": "6.4.1. Whole Distribution Difference Ratio (WhoDis).", "content": "In WhoDis, we are based on the following observation: The occurrence distributions of most dataset pruning methods share a phenomenon where the occurrence counts of other non-members are densely clustered around smaller values, while those of the redundant set are more dispersed. A detailed illustration of this can be found in the experiments (see Section 7.2). In WhoDis, we aim at inferring all data in the victim datapool. To do so, for each shadow datapool, we first identify the occurrence count that maximizes the difference between the two CDFs (for the redundant set"}, {"title": "6.4.2. Cumulative Difference Ratio (CumDis).", "content": "Here, two thresholds are determined to infer part of the datapool. Firstly, we find the occurrence count corresponding to the maximum CDR and set it as the lower threshold $T_{CL}^{(i)}$:\n$T_{CL}^{(i)} = \\mathop{\\arg \\max}_t R_{cdr}^{(i)}(t)$. (13)\nAccordingly, a flag $\\mathcal{K}_{CL}^{(i)}$ is designed to determine the higher value among the two CDFs used in $R_{cdr}^{(i)}(t)$: $\\mathcal{K}_{cL}^{(i)} = U_{cdf}(T_{cL}^{(i)})$. If $\\mathcal{K}_{cL}^{(i)}$ equals 1, redundant data constitutes the majority below the threshold $T_{CL}^{(i)}$. Therefore, we infer the data below $T_{CL}^{(i)}$ as redundant data. For the two CCDFs, we can determine the upper threshold $T_{CU}^{(i)}$ and the flag $\\mathcal{K}_{CU}^{(i)}$ in the same manner. Finally, a majority vote is conducted, similar to the process in WhoDis, to get the final thresholds $T_{CL}$, $T_{CU}$ and flags $\\mathcal{K}_{CL}, \\mathcal{K}_{CU}$."}, {"title": "6.4.3. Arbitrary Range Difference Ratio (ArraDis).", "content": "Here, we aim at identifying the interval with the most significant difference between $F_{cdf}^{(i)}(.)$ and $F_{cdf}^{(i)}(.)$. We define two thresholds to represent the beginning and the end of the interval, respectively. These thresholds are determined as:\n$T_{aL}^{(i)}, T_{aU}^{(i)} = \\mathop{\\arg \\max}_{p,q} R_{cdr}^{(i)} (p,q)$. (14)\nRecall Equation (10), the flag $\\mathcal{K}_a{(i)}$ can thus be defined as: $\\mathcal{K}_a{(i)} = U_{cdf}(T_{aL}(i), T_{aU}(i))$. Finally, we conduct a majority vote for the threshold pair and the flag, resulting in the final thresholds $T_{aL}$ and $T_{aU}$ and the final flag $\\mathcal{K}_a$."}, {"title": "6.4.4. Spike Difference Ratio (SpiDis).", "content": "In SpiDis, we aim at identifying the most venerable data records directly via the occurrence distributions, i.e., for each shadow datapool, we locate a specific occurrence count value t, where the two types of data have the most significant occurrence distribution difference. We set this specific value t as the threshold $T_{sp}^{(i)}$:\n$T_{sp} (i) = \\mathop{\\arg \\max}_t \\frac{\\text{max}\\{S_{shadow}^{(i)} (t), S_{non}^{shadow(i)}(t)\\} }{S_{shadow}^{(i)} (t) + S_{non}^{shadow(i)}(t)}$. (15)\nThe flag $\\mathcal{K}_{sp}(i) = I\\{S^{Shadow(i)}(T_{sp}(i)) > S^{Shadow(i)}(T_{sp}(i))\\}$ is then derived. Finally, a majority vote is done on all the shadow datapools to get the final threshold $T_{sp}$ and flag $\\mathcal{K}_{sp}$."}, {"title": "6.5. Threshold Calibration", "content": "According to previous discussions, the occurrence count range and the batch size of the victim datapool and the shadow datapool set by the adversary may be different. Thus, to align the shadow datapool with the victim datapool, a threshold calibration step should be conducted. Specifically, the calibration is applied on all of the designed thresholds in the previous section. We utilize the batch numbers of the two datapools, as they contain information related to both the datapool scale and the batch size. The formal definition is as follows:\n$T^{\\prime} = T\\frac{\\frac{Q_v}{\\varsigma_v}}{\\frac{Q_s}{\\varsigma_s}}$. (16)\nWe can now use the derived thresholds and flags to perform threshold-based attacks on the victim occurrence distribution. Detailed experimental results will be presented in the next section."}, {"title": "7. Experimental Evaluation", "content": "In this section, we present the practical performance of DaLI through four proposed inference attacks. Specifically, we conduct extensive experiments to address the following key inquiries (KeyIQs):\n\u2022 KeyIQ1: Is dataset pruning truly as effective as previously anticipated, ensuring a balance between utility and efficiency while also safeguarding privacy?\n\u2022 KeyIQ2: How does the risk of privacy inference change across different pruning methods?\n\u2022 KeyIQ3: Can we develop an effective single metric to measure privacy inference risk across different methods and settings, thereby facilitating privacy-level evaluations across different pruning methods?\n\u2022 KeyIQ4: How do the attacks fare when the adversary possesses varying degrees of knowledge?\n\u2022 KeyIQ5: Can we explore dedicated defense strategies to mitigate the inference effectiveness?\nOverview. Specifically, we first present the statistical rationale behind the four proposed threshold attacks and validate the performance of DaLI when the adversary possesses the most knowledge of the pruning process. Following this, we propose a unified evaluation metric, the Brimming score, which effectively measures the privacy risk of different methods under various settings. In addition, we introduce four more challenging scenarios, each based on different levels of adversary knowledge (see Section 7.4) We will show that DaLI remains effective even in the most challenging scenario. Finally, we make a preliminary attempt on the defense side."}, {"title": "7.1. Experimental Setup", "content": "Pruning Methods. We use twelve popular pruning methods: DeepFool [59], Contextual Diversity (Co.Div.) [11], Cal"}, {"title": "7.2. Evidence for Conducting Attack", "content": "To facilitate the subsequent discussion on attacks in scenarios of partial knowledge and our defense attempts, we first present the evidence and intuition behind the two proposed attacks (WhoDis and ArraDis). The evidence for the remaining two attacks will be provided in Appendix G."}, {"title": "7.3. Attack Performance", "content": ""}, {"title": "7.3.1. Attack Under Full Knowledge.", "content": "We first attempt to use DaLI under the scenario where the adversary has complete knowledge (i.e., knowing both the pruning method and fraction). In this scenario, we primarily aim to provide a negative answer to KeyIQI and a detailed response to KeyIQ2. Results are shown in Table 1. From the results, we can make the following general observations:"}, {"title": "7.3.2. Brimming Score.", "content": "Given the trends and phenomena observed regarding different pruning methods, we can clearly pose the following question: Can we propose a unified single metric to measure the privacy risk of different methods? To address this issue, which is consistent with KeyIQ3, we propose a metric named the Brimming score. The design and inspiration for this metric come from the"}, {"title": "7.4. Attack under Partial Knowledge", "content": "In this section, we consider four other realistic scenarios where the adversary's knowledge of the pruning process is somewhat incomplete, consistent with KeyIQ4."}, {"title": "7.4.1. Unknown Pruning Fraction.", "content": "In this scenario, we first conduct attacks in a simple manner: the adversary simply assumes a fraction from a set of potential values. Later, we will show that the adversary can estimate a fraction nearly identical to the ground-truth fraction using the mark-recapture method [56], [57], indicating that knowledge of the fraction is not necessary for conducting inference. Simply Guessing. In this experiment, we considered three different fraction settings: 0.2, 0.4, and 0.8. The adversary may assume any of these fractions for their pruning process (both on the shadow and victim datapool). We used three pruning methods (Cal, Glister, and GraNd) on CIFAR10. Results are shown in Figure 11 (results for other datasets are in Appendix I). From the results, the effectiveness of DaLI sees different degrees of decline under various settings. Some potential patterns are evident: when the difference between the ground truth fraction and the fraction assumed by the adversary is small (e.g., the ground truth fraction is 0.2, and the adversary assumes a fraction of 0.4), considerable attack performance is still observed. However, when the difference is large, the inference becomes less effective. This"}, {"title": "7.4.2. Unknown Pruning Method.", "content": "In this scenario, the adversary can only choose one pruning method from some potential methods to carry out the inference. We use Cal and Glister as the ground truth pruning methods (used by the service provider) and use the other three pruning methods (Co.Div., GraNd, kCent.) as the shadow pruning methods employed by the adversary. Experiments were conducted on three datasets with four different pruning fractions (Frac. in short). Results under CIFAR10 are shown in Figure 13 (results for other datasets are given in Appendix I). The results indicate that the impact of the pruning method is seemingly less significant than that of the pruning fraction. In most cases, the adversary can still exhibit extraordinary attack capabilities (with a success rate exceeding 90%). Additionally, overall, the privacy risk is most severe when"}, {"title": "7.4.3. Unknown both Pruning Method and Pruning Fraction.", "content": "As discussed in Section 7.4.1", "scenario": "the adversary simply assumes a method and fraction for inference. In this experiment, without loss of generality, we consider four methods for the victim datapool (Cal, Craig, Glist., GraNd). Two ground truth fractions (0.2, 0.4) and adversary assumed fractions (0.4, 0.8) are considered here"}]}