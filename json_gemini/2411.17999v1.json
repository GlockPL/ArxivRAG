{"title": "A Novel Pareto-optimal Ranking Method for Comparing Multi-objective Optimization Algorithms", "authors": ["Amin Ibrahim", "Azam Asilian Bidgoli", "Shahryar Rahnamayan", "Kalyanmoy Deb"], "abstract": "As the interest in multi- and many-objective optimization algorithms grows, the performance comparison of these algorithms becomes increasingly important. A large number of performance indicators for multi-objective optimization algorithms have been introduced, each of which evaluates these algorithms based on a certain aspect. Therefore, assessing the quality of multi-objective results using multiple indicators is essential to guarantee that the evaluation considers all quality perspectives. This paper proposes a novel multi-metric comparison method to rank the performance of multi-/ many-objective optimization algorithms based on a set of performance indicators. We utilize the Pareto optimality concept (i.e., non-dominated sorting algorithm) to create the rank levels of algorithms by simultaneously considering multiple performance indicators as criteria/objectives. As a result, four different techniques are proposed to rank algorithms based on their contribution at each Pareto level. This method allows researchers to utilize a set of existing/newly developed performance metrics to adequately assess/rank multi-/many-objective algorithms. The proposed methods are scalable and can accommodate in its comprehensive scheme any newly introduced metric. The method was applied to rank 10 competing algorithms in the 2018 CEC competition solving 15 many-objective test problems. The Pareto-optimal ranking was conducted based on 10 well-known multi-objective performance indicators and the results were compared to the final ranks reported by the competition, which were based on the inverted generational distance (IGD) and hypervolume indicator (HV) measures. The techniques suggested in this paper have broad applications in science and engineering, particularly in areas where multiple metrics are used for comparisons. Examples include machine learning and data mining.", "sections": [{"title": "I. INTRODUCTION", "content": "Since many real-world problems are modeled as multi-objective optimization problems, a large number of multi-objective algorithms have been developed to tackle them [1]\u2013[3]. In practical approaches, multi-objective algorithms are required to deal with a set of conflicting objectives, and thus finding the optimal solutions may not be easily observable [4]. Accordingly, the nature of conflicting objectives generates a set of trade-off solutions as the Pareto front solutions of a multi-objective optimization problem. On the other hand, the evaluation of the solution sets produced by the multi-objective algorithms, which presents various trade-offs among the objectives must be quantitatively appraised using a variety of measurement metrics [5]. While the quality evaluation of the single-objective solution is trivial (i.e., for minimization, the smaller the value of the objective, the better), measuring the quality of Pareto fronts resulting from multi-objective algorithms is complex. A Pareto front should be evaluated using aspects such as diversity, distribution, and closeness to the true Pareto front [6]. In order to assess the developed algorithms and analyze the results, a vast number of performance metrics have been proposed [7]. Miqing Li et al. [8] presented a study of 100 multi-objective optimization metrics that have been used in the specialized literature. Their paper discusses the usage, trends, benefits, and drawbacks of the most popular metrics to provide researchers with essential knowledge when selecting performance metrics.\nAudet et al. [9] classified multi-objective performance indicators into four main categories including cardinality indicators, convergence indicators, distribution and spread indicators, and convergence and distribution indicators. Cardinality indicators such as two-set coverage [10] evaluate the quality of the Pareto front based on the number of non-dominated points generated by the corresponding algorithm. Convergence indicators quantify the closeness of the resulting Pareto front to the true Pareto front. Generational distance (GD) [11] and inverted GD (IGD) [12] are two well-known metrics that belong to this category. Distribution and spread indicators measure the distribution and the extent of spacing among non-dominated solutions. Spacing [13] is an instance of distribution and spread indicators. Finally, the convergence and distribution indicators capture both the properties of convergence and distribution. One of the popular metrics in this category is the hypervolume (HV) indicator [14].\nIn addition to the categorization based on the factors that each metric captures, the performance indicators can be di-vided into unary and binary [15]. Unary metrics provide a real value after taking into account one or more of the aforementioned factors, whereas binary metrics focus primarily on the relationship between two approximation sets resulting from two algorithms to determine which one is better.\nIn general, there is no universally supreme metric since each performance indicator may have some strengths and limitations [16]. Moreover, we require at least as many in-dicators as the number of objectives in order to determine whether an approximate solution is better than another (i.e., one objective vector dominates or weakly dominates another) [17]. Each developed metric captures one or sometimes several"}, {"title": "II. BACKGROUND REVIEW ON MULTI- AND MANY-OBJECTIVE OPTIMIZATION", "content": "Multi-objective optimization targets handling two or three conflicting objectives and many-objective algorithms aim to tackle more than three conflicting objectives. In recent years, multi-objective optimization algorithms have been greatly expanded to tackle many-objective problems. The use of evolutionary algorithms has been very promising for solving such problems. As a population-based approach, it enables the generation of a set of solutions at each run, with each solution potentially interacting with the others to create even better solutions.\nDefinition 1. Multi-objective Optimization [2]\nMin/Max $F(x) = [f_1(x), f_2(x), ..., f_M(x)]$\ns.t. $L_i \\leq x_i < U_i$, $i = 1, 2, ..., d$ \t\n(1)\nsubject to the following equality and/or inequality constraints.\n$g_j(x) \\leq 0$  $j = 1, 2, ..., J$\n$h_k(x) = 0$  $k = 1, 2, ..., K$\n(2)\nwhere M is the number of objectives, d is the number of decision variables (i.e., dimension), and the value of each variable, $x_i$, is in interval $[L_i, U_i]$ (i.e., box-constraints). $f_i$ represents the objective function, which should be minimized or maximized. The hard constraints that are required to be satisfied are $g_j(x) \\leq 0$ $j = 1,2,..., J$ and $h_k(x) = 0$ $k = 1, 2, ..., K$.\nIn multi- or many-objective optimization problems, finding an optimal solution set is far more complex than in the single-objective case. As such, a trade-off must be made between the different objectives. One way to compare the various candidate solutions is to use the concept of dominance. This involves an assessment of how one solution is better than another with regard to the objectives.\nDefinition 2. Dominance Concept [23] If $x = (x_1, x_2,...,x_d)$ and $\\bar{x} = (\\bar{x}_1,\\bar{x}_2,...,\\bar{x}_d)$ are two vectors in a minimization problem search space, $x$ dominates $\\bar{x}$ ($x <\\bar{x}$) if and only if\n$\\forall i \\in {1, 2, ..., M}, f_i(x) \\leq f_i(\\bar{x}) \\land \\exists j \\in {1, 2, ..., M} : f_j(x) < f_j(\\bar{x})$\n(3)\nThis concept defines the optimality of a solution in a multi-objective space. Candidate solution x is better than $\\bar{x}$ if it is not worse than $\\bar{x}$ in any of the objectives and it has a better value in at least one of the objectives. Solutions that are not dominated by any other solution are called non-dominated solutions; they create the Pareto front set. Multi-objective al-gorithms attempt to find these solutions by utilizing generating strategies/operators and selection schemes. The non-dominated sorting (NDS) algorithm [23] is one of the popular selection strategies that work based on the dominance concept. It ranks the solutions of the population in different levels of optimality, called Pareto. The algorithm starts with determining all non-dominated solutions in the first rank. In order to identify the second rank of individuals, the non-dominated vectors are removed from the set to process the remaining candidate solutions in the same way. Non-dominated solutions of this"}, {"title": "III. LITERATURE REVIEW ON PERFORMANCE INDICATORS", "content": "There are several performance metrics to assess the qual-ity of multi and many-objective algorithms. These metrics evaluate the performance of the algorithms using aspects such as convergence, distribution, and coverage. Each metric may have its advantages and drawbacks. In this section, we review some well-known metrics that we have utilized in our experiments to design the multi-metric ranking method. The table primarily addresses three key aspects of a solution set: convergence (proximity to the theoretical Pareto optimal front), diversity (distribution and spread), and cardinality (number of solutions).\nHypervolume (HV) indicator [14]: HV is a very popular indicator that evaluates multi-objective algorithms in terms of the distribution of the Pareto front and the closeness to true Pareto front. HV indicator evaluates the diversity and convergence of a multi-objective algorithm. It calculates the volume of an M-dimensional space that is surrounded by a set of solution points (A) and a reference point $r = (r_1,r_2,...,r_m)$ where M is the number of objectives of the problem. Therefore, HV measures the volume of the region which is dominated by the non-dominated solutions in the objective space, relative to a reference point, r. A reference point is a point with worse values than a nadir point. The HV measure is defined in Eq. 4.\n$HV(A) = vol (\\bigcup_{a \\in A} [f_1(a), r_1]\\times[f_2(a), r_2]\\times...\\times[f_M(a),r_m])$,\n(4)\nwhere $f_i$ represents the objective function and $a \\in A$ is a point that weakly dominates all candidate solutions. Larger values of HV indicate that the Pareto front surrounds a wider space resulting in more diverse solutions and closer to the optimal Pareto front.\nGenerational Distance (GD) [11]: GD measures the av-erage minimum distance between each obtained objective vector from the set S and the closest objective vector on the representative Pareto front, P, which is calculated as follows:\n$GD(S, P) = \\frac{\\sqrt{\\sum_{i=1}^{|S|} d_i^2}}{||S||}$\n(5)\nwhere dist(i, P) is the Euclidean distance from an approxi-mate solution to the nearest solution on the true Pareto front q = 2. A smaller GD value indicates a lower distance to the true Pareto front and consequently better performance.\nInverted Generational Distance (IGD) [12]: The only difference between GD and IGD is that the average minimum distance measure is from each point in the true Pareto to those in the approximate Pareto front, so that:\n$IGD(S, P) = \\frac{\\sqrt{\\sum_{i=1}^{|P|} dist(i, S)^2}}{||P||}$ , \n(6)\nwhere dist(i, S) is the Euclidean distance between a point in the true Pareto front and the nearest solution on the approximate solution.\nTwo-set Coverage (C) [10]: This metric indicates the rate of solutions on the Pareto front of one algorithm that is dominated using solutions of another algorithm.\n$C(A, B) = \\frac{|\\{b \\in B, there exists a \\in A such that a < b\\}|}{||B||}$\n(7)\nFor example, C(A, B) = 0.25 means that the approximate solutions from algorithm A dominate 25% of the solutions resulting from algorithm B. Obviously, both C(A, B) and C(B, A) should be calculated for comparison.\nCoverage over the Pareto front (CPF) [24]: This is a measure of the diversity of the solutions projected through a mapping from an M-dimensional objective space to an (M \u2013 1)-dimensional space. In this process, a large set of reference points are sampled on the Pareto front, and then each solution on the resulting Pareto front is replaced by its closest reference point. Thus, a new point set P' can be generated as follows:\nP' = {argminr\u2208R  || -f(x) || |x \u2208 P},\n(8)\nwhere R is a set of reference points and P denotes the set of approximate candidate solutions. After the transformation of P' and R (i.e. projection, translation, and normalization) to project the points to a unit simplex, the ratio of the volume of P' and R is calculated as CPF.\n$CPF = \\frac{Vol(P')}{Vol(R)}$\n(9)\nThe details of this metric and the way of calculating volume are given in [24].\nHausdorff Distance to the Pareto front (Ap) [25]: This indicator combines two metrics, GD and IGD. Ap is defined as follows:\n$\\Delta_p(S, P) = max(GD(S, P), IGD(S, P))$\n(10)\nThis metric has stronger properties than the two individual indicators since it combines GD and IGD. For instance, Ap can efficiently handle outliers by considering the averaged result.\nPure Diversity (PD) [26]: Given an approximate solution (A), PD measures the sum of the dissimilarities of each solution in A to the rest of A. For this purpose, the solution with the maximal dissimilarity has the highest priority to accumulate its dissimilarities. The higher the PD metric, the greater the diversity among the solutions. PD is calculated using recursive Eq. 11:\n$PD(A) = max_{s_i \\in A}(PD(A \u2013 s_i) + d(s_i, A \u2013 s_i))$,\n(11)\nWhere,\nd(s, A) = min_{s_i \\in A}(dissimilarity(s, S_i)),\n(12)"}, {"title": "IV. PROPOSED PARETO-OPTIMAL RANKING METHOD", "content": "In this section, we present the details of the multi-metric ranking method. The proposed method ranks competing multi-or many-objective algorithms based on a variety of perfor-mance metrics, simultaneously. First, each algorithm is inde-pendently evaluated using a set of M performance indicators, hence forming M objectives. Then, it combines these M-dimensional points and groups them to Pareto dominance lev-els using the non-dominated sorting (NDS) algorithm. Finally, the ranks of each algorithm are determined using one of the four proposed ranking methods described in this section. The steps of the proposed method to rank A algorithms (for our experiment we have used A = 10 algorithms) when solving many-objective optimization problems are described below:\n1) Select M-multi-objective performance indicators (e.g., HV, IGD, GD,..., etc.).\n2) Run each algorithm R times.\n3) For each run, calculate the performance scores based on M metrics. As a result, for each algorithm, we have a matrix size of R \u00d7 M performance scores.\n4) Concatenate all performance scores from A algorithms to get a matrix size of A \u00d7 R points containing M-dimensional (objectives/criteria) vectors.\n5) Apply the NDS algorithm on A \u00d7 RM-dimensional vectors to generate different levels of ranks. Note that, in order to apply the NDS algorithm, we reverse some of the scores (e.g., since the highest HV value indicates the better algorithm, we replace this value with its inverse or change its sign in case of zeros) so that all metrics are evaluated as a minimization problem instead of a maximization.\n6) Evaluate the ranks of each algorithm based on their contribution at each Pareto level using the proposed ranking techniques discussed below.\nSuppose that we want to compare the performance of A multi/many-objective algorithms for solving a benchmark test problem. We run each algorithm R times and compute the algorithm's performance scores using M existing multi-objective metrics (e.g., HV, IGD, GD,..., etc.). After these steps, we obtain A \u00d7 R M-dimensional points (performance scores). The format of the matrix representing these scores can be illustrated as follows:\nWhere $psa_{i, jm_k}$ indicates the computed performance score for jth run of ith algorithm based on kth metric.\nNext, the proposed method applies the NDS algorithm to place AX RM-dimensional vectors. This process yields a set of Pareto levels and the corresponding points in these levels. Suppose that the NDS algorithm resulted in L levels, say $l_1, l_2, l_3, ..., l_L$. Then, for each algorithm, we count the number of points associated with each Pareto level (i.e., this step allows us to quantify the quality of each algorithm when solving a given problem). Then, the resulting matrix can be illustrated as follows:\nwhere $na_{il_j}$ indicates the number of points (i.e., M-dimensional metrics scores) of ith algorithm on jth Pareto level. Lastly, the ranks of each algorithm are determined using one (or more, in case of a tie) of the four proposed ranking techniques described below.\nOlympic method: The best algorithm is determined by evaluating the number of points each solution has on the first level. If a tie occurs between two solutions, the second level"}, {"title": "V. EXPERIMENTAL VALIDATION: CONDUCTING COMPREHENSIVE COMPARISONS", "content": "The practical application of the proposed metric is uti-lized to rank ten well-known evolutionary multi-objective algorithms submitted to the 2018 IEEE CEC competition. In this competition, participants were asked to develop a novel many-objective optimization algorithm to solve 15 MaF many-objective test problems. The competing algorithms include AGE-II [29], AMPDEA, BCE-IBEA [30], CVEA3 [31], fastCAR [32], HHCMOEA [33], KnEA [34], RPEA [35], RSEA [36], and RVEA [37]. All experiments were conducted on 3-, 5-, and 15-objective MAF test problems and the number of decision variables was set according to the setting used in [38]. Each algorithm was run independently 20 times. The maximum number of fitness evaluations was set to max(100000, 10000 \u00d7 D), and the maximum size of the population was set to 240.\nIn order to assess the efficacy of the proposed ranking method, we obtained the approximated Pareto fronts of these\nA. Experimental settings"}, {"title": "VI. CONCLUSION", "content": "In this study, we have proposed a novel ranking technique to assess the quality of many-objective optimization algorithms using a set of performance metrics. Since there is no one many-objective performance indicator that can capture all"}]}