{"title": "Enhanced Expressivity in Graph Neural Networks with Lanczos-Based Linear Constraints", "authors": ["Niloofar Azizi", "Nils Kriege", "Horst Bischof"], "abstract": "Graph Neural Networks (GNNs) excel in handling graph-structured data but often underperform in link prediction tasks compared to classical methods, mainly due to the limitations of the commonly used Message Passing GNNS (MPNNs). Notably, their ability to distinguish non-isomorphic graphs is limited by the 1-dimensional Weisfeiler-Lehman test. Our study presents a novel method to enhance the expressivity of GNNs by embedding induced subgraphs into the graph Laplacian matrix's eigenbasis. We introduce a Learnable Lanczos algorithm with Linear Constraints (LL-WLC), proposing two novel subgraph extraction strategies: encoding vertex-deleted subgraphs and applying Neumann eigenvalue constraints. For the former, we conjecture that LL-wLC establishes a universal approximator, offering efficient time complexity. The latter focuses on link representations enabling differentiation between k-regular graphs and node automorphism, a vital aspect for link prediction tasks. Our approach results in an extremely lightweight architecture, reducing the need for extensive training datasets. Empirically, our method improves performance in challenging link prediction tasks across benchmark datasets, establishing its practical utility and supporting our theoretical findings. Notably, LLWLC achieves 20x and 10x speedup by only requiring 5% and 10% data from the PubMed and OGBL-Vessel datasets while comparing to the state-of-the-art.", "sections": [{"title": "Introduction", "content": "Graphs play a crucial role in various domains, representing linked data such as social networks [1], citation networks [42], knowledge graphs [35], metabolic network reconstruction [37], and user-item graphs in recommender systems [32]. Graph Neural Networks (GNNs) have emerged as state-of-the-art tools for processing graph-structured data. Message Passing Neural Networks (MPNNs) is the most prevalent technique within GNNs, which, relying on neighborhood aggregation, exhibit expressiveness no greater than the first-order Weisfeiler-Leman (1-WL) test [46, 34, 47, 33]. Therefore, MPNNs cannot distinguish specific graph structures, e.g., k-regular graphs. Moreover, link prediction (LP) tasks cannot always be answered reliably based on pairs of node embeddings obtained from MPNNs. Specifically, the node automorphism problem arises in instances where two nodes possess identical local structures, resulting in equivalent embeddings and, consequently, identical predictions. However, their relationships to a specific node, e.g., in terms of distance, may differ [55, 13].\nEfforts aimed at augmenting the expressiveness of MPNNs have pursued four main directions: aligning with the k-WL hierarchy [34, 31, 4], enriching node features with identifiers, exploiting structural information that cannot be captured by the WL test [9, 8], and Subgraph GNNs (SGNNs) [6, 20]. SGNNs are a recent class of expressive GNNs that model graphs through a collection of subgraphs. They have emerged as a potential solution, extending GNNs by collecting extracted subgraphs explicitly or implicitly and incorporating these into the GNN architecture. Subgraph extraction can be achieved, e.g., by removing or marking specific nodes or directly counting specific substructures [38]. However, in the worst case, previous SGNNs involve computationally intensive preprocessing steps or running a GNN many times.\nTo address these limitations, we introduce a novel approach grounded in graph signal processing [36] and spectral graph theory [15]. Our method introduces a novel eigenbasis, denoted as the Learnable Lanczos with Linear Constraints (LLwLC), which can explicitly encode linear constraints, particularly those derived from extracted induced subgraphs, into the basis. We propose a low-rank approximation [16] of the Laplacian matrix based on the Lanczos algorithm with linear constraints [18].\nThe LLwLC eigenbasis enhances feature expressiveness by incorporating linear constraints derived from the graph structure. We propose two novel subgraph extraction policies, focusing on vertex-deleted subgraphs and Neumann eigenvalue constraints. We provide a conjecture that vertex-deleted subgraphs have the universal approximator capabilities of LLWLC, while Neumann constraints enable the encoding of boundary conditions and link representations between nodes, allowing to differentiate k-regular graphs that are indistinguishable by WL. Theoretical analysis indicates that LLwLC can be applied in various problem settings. We evaluate its effectiveness in link prediction tasks, where expressivity and particularly the ability to distinguish automorphic nodes is pivotal.\nOur research presents several key contributions: (i) We develop a novel, efficient eigenbasis (LLwLC) for encoding linear constraints, including induced subgraphs, using the"}, {"title": "Preliminaries", "content": "Notations A graph G(V, E, X) consists of a vertex set V, edge set E, and node features \\(X \\in \\mathbb{R}^{n \\times d}\\), where n is the number of nodes. Each column \\(x \\in \\mathbb{R}^{d}\\) represents the features of node \\(v \\in V\\). A and D are the graph's adjacency and degree matrices, respectively. The graph Laplacian is \\(L = D \u2013 A\\), with U and \u039b as its eigenvector and eigenvalue matrices. The degree of node v is denoted by \\(d_v\\).\nSpectral Graph Convolutional Networks For a graph signal \\(x \\in \\mathbb{R}^n\\), [43] defines the graph Fourier transform, Ux, and its inverse, \\(U^\\ast x\\), based on the eigenbasis of the graph Laplacian matrix L. The graph convolution is \\(U(U^\\ast x \\ast U^\\ast y) = Ug(\\Lambda)U^Tx\\) where y is the graph filter, g is the function applied over the eigenvalue matrix \u039b to encode the graph filter, and * is the elementwise multiplication. The seminal spectral GCN method [12] is cubic in the number of nodes. To address this, different g functions were defined [23, 3]. LanczosNet [29] uses the Lanczos algorithm for fast multi-scale computation with learnable spectral filters. However, like previous GNNs, LanczosNet has limited expressivity, e.g., it cannot distinguish between the k-regular graphs. To address these limitations and enhance feature expressiveness in GNNs, we introduce a novel learnable spectral basis for encoding subgraphs as linear constraints."}, {"title": "Lanczos with Linear Constraint Networks", "content": "This section outlines the Lanczos Algorithm with Linear Constraints and the construction of a constraint matrix C for subgraph embedding, followed by the development of the complete LLwLC block and LLwLCNet pipeline. It also includes a proof of convergence properties and explores subgraph extraction policies, focusing on Neumann eigenvalue constraints and vertex-deleted subgraphs.\nLanczos Algorithm with Linear Constraints\nFor a given symmetric matrix \\(L \\in \\mathbb{R}^{n \\times n}\\) and a randomly initialized vector \\(v \\in \\mathbb{R}^n\\), the K-step Lanczos algorithm [27] computes an orthogonal matrix \\(Q \\in \\mathbb{R}^{n \\times m}\\) and a symmetric tridiagonal matrix \\(T \\in \\mathbb{R}^{m \\times m}\\), such that \\(Q^TLQ = T\\). We represent \\(Q_N = [q_1,...,q_N]\\) where the column vector \\(q_i\\) corresponds to the \\(i^{th}\\) Lanczos vector. The matrices \\(B \\in \\mathbb{R}^{m \\times m}\\) and \\(R \\in \\mathbb{R}^{m \\times m}\\) represent the eigenvectors and eigenvalues of T, respectively. By investigating the \\(j^{th}\\) column of the system \\(LQ = QT\\) and rearranging terms, we obtain \\(Lq_j = B_{j+1}q_{j+1} + B_jq_{j-1} + a_jq_j\\).\nHaving the linear constraint changes the plain Lanczos algorithm by replacing \\(u_j = Lq_j - B_jq_{j-1}\\) with \\(u_j = P(Lq_j - B_jq_{j-1})\\) assuming the initial vector v is projected into the null space of the constraints [18]. Please note that the"}, {"title": "LLWLCNet", "content": "In this part, we detail our approach to computing the eigenvectors of the graph Laplacian matrix, ensuring they adhere to input graph constraints. We construct our eigenbasis by addressing a large, sparse, symmetric eigenvalue problem with homogeneous linear constraints. It requires minimizing\n\\[\\min_{f \\in \\mathbb{R}^n} \\frac{f^TLf}{f^Tf}, \\text{ s.t. } C^Tf = 0, f \\neq 0\\]\nwhere \\(C \\in \\mathbb{R}^{n \\times l}\\) with \\(n \\gg l\\) is the constraint matrix.\nTo address the problem outlined in Equation 1, we utilize the Lanczos algorithm with the linear constraints, detailed in Algorithm 1. However, differing from the iterative approach for the least square equation suggested in the Lanczos algorithm with the linear constraints to solve\n\\[Cy = b,\\]\nwe utilize the PyTorch framework [40] for our computations. This choice is due to our sparse and not overly large constraint matrix allowing for the direct QR factorization [2] within PyTorch, offering numerical stability and the capability for backpropagation. In the following, we describe constructing the constraint matrix C, where we extract subgraphs first and derive the constraint matrix accordingly.\nConstraint Matrix C The construction of the constraint matrix, denoted as C, is based on subgraphs extracted from the input graph. We address the creation of C through two distinct approaches: induced subgraphs and link representation. For the induced subgraph case, we populate its corresponding column in C with the degrees of each node (and fill the remaining entries of the column with 0 for nodes not involved in the subgraph), ensuring that the derived eigenvectors fulfill the condition \\(\\sum f(x)dx = 0\\). This condition\nensures that the features learned using this eigenbasis reflect the imposed constraints. In the case of link representation, we design the constraint matrix to ensure that the aggregate of eigenvector differences across nodes equates to zero, as expressed by \\(\\sum(f(x) \u2013 f(y)) = 0\\). The column corresponding to the subgraph is constructed based on the equation. The degrees of the nodes involved in the constraint are entered into the column to satisfy the equation. Specifically, the degrees of nodes two-hop-away are negated, and the degrees of nodes one-hop-away are included only if they are connected to nodes two hops away. The remaining entries for nodes not involved are filled with 0 (see Figure 1 for an example). We build the other columns of the constraint matrix accordingly. A single matrix encompasses all constraints and each column representing a distinct constraint. Implementing these constraints and requiring the graph Laplacian matrix's eigenvectors to comply with them allows for feature extraction aligned with the eigenbasis constraints. This enables nuanced differentiation between structures, such as k-regular graphs, as shown in Figure 1.\nAddressing the eigenvalue problem with linear constraints yields a tridiagonal matrix, denoted as T, and an orthogonal matrix, Q. The decomposition of matrix T produces matrices R and B. Here, R represents the Ritz eigenvalues, and V = QB forms the eigenbasis that satisfies the constraints imposed by matrix C. Forcing the eigenvectors to satisfy the constraints leads to having different eigenbasis for graphs where the MPNN returns the same features. This"}, {"title": "Lanczos Algorithm with Linear Constraint Convergence", "content": "In this section, we substantiate LLwLC eigenbasis's convergence properties by conducting an error analysis on perturbations and referencing Greenbaum's findings to demonstrate the existence of an exact Lanczos algorithm for any perturbed version. By establishing the upper bound for the Lanczos algorithm's low-rank approximation, we affirm the convergence of our LLwLC eigenbasis.\nPerturbation and Error Study The accuracy of the linear least square problem using QR factorization depends on the precision of the QR factorization. As discussed by Zhang, Baharlouei, and Wu [56], two types of accuracy errors are crucial in QR factorization when solving linear least square problems: The backward error for a matrix Z is defined as\n\\[\\frac{||Z-QR||}{||Z||},\\]\nand the orthogonality error of Q is measured by\n\\[||I \u2013 Q^TQ||.\\]\nIdeally, both numerical errors should be zero, but due to roundoff errors and the potential loss of orthogonality in the Gram-Schmidt QR process, the QR factorization might not be sufficiently accurate for solving the linear least square problem.\nAfter examining the impact on accuracy, we analyze the theoretical gap between the exact Lanczos algorithm and its perturbed variant due to inexact QR factorization. The inexact QR factorization applied to solve 2 will impact the accuracy of both the Lanczos vectors and the tridiagonal matrices produced. Consequently, the computed tridiagonal matrix Tj is a perturbed version of the theoretical tridiagonal matrix, denoted as T, that would be generated by an exact Lanczos iteration. This relationship can be expressed as\n\\[T_j = T + E_j,\\]\nwhere \\(E_j\\) is the perturbation matrix after the \\(j^{th}\\) step. The following theorem details the error bounds of the perturbed tridiagonal matrix in comparison to the the-oretical exact solution of T after the \\(j^{th}\\) step of the Lanczos algorithm.\nTheorem 1. Let U and \\(\\bar{U}\\) be the eigenspaces corresponding to the smallest eigenvalues \u03bb and \\(\\bar{\\lambda}\\) of the symmetric matrices L and \\(\\bar{L} = L + E\\), respectively. Then for any u \u2208 U and \\(\\bar{u} \\in \\bar{U}\\) with \\(||u||_2 = 1\\) and \\(|\\bar{u}\\|_2 = 1\\), we have\n\\[\\lambda - \\bar{\\lambda} \\approx \\sum_{i=1}^j E_j(i,i)u(i)^2 + 2 \\sum_{i=1}^{j-1} E_j(i, i+1)u(i)u(i+1),\\]\nwhere \\(E_j(s,t)\\) is the (s,t) element of \\(E_j\\).\nAfter exploring the theoretical gap between exact and perturbed Lanczos algorithms, we investigate Greenbaum's result, which shows that each perturbed Lanczos corresponds to an exact version.\nGreenbaum's Results [19] The tridiagonal matrix \\(T_j\\) generated at the end of the \\(j^{th}\\) finite precision Lanczos process satisfying \\(LQ_j = Q_jT_j + B_{j+1}q_{j+1}e_j + F_j\\), where \\(e\\) is a vector with the \\(j^{th}\\) component one and all the other components zero, \\(F = (f_1, . . ., f_j)\\) is the perturbation term with \\(||f_j||_2 \\le \\epsilon||L||_2, \\epsilon \\ll 1\\), is the same as that generated by an exact Lanczos process but with a different matrix L. The matrices L and \\(\\bar{L}\\) are close in the sense that for any eigenvalue \\(\\lambda(L)\\) of L, there is an eigenvalue \\(\\lambda(\\bar{L})\\) of \\(\\bar{L}\\) such that \\(|\\lambda(L) \u2013 \\lambda(\\bar{L})| < ||F_j||_2\\). Therefore, in our case with the constant accuracy of the QR factorization, we can show \\(PLPQ = Q_jT_j + B_jq_{j+1}e_j + F_j\\), where \\(F_j = O(\\eta n)\\) with \u03b7 corresponds to the accuracy of the QR method.\nHaving established each perturbed Lanczos corresponds to an exact algorithm, we demonstrate the theorem below to bound the approximation error, as discussed in [29].\nTheorem 2. Let \\(U\\Lambda U^T\\) be the eigendecomposition of an \\(n \\times n\\) symmetric matrix L with \\(\\Lambda_{i,i} = \\lambda_i\\), \\(\\lambda_1 \\geq . . . \\geq \\lambda_n\\) and \\(U = [u_1,..., u_n]\\). Let \\(U_j = span{u_1,..., u_j}\\). Assume k-step Lanczos algorithm starts with vector v and outputs the orthogonal \\(Q \\in \\mathbb{R}^{n \\times k}\\) and tridiagonal matrix \\(T \\in \\mathbb{R}^{k \\times k}\\). For any j with 1 < j < n and \\(\\kappa > j\\), we have\n\\[||L \u2013 QTQT || \\leq \\sum_{i=1}^{j} \\left( \\cos(\\upsilon, u_i)^2 \\left( \\prod_{k=1}^{\\kappa-1} \\frac{\\lambda_k - \\lambda_i}{\\lambda_k + \\lambda_i} \\right) + \\sum_{i=j+1}^{N} \\lambda_i \\right)^2\\],\nwhere \\(T_{\\kappa-i}(x)\\) is the Chebyshev Polynomial of degree \\(\\kappa \u2013 i\\) and \\(\\gamma_i = (\\lambda_i \u2013 \\lambda_{i+1})/(\\lambda_{i+1} \u2013 \\lambda_{\\upsilon})\\).\nBased on Greenbaum's results [19], for our computed per-turbed Lanczos algorithm, exists an exact Lanczos algorithm but for a different matrix. Based on 2, we also cognize the upper bound of the low-rank approximator of the Lanczos algorithm. Thus, the perturbed Lanczos algorithm, caused by the inaccuracy of the QR method for solving the least square equation, converges to the upper bound of the low-rank approximation of the matrix of the exact Lanczos algorithm."}, {"title": "Subgraph Extraction Policy", "content": "A subgraph selection policy is a function \\(\\pi: G \\rightarrow P(G)\\) assigning to a graph a subset of its subgraphs [6]. Here, G is the set of all graphs with n nodes or less and P(G) its power set. Although any linear constraint in the input graph satisfying full rank assumption can be encoded in C, we propose the following subgraph extraction policies.\nNeumann Eigenvalue The Neumann eigenvalue [15] is\n\\[\\Lambda_S = \\inf_{f \\neq 0} \\frac{\\sum_{x \\in S} f(x) L f(x)}{\\int_S f^2(x) dx}, \\text{ subject to } \\sum_{x \\in S} (f(x) = f(y)) = 0 \\text{ and } \\sum_{x \\in S} f(x) dx = 0,\\]\nThe function \\(f: S \\cup \\partial S \\rightarrow \\mathbb{R}\\) represents the Neumann eigenvector satisfying the Neumann conditions. The vertex boundary, \\(\\partial S\\), of an induced subgraph consists of all vertices not in S but adjacent to at least one vertex in S. Specifically, the first constraint encodes the link representation (\\(\\sum_{\\upsilon \\in S, y \\sim \\upsilon}((f(x)) - (f(y)) = 0\\), ; building on previous link prediction research, we consider nodes that are two hops away from the query nodes, where S represents the one-hop-away nodes, and \\(\\partial S\\) denotes the boundary nodes between one-hop and two-hop-away nodes). This can be reformulated as:\n\\[\\min f^TLf, \\text{ subject to } || f || = 1 \\text{ and } C^T f = 0,\\]\nwhere \\(L \\in \\mathbb{R}^{n \\times n}\\) is a symmetric and large sparse matrix, and \\(C \\in \\mathbb{R}^{n \\times l}\\) (with \\(n \\gg l\\)) is also large, sparse, and of full column rank. The time complexity involved in extracting subgraphs depends on the product of the maximum degree of nodes and the count of nodes within nodes in the boundary. When we enforce that the eigenvectors satisfy the constraints related to induced subgraphs and link representation, we ensure that the corresponding features adhere to these constraints.\nProposition 1. Applying Neumann eigenvalue constraints to the eigenbasis results in features that exhibit greater expressivity than MPNNs. Besides addressing the node automorphism problem [44], these enhanced features enable the distinction of specific k-regular graphs from each other, thereby significantly enhancing expressivity in GNNs (Proof in Appendix).\nConstraints C The second subgraph extraction policy we propose is based on vertex-deleted subgraphs. The sufficient conditions under which LLWLC can solve graph isomorphism entails that LLwLC is a universal approximator of functions defined on graphs [14]. Given that we can encode any subgraph into our eigenbasis, we can examine whether a specific substructure collection can completely characterize each graph. By the reconstruction conjecture [45], we assume we can reconstruct the graph if we have all the n -1 vertex-deleted subgraphs. According to [26], exponentially"}, {"title": "Experiments", "content": "To demonstrate the effectiveness of our method in addressing node automorphism and leveraging subgraphs, we conducted experiments focused on the link prediction task. We compared the performance of LLwLCNet against traditional heuristics (CN [5], RA [57], AA [1]), vanilla GNNS (GCN [25], SAGE [21]), GNNs modifying the input graph of MPNNS (SEAL [53], NBFNet [58]), and GNNs with manual features as pairwise representations (Neo-GNN [51], BUDDY [13]). Baseline results are from Chamberlain et al. [13]. Our evaluation includes five link prediction benchmarks: Cora, Citeseer, Pubmed [49] (Planetoid datasets), OGBL-Collab, and OGBL-Vessel [24]. Dataset statistics are\nSetup In our experiments, we used a learning rate of 0.001 for 20 training epochs. The model has two 32-channel MLP layers, each with ReLU non-linearities and dropout. We cap the eigenpairs at 10, padding missing pairs with zeros. Both PyTorch [40] and PyTorch Geometric [17] were used for implementation. The binary cross entropy loss measured training loss. Following SEAL, we altered 10% of links for test data and used the remaining 90% for training.\nLink Prediction Results While the LLWLC framework theoretically applies to various problem domains, our experiments focus on the LP tasks to demonstrate its effectiveness in addressing node automorphism, considering substructures, and thus improving expressiveness. Consistent with prior studies [13], considering two-hop nodes is sufficient for effective LP task.\nAs depicted in Table 1, LLWLC stands out as a robust framework for link prediction, consistently delivering impressive performance on link prediction benchmarks with metrics given in the first row. With only 0.02M, LLwLC outperforms previous models on the Planetoid dataset, highlighting the significance of encoding subgraph structures and selective subsets of node relations for superior link prediction results. Our model also achieves state-of-the-art performance on the OGBL-Collab dataset with just 0.02M, compared to BUDDY (1.10M) and SEAL (0.50M). Increasing blocks to 0.03M yields a 67.50% HR@50 score. On OGBL-Vessel, we achieve competitive results with just 0.019M and 10% of the training data, demonstrating LLwLC's effectiveness in a lightweight architecture.\nTime Complexity The time complexity of our method is \\(O(\\kappa E) + O(k^2n) + O(k^3)\\) for the outer loop (Lanczos algorithm), the QR factorization, and the pseudo-inverse of kxk matrix, respectively, where\u043a \u00ab n is the number of computed eigenvectors and k < n is the number of linear constraints. Table 2a compares the time complexities of LL-wLCNet and other link prediction methods.\nComparing the LLwLCNet model's time complexity with k-WL expressive models provides valuable insights. It is conjectured that LLwLCNet is a universal approximator with a worst-case time complexity of \\(O(n^3)\\) (where k = n - 1). Aligned with [5] research and supported by our empirical experiments, it has been established that graph reconstruction can be effectively achieved with just a few constraints. This leads to time complexity linear to the number of nodes. Contrastingly, established k-WL expressive models, such as k-IGNs [30] and k-GNNs [34], are known to have a higher time complexity of O(nk).\nFurthermore, we compare LLwLCNet's time complexity with GSN [11]. It explores substructures by counting specific substructures. However, it relies on prior knowledge and inductive bias for task-specific substructure selection and deals with the subgraph isomorphism. In GSN, the preprocessing step, in general, is O(nk) for a generic substructure of size k. In contrast, our experimental results demonstrate that LLWLCNet can capture graph properties without"}, {"title": "Related Work", "content": "MPNN Expressivity The expressivity of GNNs is typically expressed in terms of their ability to distinguish non-isomorphic graphs. As no polynomial-time algorithm for solving the graph isomorphism problem is known, developing GNNs that are both expressive and efficient poses a major challenge. Xu et al. [48] found that the expressivity of MPNNs is limited to that of the 1-WL test. This limitation is crucial in real-world applications, as the 1-WL test cannot"}, {"title": "Conclusion", "content": "In this work, we introduced the Learnable Lanczos algorithm with Linear Constraints (LLwLC), a novel method designed to enhance the expressivity of Graph Neural Networks (GNNs). Through the incorporation of two novel subgraph extraction strategies, we managed to construct a lightweight architecture that minimizes reliance on extensive training datasets. Empirical results show that our method significantly improves performance in link prediction tasks across various benchmark datasets. Notably, the LLwLC achieved 20\u00d7 and 10\u00d7 speedup, requiring only 5% and 10% data from the PubMed and OGBL-Vessel datasets respectively, compared to the state-of-the-art methods. These findings underscore the practical utility and theoretical advancement of our method, illustrating the LL-wLC's potential as a universal approximator and its ability to differentiate between k-regular graphs. The advancements made with the LLwLC not only represent a significant contribution to the field but also set a promising direction for future exploration and development in the realm of GNNs. As a further future work, we believe investigating the impact of learning linear constraints between nodes and edges within the input graph and encoding them to the eigenbasis of the graph Laplacian matrix, can promise further advancements in this field."}]}