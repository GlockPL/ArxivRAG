{"title": "Enhanced Expressivity in Graph Neural Networks with Lanczos-Based Linear Constraints", "authors": ["Niloofar Azizi", "Nils Kriege", "Horst Bischof"], "abstract": "Graph Neural Networks (GNNs) excel in handling graph- structured data but often underperform in link predic- tion tasks compared to classical methods, mainly due to the limitations of the commonly used Message Passing GNNS (MPNNs). Notably, their ability to distinguish non- isomorphic graphs is limited by the 1-dimensional Weisfeiler- Lehman test. Our study presents a novel method to enhance the expressivity of GNNs by embedding induced subgraphs into the graph Laplacian matrix's eigenbasis. We introduce a Learnable Lanczos algorithm with Linear Constraints (LL- WLC), proposing two novel subgraph extraction strategies: encoding vertex-deleted subgraphs and applying Neumann eigenvalue constraints. For the former, we conjecture that LL- wLC establishes a universal approximator, offering efficient time complexity. The latter focuses on link representations enabling differentiation between k-regular graphs and node automorphism, a vital aspect for link prediction tasks. Our approach results in an extremely lightweight architecture, re- ducing the need for extensive training datasets. Empirically, our method improves performance in challenging link predic- tion tasks across benchmark datasets, establishing its practi- cal utility and supporting our theoretical findings. Notably, LLWLC achieves 20x and 10x speedup by only requiring 5% and 10% data from the PubMed and OGBL-Vessel datasets while comparing to the state-of-the-art.", "sections": [{"title": "Introduction", "content": "Graphs play a crucial role in various domains, repre- senting linked data such as social networks [1], cita- tion networks [42], knowledge graphs [35], metabolic net- work reconstruction [37], and user-item graphs in rec- ommender systems [32]. Graph Neural Networks (GNNs) have emerged as state-of-the-art tools for processing graph-structured data. Message Passing Neural Networks (MPNNs) is the most prevalent technique within GNNs, which, relying on neighborhood aggregation, exhibit expres- siveness no greater than the first-order Weisfeiler-Leman (1-WL) test [46, 34, 47, 33]. Therefore, MPNNs cannot distinguish specific graph structures, e.g., k-regular graphs. Moreover, link prediction (LP) tasks cannot always be an- swered reliably based on pairs of node embeddings ob- tained from MPNNs. Specifically, the node automorphism problem arises in instances where two nodes possess identi- cal local structures, resulting in equivalent embeddings and, consequently, identical predictions. However, their relation- ships to a specific node, e.g., in terms of distance, may dif- fer [55, 13].\nEfforts aimed at augmenting the expressiveness of MPNNs have pursued four main directions: aligning with the k-WL hierarchy [34, 31, 4], enriching node features with identifiers, exploiting structural information that can- not be captured by the WL test [9, 8], and Subgraph GNNs (SGNNs) [6, 20]. SGNNs are a recent class of expressive GNNs that model graphs through a collection of subgraphs. They have emerged as a potential solution, extending GNNs by collecting extracted subgraphs explicitly or implicitly and incorporating these into the GNN architecture. Subgraph ex- traction can be achieved, e.g., by removing or marking spe- cific nodes or directly counting specific substructures [38]. However, in the worst case, previous SGNNs involve com- putationally intensive preprocessing steps or running a GNN many times.\nTo address these limitations, we introduce a novel ap- proach grounded in graph signal processing [36] and spec- tral graph theory [15]. Our method introduces a novel eigenbasis, denoted as the Learnable Lanczos with Linear Constraints (LLwLC), which can explicitly encode linear constraints, particularly those derived from extracted in- duced subgraphs, into the basis. We propose a low-rank ap- proximation [16] of the Laplacian matrix based on the Lanc- zos algorithm with linear constraints [18].\nThe LLwLC eigenbasis enhances feature expressiveness by incorporating linear constraints derived from the graph structure. We propose two novel subgraph extraction poli- cies, focusing on vertex-deleted subgraphs and Neumann eigenvalue constraints. We provide a conjecture that vertex- deleted subgraphs have the universal approximator capabil- ities of LLWLC, while Neumann constraints enable the en- coding of boundary conditions and link representations be- tween nodes, allowing to differentiate k-regular graphs that are indistinguishable by WL. Theoretical analysis indicates that LLwLC can be applied in various problem settings. We evaluate its effectiveness in link prediction tasks, where ex- pressivity and particularly the ability to distinguish automor- phic nodes is pivotal.\nOur research presents several key contributions: (i) We develop a novel, efficient eigenbasis (LLwLC) for encoding linear constraints, including induced subgraphs, using the"}, {"title": "Preliminaries", "content": "Notations A graph G(V, E, X) consists of a vertex set V, edge set E, and node features $X \\in \\mathbb{R}^{n \\times d}$, where n is the number of nodes. Each column $x \\in \\mathbb{R}^d$ represents the fea- tures of node $v \\in V$. A and D are the graph's adjacency and degree matrices, respectively. The graph Laplacian is L = D \u2013 A, with U and \u039b as its eigenvector and eigen- value matrices. The degree of node v is denoted by dv.\nSpectral Graph Convolutional Networks For a graph signal $x \\in \\mathbb{R}^n$, [43] defines the graph Fourier transform, Ux, and its inverse, $U^Tx$, based on the eigenbasis of the graph Laplacian matrix L. The graph convolution is $U(U^Tx * U^Ty) = Ug(\u039b)U^Tx$ where y is the graph fil- ter, g is the function applied over the eigenvalue matrix \u039b to encode the graph filter, and * is the elementwise multiplica- tion. The seminal spectral GCN method [12] is cubic in the number of nodes. To address this, different g functions were defined [23, 3]. LanczosNet [29] uses the Lanczos algorithm for fast multi-scale computation with learnable spectral fil-"}, {"title": "Lanczos with Linear Constraint Networks", "content": "This section outlines the Lanczos Algorithm with Linear Constraints and the construction of a constraint matrix C for subgraph embedding, followed by the development of the complete LLwLC block and LLwLCNet pipeline. It also includes a proof of convergence properties and explores sub- graph extraction policies, focusing on Neumann eigenvalue constraints and vertex-deleted subgraphs.\nLanczos Algorithm with Linear Constraints\nFor a given symmetric matrix $L \\in \\mathbb{R}^{n \\times n}$ and a randomly initialized vector $v \\in \\mathbb{R}^n$, the K-step Lanczos algorithm [27] computes an orthogonal matrix $Q \\in \\mathbb{R}^{n \\times m}$ and a symmet- ric tridiagonal matrix $T \\in \\mathbb{R}^{m \\times m}$, such that $Q^T L Q = T$. We represent $Q_N = [q_1,...,q_N]$ where the column vec- tor qi corresponds to the $i^{th}$ Lanczos vector. The matrices $B \\in \\mathbb{R}^{m \\times m}$ and $R \\in \\mathbb{R}^{m \\times m}$ represent the eigenvectors and eigenvalues of T, respectively. By investigating the $j^{th}$ col- umn of the system LQ = QT and rearranging terms, we obtain $Lq_j = B_{j+1}q_{j+1} + B_jq_{j-1} + a_jq_j$.\nHaving the linear constraint changes the plain Lanczos algorithm by replacing $u_j = Lq_j - B_jq_{j-1}$ with $u_j = P(Lq_j - B_jq_{j-1})$ assuming the initial vector v is projected into the null space of the constraints [18]. Please note that the"}, {"title": "LLWLCNet", "content": "orthogonal projector P can be obtained through the QR de- composition of C when dealing with a dense constraint ma- trix C. In situations where C is sparse and dim(N(CT)) \u2248 n, the projector is given by P = I \u2013 CC\u2020, with C\u2020 being the Moore-Penrose inverse of C. Assuming C has full col- umn rank, C\u2020 can be computed as (CTC)-1CT [7]. If we project the initial vector v into null space of the constraint matrix $v_1 = Pv \\in N(C^T)$ and notice the mathematical equivalence between computing the smallest eigenvalue of the constraint $A_P = P \\overline{L}P$ and L then one step of the Lanczos algorithm with the linear constraints is $B_{j+1}q_{j+1} = P \\overline{L}Pq_j - B_jPq_{j-1} - A_jPq_j = P(Lq_j \u2013 B_jq_{j\u22121} \u2013 A_jq_j)$. Algorithm 1 describes the steps of the Lanczos Algorithm with the Linear Constraints in detail.\nThis algorithm is structured into two main components: the 'outer loop', which is a straightforward Lanczos algo- rithm iteration, and the 'inner loop', which focuses on re- solving the least squares problem expressed as $P(b) = min || Cy - b||_2$. Here, y is defined as C\u2020b, and b is $Lq_j$.\nLLWLCNet\nIn this part, we detail our approach to computing the eigen- vectors of the graph Laplacian matrix, ensuring they adhere to input graph constraints. We construct our eigenbasis by addressing a large, sparse, symmetric eigenvalue problem with homogeneous linear constraints. It requires minimizing\n$\\min_{f} \\frac{f^T L f}{f^T f}, \\text{subject to } C^T f = 0, f \\neq 0,$   (1)\nwhere $C \\in \\mathbb{R}^{n \\times l}$ with $n \\gg l$ is the constraint matrix.\nTo address the problem outlined in Equation 1, we utilize the Lanczos algorithm with the linear constraints, detailed in Algorithm 1. However, differing from the iterative approach for the least square equation suggested in the Lanczos algo- rithm with the linear constraints to solve\n$Cy = b,$\n(2)\nwe utilize the PyTorch framework [40] for our computations. This choice is due to our sparse and not overly large con- straint matrix allowing for the direct QR factorization [2] within PyTorch, offering numerical stability and the capa- bility for backpropagation. In the following, we describe constructing the constraint matrix C, where we extract sub- graphs first and derive the constraint matrix accordingly.\nConstraint Matrix C The construction of the constraint matrix, denoted as C, is based on subgraphs extracted from the input graph. We address the creation of C through two distinct approaches: induced subgraphs and link representa- tion. For the induced subgraph case, we populate its corre- sponding column in C with the degrees of each node (and fill the remaining entries of the column with 0 for nodes not involved in the subgraph), ensuring that the derived eigen- vectors fulfill the condition $\\sum f(x) dx = 0$. This condition ensures that the features learned using this eigenbasis re- flect the imposed constraints. In the case of link represen- tation, we design the constraint matrix to ensure that the ag- gregate of eigenvector differences across nodes equates to zero, as expressed by $\\sum(f(x) \u2013 f(y)) = 0$. The column corresponding to the subgraph is constructed based on the equation. The degrees of the nodes involved in the constraint are entered into the column to satisfy the equation. Specif- ically, the degrees of nodes two-hop-away are negated, and the degrees of nodes one-hop-away are included only if they are connected to nodes two hops away. The remaining en- tries for nodes not involved are filled with 0 (see Figure 1 for an example). We build the other columns of the con- straint matrix accordingly. A single matrix encompasses all constraints and each column representing a distinct con- straint $(\\int f(x) dx = 0, [2,2,2,2,0,0,0,0,0,0]^*)$. Implementing these constraints and requiring the graph Laplacian matrix's eigenvectors to comply with them allows for feature extrac- tion aligned with the eigenbasis constraints. This enables nu- anced differentiation between structures, such as k-regular graphs, as shown in Figure 1.\nAddressing the eigenvalue problem with linear constraints yields a tridiagonal matrix, denoted as T, and an orthogonal matrix, Q. The decomposition of matrix T produces ma- trices R and B. Here, R represents the Ritz eigenvalues, and V = QB forms the eigenbasis that satisfies the con- straints imposed by matrix C. Forcing the eigenvectors to satisfy the constraints leads to having different eigenbasis for graphs where the MPNN returns the same features. This"}, {"title": "Lanczos Algorithm with Linear Constraint Convergence", "content": "is exemplified in the case described in Figure 1, where two 2-regular graphs yield two different eigenbases.\nFull Block After determining the eigenbasis, we are ready to establish the full block of the Lanczos Layer with Lin- ear Constraint (LLwLC). In this new eigenbasis, we develop spectral filters by applying a multilayer perceptron (f) to the eigenvalue matrix R. With these learned filters, we recon- struct our basis and transform the graph signals $X \\in \\mathbb{R}^{m \\times n}$ into this basis to extract features that meet our specific con- straints. L is the graph Laplacian matrix computed from the low-rank approximation of the constrained eigenvalue prob- lem. Each LLwLCNet block is\n$\\sigma(Vf(R)V^TXW) = \\varsigma(LXW).$\n(3)\nHere, $W \\in \\mathbb{R}^{n \\times m}$ represents the learnable weight matrix, and o denotes the non-linearity applied in each block (ReLU in our experiments).\nFull Architecture As represented in Algorithm 2, we in- crease the number of blocks to deepen our architecture and capture more complex features. Each block reuses the ini- tially computed eigenbasis and applies a multi-layer percep- tron (MLP) to the eigenvalue matrix R to reconstruct its cor- responding L. Our complete pipeline concludes by a global sort pooling [54] and a fully connected block in the last layer as depicted in Figure 1, used to predict link existence.\nLanczos Algorithm with Linear Constraint Convergence\nIn this section, we substantiate LLwLC eigenbasis's con- vergence properties by conducting an error analysis on per- turbations and referencing Greenbaum's findings to demon- strate the existence of an exact Lanczos algorithm for any perturbed version. By establishing the upper bound for the Lanczos algorithm's low-rank approximation, we affirm the convergence of our LLwLC eigenbasis.\nPerturbation and Error Study The accuracy of the linear least square problem using QR factorization depends on the precision of the QR factorization. As discussed by Zhang, Baharlouei, and Wu [56], two types of accuracy errors are crucial in QR factorization when solving linear least square problems: The backward error for a matrix Z is defined as $\\frac{||Z - QR||_2}{||Z||_2}$ and the orthogonality error of Q is measured by $||\nI \u2013 Q^TQ||$. Ideally, both numerical errors should be zero, but due to roundoff errors and the potential loss of orthog- onality in the Gram-Schmidt QR process, the QR factoriza- tion might not be sufficiently accurate for solving the linear least square problem.\nAfter examining the impact on accuracy, we analyze the theoretical gap between the exact Lanczos algorithm and its perturbed variant due to inexact QR factorization. The inex- act QR factorization applied to solve 2 will impact the accu- racy of both the Lanczos vectors and the tridiagonal matri- ces produced. Consequently, the computed tridiagonal ma- trix Tj is a perturbed version of the theoretical tridiagonal matrix, denoted as T, that would be generated by an ex- act Lanczos iteration. This relationship can be expressed as\n$T_j = T + E_j$, where $E_j$ is the perturbation matrix after the $j^{th}$ step. The following theorem details the error bounds of the perturbed tridiagonal matrix in comparison to the the- oretical exact solution of T after the jth step of the Lanczos algorithm.\nTheorem 1. Let U and \u016a be the eigenspaces correspond- ing to the smallest eigenvalues \u5165 and \u03bb of the symmetric matrices L and L = L + E, respectively. Then for any $u \\in U$ and $\u0169 \\in \u016a$ with $||u||_2 = 1$ and $||\u0169||_2 = 1$, we have\n$\\lambda - \\overline{\\lambda} \\approx \\sum_{i=1}^j E_j (i, i) u(i)^2 + 2 \\sum_{i=1}^{j-1} E_j (i, i + 1) u(i) u(i + 1),$\nwhere $E_j (s, t)$ is the (s,t) element of $E_j$.\nAfter exploring the theoretical gap between exact and per- turbed Lanczos algorithms, we investigate Greenbaum's re- sult, which shows that each perturbed Lanczos corresponds to an exact version.\nGreenbaum's Results [19] The tridiagonal matrix $T_j$ generated at the end of the $j^{th}$ finite precision Lanczos pro- cess satisfying $LQ_j = Q_jT_j + B_{j+1}q_{j+1}e_j^T + F_j$, where $e$ is a vector with the $j^{th}$ component one and all the other components zero, $F = (f_1, . . ., f_j)$ is the perturbation term with $||f_j||_2 \u2264 \u20ac||L||_2$, \u20ac \u226a 1, is the same as that generated by an exact Lanczos process but with a different matrix L. The matrices L and $\\overline{L}$ are close in the sense that for any eigenvalue $\u03bb(L)$ of L, there is an eigenvalue $\u03bb(\\overline{L})$ of $\\overline{L}$ such that $ |\u03bb(L) \u2013 \u03bb(\\overline{L})| < ||F_j||_2$. Therefore, in our case with the constant accuracy of the QR factorization, we can show $PLPQ = Q_jT_j + B_jq_{j+1}e_j^T + F_j$, where $F\u2081 = O(\u03b7n)$ with \u03b7 corresponds to the accuracy of the QR method.\nHaving established each perturbed Lanczos corresponds to an exact algorithm, we demonstrate the theorem below to bound the approximation error, as discussed in [29].\nTheorem 2. Let U\u039bUT be the eigendecomposition of an n \u00d7 n symmetric matrix L with Ai,i = \u03bbi, \u03bb1 \u2265 . . . \u2265 An and U = [U1,..., Un]. Let Uj = span{u1,..., uj}. Assume k-step Lanczos algorithm starts with vector v and outputs the orthogonal $Q \\in \\mathbb{R}^{n \\times k}$ and tridiagonal matrix $T \\in \\mathbb{R}^{K \\times K}$. For any j with 1 < j < n and k > j, we have\n$||L - QTQT || < \\sqrt{\\sum_{i=1}^{j}((\\frac{sin(\\nu,U_i)}{\\Pi_{k=1}^{\\kappa-1}\\lambda_{\\kappa} - \\lambda_i}))^2 + \\sum_{i=j+1}^{N} \\lambda_i^2}$"}, {"title": "Subgraph Extraction Policy", "content": "Subgraph Extraction Policy\nA subgraph selection policy is a function \u03c0: G \u2192 P(G) as- signing to a graph a subset of its subgraphs [6]. Here, G is the set of all graphs with n nodes or less and P(G) its power set. Although any linear constraint in the input graph satis- fying full rank assumption can be encoded in C, we propose the following subgraph extraction policies.\nNeumann Eigenvalue The Neumann eigenvalue [15] is\n$\\Lambda_S = \\inf_{f \\neq 0} \\frac{\\sum_{x \\in S} f(x) L f(x)}{\\sum_{x \\in S} f^2(x) dx}$, subject to $\\sum_{x \\in S, y \\sim x} (f(x) = f(y)) = 0$ and $\\sum_{x \\in S} f(x) dx = 0.$\nThe function f: SUSS \u2192 IR represents the Neumann eigenvector satisfying the Neumann conditions. The ver- tex boundary, SS, of an induced subgraph consists of all vertices not in S but adjacent to at least one vertex in S. Specifically, the first constraint encodes the link repre- sentation ($\\sum_{\\text{\u0e02} \\in S,y\\sim((f(x))} \u2013 (f(y)) = 0$), building on previous link prediction research, we con- sider nodes that are two hops away from the query nodes, where S represents the one-hop-away nodes, and 8S de- notes the boundary nodes between one-hop and two-hop- away nodes). This can be reformulated as:\n$\\min_f f^TLf, \\text{subject to } || f || = 1 \\text{ and } C^T f = 0,$\n(4)\nwhere $L \\in \\mathbb{R}^{n \\times n}$ is a symmetric and large sparse matrix, and $C \\in \\mathbb{R}^{n \\times l}$ (with n \u226b l) is also large, sparse, and of full column rank. The time complexity involved in extract- ing subgraphs depends on the product of the maximum de- gree of nodes and the count of nodes within nodes in the boundary. When we enforce that the eigenvectors satisfy the constraints related to induced subgraphs and link represen- tation, we ensure that the corresponding features adhere to these constraints.\nProposition 1. Applying Neumann eigenvalue constraints to the eigenbasis results in features that exhibit greater ex- pressivity than MPNNs. Besides addressing the node au- tomorphism problem [44], these enhanced features enable the distinction of specific k-regular graphs from each other, thereby significantly enhancing expressivity in GNNs (Proof in Appendix).\nConstraints C The second subgraph extraction policy we propose is based on vertex-deleted subgraphs. The sufficient conditions under which LLwLC can solve graph isomor- phism entails that LLwLC is a universal approximator of functions defined on graphs [14]. Given that we can encode any subgraph into our eigenbasis, we can examine whether a specific substructure collection can completely character- ize each graph. By the reconstruction conjecture [45], we assume we can reconstruct the graph if we have all the n -1 vertex-deleted subgraphs. According to [26], exponentially"}, {"title": "Experiments", "content": "To demonstrate the effectiveness of our method in address- ing node automorphism and leveraging subgraphs, we con- ducted experiments focused on the link prediction task. We compared the performance of LLwLCNet against tra- ditional heuristics (CN [5], RA [57], AA [1]), vanilla GNNS (GCN [25], SAGE [21]), GNNs modifying the input graph of MPNNS (SEAL [53], NBFNet [58]), and GNNs with manual features as pairwise representations (Neo-GNN [51], BUDDY [13]). Baseline results are from Chamberlain et al. [13]. Our evaluation includes five link prediction bench- marks: Cora, Citeseer, Pubmed [49] (Planetoid datasets), OGBL-Collab, and OGBL-Vessel [24]. Dataset statistics are"}, {"title": "Conclusion", "content": "In this work, we introduced the Learnable Lanczos algo- rithm with Linear Constraints (LLwLC), a novel method designed to enhance the expressivity of Graph Neural Net- works (GNNs). Through the incorporation of two novel subgraph extraction strategies, we managed to construct a lightweight architecture that minimizes reliance on ex- tensive training datasets. Empirical results show that our method significantly improves performance in link predic- tion tasks across various benchmark datasets. Notably, the LLWLC achieved 20\u00d7 and 10\u00d7 speedup, requiring only 5% and 10% data from the PubMed and OGBL-Vessel datasets respectively, compared to the state-of-the-art meth- ods. These findings underscore the practical utility and the- oretical advancement of our method, illustrating the LL- wLC's potential as a universal approximator and its ability to differentiate between k-regular graphs. The advancements made with the LLWLC not only represent a significant con- tribution to the field but also set a promising direction for fu- ture exploration and development in the realm of GNNs. As a further future work, we believe investigating the impact of learning linear constraints between nodes and edges within the input graph and encoding them to the eigenbasis of the graph Laplacian matrix, can promise further advancements in this field."}]}