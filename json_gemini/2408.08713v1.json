{"title": "Beyond KAN: Introducing KarSein for Adaptive High-Order Feature Interaction\nModeling in CTR Prediction", "authors": ["Yunxiao Shi", "Wujiang Xu", "Mingyu Jin", "Haimin Zhang", "Qiang Wu", "Yongfeng Zhang", "Min Xu"], "abstract": "Modeling feature interactions is crucial for click-through rate\n(CTR) prediction, particularly when it comes to high-order\nexplicit interactions. Traditional methods struggle with this\ntask because they often pre-define a maximum interaction or-\nder, which relies heavily on prior knowledge and can limit\nthe model's effectiveness. Additionally, modeling high-order\ninteractions typically leads to increased computational costs.\nTherefore, the challenge lies in adaptively modeling high-\norder feature interactions while maintaining efficiency. To\naddress this issue, we introduce Kolmogorov-Arnold Repre-\nsented Sparse Efficient Interaction Network (KarSein), de-\nsigned to optimize both predictive accuracy and computa-\ntional efficiency. We firstly identify limitations of directly\napplying Kolmogorov-Arnold Networks (KAN) to CTR, and\nthen introduce KarSein to overcome these issues. It features\na novel architecture that reduces the computational costs of\nKAN and supports embedding vectors as feature inputs. Ad-\nditionally, KarSein employs guided symbolic regression to\naddress the challenge of KAN in spontaneously learning mul-\ntiplicative relationships. Extensive experiments demonstrate\nKarSein's superior performance, achieving significant predic-\ntive accuracy with minimal computational overhead. Further-\nmore, KarSein maintains strong global explainality while en-\nabling the removal of redundant features, resulting in a sparse\nnetwork structure. These advantages also position KarSein as\na promising method for efficient inference \u00b9.", "sections": [{"title": "Introduction", "content": "In the ever-evolving realm of digital advertising and recom-\nmendation systems, Click-Through Rate (CTR) prediction\nhas emerged as a pivotal element for optimizing user en-\ngagement and enhancing revenue streams. Despite its seem-\ningly straightforward goal, CTR prediction methods aim at\ncapturing underlying feature relationships among the com-\nplicated context data.\nTypically, these methods either capture implicit feature\ninteraction by directly modeling the fusion of all features\nusing deep neural networks (Covington, Adams, and Sar-\ngin 2016; Cheng et al. 2016), or learn explicit feature in-\nteraction by manually defining the interaction form or or-\nder through factorization-based models (Covington, Adams,\nand Sargin 2016; Cheng et al. 2016). Due to the exponen-\ntial growth of combinational complexity, explicit learning\nmethods typically limit themselves to modeling low-order\ninteractions and only integer-order interactions, which hin-\nders their ability to accurately represent scenarios requiring\nhigh-order and nuanced feature interactions. Most recently,\nAFN (Cheng, Shen, and Huang 2020) and EulerNet (Tian\net al. 2023) design novel feature transformations to adap-\ntively learn high-order interactions. However, they require\na preceding embedding space and face challenges such as\ninformation loss, numerical stability issues, and computa-\ntional overhead during transformations. Therefore, here is\nthe first challenge: how to develop a method that can effec-\ntively model high-order feature interactions in different real-\nworld scenarios? Moreover, existing CTR researches (Xiao\net al. 2017; Weiping et al. 2018; Li et al. 2019; Huang,\nZhang, and Zhang 2019) combine the high-order features re-\nlying on the specific context, leading to unsufficient and lo-\ncalized explainability. The absence of convincing rationales\nbehind model predictions calls their reliability and security\ninto question. In various applications, such as medication\nrecommendation and financial services, untrustworthy and\nunreliable advertisements can lead to severe consequences,\nincluding economic loss or health issues. So the second chal-\nlenge is: how to construct a model with global explanations\nto improve the trustworthy and decrease the feature redun-\ndancy for CTR prediction?\nRecently, a novel neural network architecture known as\nKolmogorov-Arnold Networks (KAN) (Liu et al. 2024) has\nbeen proposed, garnering significant attention within the\ndeep learning community for its exceptional data fitting ca-\npabilities, intuitive interpretability, and high structural spar-\nsity. It leads us to contemplate the significant potential of\napplying KAN in feature interaction modeling for CTR pre-\ndiction. However, applying vanilla KAN for CTR prediction\nwill result in inferior performance suffering from these prob-\nlems. (1) The sensitivity to regularization settings and net-\nwork structure initialization makes it challenging to general-\nize in CTR prediction, preventing even simple multiplicative\nrelationships between basic features. (2) KAN not supports\n2D embedding vectors as input features, limits it's model-\ning feature interactions at vector-wise manner. (3) The high\ncomputational complexity of KAN is impractical for CTR.\nTo this end, we propose Kolmogorov-Arnold Repre-\nsented Sparse Efficient Interaction Network, named as Kar-\nSein, for CTR prediction. Unlike KAN, which employs mul-\ntiple activation functions per input feature, KarSein is de-\nsigned to allocate just one activation function per feature,\nthereby significantly streamlining computations. Addition-\nally, we extend KAN to support two-dimensional input fea-\ntures, allowing embedding vectors to serve as input fea-\ntures for modeling feature interactions at vector-wise level.\nFurthermore, in each KarSein layer, we incorporate an op-\ntional step of pairwise multiplication between layer's in-\nput features and basic features for enforcing the network to\nlearn multiplicative relationships, this will further facilitat-\ning latter layers capturing more multiplicative interactions\nand contribute to prediction greatly. KarSein also retains\nthe simplification technology of KAN, the strong global ex-\nplainability can support feature \u201cde-redundancy\u201d for all in-\nstances.\nIn summary, our paper offers the following contributions:\n\u2022 We are the first to explore the application of KAN in CTR\nprediction, identifying critical limitations and providing\ninsightful findings.\n\u2022 We introduce the Kolmogorov-Arnold Represented\nSparse Efficient Interaction Network (KarSein), an in-\nnovative CTR prediction model that represents a novel\napproach in the field.\n\u2022 We demonstrate our proposed KarSein achieves state-\nof-the-art performance and lowest computational cost\nacross three datasets.\n\u2022 We point out KarSein's potential for structural spar-\nsity learning, which could further significantly accelerate\nmodel inference efficiency."}, {"title": "Preliminaries", "content": "Let U and I denote the sets of users and items, respe\u0441-\ntively. For a user-item pair $(u,i) \\in U \\times I$, we define\n$X_{u,i} = [x_1,...,x_m]$ as the feature vector capturing rel-\nevant attributes, including categorical (e.g., user and item\nIDs) and numerical (e.g., age) features. The Click-Through\nRate (CTR) prediction task aims to estimate $P(y = 1 ~|$\n$x_{u,i})$, where $y \\in \\{0,1\\}$ indicates whether the user clicked\non the item. Formally, we define $\\hat{y} = f(x_{u,i}; \\Theta)$ where\n$\\hat{y}$ is the predicted click probability, $f$ is the predictive\nmodel, and $\\Theta$ represents the model parameters. Given a\ntraining dataset $D = \\{(u_j, i_j, x^{(j,j)}, y_j)\\}_{j=1}^N$ of N in-\nstances, we optimize $\\Theta$ to minimize the prediction error.\nA common approach is to minimize the log loss: $L(\\Theta) =$\n$-\\frac{1}{N} \\sum_{j=1}^N [y_j log \\hat{y_j} + (1 - y_j) log(1 - \\hat{y_j})]$"}, {"title": "Feature Interactions Modeling", "content": "In recommendation systems, the model typically includes a\ntrainable parameterized embedding layer, denoted as $E(.)$.\nGiven a categorical ID feature $x_j$, the model first maps\n$x_j$ into dense vectors in a low-dimensional latent space:\n$e_j = E(x_j) \\in \\mathbb{R}^D$, where $D$ is the dimension of the em-\nbedding. Feature interaction modeling is then conducted on\nthese vectorized embeddings. Feature interaction types can\nbe broadly categorized into two primary paradigms: implicit\nand explicit. They often complement with each other, and\nmore explanatory can be seen in Appendix D.\nExplicit Feature Interactions These interactions typi-\ncally occur via multiplication at the vector level among\nrepresenting vectors of basic field features. For a set of\nfeatures $\\{x_1,x_2,...,x_m\\}$ with corresponding embeddings\n$\\{e_1, e_2,...,e_m\\}$, and consider hadamard product (element-\nwise multiplication) as the multiplication method, then for\nthe k-th order feature interactions, we can enumerate all\ncombinations as $\\{e_{n_1} \\odot e_{n_2}... \\odot e_{n_k} ~|~ n_1, n_2, ..., n_k \\in$\n$\\{1,2,...,m\\}\\}$. By stacking all the elements, we form a\nmatrix $X_k \\in \\mathbb{R}^{m^k \\times D}$. Concatenating all interactions from\nthe first-order to the k-th order results in a matrix $X_{1~k} \\in$\n$\\mathbb{R}^{\\sum_{i=1}^k m^i \\times D}$, where each row represents a high order ex-\nplicit feature interaction.\nWhen constructing high-order features with large k, the\nnumber of rows in $X_{1\\sim k}$ is given by $s = \\sum_{i=1}^k m^i$, which\nincreases exponentially with k, specifically, $s ~ O(m^k)$.\nTraditional methods typically construct $X_{1\\sim k}$ with pre-\ndefined k (e.g., k = 2). Advance methods focus on adap-\ntively learning arbitrary high-order feature interactions with-\nout pre-defining k or exhaustively enumerating all features.\nImplicit Feature Interactions Implicit feature interac-\ntions refer to the interactions between features that are not\nexplicitly predefined. Instead, these interactions are captured\nautomatically by models. Let $e = e_1||e_2||...||e_m$ denote\nthe result of wide concatenated embedding vectors. Then let\n$W$ be the linear transformation matrix applied to $e$, and $\\sigma$\nbe the activation function for non-linearities. For an L-layer\nDNN, this implicit feature interaction modeling can be ex-\npressed as:\n$MLP(e) = (W_{L-1} \\circ \\sigma \\circ W_{L-2} \\circ \\sigma \\circ ... \\circ \\sigma \\circ W_1 \\circ \\sigma \\circ W_0)e$\nImplicit feature interactions often involve learning the in-\nteraction patterns at the bit level. This process is supported\nby the Universal Approximation Theorem, which states that"}, {"title": "Kolmogorov-Arnold Network", "content": "For a smooth function $f : [0,1]^m \\rightarrow \\mathbb{R}$, the Kolmogorov-\nArnold Theorem states:\n$f(x) = f(x_1,...,x_m) = \\sum_{q=1}^{2m+1} \\Psi_q(\\sum_{p=1}^m \\varphi_{q,p}(x_p))$\nwhere $\\varphi_{q,p}: [0,1] \\rightarrow \\mathbb{R}$ and $\\Psi_q : \\mathbb{R} \\rightarrow \\mathbb{R}$. This theorem\nimplies that a multivariate function $f(x)$ can be expressed\nusing univariate functions and summation option. This theo-\nrem simplifies the task of learning high-dimensional func-\ntions by reducing it to learning a polynomial number of\none-dimensional functions. However, these one-dimensional\nfunctions can be non-smooth or even fractal, making them\nimpractical for direct application in machine learning mod-\nels. However, Kolmogorov-Arnold Networks (KAN) (Liu\net al. 2024) presents an optimistic perspective, it employs\nhighly flexible, learnable B-Spline curves as $\\phi(\\cdot)$ for acti-\nvation, and extends the theorem to neural networks with ar-\nbitrary widths and depths. Formally, for an activation func-\ntion $\\phi(\\cdot)$, let $Silu(\\cdot)$ denote the Silu activation, and $B(\\cdot)$ rep-\nresent the B-Spline curves activation withe grid size $g$ and\norder $K$. The activation function $\\phi(x)$ is defined as $\\phi(x) =$\n$w_o (B(x) + Silu(x))$, where $B(x) = \\sum_i^{g+K} c_i N_{i,K}(x)$. Here,\nboth $w_o$ and $c_i$ are learnable parameters. Let $\\Phi^L$ is the acti-\nvation function matrix corresponding to the L-th KAN layer.\nA general KAN network is a composition of L layers, given\nan wide concatenated embedding vector e as input, the out-\nput of KAN is:\n$KAN(e) = (\\Phi^{L-1} \\circ \\phi^{L-2} \\circ ... \\circ \\phi^1 \\circ \\phi^0)e$."}, {"title": "KAN for CTR Prediction", "content": "We find that the Kolmogorov-Arnold Representation The-\norem is particularly well-suited for representing fea-\nture interactions combined with multiplicative relationship.\nGiven two basic field feature $x_1$ and $x_2$, the derived\nsecond-order feature interaction $x_1x_2$ can be expressed as\n$\\Psi(\\Phi_1(\\varphi_{1,1}(x_1) + \\varphi_{1,2}(x_2)) + \\Phi_2(\\varphi_{2,1}(x_1) + \\varphi_{2,2}(x_2)))$,\nwhere $\\Phi_1(x) = x^2$, $\\Phi_2(x) = -x$, $\\varphi_{1,1}(x) = x$, $\\varphi_{1,2}(x) =$\n$x$, $\\varphi_{2,1}(x) = x^2$, $\\varphi_{2,2}(x) = x^2$. Thus, KAN can model\nfeature interactions through symbolic regression, offering\ngreater explainability and efficiency compared to DNN.\nWe also find that KAN's symbolic regression ability is\nsensitive to the structure initialization and the regularization\nsetting, which can see details in Appendix B.1. In more gen-\neral scenarios, KAN is hard to spontaneously perform right\nsymbolic regression for learning higher-order multiplicative\nfeatures interactions. This observation underscores a regret-\ntable reality: KAN, akin to DNN, remains intrinsically lim-\nited in its ability to autonomously learn multiplicative fea-\ntures interactions. Consequently, relying solely on KAN's\nspontaneously learning may not yield optimal CTR predic-\ntion results."}, {"title": "Vanilla Application in CTR", "content": "We designed a KAN network with layers of width $mD$\n\u2013 64 \u2013 64 \u2013 1 trained on the MovieLens-1M dataset, where\nwide concatenated embedding vectors e serve as the network\ninput. This network outputs a numeric value, which is then\nactivated by sigmoid function for CTR prediction. When\nmodel is convergent, achieving an AUC score of 0.8273,\nwhereas a DNN model with the same layer and neuron con-\nfiguration achieves an AUC of 0.8403. This shows that he\nvanilla application of KAN for CTR prediction yields sub-\noptimal results. But we find that, in the optimized KAN,\nmany network connections do little contribution, using the\nnetwork Simplification techniques, the KAN network is\npruned from the initial $mD$ \u2013 64 \u2013 64 \u2013 1 neurons down\nto $mD$ \u2013 1 \u2013 1 \u2013 1 neurons, indicating that a single activa-\ntion function per feature may be sufficient, eliminating the\ncost for multiple activation functions on the same feature,\nwhich is the parameter-redundancy in KAN. The detailed\nexploration settings are in Appendix B.2."}, {"title": "KARSEIN", "content": "The preliminary exploration of KAN for CTR prediction\nhighlights the limitations of this vanilla methods, but the\nfindings also motivate us to make modifications to adapt\nKAN for the CTR task. In this section, we introduce Kar-\nSein. A schematic of its architecture is shown in Figure 2."}, {"title": "KarSein Interaction Layer", "content": "The KarSein model is constituted by multiple stacked Kar-\nSein interaction layers. The layer takes a set of embedding\nvectors as input and generates higher-order feature interac-\ntions. The feature interaction consists of three steps on em-\nbedding vectors: optional pairwise multiplication, learnable\nactivation transformation, and linear combination.\nWe begin by describing the KarSein interaction layer at\nthe L-th level, where the input dimension is $H_{L-1}$ and the\noutput dimension is $H_L$. Here, $L \\in [1,T]$, and T denotes\nthe depth of the stacked KarSein interaction layers. For the\nL-th layer, let $X_{L-1} \\in \\mathbb{R}^{H_{L-1} \\times D}$ represent the input ma-\ntrix, and $X_L \\in \\mathbb{R}^{H_L \\times D}$ represent the output matrix. Specif-\nically, we define $H_1 = m$, and $X^0 \\in \\mathbb{R}^{m \\times D}$ as the matrix\nformed by stacking $e_1, e_2,...,e_m$. Additionally, we define\n$H_T = 1$ because the final layer is designed to have a single\noutput neuron, ultimately modeling a highly intricate high-\norder feature interaction, denoted as $X^T \\in \\mathbb{R}^{1\\times D}$.\nPairwise Multiplication In our methodology, we conduct\nfeature interactions between $X_{L-1}$ and $X^0$. This process in-\nvolves computing pairwise hadamard products of features.\nWe then concatenate the results with $X_{L-1}$ yielding new\nmatrix for substituting $X_{L-1}$, which serves as the input\nfor subsequent steps. Generally, incorporating this optional\nPairwise Multiplication step only within the first two Kar-\nSein interaction layer is adequate to guide the model's learn-\ning can incorporate multiplicative relationships. As shown\nin Figure 2, the Pairwise Multiplication step generates\nsecond-order features $f^2(e_1,e_2)$. Through high-order acti-\nvations and linear transformations, these features evolve into\nmore complex multivariate high-order interactions, such as\n$f^6(e_1, e_2, e_3)$, a sixth-degree polynomial feature involving\nthree variables. While $f^6(e_1, e_2, e_3)$ does not directly cap-\nture all three-variable interactions, stacking additional Kar-\nSein interaction layers effectively addresses this limitation.\nFor instance, in the second KarSein layer, $f^6(e_1, e_2, e_3)$ un-\ndergoes further activation, resulting in more intricate inter-\nactions like $f^{18}(e_1, e_2, e_3)$, which encompasses feature in-\nteractions among all three variables. This way, the model\nsuccessfully learns richer multiplicative relationships.\nActivation Transformation We denote the basis func-\ntions of B-Spline curves be of grid size g and order \u043a as\n$\\mathcal{N}^K = [N_{1,\\kappa}, N_{2,\\kappa}, ..., N_{g+\\kappa,\\kappa}]$. For L-th layer's input\nmatrix $X_{L-1} \\in \\mathbb{R}^{H_{L-1} \\times D}$. We first activate each row of\n$X_{L-1}$ on $g + \\kappa$ basis functions, the process is denoted as\n$X_{basis} = \\mathcal{N}(X_{L-1}) \\in \\mathbb{R}^{H_{L-1} \\times D \\times (g+\\kappa)}$. Then we define\nlearnable weight matrices $C^{L-1} \\in \\mathbb{R}^{H_{L-1} \\times 1 \\times (g+\\kappa)}$. Then\nwe process the activation transformation for $X_{L-1}$ using the\nfollowing formulation:\n$X_{L-1}^b =\n\\begin{bmatrix}\nX_{basis}[1, :, :]C^{L-1}[1, :, :]^T\\\\\n:\\\\\nX_{basis}[H_{L-1}, :, :]C^{L-1}[H_{L-1}, :, :]^T\n\\end{bmatrix}$\nLinearly Combination We define the weight matrix\n$W^{L-1} \\in \\mathbb{R}^{H_L \\times H_{L-1}}$. To model the feature interactions,\nwe perform a linear combination of the activated embedding\nvectors, represented as $W^{L-1}X_{L-1}^b$. To enhance the expres-\nsiveness of the model, we introduce an additional residual\nconnection. Specifically, we apply the $SiLU(\\cdot)$ activation\nfunction to $X_{L-1}^b$ and define another weight matrix $W_s^{L-1} \\in$\n$\\mathbb{R}^{H_L \\times H_{L-1}}$ to perform a linear transformation on the acti-\nvated embeddings, expressed as $W_s^{L-1}SiLU(X_{L-1}^b)$. The\nfinal output features are then given by the following formu-\nlation:\n$X_L = W^{L-1}X_{L-1}^b + W_s^{L-1}SiLU(X_{L-1}^b)$\nIntegrating Implicit Interactions\nWe integrate implicit interactions, which is focused on bit-\nwise level feature interactions. We employ a parallel net-\nwork architecture that separates the modeling of vector-wise\nand bit-wise interactions, with both networks sharing the\nsame embedding layer.\nFor bit-wise interactions, we input a wide concatenated\nvector $e \\in \\mathbb{R}^{mD}$ into the first layer. The total number of\nstacked KarSein interaction layers is T, the final layer is de-\nsigned to have a single neuron. The output of the T-th layer\nis denoted as $e^T \\in \\mathbb{R}^1$.\nCTR Prediction\nThe final outputs from the KarSein architecture for explicit\nfeature interaction $X^T$, and the outputs from the KarSein ar-\nchitecture for implicit feature interaction $e^T$, are combined"}, {"title": "Training with Sparsity", "content": "The KAN network exhibits sparsity by applying L1 regu-\nlarization to the parameters of the activation functions and\nentropy regularization to the activated values. Our model in-\nherits this feature with enhanced efficiency. Instead of apply-\ning L1 regularization to the activation functions' parameters\nand entropy regularization to the post-activated values of in-\ntermediate input-output features, we incorporate L1 and en-\ntropy regularization into the KarSein interaction layer's lin-\near combination step to eliminate redundant hidden neurons.\nSpecifically, for the L-th layer of the KarSein interaction\nlayer, we apply L1 regularization to $W^{L-1}$ and $W_s^{L-1}$ with\nthe regularization parameter $\\lambda_1$. The L1 regularization term\nis computed as follows:\n$\\lambda_1 (||W^{L-1}||_1 + ||W_s^{L-1}||_1)$\nNext, we compute the entropy regularization term for\n$W^{L-1}$ and $W_s^{L-1}$ with the regularization parameter $\\lambda_2$.\nThe computation is as follows:\n$\\lambda_2 (H(\\frac{W^{L-1}}{||W^{L-1}||_1}) + H(\\frac{W_s^{L-1}}{||W_s^{L-1}||_1}))$\nwhere $H(.)$ denotes the entropy calculation. The total\ntraining objective is given by:\n$\\mathcal{L}_{total} = \\mathcal{L}_{pred} + \\sum_{L=1}^T (\\lambda_1 (||W^{L-1}||_1 + ||W_s^{L-1}||_1) + \\lambda_2 (H(\\frac{W^{L-1}}{||W^{L-1}||_1}) + H(\\frac{W_s^{L-1}}{||W_s^{L-1}||_1})))$"}, {"title": "Analysis", "content": "Let T is the depth of both KarSein-explicit and KarSein-\nimplicit layers, and H as the number of hidden neurons\nper layer. Additionally, K represents the number of loga-\nrithmic neurons in AFN+ (Cheng, Shen, and Huang 2020),\nwhich significantly exceeds mD. The parameter n denotes\nthe number of order vectors in EulerNet, is typically set to m\nin practical applications. Then we present a comprehensive\ncomparative analysis, in terms of floating-point operations\n(FLOPs), among the proposed KarSein model, KAN, and\nother state-of-the-art CTR methods, as outlined in Table 1.\nWe compare DNN, KAN, and KarSein-implicit in the context of implicit feature interaction modeling. DNN demonstrates higher efficiency compared to KAN; however, KAN compensates by requiring significantly fewer hidden neurons H and shallower layers T. The proposed KarSein-implicit method, with parameters g and \u043a much smaller than H, achieves computational efficiency comparable to DNN while retaining KAN's advantage of smaller H and T.\nIn comparison of AFN+, EulerNet, and KarSein-explicit models which are designed to adaptively learn high-order features, AFN+ and EulerNet involve embedding space transformation. In contrast, KarSein-explicit performs feature interactions directly within the original space. Additionally, KarSein-explicit benefits from KAN's structural sparsity pruning capability, resulting in more global explainability and feature interactions with less redundancy. Notably, KarSein-explicit exhibits computational complexity independent of D, significantly outperforming AFN+ and EulerNet, and even surpassing DNN due to its smaller T and H (often close to m ~ m\u00b2)."}, {"title": "Experiments", "content": "In this section, we are to address these research questions:\nRQ1: How does KarSein model perform compared to other\nstate-of-the-art methods for CTR prediction? RQ2: How do\nthe explicit and implicit components of the KarSein model\nperform individually in prediction? RQ3: How do different\nconfigurations of pairwise multiplication affect the model's\nperformance? RQ4: What are the learned features in Kar-\nSein? RQ5: To what extent can KarSein reduce redundant\nfeatures and achieve network structural sparsity?"}, {"title": "Experiment Setups", "content": "Datasets We conduct experiments on three datasets, in-\ncluding MovieLens 1M, Douban Movie, and Criteo, which\nhave been utilized in previous studies (Cheng, Shen, and\nHuang 2020; Tian et al. 2023). For each dataset, we ran-\ndomly split the instances by 8:1:1 for training, validation\nand test, respectively. Further detailed introduction to our\nused datasets are in Appendix A.1.\nBaseline Methods We compare our method with three\nclasses of baselines: (1) Methods only have implicit fea-\nture interactions, i.e., DNN (Covington, Adams, and Sar-\ngin 2016), KAN (Liu et al. 2024), Wide & Deep (Cheng\net al. 2016), DCNV2 (Wang et al. 2021). (2) Methods have\nimplicit feature interactions, and explicit feature interac-\ntions with predefined order, i.e., DeepFM (Guo et al. 2017),"}, {"title": "Overall Performance (RQ1)", "content": "This section provides a comparative analysis of the perfor-\nmance and parameter computation cost (excluding the em-\nbedding table parameters) between the proposed KarSein\nmodel and existing state-of-the-art baselines for CTR pre-\ndiction. The experimental results are summarized in Table 2.\nIn terms of performance, KarSein consistently outper-\nforms all baseline methods across three datasets. This\ndemonstrates the efficacy of our learnable activation func-\ntions in capturing high-order feature interactions. Notably,\nall CTR models achieve an AUC of approximately 0.83 on\nthe Douban dataset. Even advanced models like EulerNet\nstruggle to achieve a 0.001 increase in AUC. However, our\napproach significantly surpasses these methods, delivering\nan improvement of 0.002 over the 0.83 benchmark. This en-\nhancement is particularly noteworthy, given that previous\nstudies (Cheng et al. 2016; Guo et al. 2017; Wang et al.\n2021) have established that an AUC increase or Logloss re-\nduction at the 0.001 level is statistically significant.\nRegarding computational efficiency, the parameter com-\nputation cost associated with the KarSein model is remark-\nably lower than that of several SOTA methods, even outper-\nforming traditional DNN models. This finding aligns with\nour previous FLOPs analysis."}, {"title": "Ablation Study (RQ2)", "content": "We investigate the contributions of explicit and implicit\nfeature interactions in the KarSein model by isolating and\nevaluating each type independently. Our analysis involves\ndecomposing the default ensemble KarSein model, which\ncombines both interaction types, to assess the impact of\nusing only explicit or only implicit interactions. We com-\npare these models in terms of CTR prediction performance,\nmodel size, and training time. The results, detailed in Ta-\nble 3, show that the KarSein model with only explicit inter-\nactions achieves similar AUC performance to the full ensem-\nble model but with smaller model size and reduced training\ntime. Conversely, the model with only implicit interactions\nshows lower AUC performance and a significant increase in\nmodel parameters. Our findings highlight that while combin-\ning both interaction types yields superior AUC performance,\nexplicit interactions offer advantages in efficiency and pa-\nrameter size. Implicit interactions, although adding consid-\nerable model complexity, have minimal impact on training\ntime. This underscores the complementary strengths of each\ninteraction mechanism."}, {"title": "Robustness Study (RQ3)", "content": "We empirically demonstrate that performing pairwise mul-\ntiplication in only the first two KarSein interaction lay-\ners is sufficient for the model to learn multiplicative re-\nlationships effectively. Figure 3 illustrates the impact of\napplying pairwise multiplication across different layers on\nKarSein-explicit's performance using the MovieLens-1M\nand Douban datasets. When pairwise multiplication is omit-\nted (Layer Index set to None), the model's AUC perfor-\nmance is notably poor. Introducing this step in just the first\nlayer leads to significant performance gains, especially on\nthe MovieLens-1M dataset, which benefits from its six basic\nfield features. In contrast, the Douban dataset, with only two\nfeatures, shows less improvement.\nThese results also confirm our earlier statement that the\nKAN network has difficulty learning multiplicative rela-\ntionships on its own, limiting its CTR prediction effective-\nness. Furthermore, applying pairwise multiplication exclu-\nsively in the first two layers achieves the highest AUC scores\nacross both datasets. Deviations from this setup-such as\nusing only one layer or extending beyond the first two lay-\ners-result in decreased performance."}, {"title": "Explanation Study (RQ4)", "content": "Our further exploration of the KarSein model focuses on\nhow learnable activation functions transform low-order in-\nput features into output features. We visualized these acti-\nvation functions across various layers and used third-degree\npolynomials for symbolic regression. Most activation func-\ntions were well-approximated by cubic polynomials, as\nshown in the first row of Figure 4. This indicates that Kar-\nSein's activation functions effectively elevate low-order fea-\ntures to capture high-order interactions.\nWe also observed some activation functions displaying\noscillatory and irregular patterns, depicted in the second col-\numn of Figure 4. This behavior underscores the effectiveness\nof B-Spline activation functions, which offer the flexibility\nto model such complex patterns. This capability to model\nhigh-order and intricate features significantly contributes to\nKarSein's state-of-the-art performance."}, {"title": "Pruning Redundant Features (RQ5)", "content": "We assess the level of sparsity and feature redundancy re-\nduction in the optimized KarSein model. Specifically, we\ntrained the KarSein model on the MovieLens-1M dataset\nand achieved an AUC of 0.8555. We use heat maps to vi-\nsualize connections from activated inputs to outputs across\neach layer of the model, and the detailed results are in Ap-\npendix C. We find that certain input features do no contribute\nto any output features (values < 0.01). These features are\ndeemed non-essential and can be masked. In the KarSein-\nexplicit component, 66% of input features in the first layer\nare redundant. For the KarSein-implicit component, 83%,\n87%, and 44% of input features in the first, second, and third\nlayers, respectively, are redundant. We then masked these re-\ndundant features and continued training the KarSein model\nfor an additional 3 epochs until convergence, resulting in\nan AUC of 0.8533. This demonstrates the model's ability\nto remove redundant features without greatly compromis-\ning performance. This characteristic surpasses many pioneer\nCTR methods that only can provide contextual feature im-\nportance, achieving feature \"de-redundancy.\u201d The resulting\nnetwork structural sparsity may can be leveraged to acceler-\nate inference in recommendation systems."}, {"title": "Related Works", "content": "Adaptive Order Feature Interaction Learning\nRecent advancements in CTR prediction aim to surpass the\nlimits of predefined interaction orders and feature combi-\nnations, improving predictive performance while reducing\nthe computational burden of enumerating high-order inter-\nactions. Notable representatives are AFN (Cheng, Shen, and\nHuang 2020) and EulerNet (Tian et al. 2023). Both meth-\nods shift the multiplication-based"}]}