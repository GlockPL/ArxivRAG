{"title": "Beyond KAN: Introducing KarSein for Adaptive High-Order Feature Interaction Modeling in CTR Prediction", "authors": ["Yunxiao Shi", "Wujiang Xu", "Mingyu Jin", "Haimin Zhang", "Qiang Wu", "Yongfeng Zhang", "Min Xu"], "abstract": "Modeling feature interactions is crucial for click-through rate (CTR) prediction, particularly when it comes to high-order explicit interactions. Traditional methods struggle with this task because they often pre-define a maximum interaction order, which relies heavily on prior knowledge and can limit the model's effectiveness. Additionally, modeling high-order interactions typically leads to increased computational costs. Therefore, the challenge lies in adaptively modeling high-order feature interactions while maintaining efficiency. To address this issue, we introduce Kolmogorov-Arnold Represented Sparse Efficient Interaction Network (KarSein), designed to optimize both predictive accuracy and computational efficiency. We firstly identify limitations of directly applying Kolmogorov-Arnold Networks (KAN) to CTR, and then introduce KarSein to overcome these issues. It features a novel architecture that reduces the computational costs of KAN and supports embedding vectors as feature inputs. Additionally, KarSein employs guided symbolic regression to address the challenge of KAN in spontaneously learning multiplicative relationships. Extensive experiments demonstrate KarSein's superior performance, achieving significant predictive accuracy with minimal computational overhead. Furthermore, KarSein maintains strong global explainality while enabling the removal of redundant features, resulting in a sparse network structure. These advantages also position KarSein as a promising method for efficient inference.", "sections": [{"title": "Introduction", "content": "In the ever-evolving realm of digital advertising and recommendation systems, Click-Through Rate (CTR) prediction has emerged as a pivotal element for optimizing user engagement and enhancing revenue streams. Despite its seemingly straightforward goal, CTR prediction methods aim at capturing underlying feature relationships among the complicated context data.\nTypically, these methods either capture implicit feature interaction by directly modeling the fusion of all features using deep neural networks (Covington, Adams, and Sargin 2016; Cheng et al. 2016), or learn explicit feature interaction by manually defining the interaction form or order through factorization-based models (Covington, Adams,"}, {"title": "Preliminaries", "content": "Let U and I denote the sets of users and items, respectively. For a user-item pair (u,i) \u2208 U \u00d7 I, we define Xu,i = [x1,...,xm] as the feature vector capturing relevant attributes, including categorical (e.g., user and item"}, {"title": "Problem Formulation for CTR", "content": "IDs) and numerical (e.g., age) features. The Click-Through Rate (CTR) prediction task aims to estimate P(y = 1 | xu,i), where y \u2208 {0,1} indicates whether the user clicked on the item. Formally, we define \u0177 = f(xu,i; \u0398) where \u0177 is the predicted click probability, f is the predictive model, and represents the model parameters. Given a training dataset D = {(Uj, Ij, x(j,j), yj)}=1 of N instances, we optimize to minimize the prediction error. A common approach is to minimize the log loss: L(\u0398) = -\u2211i=1N [yi log \u0177i + (1 \u2212 yi) log(1 \u2013 \u0177i)]"}, {"title": "Feature Interactions Modeling", "content": "In recommendation systems, the model typically includes a trainable parameterized embedding layer, denoted as E(.). Given a categorical ID feature xj, the model first maps xj into dense vectors in a low-dimensional latent space: ej = E(xj) \u2208 RD, where D is the dimension of the embedding. Feature interaction modeling is then conducted on these vectorized embeddings. Feature interaction types can be broadly categorized into two primary paradigms: implicit and explicit. They often complement with each other, and more explanatory can be seen in Appendix D."}, {"title": "Explicit Feature Interactions", "content": "These interactions typically occur via multiplication at the vector level among representing vectors of basic field features. For a set of features {x1,x2,...,xm} with corresponding embeddings {e1, e2,...,em}, and consider hadamard product (element-wise multiplication) as the multiplication method, then for the k-th order feature interactions, we can enumerate all combinations as {e\u03b71 \u2299 e\u03b72 \u2299\u2026 \u2299 e\u03b7k | \u03b71, \u03b72, ..., \u03b7k \u2208 {1,2,...,m}}. By stacking all the elements, we form a matrix Xk \u2208 Rm* \u00d7D. Concatenating all interactions from the first-order to the k-th order results in a matrix X1~k \u2208 IR \u2211i=1k mi \u00d7 D, where each row represents a high order explicit feature interaction.\nWhen constructing high-order features with large k, the number of rows in X1\u223ck is given by s = \u2211i=1k mi, which increases exponentially with k, specifically, s ~ O(mk). Traditional methods typically construct X1~k with predefined k (e.g., k = 2). Advance methods focus on adaptively learning arbitrary high-order feature interactions without pre-defining k or exhaustively enumerating all features."}, {"title": "Implicit Feature Interactions", "content": "Implicit feature interactions refer to the interactions between features that are not explicitly predefined. Instead, these interactions are captured automatically by models. Let e = e1||e2||...||em denote the result of wide concatenated embedding vectors. Then let W be the linear transformation matrix applied to e, and \u03c3 be the activation function for non-linearities. For an L-layer DNN, this implicit feature interaction modeling can be expressed as:\nMLP(e) = (WL\u22121 \u2218 \u03c3 \u2218 WL\u22122 \u2218 \u03c3 \u2218 \u2025\u2025 \u2218 W1 \u2218 \u03c3 \u2218 W0)e\nImplicit feature interactions often involve learning the interaction patterns at the bit level. This process is supported by the Universal Approximation Theorem, which states that"}, {"title": "Kolmogorov-Arnold Network", "content": "For a smooth function f : [0,1]m \u2192 R, the Kolmogorov-Arnold Theorem states:\nf(x) = f(x1,...,xm) = \u22112m+1q=1 \u03a6q(\u2211mp=1 \u03c6\u03b1p(xp))\nwhere \u03c6q,p: [0,1] \u2192 R and Iq : R \u2192 R. This theorem implies that a multivariate function f(x) can be expressed using univariate functions and summation option. This theorem simplifies the task of learning high-dimensional functions by reducing it to learning a polynomial number of one-dimensional functions. However, these one-dimensional functions can be non-smooth or even fractal, making them impractical for direct application in machine learning models. However, Kolmogorov-Arnold Networks (KAN) (Liu et al. 2024) presents an optimistic perspective, it employs highly flexible, learnable B-Spline curves as \u03c6(\u00b7) for activation, and extends the theorem to neural networks with arbitrary widths and depths. Formally, for an activation function \u03c6(\u00b7), let Silu(\u00b7) denote the Silu activation, and B(\u00b7) represent the B-Spline curves activation withe grid size g and order K. The activation function \u03c6(x) is defined as \u03c6(x) = w0(B(x)+Silu(x)), where B(x) = \u2211CiNi,\u03ba(x). Here, the B"}, {"title": "KAN for CTR Prediction", "content": "both wo and ci are learnable parameters. Let IPL is the activation function matrix corresponding to the L-th KAN layer. A general KAN network is a composition of L layers, given an wide concatenated embedding vector e as input, the output of KAN is:\nKAN(e) = (\u03a6L\u22121 \u2218 \u03a6L\u22122 \u2218\uff65\uff65\uff65 \u2218 \u03a61 \u2218 \u03a60)e."}, {"title": "Multiplicative Relationship Learning", "content": "We find that the Kolmogorov-Arnold Representation Theorem is particularly well-suited for representing feature interactions combined with multiplicative relationship. Given two basic field feature x1 and x2, the derived second-order feature interaction x1x2 can be expressed as (\u03a61(\u03a61,1(x1) + \u04241,2(x2)) + \u03a62(\u03a62,1(x1) + \u03a62,2(x2))), where \u03a61(x) = x\u00b2, \u04242(x) = \u2212x, \u03c61,1(x) = x, \u03c61,2(x) = x, \u03c62,1(x) = x\u00b2, \u03c62,2(x) = x\u00b2. Thus, KAN can model feature interactions through symbolic regression, offering greater explainability and efficiency compared to DNN.\nWe also find that KAN's symbolic regression ability is sensitive to the structure initialization and the regularization setting, which can see details in Appendix B.1. In more general scenarios, KAN is hard to spontaneously perform right symbolic regression for learning higher-order multiplicative features interactions. This observation underscores a regrettable reality: KAN, akin to DNN, remains intrinsically limited in its ability to autonomously learn multiplicative features interactions. Consequently, relying solely on KAN's"}, {"title": "Vanilla Application in CTR", "content": "We designed a KAN network with layers of width mD 64 - 64 - 1 trained on the MovieLens-1M dataset, where wide concatenated embedding vectors e serve as the network input. This network outputs a numeric value, which is then activated by sigmoid function for CTR prediction. When model is convergent, achieving an AUC score of 0.8273, whereas a DNN model with the same layer and neuron configuration achieves an AUC of 0.8403. This shows that he vanilla application of KAN for CTR prediction yields suboptimal results. But we find that, in the optimized KAN, many network connections do little contribution, using the network Simplification techniques, the KAN network is pruned from the initial mD - 64 - 64 1 neurons down to mD \u2013 1 \u2013 1 \u2013 1 neurons, indicating that a single activation function per feature may be sufficient, eliminating the cost for multiple activation functions on the same feature, which is the parameter-redundancy in KAN. The detailed exploration settings are in Appendix B.2."}, {"title": "KARSEIN", "content": "The preliminary exploration of KAN for CTR prediction highlights the limitations of this vanilla methods, but the findings also motivate us to make modifications to adapt KAN for the CTR task. In this section, we introduce Kar-Sein. A schematic of its architecture is shown in Figure 2."}, {"title": "KarSein Interaction Layer", "content": "The KarSein model is constituted by multiple stacked Kar-Sein interaction layers. The layer takes a set of embedding vectors as input and generates higher-order feature interactions. The feature interaction consists of three steps on embedding vectors: optional pairwise multiplication, learnable activation transformation, and linear combination.\nWe begin by describing the KarSein interaction layer at the L-th level, where the input dimension is HL-1 and the output dimension is HL. Here, L \u2208 [1,T], and T denotes the depth of the stacked KarSein interaction layers. For the L-th layer, let XL\u22121 \u2208 RHL-1\u00d7D represent the input matrix, and XL \u2208 RHL\u00d7D represent the output matrix. Specifically, we define H1 = m, and X0 \u2208 Rm\u00d7D as the matrix formed by stacking e1, e2,...,em. Additionally, we define HT = 1 because the final layer is designed to have a single output neuron, ultimately modeling a highly intricate high-order feature interaction, denoted as XT \u2208 R1\u00d7D."}, {"title": "Pairwise Multiplication", "content": "In our methodology, we conduct feature interactions between XL-1 and X0. This process involves computing pairwise hadamard products of features. We then concatenate the results with XL-1 yielding new matrix for substituting XL-1, which serves as the input for subsequent steps. Generally, incorporating this optional Pairwise Multiplication step only within the first two Kar-Sein interaction layer is adequate to guide the model's learning can incorporate multiplicative relationships. As shown in Figure 2, the Pairwise Multiplication step generates"}, {"title": "Activation Transformation", "content": "second-order features f\u00b2(e1,e2). Through high-order activations and linear transformations, these features evolve into more complex multivariate high-order interactions, such as f(e1, 62, 63), a sixth-degree polynomial feature involving three variables. While f(e1, 62, 63) does not directly capture all three-variable interactions, stacking additional Kar-Sein interaction layers effectively addresses this limitation. For instance, in the second KarSein layer, f6 (e1, 62, 63) undergoes further activation, resulting in more intricate interactions like f18 (e1, 62, 63), which encompasses feature interactions among all three variables. This way, the model successfully learns richer multiplicative relationships.\nWe denote the basis functions of B-Spline curves be of grid size g and order k as NK = [N1,\u03ba, N2,\u03ba, ...,Ng+\u043a,\u03ba]. For L-th layer's input matrix XL-1 \u2208 RHL-1\u00d7D. We first activate each row of XL\u22121 on g + \u03ba basis functions, the process is denoted as Xbasis = N(XL-1) \u2208 RHL-1\u00d7D\u00d7(g+r). Then we define learnable weight matrices CL\u22121 \u2208 RHL\u22121\u00d71\u00d7(g+r). Then we process the activation transformation for XL-1 using the following formulation:\nXbL\u22121 =\n\u23a1\nXbasis [1, :,:]CL\u22121[1, :, :]T\n\u22ee\nXbasis [HL\u22121, :,:]CL\u22121[HL\u22121, :, :]T\n\u23a4"}, {"title": "Linearly Combination", "content": "We define the weight matrix WL\u22121 \u2208 RHL \u00d7 HL-1. To model the feature interactions, we perform a linear combination of the activated embedding vectors, represented as WL-1XL-1. To enhance the expressiveness of the model, we introduce an additional residual connection. Specifically, we apply the SiLU(\u00b7) activation function to X and define another weight matrix WSL\u22121 \u2208 RHL\u00d7HL-1 to perform a linear transformation on the activated embeddings, expressed as WSL-1SiLU(XL-1). The final output features are then given by the following formulation:\nXL = WS\u22121XL\u22121 + WSL\u22121SiLU(XL\u22121)"}, {"title": "Integrating Implicit Interactions", "content": "We integrate implicit interactions, which is focused on bit-wise level feature interactions. We employ a parallel network architecture that separates the modeling of vector-wise and bit-wise interactions, with both networks sharing the same embedding layer.\nFor bit-wise interactions, we input a wide concatenated vector e \u2208 RmD into the first layer. The total number of stacked KarSein interaction layers is T, the final layer is designed to have a single neuron. The output of the T-th layer is denoted as e \u2208 R1."}, {"title": "CTR Prediction", "content": "The final outputs from the KarSein architecture for explicit feature interaction XT, and the outputs from the KarSein architecture for implicit feature interaction e, are combined"}, {"title": "Training with Sparsity", "content": "The KAN network exhibits sparsity by applying L1 regularization to the parameters of the activation functions and entropy regularization to the activated values. Our model inherits this feature with enhanced efficiency. Instead of applying L1 regularization to the activation functions' parameters and entropy regularization to the post-activated values of intermediate input-output features, we incorporate L1 and entropy regularization into the KarSein interaction layer's linear combination step to eliminate redundant hidden neurons. Specifically, for the L-th layer of the KarSein interaction layer, we apply L1 regularization to WbL\u22121 and WSL\u22121 with the regularization parameter \u03bb1. The L1 regularization term is computed as follows:\n\u03bb1||WbL\u22121||1 + \u03bb1||WSL\u22121||1\nNext, we compute the entropy regularization term for WbL\u22121 and WSL\u22121 with the regularization parameter \u03bb2. The computation is as follows:\n\u03bb2H(WbL\u22121||\nWbL\u22121||1) + \u03bb2H(WSL\u22121||\nWSL\u22121||1)\nwhere H(.) denotes the entropy calculation. The total training objective is given by:\nLtotal = Lpred + \u03bb1 \u2211TL=1 (||WbL\u22121||1 + ||WSL\u22121||1) + \u03bb2 \u2211TL=1 (H(WbL\u22121||\nWbL\u22121||1) + H(WSL\u22121||\nWSL\u22121||1))"}, {"title": "Analysis", "content": "Let T is the depth of both KarSein-explicit and KarSein-implicit layers, and H as the number of hidden neurons per layer. Additionally, K represents the number of logarithmic neurons in AFN+ (Cheng, Shen, and Huang 2020), which significantly exceeds mD. The parameter n denotes the number of order vectors in EulerNet, is typically set to m in practical applications. Then we present a comprehensive comparative analysis, in terms of floating-point operations"}, {"title": "Experiments", "content": "In this section, we are to address these research questions:\nRQ1: How does KarSein model perform compared to other state-of-the-art methods for CTR prediction? RQ2: How do the explicit and implicit components of the KarSein model perform individually in prediction? RQ3: How do different configurations of pairwise multiplication affect the model's performance? RQ4: What are the learned features in Kar-Sein? RQ5: To what extent can KarSein reduce redundant features and achieve network structural sparsity?"}, {"title": "Experiment Setups", "content": "Datasets We conduct experiments on three datasets, including MovieLens 1M, Douban Movie, and Criteo, which have been utilized in previous studies (Cheng, Shen, and Huang 2020; Tian et al. 2023). For each dataset, we randomly split the instances by 8:1:1 for training, validation and test, respectively. Further detailed introduction to our used datasets are in Appendix A.1."}, {"title": "Baseline Methods", "content": "We compare our method with three classes of baselines: (1) Methods only have implicit feature interactions, i.e., DNN (Covington, Adams, and Sargin 2016), KAN (Liu et al. 2024), Wide & Deep (Cheng et al. 2016), DCNV2 (Wang et al. 2021). (2) Methods have implicit feature interactions, and explicit feature interactions with predefined order, i.e., DeepFM (Guo et al. 2017),"}, {"title": "Overall Performance (RQ1)", "content": "This section provides a comparative analysis of the performance and parameter computation cost (excluding the embedding table parameters) between the proposed KarSein model and existing state-of-the-art baselines for CTR prediction. The experimental results are summarized in Table 2. In terms of performance, KarSein consistently outperforms all baseline methods across three datasets. This demonstrates the efficacy of our learnable activation functions in capturing high-order feature interactions. Notably, all CTR models achieve an AUC of approximately 0.83 on the Douban dataset. Even advanced models like EulerNet struggle to achieve a 0.001 increase in AUC. However, our approach significantly surpasses these methods, delivering an improvement of 0.002 over the 0.83 benchmark. This enhancement is particularly noteworthy, given that previous studies (Cheng et al. 2016; Guo et al. 2017; Wang et al. 2021) have established that an AUC increase or Logloss reduction at the 0.001 level is statistically significant.\nRegarding computational efficiency, the parameter computation cost associated with the KarSein model is remarkably lower than that of several SOTA methods, even outperforming traditional DNN models. This finding aligns with our previous FLOPs analysis."}, {"title": "Ablation Study (RQ2)", "content": "We investigate the contributions of explicit and implicit feature interactions in the KarSein model by isolating and evaluating each type independently. Our analysis involves decomposing the default ensemble KarSein model, which"}, {"title": "Robustness Study (RQ3)", "content": "combines both interaction types, to assess the impact of using only explicit or only implicit interactions. We compare these models in terms of CTR prediction performance, model size, and training time. The results, detailed in Table 3, show that the KarSein model with only explicit interactions achieves similar AUC performance to the full ensemble model but with smaller model size and reduced training time. Conversely, the model with only implicit interactions shows lower AUC performance and a significant increase in model parameters. Our findings highlight that while combining both interaction types yields superior AUC performance, explicit interactions offer advantages in efficiency and parameter size. Implicit interactions, although adding considerable model complexity, have minimal impact on training time. This underscores the complementary strengths of each interaction mechanism."}, {"title": "Explanation Study (RQ4)", "content": "We empirically demonstrate that performing pairwise multiplication in only the first two KarSein interaction layers is sufficient for the model to learn multiplicative relationships effectively. Figure 3 illustrates the impact of applying pairwise multiplication across different layers on KarSein-explicit's performance using the MovieLens-1M and Douban datasets. When pairwise multiplication is omitted (Layer Index set to None), the model's AUC performance is notably poor. Introducing this step in just the first layer leads to significant performance gains, especially on the MovieLens-1M dataset, which benefits from its six basic field features. In contrast, the Douban dataset, with only two features, shows less improvement.\nThese results also confirm our earlier statement that the KAN network has difficulty learning multiplicative relationships on its own, limiting its CTR prediction effectiveness. Furthermore, applying pairwise multiplication exclusively in the first two layers achieves the highest AUC scores across both datasets. Deviations from this setup-such as using only one layer or extending beyond the first two layers-result in decreased performance.\nOur further exploration of the KarSein model focuses on how learnable activation functions transform low-order input features into output features. We visualized these activation functions across various layers and used third-degree polynomials for symbolic regression. Most activation functions were well-approximated by cubic polynomials, as shown in the first row of Figure 4. This indicates that Kar-"}, {"title": "Pruning Redundant Features (RQ5)", "content": "Sein's activation functions effectively elevate low-order features to capture high-order interactions.\nWe also observed some activation functions displaying oscillatory and irregular patterns, depicted in the second column of Figure 4. This behavior underscores the effectiveness of B-Spline activation functions, which offer the flexibility to model such complex patterns. This capability to model high-order and intricate features significantly contributes to KarSein's state-of-the-art performance.\nWe assess the level of sparsity and feature redundancy reduction in the optimized KarSein model. Specifically, we trained the KarSein model on the MovieLens-1M dataset and achieved an AUC of 0.8555. We use heat maps to visualize connections from activated inputs to outputs across each layer of the model, and the detailed results are in Appendix C. We find that certain input features do no contribute to any output features (values < 0.01). These features are deemed non-essential and can be masked. In the KarSein-explicit component, 66% of input features in the first layer are redundant. For the KarSein-implicit component, 83%, 87%, and 44% of input features in the first, second, and third layers, respectively, are redundant. We then masked these redundant features and continued training the KarSein model for an additional 3 epochs until convergence, resulting in an AUC of 0.8533. This demonstrates the model's ability to remove redundant features without greatly compromising performance. This characteristic surpasses many pioneer CTR methods that only can provide contextual feature importance, achieving feature \"de-redundancy.\u201d The resulting network structural sparsity may can be leveraged to accelerate inference in recommendation systems."}, {"title": "Related Works", "content": "Recent advancements in CTR prediction aim to surpass the limits of predefined interaction orders and feature combinations, improving predictive performance while reducing the computational burden of enumerating high-order interactions. Notable representatives are AFN (Cheng, Shen, and Huang 2020) and EulerNet (Tian et al. 2023). Both methods shift the multiplication-based feature interactions traditionally performed in the original embedding space to an alternative space where feature combinations are achieved through linear operations before being mapped back to the original space. Specifically, AFN employs logarithmic transformation (Hines 1996), whereas EulerNet utilizes Euler's formula. However, these methods introduce challenges related to numerical stability issue, potential loss of feature relationships, and additionally computational overhead. In response, we propose the KarSein method, which directly activates features within the original space to adaptive orders using a learnable activation function. Our approach is much more simpler and effective, also has intuitive symbolic regression explanations."}, {"title": "Adaptive Order Feature Interaction Learning", "content": "Achieving feature de-redundancy, and KarSein demonstrates higher efficiency compared to DNN."}, {"title": "Feature Importance Learning", "content": "Techniques, such as AFM (Xiao et al. 2017), AutoInt (Weiping et al. 2018), FiGNN (Li et al. 2019), and FiBiNET (Huang, Zhang, and Zhang 2019), leverage self-attention (Vaswani 2017) mechanisms or SENet (Hu, Shen, and Sun 2018) to provide contextual explanations for the prediction process. However, the explainability afforded by these methods is inherently local, limiting their ability to identify universally redundant features across all samples. In contrast, the global interpretability of the KarSein method allows for the pruning of unnecessary network structures by eliminating redundant feature learning, resulting in a sparser network architecture. This sparsity holds significant potential for efficient inference."}, {"title": "Conclusion and Future Work", "content": "In this paper, we introduced KarSein, a Kolmogorov-Arnold Represented Sparse Efficient Interaction Network, to address the limitations of traditional CTR prediction methods. KarSein enhances predictive accuracy and reduces computational costs through a novel architecture that uses learnable activation functions for modeling high order features from low order features efficiently. It employs enforced pairwise multiplication, guiding KarSein's symbolic regression incorporates multiplicative relationships. Our extensive experiments demonstrate KarSein's superior performance, maintaining global interpretability and a sparse network structure, thus positioning it as a highly efficient method for CTR prediction."}, {"title": "Appendix A: Experiment Setups", "content": "Appendix A.1: Datasets We conduct experiments on three datasets, including MovieLens 1M, Douban Movie, and Criteo.\nMovieLens 1M : MovieLens 1M dataset is a widely used benchmark dataset in the field of recommender systems. It consists of 1 million movie ratings provided by 6,000 users on 4,000 movies. Each rating ranges from 1 to 5 for rating prediction. For CTR prediction, the ratings of 1 and 2 are normalized to be 0, and ratings of 4 and 5 to be 1.\nDouban Movie: The Douban Movie dataset consists of 1 million movie ratings, which are collected from the Douban website and ranged between 1 to 5 for rating prediction. The dataset includes data from 10,000 users and 10,000 movies spanning the years 2008 to 2019. For CTR prediction, the ratings of 1 and 2 are normalized to be 0, and ratings of 4 and 5 to be 1.\nCriteo Dataset: The Criteo dataset comprises user logs collected over a period of 7 days. It contains 45 million examples and 39 features, including 26 categorical feature fields and 13 numerical feature fields. We discretize each numerical value using a logarithmic discretization method."}, {"title": "Appendix A.2: Baseline Methods", "content": "We compare KarSein with state-of-the-art methods in CTR prediction task, including:\n\u2022 DNN (Covington, Adams, and Sargin 2016) is a straightforward model based on deep stacked MLP architecture , which applies a fully-connected network after the concatenation of feature embeddings for CTR prediction.\n\u2022 KAN (Liu et al. 2024) offers a promising alternative to MLPs. In our approach, we replace MLPs with KAN, allowing concatenated feature embeddings to be processed through KAN for CTR prediction.\n\u2022 Wide & Deep (Cheng et al. 2016) combines a linear model for memorization of feature interactions with a DNN for generalization to capture high-order interactions.\n\u2022 DCNV2 (Wang et al. 2021) uses the kernel product of concatenated feature vectors to model high-order interactions and integrates an DNN for implicit interactions.\n\u2022 DeepFM (Guo et al. 2017) combines FM to capture second-order interactions with DNN to model high-order interactions.\n\u2022 xDeepFM (Lian et al. 2018) encodes high-order interactions into multiple feature maps and combines them with an DNN to model implicit interactions.\n\u2022 AFN+ (Cheng, Shen, and Huang 2020) AFN encodes features into a logarithmic space to adaptively learn arbitrary-order feature interactions, AFN+ additionally use an DNN for implicit interactions, further improving the model capacity.\n\u2022 EulerNet (Tian et al. 2023) leverages Euler's formula to transform features into the complex vector space, allowing for the efficient modeling high order feature interactions linearly."}, {"title": "Appendix A.3: Hyper Parameter Setting", "content": "For the sake of fair comparison, the embedding size is uniformly set to 16 across all methods. For each method, we all conduct grid search over several general hyper parameters to optimize performance. The learning rate is selected from {1e-3, 2-3, 3-3,1e-4}, while the batch size is varied across {512, 1024, 4096}. Many methods incorporate the DNN components, we experimente with hidden layer configurations of {400 \u2013 400 - 400, 128 \u2013 128 - 128, 256 - 256-256}, with a dropout rate of 0.1. The regularization weight penalty is chosen from {1e-5, le-6}.\nWe conduct further grid search for tuning the unique hyper parameters associated within each method. In the xDeepFM model, the Compressed Interaction Network (CIN) depth is varied among 1,2,3, with the hidden size chosen from {100, 200}. For DCNV2 model, the low-rank space size is set to 128, while the CrossNet depth is tested across {3, 4, 5}. For FiGNN model, the number of attention heads is set to 2, and the graph interaction steps are chosen from {2, 3, 4}. For AFN, the number of logarithmic neurons is selected from {400, 800, 1000, 1500}. For EulerNet, the number of Euler interaction layers is selected from {1, 2, 3}, and the number of order vectors is fixed at 30.\nFor the implicit components of our proposed KarSein model, we choose hidden layers from {32-32, 64-64, 64-32} for all datasets. For the explicit components, we choose the hidden layers from {50 \u2013 50, 26 \u2013 26, 16 \u2013 16} for the Criteo dataset, and {4 \u2013 4,6 \u2013 6,8 \u2013 8} for the Douban and MovieLens-1M datasets. Regularization parameters \u03bb1 and \u03bb2 are selected from {1e-2, 1e-3} and {1e-4,1e-5} respectively. The k and g are set from {(\u043a = 1, g = 3), (\u043a = 2,9 = 5), (\u043a = 3, g = 10)}."}, {"title": "Appendix B: KAN for CTR Prediction", "content": "Appendix B.1: Exploration 1\nWe are to exploration whether KAN structure can spontaneously learn multiplicative interactions. Given two basic feature a and b. We pre-define the idealized KAN structures 2-1, 2-1, and 2 - 2 - 1 for learning these interactions: f(a,b) = a\u00b2, f(a,b) = b2, and f(a,b) = ab, respectively. Beyond this idealized framework, our investigation extends to two additional network initialization settings, one incorporating regularization and the other excluding it. Comprehensive details of these configurations are delineated in Table 4.\nOur primary objective is to model second-order feature interactions to achieve a RMSE of < 0.05. We employ the adam optimizer with a learning rate of 0.001 in all configurations. We document the number of optimization steps required to meet this criterion. Additionally, we present visualizations of the model structures upon reaching the target, as shown in Figure 5. From the experimental results, we observe the following:\n\u2022 Under predefined ideal structures, the KAN not only accurately fits the feature interactions but also effectively"}]}