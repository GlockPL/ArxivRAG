{"title": "Causal Composition Diffusion Model for Closed-loop Traffic Generation", "authors": ["Haohong Lin", "Xin Huang", "Tung Phan-Minh", "David S. Hayden", "Huan Zhang", "Ding Zhao", "Siddhartha Srinivasa", "Eric M. Wolff", "Hongge Chen"], "abstract": "Simulation is critical for safety evaluation in autonomous driving, particularly in capturing complex interactive behaviors. However, generating realistic and controllable traffic scenarios in long-tail situations remains a significant challenge. Existing generative models suffer from the conflicting objective between user-defined controllability and realism constraints, which is amplified in safety-critical contexts. In this work, we introduce the Causal Compositional Diffusion Model (CCDiff), a structure-guided diffusion framework to address these challenges. We first formulate the learning of controllable and realistic closed-loop simulation as a constrained optimization problem. Then, CCDiff maximizes controllability while adhering to realism by automatically identifying and injecting causal structures directly into the diffusion process, providing structured guidance to enhance both realism and controllability. Through rigorous evaluations on benchmark datasets and in a closed-loop simulator, CCDiff demonstrates substantial gains over state-of-the-art approaches in generating realistic and user-preferred trajectories. Our results show CCDiff's effectiveness in extracting and leveraging causal structures, showing improved closed-loop performance based on key metrics such as collision rate, off-road rate, FDE, and comfort.", "sections": [{"title": "1. Introduction", "content": "Reliable closed-loop traffic simulation is essential for assessing autonomous vehicle (AV) safety in diverse and complex scenarios [1-5]. Simulations must be both realistic, capturing the intricacies of real-world driving, and controllable, allowing customization aligned with user preferences. However, balancing realism with controllability remains a significant challenge. Previous works often prioritize one aspect, optimizing either realism or user-specified objectives [3, 5]. How to jointly achieve both objectives under safety-critical conditions remains fruitful yet unresolved.\nTraffic agent simulation often resorts to either (i) data-driven approaches that generate the most probable trajectories based on scene context or (ii) rule-based approaches that maximize alignment with a user's control. However, both approaches face key limitations for effective scenario generation.\nData-driven scenario generation faces two primary challenges. First, the rarity of collision and near-miss events in public datasets limits the ability of data-driven methods to generate safety-critical scenarios. As shown in prior studies [2, 6], even a small domain mismatch, such as changes in road structure or the behavior of surrounding vehicles, can cause significant regressions. Second, closed-loop simulation requires that generated trajectories continuously interact with the simulated environment, so current predictions influence future predictions. This feedback loop often creates compounding errors, leading to distributional shifts that challenge the generation of both controllable and realistic behaviors over long horizons.\nOn the other hand, rule-based approaches to simulation [7, 8] offer precise user control, but often fail to capture the nuanced, adaptive behaviors of real-world driving, especially in unpredictable scenarios. Their rigidity can make generated behaviors feel scripted and unrealistic, particularly in closed-loop simulations where each action influences future states. This lack of adaptability often leads to compounding errors and a drift from realistic behavior distributions, limiting their effectiveness in complex, long-horizon interactions.\nRecent advances in deep generative models have enabled scalable traffic behavior simulation [9, 10], facilitating realistic scenario generation from massive offline datasets."}, {"title": "2. Related Work", "content": "Causal reasoning has seen extensive applications in trajectory modeling, with previous studies leveraging causal structures to enhance the robustness and generalizability of open-loop behavior prediction models. These efforts have included causal representation learning [15], backdoor adjustment [16], counterfactual analysis [17], and realistic causal interventions [11, 18]. Despite these advances, applying a causal approach to broader trajectory prediction tasks [19] often demands substantial human annotation efforts [20]. To address the challenges of automating spatiotemporal reasoning in traffic scenarios, state-of-the-art methods [21-24] employ factorized attention mechanisms. However, while previous work applies causal structured reasoning primarily in open-loop settings, the efficacy of causal behavior modeling in closed-loop scenarios for autonomous driving remains under-explored.\nPrior arts balance the trade-off between realism and controllability in safety-critical scenario generation by incorporating various constraints, such as inference-time sampling strategies [10], retrieval-augmented generation [5], low-rank finetuning [25], and language-conditioned generation [13]. In closed-loop simulation methods, compositional constraints in the training loss are often integrated into the simulation pipeline. For instance, TrafficSim [26] achieves a balance between realism and common sense using a time-adaptive multi-task loss design; SimNet [27] factorizes trajectory sequences using Markov processes; STRIVE [28] imposes structured priors to constrain samples, avoiding unrealistic outcomes; and BITS [29] optimizes closed-loop performance via bi-level imitation. Yet these prior methods struggle to resolve conflicts between controllability and realism objectives when these are at odds during inference.\nDiffusion models [30-32] have shown strong controllability in density estimation and generation tasks."}, {"title": "3. Problem Formulation", "content": "We formulate the closed-loop traffic simulation as an MDP problem, then utilize diffusion model for sequential modeling to learn a controllable simulation policy \u03c0. Since we would like to exploit the causal structure between the state, action, and reward space, we define the Constrained Factored MDP as follows:\nDefinition 1 (Constrained Factored MDP). A Constrained Factored Markov Decision Process (CFMDP) is a Markov Decision Process where the state space S and reward function R are factorized to exploit the structure of the problem. A CFMDP is defined by the tuple: MF = (S, A, P, R, C, s0).\nThe factored state space, denoted as $S = S^{(1)} \\times S^{(2)} \\times ... \\times S^{(N)}$, represents the motion trajectory space at the current step for each agent i. The factored action space, $A = A^{(1)} \\times A^{(2)} \\times ... \\times A^{(N)}$, consists of interventions on the subsequent driving behaviors for each agent in the scenario. The joint transition dynamics $P(s_t|s_{t-1}, a_{t-1}) = \\prod_{i=1}^N P_i(s_t^{(i)} | s_{t-1}, a_{t-1})$, are defined over the state s and action a \u2208 A pairs. In our case, P is the deterministic vehicle dynamics for each agent in our setting. The reward objective $R(s, a) = \\sum_{i=1}^{N} R^{(i)}(s^{(i)}, a)$, include collision, off-road events, over-speed, or other objectives, where each subset I_i specifies the state factors impacting the j-th reward. For a learned policy \u03c0, the constraint function $C(s, a) = D(\\pi_\\beta(\u00b7|s_t)||\\pi(\u00b7|s_t))$ indicates the realism level of generated trajectories with respect to the dataset policies $\u03c0_\u03b2$, where a lower constraint value implies greater realism. The initial state s0 lies in the factored state space S.\nWe then formulate the closed-loop scenario generation problem as a constrained optimization problem that aims to find an intervention policy \u03c0 that maximizes the controllability R(T) while maintaining an acceptable deviation in the realism C(T):\n$\\max_\\pi E_\\tau~{\\left(P,\\pi\\right)}[R(T)], \\textrm{s.t.} E_\\tau~{\\left(P,\\pi\\right)}[C(T)] \\leq \\kappa$,\nwhere the cumulative reward, R(T), represents the total reward accumulated from individual reward factors along the trajectory \u03c4: $R(T) = \\sum_{t=1}^T R^{(i)}(s_t^{(i)}, a_t) = \\sum_{t=1}^T \\sum_{i=1}^N R^{(i)}(s_t^{(i)}, a_t))$. The cumulative cost C(T) quantifies the realism constraints, measuring how closely the generated trajectory \u03c4 resembles the ground-truth trajectory \u03c4*. Following the approach in [3, 29], we use the Total Variation (TV) distance between the estimated intervention policy \u03c0(at|st) and the dataset policy \u03c0\u03b2(at|st): $C(T) = \\sum_{t=1}^T D(\\pi_\\beta(a_t|s_t) || \\pi(a_t|s_t))$.\nTo further incorporate the structure in this multi-agent decision-making problem [40], we define the Decision Causal Graph (DCG) below.\nDefinition 2 (Decision Causal Graph). For every timestep t, we define a causal graph $G \\in \\mathbb{R}^{N\\times N}$, where $G_{ij} = 0$ if and only if the future action of agent j is conditionally independent with the agent i-th history: $a_t^{(j)} \\perp s_t^{(i)} | s_t^{(-i)}$. And $G_{ij} = 1$ means there exists a causal edge $s_t^{(i)} \\rightarrow a_t^{(j)}$.\nFollowing the definition above, we can also define a set of policy $\\pi(a^{(1)}, ..., a^{(N)} | s_t) = \\prod_{i=1}^N \\pi^{(i)}(a_t^{(i)} | PA^{(i)})$, where PA(i) \u2208 {$s^{(1)}, s^{(2)}, ..., s^{(N)}$} is the causal parents to the i-th agents in graph G when making decisions."}, {"title": "3.2. Diffusion Model for Sequence Modeling", "content": "We then solve the traffic simulation inspired by the recent advancement in diffusion-guided sequential data generation [3, 41, 42]. Denote $\u03c4^{(k)} \\triangleq {(s_t^{(k)}, a_t^{(k))}}_{t=1}^T$ represent the joint state-action trajectory at the k-th diffusion step, k \u2208 {0,1,......, K}, where $\u03c4^{(0)}$ denotes the original clean trajectory. The forward diffusion process, acting on $\u03c4^{(0)}$, gradually corrupts it with Gaussian noise:\n$q(\u03c4^{(1:K)}|\u03c4^{(0)}) = \\prod_{k=1}^K q(\u03c4^{(k)}|\u03c4^{(k-1)})$, $q(\u03c4^{(k)}|\u03c4^{(k-1)}) = N(\u03c4^{(k)};\\sqrt{1-\\beta_k}\u03c4^{(k-1)}, \\beta_kI)$, where \u03b21,..., \u03b2K are pre-defined variance schedules at each diffusion step. Over the forward process, the trajectory is transformed into a standard Gaussian distribution: q(\u03c4(K)) \u2248 N(0, I). For scenario generation, the reverse diffusion process iteratively denoises from noise to recover the original trajectories. Given a context c (e.g., map features), the reverse process is:\np_{\\phi, \\psi}(\u03c4^{(0 : K)}|c) = p(\u03c4^{(K)}) \\prod_{k=1}^K p_{\\phi, \\psi}(\u03c4^{(k-1)}|\u03c4^{(k)}, c),\np_{\\phi, \\psi}(\u03c4^{(k-1)}|\u03c4^{(k)}, c) = N (\u03c4^{(k-1)}; \\pi_{\\phi, \\psi}(\u03c4^{(k)}, k, c), \\sigma I),"}, {"title": "4. Methodology", "content": "We denote a factored optimality of time-step t, a set of binary random variables as Ot = {o_{ij})}_{j=1}^N [41, 43]. The joint optimality in all reward objectives can be written as $p(O_t) = 1/T_t \\times exp (R^{(i)} (\u03c4; O^{(i)}))$. We slightly exploit the notations \u03c4 \u2252 {(st, at) }_{t=1}^T as the trajectories of state action pairs, where st \u2208 RN\u00d7ds is the state trajectories for all N agents. Given the CFMDP in Definition 1, with known transition (vehicle) dynamics, we can factorize the objective of the optimal closed-loop scenario generation as follows:\n$\\max_\\pi P(O_t = 1, \u03c4_t|\u03c4_{t-1}) = \\max_\\pi P(O_t = 1|\u03c4_t)P(\u03c4_t|\u03c4_{t-1}) \\\\= \\max_\\pi P(O_t = 1|s_t, a_t) \\pi(a_t|s_t)P(s_t|s_{t-1}, a_{t-1}) \\\\= \\max_\\pi exp (\\sum_{j=1}^{dr} R^{(j)} (s_t(s_{I_j}), a_t)) \\prod_{i=1}^N \\pi^{(i)}(a_t^{(i)} | s_t),$\nwhere the first term corresponds to controllability, i.e. the likelihood of optimality specified by some user-specified reward objective, and the second term corresponds to the realism, the likelihood of generated behaviors. We denote \u2207log P \u225c \u2207 log P(Ot = 1, \u03c4t|\u03c4t\u22121) The score function of the maximum likelihood objective in equation (1) can be written as [30, 41]:\n$\\nabla log P = \\sum_{j=1}^{dr} R^{(j)} (s_t(s_{I_j}), \\pi(s_t)) + \\sum_{i=1}^{N} \\nabla log \\pi^{(i)}(a_t^{(i)} | s_t)$,\nUnlike the normal scenarios where optimizing the imitation basically adheres with the rule compliance reward, safety-"}, {"title": "4.1. Realism Constrained Score Matching", "content": "critical guidance $R^{(i)}$ can suffer from gradient conflict [44-47]. Namely, for some i \u2208 [1, N], j \u2208 [1, dr], if\n$(\u2207log \u03c0^{(i)}(a_t^{(i)} | s_t), \u2207R^{(j)}((I_j))) < 0,$\nusing a weighted sum of all the objectives as classifier-based guidance would achieve sub-optimal performance, as illustrated in Figure 2(b). In order to resolve this gradient conflicting issues, we need to prioritize to control the agents index i \u2208 [N] that could maximize the reward while maintaining a high likelihood of the learned policies, i.e., a lower realism gap between the learned policies and behavior policies $\u03c0_\u03b2: D(\u03c0_\u03b2||\u03c0)$. We denote the flag of controllable agents as \u03c1 \u2208 [0, 1], the target simulation policies:\n$\\pi^{(i)}(a_t^{(i)} | s_t) = \\begin{cases} \\pi_\\beta(a_t^{(i)} | s_t), & p_i = 1 \\\\\u03c0_\\lambda(a_t^{(i)} | s_t), & p_i = 0 \\end{cases}$\nWe can use the Lagrangian multiplier [48] and structured projected gradient descent [49] to solve the constrained optimization with the following maximum likelihood estimation problems:\n$\\max_\\pi \\in \\Pi, \\rho \\in {0, 1}^{N\\times N} exp(\\sum_{j=1}^{dr} R^{(j)} (\u03c4; \u03c1)) \\prod_{i \\in [N], p_i=1} \u03c0^{(i)}(a_t^{(i)} | PA^{(i)})$, s.t. |G| \u2264 Csparsity, Pi \u2264 Nc.\nWe can then control the realism level by changing the constraint level of Nc, Csparsity \u2208 Z+."}, {"title": "4.2. Proposed Method: CCDiff", "content": "We hereby introduce CCDiff to optimize the simulation policy \u03c0 of equation (3) in a scalable and efficient way by decomposing the constrained optimization problem into several small components. We illustrate the pipeline in Figure 2(a). To promote realism, CCDiff first encodes the motion histories of different agents based on the spatial attention, then discovers the decision causal graph G based on the factorized attention masks and kinematic factors. CCDiff then utilizes causal interactive patterns in G to extract the importance rank \u03c1. Finally, CCDiff optimizes its controllability by masking out those unimportant agents based on \u03c1 to guide the diffusion reverse sampling process in a structured way. We zoom into the details below.\nInspired by [50, 51], the goal of causal composition scene encoder is to generate the most likely action under a parsimonious decision causal graph G with Lagrangian multiplier \u039bsparsity:\n$\\max_{G \\in [0,1]^{N\\times N}} \\prod_{i\\in[N]} \\pi^{(i)}(a_t^{(i)} | PA^{(i)}) + \\Lambda_{sparsity} |G|$\nWe parameterize our model \u03c0\u03c6,\u03c8(at|st, c, k; G) for scenario generation with a transformer-based structures for temporal attention \u03c6, spatial attention modules \u03c8, as well as some decision causal graph G. The model output at is conditioned on the agents' history st, map context c, and diffusion sample step k. Similar to the scene transformer structure in [4, 22], we first embed the history of ego and surrounding agents with temporal attention layer: [$\u03a6ego(s_t^{(i)}), \u03a6others(s_{-i} \\rightarrow s^{(i)})$, here $s_t^{(i)}$ is the history of the i-th agents, and $s_{-i} \\rightarrow s^{(i)}$ are the relative history of all the other agents than i. To facilitate the relational reasoning, we incorporate both the absolute and relative features in $\u03a6others(\u00b7)$, including the position, velocity, distance, and time-to-collision (TTC). Then we can aggregate all the temporal information into the following spatial cross-attention layer:\n$q_h^{(i)} = \u03a6_{ego}(s_t^{(i)})$, k_h^{(ij)} = v_h^{(ij)} = [\u03a6_{ego}(s_t^{(1)}), \u03a6_{others}(s_{-i} \\rightarrow s^{(j)})].$"}, {"title": "", "content": "In order to further discover useful spatial parent-to-child relationships, we design a two-step causal reasoning to identify the DCG in the spatial-temporal interaction of the traffic agents. First, we set a hard constraint over the neighborhood perception field by trimming down the unnecessary causal connection between agents' states and corresponding actions at time-step t. Second, we apply the first tunable hard constraint as a memory mask to the attention weights:\n$G_{ij} (T_t) = M_i (T_t) \\cdot softmax (\\frac{(qW_q)^{(i)} T (kW_k)^{(ij)}}{\\sqrt{d_k}})$,\nwhere the memory mask M is extracted with relative TTC features fTTC(\u00b7) with the surrounding agents given the threshold Cttc of causal graph G:\n$M_i (T_t) = \\begin{cases} 1, & f_{TTC} (P_{others} (s_{-i} \\rightarrow s^{(i)})) < C_{ttc} \\\\ 0, & otherwise \\end{cases}$\nIn practice, we can tune the threshold of Cttc here to control the sparsity of the final causal graph. We then aggregate the map information c into the decoder. The output layer aggregate the state of causal parental agents PA(i) to get the action: $a_t^{(i)} (k) = \u03a6(PA^{(i)}, c, k)$.\nWe then use the identified DCG to rank the agents' importance to the safety-critical objectives $R^{(i)} (\u03c4)$:\np_i = arg max_{i \\in [N]} (\u2207_{s_t} log \u03c0^{(i)}(a_t) | s_t), \u2207_t R^{(i)}((I_j)))\nTo automate the ranking process, we resort to the estimated causal graph G above. The causal composition scene encoder gives us G and a policy network $\u03c0_{\\phi, \\psi}(a_t|s_t; G)$. Then we design a graph-based community detector on the DCG G, then sort the time of occurrences in any cliques for all the nodes from 1 to N [15, 17, 18]. After sorting, we have the ranked id sequence {pi(T)}1N , then we can pick the top Nc key agents {pi(T)}1N at the scene, which represent the most densely interactive with the other agents. This ranking process empirically helps identify the most interactive and influential agents for the safety-critical objective. We further discuss in the appendix with more details about the specific design of relational features \u03a6 and the community detection algorithms we used.\nFor the diffusion guidance process, similar to the inpainting technique [41], we aim to trim down the controllable space by reducing the number of controllable agents with cause-and-effect ranking. With the causal reasoner and importance ranker modules, we"}, {"title": "Causal Composition Guidance", "content": "sorted out the key agents {$p_k(T))}_{k=1}^N_c$. We then apply both classifier based and classifier guidance.\nIn CCDiff, we derive a special form of classifier-free guidance [32] as a combination of unconditional scene encoding and causal interventional encoding. At timestep t, for the top-Nc controllable agents i \u2208 {PNc(Tt)}N, the classifier-free guidance is:\n(1-w)\u2207a log \u03c0\u03c6,\u03c8(a(i) | s(i) ) + w\u2207a log o\u03c0\u03c6,\u03c8 (a(i) | PAF(i) ),\nOriginal Intervened St St\nwhere w is the guidance scale. For agent i, \u03c0\u03c6\u03c8(a(i) | s(i) ) is\nthe unconditional distribution that only considered the ego histories \u03a6(sego), and \u03c0\u03c6\u03c8(a(i) | PAF(i)) is the intervened encoded results given some parental agents in causal graph G. This formulation implies that the guided distribution corresponds to a geometric mixture:\n\u03c0a(a) = \u03b1\u03c0(\u03b1(i) )1\u2212w \u03c0(a(i) | PA(i) )w .\nThus, classifier-free guidance in diffusion models can be viewed as a do-intervention [52] by specifying the causal parents of i-th agents in DCG during the generative process. The guidance scale w \u2208 [1,2) acts analogously to the strength of intervention, extrapolating the original and intervened distributions. With the causal ranking, we mask out the agents with conflicted gradients as a reweighted classifier-based guidance:\n$\\sum_{j=1}^{dr} w_j \\nabla_a R^{(j)} (\u03c4) = \\sum_{j=1}^{dr} p_j (T) \\nabla_a [R^{(j)} (\u03c4)]$\nIn practice, we use the distance-based guidance objective over the trajectories, including the map collision guidance and the agent collision guidance [3]. We also use the same classifier function for all the baseline methods, see detailed description in the appendix C.3.\nWe train the model with using the classical DDPM [31] diffusion with classifier-free guidance [32]. Specifically, the loss we solve is min\u03c6,\u03c8,G ||\u03c0\u03c6,\u03c8(\u03c4(k), c, k; G(t)) \u2013 a(0)||2. We introduce counterfactual conditions by randomly dropping the DCG G of the scene transformer by replacing the decision causal graph G as a diagonal matrix, so all agents' actions are only conditioned on the ego history. At inference time, we combine both classifier-based and classifier-free guidance to facilitate better controllability, see algorithm 1."}, {"title": "5. Experiment", "content": "In the following parts of the experiments, we aim to answer the following three research questions (RQs): RQ1: Under different sizes of total controllable agents, how are the realism and controllability of the safety-critical scenarios generated by CCDiff compared to the baselines? RQ2: With a longer generation horizon and lower frequency, how are the realism and controllability of the safety-critical scenarios generated by CCDiff compared to the baselines? RQ3: How much does the causal reasoning module in CCDiff contribute to the overall performance?\nThe remaining parts of the experiment section first introduce our experiment settings, then compare our methods with baselines in the controllability and realism to answer the research questions. Finally, we conduct ablation studies to show the effect of individual modules in CCDiff."}, {"title": "5.1. Experiment Settings", "content": "Datasets We use the nuScenes dataset [14] and traffic behavior simulation (tbsim) [29] for model training and evaluation. We train all models on scenes from the train split and evaluate on 100 scenes randomly sampled from the validation split. During evaluation phase, we initialize all the models with the same set of initial layouts and initial history trajectories of 3 seconds, the model is responsible of generating the future 10 seconds of trajectories for the driving agents in a closed-loop manner.\nBaselines We implement the following baselines in the above platform settings. To systematically illustrate the effectiveness of CCDiff, we include the following SOTAS for comparison: SimNet [27], TrafficSim [26], BITS [29], Strive [28], and CTG [3]. We compare all the baselines with CCDiff in the publicly available nuScenes dataset [14] and baseline implementations1. For a fair comparison with all the baselines, we use the rasterized map used in the previous works and encode them with ResNet-18 for the map conditioning c for all the methods.\nMetrics We compare the performance of CCDiff and all the baselines with the following categories of metrics:"}, {"title": "6. Conclusion", "content": "In this paper, we propose CCDiff, a causal composition diffusion model that aims to improve the controllability and realism in closed-loop safety-critical scenario generation for autonomous driving. Based on the formulation of constrained factored MDP, CCDiff promotes realism by first identifying the underlying causal structure between agents, then incorporating it in the scene encoder and ranking the importance of agents based on causal knowledge. CCDiff uses both interventional classifier-free guidance and masked classifier guidance to improve controllability in safety-critical scenario generation. In multi-agent generation and long-horizon generation settings, CCDiff outperforms SOTA methods over nuScenes data in closed-loop evaluation. One limitation of the current work is that the design of the causal reasoning pipeline relies on hyperparameter tuning and it is hard to directly evaluate. It would be interesting to construct a traffic reasoning benchmark and incorporate a foundation model to further scale up the traffic reasoning and generation process."}]}