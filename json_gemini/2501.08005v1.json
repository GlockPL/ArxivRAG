{"title": "DisCoPatch: Batch Statistics Are All You Need For OOD Detection, But Only If You Can Trust Them", "authors": ["Francisco Caetano", "Christiaan Viviers", "Luis A. Zavala-Mondrag\u00f3n", "Peter H.N. de With", "Fons van der Sommen"], "abstract": "Out-of-distribution (OOD) detection holds significant importance across many applications. While semantic and domain-shift OOD problems are well-studied, this work focuses on covariate shifts - subtle variations in the data distribution that can degrade machine learning performance. We hypothesize that detecting these subtle shifts can improve our understanding of in-distribution boundaries, ultimately improving OOD detection. In adversarial discriminators trained with Batch Normalization (BN), real and adversarial samples form distinct domains with unique batch statistics - a property we exploit for OOD detection. We introduce DisCoPatch, an unsupervised Adversarial Variational Autoencoder (VAE) framework that harnesses this mechanism. During inference, batches consist of patches from the same image, ensuring a consistent data distribution that allows the model to rely on batch statistics. DisCoPatch uses the VAE's suboptimal outputs (generated and reconstructed) as negative samples to train the discriminator, thereby improving its ability to delineate the boundary between in-distribution samples and covariate shifts. By tightening this boundary, DisCoPatch achieves state-of-the-art results in public OOD detection benchmarks. The proposed model not only excels in detecting covariate shifts, achieving 95.5% AUROC on ImageNet-1K(-C), but also outperforms all prior methods on public Near-OOD (95.0%) benchmarks. With a compact model size of < 25MB, it achieves high OOD detection performance at notably lower latency than existing methods, making it an efficient and practical solution for real-world OOD detection applications. The code will be made publicly available.", "sections": [{"title": "1. Introduction", "content": "Out-of-distribution (OOD) detection consists of identifying whether a given test sample significantly deviates from the known information of in-distribution (ID) data. It is often employed as a preliminary step in image-based systems, aiming to mitigate the risks associated with feeding OOD inputs to a model. Besides safeguarding a system against erroneous predictions, it also facilitates the safe handling of OOD samples, either by rejection or transfer to human intervention. However, the significance of OOD lies not only in bolstering the reliability of image processing systems, but also in its standalone role for anomaly and fault detection. A simple example of this use case can be found in the visual inspection of industrial image data, where it is easy to acquire imagery of normal samples yet virtually impossible to define the expected defects [46]. In the OOD context, these anomalies can be broadly classified into two types: (1) anomalous objects in images which refer to unexpected or rare items appearing in the frame, and (2) faulty equipment or products which refer to malfunctions or irregularities in the machinery or products under inspection. As a consequence, this task is typically cast as an OOD classification problem.\nOOD detection comprises various types of shifts in data. (a) Semantic shifts, such as encountering unseen classes, and (b) domain shifts, like distinguishing between real images and drawings, have easily established boundaries and are well-defined in literature [17, 28]. On the other hand, (c) covariate shifts, which involve perturbations in data or subtle changes in its expected variability, are often conflated with domain shifts [63]. It is essential to differentiate covariate shifts, since they pose unique challenges requiring tailored detection mechanisms.\nFigure 1 illustrates the proposed framework for interpreting shifts in a data distribution. In our definition, the ID range covers an expected semantic shift, containing a pre-defined number of different classes, as exemplified by ImageNet-1K [47], along with some degree of variability in terms of domain and covariate shifts. For instance, introducing a novel class such as bagpipes in NINCO [3] represents an OOD semantic shift, as ImageNet-1K lacks such examples. An extreme change in domain, such as a hand-drawn representation of a plane from the Sketch dataset [10], is considered OOD, despite the retaining of semantic relevance. Additionally, substantial covariate shifts, such as a blurred horse image from ImageNet-1K(-C) [16], are also classified as OOD, even though there are no explicit alterations in semantic or domain concepts."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Semantic Shift and Covariate Shift OOD", "content": "OOD detection literature predominantly focuses on semantic shift and typically falls into two categories: (a) supervised, which requires labels or OOD data, and (b) unsupervised, which relies solely on ID data [63]. Given the nature of the OOD detection problem, OOD data are often not sufficiently representative, as OOD samples can come from a wide variety of unknown distributions. As such, unsupervised methods are generally preferred. Covariate shift occurs when images have consistent semantic and domain content, but are recorded under deviating imaging settings and conditions, or corrupted in a post-processing step. Although increasing the degree of variance under these conditions can deteriorate semantic and domain content, this study focuses on covariate shifts within the same domain, as these subtle distribution shifts can cause significant drops in the classification performance of machine learning models [16]."}, {"title": "2.2. Generative-based Methods", "content": "A widely used and initially intuitive approach for OOD detection involves fitting a generative model p(x; \u03b8) to a data distribution x and evaluating the likelihood of unseen samples under this model, assuming that OOD samples will have lower likelihoods [2]. However, this assumption has been challenged, with various generative models assigning higher likelihoods to certain OOD samples [18, 37]. To address this, different approaches have been proposed, including using the Watanabe-Akaike Information Criterion (WAIC) [5], specific likelihood ratios [51, 59], and hierarchical VAEs [14]. These methods aim to correct for likelihood estimation errors, population-level background statistics, and model feature dominance. Another approach suggests labeling samples as OOD if their likelihoods fall outside the typical range of a model [4, 54], i.e., a sample may be classified as OOD not only if its likelihood is lower than that of ID data, but also if it is higher [36]."}, {"title": "2.3. Reconstruction-based Methods", "content": "Reconstruction-based methods involve training a model R to reconstruct inputs x from the training distribution, such that we obtain x = R(x). The rationale is that if R has an information bottleneck, it will struggle to accurately reconstruct OOD inputs. However, these methods face practical challenges, including difficulty in tuning the information bottleneck size [8, 41]. If it is too small, ID samples may not be faithfully reconstructed; if it is too large, the model can learn the identity function, allowing OOD samples to be reconstructed with low error. Some approaches address these issues by using the Mahalanobis distance in the Autoencoder's feature space as an OOD metric [8], or by introducing a memory module to discourage OOD sample reconstruction [11]. However, none of these methods fully resolve the bottleneck selection issue. To tackle this limitation, DDPMs have been employed, leveraging noise bottlenecks [57] and reconstructions from a range of noise values without the need for dataset-specific tuning [13] or of corrupted inputs [34]."}, {"title": "2.4. Feature-based and Logit-based Methods", "content": "Several scoring functions have been devised to differentiate between ID and OOD examples, leveraging characteristics of ID samples, but not represented in OOD ones, and vice versa. These functions primarily stem from three sources: (1) probability-based measures, such as maximum softmax probabilities [17], and minimum Kullback-Leibler (KL) divergence between softmax and mean class-conditional distributions [19]; (2) logit-based functions, including maximum logits [19], and the use of the logsumexp function computed over logits [33]; (3) feature-based functions, involving the norm of the residual between a feature and its low-dimensional embeddings [38], as well as minimum Mahalanobis distance between a feature and class centroids [27]. Some hybrid methods combine both logit and feature scores for OOD detection [55], while more recent works have introduced masked image modeling pretraining into OOD detection with promising results [29, 30]. However, the detection speed of these methods is severely constrained by their large transformer-based backbones."}, {"title": "2.5. Adversarial Variational Autoencoders", "content": "The VAE [22] consists of an encoder that predicts the parameters \u03bc and \u03c3 of the variational distribution of the input data, and a decoder that takes a sample from this distribution to reconstruct the input. VAEs are trained to maximize the Evidence Lower Bound (ELBO), which balances reconstruction fidelity with the latent space regularization to ensure that it follows a predefined probability distribution. Using latent space as a bottleneck restricts the information that can pass through, leading to uncertainty and blurriness in the reconstructions [7]. Additionally, the pixel-wise reconstruction error and the high dimensionality of natural image manifolds pose challenges for VAEs in generating high-quality and realistic samples. While natural images are assumed to lie on low-dimensional manifolds due to local scale redundancy [25], local details exist in higher-dimensional manifolds, making them difficult to capture.\nGANs [12] consist of two neural networks with adversarial objectives: the generator learns to map a random vector to the data space; the discriminator acts as a classifier trained to differentiate real samples from generated ones. Despite their success in generation tasks, GANs suffer from two primary limitations compared to VAEs. The first is mode collapse, which occurs when the generator produces only a few different types repeatedly, making it easily recognizable by the discriminator. Consequently, the discriminator's feedback lacks useful information [52]. Additionally, GANs lack an encoder network, which restricts their ability to reconstruct an input or manipulate its latent representation. AnoGAN [50] tries to circumvent this by optimizing a random latent vector to match a test sample and determining an anomaly score based on reconstruction quality and the discriminator's output.\nThe VAE and GAN have been combined by incorporating a discriminator to enhance the realism of VAE reconstructions [26]. Alternatively, the BiGAN [9] architecture features an encoder, generator, and discriminator, aiming for good unsupervised feature representations but tends to produce less accurate reconstructions. Other approaches have adapted this VAE/GAN combination to fully utilize the strengths of each architecture to improve the realism of the images produced by the model [43]. DisCoPatch aims to retain the adversarial benefits and the mode coverage of the hybrid strategy, without the final goal of image generation, thereby reducing computational requirements."}, {"title": "2.6. Batch Normalization", "content": "Batch Normalization [21] is a widely used technique to speed up the training process of deep neural networks. It became popular for GAN architectures after being utilized in the DCGAN model [44] for both the generator and the discriminator. Essentially, BN takes a batch of feature samples {$X_1,X_2,...,X_m$} and computes\n$Y_i = \\frac{X_i - \\mu_B}{\\sigma_B} \\cdot \\gamma + \\beta$,\nwhere, during training, $\u03bc_R$ and $\u03c3_R$ are the running mean and the running standard deviation, which are updated according to the input batch statistics $\u03bc_B$ and $\u03c3_B$ with a non-trainable momentum parameter m by\n$[\\mu_R , \\sigma_R] = (1-m)[\\mu_R , \\sigma_R] + m[\\mu_B , \\sigma_B]$.\nHere, \u03b3 and \u03b2 are learned parameters. It should be noted that when the BN layer is set to evaluation mode, $\u03bc_R$ and $\u03c3_R$ are fixed to the values learned throughout training (i.e. parameter m in Eq. (2) is set to zero).\nCompared to models without normalization, BN accelerates training in the early stages and leads to better performance in GANs. However, the distinct distributions of clean and adversarial samples have hindered BN's effectiveness in adversarial settings [56, 61], as models trained with BN can still suffer from instability and low generalizability [58]. Alternative normalization approaches, such as Weight Normalization [48] and Spectral Normalization [35], have demonstrated improved performance for image generation and benefits in training stability, while still accelerating GAN training."}, {"title": "3. DisCoPatch", "content": ""}, {"title": "3.1. Overview", "content": "A prevalent generative-based approach for OOD detection involves utilizing the trained generator to evaluate the likelihood of unseen samples. However, in adversarial setups, some information about the ID boundary will be incorporated into the discriminator, as it learns to assess the probability of a sample being real (ID) or synthetic (OOD). As mentioned in Section 1, in this paper, we exploit the observation that BN can help an adversarially trained discriminator to separate underlying data distributions by recognizing that clean and adversarial images are drawn from two distinct domains (i.e. ID and OOD), in such way that that it can provide a boundary for the ID set. By adjusting where the discriminator learns to draw this boundary, we can create an OOD detector.\nIt is on this premise that we propose a Discriminative Covariate Shift Patch-based Network, DisCoPatch. DisCoPatch is an Adversarial VAE-inspired architecture, as shown in Figure 2, in which both the VAE and the discriminator are trained adversarially. DisCoPatch's approach combines generative and reconstruction-based strategies to distill information about the ID set and OOD boundaries to the discriminator during training in an unsupervised manner. Unlike traditional adversarial methods, DisCoPatch's focus is on leveraging the generator's output as a tool to refine the discriminator. DisCoPatch's discriminator only utilizes the current batch's (of patches) statistics in the BatchNorm2D layer (i.e. parameter m in Eq. (2) is set to one). Subsection 4.3 details extensive ablation experiments to demonstrate its effectiveness."}, {"title": "3.2. Training", "content": "The VAE is trained to reduce the standard ELBO loss, while also producing samples (generated patches using the VAE decoder) that can fool the discriminator. The discriminator is trained to not only distinguish between generated and real patches, as in the standard GAN setup, but also reconstructed patches. Reconstructions from VAEs typically lack detail, i.e., they have a sub-optimal high-frequency representation [32], which can be found in certain types of covariate shifts, such as blurriness. On the other hand, images generated from GANs often exhibit severe high-frequency differences, leading the discriminator to focus excessively on these components [31]. This focus can hinder the generator's ability to capture low-frequency components. By training the discriminator on reconstructions and generations, and encouraging both to appear more realistic, the discriminator's boundaries of the ID frequency spectrum become tighter, strengthening its ability to detect OOD samples, as illustrated in Figure 3.\nThe VAE in DisCoPatch's framework remains unchanged compared to the traditional VAE, with parameters \u03b8 and composed of an encoder $E_\u03b8$ and a decoder $G_\u03c6$ responsible for generating an image output. The VAE is a parameterized model given by $q_\u03b8(z|x^{(i)}) = N(z; \u03bc^{(i)}, \u03c3^{2(i)}I)$, where $\u03bc^{(i)}$ and $\u03c3^{2(i)}$ are outputs of $E_\u03b8$. The prior distribution of the latent codes is $p(z) = N(z; 0, I)$. The VAE loss function combines a reconstruction term and a latent space regularization term, as demonstrated in the original paper by [22] and adversarial implementations [43]. The reconstruction term optimizes the encoding-decoding process, while the regularization term aligns the encoder distributions with a standard Gaussian. The latter is represented by the KL-divergence between the predicted distribution and the prior distribution. Both terms are represented in Figure 2 and can be written as\n$L_{VAE} = ||x^{(i)} \u2013 G_\u03c6(z)||^2 $\n$+\\frac{1}{2} \\sum_{j=1}^{dim(z)}(1 + log(\u03c3_j^{2(i)}) \u2013 \u03bc_j^{2(i)} \u2013 \u03c3_j^{2(i)})$,\nAn additional model, the discriminator $D_\u03c8$, parameterized by \u03c8, is added to the traditional VAE architecture. It has two main goals, as shown in Figure 2. First, it must discern between real patches and patches either reconstructed from $z_{real}$ or generated from random noise $z_{fake}$. This can be achieved by minimizing the cross-entropy function\n$L_D = E_{x \\sim p_{data}(x)}[log (1 \u2013 D_\u03c8(x))]$\n$+ E_{x \\sim p_{G_\u03c6} (x|z_{real})}[log (D_\u03c8(x))]$\n$+ E_{x \\sim p_{G_\u03c6}(x|z_{fake})}[log (D_\u03c8(x))]$.\nThis suggests that in addition to the discriminator's initial goal of improving generated patches (sampled from random noise), it also pushes the reconstructions toward more realism. Therefore, an adversarial loss term, which encourages the VAE to generate or reconstruct patches that fool the discriminator, is added to the loss function, so that\n$L_{Adv} = E_{x \\sim p_{G_\u03c6}(x|z_{real})}[1 \u2013 log (D_\u03c8(x))]$\n$+ E_{x \\sim p_{G_\u03c6}(x|z_{fake})}[1 \u2013 log (D_\u03c8(x))]$.\nThe final DisCoPatch loss function is thus a weighted combination of both the Vanilla VAE loss and the adversarial loss, which results in\n$L_{DCP} = ||x^{(i)} \u2013 G_\u03c6(z)||^2$\n$+ \\frac{w_{KL}}{2} \\sum_{j=1}^{dim(z)}(1 + log(\u03c3_j^{2(i)}) \u2013 \u03bc_j^{2(i)} \u2013 \u03c3_j^{2(i)})$ \n$+ w_{Rec} E_{x \\sim p_{G_\u03c6}(x|z_{real})}[1 \u2013 log (D_\u03c8(x))]$\n$+ w_{Gen} E_{x \\sim p_{G_\u03c6}(x|z_{fake})}[1 \u2013 log (D_\u03c8(x))]$."}, {"title": "3.3. Patching Strategy", "content": "The patching strategy begins by taking a high-resolution input image, typically a standard 256\u00d7256 resolution, and cropping it into N random patches, each of size 64\u00d764. This approach allows the model to capture fine-grained details in different image regions. During training, batches are composed of patches sourced from multiple images rather than from a single one. This setup accelerates training and ensures that the model learns consistent ID features across a range of images, minimizing the potential for overfitting on individual image characteristics. During inference, however, each batch is constructed from N patches taken from the same image to achieve independent results per image. The final anomaly score for an image is the mean of the scores of all patches within that same image. For the remainder of the paper, we refer to the model as DisCoPatch-N, indicating the number of patches per image used during inference. Both strategies are illustrated in Figure 4."}, {"title": "4. Experiments & Methodology", "content": ""}, {"title": "4.1. Datasets", "content": "In OOD detection benchmarks, the conventional approach involves designating an entire dataset as ID and then compiling multiple datasets that lack any semantic overlap with the ID categories to act as OOD sets. To ensure consistency in the benchmarking process, we adhere to the methodology proposed by OpenOOD [64]. Our evaluation encompasses three tasks: (1) Near-OOD, which exhibits slight semantic variation compared to ID datasets; (2) Far-OOD, which encompasses both semantic and domain shifts; and (3) Covariate Shift OOD, involving corruptions within the ID set. ImageNet-1K [47] was defined as the ID dataset. Further details on the datasets are summarized in Appendix A."}, {"title": "4.2. Evaluation Metrics", "content": "The evaluation metrics employed in OpenOOD by [64] are adopted for this work. These two main evaluation metrics are: (1) AUROC, which measures the area under the Receiver Operating Characteristic (ROC) curve, and displays the relationship between True Positive Rate (TPR) and False Positive Rate (FPR); and (2) FPR95, which measures the FPR when the TPR is equal to 95%, with lower scores indicating better performance. The full results are provided in the form \"AUROC/FPR95%\"."}, {"title": "4.3. Batch Normalization Bias Analysis", "content": "We conduct various experiments to investigate the effect of Batch Normalization's reliance on the batch statistics $\u03bc_B$ and $\u03c3_B$ (Eq. 2) during training. We conduct the experiments using the complete ImageNet-1K dataset, center-cropped and resized to 256 \u00d7 256 pixels, within the DisCoPatch framework. Unlike our main experiments that rely on patch-based batches, here we evaluate the model's sensitivity to batch-level statistics across the full-resolution images. This model was used with the standard BatchNorm2D parameters, with track_running_stats set to True and the default momentum of 0.1.\nIn the first experiment, the model is analyzed in evaluation mode, as described in Subsection 2.6. In this mode, the model utilizes the running mean, $\u03bc_R$, and variance, $\u03c3_R$, learned during training and neglects the statistics of the current batch. As mentioned in Section 1, this paper hypothesizes that the batch statistics of the batch being evaluated can be a powerful feature for OOD detection. Consequently, we also introduce an additional model, in which the track_running_stats option in PyTorch's BatchNorm2D layer is set to False. This setting causes the model to disregard the running mean and variance obtained during training, and instead, it employs solely the statistics $\u03bc_B$ and $\u03c3_B$ of the batch being tested for normalization. It can be noted that this setting does not update any of the layer weights, serving only as a tool to demonstrate the reliance on batch-level statistics during inference."}, {"title": "4.4. Baseline Models", "content": "For the ImageNet-1K benchmark, we compare our method, DisCoPatch, against SOTA public models, such as MOODv2 [30], NNGuide [39], and SCALE [62], all of which have demonstrated SOTA performance\u00b9 on Near-OOD and Far-OOD detection for these datasets. These models employ feature-based and logit-based strategies to perform OOD detection. In the case of NNGuide, we evaluate three of its available backbones to assess performance across different model types: the top-performing RegNet [45], a more efficient ResNet-50 [15], and the lightweight MobileNetV2 [49]. The implementation details are provided in Appendix B."}, {"title": "4.5. Experimental Significance", "content": "To support the main claims of this work, we perform five random runs to validate DisCoPatch's performance on the OOD benchmarks and compute the average performance."}, {"title": "5. Results", "content": "This section presents the key results of the conducted experiments. First, we demonstrate that BatchNorm exhibits a substantial reliance on batch statistics rather than the running mean and variance learned during training. Second, we showcase the results covering Near-OOD, Far-OOD, and Covariate Shift OOD detection performance. Additional detailed results are in the supplemental materials."}, {"title": "5.1. Batch Normalization Bias", "content": "Table 1 reveals a critical limitation in the model's behavior when evaluated with the BatchNorm employed in its standard evaluation mode: the model fails to distinguish between ID and OOD samples reliably. In contrast, when disabling the use of the learned statistics and instead using batch-specific statistics, the model's performance improves significantly, even with a batch size of 1. This effect demonstrates that the running statistics acquired during training are ineffective for discriminating ID from OOD, while the test batch statistics provide more discriminating power for detecting OOD samples. It should be noted that as the batch size increases, this improvement becomes more pronounced, which indicates that the model has developed a dependency/shortcut on batch-specific statistics, instead of leveraging the running mean and variance acquired during training. This means that the use of BatchNorm's running statistics compromises robustness, as it has been observed in adversarial and OOD scenarios [1, 56]."}, {"title": "5.2. ImageNet-1K Benchmark", "content": "Table 2 shows that DisCoPatch-64 surpasses state-of-the-art methods in Near-OOD and Covariate Shift benchmarks, with a particularly large performance gap in the Covariate Shift OOD detection task. Although DisCoPatch does not achieve SOTA performance in any of the Far-OOD benchmarks, it is close to matching the best performers on the iNaturalist and DTD tasks and attains competitive performance in OpenImage-O, whilst being a much smaller model. Detailed performance analysis for Covariate Shift can be found in Appendix D."}, {"title": "6. Discussion", "content": "Covariate Shift OOD. Validation on ImageNet-1K(-C) reveals that DisCoPatch achieves a substantial performance improvement over other models in this task. As hypothesized in Section 3, and supported by recent findings [31, 32], training a discriminator with VAE reconstructions enhances sensitivity to corruptions that diminish the high-frequency spectrum. This effect arises because reconstructed images generally lack high-frequency content, prompting the discriminator to classify this lack of content as \"fake\" and its presence as \"real.\" In contrast, training the discriminator with generated images strengthens its ability to detect high-frequency amplification. DisCoPatch's unique unsupervised training approach - exposing the discriminator to both generated and reconstructed patches - enables robust detection of both low- and high-frequency perturbations, leading to DisCoPatch's consistent performance across diverse corruptions.\nNear-OOD and Far-OOD. The covariate shift-focused training strategy effectively tightens the boundary between ID and OOD samples, improving detection performance on both Near-OOD and Far-OOD datasets. DisCoPatch outperforms existing models, achieving SOTA performance in Near-OOD detection. Although the same does not occur for Far-OOD, DisCoPatch's performance is competitive, and the model is only beaten by far larger and slower models, like MOODv2 and NNGuide with the RegNet backbone.\nDeployment. There are generally two main deployment scenarios for OOD detection algorithms: (1) The OOD detection algorithm is the primary focus, deployed as a standalone application. (2) The OOD detection algorithm operates alongside a main image processing algorithm, ensuring its safe and effective use. An OOD algorithm must be practical and effective in real-world scenarios, delivering strong detection performance while being highly deployable. Deployability should be assessed in the following aspects.\n1. Accessibility: Evaluated by the compute requirements necessary for the algorithm.\n2. Development Cycle: Measured by the time required for model training and deployment.\n3. Inference Speed: The time it takes for the algorithm to make predictions during deployment.\n4. Accuracy: The ability of the algorithm to provide highly accurate OOD detection.\nAn ideal OOD detection algorithm excels in all the above dimensions, ensuring it can be effectively utilized in various practical applications. As detailed in our results and Appendix C, DisCoPatch excels in all criteria. The model achieves SOTA OOD detection results, while utilizing substantially smaller and faster models. Table 5 indicates that DisCoPatch is 1 order of magnitude faster than the other evaluated models; being up to 12 times faster than MOODv2 and up to 19 times quicker than NNGuide. Training a tailored model is also efficient and flexible, as demonstrated in Table 4, making it a strong candidate for applications that require fast development cycles."}, {"title": "7. Limitations & Future Work", "content": "Comprehensive benchmarking of Covariate Shift detection across diverse architectures is vital to advance the field. While DisCoPatch shows promise for generative-based setups, evaluating more models will clarify how architecture impacts detection performance. Covariate Shift detection is particularly critical in fields like medical imaging, where identifying distribution shifts is critical for ensuring trustworthy predictions. Expanding such evaluations to high-stakes domains could expand the relevance of DisCoPatch and similar models for robust, real-world applications.\nWhile current results demonstrate the effective use of batch statistics for OOD detection, a signal-processing-focused analysis of feature propagation and suppression at each layer could provide key insights. This could reveal patterns specific to different shifts, improving model interpretability. We also aim to test more complex discriminator architectures to further explore these dynamics."}, {"title": "8. Conclusion", "content": "This paper introduces DisCoPatch, an unsupervised, lightweight framework for OOD detection. DisCoPatch, unlike traditional VAE or GAN training objectives, uses a combination of reconstructed and generated images to address a wide range of frequency-spectrum perturbations. Furthermore, this study sheds light on the inherent bias in Batch Normalization toward batch statistics. DisCoPatch effectively exploits this bias for competitive OOD detection, particularly for Covariate Shift and Near-OOD detection. DisCoPatch achieves SOTA performance on public OOD detection benchmarks. Specifically, it excels in Covariate Shift detection, achieving an AUROC of 95.5% on ImageNet-1K(-C) and outperforming all prior methods on Near-OOD detection with a score of 95.0%. DisCoPatch achieves these results with significantly lower latency (up to one order of magnitude) and a model size of < 25MB, making it a viable option for real-time applications with limited resources."}, {"title": "B. Implementation Details", "content": "This appendix provides details of the implementation of the models employed in the paper for better reproducibility."}, {"title": "B.1. MOODv2", "content": "For the implementation of MOODv2, we follow the guidelines provided in [30]. Moreover, we have employed their official repository under no official license. This work reduces the complexity of MOODv1 [29] while increasing the detection performance. In MOODv1, three steps are required. First, the Masked Image Modeling Vision Transformer (ViT) is pretrained on the ImageNet-21k [47] dataset. The second step involves fine-tuning the ViT on the same dataset. The third and final step consists of fine-tuning the ViT on the ID dataset. It can be observed that this process is costly when dealing with a substantial number of ID datasets. However, through experimental validation, MOODv2 has demonstrated that a well-prepared masked image modeling model does not require additional fine-tuning.\nThe selected encoder for MOODv2 is a BEiTv2 [40], which is already pre-trained and fine-tuned on ImageNet-21k, as provided in the aforementioned repository. Regarding the OOD score function, following the author's recommendations, ViM [55] is utilized, which merges features and logits extracted from the trained image encoder. Here, $l_i$ represents the i-th logit of feature x in the training set X; \u03b1 denotes a model-specific constant; R (with dimensions N \u00d7 (N \u2212 D)) corresponds to the portion of the eigenvector matrix Q of X, ranging from the (D + 1)-th column to the last, where N stands for the principal dimension; and C signifies the number of classes. Mathematically, the score can be expressed as\n$s(x) = \\frac{1}{C} \\sum_{i=1}^{C} \\frac{e^{\\alpha l_i}}{ \\sum_{i=1}^{C}e^{l_i} + \\alpha  \\sqrt{x^T R R^T x}}$."}, {"title": "B.2. SCALE", "content": "When employing the SCALE framework, we have followed the approach described in [62]. Moreover, we have used with the code made publicly available. It should be noted that SCALE is a post-hoc enhancement method for OOD detection which focuses on scaling network activations, rather than pruning them. The scaling factor applied to the activations r is applied uniformly across all features, preserving the model's logit ordinality and maintaining ID accuracy. Mathematically, the calculation of the logits with the scaled activations can be formulated as\n$z' = W \\cdot (a \\oslash sf(a)) + b$, where sf(a); = exp(r)."}, {"title": "B.3. NNGuide", "content": "For the implementation of NNGuide, we have followed the guidelines in [39]. Moreover, we also have employed the publicly available code , which is released under the Apache 2.0 License.\nThe technique NNGuide is a post-hoc, training-free inference method designed to improve classifier-based OOD detection scores by leveraging nearest neighbors in the ID dataset. This method aims to mitigate the overconfidence issue in Far-OOD samples while preserving fine-grained detection for Near-OOD instances. This is achieved by augmenting a classifier's confidence score, $S_{base}(x)$, by using a guidance term G(x), which is the confidence-weighted average similarity of the nearest neighbors. This term ensures that the score respects the data manifold's boundary geometry. The guidance term is defined as the average similarity between the test input and its high-confidence nearest neighbors and can be formulated as\n$S_{NNGuide}(x) = S_{base}(x) \\cdot G(x)$."}, {"title": "B.4. DisCoPatch", "content": "DisCoPatch (our proposed method) is an Adversarial VAE, which is composed of a VAE and a Discriminator. The VAE features an Encoder ($E_\u03b8$), consisting of convolutional layers with a kernel size of 3, stride 2, padding 1, and output padding of 1. All the convolution layers are followed by BN and a LeakyReLU activation function. The number of filters doubles with each layer. Encoded features are then flattened and passed through two distinct fully connected layers, one estimating $z_\u03bc$ and the other $z_\u03c3$, with outputs the size of the latent dimension. These outputs undergo the reparametrization trick to generate z, which is then fed into the VAE's decoder, referred to as the Generator ($G_\u03c6$). The Generator comprises transposed convolutions, followed by BN and a LeakyReLU activation, with the same kernel size, stride, padding, and output padding as the Encoder. The number of filters halves after each layer. A final convolutional layer with a kernel size of 3 and padding of 1, followed by a Tanh activation, generates the final output image. The generated image is subsequently fed into a Discriminator ($D_\u03c8$). The Discriminator shares the same architecture as the Encoder but replaces the two fully connected layers with a single one that generates an output of size 1, followed by a Sigmoid activation. Additionally, for its recommended setup, track_running_stats is set to False in the Discriminator. The training process is covered in detail in Subsection 3.2, but can be summarized by Algorithm 1.\nDisCoPatch is optimized using the Adam optimizer, with $\u03b2_1$ = 0.9 and $\u03b2_2$ = 0.999. Both models share the same learning rate, lr. As shown in Equation 6, three weighing"}, {"title": "C. Compute Resources", "content": "This appendix describes the computational resources employed for inference in the selected models and to train DisCoPatch."}, {"title": "C.1. Training", "content": "DisCoPatch was trained on ImageNet-1K using a system equipped with an NVIDIA H100 Tensor Core GPU (94 GB VRAM), a 32-core, 64-thread AMD EPYC 9334 CPU, and 7"}]}