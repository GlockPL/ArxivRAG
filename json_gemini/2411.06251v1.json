{"title": "Quasi-random Multi-Sample Inference for Large Language Models", "authors": ["Aditya Parashar", "Aditya Vikram Singh", "Avinash Amballa", "Jinlin Lai", "Benjamin Rozonoyer"], "abstract": "Large language models (LLMs) are often equipped with multi-sample decoding strategies. Vilnis et al. (2023) show that an LLM implicitly defines an arithmetic code book, facilitating efficient and embarrassingly parallelizable arithmetic sampling to produce multiple samples using quasi-random codes. Traditional text generation methods, such as beam search and sampling-based techniques, have notable limitations: they lack parallelizability or diversity of sampled sequences. This study explores the potential of arithmetic sampling, contrasting it with ancestral sampling across two decoding tasks that employ multi-sample inference: chain-of-thought reasoning with self-consistency and machine translation with minimum Bayes risk decoding. Our results demonstrate that arithmetic sampling produces more diverse samples, significantly improving reasoning and translation performance as the sample size increases. We observe a 3-5% point increase in accuracy on the GSM8K dataset and a 0.45-0.89% point increment in COMET score for WMT19 tasks using arithmetic sampling without any significant computational overhead.", "sections": [{"title": "1 Introduction", "content": "There have been enormous efforts in improving the performance and efficiency of inference with large language models (Ippolito et al., 2019; Su and Collier, 2023; Grubisic et al., 2024; Zhou et al., 2024; Ding et al., 2024) based on system, data, and model level enhancements. In this paper, we consider that any decoding routine can be broadly assessed by its sample diversity (A.1) and parallelizability. Search-based techniques like beam search can approximate maximum a posteriori (MAP) decoding, mitigating duplicate samples at the expense of not being embarrassingly parallel. Sampling-based methods grounded in ancestral sampling techniques are parallel but don't explicitly guarantee diverse sequences. The recently proposed arithmetic sampling (Vilnis et al., 2023) technique enables parallel inference with diverse samples \u2013 by interpreting the inference as sampling from code points from a unit interval, given code points generating sequences becomes embarrassingly parallel and the sample diversity is guaranteed by construction.\nDecoding from pre-trained LLMs requires varying strategies for different downstream tasks. For complex reasoning and question-answering tasks, chain-of-thought (CoT) prompting (Wei et al., 2022) is established for improving inference by instructing the model to generate intermediate reasoning paths. Wang et al. (2023) propose self-consistency as an additional improvement over chain-of-thought reasoning with multi-sample inference, attributable to diverse reasoning paths enhancing the confidence of the majority answer. For machine translation, minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004) is a classical approach for selecting the optimal translation from candidate translations generated by an LLM, requiring diversity to ensure performance.\nThus, the inherent diversity of sequences generated via arithmetic sampling offers significant potential for enhancing decoding strategies that rely on multi-sample inference. Recognizing the importance of exploring this approach, we apply arithmetic sampling to both reasoning and translation tasks. For CoT reasoning with self-consistency and machine translation with MBR decoding, we observe accuracy improvements on the GSM8K and Commonsense QA datasets, along with substantial COMET score gains as the number of sampled sequences increases."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Arithmetic sampling", "content": "Arithmetic sampling (Vilnis et al., 2023) reinterprets the standard ancestral sampling process as lazily constructing an arithmetic codebook in the unit [0, 1) interval where each code (point) is uniformly distributed and corresponds to a sequence from the output distribution.\nThis process ensures that the generated samples are diverse, as codes far apart in the code book usually correspond to different token prefixes. Moreover, it is embarrassingly parallel across the N samples, since the ith sample can be generated independently given its code $c_i$. Arithmetic sampling can also be applied orthogonally to other sampling-based techniques that directly manipulate the next token distribution, such as top-k, top-p (nucleus) (Holtzman et al., 2020), temperature sampling and epsilon sampling (Hewitt et al., 2022)."}, {"title": "2.2 Self-consistency", "content": "Self-consistency (Wang et al., 2023) is a method designed to improve the performance of chain-of-thought (CoT) reasoning by generating multiple reasoning paths with answers for a given prompt and then selecting the most consistent answer based on, generally, majority voting. This approach leverages the diversity of the generated candidate reasoning paths to identify the most frequent outcome, thereby enhancing the accuracy of the final answer."}, {"title": "2.3 MBR decoding", "content": "Minimum Bayes risk (MBR) (Eikema and Aziz, 2020) is based on the principle of maximizing the expected utility of a given hypothesis. When making predictions, we lack information about the ideal (target) translations and must make decisions under uncertainty. MBR allows the model to probabilistically estimate ideal decisions as it searches for the candidate that maximizes expected utility. We used COMET (Rei et al., 2020) as the utility metric.\nWe use the sampling-based approximation to MBR decoding as posited in Eikema and Aziz (2022), using the Monte Carlo estimate and formulating the candidate space from the generated N (pseudo-reference) samples:\n$y^{N-by-N} = \\frac{1}{N} \\sum_{n=1}^{N} u(y^{(n)}, h)$"}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Datasets", "content": "We conduct experiments with only validation splits of datasets aimed at chain-of-thought reasoning and neural machine translation. We use GSM8K (Cobbe et al., 2021) and CommonsenseQA (Talmor et al., 2019) for chain-of-thought self-consistency; and WMT19 De \u2013 En (German-English) and Ru-En (Russian-English) (Barrault et al., 2019) for MBR decoding machine translation experiments."}, {"title": "3.2 Models and Baselines", "content": "In our experiments, we employ Gemma-7B (Team et al., 2024), and Llama-2-7B (Touvron et al., 2023) models for CoT self-consistency; and MT0 (Muennighoff et al., 2023) for De-En and Ru-En tasks, Flan-T5 (Chung et al., 2022) for De-En task with MBR decoding.\nIn addition to greedy decoding, we adopt ancestral sampling as our baseline method to compare with arithmetic sampling for sampling reasoning paths in self-consistency and pseudo-reference translations in MBR decoding. We include additional sampling strategies such as temperature, top-k, nucleus, and epsilon sampling which are orthogonal to ancestral and arithmetic sampling."}, {"title": "3.3 Results", "content": "Subsampling We perform subsampling for all the models and datasets to estimate performance and variance for sampled sequences less than a given N which is 40 for most of our results. For both ancestral and arithmetic sampling, we averaged over multiple runs, sampling sequence sets of lengths {$d \\in N$ such that $d| N$} from a pool generated by runs with n samples. For arithmetic sampling, we randomly select an offset for each dataset instance and pick indices in fixed intervals to simulate selecting quasi-random samples from a codebook. For ancestral sampling, we randomly sample from all elements in d with replacement. This allowed us to run an experiment for 40 sampled sequences and obtain results for $d\\in$ {1, 2, 4, 5, 8, 10, 20} sequences."}, {"title": "3.3.1 Self-consistency", "content": "We primarily evaluate the performance of the aforementioned sampling techniques with parameters controlled as temperature = 1, top-k = 40, and top-p = 0.95 across 40 sampled reasoning paths per question-answer pair, using the subsampling results to plot the standard deviation as the shaded region in Figures 1 and 2. We select the majority-voting accuracy for self-consistency as our performance evaluation metric. We also analyze accuracy performance across varying n-gram diversity (controlled by setting temperatures from {0.1, 0.3, 0.5, 0.7, 0.9}).\nArithmetic reasoning For GSM8K, which consists of open-ended integral answers for grade-school math problems, we performed an 8-shot evaluation as presented in the literature by Kojima et al. (2023). From Figure 1, we observe significant performance gain from arithmetic sampling over ancestral sampling at 40 sampled se-"}, {"title": "3.3.2 Machine Translation with MBR", "content": "We ran all the experiments on 1000 machine translation examples with different parameters of temperature $T =$ {0.1,1}, top-p = {0.8,1}, top-k = {30, 50}, epsilon cutoff eps = {0.02, 0.05}. We selected COMET (Rei et al., 2020; Freitag et al., 2022) as the evaluation metric. We use subsampling to report the mean COMET score across all the data instances versus the number of sampled sequences = {1, 2, 4, 5, 8, 10, 20, 40}, plotting the standard deviation as the shaded region in Figure 3. All the experiments are performed in a zero-shot setting with a simple prompt \u201cTranslate the following German/Russian sentence to an English sentence.\"\nFigures 3 (a) - (c) show the COMET score of Flan-T5 De-En, MT0 De-En and MT0 Ru-En tasks respectively. Across all COMET score graphs presented (refer Appendix Figure 5 for more plots), it is evident that arithmetic sampling consistently outperforms ancestral sampling as the number of sampled sequences increases. Figures 3 (a), 3 (b), 3 (c) show that arithmetic has 0.89%, 0.48%, 0.45% increase in the mean COMET scores over ancestral at 5 sampled sequences (see Appendix Figure 6) without any significant computational overhead. We conduct the COMET score vs. n-gram diversity analysis on above tasks by varying temperature $T =$ {0.1, 0.3, 0.5, 0.7, 0.9} and plotting the COMET score for 20 sampled sequences (4). As the temperature increases, the diversity of both the sampling techniques increases, thus we chose a higher temperature value of $T = 1$ for reporting the results in Figure 3. The COMET score values for arithmetic outperform those of ancestral, as arithmetic demonstrates greater diversity (4).\nWe also observe that arithmetic sampling has a lower spread than ancestral (Appendix Figure 6). In addition, we conduct the paired t-test at 40 sampled sequences to compare the arithmetic and ancestral COMET scores across 1000 datapoints (Figure 3). The low p-values indicate that the difference in the mean COMET scores of arithmetic and ancestral sampling is statistically significant."}, {"title": "4 Conclusions", "content": "This work investigated the application of arithmetic sampling, a diverse parallelizable sampling technique for multi-sample inference from large language models (LLMs), in chain-of-thought reasoning with self-consistency and machine translation with minimum Bayes risk (MBR) decoding. Experiments on arithmetic reasoning and commonsense reasoning demonstrated that arithmetic sampling improves performance as the number of sampled sequences increases, yielding 3-5% and 1.05% point increases in accuracy, respectively, over ancestral sampling at 40 sampled sequences. In machine translation, arithmetic sampling consistently outperformed ancestral sampling, with improvements ranging from 0.45% to 0.89% in mean COMET scores for 5 sampled sequences across different language pairs and parameter settings. Arithmetic sampling also exhibited a lower standard deviation compared to ancestral sampling. The superior performance of arithmetic sampling without any significant computational overhead is due to its ability to generate more diverse samples, enhancing LLM performance in tasks like chain-of-thought reasoning and"}, {"title": "5 Limitations", "content": ""}, {"title": "1. Limited to random vocabulary ordering:", "content": "As the unit interval associated with arithmetic sampling corresponds to an ordering of the vocabulary tokens (from which sequences are decoded via the code points), we limited ourselves to randomizing the vocabulary ordering and then performing arithmetic sampling. At the expense of some constant overhead, creating an ordering of the vocabulary that follows some measure of semantic similarity may show greater diversity (and potentially, better performance) as the uniform lattice of code points would now be corresponding to semantically dissimilar vocabulary tokens more often \u2013 rendering this idea an interesting future direction."}, {"title": "2. Limited to one dimension:", "content": "In arithmetic sampling, tokens are limited in one dimension space with softmax sampling. It may be beneficial to consider higher dimensional embeddings of the tokens with box embeddings (Vilnis et al., 2018). Diverse sampling in high dimensional embedding spaces may involve advanced sampling techniques like Quasi-Monte Carlo (Caflisch, 1998)."}, {"title": "3. Difference between sampling and maximizing:", "content": "Arithmetic sampling was formulated to generate parallel diverse sequences, which differs from strategies that find the most probable answer like beam search. Our application of arithmetic sampling is limited to multi-sample decoding strategies that require diversity. It may be an interesting future work to extend arithmetic sampling for MAP decoding."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Definition of sample diversity", "content": "The n-gram diversity score is defined as\n$d = \\sum_{n=1}^{4} d_n$ where\n$d_n = \\frac{\\text{# of unique n-grams in N translations}}{\\text{total # of n-grams in N translations}}$"}, {"title": "A.2 Machine Translation with MBR", "content": "In this section, we present more plots on De-En, Ru-En task for Flan-t5 and MT0 models across different parameter setting as shown in the Figure 5.\nIn addition, we also report the mean and standard deviation of performance (COMET) for arithmetic and ancestral sampling for the best performing parameter setting in Figure 6."}]}