{"title": "L3D-Pose: Lifting Pose for 3D Avatars from a Single Camera in the Wild", "authors": ["Soumyaratna Debnath", "Harish Katti", "Shashikant Verma", "Shanmuganathan Raman"], "abstract": "While 2D pose estimation has advanced our ability to interpret body movements in animals and primates, it is limited by the lack of depth information, constraining its application range. 3D pose estimation provides a more comprehensive solution by incorporating spatial depth, yet creating extensive 3D pose datasets for animals is challenging due to their dynamic and unpredictable behaviours in natural settings. To address this, we propose a hybrid approach that utilizes rigged avatars and the pipeline to generate synthetic datasets to acquire the necessary 3D annotations for training. Our method introduces a simple attention-based MLP network for converting 2D poses to 3D, designed to be independent of the input image to ensure scalability for poses in natural environments. Additionally, we identify that existing anatomical keypoint detectors are insufficient for accurate pose retargeting onto arbitrary avatars. To overcome this, we present a lookup table based on a deep pose estimation method using a synthetic collection of diverse actions rigged avatars perform. Our experiments demonstrate the effectiveness and efficiency of this lookup table-based retargeting approach. Overall, we propose a comprehensive framework with systematically synthesized datasets for lifting poses from 2D to 3D and then utilize this to re-target motion from wild settings onto arbitrary avatars. The L3d-Pose dataset can be found at https://soumyaratnadebnath.github.io/L3D-Pose.", "sections": [{"title": "I. INTRODUCTION", "content": "The inherent limitation of 2D pose estimation [1]-[3] is its inability to represent depth, which is critical for accurately understanding the three-dimensional structure of poses in real-world scenarios. As a result, relying solely on 2D coordinates often leads to ambiguities, particularly in occluded or complex body configurations, where similar 2D projections can correspond to entirely different 3D poses. To address these challenges, 3D pose estimation [4], [5] offers a more comprehensive solution by incorporating depth information, enabling a richer and more accurate understanding of movement and biological action. 3D pose estimation is essential for advancing applications in virtual reality, where precise spatial awareness is crucial.\nCurating 3D pose data for animals and primates is challenging due to their unpredictable movements in natural environments. Unlike humans, animals exhibit dynamic, non-repetitive behaviours, and species diversity adds complexity to capturing accurate annotations. Traditional motion capture systems are often impractical, and camera-based methods struggle with occlusion, erratic motion, and limited viewpoints, making large-scale, high-quality 3D pose data in natural settings difficult to obtain.\nSynthetic 3D data provides a valuable solution for 3D pose estimation. By leveraging synthetic environments, models can be trained using abundant 3D data. However, while efficient and diverse, synthetic data may not fully capture real-world dynamics, leading to challenges in generalizing to real-world 3D poses due to differences in appearance and lighting.\nIn our approach, we first train models on naturally available 2D datasets to predict accurate 2D poses. We then use the priors from the synthetically generated 3D pose data to \"lift\" these 2D poses into the 3D space, effectively bridging the gap between 2D and 3D pose estimation in the wild. This method allows us to leverage the abundance of open-source 2D pose data while incorporating depth and spatial insights from synthetic 3D datasets. As a result, we can achieve accurate 3D pose predictions without relying on real-world 3D annotations. Contributions. In summary, our main contributions are: 1. We introduce a complete pipeline to create synthetic datasets encompassing diverse action sets utilizing a physics-based game engine. Following this unified approach, we develop two datasets, namely, deep macaque and deep horse. 2. We propose a lightweight Attention MLP network to lift 2D poses into 3D space independent of image data, ensuring scalability to poses occurring in the wild. Furthermore, we propose a retargeting method to transfer pose and textures onto avatars, enabling seamless graphics applications."}, {"title": "II. RELATED WORK", "content": "3D Pose Estimation. With advancements in motion capture techniques, such as those used by Vicon, Xsens, and OptiTrack, it has become easier to develop data-driven supervised methods for directly regressing 3D keypoints from images. Several methods [1], [4], [6], [7] leverage deep neural networks, such as CNNs and Transformers, to estimate 3D pose. Most research in 3D pose estimation has concentrated on humans, as acquiring 3D data from human subjects is relatively straightforward with established protocols. However, studies on 3D animal pose estimation are limited due to the scarcity of 3D annotations for animals, which is challenging to obtain compared to humans. For animals, multi-perspective methods are commonly used, which involve multi-camera setups for triangulation and the use of calibration boards [5], [8]\u2013[11]. However, these highly data-driven methods require expensive 3D motion acquisition setups, making them difficult to scale to other animal species.\nLifting 2D Pose to 3D Pose. Various approaches initially estimate 2D keypoints using off-the-shelf pose estimation techniques [2], [3], [12]\u2013[14] and subsequently apply methods to predict the 3D pose. The foundational work [15] established the basis for various subsequent studies [16]\u2013[20] that address the inverse projection problem to estimate 3D locations from 2D keypoints. More recently, deep learning approaches have been utilized to address this problem [21]\u2013[31], employing spatiotemporal convolutions and transformer-based methods and conditional random fields. While these methods have been widely used for human 3D pose analysis, their application to animal 3D pose estimation is still relatively unexplored. Our proposed approach generates 3D keypoints solely from 2D keypoints without relying on the input image. The 3D lifting is supervised using synthetically generated data, which is otherwise challenging to obtain. Leveraging anatomical constraints on pose variations and image-independent training, our method achieves realistic 3D reconstructions of 2D keypoints estimated from in-wild images."}, {"title": "III. METHODOLOGY", "content": "The process flow depicted in Figure 1 provides an overview of the underlying methodology. In this section, we describe each step in detail and the rationale behind them.\nKeypoint Selection. In traditional human pose estimation models, a standard set of landmark keypoints is typically used to capture the human skeletal structure effectively [32], [33], typical values being 17 for primates and 22 for animals. However, these standard anatomical locations do not suffice for pose retargeting on avatar models. Several animal datasets, such as [34], [35], have significant gaps, particularly in the representation of the spine and tail regions, which are crucial for accurate pose reconstruction in three-dimensional space. To this end, we have developed a novel set of anatomically important keypoints in a synthetic environment. This expanded keypoint set provides a more comprehensive representation of the primate's full pose, especially with additional key-points for the spine and tail. A comparative analysis with MacaquePose Dataset [34], Horse-10 [36] and our proposed synthetic dataset is presented in Table I. We observe that only a subset of the detected keypoints in the commonly used keypoint system align with anatomical features prevalent in graphics assets. We represent this subset of keypoints as Soft Pose keypoints $K \\in \\mathbb{R}^{k_s \\times 2}$. To address this, we manually add anatomically significant keypoints onto rigged models to prepare synthetic data for transferring poses to 3D avatars. We define this expanded keypoint set $K\\in \\mathbb{R}^{k_a \\times 2}$ as a Deep Pose Keypoint set. As presented in Table I, $(k_s, k_a) = (13, 29)$ and $(k_s, k_a) = (16,33)$ for macaque and horse, respectively. This enhanced keypoint configuration improves pose transfer accuracy and supports precise 3D pose reconstruction.\nDataset Construction. We begin by defining animation sequences on our avatars that mimic naturalistic actions performed by animals in the wild, specifically focusing on Macaques and Horses for this work. We position the subject at the origin within the 3D environment to ensure accurate data collection and then follow a predetermined camera trajectory, capturing frames at a frequency of 10 frames per second. We conduct this process across all defined animation sequences to generate distinct poses, described in Table II. This results in a comprehensive dataset comprising 8,000 instances of Macaques and 6,000 instances of Horses. For realistic rendering and simulating physics-aware animation sequences, we utilize Unity Game Engine. Figure 3(a) depicts the camera trajectory followed to generate renderings. As we follow this trajectory, we randomly adjust environmental settings, the scale of the subject, and the camera perspectives to introduce diverse variations. In addition to capturing images $I \\in \\mathbb{R}^{H \\times W}$, we save the previously discussed manually selected keypoints onto the camera's image space to create a set of 2D keypoints $K_D \\in \\mathbb{R}^{k_s \\times 2}$ for each frame. Here, $k_s$ is the number of selected keypoints defined by the Soft Pose keypoint set. Since image sizes can vary and objects can appear anywhere within the image space, we normalize the coordinates and obtain $K^{2D}$ so that their range extends from [0,1], based on the maximum extent among detected keypoints. The counterpart 3D pose data in world coordinates, denoted as $K_D \\in \\mathbb{R}^{k_a \\times 3}$ is normalized within a unit cube to maintain scale and positional representation uniformity. We store the normalized 3D coordinates in a Look-up Table to assist in inferring deep 3D poses from soft 3D poses (discussed subsequently), as illustrated in Figure 1(b)\nLifting to 3D Pose using Attention MLP. As illustrated in Figure 1(a), the core of our methodology involves the use of a modified Attention-based Multi-Layer Perceptron (Attention MLP) network, which facilitates the lifting of 2D keypoints (x, y) coordinates in image space to a normalized 3D space. We train Attention MLP on training split of synthetic data utilizing soft-keypoint set $K^{2D} \\in \\mathbb{R}^{k_s \\times 2}$ as input and predict an estimate of 3D pose represented by $K^{3D} \\in \\mathbb{R}^{k_s \\times 3}$. We train our network by employing mean square error (MSE) loss between prediction $K^{3D} \\in \\mathbb{R}^{k_s \\times 3}$ and corresponding synthetically generated ground-truth $K^{3D} \\in \\mathbb{R}^{k_s \\times 3}$. It is important to note that the training of our network is image-independent, and learning relies solely on the geometric relationships inherent in the anatomical structure of the subjects. As presented in Table I, we impose a loss on key points belonging to the soft keypoint set, i.e., $k_s$ out of $k_a$ while lifting to 3D. To derive the deep pose necessary for retargeting pose on avatars from the lifted soft pose keypoints, we utilize a pre-generated lookup table, which we explain in detail below.\nDataset Look-up and Keypoint Matching. During inference, given a frame from natural image $I \\in \\mathbb{R}^{H \\times W}$, we apply 2D pose estimation techniques, specifically DeeplabCut [12], pre-trained on the MacaquePose dataset, and the ViT-Pose model [3], pre-trained on APT-36k [35], to detect 2D keypoints, $K_D^{2D} \\in \\mathbb{R}^{k_s \\times 2}$, in the image space, and normalize them to the range [0, 1], resulting in $K^{2D}$. We then extract the $k_s$ keypoints relevant to our task from this set and project them into 3D space using our Attention MLP network to obtain Soft 3D Pose. We perform a nearest-neighbor search from values in our look-up table to find the closest match of the predicted soft 3D pose. This look-up process ensures that the selected set accurately transfers on avatars from an input image. This is illustrated in Figure 1(b).\nPose and Texture Retargeting. Finally, we calculate the angles between each joint in the lifted 3D pose. Using the detailed 3D keypoints and the computed joint angles, we implement a pose transfer mechanism in Unity to ensure that the joint angles of our rigged model match those of the lifted pose.\nDeep 3D pose reveals the animal's 3D posture, camera angle, and scale difference between the real and virtual worlds. Texture appearance is an interaction between the physical structure of the surface (e.g., fur), color pigmentation, and the interplay between the illumination source and 3D surfaces. Texture transfer requires 2D-3D corresopndence and knowledge of the animal's 3D posture, camera angle, and scale difference between the real and virtual worlds, these are revealed by Deep 3D pose. We used the avatar's UV map as a canonical representation for variations in skin structure and used histogram equalization to transfer color tones from the natural image to the avatar. Finally, the rendering process in Unity ensures correct variations in shadows around the avatar's body."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "Efficacy of Lookup Table. Although the proposed simple lightweight Attention MLP network can lift 2D keypoints to 3D efficiently, to transfer pose on avatars, we need dense locations of 2D-keypoints that are not detected by existing pose estimation methods. Since anatomical constraints limit the kinematic variability of joints in animals and humans, we leverage this fact to generate a large lookup table of poses. In Figure 3(b) and Figure 3(c), we illustrate how poses in the lookup tables are effectively clustered, with similar poses forming distinct clusters. We present a visualization of such a cluster in 2D space by plotting the two most dominant principal components of the lookup table. The clustering of similar poses and the distinct separation between clusters help us effectively map soft poses to accurate deep poses using the lookup table. We present qualitative results obtained after each step in Figure 2.\nQuantitative Comparisons. The 2D keypoint coordinate pose data from the Deep Macaque and Horse datasets is divided into a training set (80% of the data) and a validation set (20% of the data). For performance evaluation, we specifically use methods that, like ours, focus on lifting 2D keypoint sets to 3D poses and do not rely on image data. We first train the networks proposed by [23], [29], along with baseline models, on our Deep Macaque and Horse datasets on the training splits. All variants in the table are trained for 100 Epochs. We present quantitative comparisons in terms of Mean Squared Error (MSE) and Percentage of Detected Joint (PDJ) with the proposed Attention MLP network in Table III. Here, PDJ@x signifies a correct keypoint detection if it lies within x xd distance, where d represents the diagonal of the bounding box containing the target and H is the number of heads used in MHSA Block. Methods that directly regress 3D keypoints from input images, such as [1], [4], [7], are generally very resource-intensive and do not match the context of the pose lifting problem, so we do not include them in our comparisons. In addition to comparisons with state-of-the-art methods, Table III also provides quantitative results from an ablation study of various baseline networks. We find that the MLP integrated with the attention module outperforms existing approaches."}, {"title": "V. CONCLUSION", "content": "This work introduced a method for seamlessly retargeting poses from a natural image to 3D virtual avatars. To overcome the challenges of acquiring 3D annotated data for animals, we developed a pipeline to generate synthetic data and develop two datasets, Deep Macaque and Deep Horse (together L3D-Pose dataset), using a variety of action sequences defined on rigged models. Our approach employs a lookup table to map soft poses to deep poses, effectively transferring poses from natural images to synthetic avatars. Additionally, our method lifts 2D keypoints to 3D poses independently of the input image, providing better scalability in scenarios with limited data. Because our approach does not rely on images, it effectively generalizes from training on synthetic data to delivering accurate inferences on the natural images captured in the wild\u00b9."}]}