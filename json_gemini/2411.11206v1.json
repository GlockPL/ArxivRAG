{"title": "Capturing Sparks of Abstraction for the ARC Challenge", "authors": ["Martin Andrews"], "abstract": "Excellent progress has been made recently in solving ARC Challenge problems. However, it seems that new techniques may be required to push beyond 60% accuracy. Even commercial Large Language Models (LLMs) struggle to 'understand' many of the problems (when given the input and output grids), which makes discovering solutions by LLM-lead program search somewhat futile.\nIn this work, LLM \u2018understanding' is attempted from a stronger starting position: An LLM is given complete solutions to tasks in code, and then asked to explain how the task is being solved at various levels of abstraction. Specifically, the LLM was given code solutions implemented in arc-dsl-11m (an LLM-legible version of Hodel's arc-ds1) to obtain: (a) commented code; (b) code refactored into reusable functional chunks; (c) problem solution steps; and (d) high-level problem-solving tactics.\nWe demonstrate that 'Sparks of Abstraction' can be extracted from the LLM output - in a form that could be used in downstream tasks with Local LLMs eligible to enter the ARC Prize.\nBoth the arc-dsl-1lm DSL framework (with the re-engineered solutions) and the Gemini LLM-generated data (along with the generation code) are made Open Source.", "sections": [{"title": "1. Introduction", "content": "The ARC dataset introduced in Chollet (2019) has remained strikingly resistant to the scale-up in compute power that has lead to the quick obsolescence of many other benchmarks. Interestingly, both commercial LLMs (such as GPT-40) and systems that are eligible to enter the ARC Prize competition have arrived at approximately the same scores on the challenge - potentially suggesting that current approaches are unable to grapple with entire classes of problems. One possibility is that there are elements of abstraction and strategic thinking that are missing.\nIn this work, we use Gemini-Flash to 'reason' about known-good code solutions to ARC training set problems. These solutions are expressed in arc-dsl-1lm, which is an adaptation of arc-ds1 released in Hodel (2023), re-engineered to be more \u2018LLM-legible'.\nThe main idea behind our approach is that LLMs (due to their training) have some facility in dealing with code - and this 'understanding' can be captured for processing further downstream. See Figure 1 for an outline of the process. Overall, it appears that \"Sparks of Abstraction\" are present"}, {"title": "1.1. Contributions", "content": "The following are the main contributions of this work\u00b9:\n\u2022 LLM-legible ARC DSL -arc-dsl-llm is a version of arc-ds1 designed to be more readable, with additional fixes for correctness and type-safety. We release both the enhanced DSL code, and solutions for all 400 ARC training tasks, at https://github.com/mdda/arc-dsl-llm\n\u2022 Dataset release - Outputs from Gemini-Flash-002 for the 377 ARC training tasks which have passed the sanity checking process, along with the generation code and DSL manipulation utilities, are made available at https://github.com/mdda/LLM-abstraction-for-ARC"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Core Knowledge", "content": "An often overlooked resource for the abstractions used in the ARC challenge tasks is the original ARC paper (Chollet, 2019). The Core Knowledge described there, building on Spelke & Kinzler (2007), can be seen as the obvious basis for computer approaches at solving the tasks - there is no need to reach for more complex abstractions (since they are unlikely to be accessible to most humans). This idea was further explored in Moskvichev et al. (2023).\nDuring preliminary work, the LARC dataset introduced in Acquaviva et al. (2023) was explored. However, it was found (from the raw data) that humans communicating about the task to be performed were surprisingly bad narrators - and the key results of the LARC paper focused on the best performance on each task, rather than overall performance."}, {"title": "2.2. Domain Specific Language (DSL) for ARC", "content": "One of the foundations of this work is the extraordinary contribution of Hodel (2023), which included both the arc-dsl implementation, and the solution of the 400 training set ARC tasks written using the DSL.\nWhile our methods could be extended to include code writ-"}, {"title": "2.3. LLM Capabilities", "content": "The use of LLMs to solve ARC tasks attracted an initial wave of optimism: Tan & Motani (2023); Wang et al. (2024); Greenblatt (2024).\nHowever, there is now mounting evidence that merely scaling the number of samples is unlikely to be an effective way to solve tasks that involve more abstraction or compositional reasoning Brown et al. (2024)."}, {"title": "2.4. Problem tactics", "content": "The ability of LLMs to reason was explore in Lee et al. (2024), which concluded \"although current LLMs exhibit outstanding performance, they lack logical coherence, compositionality, and productivity in their processes, suggesting that they are closer to probabilistic mimicry rather than possessing autonomous reasoning abilities\". This suggests that a strategy other than generation from scratch is required for extracting 'higher-level' thinking from LLMs. Thus, in this work we start from the basis of extracting these 'higher-level' ideas from code created by humans.\nDrawing from the Self-Discover concept of Zhou et al. (2024), this paper aims to extract useful 'high-level' reasoning structures for ARC tasks (in addition to refactored code examples and Core Knowledge observations)."}, {"title": "2.5. Code Generation", "content": "As observed in Greenblatt (2024), LLMs are limited in their capability of creating new code. This will clearly also hinder program-search using a Local LM (as would be used in the compute-limited Kaggle environment for the ARC Prize competition).\nTherefore, if we want to attempt approaches such as code denoising (Kapur et al., 2024), RL in program space (Butt et al., 2024), or DreamCoder (Ellis et al., 2020), the system would likely benefit from having well annotated code examples, along with high-level goals to act as in-context prompts. This motivates this work's attempt at exploring what is possible to extract from LLMs that can only be accessed 'outside the Kaggle box'.\nHowever, LLM interactions are not the main objective : All the methods here have been developed such that the extracted data can be hosted \u2018inside the Kaggle box' (through RAG, etc). This is also the rationale for this work's title: \"Capturing the Sparks of Abstraction...\""}, {"title": "3. Methods", "content": "In order to get the best results from an LLM, it is essential to play to its strengths (which are, after all, based on reading a huge quantity of text and code from the internet). This Section illustrates how we address (and potentially harness) the priors that the LLM is likely to have.\nFirstly, since LLMs are trained on text that is largely designed to be human readable, text that is less human-readable is likely to be less familiar, and thus more difficult to extrapolate from (i.e. it is also less LLM-legible). So, we assume here that human-legibility can be used as a simple proxy for LLM-legibility (and also note that without training via Reinforcement Learning, LLMs do not have any insight into what would increase LLM-legibility)."}, {"title": "3.1. Core Knowledge", "content": "The original textual description of the Core Knowledge from Chollet (2019) was reworked until the LLM was satisfied that it was clear and interpretable. The full text is given in Appendix B."}, {"title": "3.2. LLM-legible DSL", "content": "Preliminary work with the arc-dsl of Hodel (2023) suggested that not only was the DSL code difficult to read by human coders, but also that there was a significant risk that an LLM would have difficulty. For instance, some of the DSL functions were given names that contradict common Python usage - an example being fork() used to denote a function applied to two different function applications : it was renamed to combine_two_function_results(). Following that, a large number of other similar changes were made. The solutions have also been re-written, and have been validated against the known test solutions in the ARC training set.\nA comparison between the two DSLs is given in Figure 2, and further details about arc-dsl-llm are given in Appendix A. Note that the new solver function solver_virtual (I) returns a dictionary of all the useful intermediate values, so that these can be used in subsequent analysis."}, {"title": "3.3. Actual coded solutions", "content": "This work aims to capitalise on the solutions to the 400 ARC training set problems provided by Hodel (2023). Of course, since complete solutions to 400 problems are available, the LLM does not have to start with blind search, it could be train (for instance) on completions from any point, or on a de-noising task. In addition, each code solution can be assumed to be meaningful (i.e. each line was written with intentionality), which makes the goal of explaining what each line is doing achievable."}, {"title": "3.4. Code comments", "content": "It is common practice for programmers to write code comments to explain what the code is supposed to do to the next viewer. Thus, since LLMs are trained on commented code, and appear to have some skill at writing comments, it is reasonable to hope that an LLM might recognise some 'intentionality' from valid code. Clearly, there may be elements of abstraction being used here (depending on the size of the code block being described)."}, {"title": "3.5. Language Model targets", "content": "Throughout this work, two classes of Language Model have been treated as targets for learning about and making use of abstraction:\n\u2022 Large Language Model the Gemini-Flash-002 model was chosen (after also testing Gemini-Pro), since although it is not a frontier commercial LLM, it is capable of using a long context window, while being an order of magnitude cheaper than frontier models\n\u2022 Local LMS - models that are usable within the constraints of the Kaggle competition run-time container (i.e. 2xT4 with 16Gb GPU RAM each, where we must also factor in approximately 10k tokens of context for the problem description, etc)\nThe dataset released by this work consists of outputs from the Gemini-Flash LLM that have gone through some sanity checks: (a) they have the required number of \u2018Parts' output; (b) the parts are valid Python/YAML as required; plus other factors that are given per-Part in Section 4."}, {"title": "4. Results", "content": "For the outputs shown in the Figures in this Section, we use the illustrative ARC task of Figure 1. This task was chosen for reasons of brevity (rather than being cherry-picked). Note though, that this example also illustrates the way in which many of the solutions from Hodel (2023) are coded in a somewhat unorthodox style (i.e. the code has a very functional style versus a more natural Pythonic approach).\nNote, though, that each ARC task is analysed by the LLM independently, and the LLM used (Gemini-Flash-002, a pinned version) was not fine-tuned on any data. Thus, the outputs shown here have not brought information from other ARC problems to bear - only the Core Knowledge and the DSL functional descriptions (plus the priors that come from the implementations of the solution code itself)."}, {"title": "4.1. Part 1: Code Commenting", "content": "The ability of the LLM to add straight-forward comments to the code from Figure 2 is illustrated in Figure 3.\nThe prompt for the LLM (given in full in Appendix C.1) includes instructions for:\n\u2022 General ARC Rubric - see Appendix C\n\u2022 Core Knowledge - see Appendix B\n\u2022 DSL documentation - see the arc-dsl-llm code release\n\u2022 Problem solution - expressed in arc-dsl-1lm- see Figure 2\n\u2022 Input / Output grids - this is in a one-token per pixel format, which works fine despite not being valid Python\n\u2022 Optional: Interim variable values we can also extract these, since we have valid code and inputs\n\u2022 Instruction about output formats - centers on comment style for Part 1\nFigure 3 shows that the LLM has picked up on some of the finer details of this problem's solution. While the obvious human interpretation of one of the sub-goals here is \"Choose the internal areas\", the DSL implementation reads \"Choose all objects that are black which don't touch the edges of the grid\". The LLM has 'realised' this with the third Goal comment: \"This effectively selects the internal black objects\"."}, {"title": "4.2. Part 2: Code Refactoring", "content": "The ability of the LLM to perform a light refactoring of the code is illustrated in Figure 4. The aim of this Part is to implement chunking (that, for instance DreamCoder (Ellis et al., 2020) might perform at considerable cost) 'outside the box' - i.e. the found sub-functions can be exported into a Kaggle container (the consistent commenting style makes them RAG-able).\nThe prompt (given in full in Appendix C.2) extends Part 1:\n\u2022 What makes a good sub-function - which is really a matter of taste...\n\u2022 Free to rename variables in sub-functions - to see whether the LLM can provide its own self-commenting\n\u2022 Main function required name - the main entry point must be solver_virtual_chunked(I)\n\u2022 Main function retains original variable names SO that each code sample can be indexed against the others\n\u2022 Sub-functions should not call each other so that they are independently useful \u2018inside the box'\nA number of sanity-check are applied to the refactored code output, which accounts for most of the missing entries in the dataset. These are not necessarily the most difficult/longest code solutions, the issue that the LLM seems to have is that some refactoring 'ideas' are so compelling that it cannot resist trying to apply them. The current checks include:\n\u2022 Sub-functions cannot call each other - this can be checked to some degree by analysing the code, though sometimes there may be in-line functional-synthesis-style calls that are difficult to pick up\n\u2022 The main function must have consistent variable names this can be difficult for LLMs to understand, since the prompts encourage variable renaming within new sub-functions (this may desirable for when they are later deployed in a RAG context). This consistency requirement also makes the output code less Pythonic (to the detriment of the LLM)\nOverall the refactoring demonstrated in Figure 4 is not particularly noteworthy (many of the sub-functions suggested are one-liners, despite the prompts given). However, one interesting observation is that find_internal_objects has been factored out - further emphasising that the LLM has 'understood' the goals of this (rather convoluted) method."}, {"title": "4.3. Part 3: High-Level Tactics", "content": "In order to get tactics that might be applicable in a Self-Discover (Zhou et al., 2024) framework for ARC, the LLM was open-endedly asked to generate 'at least 5' high-level tactics, and given a few examples.\nThe prompt (given in full in Appendix C.3) extends Part 2:\n\u2022 Create high-level tactics - this was intentionally very open-ended, only specifying that the tactics should be useful if the function solution was not known\nThe tactics suggested by the LLM in Figure 5 make sense for this example - but the real test is whether they are more generally applicable (so that a Self-Discover implementation could then 'order off the menu' from the available tactics, and then execute them).\nTo investigate this, tactics were gathered from across the dataset outputs, and then their sentence-embeddings (provided by jina-embeddings-v2-base-code) were clustered using UMAP (McInnes et al., 2020) and HDB-SCAN (Malzer & Baum, 2020). The results are shown graphically in Figure 6.\nThe number of points in the top 30 clusters shown suggests that the LLM has, indeed, been able to surface high-level tactics of the type required for the Self-Discover framework."}, {"title": "4.4. Part 4: Solution Steps", "content": "In order to obtain a description of the whole task (as if one were a human 'describer' for LARC), the LLM was asked to describe the steps to be taken to transition from the Input Grid to the Output Grid.\nThe prompt (given in full in Appendix C.4) extends Part 3:\n\u2022 Description of the input/output grids - specified to be for the whole task\n\u2022 Steps required to solve the task - in 'human terms'\n\u2022 Relevant variables names for each step - so that each of the Parts here can be indexed against each other\n\u2022 Core Knowledge and tactics - this was so that generic program steps could be provided for fine-tuning other models. Note that no specific list of Core Knowledge was supplied, only the rubric given in Appendix B\nThe same sample task was used to produce Figure 7, which illustrates a reasonable ability to describe the process though the Input Grid description includes \u201c(i.e. no holes exist within objects)\", which is a mistake.\nOverall, the LLM tended to be over-cautious in generalising the solving process in this Part - but this is potentially because of the requirement to specify the variables being used at each step."}, {"title": "5. Conclusions", "content": "The ARC challenge is an important benchmark due to its resistance against brute-force scaling-oriented approaches. So, while some may argue in favour raising the compute available within the ARC Prize Kaggle environment, the authors feel that keeping a tight bound on resources will spur more innovation : Necessity is the mother of invention.\nEven though there are strong arguments that 'vanilla' LLMs will not be capable of learning the abstraction abilities required to tackle the problems head-on from data alone, this work illustrates how they might be capable of producing some 'Sparks of Abstraction' which can then be captured for additional processing."}, {"title": "5.1. Further Work", "content": "A key goal of this work has been to find a way to 'smuggle the intelligence' of a commercial LLM into the lower-resource ARC Prize environment.\nPreliminary work on the actual ARC Prize challenge has been on-going, however the overall system envisioned is still being brought on-line (while the component parts appear promising, integrating them into a cohesive whole is a daunting task).\nWe look forward to making progress on the ARC challenge in the 2025 round of the ARC Prize - and would welcome the opportunity for collaboration in the future."}, {"title": "A. The LLM-legible DSL :arc-dsl-llm", "content": "The LLM-legible DSL used in this paper was initially intended to be a minor tailoring of arc-dsl from Hodel (2023). However, once a few of the functions had been renamed (and consequently more of the DSL code had been read) the number of changes began to multiply. Then, the problem of COLOR constants being used as integer values, violating the typing hints became apparent - and the initial minor tailoring became more extensive re-engineering.\nThe codebase is available at https://github.com/mdda/arc-dsl-llm, and contributions / bugfixes are welcome. The code for the dataset that this paper discusses (available at https://github.com/mdda/LLM-abstraction-for-ARC) does not rely on the DSL naming per se, but does currently use the fact that the solutions are all expressed with one function call per line (simplifying the parsing / mapping of the code)."}, {"title": "A.1. Function renaming", "content": "The reasons for changing function names include:\n\u2022 dmirror \u2192 diagonal mirror: Not obvious what the function does without the documentation\n\u2022 subgrid \u2192 smallest_subgrid_containing: Similarly...\n\u2022 product \u2192 cartesian_product: To create a contrast with multiply, which behaves as expected\n\u2022 fork \u2192 combine_two_function_results: This may be obvious to a functional programmer, but was confusing to the LLM when reading / generating Python code\n\u2022 color \u2192 get_color : The LLM sometimes treated color as a variable rather than a function\nFor a complete list of the 85 (!) DSL functions that were renamed, please see the repo."}, {"title": "A.2. COLOR constants", "content": "There were several problems with the usage of COLOR constants in the original arc-ds1, which required a large effort to correct throughout the codebase (including solutions that made assumptions about the numerical values of the defined COLOR constants):\n\u2022 Fix assumption that COLOR_BLACK==0 or COLOR_BLACK<COLOR_RED (for instance)\n\u2022 Add additional constant COLOR_BELOW (defined to be numerically smaller than other colors) that allows for sort to behave in the way expected by several solutions\n\u2022 Remove usage of COLOR_X to represent small integers (i.e. non-colors). This was frustrating.\n\u2022 Remove calculation of COLOR_X values by (for instance) doubling other COLOR_Y values (!)\nIt is believed that these are largely fixed (since in the 03 notebook the values of the COLOR constants is permuted, and the validity of the solutions is rechecked). However, it is possible that some edge-case were not detected)."}, {"title": "A.3. Type-hinting", "content": "Although arc-dsl appeared to have solid type-hinting, it appears that it was not actually checked to be valid. The arc-dsl-llm includes type-hinting such that that pyright solvers.py executes cleanly. To make it clearer how the solutions worked (including the types), two new functions were added:as_generic_tuple and make_cell.\nOne problem caused by Python's lack of proper types : Integer manipulations of variables that included COLOR elements (which could occur in, for instance, tuples with 'real' integers) cannot be 'traced through' to ensure type-correctness. To enable proper \u2018LLM-legible' rendering of both grids and the more complex interim variables, stricter type-hinting adopted (out of necessity) by the dataset utilities : COLOR constants were remapped to have values in the range [1009...1019] (corresponding to [BELOW, BLACK... PINK]), which was a workable (hacky) solution, given that the ARC Core Knowledge specifies that numerical quantities used within solutions will be no larger than (say) 20. Thus, if an integer is > 20, we know that it can be remapped as a COLOR constant when rendered for the LLM."}, {"title": "A.4. Making arc-dsl-llm available as a Python module", "content": "Due to the requirement that arc-dsl-llm could be treated as a module (to enable import for running LLM-generated code that calls DSL functions), a simple fix was to add a link \"./arc-dsl \u2192 .\", and adding a __init__.py to the repo main directory. This allows the code to run as an imported module (without moving the files around - all previous"}, {"title": "A.5. Confirmation that solutions are still valid", "content": "The arc-dsl-11m has been brought up-to-date (as-of 2024-11-12) with the latest ARC training set fixes, and PRs from the community - and all 400 training-set solutions pass cleanly.\nNote that, due to how the modularisation fix works, the command to run the DSL tests and prove the solutions on the test examples is now:python -m arc_dsl.main"}, {"title": "B. Core Knowledge Rubric", "content": "The rubric for the Core Knowledge was reformulated as follows (guided by LLM re-writing) :\n## Core Knowledge\nSolving ARC problems requires understanding and applying Core Knowledge concepts relating to spatial reasoning, \\\nobject manipulation, and basic mathematical principles. These concepts include:\n***Object cohesion**:\n+ Ability to parse grids :\nidentifying distinct objects within the grid based on properties like:\n+ For instance: color continuity, spatial contiguity, repeated patterns, or symmetries\nsegmenting the grid into zones or partitions, which can be treated as sub-grids\n+ For instance: dividing a grid with delineated quadrants into separate, potentially inter-related sub-grids\n***Object persistence**:\n+ Objects are assumed to persist despite the presence of noise or occlusion by other objects\nFor example, if a square is partially covered by a triangle, the solver should still recognize \\\nthe underlying square\nWhile generally true, there are cases where objects might disappear or transform significantly\n+ In many cases, objects from the input grid persist on the output grid, but in a transformed form but in \\\na transformed form (e.g., rotated, scaled, or recolored)\n***Object influence via contact**:\n*\n*\n*\n+ Many problems feature physical contact between objects\nFor instance: one object being translated until it is in contact with another\nOther examples: a line extending until it touches another shape; objects snapping to a grid; \\\nor an object being 'pushed' by another\n**Basic Geometry and Topology priors**:\n+ Geometric and topological reasoning is crucial. Commonly encountered concepts include:\nShapes: Lines, rectangles and simple shapes; Other objects that occur are likely to have simple motifs\nTransformations: rotation, translation, mirroring, flipping, scaling (overall or horizontal/vertical)\nRelationships: Containing/contained, inside/outside perimeter, corners, parallel lines, \\\ntopological connectedness, set relationships (inclusion, intersection, disjointness).\nActions: Drawing lines, connecting points, orthogonal projections, copying, repeating objects\nSelf-similarity via symmetries such as rotations and mirroring\n**Numbers and Counting priors**:\n+ Many ARC problems involve counting or sorting objects and/or comparing numbers, for instance:\nWhich shape or symbol appears most / least / same number of times?\nWhich object is the largest / smallest?\nWhich objects are the same size / color?\n+ Similarly actions being taken might depend on counting and/or comparing numbers\nFor example: Repeating a single shape a number of times depending on the number of different shapes present\n+ Simple arithmetic operations (addition, subtraction, multiplication, division), \\\nalthough all quantities featured will be small integers less than (say) 10\n**Goal-directedness prior**:\n+ Many ARC problems can be interpreted as depicting a sequence of actions with a specific goal\n+ For instance:\nA problem might combines the concepts of \"line extrapolation\", \"turning upon hitting an obstacle\", \\\nand \"efficiently reaching a goal\"\nArranging objects to fill a container or constructing a symmetrical pattern\n+ Some ARC problems might imply a need for planning or simulating steps towards a solution\n***Compositionality**:\n+ Successfully solving ARC problems often requires chaining the above concepts together\nFor instance: First identifying simply connected components (cohesion), then counting them (numerical),\nand finally replicating the largest component multiple times side-by-side (geometry)\nFor instance: First grouping shapes by color (cohesion and color), sorting them by size (numerical), \\\nrecoloring the most frequent (numerical and color), and reflecting it across \\\na vertical axis (geometry and symmetry)"}, {"title": "C. Gemini Prompting Scheme", "content": "The overall rubric for the ARC challenge task follows :\n# ARC Challenge problems\nEach problem in the ARC Challenge requires understanding the way in which several \"input grids\" \\\ncan be transformed into corresponding \"output grids\".\nSeveral demonstration pairs are shown, and the solution involves describing how an unknown \"output grid\" \\\ncan be derived from the given test \"input grid\".\nTo do this, we will be doing extensive code analysis."}, {"title": "C.1. Part 1 - Code Commenting Prompts", "content": "### Part 1: Add comments to original solution\nAdd comments into the program code for function 'solver_virtual (I) above, at the points indicated by % comment`.\nIf it makes sense, comments can be skipped, so that lines of code are combined into more reasonable code blocks.\nEach code block can be as short as one line, or as long as necessary to encompass a complete subtask.\nEach set of comments should relate to the code block that follows.\n#### Part 1 Answer Format\nYour answer should repeat the program code of 'solver_virtual (I) above, with the comments included according \\\nto the code blocks you decide.\nEach set of comments should be in the following format:\n* # Input:\n* # Goal:\n1\nWhat input the code is expecting at that point (in terms of types, and in terms \\\nof the overall goal of the solution)\nWhat the goal of the next line of code are (both locally, and how it relates \\\nto the overall goal of the solution).\n* # Output:\n1\nWhat the expected output of this block (in terms of types, and in terms \\\nof the overall goal of the solution)\n* (optional) # Core Knowledge:\nIf any elements of Core Knowledge are relevant to the block, \\\ndescribe them in an additional comment line."}, {"title": "C.2. Part 2 - Code Refactoring Prompts", "content": "### Part 2: Create reusable components\nCreate a new version of 'solver_virtual (I) from Part 1 called 'solver_virtual_chunked (I) \\\nwhich has the same functionality.\nTo create 'solver_virtual_chunked (I)`, examine each line of code (and surrounding lines):\n* move natural blocks of code (consisting of several lines of code each) into separate new functions, \\\nwith a call from 'solver_virtual_chunked (I)`.\n* blocks of code must return concrete variables.\n* Callables should only be used be within a block\n* if there are lines that are not easily isolated, leave them unchanged in 'solver_virtual_chunked (I) `.\nComments in the same format as Part 1 should be added to each line of 'solver_virtual_chunked (I)`.\n#### Part 2 Answer Format\nThe following example illustrates the format of two function components and 'solver_virtual_refactored (I)`:\n```python\ndef recolor_single_cell_objects (pairs: FrozenSet, color: Color) -> FrozenSet: # New function, which calls \\\nat least 2 DSL functions\n# Input: pairs (FrozenSet), color (Color), pairs of single-cell and grey objects\n# Goal: Recolor each single-cell object based on its adjacent object's color.\n# Output: recolored_objects (FrozenSet), a set of locations and recolored single-cell objects.\n# Core Knowledge: Object transformation (recoloring), Compositionality\nrecoloring_function = combine_two_function_results (recolor, compose (color, get_first), get_last) \\\n# variables named appropriately\nrecolored_objects = transform_and_flatten (recoloring_function, pairs) # variables named appropriately\nreturn recolored_objects\n#\nother new functions here\ndef solver_virtual_chunked (I): # This function calls the new functions, replacing suitable chunks. \\\nVariable names in this function are the same as in 'solver_virtual'\n# Input: I (Grid), the input grid.\n# Goal: Identify and separate objects within the input grid.\n# Output: x1 (Objects), a set of objects identified in the input grid.\n# Core Knowledge: Object cohesion (parsing grids, identifying distinct objects based on spatial contiguity)\nx1 = as_objects (I) # Retain original code (and variable names) if not moved to new function\n# Input: x1 (Objects), a set of objects.\n# Goal: Filter objects based on their size (select only single-cell objects).\n# Output: x2 (Objects), a subset of x1 containing only single-cell objects.\n# Core Knowledge: Numbers and Counting priors (size filtering).\nx2 = size_filter(x1, 1) # Retain original code (and variable names) if not moved to new function\n#\nother lines here\nwith each block also having comments in the format of Part 1.\n# Input: x2 (FrozenSet), pairs of single-cell and objects"}, {"title": "C.3. Part 3 - High-Level Tactics Prompts", "content": "### Part 3: High-level tactics\nOutline potential high-level tactics that could be used to solve this problem, \\\nif 'solver_virtual (I) was unknown.\n#### Part 3 Answer Format\nFill in the following YAML structure (the comments explain the intent of the entries):\n```yaml\ntactics:\nheading: \"\" # A short name for the tactic\ndescription: \"\" # A description of the tactic\ndsl_functions: () # A list of relevant DSL functions (as appropriate)\nReturn 5 or more tactics in this format.\n#### Part 3 Examples\nSome examples of tactics:\n```yaml\ntactics:\nheading: \"Better Representation\"\ndescription: \"Seek a better representation of the input/output grid\"\ndsl_functions: [as_objects]\nheading: \"Filter by Property\"\ndescription: \"From the list, select according to a property\"\ndsl_functions: [size_filter, most_common_color, extract_first_matching, equals]\nheading: \"Combine Results\"\ndescription: \"Combine previous results into final grid\"\ndsl_functions: (fill, paint_onto_grid]"}, {"title": "C.4. Part 4 - Overall Solution Prompts", "content": "### Part 4 : Overall solution description\nDescribe the high-level steps involved in solving the overall Problem.\nThis requires stating the overall expected contents of the Input grid, a sequence of steps required \\\nto solve the problem, and the expected contents of the Output grid.\nThe sequence of steps should be expressed in human form (not necessarily corresponding directly to lines of code).\nThe steps should be described generically (i.e. don't use specific color names or shape descriptions) \\\nso that the steps could be reused for other problems.\n#### Part 4 Answer Format\nFill in the following YAML structure (the comments explain the intent of the entries):\n``'yaml\ninput: \"\" # What input should be expected for the problem\nsteps: # An array with elements that correspond to each high-level step\ntext: \"\" # describes this key part of solving the problem\ntactic_used: \"\" # the tactic heading from Part 3 that is most relevant to this step\ncore_knowledge: [] # if any elements of Core Knowledge are relevant to this step, list them \\\n(eg: ['Object Manipulation', ...])\nvariables_input: () # if any variables in Part 1 are needed before doing this step, list them (eg: [x3, x4])\nvariables_output: () # if any variables in Part 1 are created by this step, list them (eg: [x3, x4])\noutput: \"\" # What output should be expected for the problem solution"}]}