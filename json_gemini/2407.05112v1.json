{"title": "Releasing Malevolence from Benevolence:\nThe Menace of Benign Data on Machine Unlearning", "authors": ["Binhao Ma", "Tianhang Zheng", "Hongsheng Hu", "Di Wang", "Shuo Wang", "Zhongjie Ba", "Zhan Qin", "Kui Ren"], "abstract": "Machine learning models, when trained on massive\nreal or synthetic data, typically can achieve exceptional predic-\ntion performance in various application domains. However, the\noutstanding model utility comes with growing privacy concerns\nsince the data used for model training may contain sensitive\ninformation. To mitigate privacy concerns, machine unlearning\nhas been proposed to eliminate the information of specific data\nsamples from machine learning models. While some machine\nunlearning techniques can optimize the models to forget data\nat a low cost, recent works show that a malicious user could\nrequest unlearning on perturbed data to compromise the\nunlearned model. Despite the effectiveness of the attack, the\nperturbed data does not match the original data for training\nthe original model and thus cannot pass hash verification.\nMoreover, the existing attacks on machine unlearning suffer\nfrom limited practicality and applicability, due to their demand\non additional knowledge and non-negligible attack budgets.\nTo fill the gaps in the existing unlearning attacks, we\npropose a new attack called the Unlearning Usability At-\ntack, which is model-agnostic, unlearning-agnostic, and budget-\nfriendly. An unlearning usability attack is implemented by\ndistilling data distribution information into a small quantity\nof data, which is labeled as benign data by automatic poison-\ning detection tools due to its positive contribution to model\ntraining. Although the data is benign for machine learning,\nunlearning on the data will induce significant loss of data\ninformation in the models. Our evaluation reveals that, under\ndifferent attack scenarios, unlearning the benign data, which\nis no more than 1% of the entire training data, will drop the\nmodel accuracy by up to 50%. Our evaluation also indicates\nthat the well-prepared benign data naturally act as hard-\nto-unlearn samples in recent unlearning techniques, as the\nprocess of erasing these synthetic instances demands a higher\nbudget than regular data. These new findings motivate future\nresearch to rethink \"data poisoning\" in the context of machine\nunlearning.", "sections": [{"title": "1. Introduction", "content": "Many breakthroughs in modern machine learning owe\ntheir success to the abundance of available data. However,\nsince the data may contain sensitive information of indi-\nviduals [1], organizations must adhere to data protection\nregulations during the data collection and utilization pro-\ncesses, prioritizing the privacy of the individuals to minimize\nthe risks of legal action and reputational harm. To give\nindividuals full control of their data, prominent regulations\nsuch as General Data Protection Regulation (GDPR) [2] and\nCalifornia Consumer Privacy Act (CCPA) [3] have legally\nestablished the right to \"erasure of personal data without\nundue delay.\" Notably, Google Search has received mil-\nlions of individuals' requests to remove specific URLs from\nsearch results within a five-year period [4], demonstrating\nthe substantial demand from individuals for stronger privacy\nprotection online.\nComplying with privacy regulations, the concept of ma-\nchine unlearning has been proposed and broadly studied\nin recent literature [5], [6], [7], [8]. Machine unlearning\nnecessitates that upon an individual's request for certain\ndata (i.e., the unlearned data) removal, the machine learning\nmodel owner must ensure that the unlearned data is erased\nfrom the trained model, safeguarding the information of\nthe unlearned data from potential privacy attacks, including\nmodel inversion attacks [9], data extraction attacks [10],\n[11] and membership inference attacks [12]. Retraining the\nmodel from scratch without the unlearned data is an intuitive\nsolution for machine unlearning, but it leads to high com-\nputational overhead, especially when the model is complex\nand trained on large-sized datasets. For example, it is almost\nimpossible to retrain a large language model (LLM) like\nChatGPT [13] when some unauthorized data have to be\nremoved from the model due to copyright issues [14].\nBesides naively retraining for unlearning, approximate\nunlearning methods [5], [6], [7], [15] that directly update"}, {"title": "2. Related Work", "content": "Machine Unlearning. Machine unlearning methods can\nbroadly be divided into two categories: exact unlearning and\napproximate unlearning. Exact unlearning involves retrain-\ning the model from scratch, i.e., retraining on the dataset\nminus the data slated for removal. While this kind of method\nrequires particular skills to be executed efficiently, its pri-\nmary advantage is that it guarantees the total elimination\nof any impact from the unlearned data, as the retrained\nmodel has never been trained on that data. The work [29]\nintroduced a technique for forgetting points from clustering.\nThe key idea of both methods involves partitioning the\ndata into independent sections and aggregating the final\nmodel from sub-models trained on these partitions. The\nidea is later extended to graph unlearning [30], [31]. While\neffective for forgetting data points by retraining only the\naffected partitions, this method introduces significant storage\noverhead and inference latency. Brophy et al. [32] suggest\na forest structure designed for data removal that facilitates\nthe effective unlearning of samples in random forests. Ap-\nproximate unlearning modifies the parameters of the trained\nmodel to mimic a model retrained from scratch, achieving\nthe unlearned state. Typically, approximate unlearning in-\nvolves updating the trained model's parameters through a\nlimited number of iterations, using data derived from the\ninformation to be unlearned. Guo et al. [33] presented an\nunlearning method for linear regression, but its applicability\nto nonlinear methods is limited. The works [5], [6], [7]\nfurther addressed these limitations. Graves et al. [7] pro-\nposed preserving only the parameters updated for unlearned\nsamples, allowing the model to forget the original samples\nwhen needed for unlearning. Warnecke et al. [15] suggested\nadding noise to the samples and then making the model\nforget both noisy and original samples through loss updates,\nwhich can effectively erase information related to the sam-\nples.\nAttacks on Machine Unlearning. Machine unlearning has\nyielded promising outcomes, fostering an optimistic outlook\nfor user privacy protection in third-party services. However,\nit has concurrently opened up new avenues for attackers\nto exploit model vulnerabilities. Di et al. [34] introduced\nthe camouflaged data poisoning attack, where the accuracy\nof the model's predictions is negatively impacted when the\nattacker triggers a request to remove a subset of data samples\nfrom the dataset. However, implementing this method is\nchallenging as it necessitates an understanding of the tar-\ngeted network architecture and knowledge of the training\nprocedure, aspects that are often difficult to attain in prac-\ntical applications. Marchant et al. [35] introduced a method\nthat employs projected gradient descent (PGD) to add per-\nturbations to images. These manipulated images are then\nuploaded to the server to attack linear model unlearning,\nmaking it challenging for the model to forget such adver-\nsarial examples. Hu et al. [18] proposed optimizing images\nbeyond the decision boundary and subsequently uploading\nthem through MLaaS, inducing excessive unlearning in the\nmodel. Nevertheless, both of these attack methods can be\nthwarted through a straightforward hash comparison [36]\nbetween the uploaded samples and the original samples.\nAdditionally, the works [29], [37] suggested that model\nunlearning can be achieved solely through the user-provided\nindex, making these attacks unsuccessful in such scenarios.\nDifference from Existing Attacks. The biggest difference\nbetween our unlearning usability attacks and existing at-\ntacks [18], [19] is that the existing attacks allow the at-\ntacker to post-revising their unlearned data to compromise\nthe model, while we do not allow that. In our case, the\nattacker exercises the right to be forgotten by requesting\nto unlearn exactly what the attacker provided for training\nthe model. In addition, different from existing works [21]\nthat require unlearn a large amount of data samples for\ncompromising the unlearned model, our attacks crash the\nutility of the model with the usage of only a few samples,\ne.g., no more than 1% of the whole training dataset. Last, the\nclosest work to our paper is [18], which also exploits how\nan unlearned model can unintentionally reduce its utility\nthrough over-unlearning attacks. Our work differs from this\nwork in two aspects. First, the attacker in our setting can\nnot post-modify the unlearning data. Second, the work [18]\nfocused on how the added perturbations in the unlearned\ndata can additionally reduce the utility of the unlearned\nmodel, while we focus on how benign training data can\ninvalidate the model's utility."}, {"title": "3. Threat Model", "content": "In this section, we describe the threat model of un-\nlearning usability attacks, aiming to crash the unlearned\nmodel's utility in an automatic machine unlearning pipeline\nwith hash verification. We note that the machine unlearning\npipeline may also be equipped with an automatic defensive\nmechanism to detect poisoned data.\nWe anticipate the attack scenario where users contribute\ntheir data to train a machine learning model. A model\ndeveloper is responsible for training the model and trans-\nferring the ownership of the model to a service provider\nwe call the model owner. The model owner deploys the\nmodel to provide services to the users who contributed\ntraining data as well as other \"consumer\" users who want\nto leverage the prediction ability of the model. The attack\nscenario practically simulates the Machine Learning as a\nService (MLaaS) environment: the model owner deploys\nthe model in the cloud to provide services for end-users,\nmaking profits. The users who contribute the training data\ncan submit unlearning requests to the model owner, as they\nhave the right to delete their data. We consider that among\nthe many \"contributor\" users, there exist one or several\nmalicious users who intentionally wish to destroy the model\nby misusing their authorized rights. We detail the goals,\ncapability, and knowledge of the malicious user, the model\ndeveloper, and the service provider as follows.\nMalicious User. i) Goals. A malicious user is an attacker\nwho aims to invalidate the well-performed original model\nthrough machine unlearning. The motivation of the attacker\ncan be complex and multifaceted, e.g., by disrupting the\nmodel owner's normal business (i.e., the model utility),\nthe attacker may provide a competitive advantage to rival\norganizations. ii) Capability. The capability of the attacker\nis similar to that of a normal contributor user: providing\ntraining data for training the model and requesting the\nmodel owner to unlearn exactly the data provided by the\nattacker. The difference between the attacker and the normal\ncontributor user is that the training data provided by the\nattacker is deliberately generated to achieve the goal of\nthe attack. iii) Knowledge. To simulate a practical attacker,\nwe consider the attacker has only the knowledge of the\ntraining data but without the knowledge of the target model"}, {"title": "4. Methodology", "content": "In this section, we introduce our attack strategy on\nmachine unlearning. We begin by articulating the desired\nimpact of the attack and providing a formal definition in\nthe Problem Statement. Subsequently, in the Problem For-\nmulation, we describe our method for achieving the attack\nobjective with benign data.\nProblem Statement. Our aim is to attack machine unlearn-\ning with the objective of inducing over-unlearning. Each\ndata sample $(x, y)$ consists of multidimensional features $x$\nand a label $y$. The neural network is denoted by the function\n$\\psi_{\\theta}(\\cdot)$, which takes the sample $x$ as input and outputs the\nlabel $y$. The test set $D_{\\text{test}} = \\{(s_1, t_1), ..., (s_{|N|}, t_{|N|})\\}$ is\ndeployed on the server to evaluate model performance.\n$A(\\cdot, \\cdot)$ is a unlearning method, and $\\theta$ represents the\ntrained model. We denote $D_u$ as data uploaded by normal\nusers, where $D_u \\subset D_{\\text{train}}$, and $D_m$ as data uploaded by\nmalicious users, where $D_m \\subset D_{\\text{train}}$.\nA normal user sends an unlearning request, and the\nmodel executes $A(\\psi_{\\theta}, D_u)$ to perform machine unlearning.\nSubsequently, the model parameters are updated to $\\psi_{\\theta_u}$.\nSimilarly, a malicious user sends an unlearning request,\nand the model executes $A(\\psi_{\\theta}, D_m)$ to perform unlearning,\nresulting in the model parameters being updated to $\\psi_{\\theta_m}$. $\\alpha_u$\nrepresents the accuracy of $\\psi_{\\theta_u}$ on $D_{\\text{test}}$, and $\\alpha_m$ represents\nthe accuracy of $\\psi_{\\theta_m}$ on $D_{\\text{test}}$. According to [18], if the utility\nof $\\psi_{\\theta_m}$ on $D_{\\text{test}}$ is not greater than that of $\\psi_{\\theta_u}$ on $D_{\\text{test}}$, i.e.,\nif $\\alpha_m < \\alpha_u$, it is termed as a situation of over-unlearning.\nProblem Formulation. According to over-unlearning, the\nloss of model $\\psi_{\\theta_m}$ on the $D_{\\text{test}}$ after over-unlearning will\nexceed that of model $\\psi_{\\theta_u}$ after normal unlearning. Con-\nsequently, the utility of model $\\psi_{\\theta_m}$ is lower than that\nof model $\\psi_{\\theta_u}$. To achieve the goal of over-unlearning,\nwe utilize the problem objectives outlined in [25], [38],\n[39], [40], [41], [42]. We represent our attack data as\n$M = \\{(m_1, y_1),..., (m_{|M|}, y_{|M|})\\}$, and the dataset known\nto malicious users as $T = \\{(X_1,Y_1), ..., (X_T,Y_{|T|})\\}$.\nWe describe our attack data as 'Informative Benign Data,'\nwhere a small amount of data can demonstrate performance\ncomparable to that of a substantial dataset. The formula is\npresented below:\n$E_{x \\sim P_D} [l(\\psi_{\\theta_M} (x), y)] \\sim E_{x \\sim P_T} [l(\\psi_{\\theta_T}(x), y)] \\tag{1}$\nwhere $P_D$ represents the distribution of the training data,\n$l$ denotes the loss function (such as cross-entropy loss), $\\psi$\nis a deep neural network characterized by the parameters $\\theta$,\nand $\\theta_T$ and $\\theta_M$ are the networks trained on datasets known\nto malicious users $T$ and on synthetic data $M$, respectively.\nInformative Benign Data. To achieve the goal set in For-\nmula 2 with a minimal amount of data, denoted by $M$, we\naim to synthesize $M$ to closely approximate the true training\ndata distribution. To enhance the informativeness of these\nsamples, we employ data distribution matching methods\n[25]. Given the high dimensionality of training images, accu-\nrately estimating the real data distribution $P_D$ is costly and\nimprecise. Therefore, we reduce each training image $x \\in \\mathbb{R}^d$\nto a lower-dimensional space using parameterized functions\n$\\varphi_{\\theta} : \\mathbb{R}^d \\to \\mathbb{R}^{d'}$, where $d' \\ll d$. We then use Maximum\nMean Discrepancy (MMD) [43] to estimate the distance\nbetween the actual and synthesized data distributions. As\naccess to the ground-truth data distributions is unavailable,\nwe rely on the empirical estimate of MMD:\n$\\underset{\\varphi_{\\theta}}{\\text{Eo Po}}\\[\\frac{1}{|T|} \\sum_{i=1}^{|T|} \\varphi_{\\theta}(x_i) - \\frac{1}{|M|} \\sum_{j=1}^{|M|} \\varphi_{\\theta}(m_j) \\]^2 \\tag{2}$\nWhere $P_{\\theta}$ represents the distribution of network param-\neters, and $\\theta$ denotes the parameters for $\\varphi_{\\theta}$. The expec-"}, {"title": "5. Experimental Settings", "content": "Datasets. We have considered three attack scenarios, where\nthe attacker has varying levels of knowledge about the"}, {"title": "6. Attack Performance", "content": "We follow the experimental settings in Section 5 to\nevaluate our attack and provide the results in this section.\nAll the evaluation results for different machine unlearning\ntechniques indicate unlearning informative benign data will\nbe much more harmful than unlearning normal data.\nAttack Performance in Scenario 1. In the scenario of\nuser-developer collusion (refer to the settings in Section 5),\nthe model performance across different datasets before\nunlearning is shown in Table 3. The changes in model\naccuracy after unlearning normal or informative data are\nshown in Table 4. We observe that unlearning informative\ndata can cause a decrease in model accuracy of at least 20%\nby utilizing only 0.2% of the data.\nFor amnesiac unlearning, when the model is instructed\nto erase updates generated by informative data, the model\naccuracy significantly drops to 44.33%, 10.25% and 3.17%\non MNIST, CIFAR-10, and CIFAR-100, respectively. In\ncomparison, unlearning normal data only drops the accu-\nracy to 99.04%, 86.01% and 47.38% on those datasets.\nWhen unlearning normal data, first-order and second-order\nunlearning methods can erase the influence of data features\nwith negligible impact on model accuracy. However, when\nunlearning informative data, we observed a decrease in\naccuracy of about 30% on MNIST and more than 20% on\nCIFAR-10 and CIFAR-100 datasets. For the negative gradi-\nent method, we also observed that unlearning normal data\ndoes not significantly decrease the model's accuracy. How-\never, for our informative benign data, both test and training\naccuracy dropped by at least 20% across all datasets. We\nbelieve that when performing gradient ascent on informative\nbenign data, the updates affect more crucial parameters in\nthe model, leading to a decrease in model performance."}, {"title": "7. Analysis of Attack Effectiveness", "content": "General Analysis: Information of Informative and Nor-\nmal Data. We hypothesize that unlearning informative data\n(for all the evaluated unlearning methods) causes more\ndegradation in model accuracy because it usually contains\nmore information than normal data. To verify this hypoth-\nesis, we trained two separate networks on informative and\nnormal data, respectively. We then compared their accuracy\nto assess the information of informative and normal data.\nOur results across ResNet18, LeNet, ConvNet, and AlexNet\ndemonstrated that the model trained on informative data\nconsistently learn more information and thus achieve higher"}, {"title": "8. Resistance Against Poisoning Defenses", "content": "In this section, we examine the effectiveness of poisoned\ndata detection and defense mechanisms against informative\nbenign data, as the success of such attacks hinges on the\ndata's ability to bypass poisoned data detection tools in an\nautomatic machine unlearning pipeline. Previous research\nhas focused on detecting poisoned samples within datasets\nto protect models. If the prevailing automatic poisoning de-\nfensive mechanisms can detect and remove our informative\nsamples, our attack strategy may fail. We evaluated the\nresilience of our informative data against both passive and\nactive defenses on the CIFAR-10 dataset.\nResistance to Passive Defense. Tran et al. [49] observed\nthat, while poisoned samples crafted by attacks like BadNets\nand Blend do not differ significantly from normal samples\nin the data space, their latent representations are mostly\nseparable from the representations of normal samples. Based\non this observation, Tran et al. [49] further proposed to\nuse the top right singular value of the representation as a\nmeasure to separate the representations, which yields better\nresults than using $l2$ distance as a measure.\nHere we assess if the informative benign data is sepa-\nrable from normal data in both data and latent spaces. We\nshow the results of using the $l2$ distance as the measure in\nFigure 9 and the results of using the top right singular value\nas a measure in Figure 10. The results showed that, our\ninformative data does not exhibit separation from normal\nsamples in both data and latent spaces under both of the\nmeasures. Therefore, Tran et al.'s method [49] can not detect\nour informative data, while it is very effective to detect\npoisoned data."}, {"title": "9. Conclusion", "content": "This paper introduces the concept of the \"unlearning\nusability attack\" as a novel approach to overcoming the lim-\nitations present in current unlearning attack strategies. Our\nmethod differs from conventional approaches by avoiding\nthe use of perturbed samples in unlearning requests, thus\nenabling attacks on automated machine unlearning systems\nwhile circumventing hash-based checks. We present three\nplausible attack scenarios: User-Developer Collusion, User\nCollusion, and Independent Users. Through these scenar-\nios, our study illustrates that even within black-box envi-\nronments, we can induce significant unlearning in models\nby leveraging Informative Benign Data. Additionally, our\ngenerated informative benign data demonstrates robust resis-\ntance against both passive and active defense mechanisms,\nprompting a reassessment of the conventional understanding\nof \"poisoned data\" in machine unlearning.\nGiven the increasing importance of machine unlearning\nservices amid escalating privacy concerns, our study high-\nlights the inherent vulnerabilities of these techniques within\nMLaaS frameworks, which lays the groundwork for under-\nstanding and mitigating these risks. Future research may\nfocus on developing more robust unlearning procedures and\nenhancing data detection capabilities to maintain a balance\nbetween data privacy, model functionality, and security in\nMLaaS environments."}]}