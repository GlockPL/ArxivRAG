{"title": "Releasing Malevolence from Benevolence:\nThe Menace of Benign Data on Machine Unlearning", "authors": ["Binhao Ma", "Tianhang Zheng", "Hongsheng Hu", "Di Wang", "Shuo Wang", "Zhongjie Ba", "Zhan Qin", "Kui Ren"], "abstract": "Machine learning models, when trained on massive\nreal or synthetic data, typically can achieve exceptional predic-\ntion performance in various application domains. However, the\noutstanding model utility comes with growing privacy concerns\nsince the data used for model training may contain sensitive\ninformation. To mitigate privacy concerns, machine unlearning\nhas been proposed to eliminate the information of specific data\nsamples from machine learning models. While some machine\nunlearning techniques can optimize the models to forget data\nat a low cost, recent works show that a malicious user could\nrequest unlearning on perturbed data to compromise the\nunlearned model. Despite the effectiveness of the attack, the\nperturbed data does not match the original data for training\nthe original model and thus cannot pass hash verification.\nMoreover, the existing attacks on machine unlearning suffer\nfrom limited practicality and applicability, due to their demand\non additional knowledge and non-negligible attack budgets.\nTo fill the gaps in the existing unlearning attacks, we\npropose a new attack called the Unlearning Usability At-\ntack, which is model-agnostic, unlearning-agnostic, and budget-\nfriendly. An unlearning usability attack is implemented by\ndistilling data distribution information into a small quantity\nof data, which is labeled as benign data by automatic poison-\ning detection tools due to its positive contribution to model\ntraining. Although the data is benign for machine learning,\nunlearning on the data will induce significant loss of data\ninformation in the models. Our evaluation reveals that, under\ndifferent attack scenarios, unlearning the benign data, which\nis no more than 1% of the entire training data, will drop the\nmodel accuracy by up to 50%. Our evaluation also indicates\nthat the well-prepared benign data naturally act as hard-\nto-unlearn samples in recent unlearning techniques, as the\nprocess of erasing these synthetic instances demands a higher\nbudget than regular data. These new findings motivate future\nresearch to rethink \"data poisoning\" in the context of machine\nunlearning.", "sections": [{"title": "1. Introduction", "content": "Many breakthroughs in modern machine learning owe\ntheir success to the abundance of available data. However,\nsince the data may contain sensitive information of indi-\nviduals [1], organizations must adhere to data protection\nregulations during the data collection and utilization pro-\ncesses, prioritizing the privacy of the individuals to minimize\nthe risks of legal action and reputational harm. To give\nindividuals full control of their data, prominent regulations\nsuch as General Data Protection Regulation (GDPR) [2] and\nCalifornia Consumer Privacy Act (CCPA) [3] have legally\nestablished the right to \"erasure of personal data without\nundue delay.\" Notably, Google Search has received mil-\nlions of individuals' requests to remove specific URLs from\nsearch results within a five-year period [4], demonstrating\nthe substantial demand from individuals for stronger privacy\nprotection online.\nComplying with privacy regulations, the concept of ma-\nchine unlearning has been proposed and broadly studied\nin recent literature [5], [6], [7], [8]. Machine unlearning\nnecessitates that upon an individual's request for certain\ndata (i.e., the unlearned data) removal, the machine learning\nmodel owner must ensure that the unlearned data is erased\nfrom the trained model, safeguarding the information of\nthe unlearned data from potential privacy attacks, including\nmodel inversion attacks [9], data extraction attacks [10],\n[11] and membership inference attacks [12]. Retraining the\nmodel from scratch without the unlearned data is an intuitive\nsolution for machine unlearning, but it leads to high com-\nputational overhead, especially when the model is complex\nand trained on large-sized datasets. For example, it is almost\nimpossible to retrain a large language model (LLM) like\nChatGPT [13] when some unauthorized data have to be\nremoved from the model due to copyright issues [14].\nBesides naively retraining for unlearning, approximate\nunlearning methods [5], [6], [7], [15] that directly update"}, {"title": "Motivation.", "content": "There are several works [18], [19], [20], [21],\n[22] starting to investigate the security vulnerability of\nmachine unlearning. Specifically, they assume that a mali-\ncious user can upload crafted unlearned data to compromise\nthe unlearned model, e.g., reducing the robustness of the\nmodel [21]. However, despite the effort these pioneering\nworks made to reveal the vulnerability of machine un-\nlearning, we identify three gaps in existing attacks for an\nautomatic unlearning pipeline with hash verification."}, {"title": "Unlearning Usability Attacks.", "content": "There are two steps for\na malicious user to perform the unlearning usability at-\ntack: contribute and then revoke, to be short. Specifically,\nas depicted in Figure 1, the attacker first contributes his\nwell-prepared data to train the model. Then, the attacker\nexercises his right to be forgotten for unlearning, revoking\nhis contribution to the trained model. When the unlearning\nrequests are fulfilled, the unlearned model largely reduces\nits utility, making the resulting unlearned model useless for\nother users. The attack is practically possible in the scenarios\nof insider threats, where the insider attacker intends to\ndeliberately harm the business interests of a broader group\nby misusing their authorized rights.\nThe intuition of the unlearning usability\nattack is that by providing well-prepared informative data to\npromote the training of the original model, the attacker can\nthen revoke his contribution through unlearning to compro-\nmise the unlearned model. Because such well-prepared data\nis highly informative to the original model, the unlearned\nmodel inevitably largely loses its utility, as the correspond-\ning knowledge has been deleted. To achieve the attack, we\nconsider the attacker can leverage dataset condensation [25]\ntechniques, which can condense the knowledge of many\nsamples into a few informative samples.\nNote that in our attacks, the well-prepared data is\ndifferent from the poisoning data in traditional poisoning\nattacks [26], [27], [28]. Specifically, in traditional poisoning\nattacks, the attacker intentionally poisons the training dataset\nto compromise the model, e.g., creating backdoors into the\nmodel or implementing target attacks. The model behaves\nunmorally because the poisoned data is harmful during the\ntraining process. However, in our attacks, the well-prepared\ndata is benign for machine learning since it makes a positive\ncontribution to the model training. Put differently, the data\nis helpful during the training process, while it only exhibits\nits menace on the model during the unlearning process.\nTherefore, the unlearning usability attack sheds light on a\nnew perspective to rethinking \"poisoning\u201d in the context of\nmachine unlearning."}, {"title": "3. Threat Model", "content": "In this section, we describe the threat model of un-\nlearning usability attacks, aiming to crash the unlearned\nmodel's utility in an automatic machine unlearning pipeline\nwith hash verification. We note that the machine unlearning\npipeline may also be equipped with an automatic defensive\nmechanism to detect poisoned data.\nWe anticipate the attack scenario where users contribute\ntheir data to train a machine learning model. A model\ndeveloper is responsible for training the model and trans-\nferring the ownership of the model to a service provider\nwe call the model owner. The model owner deploys the\nmodel to provide services to the users who contributed\ntraining data as well as other \"consumer\" users who want\nto leverage the prediction ability of the model. The attack\nscenario practically simulates the Machine Learning as a\nService (MLaaS) environment: the model owner deploys\nthe model in the cloud to provide services for end-users,\nmaking profits. The users who contribute the training data\ncan submit unlearning requests to the model owner, as they\nhave the right to delete their data. We consider that among\nthe many \"contributor\" users, there exist one or several\nmalicious users who intentionally wish to destroy the model\nby misusing their authorized rights. We detail the goals,\ncapability, and knowledge of the malicious user, the model\ndeveloper, and the service provider as follows.\nMalicious User. i) Goals. A malicious user is an attacker\nwho aims to invalidate the well-performed original model\nthrough machine unlearning. The motivation of the attacker\ncan be complex and multifaceted, e.g., by disrupting the\nmodel owner's normal business (i.e., the model utility),\nthe attacker may provide a competitive advantage to rival\norganizations. ii) Capability. The capability of the attacker\nis similar to that of a normal contributor user: providing\ntraining data for training the model and requesting the\nmodel owner to unlearn exactly the data provided by the\nattacker. The difference between the attacker and the normal\ncontributor user is that the training data provided by the\nattacker is deliberately generated to achieve the goal of\nthe attack. iii) Knowledge. To simulate a practical attacker,\nwe consider the attacker has only the knowledge of the\ntraining data but without the knowledge of the target model"}, {"title": "4. Methodology", "content": "In this section, we introduce our attack strategy on\nmachine unlearning. We begin by articulating the desired\nimpact of the attack and providing a formal definition in\nthe Problem Statement. Subsequently, in the Problem For-\nmulation, we describe our method for achieving the attack\nobjective with benign data.\nOur aim is to attack machine unlearn-\ning with the objective of inducing over-unlearning. Each\ndata sample (x, y) consists of multidimensional features x\nand a label y. The neural network is denoted by the function\n$\\psi_{\\theta}(\\cdot)$, which takes the sample x as input and outputs the\nlabel y. The test set $D_{\\text{test}} = {(s_1, t_1),..., (s_{|N|}, t_{|N|})}$ is\ndeployed on the server to evaluate model performance.\n$A(\\cdot, \\cdot)$ is a unlearning method, and $\\psi_\\theta$ represents the\ntrained model. We denote $D_u$ as data uploaded by normal\nusers, where $D_u \\subset D_{\\text{train}}$, and $D_m$ as data uploaded by\nmalicious users, where $D_m \\subset D_{\\text{train}}$.\nA normal user sends an unlearning request, and the\nmodel executes $A(\\psi_\\theta, D_u)$ to perform machine unlearning.\nSubsequently, the model parameters are updated to $\\psi_{\\theta_u}$.\nSimilarly, a malicious user sends an unlearning request,\nand the model executes $A(\\psi_\\theta, D_m)$ to perform unlearning,\nresulting in the model parameters being updated to $\\psi_{\\theta_m}$. $\\alpha_u$\nrepresents the accuracy of $\\psi_{\\theta_u}$ on $D_{\\text{test}}$, and $\\alpha_m$ represents\nthe accuracy of $\\psi_{\\theta_m}$ on $D_{\\text{test}}$. According to [18], if the utility\nof $\\psi_{\\theta_m}$ on $D_{\\text{test}}$ is not greater than that of $\\psi_{\\theta_u}$ on $D_{\\text{test}}$, i.e.,\nif $\\alpha_m < \\alpha_u$, it is termed as a situation of over-unlearning.\nAccording to over-unlearning, the\nloss of model $\\psi_{\\theta_m}$ on the $D_{\\text{test}}$ after over-unlearning will\nexceed that of model $\\psi_{\\theta_u}$ after normal unlearning. Con-\nsequently, the utility of model $\\psi_{\\theta_m}$ is lower than that\nof model $\\psi_{\\theta_u}$. To achieve the goal of over-unlearning,\nwe utilize the problem objectives outlined in [25], [38],\n[39], [40], [41], [42]. We represent our attack data as\n$M = {(m_1, y_1),..., (m_{|M|}, y_{|M|})}$, and the dataset known\nto malicious users as $T = {(X_1, Y_1), ..., (X_T, Y_{|T|})}$.\nWe describe our attack data as 'Informative Benign Data,'\nwhere a small amount of data can demonstrate performance\ncomparable to that of a substantial dataset. The formula is\npresented below:\n$\\mathbb{E}_{x \\sim P_D} [l(\\psi_{\\theta_M} (x), y)] \\approx \\mathbb{E}_{x \\sim P_T} [l(\\psi_{\\theta_T} (x), y)]$\\nwhere $P_D$ represents the distribution of the training data,\n$l$ denotes the loss function (such as cross-entropy loss), $\\psi$\\nis a deep neural network characterized by the parameters $\\theta$,\nand $\\theta_T$ and $\\theta_M$ are the networks trained on datasets known\nto malicious users $T$ and on synthetic data $M$, respectively.\nTo achieve the goal set in For-\nmula 2 with a minimal amount of data, denoted by M, we\naim to synthesize M to closely approximate the true training\ndata distribution. To enhance the informativeness of these\nsamples, we employ data distribution matching methods\nGiven the high dimensionality of training images, accu-\nrately estimating the real data distribution $P_D$ is costly and\nimprecise. Therefore, we reduce each training image $x \\in \\mathbb{R}^d$\nto a lower-dimensional space using parameterized functions\n$\\varphi_\\theta : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d'}$, where $d' \\ll d$. We then use Maximum\nMean Discrepancy (MMD) [43] to estimate the distance\nbetween the actual and synthesized data distributions. As\naccess to the ground-truth data distributions is unavailable,\nwe rely on the empirical estimate of MMD:\n$$\\mathbb{E}_{P_\\theta} \\Biggl||\\frac{1}{|T|} \\sum_{i=1}^{|T|} \\varphi_\\theta(x_i) - \\frac{1}{|M|} \\sum_{j=1}^{|M|} \\varphi_\\theta(m_j) \\Biggr||^2 \\qquad (2)$$\nWhere $P_\\theta$ represents the distribution of network param-\neters, and $\\theta$ denotes the parameters for $\\varphi_\\theta$. The ex\u0440\u0435\u0441-\ntation integrates over all possible configurations of model\nparameters $\\theta$, thereby enhancing the robustness, generaliza-\ntion, and reliability of the optimization process. Following\nthe approaches outlined in [25], [44], we apply differen-\ntiable Siamese augmentation $\\mathcal{E}(\\cdot, w)$ to both real data and\n$\\mathcal{M}$. During training, we randomly sample augmentations\nfor real and synthetic minibatches, where $w \\sim \\Omega$ rep-\nresents augmentation parameters such as rotation angles\nand random cropping. By incorporating data augmentation\nin training deep neural networks, the learned $\\mathcal{M}$ benefits\nfrom semantic-preserving transformations and gains spa-\ntial knowledge about the samples. Finally, the optimization\nproblem referenced in formula 3 for $\\mathcal{M}$ is transformed to\nachieve the objectives specified in formula 1.\n$\\underset{\\mathcal{M}}{\\text{min}} \\quad \\mathbb{E}_{P_\\theta} \\Biggl||\\frac{1}{|T|} \\sum_{i=1}^{|T|} \\varphi_\\theta(\\mathcal{E}(x_i, w)) - \\frac{1}{|M|} \\sum_{j=1}^{|M|} \\varphi_\\theta(\\mathcal{E}(m_j, w)) \\Biggr||^2$$\\nWe sample $\\theta$ and learn $\\mathcal{M} = {m_j}_{j=1}^{|M|}$ by minimizing\nthe discrepancy between distributions in various embedding\nspaces. Through the optimization objective in Formula 3,\nthe resulting $\\mathcal{M}$ will acquire a substantial amount of data\ninformation.\nNotably, unlearning the informative benign data results\nin a significant shift in the neural network's decision bound-\nary, whereas unlearning normal data has a less pronounced\neffect. This variance stems from the richer information of\nthe informative benign data, which is close to the centroids\nof the distribution. When malicious users request unlearning\nof these data points, the model consequently loses crucial\ninformation that is integral to many normal samples."}, {"title": "5. Experimental Settings", "content": "We have considered three attack scenarios, where\nthe attacker has varying levels of knowledge about the"}, {"title": "6. Attack Performance", "content": "We follow the experimental settings in Section 5 to\nevaluate our attack and provide the results in this section.\nAll the evaluation results for different machine unlearning\ntechniques indicate unlearning informative benign data will\nbe much more harmful than unlearning normal data.\nIn the scenario of\nuser-developer collusion (refer to the settings in Section 5),\nthe model performance across different datasets before\nunlearning is shown in Table 3. The changes in model\naccuracy after unlearning normal or informative data are\nshown in Table 4. We observe that unlearning informative\ndata can cause a decrease in model accuracy of at least 20%\nby utilizing only 0.2% of the data.\nFor amnesiac unlearning, when the model is instructed\nto erase updates generated by informative data, the model\naccuracy significantly drops to 44.33%, 10.25% and 3.17%\non MNIST, CIFAR-10, and CIFAR-100, respectively. In\ncomparison, unlearning normal data only drops the accu-\nracy to 99.04%, 86.01% and 47.38% on those datasets.\nWhen unlearning normal data, first-order and second-order\nunlearning methods can erase the influence of data features\nwith negligible impact on model accuracy. However, when\nunlearning informative data, we observed a decrease in\naccuracy of about 30% on MNIST and more than 20% on\nCIFAR-10 and CIFAR-100 datasets. For the negative gradi-\nent method, we also observed that unlearning normal data\ndoes not significantly decrease the model's accuracy. How-\never, for our informative benign data, both test and training\naccuracy dropped by at least 20% across all datasets. We\nbelieve that when performing gradient ascent on informative\nbenign data, the updates affect more crucial parameters in\nthe model, leading to a decrease in model performance."}, {"title": "7. Analysis of Attack Effectiveness", "content": "General Analysis: Information of Informative and Nor-\nmal Data. We hypothesize that unlearning informative data\n(for all the evaluated unlearning methods) causes more\ndegradation in model accuracy because it usually contains\nmore information than normal data. To verify this hypoth-\nesis, we trained two separate networks on informative and\nnormal data, respectively. We then compared their accuracy\nto assess the information of informative and normal data.\nOur results across ResNet18, LeNet, ConvNet, and AlexNet\ndemonstrated that the model trained on informative data\nconsistently learn more information and thus achieve higher"}, {"title": "8. Resistance Against Poisoning Defenses", "content": "In this section, we examine the effectiveness of poisoned\ndata detection and defense mechanisms against informative\nbenign data, as the success of such attacks hinges on the\ndata's ability to bypass poisoned data detection tools in an\nautomatic machine unlearning pipeline. Previous research\nhas focused on detecting poisoned samples within datasets\nto protect models. If the prevailing automatic poisoning de-\nfensive mechanisms can detect and remove our informative\nsamples, our attack strategy may fail. We evaluated the\nresilience of our informative data against both passive and\nactive defenses on the CIFAR-10 dataset.\nTran et al. [49] observed\nthat, while poisoned samples crafted by attacks like BadNets\nand Blend do not differ significantly from normal samples\nin the data space, their latent representations are mostly\nseparable from the representations of normal samples. Based\non this observation, Tran et al. [49] further proposed to\nuse the top right singular value of the representation as a\nmeasure to separate the representations, which yields better\nresults than using $l_2$ distance as a measure.\nHere we assess if the informative benign data is sepa-\nrable from normal data in both data and latent spaces. We\nshow the results of using the $l_2$ distance as the measure in\nFigure 9 and the results of using the top right singular value\nas a measure in Figure 10. The results showed that, our\ninformative data does not exhibit separation from normal\nsamples in both data and latent spaces under both of the\nmeasures. Therefore, Tran et al.'s method [49] can not detect\nour informative data, while it is very effective to detect\npoisoned data.\nHayase et al. [50] proposed a defense called SPEC-\nTRE that can effectively identify poisoned samples, even\nwhen the poisoning rate is low. SPECTRE estimates the\nmean and covariance of clean data and then whitens the\ndata to align the initial PCA directions with the subspace\ndifferentiating poisoned from clean samples. Hayase et al.\n[50] further proposed the Quantum Entropy (QUE) outlier"}, {"title": "9. Conclusion", "content": "This paper introduces the concept of the \"unlearning\nusability attack\" as a novel approach to overcoming the lim-\nitations present in current unlearning attack strategies. Our\nmethod differs from conventional approaches by avoiding\nthe use of perturbed samples in unlearning requests, thus\nenabling attacks on automated machine unlearning systems\nwhile circumventing hash-based checks. We present three\nplausible attack scenarios: User-Developer Collusion, User\nCollusion, and Independent Users. Through these scenar-\nios, our study illustrates that even within black-box envi-\nronments, we can induce significant unlearning in models\nby leveraging Informative Benign Data. Additionally, our\ngenerated informative benign data demonstrates robust resis-\ntance against both passive and active defense mechanisms,\nprompting a reassessment of the conventional understanding\nof \"poisoned data\" in machine unlearning.\nGiven the increasing importance of machine unlearning\nservices amid escalating privacy concerns, our study high-\nlights the inherent vulnerabilities of these techniques within\nMLaaS frameworks, which lays the groundwork for under-\nstanding and mitigating these risks. Future research may\nfocus on developing more robust unlearning procedures and\nenhancing data detection capabilities to maintain a balance\nbetween data privacy, model functionality, and security in\nMLaaS environments."}, {"title": "Appendix A.\nAblation Studies", "content": "By maintaining injection rates below 1%, we observed\nthat increasing these rates had minimal to negligible impact\non the network's original accuracy; in some instances, ac-\ncuracy even improved, which is less likely to cause concern\nin collaborative settings. Furthermore, as the injection rate\nincreased, the decline in post-unlearning accuracy in the\nnetwork became more pronounced compared to lower rates.\nThis phenomenon is illustrated in Figure 15. This trend\nsuggests that higher injection rates more effectively encap-\nsulate users' data information within the informative benign\nsamples, resulting in a greater impact on model performance\nafter unlearning.\nWe use various network architectures to generate in-\nformative benign data as our informative dataset, aiming\nto explore whether these data are effective when attacking\ndifferent network architectures. Through a series of experi-\nments, attacks were executed on multiple network structures"}, {"title": "Appendix B.\nThe impact of two types of one image on\nunlearning", "content": "The effect of separately training two networks with one\nnormal and one informative benign data from each class\non machine unlearning. We trained the model separately\nusing real data and informative data. Subsequently, we\napplied the unlearning method to individually unlearn real\ndata and informative data, observing the impact of these two\ntypes of data on the model's accuracy. For our informative\ndata, we generated informative benign data using the Con-\nvolutional (Conv) architecture. Additionally, we created one\ninformative image (1Img/Cls) for each class as training data.\nSimultaneously, we randomly selected one image (1Img/Cls)\nfrom the dataset for each class as training data. Following"}]}