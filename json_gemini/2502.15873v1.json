{"title": "Practical Principles for AI Cost and Compute Accounting", "authors": ["Stephen Casper", "Luke Bailey", "Tim Schreier"], "abstract": "Policymakers are increasingly using development cost and compute as proxies for AI model capabilities and risks. Recent laws have introduced regulatory requirements that are contingent on specific thresholds. However, technical ambiguities in how to perform this accounting could create loopholes that undermine regulatory effectiveness. This paper proposes seven principles for designing practical AI cost and compute accounting standards that (1) reduce opportunities for strategic gaming, (2) avoid disincentivizing responsible risk mitigation, and (3) enable consistent implementation across companies and jurisdictions.", "sections": [{"title": "1. Introduction", "content": "As artificial intelligence systems become more capable, policymakers face a mounting challenge: identifying which AI systems warrant heightened oversight. Recent laws and governance frameworks have approached this challenge by making regulatory requirements contingent on development costs and computational resources (e.g., European Parliament and Council, 2024; California SB1047, 2024). For example, the U.S. Bureau of Industry and Security's 2025 Framework for Artificial Intelligence Diffusion controls the export of AI model weights for systems whose development exceeded 1026 floating point operations (FLOP) in training compute (U.S. Bureau of Industry and Security, 2025).\nDevelopment costs and compute are compelling metrics for use in Al governance because they \u201ccorrelate with capabilities and risks, are quantifiable, can be measured early in the AI lifecycle, and can be verified by external actors\" (Heim & Koessler, 2024). Cost and compute thresholds also allow regulators to focus oversight on the most advanced Al models while avoiding unnecessary burdens on smaller developers. However, their practicality as regulatory tools depends on establishing clear accounting standards. This requires resolving several technical ambiguities about what should be counted (Hooker, 2024; Heim & Koessler, 2024; Reuel et al., 2024).\nThis paper asks the question: How can the cost and compute used during model development be counted in a way that is practical, limits gameability, and avoids disincentivizing responsible risk management? Key challenges include determining which activities to count, establishing reporting requirements, and allowing for standards to adapt as technology evolves.\nTo address these challenges, we propose seven principles for designing practical AI cost and compute accounting standards. We argue that these principles can resolve technical ambiguities while aligning with public interest and enabling consistent implementation across companies and jurisdictions. While we do not take positions on specific laws, thresholds, or requirements, our framework provides policymakers and standards bodies with a foundation for developing robust standards for AI cost and compute accounting."}, {"title": "2. Background", "content": "Related work. Al research has studied \"scaling laws\" demonstrating how AI model performance improves predictably with increased computational resources and data (Kaplan et al., 2020; Villalobos, 2023; Sevilla & Rold\u00e1n, 2024). While these relationships provide a scientific basis for the theoretical value of cost and compute thresholds in AI regulation (Sastry et al., 2024), researchers have identified important limitations and highlighted unsolved implementation challenges. Without standardized methodology, complex technical questions about which activities to count and how to report such counts remain unresolved (Hooker, 2024; Heim & Koessler, 2024).\nTo our knowledge, the only openly-available set of guidelines for cost and compute accounting has been previously published by the Frontier Model Forum (2024), a collaboration between major tech companies that represents industry interests (Wei et al., 2024). Here, we echo the Frontier Model Forum (2024) in the position that certain practical approximations of cost/compute should be permissible (Section 3.4). However, the forum also recommends allowing"}, {"title": "3. Principles", "content": "Here, we discuss seven principles for developing cost and compute accounting standards. As presented in Figure 1, Section 3.1 - Section 3.3 cover what to count and not to count, Section 3.4 - Section 3.5 cover how to perform and report on counts, and Section 3.6 - Section 3.7 cover how to design thresholds.\n3.1. Count all of a project's expended costs and compute\nPrinciple: Count all technical costs and compute that the developer expends in the process of developing an AI model, not simply theoretical, proximal, or upstream ones.\nPurpose: Closing loopholes, and limiting the gameability of accounting standards.\nDevelopers tend to undertake a variety of activities during the process of developing frontier AI models. However, a narrow view of what counts could be used to exclude certain activities integral to the model development process. For example:\n\u2022 Some activities are not theoretically needed for the final model to have been produced. For example, in the process of training models, there are often many multiplications or additions by zero due to the use of dropout (Srivastava et al., 2014) and sparsity (e.g., Correia et al., 2019). Even though they are not theoretically needed, they are carried out by hardware nonetheless and are often used to improve performance.\n\u2022 Some activities are not proximal to the model's training process. For example, dataset creation/curation/compression (e.g., Kaddour, 2023; Solaiman & Dennison, 2021; Chen & Mueller, 2024) or"}, {"title": "3.2. Exclude costs and compute behind pre-existing, openly-available resources", "content": "Principle: Count costs and compute that the developer directly incurs through their activities, purchases, and partnerships. Exempt the costs and compute used to produce open resources that developers obtain for free from others.\nPurpose: Practicality and focusing on proprietary resources.\nDevelopers can produce capable models through multiple sources of cost and compute. They often curate their own data and train their own models in-house. However, they can also purchase resources, query systems from external providers, and outsource parts of the development process to partners. For the reasons outlined in Section 3.1, these are generally needed for thorough accounting.\nBut, what about open resources that developers obtain freely from others? For example, a developer may simply download an open model or dataset from the web. In this case, there are two reasons not to include this in compute and cost estimates. First, due to inconsistencies with model (Mitchell et al., 2019) and data (Longpre et al., 2023) provenance, it will often be intractable to find precise information on the costs or compute behind open models, systems, and data. Second, because such resources are already openly available, they offer a zero-effort floor for widely available capabilities.\nHowever, one modification to this exemption may be necessary to close a loophole. If resources that were recently (e.g., within 6 months prior to when a model's development begins), openly released by the same developer in question, regulators may wish to require that this resource is still counted. Without this exception, developers would be able to openly release partially-developed model components (e.g., a pretrained base model) in order to exclude them from accounting.\nAs a final note, regulators may wish to uniquely handle cases in which a developer begins with an open model whose development already passed thresholds and further develops it. For example, a failed 2024 California bill (California SB1047, 2024) defined a \"covered\" model in terms of either a primary threshold or a secondary threshold for when additional development is applied to an existing \u201ccovered\""}, {"title": "3.3. Exclude activities undertaken only to reduce societal risks", "content": "Principle: Allow developers to exempt activities undertaken only for the purpose of reducing risks to society which do not have side effects of enhancing model capabilities.\nPurpose: Not disncentivizing societal risk-reduction practices.\nOver the course of developing an AI model, most key activities are undertaken either partially or entirely to improve its capabilities. For example, pretraining and fine-tuning are principally meant to make models more capable and useful. However, some activities are undertaken strictly to reduce risks. Examples include filtering child sexual abuse material (CSAM) from training data (e.g., Thiel, 2023), fine-tuning models to refuse criminal requests (e.g., Yuan et al., 2024), and testing for national security risks (e.g., Shevlane et al., 2023). To avoid disincentivizing risk mitigation measures, developers must be allowed to exclude these types of activities from their accounting.\nHow should it be determined when an activity is undertaken only for the purpose of mitigating societal risks? This can be difficult due to the lack of a clean dichotomy and the prevalence of \"safetywashing\" (Ren et al., 2024). To mitigate this challenge, developers can be required to produce a rigorous, auditable justification for why an activity only reduces risks without simultaneously increasing capabilities in an accounting report (see Section 3.5)."}, {"title": "3.4. Allow for reasonable estimates", "content": "Principle: When counting costs and compute used for a model's development, developers should be permitted (and often expected) to use reasonable estimations when precise information is not practically attainable.\nPurpose: Practicality.\nInformation about costs and compute expended during a model's development is not always precisely quantifiable. For example, developers will often not know exactly how much compute has been expended when they query a closed-source system from some outside provider. However, in a case like this, reasonable estimates can be made based on contextual knowledge and the market value of compute (Sevilla et al., 2022; Cottier et al., 2024). These types of estimates would be analogous to how similar estimates of the \"fair value\u201d of assets are commonly used in financial accounting (IFRSF, 2022). In accounting, imprecision in some items will be inevitable, but to reduce the risk of this being gamed or resulting in unreliable counts, developers can be required to provide a report on their approach to accounting that documents included estimates (see Section 3.5). Meanwhile, regulators or standards bodies could publish guidance on appropriate estimation methodologies, tolerable error margins, and suitable documentation templates."}, {"title": "3.5. Require itemized accounting reports", "content": "Principle: Require developers to produce an auditable, itemized accounting report detailing their approach, including justifications for estimates and exemptions.\nPurpose: Transparency and accountability.\nIn financial accounting, companies are regularly required to send records and reports to governing bodies (e.g., U.S. SEC, Division of Corporation Finance, 2017). This has both the direct effect of helping government oversight offices spot issues and the indirect effect of incentivizing due diligence from companies. The same applies to AI cost and compute accounting. These reports would also be key for developers to provide explanations and justifications for technical exemptions (Section 3.1), open resource exemptions (Section 3.2), risk mitigation exemptions (Section 3.3), and estimations (Section 3.4). Such reports would improve accountability around accounting practices and inform regulators about industry trends in development expenditures."}, {"title": "3.6. Use independent thresholds for costs and compute", "content": "Principle: Regulatory requirements should be independently triggered by separate thresholds for cost and compute.\nPurpose: Closing loopholes, and limiting the gameability of accounting standards.\nCost and compute are correlated, but they can still be decoupled, especially when developers have an incentive to game standards. For example, machine-generated data is cheap but computationally intensive while human-generated data is expensive but computationally free. A developer could design a project to be low-cost/high-compute or vice versa by adjusting the extent to which they use machine- versus human-generated data. As a result, having both cost and compute triggers would reduce gameability.\nSeparate cost and compute thresholds can also serve as insurance for each other in case of error or fraud. For example, if a developer purchases queries or other services from an external provider, precise information on the amount of compute used might not be available, but the costs are unambiguous and auditable. Meanwhile, different computing devices can use different amounts of power to perform the"}, {"title": "3.7. Require regular updates to thresholds and standards", "content": "Principle: Require that thresholds and accounting standards are regularly updated to reflect technological developments.\nPurpose: Ensuring standards and thresholds remain effective by adapting to technological advances and evolving societal needs.\nIn AI, state-of-the-art systems and methods change rapidly. Over time, it is not clear how trends in scaling training, scaling inference, and computational efficiency will affect costs and compute in the development of frontier AI models (Pilz et al., 2023). Accordingly, governance frameworks will need to be adaptive to ensure they remain relevant over time. To regulate incisively, government offices and/or standards bodies will need to revisit and curate standards on a regular (e.g., quarterly or semiannual) basis in response to new developments in the state-of-the-art."}, {"title": "4. Discussion", "content": "Significance: Regulating AI is challenging. It is an emerging technology shrouded in uncertainty about what impacts it will have and how it will evolve. Regulatory thresholds involving cost and compute, however, are a uniquely practical (Heim & Koessler, 2024) yet technically challenging (Hooker, 2024) strategy for designing regulations that target frontier AI models. Technical ambiguities in how cost and compute are counted are a central challenge to their effective use. Lacking sound accounting standards could result in these counts being insufficiently useful proxies for risk. Furthermore, under loose guidelines for how costs and compute are counted, developers will have a strong incentive to engage in \"creative compliance\" (Shah, 1996) to actively game them. To make cost and compute thresholds more tenable as a regulatory strategy, standards for accounting must be clear, consistent, tight, and aligned with public interest. To support the development of such standards, we have proposed a principles-first framework to resolve ambiguities and introduced seven principles designed to reduce gameability, avoid disincentivizing societal risk mitigation practices, and enable consistent implementation across companies and jurisdictions.\nLimitations: This work was not written in the context of any specific law. We make no recommendations about what kinds of regulatory requirements should be triggered and how. Key questions about how high to set thresholds, what they should trigger, and how they should be incorporated into a broader governance framework are all beyond the scope of this paper. Furthermore, as we have discussed, while the principles discussed here can resolve much am-"}]}