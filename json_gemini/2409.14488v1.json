{"title": "Enhancing LLM-based Autonomous Driving Agents to Mitigate Perception Attacks", "authors": ["Ruoyu Song", "Muslum Ozgur Ozmen", "Hyungsub Kim", "Antonio Bianchi", "Z. Berkay Celik"], "abstract": "There is a growing interest in integrating Large Language Models (LLMs) with autonomous driving (AD) systems. However, AD systems are vulnerable to attacks against their object detection and tracking (ODT) functions. Unfortunately, our evaluation of four recent LLM agents against ODT attacks shows that the attacks are 63.26% successful in causing them to crash or violate traffic rules due to (1) misleading memory modules that provide past experiences for decision making, (2) limitations of prompts in identifying inconsistencies, and (3) reliance on ground truth perception data.\nIn this paper, we introduce HUDSON, a driving reasoning agent that extends prior LLM-based driving systems to enable safer decision making during perception attacks while maintaining effectiveness under benign conditions. HUDSON achieves this by first instrumenting the AD software to collect real-time perception results and contextual information from the driving scene. This data is then formalized into a domain-specific language (DSL). To guide the LLM in detecting and making safe control decisions during ODT attacks, HUDSON translates the DSL into natural language, along with a list of custom attack detection instructions. Following query execution, HUDSON analyzes the LLM's control decision to understand its causal reasoning process.\nWe evaluate the effectiveness of HUDSON using a proprietary LLM (GPT-4) and two open-source LLMs (Llama and Gemma) in various adversarial driving scenarios. GPT-4, Llama, and Gemma achieve, on average, an attack detection accuracy of 83. 3%, 63. 6%, and 73. 6%. Consequently, they make safe control decisions in 86.4%, 73.9%, and 80% of the attacks. Our results, following the growing interest in integrating LLMs into AD systems, highlight the strengths of LLMs and their potential to detect and mitigate ODT attacks.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have been widely adapted to various fields, including robotics [5, 60], medicine [43],"}, {"title": "2 Background", "content": "Vehicle Control with LLMs. A line of recent work has demonstrated the capabilities of LLMs to be integrated into AD control systems [13, 63]. These works can be grouped into text-based and multimodal-based.\nText-based approaches construct queries for LLMs by establishing structured thought processes and output high-level control decisions (e.g., changing lanes and/or decreasing speed) [20, 46, 57, 59]. They first obtain environmental contexts, such as nearby vehicles or pedestrians, from the simulator. After reasoning environmental contexts with rules for safe driving (e.g., maintaining a 3-second spacing with a front vehicle [57]), the LLM makes a control decision.\nMultimodal-based approaches integrate LLMs with a visual language model (VLM), allowing them to take raw vision sensor readings (e.g., LiDAR and camera) as input [1, 50, 56]. For example, a recent work [56] uses a multimodal tokenizer and a multimodal large language model (MLLM) decoder to output language-based decisions that align with vehicle control commands. Another work [50] structures the AD pro- cess into a series of interconnected question-answer pairs, following logical dependencies at the object and task levels, including perception, prediction, and planning.\nSystem Prompts and Chain of Thought. System prompts are textual instructions that guide LLMs to generate the desired output. Thus, they are essential to improve the quality"}, {"title": "3 Threat Model", "content": "We study the potential of LLMs for safe control decision-making in the presence of following perception attacks against ODT of AVs: (1) Object Creation [10, 42, 62], (2) Object Deletion [8, 25], (3) Object Misclassification [17, 18, 36], (4) Bounding Box Move-in [26, 41], and (5) Bounding Box Move-out [26, 41]. The goal of the adversary is to jeopardize the AV and cause it to collide or stop unnecessarily.\nOur methodology is agnostic to the specific techniques used in ODT attacks, e.g., using adversarial patches or ma- nipulating light for object misclassification. This is achieved from our reliance on a code instrumenter (See Section 5.1) to extract AV states from the AV control software for prompt generation. The instrumenter operates independently of the attack generation process, which allows us to explain the AV's behavior regardless of the attack type.\nWe assume that the adversary cannot conduct any percep- tion attacks before the perception system is initialized. Therefore, the victim has access to multiple frames of untampered perception and tracking results. We assume that communications are cryptographically secure between the AD software and HUDSON (black dashed lines in Figure 1). Therefore, the adversary cannot tamper with 1) perception results from the AV software to HUDSON, 2) any communication inside HUDSON and 3) the output of HUDSON to the AV."}, {"title": "4 Problem Statement and Motivation", "content": "In this section, we study the decision-making capabilities of existing text-based LLM agents [37,38,47,59] for AD sys- tems under adversarial attacks targeting the ODT. We exclude the multi-modal based approaches due to their vulnerabil- ities when facing adaptive attackers who are aware of the internal operation of the AD systems. Such attackers can tar- get both the perception module and the downstream MLLM model. For example, as recently shown in [4, 14, 44], pro- jecting noise or attaching physical patches can disrupt the perception module and also compromise the MLLM model. For example, in Figure 2, if the ego vehicle uses an MLLM approach (e.g., LLaVa), an attacker can launch an adversar- ial attack that causes the MLLM model to ignore all traffic signs and pedestrians. This attack, despite multimodal input, will still lead to a collision with the pedestrian because the MLLM will ignore the pedestrian due to the adversarial at- tack. However, for text-based LLM agents, the attacker cannot tamper with the agent's system prompt, which could identify the inconsistency and make the safe control decision.\n4.1 Experiment Setup\nTo generate realistic adversarial driving scenes with reason- able physical consequences, we analyze agents and their dy- namics in benign driving scenes and generate the most suit-"}, {"title": "5 HUDSON: An Attack-aware Reasoning Agent", "content": "We introduce HUDSON, a driving reasoning agent, which im-proves state-of-the-art LLM-based driving agents with ad- ditional components to make safer decisions against ODT attacks while remaining effective in benign driving scenarios.\nSystem Overview. Fig. 3 illustrates the architecture of HUD- SON, which consists of four key components: a code instru-"}, {"title": "6 Evaluation", "content": "We use HUDSON to evaluate the capabilities of LLMs' in iden- tifying and defending against adversarial attacks on percep- tion modules. For the object detection function, we evaluate LLMs against misclassification, object removal, and object creation attacks. For the object tracking function, we evaluate LLMs against bounding box move-in and bounding box move-out attacks. Our results show that our attack-aware LLMs can effectively identify implausible scenes and suggest correct driving decisions to mitigate ODT attacks. We present our results by focusing on the following research questions.\nRQ1 What is the defense success rate (i.e., the percentage of safe driving decisions) of LLMs for each attack? (Section 6.2)\nRQ2 What is the defense success rate for each attack when different numbers of history frames are given to LLMs? (Section 6.2.1)\nRQ3 What is the performance of LLMs in benign driving scenarios (Section 6.3)?\nRQ4 What is the defense success rate of each attack without our prompt engineering (Section 6.4)?\nRQ5 What is the root cause for LLMs identifying and de- fending each attack (Section 6.5)?\nWe run HUDSON on a desktop with Intel i9-13900K CPU, 64GB RAM, GTX 2080Ti GPU, running Ubuntu 20.04.\n6.1 Experimental Setup\nWe generate a set of driving scenes, benign and adversarial, to evaluate the performance of three LLMs (GPT-4-turbo,"}, {"title": "7 Discussion and Limitations", "content": "Query Overhead. We measure the query overhead of HUD- SON with GPT-4, Llama-3-8B, and Gemma-1.1-7B. It takes 911.33 input tokens and 233.71 output tokens per query with GPT-4; 1048.67 input and 107.82 output tokens with Llama; and 986.13 input and 188.53 output tokens with Gemma. For time overhead, when HUDSON runs with GPT-4, it takes an av- erage of 11.8 seconds for a response, 3.23 seconds for Llama, and 2.39 seconds for Gemma. To make LLM-based agents cooperate with AD software in real-time, future work may reduce both the input and output tokens of the LLM.\nOther Types of AV Attacks. In the threat model of HUD- SON, we consider attacks that target an AV's vision sensors (e.g., LiDAR and camera). Yet, adversaries can also conduct"}, {"title": "8 Conclusion", "content": "We introduce HUDSON, an attack-aware LLM-based driving reasoning agent to detect and avoid perception attacks against autonomous vehicles. HUDSON instrumented and collected perception data from AD software, generated text description of the driving scene, and queried with driving instructions for detecting inconsistencies and response format for causal reasoning. We evaluated HUDSON in 3300 adversarial driving scenes with 13 attack goals in 10 weather conditions and three"}, {"title": "9 Ethics Considerations and Compliance with the Open Science Policy", "content": "This paper investigates the impacts of existing ODT attacks against text-based LLM driving agents. We introduce an algo- rithm for generating adversarial driving scenarios that targets on ODT components based on the dynamics of benign driv- ing scenarios. To limit any real-world harm, we generate all ODT attack scenarios using the CARLA simulator. We also propose and evaluate a new driving agent, HUDSON, which improves the performance of prior text-based LLM driving agents to make safe driving decisions against ODT attacks. To foster the future work on the security of LLM driving agents, we have made our adversarial driving scenarios available at https://tinyurl.com/llm-ad-eva."}]}