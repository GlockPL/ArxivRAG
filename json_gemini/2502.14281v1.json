{"title": "Correcting Noisy Multilabel Predictions: Modeling Label Noise through Latent Space Shifts", "authors": ["Weipeng Huang", "Qin Li", "Yang Xiao", "Cheng Qiao", "Tie Cai", "Junwei Liang", "Neil J. Hurley", "Guangyuan Piao"], "abstract": "Noise in data appears to be inevitable in most real-world machine learning applications and would cause severe overfitting problems. Not only can data features contain noise, but labels are also prone to be noisy due to human input. In this paper, rather than noisy label learning in multiclass classifications, we instead focus on the less explored area of noisy label learning for multilabel classifications. Specifically, we investigate the post-correction of predictions generated from classifiers learned with noisy labels. The reasons are two-fold. Firstly, this approach can directly work with the trained models to save computational resources. Secondly, it could be applied on top of other noisy label correction techniques to achieve further improvements. To handle this problem, we appeal to deep generative approaches that are possible for uncertainty estimation. Our model posits that label noise arises from a stochastic shift in the latent variable, providing a more robust and beneficial means for noisy learning. We develop both unsupervised and semi-supervised learning methods for our model. The extensive empirical study presents solid evidence to that our approach is able to consistently improve the independent models and performs better than a number of existing methods across various noisy label settings. Moreover, a comprehensive empirical analysis of the proposed method is carried out to validate its robustness, including sensitivity analysis and an ablation study, among other elements.", "sections": [{"title": "I. INTRODUCTION", "content": "MULTILABEL classification is a task where a single data point can be associated with multiple labels [1]. In contrast, multiclass classification assigns only one single label to each data point, and is thought of as a simpler task [2]. For modern deep learning problems, in particular for multilabel classifications, model performance is hugely impacted by the data and its labeling quality. However, label noise can be generated through various means and is often inevitable when creating large datasets, particularly due to human errors [3]\u2013[8]. For instance, inconsistencies may occur because multiple labeling experts, working independently, have differing subjective interpretations of the labeling guidelines. On the other hand, implementing a shared labeling process that relies on majority voting can prove to be costly for large and complex data. It is therefore crucial to tackle the challenges posed by noisy labels in multilabel classification tasks.\nSo far, the research of noisy label learning has mostly focused on the problem of multiclass classification, including [3]\u2013[15], to name but a few. As highlighted in the survey [16], label noise in multilabel classification tasks is more difficult to handle for two main reasons: 1) learning the label correlation from noisy labels is non-trivial to settle; 2) more importantly however, the label sparsity/imbalance leads to a more challenging situation. Consequently, research of noisy label learning in multilabel classification has been less active. In particular, Xia et al. [17], enhancing the method of [18], propose a statistical score to leverage the label correlation for noisy label correction during the training phase, before the model overfits prohibitively. More recently, Chen et al. [19] propose a unified framework incorporating semantic embedding and label embedding to design label noise resilient models. To the best of our knowledge, there is no prior work exploring how predictions from models built with noisy multilabels can be corrected.\nOur goal is to develop a general framework which is capable of correcting the predictions made by a classifier trained with noisy labels, for minimizing the gap to the true labels. Bae et al. [9] first propose the technical path of calibrating the noisy predictions (namely post-processing). Their model\u2014 noisy prediction calibration (NPC)\u2014copes with noisy labels for multiclass classifications. This stream of methods preserves two advantages: 1) we can apply it to the pre-trained models without easy access to re-training; 2) we can employ it on top of other noisy label handling techniques to acquire further improvements. In NPC [9], a deep generative approach which adopted and a variational auto-encoder (VAE) [20] was proposed to infer the generative model. The success of this work heavily relies on the property of multiclass classification and thus cannot be trivially extended to the case of multilabel classification. We will fully characterize this disconnection in Section IV, and present empirical evidence to support our statements in Section VI-B.\nThis work adheres to the deep generative modeling procedure that first establishes a parameterized generative process involving the use of deep neural networks. Incorporating uncertainty by considering a range of probability distributions proves to be effective in combating the impact of noise. Thereafter, we estimate the corresponding parameters through learning from the observed data. In a typical VAE, one usually defines a latent"}, {"title": "II. RELATED WORKS", "content": "Noisy labels are inevitable in realworld datasets, posing a significant challenge to the generalization ability of deep neural networks trained on them. These deep neural networks can even easily fit randomly assigned labels in the training data, as demonstrated in [23]. To tackle this challenge, many studies have been proposed for learning with noisy labels. For example, noise-cleansing methods primarily aim to separate clean data pairs from the corrupted dataset using the output of the noisy classifier [6], [24]\u2013[32]. Another line of research focuses on designing either explicit regularization or robust loss functions for training to mitigate the problem of noisy labels [7], [33]\u2013[36]. To explicitly model noise patterns, another branch of research suggests using a transition matrix T to formulate the noisy transition from true labels to noisy labels [37]\u2013[41].\nDifferent from these lines of research, Bae et. al [9] introduced NPC (Noisy Prediction Calibration), which is a new branch of method working as a post-processing scheme that corrects the noisy prediction from a pre-trained classifier to the true label. Despite these endeavors of tackling learning with noisy labels, a recent survey [42] points out that the majority of the existing methods are applicable only for a single-label multiclass classification problem, and more research is required for the multilabel classification problem where each example can be associated with multiple true class labels. Our approach focuses on multilabel classification along with recent studies. More recently, several works have been proposed for multilabel classification with noisy labels. For example, HLC [17] uses instance-label and label dependencies in an example for follow-up label correction during training. UNM [19] uses a label-wise embedding network that semantically aligns label- wise features and label embeddings in a joint space and learns the co-occurrence of multilabels. The label-wise embedding network cyclically adjusts the fitting status to distinguish the clean labels and noisy labels, and generate pseudo labels for the noisy ones for training. Our approach is orthogonal to these studies, and can be seen as a post-processor similar to NPC [9] for multiclass classification. In other words, our method can be used to correct the predictions of a pre-trained classifier such as HLC, and further improve the performance as a post-processor as we show in Section VI."}, {"title": "III. PROBLEM SETUP", "content": "This section formally describes the technical problem. Let us consider a multilabel classification task containing k labels. Let $x \\in X \\subseteq \\mathbb{R}^d$ be a d-dimensional data point and $y \\in \\mathcal{Y} = \\{0,1\\}^k$ be the true corresponding label vector where each element is binary-1 indicates that the data point contains the corresponding label, whereas 0 indicates the opposite. The dataset D is defined by $D = \\{(x_i, y_i)\\}_{i=1}^n$. Given our task for handling the noisy labels, the observable labels \u1ef9 might be shifted from the true label y. We thus denote the observed dataset as $\\tilde{D}$ where $\\tilde{D} = \\{(x_i, \\tilde{y}_i)\\}_{i=1}^n$. Finally, we denote the Kullback-Leibler divergence (KL-divergence) by $D_{KL}$.\nThe focus of this paper is the prediction correction for a pre-trained classifier. The main reason is that this stream of approaches does not conflict with the most other existing methods which try to denoise during the training phase. In practice, even with the denoising training techniques, the trained classifiers are probably still biased and can benefit from our approach. We now consider an arbitrary classifier $h : \\mathcal{X} \\rightarrow \\mathcal{Y}$ which is trained using the observed data $\\tilde{D}$. We write the prediction as $\\hat{y} = h(x)$. If the model $h(\\cdot)$ is stochastic, $\\hat{y} \\sim h(x)$ can be interpreted as the predictive posterior probability\n$\\hat{y} \\sim p(\\hat{y} | x, h)$\n$p(\\hat{y} | x, h) = p(\\hat{y} | x, \\tilde{D}) =: p_h(\\hat{y} | x),$\nas $h(\\cdot)$ is trained on $\\tilde{D}$. Compared with the raw noisy labels $\\tilde{y}$, $p_h(\\hat{y} | x)$ can be regarded as an approximated distribution over the noisy labels and thus enable the Bayesian modeling. The task is to learn a label transition model $C(\\cdot)$ for a noisy model $h(\\cdot)$ which takes the data as input and outputs the corrected"}, {"title": "IV. EXTENSION OF NPC", "content": "NPC [9] focuses on multiclass classification and thus we denote the label variable by y. Considering the reconstructed label for x is $Y_{recon}$, the VAE loss is defined as\n$\\mathcal{L}_{recon}(Y_{recon}, \\hat{y}) + D_{KL}[q(Y_{recon} | x, y) || P_h(\\hat{y} | x)].$   (1)\nHere, $\\mathcal{L}_{recon}$ is the reconstructed loss which is a cross entropy for the categorical distribution (the primary choice of distribution for handling multiclass classification). The second term is the model regularization where the authors assume a prior of generating \u0177 by the noisy classifier.\nNPC posits that, given an input x, the latent variable for reconstructing the noisy label \u0177 is the true label y. However, the connection between the latent variable, i.e., the true label y, and the noisy label \u0177 is constructed through a parameterized neural network. This lacks a directional connection between the latent variable and the noisy label and therefore may lead to an enormous space for parameter search. In a traditional Bayesian modeling process, one may design and combine the best existing statistical models to describe the transformation from \u0177 to y. Also, most of the classical Bayesian models have only a few parameters to learn, and their contraction rates and asymptotics [43] are well proven, which makes them more predictable and directed models. From an optimization perspective, the success of NPC thus lies in the regularization term which restricts the decoder $q(Y_{recon} | x, y)$ to be close to the noisy classifier $P_h (\\hat{y} | x)$, in which \u0177 serves as the ground truth. Nevertheless, the NPC allows the model to search for a distribution closer to the classifier's prediction distribution while a reasonable amount of uncertainty allows the model to search for better parameters for the distribution.\nAccording to the rationale, the extension to a multilabel case will fail since the regularization term will be greatly impacted by the sparsity of the labels. The only choice for the multilabel variables y and \u0177 will be the multivariate Bernoulli distribution. For two discrete multivariate Bernoulli distributions P and Q, the KL-divergence is\n$D_{KL} [P || Q] = \\sum_i P_i log \\frac{P_i}{q_i} + (1 - p_i) log \\frac{1 - p_i}{1 - q_i}$\nImagine that there are e.g. 20 labels, while, in each instance, there are only 1-3 labels assigned the value of 1. If two multivariate Bernoulli both set very high probabilities for the labels that are assigned 0, the KL-divergence between them is still small as most of the labels are matched and thus the overall difference is amortized. Then the regularization term loses its ability to guide the parameter search in the space."}, {"title": "V. NPC FOCUSING ON LATENT VARIABLE SHIFT (LSNPC)", "content": "Our approach regards the generation of label noise as a random shift in the latent space. Assume that a latent variable z (along with the observation x) is a main factor involved in generating the true label, then 2 is a randomly shifted variable that plays a role in generating the observed labels (which are potentially noisy). In line with the Bayesian modeling procedure, we first detail its generative process and subsequently analyze the posteriors of the model for different learning paradigms.\nFig. 1 exhibits the graphical model of the generative process. The left graph is for the unsupervised setting while the right graph is for the supervised setting. The only difference is whether the true labels y is observed. Next, the latent variable $Z \\in \\mathcal{Z} \\subset \\mathbb{R}^M$ is the variable for generating the true labels y, while $\\tilde{z} \\in \\mathcal{Z}$ is the shifted latent variable, which combines with x to generate the noisy labels \u0177. In our scenario, \u0177 is a sample drawn from the pre-trained classifier h(x). This modeling strategy follows that of the Bayesian linear regression [45], which omits the generation of the observation x but treats it as an input covariate, because this is not the component of interest to our task. Following the strict Bayesian convention to reconstruct x is unnecessary and might raise the difficulty of learning of true labels. The rationale is that its role in the objective function could potentially distract the learning process from focusing on learning the true labels.\nWe depict the specifications in Fig. 1 before elucidating all the details:\n$z \\sim Normal(0, I)$\n$\\tilde{z} \\sim Student(g_{\\psi} (z), I, \\nu_0)$\n$y | z, x \\sim Bernoulli(g_{\\phi}(x, z))$\n$\\hat{y} | \\tilde{z}, x \\sim Bernoulli(g_{\\phi}(x, \\tilde{z}))$.\nThe latent variable z is a typical multivariate Normal distribution with zero mean and identity covariance matrix. Then, 2 is a random variable of the potentially \"shifted neighbor\" of z. We"}, {"title": "A. Generative Process", "content": "In this subsection, we detail the proposed distributions of this generative model, crucial for learning the generative models in VAEs. We begin with the unsupervised learning setting, followed by the supervised setting. Combining the two parts, we can derive the semi-supervised learning solution.\n1) Unsupervised Learning: For the unsupervised fashion where y is unobserved, we consider the following marginal probability p(x, y). Let us denote the corresponding evidence lower bound (ELBO) by\n$ELBO = E_{z,\\tilde{z}\\sim q(z,\\tilde{z} | x, \\hat{y})}[log \\frac{p(x, y, z, \\tilde{z})}{q(z, \\tilde{z} | x, \\hat{y})}]$\nHence, we show\n$log p(\\hat{y}, x) \\geq E_{z,\\tilde{z}\\sim q(z,\\tilde{z}|x,\\hat{y})} [log \\frac{p(\\hat{y} | x, \\tilde{z})}{q(\\tilde{z} | x, \\hat{y})}]$\n$- E_{\\tilde{z}\\sim q(x,\\hat{y})} [log q(\\tilde{z} | x, \\hat{y})]$\n$+ E_{z,\\tilde{z}\\sim q(z,\\tilde{z}|x,\\hat{y})} [log p(\\tilde{z} | z) + log p(z)]$\n$- E_{z,\\tilde{z}\\sim q(z,\\tilde{z}|x,\\hat{y})} [log q(z | \\tilde{z})]$ (2)\nwhere\n$q(z, \\tilde{z} | x, \\hat{y}) = q(z | \\tilde{z}) q(\\tilde{z} | x, \\hat{y}).$ (3)\nFurthermore, we specify\n$q(z | \\tilde{z}) = Normal(z; \\mu_{\\kappa}(\\tilde{z}), diag(\\sigma_{\\kappa}^2(\\tilde{z})))$ (4)\n$q(\\tilde{z} | x, \\hat{y}) = Student(\\tilde{z}; \\mu_{\\theta} (x, \\hat{y}), diag(\\sigma_{\\theta}^2(x, \\hat{y})), \\nu)$ (5)\nwhere $\\mu_{\\kappa}: \\mathcal{Z} \\rightarrow \\mathcal{Z}$ and $\\sigma_{\\kappa}: \\mathcal{Z} \\rightarrow \\mathcal{Z}$ are the corresponding encoder functions for 2 with respect to mean and covariance matrix; likewise, $\\mu_{\\theta} : \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathcal{Z}$ and $\\sigma_{\\theta} : \\mathcal{X} \\times \\mathcal{Y} \\leftrightarrow \\mathcal{Z}$ are the encoder functions for x, \u0177. We denote the parameters for the encoder function by $\\Phi_{\\theta} = {\\kappa, \\theta}$.\nIn the variational Bayes framework, the objective is always to maximize the ELBO [20], [21]. It is clear that maximizing the ELBO is equivalent to minimizing the following KL-divergence:\n$max E_{z,\\tilde{z}\\sim q(z,\\tilde{z}|x,\\hat{y})} [\\frac{p(x, y, z, \\tilde{z})}{q(z, \\tilde{z} | x, \\hat{y})}]$\n$= min D_{KL} [q(z, \\tilde{z} | x, \\hat{y}) || p(z, \\tilde{z} | x, \\hat{y})] - log p(\\hat{y}, x)$\n$= min D_{KL} [q(\\mathcal{Z}z, \\tilde{z} | x, \\hat{y}) || p(z, \\tilde{z} | x, \\hat{y})].$ (6)\nIn the proposed distributions, we set \\vas a hyperparameter rather than learning it from a parameterized encoder function, for two main reasons: 1) setting a fixed value suffices to perform well (check the empirical study section); 2) we can set a value which fits in our theoretical study.\nGiven Eqs. (2) and (6), we can summarize the loss function for the unsupervised learning as\n$\\mathcal{U}(\\mathcal{D}) := \\sum_{(x,\\_)\\in \\mathcal{D},\\hat{y}\\sim h(x)} D_{KL}[q(z, \\tilde{z} | x, \\hat{y}) || p(z, \\tilde{z} | x, \\hat{y})],$  (7)\nwhere D is the training set that contains the noisy labels.\na) Correction Phase: We can now specify the label correction process given that LSNPC has been explained. Let us denote the corrected labels for x by y*. Following the rhythm of Bayesian learning, we define the label correction function C(.) as\n$y^* = C(x; h) = E[y | x, h]$\n$= \\int \\int y \\cdot p(y | x, h)dy d\\hat{y}$\n$= \\int \\int y \\cdot \\frac{p(x, y, \\tilde{z}, z) q(\\tilde{z} | \\tilde{z}) q(\\tilde{z} | x, \\hat{y})}{p(\\hat{y} | x) q(\\tilde{z} | \\tilde{z}) q(\\tilde{z} | x, \\hat{y})}dzdz \\cdot p_h(\\hat{y} | x)dy d\\hat{y}$\n$= E_{y\\sim p(y|x,z),z\\sim q(z|\\tilde{z}),\\tilde{z}\\sim q(\\tilde{z}|x,\\hat{y}),\\hat{y}\\sim p_h(\\hat{y}|x)} [y] .$   (8)\nIn practice, we approximate this equation by Monte Carlo sampling. This correction function C(\u00b7) remains consistent across unsupervised, supervised, and semi-supervised paradigms.\n2) Supervised Learning: For the supervised fashion where y is observed, we consider the marginal probability which follows\n$log p(x, y, \\hat{y})$\n$\\geq E_{z,\\tilde{z}\\sim q(z,\\tilde{z}|x,y,\\hat{y})} [\\frac{p(x, y, y, z, \\tilde{z})}{q(z, \\tilde{z} | x, y)}]$\n$= E_{z,\\tilde{z}\\sim q(z,\\tilde{z}|x,y,\\hat{y})} [log p(\\hat{y} | x, \\tilde{z}) + log p(y | x, z)$\n$+ log p(\\tilde{z} | z) + log p(z)] \u2013 E_{\\tilde{z}\\sim q(z|x,y,\\tilde{z})} [log q(\\tilde{z} | x, y)]$\n$- E_{z\\sim q(z)x,y)} [log q(z | x, y)],$ (9)\nwhere\n$q(z, \\tilde{z} | x, y, \\hat{y}) = q(z | x, y, \\tilde{z})q(\\tilde{z} | x, y).$  (10)"}, {"title": "We further propose", "content": "$q(z | x, y, \\tilde{z}) = \\eta Normal(z; \\mu_{\\theta}(x, y), diag(\\sigma_{\\theta}^2(x, y)))$\n$+ (1 - \\eta)q(z | \\tilde{z})$ (11)\n$q(\\tilde{z} | x, y) = Student(\\tilde{z}; \\mu_{\\theta} (x, \\hat{y}), diag(\\sigma_{\\theta}^2(x, \\hat{y})), \\nu).$ (12)\nApparently, Eq. (12) is identical to the proposed distribution in the unsupervised setting (Eq. (5)). However, $q(z | x, y, \\tilde{z})$ is designed to be a mixture model of two weighted distributions, controlled by \u03b7. On one hand, we are learning the distribution $q(z | \\tilde{z})$ as that in the unsupervised setting. This distribution is crucial because, during the correction phase, this distribution will be the sole component to rely on, since the true label y is unknown. On the other hand, heuristically, clean data could be used to enhance the encoder function parameters contained in $Normal(z; \\mu_{\\theta} (x, y), diag(\\sigma_{\\theta}^2(x, y)))$. This step improves the learning of the encoder functions that can only be learned through the noisy data in the unsupervised configuration. During our empirical exploration, we found that values of \u03b7 within (0,1) result in nearly identical performance. The reason maybe that, provided the two distributions are drawn a sufficient number of times during learning, the parameters are able to visit the region of the search space in which the noisy predictions can be corrected to their best. The number of training epochs remained the same and showed negligible influence. However, setting \u03b7 to exactly 0 or 1 would lead to slightly worse performance. With \u03b7 = 0, we lose the opportunity to apply the clean data to correct the learning of the decoder functions which are mainly trained by the noisy data in the unsupervised part. In contrast, setting \u03b7 = 1 disconnects $q(z | \\tilde{z})$, the main component for label correction in the inference phase (see Eq. (8)), from the clean data during training.\nSimilarly, denoting D as the clean dataset for training, we define the objective function as\n$\\mathcal{L}(\\mathcal{D}) := \\sum_{(x,y) \\in \\mathcal{D}} D_{KL}[q(z, \\tilde{z} | x, y, \\hat{y}) || p(z, \\tilde{z} | x, y, y)].$  (13)\n3) Semi-Supervised Learning: Following the paradigm in the work [21], we define the corresponding loss function as\n$\\underset{\\Phi}{min} \\mathcal{L}(\\mathcal{D}) + \\mathcal{U}(\\mathcal{D})$  (14)\nwhere $\\Phi = \\Phi_{\\alpha} \\cup \\Phi_{\\theta}$. However, we split the training into two consecutive training parts which is depicted in Algorithm 1. This suffices to leverage the clean data for correcting the learning."}, {"title": "C. Theoretical Properties", "content": "In this section, we analyze the theoretical properties of our proposed method. Due to the space limit, we defer the proofs to the supplemental material.\nIn our loss function under the unsupervised setting, when considering a single data point x, the (expected) KL-divergence between the proposal q(z | 2) and the marginalized posterior $p(z | x, y)$ is also being minimized."}, {"title": "VI. EXPERIMENTS", "content": "In this section, we first introduce experimental settings such as datasets, baselines for comparison, and evaluation metrics\u00b2. Next, we present the empirical analysis of the direct extension of NPC Then, we demonstrate the results for both the unsupervised and semi-supervised LSNPC. Finally, we conduct the sensitivity analysis and ablation study respectively.\na) Datasets: To evaluate the performance of our pro- posed method\u2014LSNPC, we used four benchmark datasets: VOC07 [52], VOC12 [52], COCO [53], and Tomato [54]. Table II summarizes the statistics of each dataset. VOC07 and VOC12 contain images from 20 common object labels, and COCO contains images from 80 common object labels. For both the VOC07 and VOC12 datasets, we used the VOC07 test set which contains 4,952 images as in [17]. The Tomato dataset contains images from 8 tomato diseases.\nWe generated synthetic noisy labels following designs from previous studies [6], [17], [37] with a class-dependent noise transition matrix T, where $T_{ij} = p(\\tilde{y} = j | y = i)$ refers to the probability of the i-th label to be flipped into the j-th label. More specifically, we adopt two noise settings: symmetric (SYM) [55] and pairflip (PAIR) [6], where SYM noise flips labels uniformly to other labels and PAIR noisifies the specific labels with the similar ones. Following [17], for each noise setting, we take into account various levels of noise (specifically, 0%, 30%, 40%, 50%), which is referred to as the noise rate (NR), to verify the effectiveness of LSNPC calibration under different noise conditions. In addition, we include the setting for clean data to analyze its generalization ability. The clean subset of data consists of a randomly selected half of the validation set.\nb) Baselines: To demonstrate the performance improve- ments with LSNPC as a post-processor, we apply it to a pre-trained classifier with baseline methods. These methods include:\nMLP is a baseline multilabel classifier (enabled by a Sigmoid layer appended to the last position) with a fully connected layer on top of the extracted features from an image encoder. We selected two typical image encoders: ResNet-50 [56] where the entire model is denoted by MLP;\nADDGCN [58] uses a semantic attention module to estimate the content-aware class-label representations for each class from extracted feature map where these representations are fed into a graph convolutional network (GCN) for final classification.\nHLC [17] is a noisy multilabel correction approach built on top of ADDGCN. It uses the ratio between the holistic scores of the example with noisy multilabels and its variant with predicted labels to correct noisy labels during training. A holistic score measures the instance-label and label dependencies in an example."}, {"title": "B. Extension of NPC", "content": "Let us analyze the training patterns for the trivial extension to the NPC discussed in Section IV using two random settings, under the unsupverised learning. Fig. 2 presents the corresponding results. More experiments were actually carried out, but they showed the same patterns and thus are not reported. For both settings involving different datasets and noise settings, the training losses (graphs in the first column) in both cases exhibit a monotonic decreasing tendency. As shown in the second column, the validation and test losses are randomly fluctuating throughout the epochs in both cases, even though the training loss appears to be properly learned. Apart from that, the value scope of the training losses is smaller than 1, while the validation and testing losses are around 50, illustrating an outstanding discrepancy. This is a strong indication that the connection between the training and validation losses is lost. It further implies that the learning process fails to drive the search for model parameters towards the desired space. Moreover, we observe from the third column that the micro- F1 for the validation and test sets share the same pattern as their losses. All of these aspects suggest that this extension is incapable of appropriately learning the distribution of the true labels."}, {"title": "C. Unsupervised Learning Results", "content": "Since the extension of NPC has been shown to be an invalid approach in the previous subsection, we exclude it from the following empirical comparisons. Following [9], we also implemented a K-nearest neighbor (KNN) method for comparison. Since KNN is a deterministic method, we ran it only once for each configuration. The hyperparameter K is defaulted to 5. Table III exhibits the comparison results for the unsupervised learning using VOC07 and Tomato. Apart from that, setting NR to 0% means that the data attains the level of cleanliness as prepared by the publisher. The term \"baseline\" refers to the base model itself without applying any post-processor, e.g., KNN or LSNPC.\nFirst, our major finding unveils that LSNPC can improve both the macro- and macro-F1 in most cases. Even for HLC, which is a method improving upon the noisy label situations, the corresponding LSNPC is able to further improve the performance. It shows that for the clean data, LSNPC performs slightly worse than the baseline model. This phenomenon is expected as our method concentrates highly on the noisy situations. We also observe that the improvement of LSNPC is even higher when the noise rate increases. It implies that our method can well address the noisy label situations. The Tomato dataset is a tiny dataset which may contain a great amount of variation. In the paper [54], which published the data, it is evident that the variation in experimental results was significant across the models, despite only slight changes in parameter size under the same model. For this complex data, some of LSNPC outcomes show a slight decrease of the micro-F1; however, it could achieve a greater increase in the macro-F1, which we also deem an overall improvement. All the results matching this pattern were found when the base model is LeViT, which is smaller than ResNet-50. For Tomato, HLC leads to a drop in performance given its base model of ADDGCN, even though HLC is proven working well for many other cases. However, our approach is still able to boost the performance on top of ADDGCN.\nSecond, the KNN method could perform the best in rare cases. It appears to be a random method that cannot be guaranteed to perform well, so that it obtains poor performance in many cases. Even by heuristics, the KNN is also not a suitable approach given the complexity of the multilabel classification cases. We emphasize that the zeros reported in the table are not mistaken."}, {"title": "D. Semi-Supervised Learning Results", "content": "As the KNN does not work properly, it is no longer included in the experiments here. Fig. 3 illustrates the outcomes focusing on the NR within 30%, 40%} for the two noise settings. The"}, {"title": "E. Sensitivity Analysis", "content": "Fig. 4 presents the sensitivity analysis results, in which each number is the averaged outcome of five repetitions. The value of 2.01 for valigns to the requirements in our theoretical analysis (Theorem 2) that \u03bd and \u03bdo must be greater than 2. Hence, we arbitrarily set \u03bd and \u03bdo to 2.01 when considering a value close to 2. The experiments for the field of \"learned\" were appended after other experiments were finished. Under the setting of \u201clearned\u201d, we therefore define a function ve(\u00b7) which reforms Eq. (12) to\n$q(\\tilde{z} | x, y) = Student (\\tilde{z}; \\mu_{\\theta} (x, \\hat{y}), diag(\\sigma_{\\theta}^2(x, \\hat{y})), v_e(x, y)).$\nGiven a specified network architecture Net(x, y), we define ve(x, y) as\n$v_e(x, y) = ReLU(Net(x, y)) + 1$ (19)\nwhere ReLU(\u00b7) guarantees the non-negativity of vand +1 is the minimal value of the degree of freedom in the Student distribution.\nWith regard to the VOC12 dataset, one may observe that the hyperparameters vo and v are generally robust with regard to the three performance. The under performing settings are merely slightly worse. We observe that the performances were decreased for the most cases of \u03bd > \u03bdo. Although learnable"}, {"title": "F. Ablation Study", "content": "In the ablation study, we compared the usage of Student and Normal distributions for the latent variable 2. For the Normal distribution setting, we accordingly modify the relevant proposal distributions such that\n$q(\\tilde{z} | x, y) = Normal(\\tilde{z}; \\mu_{\\theta} (x, \\hat{y}), diag((\\sigma_{\\theta}^2(x, \\hat{y})))).$  (20)\nThis approach is denoted by GAUSS. We randomly selected a few configurations for the ablation studies and present the results in Table IV. For certain cases, the Student distribution excels the Normal distribution significantly.\nMeanwhile, for the cases that Normal performs better, the differences are considerably close, in particular considering that the outcomes have been multiplied by 100. It assures that applying the Student distribution is at least as good as the Normal distribution. However, the findings suggest that the Normal distribution may outperform in certain scenarios, and practitioners might consider conducting a preliminary experiment to determine the most suited distribution if optimal outcomes are desired."}, {"title": "G. GradCAM Analysis", "content": "To understand what information LSNPC focuses on while classifying multilabel images, we use GradCAM method [64]. The first three columns of Fig. 5 shows three GradCAM examples on the VOC07 dataset with MLP, (first row) and LSNPC (second row), where MLP, is used as the pre-trained classifier. The labels at the top of each column indicate ground truth labels for each image, with labels highlighted in blue representing those that were initially missed by the pre-trained classifier but later corrected by LSNPC. As can be observed from the first column, LSNPC concentrates on features relevant to the label \"pottledplant\" when this label has not been assigned by the pre-trained classifier. For the second and third images, LSNPC focuses on regions relevant to labels missing from the predictions of MLP, such as the \"dining table\" in the third image. This behavior can be attributed to the fact that, as LSNPC takes both the image features and the predictions"}, {"title": "VII. CONCLUSION", "content": "In this paper, we present LSNPC, a modeling approach that applies Bayesian deep learning to model the noisy label generation. Unlike all the exiting approaches, we posit that the label noise is actually generated through a shift of the latent variable of the labels in the space. It inherently addresses the problem of the label sparsity and the label correlations in the multilabel setting since it utilizes the latent variable, which is the clustered feature, for reconstructing the true labels. The framework supports both unsupervised and semi-supervised learning paradigm. We theoretically analyze that the discrepancy between the true labels"}]}