{"title": "Gradient Multi-Normalization for Stateless and Scalable LLM Training", "authors": ["Meyer Scetbon", "Chao Ma", "Wenbo Gong", "Edward Meeds"], "abstract": "Training large language models (LLMs) typically\nrelies on adaptive optimizers like Adam (Kingma\n& Ba, 2015), which store additional state infor-\nmation to accelerate convergence but incur signif-\nicant memory overhead. Recent efforts, such as\nSWAN (Ma et al., 2024), address this by eliminat-\ning the need for optimizer states while achieving\nperformance comparable to Adam via a multi-step\npreprocessing procedure applied to instantaneous\ngradients. Motivated by the success of SWAN,\nwe introduce a novel framework for designing\nstateless optimizers that normalizes stochastic gra-\ndients according to multiple norms. To achieve\nthis, we propose a simple alternating scheme to\nenforce the normalization of gradients w.r.t these\nnorms. We show that our procedure can produce,\nup to an arbitrary precision, a fixed-point of the\nproblem, and that SWAN is a particular instance\nof our approach with carefully chosen norms, pro-\nviding a deeper understanding of its design. How-\never, SWAN's computationally expensive whiten-\ning/orthogonalization step limit its practicality for\nlarge LMs. Using our principled perspective, we\ndevelop of a more efficient, scalable, and prac-\ntical stateless optimizer. Our algorithm relaxes\nthe properties of SWAN, significantly reducing\nits computational cost while retaining its mem-\nory efficiency, making it applicable to training\nlarge-scale models. Experiments on pre-training\nLLaMA models with up to 1 billion parameters\ndemonstrate a 3x speedup over Adam with sig-\nnificantly reduced memory requirements, outper-\nforming other memory-efficient baselines.", "sections": [{"title": "1. Introduction", "content": "The training of Large Language Models (LLMs) relies\nheavily on adaptive optimization algorithms, such as\nAdam (Kingma & Ba, 2015), which dynamically adjust\nlearning rates for each parameter based on past gradient\ninformation, leading to faster convergence and improved\nstability. However, these optimizers introduce substantial\nmemory overhead due to the storage of internal states, typ-\nically moment estimates of gradients, a challenge that be-\ncomes particularly pronounced in distributed training set-\ntings where memory constraints and communication over-\nhead are critical concerns (Rajbhandari et al., 2020; Kor-\nthikanti et al., 2023; Dubey et al., 2024). In contrast, simpler\nfirst-order optimization methods such as Stochastic Gradient\nDescent (SGD) require significantly less memory but fail\nto adequately train LLMs (Zhao et al., 2024b; Zhang et al.,\n2020; Kunstner et al., 2023; 2024). As a result, there is an\nongoing need for developing new optimization strategies\nthat resolves the memory efficiency v.s. training perfor-\nmance dilemma for large-scale models training.\nRecent research has made significant strides in improving\nthe efficiency of optimization methods by reducing the mem-\nory overhead associated with saving optimizer states (Hu\net al., 2021; Lialin et al., 2023; Zhao et al., 2024a; Hao et al.,\n2024; Xu et al., 2024a; Jordan et al., 2024; Zhang et al.,\n2024; Ma et al., 2024; Zhu et al., 2024). Among these ad-\nvancements, Ma et al. (2024) introduce SWAN, a stateless\noptimizer that only performs pre-processing operations on\nthe instantaneous gradients, achieving the same memory\nfootprint as SGD while delivering comparable or even bet-\nter performances than Adam. Collectively, these advances\ndemonstrate that memory efficiency and loss throughput\nare not mutually exclusive, opening pathways for efficient\noptimization in large-scale deep learning.\nContributions. Motivated by the recent success of\nSWAN (Ma et al., 2024), we introduce a framework for\ndesigning stateless optimizers based on a novel multi-\nnormalization scheme. Unlike standard first-order meth-\nods that can be interpreted as gradient normalization ac-\ncording to a single norm (Bernstein & Newhouse, 2024),\nour approach aims at normalizing gradients according to\nmultiple norms. We demonstrate that SWAN is a specific\ninstance of our general framework. However, a key lim-\nitation of SWAN is its computational overhead: it relies\non whitening/orthogonalization operation which has com-\nplexity $O(m^2(m + n))$. This may hinder its scalability to"}, {"title": "1.1. Related Work", "content": "Gradient Normalization. Gradient normalization has\nemerged as a key technique in optimization, complementing\nits well-established role in forward-pass operations such as\nLayer Normalization (LayerNorm) (Ba et al., 2016). LARS\nand LAMB (You et al., 2017; 2019) employ global nor-\nmalization to raw gradients and Adam's layer-wise updates,\nrespectively, improving convergence and mitigating gradient\npathologies in large-batch training. Apollo (Zhu et al., 2024)\nintroduces a channel-wise scaling approach, while SWAN\n(Ma et al., 2024) replaces Adam's first-moment estimate\nwith normalized gradients to stabilize gradient distributions.\nTheoretical analyses further underscore the importance of\ngradient normalization. Hazan et al. (2015) study its conver-\ngence properties in SGD, while Cutkosky & Mehta (2020)"}, {"title": "2. Background", "content": ""}, {"title": "2.1. From Adam to Stateless Optimizers", "content": "Adam Optimizer. Adam (Kingma & Ba, 2015) relies on\naccumulating internal states throughout training in order to\nimprove the convergence. More formally, given a loss func-\ntion $(0,x) \\in \\Theta \\times \\mathcal{X} \\rightarrow L(0,x) \\in \\mathbb{R}$, where $\\Theta \\subset \\mathbb{R}^d$ is the\nset of learnable parameters and $\\mathcal{X}$ is the set where the data\nresides, Adam aims at minimizing $0 \\rightarrow \\mathbb{E}_{x\\sim P_x}(L(0,x))$\nwhere $P_x$ is the distribution of data on $\\mathcal{X}$. To achieve this,\nAdam computes at every step $t > 1$ a stochastic gradient as-\nsociated with a mini-batch of input data $x^{(t)}$, and performs\nthe following updates:\n$\\nabla_t = \\nabla_\\theta L(\\theta_t, x^{(t)})$\n$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_t, \\ \\ \\ m_1 = \\frac{m_t}{1 - \\beta_1}$\n$s_t = \\beta_2 s_{t-1} + (1 - \\beta_2) \\nabla_t^2, \\ \\ \\ \\hat{s_t} = \\frac{s_t}{1 - \\beta_2}$\n$\\theta_{t+1} = \\theta_t - \\eta_t \\frac{\\hat{m_t}}{\\sqrt{\\hat{s_t} + \\epsilon}}$\nwhere $\\odot$ is the Hadamard product, $\\eta_t > 0$ are global step-\nsizes, and $\\beta_1,\\beta_2 > 0$ are the weights of the exponential\nmoving averages (EMAs) for the first and second moments\nrespectively. During training, Adam optimizer stores two\nadditional states $(m_t, s_t)$, effectively tripling the memory\nrequired to train the model compared to a simple stochastic\ngradient descent (SGD) scheme.\nSWAN: a Stateless Optimizer. Recently, Ma et al. (2024)\npropose to move away from the paradigm of keeping track\nof internal states during the training of LLMs, and propose\nSWAN, a stateless optimizer that only pre-processes the\nstochastic gradients before updating the parameters. More\nprecisely, they propose to update the learnable weight matri-\nces involved in the model using two matrix operators. Given\na weight matrix $W \\in \\mathbb{R}^{m \\times n}$, with $m \\leq n$, at time $t > 1$,\nthe SWAN update is:\n$\\nabla_t = \\nabla_W L(W_t, x^{(t)})$\n$\\hat{\\nabla}_t = \\sqrt{n} \\mathcal{Q}(\\nabla_t)^{-1} \\nabla_t$\n$\\nabla_t = \\sqrt{n} (\\nabla_t \\nabla_t^\\top)^{-1/2} \\hat{\\nabla}_t$\n$W_{t+1} = W_t - \\eta_t \\nabla_t,$\n(1)\nwhere for a matrix $W \\in \\mathbb{R}^{m \\times n}$, $\\mathcal{Q}(W) :=\\text{Diag}(\\|W_{1,:}\\|_2,..., \\|W_{m,:}\\|_2)$ is the diagonal matrix of size"}, {"title": "2.2. Steepest Descent as Gradient Normalization", "content": "Bernstein & Newhouse (2024) interpret several gradient\ndescent schemes as steepest descent methods under specific\nnorms. More formally, they propose to minimize a local\nquadratic model of the loss $L(\\cdot, x^{(t)})$ at $\\theta_t$ w.r.t to a given\nnorm $\\| \\cdot \\|$, that is:\n$A_t \\mathcal{Q}_{||.||} (z) := L(\\theta_t, x^{(t)}) + <\\nabla_t, z> + \\frac{A_t}{2} ||z||^2$\nwhere $A_t > 0$ are the sharpness parameters and $\\nabla_t :=\\nabla_\\theta L(\\theta_t, x^{(t)})$ is the current stochastic gradient.\nAs shown in (Bernstein & Newhouse, 2024), finding a mini-\nmizer of $\\mathcal{Q}_{||.||}$ can be equivalently formulated as solving:\n$\\underset{z\\in\\mathbb{R}^d: \\|z\\|=1}{\\text{arg max}} \\frac{<\\nabla_t, z>}{\\|z\\|_*} \\text{\t\t} (2)$\nwhere $\\|x\\|_* := \\underset{z\\in\\mathbb{R}^d: \\|z\\|=1}{sup} <x, z>$ is the dual norm of\n$\\|x\\|$. Their framework encompasses a large family of opti-\nmizers that perform the following update:\n$\\nabla_t = \\nabla_\\theta L(\\theta_t, x^{(t)})$\n$\\hat{\\theta_t} \\underset{z\\in\\mathbb{R}^d: \\|z\\|=1}{\\text{arg max}} \\frac{<\\nabla_t, z>}{\\|z\\|}$\n$\\theta_{t+1} = \\theta_t - \\eta_t \\hat{\\theta_t}$\t (3)\nwhere the global step-sizes are of the form\nSeveral popular gradient-descent schemes can be recovered\nusing the above approach. For example, when the $l_2$-norm\nis used, one recovers standard gradient descent, while the\n$l_0$ leads to signed gradient descent (Carlson et al., 2015).\nHowever, this framework considers only a single norm for\npre-processing the raw gradient $\\nabla_t$. In the following, we\nextend this approach to incorporate multiple norms for gra-\ndient pre-processing, enabling the design of efficient and\nstateless optimizers for LLM training."}, {"title": "3. Multi-Normalized Gradient Descent", "content": "Before presenting our approach, let us first introduce some\nclarifying notations.\nNotations. For a vector $x \\in \\mathbb{R}^d$, we call its normalized\nprojection w.r.t to a given norm $\\| . \\|$, the solution to the"}, {"title": "3.1. Gradient Multi-Normalization", "content": "Let us now consider a finite family of $K \\geq 1$ norms\n$(g_1,..., g_K)$. In order to pre-process the gradient $\\nabla$ jointly\naccording to these norms, we propose to consider the fol-\nlowing optimization problem:\n$\\underset{z}{arg max} <\\nabla, z> \\text{ s.t. } \\forall i \\in [|1, K|], g_i(z) = 1.\\text{\t\t} (5)$\nAssuming the constraint set is non-empty, the existence of a\nmaximum is guaranteed. However, this problem is NP-hard\nand non-convex due to the constraints, making it hard to\nsolve efficiently for the general case of arbitrary norms.\nRemark 3.1. Observe that when $K = 1$, the problem (5)\nrecovers exactly the single normalization step used in (Bern-\nstein & Newhouse, 2024), as presented in (3).\nRemark 3.2. The convex relaxation of (5), defined as\n$\\underset{z}{\\text{arg max}} <\\nabla, z> \\text{ s.t. } \\forall i\\in [|1, K|], g_i(z) \\leq 1.\\text{\t\t\t} (6)$\nis in fact equivalent to the single normalization case dis-\ncussed in Section 2.2, where the norm considered is $\\|x\\| :=\\underset{i\\in[[1,K]}{max} g_i(x)$. Thus, solving (6) is equivalent to computing\nthe projection $P_{\\|.||}(\\nabla)$. In Appendix C, we provide a gen-\neral approach to compute it using the so-called Chambolle-\nPock algorithm (Chambolle & Pock, 2011).\nWhile solving (5) exactly might not be practically feasible in\ngeneral, we propose a simple alternating projection scheme,\npresented in Algorithm 1. Notably, our method assumes\nthat the projections $P_{g_i} (\\cdot)$ can be efficiently computed for\nall $i \\in [|1, K|]$. Fortunately, when the $g_i$'s correspond\nto $l_p$-norms with $p \\in [|1, +\\infty|]$, or Schatten p-norms for\nmatrices, closed-form solutions for these projections exist.\nSee Appendix C for more details."}, {"title": "3.2. On the Convergence of MultiNorm", "content": "We aim now at providing some theoretical guarantees on\nthe convergence of MultiNorm (Algorithm 1). More pre-\ncisely, following the SWAN implementation (Ma et al.,\n2024), we focus on the specific case where $K = 2$ and\nthe normalized projections associated with the norms $g_1$ and\n$g_2$ have constant $l_2$-norm. More formally, we consider the\nfollowing assumption.\nAssumption 3.3. Let g be a norm on $\\mathbb{R}^d$. We say that it\nsatisfies the assumption if for all $x \\in \\mathbb{R}^d$, $\\|P_g(x)\\|_2 = c$\nwhere $c > 0$ is an arbitrary positive constant independent of\n$x$ and $\\| \\cdot \\|_2$ represents the Euclidean norm.\nRemark 3.4. Observe that both norms in SWAN satisfies\nAssumption 3.3 and their normalized projections have the\nsame $l_2$-norm, as for any $W \\in \\mathbb{R}^{m \\times n}$ with $m \\leq n$, we\nhave $\\|P_{g_1} (W)\\|_2 = \\|P_{g_2}(W)\\|_2 = \\sqrt{nm}$.\nThis assumption enables to obtain useful properties on $P_g$\nas we show in the following Lemma:\nLemma 3.5. Let g a norm satisfying Assumption 3.3. Then\n$P_gP_g = P_g$\nand for all $x \\in \\mathbb{R}^d$, $g^*(P_g(x)) = \\|P_g(x)\\|_2 = c^2$, where\n$g^*$ is the dual norm associated with $g$.\nLet us now introduce some additional notation to clearly\nstate our result. Let $x_0 \\in \\mathbb{R}^d$ and let us define for $n \\geq 0$:\n$x_{2n+1}:= P_{g_1} (x_{2n})$\n$x_{2n+2}:= P_{g_2} (x_{2n+1})$\t (7)\nwhich is exactly the sequence generated by Algorithm 1\nwhen $K = 2$ and $x_0 = \\nabla_\\theta L(\\theta_t, x^{(t)})$. Let us now show our\nmain theoretical result, presented in the following Theorem."}, {"title": "3.3. MNGD: a New Family of Stateless Optimizers.", "content": "We now introduce our family of optimizers: Multi-\nNormalized Gradient Descents (MNGDs) (Algorithm 2).\nThe key distinction from the framework proposed in (Bern-\nstein & Newhouse, 2024) is that MNGDs normalize the gra-\ndient with respect to multiple norms using the MultiNorm\nstep, whereas in (Bernstein & Newhouse, 2024), the gra-\ndient is normalized using a single norm, as shown in (3).\nIn the following, we focus on the MNGD scheme with\na specific choice of norms, for which we can efficiently\ncompute the gradient multi-normalization step. This enables\nthe application of stateless optimizers to large LMs."}, {"title": "4. Sinkhorn: a Multi-Normalization Procedure", "content": "As in SWAN (Ma et al., 2024), we propose to normalize\nthe weight matrices according to multiple norms. We still\nleverage the row-wise $l_2$-norm to pre-process raw gradients,\nhowever, rather than using the spectral norm, we propose to\nconsider instead a relaxed form of this constraint and use\nthe column-wise $l_2$-norm. More formally, let us consider\nthe two following norms on matrices of size $\\mathbb{R}^{m \\times n}$:\n$g_1(W) := \\frac{\\underset{i\\in[[1,m]}{max} \\| W_{i,:}\\|_2}{\\sqrt{n}} , \\text{\t\t} g_2(W) := \\frac{\\underset{j\\in[[1,n]}{max} \\| W_{:,j}\\|_2}{\\sqrt{m}}$\nwhich leads to the following two normalized projections:\n$P_{g_1}(W) = \\sqrt{n}\\mathcal{Q}(W)^{-1}W \\text{\t\t}\nP_{g_2} (W) = \\sqrt{m}WR(W)^{-1}$\nwhere $R(W) := \\text{Diag}(\\|W_{:,1} \\|_2,..., \\|W_{:,n} \\|_2) \\in \\mathbb{R}^{n \\times n}$\nis the diagonal matrix of size $n$ with the $l_2$-norm of the\ncolumns of $W$ as diagonal coefficients. For such a choice\nof norms, the MultiNorm reduces to a simple procedure\nas presented in Algorithm 3.\nRemark 4.1. For such a choice of norms, we obtain\n$\\|P_{g_1}(W)\\|_2 = \\|P_{g_2}(W)\\|_2 = \\sqrt{nm}$ for any $W \\in \\mathbb{R}^{m \\times n}$.\nIn other words, both norms satisfy Assumption 3.3 and their\n$l_2$ norms are equal to $\\sqrt{nm}$.\nFor completeness we include the MNGD scheme (Al-\ngorithm 4) that replaces the MultiNorm step with\nSR-Sinkhorn (Algorithm 3).\nThe Sinkhorn Algorithm. Before explicitly showing the\nlink between Algorithm 3 and the Sinkhorn algorithm, let\nus first recall the Sinkhorn theorem (Sinkhorn, 1964) and\nthe Sinkhorn algorithm (Sinkhorn & Knopp, 1967). Given\na positive coordinate-wise matrix $A \\in \\mathbb{R}^{m \\times n}$, there exists a\nunique matrix $P \\in \\mathbb{R}^{m \\times n}$ of the form $P = QAR$ with $Q$\nand $R$ positive coordinate-wise and diagonal matrices of size\n$m$ and $n$ respectively, such that $P\\textbf{1}_n = n\\textbf{1}_m$ and $P^\\top\\textbf{1}_m =$"}, {"title": "5. Experimental Results", "content": "In this section, we evaluate the empirical performance of\napplying SinkGD optimizer to LLM pretraining tasks. All\nexperiments were performed on NVIDIA A100 GPUs."}, {"title": "5.1. LlaMA Pre-training Tasks", "content": "Setup. We evaluate SinkGD on LLM pre-training tasks\nusing a LLaMA-based architecture (Touvron et al., 2023)\nwith RMSNorm and SwiGLU activations (Zhang & Sen-\nnrich, 2019; Gao et al., 2023). We consider models with\n60M, 130M, 350M, and 1.3B parameters, all trained on\nthe C4 dataset (Raffel et al., 2020) using an effective token\nbatch size of 130K tokens (total batch size 512, context\nlength 256). Specifically, for both 130M and 350M, we\nuse 128 batch size with 4 accumulations. For 60M and\n1B, we uses 256 batch with 2 accumulation, and 32 per-\ndevice batch size with 2 accumulation and 8xA100s, re-\nspectively. Following the setup of (Zhao et al., 2024a; Zhu\net al., 2024), SinkGD is applied to all linear modules in\nboth attention and MLP blocks with L = 5 iterations for\nthe SR-Sinkhorn procedure. For all other modules, that\nare the embedding layer, the RMSnorm layers, and the last\noutput layer, Adam optimizer (Kingma & Ba, 2015) is used.\nWe use exactly the same cosine learning rate scheduler as\nin (Zhao et al., 2024a), where 10% of total training steps\nis used for warm-up. Note that, as in (Zhao et al., 2024a;\nZhu et al., 2024), we use a group-wise learning rate for\nour optimizer. The effective learning rate used for linear\nmodules in the transformer blocks is of the form $\\alpha\\eta_t$ where\n$\\eta_t$ is global learning rate provided by the scheduler and $\\alpha$ is\nfixed hyperparameter that we set to $\\alpha$ = 0.05. For Adam,\nwe use $\\eta_t$ as the learning rate.\nBaselines. We consider the following memory-efficient\noptimizers baselines: Adam (Kingma & Ba, 2015); Ga-\nlore (Zhao et al., 2024a); Fira (Chen et al., 2024b),\nApollo and Apollo-mini (Zhu et al., 2024), and\nSWAN (Ma et al., 2024). For all methods, training\nuses BF16 precision for weights, gradients and opti-\nmizer states by default, except for SWAN that uses\nFP32 precision to pre-process the gradient (Ma et al.,\n2024). We also perform a grid search of learning rate\nfor Adam over {0.01, 0.005, 0.001, 0.0005, 0.0001},\nexcept for 1B model which we search over\n{0.001, 0.0007, 0.0005, 0.0003, 0.0001}. We do not\nperform any weight decay for all optimizers."}, {"title": "5.2. Ablation Study", "content": "Throughput analysis. We also assess throughput when\ntraining a 1.3B-parameter model on 8xA100 GPUs. We use\ntwo metrics: (1) the raw throughput which is the number of\ntokens processed per second, and (2) the effective throughput\ndefined as the total training token used by Adam divided by\nthe time (in seconds) used by SinkGD to reach the same\ntest perplexities. These metrics evaluate the impact of the\nmulti-normalization step on training speed, and also account\nfor the fact that some optimizers make more effective use\nof training tokens. As shown in Table 3, SinkGD achieves"}, {"title": "C.1. A Dual Perspective", "content": "In this section, we propose an algorithmic approach to solve the convex relaxation of the problem introduced in (5). More\nformally, given a family of simple norms $(g_i)_{i=1}^{K}$ and some positive constants $(\\varepsilon_i)_{i=1}^{K}$, we consider the following problem:\n$\\underset{do\\in\\mathbb{R}^d}{\\text{max}} \\nabla L(do) \\text{ s.t. } g(do) \\leq 1. \\text{ \t\t}(15)$\nwhere\n$g(x) := \\underset{i\\in[[1,K]]}{\\text{max}} \\frac{g_i(x)}{\\varepsilon_i}$\nwhich is also a norm. For such problems, as long as $\\nabla \\neq 0$, then the solutions lies in the level set $\\{d0 : g(dd) = 1\\}$. Even\nif the subdifferentials of (the dual norm of) each $g_i$ can be derived in closed form, there is not known closed-form for the\nsubdifferential of (the dual norm of) $g$. To solve (15), we propose to consider a coordinate gradient descent on the dual. A\nsimple application of the Fenchel duality (Rockafellar, 1974) leads to the following equivalent optimization problem:\n$\\underset{\\lambda_1,...,\\lambda_K}{\\text{inf}} \\underset{i=1}{K} \\varepsilon_i g_i^*(\\lambda_i) \\text{ + } < \\lambda_i, \\nabla L> \\text{ s.t. } \\nabla L(0) = \\underset{i=1}{K} \\lambda_i (16)$\nwhere $g_i^*$ is the dual norm of $g_i$ and so for all $i \\in [[1, K]]$, from which a primal solution can be recovered by simply finding\n$y_i$ s.t. $\\lambda_i > 0$ and such that $(\\lambda_i, y_i) = g_i(yi)$ under the condition that $g_i(y_i) = \\varepsilon_i$, which is equivalent to solve:\n$y:= \\varepsilon_i \\underset{z : g_i(z)\\leq 1}{\\text{arg max}} z, \\lambda_i .\n$\\text{Proof}. \\text{Let (B(i))} 1 the ball associated with the norm (gi)1 with radius (1)1 respectively. Let us also denote for any\nset AC Rd, the indicator function as\n$1A(x) = \\{\n0 if x 0$ and such that $(\\lambda_i, y_i) = \\varepsilon_i g(y_i)$\nunder the condition that $g_i(y_i) = \\varepsilon_i$, which is equivalent to solve:\n$y:= \\varepsilon_i \\underset{z : g_i(z)\\leq 1}{\\text{arg max}} (z, \\lambda_i).\n$"}]}