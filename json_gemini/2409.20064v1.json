{"title": "Knowledge Discovery using Unsupervised Cognition", "authors": ["Alfredo Ibias", "Hector Antona", "Guillem Ramirez-Miranda", "Enric Guinovart"], "abstract": "Knowledge discovery is key to understand and interpret a dataset, as well as to find the underlying relationships between its components. Unsupervised Cognition is a novel unsupervised learning algorithm that focus on modelling the learned data. This paper presents three techniques to perform knowledge discovery over an already trained Unsupervised Cognition model. Specifically, we present a technique for pattern mining, a technique for feature selection based on the previous pattern mining technique, and a technique for dimensionality reduction based on the previous feature selection technique. The final goal is to distinguish between relevant and irrelevant features and use them to build a model from which to extract meaningful patterns. We evaluated our proposals with empirical experiments and found that they overcome the state-of-the-art in knowledge discovery.", "sections": [{"title": "Introduction", "content": "Knowledge discovery is a fundamental task in a handful of fields, from data science to knowledge engineering. Discovering knowledge is a difficult task that consist in extracting, from the source data, new knowledge not previously known to the practitioner. This knowledge can come in multiple forms and can be discovered through different ways. In this paper we will focus on two already established frameworks for discovering knowledge: pattern mining and feature selection. This knowledge can have multiple uses too, and in this paper we will focus in its use for reducing the dimensionality of the data. Finally, we will approach all these problems using as base the Unsupervised Cognition algorithm [9], that is a novel pattern matching, unsupervised learning algorithm that focuses on modelling the associations between the different features of the data. These characteristics allow it to build internal representations that contain the associativeness of data features, which makes them useful to discover association rules.\nPattern mining is the knowledge discovering framework that encompasses all techniques that aim to discover association rules that describe the original data."}, {"title": "Related Work", "content": "In the pattern mining field there are multiple techniques [5], from graph-based pattern mining techniques [6] to heuristic pattern mining ones [4,14]. However, most of these techniques should be applied over the original data, or are built over specific methods. In this paper we are presenting a novel pattern mining technique specifically defined for an Unsupervised Cognition model, thus not being comparable to the others in its conception. Moreover, comparing patterns is already a hard task, as they usually represent different associations between the features of the original data. Finally, our patterns are of the form of the original data, while most pattern mining algorithms return association rules. Thus, we are not aware of any pattern mining technique able to extract patterns from an"}, {"title": "Pattern Mining", "content": "Unsupervised Cognition [9] is an unsupervised learning method that builds representations in a hierarchical structure. To briefly resume its functioning, it builds representations by combining multiple inputs, thus each representation depicting a subset of the input domain. Then, for each representation, it builds children representations that depict subsets of the set depicted by the parent representation, until there is no more possible subsets. Moreover, each input is assigned to a representation based on a similarity measure, thus each representation depicts a cluster of the input domain. This way, the Unsupervised Cognition algorithm builds a tree of representations where the seed node contains the most generic representations, that is, those that depict the biggest clusters of the input domain; and the leaf nodes contain representations that depict only one input, that is, literal representations. Here it is important to remark that representations at the same level of tree depth do not share any input, that is, any input will belong to only one representation at that level.\nThese representations are built combining multiple inputs through average sum. To know more details about how these representations are built (i.e. details about how the algorithm decides to which representation a new input belongs) we refer to the original paper [9]. We also refer to it [9] to assess the validity of the representations built by the Unsupervised Cognition algorithm. There"}, {"title": "Dimensionality Reduction", "content": "Once we have a set of patterns selected with respect to a target feature, we propose to use them to reduce the dimensionality of our input domain. This would be useful in situations where we have plenty of features for each input, but not all are relevant for the task at hand. For example, if we have recollected a lot of genetic data, and we want to solve a classification problem where only a handful of genes are involved, it is very probable that not only most of the genes collected in the genetic data will not help to solve that task, but that they will actually hurt the efforts to solve the problem adding unnecessary and irrelevant noise. Thus, reducing data dimensionality is a fundamental step in these kind of cases.\nIdeally, the goal is to reduce the dimensionality of the input domain by removing the features from the inputs that are less relevant for the task at hand. To that end, first we need to have a target feature, that for the Unsupervised Cognition algorithm should be a metadata feature. This is a prerequisite because our dimensionality reduction method will be based in how much a feature correlates with such target feature. The rationale behind our dimensionality reduction method is that features that correlate with the target feature may contain relevant information to solve the task at hand, while features that do not correlate with the target feature are irrelevant noise.\nWith this rationale, we devised the following method: we start by picking the patterns generated by the Unsupervised Cognition algorithm, and we compute, for each feature, the correlation between the values of such feature in the patterns and the value of the target feature in the patterns. If the target feature is a numerical feature, the patterns will be the representations from the seed node of the Unsupervised Cognition model. However, if the target feature is a categorical feature, then the patterns should be those representations that belong to only one class of the classes present in the categorical feature. In this last case, the correlations will be between each class of the target feature and the values in the"}, {"title": "Feature Selection", "content": "An important property of our dimensionality reduction proposal is that it is feature based. That means that it does not transform the data between mathematical spaces like other dimensionality reduction methods do, but instead it only selects a set of relevant features. Thus, it is in fact a feature selection method too. However, our proposal has a caveat: even though the Unsupervised Cognition algorithm is deterministic, the patterns it produces are subject to the order in which the data is provided to the algorithm. Thus, different data orders produce different patterns and thus different sets of relevant features, although there are many repeated features between them.\nGiven this problem, here we propose a basic solution: to select relevant features from multiple Unsupervised Cognition models, each one trained with a different input order. Ideally, we should train at least 100 Unsupervised Cognition models, and extract a list of relevant features from each one using the mechanism presented in the previous section. Once we have a set of lists of relevant features, one for each trained Unsupervised Cognition model, we can count in how many lists appears each feature. This count normalised provides us with a confidence measure to evaluate how probable is that each feature is relevant for the task at hand. Thus, now we can set another threshold and select as relevant only those features that are over the threshold. Here we recommend a threshold of at least 50% confidence, as lower confidence means that in most of the cases that feature was not relevant for the task at hand. Ideally, the threshold would be 100% confidence, meaning that such features were relevant in all cases.\nA downside of this approach is that there is not always a feature that has 100% confidence, remarking the need for this approach but at the same time showing the huge dependency of the Unsupervised Cognition algorithm in their learning dataset input order. This stems from the fact that such algorithms are world modelling algorithms, and thus depend on how the world is presented to them. Nonetheless, this world modelling aspect of the Unsupervised Cognition algorithm is also the main factor why we decided to develop our proposals over these algorithms, because we considered that the patterns they build are going to be more robust and representative than the patterns build by other algorithms that are not based on modelling."}, {"title": "Knowledge Discovery Pipeline", "content": "First of all, we want to emphasise that, although we have presented our proposals in dependency order, our ideal pipeline for knowledge discovery will be to use the presented feature selection technique to select the truly relevant features of the dataset, then use these features to perform dimensionality reduction to train a new Unsupervised Cognition model, and finally use that newly trained Unsupervised Cognition model to extract relevant patterns, with our pattern mining proposal, for the practitioner to explore. We have to remark here that the feature selection technique already uses the pattern mining and dimensionality"}, {"title": "Experiments", "content": "To evaluate the usefulness of our proposals, we devised the following experiment: we will evaluate how good are the selected patterns to compute correlations for dimensionality reduction by comparing the accuracy of an Unsupervised Cognition model before and after reducing the dimensionality of the input domain. In an additional experiment we will evaluate the validity of our dimensionality reduction proposal by comparing our proposal with other dimensionality reduction methods by computing the accuracy of an Unsupervised Cognition model after reducing the dimensionality with each method. Finally, in our last experiment we will evaluate the validity of our feature selection proposal by comparing the accuracy of an Unsupervised Cognition model over the original data and over the selected features.\nTo perform our experiments, we selected as experimental subject the TCGA Kidney Cancers dataset [16]. This dataset has 58056 features but with only 897 entries, what makes it the ideal dataset where to perform our experiments. The idea is that, with so many more features than entries, this dataset will have a lot of noisy or irrelevant features, that are the ones we will aim to remove. We decided to perform our experiments over only one dataset because our aim is to compare methods, thus the dataset not being so critical, and because finding open, public, and accessible datasets with so many features has proven to be a really difficult task."}, {"title": "Pattern Validity", "content": "Validating the patterns that an Unsupervised Cognition generates is not a simple task. First, because the desired properties of the patterns (i.e. that they represent all the input domain, they have no overlap, etc...) are fulfilled by definition. And second because the utility of the patterns depends on the task the practitioner needs to solve. In our case, as our proposal includes a dimensionality reduction technique, we will consider that the utility of the patterns is for reducing the dimensionality of the input domain. Thus, we need to evaluate how effective this dimensionality reduction is. To measure the effectiveness of a dimensionality"}, {"title": "Dimensionality Reduction Effectiveness", "content": "To measure the effectiveness of our dimensionality reduction method, we decided to compare it with other dimensionality reduction methods. Specifically, we decided to compare it with PCA and Self-Organising Maps (SOM). To compare these methods, we decided to take the result of the dimensionality reduction and provide it to the Unsupervised Cognition algorithm in order to obtain a classification accuracy. The rationale behind this decision is that the dimensionality reduction method that better resumes the data should be that one that produces the higher accuracy after applying a classification method. We decided to use the Unsupervised Cognition algorithm for this task both because it was easier to use and because as an unsupervised learning algorithm it focuses on the input data, without being able to use the label to fix any errors.\nGiven this setup, the experiment we performed consisted in: generating a new train and test datasets using the dimensionality reduction method, training an Unsupervised Cognition model with the reduced train dataset, and computing the accuracy of the resulting model with the reduced test dataset. For our"}, {"title": "Feature Selection Validity", "content": "To measure the validity of the features selected by our feature selection method we devised a comparison experiment, where we compare the accuracy obtained by an Unsupervised Cognition model before and after reducing data dimensionality using the features selected by our method. The rationale behind this experiment is that a good feature selection method would be that one that selects the features better suited to produce a good classifier, that is, those that produce a higher accuracy after applying a classification method. Same as in the previous experiment, we decided to use the Unsupervised Cognition algorithm"}, {"title": "Threats to Validity", "content": "In this section we discuss the possible threats to the validity of our results. The first kind are the threats to internal validity, that can explain our results due to uncontrolled factors. The main threat in this category is the possibility of having a faulty code. To reduce this threat we have carefully tested each piece of code used in our experiments and developed unit tests for them, and we have relied on widely tested libraries like scikit for the PCA and SOM algorithms, and the authors implementation for the Unsupervised Cognition algorithm. Another threat in this category is the impact of randomisation in the comparison results. Every time we had this problem we performed an average and standard deviation computation over multiple runs of the experiment. We performed 100 iterations in each case due to resource constrains, but we are sure that the variation with a higher number of iterations will not be great, as the standard deviations obtained were at most of 0.03.\nThe second kind of threats are the ones to external validity, that hamper the generality of our results to other scenarios. In our case the only threat in"}, {"title": "Conclusions", "content": "Knowledge discovery is a key goal when dealing with new data. Using the Unsupervised Cognition algorithm, in this paper we have presented a pipeline for discovering knowledge from it. The practitioner will be able to easily find patterns in the data and refine its dataset using our pattern mining, feature selection and dimensionality reduction proposals. These proposals where presented in this paper and evaluated with multiple experiments. The results of the experiments show how they are state-of-the-art in their respective fields. Thus, this paper presents state-of-the-art techniques for pattern mining, feature selection and dimensionality reduction from an Unsupervised Cognition model.\nSpecifically, our knowledge discovery pipeline consist in using the presented feature selection technique to select the truly relevant features of the dataset, then use these features to perform dimensionality reduction to train a new Unsupervised Cognition model, and finally use that newly trained Unsupervised Cognition model to extract relevant patterns, with our pattern mining proposal, for the practitioner to explore. Using this pipeline we are able not only to get better, more relevant patterns, but also to build predictive models with a huge increase in accuracy with respect to a predictive model that uses all the data. For example, in our experiments, we were able to build predictive models able to obtain an increase in accuracy of almost 10%, raising it from around 80% to almost 90%.\nAs future work, we would like to explore other ways of discovering knowledge using the Unsupervised Cognition algorithm. We would also like to refine our proposals to be more efficient. We would like to explore how our proposal behaves in real-world scenarios too. And we would like to improve Unsupervised Cognition to avoid its dependency on the input order."}]}