{"title": "Discover Physical Concepts and Equations with Machine Learning", "authors": ["Bao-Bing Li", "Yi Gu", "Shao-Feng Wu"], "abstract": "Machine learning can uncover physical concepts or physical equations when prior knowledge from another one is available. However, in many cases, these two aspects are coupled and cannot be discovered independently. We extend SciNet, which is a neural network architecture that simulates the human physical reasoning process for physics discovery, by proposing a model that combines Variational Autoencoders (VAEs) with Neural Ordinary Differential Equations (Neural ODEs). This allows us to simultaneously discover physical concepts and governing equations from simulated experimental data across diverse physical systems. We apply the model to several key examples inspired by the history of physics, including Copernicus' heliocentric solar system, Newton's law of universal gravitation, the wave function together with the Schr\u00f6dinger equation, and spin-1/2 along with the Pauli equation. The results demonstrate that the neural network successfully reconstructs the corresponding theories.", "sections": [{"title": "Introduction", "content": "Einstein famously argued that \u201cIt can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience\" [1]. The basic elements of physical theories are physical concepts and equations. Finding simple and elegant physical concepts and equations that can fully describe experimental phenomena has always been the goal of theoretical physicists.\nPhysics typically advances by building upon existing theories, incorporating new experimental phenomena and mathematical tools to expand them. However, when confronting revolutionary problems, inherited theories may not naturally describe new phenomena. In such cases, breakthrough discoveries are needed to revise the old theories, and this can be a long process. For example, it took several centuries to transition from the geocentric model to the heliocentric model, and nearly 30 years from Planck's quantum hypothesis to the Schr\u00f6dinger equation. Moreover, human research into new phenomena relies on the current level of experimental capability and physical understanding. Subtle physical mechanisms, such as spin, may already be hidden within the data, but they are not easily discovered [2].\nIn recent years, the emerging field of AI for Science has developed rapidly, helping researchers gain insights unattainable by traditional scientific methods [3-5]. As a research assistant tool, AI has made significant contributions in various areas of physics, such as cosmology [6-14], high-energy physics [15\u201321], string theory [22-26], and condensed matter physics [27-31]. Beyond achieving success in these specific areas of physics, recent works [32-38] have demonstrated that AI is capable of conducting independent and autonomous research in general physics. Important physical concepts [39,40], physical properties such as symmetries [41,42], and physical laws such as Kepler's laws [43], Newton's laws [44] and conservation laws [45-47] can all be discovered by AI from data. Analytic physical equations can not only be directly obtained through symbolic regression [48\u201350], but can also be derived by causal inference based on a set of prior knowledge [43,51]. In the study of dynamical systems, purely machine learning methods [52\u201355] have been widely used to analyze complex systems [56\u201359] and solve engineering problems [60\u201362]. In parallel, physics-inspired approaches [63\u201367] have also made substantial progress. Although not formally defined, AI is expected to become the fifth paradigm of scientific research [68].\nScience is the attempt at the posterior reconstruction of existence by the process of conceptualization [69]. SciNet, proposed by Iten et al. [40], is an important neural network model that autonomously discovers physical concepts from data without requiring"}, {"title": "Method", "content": "The development of physical theories begins with the observation of natural phenomena. Physical data serve as the foundation for constructing theories. Such data often originate from direct experimental observations or from indirect methods of data generation, offering the essential facts and phenomena upon which theories are built. Physicists analyze these data to propose physical concepts, such as particles, waves, and fields, and formulate equations to explain the observed phenomena. The development of these concepts and equations is an intertwined process, with their mathematical form providing a quantitative and predictive framework for accurately describing physical laws. Throughout this process, physicists refine theoretical models by posing questions, such as, \u201cWhat would happen if we altered a specific condition?\" and seeking answers. When experimental results support the theory, it is reinforced; when they contradict the predictions, the theory may need to be revised.\nTo develop a general machine learning-based method for physics research, we model our approach on the way human physicists construct physical theories, as illustrated in Figure 1. We consider sequential observational data $(t_i, O(t_i))$, which describe the evolution of the system over time or space. Direct observational data are often difficult for humans to interpret. We assume that there exists a physical theory capable of describing the"}, {"title": "Discovering Physical Concepts", "content": "Suppose there is a connection between the direct observational data and the underlying physical concepts, such that there exists a mapping $E: O \\rightarrow C$ and an inverse mapping $D: C \\rightarrow O$. We use a Variational Autoencoders [80] to learn the mappings $E$ and $D$ between the observational data and the physical concepts. VAEs consist of a probabilistic encoder and a probabilistic decoder. The probabilistic encoder maps input data space to a continuous latent space, producing latent representations. The probabilistic decoder then uses these latent representations to reconstruct the input data or generate new data. The latent representations $z$ are sampled from a multi-dimensional Gaussian distribution, given the data $x$ and the probabilistic encoder. These representations capture the essential features and internal structure of the data. In physics,"}, {"title": "Discovering Physical Equations", "content": "The dynamical equations that govern the evolution of physical concepts are another essential element of theory. Several machine learning techniques have been developed to identify the underlying governing equations from sequential data, including Recurrent Neural Networks (RNNs) [88], Graph Neural Networks (GNNs) [89], SINDy [34], and Koopman Theory [90]. In this work, we utilize Neural Ordinary Differential Equations (Neural ODEs) to discover physical equations. Neural ODES are a class of machine learning models that integrate traditional neural networks with Ordinary Differential Equations (ODEs). Unlike discrete-time models such as RNNs, Neural ODEs can model dynamical systems in a continuous time domain. Since neural networks are capable of approximating any function, Neural ODEs are particularly well-suited for handling nonlinear dynamics, outperforming methods like Koopman Theory and SINDy, which require specific candidate functions and additional hyperparameters.\nThe governing equations of many dynamical systems can be represented by ordinary differential equations, in the general form:\n$\\frac{dy(t)}{dt} = f(t, y(t)) \\approx NN(t, y(t))$,\nwhere $t$ is time, $y(t)$ are state variables, and $f$ is the function determined by the ODEs. Neural ODEs use a neural network $NN$ to approximate the function $f$. Once $f$ is found, the governing equation of the system is determined.\nGiven an initial value for the state variable $y(t_0)$ and a randomly initialized neural network $NN$, Neural ODEs use a black-box ODE solver to find the state variable at any time step through forward propagation:\n$\\hat{y}(t_1) = y(t_0) + \\int_{t_0}^{t_1} NN(t, \\hat{y}(t)) dt,$"}, {"title": "Results", "content": "We choose four representative physical examples: Copernicus' heliocentric solar system, Newton's law of universal gravitation, and one-dimensional quantum systems governed by"}, {"title": "Copernicus' heliocentric solar system", "content": "Copernicus' heliocentric solar system holds a very important place in history, marking a major shift in humanity's understanding of the universe. These scientific achievements are not only milestones in astronomy but also signify a general research paradigm in physics that starts from observational data and seeks to discover universal and effective physical laws through simplicity and predictability. Here, we consider the Copernicus' heliocentric solar system where Mars and Earth orbit the Sun in uniform circular motion, as shown in Figure 2(a). The key question then becomes: Can AI, without any prior knowledge, replicate the work of Copernicus? This would be an essential first step for AI to discover even more complex physical theories.\nAs shown in Figure 2(a), the distance between Earth and Mars is given by:\n$d = \\sqrt{R_E^2 + R_M^2 - 2 \\cdot R_E \\cdot R_M \\cdot \\cos(\\Phi_E + \\Phi_M)}.$", "equations": ["d = \\sqrt{R_E^2 + R_M^2 - 2 \\cdot R_E \\cdot R_M \\cdot \\cos(\\Phi_E + \\Phi_M)}."]}, {"title": "Newton's Law of Universal Gravitation", "content": "Newton's law of universal gravitation provides methods for the accurate prediction of celestial orbits based on a minimal amount of observational data, allowing for the calculation of long-period celestial motions. Against this background, we hope that artificial intelligence can automatically identify the law of universal gravitation from limited observational data without prior knowledge of the specific physical principles.\nIn [79], an adaptive method called Sir Isaac was developed, which derived Newton's law of universal gravitation from simulated celestial motion data. We employ the same physical setup as in [79], simulating an object with mass $m$ moving under a gravitational field by another central body with mass $M$ where $M \\gg m$. The evolution of the distance $r(t)$ between them follows the dynamic equation:\n$\\frac{d^2r}{dt^2} = \\frac{h^2}{r^3} - \\frac{GM}{r^2},$", "equations": ["\\frac{d^2r}{dt^2} = \\frac{h^2}{r^3} - \\frac{GM}{r^2},"]}, {"title": "Wave Function with the Schr\u00f6dinger Equation", "content": "Wave function and the Schr\u00f6dinger equation form the foundation of quantum mechanics. Here, we are curious whether machine can discover them without knowing quantum mechanics itself. Wang et al. [39] proposed an introspective recurrent neural network architecture, called \"the Schr\u00f6dinger machine\", which includes a translator and a knowledge distiller. By using a two-stage method that involves Taylor expansion and linear projection to update its hidden states and generate outputs, it could extract the concept of quantum wave functions and the Schr\u00f6dinger equation. Their model is specifically designed for this\nparticular example. We want to use our more generalizable model to accomplish this task while enhancing its interpretability.\nIn this regard, we adopt the same physical setting as in [39], considering a single quantum particle moving in one-dimensional space with a specific potential function. Scientists can collect density distributions of Bose-Einstein condensates (BEC) in potential traps of various shapes through cold atom experiments.\nThe Schr\u00f6dinger equation describes the quantum state of microscopic particles. In one dimension, its stationary form is given by:\n$\\frac{h^2}{2m} \\frac{d^2\\psi(x)}{dx^2} + V(x)\\psi(x) = E\\psi(x).$", "equations": ["\\frac{h^2}{2m} \\frac{d^2\\psi(x)}{dx^2} + V(x)\\psi(x) = E\\psi(x)."]}, {"title": "Spin-1/2 with the Pauli Equation", "content": "The Stern-Gerlach experiment in 1922 revealed the splitting of silver atom trajectories in a non-uniform magnetic field, challenging classical physics and prompting Pauli in 1924 to propose the concept of two-valuedness. This idea became the foundation of the Pauli exclusion principle. In 1925, Uhlenbeck and Goudsmit introduced the notion of electron spin, which Pauli later integrated into quantum mechanics, leading to the development of spin theory, the introduction of Pauli matrices, and the two-component spin wave function.\nNow, let us consider the following scenario: if the Stern-Gerlach experiment had employed a uniform magnetic field, only a single stripe would have appeared on the screen, as illustrated in Figure 5(a). Such a setup would likely have masked the quantization of angular momentum. However, spin and the Pauli equation describe intrinsic properties and fundamental laws of particles, which remain present and operative, even if they are not directly observable in experimental data. Without the angular momentum quantization revealed by the Stern-Gerlach experiment, the discovery of spin might have been significantly delayed. Our findings demonstrate that, even in such scenarios, artificial intelligence can reconstruct the relevant concepts and equations, shedding light on these hidden principles.\nAssuming the motion of silver atoms involves only the x-direction, and the direction of the uniform magnetic field is along the z-direction, i.e., $B = B_z\\hat{k}$, the Pauli equation describing the motion of silver atoms is:\n$\\begin{bmatrix} \\frac{h^2}{2m} \\frac{d^2}{dx^2} + B_z\\mu_B\\sigma_z \\end{bmatrix} \\begin{pmatrix} \\psi_1(x) \\\\ \\psi_2(x) \\end{pmatrix} = E \\begin{pmatrix} \\psi_1(x) \\\\ \\psi_2(x) \\end{pmatrix}$", "equations": ["\\begin{bmatrix} \\frac{h^2}{2m} \\frac{d^2}{dx^2} + B_z\\mu_B\\sigma_z \\end{bmatrix} \\begin{pmatrix} \\psi_1(x) \\\\ \\psi_2(x) \\end{pmatrix} = E \\begin{pmatrix} \\psi_1(x) \\\\ \\psi_2(x) \\end{pmatrix}"]}, {"title": "Conclusion and discussion", "content": "In this study, we extend the model introduced in SciNet, which leverages machine learning to identify physical concepts and aims to emulate human physical reasoning. Building upon SciNet's questioning mechanism, we integrate Variational Autoencoders and Neural Ordinary Differential Equations to propose a neural network framework capable of simultaneously discovering physical concepts and governing equations. The feasibility and broad applicability of this framework are demonstrated through four significant case studies, ranging from classical to quantum physics.\nOne particularly notable case involves the investigation of spin-1/2 particles and the Pauli equation. Our study highlights an intriguing difference from the historical development of physics. Traditionally, physicists developed the theory by interpreting the two distinct stripes observed in the Stern-Gerlach experiment under a non-uniform magnetic field, combining experimental observations with theoretical reasoning. In contrast, our research shows that an end-to-end approach enables a machine to directly extract latent physical patterns from experimental data obtained under a uniform magnetic field, where only a single stripe is observed, ultimately reconstructing the same physical theory."}, {"title": "Discussion", "content": "Our approach is capable of automatically discovering physical concepts and governing equations, but it also faces certain limitations in its application. For instance, in the case of Newton's law of universal gravitation, the performance of the neural network deteriorates as the range of evolution time in the observational data increases. We speculate that this may be due to the growing differences among trajectories over extended time periods, which increase the complexity of the data and hinder the model's ability to effectively learn the underlying physical laws. Similarly, in the case of spin-1/2 particles with the Pauli equation, the neural network performs better on data corresponding to earlier step points compared to its performance on data from later step points.\nThese challenges can be attributed to the inherent \u201ccurse of length\" problem in Neural ODEs [101-103], which limits their ability to effectively learn and generalize physical laws over longer time scales. Several potential approaches can be explored in the future to address this issue. For example, Iakovlev et al. [102] proposed using sparse Bayesian multiple shooting techniques integrated with Transformers within Neural ODEs models. This approach segments longer trajectories into shorter fragments and optimizes them in parallel, thereby improving efficiency and stability.\nDespite these technical challenges, our study demonstrates that a machine learning framework combining the joint training of physical concept identification and equation discovery modules, along with the integration of a questioning mechanism and universal constraints, can successfully uncover hidden physical theories. Future improvements to this framework can be pursued in several directions. For instance, employing more powerful machine learning models than VAEs and Neural ODEs to learn physical concepts and equations; designing more effective questioning mechanisms to guide the learning process;"}, {"title": "Analytical Derivation of the Loss Function", "content": "Our model's loss function consists of two components: the reconstruction loss $L_R$ and the regularization loss $L_{KL}$. We introduce a hyperparameter $\u03b2$ to balance the trade-off between encouraging the independence of the latent variables and controlling the regularization strength. Additionally, we account for the parameters $\\zeta$ of Neural ODEs network. The overall loss function is then defined as:\n$L(x; \\phi, \\zeta, \\theta) = L_R(x; \\phi, \\zeta, \\theta) + \\beta \\cdot L_{KL}(x; \\phi, \\zeta, \\theta),$\nwhere\n$L_R(x; \\phi, \\zeta, \\theta) = -E_{q_{\\phi}(z|x)} [log p_{\\theta}(x|z)],$\n$L_{KL}(x; \\phi, \\zeta, \\theta) = D_{KL}(q_{\\phi}(z|x)||p_{\\theta}(z)).$\nFirst, we address the reconstruction error $L_R(x;\\phi,\\zeta,\\theta)$. For a given set of data $x^i$, assume the decoder $p_{\\theta}(x^i|z^i)$ outputs a conditional probability distribution as a Gaussian, where the decoder's output $\\hat{x}^i = f_{\\theta}(z^i)$ is the mean of the Gaussian distribution with a variance of 1. This implies that for each data point in a given set $x^i$, such as $x_1, x_2,...,x_n$, we have:\n$p_{\\theta}(x^i|z^i) = \\frac{1}{\\sigma_\\theta \\sqrt{2\\pi}} exp (-\\frac{(x^i - \\hat{x}^i)^2}{2}).$\nBased on the form of reconstruction error Eq. (A.2), we have:\n$L_R(x^i; \\phi, \\zeta, \\theta) = -E_{q_{\\phi}(z^i|x^i)} [-\\frac{(x^i - \\hat{x}^i)^2}{2} + log \\frac{1}{\\sqrt{2\\pi}}],$\nHere, $E_{q_{\\phi}(z^i|x^i)}$ represents the expectation over a function of the latent variable $z^i$ under the probability distribution $q_{\\phi}(z^i|x^i)$, determined by parameter $\\phi$, given the input $x^i$. Since direct computation of this expectation is challenging, we may use the Monte Carlo approximation by sampling k instances $z_1^i, z_2^i,..., z_k^i$ from $q_{\\phi}(z^i|x^i)$, and approximate this expectation by the sample average:\n$L_R(x^i; \\phi, \\zeta, \\theta) \\approx \\frac{1}{k} \\sum_{l=1}^k [-\\frac{(x^i - \\hat{x}^i)^2}{2}] + const,$\nIn the optimization process, the constant term in the loss function Eq.(A.6) does not affect the overall optimization, so the reconstruction error for the i-th data set $x^i$ is further expressed as:\n$L_R(x^i; \\phi, \\zeta, \\theta) = \\frac{1}{k} \\sum_{l=1}^k [(x - \\hat{x})^2].$", "equations": ["L(x; \\phi, \\zeta, \\theta) = L_R(x; \\phi, \\zeta, \\theta) + \\beta \\cdot L_{KL}(x; \\phi, \\zeta, \\theta),", "L_R(x; \\phi, \\zeta, \\theta) = -E_{q_{\\phi}(z|x)} [log p_{\\theta}(x|z)],", "L_{KL}(x; \\phi, \\zeta, \\theta) = D_{KL}(q_{\\phi}(z|x)||p_{\\theta}(z)).", "p_{\\theta}(x^i|z^i) = \\frac{1}{\\sigma_\\theta \\sqrt{2\\pi}} exp (-\\frac{(x^i - \\hat{x}^i)^2}{2}).", "L_R(x^i; \\phi, \\zeta, \\theta) = -E_{q_{\\phi}(z^i|x^i)} [-\\frac{(x^i - \\hat{x}^i)^2}{2} + log \\frac{1}{\\sqrt{2\\pi}}],", "L_R(x^i; \\phi, \\zeta, \\theta) \\approx \\frac{1}{k} \\sum_{l=1}^k [-\\frac{(x^i - \\hat{x}^i)^2}{2}] + const,", "L_R(x^i; \\phi, \\zeta, \\theta) = \\frac{1}{k} \\sum_{l=1}^k [(x - \\hat{x})^2]."]}, {"title": "Random Potential", "content": "Random potential functions play a significant role in the study of physics, extensively used to investigate the diffusion movement of electrons in disordered lattices [104, 105]. In the study of Anderson localization [104], by introducing random potential functions, one can construct the Hamiltonian of electrons in disordered systems to study the phenomenon of electron localization. Additionally, random potential functions are used to describe the random distribution of impurities in materials [106-108]. The random collisions between impurities and electrons lead to changes in the behavior of electron gas. By incorporating random potential functions into the Hamiltonian of electron gas, a more realistic theoretical model can be established.\nIn this article, the generation process for the random potential function $V_i(x)$ we use is as follows:\n1. Base Functions: Define a set of base functions:\n$f(x) = {sin(jx)|j = 1, 2, . . ., 9},$\n2. Random Coefficients: In each iteration, generate a set of random coefficients:\n$\\upsilon_{coef} = [c_1,..., c_j, ..., c_9],$\nwhere $c_j$ is drawn from a standard normal distribution (Gaussian with mean 0 and variance 1).\n3. Constructing Random Potential:\n$\\upsilon(x) = \\sum_{j=1}^9 c_j sin(jx) \u2013 1,$\n4. Condition Verification: Verify if the random potential function satisfies the condition within the testing range:\n$\u22123 < v(x) < 0, \u2200x \u2208 X_{test},$", "equations": ["f(x) = {sin(jx)|j = 1, 2, . . ., 9},", "\\upsilon_{coef} = [c_1,..., c_j, ..., c_9],", "\\upsilon(x) = \\sum_{j=1}^9 c_j sin(jx) \u2013 1,", "\u22123 < v(x) < 0, \u2200x \u2208 X_{test},"]}, {"title": "Motion in the Stern-Gerlach Experiment in Non-uniform and Uniform Magnetic Fields", "content": "In this section, we begin with the results of the Stern-Gerlach experiment in a non-uniform magnetic field and extend our analysis to the case of a uniform magnetic field, leading to the Pauli equation that describes this process.\nIn the Stern-Gerlach experiment, the spin of silver atoms causes a beam passing through a non-uniform magnetic field to split into two distinct directions, producing two symmetric stripes on a screen, as indicated by the dashed lines in Figure 5(a). For a spin-1/2 system with motion along the x-direction and a non-uniform magnetic field B(z) along the z-direction, and no motion in the y-direction, the system's Hamiltonian H is given by:\n$H = \\frac{p_x^2}{2m} + \\frac{p_z^2}{2m} + B(z)\\mu_B\\sigma_z,$\nwhere $B(z)\u03bc\u03b2\u03c3z$ is the energy term due to the interaction of the magnetic moment of silver atoms with the external magnetic field B(z), commonly referred to as the spin-magnetic field interaction term.", "equations": ["H = \\frac{p_x^2}{2m} + \\frac{p_z^2}{2m} + B(z)\\mu_B\\sigma_z,"]}, {"title": "Multivariable Linear System Analysis", "content": "In the case of the Pauli equation, we determined through ablation experiments that the optimal number of physical concepts describing the system is four. By analyzing the case of the Schr\u00f6dinger equation, we predict that the neural network will learn a linear combination of these four variables. Since directly displaying a multivariable linear system graphically is complex, we proceed with the verification through the following steps:\n1. We represent the output of each neuron in the neural network, $N(k,x)$, as a linear combination of four known real vector variables: $\u03c8_1(x)$, $d\u03c8_1(x)/dx$, $\u03c8_2(x)$, $d\u03c8_2(x)/dx$. The specific formula is:\n$N(k,x) = a \\cdot \u03c8_1(x) + b \\cdot \\frac{d\u03c8_1(x)}{dx} + c \\cdot \u03c8_2(x) + d \\cdot \\frac{d\u03c8_2(x)}{dx} + const,$\nwhere k = 1, 2, 3, 4 corresponds to the outputs of the four neurons.\n2. By fitting, we obtain the linear coefficients a, b, c, d, and constant. We use these coefficients to calculate the predicted output for 4000 sample points:\n$Predict_i(x) = a \\cdot \u03c8_{1_i}(x) + b \\cdot \\frac{d\u03c8_{1_i}(x)}{dx} + c \\cdot \u03c8_{2_i}(x) + d \\cdot \\frac{d\u03c8_{2_i}(x)}{dx} + const,$\n3. We evaluate the model performance by calculating the L2 norm of the relative error [100] between $N(k, x)$ and the predicted output $Predict(x)$:\n$\\frac{||N(k, x) \u2013 Predict(x)||_2}{||N(k, x) ||_2},$\n4. We analyze this error. If the error is minimal, it indicates that the neurons have indeed learned the linear combination of these four variables.", "equations": ["N(k,x) = a \\cdot \u03c8_1(x) + b \\cdot \\frac{d\u03c8_1(x)}{dx} + c \\cdot \u03c8_2(x) + d \\cdot \\frac{d\u03c8_2(x)}{dx} + const,", "Predict_i(x) = a \\cdot \u03c8_{1_i}(x) + b \\cdot \\frac{d\u03c8_{1_i}(x)}{dx} + c \\cdot \u03c8_{2_i}(x) + d \\cdot \\frac{d\u03c8_{2_i}(x)}{dx} + const,", "\\frac{||N(k, x) \u2013 Predict(x)||_2}{||N(k, x) ||_2},"]}]}