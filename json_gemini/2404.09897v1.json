{"title": "Progressive Knowledge Graph Completion", "authors": ["Jiayi Li", "Ruilin Luo", "Jiaqi Sun", "Jing Xiao", "Yujiu Yang"], "abstract": "Knowledge Graph Completion (KGC) has\nemerged as a promising solution to address\nthe issue of incompleteness within Knowledge\nGraphs (KGs). Traditional KGC research pri-\nmarily centers on triple classification and link\nprediction. Nevertheless, we contend that these\ntasks do not align well with real-world scenar-\nios and merely serve as surrogate benchmarks.\nIn this paper, we investigate three crucial pro-\ncesses relevant to real-world construction sce-\nnarios: (a) the verification process, which arises\nfrom the necessity and limitations of human\nverifiers; (b) the mining process, which iden-\ntifies the most promising candidates for verifi-\ncation; and (c) the training process, which har-\nnesses verified data for subsequent utilization;\nin order to achieve a transition toward more\nrealistic challenges. By integrating these three\nprocesses, we introduce the Progressive Knowl-\nedge Graph Completion (PKGC) task, which\nsimulates the gradual completion of KGs in\nreal-world scenarios. Furthermore, to expedite\nPKGC processing, we propose two accelera-\ntion modules: Optimized Top-k algorithm and\nSemantic Validity Filter. These modules sig-\nnificantly enhance the efficiency of the mining\nprocedure. Our experiments demonstrate that\nperformance in link prediction does not accu-\nrately reflect performance in PKGC. A more\nin-depth analysis reveals the key factors influ-\nencing the results and provides potential direc-\ntions for future research. Codes are available at\nhttps://github.com/hyleepp/Continue-KGC.", "sections": [{"title": "1 Introduction", "content": "Knowledge Graphs (KGs) have wide-ranging appli-\ncations across diverse domains, including question\nanswering (Mohammed et al., 2018), information\nextraction (Han et al., 2018), and recommender sys-\ntems (Zhang et al., 2016). Nevertheless, KGs fre-\nquently grapple with incompleteness, resulting in\nthe absence of critical factual links (Gal\u00e1rraga et al.,\n2017). Consequently, Knowledge Graph Comple-\ntion (KGC) assumes a pivotal role in automating\nthe enhancement of KGs (Sun et al., 2019b).\nIn the past, tasks such as link prediction and\ntriple classification required predicting the tail en-\ntity of a query and judging the correctness of the\nproposed triples, respectively. However, where do\nthese effective queries come from in real-world sce-\nnarios? No previous work has explored this issue.\nFurthermore, the performance of models in these\ntasks falls short of the high accuracy requirements\nof KG, as confirmed by prominent companies like\nGoogle (Pan et al., 2023). Therefore, we advocate\nfor simulating a more realistic scenario in KGC.\nTo meet the requirements for stringent knowledge\nprecision, it is necessary to incorporate a verifica-\ntion process to simulate human-machine collabo-\nration. Additionally, human verifiers face inherent\nlimitations in their daily data processing capacity,\nunderscoring the need for a mining process. Within\nthis process, KGC models curate a specific quota\nof the most promising facts. These freshly acquired\nfacts, in turn, facilitate the iterative refinement of\nKGC models through a training process. Finally,\nwe iteratively implement these three processes to\nform a progressive completion.\nDrawing from the above reasons, we introduce\nthe Progressive Knowledge Graph Completion\n(PKGC) task. PKGC emulates the gradual pro-\ncess of KG completion, as depicted in Figure 1.\nIt commences by training a model using a KG\nand subsequently invokes the model to identify\nthe most promising facts. Thereafter, a limited ver-\nifier selects the true facts, integrating them into\nthe KG.Furthermore, we have made substantial\nstrides in expediting the mining process, particu-\nlarly vital given the exponential growth in potential\nfacts as KGs expand in size. To address this, we\nintroduce two modules: Optimized Top-k and Se-\nmantic Validity Filter (SVF), both of which bring"}, {"title": "2 Related Work", "content": "Knowledge Graph Embedding KGC encom-\npasses a diverse array of methods (Ji et al., 2021),\nincluding embedding-based (Bordes et al., 2013;\nWang et al., 2014b; Sun et al., 2019a; Zhang\net al., 2022), rule-based (Gal\u00e1rraga et al., 2013;\nOmran et al., 2021), and reinforcement learning-\nbased (Xiong et al., 2017; Lin et al., 2018). Given\nthe prevalence of the embedding-based approach,\noften referred to as Knowledge Graph Embedding\n(KGE), this article primarily centers its focus on\nthis methodology. Notably, KGE methods have\nconsistently dominated the leaderboards of compe-\ntitions like ogb-wiki2 (Hu et al., 2020), a promi-\nnent fixture in the field of KGC. To streamline the\nscope, this paper confines its inquiry to the realm of\nstructural information, excluding description-based\napproaches (Xie et al., 2016; Wang et al., 2014a;\nAn et al., 2018; Wang et al., 2021; Yao et al., 2019)\nwithin KGE.\nThe pioneering distance-based model in KGE,\nTransE (Bordes et al., 2013), conceptualizes each\nrelation as a translation operation. TransR (Lin\net al., 2015) and TransH (Wang et al., 2014b) em-\nploy projections to handle complex relations, while\nMuRP (Balazevic et al., 2019) extends the model-\ning space to hyperbolic geometry to capture hierar-\nchical structures within KGs. In addition to transla-\ntion, RotatE (Sun et al., 2019a) introduces rotation\nas a means to represent relations. RotH (Chami\net al., 2020) further amalgamates these two opera-\ntions into hyperbolic space. RESCAL (Nickel et al.,\n2011) stands as the inaugural bilinear-based model,\nwhile CP (Hitchcock, 1927) simplifies relation ma-\ntrices to diagonal representations. Deep learning-\nbased models harness convolutional neural net-\nworks (Dettmers et al., 2018; Nguyen et al., 2018),\nTransformers (Chen et al., 2021; Vaswani et al.,\n2017), and graph neural networks (Schlichtkrull\net al., 2018; Wang et al., 2020; Liu et al., 2021; Tan\net al., 2023) as encoders, synergizing them with the\naforementioned decoder models."}, {"title": "3 Methodology", "content": "This section begins with an in-depth exploration\nof the progressive completion task, further detailed\nin Section 3.1. Following the introduction of our\nproposed task formulation, we present two accel-\neration techniques in Section 3.2: the optimized\ntop-k filtering algorithm and the semantic valid-\nity filtering module, both of which render PKGC\nfeasible."}, {"title": "3.1 Progressive Knowledge Graph\nCompletion", "content": "To commence, PKGC initiates by partitioning a\nKG, denoted as $\\mathcal{F}$, into two components: the\nknown portion, refered to as $\\mathcal{F}_{known}$, and the un-\nknown portion, labeled $\\mathcal{F}_{un}$, based on a predefined\nratio $\\rho$. This procedure is in alignment with SKGC\npractices. Distinguishing itself from SKGC, PKGC\nsystematically advances by expanding the known\nsegment, $\\mathcal{F}_{known}$, through a sequence of incremen-\ntal verifications executed by a verifier denoted as\n$\\psi$, with the capability to authenticate new candidate\nfacts during each iteration. This process adheres to\na cyclic routine that encompasses training, mining,\nand verification procedures, as elaborated in Algo-\nrithm 1. (Detailed comparison is in Appendix A)"}, {"title": "3.2 Acceleration for Mining Process", "content": "Given that the mining process can be fundamentally\nlikened to a top-k procedure, executing it without\noptimization proves to be exceedingly impractical\ndue to the sheer volume of potential facts involved.\nTo illustrate the magnitude of this issue, consider\na KG housing $|E|$ entities and $|R|$ relations. Such a\nKG spawns an astronomical $|E|^2|R|$ potential facts,\nrendering the storage and effective sorting of these\nfacts unviable through conventional means. In re-\nsponse to this formidable challenge, we introduce\ntwo purpose-built modules: Optimized Top-k and\nSVF. These modules are meticulously crafted to\nachieve substantial reductions in the time and space\nresources necessary for mining while preserving\nefficiency and effectiveness."}, {"title": "3.2.1 Optimized Top-k algorithm", "content": "In order to obviate the necessity of retaining scores\nfor every conceivable triple, we have elected to\nimplement a heap structure, which entails only a\nmodest additional space of $O(k)$. Nevertheless, the\nunrefined heap-based top-k algorithm endeavors to\nestablish a min-heap and proceeds to insert each\npotential triple into the heap contingent upon its\nrespective score. Notably, despite the algorithm's\ntime complexity of $O(n \\log(k))$ (Cormen et al.,\n2022), its practicality is hampered by the vast scale\nof $|E|^2|R|$.\nTo address this issue, we introduce an optimized\ntop-k algorithm that builds upon the fundamental\napproach. This algorithm comprises two intricately\ninterconnected components: the root filter and the\nwarm-up.\nRoot Filter In light of the impracticality of ac-\nquiring scores for all conceivable facts in a single\nsweep, we adopt a strategy of segregating them\ninto a sequence of incremental batches. This se-\nquential approach offers us a unique opportunity\nto refine the filtration process within each batch by\nleveraging insights gleaned from previous batches."}, {"title": "warm-up", "content": "While the root filter significantly ex-\npedites the processing of subsequent batches, its\neffectiveness does not extend to the initial batch.\nWe have observed that when dealing with a sizable\nbatch size, the execution of the first batch expe-\nriences notable delays. To address this challenge\nand to maximize the utility of the root filter, we\nhave introduced a warm-up schedule for the batch\nsize within each mining process. Specifically, we\ninitiate with a batch size of 1 and systematically\nincrease it exponentially until it aligns with the in-\ntended batch size, as illustrated in Figure 2b. This\napproach is designed to mitigate the performance\nissues associated with larger batch sizes during the\ninitial stages of the mining process."}, {"title": "3.2.2 Semantic Validity Filter", "content": "In addition to delving into the mining process, we\ntackle the aspect of \"what to mine\" by introducing\na pivotal component, the Semantic Validity Fil-\nter (SVF), which efficiently trims down the search\nspace. Our inspiration for this derives from the\nrecognition that not all combinations of entities\nand relations are valid (Li and Yang, 2022). For\ninstance, the head entity of the relation 'isCityOf'\ncan only be the name of a 'city', and not a 'human'.\nThe SVF implementation entails acquiring class\ninformation for each entity and maintaining a set\nthat documents each pairing of class and relation\ninitially present in the initial known KG $\\mathcal{F}_{known}$."}, {"title": "4 Experiment", "content": "In this section, our discussion unfolds in a struc-\ntured manner. We initiate by introducing the ex-\nperimental setup, elaborated upon in Section 4.1.\nSubsequently, we unveil the key findings in Sec-\ntion 4.2, culminating in a thorough analysis of the\nobserved phenomena in Section 5."}, {"title": "4.1 Experiment Setting", "content": "Datasets Our datasets are meticulously crafted,\nrooted in FB15k and WN18 (Nickel et al., 2011).\nTo maintain a consistent initial completion ratio,\ndenoted as $\\rho$, we partition the dataset into two dis-\ntinct categories: the initial triples, $\\mathcal{K}_{known}$, and the\nunexplored triples, $\\mathcal{K}_{un}$. During this data partition-\ning process, we impose stringent constraints to en-\nsure that undetermined entities and relations do not\nappear in $\\mathcal{F}_{un}$. Comprehensive dataset statistics\nare thoughtfully presented in Table 1. It's note-\nworthy that we consciously opted against utilizing\nthe FB15k-237 (Toutanova and Chen, 2015) and\nWN18RR (Dettmers et al., 2018) datasets, favoring\na pursuit of more lifelike scenarios. Our primary\nresearch focus pivots not on the dataset's inherent\ncomplexity, but rather on the nuanced exploration\nof basic model performance within the context of\nreal-world scenarios.\nPartition In the process of partitioning the\ndataset, our primary objective is to ensure the com-\nprehensive inclusion of all entities and relations\nwithin $\\mathcal{K}_{known}$. This goal guides a meticulous\nprocedure, during which we thoroughly scrutinize\neach triple, diligently recording the entities and\nrelations that make an appearance. In the event\nthat a triple is encountered, containing components\nthat have not yet found a place in our records, it is\npromptly added to the scaffold set. The remaining\ntriples undergo a division into two distinct seg-\nments, this division hinging on the predetermined\nratio $\\rho$. One segment becomes $\\mathcal{K}_{un}$, while the other\nsegment is integrated into the scaffold set to form\n$\\mathcal{K}_{known}$. For an exhaustive and step-by-step eluci-"}, {"title": "Baselines", "content": "Our study encompasses a comprehen-\nsive comparative analysis between our task and the\nwell-established traditional structure-based mod-\nels, which can be categorized into two primary\ngroups: distance-based models and bilinear mod-\nels.\nThe distance-based models considered in\nthis study are TransE (Bordes et al., 2013), Ro-\ntatE (Sun et al., 2019a), and RotE (Chami et al.,\n2020). On the other hand, the bilinear models com-\nprise RESCAL (Nickel et al., 2011), CP (Hitch-\ncock, 1927), ComplEx (Trouillon et al., 2016),\nQuatE (Zhang et al., 2019), and UniBi (Li et al.,\n2023). UniBi stands out for its unique approach of\nnormalizing both the modulus of the entity vector\nand the spectral radius of the relational matrix to 1.\nTraining The PKGC training process consists of\nthree phases: (1) Hyperparameter optimization, (2)\nModel training, and (3) Model update. During the\ninitial phase, we partition the set of known facts,\n$\\mathcal{F}_{known}$, into training and validation subsets to per-\nform hyperparameter optimization, following the\napproach established in prior SKGC research. In\nthe second phase, the model is trained using the\ncomplete dataset of $\\mathcal{F}_{known}$. In the final phase, the\nmodel is updated based on verified facts. This paper\nprimarily focuses on the first two phases, with the"}, {"title": "4.2 Main Result", "content": "The performance of the models on WN18 and\nFB15k is presented in Table 2. Additionally, we\nprovide a detailed illustration of the dynamics of\nthe models on these datasets through Figure 4a\nand Figure 4b to enhance our understanding of the\nPKGC process.\nInitially, we conduct separate analyses of the\nmodels' performance on SKGC and PKGC. Our\nfindings indicate that most models exhibit strong\nperformance on both datasets within the realm of\nSKGC, whereas this extends to only one dataset\nwithin PKGC. Notably, UniBi stands out as the sole\nmodel demonstrating commendable performance\nacross all metrics and datasets.\nFurthermore, we note that the performance of\nmodels in link prediction does not consistently\nalign with their performance in PKGC. On one\nhand, the models' performance in link prediction\nand PKGC sometimes diverges. For instance,\nRESCAL exhibits the lowest Mean Reciprocal\nRank (MRR) on FB15k but secures the third-best\nresult in Completion Ratio (CR). On the other\nhand, models that perform similarly in link pre-\ndiction may exhibit differences in PKGC perfor-\nmance. A case in point is the closeness in MRR be-\ntween UniBi-O(3) and ComplEx (0.548 vs. 0.547),\nyet they demonstrate disparities in CR (0.995 vs.\n0.986). Notably, QuatE and UniBi are the only\nmodels performing well across all metrics in both\ndatasets."}, {"title": "5 Analysis", "content": "In this section, we mainly discuss: 1) What are the\nkey factors in PKGC, for which we have conducted\na discussion related to the normalization (RQ1);\n2) The effectiveness of the acceleration module\nwe proposed (RQ2); 3) Improving this task from\nthe perspective of incremental learning, provid-\ning a feasible path for future research (RQ3); 4)\nWhether PKGC can execute in low-resource situa-\ntion (RQ4)."}, {"title": "5.1 RQ1: How Normalization Work in\nPKGC?", "content": "Given UniBi's remarkable performance over other\nmodels, we delve into an investigation to uncover\nthe factors contributing to its strength. UniBi's\nunique feature lies in its normalization of both enti-\nties and relations, which is unusual among bilinear-\nbased models. We argue that UniBi's performance\nenhancement primarily stems from its ability to\nmake triples comparable. To illustrate, consider\ntwo triples: $f_1 = (h,r_1,t)$ and $f_2 = (h,r_2, t)$,\nwith corresponding scores $s(f_1) = a$ and $s(f_2) =$\n$b$, where $a > b$. Without normalization, deter-\nmining the relative plausibility of $f_1$ and $f_2$ is\nchallenging due to the unaccounted score range.\nFor instance, consider the case where $|e| <1$\nand $s(\\cdot) = p_rh \\cdot t$, confined within $[-p_r, p_r]$,\nwith $p_{r_1} = 2a$ and $p_{r_2} = b$. Here, $s(f_2) = b$\nachieves the highest score under relation $r_2$, while\n$s(f_1) < 2a$ still has potential for improvement.\nThus, $f_2$ appears more plausible than $f_1$. UniBi\ndistinguishes itself from other bilinear-based mod-\nels by ensuring that $s(\\cdot)$ falls within $[-1,1]$ for\nany triple, enabling more rational comparisons be-\ntween relations. We present CP and ComplEx as\ncase studies, showing that both models improve\ntheir performance with relation normalization, as\nshown in Figure 5. We suggest that this advantage\nbecomes more apparent when the number of triples\nper relation is limited, shedding light on why UniBi\noutperforms on FB15k but not on WN18."}, {"title": "5.2 RQ2: Ablation on Acceleration Modules\nw.r.t. Time cost", "content": "We commence by demonstrating the efficacy of\ntwo acceleration modules proposed for knowledge\nmining. Results in Table 3 reveal that all three\nmodules contribute significantly to expediting the\nmining process, with the root filter exhibiting the\nmost substantial speedup, exceeding 2,000 times.\nMore discussion are in Appendix D."}, {"title": "5.3 RQ3: Incremental Aspect on PKGC", "content": "In order to harness freshly validated knowledge,\nwe employ the approach of incremental learning\nthrough retraining and fine-tuning, incorporating\nboth existing and recently confirmed facts into the\\ongoing training process. We conduct incremental\nmodel training at a specified frequency denoted as\n$\\Delta s$ ($\\Delta s$ = 5 in our setting). During retraining,\nour training data encompasses $\\mathcal{F}_{known}$, which com-\nprises the verified facts from the most recent $\\Delta s$\nsteps denoted as $\\mathcal{F}_{new}$. Conversely, during fine-\ntuning, our training is exclusively focused on the\nfacts in $\\mathcal{F}_{new}$ from the recent $\\Delta s$ steps. Addition-\nally, we introduce an additional term to ensure that\nthe previously acquired entity representations un-\ndergo moderate alterations (Formulated expression\nin Appendix B).\nThe outcomes, as depicted in Table 4, pertaining\nto the WN18 dataset, reveal that retraining often\nproves effective in bolstering performance, whereas\nfine-tuning may have adverse consequences. Dur-\ning progressive mining, at every stage, the recently\nacquired facts consistently receive high rankings.\nThis implies that the model inherently possesses\nconfidence in accurately identifying these factual\ntriplets. Consequently, the impact of incorporat-\ning these new data into incremental learning is\nrelatively modest. As emphasized in the context\nof active learning (Ren et al., 2022), it is crucial\nto emphasize samples near the decision boundary,\nparticularly those involving low-ranking factual\ntriplets."}, {"title": "5.4 RQ4: Low-resource PKGC", "content": "In this section, we explore low-resource PKGC\nto showcase its extensive practical applications.\nTo achieve this, we reduce $\\rho$, resulting in train-\ning the model on a smaller $\\mathcal{K}_{known}$ while exploring\nthe knowledge within a larger $\\mathcal{F}_{un}$.Table 5 demon-\nstrates that even in a low-resource scenario, UniBi\nultimately attains reasonably satisfactory comple-\ntion rates. On the WN18 dataset, when $\\mathcal{K}_{known}$\nholds only 30% of the knowledge, UniBi achieves\nnearly 60% completeness after 50 rounds of min-\ning. With an initial knowledge base comprising\n50%, this figure increases to almost 80%. On the\nFB15k dataset, after 200 completion rounds, UniBi\nenhances a knowledge base with a 50% complete-\nness rate to nearly 90%. Similarly, concerning the\nCR@k metric, UniBi consistently retains its rank-\ning position in comparison to the CP model.The re-\nsults demonstrate that even in resource-constrained\nsettings, PKGC consistently yields favorable out-\ncomes, establishing it as a pragmatic experimental\nframework."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a novel task called\nPKGC, which offers increased realism and a funda-\nmental distinction by incorporating a finite verifier,\nand shifts the status of the model to that of a candi-\ndate knowledge proposer.In order to make the task\nmore cost-effective, we have devised an optimized\ntop-k algorithm that integrates with the semantic\nvalidity filter to significantly expedite the mining\nprocess in PKGC. Furthermore, We conduct a com-\nprehensive series of experiments to demonstrate\nthe performance of baseline models and to ana-\nlyze factors that distinguish them. Our findings\nindicate that previous metrics and knowledge are\ninsufficient for accurately predicting and explain-\ning phenomena within PKGC. Additionally, We\nhave preliminarily explored the effectiveness of\nsome incremental learning methods within the con-\ntext of PKGC, highlighting the potential for further"}, {"title": "A.1 Connectivity Perspecitve", "content": "In this section, we delve into an examination of the\ntask disparities with a specific focus on their con-\nnectivity attributes. This analysis involves treating\neach fact as a vertex and each comparison between\nfacts as an edge, thereby evaluating the connectiv-\nity of the resultant graph. As elucidated in Figure\n6a, our findings reveal the following distinctions:\n(a) triple classification fails to establish any form of\ngraph due to its exclusive reliance on comparisons\nbetween facts and a predefined threshold, (b) link\nprediction yields outcomes for distinct connected\ncomponents associated with different queries, and\n(c) mining leads to the creation of a connected\ngraph.\nIt is essential to note that in cases where a\nKnowledge Graph regresses into a homogenous\ngraph characterized by a single relation, there is in-\nevitably only one connected component, which en-\nsures the graph's connectedness. This observation\nreinforces our belief that, link prediction emerges\nas a more suitable task in the context of Graph\nNeural Networks (GNN) instead of KGE."}, {"title": "A.2 Comparison Perspective", "content": "Within this section, our objective is to elucidate the\ndisparities inherent in the evaluation tasks of SKGC\nand PKGC. We focus on the scope of comparison,\nin addition to recognizing the apparent distinction\nin progressiveness.\nSKGC encompasses two fundamental tasks:\ntriple classification and link prediction. As de-\npicted in Figure 6b, triple classification entails the\ncomparison of each fact with a predefined thresh-"}, {"title": "B Supplementary Details in Incremental\nTraining", "content": "It is noteworthy that we do not apply a uniform\nregularization term for relation representations, as"}]}