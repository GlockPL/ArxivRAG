{"title": "Privacy-Preserving Decentralized AI with Confidential Computing", "authors": ["Dayeol Lee", "Jorge Ant\u00f3nio", "Hisham Khan"], "abstract": "In this paper, we discuss privacy protection based on Confidential Computing (CC) in Atoma Network, which aims to decentralize Artificial Intelligence (AI). Decentralized AI aims to distribute AI inference or training services between multiple entities so that there is no central authority. We believe that decentralization is a crucial technology for trustworthy, transparent, and robust AI. We truly believe that Atoma, a decentralized network tailored to AI, will unlock a lot of AI applications in the Web3 domain.\nOne of the primary challenges in the realm of decentralized AI is safeguarding privacy. Decentralized AI enables a diverse ecosystem where node operators, model providers, data providers, and end-users can all contribute and interact within the network without the need for central oversight or permissions. This openness promotes innovation and broader access, but it also introduces significant privacy concerns.\nConsider, for instance, data centers equipped with AI accelerators such as GPUs. These centers can contribute their computing power to the network, allowing for distributed processing and scaling. Simultaneously, owners of large foundational models, whether proprietary or open source, can deploy these models to the decentralized system, making them available for various use cases. End-users, on the other hand, can run AI inference or training workloads using their own personal data, benefiting from the network's distributed capabilities.\nHowever, this decentralized structure inherently implies that sensitive assets, such as proprietary models, confidential data, and personal information, may be shared among a network of participants who may not always be trustworthy. The permissionless nature of such systems means that not all participants are subject to stringent regulations or bound by reputational concerns, thus creating an environment where privacy violations could occur easily. Incentives to protect privacy might be misaligned, as some participants may prioritize financial gain over ethical considerations.\nFurthermore, the decentralized nature of the network complicates the enforcement of privacy standards. Traditional systems rely on centralized control to enforce privacy policies, regulate access, and ensure accountability. However,", "sections": [{"title": "1 Introduction", "content": "In this paper, we discuss privacy protection based on Confidential Computing (CC) in Atoma Network, which aims to decentralize Artificial Intelligence (AI). Decentralized AI aims to distribute AI inference or training services between multiple entities so that there is no central authority. We believe that decentralization is a crucial technology for trustworthy, transparent, and robust AI. We truly believe that Atoma, a decentralized network tailored to AI, will unlock a lot of AI applications in the Web3 domain.\nOne of the primary challenges in the realm of decentralized AI is safeguarding privacy. Decentralized AI enables a diverse ecosystem where node operators, model providers, data providers, and end-users can all contribute and interact within the network without the need for central oversight or permissions. This openness promotes innovation and broader access, but it also introduces significant privacy concerns.\nConsider, for instance, data centers equipped with AI accelerators such as GPUs. These centers can contribute their computing power to the network, allowing for distributed processing and scaling. Simultaneously, owners of large foundational models, whether proprietary or open source, can deploy these models to the decentralized system, making them available for various use cases. End-users, on the other hand, can run AI inference or training workloads using their own personal data, benefiting from the network's distributed capabilities.\nHowever, this decentralized structure inherently implies that sensitive assets, such as proprietary models, confidential data, and personal information, may be shared among a network of participants who may not always be trustworthy. The permissionless nature of such systems means that not all participants are subject to stringent regulations or bound by reputational concerns, thus creating an environment where privacy violations could occur easily. Incentives to protect privacy might be misaligned, as some participants may prioritize financial gain over ethical considerations.\nFurthermore, the decentralized nature of the network complicates the enforcement of privacy standards. Traditional systems rely on centralized control to enforce privacy policies, regulate access, and ensure accountability. However,"}, {"title": "2 Privacy in Decentralized AI", "content": "Protecting privacy is fundamental to the success of decentralized AI. In a decentralized environment, permissionless participation allows anyone to join the network as a node operator, providing computational power in exchange for rewards. However, this openness introduces significant risks, as sensitive data-including inputs, outputs, model weights, and intermediate computational states can reside in the memory of nodes operated by random, and potentially untrustworthy, participants. Additionally, some of these values may need to be publicly visible on-chain for purposes of transparency and verifiability, further complicating the privacy landscape.\nUnfortunately, many aspects of AI workloads require confidentiality and, without proper privacy protections, the decentralized AI ecosystem may struggle to gain the trust of key stakeholders. To illustrate this, consider the following scenarios that demonstrate why privacy is crucial in decentralized AI systems.\n\u2022 Inference. Many AI inference use cases involve personal or proprietary\ninformation. For example, users running AI-powered health diagnostics\nor business-specific predictive models may want to keep both the inputs\n(e.g., personal data) and the outputs (e.g., predictions) private. Without\nprivacy, users would be reluctant to take advantage of decentralized AI\nfor sensitive applications, stifling its potential.\n\u2022 Model Serving. Owners of foundational models can seek to monetize\ntheir proprietary models by deploying them on a decentralized network.\nHowever, without strong privacy guarantees, the risk of model theft in-\ncreases significantly, as malicious actors could extract or replicate the\nmodel weights. This would disincentivize private model owners from con-\ntributing their valuable resources to the network, limiting the variety and\nrichness of models available in the decentralized ecosystem.\n\u2022 Intellectual Property. Developers who create application-specific mod-\nels or prompts rely on protecting their intellectual property (IP). In a\ncompletely open, decentralized network without privacy safeguards, sensi-\ntive IP can be exposed, especially in jurisdictions with weaker regulations.\nThe lack of data privacy mechanisms could result in widespread leaks,\ndiminishing the value of proprietary technologies, and discouraging the\ndevelopment of sophisticated applications on decentralized AI platforms.\n\u2022 Model Training. Decentralized AI offers the exciting possibility of col-\nlaborative model training, where participants can contribute datasets to\nenhance model performance. However, data owners are unlikely to par-\nticipate if they are required to directly expose their data to others. Many"}, {"title": "3 Confidential Computing for Decentralized \u0391\u0399", "content": "Confidential Computing is a promising technology that can protect applications\nfrom the rest of the system by using a hardware-based isolation technique called\ntrusted execution environment (TEE). TEEs aim to isolate an application dur-\ning its execution from any other software, even the operating system. This\nmeans that they could protect the confidentiality and integrity of the applica-\ntion while it's running on a remote machine that is owned and operated by\nan untrusted operator. Thus, TEEs can offer privacy even in a decentralized\nsetting, where anyone can participate to the network as a node operator.\nCryptographic solutions like zkML exist, but they are not yet equipped to\nhandle AI workloads requiring extensive computation and large memory use.\nTo provide context, the zkLLM work cited in [7] is currently seen as the\nleading zkML protocol that aims to achieve verifiable LLM inference. It utilizes\nGPU acceleration with the CUDA library in [3] to handle BLS12-381 curve\noperations on NVIDIA GPUs. Notably, zkLLM achieves a 15-minute proof\ngeneration time for a single 13 billion-parameter LLM inference forward pass,\nwhich predicts the next token in the sequence. However, producing an entire\noutput sequence of 100 tokens (a typical short answer) requires aggregating 100\nforward pass zk proofs, resulting in at least 1500 minutes (or 25 hours) for full\nLLM inference proof generation. Furthermore, the total proving time is expected\nto be significantly higher because the attention Key and Value (KV) caches grow\nlinearly, while the softmax scores expand quadratically with sequence length.\nGiven that GPU memory, rather than compute capabilities, constrains LLM\ninference on GPUs, expanding KV caches will likely lead to even greater proof\ngeneration times in practice.\nIn contrast, TEEs offer small overhead as it relies on hardware-based mem-\nory isolation and encryption, and allow processors to compute on plain text.\nThey are readily available in most future-generation server processors by major\nvendors. Intel has been including Trusted Domain Extension (TDX) since 5th\ngeneration Xeon server processors (Emerald Rapids) and onwards, and AMD\nhas been including Secure Encrypted Virtualization for Secure Nested Paging\n(SEV-SNP) since the Milan generation of EPYC server processors and onwards.\nAlso, NVIDIA has been including Confidential Computing as its main security"}, {"title": "4 Leveraging Confidential Computing for Privacy in Decentralized \u0391\u0399", "content": "In this section, we explore how Atoma leverages Trusted Execution Environ-\nments (TEEs) within its execution nodes to enhance privacy and secure AI\nworkloads. By utilizing TEEs, Atoma ensures that sensitive data, such as the\ninputs and outputs of AI tasks, remain protected from the node operators who\nhost the execution environments. For a deeper discussion on model privacy,\nrefer to Section 6.\nTo incentivize privacy-focused infrastructure, node operators are rewarded\nfor offering TEE-capable hardware, with additional compensation for maintain-\ning these capabilities. When network users request privacy for their AI work-\nloads, Atoma intelligently routes their requests to execution nodes equipped\nwith TEEs. TEEs ensure that inputs, outputs, and models are decrypted only\nwithin an isolated and secure environment such as a virtual machine with a\npredefined initial state ensuring that even the node operators, who control the\nunderlying hardware, cannot access the sensitive data. This encryption process\nis further secured by provisioning decryption keys exclusively to the specific\nTEE instance handling the workload, thereby mitigating risks of unauthorized\naccess.\nAtoma's privacy architecture includes two distinct approaches for key provi-\nsioning and attestation: User Attestation (Figure 1) and On-Chain Attestation\n(Figure 2). These approaches ensure that the integrity and privacy of the ex-\necution environment are verified before sensitive data is decrypted, adding an\nadditional layer of trust to the system. Through this integration of TEEs and\nrobust attestation mechanisms, Atoma offers a strong solution for maintaining\nprivacy in decentralized AI networks."}, {"title": "4.1 Overview", "content": "In this section, we explore how Atoma leverages Trusted Execution Environ-\nments (TEEs) within its execution nodes to enhance privacy and secure AI\nworkloads. By utilizing TEEs, Atoma ensures that sensitive data, such as the\ninputs and outputs of AI tasks, remain protected from the node operators who\nhost the execution environments. For a deeper discussion on model privacy,\nrefer to Section 6.\nTo incentivize privacy-focused infrastructure, node operators are rewarded\nfor offering TEE-capable hardware, with additional compensation for maintain-\ning these capabilities. When network users request privacy for their AI work-\nloads, Atoma intelligently routes their requests to execution nodes equipped\nwith TEEs. TEEs ensure that inputs, outputs, and models are decrypted only\nwithin an isolated and secure environment such as a virtual machine with a\npredefined initial state ensuring that even the node operators, who control the\nunderlying hardware, cannot access the sensitive data. This encryption process\nis further secured by provisioning decryption keys exclusively to the specific\nTEE instance handling the workload, thereby mitigating risks of unauthorized\naccess.\nAtoma's privacy architecture includes two distinct approaches for key provi-\nsioning and attestation: User Attestation (Figure 1) and On-Chain Attestation\n(Figure 2). These approaches ensure that the integrity and privacy of the ex-\necution environment are verified before sensitive data is decrypted, adding an\nadditional layer of trust to the system. Through this integration of TEEs and\nrobust attestation mechanisms, Atoma offers a strong solution for maintaining\nprivacy in decentralized AI networks."}, {"title": "4.2 User Attestation", "content": "Node Registration. During the initialization phase, node operators are re-\nsponsible for launching statically configured instances on their execution nodes.\nIf their hardware both CPU and GPU-supports Trusted Execution Envi-\nronment (TEE) instances, operators can choose to deploy these secure envi-\nronments. Depending on the memory and computational requirements of the\nmodels being executed, each node can run multiple TEE instances. Once a TEE\ninstance is up and running, it will autonomously register itself with the Atoma\ncontract, signaling its readiness to handle privacy-sensitive workloads."}, {"title": "4.2.1 Discussion", "content": "For User Attestation, a local web application is required to facilitate the aTLS\nprotocol, as current web browsers do not natively support attestation within\nthe TLS handshake process. A proposal has been submitted to the IETF [8]\nto integrate attestation directly into the TLS 1.3 handshake, but widespread\nstandardization and browser support remain distant prospects. As a result, the\nmost practical solution for enabling this non-standard encrypted communication\nchannel is to utilize a local web application, which acts as an intermediary for\nsecure communication."}, {"title": "4.3 On-Chain Attestation", "content": "As an alternative to User Attestation, via the aTLS protocol, Atoma can lever-\nage on-chain attestations, which leverage on-chain verification mechanisms com-\nbined with decentralized key management. This approach introduces a layer of\ndecentralized trust by integrating TEE attestations directly into the blockchain,\nor a Data Availability alternative, ensuring that the integrity and state of the\nTrusted Execution Environment (TEE) can be verified independently and trans-\nparently by all network participants. This method draws inspiration from prior\nwork, such as that of Flashbots [4], which has successfully implemented similar\ndecentralized verification and key management techniques.\nIn contrast to User Attestation, where the user directly verifies the TEE\ninstance via a local application and aTLS protocol, On-Chain Attestation relies\non blockchain-based mechanisms to validate the TEE. Here, the TEE and user\ndo not need to engage in direct trust-building steps at the beginning of a session.\nInstead, both TEE and the user encrypt their communications using asymmetric\nkeys. The TEE's attestation report, including its public key and proof of its"}, {"title": "Asymmetric Encryption for Secure Communication.", "content": "Unlike User At-\ntestation, where a direct secure connection is established using aTLS, On-Chain\nAttestation requires the use of asymmetric encryption for message exchanges be-\ntween the user and the TEE. Once the user verifies the TEE's public key and\nattestation report on-chain, they can encrypt their requests using the TEE's\npublic key, ensuring that only the TEE instance with the corresponding pri-\nvate key can decrypt and process the data. Similarly, the TEE encrypts its\nresponses using the user's public key, ensuring confidentiality in both directions\nof communication."}, {"title": "Advantages of On-Chain Attestation.", "content": "One of the key benefits of On-\nChain Attestation is that it does not require specialized software to be installed\non the devices of the users. Additionally, it further provides a more decentral-\nized and transparent method for verifying TEE instances. Because attestation\nreports and public keys are stored on-chain, the verification process is open\nand trustless: Any participant can audit the information without relying on\nexternal parties. Additionally, this approach can scale more effectively in larger\nnetworks, as the verification process is transferred to the blockchain, eliminating\nthe need for individual users to conduct direct attestations. This reduces the\ncomplexity for end users while maintaining strong security guarantees."}, {"title": "Challenges and Trade-offs.", "content": "However, On-Chain Attestation also presents\nchallenges. The use of asymmetric encryption introduces performance over-\nhead, as encrypting and decrypting large messages can be computationally in-\ntensive. Additionally, publishing attestation reports and public keys on-chain\ncan lead to latency, depending on the speed and scalability of the underlying\nblockchain. Moreover, as this approach relies on blockchain transparency, care\nmust be taken to avoid exposing sensitive metadata that could compromise pri-\nvacy. Finally, decentralized key management, while reducing centralized trust,\nintroduces complexity in securely handling key revocation and updating mech-\nanisms."}, {"title": "4.4 Node Registration", "content": "When a node initiates a Trusted Execution Environment (TEE) instance, each\nTEE generates an asymmetric key pair-a public and private key essential for\nsecure communication and encryption. Following this, the TEE initiates a re-\nmote attestation process, binding its public key to the attestation report, which\nincludes critical details about the TEE's state and its hardware configuration.\nThis attestation process serves as a proof that the TEE instance is operating in\na secure, trusted environment, isolated from any potentially malicious software\nor hardware interference.\nThe attestation report is then submitted for verification by an on-chain veri-\nfication service. A notable example of such a service is Automata [1], which spe-\ncializes in providing decentralized, trustless verification of attestation reports.\nAutomata, or similar services, evaluates the integrity of the TEE's attestation\nreport, ensuring that it has been generated by legitimate, secure hardware, and\nthat the TEE instance adheres to the specified security guarantees. This verifi-\nfication step is critical in maintaining trust across the decentralized network, as\nit prevents compromised or falsified TEE instances from participating.\nOnce the attestation report is successfully verified, the contract governing\nthe decentralized AI network records the TEE's public key on the blockchain. By\nstoring the public key on-chain, the network ensures that all participants-whether\nthey are users, other nodes, or external validators can access and verify the\nTEE's public key without relying on a centralized authority. This decentralized\nstorage of public keys enables trustless encryption of communication between\nusers and the TEE, ensuring that only the verified TEE instance can decrypt\nand process sensitive data.\nThe registration of the public key on-chain also facilitates the wider ecosys-\ntem's interactions with the TEE. For example, when a user wants to send a\nrequest to the TEE, they can fetch the public key from the blockchain and use\nit to encrypt their data, confident that only the corresponding TEE instance,\nwith its associated private key, can decrypt and access the information. This\ncryptographic assurance protects both the user's data and the integrity of the\nAI computation being executed within the TEE.\nIn addition to recording the public key, the blockchain entry also serves as a\npublic record of the TEE's verified attestation status. This transparency allows\nanyone on the network to audit the TEE's state and verify that the node is\noperating within the prescribed security parameters. By leveraging on-chain\nverification and decentralized key management, this node registration process\nensures both security and trust in the decentralized AI network, mitigating risks\nsuch as unauthorized access, malicious computation, or tampering with sensitive\nworkloads.\nFinally, this on-chain record also facilitates future updates or key revoca-\ntions. If a TEE instance becomes compromised or needs to be reinitialized, the\nnetwork can easily revoke its public key, ensuring that compromised keys cannot\nbe used to decrypt data or interact with the system. This flexibility enhances\nthe security of the entire decentralized AI infrastructure, allowing for robust,"}, {"title": "4.5 User Requests", "content": "In the On-Chain Attestation model, users interact with the decentralized AI\nnetwork via their web browser, establishing communication through the contract\non the blockchain. When a user submits a request, the contract assigns a verified\nTEE instance to handle the task. This assignment is based on the attestation\nreports stored on-chain, ensuring that the TEE instance is trustworthy and\noperates within a secure, verified environment.\nOnce a TEE instance is assigned, the user is responsible for retrieving the\nTEE's public key, which has been previously verified and published on the\nblockchain during the node registration process. This public key plays a crucial\nrole in ensuring that the communication between the user and the TEE remains\nsecure. The user must encrypt their input data whether it's a simple text\nquery or a more complex input like a large dataset or multimedia file (e.g., im-\nages or videos) -using this public key. The encrypted data is then posted back\nto the contract, ensuring that only the designated TEE instance, possessing the\ncorresponding private key, can decrypt and process the user's input.\nFor use cases involving large files, such as image or video data, direct on-chain\nstorage can be inefficient and expensive due to the limitations of blockchain\nnetworks. As a result, Atoma will leverage an external decentralized storage\nsolutions, allowing for the scalable, cost-effective storage and retrieval of large\ndata. In this scenario, the user would upload their large input files to the\ndecentralized storage platform, and instead of posting the entire file on-chain,\nthey would include a reference (e.g., a hash or URL) to the file location in their\nencrypted request.\nThe integration of decentralized storage ensures that even large inputs can\nbe processed efficiently within the network, while maintaining the privacy and\nsecurity guarantees of the On-Chain Attestation model. By encrypting only\nthe reference to the file stored on decentralized storage, the user minimizes\nthe amount of data handled directly by the blockchain, thereby optimizing the\nperformance of the system without compromising security.\nOnce the TEE instance receives the user's encrypted input and file reference,\nit can fetch the necessary data from the decentralized storage, decrypt the input\nusing its private key, and begin processing the request securely. The user can rest\nassured that their sensitive data, whether small or large, is protected throughout\nthe entire workflow-both while stored off-chain and while being processed on-\nchain.\nMoreover, this architecture benefits from the transparency and auditability\nof the blockchain. Users can track the status of their request, verify that the\nassigned TEE instance is legitimate, and ensure that their input data remains\nsecure at every step. This seamless integration of on-chain attestation, decen-\ntralized storage, and cryptographic encryption provides a powerful framework\nfor privacy-preserving AI computations in decentralized environments."}, {"title": "4.6 Support for Current Architectures", "content": "Currently, Automata, does not provide native support for the verification of\nconfidential virtual machines (VMs) such as Intel Trusted Domain Extensions\n(TDX) or AMD Secure Encrypted Virtualization (SEV). These technologies are\ncrucial for running secure, hardware-enforced environments within decentral-\nized AI networks that ensure both the integrity and confidentiality of virtual\nmachines.\nTo address this gap, it may be necessary to develop a custom verification\nsolution specifically designed to support confidential VMs like Intel TDX and\nAMD SEV within the On-Chain Attestation framework. Building such a so-\nlution would involve implementing mechanisms that can verify the attestation\nreports generated by these confidential VMs, ensuring that they meet the same\nhigh security and integrity standards required for participation in the decentral-\nized AI network. Developing this custom verification layer involves several key\nsteps:\nAttestation Integration for Confidential VMs: First, we would need to\nintegrate the attestation processes of Intel TDX and AMD SEV into the verifi-\ncation workflow. These confidential VMs generate their own unique attestation\nreports that provide cryptographic proof of the VM's state, the hardware it runs\non, and the isolation guarantees enforced by the hardware. By capturing these\nreports, we can ensure that each VM operates in a secure environment, isolated\nfrom other processes and potential threats.\nOn-Chain Verification Logic: Next, we would need to design and imple-\nment the on-chain logic for verifying the attestation reports generated by these\nconfidential VMs. This could involve writing smart contracts capable of parsing\nand validating the cryptographic proofs included in the reports, ensuring that\nthe VMs have not been tampered with and are running trusted, verified code.\nThis on-chain logic would mirror the existing capabilities of Automata but would\nbe extended to support these specialized hardware-based VM environments.\nCommittee voting attestation verification: An alternative to the previ-\nous suggestion, is to store TEE attestation in a decentralized storage solution,\nand allow a decentralized committee of nodes to check the validity of the posted\nattestations. Each committee participant can sign the result of attestation ver-\nification, and the final aggregate signature can be submitted to the blockchain,\nas a signed committee verification agreement.\nDecentralized Key Management: Once the attestation reports are ver-\nified, the public keys of the VMs could be registered on-chain, similar to the"}, {"title": "4.7 Other Design Details", "content": "Attestation Policy. For attestation, the information about the expected\nTEE software stack and certificates should be kept up-to-date in the blockchain.\nThe policy consists of three parts: vendor policy, Atoma policy, and community\npolicy. The vendor policy includes all information about accepted measure-\nment of vendor-provided components: hardware, firmware, VMM (e.g., TDX\nSEAM), and drivers, and is governed by the hardware vendor. The Atoma\npolicy includes accepted measurements of Atoma components: VM image for\nthe execution node containing software distribution. The up-to-date policy is\nmaintained via predefined governing policy. For example, each node may fetch\nthe latest release of Atoma software stack from GitHub, and obtain the mea-\nsurement. Finally, the community policy is an extra policy that can override\nthe existing policies. For instance, the community can agree to reject a certain\nmeasurement when they find a vulnerability. We expect the combined policy to\nbe a few tens of KBs assuming there will be less than a thousand measurements\nat any time.\nInitial State Integrity. The attestation of TEE can verify the initial state\nof a system. In confidential VMs such as Intel TDX or AMD SEV-SNP, it's the\ninitial state of the virtual machine. VMs are essentially a large state machine,\nwhere its state is exponential. Thus, the initial state of the VM doesn't really tell\nmuch about the integrity of the software state when the user starts interacting\nwith the TEE via a session.\nWe could leverage a few approaches to provide the initial state guarantee\nfor each session in Atoma. First, we can rely on the confidential container\ntechnologies that are built on top of the CVMs. Confidential containers aim\nto support attestation in container granularity, but rely on the software-based\ninter-container isolation. By using confidential containers, we can bind each\nsession to a fresh container, providing the confidence that the application and\nits environment were not compromised. In addition, we can have the node rotate\nthe CVM periodically such that it will reset the TEE's state to the benign state."}, {"title": "5 Security of the Design", "content": "TEEs heavily rely on the design and implementation of the underlying hardware,\nthus may have unknown vulnerabilities that could potentially break the security\nguarantees. In this section, we analyze risks of various attacks in the context of\nAtoma's decentralized AI, and why we think TEE can provide privacy."}, {"title": "5.0.1 General TEE Threat Model", "content": "In general, TEEs provide robust security protections for applications by defend-\ning against several types of attacks:\n\u2022 Physical Memory Access. Attackers with physical access to hardware\ncan potentially exploit vulnerabilities to read data from the main mem-\nory (i.e., DRAM) through techniques like the \"cold-boot attack,\" which\ninvolves freezing the memory and retrieving active data. TEEs mitigate\nthis threat by encrypting the contents of the main memory using hardware-\nprotected keys that are inaccessible to software or external agents. This\nensures that even if an attacker gains physical access to the hardware, they\ncannot read the encrypted memory data. However, it is important to note\nthat TEEs typically do not encrypt GPU memory, as modern GPUs use\n3D-stacked High Bandwidth Memory (HBM) technology, which makes it\nnearly impossible to physically extract data without destroying the chip\nitself.\n\u2022 Privileged Software. Machine owners or attackers with administrative\nprivileges could attempt to compromise a system by installing malicious\nsoftware, modifying firmware, tampering with the operating system, or us-\ning root-level permissions to introduce bogus drivers. TEEs are designed\nto defend against such privileged adversaries by isolating the application\nfrom the underlying operating system and software stack, ensuring that\neven if the machine's firmware or OS is compromised, the application\nrunning within the TEE remains secure. TEEs achieve this level of pro-\ntection by relying on cryptographic assurances enforced by the hardware,\npreventing unauthorized access or tampering."}, {"title": "5.0.2 Attacks on TEE", "content": "As mentioned earlier, the security guarantees provided by Trusted Execution\nEnvironments (TEEs) are contingent on the correct implementation of the un-\nderlying hardware, as well as the supporting manufacturer software, such as\nmicrocode and firmware. However, no system is entirely immune to vulnera-\nbilities, and it is theoretically impossible to ensure that any complex system\nis completely free of bugs. Over the years, several studies have demonstrated\nsophisticated attacks capable of partially compromising the security properties\nof TEEs [5, 6].\nFortunately, the vast majority of these vulnerabilities are identified and\npatched swiftly, often before they become publicly known. Many of these at-\ntacks are discovered by ethical researchers who work with hardware vendors to\naddress the issue before it is disclosed to the public. This proactive approach\nhelps mitigate potential threats early in the vulnerability lifecycle. Addition-\nally, even in the case of a zero-day exploit-an attack that takes advantage of an\nundiscovered vulnerability-the decentralized AI network can implement con-\ntingency measures, such as temporarily disabling the privacy-preserving mode of"}, {"title": "6 Future Work", "content": "TEE for verifiability. We are investigating the possibility of using TEES for\nverifiability in Atoma network. We believe that the attestation capability of"}]}