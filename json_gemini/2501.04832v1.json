{"title": "Act PC-Geom: Towards Scalable Online Neural-Symbolic Learning via Accelerating Active Predictive Coding with Information Geometry & Diverse Cognitive Mechanisms", "authors": ["Ben Goertzel"], "abstract": "This paper introduces ActPC-Geom, a novel, speculative approach for accelerating Active Predictive Coding (ActPC) based learning in artificial neural networks through the integration of information geometry, specifically leveraging Wasserstein-metric-based methods for measure-dependent gradient flows.\nWe suggest the possibility of replacing ActPC's use of KL-divergence for predictive error assessment with the Wasserstein metric as well, and provide some formal observations suggesting that may enable more robust integrated behavior on the part of the overall network.\nWe propose several strategies aimed at making this approach computationally tractable at the large scale, including\n\u2022 the use of neural approximators for the inverse measure-dependent Laplacians required for information geometry calculations\n\u2022 the application of approximate kernel PCA embeddings to construct low-rank approximations for these neural approximators to output\n\u2022 the creation of compositional hypervector embeddings from these kPCA embeddings, to serve as complements to the KPCA vectors\n\u2022 where the compositional algebra of the hypervectors is configured to work effectively for concepts derived from a fuzzy FCA lattice automatically learned via a neural architecture performing analysis of overall network states\nThe result is an ActPC-based neural architecture that, we suggest, will be capable of real-time online learning, and will support robust integration between continuous ActPC neural networks (including transformer-like architectures) and discrete symbolic ActPC networks (including eg.", "sections": [{"title": "Introduction", "content": "Active Predictive Coding (ActPC) comprises a biologically inspired framework for learning and inference. By minimizing prediction errors between internal models and observed data, ActPC iteratively refines both latent representations and model parameters. Unlike backpropagation-based neural networks, ActPC emphasizes local error signals, making it inherently more suited for real-time, online learning and enabling integration of reinforcement learning and symbolic reasoning.\nTraditional gradient-based optimization in neural networks struggles to support large-scale real-time learning dynamics due to the brittleness of the underlying backpropagation algorithm, which requires carefully coordinated and synchronized updating across a large network (leading to a reliance on large batch-based updates), and which suffers convergence problems when neural architectures are too recurrent or otherwise too complex. These shortcomings lead to an unfortunate sociotechnical dynamic in which neural architectures oriented toward robust AGI-oriented learning, reasoning and memory are insufficiently pursued because they tend to involve network topologies for which backpropagation will not easily converge."}, {"title": "Addressing Scalability Challenges with Cognitive Solutions", "content": "Because of the fundamental locality of ActPC, it is relatively straightforward to modify it to use Wasserstein-metric based information geometry to accelerate its operations and optimize its intelligence (thus obtaining the overall approach we call here Act-Geom).\nWe argue that, alongside this addition, it may be effective to replace the use of KL divergence for the assessment of prediction error in ActPC with an appropriate deployment of the Wasserstein metric. This allows, as we show, elegant holistic properties of the system-wide dynamics.\nWhile information-geometric machinery can be computationally costly, we propose integrative-AI solutions to this, starting out with using neural approximators for the inverse of the measure dependent Laplacian (the key quantity needed for working with the Wasserstein metric)\nWe argue that, to make this sort of neural approximation scalable, one probably needs to have the neural approximator output an embedding vector embodying a compressed version of the inverse Laplacian matrix. For example, one might use an approximate kernel PCA based embedding (where the kernel comes from the Wasserstein metric itself) to construct LoRa approximations to these matrices for these neural approximators to output.\nTaking this approximation one step further leads in interesting cognitive directions. One can expand KPCA based embeddings into hypervector embed-"}, {"title": "Act PC-Geom Transformers", "content": "The application of these ActPC-Geom mechanisms to implement transformer-like architectures has a number of interesting potential advantages, including\n\u2022 having pretraining and instruction tuning in the same framework\n\u2022 combining few-shot activation-space-based online learning with ActPC-based link-weight-modification online learning, potentially leading to sophisticated deliberative thinking dynamics\n\u2022 incorporating Hopfield-net-like lateral connections in each layer to enable associative long-term memory retrieval\n\u2022 leveraging hypervectors obeying compositional symbolic algebra (learned as models of fuzzy concept lattices derived from system states), for guiding both weight-modification and in-context activation-space-based query processing"}, {"title": "Neural-Symbolic Synergies", "content": "There is a fairly clear route to coupling continuous-variable Act PC-Geom networks as described here with discrete ActPC-Chem networks as described in [Goe24a], in a way that incorporates both 1) activation spreading between the discrete and continuous networks, and 2) probabilistic modeling across both networks, to provide more accurate geometric information for use in information-geometric acceleration\nFor instance, the neural approximator used to estimate the measure-based Laplacian and its inverse may leverage the state and history of both the discrete and continuous networks as input data in order to make estimations regarding both networks. This can be achieved for instance by doing common kPCA and compositional hypervector embedding across the continuous and discrete networks, and guiding these with a fuzzy-FCA-style concept lattice learned by joint neural analysis of both networks."}, {"title": "Potential for Aggressive Optimization", "content": "Efficient parallel and distributed implementation should be feasible, leveraging the local nature of ActPC learning which should enable multiple processors and/or cores to concurrently process parts of the same neural network in a relatively independent way, without need for extremely precise coordination or synchronization. We elaborate how the mathematics of Galois connections can be used to explicate certain routes to this sort of optimization, particularly regarding efficient concurrency across multiple cores.\nWe argue this sort of optimization is highly critical, not only to compete effectively with backpropagation-based neural architectures, but also because highly rapid online weight updating will be needed to support real-time feedback between activation-space learning and weight-updating learning. As a crude estimate, one might aim for say a 10-1 ratio of execution speed between weight updating and activation spreading in the focus of attention of a network, to obtain the needed emergent cognitive dynamics.\nAt the end of the paper, we give a rough outline of a possible specialized custom HPC architecture intended to support this sort of optimization, which serves also as a high-level review of the various components involved in the overall system proposed and how they relate to each other."}, {"title": "Background", "content": "Active Predictive Coding (ActPC) [OK22] is a novel reinforcement learning (RL) paradigm grounded in predictive coding principles and, as one compelling option, implemented through Neural Generative Coding (NGC) circuits. It is inspired by the general notion of Predictive Coding, a biologically inspired theoretical framework founded on the idea that the brain continuously predicts sensory inputs and updates internal representations by reducing prediction errors. The overall Predictive Coding framework owes a great deal to the conceptual"}, {"title": "ActPC: Key Conceptual Aspects", "content": "ActPC leverages the predictive-coding concept to guide action selection and learning without relying on backpropagation-based gradient computations. Instead, it uses local, Hebbian-like learning rules to update synaptic weights, making the approach both biologically plausible and computationally robust (yet far more efficient than simplistic Hebbian learning heuristics).\nKey technical aspects of the ActPC approach include:\n1. Sparse Rewards: Many RL tasks, for example in robotics, provide rewards infrequently. Conventional backpropagation-based RL methods struggle here due to unstable gradient signals. ActPC mitigates this by incorporating both exploratory (epistemic) and goal-oriented (instrumental) signals, enabling effective learning in environments where explicit rewards are rare.\n2. Gradient-Free Optimization: By entirely sidestepping backpropagation, ActPC is not affected by issues like vanishing gradients and differentiability constraints. This is especially useful when employing complex, biologically realistic neuron models or when scaling to large networks."}, {"title": "ActPC: Basic Equations", "content": "We'll now give a slightly more in-depth summary of Active Predictive Coding (ActPC) in a formal-neural-network context. The goal is to illuminate (1) how states and weights update iteratively, (2) how local error signals drive these updates, and (3) how ActPC can incorporate reward signals and be amenable to advanced (e.g. info-geometry) techniques. Note that various notations and variants exist in literature; the presentation here is a generic formulation combining aspects from multiple publications.\nConsider a multi-layer (or multi-module) neural system with layers l=1,...,L. For each layer l :\n\u2022 Let $z^l$ be the latent activations (or states) of that layer.\n\u2022 Let $ \\hat{z}^l$ denote the predicted version of $z^l$, typically computed from layer l+1 or from some generative model for layer l.\n\u2022 Let $W^l$ be the weights (or parameters) that help form these predictions.\nPredictive coding posits that each layer tries to minimize a local mismatch (the \"prediction error\u201d) between $z^l$ and $ \\hat{z}^l$. It generally involves doing so iteratively rather than a single forward/back pass."}, {"title": "Overall Objective", "content": "We define a local or global prediction error measure. A simple version might be:\n$L_{pred} = \\sum_{l=1}^L ||z^l - \\hat{z}^l||^2$\nthough advanced setups may use more sophisticated geometric measures, or partial sums of errors, etc. If a reward (or utility) is also considered, we can define:\n$L_{total} = L_{pred} - \\alpha R$\nwhere R is a reward measure (or negative cost), and $\\alpha$ is a weighting. Alternatively, as we'll suggest in Section 5 below, one might incorporate a Wasserstein distance in $L_{pred}$ ."}, {"title": "Iterative Updates in ActPC", "content": "ActPC is often described as involving two primary updates: one for states $z^l$, and one for weights $W^l$\nFor latent state updates, for each layer l, we define an iterative rule:\n$z^l \\leftarrow z^l - \\eta_{z} \\nabla_{z^l} L_{total}$ ,\nwhere $\\nabla_{z^l} L_{total}$ is the partial derivative (local mismatch plus any reward-driven gradient) w.r.t. $z^l$.\nIn a simple \"prediction error only\" case, for instance:"}, {"title": "Riemannian viewpoint and \"Otto calculus\"", "content": "When one regards the space of probability densities $P(X)$ (with suitable regularity conditions) as an infinite-dimensional manifold, the Wasserstein metric endows it with a formal Riemannian structure. In broad strokes:\n\u2022 A \"tangent vector\" at a density p can be identified with a scalar function $ \\phi$ such that the infinitesimal change from p is $dp = - \\nabla \\cdot (p \\nabla \\phi)$.\n\u2022 The corresponding \"Riemannian metric\" on this tangent space is typically written as\n$\\langle \\phi, \\psi \\rangle_p = \\int_X p(x) \\nabla \\phi(x) \\cdot \\nabla \\psi(x) dx$\nFrom this perspective, geodesic equations in this Wasserstein geometry coincide with the PDE for optimal transport (the so-called \"continuity equation\" with a velocity field that is a gradient)."}, {"title": "Measure-dependent Laplacian", "content": "Continuous viewpoint In the continuous, PDE-based formulation, one often sees operators like the \"weighted\" or \"measure-dependent\" Laplacian:\n$\\Delta_p \\phi = \\nabla \\cdot (p \\nabla \\phi)$\nwhich arises when linearizing the PDE flows in this Wasserstein geometry. Concretely, $\\Delta_p$ depends on p because the velocity field that deforms p is \"weighted\u201d by p itself (reflecting mass transport). This is one way the Wasserstein metric \"knows\u201d about the underlying measure it is transporting.\nDiscrete viewpoint In a discrete setting say one approximates X by a graph with nodes i, j the measure-dependent Laplacian can be written in a form such as\n$L(p)_{ij} = w_{ij} (p_i + p_j), i \\neq j$\nwhere $w_{ij}$ encodes a notion of adjacency or cost for transport between nodes i and j. The diagonal terms of $L(p)$ are typically chosen so that row sums vanish (mimicking the continuous Laplacian property $\\sum_j L_{ij} = 0 $). The factor $(p_i + p_j)$ or variants thus ensures that the graph Laplacian \u201cadapts\u201d to the mass distribution p.\nWhy the inverse of L(p) is critical When one discretizes PDE-based formulations or tries to solve optimization problems (e.g., finding geodesics, implementing gradient flows, or computing barycenters in Wasserstein space), one encounters linear systems involving L(p) or its variations. In particular, if one wants to linearize or invert transformations in the tangent space (for example, to perform Newton-type methods for geodesic shooting), one needs $(L(p))^{\\dagger}$ (where $ \\dagger $ denotes pseudoinversion) or else efficient method to solve linear systems with L(p).\nBecause these matrices can be very large (and depend on p), developing fast linear-algebra techniques is central to making Wasserstein-based methods practical in large-scale applications. This basically demands specialized algorithms that exploit the structure of $(L(p))^{\\dagger}$.\nThe inverse measure-dependent Laplacian in ActPC-Geom The core idea of ActPC-Geom (to be elaborated just below) is that, by embedding Wasserstein geometry into ActPC, neural link weight updates become distribution-aware, aligning local corrections with the natural geometry of the underlying probability space and thus accelerating learning. Leveraging this in practice requires geodesic shooting and related methods (to allow the ActPC dynamics to follow the geodesics in the Wasserstein space), which leads to a need to efficiently estimate the inverse of the (potentially very large) measure-dependent Laplacian matrix, which leads to some mathematical intricacies and in the route that we suggest here some cognitive subtleties as well."}, {"title": "ActPC-Geom: Upgrading ActPC with Wasserstein Geometry", "content": "A central challenge in practical implementations of continuous-variable Active Predictive Coding (ActPC) is achieving stable, efficient convergence for larger or more complex networks. While simply scaling model size or data can help, a complementary approach is to introduce adaptive optimization into the underlying dynamics - e.g. by using Wasserstein-based natural gradient steps to guide the continuous update dynamics. The idea is to align gradient directions with the intrinsic geometry of the probability distributions involved, potentially improving convergence speed and stability.\nThe emphasis in ActPC on local iterative updates naturally aligns it with the incremental, geometry-aware steps afforded by Wasserstein-based optimization. Additional conceptual synergies between ActPC and information geometry include:\n\u2022 Activation Space Matching: Predictive coding focuses on reducing local distribution mismatches, a task that directly benefits from the transport-based perspective of Wasserstein geometry.\n\u2022 Real-Time Feasibility: The smaller, iterative updates in ActPC fit well with the computational demands of measure-dependent operators, as opposed to the large batch-based gradients of backpropagation."}, {"title": "Setup", "content": "We will assume a learning scenario where an agent tries to minimize an error measure or maximize a reward through local, gradient-free updates, as in standard ActPC. However, we will leverage ideas from optimal transport (Wasserstein geometry) to shape the gradient steps in the underlying probability space of parameters or distributions.\nLet $\\xi \\in R^m$ represent the agent's current parameters- assuming a neural net based agent this might include neural states, weights, or a mixture of both. (For an ActPC-Chem based system, it would comprise e.g. the rewrite rules in the system and any numerical parameters associated with them.) These parameters induce a probability distribution\n$p( \\cdot | \\xi)$\nwhich captures how the agent's model (or policy) generates predictions about the environment or next states. So, the distribution goes over future states, conditioned on present state, and captures the predictions about future states implicit in present state."}, {"title": "Error Functional", "content": "In RL-style tasks, one would typically define an error functional such as:\n$F(\\xi) = -r(\\xi) or F(\\xi) = L(\\xi)$\nwhere r(\u00b7) is a reward function (the agent tries to maximize reward by minimizing -r), and L(\u03be) might be an information-theoretic or predictive-coding loss (e.g., KL divergence or Wasserstein distance between predicted and observed distributions).\nIn Section 5 we will make an argument for having ActPC-Geom systems measure the overall prediction error of their outputs using Wasserstein distance,\n$L(\\xi) = W_2 (q(x), p(x | \\xi) )$\nfor greater consilience with the internal use of Wasserstein distance to accelerate learning. The considerations in this section, though, are applicable whether one uses Wasserstein or KL in the \"outer loop\" for measuring overall systemic prediction error. One could also, say, average together a prediction-error measure with a traditional external reward signal."}, {"title": "Wasserstein Geometry", "content": "Let p(x) be a distribution over states (or outputs) x \u2208 X.\nAs reviewed in [LM18], a measure-dependent Laplacian\n$L(p)_{ij} = w_{ij} (p_i + p_j), i \\neq j$\ni.e.\n$L(p) (x, y) = w(x, y) (p(x) + p(y))$\ncan be defined on a graph (or continuous space) capturing the ground metric w(x, y) for transporting mass from x to y."}, {"title": "The Ground Metric", "content": "Where does the ground metric come from? A simple syntactic distance between two states would be one option - e.g. an edit distance. Conceptually, one could even use a logical inference system like PLN to estimate the similarity between states. In practice, though, one needs the ground metric to be rapidly evaluable.\nA sensible idea would be to train a separate neural predictor to estimate the semantic (behavioral) distance between two neural states.\nTo make this estimator passably efficient, one could create an appropriate mapping of the neural states into embedding vectors, and have this predictor act on these vectors. As a simple example, the embedding could use an efficient approximation to kernel PCA with a kernel based on a crude syntactic distance, along the lines discussed in Section 4.2.4 below. Or one could use hypervectors manifesting a compositional algebra, harking forward to Section 6."}, {"title": "The Metric Tensor", "content": "Given a ground metric, one then obtains a metric tensor:\n$G(\\xi) = J_{\\xi}^T L(p(\\xi))^{\\dagger} J_{\\xi}$\nwhere $J_{\\xi}$ is the Jacobian mapping from parameters \u03be to the distribution $p( \\cdot | \\xi )$, and $ \\dagger$ denotes (pseudo) inversion."}, {"title": "Wasserstein Natural Gradient Update", "content": "The agent seeks to minimize F(\u03be). In a Wasserstein geometry, the update in continuous time is\n$\\frac{d \\xi}{dt} = -G(\\xi)^{-1} \\nabla_{\\xi} F (p(\\xi))$\nwhere $\\nabla_{\\xi} F$ is the standard gradient in parameter space (assuming we can differentiate F with respect to \u03be).\nIn practice, we use small discrete steps:\n$\\xi_{k+1} = \\xi_k - \\eta G (\\xi_k)^{-1} \\nabla_{\\xi} F(p(\\xi_k)),$\nwhere \u03b7 is a step size or learning rate. This update rule can be seen as a generalization of standard gradient descent that respects the underlying geometry of the probability space."}, {"title": "Predictive Coding Error", "content": "Now where does the PC come into it?\nIf z are neural states (representations), we might measure mismatch between predicted $ \\hat{z} $ and actual z with a cost such as\n$L_{pc} (\\xi) = \\sum_i ||z^l - \\hat{z}^l (\\xi) ||^2$\n(or a KL divergence or Wasserstein form).\nIf a reward function r(\u03be) is given, one can define\n$F(\\xi) = -r(\\xi) + \\alpha L_{PC} (\\xi),$\nblending RL and ActPC error."}, {"title": "Summary of Core Equations", "content": "Collating the above, we have the process flow:\n1. Compute local gradient:\n$g_k \\leftarrow \\nabla_{\\xi} F (p (\\xi_k))$\n2. Construct measure-dependent Laplacian:\n$L (p (\\xi_k) | \\omega )$\n3. Form metric tensor:\n$G(\\xi_k) = J_{X_k}^T L (p (\\xi_k)) J_{\\xi_k}$\n4. Update step:\n$\\xi_{k+1} = \\xi_k - \\eta G (\\xi_k)^{-1} g_k$"}, {"title": "Crude Pseudocode Sketch", "content": "Corresponding crude pseudocode might look like:\nInitialize parameters xi (e.g., random) // xi in R^m\nInitialize step size eta\nInitialize environment / data\nInitialize ground metric omega, define measure-dependent Laplacian L()\nfunction ACTPC_WASSERSTEIN_UPDATE(xi, environment):\n// 1. Evaluate cost and gradient\ncost_value = F(xi, environment) // e.g. combined RL + PC error\ngradient_g = compute_gradient(F, xi, environment)\n// partial derivatives wrt xi\n// 2. Construct measure-dependent Laplacian\ndist_p = compute_distribution(xi) // e.g. p(. | xi)\nL_p = build_measure_dependent_laplacian (dist_p, omega)\n// graph-based or continuous space\n// 3. Compute metric tensor G(xi)\nJ_xi = compute_jacobian_of_distribution(xi)\n// partial p(xi) wrt xi\nL_p_inv = pseudo_inverse(L_p)\n// or approximate inversion"}, {"title": "Conclusion", "content": "The details are slightly intricate - and will become significantly more so in following sections of this paper - but the core idea here is simple: By incorporating G(\u03be) into each step, we move \u03be in ways that respect the underlying geometry of the distribution. This can mitigate chaotic or inefficient updates that sometimes plague naive gradient steps, especially when data is limited or the environment's distribution is high-dimensional or complex.\nExact computation of measure-dependent Laplacians or their inverses can be expensive. Hence, approximate or stochastic versions may be necessary in real-world or large-scale problems. However, even partial use of Wasserstein geometry-for instance, using simpler or block-diagonal approximations-can stabilize and accelerate learning compared to purely local Euclidean steps.\nOverall, an \"ActPC on Wasserstein\" approach to RL unifies standard active predictive coding (ActPC) with geometry-aware gradient flows, potentially enabling faster, more stable convergence in complex or data-scarce regimes - an appealing direction for future theoretical and experimental research."}, {"title": "Neural Approximators for Measure-Dependent Operators: Making ActPC-Geom Feasible", "content": "While the use of information geometry to accelerate ActPC is conceptually appealing, the exact computation of $G(W)^{-1}$ or $\\hat{L}$ for large networks is computationally prohibitive.\nWe push forward to explore how to work around this issue and make \"ActPC on information geometry\" efficient and scalable in practice, via introducing appropriate approximations. This turns out to be a deep dive, beginning with fairly traditional mathematical approximations and leading us toward more cognitive approaches.\nOur first step is to introduce a neural approximator approach: Train a neural network $f_{\\theta}(f)$ to output low-rank approximations of $\\hat{L}^{\\dagger}$, given features f extracted from the network's state distribution.\nNext arises the question of how to train this neural approximator. As a matter of temporary scaffolding one could use a traditional backpropagation methodology, or one could use other methods such as evolutionary learning. What would be most natural here, though, would be to leverage predictive coding based learning here as well. One could go full-on recursive and use information geometry to help guide the predictive coding methods used to train the approximator network. At some level of recursion depth, though, one has to set the information geometry aside and use a simpler ML approach as a \"base case.\"\nThe approximator network can be trained (by whatever method) to minimize some reconstruction error, e.g.,"}, {"title": "Stochastic Low-Rank Decomposition of the Inverse Measure-Dependent Laplacian", "content": "One approach to making this sort of approximator work at scale would be to factor the matrix $(\\hat{L}(p))^{\\dagger}$ via low-rank decomposition or sketching methods, such as:\n\u2022 Random Projection:\nProject the Laplacian L onto a small set of random vectors, building a compressed representation:\nSolve the smaller system in this reduced dimension.\nor (probably more promising for large-scale cases)\n\u2022 Nystrom Method or Similar SVD Approximations:\nSample a subset of states or representative points $ \\{ x_i \\}$.\nConstruct an approximate factorization of $(\\hat{L}(p(\\xi)))^{\\dagger}$.\nO ($r^3$), for rank r < n.\nGiven such a dimensionally reduced version, we can train a neural network $f_{\\theta}()$ that, given the current distribution parameters (or a representative sample set) and the ground metric, outputs an approximation the inverse Laplacian $(\\hat{L}(p(\\xi)))^{\\dagger}$.\nTo make this work tractably in practice, one might do partial \"ground-truth\u201d calculations for $ \\hat{L}^{\\dagger}$ on a small batch of states, then train $f_{\\theta}$ to generalize to the broader parameter space.\nI.e., at each time step, the system:\n1. Extracts relevant features describing $p ( \\cdot | \\xi_t )$. (e.g., statistics or sampled states.)\n2. Passes them to the neural approximator network $f_{\\theta}$, which approximates a compressed representation of $\\hat{L}$ or $ \\hat{L}^{\\dagger}$\n3. Uses this approximated compressed $\\hat{L}$ or $ \\hat{L}^{\\dagger}$ to update parameters $ \\xi$ via\n$\\xi_{t+1} = \\xi_t - \\eta \\hat{G} (\\xi_t)^{-1} \\nabla_{\\xi} F$\nwhere:"}, {"title": "Direct $l^2$ Projection of Factor Matrices", "content": "A simple approach to creating a compressed representation would be:\n1. Compute $(U_k, \\Sigma_k, V_k)$ from partial SVD or rank-r factorization.\n2. Flatten or vectorize: $veck \\in R^{(n \\cdot r)} or R^{(2n-r)}$ if you include $U_k$ and $V_k$ separately.\n3. Project veck into a lower dimension d via a linear map or a small MLP.\nE.g.,\n$z_k \\leftarrow W_{proj} veck \\in R^d$\nOne could also enforce orthogonality in U and V before flattening, then the net deals with \u201cstable\u201d columns.\nThis simple approach gives dramatic dimension reduction, but doesn't necessarily incorporate the geometry of $ \\omega $ or the distributions themselves, beyond being \u201cwhatever the partial SVD gave.\u201d One might say it's purely mathematical rather than semantic or cognitive."}, {"title": "Learned Autoencoder for Factor Triples", "content": "A slightly subtler approach would be: Instead of using a linear projection, treat $(U_k, \\Sigma_k, V_k)$ as a \u201csignal\u201d and train a small autoencoder, along the lines:\n\u2022 Encoder: $Enc_{\\theta} (veck) \\rightarrow z_k \\in R^d$.\n\u2022 Decoder: $Dec_{\\theta} (z_k) \\rightarrow veck \\approx (U_k, \\Sigma_k, V_k)$.\nA separate reconstruction loss ensures\n$|| veck - \\hat{veck} ||^2$\nstays low.\nThis potentially captures nonlinear relationships among factor entries, which is a major plus, likely more than compensating for the complication of having more moving parts to train. The net itself and the autoencoder might require careful regularization to ensure stable re-inversion, e.g., orthogonality constraints or conditioning on \u03a3."}, {"title": "Kernel Methods", "content": "Kernel PCA is often used to embed data into a lower-dimensional manifold via a chosen kernel $ \\kappa (a, b)$. In this context:\n\u2022 Data: The objects are factor matrices $(U_k, \\Sigma_k, V_k)$.\n\u2022 Kernel:"}, {"title": "Approximate Kernel PCA", "content": "Several methods are available to approximate the principal subspace of K using fewer computations and memory. Just as with exact kPCA, once we have the approximate principal components, we can map each data point (factor matrix) to a low-dimensional embedding $z_k$ that captures nonlinear relationships.\nNystrom Method The Nystrom method approximates an NxN kernel matrix K using a smaller subset of columns/rows:\n1. Select a Subset of size m < N, say S = {$s_1,..., s_m$}.\n2. Partition K as"}, {"title": "Overall Flow", "content": "Leveraging any of these kPCA approximation schemes within an information-geometry approach to ActPC, the overall process then becomes:\n1. Collect Factor Matrices: From partial SVD or rank-r decompositions of the measure-dependent Laplacian at different episodes/time steps.\n2. Approximate Kernel PCA:\n\u2022 Nystrom: If we want a fairly global perspective, choose a subset S of size m; compute Ks,s fully, then approximate the rest of K.\n\u2022 Random Features: If the kernel is RBF-like on the flattened factor vectors, build a random feature map . Then do a simpler linear dimension reduction on (veck).\n\u2022 Incremental: As new factor matrices appear, keep updating the partial decomposition or random feature representation."}, {"title": "Predicting the Embedding Vector", "content": "We return now to the main flow of leveraging compressed representations for making the neural approximator of the inverse measure-dependent Laplacian scalable. Regardless of how we define the embedding zk via linear projection, autoencoder, kernel PCA or something fancier \u2013 the final step is to train a function:\n$z_k = g(v_{eck})$\nand then a neural net $f_{\\theta}$ that, given the distribution features $f_k$, predicts\n$z_k = f_{\\theta} (f_k)$\nAt inference time:"}, {"title": "Overall Methodology for Leveraging Compressed Representation to Guide Neural Approximators", "content": "Summing up, the methodology we have presented is as follows:\nOffline / Periodic Phase\n\u2022 Gather {pk} from sampling policy or from offline demonstrations.\n\u2022 For each pk, do partial factorization of L (pk).\n\u2022 Convert to some embedding zk (via linear map, autoencoder, or kernel PCA).\n\u2022 Train $f_{\\theta}: f_k \\rightarrow z_k$. Train an associated decoder if needed.\nOnline Real-time Phase\n\u2022 At each step, compute distribution features ft.\n\u2022 Evaluate $\\hat{z_t} = f_{\\theta} (f_t)$.\n\u2022 Decode or re-expand (U, \u03a3, V).\n\u2022 Form or invert $\\hat{L}^{\\dagger}$, use in Wasserstein-based ActPC update.\n\u2022 Periodically refine training with newly observed partial factorizations or new environment conditions."}, {"title": "Replacing KL with Wasserstein for ActPC Prediction Error Assessment", "content": "ActPC is traditionally formulated using KL divergence to measure the prediction error of a neural network system relative to its targets. This is a sensible enough choice, and in Friston's framework it emerges naturally as a reorganization of the \"free energy principle.\"\nFrom an Al view, however, there may be significant advantages to replacing KL divergence with Wasserstein distance in this role. These advantages are especially clear in an ActPC-Geom context, because one gets benefits of synergy with the use of Wasserstein distance in the information geometry being leveraged to accelerate and focus internal ActPC learning dynamics. However, once the possibility is opened up, it appears there are some advantages to such a switch even aside from the information-geometric context.\nWe consider in this section how one might replace KL divergence with a Wasserstein-based divergence measure in an Active Predictive Coding (ActPC) framework, and how that substitution could bring the \"reward-driven information divergence cost\" more closely into line with the system's internal local transport geometry. We focus on the why and how of using a Wasserstein-based cost function instead of KL, and on the potential benefits in aligning external reward signals with internal geometry-based updates.\nBefore digging into this, however, we briefly consider how one might conceptually justify this shift in terms of the broader principles underlying ActPC."}, {"title": "Optimal Transport as a Cognitively Relevant Least Action Principle", "content": "In Karl Friston's framework, which is one of the core motivations for the ActPC approach to neural learning, \"minimizing free energy\" is taken as a core principle and is shown to be equivalent (under certain assumptions) to minimizing a KL divergence between the organism's \"recognition density\" and the environment's \"true\" density. This underpins a sort of \"thermodynamic\" or \"variational\" interpretation of the system's self-organization."}, {"title": "Closer Connection Between External Cost & Internal Geometry", "content": "If the external cost is literally the Wasserstein distance between the agent's predicted reward distribution and the environment's actual reward distribution, then the agent's global objective (minimizing that distance) is more directly aligned with how the measure-dependent Laplacian $ \\hat{L}(p) $ shapes local parameter updates. The system's movements in parameter space can be seen as physically \"moving mass\" from the agent's predicted distribution to the environment's distribution, matching the conceptual story from both the agent's viewpoint (local updates) and environment viewpoint (reward mismatch). One obtains a more coherent alignment between \"how we measure mismatch with the environment's distribution\" and \"how we transport the agent's distribution internally.\"\nI.e.: In effect, \"outer loop\" and \"inner loop\" are describing the same transport geometry in distribution space. Because the local gradient directions from the external cost and the local geometry are not in conflict, the agent's parameter updates can proceed in a more \"consistent\u201d direction - potentially yielding smoother and more direct convergence to solutions that actually match the environment's reward distribution. For instance, if both external cost and internal geometry share the same notion of \"transport,\u201d the system might avoid contradictory gradient directions, making learning more stable or convergent.\nFurther synergy may be provided via use of a common ground metric for the Wasserstein metrics used for system-level error assessment and for internal information-geometric guidance. This should make sense given that the natural distance/cost structure on the environment should be similar to the natural distance/cost structure on the portions of a cognitive system concerned with directly modeling the environment.\nIn the next sections we articulate some formal propositions illustrating aspects of the synergy obtained by using the same measure in the inner and outer loop. These relatively simple propositions don't fully capture the cognitive or mathematical aspects of this synergy, there are merely some of the simpler aspects to articulate - dipping a foot tentatively in the deep waters."}, {"title": "Transfer of Continuity and Scale Properties from Outer to Inner Loop", "content": "We explore in this section a specific sense in which using Wasserstein for \"both external reward divergence and internal geometry\" can systematically reduce distortions between the"}]}