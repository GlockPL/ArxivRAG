{"title": "THE SCENE LANGUAGE: REPRESENTING SCENES\nWITH PROGRAMS, WORDS, AND EMBEDDINGS", "authors": ["Yunzhi Zhang", "Zizhang Li", "Matt Zhou", "Shangzhe Wu", "Jiajun Wu"], "abstract": "We introduce the Scene Language, a visual scene representation that concisely\nand precisely describes the structure, semantics, and identity of visual scenes. It\nrepresents a scene with three key components: a program that specifies the hier-\narchical and relational structure of entities in the scene, words in natural language\nthat summarize the semantic class of each entity, and embeddings that capture\nthe visual identity of each entity. This representation can be inferred from pre-\ntrained language models via a training-free inference technique, given text or im-\nage inputs. The resulting scene can be rendered into images using traditional,\nneural, or hybrid graphics renderers. Together, this forms a robust, automated\nsystem for high-quality 3D and 4D scene generation. Compared with existing\nrepresentations like scene graphs, our proposed Scene Language generates com-\nplex scenes with higher fidelity, while explicitly modeling the scene structures to\nenable precise control and editing. Project page: https://ai.stanford.\nedu/~yzzhang/projects/scene-language/", "sections": [{"title": "INTRODUCTION", "content": "How do you describe a scene? Imagine that you just traveled to Easter Island and would like to\nexplain to Alice the wondrous scene of Ahu Akivi: \u201cThere are seven moai in a row, facing the same\ndirection.\" \"What is a moai?\" Alice asked. \u201cA moai is a stone human figure without legs, but each\nof them also looks slightly different.\" At this point, you realize it seems difficult to precisely explain\nthe scene using natural language alone.\nIn fact, this example highlights a complete scene representation requires at least three types of com-\nplementary information: structural knowledge, which is about the joint distribution of multiple in-\nstances, like \u201cseven moai in a row, facing the same direction,\u201d most naturally described as programs;\ncategory-level semantics, which may be shared across instances, often described in words, such\nas \"moai\u201d; instance-level intrinsics, tied to the identity of each specific object or part, such as its\ngeometry, color, and texture, which is hard to describe but easy to recognize.\nModern AI techniques provide natural grounding for each of the three modalities, while also falling\nshort of capturing all: in-context learning of pre-trained language models (LMs) enables the in-\nference of domain-specific programs (Brown et al., 2020); LMs capture rich semantic information\nbased on words in natural language; embeddings obtained via techniques like textual inversion (Gal\net al., 2023) or low-rank adaptation (Hu et al., 2022) best capture object identity. However, none of\nthese existing representations alone is sufficient for scene generation and editing.\nWe introduce the Scene Language, a representation that integrates the three modalities\u2014programs,\nwords, and embeddings\u2014to precisely and concisely describe the structure, semantics, and identity\nof visual scenes. In the Scene Language, a program specifies a computation process that defines\nthe organization of a collection of entities in the scene, including extrinsics like poses and structural\nregularity like repetitions. Each entity is associated with a word referring to its semantic group, as\nwell as an embedding describing its instance-specific attributes.\nIn addition to the representation itself, we propose a training-free inference module using a pre-\ntrained LM as a backbone to infer the Scene Language from texts and images. When provided with\na domain-specific language (DSL) for scenes, LMs decompose the task of complex scene genera-\ntion into simpler tasks of scene component generation by predicting their corresponding modular\nfunctions."}, {"title": "2 RELATED WORK", "content": "Visual scene representations are arguably the most fundamental problem in computer vision; thus,\nfor sure, we may not enumerate all related work. As our Scene Language comprises programs,\nwords, and embeddings, we will organize our discussion accordingly into three categories: scene\nrepresentations that use program-based representations (Section 2.1), semantic graph-based repre-\nsentations (Section 2.2), and a pre-trained generative model's latent space (Section 2.3)."}, {"title": "2.1 REPRESENTING SCENES AS PROGRAMS", "content": "Programs can specify not only the relations among scene components mentioned in Section 2.2, but\nalso structural patterns such as hierarchy and repetitions, making them suitable as explicit descrip-\ntions of scene structures. Prior works have proposed to use programs in the form of sequences of\nexecution commands as object-centric representations, followed by neural executors that render the\nprograms into 3D shapes (Tian et al., 2019; Sharma et al., 2018; Deng et al., 2022). In comparison,\nShapeAssembly (Jones et al., 2020) introduces higher-level functions with semantically meaningful\nfunction names, e.g., \u201cchair\u201d and \u201cback\u201d, to its program representation. Both ShapeAssembly and\nours adopt the design principle of function abstraction, which results in clearly stated hierarchy re-\nlation among components and better program editability. However, ShapeAssembly uses cuboids\nas the shape representation and does not model appearance, while ours allows for more precise\ngeometry and appearance modeling using expressive neural embeddings.\nAll the representations mentioned above require 3D datasets for training. More recently, with the\nadvance of language models (LMs), several methods (Zhou et al., 2024b; Hu et al., 2024; Yamada\net al., 2024; Sun et al., 2023; Zhang et al., 2023a; Tam et al., 2024) have proposed to use zero-shot\nLM inference for generating programs that will be rendered into scenes. These methods operate on"}, {"title": "2.2\nREPRESENTING SCENES WITH SEMANTIC GRAPHS", "content": "Prior semantic scene representations often adopt a graph to encode semantic scene components, such\nas objects and parts. In particular, Yuille & Kersten (2006); Huang et al. (2018) propose to employ\na parse graph of context-free grammar, using terminal nodes to correspond to objects and their\nattributes, to represent a scene. Both works employ an analysis-by-synthesis approach to infer the\nrepresentation from images that heavily rely on domain-specific priors. Alternative representations\ninclude scene graph (Johnson et al., 2015; 2018; Gao et al., 2024), where each node in a graph\ncorresponds to an object and an edge corresponds to a pairwise relation, and StructureNet (Mo\net al., 2019), which focuses on an object-centric setting and uses nodes for object parts. While\nthese representations preserve the high-level semantics of scenes or objects, they leave out low-level\nprecision; thus, geometric, textural, or relational details that cannot be fully specified by language\nor hand-crafted rules are often ignored. We address this issue via the inclusion of embeddings."}, {"title": "2.3\nREPRESENTING SCENES WITH GENERATIVE MODEL LATENTS", "content": "The latent space of visual generative models can serve as a representation space for visual scenes.\nSuch latent space can effectively capture the exact visual content of scenes, including geometry and\nappearance details, and can be either directly inferred, e.g., in variational inference (Kingma, 2014)\nand model inversion (Zhu et al., 2016). More recently, text-to-image diffusion models have shown\nremarkable results in image synthesis. This class of models offers several candidate representation\nspaces including the space of textual embeddings (Gal et al., 2023), low-rank network weights (Hu\net al., 2022), full model weights (Ruiz et al., 2023), or noise vectors in the diffusion process (Song\net al., 2021; Mokady et al., 2023; Ho et al., 2020). However, such representations typically do not\noffer interpretable semantics or explicitly encode hierarchical scene structures unlike ours."}, {"title": "3 THE SCENE LANGUAGE", "content": "We aim to design a visual scene representation that encodes the structure, semantics, and visual\ncontent of scenes. Towards this goal, we propose the Scene Language, which represents a scene\nwith three components: a program that encodes scene structure by specifying the existence and\nrelations of scene components, which we will refer to as entities; words in natural language that\ndenote the semantic group of each entity in the scene; and neural embeddings that pertain the low-\nlevel visual details and identities of the entities by permitting an expressive input parameter space.\nIn the following, we will first give a formal definition of the representation (Section 3.1), and then\nintroduce a domain-specific language (DSL) (Section 3.2) as its realization."}, {"title": "3.1 FORMAL DEFINITION", "content": "The Scene Language for a scene $s$, denoted as $\\Phi(s)$, is formally defined as follows:\n$$\n\\Phi(s) := (W, P, Z) .\n$$"}, {"title": "Entity Function Definitions.", "content": "An entity function $f_w$ outputs an entity $h$ of semantic class $w$. The\nfunction $f_w$ takes two inputs: an embedding $z$ that specifies $h$'s identity, as well as an ordered set\n$\\gamma$ containing embeddings of all descendent entities of $h$. Denote $\\{h^{(i)}\\}_{i=1}^{N}$ as the $N$ sub-entities of\n$h$, where $N$ is a constant determined by $f_w$. Then we have $\\gamma = \\{z^{(i)}, \\gamma^{(i)}\\}_{i=1}^{N}$, where $z^{(i)}$ and $\\gamma^{(i)}$\nare the embeddings of $h^{(i)}$ and of the descendent entities of $h^{(i)}$, respectively. Thus, $f_w(z, \\gamma)$ is\nrecursively defined as\n$$\nh = f_w(z, \\gamma) := \\Psi_{\\text{union}} \\left(\\left\\{\\Psi_{\\text{transform}} \\left(h^{(i)}, t^{(i)}(z)\\right)\\right\\}_{i=1}^{N}\\right);\n$$\nwhere $h^{(i)} = f_{w^{(i)}} (z^{(i)}, \\gamma^{(i)})$ is a sub-entity of $h$ and $t^{(i)}(z)$ specifies its pose.\nHere, $f_{w}$ specifies the computation procedure to obtain output $h$ from $N$ sub-entities $\\{h^{(i)}\\}_{i=1}^{N}$ via\ntwo operations: $\\Psi_{\\text{transform}}$ applies an input-dependent pose $t^{(i)}(z)$ to a sub-entity $h^{(i)}$, transforming\nit from its canonical frame to the world frame of $h$, and $\\Psi_{\\text{union}}$ composes multiple sub-entities into\none single entity. Each sub-entity $h^{(i)}$ is computed by recursively applying an entity function $f_{w^{(i)}}$\nalso defined using Eq. (2). For instance, let $f_w$ denote the entity function that produces the board\nin Fig. 2 (namely, $w = \\text{board}$). This function $f_w$ composes 64 sub-entities $\\{h^{(i)}\\}_{i=1}^{64}$ of the same\nclass square, which are in turn obtained by executing the same entity function $f_{w^{(i)}} = f_{\\text{square}}$ with\ndifferent embeddings $z^{(i)}$ and $\\gamma^{(i)}$."}, {"title": "Program Execution.", "content": "To obtain a scene $s$ from the Scene Language $\\Phi(s) = (W, P, Z)$, a program\nexecutor identifies a root entity function $f_{w_1}$ from $P$ that is not dependent by any other function\n(e.g., $w_1 = \\text{chessboard}$ from Fig. 2), and evaluates this root function using the first embedding\n$z_1 \\in Z$ and the rest as its descendants, $\\gamma_1 := [z_2,\\cdots, z_J] \\subset Z$, to obtain $s = f_{w_1} (z_1, \\gamma_1)$. Evalu-\nating $f_{w_1} (z_1, \\gamma_1)$ expands the computation recursively to its children functions $h_j = f_{w_j} (z_j, \\gamma_{V_j})$\nas defined in Eq. (2), obtaining a full sequence of all the entities $h_j$ of the scene, where $j = \\newline 1,2,..., J, z_j \\in Z,w_j \\in W$. The order in $Z$ corresponds to the depth-first-search of the com-\nputation graph starting from $z_1$. An example computation graph is visualized on the right of Fig. 2."}, {"title": "3.2 THE SCENE LANGUAGE AS A PROGRAMMING LANGUAGE", "content": "We now concretize the definition in Section 3.1 with a domain-specific language (DSL) specified\nin Table 2. To define entity functions in the DSL, we introduce macro operations union for\n$\\Psi_{\\text{union}}$, union-loop which calls union on entities evaluated in a for-loop, and transform\nfor $\\Psi_{\\text{transform}}$. We use these four macro operations and function calls of dependent functions to de-\nfine entity functions. Entity functions are identified with the associated words in the DSL via two\nspecial forms: bind, which binds an entity function $f_w$ to word $w$, and retrieve, which retrieves\n$f_w$ given $w$ and applies $f_w$ on actual embedding parameters. If $w$ is never bound to a function, it"}, {"title": "4 RENDERING", "content": "Applying the proposed scene representation to image generation tasks requires rendering a Scene\nLanguage $\\Phi(s)$ into images. To do so, first, the program interpreter evaluates $\\Phi(s)$ to obtain a\ndata object of type Entity. Afterward, a graphics renderer maps the Entity data object to its\nrendering parameter space and renders it into a final image.\nRenderer Specifications. We define the specification of a graphics renderer, a module in the pro-\nposed representation, as follows. A graphics renderer is determined by primitive parameter space $\\Theta$\nand a rendering operation $R : P (\\Theta \\times T) \\to I$, where $T$ is the space of 3D affine transformations\nrepresenting poses, $P$ denotes all possible subsets, and $I$ is the space of rendered images. In order to\ndetermine a mapping from program execution outputs of type Entity (Fig. 3b) to the admissible\ninput domain of rendering operation $R$ (Fig. 3c), we assume access to a reparameterization function\n$G_{\\text{reparam}}$ that maps from Tuple [Word, Embedding] to $\\Theta$, and compute a primitive's pose $t \\in T$\nby multiplying all Matrix values along the path from the primitive to the scene (root) in the entity\nhierarchy, analogous to computing limb poses in a kinematic tree."}, {"title": "Renderer Instantiations.", "content": "An example renderer instantiation is with Score Distillation Sampling\n(SDS) (Poole et al., 2023) guidance, where $\\Theta$ is a differentiable 3D representation, and we specify\n$G_{\\text{reparam}}$ as follows. Recall that an entity is associated with a Word value, e.g., moai for one statue\nfrom Figs. 1 and 3, and an Embedding value, e.g., $<z_2>$. For each primitive entity (i.e., an entity\nwith no children), given these two value fields together with the fields of its ancestors, we use a\nmanually specified language template $c$, or a $<z_2>$ moai, in the style of $<z_1>$, 3D\nmodel in this example, to embed them into $z = g_{\\text{CLIP}}(c) \\in Z_{\\text{CLIP}}$ where $g_{\\text{CLIP}}$ is the pre-trained\nCLIP text encoder. Then, $G_{\\text{reparam}}: Z_{\\text{CLIP}} \\to \\Theta$ corresponds to the SDS-guided optimization to find\na solution in $\\Theta$ that aligns with the input condition $z \\in Z_{\\text{CLIP}}$. Output 3D scenes can be personalized\nby editing embeddings, e.g., $<z_1>$ which controls the global style in Fig. 1.\nFor the underlying 3D representation, we use 3D Gaussian Splatting (Kerbl et al., 2023) where im-\nages are rendered by splatting a set of 3D Gaussians onto the image plane; other differentiable\n3D representations such as neural fields will also be suitable. We base our implementation on"}, {"title": "5 INFERENCE VIA PRE-TRAINED LANGUAGE MODELS", "content": "We introduce a training-free method to infer the representation $\\Phi(s) = (W, P, Z)$ from text or image\ndescriptions of a scene $s$. As explained below, we first prompt a pre-trained language model (LM)\nto generate the non-neural components $(W, P)$ and then obtain neural embeddings $(Z)$ from texts\nvia the CLIP text encoder or from images with a pre-trained text-to-image diffusion model.\nLMs have shown remarkable capability in code generation with common programming languages\nsuch as Python. In our implementation, we prompt LMs to generate Python scripts. We prompt the\nLM with the input condition, i.e., a scene description in texts or an image; a Python script of helper\nfunctions converted from the DSL in Table 2; and an example script using the helper functions. We\nuse Claude 3.5 Sonnet (Anthropic, 2024) for all experiments for our method and LM-dependent\nbaselines. Full language prompts are listed in Appendix D.\nFunction arguments in the LM-generated scripts, which are numeric values or string tokens, are\nconverted to embeddings from $Z_{\\text{CLIP}}$ (Section 3.2) using language templates and the CLIP text\nencoder $g_{\\text{CLIP}}$. For example, in the raw LM output, function calls for white pieces in Fig. 2 have\ninput attribute {\\text{\"color\"}:(.9,.9,.9)}, and we prompt LM to describe the color value as a\nword, and feed the word into $g_{\\text{CLIP}}$ to compute $<z_{68}>$. For image-conditioned tasks, for each\nprimitive entity in the execution output of $P$, we first use GroundingSAM (Kirillov et al., 2023; Ren\net al., 2024) to segment out the region defined by the word associated with the entity. We then use\nTextual Inversion (Gal et al., 2023) to optimize an embedding to reconstruct the cropped image with\nthe diffusion model training objective. The full process is deferred to Appendix E.1."}, {"title": "6 APPLICATIONS", "content": "We apply the method from Section 5 to the tasks of text-conditioned 3D scene generation (Sec-\ntion 6.1) and editing (Section 6.2), image-conditioned scene generation (Section 6.3), and 4D scene\ngeneration (Section 6.4)."}, {"title": "6.1 TEXT-CONDITIONED SCENE GENERATION", "content": "Baselines. To evaluate the proposed representation, we compare our inference pipeline with 3D\nscene generation methods using alternative intermediate representations, e.g., scene graph. In par-\nticular, we compare with GraphDreamer (Gao et al., 2024) as an exemplar approach, which gener-\nates scene graphs from input texts via LM prompting and then synthesizes scenes conditioned on\nthe graphs via SDS guidance. We further ablate the role of structural representation in this task by\ncomparing ours with the backbone of our SDS-based renderer, MVDream (Shi et al., 2024), as a\ndirect scene generation approach. Full implementation details in Appendix E.2.\nResults. Text-conditioned scene generation results rendered with the SDS-based renderer are shown\nin Fig. 4. Compared to the direct 3D scene generation method MVDream, our approach is composi-"}, {"title": "6.2 TEXT-INSTRUCTED SCENE EDITING", "content": "Scenes synthesized from our proposed representation can\nfurther be edited by prompting LM with its previously gen-\nerated script and an editing instruction in natural language\ninstruction. Results are shown in Fig. 5. Our representation provides an interpretable and intuitive\ninterface for scene editing, as functions have explicit semantic meanings associated with words, and\nfunction reuse significantly improves program readability. Furthermore, since the structure of pro-\ngrams reflects the structure of scenes, editing program parameters leads to changes in the scenes\nwhile preserving the original structure, e.g., the circular arrangement of staircases in Fig. 5. Desir-\nable editing effects involving multiple primitives, or all staircases in this example, can be effectively\nachieved via only minor changes in the program space. Finally, the program structure itself, such as\nthe function header in the Jenga set example, can be adjusted for editing to achieve localized edits\nthat only affect relevant parts of the scene.\nThe compositionality of our representation\ndirectly benefits localized scene editing. In\ncomparison, MVDream from Section 6.1\ndoes not apply to this task, as the entire\nscene is parameterized with a single 3D\nrepresentation. Precisely encoding the ge-\nometric relations of scene components fur-\nther enhances the controllability of gener-\nated scenes. In comparison, GraphDreamer\nrepresents the binary relation of scene com-\nponents with coarse language descriptions;\ntherefore, it does not apply to editing tasks\ninvolving precise geometric controls, as\nshown in the first example from Fig. 5."}, {"title": "6.3 IMAGE-CONDITIONED\nSCENE GENERATION", "content": "The proposed representation can be used\nfor image parsing and generating 3D scenes\nconsistent with the parsed image structure\nand content. We compare our representa-\ntion with scene graphs by comparing our"}, {"title": "6.4 TEXT-CONDITIONED 4D SCENE GENERATION", "content": "We apply the inference method from Section 5 to generate 4D scenes. The 4D scene representation\nin this task is identical to the definition in Eq. (1), except that there is an additional 4D entity function\nin the program $P$. The corresponding DSL extends from Table 2 as specified in Appendix B.\nAllowing for a flexible set of primitive entities is crucial to make our representation suitable for\ngenerating 4D scenes of different scales, including objects with moving parts (e.g., the wind tur-"}, {"title": "6.5 DIFFERENT GRAPHICS RENDERERS", "content": "The same program can be rendered with differ-\nent renderers described in Section 4, showing the\nversatility of the proposed representation. The re-\nsults are shown in Fig. 9 with the same experi-\nment setup as in Section 6.1."}, {"title": "6.6\nVISUALIZATION\nOF DISCRIMINATIVE INFORMATION", "content": "As shown in Fig. 8, several pieces of discrimi-\nnative information can be directly obtained with\nthe proposed Scene Language: semantic maps\nin Fig. 8b, as words represent per-entity seman-\ntics; instance segmentation in Fig. 8c, as the rep-\nresentation is compositional with separable in-\nstances; correspondence of the repeated instances\nin Fig. 8d, as programs specify repetitions exist-\ning in a scene; dense temporal correspondence for\n4D scenes, as shown in Fig. 7."}, {"title": "7 CONCLUSION", "content": "We have introduced a visual scene representation, termed the Scene Language, which encodes three\nkey aspects of visual scenes: scene structure, such as hierarchy and repetition, specified via pro-\ngrams; semantics of individual components succinctly summarized via words; and identities of each\ncomponent precisely captured via neural embeddings. We formalize the representation as a program-\nming language defined using a DSL. We show that the Scene Language can be efficiently inferred\nfrom both text and image inputs using pre-trained language models. Once the program is executed,\nthe resulting scene can be rendered into images using a variety of graphics renderers. Compared\nwith existing methods, our Scene Language produces 3D and 4D scenes with significantly higher\nfidelity, preserves complex scene structures, and enables easy and precise editing."}]}