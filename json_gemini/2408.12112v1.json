{"title": "Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards", "authors": ["Shresth Verma", "Niclas Boehmer", "Lingkai Kong", "Milind Tambe"], "abstract": "LLMs are increasingly used to design reward functions based on human preferences in Reinforcement Learning (RL). We focus on LLM-designed rewards for Restless Multi-Armed Bandits, a framework for allocating limited resources among agents. In applications such as public health, this approach empowers grassroots health workers to tailor automated allocation decisions to community needs. In the presence of multiple agents, altering the reward function based on human preferences can impact subpopulations very differently, leading to complex tradeoffs and a multi-objective resource allocation problem. We are the first to present a principled method termed Social Choice Language Model for dealing with these tradeoffs for LLM-designed rewards for multiagent planners in general and restless bandits in particular. The novel part of our model is a transparent and configurable selection component, called an adjudicator, external to the LLM that controls complex tradeoffs via a user-selected social welfare function. Our experiments demonstrate that our model reliably selects more effective, aligned, and balanced reward functions compared to purely LLM-based approaches.", "sections": [{"title": "1 Introduction", "content": "Reward functions play a fundamental role in the generation of optimal policies for sequential decision-making via reinforcement learning. Previous work has shown that LLMS are an effective tool for designing reward functions that can be guided and customized via human language prompts (Ma et al. 2023; Cao et al. 2024; Kwon et al. 2023; Xie et al. 2024a; Ma et al. 2024; Yu et al. 2023; Hazra et al. 2024). We study the problem of designing high-quality reward functions aligned with human preference prompts in the context of multiagent optimization and planning problems. We present a transparent framework around LLMs that constructs effective, aligned, and balanced reward functions for complex human prompts.\nWe study the reward design problem for restless multi-armed bandits (RMABs), a popular model in multiagent systems for sequentially allocating a limited number of resources to a set of agents (Whittle 1988; Ni\u00f1o-Mora 2023). In RMABs, each agent is represented by an individual Markov Decision Process including a reward function. By choosing these reward functions, one can control which agents are more or less likely to receive a resource. RMABS have been used in various domains such as machine maintenance (Abbou and Makis 2019), anti-poaching (Qian et al. 2016a), and healthcare (Ayer et al. 2019; Verma et al. 2023). In many of them, system organizers have evolving allocation priorities based on agents' features that need to be incorporated into the resource allocation process (Deardorff et al. 2018; Verma et al. 2024). For instance, in a healthcare program, a healthcare worker might want to change the allocation policy to prioritize low-income beneficiaries who are at higher risk or older beneficiaries who have transportation barriers for healthcare access (Nelson et al. 2016; Syed, Gerber, and Sharp 2013) via the preference prompt: Prioritize low-income beneficiaries and older beneficiaries.\nTranslating such human language prompts to effective and aligned reward functions is a general, non-trivial challenge in RL (Hazra et al. 2024; Cao et al. 2024; Ma et al. 2023), e.g., as it is unclear how changes in the reward influence the used policy downstream. However, the multiagent nature of the RMAB problem adds a new twist to the problem of reward design in RL: It becomes fundamentally multi-objective. Consider the above example prompt asking for the prioritization of two subpopulations. As these subpopulations may contain different agents, selecting a reward function will most likely involve trading off the interests of the low-income vs. older beneficiaries, making this a multi-objective problem. If this multi-objective nature is ignored, a selected reward function might heavily favor one of the two groups (e.g., leading to the allocation of many resources to low-income beneficiaries, and no resources to older ones). Another tradeoff that emerges because of the multiagent nature of the problem is between aligning with the preference prompt and impacting agents not referenced in the prompt: Changing the reward functions to align with a prompt can lead to drastic changes in the number of resources distributed to subpopulations not referenced in the user prompt (e.g., in the example prompt, the selected reward might lead to taking away all resources from well-educated beneficiaries). If not adequately balanced, this can cause strong side effects unintended by the user.\nTo our knowledge, we are the first to address the multiobjective nature of LLM-powered reward design in RMABS in particular and multiagent planners in general. Closest to our paper is the work by Behari et al. (2024) who proposed"}, {"title": "2 Related Works", "content": "LLM-enhanced RL LLMs have emerged as a powerful tool to enhance RL. Recent work has used LLMs to generate reward functions based on natural language descriptions (Ma et al. 2023; Xie et al. 2024b; Yu et al. 2023). For instance, Goyal, Niekum, and Mooney (2019); Carta et al. (2022); Mirchandani, Karamcheti, and Sadigh (2021); Hazra et al. (2024) shape rewards by training an RL agent to learn and complete intermediate tasks guided by language, yet focusing on very different (non-multiagent) environments.\nThe work of Behari et al. (2024) is the first to present a Decision-Language Model for generating reward functions for RMABs from human prompts. The model performs a form of evolutionary search to find reward functions aligned with the given prompt in two interleaving phases: generation and reflection. In the generation phase, an LLM generates a set of reward functions. Based on reward function's performances, in the reflection phase (Ma et al. 2023; Shinn et al. 2023), the LLM selects the function best aligned with the prompt. This function is then included in the prompt for the next generation phase or returned. In contrast to our work, DLM mixes generation with selection and does not explicitly account for the multi-objective nature of the reward design problem. Furthermore, in contrast to our work, they focus on small RMAB instances (~ 20 arms). Throughout the paper, we will use a slightly modified variant of DLM adjusted to our setting (see Section C.2) as a baseline.\nLLMs can also be integrated into model-based and explainable RL, serving as world model simulators (Lin et al. 2023) or policy interpreters (Das, Chernova, and Kim 2023). They can also act as information processors, effectively managing environment data and task instructions (Basavatia, Murugesan, and Ratnakar 2024; Song et al. 2023) and as direct decision makers (Li et al. 2022) to improve the sample efficiency of offline RL. While this area of research is rapidly evolving, there remains a significant gap in addressing multi-objective, multi-agent scenarios within LLMenhanced RL, which we begin to address in this paper.\nMulti-Objective Reinforcement Learning (MORL) Research on MORL focuses on learning policies that maximize (and balance between) multiple objective functions, typically via scalarizing the objectives into a single reward function (Moffaert, Drugan, and Now\u00e9 2013) or approximating the Pareto front (Roijers et al. 2013; Van Moffaert and Now\u00e9 2014). In the context of multiagent systems, MORL has been used as a method to ensure the fair treatment of the individual agents (Jiang and Lu 2019; Zimmer et al. 2021; Fan et al. 2023). Closest to ours from this line of work is the paper by Fan et al. (2023), who use ideas from the resource allocation literature to combine multiple objectives into a singular nonlinear objective function. However, in contrast to our paper, their focus lies on policy learning for such non-linear objective functions. They neither consider reward design, human preference prompts, LLMs, nor RMABs.\nWe refer to Section A for additional related work."}, {"title": "3 Preliminaries", "content": "An instance of Restless Multi-Armed Bandits (RMAB) is defined by a set of N arms, a time horizon T, and a budget K. We also refer to arms as agents. Each arm $i\\in [N]$ is an independently evolving MDP with state space $S_i$, actions $A_i = {0, 1}$, transition function $P_i : S_i \\times A_i \\times S_i \\rightarrow R_{\\ge 0}$, and reward function $R_i: S_i \\rightarrow R$. We refer to 1 as the active action corresponding to pulling the arm (i.e., allocating a resource) and 0 as the passive action corresponding to not pulling the arm. We focus on the popular case where each MDP consists of two states, i.e., $S_i = {0, 1}$ for all $i \\in [N]$, yet our methodology applies to MDPs with arbitrary state spaces. We refer to 0 as the bad and 1 as the good state. For each step in which an agent is in the good state, they derive a utility of 1, while they derive a utility of 0 in the bad state. Accordingly, agents' default reward function $R^*$ is $R^*(s) = s$. We assume that there is a set of categorical features. Each arm is associated with a value of each feature. A global reward function is a reward function defined over features, which induces a reward function for each arm by plugging in its feature values (see Example 4.1).\nIn each step within the time horizon T, the planner observes the state of all arms and decides to pull a subset of"}, {"title": "4 Problem Statement & Challenges", "content": "We assume that we are given a human-language preference prompt, concatenating one or multiple preference clauses. Each preference clause specifies a single optimization goal. We explicitly consider three types of preference clauses (yet our methodology extends to arbitrary ones):\n\u2022 Give priority to agents with certain feature values, i.e., increase the utility generated by these agents,\n\u2022 do not shift the utility distribution for some feature, and\n\u2022 maximize the summed utility generated by all agents.\nWe mostly focus on the first type and refer to them as prioritization clauses and prompts. A preference prompt is a set $P = {P_1,P_2,...}$ of the involved preference clauses. We call a prompt P singular if $|P| = 1$ and composite otherwise; our focus is on the latter. We can influence the utility agents generate by selecting a single global reward function (inducing reward functions $(R_i)_{i\\in[n]}$ for all agents).\nWe want to design a single global reward function that is \"well-aligned\" with all clauses of the given human-language preference prompt. However, as clauses can contradict each other, perfect alignment with all clauses becomes impossible. For instance, if a prompt requests the prioritization of two fully disjoint subpopulations, each resource will only benefit one of the two. When picking the reward function, we need to carefully balance the interests of the two groups"}, {"title": "5 Social Choice Language Model (SCLM)", "content": "We propose a Social Choice Language Model to generate rewards from human language composite preference prompts (see Figure 2 for a visualization). Separating the generation and selection of reward functions, the model consists of two sequential components. The LLM-powered generator generates a set of candidate reward functions. Subsequently, taking a social-choice-inspired viewpoint, the adjudicator selects a reward function from the pool to be returned to the user in two steps: First, a scorer model computes an alignment score for each reward function with each prioritization clause (i.e., we judge each reward function from the perspective of all relevant \u201cobjectives\u201d). Second, a userdefined social welfare function aggregates these scores into a"}, {"title": "5.1 Generator", "content": "Given a prompt, our generator creates a set of candidate reward functions (as Python code) via a variant of evolutionary search (Behari et al. 2024): We proceed in multiple steps. First, inputting the problem description, feature descriptions and the preference prompt, we ask an LLM to generate code for a reward function. We repeat this query np times to obtain a set R of np candidate reward functions. Afterwards, for each function $R \\in R$ we compute the utility feature distributions of the policy $\\Pi(R)$ induced by the reward function R on the given RMAB instance (via repeatedly simulating the policy on the instance). Then, the prompt and the set of candidate reward functions together with the associated utility feature distributions are passed to an LLM, which is asked to select the reward function R' from R best aligned with the prompt (Ma et al. 2023; Shinn et al. 2023). Now, we repeat the whole process, this time including the selected policy R' as a seed in the reward function generation prompts. Once we have executed the process nr times, we add all generated $n_p \\cdot n_r$ candidate reward functions R to the pool R (see Section C.2 for details)."}, {"title": "5.2 Adjudicator", "content": "The adjudicator selects a reward function from a given pool of candidate reward functions returned by the generator. To handle the complex tradeoffs arising within composite prompts and the resulting multi-objective optimization problem, the adjudicator follows a social choice approach. Social choice is a discipline at the intersection of economics, philosophy, and mathematics and concerned with aggregating the potentially contradicting preferences of a set of voters into a fair compromise alternative from a given candidate set (Arrow, Sen, and Suzumura 2010; Moulin 2004). It thus provides a theoretically grounded and well-studied methodology for balancing competing interests. In our problem, we can interpret the reward functions as the candidates and the preference clauses in the prompt as the voters with their preferences over the candidates reflecting the reward function's alignment with the clause. This view gives rise to the following strategy: Given a prompt $P = {P_1,P_2,..., P_l}$, we evaluate each reward function $R \\in R$ from the perspective of each preference clause $p_i$ by computing an (alignment) score $s_i(R)$. $s_i(R)$ measures the alignment of $\\Pi(R)$ with preference clause $p_i$, i.e., how much the voter representing $p_i$ \"likes\" the candidate R. We describe at the end of this section how these scores are computed.\nSocial Welfare Functions Social welfare functions select an alternative based on input preferences of voters. The pros and cons of individual social welfare functions have been extensively researched and debated in social choice (Arrow, Sen, and Suzumura 2010; Rawls 2017). We make use of cardinal social welfare functions (Keeney and Kirkwood 1975) which take as input our alignment scores $(s_i(R))_{i\\in[l],R\\in R}$ and output the winning reward function. We consider the three arguably most popular social welfare functions:\nUtilitarian Return the reward function maximizing the sum of its scores, i.e., $\\arg \\max_{R \\in R} \\sum_{i\\in[l]} s_i(R)$.\nNash Return the reward function maximizing the product of its scores, i.e., $\\arg \\max_{R \\in R} \\prod_{i\\in[l]} s_i (R)$.\nEgalitarian Return the reward function maximizing its minimum score, i.e., $\\arg \\max_{R \\in R} \\min_{i\\in[l]} s_i(R)$.\nSocial welfare functions also allow for assigning a different importance to clauses: The user could submit an importance score $w_i$ for each clause $p_i$, which can be easily incorporated in the social welfare function, e.g., the Utilitarian welfare function becomes $\\arg \\max_{R \\in R} \\sum_{i\\in[l]} w_i \\cdot s_i(R)$.\nSelecting the social welfare function gives us control over the tradeoffs between objectives: By picking the egalitarian function, we ensure that one clause will not get prioritized over another. In contrast, the Utilitarian function prioritizes the summed alignment, allowing for mismatches between clauses; the Nash function strikes a balance between the two functions. Further, the adjudicator makes the selection process more transparent, as the different objectives, the selection criterion, and the performance of the candidate reward functions regarding the objectives become explicit."}, {"title": "5.  Preventing Unintended Utility Shifts and Utility Drop", "content": "It remains to describe how the alignment scores $s_i(R)$ are computed. We present two general methods to compute alignment scores, which we will use for prioritization clauses. Subsequently, we discuss two more customized methods for the prevention of unintended utility shifts or drops in total generated utility.\nSimulator Scorer Model (SCLM-SIM) For each preference clause $p_i \\in P$, we compute a reward function $R_i$ aligned with $p_i$ by casting it as a singular prompt to the DLM pipeline (see Section C.2). For each $R \\in R$, we compute as $s_i(R)$ the expected reward according to reward function $R_i$ produced by policy $\\Pi(R)$ (again, we approximate this quantity by running multiple simulations). $s_i(R)$ quantifies the quality of the policy induced by the candidate reward function R from the perspective of $p_i$ (as captured by $R_i$). As the scale of the reward functions can vary significantly among preference clauses, we normalize the scores by the performance of the default policy, i.e., we compute $\\frac{s_i (R) - s_i(R^*)}{s_i (R^*)}$.\nLLM Scorer Model (SCLM-LLM) The Simulator Scorer Model assumes access to reward functions capturing individual preference clauses well. If no well-aligned reward functions can be obtained, the performance of SCLM-SIM can deteriorate because it can become very noisy. Another disadvantage of SCLM-SIM is that the scores in SCLM-SIM are all computed via simulation, which can become computationally quite costly. Motivated by this, we propose a quicker and more flexible LLM-based approach, where we prompt an LLM to rate the alignment of a candidate reward function with a preference clause. In particular, for each $R \\in R$ and $p_i \\in P$, we use a prompt that includes R, $p_i$, and the utility feature distributions produced by policy $\\Pi(R)$. We ask the LLM to rank how well R aligns with the preference clause $p_i$ on a scale from 1 to 5 (see Section E for prompt texts).\nPreventing Unintended Utility Shifts and Utility Drop Aligning reward functions to a prioritization prompt might cause (unintended) utility shifts in other features (e.g., due to feature correlations, shifting utility to low-income beneficiaries might shift it away from more educated ones). See Section B for a concrete example. SCLM offers users the option to explicitly prevent these shifts by adding additional clauses (\u201cobjectives\u201d) to the prompt: Given a prompt P (e.g., the prompt from Example 4.1), for each feature not referenced in the prompt, the user can add a new preference clause requesting a minimum shift in the utility distribution of this feature (e.g., for Example 4.1 they could add \"do not change the utility distribution for feature C\u201d). To compute the alignment score $s_i (R)$ between a reward function R and a clause $p_i$=\u201cminimize utility shift for feature X\u201d, we compare feature X\u2019s utility distribution under the default policy with its utility distribution under the policy $\\Pi(R)$. Specifically, we quantify the difference using the Earth mover\u2019s distance (EMD) between the two distributions. Afterward, we apply 0-1 normalization to all scores $s_i(R)_{R \\in R}$ for prompt $p_i$, which are input to the social welfare function (along with the alignment scores for the other clauses).\nAnother potential risk of aligning a reward function with a prioritization prompt is that it can sharply decrease the summed utility generated by all agents: The user might request the prioritization of a subpopulation that does not benefit much from receiving a resource, leading to severe drops in the summed utility generated by all agents. Users can address this issue in our model by adding a clause $p_i$=\u201cmaximize the total generated utility\u201d to the prompt. As the alignment score $s_i (R)$ of $p_i$ with some reward function R we compute the summed utility generated by all agents under the policy $\\Pi(R)$ (computed via multiple simulations of the policy on the given instance). We again apply 0-1 normalization to all scores $s_i(R)_{R \\in R}$ for prompt $p_i$."}, {"title": "6 Experiments", "content": "We describe our testset (Section 6.1), the compared methods (Section 6.2), and our experimental results both for dealing with composite prioritization prompts (Section 6.3) and additionally minimizing unintended side effects (Section 6.4). Following the work of Behari et al. (2024), which constitutes our most important baseline, we use Gemini Pro (Anil et al. 2023) as the LLM in our experiments."}, {"title": "6.1 Dataset Description", "content": "Synthetic Domain We create three synthetic datasets, each with three features (A, B, and C). For some agent and feature, we randomly sample the agent's feature value between 0 and 1. Arm's passive transition probabilities ($P(s, a = 0, s')$ for $s, s' \\in [0,1]$) are sampled uniformly between 0 and 1. Our three datasets differ in how active transition probabilities are sampled. For each dataset, we set a three-dimensional weight vector $W \\in [0,1]^3$ specifying how much each feature impacts the effect of applying an active action (see Section C.3 for details). For each agent, let f denote their feature values; we sample their active transition probabilities for $s, s' \\in [0, 1]$ as $P(s, a = 1, s') = P(s, a = 0, s') + \\delta, \\delta ~ N(\\Delta, \\sigma)$, where $\\Delta = W \\cdot f^T$ and $\u03c3$ is a hyperparameter. Subsequently, we discretize the values of all features into 5 different equal-sized buckets to be consistent with the real-world domain. For each dataset, we sample 5 instances according to the above procedure with N = 2100 arms, a budget of B = 210, and a time horizon of T = 12 (to replicate the setup of the real-world domain described below where roughly 3% - 10% of beneficiaries receive a resource every week for up to three months).\nReal-World Domain ARMMAN (2024) is a non-profit in India that operates large-scale Maternal and Child Care Mobile Health programs for underserved communities. One of their programs disseminates critical health information via weekly automated voice messages. The goal of the NGO is to maximize beneficiaries' engagement, i.e., the number of messages they listen to. A limited number of beneficiaries are called by health workers every week to boost engagement. The problem of planning which beneficiaries to call has been modeled and solved as an RMAB, where the good/bad state corresponds to a high/low weekly engagement of the beneficiary. We use anonymized data from a quality improvement study conducted in January 2022 (Verma et al. 2023). For each beneficiary, we have access"}, {"title": "6.2 Models & Baselines", "content": "We analyze four different variants of SCLM differing in the used social welfare function (Utilitarian or Egalitarian) and scorer model (Simulator or LLM), e.g., we denote as SCLMSIM-Egalitarian SCLM with the Simulator Scorer Model and the Egalitarian social welfare function. In our generator, we generate 4 candidate reward functions in each step and run 5 iterations to generate a total of 20 candidate reward functions. In addition, we consider several LLM-focused baselines (see Section E for detailed descriptions):\nLLM-Zeroshot This baseline only queries the LLM once. It asks to return a reward function aligned with the given"}, {"title": "6.3 Results: Composite Prioritization Prompts", "content": "We analyze the performance on the 12 composite prompts which request the prioritization of two subpopulations (see Section D.1 for additional results).\nEvaluation Metrics As our goal is to fulfill the preferences specified by the user (in contrast to the classic goal of maximizing total utility), we need to quantify the alignment of the returned reward function with the given prompt P to evaluate our models. Due to the composite, multi-objective nature of our prompts, we start by measuring the alignment of the returned reward function R with each prioritization clause $p_i \\in P$ in a separate evaluation score $e_i (R)$. For this, we need to quantify how well a given reward function prioritizes the subpopulation specified in the prompt. However, as our prompts are written in human language, these subpopulations are not precisely defined (as the prompts only speak of agents with \u201chigh\u201d/\u201clow\u201d value of some feature X). Notably, one could think that the scores $s_i (R)$ computed in our adjudicator could be used as our evaluation scores $e_i(R)$, as they measure how well a reward R aligns with a prioritization clause $p_i$. However, this would create an unfair competitive advantage for the SCLM compared to our baselines who do not have access to these scores.\nInstead, we take a different approach and assume that the terms \"low\" and \"high\" in the input prompts refer to the most extreme feature values. Let $p_i$ be some prompt prioritizing agents with a high/low value of some feature X. As the evaluation score $e_i(R)$, we compute the summed utility generated by the agents with highest/lowest value of X under the policy $\\Pi(R)$ normalized by the utility generated by these agents under the default policy $\\Pi(R^*)$. Reflecting the"}, {"title": "6.4 Results: Utility Shifts & Cumulative Utility", "content": "As discussed in Section 5.2, we can also use our pipeline to prevent unintended side-effects of aligning reward functions with prioritization clauses, i.e., (i) shifts in the utility feature distribution of features not included in the prompt and (ii) drops in the total generated utility. We focus on (a) here and relegate the results for (b), which paint a very similar picture, to Section D.3. We analyze all 6 singular and 12 composite prompts, where we add additional clauses to prevent shifts in the utility distribution of all features not referenced in the prompt. We use SCLM-SIM-Utilitarian as our method. As baselines, we consider DLM only prompted with the prioritization clause(s) (called DLM-OP) and DLM prompted with the prioritization clause(s) and clause(s) asking for the minimizing of utility shifts (called DLM-EP).\nTo compute the alignment with the prioritization clauses, similar to Section 6.3, we compute the average change in the utility generated by the prioritized subpopulations. To quantify the unintended utility shifts, we compute the average Earth mover's distance between the utility feature distribution under the candidate and default reward function for each feature not included in one of the prioritization clauses.\nTable 1 shows the results. Comparing DLM-OP and DLM-EP, we find that adding the additional objective to the prompt does not result in a better performance for the synthetic and real-world domains. In contrast, SCLM chooses reward functions resulting in significantly higher utility increases for the prioritized subpopulations and significantly fewer unintended utility shifts. The fact that SCLM performs advantageously for both (conflicting) objectives highlights the quality of the pipeline and its capabilities to effectively address multiple objectives (of different types)."}, {"title": "7 Discussion", "content": "We present a customizable Social Choice Language Model to handle the multi-objective nature of preference prompts in reward design for RMABs. We showcase how methods from social choice can be used to improve the quality and transparency of decision-making of LLM-based frameworks, as we present an adjudicator component that makes the final decision from options generated by the LLM. SCLM significantly improves the quality of the chosen reward functions. We demonstrate that SCLM can not only handle composite prioritization prompts but arbitrary prompts containing multiple objectives, e.g., balancing the prioritization of subpopulations with the total utility generated by all agents. For future work, SCLM can be applied to other problems from multiagent planning and reinforcement learning. Further, SCLM can easily be extended to handle multiple preference prompts specified by different users."}, {"title": "A Additional Related Work", "content": "Multi-Objective Reinforcement Learning Real-world decision-making often involves trade-offs between multiple, conflicting objectives, making multi-objective reinforcement learning (MORL) essential. A common approach in MORL is to scalarize multiple objectives into a single reward function, but this can oversimplify the problem. To address this, Pareto-based methods identify Pareto-optimal solutions that reflect different trade-offs between objectives, providing a more comprehensive decision-making framework (Roijers et al. 2013; Van Moffaert and Now\u00e9 2014). Recent advancements in MORL have introduced algorithms that enhance scalability, handle non-linear objectives, and improve exploration strategies. For example, (Yang, Sun, and Narasimhan 2019) proposed an algorithm that adapts policies based on changing objective importance. Additionally, empirical methods have been developed to evaluate MORL performance across scenarios (Vamplew et al. 2011).\nIn multi-agent systems, MORL adds complexity as agents must balance their objectives while considering others'. Specialized approaches, such as extending Pareto-based methods to multi-agent contexts, aim to find Pareto-efficient solutions for all agents involved (Bloembergen et al. 2015). Moreover, the concept of fairness becomes important in multi-agent MORL, as solutions must be equitable to all agents while still optimizing overall system performance. Researchers have begun to investigate fairnessaware MORL algorithms that explicitly consider the distribution of rewards among agents, ensuring that no single agent disproportionately benefits at the expense of others (Jiang and Lu 2019; Zimmer et al. 2021).\n(Pre-LLM) Reward Function Design Designing effective reward functions for RL has long been a challenge due to the lack of robust learning signals. (Ladosz et al. 2022). Previous approaches for reward design in RL include manually specifying reward components and tuning their coefficients (Ng, Harada, and Russell 1999; Margolis and Agrawal 2023), and learning reward models from demonstrations (Ho and Ermon 2016) or preferences (Zhang and Ramponi 2023). While promising, these methods often demand substantial domain expertise, large amounts of data, or computationally expensive processes.\nLLMs and Social Choice In our adjudicator, we take a social choice perspective to model the problem of selecting a final reward function in a way that balances different objectives against each other. Principles from social choice have informed LLM-based algorithm in previous works, albeit in the very different context of creating a consensus statements for a group of users with diverse views (Bakker et al. 2022; Fish et al. 2023). Moreover, social choice principles are also increasingly used to inform the fine-tuning of language models, including its alignment with human feedback (Bakker et al. 2022; Conitzer et al. 2024; Ge et al. 2024; Dai and Fleisig 2024)."}, {"title": "C Implementation Details", "content": "C.1 Whittle Index Policy\nTo formulate the Whittle Index Policy, first we define the value function for an arm $i \\in [N]$ under the subsidy \u03bb as\n$V_i^*(s) = \\max_{a\\in{0,1}} Q_i(s, a_i, \\lambda)$.\nHere, $Q_i (s, a_i, \u03bb)$ measures the expected discounted cumulative future reward where a subsidy \u03bb is added to the reward when the passive action is taken. The Whittle index associated to the state si is then defined as:\n$\\Pi_i(s_i) := inf {\u03bb : Q_i (s_i, a_i = 0, m) = Q_i(s_i, a_i = 1, m)}.$ \nThe whittle index is thus the value of subsidy such that the passive (a = 0) and active (a = 1) actions have the same value. Intuitively, this captures the value of taking active action on an arm.\nTo implement the Whittle Index computation, we use the method in (Qian et al. 2016b) based on binary search. Additionally, for all the experiments, we cache computed whittle index for a given set of transition probabilities and reward functions to reduce computation time."}, {"title": "C.2 DLM Pipeline", "content": "In our work, we use a modified version of the DLM pipeline (Behari et al. 2024), which employs the Whittle Index policy as the planner for RMAB. Our approach differs from Behari et al. (2024) in that we use the Whittle Index policy specifically for simulating RMAB, whereas Behari et al. (2024) use the PreFeRMAB policy (Zhao and Behari et al. 2023). This modification allows for faster and more stable simulations and effectively decouples the learning problem from the planning problem.\nSpecifically, the DLM consists of three components. First, a user provides a natural language preference P. We then create a prompt for LLM which includes the context of the RMAB problem, the preference P, a description of features available, and the index of those features in the feature array. Finally, the LLM is asked to generate the Python code of the reward function in text format. We describe the prompt used in Figures 6 and 7.\nThe LLM is queried np times to obtain np reward functions. For each reward function, we also compute the reward distribution over all the features. Next, given all the generated reward functions and their corresponding reward distributions, we query the LLM to select the best reward function. We describe the prompt used in Figures 8 and 9. This is called the reflection step. The best reward function is then used inside the prompt for next step of generating np reward functions. This process is repeated nr times to obtain $n_p \\cdot n_r$ reward functions. In all our reward function generation experiments, we query the LLM $n_p = 4$ times and run the reflection loop $n_r = 5$ times resulting in 20 candidate reward functions.\nAs an LLM, we use the Gemini Pro model by Google. We query the LLM using python based API from the generativeai-python library."}, {"title": "C.3 Synthetic Dataset Generation", "content": "An RMAB problem is defined by the transition probabilities of Markov Decision Process governing every arm and the reward functions. We consider a 2-state, 2-action Markov decision process in all our experiments. This results in 8 transition probability parameters $P_i(s,a,s')$ $\\forall s \\in {0,1}, a \\in {0,1},s' \\in {0,1}$ for every arm i. Out of these 8 parameters, we only need to define 4 parameters $P_i(s,a,s' = 1)$ $\\forall s \\in {0,1},a \\in {0,1}$ independently, and the rest 4 parameters can be calculated as the compliment values $P_i(s,a,s' = 0) = 1 \u2212 P_i(s,a,s' = 1)$ $\\"}]}