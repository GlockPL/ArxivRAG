{"title": "Scaling Laws for Online Advertisement Retrieval", "authors": ["Yunli Wang", "Zixuan Yang", "Zhen Zhang", "Zhiqiang Wang", "Jian Yang", "Shiyang Wen", "Peng Jiang", "Kun Gai"], "abstract": "The scaling law is a notable property of neural network models that researchers have discovered, which greatly propels the development of large language models. Scaling laws hold great promise in guiding model design and resource allocation. Recent research increasingly shows that scaling laws are not limited to NLP tasks or Transformer architectures; they also apply to domains such as recommendation, where there are scaling laws relating to the amount of training data, model size, and model performance metrics. However, there is still a lack of literature on scaling law research in industrial online advertisement retrieval systems. This may be because 1) identifying the scaling law for resource cost and online revenue is often expensive in both time and training resources for large-scale industrial applications, and 2) varying settings for different systems prevent the scaling law from being applied across various scenarios. To address these issues, we propose a lightweight paradigm to identify the scaling law of online revenue and machine cost for a certain online advertisement retrieval scenario with a low experimental cost. Specifically, we focus on a sole factor (FLOPs) and propose an offline metric named R/R* that exhibits a high linear correlation with online revenue for retrieval models. We estimate the machine cost offline via a simulation algorithm. Thus, we can transform most online experiments into low-cost offline experiments. We conduct comprehensive experiments to verify the effectiveness of our proposed metric R/R* and to identify the scaling law in the online advertisement retrieval system of Kuaishou. With the scaling law, we demonstrate practical applications for ROI-constrained model designing and multi-scenario resource allocation in the Kuaishou advertising system. To the best of our knowledge, this is the first work to study the scaling laws for online advertisement retrieval of real-world large-scale systems, showing great potential for scaling law in advertising system optimization.", "sections": [{"title": "1 Introduction", "content": "The neural scaling laws, describing how neural network performance changes with key factors (e.g. model size, dataset size, computational cost), have been discovered in various research areas [8, 15, 18, 19, 23, 32]. Early research [14] shows that the neural network performance is predictable when scaling training data size in various tasks such as neural machine translation and language modeling. Kaplan et al.(2020) [19] further empirically verify the scaling law of Transformer architecture in language modeling, regarding the key factors (model size, data size, training cost) and training performance (PPL). Inspired by the scaling law, researchers extend the size of pre-trained language models and further empirically verify the scaling law by training GPT-3 [3]. This wave of enthusiasm has led to the creation of GPT-3.5 and GPT-4 [1], ushering in a new era of NLP research and applications.\nBased on the scaling laws, the optimal key factors of the model can be determined under given constraints, thus guiding us in model design and resource allocation. Recommendation and advertising systems are mature commercial applications that prioritize ROI, making it highly valuable to explore whether there exists scaling laws for recommendation and advertising models. Due to the lack of a thriving community and open data ecosystem similar to NLP, research on model scaling is relatively scarce in the recommendation and advertisement areas. Early studies primarily gave some qualitative conclusions about model scaling [23, 32]. Fang et al.(2024) [8] first attempt to give a quantitative scaling law of model performance and the amount of training data and model parameters based on public information retrieval benchmarks, and give the practice to solve the optimal amount of data and model parameters under a given total training resource.\nHowever, there is still a lack of literature on scaling law research in real-world online recommendation and advertising systems. We attribute this to two main challenges: 1) For commercial systems, the scaling law should describe the relationship between business revenue and machine costs, rather than the relationship between computing volume, data volume, and offline metrics as seen in traditional research. Achieving this requires training numerous models and conducting extensive online experiments, which are prohibitively expensive in terms of time, manpower, and machine costs; and 2) varying settings across different systems prevent the scaling law from being universally applicable. As a result, to use scaling laws to guide system optimization in a specific scenario, one must first incur substantial costs to obtain the necessary parameters. This makes it impractical for commercial systems to identify and apply scaling laws in many real-world settings.\nWe aim to identify scaling laws in online advertising systems with low experimental costs and explore their applications in guiding system optimization. An advertising system typically consists of several subsystems, including the advertisement retrieval subsystem and the bidding and ranking subsystem. Each subsystem has its own technology stack, making it challenging to establish a unified paradigm for lightweight identification of scaling laws. In this work, we focus on the online advertisement retrieval subsystem, which selects the top-k ads for the next stage but does not determine the billing of ads. This subsystem is described in detail in Section 3.\nTo address the issue of high experimental costs, we propose a lightweight paradigm for identifying scaling laws in online advertisement retrieval. First, we introduce a novel offline metric, R/R*, which incorporates the revenue information of each ad in the training data and exhibits a high linear correlation with online revenue for retrieval models. Note that we can obtain the specific relationship between R/R* and online revenue for our scenario through the records of daily iterations, theoretically eliminating the need for additional online experiments. Next, we conduct extensive offline experiments to verify the scaling behavior of MLP models in the retrieval scenario, where FLOPs serve as the independent variable and R/R* serves as the dependent variable. Our results show that these scaling behaviors follow a broken neural scaling law [4]. Finally, we propose a simple simulation algorithm, developed by our engineering team, to estimate the machine cost. By using the model size parameter as the decision variable and FLOPs and R/R* as intermediate variables, we can associate the model's estimated machine cost with the estimated online revenue.\nWe conduct extensive online experiments to verify the effectiveness of our proposed R/R* metric. The results show that R/R* is a more advanced offline surrogate metric for online revenue compared to traditional metrics. When there is no revenue-related information recorded at the ad granularity in the training data, NDCG can serve as a suitable substitute for R/R*. Additionally, we demonstrate the applications of the scaling law in ROI-constrained model design and multi-scenario resource allocation. These applications result in a 0.85% and 2.8% improvement in online revenue, respectively, highlighting the significant value of the scaling law in guiding system optimization.\nIn general, our main contributions are three-fold: 1) We introduce an offline metric, R/R*, which exhibits a strong linear correlation with online revenue, bringing significant convenience to the validation of scaling laws and the iterative development of retrieval models. 2) We provide a lightweight paradigm to identify the scaling laws between machine cost and online revenue and verify the existence of scaling laws in industrial advertisement retrieval scenarios. 3) We demonstrate practical applications of the scaling laws in the Kuaishou advertising system. With the scaling law of online revenue and machine cost, we can efficiently perform offline ROI estimations for hundreds of models with different sizes using just a few machines within a few days. The models obtained through this process have led to significant revenue improvement, highlighting the potential of scaling laws in optimizing advertising systems."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Neural Scaling Laws", "content": "The neural scaling laws, widely recognized in Natural Language Processing and Computer Vision areas, establish predictable relationships between model performance and key factors such as model size, dataset size, computational cost, and post-training error rates. Hestness et al.(2017) [14] introduced a unified scaling law applicable to machine translation, image classification, and speech recognition, noting that different tasks exhibit distinct scaling coefficients. Kaplan et al.(2020) [19] further elaborated on these laws, defining them through four parameters: model size (number of parameters), dataset size (number of tokens), computational cost (FLOPs), and training loss (perplexity per token). These relationships were empirically validated, including during the training of GPT-3 [3]. Subsequently, Hoffmann et al.(2022) [15] presented the Chinchilla Scaling Law, which differs somewhat from [19] because of their different training setups. In addition to estimating the training loss of the model, Isik et al.(2024) [18] further verified that there also exist scaling laws between the downstream task metrics and the model parameters. Caballero et al.(2023) [4] found that many scaling behaviors of artificial neural networks follow a smoothly broken power law (BNSL), verified on various NLP and CV tasks, covering a wide range of downstream tasks and model structures.\nIn the recommendation area, Shin et al.(2023) [23] and Zhang et al.(2024a) [32] conducted studies on whether there exist scaling laws for recommendation models and primarily provided qualitative conclusions. Fang et al.(2024) [8] first proposed a quantitative scaling law in the recommendation area, which describes the relationship between the amount of training data, the size of model parameters, and an offline metric for query-document retrieval. Based on this scaling law, the optimal amount of data and model parameter allocation can be solved under a given total training resource. However, obtaining a multivariate scaling law requires a large number of experiments, and the offline metrics might not be a good indicator for online metrics, these make it somewhat difficult to apply in real-world industrial applications. In this paper, we focus on how to obtain the scaling law function between the model's scalable hyper-parameters (such as the FLOPs) and the online revenue (the primary goal of the advertising system) with only a small amount of experimental cost."}, {"title": "2.2 Models and Evaluation for Online Advertisement Retrieval", "content": "Online advertising systems often adopt a cascade ranking framework [5, 9, 20, 26]. The cascade ranking usually includes two types of stages, namely retrieval and ranking. The retrieval stages take a set of terms as input and supply the top-k predicted terms to the next stage, which mainly focuses on order accuracy. The ranking stages in advertising systems should focus on not only the order accuracy but also the calibration accuracy [17, 22]. These lead to some technique differences in retrieval and ranking stages, such as optimization objectives, surrogate losses, design of evaluation metrics, etc. In this work, we focus on the retrieval stages of online advertising systems.\nThe retrieval stages select the top-k set for the next stage, typically including multiple ranking pathways. In the retrieval stages, which normally refers to the \"Matching\" or \"Pre-rank\" stage in industrial systems, models often adopt a twin-tower [6] or MLP [16] architecture. Some systems may also adopt more complex architectures like transformers [24]. In terms of objective design, industrial systems usually aim to estimate CTR [13] and exposure probability of ads, and use learning-to-rank methods [27, 28] to learn the results of ranking models. For evaluation of retrieval tasks, researchers commonly use OPA, NDCG, and Recall [2, 28, 29]. However, existing metrics often have complex relationships with online revenue, thus we propose R/R*, which exhibits a strong linear relationship with online revenue, to reduce the experimental cost of identifying the scaling law for industry scenarios."}, {"title": "3 Formulation of Online Advertisement Retrieval", "content": null}, {"title": "3.1 Overview of Online Advertising System", "content": "Figure 1 illustrates a typical cascade ranking system for online advertisements deployed at Kuaishou Technology. This system is comprised of four main stages: Matching, Pre-ranking, Ranking, and Mix-ranking. The \"Matching\" and \"Pre-ranking\" stages serve as retrieval mechanisms, focusing on selecting candidate ads without directly impacting their billing. Conversely, the \"Ranking\" stage plays a dual role by determining both the selection and billing of ads. It achieves this by predicting the eCPM (Effective Cost Per Mille) of each ad, serving as the standard for billing after its exposure. The \"Mix-ranking\" stage integrates the outputs from the advertising and recommendation systems to decide the final set of items presented to the user. For advertising systems, the differences in the goal of the retrieval and ranking stages lead to some technical differences, such as training objectives, system design, evaluation metrics, etc. In this work, we focus on the scaling laws of retrieval stages.\nFigure 2 shows the algorithm architecture of the Matching and Pre-ranking stages in the Kuaishou advertising system. They all adopt a multi-pathway ensemble architecture. The Matching stage can be divided into model-based pathways and rule-based pathways. The rule-based pathway can quickly filter out irrelevant ads using predefined criteria, ensuring compliance with regulations and improving ad relevance, while providing flexibility and transparency in the ad selection process. The model-based pathway uses machine learning algorithms to predict and select ads, achieving higher performance ceilings in ad selection. The Pre-ranking stage operates on the set of ads already selected by the Matching stage, and thus only employs the model-based pathways.\nModel-based pathways in the Matching and Pre-ranking stages can typically be categorized based on different modeling objectives, such as revenue-oriented, user-interest-oriented, and lifetime value-oriented pathways. In advertising systems, revenue-oriented pathways usually carry the most weight. Since the earlier stages only need to perform set selection, Kuaishou's advertising system employs Learning-to-Rank (LTR) methods to learn the optimal ranking order to maximize the revenue objective. The LTR pathway is the most prevalent and important in the system, as it is typically the primary focus for algorithm researchers to iterate and improve. Therefore, we use the Pre-ranking LTR pathway as a case study to verify scaling laws in online advertisement retrieval, which similarly applies to Matching models because they share the same multi-pathway architectures."}, {"title": "3.2 Formulation of Retrieval Tasks", "content": "Here we give the formulation of retrieval tasks represented by learning-to-rank in the Pre-ranking stage. The organization of training samples and the surrogate loss follows the ARF [28]. We randomly draw samples from the Pre-ranking, Ranking, and Mix-ranking stages. The training set can be formulated as:\n$D_{train} = \\{(f_{ui}, \\{f_{a_j^i}, ecpm_j^i \\mid 1 \\leq j \\leq n\\});\\}_{i=1}^N$ (1)\nwhere $u_i$ means the user of the i-th impression in the training set, $a_j^i$ means the j-th material for ranking in the system. N is the number of impressions of $D_{train}$. The i in $a_j^i$ means that the sample $a_j^i$ corresponds to impression i. The size of the materials for each impression is n. f(.) means the feature of (.). w is the ground truth rank index (the higher value is considered better) of the pair $(u_i, a_j^i)$. The rank index is the relative position of the sampled ads within the system queue, ordered by their original positions. ecpm is the eCPM (Effective Cost Per Mille) predicted by the Ranking stage, which can be regarded as the expected revenue for the exposure of ad. If ad does not win in the Pre-ranking and thus does not enter the Ranking stage, the ecpm equals 0. The eCPM information in the dataset is only used to construct our offline evaluation metric.\nOur learning target is to let the model pick up all the ground-truth ads to the next stage. The surrogate loss of the ARF [28] can be formulated as follows:\n$L_{total} = L_{Relax} + \\frac{1}{2\\lambda^2}L_{Global} + log(|\\alpha|) $ (2)\nIn Eq 2, $\\alpha$ is a trainable scalar, the $L_{Relax}$ and $L_{Global}$ can be formulated as Eq 3 and Eq 4 in the following:\n$L_{Relax} = -\\sum_{i=1}^N \\{ \\sum_{j=1}^n (v_i [j]^T {\\mathcal{S}}_{M(\\cdot)[i,:)]} [j] * (log(\\sum_{m} exp(M(i,m))))$ (3)\n$L_{Gobal} = - \\sum_{i=1}^N [\\sum_{j=1}^n (v_i [k]^T {\\mathcal{S}}_{M(\\cdot)[i,:)]} [k] * log( {{\\mathcal{S}}_{PM(\\cdot)[i,:)]} [m])]} (4)\nwhere ${S}$ denote the soft permutation matrices obtained by a differentiable sorting operator (e.g. NeuralSort [10]) for sorting vector (\u00b7) in descending order, M(i,) denotes the model predicted score vector of the i-th impression, $v_i$ denotes the vector of the ground-truth value for the ads in the i-th impression. o refers to the element-wise matrix multiplication operation. $\\sum_{col}$ is the column sum for a 2-D matrix. $\\{ \\} _t$ and $[\\,]$ in Eq 3 and Eq 4 denote the t-th element of the tensor. m and k are hyper-parameters of the ARF."}, {"title": "3.3 Features and Foundation Model", "content": "Regarding the features, we adopt both sparse and dense features to describe the information of users and ads in the online advertising system. Sparse features are those whose embeddings are obtained from embedding lookup tables, while dense features are the raw values themselves. The sparse features of the user primarily include the action list of ads and user profile information (e.g., age, gender, and region). The action list mainly consists of action types, frequencies, target ads, and timestamps. The sparse features of the ads primarily include the IDs of the ad and its advertiser. The user's dense features mainly consist of embeddings produced by other pre-trained models. The dense features of the ads primarily include statistical features information and multimodal features generated by multimodal models.\nRegarding the foundation model, we employ the multi-layer perceptrons (MLPs) architecture. The MLP consists of 5 layers, where each hidden layer is composed of batch normalization, linear mapping, and a PReLU [12] activation function in sequence. The output layer is a pure linear layer. Parameters are initialized using the He initialization [12]. We apply a log1p transformation as in [21] for all statistical dense features. All sparse and dense features are concatenated together and fed as input to the model."}, {"title": "3.4 Background of Online Training and Serving", "content": "Our model is trained online using a distributed training framework with synchronous training. We use Adagrad [7] as the optimizer with a learning rate of 0.01 and an epsilon value of 1e-8. The training task consumes data in a streaming manner, approximately processing samples in the order they are generated. The data stream ensures that ads within the same impression are aggregated together and perform a shuffle at the impression level within a small time interval. We use single precision floating point format (FP32) for training and saving the model.\nWe use a TensorRT-based service architecture to deploy the model for online serving. To accelerate the computation during inference, we adopt half-precision floating point format (FP16) for each layer. Additionally, we decompose the first-layer computation of the MLP into separate user-side and item-side calculations, which are then summed using TensorFlow's broadcasting mechanism. In this way, the user-side computation needs to be performed only once per impression, while the first-layer item-side computation is cached and periodically refreshed by a dedicated service. We named this strategy \"first-layer optimization\". Since most of the computational cost of an MLP is concentrated in the first layer, the \"first-layer optimization\" significantly reduces this cost compared to the original calculation strategy. However, it also introduces a more complex relationship between the model's FLOPs and the associated machine costs. Due to space limitations, more details about online training and serving are described in appendix A."}, {"title": "4 Scaling Laws of Budget and Revenue", "content": "In this section, we verify whether MLPs exhibit scaling laws in the context of online advertisement retrieval, as described in Section 3. We present a lightweight and effective paradigm for identifying the scaling laws of computation budget and revenue. In sub-section 4.1, we introduce R/R*, a carefully designed offline metric that is highly linearly correlated with online revenue. In sub-section 4.2, we demonstrate that MLPs exhibit a scaling law between FLOPs and the offline metric R/R*. In sub-section 4.3, we propose two computation cost analysis schemes based on model size parameters. Considering the clear relationship between model size parameters and FLOPs, we can approximately determine the expected computing cost and revenue benefits when the model is deployed online with the above work."}, {"title": "4.1 Effective Surrogate Metric for Online Revenue", "content": "Traditional offline metrics (e.g. OPA [28], Recall, and NDCG) used for online advertising retrieval often ignore the revenue difference of the ground-truth ads and treat them equally. This naturally creates a gap between offline metrics and online revenue. To mitigate this gap, we propose a novel metric named R/R* based on $D_{train}$.\nThe $R/R_i$ for impression i in $D_{train}$ can be formulated as Eq 5:\n$R/R^{(i)} = \\frac{\\sum (\\Phi_{M(.)} [:, m; :] * ecpm)}{\\sum (\\Phi_{V(.)} [:, m; :] * ecpm)} $ (5)\nwhere $\\Phi_{M(.)}$ and $\\Phi_{V(.)}$ denote the hard permutation matrices sorted by $M(i,\\cdot)$ and $v_i$ respectively. ecpm is a row vector that represents the expected revenue of each ad of the i-th impression. R/R* is the average of $R/R^{(i)}$ on $D_{train}$. m is the number of ground-truth ads for exposure.\nR/R* aligns better with online revenue mainly because it explicitly considers the commercial value (eCPM) of each ad, directly reflects the goal of maximizing revenue, and reduces the gap between offline evaluation and online performance. If the following two assumptions are met, a linear relationship between R/R* and online revenue is likely to exist:\nASSUMPTION 1. The improvement in R/R* for a single pathway is proportional to the improvement in R/R* of the entire stage ensemble's output set.\nASSUMPTION 2. The post-stage models generally select only the top-ranked ads from the pre-stage models, and different ads with the same eCPM have the same probability for exposure.\nIn a mature advertising system, Assumption 2 holds approximately true. Assumption 1 also holds approximately true for pathways with larger weights. Therefore, the R/R* of the LTR pathway and the revenue of the Kuaishou advertising system should be approximately linearly related.\nTo verify the effectiveness of R/R*, we conduct extensive online A/B test experiments. We deploy several different models to empirically study the relationship between online revenue and offline metrics, which differ in feature engineering, surrogate loss, model structure, and model size. Due to the limitations on experimental traffic, we can only run up to 5 A/B test groups simultaneously, which means some experimental groups were launched on different days. The online experiments corresponding to each model use at least 5% of the traffic. We collect each model's online revenue from the A/B test platform and offline metrics from the training log. Figures 3 to 6 demonstrates the relationship between the online revenue and offline metrics, including our proposed R/R* and traditional retrieval metrics, namely NDCG, Recall and OPA. To protect commercial secrets, we applied a linear transformation to the original data, and the y-axis represents the relative increase in revenue rather than the absolute value. These operations do not affect the conclusions regarding the linear relationship. Each point in these figures represents data from one day of a model being live. It is evident that:"}, {"title": "4.2 Scaling Laws of FLOPs and Offline Metric", "content": "In this section, we verify whether there is a scaling law for the model between computational effort and online revenue under the setting described in Section 3. Thanks to the strong linear correlation between R/R* and online revenue, we can transform the process of identifying the scaling law from online experiments to offline experiments. We trained 10 models with different MLP sizes offline, each for 10 days to ensure convergence. We then collected their converged R/R* metrics and calculated the FLOPs for each model to verify whether there is a scaling law for MLP models under our setting. We verify whether the collected data follows the Broken Neural Scaling Law (BNSL), which is a statistical law that has been widely validated by Caballero et al.(2023) [4] for many scaling behaviors of artificial neural networks in CV and NLP tasks. BNSL can be formulated as shown in Eq 6:\n$R/R^* = a + (b * (FLOPS - c_0)) * \\sum_{i=1}^t (\\frac{(1 + (\\frac{FLOPS}{c_i})^{1/f_i})}{d_i})^{(-c_i * f_i)}$ (6)\nwhere a,b,c0,ci, fi are parameters and t is hyper-parameter of BNSL. t characterizes the degrees of freedom of BNSL, and in practice, we typically take it as 6. The scaling factor is the FLOPs, and the dependent variable is our proposed offline metric R/R*. We use basin-hopping [25] to fit the BNSL.\nFigure 7 illustrates the curve obtained after fitting the BNSL, along with the original data points. The x-axis represents the scaling factor (FLOPs), and the y-axis represents the dependent variable (R/R*). Each data point corresponds to an MLP model of a different size, with the number of layers and units per layer annotated in the figure. The $R^2$ value of the curve fitting is 0.996, indicating a strong correlation between the predicted values and the observed data points. Therefore, it provides strong evidence that there is indeed a Broken Neural Scaling Law for the MLP models under the typical scenario (referred to Section 3) of online advertisement retrieval. With this scaling law, we can predict or understand the scaling behavior of the MLP models.\nIt is important to note that the FLOPs in Figure 7 refer to the computational cost per user-ad pair (without customized optimization, e.g. the first-layer optimization), and the training data includes about 200 billion samples (namely user-ad pairs) and 20 billion impressions. We set the maximum FLOPs for the model to 143M FLOPs. The FLOPs range is not as wide as in NLP [15, 19]. This is mainly because 143M FLOPs is about 20 times the FLOPs of our online base model, and a mature commercial system typically cannot easily allocate such a large amount of computing resources in the short term. Additionally, from the scaling laws validated in other scenarios, the marginal returns of increasing computational cost tend to decrease. Therefore, from the perspective of commercial applications of scaling law in advertising systems, which strongly focus on ROI, predicting model performance within the computational range of 0 to 143M FLOPs should be sufficient.\nThanks to the strong linear relationship between R/R* and online revenue, we can establish a prediction from FLOPs to online revenue. To further validate whether the accuracy of the FLOPs-to-revenue estimation meets the needs of model development and iteration, we deploy two models for 7 days, each with 10% of online traffic. The FLOPs for these models are 12.6M and 22.9M, respectively. Based on the scaling law, the estimated online revenue gains are 0.33% and 0.62%, while the actual online revenue increases are 0.38% and 0.69%, respectively. In our scenario, this accuracy is more than sufficient for guiding model development and iteration. Note that the MLP network size is [1024, 512, 512, 512, 1] for both models, while the dimensions of the input features are 5128 and 10128 (calculation details are shown in appendix B.1), respectively. This further indicates the generalization of the FLOPs-to-revenue estimation. We notice that the estimation errors of these two models are smaller than the average error of figure 3. This may be because the models in figure 3 were deployed for only one day, while these two models were deployed for 7 days, which means the observations of figure 3 would have higher variance."}, {"title": "4.3 Mapping Function from Model Settings to Computation Cost and FLOPs", "content": "Towards real-world applications, we need the scaling law of revenue and computation cost (namely machine cost), rather than the relationship of revenue and FLOPs. To this end, we should build a robust mapping function from FLOPs to computation cost without online deployment. Unfortunately, it's intractable to give a certain formulation that maps the FLOPs to computation cost for general scenarios, since the actual computation cost is influenced by a multitude of factors including hardware specifications, software optimization, and so on. These factors can significantly vary across different environments, leading to highly non-linear and unpredictable relationships between FLOPs and actual costs.\nTo tackle this problem, we employ the parameters of model size to bridge the gap between FLOPs and computation cost. Determining the FLOPs of a model based on its size parameters is trivial. We use the tf.profiler API of TensorFlow to calculate the FLOPs for the forward pass of one user-ad pair. Thus, the main challenge lies in estimating the computation cost based on the model size parameters without online deployment. We propose two paradigms to identify the relationship between machine cost and model size parameters: 1) machine cost estimation based on software and hardware optimization experience, and 2) machine cost estimation based on offline simulation testing.\nThe first approach relies on professionals with deep expertise in optimizing software and hardware systems to minimize or avoid online deployment operations. They employ empirical methods to establish reliable correlations between model size and actual computation costs. For instance, Fang et al.(2024) [8] estimated the machine cost for standard models like Transformers, trained using PyTorch on A100 GPUs, leveraging their expertise. However, in real-world advertising scenarios, customized training and serving frameworks (e.g., the first-layer optimization in section 3.4) and machine configurations introduce more complex nonlinearities between model size parameters and machine cost, increasing the complexity and difficulty of expertise-based cost estimation.\nThe second method is a more standardized, reproducible, and recommended approach for cost estimation. This method can be formalized as an algorithm, as shown in Algorithm 1. Thanks to the machine cost estimation tools (abbreviated as MCET) developed by our engineering team, which implement Algorithm 1, we can estimate the required machine numbers for tens of models without actually deploying or launching them. Using one machine with a single GPU, we can estimate the required number of machines for a single TensorFlow meta-file within half an hour. Combining this information with the unit price of the specified machine configuration, we can accurately calculate the expected machine cost. It is important to note that for training and serving cost estimation, the algorithm should be executed using the meta files specific to the training and serving process, respectively. For training cost estimation, labels are randomly constructed. This approach not only saves time but also provides a reliable and efficient way to estimate computation costs for model deployment and iteration."}, {"title": "5 Applications in Model Designing and Resource Allocation", "content": "In this section, we show two real-world applications of the scaling laws identified by the paradigm in section 4 in the advertising system of Kuaishou Technology. To facilitate the description, we first define the following notations:\n* SP = [a0, a1, . . ., an]: A list representing the size parameters of an MLP, where ao denotes the dimension of the MLP input, and a\u2081 to an denote the output dimensions of each layer. Let us denote the TensorFlow meta file generated from the MLP model configured with SP as metasp.\n* F(SP): The FLOPs of the network with size parameters SP.\n* G: The mapping function from R/R* to the expected online revenue, which is a linear function.\n* BNSL: The mapping function from FLOPs to R/R*, which follows the form given in Equation 6.\n* MCET: The tool (function) used to estimate machine costs, where MCET(metasp) represents the estimated machine cost for deploying an MLP with size parameters SP.\nwhere G, BNSL, F and MCET are obtained by referring to section 4.1, section 4.2, and section 4.3, respectively."}, {"title": "5.1 ROI-Constrained Model Designing", "content": "For a commercial company, there is usually a standard regarding ROI. Specifically, for an advertising system, we aim for the return on investment (ROI) from the machine to be no less than \u03bb, where \u03bb is typically determined by management based on the company's operating conditions.\nGiven a specific MLP model with size parameters SP, the ROI of the model can be estimate by Eq 7:\n$ROI = \\frac{G(BNSL(F(SP)))}{MCET(metasp)}$ (7)\nThus, given the ROI constraint, we can find the approximate optimal model size by solving the following optimization problem:\n$max_{SP} G(BNSL((F(SP))))$\ns.t. $\\frac{G(BNSL(F(SP)))}{MCET(metasp)} > \\lambda$ (8)\nNotice that SP is a vector where each element represents the number of units in each layer of the MLP. As each element in SP increases, F(SP), BNSL(SP), and G(BNSL(SP)) also increase, indicating that F, BNSL, and G are monotonically increasing functions. However, MCET is not a monotonic function and is typically a complex mapping, making the problem non-convex and unsolvable analytically or through binary search.\nFor the functions in Eq 8, the most time-consuming part is the computation of MCET(SP), which relies on a simulation testing tool. Fortunately, even the computation of MCET does not have a high time and resource cost, allowing us to perform a grid search on this problem. We use 10 T4-GPU machines to run the grid search of about 1000 configurations and find an approximate optimal solution in about two days. Due to space limitations, details of the grid search settings are introduced in appendix B.1.\nWith the specific ROI constraint of Kuaishou Technology, we solve the approximate optimal size of our foundation MLP model, the SP of the online base model and the solved model are [3328, 1024, 256, 256, 256, 1] and [16128, 1024, 512, 512, 512, 1], the new model has brought about a 0.85% increase in revenue. Such an improvement is considered significant in our advertising scenario. Without the scaling law, it would be nearly impossible to test the ROI of hundreds of model configurations through online deployment. The time and resource costs of deploying so many models would far outweigh the incremental revenue from the optimized model derived using the scaling law."}, {"title": "5.2 Multi-scenario Resource Allocation", "content": "There are often different scenarios that use different models in online advertising systems", "9": "n$max_{\\{SP_i|0<i<t\\}} \\sum_{i=0}^{t-1} G(BNSL(F(SP_i)))$\ns.t. $\\sum_{i=0}^{t-1} MCET(metasp_i) \\leq B$\ns.t. $\\forall i \\in \\{0,...,t-1\\}, \\frac{"}]}