{"title": "Multi-student Diffusion Distillation\nfor Better One-step Generators", "authors": ["Yanke Song", "Jonathan Lorraine", "Weili Nie", "Karsten Kreis", "James Lucas"], "abstract": "Diffusion models achieve high-quality sample generation at the cost of a lengthy mul-\ntistep inference procedure. To overcome this, diffusion distillation techniques produce stu-\ndent generators capable of matching or surpassing the teacher in a single step. However, the\nstudent model's inference speed is limited by the size of the teacher architecture, prevent-\ning real-time generation for computationally heavy applications. In this work, we introduce\nMulti-Student Distillation (MSD), a framework to distill a conditional teacher diffusion model\ninto multiple single-step generators. Each student generator is responsible for a subset of the\nconditioning data, thereby obtaining higher generation quality for the same capacity. MSD\ntrains multiple distilled students, allowing smaller sizes and, therefore, faster inference. Also,\nMSD offers a lightweight quality boost over single-student distillation with the same architec-\nture. We demonstrate MSD is effective by training multiple same-sized or smaller students on\nsingle-step distillation using distribution matching and adversarial distillation techniques. With\nsmaller students, MSD gets competitive results with faster inference for single-step generation.\nUsing 4 same-sized students, MSD sets a new state-of-the-art for one-step image generation:\nFID 1.20 on ImageNet-64\u00d764 and 8.20 on zero-shot COCO2014.", "sections": [{"title": "1 Introduction", "content": "Diffusion models are the dominant generative model in image, audio, video, 3D assets, protein\ndesign, and more [20, 28, 6, 2, 47]. They allow different conditioning inputs \u2013 such as class la-\nbels, text, or images \u2013 and achieve high-quality generations. However, their inference process\ntypically requires hundreds of model evaluations \u2013 with an often slow and bulky network \u2013 for a\nsingle sample. This procedure costs millions of dollars per day [65, 15]. It also prohibits applica-\ntions requiring rapid synthesis, such as augmented reality. Real-time, low-cost, and high-quality\ngeneration will have huge financial and operational impacts while enabling new usage paradigms."}, {"title": "2 Related Work", "content": "Diffusion Sampling Acceleration. While a line of work aims to accelerate diffusion models\nvia fast numerical solvers for the PF-ODE [39, 40, 82, 25, 35], they usually still require more\nthan 10 steps. Training-based methods that usually follow the knowledge distillation pipeline can\nachieve low-step or even one-step generation. Luhman and Luhman [41] first used the diffusion\nmodel to generate a noise and image pair dataset that is then used to train a single-step generator.\nDSNO [81] precomputes the denoising trajectory and uses neural operators to estimate the whole\nPF-ODE path. Progressive distillation [55, 44] iteratively halves the number of sampling steps\nrequired without needing an offline dataset. Rectified Flow [36] and follow-up works [37, 72]\nstraighten the denoising trajectories to allow sampling in fewer steps. Another approach uses\nself-consistent properties of denoising trajectories to inject additional regularization for distillation\n[16, 5, 64, 62, 42, 53, 27]."}, {"title": "3 Preliminary", "content": "We introduce the background on diffusion models in Sec. 3.1 and distribution matching distillation\n(DMD) in Sec. 3.2. We discuss applying adversarial losses to improve distillation in Sec. 3.3."}, {"title": "3.1 Diffusion Models", "content": "Diffusion models learn to generate data by estimating the score functions [63] of the corrupted data\ndistribution on different noise levels. Specifically, at different timesteps t, the data distribution\n$\\mathbb{P}_{real}$ is corrupted with an independent Gaussian noise: $$\\mathbb{P}_{t,real}(x_t) = \\int \\mathbb{P}_{real}(x)q_t(x_t|x)dx$ where\n$q_t(x_t|x) \\sim \\mathcal{N}(\\alpha_t x, \\sigma_t^2 I)$ with predetermined $\\alpha_t, \\sigma_t$ following a forward diffusion process [63, 20].\nThe neural network learns the score of corrupted data $s_{real} := \\nabla_{x_t} \\log \\mathbb{P}_{t, real} (x_t) = -(x_t-\\alpha_t x)/\\sigma_t^2$"}, {"title": "3.2 Distribution Matching Distillation", "content": "Inspired by Wang et al. [67], a series of works [43, 75, 73, 45] aim to train the single-step dis-\ntilled student to match the generated distribution of the teacher diffusion model. This is done by\nminimizing the following reverse KL divergence between teacher and student output distributions,\ndiffused at different noise levels for better support over the ambient space:\n$E_t D_{KL} (\\mathbb{P}_{t, fake} || \\mathbb{P}_{t,real}) = E_{x_t} \\bigg(\\log \\Big(\\frac{\\mathbb{P}_{t,fake} (x_t)}{\\mathbb{P}_{t,real} (x_t)}\\Big)\\bigg).$ (1)\nThe training only requires the gradient of Eq. (1), which reads (with a custom weighting $w_t$):\n$\\nabla_\\theta L_{KL} (\\theta) := \\nabla_\\theta E_t D_{KL} \\sim E_{z,t,x_t} [w_t q_t (s_{fake} (x_t, t) - s_{real} (x_t, t)) \\nabla_\\theta G_\\theta (z)],$ (2)\nwhere $z \\sim \\mathcal{N}(0, I)$, $t \\sim \\text{Uniform}[T_{min}, T_{max}]$, and $x_t \\sim q_t(x | x)$, the noise injected version of\n$x = G_\\theta(z)$ generated by the one-step student. Here, we assume the teacher denoising model\naccurately approximates the score of the real data, and a \u201cfake\u201d denoising model approximates the\nscore of generated fake data:\n$s_{real}(x_t, t) \\approx \\frac{x_t - \\alpha_t \\mu_{teacher}(x_t, t)}{\\sigma_t^2} , \\ \\ \\ s_{fake}(x_t, t) \\approx \\frac{x_t - \\alpha_t \\mu_{fake}(x_t, t)}{\\sigma_t^2}.$ (3)\nThe \u201cfake\u201d denoising model is trained with the denoising objective with weighting $\\lambda_t$:\n$L_{denoise}(\\phi) = E_{z,t,x_t} [\\lambda_t ||\\mu_{\\phi} (x_t, t) -x||_2^2].$ (4)\nThe generator and the \u201cfake\u201d denoising model are updated alternatively. To facilitate better con-\nvergence of the KL divergence, Distribution Matching Distillation (DMD) and DMD2 [74] used\ntwo distinct strategies, both significantly improving the generation performance. DMD proposes\nto complement the KL loss with a regression loss to encourage mode covering:\n$L_{reg}(\\theta) = E_{(z,y)\\sim \\mathcal{D}_{paired}} l(G_\\theta (z), y),$ (5)\nwhere $\\mathcal{D}_{paired}$ is a dataset of latent-image pairs generated by the teacher model offline, and $l$ is the\nLearned Perceptual Image Patch Similarity (LPIPS) [78]. DMD2 instead applies a two-timescale\nupdate rule (TTUR), where they update the \u201cfake\u201d score model for $N$ steps per generator update,\nallowing more stable convergence. We use distribution matching (DM) to refer to all relevant\ntechniques introduced in this section."}, {"title": "3.3 Enhancing distillation quality with adversarial loss", "content": "The adversarial loss, originally proposed by Goodfellow et al. [14], has shown a remarkable capa-\nbility in diffusion distillation to enhance sharpness and realism in generated images, thus improving\ngeneration quality. Specifically, DMD2 [74] proposes adding a minimal discriminator head to the\nbottleneck layer of the \u201cfake\u201d denoising model $p_{\\text{fake}}$, which is naturally compatible with DMD's\nalternating training scheme and the TTUR. Moreover, they showed that one should first train the\nmodel without GAN to convergence, then add the GAN loss and continue training. This yields bet-\nter terminal performance than training with the GAN loss from the beginning. We use adversarial\ndistribution matching (ADM) to refer to distribution matching with added adversarial loss."}, {"title": "4 Method", "content": "In Sec. 4.1, we introduce the general Multi-Student Distillation (MSD) framework. In Sec. 4.2,\nwe show how MSD is applied to distribution matching and adversarial distillation. In Sec. 4.3, we\nintroduce an additional training stage enabling distilling into smaller students."}, {"title": "4.1 Distilling into multiple students", "content": "We present Multi-Student Distillation (MSD), a general drop-in framework to be combined with\nany conditional single-step diffusion distillation method that enables a cheap upgrade of model\ncapacity without impairing the inference speed. We first identify the key components of a single-\nstep diffusion distillation framework and then present the modification of MSD.\nIn the vanilla one-student distillation, we have a pretrained teacher denoising diffusion model\n$\\mu_{\\text{teacher}}$, a training dataset $\\mathcal{D}$, and a distillation method. The distillation yields a single-step gener-\nator $G(z; y \\in \\mathcal{Y})$ via $G = \\text{Distill}(\\mu_{\\text{teacher}}; \\mathcal{D})$. The obtained generator $G$ maps a random latent $z$\nand an input condition $y$ into an image. In comparison, in an MSD scheme, we instead distill the\nteacher into $K$ different one-step generators $\\{G_k(z; y \\in \\mathcal{Y}_k)\\}^K_{k=1}$ via\n$G_k = \\text{Distill}(\\mu_{\\text{teacher}}, \\mathcal{D}_k = \\mathcal{F}(\\mathcal{D}, \\mathcal{Y}_k)), \\quad k = 1, ..., K$ (6)\nSpecifically, each distilled student $G_k$ is specialized in handling a partitioned subset $\\mathcal{Y}_k$ of the\nwhole input condition set $\\mathcal{Y}$. So, it is trained on a subset of the training data $\\mathcal{D}_k \\subset \\mathcal{D}$, determined\nby $\\mathcal{Y}_k$ via a filtering function $\\mathcal{F}$. Fig. 1 illustrates this idea.\nThe partition of $\\mathcal{Y}$ into $\\{\\mathcal{Y}_k\\}^K_{k=1}$ determines the input condition groups for each student. As a\nstarting point, we make the following three simplifications for choosing a partition:\n\u2022 Disjointness: This prevents potential redundant training and redundant usage of model capacity.\n\u2022 Equal size: Since students have the same architecture, the partitions $\\{\\mathcal{Y}_k\\}^K_{k=1}$ should be of equal\nsize that require similar model capacity.\n\u2022 Clustering: Conditions within each partition should be more semantically similar than those in\nother partitions, so networks require less capacity to achieve a set quality on their partition.\nThe first two conditions can be easily satisfied in practice, while the third is not straightforward.\nFor a class-conditional generation, partitioning by semantically similar and equal-sized classes\nserves a straightforward strategy, though extending it to text-conditional generation is nontrivial.\nAnother promising strategy uses pretrained embedding layers such as the CLIP [51] embedding\nlayer or the teacher embedding layer. One could find embeddings of the input conditions and\nthen perform clustering on those embeddings, which are fixed-length numerical vectors containing\nimplicit semantic information. We ablate partition strategies in Sec. 5.4.\nThe data filtering function $\\mathcal{F}$ determines the training subset data $\\mathcal{D}_k$ from $\\mathcal{Y}_k$. For example, a\nvanilla filtering strategy could set $\\mathcal{F}(\\mathcal{D}, \\mathcal{Y}_k) = \\mathcal{D}_k := \\mathcal{D}|_{\\mathcal{Y}_k}$, where $\\mathcal{D}|_{\\mathcal{Y}_k}$ denotes the subset of the\ntraining dataset $\\mathcal{D}$ that contains the condition $\\mathcal{Y}_k$. Empirically, we found that this filtering works in\nmost cases, although sometimes a different approach is justified, as demonstrated in Sec. 4.2."}, {"title": "4.2 MSD with distribution matching", "content": "As a concrete example, we demonstrate the MSD framework using distribution matching (DM)\nand adversarial distillation techniques. Inspired by the two-stage framework in [74], each of our\nstudents is trained with a distribution matching scheme at the first stage and finetuned with an\nadditional adversarial loss at the second stage (adversarial distribution matching, or ADM):\n$G_k^{(1)} = \\text{Distill}_{DM} (\\mu_{\\text{teacher}}, \\mathcal{F}_{DM} (\\mathcal{D}_{DM}, \\mathcal{Y}_k)), \\quad k = 1, ..., K,$\n$G_k^{(2)} = \\text{Distill}_{ADM} (\\mu_{\\text{teacher}}, \\mathcal{F}_{ADM} (\\mathcal{D}_{ADM}, \\mathcal{Y}_k); G_k^{(1)}), \\quad k = 1, ..., K,$ (7)\nwhere we recall that $\\mu_{\\text{teacher}}$ is the teacher diffusion model, $G_k^{(i)}$ is the $k$-th student generator at the $i$-\nth stage, $\\mathcal{F}$ is the data filtering function, $\\mathcal{D}$ is the training data, and $\\mathcal{Y}_k$ is the set of labels that student\n$k$ is responsible of. The first stage $\\text{Distill}_{DM}$ uses distribution matching with either a complemented\nregression loss or the TTUR, with details in Sec. 3.2. These two methods achieve optimal training\nefficiency among other best-performing single-step distillation methods [69, 84, 27] without an\nadversarial loss, with a detailed comparison in App. A.3. The second stage $\\text{Distill}_{ADM}$ adds an\nadditional adversarial loss (details in Sec. 3.3), by introducing a small prediction head on the fake\nscore model. This introduces minimal additional computational overhead and allows resuming\nfrom the first stage checkpoint.\nDesigning the training data From Sec. 3, the data required for DM and ADM are $\\mathcal{D}_{DM} =$\n$(\\mathcal{D}_{paired}, \\mathcal{C})$ and $\\mathcal{D}_{ADM} = (\\mathcal{D}_{real}, \\mathcal{C})$, where $\\mathcal{D}_{paired}, \\mathcal{D}_{real}, \\mathcal{C}$ represents generated paired data, real\ndata and separate conditional input, respectively. We now discuss choices for the filtering function.\nFor the first stage data filtering $\\mathcal{F}_{DM}$, we propose $\\mathcal{F}_{DM}(\\mathcal{D}_{DM}, \\mathcal{Y}_k) = (\\mathcal{D}_{paired}, \\mathcal{C}|_{\\mathcal{Y}_k})$, where $\\mathcal{C}|_{\\mathcal{Y}_k}$\ndenotes the subset of condition inputs $\\mathcal{C}$ that contains $\\mathcal{Y}_k$. In other words, we sample all input\nconditions only on the desired partition for the KL loss but use the whole paired dataset for the\nregression loss. This special filtering is based on the observation that the size of $\\mathcal{D}_{paired}$ critically\naffects the terminal performance of DMD distillation: using fewer pairs causes mode collapse,\nwhereas using more pairs challenges the model capacity. Na\u00efvely filtering paired datasets by par-\ntition reduces the paired dataset size for each student and leads to worse performance, as in our\nablation in App. A.2. Instead of generating more paired data to mitigate this imbalance, we simply\nreuse the original paired dataset for the regression loss. This is remarkably effective, which we\nhypothesize is because paired data from other input conditions provides effective gradient updates\nto the shared weights in the network.\nFor the second stage, we stick to the simple data filtering $\\mathcal{F}_{ADM}(\\mathcal{D}_{ADM}, \\mathcal{Y}_k) = (\\mathcal{D}_{real}|_{\\mathcal{Y}_k}, \\mathcal{C}|_{\\mathcal{Y}_k})$, so\nthat both adversarial and KL losses focus on the corresponding partition, given that each student\nhas enough mode coverage from the first stage."}, {"title": "4.3 Distilling smaller students from scratch", "content": "Via the frameworks presented in the last two sections, MSD enables a performance upgrade over\nalternatives for one student with the same model architecture. In this section, we investigate train-\ning multiple students with smaller architectures \u2013 and thus faster inference time \u2013 without impair-\ning much performance. However, this requires distilling into a student with a different architec-\nture, preventing initialization from pretrained teacher weights. Training a single-step student from\nscratch has previously been difficult [69], and we could not obtain competitive results with the\nsimple pipeline in Eq. 7. Therefore, we propose an additional pretraining phase $\\text{Distill}_{TSM}$, with\nTSM denoting Teacher Score Matching, to find a good initialization for single-step distillation.\nTSM employs the following score-matching loss:\n$L_{TSM} = E_t [\\lambda_t ||\\mu_{TSM} (x_t, t) - \\mu_{teacher} (x_t, t) ||_2^2],$ (8)\nwhere the smaller student with weights $\\psi$ is trained to match the teacher's score on real images\nat different noise levels. This step provides useful initialization weights for single-step distillation\nand is crucial to ensure convergence. With TSM added, the whole pipeline now becomes:\n$\\mu^{(0)} = \\text{Distill}_{TSM} (\\mu_{\\text{teacher}}, \\mathcal{D}_{real}),$\n$G_k^{(1)} = \\text{Distill}_{DM} (\\mu_{\\text{teacher}}, \\mathcal{F}_{DM} (\\mathcal{D}_{DM}, \\mathcal{Y}_k); \\mu^{(0)}), \\quad k = 1, ..., K,$\n$G_k^{(2)} = \\text{Distill}_{ADM} (\\mu_{\\text{teacher}}, \\mathcal{F}_{ADM} (\\mathcal{D}_{ADM}, \\mathcal{Y}_k); G_k^{(1)}), \\quad k = 1, ..., K.$ (9)\nAlthough a smaller student may not perfectly match the teacher's score, it still provides a good\ninitialization for stages 1 and 2. The performance gap is remedied in the latter stages by focusing\non a smaller partition for each student. This three-stage training scheme is illustrated in Fig. 3."}, {"title": "5 Experiments", "content": "To evaluate the effectiveness of our approach, we trained MSD with different design choices and\ncompared against competing methods, including other single-step distillation methods.\nIn Sec. 5.1, we compare single vs multiple students on a 2D toy problem for direct visual com-\nparison. For these experiments, we used only the DM stage. In Sec. 5.2, we investigate class-\nconditional image generation on ImageNet-64\u00d764 [11] where we have naturally defined classes to\npartition. Here, we explored training with the DM stage only, with both DM and ADM stages, and\nwith all three stages for smaller students. We then evaluate MSD for a larger model in Sec. 5.3.\nWe explored text-to-image generation on MS-COCO2014 [34] with varying training stages. We\nuse the standard Fr\u00e9chet Inception Distance (FID) [18] score to measure generation quality. Com-\nprehensive comparisons confirm that MSD outperforms single-student counterparts and achieves\nstate-of-the-art performance in single-step diffusion distillation. Finally, in Sec. 5.4, we summarize\nour ablation experiments over design choices.\nTo focus on the performance boost from multi-student distillation, we applied minimal changes to\nthe hyperparameters used by Yin et al. [75, 74] for their distribution matching distillation imple-\nmentations. More details on training and evaluation can be found in the App. B and C."}, {"title": "5.1 Toy Experiments", "content": "In Fig. 4, we show the sample density of MSD with DM stage for a 2D toy experiment, where\nthe real data distribution has 8 classes, and each class is a mixture of 8 Gaussians. We used a\nsimple MLP with EDM schedules to train the teacher and then distill into 1, 2, 4, and 8 students for\ncomparison. From the displayed samples and the $l_1$ distance from teacher generation, we observe\nthat the collective generation quality increases as the number of students increases."}, {"title": "5.2 Class-conditional Image Generation", "content": "Student architecture the same as the teacher: We trained $K = 4$ students using the MSD frame-\nwork and the EDM [25] teacher on class-conditional ImageNet-64\u00d764 generation. We applied the\nsimplest strategy for splitting classes among students: Each student is responsible for 250 consec-\nutive classes in numerical order. We compare the performance with previous methods and display\nthe results in Tab. 1. Our DM stage, which uses the complementary regression loss, surpasses\nthe one-student counterpart DMD [75], achieving a modest drop of 0.25 in FID score, making it\na strong competitor in single-step distillation without an adversarial loss. We then took the best\npretrained checkpoints and trained with the ADM stage. The resulting model achieved the current\nstate-of-the-art FID score of 1.20. It surpasses even the EDM teacher, StyleGAN-XL [57], the\nmulti-step RIN [23] due to the adversarial loss. Fig. 5(a) and (b) display a comparison of sample\ngenerations, showing that our best students have comparable generation quality as the teacher.\nStudent architecture smaller than the teacher: Next, we trained 4 smaller student models with\nthe prepended teacher score matching (TSM) stage from Sec. 4.3. This achieved a 42% reduction\nin model size and a 7% reduction in latency, with a slight degradation in FID score, offering a\nflexible framework to increase generation speed by reducing student size, and increasing gener-\nation quality by training more students. Fig. 5(c) displays sample generation from these smaller\nstudents, whereas Fig. 5(d) shows sample generations from an even smaller set of students, with a\n71% percent reduction in model size and a 23% percent reduction in latency. We observed slightly\ndegraded but still competitive generation qualities. Using more and larger students will further\nboost performance, as shown by ablations in Sec. 5.4 and App. A.4. Smaller students without the\nTSM stage fail to reach competitive performance, indicating its necessity."}, {"title": "5.3 Text-to-Image Generation", "content": "Student architecture the same as the teacher: We evaluated the performance of text-to-image\ngeneration using the MS-COCO2014 [34] evaluation dataset. We distilled 4 students from Stable\nDiffusion (SD) v1.5 [54] on a 5M-image subset of the COYO dataset [8]. For splitting prompts\namong students, we again employed a minimalist design: pass the prompts through the pre-trained\nSD v1.5 text encoder, pool the embeddings over the temporal dimension, and divide into 4 subsets\nalong 4 quadrants. We trained with a classifier-free guidance (CFG) scale of 1.75 for best FID\nperformance. Tab. 2 compares the evaluation results with previous methods. Our baseline method\nwith only the DM stage again achieved a performance boost with a 0.48 drop in FID over the\nsingle-student counterpart DMD2 without adversarial loss [74]. Continuing the ADM stage from\nthe best checkpoint yielded a terminal FID of 8.20, again surpassing the single-student counterpart\nand achieving the current state-of-the-art FID score. In addition, for better visual quality, we also\ntrain with a larger CFG scale of 8, and display corresponding samples in Fig. 2(b) and App. D.2.\nStudent architecture smaller than the teacher: As a preliminary exploration, with the prepended\nteacher score matching (TSM) stage, we train a 83% smaller and 5% faster student on a dog-related\nprompt subset of COYO (containing ~ 1210000 prompts). We trained with a CFG scale of 8 and\ndisplay the samples in Fig. 2. We observed fair generation quality despite a significant drop in\nmodel size. Improved training is likely to obtain better sample quality and generalization power."}, {"title": "5.4 Ablation Studies", "content": "Here, we ablate the effect of different components in MSD and offer insight into scaling. Unless\notherwise mentioned, all experiments are conducted for class-conditional generation ImageNet-\n64\u00d764, using only the DM stage for computational efficiency.\nMSD is still better with the same effective\nbatch size. To investigate if the performance\nboost from MSD comes from only a batch size\nincrease over single student distillation, we make\na comparison with the same effective batch size.\nAs showcased in Tab. 3, MSD with 4 students and\na batch size of 32 per student performs slightly\nbetter than the single-student counterpart with a\nbatch size of 128, indicating that MSD likely ben-\nefits from a capacity increase than a batch size\nincrease. As a takeaway, with a fixed training re-\nsource measured in processed data points, users\nare better off distilling into multiple students with\npartitioned resources each than using all resources to distill into a single student. Although multi-\nple students means multiple model weights to save, storage is often cheap, so in many applications,\nthis cost is outweighed by our improved quality or latency.\nSimple splitting works surprisingly well. We used consecutive splitting of classes in Sec. 5.2.\nIn the pretrained EDM model, class labels are fed to the diffusion model via the label embedding\nlayer. Therefore, we investigated another strategy where we performed a $K$-means clustering\n$(K = 4)$ on the label embeddings, resulting in 4 clusters of similar sizes: (230,283,280,207).\nHowever, MSD trained with these clustered partitions performs similarly to sequential partition, as\nshown in Tab. 3. For text-to-image generation, we performed $K$-means clustering on the pooled\nembeddings of prompts in the training data, resulting in clusters of vastly uneven sizes. Due to\ncomputational limitations, we opted for the simpler partition strategies outlined in Sec. 5."}, {"title": "Effect of scaling the number of students.", "content": "In Tab. 3, we study the effect of increasing $K$, the\nnumber of distilled students. We kept the per-student batch size fixed so more students induce\na larger effective batch size. We observe better FID scores for more students. We hypothesize\nthat better training strategies, such as per-student tuning, will further improve the quality. Optimal\nstrategies for scaling to ultra-large numbers of students is an interesting area for future work."}, {"title": "6 Discussion", "content": null}, {"title": "6.1 Limitations", "content": "MSD is the first work to explore diffusion distillation with multiple students, and it admits a few\nlimitations that call for future work. 1) Further explorations could offer more insights into optimal\ndesign choices for a target quality and latency on various datasets, such as the number of students,\ninput condition size for each student, and other hyperparameters. This is especially beneficial if\nthe training budget is limited. 2) We apply simple partitioning for both class- and text-conditions\nand assign them disjointly to different students. Although our empirical study shows that simple\nalternatives do not offer obvious advantages, more sophisticated routing mechanisms may help. 3)\nWe use simple channel reduction when designing smaller students to demonstrate feasibility. This\nresults in a significantly smaller latency reduction than sample size reduction. Exploring other\ndesigns of smaller students will likely increase their quality and throughput. 4) We train different\nstudents separately, but we expect that carefully designed weight-sharing, loss-sharing, or other\ninteraction schemes can further enhance training efficiency. 5) We hypothesize that MSD can be\napplied to other diffusion distillation methods and other modalities for similar benefits but leave\nthis for future work."}, {"title": "6.2 Conclusion", "content": "This work presented Multi-Student Distillation, a simple yet efficient method to increase the ef-\nfective model capacity for single-step diffusion distillation. We applied MSD to the distribution\nmatching and adversarial distillation methods. We demonstrated their superior performance over\nsingle-student counterparts in class-conditional and text-to-image generations. Particularly, MSD\nwith DMD2's the two-stage training achieves state-of-the-art FID scores. Moreover, we success-\nfully distilled smaller students from scratch, demonstrating MSD's potential in further reducing\nthe generation latency with multiple smaller student distillations. We envision building on MSD to\nenable generation in real-time, enabling many new use cases."}, {"title": "A Additional ablation studies", "content": null}, {"title": "A.1 Training curves for Sec. 5.4", "content": null}, {"title": "A.2 The effect of paired dataset size on DMD", "content": "In Sec. 4.2, we mentioned the special filtering strategy for MSD at DM stage: instead of parti-\ntioning the paired dataset for corresponding classes, we choose to keep the same complete dataset\nfor each student. Fig. 7 demonstrates that the alternative strategy discourages mode coverage and\nleads to a worse terminal performance."}, {"title": "A.3 Single-step distillation methods comparison", "content": "Table 5 justifies our choice of DMD/DMD2 as our first-stage training without adversarial loss.\nCompetitor methods suffer from training instability (SiD, FID fluctuates more than 100%), larger\ntraining data size (EMD), or worse quality (CTM and other CM-based methods). DMD/DMD2,\non the other hand, strike a good balance. We noticed DMD exhibits more stability for MSD on\nImageNet, whereas DMD2 performs better for SD v1.5, which leads to our respective choices. As\npointed out in App. B, we used a smaller-sized paired dataset (10000 images) than the original\nDMD paper (25 000 images) for ImageNet, which significantly accelerated convergence without\nimpairing the final performance. Moreover, as pointed out in Sec. 4.2, the same paired dataset can\nbe used for all students, eliminating potential additional computation."}, {"title": "A.4 More results on distilling into smaller students", "content": "In Sec. 5.2, we trained MSD4-ADM on smaller students to demonstrate the tradeoff between gen-\neration quality and speed. Here, we make a more comprehensive ablation study on the interplay\nbetween student size, number of classes covered, and training stage, with results displayed in\nFig. 8. We observe that generation quality increases with student size and decreases with more\nclasses covered. MSD offers great flexibility for users to make these choices based on computa-\ntional resources, generation quality, and inference speed requirements."}, {"title": "B Implementation Details", "content": null}, {"title": "B.1 Toy Experiments", "content": "The real dataset is a mixture of Gaussians. The radius for the outer circle is 0.5, the radius for the\n8 inner circles is 0.1, and the standard deviation for each Gaussian (smallest circle) is 0.005. The\nteacher is trained with EDM noise schedule, where use $(\\sigma_{min}, \\sigma_{max}) = (0.002, 80)$ and discretized\nthe noise schedule into 1000 steps. We train the teacher for 100000 iterations with AdamW [38]\noptimizer, setting the learning rate at 1e-4, weight decay to 0.01, and beta parameters to (0.9,\n0.999). For distillation, we first generated dataset of 1000 pairs, then used DMD [75] to train 1,\n2, 4, and 8 students, respectively, all for 200 000 iterations, with reduced learning rate at 1e-7. We\nonly sample the first 750 of the 1000 steps for distillation.\nEach subfigure of Fig. 4 is a histogram with 200 bins on 100 000 generated samples, using a custom\ncolormap. The loss is the mean absolute difference of binned histogram values."}, {"title": "B.2 ImageNet", "content": null}, {"title": "B.2.1 Same-sized students", "content": "Our ImageNet experiments setup closely follows DMD [75", "74": "papers. We distill\nour one-step generators using the EDM [25", "edm-imagenet-64x64-cond-adm": "We\nuse $\\sigma_{min} = 0.002$ and $\\sigma_{max} = 80$ and discretize the noise schedules into 1000 bins. The weight\n$w_t$ in Eq. (2) is set to $\\frac{\\alpha_t || \\mu_{\\text{teacher}} (x_t, t) - x||_1}{CS}$, where $S$ is the number of spatial locations and $C$ is the\nnumber of channels, and the weight $\\lambda_t$ in Eq. (4) is set to $(\\sigma_t^2 + 0.5^2)/(\\sigma_t^2 \\cdot 0.5"}]}