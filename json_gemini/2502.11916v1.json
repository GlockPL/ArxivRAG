{"title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models", "authors": ["Jiamin Su", "Yibo Yan", "Fangteng Fu", "Han Zhang", "Jingheng Ye", "Xiang Liu", "Jiahao Huo", "Huiyu Zhou", "Xuming Hu"], "abstract": "Automated Essay Scoring (AES) plays a crucial role in educational assessment by providing scalable and consistent evaluations of writing tasks. However, traditional AES systems face three major challenges: reliance on handcrafted features that limit generalizability, difficulty in capturing fine-grained traits like coherence and argumentation, and inability to handle multimodal contexts. In the era of Multimodal Large Language Models (MLLMs), we propose ESSAYJUDGE, the first multimodal benchmark to evaluate AES capabilities across lexical-, sentence-, and discourse-level traits. By leveraging MLLMs' strengths in trait-specific scoring and multimodal context understanding, ESSAYJUDGE aims to offer precise, context-rich evaluations without manual feature engineering, addressing longstanding AES limitations. Our experiments with 18 representative MLLMs reveal gaps in AES performance compared to human evaluation, particularly in discourse-level traits, highlighting the need for further advancements in MLLM-based AES research. Our dataset and code will be available upon acceptance.", "sections": [{"title": "1 Introduction", "content": "Automated Essay Scoring (AES) has become an essential tool in educational assessment, providing efficient and consistent scoring for large-scale writing tasks (Ye et al., 2025; Ramesh and Sanampudi, 2022; Li and Liu, 2024; Wu et al., 2024; Xia et al., 2024). While AES systems have significantly reduced the workload of human graders, they still face challenges in delivering accurate and detailed evaluations, particularly for trait-specific scoring, which assesses individual aspects of writing quality, such as coherence, creativity, and argumentation (Song et al., 2024; Pack et al., 2024; Ruseti et al., 2024). Such detailed feedback is critical for guiding students in improving their writing skills, but remains difficult to achieve with existing methods.\nTraditional AES approaches, including statistical models such as Support Vector Machines, rely heavily on handcrafted features such as word frequency and essay length (Yang et al., 2024; Jansen et al., 2024; Uto et al., 2020). As illustrated in Figure 1 (a), they often suffer from \u25cf relying on manually engineered features, thus limiting the generalizability across diverse data; failing to model fine-grained traits such as logical structure and argument persuasiveness; \u2192 inability to handle multimodal context, thus struggling to deliver comprehensive and context-aware evaluations (Lim et al., 2021; Uto, 2021; Wang et al., 2022).\nThe emergence of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) offers a promising solution to these chal-"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 AES Datasets", "content": "Existing AES datasets have advanced the field but remain some limitations (shown in Table 1) (Ke and Ng, 2019; Li and Ng, 2024b,a). For example, ASAPAES is notable for its size, enabling high-performance prompt-specific systems (Cozma et al., 2018). However, differing score ranges across prompts and heavy preprocessing (e.g., removal of paragraph structures and named entities) reduce its utility. ASAP++ is an extension of ASAP that introduces trait-specific scores (Mathias and Bhattacharyya, 2018; Li and Ng, 2024a). However, its traits are coarse-grained, with all content-based traits (e.g., coherence, persuasiveness, and thesis clarity) grouped into a single \"CONTENT\" category. The CLC-FCE dataset includes holistic scores and linguistic error annotations, supporting grammatical error detection alongside scoring tasks, but the small number of essays per prompt hinders the development of prompt-specific systems (Yannakoudakis et al., 2011; Li and Ng, 2024b). TOEFL11 dataset focuses on native language identification and provides only coarse-grained proficiency labels (low, medium & high), which do not fully capture essay quality. ICLE"}, {"title": "2.2 AES Systems", "content": "AES research focuses on three main categories: heuristic approaches, machine learning approaches, and deep learning approaches (Li and Ng, 2024a). Heuristic AES approaches focus on holistic scoring by combining trait scores such as Organization, Co-herence, and Grammar into a weighted sum. Trait-specific scores are computed using rules, like assessing Organization based on a five-paragraph format (Attali and Burstein, 2006). Machine learning approaches (e.g., Logistic Regression and Support Vector Machine) rely on handcrafted features, such as lexical (Chen and He, 2013), length-based (Vajjala, 2016; Yannakoudakis and Briscoe, 2012), and discourse features (Yannakoudakis and Briscoe, 2012), and perform well in within-prompt scoring but struggle with generalization to new prompts. Deep learning approaches, particularly those using Transformer architectures like BERT (Wang et al., 2022), have advanced AES by learning essay representations directly from text, enabling multi-trait and cross-prompt scoring. Among these, LLM-based approaches stand out for their ability to leverage commonsense knowledge and understand complex instructions (Mizumoto and Eguchi, 2023). By using prompts, LLMs can perform AES in zero-shot settings with rubrics alone (Lee et al., 2024) or in few-shot settings with minimal labeled data (Mansour et al., 2024; Xiao et al., 2024a). These methods enhance flexibility, scalability, and performance, especially in low-resource scenarios."}, {"title": "2.3 Multimodal Large Language Models", "content": "MLLMs have brought significant advancements to diverse tasks and applications (Xi et al., 2023; Huo et al., 2024; Yan et al., 2024d; Yan and Lee, 2024; Zou et al., 2025; Dang et al., 2024). Proprietary MLLMs such as GPT-40 (Hurst et al., 2024) and Gemini-1.5 (DeepMind, 2024b) have shown remarkable capabilities in multimodal challenges, excelling in areas such as multimodal reasoning and QA (Chang et al., 2024; Yan et al., 2024c,b; Zheng et al., 2024; Yan et al., 2025). At the same time, Open-source MLLMs have made considerable strides. For instance, LLaVA-NEXT (Liu et al., 2024) utilizes a pretrained vision encoder to generate visual embeddings, which are then aligned with text embeddings through a lightweight adapter, enabling effective multimodal understanding. Similarly, MLLMs such as Qwen2-VL (Wang et al., 2024), DeepSeek-VL (Lu et al., 2024a), InternVL (Chen et al., 2024, 2025), MiniCPM (Hu et al., 2024), Ovis (Lu et al., 2024b), LLaMA3 (Dubey et al., 2024) and Yi-VL (Young et al., 2024) implement innovative projection techniques to combine visual and textual features effectively, enabling many multimodal applications. These models showcase the growing potential of MLLMs in advancing both research and practical applications that rely on multimodal data (Qu et al., 2025; Zou et al., 2024; Zhou et al., 2024; Huang et al., 2024). Therefore, we introduce ESSAYJUDGE, a novel benchmark designed to evaluate MLLMs' capability to score essays with multimodal context, paving the way for AGI systems (Xiao et al., 2024b; Tate et al., 2024; Yan et al., 2024a)."}, {"title": "3 Dataset", "content": ""}, {"title": "3.1 Data Collection", "content": "This section describes the process of constructing our dataset to ensure high-quality data for analysis, as illustrated in Figure 2. Unlike traditional datasets that often rely on publicly available sources or textbook modifications, the data for this study originates from a K-12 Education Organization. This organization provides a repository of essays graded by experienced educators, guaranteeing the credibility and reliability.\nFrom the original dataset, four primary fields were retained: (i) Image, which contains the image of the writing Topics; (ii) Question, which contains the text of the writing prompt; (iii) Essay, representing the student's written work; (iv) Overall Score, reflecting the final assessment provided by professional educators. To enhance the dataset's quality, a series of processing and cleaning steps were applied. These steps included removing essays with incomplete or low-quality responses and selecting topics that met the criteria for reliability and diversity. Through this rigorous process, we curated the ESSAYJUDGE dataset consisting of 1,054"}, {"title": "3.2 Data Annotaion Scheme", "content": "Through discussion with English teachers and linguistic experts, we identified ten traits of essays and categorized them into three levels of granularity, progressing from fine-grained lexical features to broader sentence-level structures and, finally, to discourse-level characteristics. This hierarchical structure reflects a natural flow from the smallest units of language to the overall coherence and persuasiveness of an essay, providing a comprehensive framework for evaluation. The rubrics (with a scale of 0 to 5) can be seen in Appendix A. Higher scores indicate a stronger performance.\nAt the lexical level, the focus is on the precision and diversity of word usage, which forms the foundation of effective expression. Traits including lexical accuracy and lexical diversity assess how well the writer uses vocabulary to convey meaning, including the correctness of word choice, spelling, and semantic appropriateness, as well as the variety and richness of vocabulary demonstrated in the essay. These fine-grained features ensure that the language is both accurate and expressive.\nMoving to the sentence level, the evaluation shifts to the internal quality of sentences and the connections between them. Traits including grammatical accuracy and grammatical diversity examine the correctness and variety of grammatical structures, reflecting the writer's ability to construct well-formed and diverse sentences. Punctuation accuracy ensures that punctuation marks are used appropriately to enhance clarity and readability. Additionally, coherence is assessed at this level, focusing on how smoothly sentences connect through effective transitions, logical relationships, and appropriate use of conjunctions. This intermediate granularity highlights the writer's"}, {"title": "3.3 Datasets Annotation Procedure", "content": "To ensure a thorough and objective evaluation of the traits, we enlisted two experienced experts in English education, who independently assessed all 10 traits for each essay. After scoring, we compared the results and calculated the differences between the two sets of scores. For traits where the score difference was less than or equal to 1, we took the average of the two scores to establish the ground-truth score. In cases where the score difference exceeded 1, we asked another independent team consisting of three senior annotators to review the essays and discuss the traits with the team. They finally reached a consensus on the final ground-truth score, ensuring a fair and reliable outcome."}, {"title": "3.4 Data Details", "content": "ESSAYJUDGE dataset comprises a substantial collection of 1,054 multimodal essays designed for AES (See details in Appendix B). The dataset is categorized based on the number of images per question, with 66.7% being single-image questions and the remaining 33.3% multi-image questions."}, {"title": "4 Experiment and Analysis", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Evaluation Groups. We meticulously categorized diverse MLLMs into distinct groups to assess their capabilities in trait-specific AES. (i) The Open-Source MLLMs category encompassed models such as Yi-VL (Young et al., 2024), Qwen2-VL (Wang et al., 2024), DeepSeek-VL (Lu et al., 2024a), LLaVA-NEXT (Liu et al., 2024), InternVL2 (Chen et al., 2024), InternVL2.5 (Chen et al., 2025), MiniCPM-V2.6 (Hu et al., 2024), MiniCPM-LLaMA3-V2.5 (Hu et al., 2024), Ovis1.6-Gemma2 (Lu et al., 2024b), and LLaMA-3.2-Vision (Dubey et al., 2024), each demonstrating their unique strengths and capabilities in multi-granular essay scoring. (ii) The Closed-Source MLLMs featured proprietary models like Qwen-Max (Team, 2024), Step-1V (StepFun, 2024), Gemini-1.5-Pro (DeepMind, 2024b), Gemini-1.5-Flash (DeepMind, 2024a), Claude-3.5-Haiku (Anthropic, 2024a), Claude-3.5-Sonnet (Anthropic, 2024b), GPT-40-mini (OpenAI, 2024), and GPT-40 (Hurst et al., 2024), providing a comparison point for the performance of models that are not publicly accessible. (iii) Lastly, the Human Performance category served as a benchmark for human-level intelligence, enabling us to assess how closely MLLMs emulate human cognitive abilities (More details in Appendix C.2). The detailed prompts for MLLMs and sources of MLLMs are provided in Appendix C.3 and C.4.\nEvaluation Metric. We employ Quadratic Weighted Kappa (QWK) (Ke and Ng, 2019; Li and Ng, 2024b,a) as our metric for scoring the similarity, which is widely used to evaluate the agreement between model scores and the ground truth. Its formula is expressed as:\n$\\kappa = 1 - \\frac{\\Sigma_{i,j} W_{i,j} O_{i,j}}{\\Sigma_{i,j} W_{i,j} E_{i,j}}$, where $w_{i,j} = \\frac{(i-j)^2}{(N-1)^2}$ is the weight matrix penalizing larger differences between i and j, $O_{i,j}$ is the observed agreement, and $E_{i,j}$ is the expected agreement under random chance. QWK values range from -1 (complete disagreement) to 1 (perfect agreement). Higher values are expected."}, {"title": "4.2 Main Results", "content": "Closed-source MLLMs demonstrate significant superiority over open-source MLLMs in essay scoring tasks, with GPT-40 achieving the strongest overall performance. Table 2 illustrates that closed-source MLLMs consistently outperform open-source MLLMs across ten traits. This advantage is likely due to the high-quality proprietary datasets and advanced training techniques leveraged by closed-source models (Yu et al., 2024; Wang et al., 2023), enabling them to achieve more balanced and robust performance. GPT-40 stands out as the best-performing model, achieving the highest QWK scores across multiple traits except for Argument Clarity, highlighting its exceptional capability. Among open-source MLLMS, InternVL2 emerges as the best performer. However, its overall performance remains behind closed-source ones, underscoring the gap between open-source and closed-source models in terms of capturing complex and evaluative subtleties.\nClosed-source MLLMs exhibit distinct scoring patterns compared to open-source counterparts, characterized by greater score variability and stricter adherence to rubrics. As revealed in Figure 3, closed-source models demonstrate significantly higher score variance (0.49 vs. 0.34), suggesting enhanced capacity to differentiate essay quality through broader score distribution across performance levels. This contrasts with open-source models' tendency to cluster scores in the mid-range (3-4 points), reflecting limited discriminative capacity for quality extremes. As shown in Figure 5, the scoring rigor of closed-source systems is further evidenced by their consistent assignment of lower scores across critical traits including Argument Clarity, Coherence, and Linguistic Features, with human ratings typically intermediate between the two model types. This systematic conservatism stems from closed-source models' strict adherence"}, {"title": "4.3 Trait-Specific Analysis", "content": "Closed-source MLLMs perform poorly in evaluating Argument Clarity and Essay Length while excelling at lexical-level assessment. As shown in Figure 6, their low QWK values for Argument Clarity and Essay Length are because closed-source LLMs rely heavily on surface-level features like grammar and vocabulary, making them ineffective in handling complex arguments that require contextual understanding and reasoning. For Essay Length, they often hallucinate word counts (Rawte et al., 2023) and over-rely on quantitative measures, leading to an overvaluation of verbosity and an undervaluation of concise but effective writing (Jeon and Strube, 2021). In contrast, closed-source MLLMs' strong lexical performance is due to exposure to a extensive, high-quality datasets (Shi et al., 2023), enabling superior precision and variety in lexical choice.\nMLLMS demonstrate outstanding performance in assessing Coherence when grading essays related to line charts. For example, as shown in Figure 7, GPT-4o achieves a high QWK value of 0.89 for the coherence trait when evaluat-"}, {"title": "4.4 Analysis of #image Setting", "content": "Most closed-source MLLMs perform better in evaluating essays with single-image setting. As shown in Figure 8, GPT-40 performed better on single-image tasks except for JP and GD traits. Appendix E shows that among all evaluated closed-source MLLMs, only three models do not follow this pattern. Single-image tasks are simpler and more focused, typically requiring students to describe one logical theme. This clear structure makes it easier for models to capture key information and provide accurate evaluations, without the need for complex comparisons or logical integration across multiple images. In contrast, multi-image tasks involve comparing, relating, and integrating data from multiple sources, which increases task complexity and the likelihood of errors in both student responses and model evaluations.\nFor Justifying Persuasiveness, most MLLMS perform better when evaluating essays with multi-image setting. We selected the top-performing eight models from both open-source and closed-source MLLMs, as shown in Figure 9 and Figure 10. Unlike most traits, where MLLMS tend to perform better on single-image tasks, the evaluation of Justifying Persuasiveness shows a distinct advantage in multi-image settings. This may be because multi-image tasks inherently provide richer and more diverse data points, enabling students to construct more evidence-based arguments."}, {"title": "4.5 Case Study", "content": "Figure 11 shows the detailed multi-granular evaluation for one of the essays. Additional examples are shown in Appendix F. Specifically, we can find that argument clarity is the most discrepant from the ground truth. Argument clarity is crucial in AES, as it directly reflects whether the author's central ideas are in alignment with the essay requirement (Falk and Lapesa, 2023). Leading models like GPT-40 show relatively poor performance in assessing argument clarity, which is illustrated in Figure 2. However, argument clarity serves as a key indicator of a model's reasoning abilities and its capacity to integrate and process complex information, including visual elements. The ability to clearly present and logically connect ideas is essential for both multimodal understanding and reasoning, making it a critical benchmark for evaluating the AES performance of MLLMs. Addressing these challenges in future MLLMs could significantly improve their ability to assess essays with complex reasoning and enhance their multimodal integration capabilities."}, {"title": "5 Conclusion", "content": "In this work, we presented ESSAYJUDGE, the first multimodal benchmark designed to evaluate the AES capabilities of MLLMs across lexical, sentence, and discourse-level traits. Addressing longstanding limitations in traditional AES approaches, ESSAYJUDGE leverages MLLMs' inherent strengths in contextual understanding and multimodal analysis, enabling more precise, trait-specific evaluations without reliance on handcrafted features. Our comprehensive evaluation of 18 representative MLLMs highlights current limitations of MLLM-based AES systems. Notably, closed-source MLLMs such as GPT-4 demonstrate superior performance compared to open-source counterparts, yet a significant gap remains in achieving human-level accuracy.\nWe envision that ESSAYJUDGE will not only drive innovation in AES but also serve as a stepping stone toward broader applications of MLLMs in educational assessment and beyond. The research community can address the challenges identified and foster the development of more accurate and interpretable AES systems towards AGI."}, {"title": "Limitations", "content": "Despite the findings we demonstrate in our work, there still exist minor limitations:\n1. The datasets used in this study primarily consist of essays written by non-native speakers of English, making it unclear whether our conclusions apply to essays written by native speakers, such as those in the ASAP dataset. However, since our rubrics are broadly applicable and not designed specifically for non-native speakers, we believe the conclusions can be generalized to essays written by native speakers as well.\n2. Although our study covers diverse topics, including healthcare, biology, demographics, environment, education and so on, there is still a demand for a more generalized benchmark. Further expansion is needed to address a wider variety of writing contexts and disciplines, ensuring its broader applicability across different writing tasks."}]}