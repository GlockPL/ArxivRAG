{"title": "Training Agents with Weakly Supervised Feedback from Large Language Models", "authors": ["Dihong Gong", "Pu Lu", "Zelong Wang", "Meng Zhou", "Xiuqiang He"], "abstract": "Large Language Models (LLMs) offer a promising basis for creating agents that can tackle complex tasks through iterative environmental interaction. Existing methods either require these agents to mimic expert-provided trajectories or rely on definitive environmental feedback for reinforcement learning which limits their application to specific scenarios like gaming or code generation. This paper introduces a novel training method for LLM-based agents using weakly supervised signals from a critic LLM, bypassing the need for expert trajectories or definitive feedback. Our agents are trained in iterative manner, where they initially generate trajectories through environmental interaction. Subsequently, a critic LLM selects a subset of good trajectories, which are then used to update the agents, enabling them to generate improved trajectories in the next iteration. Extensive tests on the API-bank dataset show consistent improvement in our agents' capabilities and comparable performance to GPT-4, despite using open-source models with fewer parameters.", "sections": [{"title": "1 Introduction", "content": "The AI community has long pursued the goal of creating agents that can perform a broad range of tasks across various environments at a level comparable to humans, and substantial efforts have been invested in this direction (Nakano et al., 2021; Chen et al., 2022; Gur et al., 2023; Xi et al., 2023; Schick et al., 2024; Shen et al., 2024). Similar to human learning, an agent begins by learning basic knowledge and skills through imitation(Tang et al., 2023; Zeng et al., 2023; Patil et al., 2023). As the agent advances, it is anticipated to continually learn and adapt to new tasks by interacting with diverse environments(Zhang et al., 2024).\nLarge language models (LLMs) are viewed as a promising basis for building such versatile agents due to their generalization capabilities, and numerous initiatives have been undertaken in this area. One approach primarily depends on human supervision, where LLM-based agents emulate expert-provided trajectories from various environments (Chen et al., 2023; Gou et al., 2023; Chen et al., 2024; Zeng et al., 2023; Xu et al., 2023). While effective, this method relies on skilled annotators and substantial financial resources, making scalability a challenge.\nAnother research direction enables LLM-based agents to self-improve through environmental feedback, thereby reducing the reliance on human supervision (Wang et al., 2023; Huang et al., 2022; Kadl\u010d\u00edk et al.; Shao et al., 2024). This approach is rooted in the concept of reinforcement learning(Sutton and Barto, 2018), where agents learn to make decisions by experiencing the consequences of their actions. In this context, environmental feedback serves as a form of reward or penalty, guiding the agent towards optimal behavior. For instance, in coding tasks(Luo et al., 2023; Jiang et al., 2023; Ni et al., 2023), the agent might receive positive feedback when it writes a piece of code without errors, and negative feedback when the code fails to compile. Similarly, in gaming scenarios(Schrittwieser et al., 2020; Berner et al., 2019; Vinyals et al., 2019), the agent could be rewarded for winning a game or achieving a high score, and penalized for losing or performing poorly. However, a significant limitation of this approach is that it typically requires decisive environmental feedback. As a result, this method is often confined to narrow, well-defined tasks such as coding, gaming, or mathematical calculations(Huang et al., 2022; Wang et al., 2023; Kadl\u010d\u00edk et al.; Liao et al., 2024), where the results of the agent's actions can be unequivocally determined. This restricts the applicability of such agents, as many real-world tasks involve ambiguous outcomes and complex, multi-dimensional feedback."}, {"title": "2 Method", "content": "Our agents adapt to their environment through a series of iterative learning procedures. As depicted in Figure 1, given a set of instructions, the agents initially explore the environment involving multi-turn interactions with the environment, resulting in a set of trajectories. These trajectories are evaluated by the critic module. Higher scores are allocated to better trajectories. The scored trajectories are subsequently fed into the trainer module. This procedure is repeated over several rounds, allowing the agents to adapt to the environment progressively."}, {"title": "2.2 Trajectory Sampling", "content": "The trajectory sampling procedure empowers the agent to explore the environment comprehensively. The agent, also referred to as the 'actor' in Figure 1, generates K trajectories for each instruction. Consequently, for a set of N instructions,, this procedure yields a total of N \u00d7 K trajectories.\nT = {I^{(n,k)}, A^{(n,k)}_1, O^{(n,k)}_1, ... , A^{(n,k)}_M, O^{(n,k)}_M | n = 1 ... N, k = 1... K}\nIn the above equation, I^{(n,k)} represents the n th instruction used for generating its corresponding k- th trajectory. The symbol A represents the action (e.g. API call) generated by agent, while O denotes the observation (e.g. result of the API call). The maximum number of interaction steps is capped at M. A simplified trajectory is depicted in Figure 1. For a detailed example of a full trajectory, including the prompt construction, please refer to the Appendix. This trajectory sampling procedure is a crucial part of our methodology, enabling the agent to learn from a wide range of experiences and interactions within the environment."}, {"title": "2.3 Critic Module", "content": "The primary objective of this step is to identify a subset of trajectories that can be instrumental in enhancing the future performance of the agents. Each trajectory typically involves a multi-round interaction with the environment, where the agent responds to a given instruction through a series of API calls. Errors can occur at various stages along the trajectory, including API selection, API calling parameters, exception handling, and conclusion. This complexity makes it challenging to accurately assess the quality of individual trajectories.\nTo address this challenge, we have incorporated two key design considerations. Firstly, we utilize a Large Language Model (LLM) as the foundation for the critic module and design sophisticated prompts. This approach allows the critic module to evaluate the trajectories in a comprehensive manner. Secondly, we adopt a progressively iterative learning approach from environmental interactions. This strategy minimizes the risk of learning from poor-quality trajectories provided by the critic module. The detailed prompt for the critic is included in the Appendix section of this paper."}, {"title": "2.4 Supervised Fine-Tuning", "content": "In this stage, the agents adjust their model parameters in response to the training dataset. This adaptation process allows the agents to refine their performance, leading to the generation of higher-quality trajectories in subsequent rounds.\nTraining Data. During each round, we generate a collection of trajectories through interaction with the environment, which are then evaluated in the subsequent critic module. In this step, we select a subset comprising the top p% of trajectories based on their ratings. It's important to note that trajectories previously used for training in earlier iterations are excluded from selection. Additionally, we incorporate a set of chat data from the general domain to prevent overfitting. We maintain a 1:1 ratio between the trajectory data and the general data.\nTraining Loss. We minimize the negative log-likelihood loss defined as follows:\narg min_\u03b8 \u03a3_{n=1}^N \u03a3_{k=1}^K \u03a3_{m=1}^M -log P_\u03b8(A^{(n,k)}_m | I^{(n,k)}, A^{(n,k)}_1, O^{(n,k)}_1 ... A^{(n,k)}_{m-1}, O^{(n,k)}_{m-1})"}, {"title": "2.5 Implementation Details", "content": "For trajectory sampling, we sample 5 trials for each instruction, and each trial allows at most 5 rounds. For critic module top 10% trajectories of highest scores are selected for training the agents. For model training, we set initial learning rate of 5e-5 and gradually reduces to 5e-6 using cosine annealing schedule with no warm-up steps when performing SFT. The Yi-34B (Young et al., 2024) model is used to implement the critic module."}, {"title": "3 Experiment", "content": "We have selected the API-bank (Li et al., 2023) as our benchmark dataset due to its extensive coverage of over 1,000 domains and more than 2,000 APIs in total. These APIs encompass a wide range of applications, including web search, health, calculation, weather forecasting, and more. For the purposes of this study, we utilize all 315 questions from the original dataset with released API implementations. Of these, 220 are employed for the agent evolution learning process, while the remaining 95 questions are set aside for performance evaluation. We present the accuracy based on the evaluation of these 95 questions. The accuracy is determined by the correctness of the answers, which are reviewed and verified by human evaluators."}, {"title": "3.2 Ablation Study", "content": "In this part, we evaluate effectiveness of individual components of the system including the trajectory sampling, critic module and performance of evolution. The actor employs the Yi-6B(Young et al., 2024) model for this study.\nPerformance of Critic Module. The primary role of the critic module is to identify a subset of trajectories that can potentially enhance the future performance of the agents. We evaluate the accuracy of this module by randomly selecting a number of samples rated by the critic and subjecting them to human evaluation. This allows us to assess the consistency between the critic's ratings and those of the human evaluator. Our critic module achieves a precision of 70.0% and a recall of 97.2%. These results suggest that the critic is effective in retaining most of the beneficial trajectories. However, the lower precision underscores the necessity of our iterative learning design, where only a minimal number of samples with the highest confidence scores are utilized for training."}, {"title": "Performance of Evolution.", "content": "Our agents evolve progressively over training iterations. To investigate the performance gain over iterations, we plot the accuracy w.r.t. different number of iterations for different base models with different model sizes, as shown in Figure 2. These results clearly confirms a steady improvement in accuracy over iterations, for both Yi-6B (Young et al., 2024) and Llama2-13B(Touvron et al., 2023b)."}, {"title": "3.3 Benchmark", "content": "In this section, we conduct experiments to compare with commercial GPT-4 model and the following open-source methods in the literature:\nReAct-6B. We implement the ReAct(Yao et al., 2022) method with the same Yi-6B as base LLM. The detailed prompt is included in Appendix.\nAPI-Bank-7B. Checkpoint model released by the original research paper (Li et al., 2023) is used for evaluation.\nYi-6B, Yi-34B. Checkpoint models released by (Young et al., 2024).\nLlama2-13B. Checkpoint models released by (Touvron et al., 2023b).\nThe comparison results are shown in Table 2. Firstly, our method has significantly outperformed the open-source models, without any expert-crafted training trajectories. Secondly, while at training stage our method relies on Yi-34B for critic module, our trained models of much smaller sizes (e.g. 6B and 13B) outperform the Yi-34B with a clear margin. Finally, with our evolution learning framework, our performance is close to that of strong commercial model GPT-4, suggesting that our method can effectively adapt to tasks."}, {"title": "4 Conclusion", "content": "In conclusion, this paper presents a novel framework that enables the iterative self-evolution of agents, reducing dependence on expert-crafted trajectories or decisive environmental feedback. Our approach leverages a critic Large Language Model (LLM) to provide weak feedback, allowing agents to evolve progressively and learn incrementally. Despite potential errors from weak feedback, our iterative training process, which selects only a small number of high-confidence trials, ensures comprehensive environment exploration and effective learning. Notably, our framework achieves performance comparable to GPT-4 on the API-Bank public benchmark dataset using significantly smaller LLMs, demonstrating the efficacy of our approach."}, {"title": "5 Limitations", "content": "Our agents undergo iterative training, where each cycle requires multiple trials for environmental exploration. This method can be computationally demanding, especially for highly complex problems. Furthermore, the critic module's precision in evaluating trajectories remains somewhat limited, posing a potential constraint for applications that require exceptional accuracy."}, {"title": "6 Ethical Statement", "content": "This paper introduces a new training paradigm for agents, accompanied by ethical considerations. All data utilized in this study is publicly available, ensuring transparency and respect for privacy. Human evaluators were informed beforehand that their feedback might be used for product development and potentially published in a research paper. We are committed to acknowledging any limitations or potential biases in our findings. We affirm that no aspect of this research involved harm to individuals or misuse of personal data."}, {"title": "A appendix", "content": ""}]}