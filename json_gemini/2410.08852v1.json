{"title": "CONFORMALIZED INTERACTIVE IMITATION LEARNING: HANDLING EXPERT SHIFT & INTERMITTENT FEEDBACK", "authors": ["Michelle Zhao", "Reid Simmons", "Henny Admoni", "Aaditya Ramdas", "Andrea Bajcsy"], "abstract": "In interactive imitation learning (IL), uncertainty quantification offers a way for the learner (i.e. robot) to contend with distribution shifts encountered during deployment by actively seeking additional feedback from an expert (i.e. human) online. Prior works use mechanisms like ensemble disagreement or Monte Carlo dropout to quantify when black-box IL policies are uncertain; however, these approaches can lead to overconfident estimates when faced with deployment-time distribution shifts. Instead, we contend that we need uncertainty quantification algorithms that can leverage the expert human feedback received during deployment time to adapt the robot's uncertainty online. To tackle this, we draw upon online conformal prediction, a distribution-free method for constructing prediction intervals online given a stream of ground-truth labels. Human labels, however, are intermittent in the interactive IL setting. Thus, from the conformal prediction side, we introduce a novel uncertainty quantification algorithm called intermittent quantile tracking (IQT) that leverages a probabilistic model of intermittent labels, maintains asymptotic coverage guarantees, and empirically achieves desired coverage levels. From the interactive IL side, we develop ConformalDAgger, a new approach wherein the robot uses prediction intervals calibrated by IQT as a reliable measure of deployment-time uncertainty to actively query for more expert feedback. We compare ConformalDAgger to prior uncertainty-aware DAgger methods in scenarios where the distribution shift is (and isn't) present because of changes in the expert's policy. We find that in simulated and hardware deployments on a 7DOF robotic manipulator, ConformalDAgger detects high uncertainty when the expert shifts and increases the number of interventions compared to baselines, allowing the robot to more quickly learn the new behavior. Project page at cmu-intentlab.github.io/conformalized-interactive-il/.", "sections": [{"title": "1 INTRODUCTION", "content": "End-to-end robot policies trained via imitation learning (IL) have proven to be an extremely powerful way to learn complex robot behaviors from expert human demonstrations (Schaal, 1996; Price & Boutilier, 2003; Argall et al., 2009; Levine et al., 2016; Jang et al., 2022; Chi et al., 2023; Kim et al., 2024). At the same time, distribution shift is a core challenge in this domain, hampering the reliability of deploying such robot policies in the real world (Chang et al., 2021).\nOne way to combat this is via uncertainty quantification. By training an ensemble of policies (Menda et al., 2019) or via monte-carlo dropout during training (Cui et al., 2019), the robot learner can detect uncertain states and actively elicit additional action labels from the human expert online via an interactive IL framework (such DAgger (Ross et al., 2011)). At their core, these prior uncertainty-aware IL methods look to the training demonstration data as a proxy for deployment-time uncertainty. Any human expert labels requested at deployment time using this uncertainty estimate are simply stored for later re-training; the uncertainty estimate itself is not adapted online to the expert data nor does it inform any subsequent queries during the deployment episode.\nInstead, we contend that the human feedback requested and received during deployment time is a valuable uncertainty quantification signal that should be leveraged to update the robot's uncertainty estimate online. If properly accounted for, the updated uncertainty estimate will influence when"}, {"title": "2 RELATED WORK", "content": "Interactive Imitation Learning (IL) with Online Experts.\nInteractive IL is a branch of imitation learning wherein a robot learner can query a (human) expert to receive additional labels either during or after task execution (Celemin et al., 2022). A foundational approach for interactive IL is DAgger (Dataset Aggregation) (Ross et al., 2011), which iteratively augments the training dataset by aggregating data from the expert and learner policies and assuming the expert is stationary. In the online case, the robot learner can cede control to the expert at any time to get additional state-action data (also known as robot-gated feedback) or the expert can actively intervene at any time (also known as human-gated feedback) (Kelly et al., 2019). From the learner's"}, {"title": "Interacting with Non-stationary Experts.", "content": "A core kind of distribution shift we study in this work is human expert distribution shift. This can occur for a variety of reasons, from human teleoperators having biases when teaching robots (Thomaz et al., 2006; Thomaz & Breazeal, 2008), to generating different data distributions because of suboptimality (Gr\u00fcne-Yanoff, 2015; Thompson, 1999) or varied risk tolerance (Kwon et al., 2020). Moreover, simply observing robot behavior can influence how humans react or teach robots over time (Hong et al., 2024; Sagheb et al., 2023; Xie et al., 2021).\nDespite its prevalence in the real world, the majority of interactive IL works assume a stationary expert policy (Likmeta et al., 2021; Shin et al., 2023; Zheng et al., 2022). Our conformalized imitation learning approach takes a key step towards closing this gap by developing an online conformal prediction algorithm that can account for expert distribution shift."}, {"title": "Online Conformal Prediction.", "content": "Conformal prediction is a distribution-free uncertainty quantification method for constructing prediction intervals for both classification and regression problems (Angelopoulos & Bates, 2023; Romano et al., 2019; 2020; Zaffran et al., 2022), as well as for offline or online data. We focus on the online setting (e.g., timeseries) where uncertainty quantification is performed on streaming pairs of input-label data that are not necessarily i.i.d. (Gibbs & Candes, 2021). Broadly speaking, there are two predominant algorithms in this setting: adaptive conformal inference (ACI) (Gibbs & Candes, 2021; Gibbs & Cand\u00e8s, 2024; Bhatnagar et al., 2023; Zaffran et al., 2022) and quantile tracking (QT) (Angelopoulos et al., 2024a). Both are online gradient descent-based methods which guarantee asymptotic coverage in the online setting. Unlike ACI which is prone to infinitely sized intervals after a series of miscoverage events, QT directly estimates the value of the empirical quantile itself, ensuring coverage with finite intervals. In this work, we relax the assumption that labels must be observed at each time point in the streaming data and extend the online conformal paradigm to ensure coverage in the intermittent label regime."}, {"title": "Conformal Prediction for Robotics.", "content": "Recently, conformal prediction has become popular in the robotics domain in part due to the distribution-free guarantees it provides for arbitrarily complex learned models present within modern robotics pipelines. Specifically, conformal prediction has been used to provide collision-avoidance assurances (Chen et al., 2021; Lindemann et al., 2023; Dixit et al., 2023; Muthali et al., 2023; Taufiq et al., 2022; Dietterich & Hostetler, 2022; Lin & Bansal, 2024), calibrate early warning systems (Luo et al., 2022), and quantify uncertainty in large language model based planners (Ren et al., 2023; Lidard et al., 2024). There are several core challenges with the input-and-label data encountered in robotics: data is non-i.i.d. (e.g., sequential decision-making), data distributions are non-stationary (e.g. changing environment conditions), and labels are intermittently observed (e.g., limited expert feedback in the IL domain). By extending online conformal prediction to the intermittent label setting, we take a step towards addressing these challenges."}, {"title": "3 ONLINE CONFORMAL PREDICTION WITH INTERMITTENT LABELS", "content": "From the uncertainty quantification side, our core technical contribution is extending online conformal prediction to settings where ground truth labels are intermittently observed. We present our algorithm in the context of quantile tracking for relevance to our interactive IL experiments. However, we also derive an extension of ACI (Gibbs & Candes, 2021) to intermittent labels in the Appendix Section B.\nSetting. We focus on online conformal prediction in the adversarial setting, such as time-series forecast-"}, {"title": "Intermittent Quantile Tracking (IQT).", "content": "Our paradigm lifts the assumption that the ground truth label yt is observed constantly. Instead, it is observed with some probability at each timestep. Let the binary random variable $obst \\in \\{0,1\\}$ represent whether the robot observes label yt at timestep t:\n$obst := \\begin{cases} 1, & \\text{if } yt \\text{ observed} \\\\ 0, & \\text{otherwise.} \\end{cases}$"}, {"title": "4 CONFORMALDAGGER: A CALIBRATED APPROACH TO ASKING FOR EXPERT FEEDBACK", "content": "Since intermittent quantile tracking enables us to rigorously quantify the learner's uncertainty despite the fact that the labels are only revealed intermittently (i.e., when the expert intervenes), we can develop ConformalDAgger: a new way for the robot to tradeoff between acting autonomously and strategically asking for help when uncertainty increases.\nSetup. ConformalDAgger treats the robot's policy as the base model f := \u03c0\" on which we perform intermittent quantile tracking. The initial novice policy \u03c0\u2642 : X \u2192 A is trained on initial demonstration data Do of task T performed by the expert \u03c0\u03c1. Here, X represents the policy's inputs (e.g., image observations, proprioception), Y = A are the labels representing the robot's actions (e.g., future end-effector positions). During deployment, the robot policy generates a sequence of input-predicted action pairs (xt, a) \u2208 X \u00d7 Y for t = 1, 2, ..., that are temporally correlated. Relatedly, for each input xt the learner observes, there is a corresponding expert action (xt, a) \u2208 X \u00d7 Y that is the ground-truth label we seek to cover via our IQT prediction intervals Ct (xt).\nObservation Model.\nA key component of IQT is the observation likelihood model, pt = P(obst = 1 | xt). A nice byproduct of this model is that we can naturally derive a feedback model that is simultaneously human- and robot-gated by decomposing the observation model into the combination of both gating functions.\nIntuitively, the likelihood of observing the expert feedback at t is given by the probability that the human chooses to give feedback or the robot asks the expert for feedback. Let $obs^h_t \\in \\{0,1\\}$ be a random variable representing observing a human-gated feedback (if $obs^h_t = 1$) where the expert initiates providing an action label $a^*_t = \\pi^h(x_t)$ for $x_t$. Let $obs^r_t \\in \\{0,1\\}$ be a random variable representing robot-gated feedback, where the robot asks the expert for a label if $obs^r_t = 1$. We assume that if the robot elects to ask a question, the human will respond with probability 1. Our observation model takes the form:\n$p_t := P(obs_t = 1 | x_t) = P(obs^h_t = 1 \\lor obs^r_t = 1 | x_t)$\""}, {"title": "Quantifying Uncertainty: IQT.", "content": "In the interactive IL setting, IQT begins with initial upper and lower quantiles in the action (i.e. \"label\") space, $q^{lo}, q^{hi} \\in A$. The nonconformity score is a residual on the predicted (a", "vectors": "err^{lo}_t = (s^{lo}_t < q^{lo})$ and $err^{hi}_t = (s^{hi}_t < q^{hi})$. IQT then updates the quantile estimates online to obtain $q^{lo}_{t+1}, q^{hi}_{t+1}$ via the update rule from equation 5. At the next timestep, the adjusted prediction interval is constructed with $C_{t+1}(x_{t+1}) = [a^{\\pi}_{t+1} \u2013 q^{lo}_{t+1}, a^{\\pi}_{t+1} + q^{hi}_{t+1}]$. If the expert action is not observed, obst = 0, then $q^{lo}_{t+1} = q^{lo}$ and $q^{hi}_{t+1} = q^{hi}$ and the prediction interval size remains the same. Note that although errt is not known in the case where the expert does not provide an action label, IQT does not require it; IQT simply makes no change to the quantile estimate. In our simulated and hardware experiments, our action space, qlo, and qhi, are vector-valued. Our experiments instantiate IQT for continuous vectors, but the approach extends to discrete-valued action spaces."}, {"title": "Leveraging Uncertainty: Asking for Help.", "content": "Finally, the calibrated intervals Ct+1(xt+1) constructed by IQT enable us to design a new robot-gated feedback mechanism. Specifically, the model P(obs|xt) is informed by the calibrated interval size, $u(x_{t+1}) :=|| C_{t+1}(X_{t+1}) ||_2$. In our simulated experiments, we use P(obs | xt) = \u03c3(\u03b2[u(xt; \u03c0", "\u03c0": "\u03b2 is a temperature hyperparameter, and \u03c3 is the sigmoid function. In our hardware experiments, we implement 7 to as a hard threshold on on u(xt+; \u03c0"}, {"title": "5 SIMULATED INTERATIVE IMITATION LEARNING EXPERIMENTS", "content": "To evaluate ConformalDAgger, we run a series of simulated experiments with access to an oracle expert. We ground our experiments in a simulated robot goal-reaching task (left, Figure 5) and study scenarios where the distribution shift occurs due to the expert's changing preference in goal location."}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Task & Initial Learner Policy.\nThe robot learns a neural network policy \u03c0", "Stationary": "the expert has a fixed goal, go, across all deployment episodes. (2) Shift: the expert goal shifts from go to g1 at deployment episode i = 5. For example, the expert may have decided that a different cup location is easier to reach.\n(3) Drift: expert's goal slowly drifts from go to g\u2081 over the course of deployment episodes (in Figure 5 the drift from $g_0 \\rightarrow g_{1a}$ occurs at episode i = 5, $g_{1a} \\rightarrow g_{1b}$ occurs at episode i = 8, and $g_{1b} \\rightarrow g_1$ occurs at episode i = 11). For example, the expert may start with a conservative goal location initially (e.g., a goal nearby) and incrementally move the cup closer and closer to their target goal that may be further out of reach.\nInteractive Deployment Episodes & Learner Re-training. We consider M = 15 deployment episodes before re-training the learner. Each deployment episode has two interactive task executions. The task ends when the cup has reached the correct goal position, g*, or when the maximum timesteps (100) have been reached. The expert answers queries with optimal actions under their current policy ah \u03c0(x). Following DAgger (Ross et al., 2011), after each deployment episode, the state-action pairs where the expert provided action labels are aggregated into the training dataset, forming aggregated buffer Di+1 for the next deployment episode i + 1. We constrain the size of the replay buffer to 300 datapoints, dropping the old experiences.\nMethods. We compare ConformalDAgger to EnsembleDAgger (Menda et al., 2019).\nConformalDAgger uses an uncertainty threshold T = 0.09, temperature \u03b2 = 100, lookback window k = 100, Ir = 0.6, and initial $q^{lo,hi}_0 = 0.01$. The uncertainty threshold 7 is heuristically tuned to ask few, but infrequent questions in the first interactive deployment episode. EnsembleDAgger queries an expert online when there is high action prediction variance across an ensemble of learner policies, and when a safety classifier detects dissimilarity between expert and robot actions. We use 3 ensemble members and an uncertainty threshold of T = 0.06 for the ensemble disagreement (selected in a similar manner to ConformalDAgger) and a safety classifier threshold of 0.03.\nMetrics. We measure the quality of our uncertainty quantification via the miscoverage rate and human effort via the intervention percentage of the deployment trajectory. We compute the miscoverage for EnsembleDAgger using three times the standard deviation as the prediction interval. We measure the quality of the learned policy via two metrics. Decision deviation simulates the learner's behavior under its current policy and queries the expert at each learner state to obtain an expert action label. We measure the average L2 distance between the predicted and expert action. Trajectory deviation forward simulates both the expert and the learner acting independently under their policies, starting from the same initial x. We compare the L2 distance between the trajectories."}, {"title": "5.2 EXPERIMENTAL RESULTS", "content": "We focus results on infrequent human-gated feedback where P(obs = 1 | xt) = 0.2,\u2200t. In Appendix D, we present experiments with frequent (=0.9) and partial (=0.5) feedback. Irrespective of the human-gated likelihood, the learner can always actively ask for feedback based on P(obs | xt).\nTakeaway 1: When the expert shifts, ConformalDAgger asks for more help immediately compared to EnsembleDAgger, enabling the algorithm to more quickly align to the expert. Consider deployment episode i = 5 where the expert shifts from goal 0 to goal 1 (center row, Figure 5). ConformalDAgger immediately increases the number of expert feedback requests: before shift, the expert intervened ~20% of the time but in the 5th episode they intervene ~60% of the time. In contrast, EnsembleDAgger remains close to ~30%. Relatedly, ConformalDAgger has consistently lower miscoverage rate (max = 0.4 at shift; converges to 0.2) compared to the baseline (max = 1.0 at shift, converges to 0.4). Due to the extra solicited feedback, ConformalDAgger's ultimate decision and trajectory deviations are minimized in the subsequent retrained policies.\nTakeaway 2: ConformalDAgger automatically asks for more help each time the expert drifts. In the bottom row of Figure 5, we see ConformalDAgger maintain a similar level of queries as EnsembleDAgger during the first shift (at episode 5) and last shift (episode 11) but increases feedback during the intermediate shift (at episode 8). Despite these similarities, EnsembleDAgger's miscoverage rate is consistently higher than ConformalDAgger's and EnsembleDAgger's re-trained policy on average does not adapt as quickly (with higher expert decision and trajectory deviation). We hypothesize that this is because ConformalDAgger asks frequent questions consistently across all seeds compared to EnsembleDAgger (i.e. lower variance in Fig 5).\nTakeaway 3: With a stationary expert, ConformalDAgger and EnsembleDAgger are similar. We find that without distribution shift (top row, Figure 5), both methods ask for minimal help (staying near the 20% human-gated probability), and achieve similar performance in alignment to the expert."}, {"title": "6 HARDWARE EXPERIMENTS", "content": "Finally, we deployed ConformalDAgger in hardware on a 7 degree-of-freedom robotic manipulator that uses a state-of-the-art Diffusion Policy (Chi et al., 2023) trained via IL to perform a sponging task (Figure 1). The goal of our hardware experiments is to demonstrate how our approach can scale to a high-dimensional, real-world policy and understand how ConformalDAgger enables the robot learner to query a real human teleoperator. Our goal is to train the robot to perform a real-world cleaning task where it wipes up a line drawn by an Expo marker on a whiteboard with sponge."}, {"title": "7 CONCLUSION", "content": "In this work, we study uncertainty quantification for imitation learned policies that encounter distribution shift. We first extend uncertainty quantification via online conformal prediction to handle intermittent labels, such as those observed in interactive imitation learning. We then propose ConformalDAgger, a unification of our online conformal prediction algorithm with interactive imitation learning. Our approach provides asymptotic coverage guarantees for deployed end-to-end policies, uses the calibrated uncertainty measure to detect expert distribution shifts and actively query for more feedback, and empirically enables the robot learner update its policy to better align with the shifted expert distribution."}, {"title": "APPENDIX", "content": "A PROOF OF PROPOSITION 1\nWe will start by proving the following lemma. Recall that B is the upper bound on q1 \u2208 [0, B], and St \u2208 [0, B] by definition.\nLemma 1. For all t, we have -xNt-1 \u2264 qt \u2264 < B+ (1-a)Nt\u22121, where Nt = max1<r<t. B is the upper bound on q\u2081 \u2208 [0, B] and st \u2208 [0, B]."}, {"title": "A.1 SPECIAL CASE: WHEN Yt = Pt", "content": "Next, given the quantile tracking update with an intermittent observation model, we consider what would happen if yt was set as pt. When Yt = pt, then quantile tracking update becomes qt+1 = qt + (errt - a)obst and max1<t<T \\frac{\\gamma_t}{p_t} = 1.\nUnder Proposition 1, IQT with \\gamma_t = pt gives the following finite time coverage bound:"}, {"title": "B INTERMMITTENT ADAPTIVE CONFORMAL INFERENCE", "content": "We show in this section that intermittent observation of ground truth labels can be extended to Adaptive Conformal Inference (ACI) (Gibbs & Candes, 2021). To facilitate understanding, we briefly summarize ACI and discuss our extension Intermittent Adaptive Conformal Inference (IACI).\nSetup: Quantile Regression (QR). Similar to IQT, consider an arbitrary sequence of data points (xt, Yt) \u2208 X \u00d7 Y, for t = 1, 2, ..., that are not necessarily I.I.D. Our goal in ACI is to also produce prediction sets on the output of any base prediction model such that the sets contain the true label with a specified miscoverage rate a. Mathematically, at each time t, we observe xt and seek to cover the true label yt with a set Ct(xt), which depends on a base prediction model, f : X \u2192 Y. We will discuss ACI with a conformal quantile regression Romano et al. (2019) backbone. The base model takes as input the current xt and outputs prediction \u0177t as well the estimated upper and lower conditional quantiles:"}, {"title": "B.2 PROOF FOR INTERMITTENT ADAPTIVE CONFORMAL INFERENCE", "content": "Assumptions. We will assume throughout that with probability one, \u03b11 \u2208 [0,1], \u03b1 \u2208 (0,1), pt \u2208 (0,1] \u22001 \u2264 t \u2264 \u221e, and Qobs(x) is non-decreasing with Qsobs (c) = -\u221e for all c < 0, and Qsobs (c) = \u221e for all c > 1.\nLemma 2. With probability one, we have that \u2200t \u2208 [1, T],"}]}