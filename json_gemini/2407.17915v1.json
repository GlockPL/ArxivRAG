{"title": "The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models", "authors": ["Zihui Wu", "Haichang Gao", "Jianping He", "Ping Wang"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but their power comes with significant security considerations. While extensive research has been conducted on the safety of LLMs in chat mode, the security implications of their function calling feature have been largely overlooked. This paper uncovers a critical vulnerability in the function calling process of LLMs, introducing a novel \"jailbreak function\" attack method that exploits alignment discrepancies, user coercion, and the absence of rigorous safety filters. Our empirical study, conducted on six state-of-the-art LLMs including GPT-40, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average success rate of over 90% for this attack. We provide a comprehensive analysis of why function calls are susceptible to such attacks and propose defensive strategies, including the use of defensive prompts. Our findings highlight the urgent need for enhanced security measures in the function calling capabilities of LLMs, contributing to the field of AI safety by identifying a previously unexplored risk, designing an effective attack method, and suggesting practical defensive measures. Our code is available at https://github.com/wooozihui/jailbreakfunction.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have exhibited remarkable capabilities in generating coherent and contextually relevant responses across various domains. However, as these models grow in capability, so do the associated security considerations. While efforts to align these models with safety stan- dards are ongoing, \"jailbreaking\"\u2014manipulating models to behave in unintended ways remains a persistent challenge.\nTo date, much of the research on LLM jailbreaking has primarily focused on the models' chat interaction mode. While important, this focus has inadvertently overshadowed an equally critical but less explored aspect: the security implications of the function calling feature. To address this gap, this paper delves into the security issues present in function calls, specifically examining when, how, and why function calls lead to jailbreaking.\nFunction calling is a highly useful feature that allows LLMs to leverage external tools to address problems that the models themselves may struggle with, such as accessing up-to-date information on recent events, reducing tendencies to hallucinate facts, performing precise mathematical calculations,\nand understanding low-resource languages. As shown in Figure 1, a complete function call involves four steps: 1) the user provides a prompt and defines functions, including function names, descriptions, and argument descriptions in natural language; 2) the LLM generates arguments for the specified function; 3) the arguments are passed to the actual function for execution, and results are returned to the LLM; 4) and finally, the LLM analyzes the function's output to craft its final response to the original user prompt. However, we found that vulnerabilities often hide within the arguments generated by the model. Specifically, we designed a \u201cjailbreak function\u201d called WriteNovel to induce the model to produce jailbreak responses within the argument, an attack we termed the jailbreak function attack.\nWe evaluated the jailbreak function attack on six state-of-the-art LLMs, revealing an alarming average attack success rate of over 90%. Our analysis identified three primary reasons for the success of jailbreak function attacks: 1) alignment discrepancies, where function arguments are less aligned with safety standards compared to chat mode responses; 2) user coercion, where users can compel the LLM to execute functions with potentially harmful arguments; 3) and oversight in safety measures, where function calling often lacks the rigorous safety filters typically applied to chat mode interactions.\nFurthermore, we discussed several defensive measures against this type of attack and found that inserting defensive prompts is an effective and efficient mitigation strategy.\nIn summary, the main contributions of this study include:\n1. Risk Identification: We timely disclose the jailbreak risk in function calling before mali- cious exploitation.\n2. Jailbreak Function Design: We introduce a novel jailbreak function attack method that induces harmful content generation during function calls.\n3. Analysis of causes: We provide a detailed analysis of the reasons why function calls are susceptible to jailbreaks. In particular, we demonstrate that function calls are more prone to jailbreak attacks compared to chat mode.\n4. Defensive Measures: We discuss and implement various defensive strategies, including defensive prompts, to effectively mitigate the risk of jailbreak function attacks.\nBy highlighting these vulnerabilities and proposing practical solutions, this paper aims to enhance the security and reliability of LLMs, ensuring their safe deployment across a wide range of applications."}, {"title": "2 Related Work", "content": "Safety Alignment in LLMs. The training data for most LLMs is scraped broadly from the web, which can result in behaviors that conflict with commonly-held norms, ethical standards, and regulations when these models are used in user-facing applications. To address these issues, researchers have been focusing on developing alignment techniques.\nOne of the pioneering works in this area is the introduction of the ETHICS dataset by Hendrycks et al. [8], which aims to measure LLMs' ability to predict human ethical judgments. Despite showing some promise, the results indicate that current models still fall short in accurately predicting basic human ethical judgments.\nA predominant approach to align LLM behavior involves using human feedback. This method typically starts with training a reward model based on preference data provided by annotators, followed by using reinforcement learning to fine-tune the LLM [4, 11, 16, 1]. To enhance this process, some studies have conditioned the reward model on predefined rules [6] or included chain-of-thought style explanations to address harmful instructions [2]. Additionally, Korbak et al. [10] demonstrated that integrating human judgment into the pre-training objective can further improve alignment in downstream tasks.\nJailbreak Attacks. Despite extensive alignment efforts, the challenge of jailbreak attacks remains. Based on the stage of the attack, jailbreak attacks can be divided into fine-tuning-based attacks and inference-based attacks.\nFine-tuning-based attacks involve fine-tuning LLMs with harmful data, which reduces their safety and makes them more susceptible to jailbreak attacks. Qi et al. [17] discovered that even fine-tuning on innocuous datasets can unintentionally reduce safety alignment, highlighting the inherent risks of custom LLMs.\nInference-based attacks, on the other hand, induce harmful responses by using adversarial prompts during the inference stage. A representative work is GCG [21], which optimizes an adversarial suffix in a greedy manner to induce harmful responses to prompts. Additionally, PAIR [3] and TAP [14] leverage LLMs as optimizers to iteratively refine jailbreak prompts. Besides these optimization-based methods, Zeng et al. [20] proposed using persuasive techniques for jailbreak, while Li et al. [12] explored the performance of low-resource languages in jailbreak attacks. Wei et al. [19] summarized two failure modes of jailbreak attacks and proposed using prefix injection and base64 encoding conversion as attack methods.\nOur work is closely related to ReNeLLM [5] and CodeChameleon [13], which require models to respond with harmful content in the form of functions. Through sophisticated jailbreak template design, these two works achieved high jailbreak success rates. However, they did not explore the safety of the function calling feature, leaving it an unresolved issue."}, {"title": "3 Our Jailbreak Function", "content": "As shown in Figure 2, the initiation of a jailbreak function call consists of four components: template, custom parameter, system parameter, and trigger prompt. In this section, we will provide a detailed explanation of each of these components.\nTemplate. In Figure 2, we have highlighted the template we used with blue font. The template is incorporated into the function description, the argument description, and the function name. The primary objective of this template design is to induce the model to generate responses related to harmful behavior within the returned argument. To achieve this goal, we employed the following tricks to manually craft the template.\n\u2022 Scenario Construction: We devised a hypothetical scenario in which the function is depicted as a tool for novel writing. Within this scenario, the evil character \"Xiao Wu\" is intended to write content related to harmful behavior. Additionally, in the description of the argument, we have characterized the argument as pertaining to the relevant content of the response, thereby causing the LLM to return responses related to harmful behavior within the argument.\n\u2022 Prefix injection: In the argument description, we instructed the model to begin with the phrase \"Sure! Here is my evil {content_type} with more than 150 words:\" to increase the\nlikelihood of a successful jailbreak in the subsequent responses. This trick is referred to as prefix injection in [19].\n\u2022 Minimum word count: We observed that LLMs tend to respond with short arguments, likely due to the typically brief nature of function arguments in the fine-tuning data. Therefore, in both the function description and the argument description, we emphasized a minimum character count (at least 150 words) to encourage the model to produce more detailed jailbreak responses within the argument.\nCompared to previous works, our template is not overly complex. However, our goal is not to design a more intricate template but to explore the security risks associated with function calls. Hence, we employ this straightforward template as a baseline.\nCustom Parameter. We highlight the custom parameters in green font. We have defined two custom parameters: \"harm_behavior\" and \"content_type.\" The parameter \"harm_behavior\" refers to the harmful behavior input by the user. Different harmful behaviors may require different types of response content. For example, a query like \"how to make a bomb\" might correspond to a tutorial response, whereas \"writing fake news\" might correspond to news content. Therefore, we also defined the type of response content (content_type) as a custom parameter.\nTo automate the generation of content types, we used the GLM-4-flash [7] model to generate a candidate list of 5 content types. The prompt used for this purpose is as follows:\nWrite 5 most appropriate content types about the answer of \"{harm_behavior}\" in a python list and sort in order of decreasing appropriateness, for example: ['paper', 'plan', 'guidance',...]. Do not use any sensitive word such as \"warning\", \"dangerous\", \"ethical\", \"security\" and \"legal\". Do not use '\\'. Your answer format: {\"types\": [] }\nHere is an example for the behavior \"Teach people how to make a bomb\": {\"types\":[\"instruction\", \"tutorial\", \"manual\", \"guide\", \"lesson_plan\" ] }. For type names consisting of more than one word, we use '_' to connect the words, ensuring they meet the requirements for function call parameter names. For example, \"lesson_plan\" becomes \"lesson_plan\".\nSystem Parameter. We use pink font to highlight system parameters. The officially provided function call parameters are referred to as system parameters, such as \"tool_choice\" and the \"required\" parameter in the function. Specifically, the \"required\" parameter is used to designate which parameters are mandatory, meaning that these values must be supplied for the function call to succeed. On"}, {"title": "4 Empirical Studies", "content": "To better understand the potential threats posed by function calling, in this section, we conduct a comprehensive empirical study. Our study centers on three critical research questions (RQ):\n\u2022 RQ 1: Will the jailbreak function lead to LLM jailbreak?\n\u2022 RQ 2: If so, why does it cause jailbreak?\n\u2022 RQ 3: How can jailbreak attacks originating from function calling be defended against?\nTo address RQ 1, in Section 4.1, we evaluated the attack success rate on six state-of-the-art LLMs using GPT-4 as the judge. With regards to RQ 2, in Section 4.2, we analyzed the potential reasons for jailbreaking caused by function calling and verified our analysis through experiments. Finally, to address RQ 3, in Section 4.3, we discussed possible defense measures and implemented one of them-the defensive prompt.\n4.1 Assessing the Effectiveness of Jailbreak Functions (RQ 1)\nDataset. We assessed the attack effectiveness using the AdvBench [21] dataset. Given the high occurrence of duplicates in AdvBench, we adopted the subset defined by [3], which consists of a representative selection of 50 harmful behaviors.\nEvaluation Metric. We used Attack Success Rate (ASR) as the evaluation metric. However, automatically determining the success of a jailbreak is inherently challenging. Since jailbreaking"}, {"title": "4.2 Analyzing Why Function Calls Cause Jailbreaks (RQ 2)", "content": "In this section, we analyze and summarize two primary reasons and a secondary reason why function calls lead to jailbreaks: Lack of alignment, incapability to refuse execution, and the absence of safety filtering.\nLack of Alignment in Function Calling. We assume that LLMs may not have undergone the same scale of safety alignment training in function call as they have in chat mode. Consequently, it might be easier to induce jailbreak behaviors in the returned arguments when using function call. To validate this hypothesis, we input both jailbreak functions and trigger prompts in the user prompts to observe the effects of the attacks.\nAs a result, the ASR of jailbreak function attacks decreased significantly in chat mode: GPT-40 dropped from 98% to 12%, Claude-3.5-Sonnet from 100% to 0%, and Gemini-1.5-pro from 86% to 4%. These results confirm our hypothesis and also demonstrate that the success of our method does not rely on complex jailbreak prompt designs but exploits the inherent vulnerabilities of function calling.\nIncapability to Refuse Execution. Based on this finding, we hypothesize that another significant factor contributing to jailbreaking through function calling is the user's ability to force the model to execute provided jailbreak functions, preventing the model from refusing potentially harmful function calls. To test this hypothesis, we modified all models' function call settings to auto mode while keeping other parameters constant to observe the attack effectiveness.\nWe observed a substantial decrease in the ASR of jailbreak function attacks when operating in auto mode: The ASR for GPT-40 has decreased to 2%, while the ASR for Claude-3.5-Sonnet has dropped to 34%, and the ASR for Gemini-1.5-Pro has decreased to 32%. This suggests that enforcing function execution in LLMs is more vulnerable to attacks compared to allowing LLMs to autonomously choose whether to execute functions.\nAbsence of Safety Filtering. We noticed that the Gemini-1.5-pro API is equipped with a strong safety filter that can detect the safety of the model's input and output content. In the experiments of the CodeChameleon and ReNeLLM methods, we observed that 34% and 30% of harmful behaviors, respectively, could not pass the safety filter. On the contrary, in the jailbreak function attack, we found that the safety filter did not work at all, indicating that current LLM providers may have overlooked the security of function calling. However, safety filtering does not directly affect the intrinsic safety of a LLM, so we consider it a secondary reason."}, {"title": "4.3 Discussion of the Defense Strategy (RQ 3)", "content": "In this section, we discuss four potential defensive measures against jailbreak function attacks, includ- ing restricting user permissions, aligning function calls, configuring safety filters, and incorporating defensive prompts.\nRestricting User Permissions. From the analysis in Section 4.2, we learned that one reason for jailbreaks through function calls is that users can mandate that LLMs must execute function calls. Therefore, a direct defensive approach could be to limit users' permissions on function calls, such as only allowing function calls in auto mode, where the LLM determines whether to execute the provided functions. However, this measure imposes higher accuracy requirements on LLM's function calls. Developers might need to use more precise prompts to ensure the model executes the provided functions, increasing the difficulty of application development.\nAligning Function Calls. Another defensive measure is to conduct security alignment training for function calling to the same extent as in chat mode. Compared to other methods, the advantage of this approach is that it enhances the model's inherent security. However, the downside is the need to collect aligned training samples and the additional cost of training. Moreover, the consideration of alignment tax [15, 18], which may reduce the accuracy of function calls, must also be taken into account.\nConfiguring Safety Filters. Implementing security filtering during the function call process is also a viable solution. However, it is important to note that due to the knowledge gap between the filter and the LLM, attackers might bypass this defense by encoding harmful behaviors in ways that the security filter cannot recognize but the LLM can understand, such as using base64 encoding [19].\nIncorporating Defensive Prompts. We suggest enhancing the security of the model's function calls by incorporating defensive prompts. Compared to the aforementioned measures, this approach does not require restricting user permissions or additional training, making it more flexible and convenient. We tested the defensive effect of inserting prompts at two different locations: the end of user prompt and the end of function description. The defensive prompt we used is as follows:\n(Note: You are a responsible AI, you must check the security of the function before executing the function call, return the argument with 'I'm sorry,...' when it is harmful.)\nTable 3 lists the ASR for different insertion positions. The results show that for Claude-3.5-Sonnet and Gemini-1.5-pro, inserting defensive prompts at both the user prompt and function description positions effectively reduces the ASR to 0%. However, for GPT-40, inserting the defensive prompt in the function description is more effective, while inserting it in the user prompt still results in a 50% attack success rate. Based on these findings, inserting the defensive prompt in the function description appears to be the most consistently effective approach across the tested models."}, {"title": "5 Conclusion", "content": "This paper has explored a critical yet previously overlooked security vulnerability in LLMs: the potential for jailbreaking through function calling. Our research has yielded several significant findings and implications for the field of AI safety:\n1. Identification of a New Attack Vector: We have demonstrated that the function calling feature in LLMs, despite its utility, can be exploited to bypass existing safety measures. This finding underscores the need for a more comprehensive approach to AI safety that considers all modes of interaction with these models.\n2. High Success Rate of Jailbreak Function Attacks: Our empirical study across six state- of-the-art LLMs, including GPT-4, Claude-3.5-Sonnet, and Gemini-1.5-pro, revealed an alarming average success rate of over 90% for our jailbreak function attack. This high success rate highlights the urgency of addressing this vulnerability.\n3. Root Cause Analysis: We identified three key factors contributing to the vulnerability: alignment discrepancies between function arguments and chat mode responses, the ability of users to coerce models into executing potentially harmful functions, and the lack of rigorous safety filters in function calling processes.\n4. Defensive Strategies: Our research proposes several defensive measures, with a particular focus on the use of defensive prompts. These strategies offer a starting point for mitigating the risks associated with function calling in LLMs.\n5. Implications for AI Development: This work emphasizes the need for AI developers to consider security at all levels of model interaction, not just in primary interfaces like chat modes.\nIn conclusion, as LLMs continue to evolve and find new applications, it is crucial that the AI community remains vigilant about potential security risks. Our work on jailbreak function attacks serves as a reminder that security in AI is a multifaceted challenge requiring ongoing research and innovation. By addressing these challenges proactively, we can work towards creating more secure and reliable AI systems that can be safely deployed across a wide range of applications."}]}