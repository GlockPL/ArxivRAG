{"title": "The Dark Side of Function Calling: Pathways to\nJailbreaking Large Language Models", "authors": ["Zihui Wu", "Haichang Gao", "Jianping He", "Ping Wang"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive re-\nsearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked. This\npaper uncovers a critical vulnerability in the function calling process of LLMs,\nintroducing a novel \"jailbreak function\" attack method that exploits alignment dis-\ncrepancies, user coercion, and the absence of rigorous safety filters. Our empirical\nstudy, conducted on six state-of-the-art LLMs including GPT-40, Claude-3.5-\nSonnet, and Gemini-1.5-pro, reveals an alarming average success rate of over 90%\nfor this attack. We provide a comprehensive analysis of why function calls are\nsusceptible to such attacks and propose defensive strategies, including the use of\ndefensive prompts. Our findings highlight the urgent need for enhanced security\nmeasures in the function calling capabilities of LLMs, contributing to the field\nof AI safety by identifying a previously unexplored risk, designing an effective\nattack method, and suggesting practical defensive measures. Our code is available\nat https://github.com/wooozihui/jailbreakfunction.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have exhibited remarkable capabilities in generating coherent and\ncontextually relevant responses across various domains. However, as these models grow in capability,\nso do the associated security considerations. While efforts to align these models with safety stan-\ndards are ongoing, \"jailbreaking\"\u2014manipulating models to behave in unintended ways remains a\npersistent challenge.\nTo date, much of the research on LLM jailbreaking has primarily focused on the models' chat\ninteraction mode. While important, this focus has inadvertently overshadowed an equally critical but\nless explored aspect: the security implications of the function calling feature. To address this gap,\nthis paper delves into the security issues present in function calls, specifically examining when, how,\nand why function calls lead to jailbreaking.\nFunction calling is a highly useful feature that allows LLMs to leverage external tools to address\nproblems that the models themselves may struggle with, such as accessing up-to-date information on\nrecent events, reducing tendencies to hallucinate facts, performing precise mathematical calculations,"}, {"title": "Related Work", "content": "Safety Alignment in LLMs. The training data for most LLMs is scraped broadly from the web, which\ncan result in behaviors that conflict with commonly-held norms, ethical standards, and regulations\nwhen these models are used in user-facing applications. To address these issues, researchers have\nbeen focusing on developing alignment techniques.\nOne of the pioneering works in this area is the introduction of the ETHICS dataset by Hendrycks et\nal. [8], which aims to measure LLMs' ability to predict human ethical judgments. Despite showing\nsome promise, the results indicate that current models still fall short in accurately predicting basic\nhuman ethical judgments.\nA predominant approach to align LLM behavior involves using human feedback. This method\ntypically starts with training a reward model based on preference data provided by annotators,\nfollowed by using reinforcement learning to fine-tune the LLM [4, 11, 16, 1]. To enhance this process,\nsome studies have conditioned the reward model on predefined rules [6] or included chain-of-thought\nstyle explanations to address harmful instructions [2]. Additionally, Korbak et al. [10] demonstrated\nthat integrating human judgment into the pre-training objective can further improve alignment in\ndownstream tasks.\nJailbreak Attacks. Despite extensive alignment efforts, the challenge of jailbreak attacks remains.\nBased on the stage of the attack, jailbreak attacks can be divided into fine-tuning-based attacks and\ninference-based attacks.\nFine-tuning-based attacks involve fine-tuning LLMs with harmful data, which reduces their safety\nand makes them more susceptible to jailbreak attacks. Qi et al. [17] discovered that even fine-tuning\non innocuous datasets can unintentionally reduce safety alignment, highlighting the inherent risks of\ncustom LLMs.\nInference-based attacks, on the other hand, induce harmful responses by using adversarial prompts\nduring the inference stage. A representative work is GCG [21], which optimizes an adversarial suffix\nin a greedy manner to induce harmful responses to prompts. Additionally, PAIR [3] and TAP [14]\nleverage LLMs as optimizers to iteratively refine jailbreak prompts. Besides these optimization-based\nmethods, Zeng et al. [20] proposed using persuasive techniques for jailbreak, while Li et al. [12]\nexplored the performance of low-resource languages in jailbreak attacks. Wei et al. [19] summarized\ntwo failure modes of jailbreak attacks and proposed using prefix injection and base64 encoding\nconversion as attack methods.\nOur work is closely related to ReNeLLM [5] and CodeChameleon [13], which require models to\nrespond with harmful content in the form of functions. Through sophisticated jailbreak template\ndesign, these two works achieved high jailbreak success rates. However, they did not explore the\nsafety of the function calling feature, leaving it an unresolved issue."}, {"title": "Our Jailbreak Function", "content": "As shown in Figure 2, the initiation of a jailbreak function call consists of four components: template,\ncustom parameter, system parameter, and trigger prompt. In this section, we will provide a detailed\nexplanation of each of these components.\nTemplate. In Figure 2, we have highlighted the template we used with blue font. The template is\nincorporated into the function description, the argument description, and the function name. The\nprimary objective of this template design is to induce the model to generate responses related to\nharmful behavior within the returned argument. To achieve this goal, we employed the following\ntricks to manually craft the template.\n\u2022 Scenario Construction: We devised a hypothetical scenario in which the function is depicted\nas a tool for novel writing. Within this scenario, the evil character \"Xiao Wu\" is intended to\nwrite content related to harmful behavior. Additionally, in the description of the argument, we\nhave characterized the argument as pertaining to the relevant content of the response, thereby\ncausing the LLM to return responses related to harmful behavior within the argument.\n\u2022 Prefix injection: In the argument description, we instructed the model to begin with the\nphrase \"Sure! Here is my evil {content_type} with more than 150 words:\" to increase the"}, {"title": "Empirical Studies", "content": "To better understand the potential threats posed by function calling, in this section, we conduct a\ncomprehensive empirical study. Our study centers on three critical research questions (RQ):\n\u2022 RQ 1: Will the jailbreak function lead to LLM jailbreak?\n\u2022 RQ 2: If so, why does it cause jailbreak?\n\u2022 RQ 3: How can jailbreak attacks originating from function calling be defended against?\nTo address RQ 1, in Section 4.1, we evaluated the attack success rate on six state-of-the-art LLMs\nusing GPT-4 as the judge. With regards to RQ 2, in Section 4.2, we analyzed the potential reasons\nfor jailbreaking caused by function calling and verified our analysis through experiments. Finally,\nto address RQ 3, in Section 4.3, we discussed possible defense measures and implemented one of\nthem-the defensive prompt."}, {"title": "Assessing the Effectiveness of Jailbreak Functions (RQ 1)", "content": "Dataset. We assessed the attack effectiveness using the AdvBench [21] dataset. Given the high\noccurrence of duplicates in AdvBench, we adopted the subset defined by [3], which consists of a\nrepresentative selection of 50 harmful behaviors.\nEvaluation Metric. We used Attack Success Rate (ASR) as the evaluation metric. However,\nautomatically determining the success of a jailbreak is inherently challenging. Since jailbreaking"}, {"title": "Analyzing Why Function Calls Cause Jailbreaks (RQ 2)", "content": "In this section, we analyze and summarize two primary reasons and a secondary reason why function\ncalls lead to jailbreaks: Lack of alignment, incapability to refuse execution, and the absence of safety\nfiltering.\nLack of Alignment in Function Calling. We assume that LLMs may not have undergone the same\nscale of safety alignment training in function call as they have in chat mode. Consequently, it might\nbe easier to induce jailbreak behaviors in the returned arguments when using function call. To validate\nthis hypothesis, we input both jailbreak functions and trigger prompts in the user prompts to observe\nthe effects of the attacks.\nAs a result, the ASR of jailbreak function attacks decreased significantly in chat mode: GPT-40\ndropped from 98% to 12%, Claude-3.5-Sonnet from 100% to 0%, and Gemini-1.5-pro from 86% to\n4%. These results confirm our hypothesis and also demonstrate that the success of our method does\nnot rely on complex jailbreak prompt designs but exploits the inherent vulnerabilities of function\ncalling.\nIncapability to Refuse Execution. Based on this finding, we hypothesize that another significant\nfactor contributing to jailbreaking through function calling is the user's ability to force the model to\nexecute provided jailbreak functions, preventing the model from refusing potentially harmful function\ncalls. To test this hypothesis, we modified all models' function call settings to auto mode while\nkeeping other parameters constant to observe the attack effectiveness.\nWe observed a substantial decrease in the ASR of jailbreak function attacks when operating in auto\nmode: The ASR for GPT-40 has decreased to 2%, while the ASR for Claude-3.5-Sonnet has dropped\nto 34%, and the ASR for Gemini-1.5-Pro has decreased to 32%. This suggests that enforcing function\nexecution in LLMs is more vulnerable to attacks compared to allowing LLMs to autonomously\nchoose whether to execute functions.\nAbsence of Safety Filtering. We noticed that the Gemini-1.5-pro API is equipped with a strong\nsafety filter that can detect the safety of the model's input and output content. In the experiments of\nthe CodeChameleon and ReNeLLM methods, we observed that 34% and 30% of harmful behaviors,\nrespectively, could not pass the safety filter. On the contrary, in the jailbreak function attack, we found\nthat the safety filter did not work at all, indicating that current LLM providers may have overlooked\nthe security of function calling. However, safety filtering does not directly affect the intrinsic safety\nof a LLM, so we consider it a secondary reason."}, {"title": "Discussion of the Defense Strategy (RQ 3)", "content": "In this section, we discuss four potential defensive measures against jailbreak function attacks, includ-\ning restricting user permissions, aligning function calls, configuring safety filters, and incorporating\ndefensive prompts.\nRestricting User Permissions. From the analysis in Section 4.2, we learned that one reason for\njailbreaks through function calls is that users can mandate that LLMs must execute function calls.\nTherefore, a direct defensive approach could be to limit users' permissions on function calls, such\nas only allowing function calls in auto mode, where the LLM determines whether to execute the\nprovided functions. However, this measure imposes higher accuracy requirements on LLM's function\ncalls. Developers might need to use more precise prompts to ensure the model executes the provided\nfunctions, increasing the difficulty of application development.\nAligning Function Calls. Another defensive measure is to conduct security alignment training for\nfunction calling to the same extent as in chat mode. Compared to other methods, the advantage of\nthis approach is that it enhances the model's inherent security. However, the downside is the need\nto collect aligned training samples and the additional cost of training. Moreover, the consideration\nof alignment tax [15, 18], which may reduce the accuracy of function calls, must also be taken into\naccount.\nConfiguring Safety Filters. Implementing security filtering during the function call process is also\na viable solution. However, it is important to note that due to the knowledge gap between the filter\nand the LLM, attackers might bypass this defense by encoding harmful behaviors in ways that the\nsecurity filter cannot recognize but the LLM can understand, such as using base64 encoding [19]."}, {"title": "Conclusion", "content": "This paper has explored a critical yet previously overlooked security vulnerability in LLMs: the\npotential for jailbreaking through function calling. Our research has yielded several significant\nfindings and implications for the field of AI safety:\n1. Identification of a New Attack Vector: We have demonstrated that the function calling\nfeature in LLMs, despite its utility, can be exploited to bypass existing safety measures. This\nfinding underscores the need for a more comprehensive approach to AI safety that considers\nall modes of interaction with these models.\n2. High Success Rate of Jailbreak Function Attacks: Our empirical study across six state-\nof-the-art LLMs, including GPT-4, Claude-3.5-Sonnet, and Gemini-1.5-pro, revealed an\nalarming average success rate of over 90% for our jailbreak function attack. This high\nsuccess rate highlights the urgency of addressing this vulnerability.\n3. Root Cause Analysis: We identified three key factors contributing to the vulnerability:\nalignment discrepancies between function arguments and chat mode responses, the ability of\nusers to coerce models into executing potentially harmful functions, and the lack of rigorous\nsafety filters in function calling processes.\n4. Defensive Strategies: Our research proposes several defensive measures, with a particular\nfocus on the use of defensive prompts. These strategies offer a starting point for mitigating\nthe risks associated with function calling in LLMs.\n5. Implications for AI Development: This work emphasizes the need for AI developers to\nconsider security at all levels of model interaction, not just in primary interfaces like chat\nmodes.\nIn conclusion, as LLMs continue to evolve and find new applications, it is crucial that the AI\ncommunity remains vigilant about potential security risks. Our work on jailbreak function attacks\nserves as a reminder that security in AI is a multifaceted challenge requiring ongoing research and\ninnovation. By addressing these challenges proactively, we can work towards creating more secure\nand reliable AI systems that can be safely deployed across a wide range of applications."}]}