{"title": "Learning to Generate Unit Tests for Automated Debugging", "authors": ["Archiki Prasad", "Elias Stengel-Eskin", "Justin Chih-Yao Chen", "Zaid Khan", "Mohit Bansal"], "abstract": "Unit tests (UTs) play an instrumental role in assessing code correctness as well as providing feedback to a large language model (LLM) as it iteratively debugs faulty code, motivating automated test generation. However, we uncover a trade-off between generating unit test inputs that reveal errors when given a faulty code and correctly predicting the unit test output without access to the gold solution. To address this trade-off, we propose UTGEN, which teaches LLMs to generate unit test inputs that reveal errors along with their correct expected outputs based on task descriptions and candidate code. We integrate UTGEN into UTDEBUG, a robust debugging pipeline that uses generated tests to help LLMs debug effectively. Since model-generated tests can provide noisy signals (e.g., from incorrectly predicted outputs), UTDEBUG (i) scales UTGEN via test-time compute to improve UT output prediction, and (ii) validates and back-tracks edits based on multiple generated UTs to avoid overfitting. We show that UTGEN outperforms UT generation baselines by 7.59% based on a metric measuring the presence of both error-revealing UT inputs and correct UT outputs. When used with UTDEBUG, we find that feedback from UTGEN's unit tests improves pass@1 accuracy of Qwen-2.5 7B on HumanEval-Fix and our own harder debugging split of MBPP+ by over 3% and 12.35% (respectively) over other LLM-based UT generation baselines.", "sections": [{"title": "1. Introduction", "content": "With rapid advancements in training large language models (LLMs; Achiam et al., 2023; Anthropic, 2024; Gemini et al., 2023; Dubey et al., 2024), enhancing their coding abilities has garnered significant attention (Chen et al., 2021; Li et al., 2023; Roziere et al., 2023; Guo et al., 2024b; Hui et al., 2024, inter alia). However, these models are far from perfect and - much like human-written code \u2013 model-written code contains errors. Human developers often improve their code through test-driven development with iterative debugging, i.e., identifying failure cases by providing example inputs and their expected outputs, reasoning over causes of failure, and modifying the code to address the issues. Unit tests (UTs) are one form of such pairings: code inputs and expected outputs that test individual functionalities in a piece of code. A failing unit test provides targeted feedback, not only identifying that a piece of code is not operating as expected but also helping to localize the error to one part of the code (Maximilien & Williams, 2003; Nagappan et al., 2008; Ficco et al., 2011). Models have similarly been shown to benefit from iterative debugging based on explicit or implicit feedback stemming from failing unit tests (Chen et al., 2023b; Moon et al., 2023). However, the ability to provide feedback and debug incorrect code is often bottlenecked by the availability of (failing) unit tests for a given problem.\nWhile several coding benchmarks come with human-written UTs for evaluation purposes, these suites of UTs are typically small due to the laborious annotation process (Liu et al., 2024). Unit test collection is challenging for two reasons: first, it requires sampling inputs that are likely to trigger an error. For example in Figure 1, the unit test:"}, {"title": "2. Related Work", "content": "Automatic Unit Test Generation. While unit tests play a pivotal role in assessing code correctness, manually writing a suite of unit tests is laborious and often infeasible (Chen et al., 2023a; Liu et al., 2024). Therefore, prior work in software testing has often focused on automatic unit test generation (King, 1976; Cadar et al., 2008; Holler et al., 2012; Cha et al., 2015, inter alia). With the application of LLMs to program synthesis, recent work has explored using LLMs for automatic unit test generation (Chen et al., 2023a; Sch\u00e4fer et al., 2023; Liu et al., 2024). Notably, Sch\u00e4fer et al. (2023) and Liu et al. (2024) focus solely on unit test input generation, relying on the gold (correctly implemented) function being tested to generate the corresponding output of the unit test. Both prompt strong frontier models such as GPT-4, followed by additional input mutations (Liu et al., 2024) or iteratively prompting the LLM (Sch\u00e4fer et al., 2023). In contrast, our open-source models trained using UTGEN automatically generate both input-output pairs of unit tests, i.e., at inference time, UTGEN generates a candidate input and the corresponding output as per the task description without access to the gold solution. While Chen et al. (2023a) also generate both input-output unit test pairs, they only rely on standard LLM prompting to produce unit tests as a means of improving code-generation performance. As a result, their method does not evaluate or optimize for the validity of generated UTs, especially output accuracy; on the other hand, in UTGEN, we explicitly model output accuracy and train the models to reason about the output of the unit test correctly.\nLLM Debugging. Using LLMs for debugging faulty code (also known as program repair) has been a subject of prior work. Debugging approaches can be categorized into two groups: those training models to debug (Moon et al., 2023; Ni et al., 2024; Chae et al., 2024) and those providing external feedback to pretrained models (Chen et al., 2023b; Olausson et al., 2023; Zhong et al., 2024). Both groups share a reliance on (gold) unit tests to train models and/or provide feedback. Thus, UTGEN is complementary to both kinds of debugging methods and can be easily integrated into either by using unit tests generated by UTGEN when human-written unit tests are scarce or unavailable. To this end, in Section 5 we show that UTGEN's generated unit tests can be used to provide feedback to LLMs via rubber-duck debugging proposed in Chen et al. (2023b). Additionally, in Section 3.3, we propose UTDEBUG, an improvised debugging pipeline that handles noisy feedback from inaccurately generated UTs via test-time scaling and backtracking. Orthogonal to our work, another direction of prior work involves curating benchmarks to measure LLMs' debugging abilities (Muennighoff et al., 2024; Jain et al., 2024; Tian et al., 2024) \u2013 which we also use and build upon in our experiments to showcase the effectiveness of UTGEN."}, {"title": "3. UTGEN and UTDEBUG: Unit Test Generation and Automated Debugging", "content": "Given a natural language task description d for a coding task, we focus on generating unit tests (UTs) in the format of input-output pairs that are consistent with the task description d. Our setup is consistent with Chen et al. (2023a) and Jain et al. (2024) who also consider unit tests in the form of input-output pairs generated without utilizing the correct implementation of the function. More generally, our setting is akin to parameterized unit testing (Tillmann et al., 2010) and uses the notion of functional correctness, i.e., measuring correctness by simulating an exhaustive set of scenarios (UT inputs) and ensuring that the function performs as expected (correct output as per the problem description d).\nNotation and Desiderata. Let d denote the natural language description of the function to be implemented (top yellow box in Figure 1). We assume that this description specifies the input space X and output space Y. Furthermore, let the set of all functions that correctly solve the task be $F_d$. Then, a valid unit test (x, y) for task d is:\n\u2022 x \u2208 X, i.e., the input of the unit test is a valid input as per the task description d.\n\u2022 $f_r(x) = y, f_r \u2208 F_d$, i.e., y is the expected output as per the task description d, and therefore, is the result one gets by executing any correct implementation.\nFor example, in Figure 1, 120 is a valid input, as it is a number, whereas \"apple\" is not. Similarly, 121 is the expected output of the function (given 120 as input), while 122 would be an invalid output. Thus, (120, 121) is a valid unit test for the task.\nUnit Test Generator. Addressing RQ1, we define the desirable properties of an automatic unit test generator $T_\\theta$, parameterized by \u03b8. Ideally, $T_\\theta$ should generate valid unit\n\u00b2We note that the correctness of a code or function can also be determined via formal verification, i.e., by mathematically proving it functions correctly in all situations. However, we do not consider this paradigm in our work, as we focus on empirical methods and applications, e.g., code debugging."}, {"title": "3.2. UTGEN: Training LLMs for Unit Test Generation", "content": "While several lines of prior work focus on curating training data for improving code generation (Guo et al., 2024a; Muennighoff et al., 2023), there is a general lack of dedicated datasets for training the desired UT generator outlined above. Therefore, to improve along the lines of RQ2, we design a training recipe that bootstraps this data from training datasets for code generation i.e., a collection of problem descriptions and their corresponding code implementations. After collecting these examples, we perturb their gold solutions to generate faulty code for which we will be generating unit tests. For each example, we sample multiple unit tests and select the ones that are adversarial w.r.t. to the buggy code (i.e., inputs that cause it to fail), which we use to construct our training data. Below we describe each step:\nProblem Descriptions and Target Codes. We start with a collection of coding problems with problem descriptions and gold codes. Specifically, we use publicly available data from Tulu-3 (Lambert et al., 2024) due to its large scale and the improvements noted by Lambert et al. (2024) when finetuning off-the-shelf LLMs on coding tasks. We filter it to focus on Python code with functional abstractions (further details in Appendix B). This yields a total of 48.3K unique code problems. Corresponding to each problem description (prompt), the dataset contains a correct target code solution intended for finetuning, which we use as the reference (gold) code solution ($f_r$) as illustrated in Figure 2 (I).\n\u00b3 However, in order to train an adversarial unit test generator we would need access to incorrect or faulty code solutions that can be debugged, as unit tests must have some error to attack. We obtain these by using the LLM to perturb the reference code solution in realistic ways (prompt and details given in appendix Appendix E).\nAnnotating Unit Tests. As mentioned in Section 3.1, one of the goals of the unit test generator is to be able to not only generate valid unit tests, but failing unit tests (that trigger errors). To facilitate this, given a problem description, reference code $f_r$, and buggy candidate code $f_b$, we sample n different unit test inputs (via the prompts in Appendix E). A unit test (x, $f_r(x)$) is failing if $f_r(x)$ \u2260 $f_b(x)$, i.e., if the output of the candidate fails to match the reference output (cf. Figure 2 (II)). Note that, while the LLM can be used to generate the output of unit test, it can often be inaccurate (Jain et al., 2024; Gu et al., 2024); so to ensure output accuracy, we use the output of the reference code during training. We revisit the discussion of how accurate LLMs are at output prediction during inference in Section 5.\n\u00b3The code solutions in the Tulu-3 SFT mix have either been written by humans or by frontier models and are thus highly likely to be correct (Lambert et al., 2024)."}, {"title": "3.3. UTDEBUG: Debugging with Generated Unit Tests", "content": "A crucial difference between using generated unit tests (as opposed to human-generated or gold unit tests) is the degree of noise in unit test feedback. Despite training models via UTGEN or otherwise a generated unit test (x, y) may not be 100% accurate. This manifests in two ways: 1) the generated unit test input is not failing for a given code under debugging $f_b$, i.e., $f_r(x) = f_b(x)$; 2) the generated unit test output is inaccurate, i.e., not consistent with what a gold solution to the task description would yield (\u0177\u2260 $f_r(x)$). Both these types of errors can negatively impact downstream utility of unit tests (in our context, ability to debug) in different ways as illustrated in Figure 3 (left). First, a lack of adversarial input could result in a faulty code (albeit with subtle errors) being misclassified as correct, and thus, erroneously taken out of the debugging process. Next, incorrect unit test outputs could also lead to false positive, with subsequent edits introducing errors to an otherwise correct code. Moreover, even if the candidate code contains bugs, an incorrect unit test output can result in incorrect feedback given to the model during debugging, which in turn could be wasteful or detrimental to downstream performance. These considerations motivate our RQ3 on how best to incorporate potentially noisy feedback. Below we propose UTDEBUG, which contains two effective ways of mitigating noisy feedback from automatically generated unit tests (with additional details in Appendix C):\nBoosting Output Accuracy via Test-Time Scaling. Building on past work that has shown the benefits of scaling up inference-time compute (Wang et al., 2022; Lightman et al., 2023; Snell et al., 2024) for LLMs, we first improve output accuracy by allocating additional computation to the problem. Specifically, we use self-consistency (SC; Wang et al., 2022), whereby, for a given UT input, we sample k = 8 output completions (including CoT rationales) and take the most common final UT output (majority vote) as the final answer, as shown in Figure 3 (top-right). To further boost output accuracy, we upsample UT inputs and only retain those where the final answer gets over 50% of the votes (i.e., 4 votes), discarding the unit test otherwise. Our results in Section 5.3 show that this boosts accuracy by up to 11.4%.\nBack-Tracking and Cross-Validation. In addition to enabling test-time scaling to improve output accuracy, in settings with noisy feedback, it is important to know when to abstain or backtrack, e.g., discarding edits made to a candidate code. This can be beneficial when the generated"}, {"title": "4. Experimental Setup", "content": "Models. We demonstrate the effectiveness of UTGEN on three 7-8B scale LLMs across different model families that are adept at coding, namely, Llama-3 8B Instruct (AI@Meta, 2024), Llama-3.1 8B Instruct (Dubey et al., 2024), Qwen 2.5 Coder 7B (Hui et al., 2024).\nDataset. We use debugging datasets based on popular LLM coding benchmarks, HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) along with their extended evaluation suites with more unit tests as proposed by (Liu et al., 2024). We describe their construction below, with further details in Appendix A.\n\u2022 HE+Fix. We start with the HumanEvalFix dataset containing human-introduced errors to gold solutions proposed by Muennighoff et al. (2024). However, this dataset only uses minimal unit tests to evaluate code correctness (from HumanEval) which has shown to be unreliable (Li et al., 2023), as it can miss errors due to low coverage. Therefore, we filter for problems that overlap with EvalPlus (Liu et al., 2024), which contains over 80\u00d7 more unit tests. This yields 158 problems that each have a faulty solution and an expanded suite of private UTs \u2013 i.e., UTs not used for debugging \u2013 for evaluation.\n\u2022 MBPP+Fix. We construct a debugging dataset based on the MBPP+ benchmark (Austin et al., 2021; Liu et al., 2024; Ni et al., 2024). To obtain faulty codes, we sample 16 solutions for each problem from different 7-8B scale models and filter for incorrect solutions based on the"}, {"title": "Evaluation Metrics", "content": "Below we describe three intrinsic evaluation metrics for unit test generation: attack rate, output accuracy, and accuracy \u2229 attack; along with pass@1 accuracy as the extrinsic metric to measure LLM's debugging abilities using UT-feedback.\n\u2022 Attack Rate. This metric measures a UT generator $T_\\theta$'s attacking ability, i.e., its ability to generate a failing unit test input \u00ee for a given buggy solution $f_b$. We measure this by matching if the output of a gold reference solution $f_r$ for input \u00ee differs from that of the buggy code $f_b$. Note that this does not take into account the accuracy of the unit test output which we measure separately below. Mathematically, for any dataset D of coding problems, attack rate is defined as:\n$AttackRate = \\frac{100}{|D|} \u00d7 \\sum_{d \\in D} 1[f_r(x) \u2260 f_b(x)]; (x, y) \\sim T_\\theta(d, f_b)$\n\u2022 Output Accuracy. This metric measures how often the output of a generated unit test \u0177 is consistent with the problem description, i.e., generates the same output as the reference gold code $f_r$. Output accuracy does not require the generated unit test to fail. In other words,\n$Output Acc = \\frac{100}{|D|} \u00d7 \\sum_{d \\in D} 1[\\hat{y} = f_r(x)]; (x, y) \\sim T_\\theta(d, f_r)$\n\u2022 Accuracy \u2229 Attack. This metric combines both attack rate and output accuracy and represents how often a unit test generator $T_\\theta$ generates a useful (i.e., failing) unit test"}, {"title": "5. Results and Analysis", "content": "5.1. Intrinsic Evaluation of Unit Test Generation\nIn order to study the inherent UT generation abilities of different models and baselines, we use the intrinsic metrics defined by RQ1 and outlined in Section 4 in Table 1 on the most challenging debugging task MBPP+Fix (Hard) averaged over 3 runs."}, {"title": "5.2. Impact of Generated Unit Tests on Debugging", "content": "UTs from UTGEN are Best for UTDEBUG. Addressing RQ3, we measure how effective each type of UT generator is in a downstream debugging evaluation. From Table 2, we observe that across different LLM families, multi-turn debugging with UTDEBUG is more effective when using generated UTs from UTGEN than debugging with UTs generated by the baselines, as well as debugging without feedback. For instance, with Qwen2.5, after 3 rounds of debugging on MBPP+Fix, UTGEN improves over the randomly-sampled UT baseline by \u22483% (absolute), debugging by failing UTs by 7.59% and without UT feedback by 21.23% (absolute). Moreover, on the more challenging MBPP+Fix (Hard) split, UTGEN improves over randomly-sampled UTs by 12.35% and the baseline without UT feedback by 12.94%. Given that we use the same underlying LLM and similar feedback templates, the results in Table 2 show that UTs generated by UTGEN provide the most useful and effective feedback."}, {"title": "5.3. Effectiveness of the UTDEBUG Pipeline", "content": "In Section 3.3, we highlighted the challenges of debugging with imperfect model-generated unit tests and suggested remedies to make our debugging pipeline, UTDEBUG, robust to noisy feedback from such UTs. We study the effectiveness of these measures, i.e., test-time scaling of UT outputs, and backtracking, on debugging with n = 3 generated UTs for 3 rounds using Qwen2.5 on the MBPP+Fix dataset in Table 3.\nTest-time Scaling and Backtracking are Crucial for LLM Debugging with Generated UTs. From Table 3, we observe that irrespective of the underlying method for UT"}, {"title": "5.4. Scaling with Number of Generated UTs", "content": "Thus far, all our experiments use n = 3 generated UTs across baselines and models. However, as we described in Section 3.3, having multiple UTs can be advantageous because: (i) there is a higher likelihood of generating a failing UT and getting a reliable signal for when the code is correct, (ii) more robust signal for when to backtrack using"}, {"title": "5.5. Comparison to Frontier LLMs", "content": "In Table 1 we evaluated open-source LLMs that we trained using UTGEN, and in Section 3.3 we applied these models in our UTDEBUG framework for debugging, finding the trained models to be effective. Table 4 compares stronger frontier models \u2013 GPT-4o (Achiam et al., 2023) and DeepSeek-V3 (Guo et al., 2024b) \u2013 to the best open-source 7-8B model tested (Qwen2.5 7B). Here, we see that, while they outperform Qwen, even these stronger and much larger frontier models struggle at the task of generating failing unit tests. First, we find that when prompting models with faulty codes, the attack rate of the generated UTs is slightly over 50% on MBPP+Fix (Hard) dataset, showcasing the inherently challenging nature of isolating corner cases and subtle errors in partially correct code. Furthermore, both GPT-4o, DeepSeek-V3 attain comparable output accuracies to that of our trained UTGEN model on top of Qwen2.5, while being larger and more costly to run (although their Acc. \u2229 Attack scores are higher). Overall, we find that using UT-based feedback even with stronger LLMs only triggers"}, {"title": "6. Discussion and Conclusion", "content": "Our work on UTGEN contributes to the broader landscape of verification and feedback generation for LLM-generated code. While recent work has focused on training verifiers to provide feedback (Mahan et al., 2024; Zhang et al., 2024), a key challenge remains in obtaining high-quality feedback signals for debugging. UTGEN addresses this by directly generating unit tests that can identify problems in code, complementing existing work on how to effectively incorporate and present feedback for debugging (Chen et al., 2023b; Zhong et al., 2024) along with test-time scaling and backtracking incorporated in UTDEBUG. Our results demonstrate that without a quality signal to determine code correctness and/or how a faulty code is failing (in the form of unit tests), using LLMs to generate feedback and debug still proves to be challenging. This is one of the first efforts in this direction, and we hope to spark more interest in future work toward LLM-generated unit tests (both input and outputs) that reveal the model's coding errors.\nOur approach connects to and complements recent work on handling real-world programming issues. While approaches designed for SWEBench (Jimenez et al., 2024) focus on fixing known issues from GitHub by understanding and implementing fixes for bug reports, UTGEN addresses the upstream challenge of automatically discovering potential issues in new code through test generation. Both tasks share a core challenge: determining the expected behavior of code without access to correct implementations. This connects to the fundamental concept of simulatability from computability theory (Rice, 1953), where we ask whether a program can predict the behavior of another program. Recent work such as Jain et al. (2024) shows that while LLMs can often simulate existing code by tracing execution steps, they struggle more with predicting correct outputs from specifications alone. Our results align with these findings \u2013 while UTGEN can generate test inputs that trigger errors (high attack rate), predicting correct expected outputs remains challenging (lower output accuracy). This suggests that improving LLMs' ability to reason about intended program behavior from specifications remains a crucial direction for future work. Nevertheless, we find that the modifications made to debugging in UTDEBUG help boost UTGEN's accuracy and account for noise, leading to downstream gains.\nConclusion. We first identified a key trade-off between attack rate and output prediction accuracy when predicting unit tests with models. In light of this trade-off, we introduced UTGEN, a new method for creating training data and teaching models to produce unit tests. This allows us to train models to produce better unit tests, as measured by intrinsic metrics like attack rate and output accuracy. Moreover, finding that existing data contains large numbers of easy errors, we introduce a new subset of data with challenging and hard-to-diagnose errors. To enable debugging with automated unit tests, we propose UTDEBUG, wherein we augment our predictor's accuracy with test-time scaling and regularize it using a cross-validation and back-tracking procedure that prevents it from overfitting to a narrow or incorrect unit test. This, combined with our improved predictor, results in consistent increases in debugging performance across models, a gap that persists as we scale the number of unit tests."}, {"title": "A. Debugging Datasets", "content": "HE+Fix. This dataset contains a total of 158 problems each with one incorrect human-written code and has an initial pass rate (prior to debugging) of 0%, i.e., all private unit tests are failing. As mentioned in Section 4, we use the dataset provided by Muennighoff et al. (2024)4 but replace the test set for each problem from the original test suite in HumanEval (Chen et al., 2021) to that in the EvalPlus evaluation suite (Liu et al., 2024). This increases the average unit tests per problem from 8.17 to 775.18 gold unit tests. Note that we have an automatic UT extraction script for the test code in EvalPlus, and we only retain problems for which this extraction is successful (158 out of 164).\nMBPP+Fix and MBPP+Fix (Hard). We begin with 378 problems in the MBPP+ dataset (Liu et al., 2024)5 and follow the same gold UT extraction step described above, discarding problems for which the extraction fails. This leaves us with 375 problems, for which we sample 16 solutions per problem across multiple LLMs: Gemma-7B-IT (Team et al., 2024), Llama3 8B Instruct (AI@Meta, 2024), Llama3.1 8B Instruct (Dubey et al., 2024), Tulu-3 8B SFT (Lambert et al., 2024), DeepSeek 7B coder (Guo et al., 2024b), Qwen-2.5 Coder (Hui et al., 2024). To generate MBPP+Fix, we filter for incorrect solutions (i.e., with at least one failing UT) and then randomly sample one incorrect code per problem. This yields 325 problems in total, each with one faulty code. This dataset has an initial pass rate of 24.21% and an average of 107.45 gold unit tests per problem. In order to construct, MBPP+Fix (Hard), we follow a similar process but select only incorrect solutions which pass 50 - 95% of unit tests. The intuition here is that solutions that are partially correct are often harder to debug than those that are fully incorrect. We then randomly sample one such incorrect solution per problem, yielding a dataset of 170 problems with an initial pass rate of 74.83% and an average of 107.49 gold unit tests."}, {"title": "B. UTGEN Training", "content": "Preprocessing Tulu Data. We use the Tulu-3 SFT mixture dataset released by Lambert et al. (2024) which contains a total of 939.3K instances. However, it contains a mixture of prompts for instruction-following, math reasoning, and coding. Therefore, we filter for prompts involving Python coding by regex search for keywords \"python\" and \"def \" which suit our task setup described in Section 3.1. Furthermore, we filter out instances with more than 2K tokens in the prompt and ensure the prompt is a valid unicode string. This results in a total of 48.3K instances for which we use the \"prompt\" as the problem description and extract code from the response of the last turn when multi-turn interactions are present provided that the extracted code executes without any errors or issues. Finally, we prompt the LLM to be trained with UTGEN to generate 2 corrupted versions of this code to serve as the target code, and for each target code, we make 5 attempts to generate failing unit test inputs and filter out instances that do not have any such UTs. This is followed by the rationalization step (with the same underlying LLM to be trained) described in Section 3.2 and in Figure 2, which results in roughly 30K instances for each model trained with UTGEN (Qwen2.5, Llama-3, -3.1 8B).\nTraining Hyperparameters. We train each model for 10 epochs with a batch size of 16 and a learning rate of 5e-6 with a cosine learning rate scheduler. Moreover, we only compute negative log-likelihood loss over the completions. We use LORA (Hu et al., 2021) with a rank of 16, a = 32 with a dropout of 0.05. We perform checkpoints selection based on the intrinsic Acc. Attack metric."}, {"title": "C. Overall Pipeline for UTDEBUG", "content": "In Algorithm 1 we describe the process of generating UTs for a candidate buggy code ft using any UT generation method and perform test time scaling for UT output prediction. This is then used within UTDEBUG as shown in Algorithm 2, which identifies failing UTs, debugs the target code fo based on feedback from failing UTs over multiple rounds, and returns the debugged (edited code). As illustrated in Figure 3 (b), edits are accepted only if the newly generated code achieves a higher pass rate than the code before debugging, otherwise the edits are discarded.\nInference Hyperparameters. When sampling multiple UTs and for generating LLMs response to UT feedback we use temperature sampling with temp=0.7 and top-p=0.9."}, {"title": "D. Additional Intrinsic Evaluation", "content": "Similar to Table 1, we report intrinsic evaluation metrics (cf. Section 4) for HE+Fix and MBPP+Fix datasets in Table 5. Consistent with the findings in Section 5.1, without training, we observe a trade-off between attack rate and output accuracy, with randomly sampled UTs showing higher output accuracy whereas the prompted baseline exhibits higher attack rates and vice-versa. Once again UTGEN training best balances this trade-off yielding both high output accuracy and attack rate. However, due to the relative ease of attacking faulty codes in HE+Fix (initial pass rate of 0%) almost any generated UT fails and thus can be used for debugging. This, coupled with the higher output accuracy, results in the random baseline having the highest score on Acc. \u2229 Attack. Note that we mitigate this impact on downstream performance by using test-time scaling for output prediction in UTDEBUG, which especially boosts the accuracies of targeted UTs generated based on the buggy code. On"}, {"title": "E. Prompts", "content": "In the following pages, we list all the prompts we use."}]}