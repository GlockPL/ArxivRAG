{"title": "An Efficient Inference Framework for Early-exit Large Language Models", "authors": ["Ruijie Miao", "Yihan Yan", "Xinshuo Yao", "Tong Yang"], "abstract": "Building efficient inference framework has gained increasing interests for research community. Early-exit models, a variant of LLMs, improves the inference efficiency of LLMS by skipping rest layers and directly generate output tokens when they are confident enough. However, there is no work of LLM inference framework that takes early-exit models into consideration. This is non-trivial as prior art on LLM inference cannot be directly applied to early-exit models. In this work, we solves two key challenges in building efficient inference framework for early-exit models: (1) batch inference at iteration-level granularity; and (2) KV cache management. For the former, we propose to process the batch until all sequences surpass the early-exit confidence threshold. For the latter, we propose to fill the KV cache of rest layers before the iteration terminates. Our evaluation shows that, compared with the original vLLM operating at full layers, our solution achieves up to 1.25\u00d7 speed up.", "sections": [{"title": "Introduction", "content": "In recent years, various Large Language Models (LLMs) (OpenAI 2023; Touvron et al. 2023; Chowdhery et al. 2023; Hoffmann et al. 2022) have achieved excellent performance in different tasks, including translation (Zhang, Haddow, and Birch 2023; Dabre, Chu, and Kunchukuttan 2020), classification (Dixon et al. 2018; Schick and Sch\u00fctze 2020), question answering (Khashabi et al. 2020; Kwiatkowski et al. 2019; Mihaylov et al. 2018), and more (Lin et al. 2021; Zhang et al. 2024). The wide applications of LLMs in real-world scenarios lead to an increased demand for the inference services. Researchers have paid much attention to optimize the inference framework, such as FasterTransformer (NVIDIA 2023), Orca (Yu et al. 2022), vLLM (Kwon et al. 2023). The goal is to improve the serving throughput and efficiently manage the GPU memory.\nEarly-exit models (Schwartz et al. 2020; Sun et al. 2022; Elbayad et al. 2019; Schuster et al. 2021, 2022) have been proposed as a promising variant of LLMs. Normal LLMs achieves good performance with huge model size and large number of model layers, which consumes much computation resources during the inference. Early-exit models observes that such large number of layers is often unnecessary and \"skipping\" some layers during the inference can result in comparable performance. Specifically, during the inference of each token, every layer will determine whether the current output is sufficiently accurate. If it is, the remaining layers can be skipped to directly obtain the output token. By skipping layers, early-exit models can reduce the computation load of the decoding process, and therefore the serving throughput can be improved. As the inference speed of LLMs gains increasing attention, the advantages of early-exit models in enhancing inference speed are being increasingly recognized. For example, Adainf (Shubha and Shen 2023) applies the early-exit technique to accelerate the inference. We believe the early-exit models can be promising alternatives, especially for LLM inference.\nIn this work, we aim to build efficient inference framework for early-exit LLMs. Prior work on LLM inference framework is designed for common LLMs, and as far as we know, there is no work to optimize the inference framework of the early-exit LLMs. Actually those work that proposes early-exit models usually tests them with batch size equal to 1. Directly applying the inference framework of normal LLMs to the early-exit LLMs is inefficient due to following reasons. First, the early-exit LLMs explicit different inference process, and therefore brings new opportunity in improving the inference efficiency. Prior inference frameworks on the normal LLMs do not take the early-exit techniques into considerations, and therefore can miss the opportunity to skip the extra computation overhead. Besides, the skipping behaviour brings challenges in the KV cache management, as the KV tensors are not computed for skipped layers. Building inference framework for early-exit LLMs also requires re-designing of the KV cache management module.\nWe propose a unified framework for accelerating the inference of early-exit LLMs, supporting different early-exit techniques. Our solution is based on the prior art of iteration-level scheduling (Yu et al. 2022), which batches the request sequences and breaks the decoding procedure into multiple iterations, each iteration decoding one token for each sequence in the batch. To support early-exit LLMs, for each iteration we record whether the current request sequences in the batch can exit early. Every layer we update the early-exit status for those request sequences that are determined to be eligible for early exit. Once all sequences in the batch are marked as eligible for early exit, we terminate the current"}, {"title": "Background and Motivation", "content": "In this section, we first introduce the early-exit strategies in the neural network models in \u00a72.1. Then we discuss current works on the inference systems for transformer-based models in \u00a72.2."}, {"title": "Early-exit Models", "content": "Accelerating the inference of the neural network models is an important topic in machine learning. Early-exit model is a popular method for speeding up inference. When generating a token, early-exit models may stop at early layers and directly generate a token without executing the rest of layers. Compared with normal models, early-exit models execute much less number of layers in average. The key problem is how to decide whether the model should early exit at one layer. Intuitively, if the model is \u201cconfident\" that at the current layer they can generate good enough results, they can skip the rest layers. Below we discuss several ideas for building confidence measurement for early-exit models.\n\u2022 Softmax Response: The idea is to compute the difference of the top two candidates as the measurement of confidence. If the gap is large, then the model is confident that it should use the top-1 candidate. Suppose the model uses a matrix W and Softmax operator after all layers as the classifier, and in the ith layer the hidden state is $h_i$ and the confidence threshold is $\u03bb_i$. Then We compute,\n$\\text{Top-1}(\\text{Softmax}(W h_i))-\\text{Top-2}(\\text{Softmax}(W h_i)) > \\lambda_i$\nto decide whether to exit early at the ith layer.\n\u2022 Hidden states similarity: the idea is to compute the similarity of two neighboring hidden states as the measurement of confidence. If the similarity of two hidden states is high, we can infer that for the rest layers the resulting hidden states will also be similar (Geva et al. 2022), and thus we can exit early at the current layer. Suppose in the i \u2212 1th layer and the ith layer the hidden states are $h_{i-1}, h_i$, respectively, the confidence threshold is $\u03bb_i$ and cosine similarity is used. Then we compute,\n$\\cos(h_{i-1}, h_i) > \\lambda_i$\nto decide whether to exit early at the ith layer.\n\u2022 Dedicated early-exit classifier: the idea is to introduce a dedicated classifier M to decide whether to exit early. Suppose in the ith layer the hidden state is $h_i$, and the confidence threshold is $\u03bb_i$. Then we compute,\n$M(h_i) > \\lambda_i$\nto decide whether to exit at the ith layer.\nExisting works on early-exit transformer-based models\nThe idea of early-exit can be applied to wide ranges of transformer-based models. CALM (Schuster et al. 2022) applies the above three ideas on the T5 model (Raffel et al. 2020) to build early-exit language models. MuE (Tang et al. 2023) applies the idea of hidden states similarity on the OFA model (Wang et al. 2022) to build early-exit vision language models. Their code has been merged to the official OFA repository. Adainf (Shubha and Shen 2023) applies the early-exit models to accelerate the inference at the edge servers."}, {"title": "Inference Systems for Transformer-based Models", "content": "Recently, researchers have made significant progress in building efficient inference systems for transformer-based models. Two representative works are Orca (Yu et al. 2022) and vLLM (Kwon et al. 2023). Orca studies how to build an efficient inference system for inference on batches of sequences. Orca breaks the serving of a sequence batch into multiple iterations, each involving the generation of one token. During the serving of the batch, some sequences may finish early while others are still generating tokens. Orca applies the key idea of iteration-level scheduling, which evicts finished sequences and replenishes new ones after each iteration of token generation. Orca further explores how to utilize the parallelism within batch inference to speed up the inference.\nVLLM points out that the GPU memory becomes a bottleneck for inference, of which the KV cache consumes an large part. For transformer-based models, KV cache is a common technique for accelerating the inference at the cost of more GPU memory consumption. The KV cache stores key and value pairs generated at each iteration for every layer, which can be reused at the attention computation of the same layer for the following token generation. vLLM finds that pre-allocating GPU memory for KV cache of the maximum length of sequences is wasting, as the sequences may finish early. They propose a virtual KV cache management system, which utilizes the idea of virtual memory and dynamically allocate KV cache memory when it is necessary.\nOur proposed solution borrows the optimization ideas from previous approaches, specifically including: (1) batch inference, (2) iteration-level scheduling, and (3) efficient KV cache manangement."}, {"title": "System Design", "content": "In this section we introduce the design of our inference framework."}, {"title": "Batch Inference at Iteration-level Granularity", "content": "Prior works on the LLM inference show the benefits of running inference for a batch of sequences in a iteration-level granularity. We aim to achieve batch inference at iteration-level granularity for the early-exit LLMs. On each iteration of token generation, our framework additionally maintains the early-exit status for each sequence in the batch. Initially, all sequences are marked as eligible for early-exit. After each layer of computation, according to the applied early-exit techniques of the early-exit models, each sequence will update its early-exit status. Specifically, if the early-exit status of one sequence is false, and at the current layer it successfully surpasses the early-exit confidence, then the early-exit status is updated to true. Once all the sequences' early-exit status are true, then the batch is considered for early exit. It should be noticed that if the early-exit status of one sequence is already true but the current layer determines that it fails to surpass the early-exit confidence, we still consider the early-exit status to be true. We believe this is due to some fluctuations caused by the early exit mechanism. Generally the more layers the inference go through, the more confident they will be to their generated tokens."}, {"title": "KV Cache Management", "content": "The prior work on KV cache management, VLLM as the representative, targets at the normal LLMs and cannot be directly applied at the early-exit LLMs. This is because for the normal LLMs, after one iteration, the key and value pairs of all layers at the position are generated and cached. However, in early-exit LLMs, the key and value pairs after the early-exit layer are not calculated. Therefore, if the generation of one token requires computation of higher layers while one previous generated token early exits at a lower layer, then the key and value pairs of that token are missed. However, if we follow the normal LLM inference process, we should go through the rest layers to fill the KV cache, which makes early exit meaningless. Fortunately, the prior works (Elbayad et al. 2019; Schuster et al. 2022) point out that, when one iteration early exits at the lower layer, the generated final hidden states can be saturated to the higher layers. Therefore, our strategy is as follows: when executing one iteration, we first follow normal computation layer by layer, calculate key and value pairs and cache them in the KV cache in the process. Then, if the batch decide to early exit at one layer, before the termination of the current iteration, we pass the output hidden states of the last for the KV cache filling for the rest layers. We use the output hidden state to calculate the key and value pairs of the following layers and store them in the KV cache, which only involves one matrix multiplication operations per layer. In our strategy, the early-exit layer requires additional computation for filling KV cache of the rest layers. The final pseudo code of the inference process is shown in Algorithm 1."}, {"title": "Implementation", "content": "We implement a system prototype of our proposed solution based on vLLM. The original vLLM is fully designed for decoder-only models, while our target early-exit model, CALM, is finetuned on the encoder-decoder model T5. Therefore, our implementation is based on the pull request that implements encoder-decoder architecture\u00b9. We verify our implementation remains the same results for inference compared with standard implementation of the Huggingface transformers library."}, {"title": "Training Details of Early-exit Models", "content": "To evaluate our proposed early-exit inference acceleration framework, we reproduced the early-exit models described in (Schuster et al. 2022) using T5 v1.1 small (8 layers) and base (12 layers) as the backbone architectures. Specifically:\n\u2022 We implemented the early-exit mechanism for the decoder, allowing exits after each transformer layer in the decoding process.\n\u2022 We used the same three confidence measures as the original work: softmax response, hidden-state similarity, and early-exit classifier."}, {"title": "Evaluation", "content": "In this section, we evaluate the inference efficiency of our proposed framework by comparing it with full-layers inference on the original vLLM. Our experiments are done on a server with dual 26-core CPUs (104 threads, Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz) and 1 TB memory. The server is equipped with 6 NVIDIA RTX 4090 GPUs. For the experiment, we only use one GPU to evaluate the inference performance on single GPU.\nDatasets: The evaluation is conducted for the news summary task on the CNN/DM (Hermann et al. 2015) dataset, which contains a collection of news articles and corresponding summaries. The dataset is split to three parts: the training dataset, the validation dataset and the test dataset, which contains 287k, 13.4k, 11.5k rows separately. We use the test dataset for evaluation, where we select rows with the length of news less than 1024, and uses the first 512 tokens."}, {"title": "Token Generation Throughput", "content": "We compare the token generation throughput of our solution with the original full-layers vLLM on both the T5-v1.1-small and T5-v1.1-base models. Token generation throughput is defined as the total number of tokens generated divided by the total time of the inference. The results are shown in Figure 1.\nFor the T5-V1.1-small model, the original vLLM running full-layers inference generates tokens at the speed of 1046.47 tokens/s. Our proposed solution achieves token generation speed of 1081.04, 1165.73, 1065.13 tokens/s for"}, {"title": "Inner-token Latency", "content": "We also compare the inner-token latency of our solution with the original full-layers vLLM on both two models. For each sequence, we calculate the difference between its finish time and the first-token time as its decoding latency. The inner-token latency is defined as the sum of decoding latency for all sequences divided by the total number of generated tokens. The results are shown in Figure 2.\nFor the T5-V1.1-small model, the original vLLM running full-layers has inner-token latency of 0.234s. Our proposed solution achieves inner-token latency of 0.075, 0.069, 0.079s for three different early-exit techniques, respectively. For the T5-V1.1-base model, the original vLLM running full-layers has inner-token latency of 0.176s. Our proposed solution achieves inner-token latency of 0.076, 0.066, 0.078s for three different early-exit techniques, respectively.\nIn summary, our proposed solution consistently achieves lower inner-token latency compared with the original vLLM for different early-exit mechanisms. Our solution shows up to 3.39\u00d7 reduction in the inner-token latency."}, {"title": "Conclusion", "content": "Building efficient inference framework has gained increasing interests for research community. In this work, we aim to build efficient inference framework for early-exit models, which are variants of LLMs that skip rest decoding layers when confident enough. Our proposed framework considers the following two aspects. (1) batch inference at iteration-level granularity; and (2) KV cache management. For the former, we process the batch until all sequences surpass the early-exit confidence threshold. For the latter, we fill the KV cache of rest layers before the iteration terminates. We reproduce and train the state-of-the-art early-exit model CALM and evaluate our inference framework. Our evaluation shows that, compared with the original vLLM operating at full layers, our solution achieves up to 1.25\u00d7 speed up. The results also show the hidden states similarity may be the best mechanism to implement early-exit models when only considering the inference efficiency."}]}