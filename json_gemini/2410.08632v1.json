{"title": "Words as Beacons: Guiding RL Agents with\nHigh-Level Language Prompts", "authors": ["Unai Ruiz-Gonzalez", "Alain Andres", "Pedro G. Bascoy", "Javier Del Ser"], "abstract": "Sparse reward environments in reinforcement learning (RL) pose significant chal-\nlenges for exploration, often leading to inefficient or incomplete learning processes.\nTo tackle this issue, this work proposes a teacher-student RL framework that\nleverages Large Language Models (LLMs) as \"teachers\" to guide the agent's learn-\ning process by decomposing complex tasks into subgoals. Due to their inherent\ncapability to understand RL environments based on a textual description of struc-\nture and purpose, LLMs can provide subgoals to accomplish the task defined\nfor the environment in a similar fashion to how a human would do. In doing so,\nthree types of subgoals are proposed: positional targets relative to the agent, object\nrepresentations, and language-based instructions generated directly by the LLM.\nMore importantly, we show that it is possible to query the LLM only during\nthe training phase, enabling agents to operate within the environment without\nany LLM intervention. We assess the performance of this proposed framework by\nevaluating three state-of-the-art open-source LLMs (Llama, DeepSeek, Qwen) elic-\niting subgoals across various procedurally generated environment of the MiniGrid\nbenchmark. Experimental results demonstrate that this curriculum-based approach\naccelerates learning and enhances exploration in complex tasks, achieving up to 30\nto 200 times faster convergence in training steps compared to recent baselines\ndesigned for sparse reward environments.", "sections": [{"title": "1 Introduction", "content": "Let us imagine a scenario where we must master a new skill where feedback is only provided after\nlong periods of effort, i.e., no guidance, no information about the progress, just endless trial and\nerror. This is the experience of many Reinforcement Learning (RL) agents operating in sparse reward\nenvironments, where the scarcity of feedback makes the learning process slow and inefficient. In\nsuch environments, the traditional approach of random exploration\u2014where an agent learns by trying\nout different actions and receiving rewards only occasionally-often falls short. The randomness\ninherent to this exploration strategy poses significant learning challenges, as agents must rely on\nserendipity to encounter rewarding states. Without frequent or consistent feedback, the agent may\nspend vast amounts of time exploring irrelevant or ineffective actions, making the learning process\nhighly resource-intensive. Yet, even with impressive achievements [Silver et al., 2016, Mnih et al.,\n2015], RL agents still require assistance in environments where rewards are infrequent and learning\nbecomes more challenging."}, {"title": "2 Related work", "content": "Curriculum Learning (CL) Organizing learning tasks in a structured sequence has emerged as\nan effective strategy in training models. Originally conceived as \"train from easy to hard\", CL has\nattracted attention for its potential to expedite learning processes [Elman, 1993, Bengio et al., 2009].\nHowever, its efficacy is not universally consistent across implementations, where the curriculum must\nbe well-defined in order to be effective, as indicated by several studies [Graves et al., 2017, Hacohen\nand Weinshall, 2019, Platanios et al., 2019]. These findings suggest that while CL holds promise, its\napplication requires careful considerations.\nCurriculum Learning in RL is brought to life through RL teachers, which constitute a dynamic\nmethodology for guiding RL processes. One notable advancement in this area is the work by\n[Florensa et al., 2017] who developed a generative adversarial network to create goals of suitable"}, {"title": "3 Methodology", "content": "As depicted in Figure 1, our proposed goal-oriented policy framework introduces a subgoal $g_n$,\nprovided by the LLM, to the observation $o_t$ of and the reward signal $r_t$. Acting as an omniscient\nsupervisor, the LLM has full access to the environment's initial state $s_0$. At the start of each episode,\nit generates a sequence of subgoals $g_0, g_1,...,g_n$, where N + 1 represents the total number of\nsubgoals. During the episode, in each time step, the agent receives its current observation $o_t$, the\nreward signal $r_t$, and the relevant subgoal $g_n$ from this sequence. Once a subgoal $g_n$ is achieved, the\nagents transitions to the next subgoal in the sequence $g_{n+1}$ at the subsequent time step t + 1. This\ntransition implies that $g_{n+1} \\neq g_n$, ensuring that the agent progresses through a sequence of unique\nsubgoals.\nIn this framework, the RL agent learns a policy $\\pi(a_t | o_t, g_n)$ designed to maximize the expected\ncumulative reward $\\sum_{k=0}^T \\gamma^k \\cdot R_{t+k+1}(s_t|g_n)$ given both the observation and the current subgoal."}, {"title": "3.1 Subgoal generation", "content": "Subgoal generation takes place at the beginning of each episode, where the LLM, with access\nto the complete environment state $s_0$, creates a sequence of subgoals ${g_0, g_1, g_2, ..., g_n}$, which\nprogressively guides the agent towards the final goal of the RL task, reducing the reward horizon. In\naddition to the extrinsic reward $r_e$ coming from the tasks at hand, we introduce an auxiliary reward\n$r_f$ that incentives the achievement of subgoals. Therefore, the reward that the agent receives in each\ntime step is given by: $r_t = r_e + \\alpha r_f$, where $\\alpha \\in R^+$ is a scaling hyperparameter that balances the\nweight of subgoal completion with respect to the overall task achievement. The reward $r_f^g$ for each\nsubgoal $g \\in {g_1, ..., g_N }$ is defined as:\n$r_f^g = \\frac{1}{(\\frac{t_g}{t_{max}})}$       (1)\nwhere $t_g$ denotes the number of steps taken to achieve a specific subgoal, and $t_{max}$ represents the\nmaximum number of steps allowed in each episode, which acts as a normalization component.\nWe propose providing subgoals to the agent in three ways: as positions, as representations, and as\nlanguage embeddings.\nPosition-based Subgoals: These involve defining {x,y} coordinates to indicate locations of the\npredicted subgoals, focusing on relative positions due to the agent's lack of absolute awareness. This\nmethod was expected to be effective, but the agent's dependence on relative directions can make long-\nterm navigation challenging. Additionally, LLMs, which are not optimized for this task, can struggle\nwith accurately identifying object locations within 2D grids, leading to potential misalignments.\nRepresentation-based Subgoals: These subgoals use a grid encoding scheme to reflect how objects\nare perceived by the agent. This approach aligns subgoals with the agent's internal representation of\nthe environment, allowing for unique and semantically meaningful identification of objects. While\nthis method can enhance interaction with specific features, reliance on a fixed grid scheme might\nlimit the agent's adaptability to new, potentially diverse environments. For example, in an open-world\nscenario, a new type of object might appear.\nLanguage-based Subgoals: These subgoals utilize embeddings generated from a Language Model,\nconverting LLMs text output into embeddings to provide guidance in a more abstract and flexible\nform. This approach offers the potential for high adaptability and generalization across diverse tasks\nand environments. However, due to the inherently stochastic nature of LLMs, the output may vary\nfrom one instance to another. This variability can result in different phrasings for the same subgoal,\neffectively expanding the pool of potential subgoals."}, {"title": "3.2 Prompt engineering", "content": "Prompt engineering is crucial for optimizing the effectiveness of LLMs [Brown et al., 2020, Reynolds\nand McDonell, 2021], with Chain of Thought (CoT) being a mainstream technique for enhancing the\nperformance of LLMs. CoT guides LLMs to articulate their reasoning process, thereby improving\ntheir ability to generate relevant subgoals by breaking down complex tasks into logical steps. In our\nstudy, we employ Zero-shot CoT [Kojima et al., 2022] to ensure that subgoal generation is driven by\nthe LLM's inherent reasoning rather than biased by example-based prompts. This approach allows"}, {"title": "3.3 Statistical modeling of LLMs", "content": "Our initial approach uses LLMs to generate subgoals at the beginning of each episode to guide the\nagent through complex tasks. However, training in PCG environments often requires thousands to\nmillions of episodes/levels. Relying on the LLM to generate subgoals every time a new episode begins\nmay not be scalable when dealing with such a large number of episodes, due to resource-intensive\nfactors like latency, hardware requirements, and API costs (e.g., when using services like ChatGPT).\nTo mitigate these scalability issues, we propose an offline subgoal modeling method, shown in\nAlgorithm 1.\nIn our proposed method, prior to the training stage, we prompt the LLM with the initial state $s_0$\nacross a randomly selected subset of levels from the environment's entire level distribution. This\nprocess results in M levels, from which we collect a set of subgoal proposals for each level. To\nprovide optimal guidance for the agent, we assume access to an Oracle (e.g., a human expert) who can\ndecompose each level and propose optimal subgoals. These Oracle-provided subgoals are meaningful\nand logical within the environment, aligning perfectly with the overall task objectives. By comparing\nthese optimal subgoals to the LLM's proposals, we model the discrepancies (i.e., errors) between\nthe LLM's output and the desired subgoal decomposition. This allows us to build a statistical model\nthat generates subgoals for any level in the environment's distribution during training, enabling the\nagent to train over the entire level distribution-not just the M levels used for subgoal collection. As\na result, we eliminate the need to query the LLM over hundreds of thousands of episodes during the\ntraining phase."}, {"title": "4 Experiments and results", "content": "To evaluate the effectiveness of our proposed method, we conduct experiments across several challeng-\ning environments within the MiniGrid framework [Chevalier-Boisvert et al., 2023]: MultiRoomN2S4,\nMultiRoomN4S5, KeyCorridorS3R3, KeyCorridorS6R3, ObstructedMaze1Dlh and Obstruct-\nedMaze1Dlhb. We use an implementation [Andres et al., 2022] of Proximal Policy Optimization\n(PPO) [Schulman et al., 2017] to test our LLM-generated subgoals. For detailed information on the\nselected hyperparameters and model architecture, please refer to Appendix C.\nIn our experimentation, we use the modeled LLMs introduced in Section 3.3 to generate subgoals\nat the beginning of each episode. The LLM receives a full observation of the environment, $s_0$ to\ngenerate the subgoals, while, the agent has access only to a 7 \u00d7 7 \u00d7 3 observation tensor representing\nthe area directly ahead. We evaluate the performance of three open-source LLM models: Llama3-70b\n[Touvron et al., 2023], Qwen1.5-72b-chat [Bai et al., 2023], and DeepSeek-coder-33b [Bi et al., 2024],\nand analyze their impact during training when using each type of proposed subgoals\u2014representation,\nrelative, language.\nTo improve scalability and practicality, we further investigate the effects of omitting rewards and\nsubgoals during training using the top-performing LLM model and the most effective type of subgoal\nidentified in previous analyses. We evaluate two scenarios: (1) no reward, where the agent receives\nsubgoals but no additional rewards for achieving them, preserving the reward horizon for assessing\nperformance without immediate rewards; and (2) no subgoal, where subgoals are excluded from the\nagent's observation space, but rewards are still provided upon subgoal completion."}, {"title": "5 Conclusions", "content": "This work has evinced that LLMs can be harnessed as subgoal generators, to yield a valuable strategy\nfor training RL agents to tackle tasks characterized by reward sparsity. By applying curricular learning\ntechniques, we can significantly reduce the reward horizon, accelerating the training process. The\nknowledge embedded in LLMs facilitates faster and more effective training. Furthermore, if\ntrained without including the subgoals in the observation, the agent does not need to query the\nLLM during the deployment phase, reducing computational overhead and costs.\nOur proposed framework, while effective, has some limitations and avenues for future research.\nCurrently, when it comes to language-based subgoals, we rely on direct text output by the LLM,\nwhich can vary significantly and may present challenges in managing the volume of possible subgoals.\nFuture efforts will focus on filtering these outputs against a predefined pool of subgoals, aiming to\nreduce ambiguities and redundancies observed in the linguistic description of subgoals elicited by\nthe LLM and ultimately, improve the agent's guidance through the produced subgoals. Additionally,\nextending our approach to a broader range of benchmarks will help evaluate its generalizability.\nAnother key area for future development is designing a more adaptive teaching strategy that can\ndynamically adjust to the agent's evolving capabilities. For instance, as observed when using\nDeepSeek, there are still limitations due to insufficient reduction in the reward horizon in some\nenvironments. Lastly, investigating the effects of strict adherence to LLM guidance could further\nenhance the agent's generalization across diverse tasks, when encountering new tasks that are similar\nto those previously learned by the agent."}, {"title": "A Prompts used for subgoal generation", "content": "In this appendix, we detail the prompts used to instruct the LLM for subgoal generation at the start of\neach episode. These prompts are provided sequentially, using a CoT approach to maintain context\nand build upon previous responses.\nFigure 5 illustrates the initial prompt, where the LLM is asked about its knowledge of MiniGrid. This\nstep establishes a foundational understanding of the environment, which is crucial for generating\nrelevant subgoals."}, {"title": "B Subgoal types with illustrative examples", "content": "In this appendix, we present examples of the different types of subgoals used throughout our\nexperiments, including relative, representation-based, and language-based subgoals."}, {"title": "B.1 Relative-based subgoals", "content": "As shown in Figure 8, relative subgoals are represented as a 2-dimensional vector. The first value\ncorresponds to the distance along the x-axis, while the second value represents the distance in grid\nblocks along the y-axis. With this type of subgoals, the agent learns to interpret the direction it must\nmove in to reach the subgoal, adjusting its navigation based on the relative position provided."}, {"title": "B.2 Representation-based subgoals", "content": "As shown in Figure 9, representation subgoals are encoded as a 3-dimensional vectors. The first value\ndenotes the type of object, the second represents the object's color, and the third indicates its state.\nThere are 11 possible object types, including unseen (0), empty (1), wall (2), floor (3), door (4), key\n(5), ball (6), box (7), goal (8), lava (9), and agent (10). Since representation subgoals are limited\nto interactable objects within the environment, the subgoal space is restricted to doors, keys, balls,\nboxes, and goals. For colors, six options are available: red (0), green (1), blue (2), purple (3), yellow\n(4), and grey (5). The state value is particularly relevant for doors, with options for open (0), closed\n(1), and locked (2)."}, {"title": "B.3 Language-based subgoals", "content": "As shown in Figure 10, language-based subgoals are represented as embeddings retrieved from a\nLanguage Model based on the output text of the LLM. We use all-MiniLM-L6-v2 as the Language\nModel, which generates a 384-dimensional vector for each subgoal. For instance, in the subgoal\nillustrated in Figure 10 the embeddings correspond to the output text \"Go to the key.\" The resulting\nembedding size is significant, and the subgoal space is extensive since the output is directly taken\nfrom the LLM."}, {"title": "C Model architecture and hyperparameter configuration", "content": "This appendix provides detailed information on the architecture and hyperparameters used in our ex-\nperiments. The algorithm employed for training was Proximal Policy Optimization (PPO), combined\nwith a Convolutional Neural Network (CNN) architecture designed to effectively handle the state\nrepresentation in MiniGrid environments."}, {"title": "C.1 Model architecture", "content": "The architecture of our actor-critic model consists of a series of convolutional and fully connected\nlayers. Initially, the model employs three convolutional layers to extract features from the grid\nobservation. Each convolutional layer is defined with 32 filters, a 3 \u00d7 3 kernel, a stride of 2 \u00d7 2, and\npadding of 1, followed by an ELU activation layer.\nFollowing the convolutional layers, the model includes a shared fully connected layer that maps\nthe 32-dimensional output from the convolutional layers to a 256-dimensional vector. This shared\nrepresentation is then processed by two separate, fully connected networks: one for the actor and one\nfor the critic. The actor network uses a fully connected layer to map the 256-dimensional vector to a\nvector of size 7, corresponding to the number of possible actions. Meanwhile, the critic network uses\nanother fully connected layer to map the 256-dimensional vector to a scalar value representing the\nestimated value of the current state."}, {"title": "C.2 Hyperparameters values", "content": "Table 1 summarizes the hyperparameters used in our PPO implementation. Each hyperparameter\nplays a specific role in shaping the learning process:\nThe hyperparameter values in this study were selected based on the findings from Andres et al.\n[2022], where they were meticulously tuned for MiniGrid tasks, ultimately identifying an optimal\nconfiguration. By adopting this configuration, we aim to leverage proven settings that have demon-\nstrated success in similar environments, thereby enhancing the reliability and performance of our\nPPO implementation."}, {"title": "D Statistical modeling results", "content": "We conducted a comprehensive evaluation by querying three LLMs Llama3, DeepSeek, and\nQwen1.5 across 1000 distinct instances or levels in each of the six environments. This extensive\nquerying allowed us to gather and analyze their performance metrics, including accuracy and error\nrates.\nTable 2 presents a detailed comparison of these LLMs across the six environments. The metrics used\nfor evaluation include:\n\u2022 Accuracy: Measures the percentage of correct predictions made by each LLM out of the total\npredictions.\n\u2022 Correct SGs/Total: Indicates the number of correct subgoals (SGs) identified by the LLM\ncompared to the total number of subgoals it was asked to predict.\n\u2022 Correct EPs/Total: Measures the number of complete correct episodes (EPs) predicted by the\nLLM relative to the total number of episodes.\n\u2022 Manhattan Dist.: Represents the average Manhattan distance between the predicted and actual\npositions, including the standard deviation, providing a measure of spatial accuracy.\nFor position subgoals, a subgoal is considered correct if the specified position matches the actual\nposition of the object. For example, a prediction like \"pickup key (2,3)\" is correct if a key is indeed\nlocated at (2,3). If the predicted position does not match, the error is quantified using the Manhattan\ndistance between the proposed and actual subgoal positions.\nIn contrast, for representation subgoals, a subgoal is deemed correct if the object name (and color,\nif relevant) is accurately described. For instance, \"pickup key (2,3)\" would be correct if the object\n\"key\" is mentioned in the description. Errors in representation are not directly quantified in this phase\nbut are addressed in subsequent modeling stages by selecting alternative objects from the grid.\nLlama consistently outperforms the other models in both position and representation subgoals.\nConversely, DeepSeek, with its simpler 33 billion parameter architecture, often shows the lowest\nperformance compared to Llama's 70 billion parameters."}]}