{"title": "Hierarchical Structured Neural Network for Retrieval", "authors": ["Kaushik Rangadurai", "Siyang Yuan", "Minhui Huang", "Yiqun Liu", "Golnaz Ghasemiesfeh", "Yunchen Pu", "Xinfeng Xie", "Xingfeng He", "Fangzhou Xu", "Andrew Cui", "Vidhoon Viswanathan", "Yan Dong", "Liang Xiong", "Lin Yang", "Liang Wang", "Jiyan Yang", "Chonglin Sun"], "abstract": "Embedding Based Retrieval (EBR) is a crucial component of the retrieval stage in (Ads) Recommendation System that utilizes Two Tower or Siamese Networks to learn embeddings for both users and items (ads). It then employs an Approximate Nearest Neighbor Search (ANN) to efficiently retrieve the most relevant ads for a specific user. Despite the recent rise to popularity in the industry, they have a couple of limitations. Firstly, EBR relies on the Siamese Network model architecture which restricts the interaction on both the model (dot-product interactions) and feature space (only user or ad features). Secondly, ANN is a post-training operation and is not aware of the retrieval model optimization criteria. In this paper, we present Hierarchical Structured Neural Network (HSNN), a deployed jointly optimized hierarchical clustering and neural network model that can take advantage of sophisticated interactions and model architectures that are more common in the ranking stages while maintaining a sub-linear inference cost. We achieve 6.5% improvement in offline evaluation and also demonstrate 1.22% online gains through A/B experiments. HSNN has been successfully deployed into the Ads Recommendation system and is currently handling major portion of the traffic. The paper shares our experience in developing this system, dealing with challenges like freshness, volatility, cold start recommendations, cluster collapse and lessons deploying the model in a large scale retrieval production system.", "sections": [{"title": "1 INTRODUCTION", "content": "Machine learning algorithms and recommender systems play a vital role in identifying the potential interest of an ad for a specific user, which ultimately enhances the efficiency of the Ad marketplace. In order to tackle a very large number of candidate ads per request, a common practice in the industry [5, 10] is to build a cascade of recommendation systems of increasing computational cost. In this paper we focus on the first stage - known as the retrieval stage that narrows down from millions of candidates to a few thousands.\nThe Retrieval stage is limited by strict infrastructure constraints, making model architecture like Siamese Networks [2] a common choice. Siamese networks, or also called as Dual Encoder or Two Tower, incorporate a late fusion technique. Each of the towers output a fixed-sized representation (embedding) and rely on dot-product interaction. Embedding Based Retrieval (EBR) [17] using approximate nearest neighbors (ANN) search [19] algorithms are commonly used to retrieve top relevant candidates.\nDespite their popularity in the industry [7, 9, 13, 15], EBR has a few disadvantages. Firstly, EBR relies on the Siamese Network model architecture which restricts the interaction on both the model (dot-product interactions) and feature space (only user or ad features). Secondly, ANN is a post-training operation and is not aware of the retrieval model optimization criteria.\nWe propose Hierarchical Structured Neural Network (HSNN) that can be used as a drop-in replacement to any Embedding Based Retrieval (EBR) system. In HSNN, ads are organized into a hierarchy of clusters that are jointly trained with neural network. Under this framework, it is possible to adopt advanced ML model architectures and interaction features which rely on both the user and ad that are more commonly found in the ranking system. To learn the hierarchical structure, we introduce a gradient-descent based clustering algorithm - Learning To Cluster (LTC) that co-trains both the hierarchical structure and the retrieval neural network together.\nThere were several challenges in building Hierarchical Structured Neural Network:\n\u2022 Cardinality and Volatility: We handle a large volume of ads, which have a short lifespan of only a few days or weeks. This poses significant challenges in maintaining up-to-date and relevant cluster indices and representations.\n\u2022 Embedding Distribution Shift: The retrieval model is trained in an online continuous manner which enables it to stay up-to-date with new ads. However, with online training, the retrieval model keeps publishing new snapshots. This implies that even for the same inference input, the computed user and ad embeddings are changing over time. This presents additional challenges when constructing extensive embedding indices for ad retrieval.\n\u2022 Cluster Distribution: It is common practice that while developing clustering algorithms, most of the items belong to very few clusters. This problem is known as cluster collapse. In order to build a scalable retrieval system with sub-linear inference cost, it is imperative that the cluster distribution is more uniform.\nThe primary contributions of the paper include:\n\u2022 We introduce a Hierarchical Structured Neural Network model that can take advantage of sophisticated interactions and model architectures that are more common in the ranking stages while maintaining a sub-linear inference cost.\n\u2022 We propose a jointly optimized cluster learning framework that can be integrated to any model architecture.\n\u2022 We demonstrate the effectiveness of our proposed approach and showcase substantial performance improvements in both offline evaluation and online A/B experiments."}, {"title": "2 RELATED WORK", "content": "Clustering Methods. At a high level, clustering algorithms can be generally classified into two main categories: hierarchical and partitional methods. Agglomerative clustering is a hierarchical clustering algorithm that starts with many small clusters and gradually merges them together. Regarding partitioning clustering techniques, the most widely recognized is K-means [23], which aims to minimize the sum of squared distances between data points and their closest cluster centers. Other line of work in this category include expectation maximization (EM) [6], spectral clustering, and non-negative matrix factorization (NMF) [3] based clustering.\nEmbedding Based Retrieval. Embedding based retrieval (EBR) has been successfully applied in retrieval layers for search and recommendation systems [22, 29]. [17] extends this idea to incorporate text, user, context and social graph information into a unified embedding to retrieve documents that are not only relevant to the query but also personalized to the user. On the system side, a significant number of companies have built or deployed approximate nearest neighbor algorithms that can find the top-k candidates given a query embedding in sub-linear time [7, 9, 15]. Efficient MIPS or ANN algorithms include tree-based algorithms [16, 27], locality sensitive hashing (LSH) [30, 31], product quantization (PQ) [12, 20], hierarchical navigable small world graphs (HNSW) [24], etc. Another line of research attempts to encode a vector in a discrete latent space. Vector Quantized-Variational AutoEncoder (VQ-VAE) [34] proposes a simple yet powerful generative model to learn a discrete representation. HashRec [21] and Merged-Averaged Classifiers via Hashing [26] use multi-index hash functions to encode items in large recommendation systems. Hierarchical quantization methods like Residual Quantized Variational AutoEncoder (RQ-VAE) [37] and Tree-VAE [25] are also used to learn tree structure of a vector. However, all of these systems either assume a stable item vocabulary or don't take embedding distribution shift into account.\nGenerative Retrieval. Recently, generative retrieval has emerged as a new paradigm for document retrieval [1, 4, 32, 33, 35] and we"}, {"title": "3 MOTIVATION", "content": "The motivation for HSNN stems from understanding the current production baseline:\n\u2022 The retrieval model uses a Two Tower architecture which limits the interaction of users and ads both in the model and features.\n\u2022 The clustering (as part of KNN) happens as a post-training operation and hence is not aware of the retrieval optimization criteria.\n\u2022 The model is trained in a recurring fashion and hence a new embedding is available for all ads every few hours which forces new cluster indices being built. This leads to delays which affects the model quality."}, {"title": "3.1 Model Design", "content": "Features and Model Architecture. Two Tower model is a powerful tool which is widely in the industry for the retrieval stage. The late fusion property of Two Tower model makes it possible to deploy each tower model separately. In our case, we build User-to-Ad two tower model, where one tower is the user tower (which takes in user features as input) and outputs a fixed-size vector representation of the user. The other tower is the ad tower (which takes in ad features as input) and outputs the same size vector representation for the Ad. Note that the Two Tower architecture enforces the feature space to We can use user tower embedding to retrieve nearest neighbors in ad embedding space using Embedding-based Retrieval (EBR).\nTraining. The retrieval model is trained in a online manner which allows the model to learn about the new Ads on the marketplace. This frequent snapshot publishing, combined with online inference, empowers the retrieval model to consistently deliver up-to-date user and ad embeddings."}, {"title": "3.2 Disjoint Optimization of Clustering and Retrieval Model", "content": "The architecture diagram is shown in Figure 2. We identify 2 components -\n(1) A retrieval model like Siamese Networks that takes a <user, ad> pair and predicts if the user would engage on the given ad."}, {"title": "3.3 System Architecture", "content": "The following steps are performed to fetch the required cluster data and serve model at request time:\n\u2022 Data loading and Clustering: The ads embedding needed for clustering from active ads are loaded. Standalone clustering algorithms such as Approximate Nearest Neighbor (ANN) with FAISS [8] on K-means vector codec are employed to build a similarity index using ad embeddings. This process is repeated at different cluster levels to generate hierarchical cluster data.\n\u2022 Indexing: Generated cluster information is used to build the ads index containing mapping to clusters at different levels of hierarchy. This index is shipped for serving ads at request time.\n\u2022 Serving: User/Request info, cluster info and any interaction feature data are used to order clusters. Ads are processed based on cluster ordering to fetch top-k ads for given user requests.\nData loading & Clustering steps are repeated as new model snapshots are available from training and as active ads pool changes to ensure cluster information is available at serving time. This incurs overhead in terms of compute resources and time taken."}, {"title": "4 PROPOSED METHOD", "content": "In this section, we introduce HSNN, describe the LTC algorithm and also show how it could be applied to sophisticated model architectures."}, {"title": "4.1 Hierarchical Structured Neural Network (HSNN)", "content": "4.1.1 Modeling. The retrieval stage model architecture is shown in Figure 3. The ad modules have been marked in blue, while the clustering modules have been marked in pink.\nAd Modules. It consists of 3 decoupled towers - the user tower, the ad tower and the interaction tower. Each of the towers, takes in corresponding features (e.g. user features for user tower) as input and produces a fixed size representation as an intermediate output. The 3 intermediate outputs (user, ad and interaction) are then fed to a Merge Net. The MergeNet aims to captures higher order interactions and as the name suggests, it merges the inputs to produce the logit.\nCluster Modules. In order to integrate the Learning To Cluster (LTC) algorithm (Algorithm 1) into the retrieval model, we propose changes as following.\n\u2022 Ad Cluster Tower - The ad cluster embedding is generated based on the Ad embedding using the LTC algorithm, which we'll describe in the next section.\n\u2022 Interaction Cluster Tower - The user embedding and ad cluster embedding are fed into the interaction tower, where they are processed through an MLP layer to generate the cluster interaction embedding.\nLoss Functions. There are 2 main supervision losses - User<>Ad supervision loss and User<>Cluster supervision loss (discussed in line 9 in Algorithm 1). Both of these are also calibrated to ensure that the model doesn't under-predict or over-predict even at the cluster level. This is also one of the advantages of co-training the clustering module with the ad module. Besides the above supervision loss, we've added a couple of auxillary losses that help in the stability of the model - a. MSE loss between the Ad Tower and Ad Cluster tower helps in ensuring that the cluster embedding resembles the ad embedding and b. MSE loss between the Ad Interaction and Ad Cluster Interaction performs a similar role. The cluster modules aim to learn from its ad module counterpart.\n4.1.2 Indexing. The retrieval model is split into 5 parts for serving the user tower, ad tower, interaction tower, cluster model and over-arch model.\n\u2022 The user tower takes in user features and returns the user embedding.\n\u2022 The Ad tower takes in ad features and returns the ad embedding and cluster assignment (IDs) across all hierarchies. This part of the model is also used to fetch the cluster assignment for new ads and ensure there is no staleness in the system. As the LTC algorithm is lightweight, this doesn't hurt the inference QPS.\n\u2022 The interaction tower takes in interaction features (of both user and ad) and returns an embedding for the <user-ad> interaction.\n\u2022 The cluster model returns the cluster embeddings for all hierarchies.\n\u2022 The over-arch model takes input from user-tower, ad-tower, interaction-tower and returns predicted score.\nThe cluster ids across all hierarchies are now indexed i.e. an inverted index is built with each of the hierarchy acting as a separate index term. The cluster ID terms also have the relevant model information like cluster embeddings and the interaction model with them."}, {"title": "4.1.3 Serving", "content": "Now that we've indexed the cluster assignment for each of the ads, we're ready to serve them. We start by fetching the user embedding given the user ID. For each of the clusters, we use the cluster embedding for the cluster tower and a MLP + concatenation of the user embedding and the cluster embedding to get the interaction embedding. All these 3 embeddings are used to fetch the score for the cluster. As the number of clusters is much lesser than the number of Ads, this is relatively a small cost and can be performed in batch. Once we've the cluster scores, we then sort the clusters in the decreasing order of the scores. All the ads that belong to the top clusters are then pushed to a queue to be scored by the next stage ranker. This is then followed by all the ads in the next cluster and so on until either the time budget runs out or we've finished scoring all the ads."}, {"title": "4.2 Learning To Cluster (LTC)", "content": "LTC is a gradient-descent based clustering algorithm that takes the ad embedding as input and returns both the cluster assignment and centroid embedding. The LTC algorithm is described in detail in Algorithm 1 and can be explained through the following steps.\n\u2022 Initialize cluster centroids of shape (num clusters \u00d7 embedding dim).\n\u2022 For every Ad in the batch, perform steps as follows.\nCompute the distance between an Ad and every cluster centroid.\nTake the softmax of the distance to get the ad <> cluster affinity score. We call this the soft assignment of an ad to a cluster.\nTake the weighted sum of the learned cluster embedding to get the cluster embedding for this Ad.\nWe've made the 3 improvements to the LTC algorithm - curriculum learning, cluster collapse and introducing hierarchy.\n4.2.1 Curriculum Learning. Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data/task/problem to harder data/task/problem. The motivation arises from how we humans learn in an academic curriculum setting - we start with simple problems, master them and gradually"}, {"title": "Softmax Temperature.", "content": "shows the impact of the temperature parameter to softmax. When the temperature value is -1, the output value is evenly distributed but as the temperature value decreases to -1000, the distribution is skewed towards the smallest value and it is almost equivalent to a hard assignment. Note that the negative value of the temperature is due to the fact that the LTC algorithm uses the L2 distance metric (lower is closer). Note that this method shares the same spirit as Gumbel Softmax [18] with varying alpha parameter in the softmax temperature used in sampling."}, {"title": "Scheduling Strategy.", "content": "With the understanding of the importance of softmax temperature, we can now use a scheduling algorithm to change the temperature with respect to the number of iterations. We explored numerous strategies (linear, cosine and exponential) and observed the best performance with exponential scheduling strategy (chart below). This also makes intuitive sense as the model spends most of the training time converging on the soft assignment"}, {"title": "4.2.2 Cluster Distribution.", "content": "Clustering methods often face the challenge of cluster collapse, characterized by the model utilizing only a limited subset of cluster IDs. A balanced cluster distribution is a key factor ensuring that we can scale large models from ad level to the cluster level. One of the key assumptions of the the cluster based retrieval paradigm is that by reducing the number of inferences by a factor of num clusters, we can scale our model complexity. In order to truly achieve this, we assume a balanced cluster distribution - every cluster roughly has an equal number of ads. To address this issue, we employ the following two strategies.\nFLOPs Regularizer. The motivation for this idea arises from Paria et al. [28]. The primary intuition is to use the soft assignment matrix (batch size \u00d7 num clusters) to ensure that the in-batch cluster distribution is even. We introduce a regularizer that penalizes the model if all ads in a batch are assigned to the same cluster or if the cluster distribution is skewed. We achieve this by introducing a sparsity loss that minimizes the sum of squares of the mean soft assignment. While this approach works reasonably well, one disadvantage is that it is sensitive to the batch size."}, {"title": "4.2.3 Introducing Hierarchy.", "content": "In Embedding Based Retrieval (EBR) systems, it is common to build a hierarchy of clusters to deal with a large number of items for retrieval. Similarly, in HSNN, we propose to introduce hierarchy through Residual Quantization. In order to extend the model architecture, we introduce N Ad Cluster Towers where N is the depth of the hierarchy. The input to the i-th Ad Cluster Tower is the residue of Ad Embedding and the (i-1)th Ad Cluster Embedding."}, {"title": "5 ABLATION STUDIES", "content": "In this work, we prioritize the factors that affect the machine learning model's performance and we use the accuracy of prediction as our primary offline evaluation metric. Specifically, we utilize calibrated Normalized Entropy (NE) and Recall@K as our key offline evaluation metrics.\nCalibration. Calibration is the ratio of the average estimated Click Through Rate (CTR)/Conversion Rate (CVR) and empirical CTR/CVR. We should consider checking calibration when evaluating and comparing different models using NE as a metric. For example, it is possible for 2 models to have the same ordering of Ads but have different NE metrics. All the NE numbers reported in this paper are calibrated.\nNormalized Entropy (NE). Normalized Entropy [14] is equivalent to the average log loss per impression divided by what the average log loss per impression would be if a model predicted the average empirical CTR/CVR of the training data set. The lower the value, the better the model's predictions. In order to measure the impact of cluster, we use the cluster prediction to compute NE and call this as Cluster NE."}, {"title": "5.1 Interaction Arch and features", "content": "In this ablation study, we aim to capture the benefit of using interaction features and exploring other  interactions instead of a dot product interaction. To do this, we ablate the interaction features and interaction tower in Figue 3 to validate the importance of these interaction features. To understand the scale of importance, we created 2 model version 1 with 1x number of interaction features and another with 20x. We see more NE loss when ablating a stronger model with 20x interaction features. We do expect the number to plateau beyond a certain number of features as a large portion of the impact comes from the top interaction features."}, {"title": "5.2 Co-Training", "content": "In this ablation study, we aim to capture the benefit of co-training the clustering with the retrieval model. In other words, by making the clustering aware of the retrieval optimization criteria, we believe that it could improve clustering. In the EM style algorithm, we alternate between learning the cluster centroid representation given the assignment and then updating the cluster assignment given the learnt cluster centroid representation. In the LTC algorithm, both the cluster representation and ad representation are jointly optimized in a gradient descent fashion using techniques like curriculum learning to deal with discrete clustering operation."}, {"title": "5.3 LTC Algorithm Ablation", "content": "There are numerous factors that bring gains to the LTC algorithm - the curriculum learning makes the LTC algorithm trainable on the discrete clustering operation. The warmup strategy makes the learning from the ad tower more smooth and the flops regularizer and the codebook reset ensure that we don't see a cluster collapse. While the FLOPS regularizer and codebook reset were orirginally added to deal with cluster collapse, it is interesting to see them bring in accuracy (NE) gains as well."}, {"title": "6 EXPERIMENTS", "content": "6.1 Online Results\nRelevance & Efficiency Metric. The online relevance metric is proportional to revenue from high quality clicks and conversions. The motivation for this stems from the fact that we want to optimize for long-term revenue and also have a good experience for all of our users. We've deployed HSNN using the LTC algorithm (both 1 level and 2 levels of clustering) and tested the improvements over Embedding Based Retrieval (EBR) using an A/B experiment. Upon deploying the LTC algorithm in production, we observe a 0.2% improvement over K-Means algorithm in our topline metric.\nWe attribute the relevance gains to 3 reasons\n\u2022 Co-optimizing the clustering module with the retrieval model produces the centroid embedding that is not only optimized for within-cluster variance of an ad but also optimized for the <user, centroid> interaction.\n\u2022 Use of interaction features and sophisticated MergeNet brings in additional value over using just user and ad entity features with dot product interaction.\n\u2022 Freshness of centroid embedding - In the new co-trained approach, we've both the cluster assignment and centroid embedding as part of the model inference (no need to run"}, {"title": "6.2 Deployment Lessons", "content": "6.2.1 Cluster Collapse. One of the motivations of Hierarchical Structured Neural Network with 1 clustering module is that it reduces the complexity of the candidates selection from O(K \u00d7 C) to O(K+T), where K is the number of clusters, C is the number of Ads in each cluster and T is the total number of Ads to return. A uniform cluster distribution is necessary in achieving this. Without techniques like FLOPs regularizer and random replace, we observed uneven cluster distribution at best and only a fraction of clusters utilized at worst.\n6.2.2 Staleness of Cluster Centroids. Stalesness comes into picture for 2 reasons:\n\u2022 New ads are created and the system isn't quick enough to capture and score them.\n\u2022 Features of existing ads are updated and the system isn't quick enough to capture and score them.\nWe measure the impact of staleness by measuring the delta between fresh centroid embeddings vs stale centroid embeddings. As we have shown in Table 6, we find that NE when compared to a fresh model worsens over time. It is also worth noting that the cluster assignments are relatively stable compared the the centroid embeddings."}, {"title": "7 CONCLUSION AND NEXT STEPS", "content": "In this paper we presented a learnable hierarchical clustering module called Hierarchical Structured Neural Network. We show that it is possible to jointly learn and optimize hierarchical structure and neural network. We introduce interaction features to the retrieval layer and also make the clustering module aware of these features. And HSNN has been successfully deployed to a Ads recommendation system.\nIn the future iterations, we plan to explore more complex interactions between the hierarchies. We also introduce personalization by incorporating clustering to the user entity as well."}]}