{"title": "From Hype to Reality: The Road Ahead of Deploying DRL in 6G Networks", "authors": ["Haiyuan Li", "Hari Madhukumar", "Peizheng Li", "Yiran Teng", "Shuangyi Yan", "Dimitra Simeonidou"], "abstract": "The industrial landscape is rapidly evolving with the advent of 6G applications, which demand massive connectivity, high computational capacity, and ultra-low latency. These requirements present new challenges, which can no longer be efficiently addressed by conventional strategies. In response, this article underscores the transformative potential of Deep Reinforcement Learning (DRL) for 6G, highlighting its advantages over classic machine learning solutions in meeting the demands of 6G. The necessity of DRL is further validated through three DRL applications in an end-to-end communication procedure, including wireless access control, baseband function placement, and network slicing coordination. However, DRL-based network management initiatives are far from mature. We extend the discussion to identify the challenges of applying DRL in practical networks and explore potential solutions along with their respective limitations. In the end, these insights are validated through a practical DRL deployment in managing network slices on the testbed.", "sections": [{"title": "I. INTRODUCTION", "content": "In November 2023, ITU-R agreed on recommendations for the \"IMT-2030 Framework\", shaping the landscape for 6G development, standardization, and deployment [1]. The framework delineates six usage scenarios, three of which evolve from IMT-2020: Immersive Communication, Massive Communication, and Hyper-Reliable/Low-Latency Communication. The other three scenarios introduce new capabilities: Ubiquitous Connectivity, Service-oriented Artificial Intelligence (AI) and Communication, and Integrated Sensing and Communication. These usage scenarios bring new network management demands from multiple perspectives. Firstly, customizable service delivery is required to tailor network parameters to meet specific Quality of Service (QoS) and Quality of Experience (QoE) metrics. This is crucial for Immersive Communication, which requires a seamless user experience with ultra-high-definition content and minimal latency, and for Hyper-reliable and Low-Latency Communication, which necessitates guaranteed delivery times and consistent performance. Additionally, adaptive network management is needed for both Service-oriented AI and Communication and Integrated Sensing and Communication. This involves dynamic resource allocation to optimize AI-assisted applications based on real-time demands, as well as flexible integration of sensing capabilities to meet various application requirements, such as environmental monitoring and autonomous driving. Furthermore, Ubiquitous Communication demands the convergence of various communication technologies, such as satellite, terrestrial, and underwater, to provide extensive coverage. Massive communication, on the other hand, necessitates a scalable and upgradable architecture to support a growing amount of infrastructure and devices.\nTo satisfy these demands, management-oriented AI can be considered as a critical component with the capability to optimize services within complex and dynamic networks, and numerous standardization efforts are underway to incorporate AI management policies into existing network architectures. For instance, significant groundwork was laid in 3GPP Release 17 and earlier, focusing on network automation and data collection. Following that, Release 18 included more comprehensive works on AI/Machine Learning (ML) management across 5G systems. For instance, the System Aspects (SA) Working Group (WG) 5 focused on the deployment and lifecycle management of AI/ML models, and the Radio Access Network (RAN) WG3 worked on AI-enabled RAN intelligence to enhance data collection and signaling for energy saving, load balancing, and mobility optimization [2]. In June 2024, 3GPP froze Release 18, representing the first release of 5G-Advanced and a milestone for AI/ML integration in 5G networks. Similarly, ETSI has been at the forefront of advancing AI/ML and network automation standards. The Operational Coordination Group (OCG) AI has played a pivotal role in facilitating information sharing on AI activities across ETSI groups and assessed the impact of AI/ML on standards. Moreover, the Industry Specification Group (ISG) Zero-touch Network & Service Management (ZSM) aims to achieve full end-to-end automation of network and service management, while the ISG Experiential Networked Intelligence (ENI) developed an AI-based cognitive network management framework for adaptive service provisioning and resource management. Other standardization bodies, such as ITU and ISO/IEC Joint Technical Committee (JTC) are also progressing in this direction [3].\nThese advancements from standardization bodies underscore the significance of utilizing AI to improve network performance. Within the broad spectrum of AI technologies, Supervised Learning (SL) stands out by using labeled datasets to learn the intricate relationships between input features and outputs. Therefore, it excels in traffic prediction and service classification based on historical data. In contrast, Unsupervised Learning (UL) is able to identify data patterns without predefined labels, making it essential for data clustering, compression, and anomaly detection [4]. However, these paradigms face limitations in certain contexts. SL often struggles with adaptation to varying environments, requiring frequent retraining with new data. UL, while effective in pattern recognition, lacks the clear objective orientation necessary for tasks like long-term scheduling. Beyond these classic AI paradigms, Deep Reinforcement Learning (DRL) represents a distinctive approach. By considering network management as Markov Decision Processes (MDPs) and combining reinforcement learning principles with deep neural networks (NNs), DRL can handle high-dimensional input spaces, interact with the network and adaptively optimize its strategy toward maximizing long-term profits [5]. In comparison with SL and UL, the benefits of using DRL for satisfying the aforementioned 6G service demands are summarized as i) Objective orientation: DRL agents can dictate specific actions, enabling optimization by connecting rewards to specific customized service requirements and objective functions; ii) Long-term scheduling: DRL considers the long-term rewards of a sequence of decisions, making it well-suited for strategic planning in maintaining 6G service qualities; iii) Dynamic environment adaptation & Self-optimization: DRL agents learn the optimal policy through direct interaction with the environment. Furthermore, DRL agents can continuously recalibrate their strategies by exploring the environment and exploiting the policy for future services, enabling real-time adaptation and consistent performance under evolving network architecture. In anticipation of the 6G era, these characteristics are well-suited to addressing the escalating complexity of networks characterized by network function disaggregation, multi-vendor interoperability, and hierarchical deployments encompassing the RAN, Edge, and Cloud network.\nNonetheless, DRL-based network management initiatives are far from mature. Most related works focus on addressing specific problems in partial of the communication process and remain confined to the simulation stage, without guaranteeing the effectiveness of the algorithms in real-world applications [5]. Challenges may include insufficient consideration of actual deployment overhead, including extra communication costs between DRL agents while training and inference, and real network complexities, such as uncertainty of request and network resource attributes. Additionally, there are concerns about overly simplified assumptions that fail to account for the coordination between multiple algorithms, as well as the generalization and scalability limitations of the algorithms in ever-changing and large-scale networks.\nTherefore, to explore the pathway of DRL applications in the development of 6G networks, we examine the following perspectives:\n\u2022\n\u2022\n\u2022\nWe illustrate three DRL applications across the end-to-end communication process to validate the necessity of DRL for 6G networks and propose an architecture to integrate these DRL applications.\nWhile the potential of DRL for advancing 6G networks is well demonstrated, significant challenges remain toward its practical application in networks. We summarize and categorize the likely obstacles to deploying DRL algorithms in 6G networks and explore potential solutions along with their corresponding limitations.\nTo validate these discussions, we demonstrate the implementation of a DRL algorithm in managing network slices on a testbed, aiming to minimize service latency and energy consumption. The results prove the effectiveness of DRL and corroborate the summarized challenges."}, {"title": "II. APPLICATIONS OF DRL IN 6G", "content": "To demonstrate the importance and potential benefits of DRL in enhancing future networks, we present three specific DRL applications in end-to-end network management, as depicted in Fig. 1. These applications include wireless access control and baseband function placement for the establishment of communication connections, as well as network slicing coordination for the provisioning of network services."}, {"title": "A. Wireless access control", "content": "Mobile User Equipments (UEs) initiate the communication by first connecting to the Radio Unit (RU) via wireless access. Considering the extensive distribution of RUs across the network and technologies such as multi-path Transmission Control Protocol (TCP) that enables simultaneous wireless communication via different access technologies such as 5GNR, LTE, and WiFi, the UE faces complex access decisions, each presenting trade-offs in transmission power, network performance, and operational costs. These factors constitute a complex nonlinear optimization challenge to balance overall service quality, network operational costs, and UE power consumption within stochastic wireless environments.\nTraditional methods for access control often rely on static policies that struggle to cope with the dynamic nature of wireless environments. In contrast, as stated in Sec. I, DRL offers a robust alternative that can effectively manage the multitude of variables involved in the access process due to its ability to learn and adapt to complex, high-dimensional state spaces through continuous interaction with the environment. For instance, authors in [6] proposed a Deep Deterministic Policy Gradient (DDPG)-based access management system for 5G-WiFi hybrid networks. Leveraging the minute control that DDPG offers in continuous action spaces, the authors effectively improve radio resource utilization, service quality, and connection stability by controlling the traffic division ratio between the two technologies."}, {"title": "B. Baseband function placement in RAN", "content": "The collected requests at RUs will be further routed through baseband functions in RANs to finalize the connectivity. In advanced RANs, baseband functions consist of Distributed Units (DUs) for data layer processing, Core Unit User Plane (CU-UP) for user data processing, and CU Control Plane (CU-CP) for control signaling. These functions can be placed on Multi-access Edge Computing (MEC) server networks via virtualization technologies. However, the massive number of requests anticipated in 6G might overwhelm the network and compete for limited edge network resources. Meanwhile, diverse requests impose unique requirements on fronthaul, midhaul, and backhaul latencies. These resource and multi-dimensional latency constraints pose complex dilemmas in optimizing resource utilization and minimizing power consumption during network operation.\nMajor solutions formulate the problem into Mixed Integer Linear Programming (MILP) and optimize it with MILP-solvers [7]. However, the inherent complexity of MILP leads to extensive computation times, making it unsuitable for real-time applications. In contrast, DRL algorithms can achieve real-time management, adapt to traffic patterns, and self-optimize under network variations. For example, authors in [8] designed a DQN-based strategy that decides both baseband function placement and routing provisioning to minimize power consumption. By employing graph convolution networks to capture spatial information, the DQN agent can adapt to diverse networks with various topologies and autonomously adjust to evolving RAN architectures."}, {"title": "C. Network slicing coordination", "content": "To offer customized services over the established communication connections, network slicing can create multiple network slices with various Virtual Network Functions (VNFs) over a shared infrastructure. However, the coexistence of slices with distinct objectives leads to resource competition, which requires an effective long-term resource allocation policy.\nDRL is highly effective for this problem due to its ability to handle long-term scheduling and capture the dynamics and complexities of requests and resource statuses. For instance, authors in [11] introduced a multi-agent Proximal Policy Optimization (PPO)-based solution to maximize resource efficiency while guaranteeing the quality of service of slices. Within this solution, each network slice is allocated a DRL agent to simulate resource competition relationships between slices and optimize overall performance."}, {"title": "D. Taxonomy of DRL", "content": "This subsection discusses two principal technical approaches of DRL, the policy-based method and the value-based method [5], to provide a foundational understanding of the mentioned algorithms and clarify the effectiveness of these approaches. In specific, value-based DRL focuses on approximating the value function through NNs. There are two primary types of value functions: the state-value function $V(s)$, which estimates the expected return from a given state, and the action-value function $Q(s, a)$, which evaluates the expected return from taking action a at state s. Taking the Deep Q Network (DQN) as an example, the training process is achieved by minimizing the difference between the Q-value and Time Differential (TD) target defined based on the Bellman equation [5] through the gradient descent. Once the value function is learned, the policy can be derived by selecting the action corresponding to the highest estimated return. On the other hand, policy-based methods directly parameterize the policy through an NN. The policy gradient solution adjusts the NN using gradient ascent with the objective of selecting actions that maximize the expected return. Overall, the main differences between value-based and policy-based methods are summarized as shown in Table I."}, {"title": "E. Orchestration", "content": "In realizing the aforementioned use cases, we propose an orchestration framework that integrates the MEC architecture, as outlined by ETSI [9], and the Open RAN architecture, defined by the O-RAN alliance [10]. The 6G network architecture and the allocation of multiple DRL models are further demonstrated, as shown in Fig. 2.\nWithin this architecture, key network components such as DU, CU, Non-Real Time (RT) RAN Intelligent Controller (RIC), Near RT RIC and VNFs can be embedded within MEC apps. In Advanced RAN, the training process will be considered in Non-RT RIC. For access management, the optimized DRL models can be allocated on UE, RU, or DU to enable RT device management and adaptive access control services (< 10ms). Subsequently, in the case of Near RT RAN baseband function placement (10ms ~ 1s) and long-term non-RT resource scheduling (> 1s), separate DRL models can be packaged in rApps on the Near RT RIC and xApps on non-RT RIC. Several studies have demonstrated the feasibility of deploying DRL into xApp and rApp through testbed and emulation [12], [13]. Lastly, to manage services like slicing in MEC networks, the training and inference of DRL models can be positioned in the MEC application or Network Slicing Orchestrator. This combination of ETSI MEC and O-RAN architecture necessitates interoperability between different interface standards. For instance, the communication between the Non-RT RIC and the Near-RT RIC in RAN via the O1 and Al interfaces and between the Near-RT RIC and E2 nodes via the E2 interface should be harmonized through the MEC application interfaces, such as Mp1, Mp2, and Mp3 [9]. In summary, the proposed architecture can support the diverse functionality requirements and ensure the effective operation and integration of DRL models within future 6G networks."}, {"title": "III. CHALLENGES TOWARD PRACTICAL VIABILITY OF DRL", "content": "While DRL has shown substantial potential in enhancing service quality and network efficiency, its journey from theoretical promise to practical implementation still presents many challenges. To provide a comprehensive view of the implementation of DRL in 6G networks, this paper enumerates the most significant challenges and organizes them according to the three phases of the deployment process: algorithm design, DRL training, and inference. 'Algorithm design' focuses on the fundamental components of DRL models, including the architecture and the theoretical principles that dictate their functionality and efficiency. 'DRL training' covers the practical aspects of model preparation, highlighting the intricacies of dataset properties, the learning process, and the complexities of the training environment. Lastly, 'inference' concerns the application of trained models, underscoring the challenges of adapting these models to dynamic and unpredictable networks. This section thoroughly examines these challenges and their impacts on 6G networks, as well as potential solutions and corresponding limitations."}, {"title": "A. Design", "content": "1) Scalability: The optimization challenges in 6G networks often feature huge solution spaces due to increased network sizes and the volume of requests. Consequently, DRL algorithms, such as the DQN, which employ a greedy policy by evaluating and selecting the optimal action based on the highest Q-value output of the NN, will struggle to adapt to these intricate scenarios. For instance, in routing management, the number of possible paths between two servers increases exponentially with the size of the network, leading to a typical issue known as 'action space explosion'.\nTo address it and the consequent reduction in learning efficiency, two primary solutions are employed: multi-agent strategies and heuristic-assisted policies. The former decomposes the vast action space into smaller segments and allocates them across multiple agents. However, this approach is a palliative measure. As the number of agents increases, the overall effectiveness of training tends to diminish due to the increasing complexity of achieving Nash equilibrium [14] among the agents. On the other hand, the latter utilizes heuristic algorithms to downsize the action space, thereby limiting the decision space of the DRL agent. Nevertheless, it often sacrifices optimal solutions in cutting down the solution space.\n2) Event frequency variations: DRL operates based on discrete steps within an MDP, where each step is normally triggered by an event, generating a transition from one state to another following a reward. A primary concern arises as it may struggle with data drift issues caused by varying event frequencies. DRL trained at fixed time intervals may exhibit difficulties adapting to various event frequencies. Conversely, if a model is trained with dynamic event frequencies, similar state-action pairs will produce different effects on future requests due to their varying arrival times, thereby increasing the complexity of fitting the optimal state distribution function.\nA potential solution is to integrate time series prediction algorithms, such as Long Short-Term Memory (LSTM) networks or Transformers, into DRL. These NNs are firstly pre-trained by the collected data to provide the hidden time related information to the DRL agent. It endows DRL with the ability to foresee future events, enhancing its robustness and long-term reward maximization performance. However, the offline nature of these prediction algorithms undermines the real-time self-optimization and generalization capabilities of DRL. An additional offline optimization of the predictive models is necessary before updating DRL to respond to network variations.\n3) Co-existence of multiple DRLs: As discussed in Sec. II, DRL can resolve a variety of problems within 6G networks. However, these problems are interconnected in practical applications, where multiple DRL agents operate simultaneously under different temporal granularity. Consequently, they might generate conflict management commands. In addition, variations in a certain policy can cause cascading effects and impact the efficiency of other algorithms. This represents an under-explored area that calls for a systematic management approach.\nIn response, adopting principles of distributed control and centralized coordination, a hierarchical AI architecture featuring a top-level AI manager presents a promising solution. In this framework, each AI hierarchy layer concentrates on distinct network aspects, with dedicated DRL models managing these functions. Moreover, the top-level AI agent can maintain a comprehensive view of network performance, make macro-level decisions, and coordinate conflict policies. This approach offers a scalable solution adaptive to various network demands and technologies, ensuring robust, efficient, and intelligent management."}, {"title": "B. Training", "content": "1) Delayed reward & retroactive influence of actions: In the typical DRL training process, an agent receives a reward at the end of each step, influenced by the immediate action and the cumulative effects of prior actions on the network status. However, reward acquisition is often delayed due to communication and processing times in 6G networks. Within the delay interval, an agent might perform further actions, potentially altering the rewards for previous actions. This presents two challenges: 1) delayed reward reception and 2) the impact of current actions on past rewards. This latter aspect expands the traditional MDP framework to incorporate retrospective impact analysis.\nA prospective strategy to address the delayed reward involves conceptualizing the optimization problem as a Hidden Mode MDP (HM-MDP), wherein transitions without immediate rewards are temporarily frozen and stored in the replay buffer until the corresponding rewards are obtained. However, HM-MDP falls short in delineating the intertwined nature of actions and rewards in complex 6G networks, where the impact of actions might unfold over time and affect past, current, and future rewards.\n2) Sim2real problem: Optimizing a DRL model demands extensive resources, involving numerous training steps and multiple sessions dedicated to fine-tuning hyperparameters. Moreover, the random exploration of DRL agents during the training process might lead to significant service disruptions, violating ongoing service requirements in operational networks. Consequently, simulation environments are considered the preferred solution for DRL applications. However, it is important to acknowledge that simulation environments often fall short of precisely reflecting the characteristics of actual networks and generating effective DRL models.\nIn comparison, Digital Twin (DT) technology represents a significant advancement over traditional simulation environments. Through real-time monitoring and control, DT serves as a continuous, synchronized digital replica of the physical system, enhancing the seamless transition of DRL from simulated environments to real-world applications. Nevertheless, DT cannot capture the full spectrum of variables and conditions present in actual 6G networks, relegating them to the role of a pre-deployment emulator."}, {"title": "C. Inference", "content": "1) Static generalization: While NNs empower DRL with the capability to learn from environments, they introduce a critical limitation due to the static nature of NN inputs and outputs. Therefore, dispersed networks with diverse structures or operating standards present significant challenges in achieving a generalized DRL model.\nMADRL is considered a potential strategy. It shifts attention from variations in NN structure to the number of DRL agents. However, the interconnections between agents and their respective significance to the overall system are ambiguous. Another viable approach is to decouple information profiling from the DRL training, converting diverse network information into uniform NN inputs [15]. However, it is challenging to ensure that a fixed-size vector at the NN output can execute different actions across various contexts.\n2) Dynamic generalization: While DRL excels in adapting to network dynamics, including fluctuating requests and resource availability, the uncertainty in 6G networks intensified by temporal and spatial factors, may exceed the processing capabilities of DRL. For instance, a model trained on established traffic patterns may not be able to handle scenarios where competition among requests intensifies or diminishes.\nThis challenge can be conceptualized as a dynamic MDP encompassing a drifting environment. In response, two promising solutions are incremental learning and online learning, which allow the model to be adjusted in reaction to detected environmental changes. However, the reliability of these approaches is debatable due to the extensive duration and energy consumption of the adjustment process, alongside unpredictable performance caused by catastrophic forgetting when inheriting previously learned knowledge."}, {"title": "IV. USE CASE DEMONSTRATION", "content": "To validate and further explore the discussions, this section presents a case study on deploying a DRL model to coordinate the resource allocation between network slices. We outline the deployment of DRL in the testbed, the encountered challenges, the proposed solutions, and future prospects for advancing DRL applications."}, {"title": "A. Network orchestration and DRL deployment", "content": "The infrastructure and orchestration of the testbed are illustrated in Fig. 3. There are three servers located at the University of Bristol, the We the Circus Museum, and the Mshed Museum. Each server hosts a VM allocated with 4 CPU cores and 10 Gbps inter-node bandwidth limitations. These resources are configured to host multiple network slices, serving as conceptual examples for applications in Virtual Reality (VR), robot interaction, and real-time 3D model generation. The requests are emulated with requirements ranging between 27%~33% of the maximum resources. By employing a MAD-DPG approach, where an agent is allocated to each slice, the DRL algorithm aims to optimize the resource allocation among these network slice requirements, thereby minimizing long-term service delay and network energy consumption. The objective function, i.e., reward, is defined based on the unified interests of delay and energy consumption normalized by their respective minimum values $L_m$ and $E_m$. It can be represented by $\\sum (\\alpha L_m/L_s +\\beta E_m/E_s)/S$, where S is the slice number, $\\alpha$ and $\\beta$, summing to 1, indicate the performance preference. In practice, a Kubernetes cluster is deployed among the three VMs with network functions abstracted as interconnected pods. Among these pods, CPU resource is allocated by setting Kubernetes resource limits and saturated by stress command; Link capacity is allocated between the pods using tc command and saturated through iperf command. The centralized DRL model is placed on the Python-based network operator to manage the network resources. To achieve the interaction between DRL and the network, a Prometheus NodeExporter instance is deployed on each node in a Daemonset configuration to monitor the transmission and reception bandwidth, along with the total CPU utilization of each node at one-second intervals. All data collected is stored in a MongoDB database."}, {"title": "B. Encountered challenges", "content": "This section examines and contextualizes the challenges of the DRL model encountered during its design, training, and inference phase, aligning with those detailed in Sec. III.\n1) Sim2real problem: A simulation environment is first established to verify the effectiveness of the proposed algorithm, where the entire system is exclusively dedicated to the network slice functions. However, the MEC power consumption in real networks is subject to a variety of external influences, such as CPU usage by other processes and the status of cooling fans. In response, we applied a previously measured power stress curve for a server. This approach enables the isolated calculation of power usage, ensuring that any observed energy consumption is attributed exclusively to the slices involved in the experiment.\n2) Delayed reward: The energy consumption of a task is not solely determined by its resource allocation. It is also affected by the new arrival and completed tasks during its processing period. Therefore, during the training sessions, each transition is temporarily 'frozen' until all the delayed rewards are received in the future, after which it is 'thawed' and stored in the experience replay buffer.\n3) Dynamic generalization: The number of network slices may vary with task completion or reactivation, therefore undermining the efficacy of existing DRL policies. To obviate the requirement for retraining a DRL model from scratch with variations in network slice number, an incremental learning scheme has been introduced. By inheriting the previous DRL model, this approach allows the model to build upon previous knowledge and adapt to new environments, thereby reducing training costs and enhancing the DRL generalization capabilities. Its performance is summarized in Fig. 5."}, {"title": "C. Experiment results and discussions", "content": "As shown in Fig. 4, three benchmarks, including random allocation, full allocation, and a static resource portion strategy proposed in the literature [14], are examined on the testbed to evaluate the inference performance of DRL. These results indicate that the trained MADDPG model outperforms alternative approaches and enhances resource coordination efficiency among network slices. Additionally, the learning process of DRL, in both simulations and experiments, is summarized in Fig. 5. The comparison between (c) and (e), as well as (d) and (f), proves the capability of incremental learning in reducing training costs and improving the generalization capability of DRL.\nDespite the promising results observed on the small-scale testbed, there are still challenges that remain to be resolved before deploying this algorithm in operational networks. Firstly, unified rewards derived from the same DRL algorithm can differ between simulated and experimental environments. This discrepancy highlights issues related to sim-to-real transfer and the impact of experimental variability on algorithm outcomes. Additionally, the current experimental scenario has not fully captured the dynamic nature of 6G, characterized by diverse slicing, a wide range of request types, and evolving network topologies.\nOn the other hand, in multi-agent systems, unresponsive agents and anomalies pose significant risks, potentially disrupting overall performance. Effective monitoring and troubleshooting of such scenarios require additional considerations. In addition, to maintain a consistent view among agents, MADRL necessitates extensive coordination and messaging among agents to share states, actions, policies, and synchronizations, introducing significant communication overhead. Furthermore, considering the importance of timeliness in the coordination process, the Age of Information (AoI) can be considered a critical metric for evaluating the freshness of messages by serving as a timing anchor and a valuable criterion for measuring the synchronicity of agents. Designing and determining the placement of a centralized Aol orchestrator is crucial. This orchestrator has to autonomously and promptly update the state of agents in the network. Additionally, the co-design of Aol and MADRL optimization targets is an important area for further exploration."}, {"title": "V. CONCLUSION", "content": "This article emphasizes the importance of DRL and explores its unmet requirements in driving the transformation of 6G networks. We explore the efficacy of DRL through an analysis of three specific applications within an end-to-end service process and propose an underlying network orchestration. In addition, the challenges and future scopes of navigating DRL to practical utility in evolving networks are discussed and further substantiated through a case study on deploying DRL in practical networks to optimize network slicing coordination. This work serves as a foundation for researchers aiming to leverage DRL in the evolution of 6G networks."}]}