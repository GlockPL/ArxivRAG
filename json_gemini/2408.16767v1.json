{"title": "RECONX: RECONSTRUCT Any Scene FROM SPARSE VIEWS WITH VIDEO DIFFUSION MODEL", "authors": ["Fangfu Liu", "Wenqiang Sun", "Hanyang Wang", "Yikai Wang", "Haowen Sun", "Junliang Ye", "Jun Zhang", "Yueqi Duan"], "abstract": "Advancements in 3D scene reconstruction have transformed 2D images from the real world into 3D models, producing realistic 3D results from hundreds of input photos. Despite great success in dense-view reconstruction scenarios, rendering a detailed scene from insufficient captured views is still an ill-posed optimization problem, often resulting in artifacts and distortions in unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction paradigm that reframes the ambiguous reconstruction challenge as a temporal generation task. The key insight is to unleash the strong generative prior of large pre-trained video diffusion models for sparse-view reconstruction. However, 3D view consistency struggles to be accurately preserved in directly generated video frames from pre-trained models. To address this, given limited input views, the proposed ReconX first constructs a global point cloud and encodes it into a contextual space as the 3D structure condition. Guided by the condition, the video diffusion model then synthesizes video frames that are both detail-preserved and exhibit a high degree of 3D consistency, ensuring the coherence of the scene from various perspectives. Finally, we recover the 3D scene from the generated video through a confidence-aware 3D Gaussian Splatting optimization scheme. Extensive experiments on various real-world datasets show the superiority of our ReconX over state-of-the-art methods in terms of quality and generalizability.", "sections": [{"title": "1 INTRODUCTION", "content": "With the rapid development of photogrammetry techniques such as NeRF (Mildenhall et al., 2020) and 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023), 3D reconstruction has become a popular research topic in recent years, finding various applications from virtual reality (Dalal et al., 2024) to autonomous navigation (Adamkiewicz et al., 2022) and beyond (Martin-Brualla et al., 2021b; Liu et al., 2024a; Wu et al., 2024a; Charatan et al., 2024). However, sparse-view reconstruction is an ill-posed problem (Gao et al., 2024; Yu et al., 2021) since it involves recovering a complex 3D structure from limited viewpoint information (i.e., even as few as two images) that may correspond to multiple solutions. This uncertain process requires additional assumptions and constraints to yield a viable solution.\nRecently, powered by the efficient and expressive 3DGS (Kerbl et al., 2023) with fast rendering speed and high quality, several feed-forward Gaussian Splatting methods (Charatan et al., 2024; Szymanowicz et al., 2024b; Chen et al., 2024a) have been proposed to explore 3D scene reconstruc- tion from sparse view images. Although they can achieve promising interpolation results by learning scene-prior knowledge from feature extraction modules (e.g., epipolar transformer (Charatan et al., 2024)), insufficient captures of the scene still lead to an ill-posed optimization problem (Wu et al., 2024b). As a result, they often suffer from severe artifact and implausible imagery issues when rendering the 3D scene from novel viewpoints, especially in unseen areas."}, {"title": "2 RELATED WORK", "content": "Sparse-view reconstruction. NeRF and 3DGS typically demand hundreds of input images and rely on the multi-view stereo reconstruction (MVS) approaches (e.g., COLMAP (Sch\u00f6nberger & Frahm, 2016)) to estimate the camera parameters. To address the issue of low-quality 3D recon- struction caused by sparse views, PixelNeRF (Yu et al., 2021) proposes using convolutional neural networks to extract features from the input context. Moreover, FreeNeRF (Yang et al., 2023) adopts the frequency and density regularized strategies to alleviate the artifacts caused by insufficient inputs without any additional cost. To mitigate the overfitting to input sparse views in 3DGS, FSGS (Zhu et al., 2023) and SparseGS (Xiong et al., 2023) employ a depth estimator to regularize the optimiza- tion process. However, these methods all require known camera intrinsics and extrinsics, which is\nRegression model for generalizable view synthesis. While NeRF and 3DGS are optimized per- scene, a line of research aims to train feed-forward models that output a 3D representation directly from a few input images, bypassing the need for time-consuming optimization. Splatter image (Szy- manowicz et al., 2024b) performs an efficient feed-forward manner for monocular 3D object re- construction by predicting a 3D Gaussian for each image pixel. Meanwhile, pixelSplat (Charatan et al., 2024) proposes predicting the scene-level 3DGS from the image pairs, using the epipolar transformer to better extract scene features. Following that, MVsplat (Chen et al., 2024a) introduces the cost volume and depth refinements to produce a clean and high-quality 3D Gaussians in a faster way. To reconstruct a complete scene from a single image, Flash3D (Szymanowicz et al., 2024a) adopts a hierarchical 3DGS learning policy and depth constraint to achieve high-quality interpola- tion and extrapolation view synthesis. Although these methods leverage the 3D data priors, they are limited by the scarcity and diversity of 3D data. Consequently, these methods struggle to acquire high-quality renderings in unseen areas, especially when OOD data is used as input.\nGenerative models for 3D reconstruction. Constructing comprehensive 3D scenes from limited observations demands generating 3D content, particularly for unseen areas. Earlier studies distill the knowledge in the pre-trained text-to-image diffusion models (Rombach et al., 2022; Saharia et al., 2022; Ramesh et al., 2022) into a coherent 3D model. Specifically, the Score Distillation Sam- pling (SDS) technique (Wu et al., 2024b; Lin et al., 2023; Liu et al., 2024c; Wang et al., 2024b) is adopted to synthesize a 3D object from the text prompt. To enhance the 3D consistency, several approaches (Wu et al., 2024a; Shi et al., 2023; Liu et al., 2023) inject the camera information into diffusion models, providing strong multi-view priors. Furthermore, ZeroNVS (Sargent et al., 2023) and CAT3D (Gao et al., 2024) extend the multi-view diffusion to the scene level generation. More recently, video diffusion models (Blattmann et al., 2023a; Xing et al., 2023) have shown an impres- sive ability to produce realistic videos and are believed to implicitly understand 3D structures (Liu et al., 2024b). SV3D (Voleti et al., 2024) and V3D (Chen et al., 2024b) explore fine-tuning the pre-trained video diffusion model for 3D object generation. Meanwhile, MotionCtrl (Wang et al., 2024c) and CameraCtrl (He et al., 2024) achieve scene-level controllable video generation from a single image by explicitly injecting the camera pose into video diffusion models. However, they don't work for the unconstrained sparse-view reconstruction, which requires strong 3D consistency."}, {"title": "3 PRELIMINARIES", "content": "Video Diffusion Models. Diffusion models (Ho et al., 2020; Song et al., 2020) have emerged as the cutting-edge paradigm to generate high-quality videos. These models learn the underlying data distribution by adding and removing noise on the clean data. The forward process aims to transform a clean data sample x0 ~ p(x) to a pure Gaussian noise xt ~ N(0, I), following the process:\nxt = \u221a\u0101t x0 + \u221a1 \u2212 \u0101t \u03f5, \u03f5 ~ N(0, 1), (1)\nwhere xt and \u0101t denotes the noisy data and noise strength at the timestep t. The denoising neural network \u03f5\u03b8 is trained to predict the noises added in the forward process, which is achieved by the MSE loss:\nL = E\u00e6~p,\u03f5~N(0,1),c,t [[\u03f5 \u2212 \u03f5\u03b8(xt, t, c) ||2], (2)\nwhere c represents the embeddings of conditions like text or image prompt. For the video diffusion models, Latent Diffusion Models (LDMs) (Rombach et al., 2022), which compress images into the latent space, are commonly employed to mitigate the computation complexity while maintaining competitive performance.\n3D Gaussian Splatting. 3DGS (Kerbl et al., 2023) represents a scene explicitly by utilizing a set of 3D Gaussian spheres, achieving a fast and high-quality rendering. A 3D Gaussian is modeled by a position vector \u03bc\u2208 R\u00b3, a covariance matrix \u03a3 \u2208 R3\u00d73, an opacity \u03b1 \u2208 R, and spherical harmonics (SH) coefficient c\u2208 Rk (Ramamoorthi & Hanrahan, 2001). Moreover, the Gaussian distribution is formulated as the following:\nG(x) = e\u2212(x\u2212\u03bc)T \u03a3\u22121(x\u2212\u03bc), (3)"}, {"title": "4 MOTIVATION FOR RECONX", "content": "In this paper, we focus on the fundamental problem of 3D scene reconstruction and novel view synthesis (NVS) from very sparse view (e.g., as few as two) images. Most existing works (Chen et al., 2024a; Yu et al., 2021; Charatan et al., 2024; Szymanowicz et al., 2024a) utilize 3D prior and geometric constraints (e.g., depth, normal, cost volume) to fill the gap between observed and novel regions in sparse-view 3D reconstruction. Although capable of producing highly realistic images from the given viewpoints, these methods often struggle to generate high-quality images in areas not visible from the input perspectives due to the inherent problem of insufficient viewpoints and the resulting instability in the reconstruction process. To address this, a natural idea is to create more observations to collapse the under-determined 3D creation problem into a fully constrained 3D re- construction setting. Recently, video generative models have shown promise for synthesizing video clips featuring 3D structure (Voleti et al., 2024; Blattmann et al., 2023a; Xing et al., 2023). This in- spires us to unleash the strong generative prior of large pre-trained video diffusion models to create temporal consistent video frames for sparse-view reconstruction. Nevertheless, it is non-trivial as the main challenge lies in poor 3D view consistency among video frames, which significantly limits the downstream 3DGS training process. To achieve 3D consistency within video generation, we first analyze the video diffusion modeling from a 3D distributional view. Let \u00e6 be the set of rendering 2D images from any 3D scene in the world, q(x) be the distribution of the rendering data \u00e6, our goal is to minimize the divergence D:\nmin D (q(x)||p\u03b8,f(x)), (8)\n\u0398\u2208\u0398,\u03a8\u2208\u03a8\nwhere p\u03b8, be a diffusion model parameterized by \u03b8 \u2208 \u0398 (the parameters in the backbone) and \u03c8 \u2208 \u03a8 (any embedding function shared by all data). In vanilla video diffusion model (Xing et al., 2023), they choose CLIP (Radford et al., 2021) model g to add an image-based condition (i.e., \u03c8 = g). However, in sparse-view 3D reconstruction, only limited 2D images as the condition cannot provide sufficient guidance for approximating q(x) (Charatan et al., 2024; Chen et al., 2024a; Wu et al., 2024b). Motivated by this, we explore the potential of incorporating the native 3D prior"}, {"title": "5 METHOD", "content": "5.1 OVERVIEW OF RECONX\nGiven K sparse-view (i.e., as few as two) images I = {I}Krefi=1, (I\u00b2 \u2208 RH\u00d7W\u00d73), our goal is to reconstruct the underlying 3D scene, where we can synthesize novel views of unseen viewpoints. In our framework ReconX, we first build a global point cloud P = {pi, 1 \u2264 i \u2264 N} \u2208 RN\u00d73 from I and project P into the 3D context representation space F as the structure guidance F(P) (Sec. 5.2). Then we inject F(P) into the video diffusion process to generate 3D consistent video frames I' = {I'i}K\u2032i=1, (K' > K), thus creating more observations (Sec. 5.3). To alleviate the negative artifacts caused by the inconsistency among generated videos, we utilize the confidence maps C = {Ci}K\u2032i=1 from the DUSt3R model and LPIPS loss (Zhang et al., 2018a) to achieve a robust 3D reconstruction (Sec. 5.4). In this way, we can unleash the full power of the video diffusion model to reconstruct intricate 3D scenes from very sparse views.\n5.2 BUILDING THE 3D STRUCTURE GUIDANCE\nGrounded by the theoretical analysis in Sec. 4, we leverage an unconstrained stereo 3D reconstruc- tion method DUSt3R (Wang et al., 2024a) with point-based representations to build the 3D structure guidance F. Given a set of sparse images I = {I}Krefi=1, we first construct a connectivity graph G(V, E) of K input views similar to DUSt3R, where vertices V and each edge e = (n, m) \u2208 E indi- cates that the images In and Im shares visual contents. Then we use G to recover a globally aligned point cloud P. For each image pair e = (n, m), we predict pairwise pointmaps Pn,n, Pm,n and their corresponding confidence maps Cn,n, Cm, n \u2208 RH\u00d7W\u00d73. For clarity, let's denote Pn,e := Pn,n and pm,e := pm,n. Since we aim to rotate all pairwise predictions into a shared coordinate frame, we"}, {"title": "6 EXPERIMENTS", "content": "In this section, we conduct extensive experiments to evaluate our sparse-view reconstruction frame- work ReconX. We first present the setup of the experiment (Sec 6.1). Then we report our qualitative and quantitative results compared to representative baseline methods in various settings such as different angle variances from input views and cross-dataset generalization (Sec 6.2). Finally, we conduct ablation studies to further verify the efficacy of our framework design (Sec 6.3).\n6.1 EXPERIMENT SETUP\nImplementation Details. In our framework, we choose DUSt3R (Wang et al., 2024a) as our un- constrained stereo 3D reconstruction backbone and I2V model DynamiCrafter (Xing et al., 2023) (@512 \u00d7 512 resolution) as the video diffusion backbone. We first finetune the image cross- attention layers with 2000 steps on the learning rate 1 \u00d7 10-4 for warm-up. Then we incorporate the 3D structure condition Cstruc into the video diffusion model and further finetune the spatial layers with 30K steps on the learning rate of 1 \u00d7 10\u22125. Our video diffusion was trained on 3D scene datasets by sampling 32 frames with dynamic FPS at the resolution of 512 \u00d7 512 in a batch. The AdamW (Loshchilov & Hutter, 2017) optimizer is employed for optimization. At the inference of our video diffusion, we adopt DDIM sampler (Song et al., 2022) using multi-condition classifier free guidance (Ho & Salimans, 2022). Similar to Xing et al. (2023), we adopt tanh gating to learn AF\n6.2 COMPARISON WITH BASELINES\nComparison for small angle variance in input views. For fair comparison with baseline methods like MuNeRF (Xu et al., 2024), pixelSplat (Charatan et al., 2024), and MVSplat (Chen et al., 2024a), we first compare our reconX with baseline method from sparse views with small angle variance (see Table 1 and Figure 3). We observe that our ReconX surpasses all previous state-of-the-art models in terms of all metrics on visual quality and qualitative perception.\nComparison for large angle variance in input views. As MVSplat and pixelSplat are much better than previous baselines, we conduct thorough comparisons with them in more difficult settings. In more challenging settings (i.e., given sparse views with large angle variance), our proposed ReconX demonstrate more significant improvement than baselines, especially in unseen and generalized viewpoints (see Table 2 and Figure 4). This clearly shows the effectiveness of ReconX in creat- ing more consistent observations from video diffusion to mitigate the inherent ill-posed sparse-view reconstruction problem.\nCross-dataset generalization. Unleashing the strong generative power of the video diffusion model through 3D structure guidance, our ReconX is inherently superior in generalizing to out-of- distribution novel scenes. To demonstrate our strong generalizability, we conduct two cross-dataset"}, {"title": "7 CONCLUSION", "content": "In this paper, we introduce ReconX,\na novel sparse-view 3D reconstruc-\ntion framework that reformulates the\ninherently ambiguous reconstruction\nproblem as a generation problem.\nThe key to our success is that we un-\nleash the strong prior of video diffu-\nsion models to create more plausible\nobservations frames for sparse-view\nreconstruction. Grounded by empiri-\ncal study and theoretical analysis, we\npropose to incorporate 3D structure\nguidance into the video diffusion pro-\ncess for better 3D consistent video frames generation. What's more, we propose a 3D confidence- aware scheme to optimize the final 3DGS from generated frames, which effectively address the uncertainty issue. Extensive experiments demonstrate the superiority of our ReconX the latest state- of-the-art methods in terms of high quality and strong generalizability in unseen data.\nLimitations and Future Work. Although our ReconX achieves remarkable reconstruction results in novel viewpoints, the quality still seems to be limited to the backbone itself as we choose the U-Net based diffusion model DynamiCrafter (Xing et al., 2023) in this work. We expect them to be solved with open-sourced larger video diffusion models (e.g., DiT-based framework). In the future, it is interesting to integrate 3DGS optimization directly with the video generation model, enabling more efficient end-to-end 3D scene reconstruction. We are also interested in exploring consistent 4D scene reconstruction. We believe that ReconX provides a promising research direction to craft intricate 3D worlds from video diffusion models and will inspire more works in the future."}, {"title": "A THEORETICAL PROOF", "content": "Proposition 1. Let \u03b8*, \u03c8* = g* be the optimal solution of the solely image-based conditional diffusion scheme and 8*,* = {g*, F*} be the optimal solution of diffusion scheme with native 3D prior. Suppose the divergence D is convex and the embedding function space \u03a8 includes all measurable functions, we have D(q (x) ||P\u00f5*,* (x)) < D (q(x) ||p\u03b8*,* (x)).\nProof. According to the convexity of D and Jensen's inequality D(E[X]) \u2264 E[D(X)], where X is a random variable, we have:\nD (q(x)||P\u00f5*,* (x)) = D (Eq(s)q(x|s)||[Eq(s)P\u00f5*,* (x|s))\n< Eq(s)D (q(x|s)||P\u0101,* (x|s))\n= Eq(s)D (q(x|s)||P\u0101", "g": "F=(X\\s)), (15)\nwhere we incorporate an intermediate variable s, which represents a specific scene. q(x|s) indicates the conditional distribution of rendering data \u00e6 given the specific scene s. According to the definition of \u03b8*, g*, F*, we have:\nEq(s)D (q(x|s)||P\u014d*,g", "have": "nD (q(x)||P(x)) \u2264 min Eg(s) min min D (q(xls)||P\u04e9,9,E(X))\n< min D (q(x)||P\u04e9,9,E(x)) =\nmin D (q(x)||pe,g,E(\u00d8)(x)) (17)\n0,9,E\n0,9,E(\u00d8)\n= min D (q(x)||p\u04e9,y(x)) = D (q (x) ||\u0440\u04e9*,* (x)) .\n0,\u03c8\nThe second inequality holds because given any specific scene s in any parameter 0 \u2208 \u0398, approximating q(xls) is simpler than q(x) by only tuning the encoder E of p\u03b8,\u0434,\u0415\u00b9, i.e., mine D (q(x|s)||p\u03b8,9,E(x)) <mine D (q(x)||p\u03b8,9,E(X)).\nConsequently, the proof of Proposition 1 has been done."}]}