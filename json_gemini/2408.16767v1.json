{"title": "RECONX: RECONSTRUCT Any Scene FROM SPARSE VIEWS WITH VIDEO DIFFUSION MODEL", "authors": ["Fangfu Liu", "Wenqiang Sun", "Hanyang Wang", "Yikai Wang", "Haowen Sun", "Junliang Ye", "Jun Zhang", "Yueqi Duan"], "abstract": "Advancements in 3D scene reconstruction have transformed 2D images from the real world into 3D models, producing realistic 3D results from hundreds of input photos. Despite great success in dense-view reconstruction scenarios, rendering a detailed scene from insufficient captured views is still an ill-posed optimization problem, often resulting in artifacts and distortions in unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction paradigm that reframes the ambiguous reconstruction challenge as a temporal generation task. The key insight is to unleash the strong generative prior of large pre-trained video diffusion models for sparse-view reconstruction. However, 3D view consistency struggles to be accurately preserved in directly generated video frames from pre-trained models. To address this, given limited input views, the proposed ReconX first constructs a global point cloud and encodes it into a contextual space as the 3D structure condition. Guided by the condition, the video diffusion model then synthesizes video frames that are both detail-preserved and exhibit a high degree of 3D consistency, ensuring the coherence of the scene from various perspectives. Finally, we recover the 3D scene from the generated video through a confidence-aware 3D Gaussian Splatting optimization scheme. Extensive experiments on various real-world datasets show the superiority of our ReconX over state-of-the-art methods in terms of quality and generalizability.", "sections": [{"title": "1 INTRODUCTION", "content": "With the rapid development of photogrammetry techniques such as NeRF and 3D Gaussian Splatting (3DGS) , 3D reconstruction has become a popular research topic in recent years, finding various applications from virtual reality to autonomous navigation and beyond. However, sparse-view reconstruction is an ill-posed problem since it involves recovering a complex 3D structure from limited viewpoint information (i.e., even as few as two images) that may correspond to multiple solutions. This uncertain process requires additional assumptions and constraints to yield a viable solution.\nRecently, powered by the efficient and expressive 3DGS with fast rendering speed and high quality, several feed-forward Gaussian Splatting methods have been proposed to explore 3D scene reconstruction from sparse view images. Although they can achieve promising interpolation results by learning scene-prior knowledge from feature extraction modules (e.g., epipolar transformer), insufficient captures of the scene still lead to an ill-posed optimization problem. As a result, they often suffer from severe artifact and implausible imagery issues when rendering the 3D scene from novel viewpoints, especially in unseen areas."}, {"title": "2 RELATED WORK", "content": "Sparse-view reconstruction. NeRF and 3DGS typically demand hundreds of input images and rely on the multi-view stereo reconstruction (MVS) approaches (e.g., COLMAP ) to estimate the camera parameters. To address the issue of low-quality 3D reconstruction caused by sparse views, PixelNeRF proposes using convolutional neural networks to extract features from the input context. Moreover, FreeNeRF adopts the frequency and density regularized strategies to alleviate the artifacts caused by insufficient inputs without any additional cost. To mitigate the overfitting to input sparse views in 3DGS, FSGS and SparseGS employ a depth estimator to regularize the optimization process. However, these methods all require known camera intrinsics and extrinsics, which is not practical in real-world scenario. Benefiting from the existing powerful 3D reconstruction model (i.e., DUSt3R), InstantSplat is able to acquire accurate camera parameters and initial 3D representations from unposed sparse-view inputs, leading to the efficient and high-quality 3D reconstruction.\nRegression model for generalizable view synthesis. While NeRF and 3DGS are optimized per-scene, a line of research aims to train feed-forward models that output a 3D representation directly from a few input images, bypassing the need for time-consuming optimization. Splatter image performs an efficient feed-forward manner for monocular 3D object reconstruction by predicting a 3D Gaussian for each image pixel. Meanwhile, pixelSplat proposes predicting the scene-level 3DGS from the image pairs, using the epipolar transformer to better extract scene features. Following that, MVsplat introduces the cost volume and depth refinements to produce a clean and high-quality 3D Gaussians in a faster way. To reconstruct a complete scene from a single image, Flash3D adopts a hierarchical 3DGS learning policy and depth constraint to achieve high-quality interpolation and extrapolation view synthesis. Although these methods leverage the 3D data priors, they are limited by the scarcity and diversity of 3D data. Consequently, these methods struggle to acquire high-quality renderings in unseen areas, especially when OOD data is used as input.\nGenerative models for 3D reconstruction. Constructing comprehensive 3D scenes from limited observations demands generating 3D content, particularly for unseen areas. Earlier studies distill the knowledge in the pre-trained text-to-image diffusion models into a coherent 3D model. Specifically, the Score Distillation Sampling (SDS) technique is adopted to synthesize a 3D object from the text prompt. To enhance the 3D consistency, several approaches inject the camera information into diffusion models, providing strong multi-view priors. Furthermore, ZeroNVS and CAT3D extend the multi-view diffusion to the scene level generation. More recently, video diffusion models have shown an impressive ability to produce realistic videos and are believed to implicitly understand 3D structures . SV3D and V3D explore fine-tuning the pre-trained video diffusion model for 3D object generation. Meanwhile, MotionCtrl and CameraCtrl achieve scene-level controllable video generation from a single image by explicitly injecting the camera pose into video diffusion models. However, they don't work for the unconstrained sparse-view reconstruction, which requires strong 3D consistency."}, {"title": "3 PRELIMINARIES", "content": "Video Diffusion Models. Diffusion models have emerged as the cutting-edge paradigm to generate high-quality videos. These models learn the underlying data distribution by adding and removing noise on the clean data. The forward process aims to transform a clean data sample $x_0 \\sim p(x)$ to a pure Gaussian noise $x_t \\sim N(0, I)$, following the process:\n$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, \\epsilon \\sim N(0, 1)$,\nwhere $x_t$ and $\\bar{\\alpha}_t$ denotes the noisy data and noise strength at the timestep t. The denoising neural network $\\epsilon_{\\theta}$ is trained to predict the noises added in the forward process, which is achieved by the MSE loss:\n$L = E_{x \\sim p, \\epsilon \\sim N(0, I), c, t} [\\epsilon - \\epsilon_{\\theta}(x_t, t, c) |_2^2]$,\nwhere c represents the embeddings of conditions like text or image prompt. For the video diffusion models, Latent Diffusion Models (LDMs) , which compress images into the latent space, are commonly employed to mitigate the computation complexity while maintaining competitive performance.\n3D Gaussian Splatting. 3DGS represents a scene explicitly by utilizing a set of 3D Gaussian spheres, achieving a fast and high-quality rendering. A 3D Gaussian is modeled by a position vector $\\mu \\in \\mathbb{R}^3$, a covariance matrix $\\Sigma \\in \\mathbb{R}^{3\\times 3}$, an opacity $\\alpha \\in \\mathbb{R}$, and spherical harmonics (SH) coefficient $c \\in \\mathbb{R}^k$. Moreover, the Gaussian distribution is formulated as the following:\n$G(x) = e^{-(x-\\mu)^\\intercal \\Sigma^{-1}(x-\\mu)}$"}, {"title": "4 MOTIVATION FOR RECONX", "content": "In this paper, we focus on the fundamental problem of 3D scene reconstruction and novel view synthesis (NVS) from very sparse view (e.g., as few as two) images. Most existing works utilize 3D prior and geometric constraints (e.g., depth, normal, cost volume) to fill the gap between observed and novel regions in sparse-view 3D reconstruction. Although capable of producing highly realistic images from the given viewpoints, these methods often struggle to generate high-quality images in areas not visible from the input perspectives due to the inherent problem of insufficient viewpoints and the resulting instability in the reconstruction process. To address this, a natural idea is to create more observations to collapse the under-determined 3D creation problem into a fully constrained 3D reconstruction setting. Recently, video generative models have shown promise for synthesizing video clips featuring 3D structure. This inspires us to unleash the strong generative prior of large pre-trained video diffusion models to create temporal consistent video frames for sparse-view reconstruction. Nevertheless, it is non-trivial as the main challenge lies in poor 3D view consistency among video frames, which significantly limits the downstream 3DGS training process. To achieve 3D consistency within video generation, we first analyze the video diffusion modeling from a 3D distributional view. Let \u00e6 be the set of rendering 2D images from any 3D scene in the world, q(x) be the distribution of the rendering data \u00e6, our goal is to minimize the divergence D:\n$min D (q(x)||p_{\\theta,\\psi}(x))$,\nwhere $p_{\\theta,\\psi}$ be a diffusion model parameterized by $\\theta \\in \\Theta$ (the parameters in the backbone) and $\\psi \\in \\Psi$ (any embedding function shared by all data). In vanilla video diffusion model, they choose CLIP model g to add an image-based condition (i.e., $\\psi = g$). However, in sparse-view 3D reconstruction, only limited 2D images as the condition cannot provide sufficient guidance for approximating q(x) . Motivated by this, we explore the potential of incorporating the native 3D prior (denoted by F) to find a more optimal solution in Equation 8 and derive a theoretical formulation for our analysis in Proposition 1."}, {"title": "5 METHOD", "content": "5.1 OVERVIEW OF RECONX\nGiven K sparse-view (i.e., as few as two) images $\\mathcal{I} = {I_i}_{i=1}^K$, ($I^i \\in \\mathbb{R}^{H\\times W\\times 3}$), our goal is to reconstruct the underlying 3D scene, where we can synthesize novel views of unseen viewpoints. In our framework ReconX, we first build a global point cloud $\\mathcal{P} = {p_i, 1 \\le i \\le N} \\in \\mathbb{R}^{N\\times 3}$ from $\\mathcal{I}$ and project $\\mathcal{P}$ into the 3D context representation space F as the structure guidance $\\mathcal{F}(\\mathcal{P})$ (Sec. 5.2). Then we inject $\\mathcal{F}(\\mathcal{P})$ into the video diffusion process to generate 3D consistent video frames $\\mathcal{I}' = {I_i'}_{i=1}^{K'}$, ($K' > K$), thus creating more observations (Sec. 5.3). To alleviate the negative artifacts caused by the inconsistency among generated videos, we utilize the confidence maps $\\mathcal{C} = {C_i'}_{i=1}^{K'}$ from the DUSt3R model and LPIPS loss to achieve a robust 3D reconstruction (Sec. 5.4). In this way, we can unleash the full power of the video diffusion model to reconstruct intricate 3D scenes from very sparse views. Our pipeline is depicted in Figure 2.\n5.2 BUILDING THE 3D STRUCTURE GUIDANCE\nGrounded by the theoretical analysis in Sec. 4, we leverage an unconstrained stereo 3D reconstruction method DUSt3R with point-based representations to build the 3D structure guidance F. Given a set of sparse images $\\mathcal{I} = {I^i}_{i=1}^K$, we first construct a connectivity graph G(V, E) of K input views similar to DUSt3R, where vertices V and each edge $e = (n,m) \\in E$ indicates that the images $I_n$ and $I_m$ shares visual contents. Then we use G to recover a globally aligned point cloud P. For each image pair $e = (n, m)$, we predict pairwise pointmaps $P_{n,m}, P_{m,n}$ and their corresponding confidence maps $C_{n,m}, C_{m, n} \\in \\mathbb{R}^{H\\times W\\times 3}$. For clarity, let's denote $P_{n,e} := P_{n,n}$ and $P_{m,e} := P_{m,n}$. Since we aim to rotate all pairwise predictions into a shared coordinate frame, we introduce transformation matrix $T_e$ and scaling factor $\\sigma_e$ associated with each pair $e \\in E$ to optimize global point cloud P as:\n$P^* = arg \\underset{P,T, \\sigma}{min} \\sum_{e \\in E} \\sum_{v \\in e} \\sum_{i=1}^{HW} C^i_{v,e} ||P_v - \\sigma_e T_eP^i_e||^2$.\nMore details of the point cloud extraction can be found in Wang et al. . Having aligned the point clouds P, we now project it into a 3D context representation space F through a transformer-based encoder for better interaction with latent features of the video diffusion model. Specifically, we embed the input point cloud P into a latent code using a learnable embedding function and a cross-attention encoding module:\n$\\mathcal{F}(\\mathcal{P}) = FFN \\ (CrossAttn(PosEmb(\\mathcal{P}), PosEmb(\\mathcal{P})))$,\nwhere $\\mathcal{P}$ is a down-sampled version of P at 1/8 scale to efficiently distill input points to a compact 3D context space. Finally, we get the 3D structure guidance $\\mathcal{F}(\\mathcal{P})$ which contains sparse structural information of the 3D scene that can be interpreted by the denoising U-Net.\n5.3 3D CONSISTENT VIDEO FRAMES GENERATION\nIn this subsection, we incorporate the 3D structure guidance $\\mathcal{F}(\\mathcal{P})$ into the video diffusion process to obtain 3D consistent frames. To achieve consistency between generated frames and high-fidelity rendering views of the scene, we utilize the video interpolation capability to recover more unseen observations, where the first frame and the last frame of input to the video diffusion model are two reference views. Specifically, given sparse-view images $\\mathcal{I} = {I^{ref}_i}_{i=1}^{K}$ as input, we aim to render consistent frames $f(I^{ref}_1, I^{ref}_K) = {I^{ref}_1, I'_2, ..., I'_T, I^{ref}_K} \\in \\mathbb{R}^{(T+2)\\times 3\\times H \\times W}$ where T is the number of generated novel frames. To unify the notation, we denote the embedding of image condition in the pretrained video diffusion model as $\\mathcal{F}_g = g(I^{ref})$ and the embedding of 3D structure guidance as $\\mathcal{F}_F = \\mathcal{F}(\\mathcal{P})$. Subsequently, we inject the 3D guidance into the video diffusion process by interacting with the U-Net intermediate feature $F_{in}$ through the cross-attention of spatial layers:\n$F_{out} = Softmax(\\frac{QK^\\intercal}{\\sqrt{d}})V_g + A_F \\cdot Softmax(\\frac{QK^\\intercal}{\\sqrt{d}})V_F$,\nwhere $Q = F_{in}W_Q,K_g = \\mathcal{F}_gW_K,V_g = \\mathcal{F}_gW_V, K_F = \\mathcal{F}_FW'_k,V_F = \\mathcal{F}_FW'_v$ are the query, key, and value of 2D and 3D embeddings respectively. $W_Q,W_K,W'_K,W_V, W'_V$ are the projection matrices and $A_F$ denotes the coefficient that balances image-conditioned and 3D structure-conditioned features. Given the first and last two views condition $C_{view}$ from $F_g$ and 3D structure condition $C_{struc}$ from $\\mathcal{F}_F$, we apply the classifier-free guidance strategy to incorporate the condition and our training objective is:\n$\\mathcal{L}_{diffusion} = E_{x \\sim p, \\epsilon \\sim N(0, I), C_{view}, C_{struc}, t}[|\\epsilon - \\epsilon_{\\theta}(x_t, t, C_{view}, C_{struc})|_2^2]$,\nwhere $x_t$ is the noise latent from the ground-truth views of the training data.\n5.4 CONFIDENCE-AWARE 3DGS OPTIMIZATION.\nBuilt upon the well-designed 3D structure guidance, our video diffusion model generates highly consistent video frames, which can be used to reconstruct the 3D scene. As conventional 3D reconstruction methods are originally designed to handle real-captured photographs with calibrated camera metrics, directly applying these approaches to these generated videos are not perfect to recover the coherent scene due to the uncertainty of unconstrained images . To alleviate the uncertainty issue, we adopt a confidence-aware 3DGS mechanism to reconstruct the intricate scene. Different from recent approaches which model the uncertainty in per-image, we instead focus on a global alignment among a series of frames. For the generated frames ${I'_i}_{i=1}^{K'}$, we denote $C_i$ and $C_i$ the per-pixel color value for generated and ground-truth view i. Then, we model the pixel values as a Gaussian distribution in our 3DGS, where the mean and variance of $I_i$ are $C_i$ and $\\sigma_i$. The variance $\\sigma_i$ measures the discrepancy between the generated view and the real view. Our target is to minize the following negative log-likelihood among all frames:\n$\\mathcal{L}_{total} = - \\sum_{i=1}^{K'} log (\\frac{1}{\\sqrt{2\\pi \\sigma_i^2}}exp(-\\frac{||C_i - \\hat{C_i}||^2}{2\\sigma_i^2}))$,\nwhere $\\hat{C_i} = A(\\hat{C_i}, {\\hat{C_i}'}_{i=1}^{K'} \\setminus \\hat{C_i})$ and A is the global alignment function. Through empirical study, we find a well-aligned mapping function A from the transformer decoder of DUSt3R, which builds the confidence maps {$\\{C'\\}_i\\\\}_{i=1}^{K'}$ for each generated frames {$\\{\\mathcal{I}'\\}_i\\}_{i=1}^{K'}$. Specifically, the confidence score tends to be lower in areas that are difficult to estimate (e.g., regions with solid colors) while the score will be higher in areas with less uncertainty. Moreover, we introduce LPIPS loss to remove the artifacts and further enhance the visual quality. Towards this end, we formulate the confidence-aware 3DGS loss as:\n$\\mathcal{L}_{conf} = \\sum_{i=1}^{K'} C_i (A_{rgb} \\mathcal{L}_{L1}(\\hat{I_i}, I_i) + A_{ssim} \\mathcal{L}_{SSIM}(\\hat{I_i}, I_i) + A_{lpips} \\mathcal{L}_{LPIPS}(\\hat{I_i}, I_i))$."}, {"title": "6 EXPERIMENTS", "content": "In this section, we conduct extensive experiments to evaluate our sparse-view reconstruction framework ReconX. We first present the setup of the experiment (Sec 6.1). Then we report our qualitative and quantitative results compared to representative baseline methods in various settings such as different angle variances from input views and cross-dataset generalization (Sec 6.2). Finally, we conduct ablation studies to further verify the efficacy of our framework design (Sec 6.3).\n6.1 EXPERIMENT SETUP\nImplementation Details. In our framework, we choose DUSt3R as our unconstrained stereo 3D reconstruction backbone and I2V model DynamiCrafter (@512 \u00d7 512 resolution) as the video diffusion backbone. We first finetune the image cross-attention layers with 2000 steps on the learning rate $1 \u00d7 10^{-4}$ for warm-up. Then we incorporate the 3D structure condition $\\mathcal{C}_{struc}$ into the video diffusion model and further finetune the spatial layers with 30K steps on the learning rate of $1 \u00d7 10^{-5}$. Our video diffusion was trained on 3D scene datasets by sampling 32 frames with dynamic FPS at the resolution of 512 \u00d7 512 in a batch. The AdamW optimizer is employed for optimization. At the inference of our video diffusion, we adopt DDIM sampler using multi-condition classifier free guidance . Similar to Xing et al., we adopt tanh gating to learn $A_F$ adaptively."}, {"title": "7 CONCLUSION", "content": "In this paper, we introduce ReconX, a novel sparse-view 3D reconstruction framework that reformulates the inherently ambiguous reconstruction problem as a generation problem. The key to our success is that we unleash the strong prior of video diffusion models to create more plausible observations frames for sparse-view reconstruction. Grounded by empirical study and theoretical analysis, we propose to incorporate 3D structure guidance into the video diffusion process for better 3D consistent video frames generation. What's more, we propose a 3D confidence-aware scheme to optimize the final 3DGS from generated frames, which effectively address the uncertainty issue. Extensive experiments demonstrate the superiority of our ReconX the latest state-of-the-art methods in terms of high quality and strong generalizability in unseen data.\nLimitations and Future Work. Although our ReconX achieves remarkable reconstruction results in novel viewpoints, the quality still seems to be limited to the backbone itself as we choose the U-Net based diffusion model DynamiCrafter in this work. We expect them to be solved with open-sourced larger video diffusion models (e.g., DiT-based framework). In the future, it is interesting to integrate 3DGS optimization directly with the video generation model, enabling more efficient end-to-end 3D scene reconstruction. We are also interested in exploring consistent 4D scene reconstruction. We believe that ReconX provides a promising research direction to craft intricate 3D worlds from video diffusion models and will inspire more works in the future."}, {"title": "A THEORETICAL PROOF", "content": "Proposition 1. Let $\\theta^*, \\psi^* = g^*$ be the optimal solution of the solely image-based conditional diffusion scheme and $\\bar{\\theta}^*, \\bar{\\psi}^* = {g^*, \\mathcal{F}^*}$ be the optimal solution of diffusion scheme with native 3D prior. Suppose the divergence D is convex and the embedding function space \\Psi includes all measurable functions, we have D(q (x) ||p_{\\bar{\\theta}^*,\\bar{\\psi}^*} (x)) < D (q(x) ||p_{\\theta^*,\\psi^*} (x)).\nProof. According to the convexity of D and Jensen's inequality D(E[X]) \u2264 E[D(X)], where X is a random variable, we have:\nD (q(x)||p_{\\bar{\\theta}^*,\\bar{\\psi}^*} (x)) = D (E_{q(s)}q(x|s)||[E_{q(s)}p_{\\bar{\\theta}^*,\\bar{\\psi}^*} (x|s))\n< E_{q(s)}D (q(x|s)||p_{\\bar{\\theta}^*,\\bar{\\psi}^*} (x|s))\n= E_{q(s)}D (q(x|s)||p_{\\theta^*,g^*,\\mathcal{F}^*(X|s)}),\nwhere we incorporate an intermediate variable s, which represents a specific scene. q(x|s) indicates the conditional distribution of rendering data \u00e6 given the specific scene s. According to the definition of $\\bar{\\theta}^*, g^*, \\mathcal{F}^*$, we have:\n$E_{q(s)}D (q(x|s)||p_{\\bar{\\theta}^*,g^*,\\mathcal{F}^*(X|s)}) = min_{\\theta} E_{q(s)} D (q(xls)||p_{\\theta,g,\\mathcal{F}(X|s)})$\n= min_{\\theta} E_{q(s)} min_{g(s),\\mathcal{F}(s)} D (q(xls)||p_{\\theta,g(s),\\mathcal{F}(s)(x)})$\nmin_{\\theta} min_{g,\\mathcal{F}} E_{q(s)} min D (q(x|s)||p_{\\theta,g,\\mathcal{F}(X)}),\nwhere F is the general 3D encoder in 3D structure conditional scheme while it is a redundant embedding in solely image-based conditional scheme, i.e., $\\psi = {g, \\mathcal{E}(\\emptyset)}$. Combining Equation 15 and 16, we have:\nD (q(x)||p_{\\bar{\\theta}^*,\\bar{\\psi}^*} (x)) \u2264 min_{\\theta,g,\\mathcal{F}} min E_{q(s)} min D (q(xls)||p_{\\theta,g,\\mathcal{F}(X)})$\n< min_{\\theta,g,\\mathcal{F}} D (q(x)||p_{\\theta,g,\\mathcal{F}(x)}) =\nmin_{\\theta,g,\\mathcal{F}(\\emptyset)} D (q(x)||p_{\\theta,g,\\mathcal{F}(\\emptyset)(x)})$\n= min_{\\theta,\\psi} D (q(x)||p_{\\theta,\\psi}(x)) = D (q (x) ||p_{\\theta^*,\\psi^*} (x)) .\n The second inequality holds because given any specific scene s in any parameter $\\theta \\in \\Theta$, approximating q(xls) is simpler than q(x) by only tuning the encoder E of p_{\\theta,g,\\mathcal{E}1}, i.e., $min_{\\mathcal{E}} D (q(x|s)||p_{\\theta,g,\\mathcal{E}(x)}) <min_{\\mathcal{E}} D (q(x)||p_{\\theta,g,\\mathcal{E}(X)})$.\nConsequently, the proof of Proposition 1 has been done."}]}