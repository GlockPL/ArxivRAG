{"title": "CalliffusionV2: Personalized Natural Calligraphy Generation with Flexible\nMulti-modal Control", "authors": ["Qisheng Liao", "Liang Li", "Yulang Fei", "Gus Xia"], "abstract": "In this paper, we introduce CalliffusionV2, a novel system\ndesigned to produce natural Chinese calligraphy with flexi-\nble multi-modal control. Unlike previous approaches that\nrely solely on image or text inputs and lack fine-grained\ncontrol, our system leverages both images to guide gen-\nerations at fine-grained levels and natural language texts\nto describe the features of generations. CalliffusionV2 ex-\ncels at creating a broad range of characters and can quickly\nlearn new styles through a few-shot learning approach. It is\nalso capable of generating non-Chinese characters without\nprior training. Comprehensive tests confirm that our sys-\ntem produces calligraphy that is both stylistically accurate\nand recognizable by neural network classifiers and human\nevaluators.", "sections": [{"title": "1. Introduction", "content": "Throughout history, written culture has been a distinctive\nfeature of humanity, particularly evident in the millennia-\nold Chinese culture. The development of writing to some\nextent reflects the progress of human civilization. From ora-\ncle bone script to seal script, from clerical script to standard\nscript, the evolution of Chinese characters bears witness to\nthe development of Chinese culture. This influence extends\nbeyond China, impacting other East Asian countries such as\nKorea and Japan, where Chinese calligraphy has also played\na significant role. Despite its historical significance, in mod-\nern times, mastering calligraphy requires a significant time\ninvestment that many people today find difficult to accom-\nmodate in their busy lives.\nPreviously, many state-of-the-art techniques for charac-\nter generation have been proposed in the Few-shot Font\nGeneration (FFG) task. pix2pix [9], an image-to-image\ntranslation generative adversarial network (GAN) [6], is\nwidely used in this area. Based on pix2pix, Zi2zi [28] is\nthe first method to generate the complex characters of logo-\ngraphic languages but only effective for pre-trained fonts.\nSimilar end-to-end font generation methods [1, 14, 15, 29,\n31] have been proposed with different improvements and\nhave proven that image-to-image translation is effective in\ngenerating any kind of Chinese font or other logographic\nlanguages. In addition, with the development of Denoising"}, {"title": "2. Related Work", "content": "2.1. Chinese Font and Calligraphy Generation\nMost previous works did image-to-image translation by\nGenerative Adversarial Networks (GANs) to do font or cal-\nligraphy generation. Zi2zi [28] is the first approach that\nemploys GANs to generate Chinese characters. Its primary\nfunction is to convert character images from one font style\nto various other font styles. Based on this basic GAN ar-\nchitecture, LF-Font [14] and MX-Font [15] are proposed.\nTheir main idea is to learn the components of each char-\nacter as each Chinese character can be divided into many\nsmaller components. CalliGAN [30] also has similar meth-\nods but uses real calligraphy data in training instead of font\ndata. While DG-Font [31] and CF-Font [29] utilized com-\npletely different methods, feature deformation skip connec-\ntion modules, to transform the low-level feature of con-\ntent images and preserve the pattern of character including\nstrokes and radicals.\nWith the development of Diffusion models, FontDif-\nfuser [32] has shown state-of-the-art performance in gener-\nating diverse characters and styles, especially for complex\ncharacters by diffusion models. While for pure Chinese\ncalligraphy, Calliffusion [12] and CalliPaint [11] showed\nthat, with a small amount of training data and a very simple\nDDPM structure, Chinese calligraphy can be generated and\nis hard to distinguish whether it is generated by machine or\nnot by human beings."}, {"title": "2.2. Diffusion Models with Fine-tuning", "content": "Denoising Diffusion Probabilistic Models (DDPMs) [7]\ngenerate samples that match the data after a certain amount\nof time. In the forward diffusion process, DDPMs add\na small amount of Gaussian noise to a data point sam-\npled from a real data distribution in multiple steps, result-\ning in a sequence of noisy samples. While in the reverse"}, {"title": "3. Methods", "content": "3.1. Overview\nOur natural Chinese calligraphy generation system is an\nend-to-end system shown in Figure 2. The system is flex-\nible to accept any type of image (CalliffusionV2-pro) or\neven no images but pre-trained Chinese characters as inputs\n(CalliffusionV2-base). The system can also generate cal-\nligraphy with untrained new styles using a few shots fine-\ntuning method."}, {"title": "3.2. Preliminaries", "content": "In our study, we leverage a U-net [20] model as the\nbackbone model and incorporate the methodology of\nDDPMs [7]. This approach encompasses a forward process\nthat systematically adds noise to the initial data, denoted as\nx0, and a reverse process that is trained to recover the orig-\ninal data 20 from the noised input. Here, xo specifically\npertains to the calligraphy image. The forward process en-\ntails the introduction of Gaussian noise across N diffusion\nstages, as depicted in Equation 1 below:\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)$.\nwhere B1, B2, ..., BN control the variance scheduling dur-\ning the diffusion procedure. Conversely, the model estab-\nlishes a Markov chain for the reverse process that progres-\nsively reconstructs the calligraphy image xo from a noised\ninput XN, where the noise adheres to a normal distribution\nN(0, I). The following Equation 2 illustrates the steps in-\nvolved in the reverse process:\n$p_{\\theta} (x_{0:T}) = p(x_T) \\prod_{t=1}^{T} q(x_{t-1}|x_{t})$.\nDuring the training phase, our objective is to minimize\nthe target loss by optimizing the model parameters denoted\nas ee, as indicated in equation 3, where t is uniformly\nsampled from [1,N] and e ~ N(0,I), at := 1\n\\beta_t,\n\\overline{a}_t := \\prod_{i=1}^{t} a_i. $\n$L(\\theta) = E_{x_0,\\epsilon,t} [||\\epsilon - \\epsilon_{\\theta}(\\sqrt{\\alpha_t}x_0 + \\sqrt{1 - \\alpha_t}\\epsilon, t)||^2]$."}, {"title": "3.3. Training", "content": "In Figure 2 (a), we show how to train our dual-mode system\nand optimize the model to get similar outputs in both modes.\nBefore training, we assign indices to all characters that\ncan be trained on and establish a character lookup embed-\nding table. Each training sample is accompanied by text\nprompts that detail features such as scripts and styles. Dur-\ning the training process, in order to train both modes to-\ngether, we prepare two unique sets of input components for\neach sample. The text prompts are identical across both\nmodes. In Input 1 (CalliffusionV2-pro mode), we con-\nvert the ground truth images into skeleton images using the\nZhang-Suen thinning algorithm [33]. In contrast, in Input\n2 (CalliffusionV2-base mode), we use blank images as in-\nputs and assign character indices that align with the ground\ntruth images. These input components are then passed into\nencoders and the embedding table to get the cross-attention\nembeddings. For Input 1, the cross-attention embedding is\nformed by concatenating the outputs from both the vision\ntransformer and Chinese BERT encoders. Input 2 intro-\nduces an additional step where the output from the character"}, {"title": "3.4. Inference", "content": "In Figure 2 (b), the inference process of our system is de-\npicted, demonstrating the final end-to-end system available\nfor user interaction. All generated outputs presented in sub-\nsequent sections originate from this system. To expedite\nthe generation process, we employ the UniPC [34] sampler,\nwhich restricts generation time to under one second while\nmaintaining high quality.\nDuring inference, users input prompts specifying the de-\nsired script, style, and other characteristics. Depending on\nthe mode selected, CalliffusionV2-pro or CalliffusionV2-\nbase, users either upload images or enter specific characters\nfor generation. Our system processes these inputs accord-\ningly: in pro mode, images are converted into skeleton im-\nages; in base mode, characters are converted into indices\nand associated with blank images.\nFor CalliffusionV2-pro, the transformed skeleton images\nand user-provided prompts are fed into our trained trans-\nformer encoders, and the outputs are concatenated to form\nthe final cross-attention embeddings. In CalliffusionV2-\nbase, though the procedure is similar with the blank images\nand prompts, the character indices from the lookup embed-\nding table are used to generate character embeddings, which\nare then accumulated into the cross-attention embeddings.\nThe trained U-NET finally generates the calligraphy based\non the cross-attention embeddings."}, {"title": "3.5. New Style Generation via Few-shot Fine-tuning", "content": "We employ a fine-tuning process primarily based on the\nLORA [8] method to adapt our model to new styles, in-\ncluding some digital fonts.\nDuring the fine-tuning phase, we keep the core compo-\nnents of our architecture, the vision transformer encoder,"}, {"title": "4. Experiments", "content": "4.1. Experiment Setups\nAll data in this paper are collected from the Internet by our-\nselves. We only collected real Chinese calligraphy with a\nwhite background. Our dataset contains 60,000 images, in-\ncluding 4000 characters, 5 scripts, and 150 styles. In train-\ning, we manually exclude some famous styles and common\ncharacters. The evaluation experiments then contain fine-\ntuning generation for these famous styles and zero-shot gen-\neration for these common characters. The training was con-\nducted using two A100-40G GPUs for a total of 60 hours."}, {"title": "4.2. Generations", "content": "4.2.1 General Use Case\nFigure 3 illustrates the general use case of our system in\nCalliffusionV2-pro. Users can submit various types of in-\nput images, including authentic Chinese calligraphy, per-\nsonally handwritten characters, or digital fonts with differ-\nent prompts. Specifically, in Figure 3, we provide two gen-\nerations in each column and use different prompts displayed\nbeneath the images for comparisons. Columns (1), (7),\nand (8) utilize the prompts \"Standard script, Liugongquan\"\nand \"Standard script, Yanzhenqin\u201d, representing the distinct\nstyles of Liugongquan (Liu style) and Yanzhenqin (Yan\nstyle) respectively. A notable distinction between these\nstyles is the robustness of each stroke in the Yan style, con-\ntrasting with the more delicate strokes of the Liu style. This\nvariance is accurately depicted in our generated images. In\ncolumn (2), the initial generation is prompted with \"Stan-\ndard script\", while the subsequent one is guided by \"Semi-\ncursive script\". A significant variation between these scripts\nis that in the semi-cursive style, two strokes may merge into\none. This difference is exemplified in our generated im-\nages and highlighted with red circles: the first generation\nshows two distinct strokes in the upper left corner, whereas\nthe second generation depicts them as a single, connected\nstroke. Columns (3) and (4) showcase the contrast between\nclerical script and seal script. The uniform line width in seal\nscript distinctly separates it from clerical script. Column (5)\nelucidates the difference between Maozedong and Wangx-\nizhi styles within the cursive script. Mao's style is charac-\nterized by its broader, more hastily drawn strokes. Lastly,"}, {"title": "4.2.2 Few-shots Fine-tuning", "content": "Figure 4 displays the model's output for four newly fine-\ntuned styles. For each style, we fine-tune the model with\nfive new samples and also provide the pre-fine-tuning gen-\nerations for comparison. In examples (1) and (2), the model\nis tasked with learning two distinct calligraphy styles: \"cler-\nical script, Taixuanzong style\u201d and \u201cseal script, Kuaijikeshi\nstyle\". The strokes in the Taixuanzong style are character-\nized by their strength and boldness, whereas the Kuaijikeshi\nstyle features strokes that are exceptionally thin and uni-\nformly wide. In examples (3) and (4), the model adapts to\ntwo new digital font styles. The first, \u201cKai style\", displays\na slight boldness with strokes of consistent width, while the\nsecond, \"Song style\", is defined by thinner strokes that ter-\nminate in a unique, small triangular shape."}, {"title": "4.2.3 Fine-grained Modification", "content": "Figure 5 demonstrates the capability of our system for fine-\ngrained modification within generated outputs. We can sub-\ntly control the generation by adding, modifying, or deleting\nany parts of the input.\nThe initial three examples illustrate that users, if dissat-\nisfied with specific strokes, have the option to retain the ma-\njority of the output while making minor adjustments to par-\nticular segments, such as incorporating an additional stroke\nor altering the angle of an existing one. In the first example,\nan adjustment is made to the angle of a stroke, highlighted\nin a red circle, followed by the addition of a small dot, trans-\nforming it into a different character. The second example\nfeatures the elongation of the final stroke, also emphasized\nin a red circle. In the third example, an additional stroke\nis introduced in the middle of the character, altering it into\nanother character. For each example, two generations are\npresented under the specific prompts: \u201cStandard script, Li-\nugongquan\" and \"Standard script, Yanzhenqin\" (Liu style\nand Yan style). As previously noted, a key characteristic\ndistinguishing these styles is the boldness of strokes in\nthe Yan style, contrasting with the finer, more delicate strokes\nof the Liu style. Our generated images accurately reflect\nthese stylistic nuances.\nIn examples (4), (5), and (6), we showcase the model's\nability to create non-existent Chinese characters by modify-\ning existing calligraphy. This feature was tested using two\nJapanese characters, \"Tsuji\" and \"Touge\u201d, which are not\nrecognized in traditional Chinese calligraphy and are likely\nunfamiliar to most Chinese calligraphers. To create the\ncharacter \"Tsuji\", example (4) involves the merging of two"}, {"title": "4.2.4 Other Generations", "content": "In Figure 6 (a), we showed the generation for out-of-domain\nnon-Chinese characters. As in CalliffusionV2-pro, the input\nimages control the generation, so we can do zero-shot non-\nChinese generation by providing non-Chinese input images.\nOur tests cover a broad spectrum of characters, including a\ndigit, a Latin alphabet letter, an Arabic letter, a Japanese\nHiragana, and a Greek letter, across four different prompts.\nNotably, the figure employs red circles to underscore spe-\ncific differences among these prompts. For standard script,\nthe direction of the end of a horizontal line towards down,"}, {"title": "4.3. Evaluation", "content": "Evaluating calligraphy generation presents a unique chal-\nlenge due to its artistic nature, which can lead to varied in-\nterpretations among individuals. To address this, we em-\nploy both objective and subjective evaluation to assess the\nmodel's performance and the quality of its generated output."}, {"title": "4.3.1 Objective Evaluation", "content": "Firstly, we compare the generated outputs with the orig-\ninal inputs within our Chinese calligraphy dataset to cal-\nculate various objective metrics, including LPIPS (Learned\nPerceptual Image Patch Similarity), L1 loss, RMSE (Root\nMean Square Error), and SSIM (Structural Similarity In-"}, {"title": "4.3.2 Subjective Evaluation", "content": "In subjective evaluation, we design a survey that involves\nhuman participants. This survey aims to evaluate the con-\nsistency in the styles produced by our model, ensuring it can\naccurately replicate and distinguish between various callig-\nraphy styles. Respondents are from a diverse group of in-\ndividuals, including native Chinese speakers, who have or\ndo not have a calligraphy knowledge background, as well\nas participants from varied cultural backgrounds.\nFor each question in our survey, we present a table out-\nlining the differences between various styles. Accompany-\ning this table are two versions of a selected character: one\nfrom a print version and the other from our generation. We\nthen asked respondents to identify the style of the image we\ngenerated. We also use our trained classifier to evaluate the\ncalligraphy in the survey.\nThe results from the survey are presented in Table 3, re-\nvealing an overall average accuracy rate of 0.710 across all\nparticipants. The classifier also correctly identified these\nquestions with an accuracy rate of 0.762. Consistent with\nexpectations, Chinese individuals with a background in\ncalligraphy demonstrated better performance compared to\ntheir Chinese counterparts lacking such expertise. Notably,\nparticipants from different cultural backgrounds achieved a\nhigher accuracy rate of 0.8. This intriguing outcome may\nbe attributed to the subtle differences across many styles,\nwhich require meticulous examination to discern correctly.\nThe superior performance of individuals from different cul-\ntural backgrounds suggests they may approach the task with\ngreater care. Overall, our system can generate different fea-\ntures and those are identifiable by human participants at an\nacceptable rate."}, {"title": "4.4. Comparisons with Other Tools", "content": "In Figure 7, we compare our generations with some state-\nof-the-art few-shot font generation tools, CF-Font [29], DG-\nFont [31], Fontdiffuser [32], and LF-Font [14]. These com-\nparisons are mainly used to emphasize the differences be-\ntween the Chinese calligraphy generation and few shots font\ngeneration task and we highlight the important differences\nwith red boxes or circles.\nIn our first comparison, we highlight a fundamental dis-\ntinction that font generation produces identical replicas of\nthe same character, authentic calligraphy, as created by hu-\nman, never results in two perfectly identical pieces.\nWhen comparing our approach to DG-Font, two main\ndifferences emerge. First, font generation typically yields\ncharacters of uniform size, whereas, in calligraphy, the di-"}, {"title": "5. Conclusion", "content": "In this paper, we design a multimodal natural Chinese cal-\nligraphy generation system. The generation is guided by\ntext prompts and input images. Our experiments validate\nthat the system is capable of producing high-quality Chi-\nnese calligraphy, capturing the essence of various styles.\nFurthermore, the system is flexible in terms of many dimen-\nsions. It can perform few-shot fine-tuning for new styles,\ngenerate non-Chinese characters without prior training, or\ngenerate calligraphy solely based on text prompts.\nLooking ahead, there's significant potential for investi-\ngating the use of pre-trained large language models in the\nfield of Chinese calligraphy. A fascinating area for future\nstudy could involve employing natural language descrip-\ntions to detail the creation process or to make precise ad-\njustments. Furthermore, these creations could be integrated\ninto other image generation models. At present, a major\nchallenge in image generation is that the characters or lan-\nguages on AI-generated images are often illegible and inac-\ncuate."}]}