{"title": "DAP-LED: Learning Degradation-Aware Priors with CLIP for Joint\nLow-light Enhancement and Deblurring", "authors": ["Ling Wang", "Chen Wu", "Lin Wang"], "abstract": "Autonomous vehicles and robots often struggle\nwith reliable visual perception at night due to the low illu-\nmination and motion blur caused by the long exposure time\nof RGB cameras. Existing methods address this challenge\nby sequentially connecting the off-the-shelf pretrained low-\nlight enhancement and deblurring models. Unfortunately, these\nmethods often lead to noticeable artifacts (e.g., color distortions)\nin the over-exposed regions or make it hardly possible to learn\nthe motion cues of the dark regions. In this paper, we interest-\ningly find vision-language models, e.g., Contrastive Language-\nImage Pretraining (CLIP), can comprehensively perceive di-\nverse degradation levels at night. In light of this, we propose\na novel transformer-based joint learning framework, named\nDAP-LED, which can jointly achieve low-light enhancement\nand deblurring, benefiting downstream tasks, such as depth\nestimation, segmentation, and detection in the dark. The key\ninsight is to leverage CLIP to adaptively learn the degradation\nlevels from images at night. This subtly enables learning rich\nsemantic information and visual representation for optimization\nof the joint tasks. To achieve this, we first introduce a CLIP-\nguided cross-fusion module to obtain multi-scale patch-wise\ndegradation heatmaps from the image embeddings. Then, the\nheatmaps are fused via the designed CLIP-enhanced trans-\nformer blocks to retain useful degradation information for\neffective model optimization. Experimental results show that,\ncompared to existing methods, our DAP-LED achieves state-\nof-the-art performance in the dark. Meanwhile, the enhanced\nresults are demonstrated to be effective for three downstream\ntasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous vehicles and intelligent robots are increas-\ningly being deployed in a variety of environments, rang-\ning from different lighting conditions. A critical challenge\nin these applications is to ensure robust visual perception\nunder various adverse conditions [1], particularly at night.\nTraditional frame-based cameras often struggle in low-light\nenvironments due to the long exposure times needed to cap-\nture sufficient illumination. This often results in significant\nmotion blur [2], leading to poor visibility and reduced scene\nunderstanding accuracy. For example, as shown in Fig. 1,\nwithout the image restoration, a dark image leads to sparse\ndepth estimation, reduces the contrast and sharpness, and loss\nof depth details; More seriously, under low-light conditions,\nobject detection is difficult, resulting in the loss of large\nnumber of targets. In scenarios where intelligent robots are\nrequired to interact with dynamic environments, acquiring\nhigh-quality images with good visibility is crucial for safe\nand efficient operation. As a result, improving visibility\nin low-light conditions and addressing blurring issues are\nessential for vision-related tasks, especially for downstream\napplications like depth estimation, object detection, and\nsemantic segmentation.\nPrevious approaches typically address the two tasks sep-\narately, namely, low-light enhancement [5], [6] and image\ndeblurring [7]\u2013[9], each making independent assumptions\nabout the specific problem [10]. Therefore, simply combining\nthese methods cannot effectively address the joint degrada-\ntion caused by low light and motion blur, and may even exac-\nerbate the degradation due to error accumulation, see Tab. I.\nSpecifically, methods for low light enhancement [5], [6] over-\nlook the spatial degradation caused by motion blur and would\nobscure clues to deblurring due to image smoothing [10].\nConsequently, performing low light enhancement followed\nby deblurring would lead to overexposure of the saturated\nareas of the night-blurred images, resulting in worsening\nblur [11]. On the other hand, methods for deblurring [8],\n[9], [12] are typically trained on datasets that contain only\ndaytime scenes, making them unsuitable for the challenging\ntask of directly debluring images at night due to two reasons.\nFirstly, the low dynamic range makes motion cues in dark\nregions difficult to learn. Secondly, night-blurred images\noften contain saturated regions [10], e.g. light streaks, where\nthe pixel values do not conform to the blur models learned\nfrom daytime data [7], [13]. For these reasons, LEDNet [10]\ntakes a step forward with a unified approach to the joint task\nof low-light enhancement and deblurring.\nBuilding up the challenges, we observe that vision-\nlanguage models, e.g., Contrastive Language-Image Pretrain-\ning (CLIP) [14], offer the potential to inherently perceive\nand adaptively learn the joint degradation levels at night."}, {"title": "II. RELATED WORK", "content": "Low-Light Image Enhancement (LLIE). Methods based on\nRetinex theory are the mainstream approaches for learning-\nbased LLIE. For instance, RetinexNet [15] incorporates\nthe Retinex theory with a Decom-Net for image decom-\nposition and an Enhance-Net for adjusting illumination.\nRetinexformer [16] also integrates Retinex theory into the\ntransformer to guide the modeling of non-local interactions\namong regions with varying lighting conditions. However,\nthese methods often focus on brightness enhancement and\ndenoising while neglecting the motion blur that often occurs\nin the robotic scene understanding process.\nImage Deblurring. DeepDeblur [17], DMPHN [8], and\nMPRNet [18] utilize a multi-stage architecture that progres-\nsively learns the degradation patterns. By contrast, some\nmethods, e.g. Uformer [19] and Stripformer [20] employ\nlocal self-attention to capture long-range dependencies while\nmaintaining low complexity. Restormer [21] models global\ncontext using global channel self-attention. However, these\nmethods primarily focus on deblurring tasks under normal\nlighting conditions, often resulting in subpar performance\nwhen applied to nighttime scene perception.\nJoint Low-light Enhancement and Deblurring. Zhou et\nal. [10] proposed the dataset LOL-Blur, which contains\n12K low-blur/normal-sharp pairs with diverse dark and blur\nlevels in different scenarios. They also propose LEDNet [10],\nwhich contains an encoder for light enhancement and a de-\ncoder for deblurring. The decoder applies spatially adaptive\ntransformation using filters generated by filter adaptive skip\nconnection from enhanced features. ASP-LED [11] leverages\nstructural priors and transformer backbones to address these\nissues by effectively capturing global and local features.\nFourierDiff [22] leverages Fourier priors within pre-trained\ndiffusion models, enabling zero-shot learning that effectively\nhandles luminance and structure degradation without need-\ning paired data or strict degradation assumptions. Rather\nthan relying on predefined structural assumptions or Fourier\npriors, by contrast, our DAP-LED framework leverages the\ndegradation-aware CLIP priors to simultaneously address\nlow-light enhancement and deblurring.\nContrastive Language-Image Pre-Training. Contrastive\nLanguage-Image Pre-Training, i.e. CLIP, has been widely\nutilized for various tasks due to its outstanding cross-modal\nrepresentation capabilities. These applications include areas"}, {"title": "III. PROPOSED METHOD", "content": "The overview of our DAP-LED is depicted in Fig. 3.\nGiven an input image $I \\in R^{H\\times W\\times 3}$, DAP-LED first applies a\n3 \u00d7 3 conv layer to generate a low-level feature representation\n$F_{in} \\in R^{H\\times W\\times C}$; where $H \\times W$ is the spatial resolution and\n$C$ denotes the channel. Meanwhile, we use CLIP's image\nencoder and text encoder, i.e. CLIP-guided Cross-fusion\nModule (CCM) (Sec III-B), to map the image $I$ to the\nCLIP embedding space and obtain the image and text cross\nfeature embeddings $F_{weight}$. And $F_{weight}$ gets downsampled 3\ntimes to correspond to the size of each level. The low-level\nrepresentation $F_{in}$ and CLIP-aware degradation weighted rep-\nresentation $F_{weight}$ is then processed through a 4-level UNet-\nlike hierarchical encoder-decoder, producing deep features\n$F_{latent} \\in R^{\\frac{H}{2^3}\\times \\frac{W}{2^3}\\times 8C}$. The encoder consists of multiple trans-\nformer blocks, and the decoder consists of CLIP-enhanced\nTransformer Blocks (CeTBs) (Sec III-C). Finally, DAP-LED\nemploys a conv layer to generate the residual image $R \\in$\n$R^{H\\times W\\times 3}$ from $F_{latent}$. The restored image $\\hat{I}$ is then obtained\nby adding the degraded image to the residual, i.e., $\\hat{I} = I + R$.\nNow, let's describe the details of CCM and CeTBs."}, {"title": "B. CLIP-guided Cross-fusion Module (CCM)", "content": "The CLIP-guided Cross-fusion Module (CCM) plays a\npivotal role in leveraging CLIP to jointly perceive and\nlearn the low-light and blur characteristics of images. As\nshown in Fig. 4, the module begins by passing the input"}, {"title": "C. CLIP-Enhanced Transformer Blocks (CeTBs)", "content": "As shown in Fig. 3, these blocks are designed to facilitate\nmulti-scale joint-degradation feature learning in that each\ntype of image degradation exhibits distinct perturbed pat-\nterns [19]. Previous studies [19], [31] have shown that in-\ncorporating multi-scale noise components into feature maps\ncan significantly improve a model's ability to handle diverse\ndisturbances, enhancing the recovery of detailed information\nfrom degraded images. However, using uncontrollable noise\nterms in image restoration tasks can be problematic, as it\nintroduces randomness, making it difficult to regulate the\nfiner details of the recovered image [32].\nTo address this issue, we integrate degradation-aware CLIP\npriors at each level of the decoder, analogous to controllable\nnoise elements. The CeTB is based on the Transformer\nblock [21] but with a key distinction: incorporating joint\ndegradation-aware CLIP priors within the attention mech-\nanism. This results in an updated formulation of the CLIP\nEnhanced Attention:\n$\\begin{aligned}\nF_{in} &= F_{in}+Modulation(F_c), \\\\\nF_{out} &= Attn(W_qF_{in}, W_kF_{in}, W_vF_{in}) + F_{in},\n\\end{aligned}$"}, {"title": "D. Optimization", "content": "To eliminate the joint-degradation in the observed image,\nwe utilize the Charbonnier loss as the basic reconstruction\nloss $L_{rec}$ in the RGB color space, which can be denoted as:\n$L_{rec} = \\sqrt{ ||\\hat{I}-I||^2 + \\epsilon^2 }$,\nwhere $I$ and $\\hat{I}$ represent the restored image and ground truth,\nrespectively. $\\epsilon$ is a constant. We also introduce the CLIP-\naware loss to guide the restored result. The CLIP-aware loss\ncomprises two components: the identity loss $L_{identity}$ and the\nCLIP loss $L_{clip}$. By integrating the degradation-aware $L_{clip}$,\nOur method promotes the restored image to align closely\nwith the ground truth in the CLIP embedding space, ensuring\nthe preservation of semantic consistency. Mathematically,\n$\\begin{aligned}\nL_{identity} &= ||E_i(\\hat{I}) - E_i(I)||^2, \\\\\nL_{clip} &= \\frac{ E_i(\\hat{I}).E_t(prompt) }{ |E_i(\\hat{I})||E_t(prompt)| },\n\\end{aligned}$"}, {"title": "IV. EXPERIMENTS", "content": "Implementation. We conduct experiments in PyTorch on 4\nNVIDIA A800 GPUs. To optimize the network, we employ\nthe Adam optimizer with a learning rate 1 \u00d7 10-4. The\nnetwork is trained for 500k iterations with a batch size\nof 4. During training, we utilize cropped patches of size\n512x512 as input, and to augment the training data, random\nhorizontal and vertical flips are applied to the input images.\nThe architecture of our method consists of a 4-level encoder-\ndecoder, with varying numbers of Transformer blocks at\neach level, specifically [1,2,2,4] from level-1 to level-4."}, {"title": "C. Results on the Real-World Dataset", "content": "Tab. II presents the quantitative evaluation of different\nmethods on the Real-LOL-Blur dataset using the MUSIQ,\nNRQM, and NIQE metrics. Our proposed DAP-LED method\nachieves the best performance across the MUSIQ and NRQM\nscores. Although our NIQE score is slightly higher than ASP-\nLED's, it remains highly competitive, further confirming the\neffectiveness of our model in restoring image quality under\nchallenging low-light and blur conditions. Fig. 6 compares\nthe restored images across different methods, further demon-\nstrating the superiority of our approach. In particular, our\nmodel recovers more details and improves the clarity of both\nthe environment and the textual information on the signage,\nas shown in the magnified regions. Compared to the input and\nother methods like SNR, LEDNet and ASP-LED, our results\noffer better contrast and sharpness, eliminating much of the\nblurring and enhancing the overall visibility in low-light\nscenarios. This visual quality comparison further confirms\nthe strong quantitative results, highlighting the effectiveness\nof our method in practical scenarios."}, {"title": "D. Ablation Study", "content": "In Tab. III and Tab. IV, we present the results of the\nablation studies performed on DAP-LED to evaluate the\nimpact of different prompt configurations and loss functions\non image restoration performance. Tab. III compares the\neffectiveness of using different prompt learning strategies,\nincluding deblur prompt, lowlight prompt, and their com-\nbination (Joint Prompt). The results indicate that while\nboth individual prompts improve image quality, the joint\nprompt yields the highest PSNR and SSIM, demonstrating\nthe benefits of leveraging both aspects simultaneously.\nTab. IV presents an ablation study on the losses used\nduring training, showing the impact of adding identity loss\n($L_{identity}$) and CLIP loss ($L_{clip}$) alongside the reconstruction\nloss ($L_{rec}$). The results show a progressive improvement in\nPSNR and SSIM as these additional losses are included.\nThe highest performance is achieved with the full loss com-\nbination, highlighting the importance of incorporating both\nidentity and CLIP constraints for improved image restoration."}, {"title": "E. Applications to Downstream Vision Tasks", "content": "To verify the effectiveness of our DAP-LED framework,\nwe further apply the enhanced results to three downstream\nvision tasks, depth estimation (with DepthAnything V2 [37]),\nsegmentation (SAM [38]), and detection (YoloV8 [39]). We\nprovide comparisons of three downstream tasks for the night-\nblurred image and enhanced image. Fig. 7 and Fig. 8 depict\nthe qualitative results for 1) the synthetic LOL-Blur dataset\nand 2) the real-world Real-LOL-Blur dataset, respectively.\nThe bottom row of each figure demonstrates better results\nwith our enhanced image as input. YOLO V8 enhances both\nthe precision of object detection and the total number of\ndetected objects. SAM delivers clearer segmentation bound-\naries, while Depth Anything V2 generates more detailed and\naccurate depth maps."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we introduced DAP-LED, a novel joint low-light enhancement and deblurring framework that leverages\nthe powerful multi-modal perception capabilities of the CLIP\nmodel. By utilizing CLIP's text-image embedding space, our\nmethod can perceive and restore complex image degradations\nmore effectively. Through extensive experiments, we demon-\nstrated that DAP-LED achieves state-of-the-art performance\non real night-blurred images, beneficial for three downstream\ntasks in the dark for robotic scene understanding. Future\nwork will focus on expanding this framework to handle more\ndiverse degradations and further optimizing the restoration\nprocess for real-time applications."}]}