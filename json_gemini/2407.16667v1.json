{"title": "RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent", "authors": ["Huiyu Xu", "Wenhui Zhang", "Zhibo Wang", "Feng Xiao", "Rui Zheng", "Yunhe Feng", "Zhongjie Ba", "Kui Ren"], "abstract": "Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot. These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of threats. Among them, jailbreak attacks that induce toxic responses through carefully constructed jailbreak prompts have raised critical safety concerns. To effectively identify these threats, a growing number of red team approaches simulate potential adversarial scenarios by crafting jailbreak prompts to test the target LLM. However, existing red teaming methods do not consider the unique vulnerabilities of LLM in different scenarios (e.g., code-related tasks), making it difficult to adjust the jailbreak prompts to find context-specific vulnerabilities, thereby lacking efficiency. Meanwhile, these methods are limited to refining handcrafted jailbreak templates using a few mutation operations (such as synonym replacement), lacking the automation and scalability to continuously adapt to different scenarios. To enable context-aware and efficient red teaming, we abstract and model existing attacks into a coherent concept called \"jailbreak strategy\" and propose a multi-agent LLM system named RedAgent that leverages these strategies to generate context-aware jailbreak prompts. By self-reflecting on contextual feedback and red teaming trials in an additional memory buffer, RedAgent continuously learns how to leverage these strategies to achieve more effective jailbreaks in specific contexts. Extensive experiments demonstrate that our system can jailbreak most black-box LLMs within just five queries, improving the efficiency of existing red teaming methods by two times. Additionally, RedAgent can jailbreak customized LLM applications more efficiently. By generating context-aware jailbreak prompts towards trending applications on GPTs, we discover 60 severe vulnerabilities of these real-world applications with only two queries per vulnerability. We have reported all found issues and communicated with OpenAI and Meta for bug fixes. Furthermore, our results indicate that LLM applications enhanced with external data or tools are more vulnerable to jailbreak attacks than foundation models.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) such as GPT-4 [1] and Gemini [50], are trained on a wide range of public domain language corpora [11], [16], [56], [64] and perform well in a variety of tasks such as natural language processing [14], [63], code generation [35], and tool usage [9]. Due to impressive capabilities, LLMs have been employed as foundational components for numerous applications aimed at addressing real-world tasks, such as GPTs [36]. These LLM-integrated applications leverage domain-specific knowledge to enhance their capabilities and adaptability to specialized tasks. However, these applications significantly expand the attack surface of LLM, exposing it to various prompt-level security risks, which may induce undesirable toxic or sensitive outputs [13], [20], [25]. Among them, jailbreak attacks use carefully crafted prompts to bypass LLM's security mechanisms and elicit harmful responses. Due to its low cost and critical safety impact on LLM applications, it is listed as the top threat by OWASP [39]. For example, as shown in Figure 1, a jailbreak attack might result in a response that contains a detailed tutorial on creating the drug heroin in a custom LLM math [41]. Despite extensive efforts by major LLM developers [5], [17], [37] in developing usage policies and implementing various alignment technologies such as RLHF [38] in their models, defending against jailbreak attacks to prevent the generation of harmful content remains a pressing challenge. This is evidenced by tens of thousands of effective jailbreak prompts in the wild [22].\nTo identify the jailbreak vulnerabilities of LLMs, there has been a growing number of red teaming methods [2], [10], [29], [31], [60], [62] that focus on creating jailbreak prompts to elicit harmful responses from the target LLM, simulating potential adversarial scenarios in a black-box setting. Typically, given a malicious goal to jailbreak (e.g., generate violent content in a video script), a red teaming method first requires human effort to craft specific jailbreak templates (i.e., human red team), such as the early well-known 'DAN' Jailbreak template [2]. Based on these jailbreak templates, some red teaming methods [10], [29], [31], [60], [62] further leverage an LLM to refine (e.g., syntax transformation and synonym replacement) these human-crafted templates as jailbreak prompts to query the target LLM. These prompts are iteratively refined based on the target LLM's responses to increase their effectiveness, as demonstrated in Figure 2.\nHowever, existing red teaming methods are difficult to overcome the following challenges: 1) Lack of high-quality jailbreak prompts: While existing work can generate thousands of prompts in minutes, we found that these prompts are"}, {"title": "II. BACKGROUND & PROBLEM STATEMENT", "content": "This section first introduces the definition of LLMs and the jailbreak attacks targeting them. Subsequently, we present the system model of typical red teaming methods used to identify jailbreak threats in LLMs and further illustrate the general concepts of language agents that inspire our work.\nA. LLM\nA Large Language Model (LLM) is an advanced type of artificial intelligence that processes and generates human-like text based on patterns and information learned from vast amounts of data, including public domain sources like Wikipedia [56], Reddit [43], and collections of books. LLMs use transformers to understand relationships between all words in a sentence in a self-attentive manner, regardless of their order, allowing them to generate coherent and contextually appropriate text. Due to their impressive capabilities in reasoning and tool using, LLM-integrated applications have emerged and perform well in various tasks, including code generation (e.g., GitHub Copilot [15]), writing assistance (e.g., Grammarly [18]), and advanced search engines (e.g., Bing Search [32]).\nPrompts. In the inference stage of LLMs, a prompt commonly refers to the input text provided by the user, which sets the context and specifies the task for the model to address. This helps guide the generation of relevant and coherent outputs. Alongside the user-provided prompt, there is often a system prompt, which acts as a default prefix implicitly added to the user's input. This system prompt contains pre-configured instructions, assisting LLMs in better understanding and responding to the user's request.\nB. LLM Jailbreak\nLLM jailbreak refers to the manipulation of LLMs to bypass their built-in safety mechanisms and content filters by crafting jailbreak prompts through prompt engineering. By exploiting specific vulnerabilities in the model's responses, a jailbreak prompt aims to elicit unexpected or forbidden outputs, essentially tricking the model into violating its safety policies. Some studies [30], [46], [54], [61] summarize common patterns of jailbreak prompts into various strategies, where prompts within each strategy share similar patterns.\nC. Threat Model\nTarget LLM. In this study, we focus on text-based LLMs, which serve as the targets for adversaries. These LLMs are securely trained, free from poisoning or any other form of malicious tampering. They can enhance their capabilities by using additional domain-specific data for fine-tuning or by leveraging external databases for retrieval-augmented generation. Additionally, they may utilize online tools or other built-in functionalities (e.g., calculators), effectively functioning as holistic agents.\nMalicious Goal. In this study, we focus on common jailbreak questions (e.g., how to make a bomb) that aim to elicit harmful outputs, including hateful, harassing, or violent"}, {"title": "III. REDAGENT", "content": "In this section, we present our novel agent-based red team system, RedAgent, to achieve efficient context-aware jailbreak prompts generation. In Section III-A, we begin by illustrating our design intuition. In Section III-B, we provide an overview of our proposed system. Then we introduce the Skill Memory in Section III-C and the details of three stage within RedAgent in Section III-D, III-E, and III-F.\nA. Design Intuition\nFigure 1 illustrates the comparison of using general jailbreak strategy and context-aware strategy in jailbreaking real-world LLM application LLM Math [41] under the same malicious goal of making Heroin.\nContext-aware jailbreak prompts are of higher quality and play a key role in jailbreak specific LLM applications. As shown in Figure 1, we can observe that the custom LLM Math [41] designed for math-related tasks is effectively jailbroken under the context-aware jailbreak prompt, while refusing to respond under the general jailbreak prompt. This context-aware jailbreak prompt conceals the malicious goal within a task that the model is good at, thus increasing the effectiveness of this attack. Therefore, adjusting the prompts to more relevant contexts can significantly improve the effectiveness of jailbreaking specific LLM applications. This is because specific LLM applications are often fine-tuned or hinted with additional data, resulting in different unique vulnerabilities. Given the different weaknesses inherent in different LLMs (either foundation models or specific LLM applications), it is important to generate context-aware jailbreak prompts in red team approaches. Motivated by such observation, we propose to generate context-aware jailbreak prompts that capture the context information of different LLM models and applications to guide the generation of jailbreak prompts.\nLack of automation and scalability further limits existing red teaming methods from continually gaining longer-term insights to improve the efficiency. Existing red teaming methods are limited to using a few mutation operations (e.g., synonym replacement) to refine the human-written jailbreak"}, {"title": "B. Overview", "content": "Based on our observations in Section III-A, we propose the agent-based red teaming system, RedAgent, to 1) generate context-aware jailbreak prompts by obtaining contextual information from model feedback; 2) continuously learn to exploit jailbreak strategy that self-reflected in an additional memory buffer. Figure 3 shows the architecture of RedAgent, which consists of three main stages.\nSpecifically, in the Context-aware Profiling stage, the Profile Constructor perceives the specific context of the target LLM to craft a context-aware malicious goal (e.g., guide the target LLM to output harmful code of a Trojan). The planner in the Adaptive Jailbreak Planning phase then reasons from skill memory to develop an attacking plan that guides the Attacking and Reflection phase until the last trial succeed. Guided by the attacking plan, the Attacking and Reflection Stage generates the jailbreak prompts and queries the target LLM, obtaining and evaluating its response. For each iteration, the evaluator of this stage gathers context feedback and performs self-reflection on the interaction to update the Skill Memory."}, {"title": "C. Skill Memory", "content": "Based on the observations described in Section III-A, our key design goal is to enable the red team to continuously exploit jailbreak strategies self-reflected on trials to adapt to varying scenarios, thereby improving the efficiency of red teaming. To achieve this, RedAgent incorporates a Skill Memory that stores the understanding of past experiences during red teaming to maintain this learning behavior.\nSpecifically, the Skill Memory consists of two parts: long-term memory and short-term memory, as decipted in Figure 3. For long-term memory, we collect common jailbreak strategies from the study [62] and further leverage LLM to summarize in-the-wild jailbreak prompts into these strategies or add new strategies. Additionally, the long-term memory stores past successful trials and their associated experiences, marked with tags such as the scenarios of the malicious goal, the used strategy, and the key parts of the successful jailbreak prompts. This tagging considers the relationship between effective strategies for different malicious goals, enhancing the understanding of past experiences when the planner reasons about similar malicious goals. Short-term memory stores detailed experiences from the last few iterations, providing rich contextual information about the interactions. This includes evaluation scores, context feedback, and how to improve effectiveness, obtained from the evaluator's self-reflection outputs."}, {"title": "D. Context-aware Profiling", "content": "Since the capabilities of different LLMs vary, it is crucial to craft suitable malicious goals to identify relevant vulnerabilities. The Context-aware Profiling Stage perceives and crafts context-aware malicious goals to ensure they lie on the safety boundary of the target LLM, especially in specific domains (e.g., code assistant).\nSpecifically, during the initial interaction with the target LLM, probing sentences such as \u201cHello, who are you?\u201d are used to gather initial information about the target LLM's functionality. This information is tagged as the basic setting of"}, {"title": "E. Adaptive Jailbreak Planning", "content": "To improve the effectiveness of the chosen strategies of red team for the specific scenario, the planner in the Adaptive Jailbreak Planning stage leverages the crafted malicious goal to retrieve relevant memory entries. Then, it crafts an initial plan in the first iteration. This initial plan includes the reasoning process, the attack role with basic descriptions, and the strategy and its example for the attacker, for guiding the LLMs in the Attacking and Reflection stage.\nWe then extend the action space of refinement by allowing the planner to determine the next action to refine the current attack, enabling the agentic system to improve intelligence. For each refinement, the planner is engaged and directly reasons on the Skill Memory and the malicious goal to select the most effective strategies. Our agent's action space mirrors several predefined states:\n\u2022 Align the goal: If the attack begins to deviate from the original goal, evoke an instruction for the planner to reassess and possibly modify the goal and craft a new plan.\n\u2022 Refine the attack strategy: If the current strategy (as outlined in the response) fails to elicit the desired response without achieving the malicious goal, evoke a corresponding instruction for the planner to consider refining the strategy.\n\u2022 Retry the attack prompt: If the response fails to achieve the malicious goal due to randomness in the language model, and the adopted strategy succeeded multiple times in memory, continue using this prompt and retry it.\n\u2022 Refine the attack prompt: Evoke a corresponding instruction to let attacker use the current strategy, but introduce more variety while maintaining the original intention by leveraging the understanding of strategy.\n\u2022 End this goal: Conclude the interaction once the response has successfully met the objective and achieved a score above threshold.\nEach action plays a unique role in refining the attack prompts in a human-like style, thereby enhancing the intelligence of red teaming."}, {"title": "F. Attacking and Reflection", "content": "In the Attacking and Reflection Stage, we enhance the efficiency of red teaming by incorporating self-reflection mechanisms. This allows RedAgent to continuously refine the Skill Memory by updating its long-term part with summaries of past effective trials and its short-term part with context feedback, aiming to better guide the planner in autonomous refinement.\nSpecifically, the attacker first crafts jailbreak prompts according to the guidance in the attacking plan and queries the target LLM. Then, the evaluator assesses the jailbreak response of the target LLM to determine whether the response is jailbroken and provides an analysis in the evaluation results to guide further refinement.\nBased on the observations in Section III-A, LLMs have unique weaknesses in certain contexts and behave differently (e.g., at different levels of detail and in different rejection behaviors), we aim to mine these fine-grained differences and add them to the evaluation results to better guide the planner in refinement. In RedAgent, the evaluator explicitly analyzes the target LLM's intentions, confidence levels, and security mechanisms in each attack iteration. This thorough analysis provides a global view, helping to approach effective attack prompts more strategically. It prevents repeated attempts on the same errors by addressing the uncompressed and lengthy responses that may mislead the attacker, ensuring key parts are refined in subsequent steps.\nSuch incorporation of context feedback offers several advantages: Firstly, context information highlights the differences from general LLM outputs that lack domain-specific knowledge and format requirements. This context can indicate what the target LLM can and cannot answer, indirectly representing the security mechanisms of the target LLM. Leveraging this information helps identify specific vulnerabilities within the target LLM and avoid crafting out-of-scope attack prompts. Secondly, the response reveals the target LLM's intentions and certainty in answering a question, particularly in more advanced LLM applications with extensive data and complex reasoning processes. By incorporating context feedback in the evaluation results, RedAgent obtain sufficient information to guide the generation of context-aware jailbreak prompts.\nTo improve the refinement quality of updating skill memory to enhance the learning behavior of RedAgent, we exploit the self-reflection ability of LLM to abstract key parts and label long and complex contexts. By adding this abstraction layer, the evaluator marks the key features that contributed to success as a new entry of the Skill Memory, as shown in the Figure 3. This includes identifying effective strategies and summarizing the attack scenario in a compact form. Such updates contribute to the ever-changing understanding of jailbreaking strategies in specific contexts, which is enhanced by effective trials and specific patterns (e.g., some skills of it) added to the strategy description.\nImplementation for each components. In our experiments, we implement each component (e.g., role) of RedAgent with individual LLMs, which are used via API. We employ the LLM-empowered evaluator with in-context learning to implement the evaluator, measuring the effectiveness of the attack by predicting a score. Following the study [42], we use the mainstream OpenAI usage policy as the evaluation preference for jailbreak evaluation, rating responses on a scale from 1 to 5."}, {"title": "IV. EVALUATION", "content": "To evaluate the effectiveness of RedAgent, we conduct evaluations on both general LLM chatbots and specific LLM-centered applications. We will first present an overview of our evaluation and highlight our findings in Section IV-A. In Section IV-B, we will introduce the experimental setup. Finally, we will discuss our evaluation results through three research questions:\n\u2022 RQ1: Do various LLMs have unique weaknesses that make them susceptible to specific attack strategies? (Section IV-C)\n\u2022 RQ2: Can RedAgent efficiently achieve context-aware red teaming compared to existing works? (Section IV-D)\n\u2022 RQ3: How does the Skill Memory contribute to the attack performance improvement in RedAgent? (Section IV-E)\nA. Evaluation Overview\nTo study RQ1, we analyze successful strategies against different LLMs, targeting 2 open-source and 4 closed-source models to reveal 290 novel jailbreak cases. By analyzing the"}, {"title": "D. Efficiency of RedAgent in Red Teaming LLMs (RQ2)", "content": "In this subsection, we will demonstrate the efficiency of our system in generating context-aware jailbreak prompts to find the most relevant vulnerabilities of the target LLM compared with the state-of-the-art model red team methods. We first compare the attack performance of RedAgent in"}, {"title": "E. Ablation Study (RQ3)", "content": "RedAgent has demonstrated exceptional performance in jailbreaking general LLM chatbots and specific LLM-centered applications, particularly enhancements in effectiveness (ASR) and efficiency (ANQ). To further explore the origins of these capabilities and identify which design elements are critical, we"}, {"title": "V. DISCUSSION", "content": "In this section, we will first present the ethical considerations of our work, and then discuss the limitations and potential future directions of RedAgent to improve its efficiency and the quality of the generated jailbreak prompts.\nA. Ethical Considerations\nIn this study, we adhered to ethical standards to ensure safety and privacy. Our experiments were conducted using platforms provided officially or through open-source models deployed in a closed environment. We did not disseminate any harmful or illicit content to the public or others. The datasets\nB. Limitations and Future Work\nLimitations Although RedAgent demonstrates impressive attack performance, we acknowledge several limitations of our method, which are discussed below. Firstly, the memory mechanism in RedAgent is not highly efficient. For instance, we simply inputs all memory entries as text to the planner, which may limit its scalability as the number of entries increases. And the current memory tags does not take into account the model-specific details, reducing the effectiveness of cross-model adaptability.\nBesides, our methodology is limited to text-based modality, which restricts its applicability as LLM applications increasingly integrate multimodal functionalities. Expanding RedAgent to handle other modalities, such as images, audio, and video, would broaden its usefulness and robustness in real-world settings.\nFurther, RedAgent cannot handle well in more complex scenarios such as the GPTs with diverse interaction types (e.g., requiring data uploads or specific inputs for task completion). Addressing this limitation requires enhancing RedAgent's ability to handle varied and intricate use cases effectively.\nFuture Work To tackle the above limitations, we propose several future directions to enhance our red teaming tool. Firstly, we could develop more sophisticated methods for planner input and retrieval. One approach could be integrating the Retrieval-Augmented Generation (RAG) system, which would enable the planner to dynamically retrieve relevant information from extensive documents, allowing for more efficient and effective memory entry handling.\nAdditionally, we could incorporate model-specific information into the memory tags to improve cross-model performance, such as model vulnerability. This could involve creating a simple database of known vulnerabilities for different models and tagging memory entries with relevant vulnerability information. By doing so, RedAgent can tailor its attack based on the specific weaknesses of the model in trials, leading to more targeted and effective red-teaming exercises.\nLastly, we could enhance the system's capability to support multimodal functionalities and manage complex interactions. This could involve integrating visual and auditory input processing capabilities, allowing the tool to handle a wider range of scenarios. We could explore these directions in the future."}, {"title": "VI. RELATED WORK", "content": "In this section, we provide a comprehensive overview of jailbreak attacks, which can be broadly categorized into three"}, {"title": "A. Human-centered Jailbreak Attacks", "content": "Human-centered jailbreak attacks are among the earliest jailbreak methods, emerging shortly after the release of large language models. These approaches heavily rely on human intuition, experience, and creativity to craft jailbreak prompts. Since the advent of ChatGPT, users have shared numerous jailbreak prompts via social media and websites [6], [8], [21], [33], [49], [51], leading to models generating unexpected or offensive responses, such as pornographic content and hate speech. For instance, attackers have heuristically designed various static jailbreak templates [22] or translated harmful prompts into low-resource languages [12], [59] to circumvent alignment filters. More sophisticated methods such as the ASCII attack [23], DrAttack [26], many-shot jailbreaking [4] and crafting imaginary scenes with various characters (Deep-Inception) [27] have also shown effectiveness in generating these attacks."}, {"title": "B. Algorithm-centered Jailbreak Attacks", "content": "Algorithm-centered jailbreak attack employ computational algorithms to discover effective prompts. For example, Zou et al. [65] introduced the Greedy Coordinate Gradient (GCG) algorithm, which optimizes jailbreak suffixes to maximize the likelihood of affirmative responses to malicious queries based on the gradients of the target LLM. Researchers have also developed genetic algorithms [24], [29] to mutate and select effective prompts. Furthermore, some studies have drawn connections between jailbreak prompt generation and controlled text generation, employing energy functions to guide the attack process, as seen in the COLD-Attack methodology [19]."}, {"title": "C. Model-centered Jailbreak Attacks", "content": "Model-centered jailbreak attacks leverage the capabilities of large language models to generate and refine jailbreak prompts. GPTFuzzer [60] utilizes a mutating LLM to generate jailbreak prompts based on manually crafted seed templates, effectively automating the process of prompt variation and exploration. The PAIR framework [10] employs an attack model to create and iteratively improve jailbreak prompts based on feedback from the target LLM and the evaluator. Building on this, Mehrotra et al. [31] developed TAP, which introduces tree-of-thought reasoning and filters out irrelevant and low-scoring prompts, thus reducing the average number of jailbreak queries and improving the overall success rate. Additionally, PAP [62] regard the target LLM as a human-like communicator and using an attack LLM to generate persuasive jailbreak prompts based on descriptions of persuasion techniques."}, {"title": "VII. CONCLUSION", "content": "In this work, we designed and implemented RedAgent, an agent-based red teaming method tailored for automated context-aware jailbreaking testing. Our method improved the efficiency of red teaming by autonomously exploiting jailbreak strategies stored in an additional memory buffer that allows the system to continuously adapt to different scenarios. Additionally, RedAgent extended the action space of refinement and autonomously determined the action based on contextual feedback to generate context-aware jailbreak prompts. Our experiments on two open-source and four closed-source LLMs demonstrated that RedAgent was two times more efficient (i.e., within just five queries) than state-of-the-art red teaming methods, with an even higher success rate (i.e., higher than 90% on average). By jailbreaking 60 widely-used custom services on GPTs marketplace of OpenAI and identifying 60 severe vulnerabilities of them, we also demonstrated the capability of RedAgent to generate context-aware jailbreak prompts in testing real-world LLM applications. Furthermore, we found that LLM applications enhanced with external data or tools are more vulnerable to jailbreak attacks than foundation models."}, {"title": "APPENDIX", "content": "A. Target LLM System Prompt\nFor all target LLMs used in our experiments, we used default system prompts, except for Llama 2, which is set"}]}