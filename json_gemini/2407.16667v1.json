{"title": "RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent", "authors": ["Huiyu Xu", "Wenhui Zhang", "Zhibo Wang", "Feng Xiao", "Rui Zheng", "Yunhe Feng", "Zhongjie Ba", "Kui Ren"], "abstract": "Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot. These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of threats. Among them, jailbreak attacks that induce toxic responses through carefully constructed jailbreak prompts have raised critical safety concerns. To effectively identify these threats, a growing number of red team approaches simulate potential adversarial scenarios by crafting jailbreak prompts to test the target LLM. However, existing red teaming methods do not consider the unique vulnerabilities of LLM in different scenarios (e.g., code-related tasks), making it difficult to adjust the jailbreak prompts to find context-specific vulnerabilities, thereby lacking efficiency. Meanwhile, these methods are limited to refining handcrafted jailbreak templates using a few mutation operations (such as synonym replacement), lacking the automation and scalability to continuously adapt to different scenarios. To enable context-aware and efficient red teaming, we abstract and model existing attacks into a coherent concept called \"jailbreak strategy\" and propose a multi-agent LLM system named RedAgent that leverages these strategies to generate context-aware jailbreak prompts. By self-reflecting on contextual feedback and red teaming trials in an additional memory buffer, RedAgent continuously learns how to leverage these strategies to achieve more effective jailbreaks in specific contexts. Extensive experiments demonstrate that our system can jailbreak most black-box LLMs within just five queries, improving the efficiency of existing red teaming methods by two times. Additionally, RedAgent can jailbreak customized LLM applications more efficiently. By generating context-aware jailbreak prompts towards trending applications on GPTs, we discover 60 severe vulnerabilities of these real-world applications with only two queries per vulnerability. We have reported all found issues and communicated with OpenAI and Meta for bug fixes. Furthermore, our results indicate that LLM applications enhanced with external data or tools are more vulnerable to jailbreak attacks than foundation models.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) such as GPT-4 [1] and Gemini [50], are trained on a wide range of public domain language corpora [11], [16], [56], [64] and perform well in a variety of tasks such as natural language processing [14], [63], code generation [35], and tool usage [9]. Due to impressive capabilities, LLMs have been employed as foundational components for numerous applications aimed at addressing real-world tasks, such as GPTs [36]. These LLM-integrated applications leverage domain-specific knowledge to enhance their capabilities and adaptability to specialized tasks. However, these applications significantly expand the attack surface of LLM, exposing it to various prompt-level security risks, which may induce undesirable toxic or sensitive outputs [13], [20], [25]. Among them, jailbreak attacks use carefully crafted prompts to bypass LLM's security mechanisms and elicit harmful responses. Due to its low cost and critical safety impact on LLM applications, it is listed as the top threat by OWASP [39]. For example, as shown in Figure 1, a jailbreak attack might result in a response that contains a detailed tutorial on creating the drug heroin in a custom LLM math [41]. Despite extensive efforts by major LLM developers [5], [17], [37] in developing usage policies and implementing various alignment technologies such as RLHF [38] in their models, defending against jailbreak attacks to prevent the generation of harmful content remains a pressing challenge. This is evidenced by tens of thousands of effective jailbreak prompts in the wild [22].\nTo identify the jailbreak vulnerabilities of LLMs, there has been a growing number of red teaming methods [2], [10], [29], [31], [60], [62] that focus on creating jailbreak prompts to elicit harmful responses from the target LLM, simulating potential adversarial scenarios in a black-box setting. Typically, given a malicious goal to jailbreak (e.g., generate violent content in a video script), a red teaming method first requires human effort to craft specific jailbreak templates (i.e., human red team), such as the early well-known 'DAN' Jailbreak template [2]. Based on these jailbreak templates, some red teaming methods [10], [29], [31], [60], [62] further leverage an LLM to refine (e.g., syntax transformation and synonym replacement) these human-crafted templates as jailbreak prompts to query the target LLM. These prompts are iteratively refined based on the target LLM's responses to increase their effectiveness, as demonstrated in Figure 2.\nHowever, existing red teaming methods are difficult to overcome the following challenges: 1) Lack of high-quality jailbreak prompts: While existing work can generate thousands of prompts in minutes, we found that these prompts are"}, {"title": "II. BACKGROUND & PROBLEM STATEMENT", "content": "This section first introduces the definition of LLMs and the jailbreak attacks targeting them. Subsequently, we present the system model of typical red teaming methods used to identify jailbreak threats in LLMs and further illustrate the general concepts of language agents that inspire our work.\nA. LLM\nA Large Language Model (LLM) is an advanced type of artificial intelligence that processes and generates human-like text based on patterns and information learned from vast amounts of data, including public domain sources like Wikipedia [56], Reddit [43], and collections of books. LLMs use transformers to understand relationships between all words in a sentence in a self-attentive manner, regardless of their order, allowing them to generate coherent and contextually appropriate text. Due to their impressive capabilities in reasoning and tool using, LLM-integrated applications have emerged and perform well in various tasks, including code generation (e.g., GitHub Copilot [15]), writing assistance (e.g., Grammarly [18]), and advanced search engines (e.g., Bing Search [32]).\nPrompts. In the inference stage of LLMs, a prompt commonly refers to the input text provided by the user, which sets the context and specifies the task for the model to address. This helps guide the generation of relevant and coherent outputs. Alongside the user-provided prompt, there is often a system prompt, which acts as a default prefix implicitly added to the user's input. This system prompt contains pre-configured instructions, assisting LLMs in better understanding and responding to the user's request.\nB. LLM Jailbreak\nLLM jailbreak refers to the manipulation of LLMs to bypass their built-in safety mechanisms and content filters by crafting jailbreak prompts through prompt engineering. By exploiting specific vulnerabilities in the model's responses, a jailbreak prompt aims to elicit unexpected or forbidden outputs, essentially tricking the model into violating its safety policies. Some studies [30], [46], [54], [61] summarize common patterns of jailbreak prompts into various strategies, where prompts within each strategy share similar patterns.\nC. Threat Model\nTarget LLM. In this study, we focus on text-based LLMs, which serve as the targets for adversaries. These LLMs are securely trained, free from poisoning or any other form of malicious tampering. They can enhance their capabilities by using additional domain-specific data for fine-tuning or by leveraging external databases for retrieval-augmented generation. Additionally, they may utilize online tools or other built-in functionalities (e.g., calculators), effectively functioning as holistic agents.\nMalicious Goal. In this study, we focus on common jailbreak questions (e.g., how to make a bomb) that aim to elicit harmful outputs, including hateful, harassing, or violent"}, {"title": "D. Language Agent.", "content": "content, which violates the usage policies of mainstream LLM applications, as detailed in Appendix C.\nAdversary Scenarios. Our study focuses on red teaming target LLMs by automatically crafting jailbreak prompts to identify their vulnerabilities in black-box scenarios, which are implemented with default security mechanisms. Typically, given the malicious goal $g$ as the goal of the jailbreak (e.g., generate violent content in a video script), a red teaming method first requires human effort to craft specific jailbreak templates $x_0$, such as the early well-known 'DAN' [2]. Then, the red teaming method leverages the natural language processing capabilities of LLMs to mutate the human-crafted templates $x_0$ (e.g., syntax transformation and synonym replacement) to obtain jailbreak prompts $x_1$ to query the target LLM. The response $y_1$ returned by the target model will be judged, if the response is harmful enough to fulfill the malicious goal $g$. If not, the red teaming method further iteratively refines the jailbreak prompts to $x_2$ based on the response $y_1$ to enhance their effectiveness. This process is repeated, with $x_k$ and the response $y_k$ being input into the target model, iteratively refine $x_{k+1}$ in the $k$iteration until it effectively exposes the vulnerabilities of the target model, i.e., fulfill the malicious goal $g$, as illustrated in Figure 2. In our study, we primarily focus on single-round attacks, as the attacker cannot manipulate the ongoing chat context with the target LLM.\nD. Language Agent.\nA language agent is a sophisticated software program designed to interact autonomously with its environment, capable of understanding and generating natural language to accomplish specific goals [3]. This concept was originally proposed by Allen Newell in 1959 [34], and has received increasing attention because LLM can integrate and perform complex tasks without the need for users to explicitly define rules. These LLM-based agents leverage the extensive language understanding and generation capabilities of LLM to perform complex tasks, interact in a human-like manner, and make decisions based on a variety of inputs. The key design of LLM-based agents revolves around four main modules: analysis, memory, planning, and action. The analysis module defines the agent's role, the memory module allows the agent to recall past actions, the planning module enables the agent to design future actions, and the action module translates these decisions into specific outputs.\nProfiling. The profiling module defines the agent's role and characteristics by incorporating demographic, personality, and social information. For example, Generative Agent [40] uses manually crafted profiles for agent roles and responsibilities, while RecAgent [53] generates profiles using LLMs.\nMemory. Memory enables agents to store, recall, and utilize past experiences to inform future actions. For instance, Reflexion [47] employs a sliding window for recent feedback, and Generative Agent [40] combines short-term and long-term memories with self-reflection mechanisms."}, {"title": "III. REDAGENT", "content": "Planning. Planning helps agents break down tasks and devise strategies for task completion. Methods include single-path reasoning (CoT [55]) and multi-path reasoning (ToT [57]). Additionally, some researchers explore planning with environmental feedback and human inputs [52], [58].\nAction. The action module execute the plan and interact with the environment, which often leverages internal knowledge and external tools. For example, Toolformer [45] integrates APIs to enhance functionality, and ChemCrow [7] utilizes external models for complex tasks.\nOur research aligns with the language agent paradigm to effectively simulate a practical attacker. In this context, our goal is to develop an LLM-based multi-agent system to dynamically enhance its understanding of jailbreak strategies in various scenarios (including different LLMs and contexts), thereby improving the effectiveness in jailbreaking target LLMs.\nIII. REDAGENT\nIn this section, we present our novel agent-based red team system, RedAgent, to achieve efficient context-aware jailbreak prompts generation. In Section III-A, we begin by illustrating our design intuition. In Section III-B, we provide an overview of our proposed system. Then we introduce the Skill Memory in Section III-C and the details of three stage within RedAgent in Section III-D, III-E, and III-F.\nA. Design Intuition\nFigure 1 illustrates the comparison of using general jailbreak strategy and context-aware strategy in jailbreaking real-world LLM application LLM Math [41] under the same malicious goal of making Heroin.\nContext-aware jailbreak prompts are of higher quality and play a key role in jailbreak specific LLM applications. As shown in Figure 1, we can observe that the custom LLM Math [41] designed for math-related tasks is effectively jailbroken under the context-aware jailbreak prompt, while refusing to respond under the general jailbreak prompt. This context-aware jailbreak prompt conceals the malicious goal within a task that the model is good at, thus increasing the effectiveness of this attack. Therefore, adjusting the prompts to more relevant contexts can significantly improve the effectiveness of jailbreaking specific LLM applications. This is because specific LLM applications are often fine-tuned or hinted with additional data, resulting in different unique vulnerabilities. Given the different weaknesses inherent in different LLMs (either foundation models or specific LLM applications), it is important to generate context-aware jailbreak prompts in red team approaches. Motivated by such observation, we propose to generate context-aware jailbreak prompts that capture the context information of different LLM models and applications to guide the generation of jailbreak prompts.\nLack of automation and scalability further limits existing red teaming methods from continually gaining longer-term insights to improve the efficiency. Existing red teaming methods are limited to using a few mutation operations (e.g., synonym replacement) to refine the human-written jailbreak"}, {"title": "C. Skill Memory", "content": "templates. Since the effectiveness of refined jailbreak prompts depends heavily on these templates, such limitations further restrict their efficiency. Since we have concluded that no jailbreak attack prompt can be effective to all LLMs in all scenarios, as they have inherent unique weaknesses in different scenarios, it is important to continuously improve adaptability. Therefore, it is important to equip red teaming methods with learning mechanisms to continuously adapt to different scenarios. To achieve this goal, we propose a simple but effective abstraction layer that incorporates existing jailbreak attacks into a unified verbal representation, called jailbreak strategy. This abstraction shows good scalability and automation due to its diversity in leveraging strategies to generate detailed prompts, as shown in Figure 3. By learning to exploit these strategies, we improve the efficiency of red teaming methods via continuing gain longer-term insights.\nB. Overview\nBased on our observations in Section III-A, we propose the agent-based red teaming system, RedAgent, to 1) generate context-aware jailbreak prompts by obtaining contextual information from model feedback; 2) continuously learn to exploit jailbreak strategy that self-reflected in an additional memory buffer. Figure 3 shows the architecture of RedAgent, which consists of three main stages.\nSpecifically, in the Context-aware Profiling stage, the Profile Constructor perceives the specific context of the target LLM to craft a context-aware malicious goal (e.g., guide the target LLM to output harmful code of a Trojan). The planner in the Adaptive Jailbreak Planning phase then reasons from skill memory to develop an attacking plan that guides the Attacking and Reflection phase until the last trial succeed. Guided by the attacking plan, the Attacking and Reflection Stage generates the jailbreak prompts and queries the target LLM, obtaining and evaluating its response. For each iteration, the evaluator of this stage gathers context feedback and performs self-reflection on the interaction to update the Skill Memory.\nC. Skill Memory\nBased on the observations described in Section III-A, our key design goal is to enable the red team to continuously exploit jailbreak strategies self-reflected on trials to adapt to varying scenarios, thereby improving the efficiency of red teaming. To achieve this, RedAgent incorporates a Skill Memory that stores the understanding of past experiences during red teaming to maintain this learning behavior.\nSpecifically, the Skill Memory consists of two parts: long-term memory and short-term memory, as decipted in Figure 3. For long-term memory, we collect common jailbreak strategies from the study [62] and further leverage LLM to summarize in-the-wild jailbreak prompts into these strategies or add new strategies. Additionally, the long-term memory stores past successful trials and their associated experiences, marked with tags such as the scenarios of the malicious goal, the used strategy, and the key parts of the successful jailbreak prompts. This tagging considers the relationship between effective strategies for different malicious goals, enhancing the understanding of past experiences when the planner reasons about similar malicious goals. Short-term memory stores detailed experiences from the last few iterations, providing rich contextual information about the interactions. This includes evaluation scores, context feedback, and how to improve effectiveness, obtained from the evaluator's self-reflection outputs.\nD. Context-aware Profiling\nSince the capabilities of different LLMs vary, it is crucial to craft suitable malicious goals to identify relevant vulnerabilities. The Context-aware Profiling Stage perceives and crafts context-aware malicious goals to ensure they lie on the safety boundary of the target LLM, especially in specific domains (e.g., code assistant).\nSpecifically, during the initial interaction with the target LLM, probing sentences such as \u201cHello, who are you?\u201d are used to gather initial information about the target LLM's functionality. This information is tagged as the basic setting of"}, {"title": "E. Adaptive Jailbreak Planning", "content": "the target and shared by all components of RedAgent, providing a foundational understanding of the LLM's purpose and functionality. Motivated by the goal to understand the target LLM scope and supported functions, the profile constructor continuously interacts until sufficient information is obtained. By determining the target LLM scope and supported functions of the target LLM (e.g., only text generation or connecting to external tools), the profile constructor then crafts a malicious goal relevant to the target LLM.\nE. Adaptive Jailbreak Planning\nTo improve the effectiveness of the chosen strategies of red team for the specific scenario, the planner in the Adaptive Jailbreak Planning stage leverages the crafted malicious goal to retrieve relevant memory entries. Then, it crafts an initial plan in the first iteration. This initial plan includes the reasoning process, the attack role with basic descriptions, and the strategy and its example for the attacker, for guiding the LLMs in the Attacking and Reflection stage.\nWe then extend the action space of refinement by allowing the planner to determine the next action to refine the current attack, enabling the agentic system to improve intelligence. For each refinement, the planner is engaged and directly reasons on the Skill Memory and the malicious goal to select the most effective strategies. Our agent's action space mirrors several predefined states:\n\u2022\tAlign the goal: If the attack begins to deviate from the original goal, evoke an instruction for the planner to reassess and possibly modify the goal and craft a new plan.\n\u2022\tRefine the attack strategy: If the current strategy (as outlined in the response) fails to elicit the desired response without achieving the malicious goal, evoke a corresponding instruction for the planner to consider refining the strategy.\n\u2022\tRetry the attack prompt: If the response fails to achieve the malicious goal due to randomness in the language model, and the adopted strategy succeeded multiple times in memory, continue using this prompt and retry it.\n\u2022\tRefine the attack prompt: Evoke a corresponding instruction to let attacker use the current strategy, but introduce more variety while maintaining the original intention by leveraging the understanding of strategy.\n\u2022\tEnd this goal: Conclude the interaction once the response has successfully met the objective and achieved a score above threshold.\nEach action plays a unique role in refining the attack prompts in a human-like style, thereby enhancing the intelligence of red teaming."}, {"title": "F. Attacking and Reflection", "content": "F. Attacking and Reflection\nIn the Attacking and Reflection Stage, we enhance the efficiency of red teaming by incorporating self-reflection mechanisms. This allows RedAgent to continuously refine the Skill Memory by updating its long-term part with summaries of past effective trials and its short-term part with context feedback, aiming to better guide the planner in autonomous refinement.\nSpecifically, the attacker first crafts jailbreak prompts according to the guidance in the attacking plan and queries the target LLM. Then, the evaluator assesses the jailbreak response of the target LLM to determine whether the response is jailbroken and provides an analysis in the evaluation results to guide further refinement.\nBased on the observations in Section III-A, LLMs have unique weaknesses in certain contexts and behave differently (e.g., at different levels of detail and in different rejection behaviors), we aim to mine these fine-grained differences and add them to the evaluation results to better guide the planner in refinement. In RedAgent, the evaluator explicitly analyzes the target LLM's intentions, confidence levels, and security mechanisms in each attack iteration. This thorough analysis provides a global view, helping to approach effective attack prompts more strategically. It prevents repeated attempts on the same errors by addressing the uncompressed and lengthy responses that may mislead the attacker, ensuring key parts are refined in subsequent steps.\nSuch incorporation of context feedback offers several advantages: Firstly, context information highlights the differences from general LLM outputs that lack domain-specific knowledge and format requirements. This context can indicate what the target LLM can and cannot answer, indirectly representing the security mechanisms of the target LLM. Leveraging this information helps identify specific vulnerabilities within the target LLM and avoid crafting out-of-scope attack prompts. Secondly, the response reveals the target LLM's intentions and certainty in answering a question, particularly in more advanced LLM applications with extensive data and complex reasoning processes. By incorporating context feedback in the evaluation results, RedAgent obtain sufficient information to guide the generation of context-aware jailbreak prompts.\nTo improve the refinement quality of updating skill memory to enhance the learning behavior of RedAgent, we exploit the self-reflection ability of LLM to abstract key parts and label long and complex contexts. By adding this abstraction layer, the evaluator marks the key features that contributed to success as a new entry of the Skill Memory, as shown in the Figure 3. This includes identifying effective strategies and summarizing the attack scenario in a compact form. Such updates contribute to the ever-changing understanding of jailbreaking strategies in specific contexts, which is enhanced by effective trials and specific patterns (e.g., some skills of it) added to the strategy description.\nImplementation for each components. In our experiments, we implement each component (e.g., role) of RedAgent with individual LLMs, which are used via API. We employ the LLM-empowered evaluator with in-context learning to implement the evaluator, measuring the effectiveness of the attack by predicting a score. Following the study [42], we use the mainstream OpenAI usage policy as the evaluation preference for jailbreak evaluation, rating responses on a scale from 1 to 5."}, {"title": "IV. EVALUATION", "content": "IV. EVALUATION\nTo evaluate the effectiveness of RedAgent, we conduct evaluations on both general LLM chatbots and specific LLM-centered applications. We will first present an overview of our evaluation and highlight our findings in Section IV-A. In Section IV-B, we will introduce the experimental setup. Finally, we will discuss our evaluation results through three research questions:\n\u2022\tRQ1: Do various LLMs have unique weaknesses that make them susceptible to specific attack strategies? (Section IV-C)\n\u2022\tRQ2: Can RedAgent efficiently achieve context-aware red teaming compared to existing works? (Section IV-D)\n\u2022\tRQ3: How does the Skill Memory contribute to the attack performance improvement in RedAgent? (Section IV-E)\nA. Evaluation Overview\nTo study RQ1, we analyze successful strategies against different LLMs, targeting 2 open-source and 4 closed-source models to reveal 290 novel jailbreak cases. By analyzing the"}, {"title": "C. Vulnerabilities of Various LLMs to Jailbreak Strate-gies (RQ1)", "content": "distribution of the employed strategies in varying scenarios, we found that each LLM and malicious goal has unique weaknesses. To study RQ2, we compare our system with existing works against the latest benchmarks. Our system achieves successful vulnerability discovery with two times fewer queries (fewer than 5 queries on average) while maintaining a jailbreak success rate above 90%. Additionally, to demonstrate our system's generalizability across different scenarios, we jailbreak 60 trending applications on GPT marketplaces and find severe vulnerabilities of them.\nB. Experimental Setup\nDatasets. To comprehensively identify vulnerabilities of general-use LLMs in various malicious goals, we follow the settings of previous work [10], [31], [60], [62] and collect 50 malicious goals from two open datasets [65], covering a wide range of policy-violating requests corresponding to crime, hacking, and discrimination scenarios. These malicious goals are either manually written by the authors or generated through crowd-sourcing, making them more reflective of real-world scenarios. Further, we follow the OpenAI usage policies and categorize these goals into 14 categories: Children Harm, Economic Harm, Financial Advice, Fraud, Government Decision, Hate Speech, Health Consultation, Illegal Activity, Legal Opinion, Malware, Physical Harm, Political Lobbying, Pornography, and Privacy Violation. For the malicious goals used in jailbreaking specific LLM applications, we leverage our system to generate corresponding malicious goals according to their application scopes, as described in Section III-D.\nTargeted LLM-centered applications. During our experiments, we mainly utilized the following LLMs:\n\u2022\tLLM for general use (i.e., foundation model): We test mainstream foundation models in both black-box and white-box settings. These include ChatGPT (GPT-3.5 and GPT-4) developed by OpenAI, specifically utilizing both the \u201cgpt-3.5-turbo-1106\u201d and \u201cgpt-4-1106-preview\u201d models, which are implemented with default system prompts for safety. Additionally, we test Gemini-pro, Claude-3-5-Sonnet-20240620 and two open-source LLMs: Vicuna-7b-v1.5 and LLaMA-2-7b-chat-hf, both using the officially released model weights with safety alignment.\n\u2022\tLLM-integrated application: To test our system on state-of-the-art practical LLM-integrated applications, we select 60 trending GPTs during the first two seasons of 2024. These GPTs encompass diverse tasks such as writing, productivity, research, coding, education, and lifestyle.\nMetrics To evaluate the effectiveness of our approach, we use the Attack Success Rate (ASR) as our primary metric. ASR measures the ratio of questions that receive a jailbreak response from generated jailbreak attack prompts to the total number of malicious goals, all conducted within a predefined budget. For a fair comparison with baseline methods, we set this budget at 60. We further utilize Average Number of Queries (ANQ) to measure the efficiency of the red teaming\nC. Vulnerabilities of Various LLMs to Jailbreak Strategies (RQ1)\nIn this subsection, we will demonstrate that various LLMs exhibit distinct vulnerabilities, which are well presented by the differences in effective jailbreak strategies for different models when the malicious goal is the same. We jailbreak 2 open-source and 4 closed-source LLMs using our proposed system and collect the effective jailbreak prompts along with the strategies employed. We then analyze the statistical distribution of the effective strategies of different tested LLMs to determine whether they exhibit different vulnerabilities, and further, we explore whether these effective jailbreak strategies are closely related to specific scenarios in 14 malicious goal categories by analyzing the statistical distribution of strategies across different scenarios. Furthermore, we study the context-specific nature of these vulnerabilities by comparing the responses of effective jailbreak strategies on specific applications and those of ChatGPT, all of which are built on LLM GPT-4-1106-preview.\nThe vulnerabilities of varying LLMs are dominant by some general effective strategies, while also exhibiting distinct vulnerabilities in other effective strategies. Figure 4 shows the top-5 most frequent strategies for each tested LLM in all malicious goals. We can observe that each model exhibits a dominant vulnerability strategy, with \u2018Misrepresentation' emerging as the most frequent across most LLMs, notably comprising 58.7% of effective strategies for GPT-4-1106-preview and 37.2% for Gemini-Pro. Overall, the top-5 effective strategies of each LLM account for more than 50% of all effective strategies, showing that a small number of strategies are responsible for the majority of successful jailbreaks across different models. Figure 5 shows the heatmap for the freqency of effective jailbreak strategies excluded the general strategy 'Misrepresentation' and 'Expert Endorsement' across various LLMs. We can observe that different LLMs exhibit distinct vulnerabilities to various effective strategies. For instance,"}, {"title": "D. Efficiency of RedAgent in Red Teaming LLMs (RQ2)", "content": "GPT-4-1106-preview shows a higher vulnerability to 'False Information' and 'Relative Ethical Appeals,' while Vicuna-7b-v1.5 is more susceptible to \u2018Exploitation of Base Values' and \u2018Encouragement.'\nThe same LLM are prone to be jailbroken by different strategies in different malicious goals. To analyze the effectiveness of diverse jailbreak strategies in different context, we plot the heatmap for the frequency of strategies of successful jailbreak prompts across various categories of malicious goals for different LLMs, as shown in Figure 6.(a) to 6.(c), representing the results for Vicuna-7b-v1.5, GPT-3.5-turbo-1106, and Gemini-Pro correspondingly. Each cell in the heatmap shows the percentage of successful jailbreak prompts in one category, with the color intensity indicating the frequency, where darker colors represent higher counts. From the heatmap, we observe several key patterns. Firstly, there is a notable diagonal dominance especially in Vicuna-7b-v1.5, indicating that each category of malicious goal is more frequently successful in specific jailbreak strategies compared to others, suggesting the vulnerabilities of jailbreaks are context-specific. Additionally, some off-diagonal cells with significant counts reveal that jailbreak prompts from one model can be effective on other models, highlighting cross-model vulnerabilities. The heatmap also reveals a sparse matrix, where many cells have zero or low counts, suggesting that not all combinations of jailbreak strategies and malicious goals are successful. Finally, we can observe that for different malicious goals, there remains common effective strategies in different malicious goals such as 'Misrepresentation'.\nD. Efficiency of RedAgent in Red Teaming LLMs (RQ2)\nIn this subsection, we will demonstrate the efficiency of our system in generating context-aware jailbreak prompts to find the most relevant vulnerabilities of the target LLM compared with the state-of-the-art model red team methods. We first compare the attack performance of RedAgent in"}, {"title": "RedAgent can support the most advanced jailbreak strate-gies and has good versatility and scalability.", "content": "RedAgent can support the most advanced jailbreak strate-gies and has good versatility and scalability. To verify the generality of our system in supporting state-of-the-art jailbreak strategies, we collected 45 different types of strategies and classified them into three categories based on their usability and modifiability:\n\u2022\tStatic Templates: These templates allow only minimal modifications that do not alter the main style or expression.\n\u2022\tSyntax-based Techniques: This category supports modifications through predefined operations, such as word-level character split [28] (e.g., \"how to rob a bank vault\" obfuscated to \"Ho to ro a nk vau lt\").\n\u2022\tPersuasive Techniques: These techniques, also known as deception techniques, require the attacker to understand the nuances of expression and allow for flexible modification based on different malicious goals. This category of techniques relies heavily on demonstration and policy understanding of imitation.\nConsidering the ability of a language agent to both understand semantic descriptions and effectively learn from demonstrations, we divide the collected strategies into three key parts: strategy types, strategy descriptions, and demonstrations. The strategy type is manually labeled as one of the three categories. This part helps tailor and constrain the modifications our system can apply to these strategies. The strategy description detail the semantic features and provide explanations on how to effectively utilize the policy. It helps our system better understand the nuances of each approach. Demonstrations are jailbreak prompts that are consistent with the corresponding strategy, which serves as a typical example for the system to imitate. The attack strategies we collected mainly come from two sources: technical reports and jailbreak templates in the wild. We use the following method to build our strategy list: For jailbreak strategies from technical reports, we use GPT-4 to preprocess text (e.g., pages and source files of papers) to summarize descriptions and extract demonstrations. For jailbreak templates in the wild, we first use GPT-4 to compare them with strategies collected from technical reports. If there is a match, we attach the demonstration to the corresponding strategy. If there is a mismatch, GPT-4 summarizes the information and"}, {"title": "RedAgent can effectively discover vulnerabilities of specific LLM applications by generating context-aware attack prompts.", "content": "GPTFuzzer will, in some cases, find common vulnerabilities early in the red teaming process to successfully jailbreak most malicious goals, especially for some models that have not fixed these popular jailbreak templates (such as Vicuna and Gemini-Pro). Although this approach improves efficiency, it comes at the expense of the diversity of generated jailbreak prompts, which rely heavily on human-crafted templates. When these templates are invalid, the jailbreak prompts may also become ineffective. For exmaple, GPTFuzzer failed to jailbreak Claude in any malicious goal (i.e. ASR = 0). In contrast, RedAgent does not rely on any human-made templates, but rather relies on jailbreak strategies abstracted by learning from past experiences and collected strategies, which are more hidden and difficult to fix.\nRedAgent can support the most advanced jailbreak strate-gies and has good versatility and scalability. To verify the generality of our system in supporting state-of-the-art jailbreak strategies, we collected 45 different types of strategies and classified them into three categories based on their usability and modifiability:\n\u2022\tStatic Templates: These templates allow only minimal modifications that do not alter the main style or expression.\n\u2022\tSyntax-based Techniques: This category supports modifications through predefined operations, such as word-level character split [28] (e.g., \"how to rob a bank vault\" obfuscated to \"Ho to ro a nk vau lt\").\n\u2022\tPersuasive Techniques: These techniques, also known as deception techniques, require the attacker to understand the nuances of expression and allow for flexible modification based on different malicious goals. This category of techniques relies heavily on demonstration and policy understanding of imitation.\nConsidering the ability of a language agent to both understand semantic descriptions and effectively learn from demonstrations, we divide the collected strategies into three key parts: strategy types, strategy descriptions, and demonstrations. The strategy type is manually labeled as one of the three categories. This part helps tailor and constrain the modifications our system can apply to these strategies. The strategy description detail the semantic features and provide explanations on how to effectively utilize the policy. It helps our system better understand the nuances of each approach. Demonstrations are jailbreak prompts that are consistent with the corresponding strategy, which serves as a typical example for the system to imitate. The attack strategies we collected mainly come from two sources: technical reports and jailbreak templates in the wild. We use the following method to build our strategy list: For jailbreak strategies from technical reports, we use GPT-4 to preprocess text (e.g., pages and source files of papers) to summarize descriptions and extract demonstrations. For jailbreak templates in the wild, we first use GPT-4 to compare them with strategies collected from technical reports. If there is a match, we attach the demonstration to the corresponding strategy. If there is a mismatch, GPT-4 summarizes the information and\nRedAgent can effectively discover vulnerabilities of specific LLM applications by generating context-aware attack prompts. We jailbreak 60 popular applications on the GPT"}, {"title": "E. Ablation Study (RQ3)", "content": "marketplace and identify 60 severe vulnerabilities. Figure 9 shows an example of harmful jailbreak response elicited by context-aware jailbreak prompts generated by RedAgent. Table II shows the overall performance of RedAgent in finding these vulnerabilities. The results show that RedAgent can effectively jailbreak specific applications in less than 2 queries on average. This efficiency is two times higher than jailbreaking GPT-4-1106-preview.\nE. Ablation Study (RQ3)\nRedAgent has demonstrated exceptional performance in jailbreaking general LLM chatbots and specific LLM-centered applications, particularly enhancements in effectiveness (ASR) and efficiency (ANQ). To further explore the origins of these capabilities and identify which design elements are critical, we"}, {"title": "V. DISCUSSION", "content": "have conducted ablation studies in this section to quantitatively assess the impact of these components on the final red teaming test performance.\nThe ablation studies organized in this section specifically focus on the demonstrated efficacy of the design aspects of the Skill Memory. We measured the impact of both the memory capacity of the Skill Memory module and the design of its memory entry tags on the final performance. To visually demonstrate{\n        \"title\": \"V. DISCUSSION\""}, {"content": "demonstrate the influence of these parameter designs on the outcomes, all tests were conducted on GPT-3.5-turbo-1106, with the attack success rates compared under different parameter settings to identify the critical parameter configurations.\nCapacity. To explore whether the number of memory entries within the long-term parts of the Skill Memory affects the efficiency and effectiveness of RedAgent, we present the results of jailbreaking GPT-3.5-turbo-1106 under different maximum numbers of memory entries (0, 10, 25, 50) with the same query budget, as shown in Table III.\nAdding memory entries improves RedAgent's efficiency, but too many can diminish effectiveness despite the efficiency gains. We can observe that as more memory entries"}, {"title": "A. Ethical Considerations", "content": "added, the efficiency of jailbreaking improves, two times better than that without memory (i.e., capacity = 0). Since useful attack experiences need to be frequently deleted from the repository, a too-small capacity limit restricts the planner from obtaining effective strategies, thereby limiting the performance. On the contrary, too large a capacity (e.g., 50) may introduce redundant information, which can exceed the model's ability to process long texts and lead to a decrease in the effectiveness of strategy crafting (e.g., decreased ASR of 72% when capacity = 50), although efficiency may improve in successful cases.\nA. Ethical Considerations\nIn this study, we adhered to ethical standards to ensure safety and privacy. Our experiments were conducted using platforms provided officially or through open-source models deployed in a closed environment. We did not disseminate any harmful or illicit content to the public or others. The datasets"}, {"title": "B. Limitations and Future Work", "content": "we employed were obtained from public repositories and did not contain any personal information. The main objective of this study is to highlight potential vulnerabilities in LLMs, especially given the rapid pace of their adoption. Moreover, we have responsibly disclosed our findings to OpenAI and Meta. In order to assist the industry in patching vulnerabilities, the experiments code will be made public after mitigation of known attack threats on the service.\nB. Limitations and Future Work\nLimitations Although RedAgent demonstrates impressive attack performance, we acknowledge several limitations of our method, which are discussed below. Firstly, the memory mechanism in RedAgent is not highly efficient. For instance, we simply inputs all memory entries as text to the planner, which may limit its scalability as the number of entries increases. And the current memory tags does not take into account the model-specific details, reducing the effectiveness of cross-model adaptability.\nBesides, our methodology is limited to text-based modality, which restricts its applicability as LLM applications increasingly integrate multimodal functionalities. Expanding RedAgent to handle other modalities, such as images, audio, and video, would broaden its usefulness and robustness in real-world settings.\nFurther, RedAgent cannot handle well in more complex scenarios such as the GPTs with diverse interaction types (e.g., requiring data uploads or specific inputs for task completion). Addressing this limitation requires enhancing RedAgent's ability to handle varied and intricate use cases effectively.\nFuture Work To tackle the above limitations, we propose several future directions to enhance our red teaming tool. Firstly, we could develop more sophisticated methods for planner input and retrieval. One approach could be integrating the Retrieval-Augmented Generation (RAG) system, which would enable the planner to dynamically retrieve relevant information from extensive documents, allowing for more efficient and effective memory entry handling.\nAdditionally, we could incorporate model-specific information into the memory tags to improve cross-model performance, such as model vulnerability. This could involve creating a simple database of known vulnerabilities for different models and tagging memory entries with relevant vulnerability information. By doing so, RedAgent can tailor its attack based on the specific weaknesses of the model in trials, leading to more targeted and effective red-teaming exercises.\nLastly, we could enhance the system's capability to support multimodal functionalities and manage complex interactions. This could involve integrating visual and auditory input processing capabilities, allowing the tool to handle a wider range of scenarios. We could explore these directions in the future."}, {"title": "VI. RELATED WORK", "content": "VI. RELATED WORK\nIn this section, we provide a comprehensive overview of jailbreak attacks, which can be broadly categorized into three"}]}