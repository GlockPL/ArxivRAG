{"title": "KD-LoRA: A Hybrid Approach to Efficient Fine-Tuning with LoRA and Knowledge Distillation", "authors": ["Rambod Azimi", "Rishav Rishav", "Marek Teichmann", "Samira Ebrahimi Kahou"], "abstract": "Large language models (LLMs) have demonstrated remarkable performance across various downstream tasks. However, the high computational and memory requirements of LLMs are a major bottleneck. To address this, parameter-efficient fine-tuning (PEFT) methods such as low-rank adaptation (LoRA) have been proposed to reduce computational costs while ensuring minimal loss in performance. Additionally, knowledge distillation (KD) has been a popular choice for obtaining compact student models from teacher models. In this work, we present KD-LoRA, a novel fine-tuning method that combines LoRA with KD. Our results demonstrate that KD-LORA achieves performance comparable to full fine-tuning (FFT) and LoRA while significantly reducing resource requirements. Specifically, KD-LoRA retains 98% of LoRA's performance on the GLUE benchmark, while being 40% more compact. Additionally, KD-LoRA reduces GPU memory usage by 30% compared to LORA, while decreasing inference time by 30% compared to both FFT and LoRA. We evaluate KD-LORA across three encoder-only models: BERT, RoBERTa, and DeBERTaV3. Code is available at https://github.com/rambodazimi/KD-LoRA.", "sections": [{"title": "Introduction", "content": "With advancements in transformer (Vaswani et al., 2017) architectures and hardware capabilities, including GPUs and distributed training, researchers have been able to develop LLMs with billions of parameters (Li et al., 2020; Narayanan et al., 2021; Dash et al., 2023), such as LLaMA 3.1 (Dubey et al., 2024) which boasts up to 405 billion parameters. These models, trained on trillions of tokens, exhibit remarkable capabilities across various downstream tasks (Brown et al., 2020; Zhuang et al., 2021; Wei et al., 2022). However, fine-tuning these models requires substantial energy and memory demands (Samsi et al., 2023). Furthermore, in recent years, the growth in the number of parameters in LLMs has significantly outpaced the advancements in available GPU memory (Lialin et al., 2023), amplifying the challenges of managing memory during fine-tuning (Singh et al., 2024; Kim et al., 2024; Dong et al., 2024).\nTo address these challenges, PEFT techniques (Houlsby et al., 2019) have emerged as effective solutions, which fine-tune a small subset of parameters while keeping the majority fixed. As shown in Figure 1, one popular PEFT technique, LoRA (Hu et al., 2022), and its variants (Zhang et al., 2023a; Zi et al., 2024; Ren et al., 2024; Zhao et al., 2024a; Liu et al., 2024) reduce the number of trainable parameters by introducing small, trainable rank decomposition matrices, maintaining performance as FFT across many tasks (Dettmers et al., 2023). For example, DoRA (Liu et al., 2024) enhances LoRA by decomposing pre-trained weights into magnitude and direction, applying LORA to directional updates for reduced trainable parameters. Similarly, AdaLoRA (Zhang et al., 2023a) improves LoRA by dynamically allocating parameters based on their importance, optimizing efficiency and performance, particularly under tight budget constraints."}, {"title": "Method", "content": "We propose KD-LoRA, a novel fine-tuning methodology that integrates LoRA with KD. The proposed method involves three main steps: (1) selecting and fine-tuning a teacher model, (2) initializing a smaller student model with LoRA modules, and (3) performing distillation to transfer knowledge from the teacher model to the student model.\nTeacher Model. Let T denote the teacher model, initialized from a pre-trained language model (e.g., BERT, RoBERTa, DeBERTa). The teacher model is fine-tuned on a specific task $D_{task}$, using FFT, where all parameters of the model are updated (Lv et al., 2024). The objective function for fine-tuning the teacher model is:\n$L_{task} = \\frac{1}{|D_{task}|} \\sum_{(x_i, y_i) \\in D_{task}} L_{CE}(T(x_i), y_i)$ (1)\nwhere $L_{CE}$ denotes the cross-entropy loss (CEL) loss, $x_i$ represents the input data, and $y_i$ denotes the corresponding label. This loss function measures the discrepancy between the predicted probabilities T(xi) and the true labels yi. The fine-tuned teacher model T then serves as the source of distilled knowledge.\nStudent Model with LoRA. The student model S is initialized from a smaller version within the same model family as the teacher model T. We modify the student model by injecting LoRA modules into its architecture. Specifically, LoRA is applied to the attention layers, where the weight matrices $W_q$ and $W_v$ (corresponding to the query and value projections) are decomposed as follows:\n$W_q = W_q^{base} + A_qB_q, W_v = W_v^{base} + A_vB_v$ (2)\nwhere $W_q^{base}$ and $W_v^{base}$ are the pre-trained weight matrices, while $A_q$, $B_q$, $A_v$, and $B_v$ are the low-rank matrices, the only parameters updated during fine-tuning.\nKD-LORA. With LoRA modules already in place, the KD process is performed, where the student model S learns from the teacher model. During this phase, the student model, equipped with LoRA, adapts its low-rank matrices to capture the knowledge transferred from the teacher. The student model is trained on the target task $D_{task}$ using the combined loss function $L_{total}$, which is given by:\n$L_{total} = \\alpha L_{task} + (1 - \\alpha) L_{KD}(z_S, z_T)$ (3)\nwhere $z_T$ and $z_S$ are the logits (outputs before the softmax layer) of the teacher and student models, respectively. The KD loss $L_{KD}$ is computed as the Kullback-Leibler divergence (KL) (Shlens, 2014) between the softened output probabilities of the teacher model T and the student model S. The parameter \u03b1 balances the task-specific loss $L_{task}$ and the KD loss $L_{KD}$. During each training step, the student model's low-rank matrices are updated to minimize the loss in Eq. 3."}, {"title": "Experiments", "content": ""}, {"title": "Conclusion", "content": "We present KD-LoRA, a novel fine-tuning method that integrates LoRA modules into a student model while leveraging KD from a larger teacher model. Empirical results on the GLUE benchmark show that KD-LORA retains approximately 97% of FFT performance and 98% of LoRA performance, all while reducing model size by around 40%. KD-LoRA also lowers trainable parameters by 99% compared to FFT and 49% compared to LoRA, reduces GPU memory usage by 75% compared to FFT and 30% compared to LoRA, and cuts inference time by 30%."}]}