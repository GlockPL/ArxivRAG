{"title": "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models", "authors": ["Fei Liu", "Zejun Kang", "Xing Han"], "abstract": "With the growing demand for offline PDF chatbots in automotive industrial production environments, optimizing the deployment of large language models (LLMs) in local, low-performance settings has become increasingly important. This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques for processing complex automotive industry documents using locally deployed Ollama models.\nBased on the Langchain framework, we propose a multi-dimensional optimization approach for Ollama's local RAG implementation. Our method addresses key challenges in automotive document processing, including multi-column layouts and technical specifications. We introduce improvements in PDF processing, retrieval mechanisms, and context compression, tailored to the unique characteristics of automotive industry documents. Additionally, we design custom classes supporting embedding pipelines and an agent supporting self-RAG based on LangGraph best practices.\nTo evaluate our approach, we constructed a proprietary dataset comprising typical automotive industry documents, including technical reports and corporate regulations. We compared our optimized RAG model and self-RAG agent against a naive RAG baseline across three datasets: our automotive industry dataset, QReCC, and CoQA. Results demonstrate significant improvements in context precision, context recall, answer relevancy, and faithfulness, with particularly notable performance on the automotive industry dataset.\nOur optimization scheme provides an effective solution for deploying local RAG systems in the automotive sector, addressing the specific needs of PDF chatbots in industrial production environments. This research has important implications for advancing information processing and intelligent production in the automotive industry.", "sections": [{"title": "1 INTRODUCTION", "content": "The automotive industry is undergoing a significant digital transformation, with an increasing reliance on complex technical documentation for various processes [1]. This shift encompasses design, manufacturing, and quality control, all of which now heavily depend on efficient information management systems [2]. The growing volume of technical documents, often in PDF format, has created a pressing need for advanced information retrieval and question-answering capabilities in industrial settings [3].\nLarge Language Models (LLMs) have emerged as powerful tools in natural language processing, demonstrating remarkable abilities in tasks such as document understanding and question answering [4]. These models have shown potential in handling the complex, domain-specific language often found in automotive documentation. However, the application of LLMs in industrial environments presents unique challenges, particularly in terms of computational resources and data privacy [5].\nAmong the various techniques developed to enhance LLM performance, Retrieval-Augmented Generation (RAG) has gained significant attention [6]. RAG combines the generative capabilities of LLMs with external knowledge retrieval, allowing for more accurate and contextually relevant responses. This approach, initially proposed by Lewis et al., has shown superior performance in generating specific, diverse, and factual language compared to traditional models [7].\nThe implementation of RAG techniques in the automotive industry, however, faces several industry-specific challenges:\n1. Document Complexity: Automotive technical documents often feature intricate layouts, including multi- column formats and complex tables. These structural elements pose significant challenges for standard document processing methods [8].\n2. Data Privacy: The automotive industry deals with highly confidential information related to proprietary designs and manufacturing processes. This necessitates solutions that can operate securely within the company's infrastructure, without relying on external cloud services [9].\n3. Resource Constraints: Many industrial environments operate with limited computational resources. This constraint requires the development of optimized, lightweight models capable of running efficiently on standard hardware [10].\n4. Domain Specificity: The automotive sector employs a vast array of specialized terminology and concepts. Generic language models often lack the specific knowledge required to accurately interpret and respond to queries about automotive processes and specifications [11].\n5. Real-time Performance: In fast-paced manufacturing environments, the ability to quickly retrieve and process relevant information is crucial. This necessitates high-performance information retrieval systems capable of operating under time constraints [12].\nThe open-source large language model service framework Ollama has gained attention for its ability to rapidly deploy LLMs in low-performance environments [13]. This framework offers potential solutions to some of the resource constraints faced in industrial settings. However, its application in the context of RAG for automotive documentation processing remains an area ripe for exploration and optimization.\nAs the automotive industry continues to evolve, particularly with the advent of electric and autonomous vehicles, the complexity and volume of technical documentation are expected to increase further [14]. This evolution underscores the importance of developing robust, efficient, and secure information retrieval systems tailored to the specific needs of the automotive sector [15].\nThe intersection of these technological advancements and industry-specific challenges presents a unique opportunity for research [16]. By addressing the particular needs of the automotive industry in the context of RAG and local LLM deployment, there is potential to significantly enhance information access and utilization in automotive engineering and manufacturing processes [17]."}, {"title": "1.2 Research Status and Gaps", "content": "Recent advancements in RAG techniques have shown promise in various domains. Jiang et al. proposed FLARE, which uses predicted next-sentence content to proactively retrieve relevant information [18]. Wang et al. introduced FILCO, a method for identifying and filtering useful contexts to improve generation quality [19]. These approaches demonstrate the potential for more context-aware retrieval in complex document environments, such as those found in automotive engineering.\nThe concept of self-reflective RAG, as explored by Asai et al [20], introduces a novel framework designed to enhance the quality and factual accuracy of LLMs through on-demand retrieval and a self-reflection mechanism. This approach could be especially valuable in the automotive context, where precision and accuracy in technical information are paramount.\nIn the domain of optimizing RAG for specific industries, Rajpathak et al [21], proposed a domain-adaptive retrieval method that is particularly relevant for the automotive sector's unique terminology and document structures. Similarly, the work of Siriwardhana et al [22], on improving retrieval efficiency in large-scale industrial datasets offers insights that could be applied to the vast repositories of technical documentation in automotive manufacturing.\nThe open-source large language model service framework Ollama has gained attention for its ability to rapidly deploy LLMs in low-performance environments [23], Burgan et al. developed RamChat, an Al chatbot aimed at improving accessibility [24], These developments in local LLM deployment are particularly relevant to the automotive industry's need for on-premises, resource-efficient solutions.\nRecent work by Wang et al [25], on on-device language models for function calling of software APIs presents potential applications in integrating RAG systems with existing software infrastructure in automotive production environments. This could lead to more seamless integration of Al-powered information retrieval within established industrial processes.\nThe challenge of processing complex PDF documents, a common format for technical specifications in the automotive industry, has been addressed by several researchers. Lin et al [26], proposed an advanced PDF parsing technique that could be adapted to handle the multi-column layouts and intricate tables often found in automotive documentation. Furthermore, the work of Bensch et al [27], on information extraction from semi-structured documents offers promising approaches for dealing with the varied formats of automotive technical literature.\nIn the realm of domain-specific language understanding, the research of Faysse et al [28], on fine-tuning language models for specialized industries provides valuable insights that could be applied to tailoring RAG systems for automotive terminology and concepts. This is complemented by the work of Kumar et al [29], on entity recognition in technical documents, which could enhance the precision of information retrieval in automotive contexts.\nThe integration of RAG with other Al methodologies has also shown promise. For instance, the combination of RAG with reinforcement learning, as explored by Belhadj et al [30], could lead to more adaptive and context-aware retrieval systems capable of handling the diverse query types encountered in automotive engineering and production.\nPrivacy and security concerns, which are paramount in the automotive industry, have been addressed in the context of RAG by researchers such as Zeng et al [31], who proposed privacy-preserving retrieval methods that could be crucial for protecting proprietary automotive designs and processes.\nThe challenge of maintaining coherence in long-form text generation, often necessary when addressing complex automotive queries, has been tackled by researchers like Borgeaud et al [32], whose work on improving long-range dependencies in language models could enhance the quality of responses in automotive RAG applications.\nRecent advancements in few-shot learning, as demonstrated by Izacard et al [33], with GPT-3, offer potential for rapidly adapting RAG systems to new automotive subdomains or emerging technologies without extensive retraining. This could be particularly valuable in the fast-evolving landscape of automotive technology.\nThe application of RAG in multilingual settings, as explored by Ahmad et al [34], is especially relevant for global automotive companies dealing with documentation in multiple languages. Their work on cross-lingual retrieval and generation could facilitate more efficient knowledge sharing across international teams.\nIn the domain of optimizing retrieval mechanisms, the research of Su et al [35], on dense retrieval methods offers potential improvements in the speed and accuracy of information lookup, crucial for real-time query resolution in fast-paced automotive production environments.\nThe challenge of handling numerical data and calculations, often present in automotive specifications and performance metrics, has been addressed by researchers like Noorbakhsh et al. [36], whose work on integrating symbolic mathematics with neural language models could enhance the precision of RAG systems when dealing with quantitative automotive data.\nHowever, despite these advancements, there remains a significant gap in research specifically addressing the unique challenges of implementing RAG systems in the automotive industry, particularly in resource-constrained, offline environments.\nThe significance of this research lies in:\n1. Providing an effective optimization scheme for local RAG deployment of Ollama in automotive industrial environments, addressing key challenges in document processing and information retrieval.\n2. Exploring the application of self-RAG in offline, industry-specific scenarios, offering new insights into function calling implementations for domain-specific tasks.\n3. Contributing to the advancement of intelligent information processing in automotive manufacturing, potentially improving efficiency and accuracy in technical document analysis and query resolution."}, {"title": "1.3 Research Objectives and Significance", "content": "Given the identified challenges and research gaps, this study aims to develop a multi-dimensional optimization scheme for applying RAG technology with Ollama in local, low-performance automotive industry environments. Our specific research objectives include:\n\u2022 Proposing a PDF file processing method optimized for automotive industry documents, capable of handling multi-column layouts and complex tables.\n\u2022 Developing an advanced RAG system based on the Langchain framework, introducing reranking models and BM25 retrievers to build an efficient context compression pipeline.\n\u2022 Designing an intelligent agent that supports self-RAG and exploring a function calling mechanism to enhance Ollama's response generation in automotive-specific scenarios.\n\u2022 Evaluating the proposed system using a proprietary dataset of automotive industry documents, alongside public datasets, to demonstrate its effectiveness in real-world industrial applications.\nThe significance of this research lies in:\n\u2022 Providing an effective optimization scheme for local RAG deployment of Ollama in automotive industrial environments, addressing key challenges in document processing and information retrieval.\n\u2022 Exploring the application of self-RAG in offline, industry-specific scenarios, offering new insights into function calling implementations for domain-specific tasks.\n\u2022 Contributing to the advancement of intelligent information processing in automotive manufacturing, potentially improving efficiency and accuracy in technical document analysis and query resolution.\nThis research builds upon and extends existing work in several key areas:\n\u2022 The potential of RAG systems to support decision-making processes, a critical application in automotive design and manufacturing, has been explored by researchers such as Gamage et al [37]. Their work on using RAG for few evidences-based reasoning could be adapted to support complex decision-making scenarios in automotive engineering.\n\u2022 Recent developments in efficient transformer architectures, such as the work of Zhuang et al [38], on Reformer, offer potential for deploying more powerful RAG models within the computational constraints of industrial environments.\n\u2022 The integration of visual information with text-based retrieval, as explored by Chen et al [39], presents opportunities for enhancing RAG systems to handle technical diagrams and schematics common in automotive documentation.\nBy addressing these research objectives and building upon recent advancements in the field, this study aims to significantly enhance the applicability and effectiveness of RAG technologies in the automotive industry, potentially transforming how technical information is accessed, processed, and utilized in automotive engineering and manufacturing processes."}, {"title": "2 MATERIALS AND METHODS", "content": "Our research builds upon the Langchain framework and Ollama model, adapting them to meet the specific needs of the automotive industry. We began by constructing a preliminary retrieval-based chatbot framework using Langchain's components, which we then optimized for processing automotive technical documents.\nThis architecture integrates advanced document loading capabilities for various file formats, efficient text splitting, and a robust retrieval mechanism using the Chroma vector store. The basic framework includes:\n\u2022 Document Loading: Utilizing Langchain's Loader class to recursively load PDF documents from specified directories, simulating the document structure in automotive manufacturing environments.\n\u2022 Text Chunking: Implementing an embedded tokenization model to split documents into fixed-length text chunks, optimized for technical specifications and multi-column layouts common in automotive documentation.\n\u2022 Vector Storage: Encoding text chunks into semantic vectors and storing them in the Chroma vector database, creating an indexed text retriever tailored for automotive terminology and concepts.\n\u2022 Dialogue Generation: Employing Langchain's ConversationalRetrievalChain to process user queries and retrieved context, generating responses using the locally deployed Ollama model.\nThis foundational setup serves as the baseline for our subsequent optimizations, each designed to address specific challenges in automotive document processing and information retrieval."}, {"title": "2.2 PDF File Processing Optimization", "content": "To address the unique challenges posed by automotive industry documents, we developed an enhanced PDF processing method combining PDFMiner and Tabula libraries."}, {"title": "2.2.1 Overview of PDFMiner and Tabula", "content": "PDFMiner is a Python library used for extracting information from PDF documents. It is capable of extracting text, images, metadata, and structural information from PDF documents. PDFMiner provides both high-level and low- level APIs to satisfy different levels of requirements. Automotive technical documents often feature multi-column layouts and complex diagrams. We implemented a custom algorithm using PDFMiner's extract_pages function to accurately extract content while preserving the logical flow of information:\n1. Page Segmentation: We analyze each page to identify distinct content regions, including text columns, diagrams, and tables.\n2. Content Ordering: Implementing a left-to-right, top-to-bottom reading order algorithm to ensure proper sequencing of extracted information.\n3. Diagram Extraction: Utilizing PDFMiner's image extraction capabilities to preserve technical diagrams crucial for understanding automotive specifications.\nTabula is a Java library used for extracting tabular data from PDF files. It provides a Python wrapper, making it more convenient to use Tabula in Python. Tabula can automatically detect table boundaries and convert tabular data into DataFrame objects, facilitating subsequent data analysis and processing."}, {"title": "2.2.2 Multi-column Layout and Table Information Recognition Optimization", "content": "Tables in automotive documents often contain critical data such as part specifications, test results, or compliance information. Our approach uses Tabula's read_pdf function with custom parameters like Algorithm 1:\n1. Table Detection: Implementing heuristics to identify table structures within automotive documents, considering common formats used in the industry.\n2. Data Extraction: Converting recognized tables into structured DataFrame objects, preserving relationships between data points.\n3. Contextual Integration: Seamlessly integrating extracted table data with surrounding text to maintain document coherence.\nWhen integrating text and table information, we first write the text information extracted by the multi-column layout recognition algorithm into the Markdown file in the order of appearance. Then, after the text information of each page, we convert the table information recognized on that page into Markdown format and write it into the file. By doing so, we can ensure that the content order in the generated Markdown file is consistent with the original PDF file, and the table information can be correctly embedded in the corresponding positions.\nTo illustrate the effectiveness of this approach, let's examine two figures that demonstrate the conversion process for a complex academic paper layout."}, {"title": "2.3 Optimization of Advanced RAG Based on Langchain", "content": "To enhance the RAG system's performance for automotive industry applications, we introduced several optimizations to the Langchain-based implementation. We introduce the BGE reranker model and BM25 algorithm. Building upon the groundwork laid in previous sections, we first combine the BM25Retriever with the default retriever using EnsembleRetriever in Langchain, assigning different weights to achieve more comprehensive and accurate retrieval. Then, a custom class is designed to integrate the reranking model into the context compression pipeline, further enhancing the retrieval and generation quality of the model."}, {"title": "2.3.1 Overview of BGE Reranker Model and BM25 Retriever", "content": "A reranker model is a general semantic vector model used to optimize the ranking of retrieval results. It can adapt to prioritize automotive-relevant information in retrieved contexts. In this study, we employ BAAI/bge-reranker- large as the reranker model. BGE (BAAI General Embedding) is a reranker model proposed by the Beijing Academy of Artificial Intelligence (BAAI), specifically optimized for Chinese queries [40]."}, {"title": "2.3.2 Building the Context Compression Pipeline and Custom Class Design", "content": "To optimize the context information processing of the RAG model, we construct a context compression pipeline DocumentCompressorPipeline that is pecifically tailored for automotive technical content. In addition to the collection retrievers mentioned earlier, we also introduce:\n\u2022 EmbeddingsRedundantFilter: An embedding-based redundancy filter to remove redundant information from the retrieval results.\n\u2022 LongContextReorder: Optimizes the order of context information, prioritizing key automotive specifications and procedures.\n\u2022 BgeRerank: A custom class that inherits from BaseDocumentCompressor. Since Langchain's official support for the BGE model is relatively limited, the purpose of designing this class is to seamlessly integrate the BGE reranker model into the pipeline. This custom class can improve relevance scoring for automotive queries."}, {"title": "2.4 Optimization of Advanced RAG Based on Langchain", "content": "In the previous section, we completed the basic design of the RAG system. However, integrating LLMs into practical applications and constructing end-to-end intelligent systems still present numerous challenges. To address the complex, multi-step problem of querying PDF profiles, which is common in automotive engineering and manufacturing processes, we have developed an advanced Self-RAG agent based on the LangGraph framework."}, {"title": "2.4.1 Overview of SELF-RAG", "content": "Self-Reflective Retrieval-Augmented Generation (SELF-RAG) is a novel framework designed to enhance the quality and factual accuracy of LLMs through on-demand retrieval and a self-reflection mechanism [42].\nUnlike traditional RAG methods, SELF-RAG endows LLMs with the following capabilities:\n1. On-demand Retrieval: The LLMs autonomously determines whether to retrieve relevant information from an external knowledge base based on the input it receives.\n2. Self-Reflection: The LLMs evaluates and reflects upon both the retrieved information and its own generated content, thereby improving the quality and reliability of its output.\nThe training process of SELF-RAG consists of two stages:\n1. Offline Critic Model Training: An independent critic model is trained to generate \"reflection tokens\". These tokens are inserted into the LLMs' output to guide its self-reflection process.\n2. Generative Model Training: The LLMs is fine-tuned using a corpus that includes reflection tokens and retrieved documents. This enables the LLMs to understand and utilize these tokens, incorporating self- reflection into its generation process.\nDuring inference, the LLMs dynamically decides whether to retrieve information based on the requirements of the task at hand. It also leverages the retrieved information and the self-reflection mechanism to generate high- quality output. For instance, in tasks demanding factual accuracy, the LLMs is more inclined to retrieve and utilize relevant information, while in more open-ended tasks, it may prioritize creativity and rely less on retrieval."}, {"title": "2.4.2 Agent Design Supporting Self-RAG", "content": "Considering this, based on Langchain, we designed an intelligent Agent class called AgenticRAG, which aims to utilize Self-RAG technology to answer user questions. AgenticRAG combines various components from the LangGraph and LangChain ecosystems to achieve a modular and scalable question-answering system.\nThe core of the AgenticRAG class is the create_graph method, which defines a workflow based on a directed acyclic graph (DAG). The workflow consists of multiple nodes, each responsible for a specific task in the question- answering process. The nodes are connected by directed edges, forming a complete question-answering flow. The main components in the AgenticRAG class are as follows:\n\u2022 GraphState: Represents the current state of the graph, including the user's question, retrieved documents, generated answer, and chat history.\n\u2022 Node: Represents a step or task in the question-answering process, accepts the current state as input, performs specific operations, and returns the updated state.\nWe designed the AgenticRAG class to handle sophisticated automotive-related questions. Key components as Figure 4:\n1. Retrieve Node: Optimized to fetch relevant information from automotive technical documents, considering industry-specific terminology and concepts.\n2. Grade Documents Node: Evaluates retrieved documents based on their relevance to automotive queries, considering factors like technical accuracy and applicability to specific manufacturing processes.\n3. Generate Node: Produces responses tailored to automotive industry needs, incorporating technical specifications and industry standards searched from documents.\n4. Transform Query Node: Refines queries to better capture the intent behind automotive-specific questions, improving retrieval accuracy in subsequent iterations."}, {"title": "2.4.3 Function Calling Design for Optimizing Ollama Output in RAG Scenarios", "content": "Function calling is a powerful technique that significantly enhances the output quality of LLMs in RAG scenarios [43]. By integrating external functions with LLMs, specialized logic and algorithms can be employed to guide the model, resulting in more accurate, coherent, and informative responses. This approach leverages the few-shot learning capabilities of LLMs, enabling them to adapt to new tasks while mitigating their inherent limitations in reasoning and computation.\nHowever, most of the relevant work has focused on standard transformer-based LLMs, with a lack of implementation for optimizing Ollama callbacks, especially in RAG scenarios. Although Langchain officially provides ollamafunction.bind to call functions, further research on this method is currently scarce. Therefore, to enhance the agent's capability in handling automotive-specific tasks, we designed a custom class adapted to ollamafunction.bind, called ChatFunction. It inherits from the BaseFunction class and overrides the_init_ and _call_ methods. The main features of ChatFunction are as Algorithm 3:\nThe initialization accepts three parameters: the Al assistant's personality description personality, language preference language, and retry count retry_count.\nTwo properties are defined: query_input and output, representing the complete input (including personality description, language instructions, and conversation history) and the Al assistant's response, respectively. This ensures that the model is provided with the necessary background knowledge for each retry, enabling it to generate responses based on the specified role characteristics and language style.\nThe constructor of the parent class BaseFunction is called, passing the function name, description, and parameter list.\nWhen invoked, it accepts an arguments dictionary containing two required parameters: output and query_input.\nAccording to the retry count retry_count in the self-rag Agent in 3.3.2, the level of detail in the answer detail_prompt is dynamically adjusted to modify prompts based on the complexity of automotive queries and the depth of technical detail required.\nThe query_input for the next round is constructed by encoding the personality description, language preference, detail level requirement, and current conversation history. To maintain awareness of the ongoing conversation context, crucial for addressing multi-step automotive processes or complex diagnostic queries.\nA dictionary is returned, containing the tool field (specifying the callback function name) and the tool_input field (containing the optimized response and updated query_input), allowing the next round of callback requests to continue optimizing based on the current results and state, forming a closed-loop self-improvement process."}, {"title": "3 RESULTS", "content": "RAGAS (Retrieval Augmented Generation Assessment Suite) is a comprehensive evaluation framework for assessing the performance of RAG models. It provides a series of metrics to quantify the quality of generated results, with a particular focus on the impact of information retrieval on the generation process. The evaluation metrics in RAGAS include:\n\u2022 Context Precision: Measures the precision of relevant contextual information contained in the generated results.\n\u2022 Faithfulness: Evaluates the faithfulness of the generated results to the original contextual information, i.e., the consistency between the generated content and the original information.\n\u2022 Answer Relevancy: Assesses the relevance of the generated answers to the questions, i.e., whether the answers are on-topic and meet the requirements of the questions.\n\u2022 Context Recall: Measures the coverage of relevant contextual information in the generated results, i.e., how much key information is captured.\nWe adopt RAGAS as the evaluation framework to compare and analyze the performance of different RAG models. The quantitative metrics provided by RAGAS enable us to objectively evaluate the strengths and weaknesses of the models in terms of contextual information utilization and generated content quality, providing important references for model selection and optimization."}, {"title": "3.2 Experimental Results and Analysis", "content": "In this paper, we select two public datasets, QReCC [44], and CoQA [45], and introduce a self-constructed dataset as the experimental data sources. The QReCC dataset contains 10,000 questions, each corresponding to a background knowledge text, with questions presented in the form of multi-turn dialogues. The CoQA dataset contains over 8,000 multi-domain dialogues, each based on a given text and consisting of multiple question-answer turns.\nThe self-constructed dataset is a automotive industry proprietary dataset compiled from internal documents of a leading automotive manufacturer. This dataset includes:\n\u2022 Technical specifications and design documents\n\u2022 Manufacturing process guidelines\n\u2022 Quality control procedures\n\u2022 Corporate regulations and standards\nDue to the confidential nature of these documents, we cannot provide detailed statistics or examples. However, this dataset represents the core focus of our study, reflecting real-world challenges in automotive document processing.\nWhile the self-constructed dataset serves as our primary testbed, we included QReCC and CoQA for several crucial reasons:\n1. Structural Similarity: The question-answer pairs in QReCC and CoQA share a similar format with our custom- designed test cases for the self-constructed dataset. This structural consistency allows for a fair comparison of our model's performance across different domains.\n2. Conversational Nature: Both QReCC and CoQA feature multi-turn dialogues, mirroring the complex, context- dependent queries often encountered in automotive engineering and manufacturing processes.\n3. Diverse Domain Coverage: These datasets cover a wide range of topics, helping us evaluate our model's generalization capabilities beyond the automotive domain.\n4. Benchmark Comparability: As widely used public datasets, QReCC and CoQA enable us to benchmark our system against other state-of-the-art models in a reproducible manner.\n5. Confidentiality Compliance: By using these public datasets alongside our proprietary data, we can openly discuss and compare results without compromising sensitive corporate information.\n6. Robustness Testing: The inclusion of these datasets helps demonstrate that our optimizations, while tailored for automotive applications, do not compromise performance on general conversational tasks.\nConsidering that our research aims at the requirements of PDF dialogue chatbots and uses the BGE reordering model, we first translate the knowledge or link information in the QReCC and CoQA datasets into Chinese and convert it to PDF format. This step ensures the consistency of experimental data with complex PDF documents in real application scenarios and facilitates the evaluation of the performance of different RAG models and their optimization schemes in practice. The self-constructed dataset does not require additional processing as it is already in PDF format.\nAfter data preparation, we design or directly utilize a number of question-answer pairs with contextual memory content from these datasets as test cases. Here, we take a set of question-answer pairs from the QReCC dataset as an example, given a background text:\n\"John is a boy who likes to play outside. After school, he always goes to the park to meet his friends.\"\nThe following is a set of multi-turn question-answering based on this background text:\n\"Q: What did John do after school?\nA: John went to the park after school.\nQ: Who did he meet at the park?\nA: He met his friends at the park.\"\nWe designed our experiments to evaluate the performance of four systems:\n\u2022 Naive RAG (Baseline)\n\u2022 Advanced RAG (Our Optimized Model)\n\u2022 Self-RAG Agent (Baseline)\n\u2022 Self-RAG Agent (Our Proposed Agent with Custom Function Calling)\nFor each dataset, we created test sets comprising:\n\u2022 500 question-answer pairs from the self-constructed dataset\n\u2022 500 pairs from QReCC\n\u2022 500 pairs from CoQA\nThese test sets were carefully curated to ensure a balance of simple queries, multi-turn conversations, and complex technical questions, mirroring real-world usage scenarios in the automotive industry and beyond."}, {"title": "3.2.2 Optimization Effects of Langchain-based RAG", "content": "According to the experimental results show that the proposed Langchain-based RAG optimization approach achieves certain improvements on some metrics compared to the naive RAG model.\nBy introducing the BGE reordering model and BM25 retriever, and constructing a context compression pipeline, the optimized RAG model improves context precision by , , and on the QReCC, CoQA, and self- constructed dataset, respectively. The context recall also increases by on the QReCC and self- constructed dataset, but decreases on the CoQA dataset. These results indicate that while the optimized model shows some improvements in capturing question-relevant background knowledge, it is not always the case.\nHowever, the optimized model exhibits 3.7%, 7.4%, and 7.6% relative improvements in answer relevancy, as well as , and boosts in faithfulness on QReCC, CoQA, and self-constructed dataset, respectively. These results underscore the effectiveness of our optimizations in handling complex automotive documentation and queries. The Advanced RAG model also showed improvements, but the Self-RAG Agent's performance was notably superior, particularly in dealing with multi-step technical queries common in automotive applications."}, {"title": "3.2.3 Optimization Effects of Langgraph-based Self-RAG Agent", "content": "According to the evaluation results show that the self-RAG agent obtains more significantly improvements over the naive RAG model on most metrics across the three datasets. Specifically, it surpasses the naive RAG by in answer relevancy, in faithfulness, in context precision, and in context recall on QReCC, CoQA, and self-constructed dataset, respectively. The superior performance on most metrics demonstrates the effectiveness of the proposed self-asking and self-verification mechanism in enabling more targeted and reliable context retrieval and answer inference.\nFurthermore, by introducing the custom function calling mechanism that optimizes the Ollama output, the self- RAG agent obtains additional gains in answer relevancy, increases in faithfulness, improvements in context precision, as well as boosts in context recall on the three datasets compared to the naive RAG model. These results validate that dynamically adjusting question and context inputs to the Ollama model based on conversation states can effectively guide it to generate more accurate, informative, and coherent responses that better satisfy user needs.\nThis performance demonstrates that our optimizations enhance general conversational Al capabilities while excelling in domain-specific tasks. The consistent improvement across datasets suggests that our approach successfully balances domain-specific optimization with general language understanding."}, {"title": "4 DISCUSSION", "content": "Our innovative approach to handling complex automotive queries is exemplified in Figure 7, which depicts the Self- RAG process flow with custom function calling, applied to an Anti-lock Braking System (ABS) query. This diagram showcases the integration of our custom ChatFunction within the Self-RAG framework, demonstrating how it dynamically adjusts the detail and focus of responses based on the system's self-assessment and retry count. The process illuminates the system's capability to iteratively refine its answers, ensuring high relevance and accuracy in the context of specialized automotive knowledge.\nWhen comparing performance across datasets, we observed the fact that while our models were optimized for automotive applications, they also showed improvements on the QReCC and CoQA datasets:\n\u2022 The most significant improvements were on the AID, aligning with our focus on automotive applications.\n\u2022 Performance gains on QReCC and CoQA, while smaller, were still substantial, indicating the robustness of our approach.\n\u2022 The Self-RAG Agent showed the most consistent performance across all datasets, highlighting its adaptability to various query types and domains.\nThese results validate our approach of using diverse datasets for evaluation. While our primary focus remains on optimizing for automotive applications, the improvements seen across datasets suggest that our methods enhance fundamental aspects of information retrieval and generation, benefiting both specialized and general applications."}, {"title": "5 DISCUSSION", "content": "This study presents a comprehensive approach to optimizing RAG techniques for automotive industry applications, specifically focusing on PDF chatbots deployed in local, low-performance environments. Our research addresses critical challenges in processing complex automotive documentation and responding to industry-specific queries."}, {"title": "5.1 Key Contributions", "content": "\u2022 Enhanced PDF Processing: We developed a novel method combining PDFMiner and Tabula to effectively handle multi-column layouts and complex tables prevalent in automotive technical documents. This significantly improves information extraction accuracy from industry-specific PDFs.\n\u2022 Advanced RAG Optimization: Our Langchain-based RAG system, featuring a custom retriever ensemble and context compression pipeline, demonstrates substantial improvements in retrieving and utilizing automotive-specific information.\n\u2022 Self-RAG Agent Design: The proposed AgenticRAG, enhanced with a custom function calling mechanism, shows superior performance in handling complex, multi-step queries typical in automotive engineering and manufacturing processes.\n\u2022 Cross-Domain Effectiveness: While optimized for automotive applications, our approach also shows improvements in general conversational Al tasks, as evidenced by performance gains on QReCC and CoQA datasets."}, {"title": "5.2 Implications for the Automotive Industry", "content": "Our research has significant implications for the automotive sector:\n1. Improved Information Access: The optimized PDF chatbot can greatly enhance access to technical information for engineers, technicians, and other stakeholders in the automotive industry.\n2. Enhanced Decision Making: By providing more accurate and contextually relevant information, our system can support better decision-making in design, manufacturing, and quality control processes.\n3. Resource Efficiency: The ability to deploy these advanced capabilities in low-performance, local environments addresses the industry's needs for data privacy and resource constraints."}, {"title": "5.3 Limitations and Future Work", "content": "While our study demonstrates significant advancements, there are areas for further research:\n1. Expanding Domain Coverage: Future work could focus on adapting the system to cover a broader range of automotive sub-domains, such as electric vehicle technology or autonomous driving systems.\n2. Real-Time Performance Optimization: Further research is needed to enhance the system's real-time performance in resource-constrained industrial environments.\n3. Multi-Modal Integration: Incorporating the ability to process and respond to queries about visual elements in technical diagrams and schematics could greatly enhance the system's utility.\n4. Longitudinal Study: A long-term study in real automotive manufacturing settings could provide insights into the system's impact on operational efficiency and decision-making processes.\n5. Ethical and Privacy Considerations: As the system deals with proprietary information, future work should explore advanced methods for ensuring data privacy and ethical use of Al in industrial settings."}]}