{"title": "Drone Detection using Deep Neural Networks Trained on Pure Synthetic Data", "authors": ["Mariusz Wisniewski", "Zeeshan A. Rana", "Ivan Petrunin", "Alan Holt", "Stephen Harman"], "abstract": "Drone detection has benefited from improvements in deep neural networks, but like many other applications, suffers from the availability of accurate data for training. Synthetic data provides a potential for low-cost data generation and has been shown to improve data availability and quality. However, models trained on synthetic datasets need to prove their ability to perform on real-world data, known as the problem of sim-to-real transferability. Here, we present a drone detection Faster-RCNN model trained on a purely synthetic dataset that transfers to real-world data. We found that it achieves an AP50 of 97.0% when evaluated on the MAV-Vid - a real dataset of flying drones compared with 97.8% for an equivalent model trained on real-world data. Our results show that using synthetic data for drone detection has the potential to reduce data collection costs and improve labelling quality. These findings could be a starting point for more elaborate synthetic drone datasets. For example, realistic recreations of specific scenarios could de-risk the dataset generation of safety-critical applications such as the detection of drones at airports. Further, synthetic data may enable reliable drone detection systems, which could benefit other areas, such as unmanned traffic management systems. The code\u00b9 and datasets\u00b2 are available.", "sections": [{"title": "I. INTRODUCTION", "content": "DRONE intrusions pose a risk to airports. In 2018, a reported drone sighting disrupted the operations at the Gatwick airport, UK, for 3 days. Since then, drones have been used for other illicit purposes, such as smuggling drugs into prisons [1], [2]. Although a serious disruption, such as the one in Gatwick, has not occurred since, reports of drone sightings at UK airports persist [3]\u2013[5]. To prevent this, the UK Government presented a counter-unmanned aircraft strategy [6]. It sets the detection of drones as a key objective.\nOn the other hand, UAS traffic management (UTM) is a relatively new area of research. It is concerned with integrating drones and air taxis with the current air traffic management (ATM). For UTM to work in urban environments, camera systems will likely have to work alongside RF/radar sensors due to interference from buildings. Accurate detection systems will enable the use of authorized drones for approved unmanned operations, whilst preventing unauthorized drones from flying in excluded parts of the airspace.\nDrones can be detected using different sensors such as radar, radio frequency (RF), acoustic, and electro-optical/infrared [7]. State-of-the-art detection methods typically fuse some of these sensors. Our research focuses on drone detection using a visible spectrum camera.\nRecent advances in neural networks (NNs) allow for much more accurate detection, tracking, and classification of drones in images or video feeds. However, accurate data for training the NN models remains difficult to obtain. In this article, we explore the use of structured domain randomization (SDR) within a simulated environment to generate a synthetic dataset of drones. We test whether a synthetic dataset generated using this method is generalizable to real-world datasets, by comparing the results to an established benchmark."}, {"title": "A. Literature", "content": "We focus on the optical detection of drones using NNs trained on synthetic data. Although object detection in images has been extensively studied, drone detection poses unique challenges, usually caused by environmental, or systematic issues. For example, weather effects impact the image, the drones can be small in the video frame making them hard to detect or classify, and the movement of UAVs is complicated to predict. Classification of the objects in the sky such as drones and birds is also a key issue. We investigate the literature on NNs, the use of synthetic data for training NNs, applications of NNs for drone detection, and finally, the use of synthetic data for drone detection.\n1) DNN advances and architectures: Recent advances in deep neural network (DNN) architectures (e.g. AlexNet [8]) became the state-of-the-art method for object recognition. They were later expanded to detect object bounding boxes, segment objects, and track them across videos. Object detection architectures comprise of single-stage and two-stage detectors. Examples of one-stage detectors are YOLO [9], [10] and SSD [11]. Examples of two-stage detectors are Fast-RCNN [12], Faster-RCNN [13]. One-stage detectors do the detection/classification directly from the extracted features, without a separate region proposal. Two-stage detectors generate region proposals (i.e. regions of the image where it thinks an object might exist) and then classify each of the proposed regions. In theory, two-stage detectors should produce better accuracy at a higher computational cost than one-stage detectors. However, in practice, they tend to vary for different applications and are sensitive to training parameters. An example of this is Isaac-Medina et al. [14], where the results of different architectures vary across different datasets. More recently, transformers [15] which use an attention mechanism and an encoder-decoder architecture, showed improvements in performance over traditional RNNs and CNNs as they are more parallelizable. This led to detection architectures such as DETR [16]. DETR matches Faster R-CNN in terms of performance on the COCO object detection dataset.\n2) Synthetic Data, Sim-to-Real, and Domain Randomization: Accurate data for training neural networks is expensive and may be hard to obtain. Creating real-life images or videos has a cost associated with it, which varies depending on the application. After the data is recorded, it needs to be manually labelled with the ground truth by a human. Synthetic data offers an automated alternative. By using 3D models and rendering software, synthetic images along with accurate ground truth labels can be generated. This presents a research question: can images rendered by 3D software be used to train NN models and transferred to real-world applications?\nThe use of synthetic data comes with advantages and disadvantages. The primary advantage is the potential reduction in the time cost required to generate the dataset. An algorithm can generate a very large dataset with no human input required. The primary disadvantage is that NN models trained on synthetic data are hard to transfer to real-world problems. Hence, the primary objective of synthetic data research is bridging the sim-to-real gap the transfer of a NN model trained on synthetic data to perform equivalently on real-world data.\nHowever, machine learning algorithms used to train neural networks rely on the training and evaluation datasets coming from the same source distribution. We are fundamentally breaking this assumption by using synthetic data for training and evaluating on real-world data. This is referred to as the out-of-distribution generalization [17].\nThere exist multiple strategies for achieving out-of-distribution generalization such as transfer learning [18], domain adaptation [19], and domain generalization [20]. Each of these relies on different assumptions between the training data, access to test data, and training conditions. To achieve sim-to-real transfer, domain generalization is the most relevant to our problem, as we assume that we cannot see the real-world test data during training. For the problem of sim-to-real for images, the most popular method of domain generalization is domain randomization.\nDomain randomization (DR) is a method of introducing model generalizability. This is done by randomizing parameters used to synthesize the source distribution, with the goal of making the data distribution so wide and varied that the model trained on this dataset is transferable to the target distribution. Lighting, textures, poses, and other parameters are randomized. Popular approaches aim to make the scenes as unrealistic as possible. Tobin at al. [21] is an example, which employs DR by randomizing the textures of the objects to make the network invariant to changes in scene conditions. Trembley et al. [22] use DR to bridge the sim-to-reality gap for object detection of cars for an autonomous driving application. Their approach employs DR by inputting random shapes, textures, and cars in unrealistic positions, with the aim of generating more variety to focus the neural networks on the structure of the object of interest. Prakash et al. [23] propose the use of structured domain randomization (SDR) - a variant of domain randomization, which takes into account the context of the scene. Hence instead of generating random scenes with unrealistic textures and objects, it aims to generate realistic scenes, while randomizing other parameters in a structured way. They find that SDR provides performance improvements over DR. Borrego et al. [24] propose the use of synthetic DR datasets as an alternative to pre-trained networks. Alghonaim & Johns [25] investigate the parameters used to randomize the simulation such as camera position, target colour, and lighting variations to find the most important parameters for sim-to-real transfer. They found that more high-quality realistic images improved the performance of sim-to-real. This comes in contrast with some of the earlier works (Tobin et al. [21] and Trembley et al. [22]) which focused on unrealistic, low-quality generated images. They also found that the randomization of distractor objects and background textures was important for generalising to new environments. Hinterstoisser et al. [26] propose a 'trick' to freeze the layers responsible for feature extraction of a NN trained on real-world images, and train only the remaining layers on synthetic images. They show that their approach works well on architectures such as Faster-RCNN, and Mask-RCNN.\nHence, there appear two schools of thought. DR produces unrealistic images with the aim of expanding the domain to allow the NN to better learn the features in settings that might not necessarily be used during test time. SDR produces realistic images with the aim of making the training data similar to the real-world data used at test time."}, {"title": "B. Visual Drone Detection", "content": "Visual drone detection is the process of finding the position of a drone in images or video feeds produced by visible-spectrum cameras. It has become a popular area of research, largely driven by the recent advances in machine learning. Most approaches [27]\u2013[30] typically use some of the detection architectures described in section I-A1, or a modification of the network for drone detection purposes, trained and tested on one of the publicly available drone datasets, or on a private dataset.\nIsaac-Medina et al. [14] present a benchmark of multiple DNN algorithms (SSD, YOLOv3, Faster R-CNN, and DETR) on publicly available datasets of flying drones (MAV-VID, Drone-vs-Bird, Anti-UAV). They present two benchmarks: the accuracy of detection, and the accuracy of tracking algorithms. They present the results of each of the models trained and tested on each of the datasets. Note that they create three distinct sets of weights for each of the datasets. We were able to independently reproduce the test metrics shown by Isaac-Medina et al. based on their published model weights for the Faster-RCNN model (although, we did not attempt to reproduce the training of the models).\nFreudenmann et al. [31] explore the effects of data augmentation strategies on the performance of their NN models. They set up their experiments by training a Faster R-CNN model on the Drone vs Bird dataset. They then test their NN model on other datasets: a subset of the Drone vs Bird dataset, Anti-UAV dataset, background test dataset (images of drones scraped from the internet), Disturbing Objects dataset (to find false positives), and New UAV Types (unseen UAV images scraped from the internet). They find that by test a model trained on the DvB dataset, on the Anti-UAV dataset, the accuracy drops significantly. The authors propose the cause to be the mosaics and artefacts found in the Anti-UAV dataset. To reduce this effect, the authors propose the use of mosaic + standard dataset transforms. This increases the mAP to 78.7% from 54.2% with no augmentations. The augmentations appear to train the model to reduce the false positive rate. Still, this falls short of the results presented by Isaac-Medina et al. where the Faster R-CNN model trained on the Anti-UAV dataset and tested on the subset of the Anti-UAV dataset achieves 97.7%. This highlights the challenge of transferring models trained on one dataset to other datasets a drone detection model trained on one dataset is not guaranteed to perform equivalently on another dataset.\nMediavilla et al. [32] present a benchmark of Swin, YOLOv4, CenterNet, CenterNet2, and Faster-RCNN architectures on CACHOW a helicopter dataset - and Drone vs Bird datasets. However, the results are not directly comparable with Isaac-Medina et al. as the test configurations are different.\nOther approaches consider the temporal dimension. Thai et al. [33] present a spatio-temporal NN design, in which multiple frames of the video feed are stacked together before being input into the NN. They show that this approach is more accurate, but comes with a higher computational cost as more data needs to be processed. Craye and Ardjoune [34] use U-Net to segment the position of the drone, classify it, and then use a spatio-temporal filter. Sangam et al. [35] present a spatio-temporal transformer for drone-to-drone detection."}, {"title": "C. Synthetic Data for Drone Detection", "content": "Synthetic data approaches have been attempted in the field of drone detection. One of the first attempts that we were able to find is Rozantsev et al. [36] which uses domain randomization techniques (without referring to them as such the paper was published before domain randomization became a common term in literature), such as randomizing the position of the drones, varying the motion blur, adding random noise, and randomizing object material properties. They also repeat this process for aircraft and cars.\nMarez et al. [37] create a synthetic dataset of UAVs using DR and test it on the Drone vs Bird dataset. They create a baseline model (trained only on DvB) and compare the models trained only on the DR datasets, and models pre-trained on the DR datasets and finetuned on real-world data. They find that DR-only datasets perform the worst, while the finetuned models perform better but still fall short of the baseline. This highlights the challenge of transferring synthetically trained models to real-world datasets. Peng et al. [38] use 3D models of drones and random HDRIs to generate a photorealistic synthetic dataset, and train a Faster-RCNN model. It is tested on a real-life dataset of images downloaded from the internet. They find that using a pre-trained model improves their performance significantly, and using occluded drone images improves their performance by 1%. However, their test dataset of images downloaded from the internet might not be representative of a real-world security scenario. We were also unable to find a copy of their test dataset online which makes it hard to directly compare the results to. Dieter et al. [39] attempt to bridge the sim-to-real gap by generating a synthetic dataset in Unreal Engine. They test their models on synthetic and real data. Overall, they find that, generally, models trained on synthetic data fall short in terms of the performance of the models trained on real-world data. However, by using a small share of real-world data, they improve the results and get close to the results produced by using real-world data only. DronePose [40] uses 3D models of a DJI Mavic, and a DJI Inspire, to create a synthetic dataset using Unreal Engine. They also create a neural network based on a U-Net architecture, to segment parts of the drone, find the orientation, and identify the drone model. The use of GANs to generate a synthetic dataset was attempted by Li et al. [41]. This is done to better understand the deep feature space of how drones are represented by neural networks. Further, they use Topological Data Analysis to acquire missing data. This study looks at something that not many other studies consider how neural networks learn to represent drones within the latent space. Carrio et al. [42] use synthetic images to predict drones using depth maps for the application of obstacle detection of avoidance. By using a synthetic environment, they are able to create accurate ground truth depth maps, which they later use to train a NN."}, {"title": "D. Outro Contribution and Outline", "content": "This work is largely a continuation of Wisniewski et al. [43], [44]. It expands the method of synthetic data generation to object detectors, which can detect the position of the drone in each of the frames. Isaac-Medina et al. created a UAV detection benchmark, in which they compare common object detection architectures. This benchmark is used as a baseline to compare the results from our models trained on synthetic data.\nWe present a method of systematically generating a synthetic dataset of drones alongside pixel-accurate ground truth segmentation masks using a form of structured domain randomization. This presents an advantage over real-world datasets which are costly to annotate and may not be as accurate. Further, synthetic datasets allow flexibility over standard approaches such as being able to add new drones to the dataset using only a 3D model. They also allow for the creation of complex scenarios that may be hard to record in real life such as forest fires, or drones flying in stormy conditions. Generally, drones should not be flown in windy conditions in real life and this might make creating a research study case hard due to safety concerns.\nThe challenge with this approach is that the synthetic dataset may not be representative of the real-world data, and a model trained on this dataset might not translate to real-world datasets this is the sim-to-real problem of transferring domains, as explored in section I-A2. We present and compare multiple datasets which are generated by processes of SDR and DR. The reasons for focusing on the dataset are:\n\u2022 The publicly available drone datasets vary in quality. Because they use human-labeled ground truth boxes, these are imperfect at times. They are also limited in that they do not have the segmentation ground truth.\n\u2022 Some of the previously publicly available datasets are no longer available.\n\u2022 New real-world datasets are costly to produce.\nThese items make researching drone detection and comparing results between studies challenging. We address these points by releasing our synthetic dataset. The contributions of this publication are:\n\u2022 Multiple synthetic datasets with pixel-accurate segmentation masks, generated by the process of structured domain randomization.\n\u2022 A study into the ability of the neural network model trained on synthetic data to generalize to real-world drone detection scenarios. This differs from SOTA in the review because the synthetic drone detection applications mixed real-world data with their synthetic data for the training to achieve sim-to-real. They also didn't use open-access datasets such as Drone-vs-Bird or Anti-UAV.\n\u2022 Study of different domain randomization styles.\n\u2022 Generalizable Faster R-CNN model weights for drone detection; trained purely on synthetic images, tested on multiple real-life datasets.\nThrough our literature review, we have identified several faults within some of the research in the drone detection area that we attempted to remedy during our study:\n\u2022 Use of private datasets. Using private datasets makes it impossible for other researchers to reproduce results. To remedy this, we open-source our dataset, and we use publicly available datasets for testing.\n\u2022 Lack of a common baseline to compare the results to. This is a common problem which makes it hard to directly compare the results of one method over another. To remedy this, we use the results of the Isaac-Medina et al. benchmark to compare our results.\n\u2022 Repeatability of results. Many publications present the results as a single value. However, due to the randomness of neural network training, retraining yields different results. To remedy this, we present our results as a mean with a 95 per cent confidence interval.\n\u2022 Reproducibility of results. A lot of research in this area is hard to reproduce. To remedy this, we open-source our synthetic dataset, as well as the code used for training and testing of the NN models.\nWe believe that this is one of the first attempts to study the use of structured domain randomization for the generation of synthetic drone datasets. Further, we perform a study comparing different domain randomization techniques to find out how much they influence drone detection. Although we have found attempts in the literature to use domain randomization techniques for drone detection, we were unable to find systematic studies to quantify which aspects of domain randomization improve the results of drone detection.\nTo the best of our knowledge, previous attempts to test the sim-to-real transferability used private real-world datasets. Instead, we directly compare our results to the benchmark presented by Isaac-Medina et al. [14] which uses publicly available datasets.\nWe present a successful sim-to-real transfer for the application of drone detection. We prove that our NN model trained on a purely synthetic dataset successfully transfers to the MAV-Vid dataset. Our model achieves a mean of 97.0% compared with the Isaac-Medina et al. model, which was trained on the MAV-Vid dataset, of 97.8%.\nThis paper is divided into five sections. In section II we explain the methodology. This includes explaining the rendering process used to generate the datasets. We explain different styles of datasets generated using different domain randomization styles. We describe the NN training process, we explore the use of data augmentations, in particular noise and JPEG compression, and finally, we explain the testing procedure, to measure the accuracy of sim-to-real transfer. In section III we explain the results. We perform tests on camera bounds randomization, effects of data augmentations, and variation in domain randomization styles, and lastly, we compare our results to the literature. In section IV we conclude the findings. In section V we discuss further work."}, {"title": "II. METHODOLOGY", "content": "This section describes the 3 stages of our experimental process: synthetic dataset generation (section II-A), training of neural networks (NNs) (section II-B), and the testing of the NN models on real-world datasets (section II-C). The experimental process broken up into these stages is shown in figure 1."}, {"title": "A. Synthetic Dataset Generation using Structured Domain Randomization", "content": "This section describes the methodology used to create synthetic datasets using Blender [45] for the purpose of drone detection. This is done by the process of Structured Domain Randomization (SDR) [23]. Blender allows the rendering of photorealistic images using a ray-trace-based Cycles engine. It also allows for programmatic randomization of simulation parameters, and the creation of pixel-level accurate ground truth segmentation masks. The primary advantage of this approach is that the labelling of ground truth is automated compared with real-life approaches, where manual labelling is required. We continue this section by describing the process of SDR used to generate the datasets and the multiple datasets that are generated using different styles.\nSDR is a technique for generating random synthetic images. It differs from domain randomization (DR) in that it aims to generate scenes that have a realistic context. DR aims to expand the domain used to train the NN model outside of the operating domain to improve the performance of the model within the operating domain. This is achieved by implementing unrealistic textures, backgrounds, and contexts. SDR differs in that it aims to recreate the operating domain within the simulation and uses realistic textures, backgrounds, and contexts, but continues to randomize global parameters (such as lighting, camera poses, and layout of scenarios).\nHence, we will refer to realistic datasets as SDR, and unrealistic datasets as DR. We will refer to a drone with realistic textures in a realistic context as SDR, and we will refer to a flying cube object with a random texture as DR. Although our drone dataset is generated using the SDR technique, we also attempt the use of DR methods by creating unrealistic distractors in datasets with generic and realistic distractors (these can be seen in Figure 2).\n1) Rendering Process: We use a Python script within Blender to generate the datasets. We use 5 different 3D models of drones: DJI Phantom, DJI Inspire, 2 DJI Mavics, and a DJI FPV. We model the drones as a swarm travelling in a path from one point to another. The drones are animated for 300 frames. After the 300 frames, a new path is generated and the HDRI (background and lighting) is changed. Every frame, the position of the camera and the focal length is randomized. The camera is influenced to look towards the drones. The focus point of the camera is set to a very large distance. This is done to model security cameras, which are typically focused on infinity. A limitation of Blender is that it does not model the function to focus on infinity within its camera model, so we get around this limitation by setting the focus distance to a very large value. The illustrations of the DJI drone models and the overall dataset generation process are shown at the top of figure 1).\n2) Variations in Synthetic Datasets: We create a few variations of the datasets based on different methods from the literature to test if they improve the performance of the detection model. The variations that we create are drones with birds, drones with generic distractors, drones with realistic distractors, and drones with unrealistic backgrounds.\nDrones Only (SDR) shown in figure 2(a) is a dataset of flying drones. It is the standard version of the dataset as described in section II-A1.\nDrones with Birds (SDR) shown in figure 2(b), is a variation of the drones-only dataset, with the addition of animated birds flying alongside the drones. It is designed as a synthetic version of the Drone-vs-Bird dataset, which contains images of drones with birds acting as distractors. The advantage of the synthetic version is that the birds are labelled in the ground truth mask, unlike in the real dataset, where the birds are not labelled at all. But, for consistency with the Drone-vs-Bird dataset, we will not be using the bird labels during training.\nDrones with Generic Distractors (DR) shown in figure 2(c), is a variation of the drones-only dataset, with the addition of generic distractors - cubes, cones, and other basic geometric shapes with random colours added to the scene with the aim of 'distracting' the model. This is based on the findings of Alghonaim & Johns [25] who report that adding distractors to the synthetic dataset is a key technique for reducing sim-to-real error. This technique is common across domain randomization applications such as Marez et al. [37] who use distractors for drone detection, and Trembley et al. [22] who add generic distractors to improve car detection.\nDrones with Realistic Distractors (DR) shown in figure 2(d), is designed as a variation of the generic distractors dataset, with a different style of distractors. The distractors consist of realistic 3D models of objects such as cars, lampposts, cones, traffic signs, etc. (as opposed to geometric shapes in the generic distractors dataset). We label this as a variation of DR and not SDR, because although the objects themselves are realistic, they are not structured in any realistic way.\nDrones with Unrealistic Backgrounds (DR) shown in figure 2(e) is a variation of the drones-only dataset, with the realistic HDRI backgrounds replaced by random coloured/patterned backgrounds. This was not done in the style of a particular paper, although Alghonaim & Johns did experiment with different background types. We hypothesized that if the unrealistic textures and objects can improve the performance of the detector (as shown by the majority of domain randomization literature), random backgrounds should act similarly. The random backgrounds are generated using random colour palettes, with random shapes. Different blending modes are also applied to the images."}, {"title": "B. Neural Network Architecture and Training", "content": "The primary aim of this study is to compare the results with Isaac-Medina et al. hence any of the architectures they used (Faster R-CNN, SSD, YOLO, DETR) was acceptable. Faster R-CNN was also used in other publications such as Freudenmann et al. [31], hence we chose it as our architecture. Keeping the architecture common allows us to compare the impact of the dataset while controlling for other variables.\nThe Faster R-CNN [13] network is trained using ResNet-50 as a backbone. The network is pretrained on the MS COCO dataset [46]. Unless otherwise specified, the settings of the model are unchanged from the default implementation in PyTorch [47], [48], an SGD optimizer is used with a learning rate of 0.0003, momentum of 0.9, and weight decay of 0.0005. We use a step learning rate scheduler which decays the learning rate by 0.1 every 3 epochs. The networks are trained for 10 epochs.\n1) Data Augmentations: Cropping, Noise, and Compression: Augmentations are a common way to improve the diversity of the dataset by performing common operations such as cropping, changing contrast, blurring, etc. Adding noise to datasets as an augmentation has been tried in synthetic data literature with conflicting reports. Some articles report a performance improvement [22], while others report it to be negligible [21]. Wisniewski et al. [43] find that adding synthetic noise improves the sim-to-real performance of drone classification. The images generated by Blender are in the lossless PNG format. However, the real-life datasets are compressed in the JPEG format, reducing the size, but also reducing the quality. We speculated that this might affect the ability of the network to transfer to real-world datasets. Poyser et al. [49] investigate the impact of compression on the performance of CNNs. They show that, in the task of object detection, a model not trained on compressed imagery drops performance when tested on compressed imagery. However, they find that after retraining the model on compressed imagery the performance improves for even highly compressed images (although the performance of the FRCNN model still drops as the images are compressed more).\nTo quantify the effects of this, we perform an experiment where we compare a model trained on only lossless PNG images, with a model where 50% of the images in the dataset are compressed uniformly in a range of 0-95%. To achieve this we use the imgaug library [50]."}, {"title": "C. Testing: Datasets and Performance Metrics", "content": "The goal of this study is to find whether the use of a purely synthetic drone dataset to train the neural network for sim-to-real transfer is feasible. Ideally, the model should be transferrable to many different real-life datasets (and not just a single one). This goes against the ethos of neural networks, which assume that the testing dataset should be a subset of the training dataset. Instead, we are aiming for out-of-domain generalization, wherein the training datasets and testing datasets originate from different domains, hence bridging the sim-to-real gap. To this end, we test our neural network model on three different real-life datasets: MAV-VID, Anti-UAV, and Drone-vs-Bird. We compare our results with a benchmark developed by Isaac-Medina et al., where they trained a model for each of the datasets, and tested each model on a subset of the dataset that they were trained on. Instead, we propose a single set of weights which is tested on unseen real-life datasets.\nWe aim for the sim-to-real to be accurate, and we also aim for it to be generalizable we want the weights to perform well on any drone dataset. We use the publicly available real-world datasets used by Isaac-Medina et al. in their benchmark: MAV-Vid, Drone-vs-Bird, and Anti-UAV.\nMAV-VID [51] consists of 29,500 images used for training and 10,732 images for validation. It contains videos of drones captured by other flying drones and drones captured from a ground camera.\nDrone-vs-Bird [52], [53] consists of 85,904 images used for training and 28,856 images for validation. The videos are taken from static cameras near backdrops of buildings, fields, lakes, and landscapes. The aim of the dataset is to reduce false positives with regard to birds coming up as positive objects instead of drones. However, the birds are not labelled in the dataset (only drones are).\nAnti-UAV [54] consists of 149,478 images used for training and 37,016 images for validation. It features videos of drones captured by a pan-tilt-zoom (PTZ) security camera, on a backdrop of cities. The PTZ camera is manually operated to track the drones. The dataset features videos from an optical camera in the visible spectrum and from an infrared (IR) camera. We will focus on the visible spectrum images only. Although its aim is to benchmark UAV tracking capabilities, we can still use it to benchmark drone detection.\nThe datasets described contain only drone labels and do not contain labels for other objects (such as birds, or other flying objects). COCO metrics (that we will be using for performance metrics in this chapter) work by finding the Intersection over Union between the prediction and the ground truth.\nThe accuracy of the manual ground truth labels within the datasets varies in quality. Figure 3 shows an example of a ground truth (white rectangle) which is inaccurate. In this case, the model prediction (blue rectangle) is more accurate than the ground truth, but because it falls below the intersection over union (IoU) of 0.5 it is not counted as a correct prediction. This is generally the case, especially for smaller instances of drones. Because it is hard to quantify how many of the ground truth labels are inaccurate, and costly to remedy, we will take the labels as they are.\nD. Repeatability\nThe process of NN training inherently contains randomness, hence repeating training with the same settings produces different results. This is rarely presented in results, with most publications that we have observed simply presenting the results from a single run. The initial seed has an effect on the results of the training, as presented by Picard [55]. Picard shows that the distributions generated from random seeds generally follow a normal distribution (even though some of their results do appear skewed). Although he mentions black swan events [56], we believe they meant this in the context of extreme values on the sides of the Gaussian distribution as opposed to a fat-tailed distribution. Hence, we assume the results from our training runs will follow a Gaussian distribution. To report our results we will take 8 samples at each configuration and report the mean alongside the 95 percent confidence interval."}, {"title": "III. RESULTS", "content": "This section shows the results of the Faster-RCNN object detection architecture trained on a purely synthetic dataset of drones generated in Blender described in section II-A. The trained model is tested on three separate datasets: MAV-Vid, Drone-vs-Bird (DvB), and Anti-UAV described in section II-C. The aim of these experiments is to understand whether sim-to-real, the process of transferring a model trained on synthetic data to real-world scenarios, is possible for drone detection, and if so, which variables impact the sim-to-real transfer.\nWe perform the following studies: camera bounds randomization (section III-A), dataset augmentations (section III-B, dataset size (section III-C), and domain randomization styles (section III-D). Lastly, we compare our results to literature in section III-E. Unless otherwise specified, the experiments use the training parameters described in section II-B, a camera bounding size of 40 m is used, a dataset size of 5,000 is used, JPEG compression is enabled, noise is enabled, and each experiment is repeated 8 times to produce a mean and a 95 per cent confidence interval."}, {"title": "A. Camera Bounds Randomization", "content": "The position of the camera with respect to drones is one of the parameters randomized in the simulation. The size of camera bounds in turn is correlated to the size of the drones in the images in the dataset. Smaller camera bounds translate into a higher proportion of large-sized drones, and larger camera bounds translate into a higher proportion of small-sized drones in the dataset. Smaller images of drones may make it harder for the network to learn their shape, but may also improve the performance of the network on datasets which contain drones further away from the camera. Hence it is important to find a balance between the two. To understand the relationship between the bounds of the camera position randomization and the performance on the real-world data, we create 5 datasets with different bounds: 20 m, 40 m, 80 m, 160 m, and 320 m."}, {"title": "B. Data Augmentations (JPEG Compression and Noise)", "content": "We investigate the effects of JPEG compression and Gaussian noise data augmentations, as described in II-B1.\nFigure 6 shows the results of the data augmentations study, testing whether JPEG compression and noise have any effects on the sim-to-real transferability. The dataset with no augmentations applied achieves a mean AP0.5 of 96.6% on MAV-Vid, 48.4% on DvB, and 63.0% on the Anti-UAV dataset. When applying JPEG compression, the results do not show a significant difference. Adding noise seems to have a slight effect, producing a mean AP0.5 of 97.1% on MAV-Vid, 49.7% on DvB, and 66.7% on"}]}