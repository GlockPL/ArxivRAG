{"title": "Discovering symbolic expressions with parallelized\ntree search", "authors": ["Kai Ruan", "Ze-Feng Gao", "Yike Guo", "Hao Sun", "Ji-Rong Wen", "Yang Liu"], "abstract": "Symbolic regression plays a crucial role in modern scientific research thanks to its capability of\ndiscovering concise and interpretable mathematical expressions from data. A grand challenge lies\nin the arduous search for parsimonious and generalizable mathematical formulas, in an infinite\nsearch space, while intending to fit the training data. Existing algorithms have faced a critical\nbottleneck of accuracy and efficiency over a decade when handling problems of complexity, which\nessentially hinders the pace of applying symbolic regression for scientific exploration across\ninterdisciplinary domains. To this end, we introduce a parallelized tree search (PTS) model\nto efficiently distill generic mathematical expressions from limited data. Through a series of\nextensive experiments, we demonstrate the superior accuracy and efficiency of PTS for equation\ndiscovery, which greatly outperforms the state-of-the-art baseline models on over 80 synthetic\nand experimental datasets (e.g., lifting its performance by up to 99% accuracy improvement and\none-order of magnitude speed up). PTS represents a key advance in accurate and efficient data-\ndriven discovery of symbolic, interpretable models (e.g., underlying physical laws) and marks a\npivotal transition towards scalable symbolic learning.", "sections": [{"title": "Introduction", "content": "Over the centuries, scientific discovery has never departed from the use of interpretable math-\nematical equations or analytical models to describe complex phenomena in nature. Pioneering\nscientists discovered that behind many sets of empirical data in the real world lay succinct gov-\nerning equations or physical laws. A famous example of this is Kepler's discovery of three laws of\nplanetary motion using Tycho Brahe's observational data, which laid the foundation for Newton's\ndiscovery of universal gravitation. Automated extraction of these natural laws from data, as a class\nof typical symbolic regression (SR) problems [1], stands at the forefront of data-driven scientific ex-\nploration in natural sciences and engineering applications [2\u201310]. However, uncovering parsimonious\nclosed-form equations that govern complex systems (e.g., nonlinear dynamics) is always challeng-\ning. Data revolution, rooted in advanced machine intelligence, has offered an alternative to tackle"}, {"title": "Results", "content": "SR aims to discover concise, precise, and human-interpretable mathematical expressions hidden\nwithin data, offering significant value in aiding scientists to decipher the underlying meanings of\nunknown systems. Mathematically speaking, SR can be cast as the process of finding an expression\n$f:R^m \\rightarrow R$ that satisfies $y = f(X)$ given data $D = (X,y)$, where $X \\in R^{n \\times m}, y \\in R^{n \\times 1}$,\n$n$ represents the number of data samples and $m$ the number of independent variables. Here, the\nform of $f$ is usually constructed by a finite set of math symbols based on a given token library\n$\\Theta = {+,-, \\times, \\div, sin(\\cdot), exp(\\cdot), ..., const.}$. To make the underlying expression interpretable and\nparsimonious, SR algorithms are required to swiftly produce a Pareto front, balancing the error\nand complexity in underlying expressions. In other words, there is a need for a new SR algorithm\nthat is both computationally efficient and exhibits a high symbolic recovery rate (e.g., also referred\nas accuracy; see Supplementary Note 2.4 for specific definition). However, existing SR algorithms\nare faced with significant bottlenecks associated with low efficiency and poor accuracy in finding\ncomplex mathematical expressions. To this end, we propose a novel parallelized tree search (PTS)\nmodel, as shown in Fig. 1, to automatically discover mathematical expressions from limited data."}, {"title": "Discovery of chaotic dynamics", "content": "Nonlinear dynamics is ubiquitous in nature and typically governed by a set of differential equa-\ntions. Distilling such governing equations from limited observed data plays a crucial role in better\nunderstanding the fundamental mechanism of dynamics. Here, we test the proposed PTS model\nto discover a series of multi-dimensional autonomous chaotic dynamics (e.g., Lorenz attractor [51]\nand its variants [52]). The synthetic datasets of these chaotic dynamical systems are described in\nSupplementary Note 2.2. It is noted that we only measure the noisy trajectories (e.g., with 1%"}, {"title": "Electro-mechanical positioning system", "content": "Real-world data, replete with intricate noise and nonlinearity, may hinder the efficacy of SR\nalgorithms. To further validate the capability of our PTS model in uncovering the governing equa-\ntions for real-world dynamical systems (e.g., mechanical devices), we test its performance on a set\nof lab experimental data of an electro-mechanical positioning system (EMPS) [53], as shown in Fig.\n4a-b. The EMPS setup is a standard configuration of a drive system used for prismatic joints in\nrobots or machine tools. Finding the governing equation of such a system is crucial for designing\nbetter controllers and optimizing system parameters. The dataset was bifurcated into two even\nparts, serving as the training and testing sets, respectively. The reference governing equation is\ngiven by $M\\ddot{q} = -F_v\\dot{q} - F_c sign(\\dot{q}) + \\tau - c$ [53], where $q, \\dot{q}$, and $\\ddot{q}$ represent joint position, velocity,\nand acceleration. Here, $\\tau$ is the joint torque/force; $c, M, F_v$, and $F_c$ are all constant parameters in\nthe equation. In EMPS, there exists friction that dissipates the system energy. Based on this prior\nknowledge, we include the sign operator to model such a mechanism. Hence, the candidate math\noperators we use to test the SR algorithms read $\\{+,-, \\times, \\div, sin, cos, exp, log, sign\\}$.\nWe compare our PTS model with three pivotal baseline models, namely, PySR [50], NGGP [45],\nand BMS [12]. We execute each SR model 20 trials on the training dataset to ascertain the Pareto\nfronts. Subsequently, we select the discovered equation from the candidate expression set of each\nPareto front based on the test dataset which exhibits the highest reward value delineated in Eq. (8).\nThe reward is designed to balance the prediction error and the complexity of the discovered equation,\nwhich is crucial to derive the governing equation that is not only as consistent with the data as\npossible but also parsimonious and interpretable. Finally, we select among 20 trials the discovered\nequation with the median reward value to represent each SR model's average performance. The"}, {"title": "Governing equation of turbulent friction", "content": "Uncovering the intrinsic relationship between fluid dynamics and frictional resistance has been\nan enduring pursuit in the field of fluid mechanics, with implications spanning engineering, physics,\nand industrial applications. In particular, one fundamental challenge lies in finding a unified formula\nto quantitatively connect the Reynolds number $(Re)$, the relative roughness $r/D$, and the friction\nfactor $\\lambda$, based on experimental data. The Reynolds number, a dimensionless quantity, captures\nthe balance between inertial and viscous forces within a flowing fluid, which is a key parameter in\ndetermining the flow regime, transitioning between laminar and turbulent behaviors. The relative"}, {"title": "Model performance analysis", "content": "The performance of the proposed PTS model depends on several factors, including the noise\nlevel of the given data, model hyper-parameters, and whether certain modules are used. Herein, we\npresent an analysis of these factors as well as the model efficiency and the memory footprint."}, {"title": "Model ablation study", "content": "We conduct three cases of model ablation study to evaluate the role\nof certain modules. First, we investigate how much improvement the use of MCTS for automatic\ndiscovery of admissible tokenized input brings. Second, we conduct a sensitivity analysis of the\ntoken constants range. Third, we investigate the extent of the benefit of using DR Mask. The\nresults of the ablation experiments are shown in Fig. 5a-c.\nFirstly, we evaluate the symbolic recovery rate to observe the impact of replacing MCTS with\nrandom generation of input tokens for PSRN. The tests are conducted on the Nguyen-7/12, R-1/2/3\nand Livermore-1/3/5/12/13/15/18/22 benchmark expressions, selected because of their higher com-\nplexity. It can be observed in Fig. 5a that the recovery rate diminishes if MCTS is removed from\nPTS, indicating its vital role in admissible token search that signals the way forward for expression\nexploration. Secondly, to investigate the sensitivity of our model to the randomly sampled token\nconstants, we set the range to [0,1], [0,3], and [0, 10] respectively, and performed the experiments\non the Nguyen-c benchmark expressions. The result in Fig. 5b shows that when the token constants\nare sampled from the range excluding the ground truth, the model needs to spend extra search effort\nto retain the accuracy (e.g., recovery rate). Last but not least, we test the efficacy of DR Mask for\nmemory saving. The result in Fig. 5c illustrates that DR Mask is able to save the graphic memory\naround 50%, thus improving the capacity of the operator set (e.g., more operators could be included\nto represent complex expressions)."}, {"title": "Robustness to noise", "content": "To test the robustness of our PTS model to measurement noise, we con-\nducted experiments with different levels of noise (e.g., Gaussian-type) and data availability for\nthe target equation $f(x) = 0.3x^3 + 0.5x^2 + 2x$ where $x \\in [-1,1]$ [42]. The operators used were\n$\\{+,-, \\times, \\div\\}$. Since noise may cause inaccurate constant fitting, our evaluation criterion is set to\nconsider the equation uncovered successfully as long as the correct equation form is found. The\nheatmap in Fig. 5d shows the effectiveness of our model, which indicates that PTS has a fairly high\nlevel of robustness against data noise and scarcity."}, {"title": "Expression search efficiency", "content": "We compare the efficiency of PTS in the context of evaluation\nof large-scale candidate expressions, in comparison with two brute-force methods (e.g., NumPy [58]\nthat performs CPU-based evaluation serially, and CuPy [59] that operates on GPUs in parallel based\non batches). Note that PTS possesses the capability of automatic identification and evaluation of\ncommon subtrees, while NumPy and CuPy do not have such a function. Assuming independent\ninput variables $\\{x_1,...,x_5\\}$ with operators $\\{+, \\times, identity, neg, inv, sin, cos, exp, log\\}$ for a maxi-\nmum tree depth of 3, the complete set of generated expressions is denoted by $F$. We consider the\ncomputational time required to evaluate the loss values of all the expressions, e.g., $\\|y \u2013 f(X)\\|$,\nwhere $f\\in F$, under different sample sizes (e.g., $10^1 ~ 10^4$ data points).\nThe result shows that PTS can quickly evaluate all the corresponding expressions, clearly sur-\npassing the brute-force methods (see Fig. 5e). When the number of samples is less than $10^4$, the\nsearch speed of PTS is about 4 orders of magnitude faster, exhibiting an unprecedented increase in\nefficiency, thanks to the reuse of common subtree evaluation in parallel. Notably, when the number"}, {"title": "Space complexity", "content": "We categorize the operators used in PTS into three types: unary, binary-\nsquared, and binary-triangled, which are represented by u, bs, and by, respectively. Binary-squared\noperators represent non-commutative operators (e.g., - and \u00f7) depending on the order of operands,\nwhich requires $\\omega_{i-1}$ space on GPU during PSRN forward propagation (here, $\\omega_{i-1}$ represents the\ninput size of the previous symbol layer). Binary-triangled operators represent commutative oper-\nators (e.g., + and \u00d7) or the memory-saving version of non-commutative operators that only take\nup $\\omega_{i-1}(\\omega_{i-1} + 1)/2$ space (e.g., the SemiSub and SemiDiv symbols, described in Methods: Sym-\nbol layer, in the Feynman benchmark which are specifically designed to save memory and support\noperations in only one direction).\nWith the number of operators in each category denoted by $N_u$, $N_{bs}$ and $N_{bt}$, and the number of\nindependent variables and layers of PTS denoted by m and l, respectively, the number of floating-\npoint values required to be stored in PTS can be analyzed. Ignoring the impact of DR Mask, there\nis a recursive relationship between the tensor dimension of the (i - 1)-th layer (e.g., $\\omega_{i-1}$) and that\nof the i-th layer (e.g., $\\omega_i$), namely,\n$\\omega_i = N_u\\omega_{i-1} + N_{bs} \\omega_{i-1}^2 + N_{bt} \\omega_{i-1} \\frac{\\omega_{i-1} + 1}{2} < \\kappa \\omega_{i-1}^2,$\nwhere $\\kappa = N_u + N_{bt} + N_{bs}$. Thus the complexity of the number of floating-point values required\nto be stored by an l-layer PSRN can be calculated as $O(\\kappa^{2^{l-1}} \\omega_0^2)$, where $\\omega_0$ represents the number\nof input slots.\nClearly, the memory consumption of PSRN increases dramatically with the number of layers. If\neach subtree value is a single-precision floating-point number (e.g., 32-bit), with the input dimension\nof 20 and operator set $\\{+, -, \\times, \\div, identity, sin, cos, exp, log\\}$, the required memory will reach over\n$10^5$ GBs when the number of layers is 3, which requires a large compute set. Hence, finding a new\nstrategy to relax the space complexity and alleviate the memory requirement is needed to further\nscale up the proposed model. Fig. 5f illustrates the graphic-memory footprint of various three-\nlayered PSRN architectures, each characterized by a different operator set and the number of input\nslots. While the rise in memory demands serves as a constraint, this escalation is directly tied to\nthe scalable model's ability to evaluate a greater number of candidate expressions within a single\nforward pass. This result also shows that PSRN follows the scaling law (e.g., the model capacity\nand size scale with the number of token inputs, given the fixed number of layers). The detailed\nhardware settings are found in Supplementary Note 2.5."}, {"title": "Discussion", "content": "This paper introduces a new SR method, called PTS, to automatically and efficiently discover\nparsimonious equations to interpret the given data. In particular, we propose a PSRN architecture\nas the core search engine, which (1) automatically captures common subtrees of different symbolic\nexpression trees for shared evaluation to expedite the computation, and (2) capitalizes on GPU-\nbased parallel search with a notable performance boost. By recognizing and exploiting common\nsubtrees in a vast number of candidate expressions, PSRN effectively bypasses the inefficiency and\nredundancy of independently evaluating each candidate. Such a strategy not only increases the"}, {"title": "Methods", "content": "The NP-hardness of SR has been noted in existing studies [28, 30, 45], followed by a formal proof\nrecently [46]. This implies the absence of a known solution that can be determined in polynomial\ntime for solving SR problems across all instances, and reflects that the search space for SR is indeed\nvast and intricate. Almost all the existing SR methods involve a common procedure, which is to\nassess the quality of candidate expressions $F = \\{f_1, f_2, ...\\}$ based on the given data $D = (X,y)$,"}, {"title": "Symbol layer", "content": "A mathematical expression can be equivalently represented as a parse tree [13], where the internal\nnodes denote mathematical operators and the leaf nodes the variables and constants. The crux of\nthe aforementioned computational issue lies in the absence of temporary storage for subtree values\nand parallel evaluation. Consequently, the common subtrees in different candidate expressions are\nrepeatedly evaluated, resulting in significant computational wastage.\nTo this end, we introduce the concept of Symbol Layer (see Fig. 1c), which consists of a\nseries of mathematical operators. The Symbol Layer serves to transform the computation results\nof shallow expression parse trees into deeper ones. From the perspective of avoiding redundant\ncomputations, the results of the Symbol Layer are cached and can be utilized by parse trees with\ngreater heights. From the view of parallel computation, the Symbol Layer can leverage the power of\nGPU to compute the results of common subtrees in parallel, significantly improving the overall speed\n(see Supplementary Fig. S.1). We categorize the mathematical operators in the Symbol Layer into\nthree types: (1) unary operators, (e.g., sin and exp); (2) binary-squared operators (e.g., - and \u00f7),\nrepresenting non-commutative operators; (3) binary-triangled operators, representing commutative\noperators (e.g., + and \u00d7) or a variant of non-commutative operators with low-memory footprint\n(e.g., SemiSub and SemiDiv that output $x_i - x_j$ and $x_i \\div x_j$ for $i < j$, respectively). Note that\nthe binary-triangled operators only take up half of the space compared with the binary-squared\noperators. We denote these three types of operators as u, bs, and by, respectively. Mathematically,\na Symbol Layer located at the l-th level can be represented as follows:\n$\\mathcal{H}^{(l)} = \\big(\\substack{N_u \\\\ \\\\ i=1} u_i(\\mathcal{H}^{(l-1)}) \\big) \\big\\Vert \\big(\\substack{N_{bs} \\\\ \\\\ i=1} bs_i(\\mathcal{H}_i^{(l-1)}, \\mathcal{H}_j^{(l-1)})\\big) \\big\\Vert \\big(\\substack{N_{bT} \\\\ \\\\ i=1} b_T(\\mathcal{H}_i^{(l-1)}, \\mathcal{H}_j^{(l-1)})\\big)$\nwhere\n$u(h) = \\underset{i}{\\big\\Vert} u(h_i), bs(h) = \\underset{i,j}{\\big\\Vert} bs(h_i, h_j), b_T(h) = \\underset{i<j}{\\big\\Vert} b_T(h_i, h_j).$\nHere, $\\big\\Vert$ represents the concatenation operation; $N_u$, $N_{bs}$ and $N_{bT}$ denote the numbers of unary\noperators, binary-squared operators and binary-triangled operators.\nFor example, let us consider independent variables $\\{x_1,x_2\\}$ and dependent variable z, with the\ntask of finding an expression that satisfies $z = f(x_1,x_2)$. When the independent variables $\\{x_1,x_2\\}$\nare fed into a Symbol Layer, we obtain the combinatorial results (e.g., $\\{x_1, x_2, sin(x_1), sin(x_2),...,x_1+$\n$x_1,x_1+x_2, x_2+x_2,...\\}$). Notably, each value in the output tensor of the Symbol Layer corresponds\nto a distinct sub-expression. This enables PTS to compute the common subtree values just once,"}, {"title": "Parallel symbolic regression network", "content": "By stacking multiple Symbol Layers, we can then construct a PSRN as shown in Fig. 1c. This\nnetwork takes in a set of admissible base expressions, denoted by s, along with their corresponding\ndata tensor X, and is capable of utilizing GPU for rapid parallel forward computation on the data\n(e.g., typically within a mere few seconds). Based on the operators set $\\Theta$, a multitude of distinct\nsubtree values are efficiently computed layer by layer. Once the calculations of the final layer are\ncompleted, hundreds of millions of expression parse tree values $F(X) = \\{f_1(X), f_2(X),...\\}$ could\nbe obtained. These generated candidate expressions make use of intermediate results of common\nsubtrees, thus avoiding extensive redundant computations. Subsequently, these candidate expression\nvalues are used to compute the MSE defined in Eq. (1) for each $f_i (i = 1, 2, ..., |F|)$, in order to\nidentify the position of the minimum value in the error tensor. Then, the pre-generated offset tensor\n$\\Theta^{(l)}$ in each layer, containing the indices of each node's children, is leveraged to efficiently trace where\ntheir constituent sub-expressions originate and recover the top-level operators among them. This\nenables recursive inference of the optimal candidate expression(s) in a layer-wise manner. Extended\nData Fig. 1 provides a more detailed elucidation of the implementation specifics of PSRN.\nAs the number of layers increases, the required GPU memory also grows rapidly. In Model\nperformance analysis: Space complexity, the space complexity of PSRN is discussed in detail. For\nthe experiments, we use a 3-layer PSRN configuration by default. In the SR benchmark tasks,\nwe employ a 5-input PSRN with the operator set $\\Theta_{Koza} = \\{+, \\times, -, \\div, identity, sin, cos, exp, log\\}$,\nexcept for the Feynman expression set which uses a 6-input PSRN with the operator set $\\Theta_{SemiKoza} =$"}, {"title": "PSRN regressor with Monte Carlo tree search", "content": "Given the data tensor X and the corresponding base expression set s, we obtain the values\nof expressions for all parse trees with depths up to 1, namely, $\\{f(X)|d(f) \\leq 1\\}$, where I denotes\nthe number of Symbol Layers in PSRN (e.g., l = 3 for typical cases). On a graphics card with\n80GB of memory, we can allocate space for up to 5 inputs for PSRN with the operators set $\\Theta_{Koza}$.\nApart from the independent variables, the remaining input slots can be used for more complex\nsub-expressions. For example, given independent variables 11 and 12, the additional slots can\nbe employed to try other base expressions like $x_1 + x_1, x_1 \\times x_2$, and sin(x1), which enables the\nnetwork to derive deeper parse trees based on these additional inputs of the slot. Therefore, the\ntask of identifying admissible inputs can be approached as a search problem. The algorithm is\nanticipated to progressively expand the base expression set until all the slots are used, resulting\nin a terminal state $s_T = \\{x_1,x_2,g_1(x_1, x_2), g_2(x_1, x_2),g_3(x_1,x_2)\\}$. Then, a forward propagation is\nconducted by PSRN using st as the base expression set to enrich and diversify the generation of\npotential expression trees. Such an approach could facilitate the exploration of deeper expressions\n(e.g., $\\{f(X)|d(f) > 1\\}$) and broadens the spectrum of candidate expressions.\nHere, we utilize MCTS to tackle this search problem. MCTS is a decision-making algorithm\ndesigned for exploring vast combinatorial spaces represented as search trees. This approach follows\nthe best-first search principle, relying on evaluations from stochastic simulations. MCTS consists of\nfour iterative steps: selection, expansion, simulation, and backpropagation (see Fig. la) described\nin the following.\n(1) Selection: Starting from the initial base expression set so as the root node, the algorithm\niteratively chooses a new node with the highest Upper Confidence Bounds applied to Trees\n(UCT) value [38] defined as\n$UCT(s, a) = Q(s, a) + c \\sqrt{\\frac{\\text{ln } N(s)}{N(s,a)}},$\nwhere a represents the action of inserting a new base expression f' into the current node s.\n$Q(s, a)$ is the expected value associated with node s and action a. N(s) is the total number\nof visits to nodes. $N(s,a)$ is the number of visits to nodes after taking action a. cis an\nexploration parameter. Each node in the search tree denotes a distinct base expression set.\n(2) Expansion: If a node st is expandable, MCTS expands it by randomly selecting an unvisited\nnode $s_{t+1}$. During the expansion of node st, a new base expression f' is then generated by\ncombining the base expressions present in st. The new base expression set is obtained by\nadding f' to st, expressed as\n$s_{t+1} = s_t \\cup \\{f'\\}.$\n(3) Simulation: Following the expansion, if the current node is non-terminal, a playout sequence\nis initiated that continues through successive states until a terminal node, denoted by st, is\nreached. Then, the PSRN conducts a forward propagation, combining all the base expressions"}, {"title": "Coefficients tuning", "content": "When handling expressions with coefficients, we pre-select a sampling range for token constants\nand reserve multiple input slots of the network. Before excuting each PSRN forward propagation,\nseveral constants are sampled and fed into the PSRN along with the base expressions (Fig. 1b).\nThe sampled token constants can be combined with various operators within the network to form\na more diverse set of constants. For example, 1.6 and 2.3 can be combined as 1.6 + 2.3, exp(2.3),\n2.3/sin(1.6), etc. This allows the network to rapidly increase the range of coefficients it can represent\nas the number of network layers deepens. After each forward propagation, we obtain expressions\n$F = \\{f_1, f_2,..., f_k\\}$ that best fit the given data, where k is a pre-defined number, denoted as\n$F = PSRN_{const}(X, y).$\nWe parse the searched expression f, take out its coefficients as the initial values, and apply least-\nsquares (LS) optimization to derive the optimized expression $f^*$ given by\n$f^* = LS(f_i, X, y), i = 1,..., |F|.$\nIt is noted that, due to PSRN's tendency to prefer relatively complex expressions when searching\nequations containing coefficients, we introduce a linear regression step for the given independent\nvariables before each MCTS begins. This step was implemented to expedite the process of discov-\nering simpler expressions using PTS."}, {"title": "Duplicate removal mask", "content": "One major limitation of standard PSRN lies in its high demand for graphics memory. When using\nbinary operators such as addition and multiplication, the graphics memory required for each layer\ngrows quadratically with respect to the tensor dimensions of the previous layer. Therefore, reducing\nthe output dimension of the penultimate layer can significantly reduce the required graphics memory.\nWe propose a technique called Duplicate Removal Mask (DR Mask) as shown in Fig. le. Before\nthe last PSRN layer, we employ the DR Mask to remove repeated terms and generate a dense input\nset. Specifically, assuming the input variables $x_1, x_2, ..., x_m$ are independent, we extract the output"}]}