[{"title": "Results", "authors": ["Kai Ruan", "Ze-Feng Gao", "Yike Guo", "Hao Sun", "Ji-Rong Wen", "Yang Liu"], "abstract": "Symbolic regression plays a crucial role in modern scientific research thanks to its capability of discovering concise and interpretable mathematical expressions from data. A grand challenge lies in the arduous search for parsimonious and generalizable mathematical formulas, in an infinite search space, while intending to fit the training data. Existing algorithms have faced a critical bottleneck of accuracy and efficiency over a decade when handling problems of complexity, which essentially hinders the pace of applying symbolic regression for scientific exploration across interdisciplinary domains. To this end, we introduce a parallelized tree search (PTS) model to efficiently distill generic mathematical expressions from limited data. Through a series of extensive experiments, we demonstrate the superior accuracy and efficiency of PTS for equation discovery, which greatly outperforms the state-of-the-art baseline models on over 80 synthetic and experimental datasets (e.g., lifting its performance by up to 99% accuracy improvement and one-order of magnitude speed up). PTS represents a key advance in accurate and efficient data-driven discovery of symbolic, interpretable models (e.g., underlying physical laws) and marks a pivotal transition towards scalable symbolic learning.", "sections": [{"title": "Introduction", "content": "Over the centuries, scientific discovery has never departed from the use of interpretable mathematical equations or analytical models to describe complex phenomena in nature. Pioneering scientists discovered that behind many sets of empirical data in the real world lay succinct governing equations or physical laws. A famous example of this is Kepler's discovery of three laws of planetary motion using Tycho Brahe's observational data, which laid the foundation for Newton's discovery of universal gravitation. Automated extraction of these natural laws from data, as a class of typical symbolic regression (SR) problems [1], stands at the forefront of data-driven scientific exploration in natural sciences and engineering applications [2\u201310]. However, uncovering parsimonious closed-form equations that govern complex systems (e.g., nonlinear dynamics) is always challenging. Data revolution, rooted in advanced machine intelligence, has offered an alternative to tackle this issue. Although well-known regression methods [11] have been widely applied to identify the coefficients of given equations in fixed forms, they are no longer effective for natural systems where our prior knowledge of the explicit model structure is vague.\nAttempts have been made to develop evolutionary computational methods to uncover symbolic formulas that best interpret data. Unlike traditional linear/nonlinear regression methods which fit parameters to equations of a given form, SR allows free combination of mathematical operators to obtain an open-ended solution and thus makes it possible to automatically infer an analytical model from data (e.g., by simultaneously discovering both the form of equations and controlling parameters). Monte Carlo sampling [12] and evolutionary algorithms, such as genetic programming (GP), have been widely applied to distill mathematical expressions and governing laws that best fit available measurement data for nonlinear dynamical systems [1, 13\u201318]. However, this type of approach is known to scale poorly to problem's dimensionality, exhibits sensitivity to hyperparameters, and generally suffers from extensive computational cost in an extensively large search space.\nAnother remarkable breakthrough leverages sparse regression, in a restricted search space based on a pre-defined library of candidate functions, to select an optimal analytical model [19]. Such an approach quickly became one of the state-of-art methods and kindled significant enthusiasm in data-driven discovery of ordinary or partial differential equations [9, 20-26] as well as state estimation [27]. However, the success of this sparsity-promoting approach relies on a properly defined candidate function library that operates on a fit-complexity Pareto front. It is further restricted by the fact that the linear combination of candidate functions is usually insufficient to express complicated mathematical formulas. Moreover, when the library size is overly massive, this approach generally fails to hold the sparsity constraint.\nDeep learning has also been employed to uncover generic symbolic formulas from data, e.g., recurrent neural networks with risk-seeking policy gradient formulation [28], variational grammar autoencoders [29], neural-network-based graph modularity [30, 31], pre-trained transformers [32\u201334], etc. Another notable work has leveraged symbolic neural networks to distill physical laws of dynamical systems [35, 36], where commonly seen mathematical operators are employed as symbolic activation functions to establish intricate formulas via weight pruning. Nevertheless, this framework is primarily built on empirical pruning of the weights, thus exhibits sensitivity to user-defined thresholds and may fall short to produce parsimonious equations for noisy and scarce data.\nVery recently, Monte Carlo tree search (MCTS) [37, 38], which gained acclaim for powering the decision-making algorithms in AlphaGo [39], AlphaZero [40] and AlphaTensor [41], has shown a great potential in navigating the expansive search space inherent in SR [42]. This method uses stochastic simulations to meticulously evaluate the merit of each node in the search tree and has empowered several SR techniques [43, 44]. However, the conventional application of MCTS maps each node to a unique expression, which tends to impede the rapid recovery of accurate symbolic representations. This narrow mapping may limit the strategy's ability to efficiently parse through the complex space of potential expressions, thus presenting a bottleneck in the quest for swiftly uncovering underlying mathematical equations.\nSR involves evaluating a large number of complex symbolic expressions composed of various operators, variables, and constants. The operators and variables are discrete, while the value of the coefficients is continuous. The NP-hardness of a typical SR process, noted by many scholars [28, 30, 45], has been formally established [46]. This nature makes the algorithm need to traverse various possible combinations, resulting in a huge and even infinite search space and thus facing the problem of combinatorial explosion. Unfortunately, all the existing SR methods evaluate each candidate expression independently, leading to significantly low computational efficiency, and although some works considered caching evaluated expressions to prevent recomputation, they do not reuse these results as subtree values for evaluating deeper expressions. Consequently, the majority of these methods either rely on meta-heuristic search strategies, narrow down the search space based on specific assumptions, or incorporate pre-trained models to discover relatively complex expressions, which make the algorithms prone to producing specious results (e.g., local optima). Therefore, the efficiency of candidate expression evaluation is of paramount importance. By enhancing the efficiency of candidate expression evaluation, it becomes possible to design new SR algorithms that are less reliant on particular optimization methods, while increasing the likelihood of directly finding the global optima in the grand search space. Thus, we can improve the SR accuracy (in particular, the symbolic recovery rate) and, meanwhile, drastically reduce the computational time.\nTo this end, we propose a novel parallelized tree search (PTS) model to automatically distill symbolic expressions from limited data. Such a model is capable of efficiently evaluating potential expressions, thereby facilitating the exploration of hundreds of millions of candidate expressions simultaneously in parallel within a mere few seconds. In particular, we propose a parallel symbolic regression network (PSRN) as the cornerstone search engine, which (1) automatically captures common subtrees of different math expression trees for shared evaluation that avoids redundant computations, and (2) capitalizes on graphics processing unit (GPU) based parallel search that results in a notable performance boost. It is notable that, in a standard SR process for equation discovery, many candidate expressions share common subtrees, which leads to repeatedly redundant evaluations and, consequently, superfluous computation. To address this issue, we propose a strategy to automatically cache common subtrees for shared evaluation and reuse, effectively circumventing significantly redundant computation. We further execute PTS on a GPU to perform large-scale evaluation of candidate expressions in parallel, thereby augmenting the efficiency of the evaluation process. To our astonishment, the synergy of these two techniques could yield up to four orders of magnitude efficiency improvement in the context of expression evaluation. In addition, to expedite the convergence and enhance the capacity of PSRN for exploration and identification of more intricate expressions, we amalgamate it with the MCTS strategy which identifies a set of admissible base expressions as tokenized input. The remarkable efficacy and efficiency of the proposed PTS model have been demonstrated on a variety of benchmark and lab test datasets. The results show that PTS surpasses evidently several existing baseline methods, achieving higher symbolic recovery rate and efficiency."}, {"title": "Results", "content": "SR aims to discover concise, precise, and human-interpretable mathematical expressions hidden within data, offering significant value in aiding scientists to decipher the underlying meanings of unknown systems. Mathematically speaking, SR can be cast as the process of finding an expression $f: \\mathbb{R}^m \\rightarrow \\mathbb{R}$ that satisfies $y = f(X)$ given data $\\mathcal{D} = (X,y)$, where $X \\in \\mathbb{R}^{n \\times m}$, $y \\in \\mathbb{R}^{n \\times 1}$, $n$ represents the number of data samples and $m$ the number of independent variables. Here, the form of $f$ is usually constructed by a finite set of math symbols based on a given token library $\\mathcal{O} = \\{ +, -, \\times, \\div, \\sin(\\cdot), \\exp(\\cdot), ..., \\text{const.} \\}$. To make the underlying expression interpretable and parsimonious, SR algorithms are required to swiftly produce a Pareto front, balancing the error and complexity in underlying expressions. In other words, there is a need for a new SR algorithm that is both computationally efficient and exhibits a high symbolic recovery rate (e.g., also referred as accuracy; see Supplementary Note 2.4 for specific definition). However, existing SR algorithms are faced with significant bottlenecks associated with low efficiency and poor accuracy in finding complex mathematical expressions. To this end, we propose a novel parallelized tree search (PTS) model, as shown in Fig. 1, to automatically discover mathematical expressions from limited data."}, {"title": "Symbolic expression discovery with parallel evaluation", "content": "In particular, we develop a PSRN regressor as the core search engine, depicted in Fig. 1b-c, to enhance significantly the efficiency of candidate expression evaluation. The architecture of PSRN is designed parallelism-friendly, thereby enabling rapid GPU-based parallel computation (see Fig. 1c). The PSRN is empowered by automatically identifying and reusing intermediate calculation of common subtrees (see Fig. 1d).\nTo further bolster the model's discovery capability, we also integrate MCTS to locate a set of admissible base expressions as token input to PSRN for exploring deeper expressions (see Fig. 1a). Specifically, the PTS model revolves around PSRN continually activating MCTS iterations, starting with a root node with several available independent variables. During these iterations, the set of base expressions within each node are expanded by progressively incorporating more complex base expressions. When the search reaches a terminal node, the token constants are sampled and, together with the base expression set, are input into PSRN for evaluation and search of optimal symbolic expressions (see Fig. 1b). Typically, PSRN can rapidly (e.g., within a matter of seconds) identify the expression with the smallest error or a few best candidates from hundreds of millions of symbolic expressions. This represents a significant speed improvement compared to existing approaches which independently evaluate candidate expressions. Additionally, we design a duplicate removal mask step (e.g., DR Mask) for PSRN to reduce memory usage (see Fig. 1e). In the PSRN regressor, the coefficients of the most promising expressions are identified and fine-tuned based on least squares estimation (see Fig. 1f). The rewards on the given data are computed and then back-propagated for subsequent MCTS searches. As the search progresses, the Pareto front representing the optimal set of expressions is continuously updated to report the final result. Further details of the proposed model are provided in Methods."}, {"title": "Symbolic regression benchmarks", "content": "We firstly demonstrate the efficacy of PTS on recovering specified mathematical formulas given multiple benchmark problem sets (including Nguyen [47], Nguyen-c [48], R [49], Livermore [45] and Feynman [30], as described in Supplementary Note 2.1), commonly used to evaluate the performance of SR algorithms. Each SR puzzle consists of a ground truth equation, a set of available math operators, and a corresponding dataset. These benchmark data sets contains various math expressions, e.g., $x^3 + x^2 + x$ (Nguyen-1), $3.39x^3 + 2.12x^2 + 1.78x$ (Nguyen-1c), $(x + 1)^3/(x^2 - x + 1)$ (R-1), $1/3 + x + sin(x^2)$ (Livermore-1), $x_1 - x_2 + x_1^2 - x_2^2$ (Livermore-5), and $x_1x_2x_3 \\text{log} (x_5/x_4)$ (Feynman-9), which are listed in detail in Supplementary Tables S.1-S.2. Our objective is to uncover the Pareto front of optimal mathematical expressions that balance the equation complexity and error. The performance of PTS is compared with four baseline methods, e.g., symbolic physics learner (SPL) [42], neural-guided genetic programming (NGGP) [45], deep generative symbolic regression (DGSR) [34] and PySR [50]. For the Nguyen-c dataset, each model is run for at least 20 independent trials with different random seeds, while for all other puzzles, 100 distinct random seeds are utilized. We mark the successful case if the ground truth equation lies in the discovered Pareto front set.\nThe SR results of different models, in terms of the symbolic recovery rate and computational time, are depicted in Fig. 2. It can be seen that the proposed PTS method evidently outperforms the baseline methods for all the benchmark problem sets in terms of recovery rate, meanwhile maintaining the highest efficiency (e.g., expending the minimal computation time) that achieves up to two orders of magnitude speedup. The detailed results for each SR problem set are listed in Supplementary Tables S.7-S.12. An intriguing finding is that PTS achieves an impressive symbolic recovery rate of 99% on the R benchmark expressions, while the baseline models almost fail (e.g., recovery rate < 2%). We attribute this to the mathematical nature of the R benchmark expressions, which are all rational fractions like $(x + 1)^3/(x^2 - x + 1)$ (R-1). Confined to the sampling interval $x\\in [-1,1]$, the properties of the R expressions bear a high resemblance to polynomial functions, resulting in intractable local minima that essentially lead to the failure of NGGP and DGSR. This issue can be alleviated given a larger interval, e.g., $x \\in [-10,10]$, as illustrated in the R* dataset (see Supplementary Tables S.10). Notably, the performance of DGSR based on pre-trained language models stems from the prevalence of polynomial expressions in the pre-training corpora, while NGGP collapses on account of its limited search capacity in an enormously large search space. In contrast, owing to the direct and parallel evaluation of multi-million expression structures, PTS possesses the capability of accurately and efficiently recovering complex expressions."}, {"title": "Discovery of chaotic dynamics", "content": "Nonlinear dynamics is ubiquitous in nature and typically governed by a set of differential equations. Distilling such governing equations from limited observed data plays a crucial role in better understanding the fundamental mechanism of dynamics. Here, we test the proposed PTS model to discover a series of multi-dimensional autonomous chaotic dynamics (e.g., Lorenz attractor [51] and its variants [52]). The synthetic datasets of these chaotic dynamical systems are described in Supplementary Note 2.2. It is noted that we only measure the noisy trajectories (e.g., with 1% Gaussian noise added to the clean data) while determining the velocity states via smoothed numerical differentiation. We compare PTS with three pivotal baseline models (namely, Bayesian machine scientist (BMS) [12], PySR [50] and NGGP [45]) and run each model on 50 different random seeds to calculate the average recovery rate for each dataset. Here, we dropped DGSR [34] for comparison given its poor performance in the previous benchmark tests, and included BMS for comparison since it is specifically designed to perform discovery of equations with coefficients. Considering the noise effect, the criterion for successful equation recovery in this experiment is defined as follows: the discovered Pareto front covers the structure of the ground truth equation (allowing for a constant bias term). Since the dynamics of each system is governed by multiple coupled differential equations (e.g., 3~4 as shown in Supplementary Table S.3-S.6), the discovery is conducted independently to compute the average recovery rate for each equation, among which the minimum rate is taken to represent each model's overall capability.\nOur main focus herein is to investigate whether SR methods can successfully recover the underlying differential equations without any a priori knowledge under the limit of a short period of computational time (e.g., one minute). In our experiments, we set the candidate binary operators as +, -, \u00d7, and\u00f7, while the candidate unary operators include sin, cos, exp, log, tanh, cosh, abs, and sign. Fig. 3 depicts the results of discovering the closed-form governing equations for 16 chaotic dynamical systems. The experiments demonstrate that the proposed PTS approach can achieve a much higher symbolic recovery rate (see Fig. 3b), while maintaining a much lower computational cost as depicted in Fig. 3c), enabling to identify more accurately the underlying governing equations, even under noise effect, to better describe the chaotic behaviors (see Fig. 3a). This substantiates the capability and efficiency of our method on data-driven discovery of governing laws for more complex chaotic dynamical systems beyond the Lorenz attractor. More detailed results are listed in Supplementary Fig. S.3 and Supplementary Table S.13."}, {"title": "Electro-mechanical positioning system", "content": "Real-world data, replete with intricate noise and nonlinearity, may hinder the efficacy of SR algorithms. To further validate the capability of our PTS model in uncovering the governing equations for real-world dynamical systems (e.g., mechanical devices), we test its performance on a set of lab experimental data of an electro-mechanical positioning system (EMPS) [53], as shown in Fig. 4a-b. The EMPS setup is a standard configuration of a drive system used for prismatic joints in robots or machine tools. Finding the governing equation of such a system is crucial for designing better controllers and optimizing system parameters. The dataset was bifurcated into two even parts, serving as the training and testing sets, respectively. The reference governing equation is given by $M \\ddot{q} = -F_v\\dot{q} - F_c \\text{sign}(\\dot{q}) + \\tau - c$ [53], where $q$, $\\dot{q}$, and $\\ddot{q}$ represent joint position, velocity, and acceleration. Here, $\\tau$ is the joint torque/force; $c$, $M$, $F_v$, and $F_c$ are all constant parameters in the equation. In EMPS, there exists friction that dissipates the system energy. Based on this prior knowledge, we include the sign operator to model such a mechanism. Hence, the candidate math operators we use to test the SR algorithms read $\\{ +, -, \\times, \\div, \\sin, \\cos, \\exp, \\log, \\text{sign} \\}$.\nWe compare our PTS model with three pivotal baseline models, namely, PySR [50], NGGP [45], and BMS [12]. We execute each SR model 20 trials on the training dataset to ascertain the Pareto fronts. Subsequently, we select the discovered equation from the candidate expression set of each Pareto front based on the test dataset which exhibits the highest reward value delineated in Eq. (8). The reward is designed to balance the prediction error and the complexity of the discovered equation, which is crucial to derive the governing equation that is not only as consistent with the data as possible but also parsimonious and interpretable. Finally, we select among 20 trials the discovered equation with the median reward value to represent each SR model's average performance. The runtime of each model is limited to around 1.5 minutes. The results demonstrate that our PTS model achieves the best performance in successfully discovery of the underlying governing equation (see Fig. 4c-f)."}, {"title": "Governing equation of turbulent friction", "content": "Uncovering the intrinsic relationship between fluid dynamics and frictional resistance has been an enduring pursuit in the field of fluid mechanics, with implications spanning engineering, physics, and industrial applications. In particular, one fundamental challenge lies in finding a unified formula to quantitatively connect the Reynolds number ($Re$), the relative roughness $r/D$, and the friction factor $\\lambda$, based on experimental data. The Reynolds number, a dimensionless quantity, captures the balance between inertial and viscous forces within a flowing fluid, which is a key parameter in determining the flow regime, transitioning between laminar and turbulent behaviors. The relative roughness, a ratio between the size of the irregularities and the radius of the pipe, characterizes the interaction between the fluid and the surface it flows over. Such a parameter has a substantial influence on the flow's energy loss and transition to turbulence. The frictional force, arising from the interaction between the fluid and the surface, governs the dissipation of energy and is crucial in determining the efficiency of fluid transport systems. The groundbreaking work [54] dated in the 1930s stepped out the first attempt by meticulously cataloging in the lab the flow behavior under friction effect. The experimental data, commonly referred to as the Nikuradse dataset, offers insights into the complex interplay between these parameters ($Re$ and $r/D$) and the resultant friction factor ($\\lambda$) in turbulent flows. We herein test the performance of the proposed PTS model in uncovering the underlying law that governs the relationship between fluid dynamics and frictional resistance based on the Nikuradse dataset.\nWe firstly transform the data by a data collapse approach [55], a common practice used in previous studies [56, 57]. Our objective is to find a parsimonious closed-form equation given by $\\bar{\\lambda} = h(x)$, where $\\bar{\\lambda} = \\lambda^{-1/2} + 2\\text{log} (r/D)$ denotes the transformed friction factor, $x = Re\\sqrt{\\lambda/32}(D/r)$ an intermediate variable, and $h$ the target function to be discovered. We consider three baseline models for comparison, namely, PySR [50], NGGP [45], and BMS [12]. The candidate operators used in these models read $\\{ +, \\times, -, \\div, \\sin, \\cos, \\exp, \\log, \\tanh, \\cosh, \\cdot^2, \\cdot^3 \\}$. We run each SR model for 20 independent trials with different random seeds under the time budget of 1.5 minutes and choose the identified expression with the median reward as the representative result. For each trial, we report the expression with the highest reward (see Eq. (8)) on the discovered Pareto front. Fig. 4g illustrates discovered governing equations for turbulent friction. The fitting performance of each SR method and the prediction error distribution are shown in Fig. 4h-i. It can be observed that our PTS model achieves the best performance."}, {"title": "Model performance analysis", "content": "The performance of the proposed PTS model depends on several factors, including the noise level of the given data, model hyper-parameters, and whether certain modules are used. Herein, we present an analysis of these factors as well as the model efficiency and the memory footprint."}, {"title": "Model ablation study", "content": "We conduct three cases of model ablation study to evaluate the role of certain modules. First, we investigate how much improvement the use of MCTS for automatic discovery of admissible tokenized input brings. Second, we conduct a sensitivity analysis of the token constants range. Third, we investigate the extent of the benefit of using DR Mask. The results of the ablation experiments are shown in Fig. 5a-c.\nFirstly, we evaluate the symbolic recovery rate to observe the impact of replacing MCTS with random generation of input tokens for PSRN. The tests are conducted on the Nguyen-7/12, R-1/2/3 and Livermore-1/3/5/12/13/15/18/22 benchmark expressions, selected because of their higher complexity. It can be observed in Fig. 5a that the recovery rate diminishes if MCTS is removed from PTS, indicating its vital role in admissible token search that signals the way forward for expression exploration. Secondly, to investigate the sensitivity of our model to the randomly sampled token constants, we set the range to [0,1], [0,3], and [0, 10] respectively, and performed the experiments on the Nguyen-c benchmark expressions. The result in Fig. 5b shows that when the token constants are sampled from the range excluding the ground truth, the model needs to spend extra search effort to retain the accuracy (e.g., recovery rate). Last but not least, we test the efficacy of DR Mask for memory saving. The result in Fig. 5c illustrates that DR Mask is able to save the graphic memory around 50%, thus improving the capacity of the operator set (e.g., more operators could be included to represent complex expressions)."}, {"title": "Robustness to noise", "content": "To test the robustness of our PTS model to measurement noise, we conducted experiments with different levels of noise (e.g., Gaussian-type) and data availability for the target equation $f(x) = 0.3x^3 + 0.5x^2 + 2x$ where $x \\in [-1,1]$ [42]. The operators used were $\\{+,-, \\times, \\div\\}$. Since noise may cause inaccurate constant fitting, our evaluation criterion is set to consider the equation uncovered successfully as long as the correct equation form is found. The heatmap in Fig. 5d shows the effectiveness of our model, which indicates that PTS has a fairly high level of robustness against data noise and scarcity."}, {"title": "Expression search efficiency", "content": "We compare the efficiency of PTS in the context of evaluation of large-scale candidate expressions, in comparison with two brute-force methods (e.g., NumPy [58] that performs CPU-based evaluation serially, and CuPy [59] that operates on GPUs in parallel based on batches). Note that PTS possesses the capability of automatic identification and evaluation of common subtrees, while NumPy and CuPy do not have such a function. Assuming independent input variables $\\{x_1, ..., x_5\\}$ with operators $\\{ +, \\times, \\text{identity}, \\text{neg}, \\text{inv}, \\sin, \\cos, \\exp, \\log\\}$ for a maximum tree depth of 3, the complete set of generated expressions is denoted by $\\mathcal{F}$. We consider the computational time required to evaluate the loss values of all the expressions, e.g., $||y - f(X)||$, where $f\\in \\mathcal{F}$, under different sample sizes (e.g., $10^1 \\sim 10^4$ data points).\nThe result shows that PTS can quickly evaluate all the corresponding expressions, clearly surpassing the brute-force methods (see Fig. 5e). When the number of samples is less than $10^4$, the search speed of PTS is about 4 orders of magnitude faster, exhibiting an unprecedented increase in efficiency, thanks to the reuse of common subtree evaluation in parallel. Notably, when the number of samples is big, downsampling of the data is suggested in the process of uncovering the equation structure in order to take the speed advantage of PTS during forward propagation. In the coefficient estimation stage, all samples should be used. This could further increase the efficiency of PTS while maintaining accuracy."}, {"title": "Space complexity", "content": "We categorize the operators used in PTS into three types: unary, binary-squared, and binary-triangled, which are represented by $u$, $bs$, and $by$, respectively. Binary-squared operators represent non-commutative operators (e.g., - and \u00f7) depending on the order of operands, which requires $\\omega_{i-1}$ space on GPU during PSRN forward propagation (here, $\\omega_{i-1}$ represents the input size of the previous symbol layer). Binary-triangled operators represent commutative operators (e.g., + and \u00d7) or the memory-saving version of non-commutative operators that only take up $\\omega_{i-1}(\\omega_{i-1} + 1)/2$ space (e.g., the SemiSub and SemiDiv symbols, described in Methods: Symbol layer, in the Feynman benchmark which are specifically designed to save memory and support operations in only one direction).\nWith the number of operators in each category denoted by $N_u$, $N_{bs}$ and $N_{by}$, and the number of independent variables and layers of PTS denoted by $m$ and $l$, respectively, the number of floating-point values required to be stored in PTS can be analyzed. Ignoring the impact of DR Mask, there is a recursive relationship between the tensor dimension of the $(i - 1)$-th layer (e.g., $\\omega_{i-1}$) and that of the i-th layer (e.g., $\\omega_{i}$), namely,\n$\\omega_i = N_u\\omega_{i-1} + N_{bs}\\omega_{i-1} + N_{by}\\frac{\\omega_{i-1}(\\omega_{i-1} + 1)}{2} \\leq \\kappa \\omega_{i-1}^2$,\nwhere $\\kappa = N_u + N_{br} + N_{bs}$. Thus the complexity of the number of floating-point values required to be stored by an l-layer PSRN can be calculated as $O(\\kappa^{2^{l-1}}\\omega_0^2)$, where $\\omega_0$ represents the number of input slots.\nClearly, the memory consumption of PSRN increases dramatically with the number of layers. If each subtree value is a single-precision floating-point number (e.g., 32-bit), with the input dimension of 20 and operator set $\\{ +, -, \\times, \\div, \\text{identity}, \\sin, \\cos, \\exp, \\log\\}$, the required memory will reach over $10^5$ GBs when the number of layers is 3, which requires a large compute set. Hence, finding a new strategy to relax the space complexity and alleviate the memory requirement is needed to further scale up the proposed model. Fig. 5f illustrates the graphic-memory footprint of various three-layered PSRN architectures, each characterized by a different operator set and the number of input slots. While the rise in memory demands serves as a constraint, this escalation is directly tied to the scalable model's ability to evaluate a greater number of candidate expressions within a single forward pass. This result also shows that PSRN follows the scaling law (e.g., the model capacity and size scale with the number of token inputs, given the fixed number of layers). The detailed hardware settings are found in Supplementary Note 2.5."}, {"title": "Discussion", "content": "This paper introduces a new SR method, called PTS, to automatically and efficiently discover parsimonious equations to interpret the given data. In particular, we propose a PSRN architecture as the core search engine, which (1) automatically captures common subtrees of different symbolic expression trees for shared evaluation to expedite the computation, and (2) capitalizes on GPU-based parallel search with a notable performance boost. By recognizing and exploiting common subtrees in a vast number of candidate expressions, PSRN effectively bypasses the inefficiency and redundancy of independently evaluating each candidate. Such a strategy not only increases the likelihood of directly finding the global optima in the grand search space but also significantly expedites the discovery process. Furthermore, resorting to high-performance GPUs, the proposed PTS model marks a pivotal transition towards a more rapid and efficient SR paradigm. When coupled with MCTS, the model's ability to delve into complex expressions is further magnified, showing a promising potential to push the boundaries of solving more complex SR problems.\nThe efficacy of PTS has been extensively evaluated in discovering a variety of complex mathematical equations based on both synthetic and experimental datasets, including multiple benchmark problem sets (e.g., Nguyen, Nguyen-c, R, Livermore, and Feynman), nonlinear chaotic dynamics, and two datasets collected via lab experiments (e.g., EMPS and turbulent friction). We have demonstrated that PTS possesses a powerful capability in general-purpose SR, exhibiting a remarkably superior recovery rate and speed. The performance of PTS exceeds comprehensively several representative baseline models, achieving up to two orders of magnitude efficiency improvement while maintaining a much better accuracy. Moreover, even for a target equation in a very complex form, the PTS model is still capable of swiftly and reliably uncovering the ground truth directly, rather than being led astray by the ambiguous patterns present in the data. Consequently, PTS excels in discovering accurate and parsimonious expressions from very limited data in a short time of budget.\nDespite its demonstrated efficacy and potential, the PTS model is faced with several challenges that need to be addressed in the future. Firstly, the PSRN module has a rapidly increasing demand for memory while increasing the number of symbol layers (see Model performance analysis: Space complexity). Currently, a brute-force implementation of PSRN can only directly handle expressions with a parse tree depth \u2264 3 under conventional settings. Otherwise, such a method relies on MCTS, which locates advanced tokenized input, to extend PSRN's capacity to interpret deeper parse trees. This bottleneck impedes the PTS's exploration of much deeper expressions. An ongoing work to tackle this issue lies in designing a learnable score-based sampling strategy to select a finite number of optimal subtrees for the generation of candidate expressions at a deeper layer. This has the potential to deepen the symbol layers meanwhile saving the graphic memory requirement. Secondly, it should be noted that our model is of limited heuristic guidance. This arises from the fact that MCTS primarily serves to offer directional cues and steer PSRN away from re-searching the base expressions set that has already undergone forward computation. It lacks the continuous ability to analyze expressions within the Pareto front. This shortfall implies that over an extended runtime, the advantage of our PTS method compared to heuristic techniques (e.g., GP) tends to diminish. However, this sheds light on the path for our future work on incorporating a Pareto front analysis component (e.g. meta-heuristic algorithms) into PTS to further improve its performance. Lastly, the current work does not take into account our prior knowledge or dimensional constraints (e.g., the unit of a physical quantity). Another exciting ongoing work by the authors attempts to integrate the units of the input variables to identify expressions that comply with dimensional constraints. Such a strategy has the potential to conserve the graphic memory requirement and, at the same time, expedite the search. We intend to continue addressing these challenges methodically in our forthcoming research."}, {"title": "Methods", "content": "The NP-hardness of SR has been noted in existing studies [28, 30, 45], followed by a formal proof recently [46]. This implies the absence of a known solution that can be determined in polynomial time for solving SR problems across all instances, and reflects that the search space for SR is indeed vast and intricate. Almost all the existing SR methods involve a common procedure, which is to assess the quality of candidate expressions $\\mathcal{F} = \\{ f_1, f_2, ... \\}$ based on the given data $\\mathcal{D} = (X,y)$,"}, {"title": "Symbol layer", "content": "where $X \\in \\mathbb{R"}, {"as": "n$\\text{MSE"}, "frac{1}{n} \\sum_{i=1}^n || f(X) - y ||^2$\nDuring the search process, a large number of candidate expressions need to be evaluated, while the available operators and variables are limited, leading to the inevitable existence of vast repeated sub-expressions. Existing methods evaluate candidate expressions sequentially and independently. As a result, the common intermediate expressions are repeatedly calculated, leading to significant computational burden (see Supplementary Fig. S.1). By reducing the amount of repeated computation, the search process can be significantly accelerated.\nA mathematical expression can be equivalently represented as a parse tree [13], where the internal nodes denote mathematical operators and the leaf nodes the variables and constants. The crux of the aforementioned computational issue lies in the absence of temporary storage for subtree values and parallel evaluation. Consequently, the common subtrees in different candidate expressions are repeatedly evaluated, resulting in significant computational wastage.\nTo this end, we introduce the concept of Symbol Layer (see Fig. 1c), which consists of a series of mathematical operators. The Symbol Layer serves to```json\n{\n  \"title\": \"Discovering symbolic expressions with parallelized tree search\",\n  \"authors\": [\n    \"Kai Ruan\",\n    \"Ze-Feng Gao\",\n    \"Yike Guo\",\n    \"Hao Sun\",\n    \"Ji-Rong Wen\",\n    \"Yang Liu\"\n  ],\n  \"abstract\":", "Symbolic regression plays a crucial role in modern scientific research thanks to its capability of discovering concise and interpretable mathematical expressions from data. A grand challenge lies in the arduous search for parsimonious and generalizable mathematical formulas, in an infinite search space, while intending to fit the training data. Existing algorithms have faced a critical bottleneck of accuracy and efficiency over a decade when handling problems of complexity, which essentially hinders the pace of applying symbolic regression for scientific exploration across interdisciplinary domains. To this end, we introduce a parallelized tree search (PTS) model to efficiently distill generic mathematical expressions from limited data. Through a series of extensive experiments, we demonstrate the superior accuracy and efficiency of PTS for equation discovery, which greatly outperforms the state-of-the-art baseline models on over 80 synthetic and experimental datasets (e.g., lifting its performance by up to 99% accuracy improvement and one-order of magnitude speed up). PTS represents a key advance in accurate and efficient data-driven discovery of symbolic, interpretable models (e.g., underlying physical laws) and marks a pivotal transition towards scalable symbolic learning.", "sections\": [\n    {\n      \"title\": \"Introduction", "content\": \"Over the centuries, scientific discovery has never departed from the use of interpretable mathematical equations or analytical models to describe complex phenomena in nature. Pioneering scientists discovered that behind many sets of empirical data in the real world lay succinct governing equations or physical laws. A famous example of this is Kepler's discovery of three laws of planetary motion using Tycho Brahe's observational data, which laid the foundation for Newton's discovery of universal gravitation. Automated extraction of these natural laws from data, as a class of typical symbolic regression (SR) problems [1], stands at the forefront of data-driven scientific exploration in natural sciences and engineering applications [2\u201310]. However, uncovering parsimonious closed-form equations that govern complex systems (e.g., nonlinear dynamics) is always challenging. Data revolution, rooted in advanced machine intelligence, has offered an alternative to tackle this issue. Although well-known regression methods [11] have been widely applied to identify the coefficients of given equations in fixed forms, they are no longer effective for natural systems where our prior knowledge of the explicit model structure is vague.\nAttempts have been made to develop evolutionary computational methods to uncover symbolic formulas that best interpret data. Unlike traditional linear/nonlinear regression methods which fit parameters to equations of a given form, SR allows free combination of mathematical operators to obtain an open-ended solution and thus makes it possible to automatically infer an analytical model from data (e.g., by simultaneously discovering both the form of equations and controlling parameters). Monte Carlo sampling [12] and evolutionary algorithms, such as genetic programming (GP), have been widely applied to distill mathematical expressions and governing laws that best fit available measurement data for nonlinear dynamical systems [1, 13\u201318]. However, this type of approach is known to scale poorly to problem's dimensionality, exhibits sensitivity to hyperparameters, and generally suffers from extensive computational cost in an extensively large search space.\nAnother remarkable breakthrough leverages sparse regression, in a restricted search space based on a pre-defined library of candidate functions, to select an optimal analytical model [19]. Such an approach quickly became one of the state-of-art methods and kindled significant enthusiasm in data-driven discovery of ordinary or partial differential equations [9, 20-26] as well as state estimation [27]. However, the success of this sparsity-promoting approach relies on a properly defined candidate function library that operates on a fit-complexity Pareto front. It is further restricted by the fact that the linear combination of candidate functions is usually insufficient to express complicated mathematical formulas. Moreover, when the library size is overly massive, this approach generally fails to hold the sparsity constraint.\nDeep learning has also been employed to uncover generic symbolic formulas from data, e.g., recurrent neural networks with risk-seeking policy gradient formulation [28], variational grammar autoencoders [29], neural-network-based graph modularity [30, 31], pre-trained transformers [32\u201334], etc. Another notable work has leveraged symbolic neural networks to distill physical laws of dynamical systems [35, 36], where commonly seen mathematical operators are employed as symbolic activation functions to establish intricate formulas via weight pruning. Nevertheless, this framework is primarily built on empirical pruning of the weights, thus exhibits sensitivity to user-defined thresholds and may fall short to produce parsimonious equations for noisy and scarce data.\nVery recently, Monte Carlo tree search (MCTS) [37, 38], which gained acclaim for powering the decision-making algorithms in AlphaGo [39], AlphaZero [40] and AlphaTensor [41], has shown a great potential in navigating the expansive search space inherent in SR [42]. This method uses stochastic simulations to meticulously evaluate the merit of each node in the search tree and has empowered several SR techniques [43, 44]. However, the conventional application of MCTS maps each node to a unique expression, which tends to impede the rapid recovery of accurate symbolic representations. This narrow mapping may limit the strategy's ability to efficiently parse through the complex space of potential expressions, thus presenting a bottleneck in the quest for swiftly uncovering underlying mathematical equations.\nSR involves evaluating a large number of complex symbolic expressions composed of various operators, variables, and constants. The operators and variables are discrete, while the value of the coefficients is continuous. The NP-hardness of a typical SR process, noted by many scholars [28, 30, 45], has been formally established [46]. This nature makes the algorithm need to traverse various possible combinations, resulting in a huge and even infinite search space and thus facing the problem of combinatorial explosion. Unfortunately, all the existing SR methods evaluate each candidate expression independently, leading to significantly low computational efficiency, and although some works considered caching evaluated expressions to prevent recomputation, they do not reuse these results as subtree values for evaluating deeper expressions. Consequently, the majority of these methods either rely on meta-heuristic search strategies, narrow down the search space based on specific assumptions, or incorporate pre-trained models to discover relatively complex expressions, which make the algorithms prone to producing specious results (e.g., local optima). Therefore, the efficiency of candidate expression evaluation is of paramount importance. By enhancing the efficiency of candidate expression evaluation, it becomes possible to design new SR algorithms that are less reliant on particular optimization methods, while increasing the likelihood of directly finding the global optima in the grand search space. Thus, we can improve the SR accuracy (in particular, the symbolic recovery rate) and, meanwhile, drastically reduce the computational time.\nTo this end, we propose a novel parallelized tree search (PTS) model to automatically distill symbolic expressions from limited data. Such a model is capable of efficiently evaluating potential expressions, thereby facilitating the exploration of hundreds of millions of candidate expressions simultaneously in parallel within a mere few seconds. In particular, we propose a parallel symbolic regression network (PSRN) as the cornerstone search engine, which (1) automatically captures common subtrees of different math expression trees for shared evaluation that avoids redundant computations, and (2) capitalizes on graphics processing unit (GPU) based parallel search that results in a notable performance boost. It is notable that, in a standard SR process for equation discovery, many candidate expressions share common subtrees, which leads to repeatedly redundant evaluations and, consequently, superfluous computation. To address this issue, we propose a strategy to automatically cache common subtrees for shared evaluation and reuse, effectively circumventing significantly redundant computation. We further execute PTS on a GPU to perform large-scale evaluation of candidate expressions in parallel, thereby augmenting the efficiency of the evaluation process. To our astonishment, the synergy of these two techniques could yield up to four orders of magnitude efficiency improvement in the context of expression evaluation. In addition, to expedite the convergence and enhance the capacity of PSRN for exploration and identification of more intricate expressions, we amalgamate it with the MCTS strategy which identifies a set of admissible base expressions as tokenized input. The remarkable efficacy and efficiency of the proposed PTS model have been demonstrated on a variety of benchmark and lab test datasets. The results show that PTS surpasses evidently several existing baseline methods, achieving higher symbolic recovery rate and efficiency."], "content": "SR aims to discover concise, precise, and human-interpretable mathematical expressions hidden within data, offering significant value in aiding scientists to decipher the underlying meanings of unknown systems. Mathematically speaking, SR can be cast as the process of finding an expression $f: \\mathbb{R}^m \\rightarrow \\mathbb{R}$ that satisfies $y = f(X)$ given data $\\mathcal{D} = (X,y)$, where $X \\in \\mathbb{R}^{n \\times m}$, $y \\in \\mathbb{R}^{n \\times 1}$, $n$ represents the number of data samples and $m$ the number of independent variables. Here, the form of $f$ is usually constructed by a finite set of math symbols based on a given token library $\\mathcal{O} = \\{ +, -, \\times, \\div, \\sin(\\cdot), \\exp(\\cdot), ..., \\text{const.} \\}$. To make the underlying expression interpretable and parsimonious, SR algorithms are required to swiftly produce a Pareto front, balancing the error and complexity in underlying expressions. In other words, there is a need for a new SR algorithm that is both computationally efficient and exhibits a high symbolic recovery rate (e.g., also referred as accuracy; see Supplementary Note 2.4 for specific definition). However, existing SR algorithms are faced with significant bottlenecks associated with low efficiency and poor accuracy in finding complex mathematical expressions. To this end, we propose a novel parallelized tree search (PTS) model, as shown in Fig. 1, to automatically discover mathematical expressions from limited data."}, {"title": "Symbolic expression discovery with parallel evaluation", "content": "In particular, we develop a PSRN regressor as the core search engine, depicted in Fig. 1b-c, to enhance significantly the efficiency of candidate expression evaluation. The architecture of PSRN is designed parallelism-friendly, thereby enabling rapid GPU-based parallel computation (see Fig. 1c). The PSRN is empowered by automatically identifying and reusing intermediate calculation of common subtrees (see Fig. 1d).\nTo further bolster the model's discovery capability, we also integrate MCTS to locate a set of admissible base expressions as token input to PSRN for exploring deeper expressions (see Fig. 1a). Specifically, the PTS model revolves around PSRN continually activating MCTS iterations, starting with a root node with several available independent variables. During these iterations, the set of base expressions within each node are expanded by progressively incorporating more complex base expressions. When the search reaches a terminal node, the token constants are sampled and, together with the base expression set, are input into PSRN for evaluation and search of optimal symbolic expressions (see Fig. 1b). Typically, PSRN can rapidly (e.g., within a matter of seconds) identify the expression with the smallest error or a few best candidates from hundreds of millions of symbolic expressions. This represents a significant speed improvement compared to existing approaches which independently evaluate candidate expressions. Additionally, we design a duplicate removal mask step (e.g., DR Mask) for PSRN to reduce memory usage (see Fig. 1e). In the PSRN regressor, the coefficients of the most promising expressions are identified and fine-tuned based on least squares estimation (see Fig. 1f). The rewards on the given data are computed and then back-propagated for subsequent MCTS searches. As the search progresses, the Pareto front representing the optimal set of expressions is continuously updated to report the final result. Further details of the proposed model are provided in Methods."}, {"title": "Symbolic regression benchmarks", "content": "We firstly demonstrate the efficacy of PTS on recovering specified mathematical formulas given multiple benchmark problem sets (including Nguyen [47], Nguyen-c [48], R [49], Livermore [45] and Feynman [30], as described in Supplementary Note 2.1), commonly used to evaluate the performance of SR algorithms. Each SR puzzle consists of a ground truth equation, a set of available math operators, and a corresponding dataset. These benchmark data sets contains various math expressions, e.g., $x^3 + x^2 + x$ (Nguyen-1), $3.39x^3 + 2.12x^2 + 1.78x$ (Nguyen-1c), $(x + 1)^3/(x^2 - x + 1)$ (R-1), $1/3 + x + \\sin(x^2)$ (Livermore-1), $x_1 - x_2 + x_1^2 - x_2^2$ (Livermore-5), and $x_1x_2x_3 \\text{log} (x_5/x_4)$ (Feynman-9), which are listed in detail in Supplementary Tables S.1-S.2. Our objective is to uncover the Pareto front of optimal mathematical expressions that balance the equation complexity and error. The performance of PTS is compared with four baseline methods, e.g., symbolic physics learner (SPL) [42], neural-guided genetic programming (NGGP) [45], deep generative symbolic regression (DGSR) [34] and PySR [50]. For the Nguyen-c dataset, each model is run for at least 20 independent trials with different random seeds, while for all other puzzles, 100 distinct random seeds are utilized. We mark the successful case if the ground truth equation lies in the discovered Pareto front set.\nThe SR results of different models, in terms of the symbolic recovery rate and computational time, are depicted in Fig. 2. It can be seen that the proposed PTS method evidently outperforms the baseline methods for all the benchmark problem sets in terms of recovery rate, meanwhile maintaining the highest efficiency (e.g., expending the minimal computation time) that achieves up to two orders of magnitude speedup. The detailed results for each SR problem set are listed in Supplementary Tables S.7-S.12. An intriguing finding is that PTS achieves an impressive symbolic recovery rate of 99% on the R benchmark expressions, while the baseline models almost fail (e.g., recovery rate < 2%). We attribute this to the mathematical nature of the R benchmark expressions, which are all rational fractions like $(x + 1)^3/(x^2 - x + 1)$ (R-1). Confined to the sampling interval $x\\in [-1,1]$, the properties of the R expressions bear a high resemblance to polynomial functions, resulting in intractable local minima that essentially lead to the failure of NGGP and DGSR. This issue can be alleviated given a larger interval, e.g., $x \\in [-10,10]$, as illustrated in the R* dataset (see Supplementary Tables S.10). Notably, the performance of DGSR based on pre-trained language models stems from the prevalence of polynomial expressions in the pre-training corpora, while NGGP collapses on account of its limited search capacity in an enormously large search space. In contrast, owing to the direct and parallel evaluation of multi-million expression structures, PTS possesses the capability of accurately and efficiently recovering complex expressions."}, {"title": "Discovery of chaotic dynamics", "content": "Nonlinear dynamics is ubiquitous in nature and typically governed by a set of differential equations. Distilling such governing equations from limited observed data plays a crucial role in better understanding the fundamental mechanism of dynamics. Here, we test the proposed PTS model to discover a series of multi-dimensional autonomous chaotic dynamics (e.g., Lorenz attractor [51] and its variants [52]). The synthetic datasets of these chaotic dynamical systems are described in Supplementary Note 2.2. It is noted that we only measure the noisy trajectories (e.g., with 1% Gaussian noise added to the clean data) while determining the velocity states via smoothed numerical differentiation. We compare PTS with three pivotal baseline models (namely, Bayesian machine scientist (BMS) [12], PySR [50] and NGGP [45]) and run each model on 50 different random seeds to calculate the average recovery rate for each dataset. Here, we dropped DGSR [34] for comparison given its poor performance in the previous benchmark tests, and included BMS for comparison since it is specifically designed to perform discovery of equations with coefficients. Considering the noise effect, the criterion for successful equation recovery in this experiment is defined as follows: the discovered Pareto front covers the structure of the ground truth equation (allowing for a constant bias term). Since the dynamics of each system is governed by multiple coupled differential equations (e.g., 3~4 as shown in Supplementary Table S.3-S.6), the discovery is conducted independently to compute the average recovery rate for each equation, among which the minimum rate is taken to represent each model's overall capability.\nOur main focus herein is to investigate whether SR methods can successfully recover the underlying differential equations without any a priori knowledge under the limit of a short period of computational time (e.g., one minute). In our experiments, we set the candidate binary operators as +, -, \u00d7, and\u00f7, while the candidate unary operators include sin, cos, exp, log, tanh, cosh, abs, and sign. Fig. 3 depicts the results of discovering the closed-form governing equations for 16 chaotic dynamical systems. The experiments demonstrate that the proposed PTS approach can achieve a much higher symbolic recovery rate (see Fig. 3b), while maintaining a much lower computational cost as depicted in Fig. 3c), enabling to identify more accurately the underlying governing equations, even under noise effect, to better describe the chaotic behaviors (see Fig. 3a). This substantiates the capability and efficiency of our method on data-driven discovery of governing laws for more complex chaotic dynamical systems beyond the Lorenz attractor. More detailed results are listed in Supplementary Fig. S.3 and Supplementary Table S.13."}, {"title": "Electro-mechanical positioning system", "content": "Real-world data, replete with intricate noise and nonlinearity, may hinder the efficacy of SR algorithms. To further validate the capability of our PTS model in uncovering the governing equations for real-world dynamical systems (e.g., mechanical devices), we test its performance on a set of lab experimental data of an electro-mechanical positioning system (EMPS) [53], as shown in Fig. 4a-b. The EMPS setup is a standard configuration of a drive system used for prismatic joints in robots or machine tools. Finding the governing equation of such a system is crucial for designing better controllers and optimizing system parameters. The dataset was bifurcated into two even parts, serving as the training and testing sets, respectively. The reference governing equation is given by $M \\ddot{q} = -F_v\\dot{q} - F_c \\text{sign}(\\dot{q}) + \\tau - c$ [53], where $q$, $\\dot{q}$, and $\\ddot{q}$ represent joint position, velocity, and acceleration. Here, $\\tau$ is the joint torque/force; $c$, $M$, $F_v$, and $F_c$ are all constant parameters in the equation. In EMPS, there exists friction that dissipates the system energy. Based on this prior knowledge, we include the sign operator to model such a mechanism. Hence, the candidate math operators we use to test the SR algorithms read $\\{ +, -, \\times, \\div, \\sin, \\cos, \\exp, \\log, \\text{sign} \\}$.\nWe compare our PTS model with three pivotal baseline models, namely, PySR [50], NGGP [45], and BMS [12]. We execute each SR model 20 trials on the training dataset to ascertain the Pareto fronts. Subsequently, we select the discovered equation from the candidate expression set of each Pareto front based on the test dataset which exhibits the highest reward value delineated in Eq. (8). The reward is designed to balance the prediction error and the complexity of the discovered equation, which is crucial to derive the governing equation that is not only as consistent with the data as possible but also parsimonious and interpretable. Finally, we select among 20 trials the discovered equation with the median reward value to represent each SR model's average performance. The runtime of each model is limited to around 1.5 minutes. The results demonstrate that our PTS model achieves the best performance in successfully discovery of the underlying governing equation (see Fig. 4c-f)."}, {"title": "Governing equation of turbulent friction", "content": "Uncovering the intrinsic relationship between fluid dynamics and frictional resistance has been an enduring pursuit in the field of fluid mechanics, with implications spanning engineering, physics, and industrial applications. In particular, one fundamental challenge lies in finding a unified formula to quantitatively connect the Reynolds number ($Re$), the relative roughness $r/D$, and the friction factor $\\lambda$, based on experimental data. The Reynolds number, a dimensionless quantity, captures the balance between inertial and viscous forces within a flowing fluid, which is a key parameter in determining the flow regime, transitioning between laminar and turbulent behaviors. The relative roughness, a ratio between the size of the irregularities and the radius of the pipe, characterizes the interaction between the fluid and the surface it flows over. Such a parameter has a substantial influence on the flow's energy loss and transition to turbulence. The frictional force, arising from the interaction between the fluid and the surface, governs the dissipation of energy and is crucial in determining the efficiency of fluid transport systems. The groundbreaking work [54] dated in the 1930s stepped out the first attempt by meticulously cataloging in the lab the flow behavior under friction effect. The experimental data, commonly referred to as the Nikuradse dataset, offers insights into the complex interplay between these parameters ($Re$ and $r/D$) and the resultant friction factor ($\\lambda$) in turbulent flows. We herein test the performance of the proposed PTS model in uncovering the underlying law that governs the relationship between fluid dynamics and frictional resistance based on the Nikuradse dataset.\nWe firstly transform the data by a data collapse approach [55], a common practice used in previous studies [56, 57]. Our objective is to find a parsimonious closed-form equation given by $\\bar{\\lambda} = h(x)$, where $\\bar{\\lambda} = \\lambda^{-1/2} + 2\\text{log} (r/D)$ denotes the transformed friction factor, $x = Re\\sqrt{\\lambda/32}(D/r)$ an intermediate variable, and $h$ the target function to be discovered. We consider three baseline models for comparison, namely, PySR [50], NGGP [45], and BMS [12]. The candidate operators used in these models read $\\{ +, \\times, -, \\div, \\sin, \\cos, \\exp, \\log, \\tanh, \\cosh, \\cdot^2, \\cdot^3 \\}$. We run each SR model for 20 independent trials with different random seeds under the time budget of 1.5 minutes and choose the identified expression with the median reward as the representative result. For each trial, we report the expression with the highest reward (see Eq. (8)) on the discovered Pareto front. Fig. 4g illustrates discovered governing equations for turbulent friction. The fitting performance of each SR method and the prediction error distribution are shown in Fig. 4h-i. It can be observed that our PTS model achieves the best performance."}, {"title": "Model performance analysis", "content": "The performance of the proposed PTS model depends on several factors, including the noise level of the given data, model hyper-parameters, and whether certain modules are used. Herein, we present an analysis of these factors as well as the model efficiency and the memory footprint."}, {"title": "Model ablation study", "content": "We conduct three cases of model ablation study to evaluate the role of certain modules. First, we investigate how much improvement the use of MCTS for automatic discovery of admissible tokenized input brings. Second, we conduct a sensitivity analysis of the token constants range. Third, we investigate the extent of the benefit of using DR Mask. The results of the ablation experiments are shown in Fig. 5a-c.\nFirstly, we evaluate the symbolic recovery rate to observe the impact of replacing MCTS with random generation of input tokens for PSRN. The tests are conducted on the Nguyen-7/12, R-1/2/3 and Livermore-1/3/5/12/13/15/18/22 benchmark expressions, selected because of their higher complexity. It can be observed in Fig. 5a that the recovery rate diminishes if MCTS is removed from PTS, indicating its vital role in admissible token search that signals the way forward for expression exploration. Secondly, to investigate the sensitivity of our model to the randomly sampled token constants, we set the range to [0,1], [0,3], and [0, 10] respectively, and performed the experiments on the Nguyen-c benchmark expressions. The result in Fig. 5b shows that when the token constants are sampled from the range excluding the ground truth, the model needs to spend extra search effort to retain the accuracy (e.g., recovery rate). Last but not least, we test the efficacy of DR Mask for memory saving. The result in Fig. 5c illustrates that DR Mask is able to save the graphic memory around 50%, thus improving the capacity of the operator set (e.g., more operators could be included to represent complex expressions)."}, {"title": "Robustness to noise", "content": "To test the robustness of our PTS model to measurement noise, we conducted experiments with different levels of noise (e.g., Gaussian-type) and data availability for the target equation $f(x) = 0.3x^3 + 0.5x^2 + 2x$ where $x \\in [-1,1]$ [42]. The operators used were $\\{+,-, \\times, \\div\\}$. Since noise may cause inaccurate constant fitting, our evaluation criterion is set to consider the equation uncovered successfully as long as the correct equation form is found. The heatmap in Fig. 5d shows the effectiveness of our model, which indicates that PTS has a fairly high level of robustness against data noise and scarcity."}, {"title": "Expression search efficiency", "content": "We compare the efficiency of PTS in the context of evaluation of large-scale candidate expressions, in comparison with two brute-force methods (e.g., NumPy [58] that performs CPU-based evaluation serially, and CuPy [59] that operates on GPUs in parallel based on batches). Note that PTS possesses the capability of automatic identification and evaluation of common subtrees, while NumPy and CuPy do not have such a function. Assuming independent input variables $\\{x_1, ..., x_5\\}$ with operators $\\{ +, \\times, \\text{identity}, \\text{neg}, \\text{inv}, \\sin, \\cos, \\exp, \\log\\}$ for a maximum tree depth of 3, the complete set of generated expressions is denoted by $\\mathcal{F}$. We consider the computational time required to evaluate the loss values of all the expressions, e.g., $||y - f(X)||$, where $f\\in \\mathcal{F}$, under different sample sizes (e.g., $10^1 \\sim 10^4$ data points).\nThe result shows that PTS can quickly evaluate all the corresponding expressions, clearly surpassing the brute-force methods (see Fig. 5e). When the number of samples is less than $10^4$, the search speed of PTS is about 4 orders of magnitude faster, exhibiting an unprecedented increase in efficiency, thanks to the reuse of common subtree evaluation in parallel. Notably, when the number of samples is big, downsampling of the data is suggested in the process of uncovering the equation structure in order to take the speed advantage of PTS during forward propagation. In the coefficient estimation stage, all samples should be used. This could further increase the efficiency of PTS while maintaining accuracy."}, {"title": "Space complexity", "content": "We categorize the operators used in PTS into three types: unary, binary-squared, and binary-triangled, which are represented by $u$, $bs$, and $by$, respectively. Binary-squared operators represent non-commutative operators (e.g., - and \u00f7) depending on the order of operands, which requires $\\omega_{i-1}$ space on GPU during PSRN forward propagation (here, $\\omega_{i-1}$ represents the input size of the previous symbol layer). Binary-triangled operators represent commutative operators (e.g., + and \u00d7) or the memory-saving version of non-commutative operators that only take up $\\omega_{i-1}(\\omega_{i-1} + 1)/2$ space (e.g., the SemiSub and SemiDiv symbols, described in Methods: Symbol layer, in the Feynman benchmark which are specifically designed to save memory and support operations in only one direction).\nWith the number of operators in each category denoted by $N_u$, $N_{bs}$ and $N_{by}$, and the number of independent variables and layers of PTS denoted by $m$ and $l$, respectively, the number of floating-point values required to be stored in PTS can be analyzed. Ignoring the impact of DR Mask, there is a recursive relationship between the tensor dimension of the $(i - 1)$-th layer (e.g., $\\omega_{i-1}$) and that of the i-th layer (e.g., $\\omega_{i}$), namely,\n$\\omega_i = N_u\\omega_{i-1} + N_{bs}\\omega_{i-1} + N_{by}\\frac{\\omega_{i-1}(\\omega_{i-1} + 1)}{2} \\leq \\kappa \\omega_{i-1}^2$,\nwhere $\\kappa = N_u + N_{br} + N_{bs}$. Thus the complexity of the number of floating-point values required to be stored by an l-layer PSRN can be calculated as $O(\\kappa^{2^{l-1}}\\omega_0^2)$, where $\\omega_0$ represents the number of input slots.\nClearly, the memory consumption of PSRN increases dramatically with the number of layers. If each subtree value is a single-precision floating-point number (e.g., 32-bit), with the input dimension of 20 and operator set $\\{ +, -, \\times, \\div, \\text{identity}, \\sin, \\cos, \\exp, \\log\\}$, the required memory will reach over $10^5$ GBs when the number of layers is 3, which requires a large compute set. Hence, finding a new strategy to relax the space complexity and alleviate the memory requirement is needed to further scale up the proposed model. Fig. 5f illustrates the graphic-memory footprint of various three-layered PSRN architectures, each characterized by a different operator set and the number of input slots. While the rise in memory demands serves as a constraint, this escalation is directly tied to the scalable model's ability to evaluate a greater number of candidate expressions within a single forward pass. This result also shows that PSRN follows the scaling law (e.g., the model capacity and size scale with the number of token inputs, given the fixed number of layers). The detailed hardware settings are found in Supplementary Note 2.5."}, {"title": "Discussion", "content": "This paper introduces a new SR method, called PTS, to automatically and efficiently discover parsimonious equations to interpret the given data. In particular, we propose a PSRN architecture as the core search engine, which (1) automatically captures common subtrees of different symbolic expression trees for shared evaluation to expedite the computation, and (2) capitalizes on GPU-based parallel search with a notable performance boost. By recognizing and exploiting common subtrees in a vast number of candidate expressions, PSRN effectively bypasses the inefficiency and redundancy of independently evaluating each candidate. Such a strategy not only increases the likelihood of directly finding the global optima in the grand search space but also significantly expedites the discovery process. Furthermore, resorting to high-performance GPUs, the proposed PTS model marks a pivotal transition towards a more rapid and efficient SR paradigm. When coupled with MCTS, the model's ability to delve into complex expressions is further magnified, showing a promising potential to push the boundaries of solving more complex SR problems.\nThe efficacy of PTS has been extensively evaluated in discovering a variety of complex mathematical equations based on both synthetic and experimental datasets, including multiple benchmark problem sets (e.g., Nguyen, Nguyen-c, R, Livermore, and Feynman), nonlinear chaotic dynamics, and two datasets collected via lab experiments (e.g., EMPS and turbulent friction). We have demonstrated that PTS possesses a powerful capability in general-purpose SR, exhibiting a remarkably superior recovery rate and speed. The performance of PTS exceeds comprehensively several representative baseline models, achieving up to two orders of magnitude efficiency improvement while maintaining a much better accuracy. Moreover, even for a target equation in a very complex form, the PTS model is still capable of swiftly and reliably uncovering the ground truth directly, rather than being led astray by the ambiguous patterns present in the data. Consequently, PTS excels in discovering accurate and parsimonious expressions from very limited data in a short time of budget.\nDespite its demonstrated efficacy and potential, the PTS model is faced with several challenges that need to be addressed in the future. Firstly, the PSRN module has a rapidly increasing demand for memory while increasing the number of symbol layers (see Model performance analysis: Space complexity). Currently, a brute-force implementation of PSRN can only directly handle expressions with a parse tree depth \u2264 3 under conventional settings. Otherwise, such a method relies on MCTS, which locates advanced tokenized input, to extend PSRN's capacity to interpret deeper parse trees. This bottleneck impedes the PTS's exploration of much deeper expressions. An ongoing work to tackle this issue lies in designing a learnable score-based sampling strategy to select a finite number of optimal subtrees for the generation of candidate expressions at a deeper layer. This has the potential to deepen the symbol layers meanwhile saving the graphic memory requirement. Secondly, it should be noted that our model is of limited heuristic guidance. This arises from the fact that MCTS primarily serves to offer directional cues and steer PSRN away from re-searching the base expressions set that has already undergone forward computation. It lacks the continuous ability to analyze expressions within the Pareto front. This shortfall implies that over an extended runtime, the advantage of our PTS method compared to heuristic techniques (e.g., GP) tends to diminish. However, this sheds light on the path for our future work on incorporating a Pareto front analysis component (e.g. meta-heuristic algorithms) into PTS to further improve its performance. Lastly, the current work does not take into account our prior knowledge or dimensional constraints (e.g., the unit of a physical quantity). Another exciting ongoing work by the authors attempts to integrate the units of the input variables to identify expressions that comply with dimensional constraints. Such a strategy has the potential to conserve the graphic memory requirement and, at the same time, expedite the search. We intend to continue addressing these challenges methodically in our forthcoming research."}, {"title": "Methods", "content": "The NP-hardness of SR has been noted in existing studies [28, 30, 45], followed by a formal proof recently [46]. This implies the absence of a known solution that can be determined in polynomial time for solving SR problems across all instances, and reflects that the search space for SR is indeed vast and intricate. Almost all the existing SR methods involve a common procedure, which is to assess the quality of candidate expressions $\\mathcal{F} = \\{ f_1, f_2, ... \\}$ based on the given data $\\mathcal{D} = (X,y)$,"}, {"title": "Symbol layer", "content": "where $X \\in \\mathbb{R"}, {"as": "n$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n || f(X) - y ||^2$\nDuring the search process, a large number of candidate expressions need to be evaluated, while the available operators and variables are limited, leading to the inevitable existence of vast repeated sub-expressions. Existing methods evaluate candidate expressions sequentially and independently. As a result, the common intermediate expressions are repeatedly calculated, leading to significant computational burden (see Supplementary Fig. S.1). By reducing the amount of repeated computation, the search process can be significantly accelerated.\nA mathematical expression can be equivalently represented as a parse tree [13], where the internal nodes denote mathematical operators and the leaf nodes the variables and constants. The crux of the aforementioned computational issue lies in the absence of temporary storage for subtree values and parallel evaluation. Consequently, the common subtrees in different candidate expressions are repeatedly evaluated, resulting in significant computational wastage.\nTo this end, we introduce the concept of Symbol Layer (see Fig. 1c), which consists of a series of mathematical operators. The Symbol Layer serves"}]