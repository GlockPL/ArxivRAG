{"title": "Traceable LLM-based validation of statements in knowledge graphs", "authors": ["Daniel Adam", "Tom\u00e1\u0161 Kliegr"], "abstract": "This article presents a method for verifying RDF triples using LLMs, with an emphasis on providing traceable arguments. Because the LLMs cannot currently reliably identify the origin of the information used to construct the response to the user query, our approach is to avoid using internal LLM factual knowledge altogether. Instead, verified RDF statements are compared to chunks of external documents retrieved through a web search or Wikipedia. To assess the possible application of this workflow on biosciences content, we evaluated 1,719 positive statements from the BioRED dataset and the same number of newly generated negative statements. The resulting precision is 88%, and recall is 44%. This indicates that the method requires human oversight. We demonstrate the method on Wikidata, where a SPARQL query is used to automatically retrieve statements needing verification. Overall, the results suggest that LLMs could be used for large-scale verification of statements in KGs, a task previously unfeasible due to human annotation costs.", "sections": [{"title": "1. Introduction", "content": "Recent research has shown that many natural language processing (NLP) tasks, which previously required dedicated algorithmic approaches and model training, such as relation extraction or named entity recognition, can be performed by LLMs in a zero-shot manner. In this article, we investigate using zero-shot LLMs for validating statements in knowledge graphs. This task differs from other NLP tasks in that it emphasises the correctness of the output and particularly its traceability: an integral part of the validation process is the provision of information on why the verified statement is true, providing a reference to a specific part of a document addressing the topic in question. Despite the advances in LLM technology, between 50% to 90% of LLM responses are not fully supported by the sources provided (Wu et al, 2024). This article presents an intrinsically traceable approach to verification, which is based on avoiding using internal LLM factual knowledge and instead uses the LLM to find justification for the statement in the supplied text snippet. This method directly generates references, eliminating the error-prone phase of attribution of the response of the LLM to the training data (Lee et al, 2024)."}, {"title": "2. Related work", "content": "Prior research on verifying RDF statements has so far largely relied on human annotators and crowdsourcing (Kontokostas et al., 2013; Acosta et al., 2013), which is a costly process that can be applied only to a small fraction of triples in need of verification. A detailed review of these approaches was done, e.g., by Xue and Zou, 2022. In the following, we will focus on the new generation of approaches that use LLMs, which have the potential to be more resource-efficient and allow for large-scale deployment.\n\nThe closest work to ours is the MiniCheck system (Tang et al., 2024), which is a trained sentence-level LLM-based fact-checker trained on synthetically generated data with GPT-4. Our approach is similar in that we also strive not only to verify a specific statement but also to provide evidence for that claim by linking the statement to a grounding document. However, in Tang et al., 2024, the grounding is a post-hoc process separate from the actual fact check. This, together with the need to pre-train a specific model, results in lower transparency and increases the complexity of the process. On the other hand, our approach uses an open-weight LLM without any pretraining (zero-shot). In our approach, the LLM is used as a tool to compare information in the grounding document with the verified statement. For this, we do not need pretraining, in fact, a key element\nin our design is to instruct the LLM not to use any of its internal subject matter expertise. Also, the\nMiniCheck system was designed as a generic fact-checking tool. In contrast, our approach was\nspecifically designed for RDF statements, and, as such, it covers the identification of knowledge\ngraph statements in need of validation. It also includes a Wikidata-specific approach to the\nretrieval of grounding documents, which bypasses the need to use internet search. On the other\nhand, we do not need to address the decomposition of input text into facts like generic fact-\ncheckers since individual RDF statements already correspond to atomic facts.\n\nWei et al., 2023 investigated the performance of zero-shot LLMs for several information extraction\ntasks relevant to our goal to perform zero-shot LLM-based verification of RDF statements. The\nevaluation on multiple datasets has shown competitive results to full-shot systems. This was\nachieved by dividing the task into several simpler subtasks corresponding to individual prompts,\nwhich we also performed. Instrumental to the result is a suitable prompt structure; specifically,\nthe authors obtained better results with chat-like prompts (containing questions rather than a list\nof tasks), which we also adopt in our approach.\n\nA comprehensive evaluation of ChatGPT on the information extraction task performed by Han et\nal., 2023 identifies additional recommendations for improving LLM-based information extraction\nresults. For our purposes, the most significant is the observation that irrelevant contexts before\nand after the core text fragment can sometimes lead to a decrease in accuracy of up to 48%. In\nour LLM chaining workflow, we reflect this by breaking down the grounding documents into\nparagraphs, which are independently compared against the verified statement.\n\nRelevant is also the recent study of Efeoglu and Paschke, 2024, who evaluated fine-tuned LLMs\nwithin Retrieval Augmented Generation (RAG) workflows with a focus on sentence-level relations.\nWhile our approach could also be viewed as having an RAG component, an important\ndifferentiator from most RAG workflows is that we do not use LLMs to retrieve external text, nor do\nwe use the external text for fine-tuning the LLM or blend it with its trained knowledge.\n\nAnother finding made by Han et al., 2023 is that ChatGPT (version not given) does not often\nunderstand the directions of relations between two concepts well, especially if these are\nasymmetrical relations. In an experiment they swapped the order of subject and predicate\nconcepts and observed that only in 30% of the situations the model recognized the inaccurate\ndirection. We reflected this finding when designing our evaluation dataset, into which we included\nasymmetric relations.\n\nAn important element of our approach is the ability to generate traces, or explanations of the\nverifications. The verification result is technically a new edge, which links the verified statement\nwith an external document that provides a reference and a justification. The verification task can\nthus be viewed as a particular instance of a link prediction task for which there is a growing\ninterest in generating explanations. Unlike existing works that insert links between nodes already\npresent in the graph and thus use information in the graph to generate the explanations (d'Amato\net al, 2021), we generate the explanation from the external content and using an LLM.\n\nA possible practical application of our work is a large-scale verification of facts in Wikidata.\nAlthough LLM-based verification may be more resource-effective than human-based verification,\na cost-benefit analysis should be conducted to weigh the financial and environmental costs\nagainst the number of incorrect statements that could be identified by such a process. To this\nend, Santos et al., 2023 investigate the trustworthiness of Wikidata statements. They mainly focus\non statements that share the same subject and predicate but connect to a different object. While\nin some cases, such statements are not errors (for example, one person can be in the award\nreceived relationship with multiple awards), these statements could be shortlisted for\nverification, especially if they miss some type of required qualifier. Santos et al., 2023 established\nthe prevalence of statements with multiple values that have one or more mandatory required\nqualifier constraint violations at 0.24%, amounting to over a million such statements in need of\nverification in Wikidata. There were other previous attempts at automatically identifying errors\nthrough machine learning, such as the work of Rico et al, 2018. However, this and similar\napproaches were able to spot errors typically on the schema or metadata level, while our work\naims to verify the factual correctness of arbitrary statements using external knowledge."}, {"title": "3. Proposed method: Traceable LLM-based claim verification", "content": "This section describes the core of the proposed method: the verification of triples with LLMs\nagainst a given snippet of text. Here, we focus on the prompts used to reflect the paradigm that\nthe model is asked to only use the information explicitly mentioned in the given snippet to\nsuppress the use of the model's internal knowledge. In Section 4, we describe how this method\nis used to verify Wikidata statements, including the lookup of internet sources for Wikidata entries\nwith missing references or parsing of Wikipedia entries for primary references."}, {"title": "3.1. Prompt generation", "content": "The prompt requires four text inputs. The subject, predicate, and object of the RDF statements\nand the text snippet from the grounding document against which the statement is verified. The\nidentification of RDF statements for verification can be performed in several ways, as discussed\nin Section 4."}, {"title": "3.1.1. Text chunking", "content": "When transforming input text into text snippets, it is necessary to account for the maximum input\ntoken limit of each model. While the closed-source SOTA models developed by OpenAl, such as\nGPT-40 mini and GPT-40 Turbo can receive up to 128.000 tokens, which corresponds to\napproximately 96.000 words, some open-source models, such as Llama 2 70B by Meta can\nreceive only up to 4.000 tokens or 3.000 words. We used the following chunking methods.\n\n1. Division by paragraphs.\n2. Filling the input token limit.\n\nThe advantage of the first method is that it is the most natural, while the second method results\nin increased efficiency as fewer total requests are sent to the LLM API. In Use Case 1 (section 4.1),\nwhere we process documents retrieved by web search, we used the division by paragraphs, and\nin Use Case 2, which verifies statements using the corresponding Wikipedia page, we used a\ncombination of both approaches. The Wikipedia article is first divided into chunks of text. Each\nchunk comprises paragraphs that together do not exceed the threshold of 10.000 characters. If a\nsupportive statement is found in the chunk of text, each paragraph is examined separately."}, {"title": "3.1.2. Prompt structure", "content": "The structure of the prompt used is shown in Figure 3. The prompt was designed so that the output\nof the LLM can be used for binary classification of the statement, either as confirmed by the\npassed snippet or as not being confirmed. There are three answer options, but only the first one"}, {"title": "4. Application in knowledge graphs: Verifying data in Wikidata", "content": "Whether the content was added manually or using software, a small portion of the data in\nknowledge graphs is erroneous (Xue and Zou, 2022). An automated verifying system could help\nensure high-quality data in knowledge bases in a cost-efficient and timely manner.\n\nWikidata belongs to the Wikimedia group of projects, including also Wikipedia and Wiktionary.\nWikidata is a central publicly available knowledge base that serves as a foundation for other\nWikimedia projects; thus, the integrity of its information does not affect only its direct users but\nalso the visitors of Wikipedia. Wikidata contains approximately 1.5 billion item statements, which\nare represented as RDF triples (Wikidata, 2024).\n\nIn the following, we will present two use cases to verify statements in Wikidata. The two use cases\ndiffer in several ways to demonstrate the versatility of the approach, as shown in Table 1."}, {"title": "4.1. Use case 1: Using web search to retrieve candidate grounding documents", "content": "In this use case, a web search is used to identify candidate grounding documents, while the\nsecond use case covered in Section 4.2 takes advantage of the connection between Wikidata\nentries and Wikipedia to identify links to grounding documents from Wikipedia pages. Note that\nan overall workflow of both use cases was included in the graphical abstract in Figure 1."}, {"title": "4.1.1. Identifying unsourced triples through SPARQL query", "content": "While RDF triples correspond to atomic facts, a typical pattern is that there is a core statement\nand then multiple supportive statements that elaborate on the core statement. For example, a\ncore statement that a person was awarded a certain award can be accompanied by a supportive\nstatement (qualifier) that indicates the year the award was received. Another important type of\nsupporting statement is a citation of a grounding document that provides a source for the core\nclaim. Some statements with missing citations are flagged by the Wikidata interface, as shown in\nFigure 5."}, {"title": "4.1.2. Finding candidate grounding documents using a web search engine", "content": "Next, we take each statement and use it as an input for automated Google search. We only used\nthe subject, predicate and object separated by space and add a parameter \u201c-wikipedia\u201d to\nexclude results containing Wikipedia (Wikipedia-based verification is a separate mode of\noperation described in Section 4.2). We also limit the search results to the top five hits. Finally,\nwe use a Python library Requests to retrieve the HTML content of the retrieved resources.\n\nWhen applied for the first statement (\"V\u00e1clav Havel\u201d, \u201caward received\", \"Gottlieb Duttweiler\nPrize\") we obtained a list of five URLs, with the first three being web pages of different winners of\nthe Gottlieb Duttweiler Prize and the last two being annual reports (in pdf) from the Vaclav Havel\nfoundation. Detailed results are available in the supplementary resources. Since non-HTML\ncontent such as PDF is currently unsupported by our implementation, only the first three URLs\nenter into the LLM-based verification."}, {"title": "4.1.3. LLM-based verification", "content": "If the content of the retrieved document is available, we go through all paragraph tags and if their\ncontent is valid (at least 100 characters long), we use them as individual inputs to the LLM-based\nverification. Subsequently, we invoke the prompt from Figure 2 on the individual paragraphs. If the\nLLM returns a positive answer, we end the loop and save the current paragraph that yielded the\nconfirmation for documentation purposes. In this case, the remaining document content is not\nprocessed to conserve resources. When a match is found, or none was found but all paragraphs\nof the current document were processed, we proceed to the next document.\n\nFor our sample statement, the verification process was executed against all five retrieved\ndocuments described in Section 4.1.1. The LLM (LLama 3 70B Instruct) found a supporting\nparagraph in documents number 1 and 3, both mentioning the award to V\u00e1clav Havel when\nannouncing new award winners. The verification result for the example statement according to\nthe first document is included in Figure 6."}, {"title": "4.2. Verifying unsourced Wikidata statements by a corresponding Wikipedia page", "content": "This use case is aimed at verification of specific Wikidata statements from the biological domain.\nWe manually chose five statements about the subject \"Bioluminescence\u201d (Q179924) shown in\nTable 3. We used ChatGPT-1106-preview LLM for this experiment."}, {"title": "4.2.1. Mapping Wikidata entries to Wikipedia", "content": "Unlike in Use Case 1 (Section 4.1.2) where the grounding documents are obtained directly via a\nweb search, in this use case, we utilize the correspondence between the Wikidata entries and\nWikipedia articles. The name of the subject, which was chosen as a parameter\n(Bioluminescence), is used in a Wikidata API call, which returns the URL of the corresponding\nWikipedia page."}, {"title": "4.2.2. Identifying grounding documents from Wikipedia references", "content": "Once the Wikipedia article corresponding to the subject of the verified statements has been\nestablished, its content is fetched and split first into larger text groups. Given the typical longer\nlength of Wikipedia articles and the relatively lower likelihood that the subject will even be\nmentioned in a specific paragraph, we split the documents into chunks of several paragraphs\neach (observing the maximum of 10.000 characters) to increase the efficiency of the processing.\nIn the first phase, the prompt in Figure 2 was used on these larger chunks. If there was a positive\nresult, the chunk was broken down into paragraphs and the process repeated. A positive match\nwas thus associated with a precise part (a paragraph) of the Wikipedia article. However, since\nWikipedia is not generally considered an authoritative source on its own, the location of the\ngrounding text in the Wikipedia article was used as input for subsequent processing aimed at\nfinding the primary grounding document. To do this, we extracted the reference numbers that were"}, {"title": "4.2.3. LLM validation against primary grounding document", "content": "After the process obtained one or more references from the Wikipedia article, these are attempted\nto be downloaded. If the resource is not available, we attempt to obtain its archived copy from the\nWeb Archive (archive.org).\n\nOnce we received the content either directly or via the Web Archive, we performed the proposed\nmethod described in Section 3 \u2013 we split the content into paragraphs and iteratively applied the\nprompt in Figure 2 to verify the RDF statement against each of the paragraphs."}, {"title": "4.4. Reference implementations", "content": "Both use cases were implemented as generic Python applications, which are available in the\nsupplementary material stored in a GitHub repository. Since the recursive search through\nWikipedia articles generates multiple outputs, a reference implementation of the proposed\nworkflow also supports automatic documentation following the W3C XML standard technologies.\nA complete process trace is saved to an XML output file conformant to an XSD document that\nvalidates this XML. This is accompanied by an XSL Transformation, which produces the HTML\nreports, samples of which are shown in Tables 4 and 5."}, {"title": "5. Evaluation on the BioRED dataset", "content": "The evaluation of the proposed method for verifying RDF triples was conducted on a new dataset\nderived from BioRED dataset (Luo et al., 2022), which covers five concept types (such as\nchemicals and diseases) and 8 relationship (predicate) types. Annotation was primarily done\nmanually but supported by automatic tools such as TeamLat (Islamaj et al., 2020) and PubTator\n(Wei et al., 2019). As the BioRED dataset was originally designed for relation extraction and named\nentity recognition tasks, we had to perform several modifications and extensions to allow its use for the benchmarking of RDF statement verification methods. The summary of the resulting\ndataset, which we call BioRED-Verify, is included in Table 6, and the details are presented in the\nfollowing subsection."}, {"title": "5.1. Dataset preparation: BioRED-Verify", "content": "From the original BioRED dataset, we used all concept types and their combinations (such as\n\"GeneOrGeneProduct\u201d or \u201cChemicalEntity\"), except for the combination of \u201cSequenceVariant\"\nand \"ChemicalEntity\" which was excluded due to low occurrence (there were only 10 records in\ntotal for both \u201cPositive_Correlation\u201d and \u201cNegative_Correlation\u201d).\n\nOut of the eight relationship types contained in the original BioRED dataset, we chose the\nrelations \"Positive_Correlation\u201d and \u201cNegative_Correlation\u201d. The reason for this is that they are\nunambiguous and exclusive. For example, we considered the relation \"Association\", which is also\npart of the dataset, inappropriate for our purposes because it is a general relation that subsumes\nboth positive and negative correlation. An overview of dataset size by relation is included in Table\n6. The details on how the positive and negative examples were generated are in the following\nsubsections.\""}, {"title": "5.1.1. Positive examples", "content": "As mentioned before, BioRED is a relation-extraction dataset which means its structure and\nformat are unsuitable for a verification task. The dataset consists of articles with each article\nbeing associated with a list of entities and a list of relations between these entities that can be\nfound in the article. The original use of the dataset was for information extraction methods to\nidentify relations in the given text, which were then confirmed against the ground truth relations\nin the dataset. We essentially reversed this task by using the dataset to evaluate whether the\nassociated text supports a given ground truth relation from BioRED. In this way, we obtained the\npositive examples."}, {"title": "5.1.2. Negative examples", "content": "Since the BioRED dataset contained no negative examples, these had to be introduced. We\ndecided to use the method of creating corrupted triples by replacing the tail entity of the triple (the\nobject) with a different entity of the same type. This method is used in frequently referenced link\nprediction datasets such as WN18RR (Bordes et al., 2013), derived from WordNet, and FB15k-237\n(Toutanova and Chen, 2015), derived from Freebase. Also, following the protocol for generating a\nfiltered version of the dataset according to Bordes et al., 2013, we excluded any randomly created\ncorrupted triples that corresponded with existing ground-truth triples."}, {"title": "5.2. Evaluation using Llama 3 70B", "content": "Using Llama 3 70B and the API service hosted by Replicate.com we evaluated the workflow\nproposed in Section 3 on the BioRED-Verify dataset."}, {"title": "5.3 Replicability", "content": "The model used for the verification task was Llama 3 70 B hosted at Replicate.com. To make the\nresults replicable, we used a random seed and fixed hyperparameters as shown in Table 9. The\nused\nfor\nthe\nat\ndataset\nevaluation\nis publicly available\nhttps://ftp.ncbi.nlm.nih.gov/pub/lu/BioRED/. Llama 3 is an open LLM available at\nhttps://llama.meta.com/. The raw LLM responses used for evaluation are available at:\nhttps://github.com/danieladam2001/LLM-based-validation."}, {"title": "6 Discussion and conclusions", "content": null}, {"title": "6.1 Use cases", "content": "We described two use cases using LLM-based verification for checking Wikidata content, which\ndiffer mainly in how the primary grounding documents were found. We found searching for\ninternet sources directly via a web search API to be fast and efficient. Its disadvantage is that web\nsearch may return resources with varying credibility. The alternative option based on the mapping\nbetween Wikidata entries and Wikipedia articles is more resource intensive and results in lower\nrecall. Compared to the number of grounding candidates reachable by web search, Wikipedia\narticles contain only a smaller number of primary documents. However, the sources used in the\nReferences section of Wikipedia are subject to community-enforced quality guidelines, which\nmay result in higher reliability of this method if it returns a positive result. Also, given the curated\nnature of the mapping between Wikidata and Wikipedia, the references obtained from Wikipedia\nare naturally disambiguated, which is not the case for retrieving documents from web search."}, {"title": "6.1 Key findings from quantitative evaluation", "content": "The quantitative evaluation was performed on a modified version of the BioRED dataset\ncontaining 3,438 statements. The observed differences between the results for positive and\nnegative correlation relations imply that the specific relation poses a distinct set of challenges for\nLLMs. From the application perspective, the comparatively high precision suggests that the\nintended workflow aimed at the use of LLMs to propose missing references is feasible. The\naverage precision, as evaluated on the BioRED dataset, is 88%. While this value may appear\nsufficient for some uses, in our view, the 12% false positive rate still implies the need for human\noversight. On the other hand, we consider the average recall at 44% satisfactory, as typically, a\ngiven statement needs only one grounding document, while multiple grounding documents (or\ntheir confirming passages) are typically available, particularly if the web search candidate\nretrieval method is used.\n\nAs shown in the evaluation results, the performance varies significantly by the type of entities in\nthe subject and object positions and by the type of the relation. Given that the data originated\nfrom a highly specialized biosciences domain, the predictive performance may be higher on\ncommon sense or general-knowledge topics. On the other hand, a real-world performance when"}, {"title": "6.2 Theoretical contributions", "content": "In this article, we described a novel zero-shot method for verifying statements in knowledge\ngraphs. It differs from the previous work on LLM-based fact-checking in that it emphasizes utilizing\nthe LLM as a tool to interpret and compare possible grounding documents with verified\nstatements rather than relying on any domain-specific knowledge the LLM may have. The code for\nthe proposed method is freely available, and the quantitative evaluation is reproducible, as it was\ncarried out on an openly available dataset using an open-weight LLM. The average precision at\n88% was unexpectedly high, considering we used a generic Llama 3 model with no finetuning. In\none of the use cases, we have shown that the workflow can also be used with a closed LLM,\npotentially resulting in even better performance."}, {"title": "6.3 Practical implications", "content": "The proposed workflow with accompanying source code can foster large-scale, automated\nverification of knowledge graph statements. We also created and made available a new dataset,\nBioRED-Verify, to evaluate the performance of knowledge graph verification methods. While our\nuse cases focused on widely known Wikidata, we foresee the main application in verifying\nstatements in specialized biological and biochemical knowledge graphs, such as KBase (Arin et\nal, 2018) or Monarch (Shefchek et al, 2020). Our proposed workflow could be integrated into\nknowledge graph ingestion and maintenance frameworks such as KG-Hub (Caufield, 2023)."}, {"title": "6.4 Limitations and Future Work", "content": "In the future, we intend to improve the implementation of the verification workflow on Wikidata.\nTo make the process effective, we would like to make a narrower selection of data to be verified\nbecause most Wikidata entries are still believed to be true and factual. For example, we could\nfocus on the claims with multiple values that have one or more mandatory required qualifier\nconstraint violations, which are presented as possibly conflicting.\n\nThe performance of the web search lookup could be improved by factoring in a disambiguation\nelement. In the current version, the web search query is constructed only from the given\nstatement. In future work, we plan to experiment with query expansion by adding contextual\ninformation extracted from connected Wikidata statements.\n\nThe evaluation was performed on the BioRED dataset, which was expanded with negative\nstatements. The advantages of this approach were that we evaluated against a dataset used in\nrelated work, and we could obtain estimates of predictive performance without the need for\nmanual checking, which is typically used in knowledge graph statement validation. However, as\nnoted above, this approach may not fully represent performance on Wikidata, and therefore an\nimportant next step is to perform the evaluation directly on Wikidata or a similar knowledge graph,\nsuch as DBpedia (Bizer et al., 2009)."}, {"title": "6. Conclusion", "content": null}, {"title": "CRediT authorship contribution statement", "content": "Daniel Adam: Writing - original draft, Writing - review and editing, Investigation, Methodology,\nData curation, Resources, Software, Validation, Visualization. Tom\u00e1\u0161 Kliegr: Writing \u2013 original\ndraft, Writing \u2013 review and editing, Conceptualization, Methodology, Supervision."}]}