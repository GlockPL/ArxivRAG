{"title": "Automating Quantum Software Maintenance: Flakiness Detection and Root Cause Analysis", "authors": ["Janakan Sivaloganathan", "Ainaz Jamshidi", "Andriy Miranskyy", "Lei Zhang"], "abstract": "Flaky tests, which pass or fail inconsistently without code changes, are a major challenge in software engineering in general and in quantum software engineering in particular due to their complexity and probabilistic nature, leading to hidden issues and wasted developer effort.\nWe aim to create an automated framework to detect flaky tests in quantum software and an extended dataset of quantum flaky tests, overcoming the limitations of manual methods.\nBuilding on prior manual analysis of 14 quantum software repositories, we expanded the dataset and automated flaky test detection using transformers and cosine similarity. We conducted experiments with Large Language Models (LLMs) from the OpenAI GPT and Meta LLAMA families to assess their ability to detect and classify flaky tests from code and issue descriptions.\nEmbedding transformers proved effective: we identified 25 new flaky tests, expanding the dataset by 54%. Top LLMs achieved an F1-score of 0.8871 for flakiness detection but only 0.5839 for root cause identification.\nWe introduced an automated flaky test detection framework using machine learning, showing promising results but highlighting the need for improved root cause detection and classification in large quantum codebases. Future work will focus on improving detection techniques and developing automatic flaky test fixes.", "sections": [{"title": "I. INTRODUCTION", "content": "Flaky tests, which exhibit non-deterministic behavior by failing or passing inconsistently without any changes to the code under test, pose significant challenges for software maintenance and reliability [1]. In the field of quantum software engineering, flaky tests are particularly problematic due to the inherent complexities and probabilistic nature of quantum computations. They can obscure genuine issues, waste developers' time, and undermine confidence in test suites [2], [3].\nIn previous work, Zhang et al. [2] explore the code and bug-tracking repositories of 14 quantum software and identify 46 unique quantum flaky tests in 12 quantum projects (ranging from 0.26% to 1.85% of bug reports). They search for the 10 keywords related to flaky tests (e.g., flaky and flakiness) in issue reports (IRs) and pull requests (PRs) to identify flaky tests. They then identify and categorize eight types of flakiness and seven common fixes. Randomness is the most common cause of quantum flakiness, and the most common solution is to fix pseudo-random number generator (PRNG) seeds. However, the findings (46 instances of flakiness) are\nconstrained by the limitations inherent in their vocabulary-based method. In addition, they manually examine and identify all flaky tests, which is time-consuming.\nOur research explores a more effective and efficient flaky test detection technique for quantum software by answering the following research questions.\n\u2022 RQ1: How can we detect if a given IR or PR is related to a flaky test?\n\u2022 RQ2: How can we detect if a given IR or PR is related to a flaky test with additional code context?\n\u2022 RQ3: How can we identify the root cause of a flaky test?\nOur main contributions are as follows.\n\u2022 We have enriched the existing dataset of flaky tests [2] by adding more observations, as well as including the buggy code causing flakiness and the corresponding fixes, which were absent from the original dataset. The extended dataset is publicly available at: https://doi.org/10.5281/ zenodo.13913775.\n\u2022 We developed a method to semi-automatically detect flaky test-related IRs and PRs by mining software repos-itories using embedding transformers and cosine similar-ity.\n\u2022 We propose a method to automatically detect flaky issues using LLMs with zero-shot prompting.\nBy automating the detection of flaky tests, our framework aims to improve the reliability and maintainability of quantum software systems."}, {"title": "II. METHOD", "content": "A. Dataset\nOur study builds upon a prior manual analysis by Zhang et al. [2], who examined 14 open-source quantum software repositories from platforms such as IBM Qiskit, Microsoft Quantum Development Kit, TensorFlow Quantum, and NetKet. They identified 46 flaky test reports across 12 repositories by searching closed GitHub issues using keywords related to flakiness (e.g., flaky, intermittent, nondeterministic).\n1) Using transformers and cosine similarity for flaky tests detection: To extend this research and detect more flaky reports systematically and automatically, we employed embedding transformers to represent GitHub IRs and PRs and"}, {"title": "B. Detecting Flaky Tests with LLMs", "content": "Our experiments demonstrate an automated LLM-based framework (see Figure 1) that efficiently gathers resources, configures inputs, and classifies bugs by streamlining the standard software development workflow with GitHub as the\nsoftware engineering process for bug resolution.\n1) LLM Inference Configuration: Leveraging the extracted codebase data (discussed in Section II-A2), we crafted input prompts to address our research questions. The prompts are provided in the supplementary material (https://doi.org/10. 5281/zenodo.13913775).\nTo explore our research questions and assess how context size affects the answers, we designed the following experi-ments.\nFor RQ1, which aims to classify whether a particular IR (or PR, if no issue is associated with it) is flaky or non-flaky, we tested two levels of context: Rp (partial), which includes only the initial IR (or PR) description, and Rf (full), which includes the description along with all associated comments.\nFor RQ2, we expanded the context for the language model by adding the code involved in the PR before the fix was applied, also at two levels: Cp (partial), which includes the method-level code, and Cf (full), which provides the complete code listing.\nBy combining the context levels from RQ1 and RQ2, we generated four experimental conditions: {Rp, Rf}\u00d7 {Cp, Cf}. These conditions range from (Rp, Cp), which uses only the de-scription and method-level code, to (Rf, Cf), which includes the description with comments and the full code listing.\nFor RQ3, the amount of information provided did not change; we simply followed up by asking which specific root cause a particular flaky test relates to, using the nine classes of root causes defined by [2]: \"Randomness (PRNG),\u201d \u201cFloating Point Operations,\u201d \u201cSoftware Environment,\u201d \u201cMulti-threading,"}, {"title": "III. RESULTS AND ANALYSIS", "content": "A. LLM Detection: Interpretability and Challenges\nWe adopted four LLMs GPT-40, GPT-4o-mini, LLaMA-405B-Instruct, and LLaMA-70B-Instruct, and evaluated the performance of the LLM in classifying RQ1 based on both flaky and non-flaky observations, and RQ2 and RQ3 based solely on the ground truths of the flaky observations. The results can be found in Table I; we employ the F1-score, Mathews Correlation Coefficient (MCC), recall, and the number of detected flaky/non-flaky tests to compare the performance.\nWe also evaluated two additional LLMs on-site using the Ollama framework [20]. However, the results for the non-instruct tuned LLaMA-8B and LLaMA-70B models were excluded due to insufficient performance, as many outputs were either empty or corrupted. This underperformance is likely due to these being the smallest non-fine-tuned models in our study. Additionally, GPT-40-mini was the only model that required explicit manual modifications before scoring."}, {"title": "B. Model Performance", "content": "1) RQ1: Using the full context, the best-performing model is GPT-40-mini, with an F1-score of 0.8871 and an MCC of 0.7774. This outcome is somewhat unexpected, as GPT-40 is generally considered more powerful. However, when only partial context is provided, GPT-40 outperforms GPT-40-mini with an F1-score of 0.8443 vs. 0.8229 and MCC of 0.6971 vs. 0.6558. Thus, GPT-40 effectively identifies flakiness with respectable performance when provided just the initial report. This suggests that classification with limited context might be more challenging, and a more sophisticated model excels in such cases. LLaMA-405B-Instruct and LLaMA-70B-Instruct ranked third or fourth (depending on the context).\n2) RQ2: We assess whether adding code at the method-level (Cp) or including a full code listing (Cf) improves classification accuracy. We compare recall values from RQ1 and RQ2 to evaluate this. For GPT-40, adding method-level code (Cp) increases recall, with the highest score observed in the {Rf, Cp} setup, as expected. Interestingly, providing a complete code listing (Cf) reduces recall, which aligns with the observation that a model, much like a human programmer, might struggle to identify specific methods to focus on when faced with the entire codebase.\nFor GPT-40-mini, recall decreases for RQ2, but less so for Cp compared to Cf. This suggests that analyzing both natural language and code is a more complex task, one that requires a more advanced model.\nThe behavior of LLaMA-405B-Instruct is similar to GPT-40, showing an increase in recall for Cp and a decrease in recall for Cf. In contrast, LLaMA-70B-Instruct exhibits"}, {"title": "C. Context", "content": "Based on the above discussion, we observe that, in general, Rf aids models in making better decisions for RQ1 and RQ2, but not for RQ3. This is somewhat counterintuitive and may be related to the complexity of RQ3, warranting further investigation. For RQ1 and RQ2, the performance drop is relatively small, indicating that models can still provide practical value when an issue or pull request is initially opened.\nMethod-level code (Cp) appears to yield better results than full code listings (Cf), but further analysis is necessary, as the number of observations differs between the two setups."}, {"title": "IV. THREATS TO VALIDITY", "content": "Validity threats are classified according to [21], [22].\nInternal and construct validity. Data collection and la-beling are error-prone processes. In our study, the potential flaky tests are collected based on cosine similarity and manual labeling. A flaky test can be mislabeled. Our remedy is to have at least two authors cross-examine the potential flaky tests and confirm all positive cases. Non-flaky tests are also collected based on cosine similarity (when its value is less than 0.5) and from those that two authors labeled as non-flaky tests. To mitigate potential errors, at least two authors cross-examined all the non-flaky tests.\nExternal and conclusion validity. Generally, software engineering studies suffer from real-world variability, and the generalization problem can only be solved partially [23]. One threat to external validity is the limited scope of our dataset, which, although enriched from previous studies, still focuses on a subset of quantum software repositories. As a result, the findings may not be fully representative of the broader population of quantum software projects, especially those utilizing different testing frameworks or methodologies. Through future research, we hope to expand our dataset and findings."}, {"title": "V. FUTURE PLANS", "content": "Our future plans include improving detection methods by exploring and fine-tuning various LLMs, developing automatic"}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we have presented an automated framework for detecting and resolving flaky tests in quantum software, leveraging embedding transformers and LLMs. Our approach enhances the existing dataset of flaky tests and provides methods for semi-automatic and automatic detection. The scores showcase that LLMs perform well, but can be greatly improved for detecting flakiness and root cause detection in quantum software bugs."}]}