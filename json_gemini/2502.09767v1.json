{"title": "Non-Markovian Discrete Diffusion with Causal Language Models", "authors": ["Yangtian Zhang", "Sizhuang He", "Daniel Levine", "Lawrence Zhao", "David Zhang", "Syed A Rizvi", "Emanuele Zappala", "Rex Ying", "David van Dijk"], "abstract": "Discrete diffusion models have emerged as a flexible and controllable paradigm for structured sequence modeling, yet they still lag behind causal language models in expressiveness. To bridge the gap between two paradigms, we introduce CaDDi, a causal discrete diffusion model that unifies sequential and temporal modeling within a non-Markovian diffusion framework. Unlike conventional diffusion models that operate step by step with no access to prior states, CaDDi integrates the temporal trajectory, enabling more expressive and controllable generation. Our approach also treats causal language models as a special case, allowing seamless adoption of pre-trained large language models (LLMs) for discrete diffusion without the need for architectural modifications. Empirically, we demonstrate that CaDDi outperforms state-of-the-art discrete diffusion models on both natural language and biological sequence tasks, narrowing the gap between diffusion-based methods and large-scale autoregressive transformers.", "sections": [{"title": "1. Introduction", "content": "Autoregressive transformers have become a dominant approach for sequence modeling (Vaswani, 2017; Chowdhery et al., 2023; Touvron et al., 2023a), achieving state-of-the-art performance in many natural language and biological tasks. Their left-to-right decoding paradigm simplifies training via next-token prediction and is supported by large-scale pre-training, unlocking broad linguistic (or domain) knowledge. However, these models can be less flexible for bidirectional or partially specified generation, such as text infilling or prompting from arbitrary locations."}, {"title": "2. Preliminary", "content": ""}, {"title": "2.1. Variational Perspective of Diffusion Models", "content": "Diffusion models can be viewed as a special class of hierarchical variational autoencoders (HVAEs), where the latent variables consist of progressively corrupted versions of the original data. HVAEs are trained by maximizing the evidence lower bound (ELBO) on the data log-likelihood. Concretely, let xo be the original data and x1:T be the latent variables at timesteps 1,..., T. The ELBO objective can be written as:\n$\\max \\mathcal{L}_{\\theta,\\varphi}^{CELBO} = \\mathbb{E}_{x_{1:T}\\sim q_{\\varphi}(x_{1:T}|x_0)} \\left[ \\sum_{t=1}^{T} \\log p_{\\theta}(x_{t-1} | x_{t:T}) + \\log p_{\\theta}(x_T) \\right]$\n+ log po (xT) (1)"}, {"title": "Markovian Assumptions", "content": "Traditional diffusion models (viewed as a special case of HVAEs) make the following Markovian assumptions:\n\u2022 Forward Process: A non-learnable Markov chain\n$q(X_{1:T} | X_0) = \\prod_{t=1}^{T} q(x_t | x_{t-1}).$\n\u2022 Reverse Process: A learnable Markov chain po (xt\u22121 |\nXt:T)=po (Xt\u22121 | Xt).\nEnforcing the reverse process to be Markovian simplifies the sampling process: at each timestep, the model only conditions on the current latent variable xt to predict xt-1. While this constraint facilitates efficient generation, it can limit the model's capacity to capture long-range dependencies within the latent chain."}, {"title": "2.2. Discrete Diffusion Language Models", "content": "Recently, discrete diffusion models (Austin et al., 2023) have recently emerged as a powerful framework. In contrast to their continuous counterparts, which corrupt data by adding Gaussian noise in a real-valued space, discrete diffusion models operate on categorical variables, gradually corrupting tokens before reconstructing them through a learned denoising process.\nForward Process. Let xo = (x1, x2,...,xf) be a sequence of discrete tokens from a vocabulary V of size |V|. The forward (noising) process produces X1, X2,..., xT by independently corrupting each token according to a time-dependent transition matrix Qt:\nq(xt | Xt-1) = Cat (xt; Qt xt-1), (2)\nwhere Cat(\u00b7; \u03c0) denotes the categorical distribution with parameter \u03c0, and [Qt]ij gives the probability of transitioning from state i to state j at time t.\nReverse Process. The reverse (denoising) model\nPo (xt-1 | xt) is learned to invert the corruption process. Employing the xo-parameterization (Austin et al., 2023), one can write:\n$p_{\\theta}(x_{t-1}|x_t) \\propto \\sum_{x_0} q(x_{t-1}, x_t | x_0) p_{\\theta}(x_0 | x_t),$\nwhere po (Xo | xt) is a time-dependent denoiser mapping xt back to an estimate of the original xo.\nTraining Objectives. Training commonly involves maximizing the variational lower bound on log po (x0). In practice, (Austin et al., 2023) have shown that a simple denoising objective can achieve better generative quality under"}, {"title": "3. Non-Markovian Discrete Diffusion", "content": "Previous methods have modeled the discrete diffusion process as a Markovian process, where the model learns an instantaneous reverse process to denoise xt and reconstruct xt-1 by po(xt-1 | xt) (Austin et al., 2023). Despite efficient generation, this Markovian constraint can limit the model's ability to capture long-range dependencies within the latent chain. All relevant information is compressed into single state xt, potentially leading to a non-robust inference procedure.\nIn this paper, we extend the non-Markovian diffusion process to discrete data modeling following (Gu et al., 2024). Specifically, recent studies have demonstrated that the Markovian assumption is not strictly necessary in the inference process. Breaking this assumption allows for the incorporation of the entire temporal trajectory xt:T to denoise xt-1 by po(Xt\u22121 | Xt:T) in autoregressive manner, leading to a more expressive and robust inference process.\nIn the following, we will describe how the non-Markovian discrete diffusion process is constructed. Crucially, we will see that the resulting non-Markovian autoregressive inference mechanism essentially aligns with a causal language model plus an additional temporal dimension-laying the groundwork for our unified spatial-temporal framework (Section 4)."}, {"title": "3.1. Hybrid Non-Markovian Forward Trajectory", "content": "A key challenge in realizing non-Markovian inference is how to design the forward trajectory so that future states Xt+1:T carry more complementary information aboutxt. The straightforward Markovian absorbing process\n9(\u0445\u043e:\u0442) = \u03a0-1 q(xt|xt\u22121) is ill-suited for this purpose, as information is highly limited and redundant. To ensure each timestep retains complementary information about the original data, we (1). construct the forward trajectory by independently corrupting x0. (2). Mix an abosrbing kernel with uniform kernal to produce more diverse noisy states."}, {"title": "Independent Corruption", "content": "As shown in Fig. 1, the diffusion trajectory x0:T is created where we add independent noise to xo at different timesteps, rather than relying on the previous state as the noise source. The forward trajectory is constructed as:\nq(x0:T):=q(x0, X1, X2, ..., XT) (5)\n=q(x0) \u03a0qt(x+x0:t-1) (6)\nt=1\n=q(x0) \u03a0qt(\u00d711\u00d70), (7)\nt=1\nwhere q(xt xo) is the marginal conditional distribution, which is obtained from a standard Markovian diffusion kernel but applied directly to x0. For example, in absorbing or uniform diffusion processes, we can write:\nq(xt | xo) = Cat (xt; x0Qt) (8)\n= Cat Xt;XO T Qk (9)\nk=1\nwhere Qk is the transition matrix at step k, and Qt is the product of all such transitions up to t. This construction generalizes the Markovian forward trajectory (see Appendix for further details) and creates a sequence of noisy states that better preserve intermediate information across timesteps.\nProposition 3.1 (Discrete Non-Markovian Information Gain). Let {x0, X1,...,XT} be a discrete forward diffusion process that is not strictly Markovian. Suppose there exists at least one timestep t such that\u00b9\nXt-1 / Xt+1:T | Xt.\nThen the conditional mutual information\nI(xt-1; Xt+1:Txt) > 0,\nwhich implies that conditioning on xt+1:T in the reverse process po(Xt\u22121 | Xt:T) strictly reduces the uncertainty about xt-1 compared to conditioning on xt alone.\nIn other words, when the non-Markovian forward trajectory is designed with independent corruption, future noisy states can complement each other, bolstering the reverse inference process."}, {"title": "Hybrid Diffusion Kernel", "content": "Most prior discrete diffusion methods rely on a single kernel, such as purely absorbing (where tokens are replaced by [MASK]) or purely uniform corruption. However, sticking to one kernel can lead to\n\u00b9it is trivial to find such t in our case with indendent noise corruption."}, {"title": "3.2. Non-Markovian Inference Process", "content": "We train the non-Markovian reverse model po (Xt\u22121 | Xt:T) by minimizing a weighted ELBO objective derived from the variational perspective of diffusion:\n$\\mathcal{L}_{non-markov} = \\mathbb{E}_{x_{1:T}\\sim q(x_{1:T}|x_0)} \\left[ \\sum_{t=1}^{T} w_t \\log p_{\\theta} (x_{t-1} | x_{t:T}) \\right]$ (11)\nHere, the key difference from standard discrete diffusion is that each term conditions on xt+1:T. By Theorem 3.1, this inclusion strictly reduces the conditional entropy when the forward process is indeed non-Markovian. Consequently, Po (Xt-1 | Xt:T) has a more robust denoising pathway.\nOnce trained, sampling proceeds in an autoregressive manner, iterating backward over time by conditioning on the entire future trajectory xt:T.\nxo-Parameterization. Similar to standard discrete diffusion approaches (Schiff et al., 2024; Gat et al., 2024), we adopt an xo-parameterization to simplify training. In this view, we directly predict the clean sequence x0 at each step, which leads to a simpler denoising objective:\n$\\mathcal{L}_{non-markov} = \\sum_{t=1}^{T} \\mathbb{E}_{x_{1:T}\\sim q(x_{1:T}|x_0)} \\left[\\log p_{\\theta} (x_0 | x_{t:T})\\right]$\n(See Appendix for the derivation.) At inference time, we first sample xo from the learned denoiser po (x0 | Xt:T) and then use the forward kernel q(\u00b7 | X0) to obtain xt\u22121. This procedure iterates backward through time until we reach x0. Pseudocode is presented in Algorithm 1."}, {"title": "4. CaDDi: Causal Discrete Diffusion Model", "content": "The autoregressive property of non-Markovian inference (Section 3) naturally lends itself to a causal model that predicts tokens in a left-to-right fashion. Motivated by this insight, we propose CaDDi, a causal language model that"}, {"title": "4.1. Unified Sequential and Temporal Modeling", "content": "In the non-Markovian setting (Section 3), the reverse process is inherently autoregressive: we decode the entire sequence of latent states xt:T to retrieve xt-1. This naturally aligns with the standard left-to-right generation of language models. However, unlike a conventional single-dimensional token sequence, we must now account for a temporal dimension.\nConstructing the Training Trajectory. To accommodate both sequential and temporal dependencies in a single model, CaDDi constructs a non-Markovian forward trajectory for each data instance:\n((x),...,x),(x1,...,x)),\nwhere the upper index (i) denotes the token position in the original sequence of length L, and the subscript denotes the diffusion timestep. We then train a causal language model to predict the next token at each position via a standard next-token prediction loss. Crucially, the target for each position i is always the original clean token x(2) (2) under xo-parameterization.\nContext Window. In practice, feeding all timesteps into the model can be prohibitively large. Therefore, we restrict the model's temporal context to the most recent n timesteps. More formally, for each timestep t and token position i, we define the context Ct,i as follows:\nCt,i = {x() | t \u2212 1 < r < min(t + n,T), 0 \u2264 j \u2264 L}\n\u222a {x | 0 \u2264 j < i} (13)\nwhere is the predicted clean token\u00b2 at position j at current timestep t. The token-level training objective is then:\nLtoken = E [-log pe (x) | Ct,i)]. (14)\nThis design ensures that CaDDi captures both local temporal structure and token-level dependencies in a scalable manner. In practice, there is a inherent trade-off between scalability and the ability to model long-range temporal dependencies. We found n = 4 a reasonable choice through empirical evaluation.\n\u00b2In teacher-forced training, this is the ground truth clean token"}, {"title": "4.2. 2D Rotary Positional Encoding for Sequence Position & Diffusion Timestep", "content": "A modern causal language model typically encodes only the sequential dimension, via rotary positional encodings (Su et al., 2024). However, for non-Markovian discrete diffusion, we must capture not just the standard token-level sequence but also a temporal dimension corresponding to diffusion timesteps. To address this, we extend the original 1D rotary scheme to a 2D variant, allowing the model to incorporate positional information across both the sequence index i and the diffusion timestep t. This enhanced encoding enables the model to more effectively learn joint dependencies between tokens and their progression through multiple diffusion steps.\nSpecifically, standard RoPE in modern language models like Pythia (Biderman et al., 2023b) rotates a subset of the query/key dimensions according to the token position i. If R(i) denotes the rotation matrix parameterized by i, the attention weight between positions i and j becomes\n(R(2)q(i)) (R(i)k(i)), where\nR(i) =\n$\\left(\\begin{array}{ccc}\\cos (i\\theta_{12}) & - \\sin (i\\theta_{12}) & 0\\\\\\sin (i\\theta_{12}) & \\cos (i\\theta_{12}) & 0\\\\0 & 0 & :\\\\0 & 0 & :\\end{array}\\right)$.\nAs shown in Figure 2, we generalize this approach by introducing additional rotation for the timestep dimension:\nR() =\n$\\left(\\begin{array}{cccc}\\cos (i\\theta_{12}) & - \\sin (i\\theta_{12}) & 0 & 0\\\\\\sin (i\\theta_{12}) & \\cos (i\\theta_{12}) & 0 & 0\\\\0 & 0 & \\cos (i\\theta_{34}) & - \\sin (i\\theta_{34})\\\\0 & 0 & \\sin (i\\theta_{34}) & \\cos (i\\theta_{34})\\end{array}\\right)$.\nThis 2D encoding allows the model to disentangle positional-based rotation (the token dimension i) from temporal-based rotation (the temporal dimensiont), letting CaDDi jointly reason about sequential and temporal positions.\nConsistency with Standard Language Modeling. By interleaving temporal based rotation in these additional dimensions, it's easy to observe that when two tokens share the same timepoint t:\n(R) (RK) =qRj-k()\nT = (R(q)) (R3)k())\nwhich means that in the same timepoint the sequential attention pattern is identical to that of a conventional causal language model, and R (i) reduces to the usual (1D) rotation in i."}, {"title": "4.3. Adapt LLMs for Discrete Diffusion", "content": "A key observation is that standard causal language modeling can be seen as a special case of our proposed framework under particular settings: namely, a single-step diffusion (T = 1) and a minimal context window restricted to the current timestep. When T = 1, the forward trajectory is simply x0, and the reverse process is a single-step denoising process which autoregressively predicts next clean token, closly mirroring the standard language modeling paradigm.\nMoreover, as shown in 4.2, the 2D rotary positional encoding can be seamlessly integrated into existing language models, allowing for a unified treatment of both sequential and temporal dimensions. Given these equivalence, one can take a pretained LLM (trained in a standard causal fashion) and further fine-tune it under our non-Markovian diffusion objective. By allowing the model to condition on the previous timesteps in the diffusion chain xt+1:T, we equip it with iterative denoising capabilities beyond standard next-token prediction. This straightforward adaptation:\n\u2022 Expands Generation Modes: The LLM can perform text infilling or partial prompting from arbitrary positions, rather than strictly appending text at the end, as shown in Figure 3.\n\u2022 Leverages Pretraining Knowledge: Since large LLMs are already trained on vast corpora, fine-tuning under our discrete diffusion objective benefits from a strong initialization and broad linguistic knowledge.\n\u2022 No Architectural Changes: We only replace the original (causal) loss with a non-Markovian diffusion loss and provide noise-corrupted sequences as training data, preserving the underlying transformer structure."}, {"title": "4.4. Inference Bottleneck of Naive CaDDi", "content": "Naive CaDDi inference can be slower than standard discrete diffusion, typically requiring O(L\u00d7T) function evaluations for a sequence of length L over T timesteps. However, by leveraging the unique properties of causal language modeling, we propose a semi-speculative decoding strategy that substantially reduces inference time while maintaining generation quality.\nSemi-Speculative Decoding. Although causal language models generate tokens sequentially, they can verify the"}, {"title": "6. Conclusion", "content": "We introduced CaDDi, a causal discrete diffusion framework that relaxes the traditional Markovian assumptions in favor of an autoregressive inference process. By explicitly conditioning each denoising step on the entire future trajectory, CaDDi captures richer temporal dependencies and leverages iterative refinement. Critically, our approach can also be built atop existing causal language models- bridging standard sequence modeling with powerful diffusion capabilities\u2014while preserving both knowledge from large-scale training and the flexibility of iterative editing."}, {"title": "Impact Statement", "content": "This paper aims to advance the field of generative models. Beyond the established ethical considerations in this area\u2014such as potential biases\u2014our approach does not introduce any unique risks. The current scope and scale of our work are not sufficient to pose significant concerns in these areas."}, {"title": "5. Experiments", "content": "Baseline. We compare CaDDi with several established discrete diffusion models, including D3PM (Austin et al., 2023), SEDD (Lou et al., 2024), MDLM (Sahoo et al., 2024), UDLM (Schiff et al., 2024), and Discrete Flow Matching (Gat et al., 2024). For MDLM and UDLM, we utilized their official implementations, which also include implementations of D3PM and SEDD. Additionally, we implemented Discrete Flow Matching based on the MDLM repo with the discrete path and denoising loss defined in (Gat et al., 2024).\nOur core causal language model is based on Pythia-160M (Biderman et al., 2023a), with a customized tokenizer and embedding layers tailored to the specific task. For baseline models, we follow their original implementations, using Diffusion Transformers (Peebles & Xie, 2023) with rotary positional encoding and timestep embeddings. To ensure a fair comparison, we match the number of learnable parameters across all models, with CaDDi using slightly fewer parameters (see Table 4 for details). All discrete diffusion and flow-matching models use 1000 diffusion steps, while CaDDi is configured with a context window of 4 time points and 64 diffusion steps (see Section D for an ablation study). All models are trained with a learning rate of 3e-4, 2500 warm-up steps, and linear learning rate annealing. For evaluation, we sample the same number of sequences across all models."}, {"title": "5.1. Biological Sequence Generation", "content": "We test the sequence generation capability of CaDDi on the AcyP protein dataset, comprising 26,878 protein sequences from the Acylphosphatase family, each containing 64 to 128 residues, sourced from UniProt (Consortium, 2024).\nMetrics We assess the quality of generated sequences using the following metrics: pLDDT (Jumper et al., 2021) and self-consistency perplexity (scPPL), which measure the feasibility of sequences to fold into stable protein structures; and TM-score, RMSD, and H-prob, which evaluate the structural similarity of generated sequences to known structures in the PDB database (Berman et al., 2000). Detailed descriptions of these metrics are provided in C.1.\nAs shown in Table 1, CaDDi consistently outperforms all baseline models across all metrics. High pLDDT scores and low scPPL values indicate that sequences generated by CaDDi are highly likely to fold into viable protein structures, as visualized in Figure 6. Furthermore, TM-score, RMSD, and H-prob demonstrate that CaDDi generates realistic sequences with strong homology to known structures in the PDB database.\nWe test CaDDi's capability in modeling more complicated sequences on One Billion Words (Chelba et al., 2014), a large, real world natural language dataset consisting over 30M English language sentences with varying lengths. We follow the tokenization and training setup in DiffusionBert (He et al., 2022). For UDLM, we directly use pretrained"}, {"title": "5.2. Unconditional Text Generation", "content": "Metrics To evaluate the quality of model-generated texts, we report guided generative perplexity, a refined version of generative perplexity that evaluates texts within a natural language context. This adjustment helps mitigate the degenerate behaviors observed with standard generative perplexity (Holtzman et al., 2020) The guided generative perplexity is computed using various large language models, including GPT-2 (Radford et al., 2019), Llama-2 (7B parameters) (Touvron et al., 2023b), and Llama-3 (3B parameters) (Dubey et al., 2024), all of which are pretrained on large natural language corpora. To further evaluate the diversity of generated texts, we compute the self-BLEU (Zhu et al., 2018) score of the set of generated texts. Details of the metrics can be found in C.1).\nCaDDi achieves strong generative perplexities across all three large language models evaluated, outperforming baselines in all but one case with only a marginal difference. This demonstrates CaDDi's capability in generating coherent text unconditionally. Additionally, CaDDi achieves a comparable self-BLEU score with other models, highlighting its ability to generate diverse and coherent text samples. As shown in Figure 4, with the help of semi-speculative decoding, our model can achieve better performance while maintaining an inference cost comparable to other models."}, {"title": "5.3. Conditional Text Generation", "content": "We evaluate conditional text generation on the Amazon Polarity dataset (McAuley & Leskovec, 2013), which consists of 3.6M Amazon reviews labeled as positive or negative. We adapt this task as text infilling by prepending a label-based prompt to each review (see Appendix C.2 for details). We train the conditional generator pe (xo xt, y) alongside unconditional one po (xo | xt), by preserving certain parts of the text as fixed."}, {"title": "D. Ablation Study", "content": "We conduct an ablation study to investigate the impact of the number of diffusion time points on CaDDi's performance. Due to limited computational resources, we did abalation study on a subset of the AcyP protein dataset, as a result the pLDDT scores and scPerplexity appear better than the main results, while varying the number of diffusion time points. Our results demonstrate that increasing the number of diffusion time points significantly improves the generation quality of CaDDi."}]}