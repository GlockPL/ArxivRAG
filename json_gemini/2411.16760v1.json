{"title": "LibraGrad: Balancing Gradient Flow\nfor Universally Better Vision Transformer Attributions", "authors": ["Faridoun Mehri", "Mahdieh Soleymani Baghshah", "Mohammad Taher Pilehvar"], "abstract": "Why do gradient-based explanations struggle with Trans-\nformers, and how can we improve them? We identify gradi-\nent flow imbalances in Transformers that violate FullGrad-\ncompleteness, a critical property for attribution faithfulness\nthat CNNs naturally possess. To address this issue, we in-\ntroduce LibraGrad\u2014a theoretically grounded post-hoc ap-\nproach that corrects gradient imbalances through pruning\nand scaling of backward paths, without changing the for-\nward pass or adding computational overhead. We evaluate\nLibraGrad using three metric families: Faithfulness, which\nquantifies prediction changes under perturbations of the\nmost and least relevant features; Completeness Error, which\nmeasures attribution conservation relative to model out-\nputs; and Segmentation AP, which assesses alignment with\nhuman perception. Extensive experiments across 8 archi-\ntectures, 4 model sizes, and 4 datasets show that LibraGrad\nuniversally enhances gradient-based methods, outperform-\ning existing white-box methods\u2014including Transformer-\nspecific approaches\u2014across all metrics. We demonstrate\nsuperior qualitative results through two complementary\nevaluations: precise text-prompted region highlighting on\nCLIP models and accurate class discrimination between\nco-occurring animals on ImageNet-finetuned models\u2014two\nsettings on which existing methods often struggle. Libra-\nGrad is effective even on the attention-free MLP-Mixer ar-\nchitecture, indicating potential for extension to other mod-\nern architectures.", "sections": [{"title": "1. Introduction", "content": "Understanding how deep learning models make decisions\nis crucial for deploying them in critical applications such as\nhealthcare and autonomous driving. Input attribution meth-\nods, which quantify the influence of individual input fea-\ntures on a model's output [12, 47, 48, 66], help us under-\nstand a model's decision for a single input and also serve\nas building blocks for advanced explanation techniques like\nCRAFT [31]."}, {"title": "2. Background and Related Work", "content": "Given a multi-output neural model, let \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) be\na selected output function. For instance, if Model(x) =\n(P1, ..., Pk) represents class probabilities, we might choose\n\\(f(x) = p_i\\) to analyze the model's prediction for the i-th\nclass. An attribution method A generates relevance scores\n\\(A(f)(x_i)\\) for each feature xi."}, {"title": "2.1. Gradient-Based Attribution Methods", "content": "Input \u00d7 Grad. IxG [4, 71, 72] assigns feature relevance\nby \\(IxG (f)(x) = x \\odot \\nabla_x f(x)\\), where \\(\\odot\\) denotes element-\nwise multiplication.\nFullGrad. Expanding on Input \u00d7 Grad, FullGrad [75] in-\ncludes not only the input features but also the bias terms of\neach layer in the neural network."}, {"title": "2.2. Other Attribution Methods", "content": "In addition to the primary gradient-based methods\nabove, we apply LibraGrad to several other general-\npurpose gradient methods, including HiResCAM [26],\nGradCAM PLUS (henceforth GradCAM+) [41, 49, 67],\nand XGradCAM+ PLUS (henceforth XGradCAM+) [33,\n49]. We further apply it to hybrid attention-gradient ap-\nproaches specifically designed for Transformer architec-\ntures: GenAtt (also known as GAE) [16], TokenTM [87],\nand AttCAT [61]. To ensure a comprehensive evalua-\ntion, we also compare against attention-based attribution\nmethods RawAtt [15, 17, 35], Attention Rollout [1], and\nDecompX-NoBias (henceforth DecompX) [52], as well as\nTransformer-specific Layer-Wise Relevance Propagation\n(LRP)-based [6] techniques Conservative-LRP (henceforth\nAliLRP) [3] and AttnLRP [2]. For a detailed overview of\nrelated work, see Appendix E."}, {"title": "3. Method", "content": "Understanding how input features contribute to a model's\noutput is a central goal of attribution methods. For gradient-\nbased attributions to be faithful, they must accurately reflect"}, {"title": "3.1. FG-Completeness of Classical Architectures", "content": "We begin by demonstrating that classical convolutional neu-\nral networks (CNNs) and multilayer perceptrons (MLPs)\nsatisfy FG-completeness, which explains why gradient-\nbased attribution methods are effective for these architec-\ntures. First, we introduce the concept of a locally affine\nfunction."}, {"title": "3.2. Non-Locally-Affine Layers in Transformers", "content": "Despite the FG-completeness of classical architectures,\nmodern Transformer models introduce several non-locally-\naffine operations that disrupt this property:\n1. Gated Activations: Functions like GELU and SiLU\n(Swish) [69] involve non-linear gating mechanisms.\n2. Attention Mechanisms: Self-attention and cross-\nattention layers perform weighted averaging based on\nnonlinear attention scores.\n3. Multiplicative Feature Fusions: Operations such as\nself-gating (e.g., SwiGLU [69], MambaOut [91]) involve\nelement-wise multiplication of different feedforward\nbranches.\n4. Normalizations: LayerNorm divides by the standard\ndeviation, introducing a division operation.\nThese operations involve multiplicative (of which divi-\nsion is a special case) interactions and non-linear trans-\nformations that break the linearity required for FG-\ncompleteness, leading to imbalanced gradient flow and at-\ntribution failures, as we will discuss in the next section."}, {"title": "3.3. Analysis of Gradient Flow Imbalance", "content": "We now analyze how each non-locally-affine operation af-\nfects gradient flow. First, consider the element-wise multi-\nplication of two FG-complete functions:"}, {"title": "3.4. LibraGrad: Theoretical Foundations", "content": "We now develop theoretical solutions to restore balanced\ngradient flow."}, {"title": "3.5. LibraGrad: Practical Implementation", "content": "Libra Neural Operations. We now define FG-complete\nversions of common non-affine operations:\nLibra Attention: In attention mechanisms, we re-\nstrict gradient propagation to the value branch exclusively,\nrendering this operation locally affine and therefore FG-\ncomplete (Theorem 1):\nLibra Gated Activation: For gated activations like\nGELU and SiLU, we discard the non-linear gate's gradient:\nLibra Self-Gating: In self-gating operations like\nSwiGLU, the input flows through dual parallel feedforward\npaths (f1, f2) and reunifies via element-wise multiplication.\nTo balance the gradient flow between branches, we scale\neach branch's gradient by:\nUniversal Improvement. While our theoretical discus-\nsion focuses on achieving FG-completeness, empirical\nresults demonstrate that LibraGrad's gradient balancing\nmechanism universally enhances gradient-based attribution\nmethods. Intuitively, this is because standard gradient flow\nsuffers from two fundamental flaws: it overemphasizes lo-\ncally sensitive modules and assigns counterproductive nega-\ntive signals to denominators in operations like LayerNorm."}, {"title": "4. Experiments", "content": "We evaluate LibraGrad through three complementary met-\nrics: Faithfulness, Completeness Error, and Segmentation.\nFor statistical validity, we report standard deviation upper\nbounds for all empirical results. In tables, we denote the\nbest and second-best results in each column with bold and\nunderline formatting, respectively."}, {"title": "4.1. Experimental Setup", "content": "Our evaluation spans two dimensions:\n\u2022 Architectures: Eight model families (ViT [25],\nEVA2 [28, 29, 76], BEiT2 [7, 59], FlexiViT [11],\nSigLIP\u00b9 [92], CLIP [62], DeiT3 [80, 81], MLP-\nMixer [79]), using their largest ImageNet-1k [24]\nfintuned variants.\n\u2022 Model Sizes: All ViT variants: tiny (ViT-T), small (ViT-\nS), base (ViT-B), and large (ViT-L).\nFaithfulness Metrics. We evaluate various attribution\nmethods using faithfulness metrics, which quantify how ac-\ncurately the attribution scores reflect the importance of in-\nput features in the model's predictions. These widely used\nmetrics [13, 20, 32, 49, 52, 54, 87] measure changes in\nmodel behavior as we progressively occlude input features\nin different orders. Here, we report the Most-Influential-"}, {"title": "4.2. Quantitative Results", "content": "Our evaluations demonstrate that LibraGrad universally en-\nhances gradient-based attribution methods across all tested\nmodels, architectures, and datasets (see Appendix D for\ncomprehensive results). Significant improvements are ob-\nserved in both faithfulness and segmentation metrics (Ta-\nbles 6 and 3), and Libra FullGrad achieves optimal Com-\npleteness Error (Table 4). These enhancements remain\nconsistent across different model scales (Appendix D.3)\nand datasets (Table 2, Appendix D.4), and extend to the\nattention-free MLP-Mixer (Appendix D.5.1), validating\nthat gradient flow imbalance, not attention mechanisms, is\nthe core issue.\nIntegrated Gradients. We also extend IG [77] and com-\npose it with other gradient-based methods, and compare the\nuniversal improvement aspect of LibraGrad and IG in Ap-\npendix D.1, showing that LibraGrad vastly outperforms IG.\nDue to numerical instability, the practical approximation of\nIG fails to meet its theoretical promise of completeness rel-\native to the zero baseline (Table 4). Furthermore, we prove\nthat the numerical instability observed is theoretically un-\navoidable for a fixed-step approximation (Proposition 5 in\nthe Appendix).\nGeneral-Purpose Methods Are Enough. Once gradi-\nent flow is corrected, the general-purpose FullGrad+ out-\nperforms Transformer-specific methods like GenAtt, To-\nkenTM, and AttCAT across most metrics and models, with\nonly a few exceptions where its performance remains com-\npetitive. This suggests that specialized architectures may\nnot require specialized attribution methods when gradient\nflow is properly balanced.\nAblation Studies. Our ablation study (Table 5) reveals\nthree key insights: First, while gated activations theoret-\nically break FG-completeness (Proposition 4), their prac-"}, {"title": "4.3. Qualitative Analysis", "content": "We evaluate Libra FullGrad+ through two complementary\nscenarios: (1) text-prompted region attribution using CLIP\nmodels, demonstrating precise localization of prompted el-\nements in complex scenes (Fig. 1, Appendix C.1), and (2)\nclass discrimination on COCO [46] images, showing accu-\nrate distinction between co-occurring animals (Fig. 2, Ap-\npendix C.2). Both reinforce our quantitative findings that\nproper gradient flow enables general-purpose methods to\noutperform specialized approaches. Detailed protocols are\nin Appendix B.4."}, {"title": "5. Conclusion", "content": "We introduced LibraGrad, correcting gradient flow im-\nbalances via pruning and scaling backward paths. FG-\ncompleteness, formalized here, ensures attributions decom-\npose outputs faithfully. We prove that while classical\nCNNs were naturally FG-complete (explaining their histor-\nical success with gradient-based methods), several opera-\ntions in modern Transformers break this property. We pro-\nvide both theoretical proofs for restoring FG-completeness\nand practical solutions that require no forward-pass mod-\nifications. Empirically, LibraGrad universally enhances\ngradient-based attributions across architectures, model\nsizes, and datasets, enabling general-purpose methods like\nFullGrad+ to outperform Transformer-specific approaches.\nThis suggests that specialized architectures may not require\nspecialized attribution methods when gradient flow is prop-\nerly balanced. Our qualitative results further validate this\ninsight. Future work can explore compositions with other\ngradient-based methods, applications as a gradient regular-\nizer, and extensions to emerging architectural innovations."}]}