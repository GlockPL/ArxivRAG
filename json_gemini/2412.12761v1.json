{"title": "Revealing the impact of synthetic native samples and multi-tasking strategies in Hindi-English code-mixed humour and sarcasm detection", "authors": ["Debajyoti Mazumder", "Aakash Kumar", "Jasabanta Patro"], "abstract": "In this paper, we reported our experiments with various strategies to improve code-mixed humour and sarcasm detection. We did all of our experiments for Hindi-English code-mixed scenario, as we have the linguistic expertise for the same. We experimented with three approaches, namely (i) native sample mixing, (ii) multi-task learning (MTL), and (iii) prompting very large multilingual language models (VMLMs). In native sample mixing, we added monolingual task samples in code-mixed training sets. In MTL learning, we relied on native and code-mixed samples of a semantically related task (hate detection in our case). Finally, in our third approach, we evaluated the efficacy of VMLMs via few-shot context prompting. Some interesting findings we got are (i) adding native samples improved humor (raising the F1-score up to 6.76%) and sarcasm (raising the F1-score up to 8.64%) detection, (ii) training MLMs in an MTL framework boosted performance for both humour (raising the Fl-score up to 10.67%) and sarcasm (increment up to 12.35% in Fl-score) detection, and (iii) prompting VMLMS couldn't outperform the other approaches. Finally, our ablation studies and error analysis discovered the cases where our model is yet to improve. We provided our code for reproducibility.", "sections": [{"title": "1 Introduction:", "content": "Humour and sarcasm are complex and subjective emotions that impact the nature of human communication. They can appear in different forms such as exaggeration, dark humour, gross humour, adult or slang expression, insult, offense, etc. (Frenda et al., 2018; Ahuja, 2019). Past study (Bleakley and Sailofsky, 2023) highlighted how they can affect politics amid tragedy. Thus, detecting these emotions is essential. It is also challenging, especially in a code-mixed setting, where two languages are mixed in a single sentence. This is because models now need to understand the context expressed through language alteration and detect humour and sarcasm in it. Code-mixing is a linguistic phenomenon common across multilingual speakers. Multilingual speakers are believed to outnumber monolingual speakers globally (Tucker, 1999). A large chunck of population in Asia, Europe, and North America know more then one language, this allows them to communicate among themselves by switching languages in a single utterance. It is more prominent in informal communications like social media posts and voice mails (Patro et al., 2017). An example of humorous and sarcastic expression in Hindi-English code-mixed language is given in the following. More examples are presented in Figure 1 of Appendix B. In the following example, the English parts are marked in red and Hindi parts are marked in blue. We have provided translations for readability.\n\u2022 Humor: Never take a moral high ground. Wahan railing nahi hai aur kabhi bhi gir sakte hain.\n(Translation: Never take a moral high ground. There are no railings and one can fall at any time.)\n\u2022 Sarcasm: Kuch logo ka number iss liye save krte hain ki galti se uth naa jaye.... #sarcasm\n(Translation: Some people save their numbers so that they don't get called by mistake.... #sarcasm)\nThe NLP community has shown significant interest in humour and sarcasm detection,"}, {"title": "2 Datasets:", "content": "In this section, we reported the details of code-mixed and native datasets considered in our study. Please note that we ignored the datasets containing dialogues and multi-modal samples to simplify our task formulation. The list of the datasets and their basic statistical details were reported in Table 1. The dataset examples were illustrated in Figure 1 in the appendix. A brief description of the individual datasets were reported in Appendix B. Apart from the class distribution, we also provided the Kullback-Leibler (KL) divergence (Kullback and Leibler, 1951) values for the individual datasets in Table 1. The KL divergence measures the difference between two probability distributions. Here, it quantifies lexical variation, i.e., how the word distribution in class differs from the other. A higher value of KL divergence suggests that the classes are well-separated in terms of word distributions. We also reported the fraction of total samples containing English hurtful (offensive, aggressive and hateful) keywords in each class. We used the lexicon given by Bassignana et al. (2018) to calculate it. Finally, we also reported the inter-annotator agreement (IAA) scores published with individual datasets."}, {"title": "3 Experiments:", "content": "In this section, we reported the details of our experiments conducted as a part of this study. Apart from reproducing the baselines (section 3.1), we designed three experiments each based on a unique research philosophy. The three philosophies are i) native sample mixing, ii) multi-task learning and iii) prompting very large multilingual language models (VMLMs). While the native sample mixing strategy intends to improve the code-mixed tasks by adding monolingual native samples to the code-mixed training sets, the multi-task learning strategy tries to do the same by learning linguistic knowledge from the samples of a third task. Our last strategy, i.e. prompting VMLMs, evaluates the performance of very large multilingual language models for the considered code-mixed tasks in a few-shot context prompting scenario. The detailed experimental set up is given in Appendix D.1."}, {"title": "3.1 Baselines:", "content": "In this section, we reported the previously proposed best performing methods as baselines. They were proposed for code-mixed humour and sarcasm detection in Hindi-English code-mixed scenario. Please note that some baseline papers did not share their code; thus, we reimplemented them to the best of our knowledge. Further, we made our codebase public for reproducibility. In the results section, we reported both the reproduced and original results as per the baseline paper. However, we considered the reproduced results for comparative analysis.\nHumor: We considered two previous works published by Agarwal and Narula (2021) and Muttaraju et al. (2022) as our baselines. Further details of individual approaches are provided in Appendix - C.1.\nSarcasm: We considered two previous works published by Pandey and Singh (2023) and"}, {"title": "3.2 Exp. 1- Impact of mixing native language samples:", "content": "Our first experiment explored the impact of native language samples after adding them to the code-mixed training sets. Past study (Mazumder et al., 2024) reported that this strategy works for code-mixed hate detection. However, no one tested it for code-mixed humour and sarcasm detection. Further, in our case, even though there are publicly available English humour and sarcasm datasets (Table 1), we couldn't find any Hindi datasets for the same. Thus, we choose to create silver annotated datasets by translating some portion of English datasets to Hindi using Google Translator API\u00b9. From the methodological point of view, we considered two types of models.\n\u2022 Statistical classifiers: We considered three statistical classifiers i.e. Naive Bayes (NB), Random Forest (RF), and Support Vector Machine (SVM) in our study. NB is known to perform better when there is high KL divergence between classes. We utilized word-level unigrams, bigrams, and trigrams as features. Past works showed these features to perform best (Khandelwal et al., 2018; Swami et al., 1805).\n\u2022 Multilingual Language Models (MLMs): We also considered four widely used MLMs for our study. They are, i) mBERT(Devlin et al., 2019), ii) XLM-R(Conneau et al., 2019), iii) MuRIL(Khanuja et al., 2021) and iv) IndicBERT(Doddapaneni et al., 2023). Out of them, mBERT and XLM-R are general-purpose MLMs trained on 100+ languages, while MuRIL and IndicBERT are specialized models specifically trained for Indic languages. We froze all but the last four layers of MLMs during fine-tuning."}, {"title": "3.3 Exp. 2- Multi-task learning:", "content": "In our second approach, we explored the efficiency of multi-task framework to detect code-mixed humour and sarcasm by learning linguistic knowledge from the samples of a third task, i.e., here it is, hate detection. We chose hate detection as a third task because (i) it is semantically related to humour and sarcasm, (ii) samples of humour and sarcasm datasets contained hateful keywords (refer Table 1), and (iii) we could found Hindi, English and code-mixed samples available for this task. Our multi-task framework is inspired by the framework proposed by Rotman and Reichart (2022). They utilized a BERT-like architecture divided into two parts. The bottom module consists of eight lower layers of a transformer-based language model like BERT were common to the participating tasks. The top module, containing top four layers, were separately present for the individual tasks. Apart from that there is an additional top module present for parameter sharing. A gating mechanism connects all of the task-specific top modules with the additional top module. Authors experimented this framework on tasks like dependency parsing and named entity recognition and found that it performs better. In our case (i) we added a regularization term for soft parameter sharing of the final layers from the top module, and (ii) we froze the parameters of the bottom module during fine-tuning. The architecture of our framework is shown in Figure 2. We used embeddings from three widely used MLMs, i.e., (i) mBERT, (ii) XLM-R and (iii) MuRIL to initialize the layers of our MTL framework. We conducted an ablation study (refer Appendix F.1) to examine the role of the gating mechanism within the MTL models by removing the gate and comparing the performance with models that included the gating mechanism."}, {"title": "3.4 Exp. 3- Impact of in-context learning on very large multilingual language models:", "content": "In our third experiment, we evaluated the performance of very large multilingual language models in detecting humour and sarcasm in code-mixed texts. As their name suggests, they have a lot of parameters, thus, they are designed to work with prompting approaches rather than fine-tuning them. We utilized four VMLMs: (i) Gemma (Team et al., 2024), (ii) Aya Expanse (\u00dcst\u00fcn et al., 2024), (iii) Llama 3.1 (Dubey et al., 2024) and (iv) GPT-4 (Achiam"}, {"title": "4 Results and discussion:", "content": "As the datasets are not balanced, we evaluated our models and the baselines using the F1-score(F1). The reported values are the average of three random seeds over separate runs. In the following subsections, we reported the results of our baseline and three experiments. Statistically significant results (p < 0.05) are identified with an '*' mark."}, {"title": "4.1 Baseline results:", "content": "Most of the baseline papers reported accuracy values as their performance measures. However, since the datasets are class-imbalanced, we believe F1-scores best measures their performance. We implemented the baselines and reported the F1-scores in Table 2. Out of all methods, we found that IndicBERT performed best for humour detection. Similarly, LSTM-BERT gave the best result for sarcasm detection."}, {"title": "4.2 Observations from Exp. 1:", "content": "In this section, we reported our observations from the first experiment. The F1-scores obtained from all models over the considered datasets and training scenarios are reported in Table 3. The F1-scores of best-performing models for the individual training scenarios (column-wise in Table 3) were marked in bold, while the second-best results are underlined. Similarly, the best-performing scenarios giving the highest F1 scores for individual models (row-wise in Table 3) were kept inside the parenthesis, while the second best scores are marked with '#' superscript. The highest score for both tasks across all training scenarios and models are marked in blue. Following are our takeaways,\n\u2022 When trained with only code-mixed samples, mBERT gave the highest F1-score of 0.78 for humour detection, followed by XLM-R and MuRIL with an F1-score of 0.75 each. Similarly, for sarcasm detection, MuRIL reported the highest F1-score of 0.83 followed by IndicBERT and XLM-R, with an F1-score of 0.81 each.\n\u2022 On adding native samples to the code-mixed training sets, the Naive Bayes classifier showed no significant improvement in code-mixed Humour detection. The F1-scores still remained in the range of 0.74 - 0.75. On the other hand, For sarcasm detection, the performances declined sharply to the range of 0.32 0.42, compared to 0.74 when models were trained with only code-mixed samples. RF models have shown a performance decline for both code-mixed humour (decrease upto 0.04 in F1-score) and sarcasm (decrease upto 0.26 in F1-score) detection, except when synthetic Hindi samples from the HaHa dataset were added to the code-mixed training set. The improvement is (an increase of 0.01 in F1-score), however, not statistically significant. SVM models, too, have registered a decline in performance for both code-mixed humour (up to a decline of 0.07 in F1 scores) and sarcasm (up to a decline of 0.15 in F1 scores) detection after native samples were mixed with the code-mixed training sets.\n\u2022 Among the MLMs, after native sample mixing, we saw a performance decline for mBERT (up to 0.13 decrease in F1-score) in code-mixed humour detection. On the other hand, it showed significant improvement with an F1-score of 0.84 in detecting"}, {"title": "4.3 Observations from Exp. 2:", "content": "In this section, we presented our observations from the second experiment. The F1-scores obtained from all models for the considered datasets and training scenarios are reported in Table 4. The best-performing F1-scores for individual models (column-wise in Table 4) across all training scenarios were marked in bold, and the second best were underlined. Similarly, the F1-scores of best performing models for the individual training scenarios (row-wise in Table 4) were kept in parenthesis, while the second best were marked with '#' superscript. The overall best scores were marked in red. Following were our takeaways,\n\u2022 For code-mixed humor detection, MURILMTL reported the highest F1 score of 0.83, followed by XLM-RMTL (0.81) and mBERTMTL (0.80). On the other hand, for code-mixed sarcasm detection, XLM-RMTL outperformed others with 0.91 F1 score, followed by MURILMTL"}, {"title": "4.4 Observations from Exp. 3:", "content": "In this section, we reported our observations from the third experiment. Out of the range of number of few shots listed in Table 7, the VMLMs achieved the highest F1-scores with eight few-shot examples. The F1-scores obtained from all models are reported in Table 5. The F1-scores of best-performing models for the individual prompting scenarios (column-wise in Table 5) were marked in bold, while the second-best results are underlined. Similarly, the best-performing F1-scores for individual models (row-wise in Table 5) were kept inside the parenthesis, while the second best scores"}, {"title": "5 Conclusion:", "content": "From our findings, we drew the following conclusions:\n\u2022 Approach wise we got a F1-score of i) 0.79 for humor and 0.89 for sarcasm detection using native sample mixing, ii) 0.83 for humor and 0.91 for sarcasm detection using multitask learning, and iii) 0.75 for humor and 0.78 for sarcasm detection using in-context learning. Thus, among the three strategies, MTL performed best.\n\u2022 Among these three strategies, MTL reported the most significant improvement, with F1-score increments upto 0.08 for humor and 0.10 for sarcasm. Native sample mixing followed, with increments upto 0.05 for humor and 0.07 for sarcasm, while in-context learning showed no improvement in F1-scores.\n\u2022 The ablation study highlighted the importance of the gating mechanism within the MTL framework, particularly in samples with 'shorter context lengths' and those containing 'misspelled words'."}, {"title": "6 Limitation:", "content": "In this section, we reported some of the limitations of our work.\n\u2022 Our MTL models occasionally failed on non-humorous and non-sarcastic samples. The possible reason behind it could be task and domain interference. We reported a"}, {"title": "Ethics statement:", "content": "All the datasets used in this paper are either publicly available or gathered directly from corresponding author with a permission to use for research purpose. No new data collection or annotation was done as part of our work, and hence we aren't releasing any dataset. It is important to note that the paper may contain offensive, mockery or discriminatory language towards certain individuals or groups. We acknowledge this and want to clarify that we do not agree with or support these views in any way. We strictly adhere to the Google Translate API's Terms of Service\u00b3 for generating the translated Hindi datasets."}]}