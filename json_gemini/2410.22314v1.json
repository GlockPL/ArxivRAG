{"title": "An Efficient Approach to Generate Safe Drivable Space by\nLiDAR-Camera-HDmap Fusion", "authors": ["Minghao Ning", "Ahmad Reza Alghooneh", "Chen Sun", "Ruihe Zhang", "Pouya Panahandeh", "Steven Tuer", "Ehsan Hashemi", "Amir Khajepour"], "abstract": "In this paper, we propose an accurate and robust\nperception module for Autonomous Vehicles (AVs) for drivable\nspace extraction. Perception is crucial in autonomous driving,\nwhere many deep learning-based methods, while accurate on\nbenchmark datasets, fail to generalize effectively, especially in\ndiverse and unpredictable environments. Our work introduces\na robust easy-to-generalize perception module that leverages\nLiDAR, camera, and HD map data fusion to deliver a safe\nand reliable drivable space in all weather conditions. We\npresent an adaptive ground removal and curb detection method\nintegrated with HD map data for enhanced obstacle detection\nreliability. Additionally, we propose an adaptive DBSCAN\nclustering algorithm optimized for precipitation noise, and a\ncost-effective LiDAR-camera frustum association that is re-\nsilient to calibration discrepancies. Our comprehensive drivable\nspace representation incorporates all perception data, ensuring\ncompatibility with vehicle dimensions and road regulations.\nThis approach not only improves generalization and efficiency,\nbut also significantly enhances safety in autonomous vehicle\noperations. Our approach is tested on a real dataset and its\nreliability is verified during the daily (including harsh snowy\nweather) operation of our autonomous shuttle, WATonoBus [1].", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous vehicles hold the promise of enhancing traf-\nfic safety and efficiency. Accurate and reliable perception\nis crucial for the safety of Autonomous Driving Systems\n(ADS), particularly in the context of Level 4 and Level 5\nautonomous driving [2]. Current ADS employ a range of\nsensing modalities, including cameras, LiDAR, and radar.\nDespite this technological diversity, the limitations of in-\ndividual sensors, deficiencies in perception algorithms, and\nshortcomings in the representation of perception results con-\ntinue to impede the reliability of autonomous driving perfor-\nmance. Consequently, a 2018 survey by the U.S. Intelligent\nTransportation Systems Office reveals that 50% of consumers\nremain skeptical of self-driving technologies [3]. Therefore,\nthe need to develop a reliable perception module remains\nessential.\nOne of the challenges facing the development of a reli-\nable perception module is the performance during inclement\nweather. Degraded sensor resolution under severe environ-\nmental conditions can reduce sensors' effectiveness in de-\ntecting objects accurately. In such conditions, the perception\nsystem may struggle to detect obstacles or other traffic\nparticipants, increasing the likelihood of accidents. Despite\nthe cost-effectiveness of camera sensors, they are vulnerable\nto glare and reflections, which can obscure objects in the\ncaptured images [4]. In addition, LiDARs also suffer from\nenvironmental impact, heavy data processing requirements,\nand short detection range drawback respectively [5]. To\novercome these individual sensor's limitations, sensor fusion\nworks are performed to further improve the reliability of\nperception performance. Specifically, point-cloud and image\nfusion techniques are heavily studied, these works employed\ndifferent sensing modalities for detecting cars, pedestrians,\nand cyclists [6], [7]. Nevertheless, these existing fusion-based\nstudies have some shortcomings like dependence on accurate\ndata calibration or association. Furthermore, raw data gen-\nerally includes numerous redundant information, which can\nbe challenging to implement for real-time processing.\nThe dynamic traffic environment and diverse interacting\nobjects often pose an Out of Distribution (OOD) challenge\nto the perception system. In real-world driving, there may\nbe small animals, pedestrians in non-conventional attire, or\nspecialized construction vehicles that are not included in the\ntraining set of the perception system [8]. To address the OOD\nchallenge, robust learning techniques have been proposed to\nmake neural networks more resilient to distribution shifts\n[9], [10]. Some research focuses on real-time detection of\nOOD cases to enable a safe transition to manual driving [11].\nHowever, validating robustness against real-world driving\ndistributions is challenging. In recent years, researchers have\nexplored approaches to bypass the OOD problem using\nmore effective representations of perception results, such as\noccupancy grids or drivable space, to mitigate the impact of\nunknown classes.\nThe contributions of this paper are highlighted as follows:\n\u2022\nAn adaptive ground removal and curb detection method\nto accommodate OOD cases like snow piles, small ani-\nmals and irregular shaped obstacles, to enhance safety.\n\u2022\nAn adaptive and efficient DBSCAN clustering robust\nto precipation noises e.g. snow and rain. The approach\nis able to maintain a consistently low false alarm\nrate in harsh weather through adjusting the DBSCAN\nparameters based on the scanning pattern of the LiDAR.\n\u2022\nA cost based LiDAR-camera frustum association con-\nsidering semantics and prior-based depth estimations"}, {"title": "II. METHODS", "content": "Our method involves a series of precise steps to determine\nthe drivable space for autonomous vehicles. Initially, the\npoint clouds are cropped to the region of interest (ROI)\ndefined by HD map data. Next, an adaptive ground removal\nmethod is proposed to segment points above the ground and\ncurb detection are performed to non-ground points with the\nguide of HD map. Following this, an adaptive clustering\nmethod is used to group the non-ground points. Concurrently,\ntwo optimized neural networks are applied to the images\nto detect lanes and objects, which are then fused with the\npreviously clustered point data. Finally, we utilize the data\nfrom obstacles, the right curb, and the centerline to precisely\ndelineate the final drivable space. The framework of our\napproach is illustrated in Fig. 1."}, {"title": "A. Inputs Pre-processing", "content": "1) Point Cloud Pre-processing, Consistent Concatena-\ntion: To achieve 360\u00b0 perception coverage, vehicles can\nare equipped with multiple LiDAR sensors, each producing\na point cloud $P_k = \\{P_1,P_2,...,P_{n_k}\\}$ where $P_i =$\n$\\{x, y, z, intensity, ring, timestamp\\}$. These clouds are trans-\nformed into a unified coordinate system G and concatenated\ninto a comprehensive cloud P. Due to motion during scans,\ntimestamps of the points are utilized to align points accu-\nrately using odometry data.\n2) HD Map Utilization: HD maps record the positions of\nthe left and right road boundaries (curbs) and the localization\ninformation is used to transform the boundaries to vehicle's\nframe. While the maps provide road boundary data, the\nprecision is not centimeter-level but sufficient for defining\ncoarse road boundaries for the perception tasks."}, {"title": "B. Adaptive Grid Ground Removal and Curb Detection", "content": "Initially, the entire point cloud is divided into grids based\non the coarse road boundaries, where each grid undergoes\na plane estimation to differentiate obstacle points from the\nground. Non-ground points close to road boundaries are\nmarked as candidate points for curb detection. For the region\nclosest to the vehicle, it uses the height of the LiDAR\nsensor as the initial plane model, then iteratively improves\nthe plane fitting. This estimation serves as the initial value\nfor ground estimation in other regions. The output for the\nadaptive ground removal is a list of estimated ground plane"}, {"title": "C. Camera based Perception", "content": "Cameras excel in capturing color and texture details but\nlack depth information. So, the images are used for 2D\ndetection, i.e., detecting features on the image plane. Camera\n2D detection, prominent in deep learning research, has\nmore public datasets compared to camera 3D detection, as\nit's much easier to label 2D features on the image plane\nthan 3D coordinates which often require complex setups\nfor depth acquisition. Moreover, 2D detection methods are\nmore consistent across different camera setups compared\nto 3D methods. In our research, two state-of-the-art neural\nnetworks are optimized and then used to provide 2D object\ndetection and 2D lane detection.\n1) YOLOv8: YOLOv8 [12] is one of YOLO (You Only\nLook Once) series of real-time object detectors. It employs\nstate-of-the-art backbone and neck architectures, making\nit achieving high accuracy with high inference speed. A\ncustomized dataset for outdoor autonomous driving usage\nhas been created by selecting interesting objects from public\ndatasets including COCO (Common Objects in Context)\n[13], Argoverse [14] and nuScenes [15]. Then the YOLOv8\nis trained on 90% of the dataset, and evaluated on the rest\n10% of the dataset, and mAP50 of 0.708 has been achieved.\nFinally, the trained model is converted to TensorRT model\nfor best inference speed.\n2) UFLDv2: UFLDv2 [16] treats the lane detection task\nas an anchor-driven ordinal classification problem using\nglobal features, it can achieve both remarkable speed and\naccuracy. The output of the UFLDv2 is a set of lane marker\npoints in the image plane, to obtain the lane marker position\nin the real world, the ground plane models F from the\nadaptive ground removal method are used to provide the\nroad geometry information, then the 3D positions of the lane\nmarkers can be inferred using the inverse camera projection\nunder the assumption that the markers are located on the\nestimated ground planes."}, {"title": "D. Adaptive Clustering for All Weather Conditions", "content": "Falling rain and snow can introduce amounts of noisy\nreflections to the LiDAR sensors. Failing to de-noise point\ncloud may lead to false detection, and even trigger the\nemergency stop. Although clustering methods like DBSCAN\n[17] can somehow ease this by setting a smaller cluster-\ning distance threshold $e$ or a larger minimum number of\npoints required to form a core region $minPts$. However, this\nwill inevitably remove points from actual objects, and also\nintroduce over-segmentation issue, where the points from\nthe same object will be clustered into several groups. We\nimprove this by proposing an adaptive DBSCAN clustering\nmethod that uses the LiDAR scanning pattern to adaptively\nadjust the $e$ and $minPts$.\nConsider a plane object with height $h$ and width $w$ posi-\ntioned at a distance of $s$ from the LiDAR, whose horizontal\nscanning resolution is $\\Delta\\varphi$ and vertical resolution is $\\Delta\\alpha$.\nFor a more general case, the point cloud is usually first\ndownsampled with a voxel size $\\Delta d$ to reduce the processing\ntime. Then the number of scan lines $N_s$ and the number of\npoints in each line $N_{pl}$ falling to the plane object will be,\n$N_s = floor(\\frac{h}{max(s\\Delta\\alpha, \\Delta d)})$ (1)\n$N_{pl} = floor(\\frac{w}{max(s\\Delta\\varphi, \\Delta d)})$ (2)\nHere, we propose an Adaptive-DBSCAN that dynamically\nadjusts the $e$ and $minPts$ based on the scanning pattern. In\nthis method, the $e$ is adjusted to have a constant $N_{pl}$,\n$N_{pl} = floor(\\frac{w_{min}}{\\Delta d})$ (3)\nwhere $w_{min}$ is the minimum width of the objects in the\nenvironment. Then the clustering distance threshold will be,\n$e(s) = max(e_{min}, N_{pl}\\Delta\\varphi s)$ (4)\nThe number of scanning lines will be,\n$N_s(s) = max(1, floor(\\frac{h_{min}}{max(s\\Delta\\alpha, \\Delta d)}))$ (5)\nwhere $h_{min}$ is the minimum height of the objects in the\nenvironment, and the $minPts = N_sN_{pl}$."}, {"title": "E. Robust LiDAR Camera Fusion", "content": "This part aims to fuse the object clustering results from\nthe LiDAR and the 2D object detection results from the\ncameras to obtain clusters with semantic information. To\nachieve this, an association cost is calculated for each cluster\nand each 2D bounding box, and the Hungarian algorithm [18]\nis used to find the best associations. The key to improving the\nLiDAR-camera fusion performance is the way to calculate\nthe association cost. The 2D association cost based on the\nintersection over union (IOU) of the camera bounding box\nand the projected cluster points bounding box is a common\nchoice due to its simplicity. However, it fails in handling\ndistant objects where the bounding box is small and the\nintersection region could be zero due to imperfect calibration\nor object motion. We fix this issue by introducing the 3D\ninformation into the association cost. '1\nA depth and its uncertainty estimation method using\nthe semantic 2D bounding box is proposed. Considering a\ncommon camera placement scheme as shown in Fig. 2, where"}, {"title": "F. Drivable Space Detection", "content": "In constructing a comprehensive representation of drivable\nspace, our method integrates the above lane and curb detec-\ntion, objects with class information, and the HD map data.\nIt accounts for ego vehicle dimensions and dynamics, object\nclass information and road regulations, thereby facilitating a\nsafe and lawful autonomous driving behavior.\nFirstly, detected objects are projected onto the HD map\nto get the identification of objects positioned within the ego\nlane, the opposing lane, on sidewalks, or at crosswalks. Sub-\nsequently, a binary occupancy map is constructed utilizing\nthe positional data of objects, lanes, and curbs. This map\nshows occupied regions, which are further adjusted based on\nobject class, as well as the dimensions and velocity of the\nego vehicle to define a safety envelope for navigation. For\nlateral expansion, the clearance required for each object is\ncomputed as $d_{class} + w_{ego}/2$, where $d_{class}$ is the predefined\ndistance relative to the object's class, and $w_{ego}$ is the width of\nthe ego vehicle. The longitudinal expansion incorporates an\nadditional term representing the minimum braking distance,\ncalculated as $v^2/(2a)$, where $v$ is the velocity of the ego\nvehicle and $a$ is the deceleration rate. Vulnerable road users,\nsuch as pedestrians and cyclists, are allocated an increased\nclearance for safety reasons. A special case is applied to\npedestrians on crosswalks, the lateral expansion is adjusted\nto span the entire road width so the ego vehicle will stop\nuntil the object walks off the crosswalk.\nThe refined binary occupancy grid is processed and sim-\nplified into a series of boundary points, which is the drivable\nspace boundary. This is achieved by building a connection\ntree starting from the ego position that grows along the lon-\ngitudinal direction and connects the safety region segments.\nThis tree is searched and gives the final left-side and right-\nside drivable space boundary."}, {"title": "III. EXPERIMENT", "content": "The performance of the proposed method is evaluated on\nthe Waterloo all-weather autonomous shuttle (WATonoBus)\n[1], which is equipped with 3 Robosense 32-line LiDARS,\nand 6 Basler cameras, a centimeter-level localization system\nTrimble APX-18 Land, and a computing unit NVIDIA Jetson\nAGX Orin. The bus travels along the 2.7 km Ringroad at\nthe University of Waterloo, while high traffic volume and\nvarying weather conditions make it a comprehensive test bed\nfor autonomous driving.\nThe evaluation dataset contains 2 loops of sunny weather,\n2 loops of heavy snow, 1 loop of light snow and a customized\ncase where traffic cones are used to dynamically modify the\ndrivable space region. The drivable space and the objects of\ninterest are manually labeled."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "Fig. 4 presents a screenshot of the ego vehicle as it\napproaches an intersection during heavy weather condi-\ntions. The pre-trained neural networks failed to detect the\npedestrian in front of the ego vehicle. This failure was\ndue to the dense noise points surrounding the pedestrian,\nwhich disrupted the detection patterns learned from datasets\ncollected under good weather conditions. In contrast, our\nmethod effectively distinguishes between noise and obstacle\npoints. It successfully detects pedestrians and generates the\ndrivable space, thereby enabling the bus to safely stop for\npedestrians crossing in front.\nIn sunny conditions, our approach achieved an MR of\n0.37% with zero false alarms, thereby highlighting the re-\nmarkable accuracy of the proposed ground removal method\nin segmenting almost all obstacle points correctly. In con-\ntrast, deep-learning methods encountered false alarms, often\ncaused by inaccurate orientation estimation for objects such\nas cars, and missed detections, primarily due to the sparsity\nof points for distant objects.\nThe Traffic Cone test case was particularly challenging\ndue to the presence of dynamically placed small-sized traffic\ncones and the movement of people, where occlusions and\nsparse point data typically lead to elevated MRs. Our method\nexhibited an MR of 3.57% without any false alarms, demon-\nstrating adeptness in managing varying point density issues\nfor the same objects at different distances. Deep-learning\nmethods performed poorly in this scenario, attributed mainly\nto the point sparsity, especially concerning the traffic cones."}, {"title": "V. CONCLUSION", "content": "In conclusion, this paper presents several significant ad-\nvancements in the perception systems of autonomous ve-\nhicles that collectively enhance driving safety and system\nreliability, particularly under challenging environmental con-\nditions. Our innovative adaptive ground removal and curb\ndetection method not only addresses small and irregular\nobstacles, but also integrates seamlessly with HD map data\nto improve detection reliability. The adaptive and efficient\nDBSCAN clustering technique we developed maintains a\nlow false alarm rate even in adverse weather conditions by\ndynamically adjusting parameters according to LiDAR scan-\nning patterns. Furthermore, our cost-based LiDAR-camera\nfrustum association method successfully mitigates the chal-\nlenges posed by imperfect calibration and synchroniza-\ntion discrepancies, proving robust across various scenarios.\nLastly, the comprehensive drivable space representation our\nsystem employs not only leverages all available perception\ndata but also respects vehicle dimensions and road laws,\nensuring that our approach is both practical and compliant\nwith existing regulations. These contributions demonstrate\na significant leap forward in the practical application of\nautonomous driving technologies, paving the way for safer\nand more reliable autonomous vehicle operations in diverse\nand unpredictable environments."}]}