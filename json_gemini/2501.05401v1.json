{"title": "BRATI: Bidirectional Recurrent Attention for Time-Series Imputation", "authors": ["Armando Collado-Villaverde", "Pablo Mu\u00f1oz", "Mar\u00eda D. R-Moreno"], "abstract": "Missing data in time-series analysis poses significant challenges, affecting the reliability of downstream applications. Imputation, the process of estimating missing values, has emerged as a key solution. This paper introduces BRATI, a novel Deep Learning model designed to address multivariate time-series imputation by combining Bidirectional Recurrent Networks and Attention mechanisms. BRATI processes temporal dependencies and feature correlations across long and short time horizons, utilizing two imputation blocks that operate in opposite temporal directions. Each block integrates recurrent layers and attention mechanisms to effectively resolve long-term dependencies.\nWe evaluate BRATI on three real-world datasets under diverse missing-data scenarios: randomly missing values, fixed-length missing sequences, and variable-length missing sequences. Our findings demonstrate that BRATI consistently outperforms state-of-the-art models, delivering superior accuracy and robustness in imputing multivariate time-series data.\nKeywords: Time Series, Imputation, Neural Networks, Attention Mechanisms, Missing Data", "sections": [{"title": "1 Introduction", "content": "Multivariate time-series data are used in a wide range of applications, such as economics [HHY11, BSP16], meteorology [SCW+15, YZZL16, ZD20], transportation [LCYL18, CC07, KV11, WCLY17, ZZQ17], health-care [CXSS18, KCS+20, MMSC20, GAG+13] or predictive maintenance [KW20, ALY19, KS17, BFRLL+21]. These data are often employed for classification and regression tasks but they frequently present missing values due to unexpected reasons, such as malfunctions in measurement devices, communication errors, or cost-saving strategies, especially in the healthcare domain. The presence of missing values can significantly degrade the performance of downstream models, making it essential to handle missing data effectively.\nThere are two common approaches to deal with missing values. The first one is deletion, which consists of removing samples that contain any missing values. Nevertheless, this practice makes the data incomplete and can destroy the time relationship of the data, especially when the missing rate is high [CPC+18a]. Additionally, removing samples with any missing values leads to biased predictions by the downstream models. The second approach is to impute the missing values, that is, to estimate the missing data using the observed values. This option offers some critical advantages over deletion. Partially observed data can still contain useful information that otherwise would be discarded. Moreover, correctly imputed values can also remove the bias that could be"}, {"title": "2 Related Work", "content": "There is a considerable amount of literature in the time series imputation field. However, we only review the approaches based in DL since it has become the predominant approach. The different approaches can be categorized into 3 groups, depending on their core layers.\n\u2022 RNN: the trend to use RNNs for imputation on time series was started by Che et al. [CPC+18a] who proposed an improvement over the Gated Recurrent Unit (GRU) called GRU-D, designed for imputing missing values in health-care data with a smooth fashion. It introduces the concept of time decay after the last valid observation, gradually diminishing the impact of the missing variable. Their proposal relies on the assumption that the"}, {"title": "3 Methodology", "content": "The proposed methodology consists of a weighted combination of two imputation blocks that process the time series in both directions. The BRATI model is trained using the Joint-optimization training approach proposed by Du et al. [DCL23] for imputing and reconstructing the time series while maintaining the consistency across both blocks of the model."}, {"title": "3.1 Multivariate Time-Series with Missing values", "content": "Given a multivariate time series with D dimensions and T observations, we denote it as X = {X1, X2, ..., XT} \u2208 RTxD. For each t \u2208 1, 2, ..., T, xt \u2208 RD represents the t-th observation for each of"}, {"title": "3.2 Joint-optimization Training approach", "content": "We follow Du et al. [DCL23] training recommendations for self-attention based imputation models. Their approach consists of two learning tasks named Masked Imputation Task (MIT) and Observed Reconstruction Task (ORT). Each task produces a loss that will be optimized during training.\nIn addition to those two losses, we also introduce a Consistency Task (CT), similar to the one used in the BRITS paper [CWL+18], to force the two branches of the model to behave in a consistent manner. Each task is described as follows:\n\u2022 Masked Imputation Task (MIT): MIT consists of artificially masking values during training: those values will be reserved and will be fed as missing values to the model, so it will impute them. Since we have the ground truth of the missing values, the imputation loss can be calculated and the model can be optimized to impute missing values as accurately as possible. By using this approach, the number of training samples gets also naturally increased, since on"}, {"title": "3.3 BRATI", "content": "Figure 3 depicts the architecture of our proposed model, Bidirectional Recurrent Attention for Time series Imputation (BRATI). It consists of two imputation blocks and a weighted combination of their representations. Each imputation block is composed of two Multi-Head Attention layers, a GRU layer and a position-wise Feed-Forward network. In both imputation blocks, the GRU layer processes the time series in different directions."}, {"title": "3.3.1 Embedding layer and Positional Encoder", "content": "Before the input features X are fed into the imputation blocks, they are concatenated to the missing mask vector M and projected into dmodel dimensions. Later on, we add a Positional Encoder vector, denoted as PosEnc to give the Self-Attention layers information regarding the position of each value. This produces the vector e that will be used by the imputation block, as noted in Eq. 7, where e will be the input for the forward imputation block and e' will be the input for the backward imputation block and Wemb \u2208 Rdmodel \u00d7D O, bemb\u2208 Rdmodel. Before the vector is fed to the block that processes the series in the backwards direction, it will be reversed. To differentiate the backward block from the forward one, it will be denoted as e'. From now on, any mathematical symbol followed by 'will denote the backwards counterpart.\ne = [Concat(X, M)Wemb + bemb] + PosEnc\ne' = Reverse e"}, {"title": "3.3.2 Attention", "content": "Similar to the original Transformer architecture proposed by Vaswani et al. [VSP+17], we apply the Scaled Dot-Product Attention. Despite being originally designed to tackle the machine language translation problem, it has also been successfully applied in other sequence processing tasks [LJX+19, ZZP+21], or even image processing [HWC+22].\nThe attention function takes three vectors as an input. They are projected into a query vector, denoted as Q, with a dimensionality of dk, and a key vector, K, with a dimensionality of dk and a value vector, V, with a dimensionality of dr. Then, the attention weights are calculated using a scaled dot-product operation between Q and K. After that, the softmax function is applied and the result is multiplied by V, obtaining an attention-weighted representation of V. The computation is described in Eq 9.\nAttention(Q, K, V) = softmax(QKT/\u221adk)V\nIn our particular case, we will work both with self-attention, and normal attention. For the self-attention case, the vectors Q, K and V are all the same whereas on normal attention they are not all the same. Moreover, we adopt the Multi-Head approach. Instead of performing a single attention function, the vectors are projected h times, where h is the number of heads, and each head performs the attention function in parallel. Finally, the obtained values are concatenated and projected one final time. This approach enables the model to attend to information from different subspaces located at different positions at the same time. Multi-Head Attention is described in Eq. 10, where headi = Attention(QW, KW, WV) and the projections are parameter matrices Wa\u2208 Rdmodel dk, WK \u2208 Rdmodel dk, WV \u2208 Rdmodel xdu, and WO \u2208 Rhdv\u00d7dmodel\nMultiHeadAttention(Q, K, V) = Concat(head1, head2, ..., headh)WO"}, {"title": "3.3.3 GRU", "content": "To exploit the advantages of RNNs when working with temporal dependencies [CPC+18a, CWL+18], we use a standard GRU layer, proposed by Cho et al. [CvMG+14] to process the input time-series. The input of each GRU unit is the hidden state computed by the previous unit, denoted as ht\u22121, and the current input Xt. The GRU layer features an update gate and a reset gate that produce the output ht for the given timestep. The computation is described in Eq. 11, where o represents the sigmoid function, ht is the hidden state at time t, xt is the input at the time t, h(t-1) is the"}, {"title": "3.3.4 Feed-Forward network", "content": "In addition to the GRU and Attention Layers, we complement them with a position-wise Feed-Forward network, as is a common practice in any Transformer-based model. It consists of two Linear layers, the first one with a different dimensionality than the input and the second one with the same dimensionality as the input to the Feed-Forward Network, so we can employ residual connections. Particularly, we use the ELU activation function between the linear layers as shown in Eq. 12. We chose the ELU activation function instead of the more usual ReLU function since it showed a slight increase in performance. This operation is applied to each time-step separately.\nFFN(x) = ELU(xW1+b1)W2 + b2"}, {"title": "3.3.5 Imputation blocks", "content": "The internal architecture of both Imputation Blocks is the same, the only difference is that the forward Imputation Block takes e as the input, while the Backwards one takes e', both vectors obtained as in Eq. 7.\nFirst, the embedded vector e is reweighted as in Eq. 13 using a Multi-Head Attention layer, following Eq. 10, to produce \u03b1. In this case, the Attention layer will work as a Self-Attention layer, since all the input vectors, Q, K and V are the same, e. Then, \u03b1 is recursively processed by the GRU layer as in Eq. 14, following Eq. 11, to produce \u03b2. The next Multi-Head Attention Layer will perform the Attention operation over both \u03b1 and \u03b2, using \u03b1 as the Query Vector (Q) and \u03b2 as the Key and Value vectors (K, V), following Eq. 15. This equation produces \u03b3, which is the output of the Multi-Head Attention layer, and \u03b3\u03c9, which denotes the attention weights averaged across all the heads. The attention weights will be used later in the weighted combination of both Imputation Blocks. On the last step, the Feed-Forward network will produce \u03b4 using the addition of \u03b3 and \u03b2as input as in Eq 16\u00b9. The Imputation Blocks can be stacked N times. Each produced \u03b4 will be the input for the following block (e). The last \u03b4 and \u03b3\u03c9 will be the ones used in the next section.\n\u03b1 = MultiHeadAttention(e, e, e)\n\u03b2 = GRU(\u03b1)\n\u03b3, \u03b3\u03c9 = MultiHeadAttention(\u03b2, \u03b1, \u03b1)\n\u03b4 = FFN(\u03b3 + \u03b2)"}, {"title": "3.3.6 Weighted Combination", "content": "After each Imputation block, we will reduce the dimensionality of the output of the Imputation from dmodel to D, the original dimensionality of X, where Wz \u2208 Rdmodel\u00d7D, bz \u2208 RD. This produces the learned representation of the input vector, X, as in Eq 17, since each Imputation block produces a different representation. In Eq. 18 we use the attention weights produced by each block \u03b3\u03c9 and Y', together with the input mask M to produce the combining weights \u03a9 \u2208 (0,1)T\u00d7D. Where W\u03a9 \u2208 R(T\u00b72+D)\u00d7D and by \u2208 RD are learnable parameters. Then, in Eq. 19 the representations of each block, X and X', will be combined into the joint-representation X; using the combining weights from Eq. 19. To obtain the final imputation \u00ce, the missing mask M is used to substitute the missing values in the original input vector X for the imputed values obtained by the model as in Eq. 20.\nX = \u03c3(Wz\u03b4) + bz\n\u03a9 = Sigmoid(Wo\u00b7 (M|yw|w) + b)\nX\u2081 = X \u00a9 \u2229 + \u00c2\u00b4 \u2299 (1 \u2013 \u03a9)\n\u00ce = X \u00a9 M + X\u2081 (1 \u2013 M)"}, {"title": "3.3.7 Loss functions", "content": "The loss that our model tries to minimize is composed of three different losses, the Masked Imputation Loss (MIL), depicted in Eq. 21, the Observed Reconstruction Loss (ORL), in Eq. 22 and a Consistency Loss, in Eq. 23. The MIL and ORL follow the joint-optimization training approach proposed by Du et al. described earlier. Additionally, we also make use of the Consistency Loss, multiplied by a consistency factor \u03c1, introduced by Cao et al. [CWL+18], to try to make the model as consistent as possible in both directions and to help it to converge faster. Finally, we combine all the tree losses, each one multiplied by a weight which can be tuned to achieve a better trade-off between the loss functions, represented as \u03bb: LMIL\u00b7AMIL, LORLAORL and Lcons\u00b7 Acons, into the total loss Ltotal, which will be minimized to improve the model's performance, as in Eq. 24.\nLORL = 1/2 MAEmasked (X, X, M) + 1/4 MAEmasked (X, X, M) + 1/4 MAEmasked (X', X, M)\nLMIL = MAEmasked (Xj, X, I)\nLcons = Discrepancy(X, X')\u00b7\u03c1\nLtotal = LMIL\u00b7AMIL + LORL\u00b7AORL + Lcons \u00b7 Acons"}, {"title": "4 Experiments", "content": "This section describes the experimental evaluation to compare the performance of our proposed architecture to previous existing models. First, the different ways in which the artificial missing values are generated, are explained. Next, the three used datasets are presented, indicating the division in the training, validation, and testing subsets. After that, the baseline models with which we compare our model are presented. Then, we introduce the evaluation metrics and the training settings for each model. Finally, the experimental results are presented."}, {"title": "4.1 Artificial generation of missing values", "content": "Since the real values of the missing data are not available to evaluate the performance of the imputation models, we artificially remove parts of the observed values in the validation and test sets and use these values as ground truth for evaluation. The goal is to train the models to handle both independent missing values and sequences of missing values, as sequences are commonly found in the target datasets. The values removed and used as ground truth are selected to simulate different missing patterns. To ensure a comprehensive evaluation, 10% and 20% of the observed data are removed for each pattern, simulating the following scenarios:\n1. Independent missing values: values are randomly removed using a uniform distribution until 10% or 20% of the observed values are eliminated, simulating a MCAR scenario.\n2. Missing in a sequence of fixed length: instead of randomly removing independent values, sequences of 5 consecutive observed values are removed. If there are insufficient sequences of 5 consecutive observed values, additional values are randomly removed to meet the target elimination rate of 10% or 20%.\n3. Missing in a sequence of random length: sequences of observed values with lengths randomly varying between 3 and 10 are removed. The length of each sequence is chosen from a uniform distribution. If there are not enough sequences of consecutive observed values, additional values are randomly removed as in the first scenario.\nExperimental results show that the second and third scenarios are considerably more difficult across all the evaluation metrics than the MCAR one, being the last case the most difficult one."}, {"title": "4.2 Datasets", "content": "We evaluate our proposed model and the baseline methods on three different real-world datasets from different domains: the PhysioNet 2012 Mortality Prediction Challenge dataset\u00b2, a Water Quality dataset\u00b3, and a Space Weather measurements dataset. General information regarding these datasets is summarized in Table 1. All the data have been standardized during the preprocessing.\n1. PhysioNet: this dataset contains healthcare data from the PhysioNet Challenge 2012 [GAG+13, SMS+12]. It consists of 12,000 records from de-identified patients in Intensive Care Units (ICU), covering approximately 48 hours of multivariate time-series data with 37 features, including blood pressure, temperature, and respiration rate. Measurements are recorded irregularly, with some collected at regular intervals (hourly or daily) and others, such as specific tests, only on-demand. This dataset has been widely used in time-series imputation works, such as SAITS [DCL23] and BRITS [CWL+18]."}, {"title": "4.3 Baseline methods", "content": "To evaluate the performance of our proposed model, we compare it against two naive imputation methods and four DL models that have previously achieved SoA results. Although the Water Quality dataset has been used to evaluate the MBGAN architecture [NC22], we chose not to include MBGAN in the comparison because it is limited to imputing a single variable, whereas the three datasets under evaluation present missing values in multiple variables.\n\u2022 Naive imputation methods: we employ two basic strategies for comparison:\nMedian imputation: missing values are filled using the median value of each column, calculated over the training set.\nLast observation carried forward: missing values are filled with the last observed value for each column. If there are no previous observations, the missing value is filled with 0.\n\u2022 Deep Learning models: we compare our proposed model to the following:\nM-RNN: a model based on bidirectional RNNs designed for multivariate time-series imputation [YZvdS18].\nBRITS: a method that models missing values as variables in a bidirectional RNN framework, considering feature correlations [CWL+18]."}, {"title": "4.4 Experimental settings", "content": "We use three metrics to evaluate the imputation performance of the models: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Relative Error (MRE). These metrics are only calculated for the values that have been artificially masked in the test sets, as the ground truth is known for these values. This corresponds to the positions where the Indicating Mask I, introduced earlier, is equal to 1. The metrics are defined in Eq. 25 to 27.\nMAE(imputation, target, mask) = \u03a3d \u03a3t |(imputation \u2013 target) \u2299 mask|d / \u03a3d \u03a3t maskd\nRMSE(imputation, target, mask) = sqrt(\u03a3d \u03a3t (((imputation \u2013 target) \u2299 mask)2) / \u03a3\u03a3 mask)\nMRE(imputation, target, mask) = \u03a3\u03a3 |(imputation \u2013 target) \u2299 mask| / \u03a3\u03a3 |target mask|\nTo ensure a fair comparison, all models are trained using the Adam optimizer with \u03b2\u2081 = 0.9, B2 = 0.98, and e = 10-9, following the configuration proposed by Vaswani et al. [VSP+17]. The learning rate is scheduled using Eq. 28, with warmup_steps set to 4000. This scheduler eliminates the need to search for an additional learning rate hyperparameter, as it adjusts the learning rate based on the model dimensionality. Training is stopped early if the imputation MAE does not decrease after 30 epochs, and the best validation weights are restored before evaluating the test dataset.\nlr = dmodel-0.5 * min(step_num-0.5, step_num * warmup_steps-1.5)\nMoreover, a hyperparameter search is performed for each model and dataset using Microsoft's Neural Network Intelligence system [Mic21], which implements the Tree-structured Parzen Estimator (TPE), a Sequential Model-Based Optimization (SMBO) algorithm. TPE models the conditional probability P(x|y) and the marginal probability P(y), where x represents the hyperparameters and y the evaluation result. It substitutes the configuration priors with non-parametric densities for efficient optimization [BBBK11].\nThe details of the hyperparameter search are defined as follows:\n\u2022 RNN-based models (MRNN, BRITS): the RNN hidden size is sampled from (32, 64, 128, 256, 512, 1024).\n\u2022 Self-Attention models (Transformer, SAITS): the number of encoder blocks, N is sampled from (1, 2, 4, 5, 6), the dimensionality of the model dmodel is sampled from (64, 128, 256, 512), the dimensionality of the feed-forward network, dffin is sampled from (128, 256, 512, 1024), the dimensionality of the key and value vectors dk and du for the attention layers are sampled from (32, 64, 128, 256), the number of heads is sampled from (2,4,8). The dropout is sampled from (0, 0.1, 0.2, 0.3, 0.4, 0.5). The dropout is sampled from (0, 0.1, 0.2, 0.3, 0.4, 0.5)."}, {"title": "4.5 Experimental results", "content": "The main limitation of RNNs lies in their tendency to treat all input elements equally, which can lead to suboptimal performance when sequences are very long or when certain time-steps carry more informative content. Attention layers address this limitation by dynamically reweighting the importance of each input element before processing the sequence. By combining RNNs with Attention layers, models can better capture context and dependencies, improving their ability to impute missing values effectively.\nWe use the 3 datasets described in Section 4.2 to evaluate and compare the imputation performance of BRATI. The results are summarized in Tables 2 and 3 for the PhysioNet dataset, Tables 4 and 5 for the Water Quality dataset, and Tables 6 and 7 for the Space Weather dataset. Below, we provide a detailed analysis of the results for each dataset.\nPhysioNet dataset Tables 2 and 3 report the metrics for the PhysioNet dataset, the most challenging due to its high sparsity. Imputation becomes significantly harder across all models when missing values appear in sequences, as these configurations disrupt temporal dependencies. MRNN outperforms Median imputation but is surpassed by propagating the last observed value. BRITS, leveraging its recurrent structure, captures temporal dependencies more effectively, particularly in scenarios with sequences of fixed length, and outperforms both Naive imputation methods.\nSelf-Attention models (Transformer and SAITS) demonstrate superior performance compared to RNN-based methods. While SAITS significantly outperforms the Transformer model in the MCAR scenario, their performance is comparable in other configurations. BRATI consistently achieves the best results across all configurations, with the largest improvements observed in scenarios with missing values in sequences of random length, highlighting its ability to handle complex missing patterns effectively.\nWater Quality dataset Tables 4 and 5 show the results for the Water Quality dataset. For the RNN-based models, the results follow a similar trend to the PhysioNet dataset: MRNN outperforms Median imputation, and BRITS achieves better performance than both Naive methods in most configurations. However, propagating the last observed value yields better results for the most challenging scenario, missing values in sequences of random length.\nThe Self-Attention models exhibit peculiar behavior, struggling with convergence in the MCAR scenario, where BRITS outperforms them. This suggests that RNN-based models, including BRATI, handle this configuration more effectively. However, in scenarios with missing values in sequences of fixed and random lengths, the Self-Attention models significantly outperform RNN-based methods, delivering performance very close to BRATI. Even so, BRATI achieves the best results across all six configurations, showing notable improvements in MAE and MRE metrics for the MCAR and fixed sequence length cases. For sequences of random length, while the differences in MAE and MRE metrics between BRATI and the Self-Attention models are negligible,"}, {"title": "4.6 Ablation experiment", "content": "In this section, we conduct two experiments to analyze the impact of key components in the BRATI architecture. The first experiment evaluates the performance of the model when only one"}, {"title": "5 Analysis and Discussion of the Results", "content": "The experimental results demonstrate the strengths of the proposed model, particularly when compared to other baseline methods and ablation variants. Below, we provide a critical analysis of why BRATI achieves superior performance in the evaluated scenarios and the factors influencing its behavior."}, {"title": "5.1 Role of Attention Mechanisms and Recurrent Layers", "content": "BRATI combines attention mechanisms with RNN-based layers, offering a unique advantage over both purely recurrent and self-attention-based architectures. The attention mechanisms dynamically prioritize relevant time steps, which is crucial for imputing long sequences or dealing with sparse data. Meanwhile, the GRU layers ensure the capture of local temporal dependencies."}, {"title": "5.2 Performance in Missing Values in Sequences of Random-Length", "content": "The random-length missing sequence configuration represents the most challenging scenario, as highlighted in Tables 8 to 13. The superior performance of BRATI in these cases can be attributed to:\n\u2022 Attention mechanisms: these mechanisms allow the model to focus on distant, non-missing time steps, which are critical for imputing long gaps.\n\u2022 Bidirectional imputation: by processing sequences in both forward and backward directions, BRATI integrates information from both past and future time steps, providing a complete context.\nAdditionally, compared to models such as BRITS or SAITS, BRATI exhibits more consistent performance in handling random-length sequences due to its ability to dynamically adapt to varying gaps in the data."}, {"title": "5.3 Insights from Ablation Experiments", "content": "The ablation experiments (Tables 8 to 13) provide valuable information on the contributions of each architectural component:\n\u2022 Single Imputation Block: removing one of the imputation blocks reduces the model's capacity to process bidirectional dependencies. As a result, the model performs worse in scenarios requiring complex imputation, such as random-length sequences, as observed in Tables 9 and 13.\n\u2022 Weighted Combination: replacing the learned combination weights with a simple average diminishes the model's ability to dynamically prioritize forward or backward imputation. This limitation is evident in the Space Weather dataset (Tables 12 and 13), where the complete model consistently outperforms this ablation variant in most configurations.\nThese experiments highlight that both components -the bidirectional structure and the learned combining weights are critical for achieving SoA performance, particularly in complex scenarios."}, {"title": "5.4 Comparison with Baseline Models", "content": "The results from Tables 2 to 13 show that BRATI outperforms all baseline methods in almost every metric in all datasets. However, the advantages are more pronounced in the following cases:\n\u2022 PhysioNet dataset: due to its high sparsity, BRATI ability to capture long- and short-term dependencies gives it a clear edge over models such as MRNN and BRITS, particularly in configurations with sequential missing values.\n\u2022 Water Quality dataset: in scenarios with high missing rates, BRATI's ability to dynamically adapt to irregular patterns results in consistently lower RMSE and MAE compared to"}, {"title": "5.5 Relevance of the Space Weather Dataset", "content": "The Space Weather dataset, although less commonly used compared to PhysioNet or Water Quality datasets, represents a critical domain for analyzing complex time series. Missing data in this context often arises in satellite observations, where gaps can severely impact predictions of Space Weather phenomena, such as solar storms or geomagnetic disturbances. Accurate imputation in this domain is essential for improving models used to forecast disruptions to satellite navigation, telecommunications, or power grid infrastructure. The results show that BRATI performs well in handling irregular and challenging missing patterns, making it a suitable candidate for such high-stakes applications."}, {"title": "6 Conclusions", "content": "Time series are widely used and hold significant value in multiple data analysis applications. However, in most real-world scenarios, data is often imperfect, and the presence of missing values is a common occurrence that adversely impacts downstream applications. The causes of missing data are diverse and can follow various patterns. For instance, missing values may occur at random, with no discernible relationship, or they may be linked to malfunctions or interruptions in the communication systems of measurement devices. In such cases, missing values are likely to appear in sequences of arbitrary length.\nBuilding on previous RNN and Self-Attention models, we proposed BRATI, a novel approach for time series imputation that combined the strengths of both techniques. BRATI was evaluated on three datasets with three distinct missing data patterns: missing values completely at random (MCAR), missing values in sequences of fixed length, and missing values in sequences of variable length. Across all datasets and configurations, the proposed model consistently outperformed the baseline models, demonstrating its robustness and adaptability.\nAnother contribution of this work is the introduction of the Space Weather dataset, a realistic benchmark derived from space probe observations. By capturing complex missing data patterns, including Missing Not At Random (MNAR) scenarios, this dataset provides a valuable resource for the Deep Learning community. We hope its availability fosters contributions from the community, advancing research and innovation in the domain."}]}