{"title": "Medical Video Generation for Disease Progression Simulation", "authors": ["Xu Cao", "Kaizhao Liang", "Kuei-Da Liao", "Tianren Gao", "Wenqian Ye", "Jintai Chen", "Zhiguang Ding", "Jianguo Cao", "James M. Rehg", "Jimeng Sun"], "abstract": "Modeling disease progression is crucial for improving the quality and efficacy of clinical diagnosis and prognosis, but it is often hindered by a lack of longitudinal medical image monitoring for individual patients. To address this challenge, we propose the first Medical Video Generation (MVG) framework that enables controlled manipulation of disease-related image and video features, allowing precise, realistic, and personalized simulations of disease progression. Our approach begins by leveraging large language models (LLMs) to recaption prompt for disease trajectory. Next, a controllable multi-round diffusion model simulates the disease progression state for each patient, creating realistic intermediate disease state sequence. Finally, a diffusion-based video transition generation model interpolates disease progression between these states. We validate our framework across three medical imaging domains: chest X-ray, fundus photography, and skin image. Our results demonstrate that MVG significantly outperforms baseline models in generating coherent and clinically plausible disease trajectories. Two user studies by veteran physicians, provide further validation and insights into the clinical utility of the generated sequences. MVG has the potential to assist healthcare providers in modeling disease trajectories, interpolating missing medical image data, and enhancing medical education through realistic, dynamic visualizations of disease progression.", "sections": [{"title": "1. Introduction", "content": "Disease progression refers to the way an illness evolves in an individual over time. Understanding this progression enables healthcare professionals to develop effective treatment strategies, anticipate complications, and adjust care plans accordingly. Disease progression modeling can also be seen as a form of human digital twin, laying the foundation for future precision medicine [37, 68, 70]. However, modeling disease progression on medical images presents significant challenges. These challenges arise primarily from the lack of continuous monitoring of individual patients over time, as well as the high cost and risks associated with collecting longitudinal imaging data [13, 38, 61]. The intricate and multifaceted dynamics of disease progression, combined with the lack of comprehensive and continuous image or video data of individual patients, result in the absence of established methodologies for medical imaging trajectories simulation [34].\nRecent advancements in image and video generation models present promising opportunities for simulating realistic medical videos, potentially enriching existing databases and addressing data limitations. To incorporate generative models into disease progression simulations, we establish three key criteria that medical video generation models must meet: (i) The model should generate videos presenting long disease progression under zero-shot setting, as there are no existing"}, {"title": "2. Related Works", "content": "Disease Progression Simulation. Longitudinal disease progression data derived from individual electronic health records offer an exciting avenue to investigate the nuanced differences in the progression of diseases over time [46, 60, 65]. However, most of the previous works are based on HMM [38, 71] or deep probabilistic models [3] without using data from imaging space. Some recent works have started to resolve image disease progression simulation by using deep-generation models. [25, 56] utilized the Generative Adversarial Networks (GANs) based model and linear regressor with individual sequential monitoring data for Alzheimer's disease progression simulation in MRI imaging space. All these methods have to use full sequential images as training sets and are hard to adapt to the general medical imaging domain.\nGenerative Models. Recently, Denoising Diffusion Models [21, 26, 58, 64] have become increasingly popular due to their ability to create high-resolution realistic images from textual descriptions. One major advantage of these models is they can use CLIP [54] embedding to guide image editing based on contextual prompts. Among the various text-to-image models, latent diffusion model (LDM) [16, 58] and its follow-up image-to-image editing works [9, 49, 51] has received considerable attention because of its impressive performance in generating high-quality images and its ability to edit scenarios across multiple modalities.\nWhile image generation has seen substantial progress in general domains, its application in the medical field remains less explored [76]. Earlier work using Variational Autoencoders (VAEs) [29] and GANs [18] focused on generating medical images like X-rays and MRIs to address the issue of limited training data [14, 43, 48, 78]. The introduction of LDMs significantly improved the quality of these images [27, 47, 50], even extending to 3D synthesis [15, 28]. Recently, efforts have been made to unify medical report generation with image synthesis [7, 35], and design image editing pipeline for counterfactual medical image generation [19, 33].\nText-to-Video Generation. Text-to-image models have attracted significant attention from both academia and industry, as evidenced by advancements like DALL\u00b7E [4], Midjourney [44], and Stable Diffusion 3 [16]. These innovations have significantly impacted the text-to-video domain [66], leading to the development of models such as Sora [45], Pika [52], and Stable Diffusion Video [5]. The core of these text-to-video models often involves fine-tuning or integrating additional modules or priors into pre-trained text-to-image diffusion models using video data, as seen in Make-A-Video [63], PYoCo [17], and LaVie [72], SEINE [11], AnyV2V [31]. However, applying video generation models in the healthcare domain presents challenges, particularly because time-series medical imaging data for disease progression is difficult to collect. While some studies have explored video generation in medical imaging [36, 67], they have not focused on simulating disease progression."}, {"title": "3. Problem Statement", "content": "Traditional image to video generation models need to train with a large amount of text-to-video or image-to-video data. However, it is almost impossible to obtain large-scale longitude medical imaging data (can be also considered as a type of medical video data) as most patients may not go to the same hospital for follow-up treatment and the hospitals also lack medical imaging and clinical reports in the early stages of diseases.\nIn our paper, we reconsider this problem in another way. Given an input medical image xo, and clinical report and medical history label yo. Experienced medical doctors can predict the disease progression of the patient based on their clinical prior knowledge, denoted as yn, where N + 1 is the total number of states of the predicted disease. The predicted disease progression is a video sequence X, which can be separated by a set of short video clips {X0, X1, X2, ..., XN\u22121}, where xi \u2208 RK\u00d7H\u00d7W\u00d7C is a video clip between disease image state xi and xi+1. K, H, W, C denote the number of frames, height, width, and channels of the video clip. K is a very small number to control the disease progression change in a limited medical imaging space. In fi, the starting frame xi \u2208 RH\u00d7W\u00d7C is the initial disease state and end frame Xi+1 \u2208 RH\u00d7W\u00d7C is the end disease state.\nWe separate the disease progression video generation into a two stage strategy. In the first stage, the key idea is to generate discrete disease progressive states {X0, X1,X2, ..., XN }:\nX1:T = fo(x0, \u0443\u0442)\nIn the training phase of the first stage, fe is a denoising diffusion model learned from independent identically distributed (x, y) from different patients.\nIn the second stage, we adopt video latent diffusion models finetuned with video data in the general domain. In doing so, we convert the disease progression video generation task into a frame-level transition generation problem:\nXi = 9$(Xi, Xi+1)\nThe output videos {x0, X1, X2, ..., XN} finally concatenate into the disease progression video X\n\u2208 RKN\u00d7H\u00d7W\u00d7C"}, {"title": "4. Medical Video Generation (MVG)", "content": "As shown in Figure 3, MVG contains two main components: (i) Progressive disease image editing (PIE) with medical domain-specific diffusion model and (ii) Transition Generation Process between generated disease states with video latent diffusion model.\nThe first component PIE is a long-sequence medical image editing framework proposed to refine and enhance images iteratively and discretely, allowing clinical report-based prompts for precise adjustments to simulate disease development while keeping realism. Unlike traditional image editing techniques, PIE involves a multi-stage process where each step builds upon the previous one, intending to achieve a final result that is more refined than if all changes were made at once. Transition generation is used in the long video generation model to connects different narrative moments. Once the frame-level sequence is generated by PIE, we will provide each pair of adjacent frames and use transition prompts and disease region mask to control the style and content, creating intermediate frames that further illustrate the transition or progression within the medical video sequence.\n4.1. Progressive Image Editing (PIE)\nProcedure. The inputs to PIE are a discrete medical image xo depicting any start or middle stage of a disease and a corresponding terminal stage clinical report YN inferred by medical doctor and then re-captioned by GPT-4 [2], providing the potential hint of the patient's disease progression. The Latent y will be the text conditioning of the diffusion model [58]. y is generated from a pretrained text encoder from CLIP [54] (clip-vit-large-patch14), where the text input is yn. The output generated by PIE is a sequence of images presenting the disease progression, {x8, x\u2081,\u00b7\u00b7\u00b7, x}. The iterative PIE procedure is defined as follows:\nProposition 1. Let x ~ x, where x is distribution of photo-realistic medical images, y be the text conditioning, running PIEn(\u00b7, \u00b7) recursively is denoted as following, where n = {N, N \u2013 1, \u2026\u2026\u20261},\nxn = PIEn (xn-1, Y)\nXN X = PIENO PIEN-10\uff65\uff65\uff65 \u25cb PIE1 (x, y)\nN times"}, {"title": "4.2. Transition Generation Process", "content": "The concept of scene transition generation is first proposed by SEINE [11], which is a short-to-long video diffusion model. In MVG, we use MROI to control SEINE to connect the disease progression between each step generated by PIE,\nXn = Concat(x-1, \u03b5,..., \u03b5,xn)\nxn' = (xn-1 + xn)/2 (1 \u2013 MROI) + g(xn')\u00b7MROI\n,where In is a video clip with the first and last frames are the input xm-1 and output xm from progression stage n in PIE. Between 2-1 and 2, all frames are masks with random noise. By predicting and modeling the noise, the transition generation process g(\u00b7) aims to extend realistic, visually coherent transition frames that seamlessly integrate the visible frames with the unmasked ones."}, {"title": "5. Experiments and Results", "content": "In this section, we present experiments on various disease progression tasks. Experiments results demonstrate that MVG can simulate the disease-changing trajectory that is influenced by different medical conditions. Notably, MVG also preserves unrelated visual features from the original medical imaging report, even as it progressively edits the disease representation. Figure 5 showcases a set of disease progression simulation examples across three distinct types of medical imaging. Details for Stable Diffusion fine-tuning, pretraining model for confidence metrics settings are available in the Supplementary."}, {"title": "5.1. Experimental Setups", "content": "Implementation Details. For experiments in Table 2, PIE and the baselines are using publicly available Stable Diffusion checkpoints (CompVis/stable-diffusion-v1-4) and then we further finetune on the training sets of each of the target datasets. This is because the pipeline of the other two baselines only support the model weight from original Stable Diffusion 1.4 version. For user study in Table 3, we adopt Stable Diffusion 3 medium as the model weight and finetune it with three medical domain. The weight for transition generation model is from SEINE [11]. Our code and checkpoints will be publicly available upon publication. All experiments are conducted on 4 NVIDIA H100 GPUs.\nDatasets for Disease Progression. We evaluate the pretrained domain-specific stable diffusion model on three different types of disease datasets from different tasks: CheXpert Plus [10] and MIMIC-CXR [24] for chest X-ray classification and report generation [10, 23, 24], ISIC 2024 and ISIC 2018 [12, 32, 69] for skin cancer prediction, and Kaggle Diabetic Retinopathy Detection Challenge [1]. Each of these datasets presents unique challenges and all of them having large-scale of data, making them suitable for testing the robustness and versatility of MVG. We also collected over 50 data among the test set from these datasets as initial input data for disease progression video generation. These data were used for disease progression simulation. Three groups of progression visualization results can be found in Figure 5.\nEvaluation Metrics. The evaluation of generated disease progression images focuses on two key aspects: alignment with the intended disease features and preservation of patient identity. To assess these aspects, we employ two primary metrics: the CLIP-I score and classification confidence score, allowing us to compare the baselines and PIE (stage 1 of MVG) under consistent conditions.\nThe CLIP-I score (theoretically ranging from [0, 1]) represents the average pairwise cosine similarity between the CLIP embeddings of the generated medical image sequence and the initial real medical images [54, 59]. A high CLIP-I score indicates strong patient identity consistency but also means minimal changes between the edited sequence and the original input. Therefore, an ideal disease progression"}, {"title": "5.2. Disease State Simulation", "content": "In order to demonstrate the superior performance of MVG in disease progression simulation over other single-step editing"}, {"title": "Then, the resulting final output x maximizes the posterior probability p(x|x0, y).", "content": "To run the inference pipeline of PIE to generate a discrete disease progression image sequence, we use the original input image x as the start point. The hyperparameters are the number of progression stage N, the number of diffusion steps T, text conditional vector y, noise strength y, diffusion parameterized denoiser eg, and a region of interest (ROI) mask MROI, where each pixel in MROI \u2208 [0, 1].\nSince PIE is a recursive process, at progression stage n, the input image is x-1. From diffusion step k to 1,\nx'\u2190 \u221aat-1(\n        x' - \u221a1 - ate) (x', y)\n      -)+\n    Vat\n    \u221a1 1- -1 et) \u0395\u03c1 (x, y)\nwhere x' in step k is xn\u22121, k is y \u03a4, \u03b5\u03c1 (t) (x', y) is the noise prediction by UNet or Transformer, where \u03b8 is the parameter in the denoiser. After the last step, we use the MROI initially generated by pretrained Med-SAM [42] and then slightly edit by human to control and refine the final output:\nx' \u2190 (\u03b2\u2081 \u00b7 (x' \u2013 x) +x0) \u00b7 (1 \u2013 MROI)+\n(\u03b22 (x' - x) +x0) MROI\nwhere B1, B2 are hyperparameter to control the interpolation between generated result and the input image. The last output image x' is x-1, which is also the input xn of the next step (n + 1 step) disease state generation. Equation 6 guarantees the editing is regional based and avoids the image distortion caused by multiple times image editing. It is worth noting that Equation 6 can generalize to arbitrary diffusion backbones including Stable Diffusion-1.4 [58], Stable Diffusion 3 [16].\nWith each round of editing as shown in the middle part of Figure 3, the image gets closer to the objective by moving in the direction of - log p(xy). The step size would gradually decrease with a constant factor. The iterative convergence analysis is as follows:"}, {"title": "Proposition 2. Assuming ||x0|| \u2264 C\u2081 and ||\u20ac\u0473(x, y)|| \u2264 C2, (x, y) \u2208 (\u03c7, \u0393), for any d > 0, if", "content": "Assuming ||x0|| \u2264 C\u2081 and ||\u20ac\u0473(x, y)|| \u2264 C2,\nProposition 2. (x, y) \u2208 (\u03c7, \u0393), for any d > 0, if\n2\nn>\nlog(ao)\n(log(d) \u2013 C)\nthen,\n||xn+1 - xn|| < \u03b4\nwhere, A = \u221a00-0001-01-0001, X is the image distribution, \u0393 is the text condition distribution, and C = log((\u22121). C\u2081 + A. C2)\nProposition 2 shows as n grows bigger, the changes between steps would grow smaller. Eventually, the difference between steps will get arbitrarily small. The convergence of PIE is guaranteed, and modifications to any medical imaging inputs are bounded by a constant. The proof of Proposition 2 is shown in the supplementary material."}, {"title": "5.3. Disease Progression Video Simulation", "content": "Table 3 shows the comparison results between MVG and three image-to-video generation baselines. We did not compare our method with text-to-video generation models like Stable Diffusion Video [5], as these models do not support video generation from an initial medical image. Compared to PixVerse [53], CogVideoX [75], and Pika [52], our method demonstrates significantly higher clinician preference, achieving average win rates of 79%, 70%, and 66% for Cardiomegaly in chest X-ray, diabetic retinopathy, and benign skin lesion disease progression simulations, respectively. In contrast, for the A/B tests comparing the other video generation methods, clinicians were generally unable to differentiate between them, with win rates averaging around 50% for both A method and B method, indicating no clear preference. The results of the clinician preference study indicate that MVG is capable of generating high-fidelity disease progression sequences that align well with clinical context. The disease progression video data generated by MVG is available in the Supplementary material."}, {"title": "5.4. Ablation Study", "content": "During the MVG simulation, the region guide masks play a big role as prior information. Unlike other randomly inpainting tasks [40], ROI mask for medical imaging can be extracted from clinical reports [8, 39] using domain-specific Segment Anything models [30, 41]. It helps keep unrelated regions consistent through the progressive changes using MVG or baseline models. In order to generate sequential disease imaging data, MVG uses noise strength y to control the influence from the patient's clinically reported and expected treatment regimen at time n. N is used to control the duration of the disease occurrence or treatment regimen. MVG allows the user to make such controls over the iterative process, and running PIEn multiple times can improve the accuracy of disease imaging tracking and reduce the likelihood of missed or misinterpreted changes. We showed ablation study for MROI in Table 4, y in Table 5, N in Table 6, B1 and B2 in Table 7. The experimental results demonstrate that MROI is a good controller to balance the alignment with the intended disease features and preservation of patient identity. From these experiments, we also finalize the best hyperparameter (N = 10, \u03b3 = 0.6, \u03b2\u2081 = 0.01, \u03b22 = 0.75) for the main experiment."}, {"title": "5.5. Compare with Real Longitude Medical Imaging Sequence.", "content": "Due to the spread of COVID, part of the latest released dataset contains limited longitudinal data. In order to validate the disease sequence modeling that MVG can match real disease trajectories, we conduct an ablation study on generating Edema disease progression from 10 patients in BrixIA COVID-19 Dataset [62] who's radiology report showed Edema. The input image is the day 1 image, and we use MVG to generate future disease progression based on real clinical reports for edema. Experimental results show that after the disease state seqeuence simulation of MVG, the mean absolute error between MVG's simulated image and real disease progression image from the same patient is approximately 0.0658. Figure 7 shows an example of the comparison."}, {"title": "5.6. User Study", "content": "To further assess the quality of our generated disease state sequences, we conducted another comprehensive user study from 35 physicians and radiologists with 14.4 years of experience on average to answer a questionnaire on chest X-rays. The questionnaire includes disease classifications on the generated and real X-ray images and evaluations of the realism of generated disease progression video of Cardiomegaly, Edema, and Pleural Effusion. More details of the questionnaire and the calculation of the statistics are presented in Supplementary. The participating physicians have agreed with a confidence of 76.2% that MVG simulated disease state progressions on the targeted diseases fit their expectations. One plausible explanation is due to the nature of MVG, the result of running progressive image editing makes pathological features more evident. The aggregated results from the user study demonstrate our framework's ability to simulate disease progression to meet real-world standards."}, {"title": "6. Conclusion and Outlook", "content": "In conclusion, our proposed framework, Medical Video Generation (MVG) for disease progression simulation, holds great potential as a tool for medical research and clinical practice in simulating disease progression to augment lacking longitude data. Theoretical analysis also shows that the iterative refining process in the stage 1 of MVG is equivalent to gradient descent with an exponentially decayed learning rate, and practical experiments on three medical imaging datasets demonstrate that MVG surpasses baseline methods. The clinician human preference study from 30 medical doctors also shows that the disease progression video sequences generated by MVG are both real and consistent with the corresponding clinical text descriptions. Despite current limitations due to the lack of large amounts of longitude medical imaging data, our framework has vast potential in restoring missing data from previous electronic health records (EHRs), improving clinical education. Moving forward, we aim to incorporate more types of medical imaging data with richer clinical descriptions into medical video generation, enabling our framework to more precise control over disease simulation through text conditioning."}]}