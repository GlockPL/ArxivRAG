{"title": "See Where You Read with Eye Gaze Tracking and Large Language Model", "authors": ["Sikai Yang", "Gang Yan"], "abstract": "Losing track of reading progress during line switching can be frustrating. Eye gaze tracking technology offers a potential solution by highlighting read paragraphs, aiding users in avoiding wrong line switches. However, the gap between gaze tracking accuracy (2-3 cm) and text line spacing (3-5 mm) makes direct application impractical. Existing methods leverage the linear reading pattern but fail during jump reading. This paper presents a reading tracking and highlighting system that supports both linear and jump reading. Based on exper-imental insights from the gaze nature study of 16 users, two gaze error models are designed to enable both jump reading detection and relocation. The system further leverages the large language model's contextual perception capability in aiding reading tracking. A reading tracking domain-specific line-gaze alignment opportunity is also exploited to enable dynamic and frequent calibration of the gaze results. Controlled experiments demonstrate reliable linear reading tracking, as well as 84% accuracy in tracking jump reading. Furthermore, real field tests with 18 volunteers demonstrated the system's effectiveness in tracking and highlighting read paragraphs, improving reading efficiency, and enhancing user experience.", "sections": [{"title": "I. INTRODUCTION", "content": "Losing track of reading progress can be annoying. This often happens when a reader tries to move to a new line after finishing the previous one but accidentally switches to the wrong line, requiring them to double-check the sentences and find the correct spot.\nAdvancements in eye gaze tracking technology [1], [2], [3], [4] have provided new opportunities to assist with reading and address this issue. By tracking reading progress through gaze and highlighting read paragraphs, users receive visual assistance that prevents them from switching to the wrong line. This technology also facilitates other applications [5], [6], such as person tracking [7]. In this paper, we discuss the prospect of enabling practical and robust reading tracking application using existing gaze tracking technology (e.g., Tobii Eye Tracking [8]).\nHowever, there is a significant gap between gaze tracking accuracy and the level required for reliable reading tracking. Typical gaze tracking accuracy ranges from 2 to 3 centimeters [1], [9], [10], while the spacing between lines in reading materials is typically 3 to 5 millimeters. This discrepancy"}, {"title": "Jump Reading Relocation", "content": "Jump Reading Relocation: Relocating jump reading can be difficult, as gaze tracking error largely exceeds the line spacing, making it difficult to pinpoint the exact new line the user is reading. To provide guidance for the relocation, we observe an opportunity of using punctuation marks as relocation anchors, since each sentence divided by punctuation marks represents a complete linguistic unit. Users are more likely to follow punctuation marks for efficient review or preview. Therefore, when jump reading is detected, we track the user's gaze trajectory and search for punctuation marks within the error range of this trajectory to identify potential relocation candidates. If there is only one candidate found within the error range of the trajectory, we use it as the new starting point. When multiple candidates are identified, the situation becomes more complex, requiring a suitable mechanism for candidate selection."}, {"title": "LLM Assisted Candidate Election", "content": "LLM Assisted Candidate Election: When multiple candi-dates are identified, it is essential to determine the best option using both gaze tracking results and contextual information. Each punctuation mark candidate is followed by a sentence. We first calculate the match ratio between the candidate sentences' locations and the gaze trajectory. Specifically, we propose a finer probabilistic gaze error distribution model for the calculation, which is also developed from the gaze data collected from the 16 users. This match ratio will serve as a major metric for evaluating the candidates.\nSecondly, since we are dealing with reading materials, contextual information can also provide guidance. Recent advancements in large language models (LLMs) offer new possibilities for enhancing reading tracking with language perception capabilities. However, collecting a large ground truth dataset without disrupting users' natural reading patterns is challenging, making it impractical to fine-tune the LLM with domain-specific data [19]. On the other hand, the original language perception capability of the LLM is adequate for understanding the reading material and assisting in the candi-date election. As a result, we employ e prompt engineering techniques to effectively utilize the LLM [20], [21], [22], [23]. Based on the most recent linear reading history and the sentences following the punctuation mark candidates, we query the LLM to identify the most probable next reading can-didate based on contextual relevance and logic. The candidate"}, {"title": "Gaze Drifting and Dynamic Calibration", "content": "Gaze Drifting and Dynamic Calibration: Reading can be a time-consuming task, during which gaze tracking calibration may gradually degrade due to factors such as user reposi-tioning. This causes the gaze tracking result to drift away from its original accuracy, necessitating periodic re-calibration to correct gaze tracking results. Classical gaze tracking cal-ibration methods require active user attention, which will interrupt the current reading activity and demand the user's participation. Fortunately, we identified a valuable opportunity for calibration uniquely suited to the reading tracking domain\nSimilar to the idea of saliency-based calibration [24], [25], we leverage the assumption that during linear reading, the user is actively focusing on the current line. The alignment between the gaze and the line location provides valuable dynamic calibration resources during reading without specific user participation. Specifically, we focus on calibrating the Y-axis (vertical) of gaze estimations, as it is the bottleneck of reading tracking. For each completed line, we calculate the average Y-axis gaze locations and compare them with the vertical location of the finished line. With multiple gaze-line pairs, we perform linear regression to fit the gaze estimations to the lines' vertical location. This approach enables continuous calibration during linear reading and also after jump reading re-locations, ensuring consistent tracking accuracy."}, {"title": "II. DESIGN", "content": "In this section we introduce the design details of our reading tracking system $RT^2H$. Section II-A introduces the basic methods used for linear tracking. Section II-B and II-C introduce the techniques designed to handle the complex situations of jump reading. Lastly, Section II-D demonstrates the possibility of exploiting a reading tracking domain-specific opportunity, i.e., the gaze-line alignment during linear reading, to enhance gaze tracking accuracy and stability."}, {"title": "A. Linear Reading Tracking", "content": "In this work we mainly focus on handling jump reading, and design simple but effective linear reading tracking methods. Text lines are extremely slender objects placed horizontally. The millimeter-grade line height poses a great challenge for centimeter-grade gaze tracking. Fortunately, the linear reading assumption greatly mitigates this challenge on the vertical aspect. It represents the regular reading pattern, where the user reads from left to right within each line, and line after line.\n1) Horizontal Tracking: For linear reading within each line, we directly use the horizontal gaze location as the reading progress of the line, since the horizontal scale of the text line largely exceeds the gaze tracking error.\n2) Vertical Tracking and Z-cut: Humans read from left to right. If the gaze moves from right to left for a long distance, it highly likely indicates the user has finished reading a line and is moving to read the next line. Therefore, for the line switching, we rely on the detection of 'Z-cut', namely, the user reaches the right border and then returns to the left border to start reading a new line. Due to the fact that humans read in saccades and fixations, the user's gaze may not exactly pass the border. Therefore, we set a closer threshold for both left and right border checkpoints (e.g., 20% of line width)."}, {"title": "B. Jump Reading Detection", "content": "Jump reading is much more challenging than linear reading as it breaks the line-by-line reading assumption. It includes review, where the user returns to text that has been read, and preview, where the user skips ahead to read other information, as illustrated in Figure 3. In handling jump reading, as the first step, we detect jump reading via monitoring whether the gaze escapes the current line for too long. From the temporal aspect, we set a time threshold of 2.5 seconds of accumulative active gazing to determine if jump reading is initiated. Specifically, if the gaze is not detected, it will not be accumulated into the threshold timer, as it probably means the user is looking away from the screen.\nFrom the spatial aspect, we monitor if the gaze escapes a certain range from the line that is currently being read. However, setting this range is not straightforward, as it is expected to suit different users' gaze patterns. To that end, we rely on a real user study to generalize a gaze error range model."}, {"title": "Gaze Error Range Model", "content": "Gaze Error Range Model: We collected gaze data from 16 users to determine the gaze error range, as well as its screen-based distribution. We use the same gaze tracking device, i.e., Tobii Pro Spark. Each user collected 4,000 aligned gaze data samples via staring at visual anchors. Figure 4 depicts the average gaze error of each user. The overall average error among all users is 1.9455 cm. With the gaze point estimated by Tobii Pro Spark and the screen-based coordinate of the visual anchors as ground truth, we analyse the gaze error range distribution. We aggregate all users' gaze error and calculate its distribution on the 1920\u00d71080 screen. Figure 5 demonstrates the screen-based error distribution. We can see the gaze error tends to be higher near the borders, especially the left and right sides. Additionally, higher error concentrates in the bottom area, which is close to the Tobii Pro Spark hardware. We save this distribution map as the gaze error range model, and use it to dynamically adjust the estimated gaze error range based on the gaze location. When the gaze vertically escapes the current line for over 2.5 seconds, we consider it an event of jump reading."}, {"title": "C. Jump Reading Relocation", "content": "Now that the jump reading has been initiated, the next step is to find the destination of the jump reading, i.e., to anticipate where the user is reading next. Relocating the reading using only gaze information is almost impossible, since gaze error range largely exceeds line spacing. Fortunately, we observe an opportunity hidden within the language context that can potentially aid jump reading relocation: Punctuation marks serve as logical anchors in writing [26]. They help organize and clarify the meaning of sentences by indicating pauses, separating ideas, and showing relationships between different parts of the text. This makes users more likely to follow punctuation marks for efficient review or preview. Even if a user does not specifically follow any punctuation mark as the starting point for review or preview, it is very likely that the user will soon encounter a punctuation mark on the way of resumed reading, which brings the situation back to the assumption that punctuation marks can be used as anchors."}, {"title": "Relocation Candidate Identification", "content": "1) Relocation Candidate Identification: As a result, we use punctuation marks as guidance for jump reading relocation. After jump reading is detected, as depicted in Figure 3, we record the trajectory of the user's gaze, and search for punctuation marks that fall within the error range of the trajectory as candidates for potential relocation destinations. As mentioned in Section II-A2, humans read from left to right, and looking from right to left for a long distance would highly possibly indicate line switching. In other words, Z-cuts serve as effective indicators showing that the user has finished relocating and has resumed linear reading at a new starting point. Therefore when a Z-cut is detected, we review the punctuation marks captured along the trajectory and begin determining the new starting point of linear reading."}, {"title": "Candidate Election Algorithm", "content": "2) Candidate Election Algorithm: If there is only one candidate captured with the error range of the trajectory, we use that punctuation mark as the new starting point to resume linear reading tracking. However, if there are two or more candidates captured, it would be uncertain which one of them is the correct starting point. To that end, we design a candidate election algorithm to find the best among them, using both gaze tracking results and contextual information as reference.\nAs punctuation marks serve as language anchors, each candidate has a sentence following it. We first use the location and length of the candidate sentences to calculate their match ratio with the trajectory. An intuitive solution to evaluate the match ratio would be calculating the area of overlapping between the sentence and the error range of the trajectory. However, the previously proposed gaze error range model is merely a shape that lacks detailed information, such as how close the sentence is to the trajectory. To fill this void with useful information, we further propose a finer gaze error vector distribution model, which also originates from the gaze data collected from the 16 users."}, {"title": "Gaze Error Vector Distribution Model", "content": "Gaze Error Vector Distribution Model: To better model the probability of a candidate sentence being the actual sen-tence that the user is reading, we conduct real user study to analyse the distribution of the gaze error vectors, i.e., vectors from the gaze ground truth location to the estimated gaze location. We aggregate the gaze error vector from all users' data, as depicted in Figure 6. We can see the error vector distribution forms an oval cloud, where samples concentrate around the center. Specifically, its horizontal and vertical standard deviations are 1.8471 cm and 1.2289 cm, respectively. This distribution aggregates the gaze error vector samples regardless of the screen-based gaze location, as it would be too sparse if we also consider the screen-based location. To ensure computational efficiency, we further randomly collect 500 samples from the cloud and save them as the gaze error vector distribution model. To use the model for match ratio calculation, for any specific candidate sentence, and for every gaze location recorded within the jump reading trajectory, we attach the model onto it, and count what percent of the cloud samples falls on a candidate sentence. We then accumulate the results of all gaze locations in the trajectory as the value of the match ratio for that candidate sentence. This match ratio will then serve as a metric of evaluation for the candidates' election."}, {"title": "Large Language Model Assisted Decision Making", "content": "3) Large Language Model Assisted Decision Making: The match ratio algorithm alone could only leverage the spatial relationship between the jump reading trajectory and candidate sentences following punctuation marks. This reminds us that we are handling reading materials, which hold rich contextual information that could also provide guidance for the candidate election. The recent advancement of large language models has introduced new possibilities for aiding reading tracking tasks with their superior language perception capabilities. Therefore, to further exploit the contextual information embedded in the text, we propose using LLM to aid the jump reading candidate election. The inputs provided to the LLM include the text paragraph, the reading history before the detection of jump reading, and the candidate sentences. The expectation for the LLM agent is that it judges which candidate sentence is most likely to be the new starting point, based on logical and linguistic relationships."}, {"title": "Absence of Ground Truth", "content": "Absence of Ground Truth: Typical methods of using LLMs are fine-tuning the LLMs with abundant domain-specific datasets to transform their perception capabilities. However, in the domain of reading tracking, collecting a massive ground truth dataset is extremely hard, as there is no reliable way of labeling the reading progress. A substitute method may be asking the user to use the cursor to indicate the reading progress. However, such a method will inevitably divert the user's attention to actively staring at the cursor in order to acquire accurate ground truth, which would then impair the natural reading pattern. Similar reason applies to other substitutes, such as vocally reading the text out loud. As a result, the absence of ground truth data eliminates the possibility of fine-tuning the LLM to assist reading tracking."}, {"title": "Prompt Engineering", "content": "Prompt Engineering: The inherent language perception capabilities of LLMs are sufficient for understanding reading material and aid in electing candidates for jump reading, mak-ing fine-tuning less necessary. Therefore, we employ prompt engineering techniques to leverage the LLM's perception abili-ties. Prompt engineering capitalizes on the foundation model's pre-encoded knowledge, which can be activated by a specific prompt describing the task. To implement this, we augment all samples with a leading prompt, such as:\n\"The user was just reading: <Reading Material>, which option is most likely to be read next by the user?\""}, {"title": "Dynamic Calibration via Line-Gaze Alignment", "content": "D. Dynamic Calibration via Line-Gaze Alignment\nReading is a relatively time-consuming task. Based on the field test (Section V), the average time spent on reading a 200-word paragraph is 138.9 seconds. During such a time span, we often find the users slightly and inadvertently drifting away from the position, where they conducted the calibration with the eye tracker. This could cause the gaze tracking calibration results to gradually degrade or fail, resulting in a slowly increasing gaze tracking error. To demonstrate the effect of drifting, we aggregate the gaze data traces of all 16 users along one time axis to reflect the overall temporal trend of gaze tracking error. As depicted in Figure 7, we can see that the gaze tracking error gradually increases over time. According to the linear fitting results, the gaze error could grow from 1.9244 cm to 2.2015 cm within 330 seconds."}, {"title": "III. IMPLEMENTATION", "content": "We implemented $RT^2H$ using Matlab, on a laptop with a 15.6\" screen of 1920x1080 resolution. The laptop is equipped with Intel(R) Core(TM) i7-8750 CPU @ 2.20GHz. As demon-strated in Figure 8, we render the text paragraph within a blank plot window, and thus acquire in first place the coordinate of all words, letters, and punctuation marks. We set the default line spacing as 4.5mm."}, {"title": "A. Gaze Tracking", "content": "We use the state-of-the-art eye gaze tracker, Tobii Pro Spark [8], as gaze tracking input. It uses infrared emitters and sensors to capture featured eye reflection to track gaze. With the infrared feature, it can work in all lighting conditions, in-cluding dark environments. We follow the standard calibration procedure of the Tobii Pro Eye Tracker Manager software to calibrate the Tobii Pro Spark for every user before the test."}, {"title": "B. Text Highlighting", "content": "We highlight the words that have been read by the user according to the reading tracking results. The highlighting is implemented via a light green area under the text with 80% transparency, serving as visual guidance without greatly distracting the users. Reading the same words multiple times will enhance the existing highlighting. Additionally, upon system initialization, we highlight all punctuation marks within the text material using yellow shades, specifically periods, exclamation marks, and question marks."}, {"title": "C. Large Language Model", "content": "To enhance the capabilities of $RT^2H$, we integrate Matlab and Python to access the OpenAI GPT-API. This integration allows us to leverage the advanced language processing capa-bilities of state-of-the-art language models.\nSpecifically, for the large language model, we use GPT-40 mini. GPT-40 mini is a variant of the GPT-4 model, designed to provide high performance while being more efficient in terms of computational resources."}, {"title": "D. Forced Relocation via Double Clicking", "content": "In the case that $RT^2H$ is not tracking and highlighting accurately, the users are provided with a function of forced relocation by double clicking. When they double click on a word, the tracking will strictly relocate onto that word and resume tracking. The system will return a beep sound as confirmation to the user if the double clicking lands on a valid word, instead of another invalid blank area. The jump reading detection will reset upon the double clicking as we assume the user will look at that word when performing double clicking. The frequency of this function being used also reflects the reliability of $RT^2 H$. Lower frequency indicates lower demand of user participation, as well as potentially higher satisfaction."}, {"title": "IV. CONTROLLED EXPERIMENT", "content": "In this section we conduct controlled experiments to statis-tically evaluate the reading tracking accuracy."}, {"title": "A. Experiment Setting", "content": "We collect reading tracking ground truth via asking the user to look at the cursor, and use it to indicate the progress of reading. However, this ground truth data collection method inevitably impacts the natural reading pattern, making certain evaluations relatively unrealistic. In another word, we could only simulate reading when using cursor to collect ground truth. We rely on the field test for more real-case evaluations."}, {"title": "B. Linear Reading Tracking Accuracy", "content": "We first evaluate the reading tracking accuracy during linear reading. We set the evaluation metric as the distance between the current word being read according to the reading tracking result, and the ground truth reading progress labeled by the cursor. Specifically, if the cursor falls exactly on the word that is anticipated by the tracking algorithm as the current word being read, we mark the tracking error as zero. As shown in Figure 9, the reading tracking error concentrates on both low and high levels. It indicates that $RT^2H$ is able to firmly track the words in the same line, while the high error is probably due to line switching, during which the gaze needs to move a long distance from right to left. The overall average linear reading tracking error is 1.3161 cm, far lower than the 2 cm-level raw gaze tracking error, demonstrating the advantage of adopting linear reading assumption."}, {"title": "C. Jump Reading Tracking Accuracy", "content": "We evaluate the accuracy of jump reading tracking via checking whether the jump reading relocation arrives at the correct line. We also monitor the amount of candidates cap-tured along the jump reading trajectory to study its impact on the relocation accuracy. As shown in Table I, the overall accuracy of jump reading relocation is 84.21%, and the accuracy is 94.44% when there is only one candidate. The gap between 94.44% and 100% originates from the inherent gaze tracking error that could cause the trajectory to miss the correct candidate. When there are two candidates contending for the election, the accuracy is 84.62%. Usually the match ratio of the correct candidate would largely exceed the other candidate, making the election relatively robust. However, when there are three or more candidates captured along the trajectory, the accuracy drops to 57.14%. Multiple candidates being considered usually implies that the destination area is crowded with candidates, whose locations may fit the gaze error vector model with similar match ratio, making the election difficult to distinguish the correct candidate."}, {"title": "D. Ablation Study on Dynamic Calibration", "content": "To examine the drift-suppressing effect of the dynamic cal-ibration component, we compare $RT^2H$ against $RT^2H$ without the dynamic calibration mechanism ($RT^2H$ w/o DC). We test the two baseline systems on the same reading material for 20 times, and aggregate all data traces onto one time axis to reflect the overall error trend of the two baseline systems. Specifically, we focus on the Y-axis (vertical) gaze error. As shown in Figure 10, despite an error peak at the beginning, $RT^2H$ demonstrates higher stability when compared to the $RT^2H$ w/o DC. Without the drift suppression enabled by the dynamic calibration design, it tends to destabilize after 60 seconds of active usage. Statistically, the average Y-axis error over 120 seconds is 0.3904 cm, and 0.6609 cm, for $RT^2H$ and $RT^2H$ w/o DC, respectively."}, {"title": "E. Computation and Communication Latency", "content": "Based on real experiment, we observe that every iteration in linear reading tracking takes an average of 0.000127 second to finish, which potentially supports up to 7500+ Hz inference. In the actual implementation, we added a forced pause of 0.05 second for linear reading tracking to cap its iteration frequency. As for jump reading relocation, the candidate election pro-cedure takes an average of 0.7893 second to complete, with 0.6382 second of it consumed by LLM API communication. Such latency does not greatly impact users' experience."}, {"title": "V. FIELD TEST", "content": "We propose the real-time highlighting function in order to help users track their reading progress and reduce the chance of switching to wrong lines after finishing reading one. However, the real-time highlighting is a dual-way interaction. Both the system and the user receives feedback from another in real-time. Such attention-sensitive interaction requires users to fully immerse themselves in the system, making simulated experiments insufficient. Therefore, we put $RT^2H$ into the real field test, to evaluate the combined effect of reading tracking and real-time highlighting."}, {"title": "A. Baselines", "content": "\u2022 $RT^2H$: The full version of our reading tracking and real-time highlighting system, whose design components are introduced in this paper.\n\u2022 RTH: The simple version without reading tracking com-ponents. It directly uses the gaze tracking results to highlight the words that the gaze falls onto. By comparing RTH and $RT^2H$, we demonstrate the effectiveness of our design components.\n\u2022 Blank: The blank version, where the system only renders the paragraph with the provided text material, using the same rendering format as other baselines, e.g., font size, line spacing, paragraph width, etc.."}, {"title": "B. Reading Materials", "content": "Since we assigned questions for the volunteers to answer, each reading material would expire for that volunteer after being read, and cannot be used twice for the same user. Additionally, there are three versions of the system to be tested. Therefore, we arranged three different reading materials for the reading test. The three reading materials contain 272, 257, and 249 words, respectively."}, {"title": "C. Reading Efficiency", "content": "We evaluate the reading efficiency with different versions of the system via recording the time spent by the volunteers to come up with the right answer. As illustrated in Table II, our algorithm, $RT^2H$, demonstrated satisfying performance compared to the baselines. $RT^2H$ required an average time of 98.2 seconds, compared to 113.5 seconds for blank version and 115.7 seconds for RTH. This verifies that highlighting text could potentially enhance reading efficiency."}, {"title": "D. User Experience on Different Baselines", "content": "After volunteers finished testing all three versions, we asked their experience (ranges from 1 to 5) of using the systems via a questionnaire. RTH achieved a score of 2.7, which is even lower than the score of 3.3 of the blank version. This indicates that directly applying the gaze tracking results to highlight text could produce negative effects. Meanwhile, $RT^2H$ achieved the highest overall user experience score of 3.8, showcasing the effectiveness of the algorithmic designs of $RT^2H$."}, {"title": "E. User Evaluation on System Features", "content": "We also collected feedback on several important features of $RT^2H$, including the system's helpfulness, whether it causes distraction, accuracy of jump reading tracking, and overall interest to see it on the market. Overall, $RT^2 H$'s strong performance across these features underscores its robustness and efficiency. These findings suggest $RT^2H$'s potential for broader application in real-world scenarios."}, {"title": "Whether Highlighting Is Distraction", "content": "1) Whether Highlighting Is Distraction: To address a com-mon concern on whether the highlighted area would distract the user, we add in the questionnaire a specific question on whether it distracts them. We receive an evaluation feedback of 3.8 \u00b1 1.1, indicating the highlighting does not cause great distraction, possibly due the 80% transparency."}, {"title": "Jump Reading Tracking", "content": "2) Jump Reading Tracking: We specifically notify the vol-unteers that $RT^2H$ supports jump reading. After the test, we investigate their satisfaction on the jump reading tracking accuracy, for which $RT^2H$ receives a score of 3.6, indicating relatively reliable jump reading tracking support."}, {"title": "F. Force Relocation Frequency", "content": "As introduced in Section III-D, the volunteers are provided with a function of force relocation by double clicking. Lower frequency of force relocation being performed would indicate higher reliability of our system. We make a clear introduction on this function to every volunteer before the testing of $RT^2H$. During the test, we count the amount of valid double clicks by listening to the confirmation beep. According to the test records, the average number of double clicking is 0.28, indicating relatively high robustness of $RT^2H$."}, {"title": "VI. RELATED WORK", "content": "A. Eye gaze tracking.\nEye gaze tracking has garnered significant research interest due to its diverse applications in human-computer interaction [10], [1], [27], [28], [9], psychology [29], [30], [31], and medicine [28], [32], [33], [34]. Recent studies have explored innovative applications of this technology in interactive sys-tems and wearable devices. For instance, Khamis et al. in-troduced GazeTouchPIN, a multimodal authentication system integrating gaze input and touch interaction to protect sensitive data on mobile devices [35]. Jansen et al. presented EyeScout, a system for active eye tracking enabling position-independent gaze interaction with large public displays [36].\nGeometric methods are a fundamental approach in gaze tracking, leveraging the geometric relationships between the eye and a camera or sensor to estimate gaze direction [10], [9], [37]. These methods rely on precise calibration and mathematical models to accurately infer gaze direction. Early techniques, such as the corneal reflection method [9], used the reflection of light on the cornea and pupil to estimate gaze direction [10], [37]. By analyzing the positions of corneal reflections relative to known eye or environmental features, researchers could determine gaze direction with reasonable accuracy. However, these methods required controlled lighting and were sensitive to head movements.\nAdvancements in geometric gaze tracking have led to the development of non-invasive, camera-based systems capable of real-time gaze direction estimation. Model-based tracking techniques, using 3D models of the eye and surrounding features, have improved accuracy and robustness [1]. AS-Gaze, for example, uses an iris model to estimate the gaze ray, allow-ing gaze tracking on various surfaces such as mobile phone screens, computer displays, or whiteboards [2]. However, AS-Gaze faces limitations with free movement, depth information acquisition, and real-time performance, as it uses a single camera and requires significant computational time to match"}, {"title": "B. Reading tracking and LLMs", "content": "Eye-gaze tracking for reading progression has seen in-novative methods aimed at mitigating noisy data and the limitations of commercial eye-tracking devices. Bottos and Balasingam [15] introduced a Slip-Kalman Filter for tracking reading progression, significantly improving line detection accuracy and noise reduction over standard Kalman filters. However, this method assumes sequential reading without repetition, which may not reflect natural reading behaviors. Alternative methods like hidden Markov models [41], [42], [43] for classifying eye-gaze fixation points [44] improve line detection accuracy but require initial parameter estimation and a consistent text layout, limiting their flexibility. Least squares batch estimation techniques have been used to filter noisy data [16], enhancing reading smoothness but facing limitations in initial line detection accuracy and managing repeated readings of the same line. Combining least squares batch estimation with hidden Markov models offers smoother progression but necessitates predefined line numbers and initial training with synthetic data [17].\nA significant challenge in the field is addressing jumping reading behaviors, where readers skip forward or backward within a text. This non-linear reading pattern, common in real-world scenarios, challenges the effectiveness of sequentially oriented methods like the Slip-Kalman Filter. This highlights the need for more flexible and adaptive eye-gaze tracking methods. Therefore, we propose integrating LLMs into our design, leveraging their adaptability in understanding and gen-erating human-like text [45], [46], [47], [48], [49], [50]. While fine-tuning LLMs involves extensive data collection [19], we employ prompt engineering to guide the LLM in generating desired outputs without extensive retraining [20], [21], [22], [23]. This approach enhances the flexibility and adaptability of eye-gaze tracking systems, particularly in managing non-linear reading behaviors.\nPrompt engineering has been successfully applied in various domains to enhance the performance of LLMs. For instance, in clinical named entity recognition, detailed instructions and contextual specifications significantly improve the extraction of critical medical information from clinical notes and safety reports [21]. Similarly, LLMs have been used to predict chem-"}, {"title": "C. Jump Reading Relocation", "content": ""}, {"title": "D. Forced Relocation via Double Clicking", "content": ""}, {"title": "E. Computation and Communication Latency", "content": ""}]}