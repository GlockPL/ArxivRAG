{"title": "HOG-Diff: Higher-Order Guided Diffusion for Graph Generation", "authors": ["Yiming Huang", "Tolga Birdal"], "abstract": "Graph generation is a critical yet challenging task as empirical analyses require a deep understanding of complex, non-Euclidean structures. Although diffusion models have recently made significant achievements in graph generation, these models typically adapt from the frameworks designed for image generation, making them ill-suited for capturing the topological properties of graphs. In this work, we propose a novel Higher-order Guided Diffusion (HOG-Diff) model that follows a coarse-to-fine generation curriculum and is guided by higher-order information, enabling the progressive generation of plausible graphs with inherent topological structures. We further prove that our model exhibits a stronger theoretical guarantee than classical diffusion frameworks. Extensive experiments on both molecular and generic graph generation tasks demonstrate that our method consistently outperforms or remains competitive with state-of-the-art baselines. Our code is available at https://github.com/Yiminghh/HOG-Diff.", "sections": [{"title": "1. Introduction", "content": "Graphs provide an elegant abstraction for representing complex empirical phenomena by encoding entities as vertices and their relationships as edges, thereby transforming unstructured data into analyzable representations. Modelling the underlying distribution of graph-structured data is a crucial yet challenging task with broad applications, including social network analysis, motion synthesis, drug discovery, protein design, and urban planning (Zhu et al., 2022). The study of graph generation seeks to synthesize graphs that align with the observed distribution and traces back to seminal models of random network models (Erd\u0151s et al., 1960; Barab\u00e1si & Albert, 1999). While these models offer foundational insights, they are often too simplistic to capture the complexity of graph distributions we encounter in practice.\nRecently, advances in generative models have leveraged"}, {"title": "2. Preliminaries", "content": "Higher-order Networks. Graphs are elegant and useful abstractions for various empirical objects; typically, they can be represented as $G \\equiv (V, E, X)$. Here, $V$ denote the node set, $E \\subset V \\times V$, and $X$ denotes the nodes feature matrix. However, many empirical systems exhibit group interactions that extend beyond simple pairwise relationships (Battiston et al., 2020). To capture these complex interactions, higher-order networks\u2014such as hypergraphs, simplicial complexes, and cell complexes-offer more expressive alternatives by capturing higher-order interactions among entities (Papamarkou et al., 2024). Among these, cell complexes are fundamental in algebraic topology, offering a flexible generalization of pairwise graphs (Hatcher, 2001).\nDefinition 2.1 (Regular cell complex). A regular cell complex is a topological space $S$ with a partition into subspaces (cells) {xa}a\u2208Ps, where Ps is an index set, satisfying the following conditions:\n1. For any $x \\in S$, every sufficiently small neighborhood of $x$ intersects finitely many cells.\n2. For each cell $x_a$, the boundary $\\partial x_a$ is a union of finitely many cells, each having dimension less than that of $x_a$.\n3. Each cell $x_a$ is homeomorphic to $R^{n_a}$, where $n_a$ is dimension of $x_a$.\n4. (Regularity) For every $a \\in Ps$, there exists a homeomorphism $\\phi$ of a closed ball $B^{n_a} \\subset R^{n_a}$ to the closure $\\overline{X}$ such that the restriction of $\\phi$ to the interior of the ball is a homeomorphism onto $x_a$.\nFrom this definition, we can derive that $S$ is the union of the interiors of all cells, i.e., $S = \\bigcup_{a \\in P_s} int(x_a)$, where $int(x_a)$ denotes the interior of the cell $x_a$. Intuitively, a cell complex can be constructed hierarchically through a gluing procedure. It begins with a set of vertices (0-cells), to which edges (1-cells) are attached by gluing the endpoints of closed line segments, thereby forming a graph. This process can be extended by taking a two-dimensional closed disk and gluing its boundary (i.e., a circle) to a simple cycle in the graph. While we typically focus on dimensions up to two, this framework can be further generalized by gluing the boundary of n-dimensional balls to specific (n - 1)-cells in the complex. In this work, we also utilize simplicial complexes (SCs), a simpler and more restrictive subclass where 2-cells are limited to triangle shapes. A further introduction to higher-order networks can be found in App. B.\nScore-based Diffusion Models. A fundamental goal of generative models is to produce plausible samples from an unknown target data distribution $p(x_0)$. Score-based diffusion"}, {"title": "3. Higher-order Guided Diffusion Model", "content": "We now present our Higher-order Guided Diffusion (HOG-Diff) model, which enhances graph generation by exploiting higher-order structures. We begin by detailing a coarse-to-fine generation curriculum that incrementally constructs graphs, followed by the introduction of three essential supporting techniques: the diffusion bridge, spectral diffusion, and a denoising model, respectively. Finally, we provide theoretical evidence validating the efficacy of HOG-Diff.\nCoarse-to-fine Generation. We draw inspiration from curriculum learning, a paradigm that mimics the human learning process by systematically organizing data in a progression from simple to complex (Soviany et al., 2022). Likely, an ideal graph generation curriculum should be a composition of multiple easy-to-learn and meaningful intermediate steps. Additionally, higher-order structures encapsulate rich structural properties beyond pairwise interactions that are crucial for various empirical systems (Huang et al., 2024). As a graph-friendly generation framework, HOG-Diff incorporates higher-order structures during the intermediate stages of forward diffusion and reverse generative processes, thereby realizing a coarse-to-fine generation curriculum.\nTo implement our coarse-to-fine generation curriculum, we introduce a key operation termed cell complex filtering (CCF). As illustrated in Fig. 2, CCF generates an intermediate state of a graph by pruning nodes and edges that do not belong to a given cell complex.\nDefinition 3.1 (Cell complex filtering). Given a graph $G = (V, E)$ and its associated cell complex $S$, the cell complex filtering operation produces a filtered graph $G' = (V', E')$ where $V' = \\{v \\in V | \\exists x_a \\in S : v \\in x_a\\}$, and $E' = \\{(u, v) \\in E | \\exists x_a \\in S : u, v \\in X_\\alpha\\}$.\nThis filtering operation is a pivotal step in decomposing the graph generation task into manageable sub-tasks, with the filtered results serving as natural intermediaries in hierarchical graph generation. The overall framework of our"}, {"title": "Diffusion Bridge Process", "content": "We realize the guided diffusion based on the generalized Ornstein-Uhlenbeck (GOU) process (Ahmad, 1988; Luo et al., 2023b), a stationary Gaussian-Markov process characterized by its mean-reverting property. Over time, the marginal distribution of the GOU process stabilizes around a fixed mean and variance, making it well-suited for stochastic modelling with\nterminal constraints. The GOU process $G$ is governed by the following SDE:\n$Q: dG_t = \\theta_t(\\mu \u2013 G_t)dt + g_t(G_t)dW_t,                                   (8)$\nwhere $\\mu = G_T \\in R$ is the target terminal state, $\\theta_t$ denotes a scalar drift coefficient and $g_t$ represents the diffusion coefficient. To ensure the process remains analytically tractable, $\\theta_t$ and $g_t$ are constrained by the relationship $g_t^2/\\theta_t = 2\\sigma^2$ (Luo et al., 2023b), where $\\sigma^2$ is a given constant scalar. Under these conditions, its transition probability admits a closed-form solution:\n$p(G_t | G_s) = N(m_{s:t}, v_{s:t}^2I)$\n$= N (\\mu + (G_s - \\mu) e^{-\\theta_{s:t}}, \\sigma^2 (1-e^{-2\\theta_{s:t}})),                               (9)$\nHere, $\\theta_{s:t} = \\int_0^z dz$, and for notional simplicity, $\\theta_{0:t}$ is replaced by $\\theta_t$ when $s = 0$. At time t progress, $p(G_t)$ gradually approaches a Gaussian distribution characterized by mean $\\mu$ and variance $\\sigma^2$, indicating that the GOU process exhibits the mean-reverting property.\nThe Doob's h-transform can modify an SDE such that it passes through a specified endpoint. When applied to the GOU process, this eliminates variance in the terminal state, driving the diffusion process toward a Dirac distribution centered at $G_T$ (Heng et al., 2021; Yue et al., 2024).\nProposition 3.2. Let $G_t$ evolve according to the generalized OU process in Eq. (8), subject to the terminal conditional $\\mu = G_{T_k}$. The conditional marginal distribution $p(G_t | G_{T_k})$ then evolves according to the following SDE:\n$dG_t = \\theta_t (1 + \\frac{2}{e^{2 \\theta_{t:T_k}} - 1}) (G_{T_k} - G_t)dt + g_{k,t}dW_t.$\\nThe conditional transition probability $p(G_t | G_{T_{k-1}}, G_{T_k})$ has analytical form as follows:\n$p(G_t | G_{T_{k-1}}, G_{T_k}) = N(m_t, \\bar{\\nu}I),$\\n$m_t = G_{T_k} + (G_{T_{k-1}} - G_{T_k})e^{-\\theta_{T_{k-1}t}}, $\\$\\bar{\\nu} = \\nu_{T_{k-1}:t} \\nu_{t:T_k} /\\nu_{T_{k-1}:T_k}.                                            (10)$\nHere,$\\int_a^b ds$, and $\\nu_{a:b} = \\sigma^2(1 \u2013 e^{-2\\theta_{a:b}})$.\nWe can directly use the closed-form solution in Prop. 3.2 for one-step forward sampling without performing multi-step forward iteration using the SDE. The reverse-time dynamics of the conditioned process can be derived using the theory of SDEs and take the following form:\n$dG_t = [f_{k,t}(G_t) - g_{k,t}\\nabla_{G_t} log p(G_t|G_{T_k})] dt + g_{k,t} d\\tilde{W}_t,$\\nwhere $f_{k,t}(G_t) = \\theta_t (1 + \\frac{2}{e^{2 \\theta_{t:T_k}} - 1}) (G_{T_k} - G_t).$"}, {"title": "Spectral Diffusion", "content": "Generating graph adjacency matrices presents several significant challenges. Firstly, the non-uniqueness of graph representations implies that a graph with n vertices can be equivalently modelled by up to n! distinct adjacency matrices. This ambiguity requires a generative model to assign probabilities uniformly across all equivalent adjacencies to accurately capture the graph's inherent symmetry. Additionally, unlike densely distributed image data, graphs typically follow a Pareto distribution and exhibit sparsity (Ghavasieh & De Domenico, 2024), so that adjacency score functions lie on a low-dimensional manifold. Consequently, noise injected into out-of-support regions of the full adjacency space severely degrades the signal-to-noise ratio, impairing the training of the score-matching process. Even for densely connected graphs, isotropic noise distorts global message-passing patterns by encouraging message-passing on sparsely connected regions. Moreover, the adjacency matrix scales quadratically with the number of nodes, making the direct generation of adjacency matrices computationally prohibitive for large-scale graphs.\nTo address these challenges, inspired by Martinkus et al. (2022); Luo et al. (2023a), we introduce noise in the eigenvalue domain of the graph Laplacian matrix $L = D \u2013 A$, instead of the adjacency matrix $A$, where $D$ denotes the diagonal degree matrix. As a symmetric positive semi-definite matrix, the graph Laplacian can be diagonalized as $L = U\\Lambda U^T$. Here, the orthogonal matrix $U = [u_1,\u2026\u2026, u_n]$ comprises the eigenvectors, and the diagonal matrix $\\Lambda = diag(\\lambda_1,\u2026\u2026,\\lambda_n)$ holds the corresponding eigenvalues. The relationship between the Laplacian spectrum and the graph's topology has been extensively explored (Chung, 1997). For instance, the low-frequency components of the spectrum capture the global structural properties such as connectivity and clustering, whereas the high-frequency components are crucial for reconstructing local connectivity patterns. Therefore, the target graph distribution $p(G_0)$ represents a joint distribution of $X_0$ and $\\Lambda_0$, exploiting the permutation invariance and structural robustness of the Laplacian spectrum. Consequently, we split the reverse-time SDE into two parts that share drift and diffusion coefficients as\n$\\begin{cases}\\ndX_t = [f_{k,t}(x_t) - g_tx log p_t(G_t|G_{Tk})] dt + g_{k,t}dW^{\\top}\\\\nd\\Lambda_t = [f_{k,t}(\\Lambda_t) - g_tx log p_t(G_t|G_{Tk})] dt + g_{k,t}d\\tilde{W}^?{}\\end{cases}$\nHere, the superscript of $X_t^{(k)}$ and $\\Lambda_t^{(k)}$ are dropped for simplicity, and $f_{k,t}$ is determined according to Prop. 3.2.\nTo approximate the score functions $\\nabla x_t log p_t(G_t|G_{T_k})$ and $\\nabla \\Lambda_t log p_t(G_t|G_{T_k})$, we employ a neural network $s_\\theta^{(k)}(G_t, G_{T_k}, t)$, composed of a node ($s_\\theta^x (G_t, G_{T_k}, t)$) and a spectrum ($s_\\theta^{\\Lambda} (G_t, G_{T_k}, t)$) output, respectively. The"}, {"title": "4. Experiments", "content": "We assess HOG-Diff against state-of-the-art baselines for both molecular and generic graph generation. Further ablation studies are conducted to analyze the impact of different topological guides. More details can be found in App. F.\n4.1. Molecule Generation\nExperimental Setup. To assess the capability of the proposed method in molecular generation, we conduct evaluations on two well-known molecular datasets: QM9 (Ramakrishnan et al., 2014) and ZINC250k (Irwin et al., 2012), and obtain the intermedia higher-order skeletons using the 2-cell complex filtering. We evaluate the quality of 10,000 generated molecules with six metrics as in Jo et al. (2022): Neighborhood Subgraph Pairwise Distance Kernel (NSPDK) MMD (Costa & Grave, 2010), Fr\u00e9chet ChemNet Distance (FCD) (Preuer et al., 2018), Validity (Val.), Validity without correction (Val. w/o corr.), Uniqueness (Uni.), and Novelty (Nov.) (Jo et al., 2022).\nBaselines. We evaluate our model against state-of-the-art molecular generation models, including auto-regressive methods, GraphAF (Shi et al., 2020), and GraphDF (Luo et al., 2021). For a fair comparison, as recommended by Jo et al. (2022), we extend GraphAF and GraphDF to account for formal charges in the molecular generation, termed GraphAF+FC and GraphDF+FC, respectively. We also compare our HOG-Diff with various flow-based and diffusion-based methods, including MoFlow (Zang & Wang, 2020), EDP-GNN (Niu et al., 2020), Graph-EBM (Liu et al., 2021), GDSS (Jo et al., 2022), and DiGress (Vignac et al., 2023).\nSampling Quality. We visualize the molecule generation process in Fig. 3 with more examples deferred to App. G. It can be observed that our model explicitly preserves higher-order structures during the generation process. Tab. 1 indicates that HOG-Diff consistently outperforms both auto-regressive and one-shot models. Notably, the dramatic decrease in NSPDK and FCD implies that HOG-Diff is able"}, {"title": "4.2. Generic Graph Generation", "content": "Experimental Setup. To display the topology distribution learning ability, we access HOG-Diff over three common generic graph datasets: (1) Community-small, containing 100 randomly generated community graphs; (2) Ego-small, comprising 200 small ego graphs derived from the Citeseer network dataset; (3) Enzymes, featuring 587 protein graphs representing tertiary structures of enzymes from the BRENDA database. Intermediate higher-order skeletons are obtained through 3-simplicial complex filtering, which prunes nodes and edges that do not belong to three-dimensional simplicial complexes. We employ the same train/test split as Jo et al. (2022) for a fair comparison with baselines. Consistent with You et al. (2018), we employ the maximum mean discrepancy (MMD) to quantify the distribution differences across key graph statistics, including degree (Deg.), clustering coefficient (Clus.), and the number of occurrences of orbits with 4 nodes (Orbit). To provide a holistic evaluation, we further calculate the mean MMD across these metrics, which is reported in the Avg. column as the overall assessment index. A lower MMD signifies a closer alignment between the generated and evaluation datasets, indicating superior generative performance.\nBaselines. We compare our model with prominent auto-regressive and one-shot graph generation approaches. Auto-"}, {"title": "4.3. Ablations: Topological Guide Analysis", "content": "During the experiments, we observe that HOG-Diff exhibits superior performance on complex datasets such as QM9 and Zinc250k, but comparatively modest results on the Ego dataset. Visualizations and statistics in Apps. F and G indicate that Ego contains the fewest higher-order structures among the datasets analyzed, suggesting that the choice of guide plays a pivotal role in the effectiveness of generation. To validate this hypothesis, we conduct further ablations using different types of topological information as guides.\nSpecifically, we employ three types of guiders: structures derived from 2-cell filtering (Cell), peripheral structures obtained by removing cell components (Peripheral), and Gaussian random noise (Noise). Employing noise as the guide aligns with classical generation models which generate samples by denoising noisy data. Fig. 4 (plot) visualizes how the spectrum loss changes during the training process and demonstrates that training the proposed model converges faster compared to the classical method, which aligns with the theoretical results in Prop. 3.3.\nThe sampling results in Fig. 4 (table) show that both peripheral and noise guides are inferior compared to using cell structures as the guide, empirically supporting Prop. 3.4. This highlights that certain topological structures, such as cells, are more effective in guiding the generation, likely due to their higher-order connectivity and structural significance. These observations stress the importance of identifying and leveraging proper topological structures as guides, which play a critical role in steering the generative process toward meaningful outputs. Moreover, this property opens up promising avenues for exploring the guide's potential as a tool to diagnose whether a specific component is integral and essential for the architecture. Furthermore, by systematically analyzing the impact of various guides, we can deepen our understanding of the interplay between structural characteristics and generative performance, thereby advancing the design of more effective graph generative models."}, {"title": "5. Related Works", "content": "We review graph generation methods along with higher-order generation. App. C presents a more detailed review.\nDeep Generative Models. Graph generative models make great progress by exploiting the capacity of deep neural networks. These models typically generate nodes and edges either in an autoregressive manner or simultaneously, utilizing techniques such as variational autoencoders (VAE) (Jin et al., 2018; Simonovsky & Komodakis, 2018), recurrent neural networks (RNN) (You et al., 2018), normalizing flows (Zang & Wang, 2020; Shi et al., 2020; Luo et al., 2021), and generative adversarial networks (GAN) (De Cao & Kipf, 2018; Martinkus et al., 2022).\nDiffusion-based Graph Generation. A breakthrough in graph generative models has been marked by the recent progress in diffusion-based generative models (Niu et al., 2020). Recent models employ various strategies to enhance the generation of complex graphs, including capturing node-edge dependency (Jo et al., 2022), addressing discretization challenges (Vignac et al., 2023; Huang et al., 2023), exploiting low-to-high frequency generation curriculum (Mo et al., 2024), and improving computational efficiency through low-rank diffusion processes (Luo et al., 2023a). Recent studies have also enhanced diffusion-based generative models by incorporating diffusion bridge processes, i.e., processes conditioned on the endpoints (Wu et al., 2022; Boget et al., 2024; Jo et al., 2024). Despite these advances, existing methods either overlook or inadvertently disrupt higher-order structures during graph generation, or struggle to model the topological properties, as denoising the noisy samples does not explicitly preserve the intricate structural dependencies required for generating realistic graphs.\nHigher-order Generative Models. Generative modelling uses higher-order information mostly in the form of hypergraphs. Models such as Hygene (Gailhard et al., 2024) and HypeBoy (Kim et al., 2024) aim to generate hypergraphs. Dymond (Zeno et al., 2021) focuses on higher-order motifs in dynamic graphs. To the best of our knowledge, we are the first to consider higher-order guides for graph generation."}, {"title": "6. Conclusion", "content": "We introduce HOG-Diff, a coarse-to-fine generation framework that explicitly exploits higher-order graph topology. It decomposes the complicated generation process into easier-to-learn sub-steps, which are implemented using a generalized OU bridge process. Our theoretical analysis justifies the effectiveness of HOG-Diff over classical diffusion approaches, which is validated by superior experimental results on both molecular and generic graph generation tasks. Our framework further promises to improve interpretability by enabling the analysis of different topological guides' performance in the generation process. This work is a key step in topological diffusion models, highlighting the impact of higher-order features absent in data and opening ample room for future work."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of deep generative models. Positive applications include generating graph-structured data for scientific discovery and accelerating drug discovery by generating novel molecular structures. However, like other generative technologies, our work could potentially be misused to synthesize harmful molecules, counterfeit social interactions, or deceptive network structures."}, {"title": "Appendix", "content": "Organization. The appendix is structured as follows: We first present the derivations excluded from the main paper due to space limitation in Section A. Section B introduces the concept and examples of higher-order networks. Additional explanations on related work are provided in Section C. Section D details the generation process, including the architecture of the proposed denoising network, as well as the training and sampling procedures. Computational efficiency is discussed in Section E. Section F outlines the experimental setup, and Section G concludes with visualizations of the generated samples.\nA. Formal Statements and Proofs\nThis section presents the formal statements of key theoretical results and their detailed derivations. We will recall and more precisely state the propositions before presenting the proof.\nA.1. Diffusion Bridge Process\nIn the following, we derive the Generalized Ornstein-Uhlenbeck (GOU) bridge process using Doob's h-transform (Doob & Doob, 1984) and analyze its relationship with the Brownian bridge process.\nRecall that the generalized Ornstein-Uhlenbeck (GOU) process is the time-varying OU process. It is a stationary Gaussian-Markov process whose marginal distribution gradually tends towards a stable mean and variance over time. The GOU process $Q$ is generally defined as follows (Ahmad, 1988; Luo et al., 2023b):\n$Q: dG_t = \\theta_t(\\mu \u2013 G_t) dt + g_t(G_t)dW_t,                                   (13)$\nwhere $\\mu$ is a given state vector, $\\theta_t$ denotes a scalar drift coefficient and $g_t$ represents the diffusion coefficient. At the same time, we require $\\theta_t$, $g_t$ to satisfy the specified relationship $g_t^2/\\theta_t = 2\\sigma^2$, where $\\sigma^2$ is a given constant scalar. As a result, its transition probability possesses a closed-form analytical solution:\n$p(G_t | G_s) = N(m_{s:t}, v_{s:t}^2I),$\\n$m_{s:t} = \\mu + (G_s \u2212 \\mu) e^{-\\theta_{s:t}}, $\\$\nv_{s:t}^2 = \\sigma^2 (1-e^{-2\\theta_{s:t}}).                               (14)$\nHere, $\\theta_{s:t} = \\int_s^t Ozdz$. When the starting time t = 0, we substitute $\\theta_{0:t}$ with $\\theta_t$ for notation simplicity.\nProposition 3.2. Let $G_t$ evolve according to the generalized OU process in Eq. (8), subject to the terminal conditional $\\mu = G_{T_k}$. The conditional marginal distribution $p(G_t | G_{T_k})$ then evolves according to the following SDE:\n$dG_t = \\theta_t (1 + \\frac{2}{e^{2 \\theta_{t:T_k}} - 1}) (G_{T_k} - G_t)dt + g_{k,t}dW_t.$\nThe conditional transition probability $p(G_t | G_{T_{k-1}}, G_{T_k})$ has analytical form as follows:\n$p(G_t | G_{T_{k-1}}, G_{T_k}) = N(m_t, \\bar{\\nu}I),$\\n$m_t = G_{T_k} + (G_{T_{k-1}} - G_{T_k})e^{-\\theta_{T_{k-1}t}}\\frac{ \\nu_{t:T_k}}{ \\nu_{T_{k-1}:T_k}}, $\\$\n\\bar{\\nu} = \\nu_{T_{k-1}:t} \\nu_{t:T_k} /\\nu_{T_{k-1}:T_k}.                                            (16)$\nHere, $\\int_a^b fds$, and $\\nu_{a:b} = \\sigma^2(1 \u2013 e^{-2\\theta_{a:b}})$.\nProof. To simplify the notion, in the k-th generation step, we adopt the following conventions: $T = T_k$, $x_t = G_t^{(k)}$, $\\theta = T_{k\u22121}$, $x_0 = G_{T_{k\u22121}}$, $x_T = G_{T_k}$.\nFrom Eq. (9), we can derive the following conditional distribution\n$p(x_T | x_t) = N(x_t + (x_t - x_T)e^{\\theta_t:T},v^2_TI).$\\n"}, {"title": "HOG-Diff: Higher-Order Guided Diffusion for Graph Generation", "content": "Hence, the h-function can be directly computed as:\n$h(x_t, t, x_T, T) = \\nabla x_t log p(x_T | X_t)$\n$= - \\nabla x_t \\frac{(x_T - X_t)^2e^{-2\\theta_{t:T}}}{\\nu_{t:T}} + const^2$\n$= (x_T - x_t) \\frac{e^{-2\\theta_{t:T}}}{\\nu_{t:T}},$\\n$= (x_T \u2013 x_t)\\frac{\\theta}{\\sigma^2} \\frac{1}{(e^{2\\theta_{t:T}} - 1)}.$                                        (18)\nThen the Doob's h-transform yields the representation of an endpoint $x_T$ conditioned process defined by the following SDE:\n$dx_t = [f(x_t,t) + g_t^2 h(x_t, t, x_T,T)] dt + g_tdw_t$\n$= (\\theta_t + \\frac{g_t^2}{\\sigma^2 (e^{2\\theta_{t:T}} - 1)}(x_T - x_t)) dt + g_tdw_t $\n$= \\theta_t (1 + \\frac{2}{e^{2\\theta_{t:T}} - 1}) (x_T - x_t)dt + g_tdw_t.   (19)$\nGiven that the joint distribution of $[x_0, x_t, x_T]$ is multivariate normal, the conditional distribution $p(x_t | x_0,x_T)$ is also Gaussian:\n$p(x_t | X_0, X_T) = N(m_t, \\nu_t^2I),$\\nwhere the mean $m_t$ and variance $\\nu_t^2$ are determined using the conditional formulas for multivariate normal variables:\n$m_t = E[x_t | X_0 | X_T] = E[x_t | X_0] + Cov(x_t, X_T | X_0) Var(X_T | X_0) ^{-1}(X_T - E[X_T | X_0]),$\\n$\\nu^2_t = Var(x_t | X_0 | X_T) = Var(x_t | X_0) - Cov (x_t, X_T | X_0) Var(X_T | X_0)^{-1}Cov(X_T, X_t | X_0).$\\nNotice that\n$Cov(x_t, X_T | X_0) = Cov \\left(x_t, (x_t \u2013 X_0)e^{-\\theta_{t:T}} | X_0 \\right) = e^{-\\theta_{t:T}} Var(x_t | x_0).$\\nBy substituting this and the results in Eq. (9) into Eq. (21), we can obtain\n$m_t = (x_T + (X_0 - x_T)e^{-\\theta_{t}}) + (e^{-\\theta_{t:T}} \\nu_t^2 / \\nu_T^2) (X_T - x_T - (X_0 - X_T)e^{-\\theta_{t}})$\n$m_t = (X_T + (X_0 - x_T)e^{-\\theta_{t}}) + (e^{-\\theta_{t:T}} \\nu_t^2 / \\nu_T^2) (X_T - x_T - (X_0 - X_T)e^{-\\theta_{t}})\n= x_T + (X_0 - X_T) (e^{-\\theta_{t}} + \\frac{e^{-\\theta_{t:T}}\\sigma^2}{ 1 \u2013 e^{-2\\theta_t}} ) \\frac{(1 \u2013 e^{-2 \\theta_t})}{ (\\sigma^2)}\n= x_T + (X_0 - X_T)e^{-\\theta_{t}}\\frac{ \\nu_t^2}{ \\nu_T^2}.                                                                                                 (23)$\nand\n$\\nu = \\nu_t^2 - (e^{-2\\theta_{t:T}} \\nu_t^2)^2 /\\nu_T^2$\\n$= \\frac{\\sigma^2}{2(1 - e^{-2\\theta_T}} ( 1 - e^{-2 \\theta_t} -e^{-2\\theta_{t:T}} (1 \u2013 e^{-2 \\theta_t}))$\\n$= \\nu_t^2\\frac{\\sigma^2}{\\nu_T^2}.                                                                                           (24)$\nFinally, we conclude the proof by reverting to the original notations."}, {"title": "HOG-Diff: Higher-Order Guided Diffusion for Graph Generation", "content": "Note that the generalized OU bridge process", "a": "b"}, "int_a^b \\theta_s ds = @ \\theta(b \u2212 a) \u2192 0.$                           (25)\nConsider the term $e^{2\\theta_{t:T_k}} - 1$, we approximate the exponential function using a first-order Taylor expansion for small $\\theta_{t:T_k}$:\n$e^{2\\theta_{t:T_k}} - 1 \u2248 2\\theta_{t:T_k} \u2192 2@\\theta(T_k - t).$\nHence, the drift term in the generalized OU bridge simplifies to\n@\u2248$$\\frac{\\theta_t (1 +  2/(e^{2{\\theta_{t:T_k}} - 1)}}{T_k - t}.          (26)$$\nConsequently, in the limit $\\theta_t \u2192 0$, the generalized OU bridge process described in Eq. (15) can be modelled by the following SDE:\n$dG_t = \\frac{G_{T_k}-G_t}{T_k -t} dt + g_{k,t}dW_t.$\nThis equation precisely corresponds to the SDE representation of the classical Brownian bridge process.\n(27)\nIn contrast to the generalized OU bridge process in Eq. (15), the evolution of the Brownian bridge is fully determined by the noise schedule $g_{k,t}$, resulting in a simpler SDE representation. However, this constraint in the Brownian bridge reduces the flexibility in designing the generative process.\nNote that the Brownian bridge is an endpoint-conditioned process relative to a reference Brownian motion, which the SDE governs:\n$dG_"]}