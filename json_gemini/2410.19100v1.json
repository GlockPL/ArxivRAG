{"title": "VIDEOWEBARENA:\nEVALUATING LONG CONTEXT MULTIMODAL AGENTS\nWITH VIDEO UNDERSTANDING WEB TASKS", "authors": ["Lawrence Jang", "Yinheng Li", "Charles Ding", "Justin Lin", "Paul Pu Liang", "Dan Zhao", "Rogerio Bonatti", "Kazuhito Koishida"], "abstract": "Videos are often used to learn or extract the necessary information to complete tasks\nin ways different than what text and static imagery alone can provide. However,\nmany existing agent benchmarks neglect long-context video understanding, instead\nfocusing on text or static image inputs. To bridge this gap, we introduce VideoWe-\nbArena (VideoWA), a benchmark for evaluating the capabilities of long-context\nmultimodal agents for video understanding. VideoWA consists of 2,021 web agent\ntasks based on manually crafted video tutorials, which total almost four hours of\ncontent. For our benchmark, we define a taxonomy of long-context video-based\nagent tasks with two main areas of focus: skill retention and factual retention.\nWhile skill retention tasks evaluate whether an agent can use a given human demon-\nstration to complete a task efficiently, the factual retention task evaluates whether\nan agent can retrieve instruction-relevant information from a video to complete\na task. We find that the best model achieves 13.3% success on factual retention\ntasks and 45.8% on factual retention QA pairs, far below human performance\nat 73.9% and 79.3%, respectively. On skill retention tasks, long-context models\nperform worse with tutorials than without, exhibiting a 5% performance decrease\nin WebArena tasks and a 10.3% decrease in VisualWebArena tasks. Our work\nhighlights the need to improve the agentic abilities of long-context multimodal\nmodels and provides a testbed for future development with long-context video\nagents.", "sections": [{"title": "INTRODUCTION", "content": "Humans often use videos to complete daily tasks, whether to learn from tutorials or retrieve informa-\ntion from within one or several videos. As we build AI assistants, these multimodal agents must also\npossess similar capabilities to understand and process videos to accomplish tasks or learn how to\naccomplish a workflow, plan, and make decisions.\nWhile videos can provide a rich source of information, capturing spatial and temporal dynamics that\nimages or text alone may not convey, integrating video input into multimodal models introduces\nunique challenges relating to temporal coherence, context retention, or efficient information retrieval\nover lengthy, extended sequences. These challenges can be further compounded when models are\ndeployed as autonomous agents operating in complex environments. In these scenarios, the ability of\nmodels to maintain long-term memory, perform informational retrieval, and adapt to new information\ncontinuously is critical for tasks that require sustained engagement over time.\nRecent improvements in long-context understanding of large video-capable vision language models\n(e.g., LLaVaNeXt, LongVILA) have enabled agents to process and understand more information than\nbefore, including long video understanding. However, from an evaluative perspective, there remains\na significant gap in existing benchmarks that can comprehensively evaluate the agentien capabilities\nof these models across diverse multimodal scenarios, particularly those involving video inputs. The\nrequirement for agents to operate across varying modalities and time frames makes developing and\nproperly evaluating long-context multimodal models essential."}, {"title": "BACKGROUND", "content": "Large Vision Language Models. Large vision language models (VLMs) have been popular study\nsubjects for incorporating video input and can be characterized by their learning mechanisms or\noverall architectural design. Popular state-of-the-art (SOTA) models like the GPT-4 (OpenAI, 2024)\nfamily of models, Claude (AI Anthropic, 2024), and Gemini (Google, 2023) are now able to handle\nnot just text but also visual and even audio input.\nGenerally, similar to LLMs, VLM architectures typically revolve around two types-models with\neither a joint encoder-decoder architecture such as LLaVA and its variations (Liu et al., 2023) or a\ndecoder-only architecture. Encoder-decoder VLMs tend to project different modalities through a\nshallow neural network or fully connected layer to link modalities. Decoder-style models typically\nrely on a decoder-only LLM that processes raw inputs (e.g., text tokens, image patches, etc.) such as"}, {"title": "VIDEOWEBARENA ENVIRONMENT", "content": "SUMMARY & OVERVIEW\nVideoWA centers around six key thematic environments created by VisualWebArena (Koh et al.,\n2024a) and WebArena (Zhou et al., 2024): Reddit, Classifieds, Shopping, Shopping Admin, Map,\nand Gitlab. Tables 1 and 2 for a finer characterization of the tasks and videos within the benchmark.\nThese domains' websites are locally hosted since the docker images for each website are publicly\navailable online. There is an Amazon Machine Image and instructions dedicated to hosting these\nwebsites on an EC2 instance; we refer readers to the codebase for further information. By doing this,\nwe can make our benchmark realistic but reproducible, leveraging data and code from real and popular\nwebsites on the internet. We refer readers to WebArena (Zhou et al., 2024) and VisualWebArena\n(Koh et al., 2024a) for more information on each site and their setup."}, {"title": "ENVIRONMENT DETAILS", "content": "FRAMEWORK\nWe can define an agent's trajectory on our tasks as a partially observable Markov decision process\n(POMDP) (S, O, A, T, R) with state space S, observation space O, action space A containing\nactions a, transition function T : S \u00d7 A \u2192 S, and reward function R : S\u00d7A \u2192 R. Given current"}, {"title": "OBSERVATION SPACE", "content": "The observation space for the VideoWA environment is strictly predicated on the Set-of-Marks\nobservation space in VisualWebArena. The environment uses executable JavaScript code at each\nstep to extract the interactable HTML elements from the webpage and present them in a top-down\norder. Similarly, the JavaScript code extracts the bounding boxes of each interactable element and\na screenshot of the webpage with bounding boxes over the interactable elements is generated for\ninput to the agent along with the text state representation. At each time step, the agent is presented\nwith the overlayed Set-of-Marks screenshot and text observation space, along with the chosen video\ninformation to be put into context."}, {"title": "ACTION SPACE", "content": "The action space for the agents in the VideoWA environment can be seen in Table 3. The agents are\nprompted to generate a single action from the action space at each time step. Each action is associated\nwith Playwright Python code that automatically performs the action within the browser. The 'elem'\nparameter in the action represents the unique Set-of-Marks element that can be interacted with from\nthe observation space provided through the environment's JavaScript code."}, {"title": "TASK DESIGN", "content": "The taxonomy covers two subsets of tasks skill retention and factual retention- inspired by\nreal-world use cases. We illustrate the taxonomy breakdown in Figure 6. We define skill retention\nas the ability to learn from and use a given human demonstration to efficiently complete a task For\nexample, using YouTube tutorials or screen recordings of expert demonstrations to learn how to\nperform a task is a form of skill retention. On the other hand, factual retention is the ability to retrieve\ninformation relevant to a user's specific question/task present in a video that may not be the video's\nmain focus (e.g., an incidental detail). For example, one might want to buy the shoes a particular\nNBA player is wearing that are shown within a short duration of a much longer basketball highlights\nvideo. To complete the task, the model must extract not only information about the specific player\nbut also their shoes, even if this information is secondary to the main content of the video.\nWe present example tasks in Table 4 and an example of an agent on a stylized task in Figure 7.\nEach task has an 'intent', which is the objective of the task. For all of the newly created factual\nretention tasks, there is also an 'intermediate_intent', a video-based question that must be answered\ncorrectly to have the information necessary to complete the task. Each task also has an automatic\nevaluator function for both 'intent' and 'intermediate_intent' that returns a score of 0 or 1 based on the\nenvironment and response given by the LLM agent. Each task also has an agentic difficulty, distributed\nbetween easy, medium, and hard. The agentic difficulty for each task signifies the complexity of the\naction sequence needed to complete an intent successfully. For agentic difficulty, we classify a task\nas easy if it can be completed in 1-3 steps, medium if it can be completed in 4-9 steps, and hard if it\ncan be completed in more than 9 steps. This classification is adopted from VisualWebArena (Koh\net al., 2024a). Figure 2 provides a more detailed breakdown of task difficulty."}, {"title": "VIDEO CREATION AND SKILL RETENTION TASKS", "content": "Our benchmark contains 74 unique videos, totaling almost 4 hours of video content (see Table 1 for\ndetails) all of our video tutorials are based on tasks in WebArena and VisualWebArena. We provide\nour videos online through a YouTube channel and a Google Drive link containing the zip file of all the\nvideos. We formulated these videos by accumulating all the feasible intent templates in WebArena\nand VisualWebArena. We take 297 unique templates from VisualWebArena and 220 unique templates\nfrom WebArena, totaling 1621 total intents. Further details can be found in Appendix A.1\nWe map each of our video tutorials to the respective tasks in the WebArena and VisualWebArena task\nset to create skill retention tasks. We then create 400 original factual retention tasks based on these\nsame tutorials. We had three of the paper's authors create videos and corresponding tasks for each\nvideo they created. We then conducted cross-validation quality assurance with an author who did not\nmake the video/tasks to ensure the task was understandable and able to be completed. We conduct\nhuman performance tests similarly, having an author who didn't create the video or tasks attempt the\ntask and have it evaluated by a third author. Further details on task creation and human evaluation\ncan be found in A.2 and A.3."}, {"title": "FACTUAL RETENTION TASKS", "content": "For factual retention, we further divide this category into four finer sub-categories: (1) Visual\nPerception (OCR, Spatial Reasoning), (2) Audio Perception, (3) Full Video Understanding (i.e., tasks\nthat require information across several parts of the video), and (4) Temporal Reasoning (i.e., tasks\nthat require understanding the video with respect to time). One key difference between the factual\nand skill retention tasks is the intermediate intent and evaluation we create for each factual retention\ntask. The intermediate intent is the video-based question that must be answered correctly to have the\ninformation necessary to complete the task. This is intended to decouple the evaluation of agentic\nabilities in long context video models for video information retrieval tasks; by checking if the model\ncan extract information necessary to complete the task from the video and evaluating that separately\nfrom the agent's success, this process can pinpoint the failure modes of the model, whether they come\nfrom generating agent actions or video processing.\nAdditionally, we provide video difficulty ratings for all intermediate intents, distributed between easy,\nmedium, and hard. The video difficulty ratings signify the complexity of returning the correct answer\nfor a given task's intermediate intent. Easy tasks require returning one piece of information and can\nbe solved with less than 3 frames, medium tasks require returning 2 to 3 things and can be solved\nwith less than half the video, and hard tasks require returning more than 3 things and require watching\nmore than half the video. We provide a breakdown of each task type in our benchmark in Table 4.\nVideoWA contains 111 unique intent templates across the 400 intents in the factual retention task set."}, {"title": "TASK EVALUATION", "content": "Each task has an eval and intermediate eval function. We import the automatic functionality from\nVisualWebArena (Koh et al., 2024a) and WebArena (Zhou et al., 2024) to evaluate our agent tasks.\nFor the intermediate intent evaluation, we use the string-based existing functionality evaluators to\nassess the agent's response to the video-based question.\nAll of our tasks have a final evaluation function (i.e., evaluator) that determines an agent's reward on\neach task. The reward is typically binary, returning zero or unity depending on whether the agent\nperforms the task unsuccessfully or successfully. Reward values are determined by evaluating the\nstate of the environment at the end of the agent's trajectory to determine if said state matches the\ncorrect state corresponding to the correct task execution."}, {"title": "BASELINE AGENTS", "content": "We evaluate our benchmark using three different types of baseline agents with multimodal models as\na backbone. At each step, the agent is given the task objective, 2 in-context examples, current state s,\nand the input video to the objective as context to generate one action."}, {"title": "VIDEO IN-CONTEXT AGENT", "content": "We define a video input agent that takes the video in at every time step to generate actions. We provide\nthe whole video in-context to the model with the Gemini model. The Gemini model automatically\nprocesses the audio, eliminating the need to process audio secretly. The specific prompts we use are\nin Appendix D. We use Set-of-Marks on the website HTML page, the Set-of-Marks element tree\nstring, and the prompt along with the video as input to the model."}, {"title": "VIDEO FRAMES IN-CONTEXT AGENT", "content": "We define a video input agent that takes a set amount of video frames at every time step along with\nthe video audio to generate actions. To obtain the information from the video, we follow the practice\nfrom (Wu et al., 2024). We sample 1 frame per second (max 60 frames) for the video and include\nthem into the context for the LLM. In addition, we use OpenAI's Whisper (Radford et al., 2022) to\ntranscribe the audio and append it to the context. In this way, we can still pass the information from\nthe video to our LLM. This may not be a perfect method as the video information remains missing\nduring framing sampling. However, since most LLMs in the market only support image and text\ninput, it is essential to experiment with this setting. We use GPT-40, and the prompt can be seen in"}, {"title": "VIDEO SUMMARY IN-CONTEXT AGENT", "content": "We define a summary in-context agent that takes a video summary related to the objective at hand\nin-context at every time step to generate actions. To obtain this summary, we call GPT-40 and feed\nthe video using 60 frames and the Whisper transcription into the model and prompt it to summarize\nthe video concerning the task at hand. Again, our prompt can be seen in Appendix D. Similarly,\nthe summary agent also uses Set-of-Marks for the observation space and generates actions in the\naforementioned action space."}, {"title": "RESULTS", "content": "MODEL PERFORMANCE\nFrom Table 5, Table 6 and Table 7, we see varying degrees of agentic performance across the video-\ncapable Gemini and GPT family of models; however, we note several consistent trends across LLM\nagent results. We comprehensively outline the failure modes in Appendix B. There is no winning\nbaseline agent or model family across skill and factual retention tasks. For factual retention tasks,\nthe summary agent performs the best in task success at 13.3% while the 30 and 100 Frame GPT-40\nAgent perform the best in intermediate intent success at 45.8%. For skill retention tasks, we see that\nlong-context models with tutorials actually perform significantly worse than models without tutorials,\nsuggesting that the tutorials introduce negative noise that hurt action selection. Although intermediate\nscores tend to be higher than final scores, this does not necessarily translate to task success. This is a\nconstant failure mode of the long-context agents, as they can perform the necessary VQA to extract\nthe necessary information for the task at hand but fall short due to hallucinations, action grounding,\nand high-level planning errors. For example, in Figure 5, the LLM agent successfully identifies the\nitem to buy from the video. Still, it does not successfully plan and complete the intent. We tested on\na smaller subset of tasks with the GPT4-o agent and tested on the full set of tasks with the Gemini\nagent due to compute constraints."}, {"title": "HUMAN PERFORMANCE", "content": "To understand the level of human performance expected on the tasks within VideoWA, three authors\nattempted the tasks and provide intermediate answers for a random sample of each unique task"}, {"title": "DISCUSSION", "content": "We present VideoWebArena, a rigorous video-based agent benchmark that tests the agentic ability of\nlong-context multimodal models. We define a task taxonomy of video-based agent tasks, utilizing a\nwide coverage of task types including skill retention and factual retention to create a comprehensive\ntest bed for the setting of video agents. We provide 2021 tasks that are all video-based, along with\n74 manually created videos. According to our experiments, the baseline agent does not perform\nwell on most of tasks compared with human performance. There is still a long way in developing\nintelligent agents. For future work, it is important to analyze the failure cases explore better video\nagent architectures with different LLMs on this benchmark. We hope our environment and benchmark\nfacilitate improvement and additional work on improving long-context multimodal agents."}, {"title": "ETHICS STATEMENT", "content": "Our benchmark is intended for safe and responsible innovations of video-based LLM agents. With the\nrising popularity of LLM agents and the excitement around their deployment, measures to ensure their\nsafe practical deployment and use cases must be present. The authors are committed to the ethical\ndevelopment of LLM agents. For our paper, we did not use human subjects, find any potentially\nharmful insights, or any ethical concerns. Our benchmark is a self-contained environment to test\nagents on synthetic tasks."}, {"title": "B FAILURE MODES", "content": "COMMON LLM AGENT FAILURE MODES\nMany of the basic failures captured in the baseline agents were common repeats of agent errors\nseen in other agent academic benchmarks. These include hallucinations, where the agent produces a\nnonsensical action unrelated to its context or task at hand. We attribute this to the lack of instruction\ntuning and model alignment on agentic tasks. Another common failure mode displayed in the baseline\nagents was failure to do visual grounding. The agents will recognize the correct plan of action, but\nchoose the wrong element with respect to the Set-of-Marks image input and take the wrong action.\nAction grounding and planning was also a common failure mode of the baseline agents. An agent can\nsimply generate the wrong plan or action that will yield unsuccessful trajectories, and not change this\nplan even with negative feedback from the environment. This suggests using inference time search or\nmemory based methods can be effective to combat these failures. Incorporating self-reflection during\ninference can also help the agents recover from failures in action grounding and planning. The lack\nof self-reflection is especially seen when the agent generates the same action repetitively, leading the\ntask to terminate. Even though an action is shown to be unsuccessful towards completing a task, an\nagent will continue to repeatedly attempt the same action to try and complete the task."}, {"title": "LONG-CONTEXT SPECIFIC FAILURE MODES", "content": "Within our skill and factual retention tasks, there were many failure modes that presented issues\nrelevant to long-context modeling. One constant issue we noticed was failure to adhere to the\nprompt instructions for generating actions. With the extra noise provided with the video information\nin-context, the agent did not always adhere to the action generation guidelines provided in the prompt.\nFor example, under the Set-of-Marks elements, a click action must be generated using click [elem]\nwhere elem is the numeric ID of the SoM element. However, the agent would return click [elem]\nwhere elem was the name of the element. This formatting issue persisted for other actions with the\nlonger prompt.\nA common issue for skill retention tasks was the agent began generating multi-action responses when\nthe prompt explicitly says to generate one action. Given the tutorial or summary to complete a similar\ntask, the agent would get distracted by the comprehensive plan and generate multiple actions from the\nvideo information, straying away from the prompt guidelines. This led to failure to complete tasks.\nA common issue for factual retention tasks was video grounding. Specifically, we could pinpoint\nthat the video-frame and summary agents would simply miss visual information due to the nature of\ntheir video processing. Additionally, the video agent also showcased many of these video grounding\nerrors. For example, a common task was to Take me to the page in the video when event\nhappened. However, if the frames or summary did not include this page, there was no way for the\nagent to get to this page or know about its existence. This issue was exacerbated in tasks that required\nfull video understanding or temporal reasoning across the video. This is a flaw in the baseline agent\nsetup we proposed. Many of the audio tasks were completed at a much higher rate than the video\nperception tasks, citing that video grounding is a larger issue than audio grounding when processing\nthese modalities within videos. We encourage better video understanding agent systems with our\nbenchmark."}, {"title": "C RESULTS", "content": "ADDITIONAL RESULTS\nWe provide another result breakdown plot at Table 9. This shows the average steps per task type."}, {"title": "D AGENT PROMPTS", "content": "VIDEO AGENT TASK PROMPT\nYou are an autonomous intelligent agent tasked with navigating a web browser. You will be given web-based tasks that can be done\nbased on information in a video. These tasks will be accomplished through the use of specific actions you can issue.\nHere's the information you'll have:\n1. The user's objective: This is the task you're trying to complete."}, {"title": "VIDEO FRAME AGENT TASK PROMPT", "content": "You are an autonomous intelligent agent tasked with navigating a web browser. You will be given web-based tasks that can be done\nbased on information in a video. These tasks will be accomplished through the use of specific actions you can issue.\nHere's the information you'll have:\n1. The user's objective: This is the task you're trying to complete.\nYou are an autonomous intelligent agent tasked with extracting information from videos. You will be given a list of frames sampled from a video\nand its audio transcription. You need to answer the question based on the video provided.\nExample 1:"}, {"title": "VIDEO SUMMARY AGENT TASK PROMPT", "content": "You are an autonomous intelligent agent tasked with navigating a web browser. You will be given web-based tasks that can be done\nbased on information in a video. These tasks will be accomplished through the use of specific actions you can issue.\nHere's the information you'll have:\n1. The user's objective: This is the task you're trying to complete.\n2. A summary from a tutorial for a similar task: This provides useful information for solving this task.\n3. The current web page's accessibility tree: This is a simplified representation of the webpage, providing key information.\n4. The current web page's URL: This is the page you're currently navigating.\n5. The open tabs: These are the tabs you have open.\n6. The previous action: This is the action you just performed. It may be helpful to track your progress."}, {"title": "VIDEO AGENT INTERMEDIATE TASK PROMPT", "content": "You are an autonomous intelligent agent that extracts information from videos. You will be given this video and a question. You need to\nanswer the question based on the video provided.\nExample 1:"}, {"title": "VIDEO FRAME AGENT INTERMEDIATE TASK PROMPT", "content": "You are an autonomous intelligent agent that extracts information from videos.\nExample 1:\nVIDEO FRAMES: 5 Frames from Shopping Tutorial"}, {"title": "VIDEO SUMMARY AGENT INTERMEDIATE TASK PROMPT", "content": "You are an autonomous intelligent agent that extracts information from summaries. You will be given a summary of a video and a\nquestion about the video. You need to answer the question based on the summary provided.\nExample 1:"}, {"title": "VIDEO SUMMARIZATION PROMPT", "content": "You are an autonomous intelligent agent tasked with learning from a video to accomplish a task. You will be given a video. You will be\ngiven a task to complete. You will need to extract useful information to accomplish the task.\nExample 1:\nVIDEO: Shopping Tutorial MOV File"}, {"title": "VIDEO FRAME SUMMARIZATION PROMPT", "content": "You are an autonomous intelligent agent tasked with learn froming a video to accomplish a task. You will be given a list of frames\nsampled from a video and its audio transcription. You will be given a task to complete. You will need to extract useful information to\naccomplish the task.\nExample 1:\nVIDEO FRAMES: 5 PNG Frames from Shopping Tutorial"}]}