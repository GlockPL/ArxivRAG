{"title": "Automating Knowledge Discovery from Scientific Literature via LLMs: A Dual-Agent Approach with Progressive Ontology Prompting", "authors": ["Yuting Hu", "Dancheng Liu", "Qingyun Wang", "Charles Yu", "Heng Ji", "Jinjun Xiong"], "abstract": "To address the challenge of automating knowledge discovery from a vast volume of literature, in this paper, we introduce a novel framework based on large language models (LLMs) that combines a progressive ontology prompting (POP) algorithm with a dual-agent system, named LLM-Duo, designed to enhance the automation of knowledge extraction from scientific articles. The POP algorithm utilizes a priori-tized breadth-first search (BFS) across a predefined ontology to generate structured prompt templates and action orders, thereby guiding LLMs to discover knowledge in an automatic manner. Additionally, our LLM-Duo employs two specialized LLM agents: an explorer and an evaluator. These two agents work collaboratively and adversarially to enhance the reliability of the discovery and annotation processes. Experiments demonstrate that our method outperforms advanced baselines, enabling more accurate and complete annotations. To validate the effectiveness of our method in real-world scenarios, we employ our method in a case study of speech-language intervention discovery. Our method identifies 2,421 interventions from 64,177 research articles in the speech-language therapy domain. We curate these findings into a publicly accessible intervention knowledge base that holds significant potential to benefit the speech-language therapy community.", "sections": [{"title": "Introduction", "content": "With millions of research articles published annually, the overwhelming amount of existing scientific knowledge poses extreme challenges and opportunities for researchers to access knowledge through advanced analytical tools and interdisciplinary approaches. Knowledge discovery from scientific literature enables researchers to keep abreast of the latest developments in their domain and facilitate valuable insights that can significantly enhance their work (Usai et al. 2018; Wang et al. 2023). However, in such a vast ocean of data, only a very limited fraction of knowledge is collected and curated due to the low efficiency of the human review process. For example, in healthcare, evidence-based intervention refers to practices and treatments that are grounded in systematic research and have been proven effective through controlled studies (Rutten et al. 2021; Melnyk and Fineout-Overholt 2022). It emphasizes the use of evidence from well-designed and well-conducted research as the foundation for healthcare decision-making (Sackett 1997). One of the biggest challenges for healthcare providers is efficiently identifying relevant intervention evidence from research articles, thus underscoring the necessity of automating knowledge discovery.\nAdvancements in large language models (LLMs) present a significant opportunity for automating knowledge discovery in scientific literature. LLMs have been employed to categorize research papers, extract key findings, summarize complex studies, and streamline the review process effectively (Achiam et al. 2023; Guo et al. 2023). Some recent research (Li et al. 2023; Kim et al. 2024) utilize LLMs in conjunction with human annotators to reduce the human annotation burden. However, facing such a vast volume of domain knowledge, automating knowledge discovery is necessary to process and analyze information efficiently. For automating knowledge discovery from literature with LLMs, one big challenge is the limited context window length of LLMs (Wadhwa, Amir, and Wallace 2023). This constraint restricts the amount of input text that the models can process at one time, potentially leading to incomplete analysis and missed connections between data points spread across larger documents. To address this issue, the Retrievel-Augmented Generation (RAG) technique (Lewis et al. 2020; Gao et al. 2023) can be instrumental by combining a powerful retrieval component with a generative model, allowing the system to access a broader corpus of information beyond the immediate context window of a single model.\nIn this paper, we address the challenge of automating knowledge discovery through LLMs and formulate it as a problem of prompt design and scheduling based on a predefined ontology graph structure. To tackle this problem, we introduce a progressive ontology prompting (POP) algorithm that employs an outdegree-prioritized breadth-first search (BFS) across the ontology graph to create a series of prompt templates and action orders to guide LLMs. Additionally, to enhance the annotation quality, we further propose LLM-Duo, an interactive annotation framework for domain-specific knowledge discovery by leveraging the power of LLMs while addressing the limitations of LLMs. Specifically, this framework integrates two LLM agents working both collaboratively and adversarially to improve the discovery quality: 1) explorer, a chatbot based on"}, {"title": "Preliminaries", "content": "Knowledge graph (KG) is a semantic network structured as an ontology, comprising concepts and their relationships with clear, interpretable structures at scale (Ehrlinger and W\u00f6\u00df 2016; Peng et al. 2023). For knowledge discovery from literature, LLMs can enhance this process by leveraging their capabilities to understand long-range text. This allows for transforming unstructured data into structured formats, and finally, populating the ontology to create KG.\nFocusing on knowledge discovery from literature, we aim to gather domain-specific knowledge for researchers and practitioners across various fields. In our approach, the ontology is predefined by domain experts, which can be represented by a directed acyclic graph (DAG) $G = (E,R, F)$. Here $E$, $R$, and $F$ are sets of concepts, relationships, and semantic triples respectively. $F$ is a collection of triples $(h, r, t)$ with a head concept $h \\in E$, a tail concept $t \\in E$, and a relation $r \\in R$ (Gruninger 1995). Named entity recognition (NER) and relation extraction (RE) are foundational elements for constructing knowledge graphs. To effectively instruct LLMs for knowledge discovery anchored to $G$, the design of prompts and the sequencing of queries for both NER and RE tasks are essential for automating knowledge graph completion. We frame the problem of automated knowledge discovery via LLMs as one of prompt design and scheduling, described by the following equation:\n$f(G(E, R, F)) = {\\{(Prompt_i, Order_i)|i \\in [1,N]\\}}$\nwhere $f$ is a function that translates KG ontology into a set of prompt and action orders for the LLMs. A simple case of $f$ is directly injecting the whole ontology into a single prompt to instruct LLMs to generate KG annotation at once such as the annotation methods used in (Mihindukulasooriya et al. 2023; Kommineni, K\u00f6nig-Ries, and Samuel 2024; Mihindukulasooriya et al. 2023)."}, {"title": "Methodology", "content": "In this section, we first introduce the POP algorithm to convert a predefined knowledge ontology into a set of prompt templates and action orders, then propose an interactive annotation framework based on two LLM agents to enable more convincing and accurate annotation generations."}, {"title": "Ontology Prompting Algorithm", "content": "In our work, we develop a progressive ontology prompting (POP) algorithm that employs a prioritized BFS on the ontology graph $G(E, R, F)$ to generate a set of prompt templates and action orders for LLMs. As illustrated in Figure 1, in our method, the prompt design and scheduling follow a progressive manner. The annotation process begins by annotating the concept at a source node (i.e., nodes that only have outgoing edges) and then proceeds to its neighbors following the traversal order of a prioritized BFS. To allow BFS to quickly access a large portion of the graph, we modify BFS by sorting the neighbor nodes according to the out-to-in ratio $R(v)$, which is defined by:\n$R(v) = \\frac{|\\{(h,r,t) \\in F\\} |h = v|}{| \\{(h, r,t) \\in F\\} |t = v|}$\nTherefore, the algorithm will choose the neighbor node with higher R to visit at the next step. For example, in Figure 1, visiting the 'Patient' node before the \u2018Disorder' node can provide more information for annotating the \u2018Disorder' concept. For any concept node v, we use the visited nodes within its k-hop neighborhood as its context. The Prompt for identifying concept v is crafted based on its own concept type and knowledge discoveries within its context. The action order of this prompt Order is determined by the order in which node v is visited during the prioritized BFS traversal.\nThe pseudocode of the POP algorithm is shown in Algorithm 1. The algorithm first follows prioritized BFS traversal to capture the context and visit order of a specific concept node, then composes annotation prompts based on the local ontology structure and discoveries within its context as follows:\n$Prompt(v) \\leftarrow {Prefix (N_{k-1}(u)) Question ((v, e, u) | (v, e, u) \\in F) | u \\in N_1(v)}$\nwhere $ is the concatenation. We introduce two parameters in POP: context size k and prompt template T. As the"}, {"title": "LLM-Duo Annotation Framework", "content": "To guarantee the integrity and reliability of knowledge discoveries via LLMs, we propose LLM-Duo. The architecture of LLM-Duo is illustrated in Figure 2. It comprises two primary LLM components: the explorer and the evaluator. Specifically, the explorer is a chatbot developed with RAG and works for the intervention discovery and annotation by zero-shot question answering (QA). RAG is utilized to ground its generations within the given literature context. Even though RAG can reduce the hallucination of LLMs by providing references to guarantee the integrity and reliability of explorers' generations, the explorer might still make mistakes. To ensure maximized accuracy and reliability of annotations, we introduce another LLM, called the evaluator, tasked with inspecting the explorer's responses to enhance the annotation quality.\nIn our annotation framework, the interaction between explorer and evaluator is collaborative and adversarial. Given a research paper, LLM-Duo will be tasked with prompts following a specific action order obtained from the POP algorithm. Depending on the exact requirements of annotations for different knowledge domains, the inspection mechanism of the evaluator can be customized. In our test case of speech-language intervention discovery, we emphasize the factual correctness and completeness of the annotations for different concepts, which are also typical requirements for annotation quality in most scenarios. LLMs cannot directly assess the correctness of the answer since they do not have prior knowledge of the correct response. Inspired by the concept of LLMs self-correcting through reasoning, we convert the correctness inspection to the answer's rationality assessment.\nIn an interaction cycle of LLM-Duo for annotating concepts emphasizing rationality, the answer along with its explanatory reason is presented to the evaluator. The evaluator scrutinizes the answer's rationality and gives feedback to the explorer. Depending on the feedback, the explorer may either adjust its answer accordingly or, if disagreeing, present stronger evidence to support its original answer to challenge the evaluator's assessments. For annotating concepts emphasizing completeness, in each round, the evaluator will extract the aspects covered in the explorer's answer and integrate them with aspects of the last round to form a new aspect collection and greedily prompt the explorer to delve deeper and explore more beyond the new aspect collection. The iterative refinement process will continue until the annotation achieves consistency. The key idea behind our LLM-Duo framework is using the feedback loop to assist the explorer refine its annotations. As an example shown in Figure 3, by facilitating interactive loops between two LLM agents, LLM-Duo enables more convincing and accurate annotations."}, {"title": "Experiments", "content": "In the LLM-Duo framework, the explorer is a chatbot built on LLM with RAG using Llamaindex. We use OpenAI 'text-embedding-3-large' as the embedding model and set the chunk size to 256 tokens with an overlapping size of 128. Particularly, we use 'FastCoref' (Otmazgin, Cattan, and Goldberg 2022) to process text chunks for coreference resolution before text embedding. Besides, we pass the document ID as metadata and apply a metadata filter on the chat engine to ensure the explorer only answers within the text of the specific document to be annotated. We set the retrieval to be on the top 8 text chunks based on similarity scores reranked with SentenceTransformerRerank employing the 'cross-encoder/ms-marco-MiniLM-L-2-v2' model in Llamaindex. The evaluator is an external LLM who does not share any document context with the explorer."}, {"title": "Case Description: Speech-language Intervention Discovery", "content": "Speech-language therapy provides interventions for individuals with speech-language deficits, enhancing their quality of life across various life stages. When choosing an intervention, evidence-based practice (EBP) is attractive as it integrates research evidence from literature into the decision-making process to ensure high-quality patient care (Law et al. 1996). Intervention research, especially studies that offer clear intervention frameworks and comprehensive case studies, are valuable references to guide EBP designs. Intervention discovery aims to extensively gather speech-language interventions from the literature corpus as references to facilitate EBP design. It involves identifying relevant studies and extracting essential features of interventions including target disorder, procedure, efficacy, case study, therapy activity, etc., which is extremely labor-intensive for"}, {"title": "Evaluation", "content": "In our experiment of comparing with baseline approaches, we report six types of metrics: 1) Consistency Rounds (CR): the number of refine loops the method makes before achieving the annotation consistency; 2) Verbosity Index (VI): the number of aspects per 1k tokens of the annotations, which is an important metric for annotation emphasizing content completeness; 3) Enumeration Quantity (EQ): the number of items listed in the annotations (i.e., therapy activities, therapy goals in our test case.); 4) Faithfulness (Faith): the extent of the annotation faithful to the provided text, which is measured by FaithfulnessEvaluator of Llamaindex. 5) Accuracy (ACC): the percent of correct annotations in all LLM-provided annotations. 6) Cover: the percent of correct LLM-provided annotations to the total mentioned concept entities in the provided text."}, {"title": "Results", "content": "In this section, we first provide a detailed evaluation of our progressive ontology prompting algorithm and the LLM-Duo annotation framework. Then, we showcase the results of speech-language intervention discoveries using our automated knowledge discovery framework, which further demonstrates the effectiveness of our framework in addressing practical scientific knowledge discovery tasks."}, {"title": "Context Size Analysis", "content": "In our progressive ontology prompting algorithm, when composing the annotation prompt for a concept node, the knowledge discoveries and the local ontology structure within the context (visited nodes within k-hop neighborhood) of a specific concept node serve as a prefix to provide conditions for annotation questions. The k value determines the diversity and volume of information provided. To evaluate how the context size of the POP algorithm affects annotation quality, we conducted experiments by applying different k values to generate prompts for LLM-Duo-RAG annotation. As shown in Figure 5a, we selected an ontology substructure related to the 'participant' concept for the experiment, which was based on a random selection of 8 speech-language therapy papers.\nThe resulting annotation accuracy of the participant concept is shown in Figure 5b. The results indicate that as the context hop k increases, annotation accuracy improves significantly, suggesting that a larger context provides more informative prompts, thereby enhancing annotation quality. Moreover, GPT-4-turbo consistently outperforms GPT-3.5-turbo across all k values, demonstrating that more advanced language models can further improve annotation accuracy."}, {"title": "LLM-Duo with Baseline Annotation Methods", "content": "To evaluate the performance of our LLM-Duo annotation framework, we compare it with baseline methods in the context of speech-language intervention discovery. We randomly pick 8 papers from our speech-language literature base for annotation using various baselines. The comparison focuses on three key dimensions: 1) Intervention Recognition (IR), identifying intervention entities within the paper; 2) Intervention Aspect Summary (IAS), annotating an aspect (e.g., procedure, therapy activity, therapy goals) of an intervention approach, which requires capture and summarize all related information fragments from the paper; and 3) Intervention Knowledge Completion (IKC), linking interventions to theme classes (e.g., speech awareness, speech articulation, comprehension, foundation skills, etc.) and setting concept nodes (e.g., home, healthcare facilities, early childhood centers, schools, teletherapy, etc.). For IR and IKC tasks, we ask human annotators to complete annotation as golden results for comparison. For the IAS task, due to the individual bias of human interpretation, we only ask human annotators to tag the text fragments in the paper pertaining to a specific intervention aspect.\nThe experimental results are reported in Table 1. It should be noted that we implemented 'ShortContext' using Llama3-instruct-70b (FP16) and Mistral-instruct-8x22b"}, {"title": "Ablation Study", "content": "In our framework, the chatbot explorer uses the RAG technique. We enhance generation quality with 'FastCoref' for coreference resolution and rerank retrieved chunks by similarity score using the 'cross-encoder/ms-marco-MiniLM-L-2-v2 model'. This section presents ablation studies for both components. Since recognizing the intervention entity is the critical first step following the POP algorithm, we report the accuracy of intervention recognition in this study. As shown in Figure 6a, the results demonstrate that removing these components significantly decreases annotation accuracy, showing the necessity of each module."}, {"title": "Speech-Language Intervention Discovery", "content": "Our knowledge discovery framework has identified 2,421 interventions supported by case studies from 64,177 research papers in the field of speech-language therapy. The statistics of discovered interventions are presented in Figure 6b and Figure 6c. A complete intervention annotation example is available in Appendix 3. 19 clinicians and students reviewed our annotations through online Google forms. In checking the intervention annotation and reading the corresponding literature, they reported that 89% of our annotations are valid and complete. Based on the intervention ontology in Figure 4 and annotations, we have constructed an intervention knowledge graph that will be publicly accessible. This knowledge graph is expected to be a valuable resource for speech-language domain experts, supporting evidence-based clinical decision-making, question-answering and recommendation, ultimately enhancing healthcare outcomes."}, {"title": "Conclusion", "content": "In this paper, we developed a novel automatic knowledge discovery framework based on LLMs, which is characterized by a progressive ontology prompting algorithm and a dual-agent annotation system. The proposed method achieves superior performance compared with advanced baselines, enabling more accurate knowledge discovery. Applied to speech-language intervention discovery, this framework also curates a valuable, accessible intervention knowledge base for the speech-language therapy community."}, {"title": "Appendix", "content": "1.1 Intervention Ontology The intervention ontology is shown in Figure 4 of the main paper. In this section, we provide a detailed explanation of the concepts introduced in the ontology as follows:\n\u2022 Intervention represents a targeted treatment practice designed to enhance an individual's communication skills.\n\u2022 Disorder represent the type of disorder that causes difficulties in an individual's voice, speech, language, or swallowing functions.\n\u2022 Setting represents a specific environment where interventions are implemented. We identify six key settings: home, healthcare facilities (such as hospitals or rehabilitation centers), early childhood centers (like nurseries or daycare), schools, clinics and private practices, and teletherapy.\n\u2022 Theme represents the theme of the intervention. As shown in Table 2, we categorize interventions into 10 themes based on their characteristics and therapy goals.\n\u2022 Therapy Activity represents a task designed to address a particular speech or language challenge in an individual, such as using a minimal pairs activity to enhance phonological awareness.\n\u2022 Therapy Goal represents a specific area that the intervention is designed to enhance.\n\u2022 Procedure represents a comprehensive description of how the intervention is carried out.\n\u2022 Efficacy represents the conclusion about the effectiveness of intervention.\n\u2022 Frequency/Dosage/Duration represents the frequency/dosage/ duration of the intervention practiced in the case study that demonstrates its efficacy.\n\u2022 Case Study represents a detailed examination of the intervention on a particular individual or group with communication disorders. The purpose of a case study is to provide a deep understanding of the patient's unique needs and assess the intervention effectiveness.\n\u2022 Participant represents the individuals or populations that are involved in the case study of intervention.\n\u2022 Age represent the age of experiment participant or claimed target population of the intervention. The age is quantified with a granularity of half a year. We additionally convert age to age groups including \u201cnewborn, infants, toddlers, children, preschoolers, school-age children, older children, youth, teens, adolescents, adult, young adult, middle aged, aged, senior\u201d. Each specific age may be associated with multiple age groups. For instance, an individual aged 13 years could be categorized to the 'teens,' 'adolescents,' and 'children' age groups.\n\u2022 Language represent the speaking language of experiment participant in the case study of the intervention."}, {"title": "1.2 Speech-language Therapy Literature Collection", "content": "We cultivate a literature corpus within the domain of speech-language therapy as shown in Table 3 to facilitate the discovery of intervention methods. To conduct our literature search, we use a collection of carefully selected keywords drawn from a glossary of commonly used terms in speech-language therapy. These keywords include: \u201cspeech language therapy, speech language disorder, speech sound disorder, articulation disorder, speech intervention, language intervention, auditory discrimination, auditory processing disorder, phonological awareness, phonological processes, auditory perception, babbling, motor speech disorder, morpheme, phonology, prosody, stuttering, language impairment, speech language pathologist, speech and language therapist, babbling, expressive language delay, cleft speech disorder, autism spectrum disorder, developmental phonological disorder, developmental stuttering, phonological impairment, developmental dysarthria, down syndrome, swallowing disorder, communication impairment, articulation impairment, dyslexia, apraxia, dysarthria, dysphagia, communication disorder, expressive language disorder, dyspraxia, aphasia, augmentative and alternative communication, central auditory processing disorder, cleft lip and palate, down syndrome, fluency disorders, hearing loss, orofacial myofunctional disorders, spoken language disorders, written language disorders, acquired brain injury, apraxia of speech, auditory comprehension, literacy impairments, voice difficulties, language-based learning disabilities.\u201d"}, {"title": "2 Prompting Methods", "content": "2.1 POP Prompting\nIn the POP algorithm, the annotation prompt for a specific concept node is generated from a template T, which is constructed using the node's context (the k-hop visited neighborhood) and the knowledge discoveries within that context. We carefully design the prompt to task LLM generate T as follows:\nThe following triplets outline an annotation ontology:{(Intervention, StudiedIn,\nCase Study), (Intervention, Include,\nParticipant), (Case Study, UsedWith,\nFrequency)}. All concept nodes have been\nannotated except from {#Frequency}. Your\ntask is to create all possible annotation\nprompt templates for the Frequency},\nleveraging the ontology structure.\nExample:\nOntology: ((Intervention, StudiedIn, Case\nStudy), (Intervention, TargetAt, Disorder),\n(Case Study, Include, Participant),"}]}