{"title": "Compositional API Recommendation for Library-Oriented Code Generation", "authors": ["Zexiong Ma", "Shengnan An", "Bing Xie", "Zeqi Lin"], "abstract": "Large language models (LLMs) have achieved exceptional perfor- mance in code generation. However, the performance remains un- satisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs. Previous work utilizes API recommendation technology to help LLMs use libraries: it retrieves APIs related to the user requirements, then leverages them as context to prompt LLMs. However, developmental require- ments can be coarse-grained, requiring a combination of multiple fine-grained APIs. This granularity inconsistency makes API rec- ommendation a challenging task.\nTo address this, we propose CAPIR (Compositional API Recom- mendation), which adopts a \"divide-and-conquer\" strategy to rec- ommend APIs for coarse-grained requirements. Specifically, CAPIR employs an LLM-based Decomposer to break down a coarse-grained task description into several detailed subtasks. Then, CAPIR applies an embedding-based Retriever to identify relevant APIs correspond- ing to each subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter out redundant APIs and provides the final recom- mendation.\nTo facilitate the evaluation of API recommendation methods on coarse-grained requirements, we present two challenging bench- marks, RAPID (Recommend APIs based on Documentation) and LOCG (Library-Oriented Code Generation). Experimental results on these benchmarks, demonstrate the effectiveness of CAPIR in comparison to existing baselines. Specifically, on RAPID's Torchdata- AR dataset, compared to the state-of-the-art API recommendation approach, CAPIR improves recall@5 from 18.7% to 43.2% and preci- sion@5 from 15.5% to 37.1%. On LOCG's Torchdata-Code dataset, compared to code generation without API recommendation, CAPIR improves pass@100 from 16.0% to 28.0%.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) [16] have demonstrated impressive performance in code generation tasks [11, 24, 49]. LLM-based pro- gramming assistance tools, like Copilot [3, 12] and ChatGPT [2], have been widely applied in the development process. These tools can leverage the knowledge learning from the training data and generate executable code for developers.\nLibrary-oriented code generation [27, 48], which involves gener- ating code based on APIs within a specific library, plays a significant role in enhancing practical development efficiency, but existing LLM-based programming tools have unsatisfactory performance in this scenario. Specifically, the LLM-based programming tools are unable to generate code that import libraries not present or rarely present in the training data of LLM. For example, the training data of ChatGPT is collected from internet before September 2021. This makes ChatGPT unable to generate code that import libraries released after that, while new libraries are continually being re- leased in the open-source community. Moreover, some libraries are domain-specific or used in enterprise development, which also lim- its their representation in the training data for LLMs. Some recent surveys [13, 27] show that when using programming assistance tools, developers often expect the tool to generate code that invokes specific third-party libraries. When developers want to use such low-resource libraries, they can only learn about the usage of the APIs through official documentation or community forums [6].\nPrevious work [46, 51] utilizes API recommendation to help LLMs invoke libraries, but existing approaches face two challenges for practical applications: coarse-grained requirements and lack relevant resources. API recommendation has been an extensive re- search subject for many years [10, 20, 22, 33, 44]. Its primary goal is to recommend APIs to users based on their developmental require- ments. From a methodological perspective, these approaches can be broadly categorized into two types: learning-based methods and retrieval-based methods. However, when it comes to library-based code generation with LLMs, existing API recommendation methods encounter two challenges: (1) The inconsistency in the granularity of requirements and APIs. Some recent work [46, 47, 51] has at- tempted to retrieve APIs from documents based on users' require- ments and incorporate them into LLM prompts to assist in library- oriented code generation. Some recent surveys [6, 38] show that when using programming assistance tools, users' requirements can be coarse-grained, needing a combination of multiple APIs. Directly retrieving from documentation may lead to unsatisfactory results. (2) Insufficient training data. Learning-based methods require ex- pensive retraining for specific libraries. Some libraries lack relevant resources, making it challenging to obtain sufficient data for train- ing. For example, Torchdata was released on Mar 11, 2022, which makes ChatGPT unable to generate related code. It is prohibitively expensive to retrain models specifically for Torchdata. Moreover, there is a scarcity of relevant trainable resources on the internet. As of April 10, 2023, there are only around 1k unduplicated related code files on GitHub. Additionally, there are only 3 posts tagged with Torchdata on StackOverflow, which is insufficient for fine- tuning language models. To help LLMs with library-oriented code generation, we need an API recommendation approach that does not require training data and can address coarse-grained tasks.\nWe present CAPIR (Compositional API Recommendation), a novel compositional API recommendation approach based on API documents. CAPIR utilizes the powerful reasoning capabilities of LLMs to decompose coarse-grained user requirements into fine- grained subtask sequence. These subtasks are then used to retrieve corresponding APIs from API documentation. While LLMs have demonstrated strong reasoning abilities [42], the key challenge lies in guiding the language model to decompose the requirement de- scriptions at the granularity of APIs in the target library. We use few-shot examples to demonstrate the decomposition granularity. CAPIR extracts code examples from development documents, and use LLMs (Summarizer) to generate summarizations [37] of these examples. The pairs of <summarization, API_functions> serve as examples of <task, subtasks>. These <task, subtasks> examples are included in the prompt of LLMd (Decomposer) to guide its de- composition of requirements into API-level subtasks. Then CAPIR uses an off-the-shelf embedding model as Retriever to retrieve APIs, and leverages LLM, (Reranker) to rerank the API retrieval results. Specifically, we utilize ada-embedding-002 as Retriever, while employing gpt-3.5-turbo as the Summarizer, Decomposer, and Reranker.\nIn addition to the proposed CAPIR, this work introduces two benchmarks, RAPID and LOCG, to promote the evaluation of API recommendation and code generation for coarse-grained require- ments. Specifically, RAPID encompasses four single-library API recommendation tasks and one multi-library task, and LOCG con- tains one single-library code generation task and one multi-library task. Experimental results on these two benchmarks demonstrate"}, {"title": "2 MOTIVATING EXAMPLE", "content": "Figure 1 shows the importance of API recommendation for LLMs and the limitation of raw-requirement-based API recommenda- tion. The development task is \"Load and only parse CSV files in the folder using Torchdata\", which needs to use four APIs in Torchdata: \"FileLister\", \"Filter\", \"FileOpener\" and \"parse_csv\". As shown in Figure 1a, when the prompt of LLM does not include the corresponding APIs and their descriptions, LLM can only generate code that looks reasonable but cannot be executed correctly.\nThe example in Figure 1a illustrates the importance of including API information in the prompt, but the key challenge lies in accu- rately identifying the required APIs. The task description can be coarse-grained, making it difficult to directly retrieve APIs from doc- uments. As shown in figure 1b, directly using the raw-requirement to retrieve from API documentation only yields \"parse_csv\", which makes LLMs unable to generate the correct code.\nTo address this, we propose CAPIR, which decomposes the task into subtask sequence and uses these subtasks to retrieve APIs from documents. For the task \"Load and only parse CSV files in the folder using Torchdata\", we decompose it into four subtasks: {\"List all files in the folder\", \"Filter out non-CSV files\", \"Open the CSV files\", \"Parse the CSV files\"}. Then, we can retrieve corresponding APIs from the API documentation for each subtask: {\"FileLister\", \"Filter\", \"FileOpener\", \"parse_csv\"}. As shown in Figure 1c, after adding the APIs recommended by CAPIR into the prompt, LLMs can generate the correct code. Previous work has demonstrated the impressive reasoning capabilities of LLMs [23, 25, 39, 42], so we utilize LLMs to decompose tasks into subtasks. Given that LLMs lack API-level granularity information, a critical challenge arises in determining the appropriate granularity for this decomposition process. The in- put length limitations of LLMs prevent us from providing an entire library documentation as input. We use in-context learning [15] to address this issue. Low-resource libraries typically include API"}, {"title": "3 APPROACH", "content": "In this section, we will provide a detailed explanation of CAPIR approach. Figure 2 illustrates the pipeline of CAPIR, which consists of three main components: (1) Task Decomposition: CAPIR uti- lizes the LLM-based Decomposer to break down a coarse-grained task into subtasks through few-shot prompting (Section 3.1). (2) Subtask API Retrieval: For each subtask, CAPIR employs the embedding-based Retriver to search relevant APIs from the API documentation. (Section 3.2). (3) API Reranking: With the APIs re- trieved from all subtasks, CAPIR leverages an LLM-based Reranker to provide the final recommendation results. (Section 3.3)."}, {"title": "3.1 Task Decomposition", "content": "CAPIR employs the LLM as a Decomposer to effectively decom- pose coarse-grained development tasks into several subtasks. LLMs have already demonstrated powerful reasoning capabilities in var- ious tasks [12, 42]. The key challenge is how to demonstrate the granularity of task decomposition to the language model. As previ- ous work has shown the in-context learning [4, 5, 7, 15] ability of LLMs, we use few-shot examples to demonstrate the decomposition granularity."}, {"title": "3.1.1 Examples Construction.", "content": "CAPIR constructs <task, sub- tasks> pairs as the decomposition examples. CAPIR collects code snippets that invoke multiple APIs, generates task descriptions for the code, and subtask descriptions for each API. Due to the lack of resources for low-resource libraries on the internet, CAPIR leverages the usage examples in API documentation to construct example bank. The process contains three steps: First, CAPIR crawls API documentation of library Lo, denoted as Do. Second, it extracts code examples that call more than two APIs from Do. Third, it utilizes a Large Language Model LLMs as Summarizer to gener- ate code summarization and the API function descriptions. The summarization process is denoted by the equation:\n< S_i, S_i >= LLM_s (c_i, A_i), (1)\nwhere s_i corresponds to the summarization of code snippet c_i, A_i refers to the API sequence that used by c_i, and S_i = {st_{i1}, ..., st_{im_i} } refers to the functional descriptions of the APIs called by c_i. We con- sider summarization s_i as the task corresponding to example code c_i and treat the functionality description S_i as the API-level sub- tasks. LLMs refers to a large language model with specific prompt to generate <task, subtasks> pairs."}, {"title": "3.1.2 Examples Selection.", "content": "Due to the input length limitation of LLM, CAPIR selects ke most relevant examples to add into the prompt. For a given task t_i, CAPIR adopts an embedding-based Selector to select examples based on their functional similarity to t_i. In contrast to previous approaches [43], where the embedding model is fine-tuned using training data. As low-resource libraries do not have enough resources for training, CAPIR employs an off- the-shelf embedding model. This choice results a more efficient and effective Selector. The Selector has three steps: First, it calcu- lates the embedding feature of the task description t_i and the code summarization s_j of example c_j. Second, it calculates the cosine similarity based on the embeddings. Third, it selects the k_e most similar examples as the few-shot examples for Decomposer. The cosine similarity is computed using the following formula:\nsim(t_i, s_j) = \\frac{Emb(t_i) \\cdot Emb(s_j)}{||Emb(t_i)|| \\cdot ||Emb(s_j)||}, (2)\nwhere Emb represents the embedding model, sim(t_i, s_j) quanti- fies the cosine similarity between the embeddings of task t_i and summarization s_j."}, {"title": "3.1.3 Decomposer.", "content": "CAPIR employs an LLM-based Decomposer to decompose the task into several subtasks. For a given task t_i, CAPIR prepares the input for the Decomposer by including k_e rele- vant examples, alongside the task t_i itself. The output of this process is a subtask sequence. Prompt 2 shows the Decomposer prompt. The output of Decomposer is the decomposed subtask sequence. For- mally, the decomposition process is expressed as:\nS_i = LLM_d (\\&_i, t_i), (3)\nwhere LLM_d represents the Decomposer, S_i = {st_{i1}, ..., st_{im_i} } de- notes the subtask sequence, \\&_i = {< s_1, S_1 >, ..., < s_{k_e}, S_{k_e} >} corresponds to the few-shot examples for task t_i."}, {"title": "3.2 Subtask API Retrieval", "content": "After decomposing the task t_i into subtasks S_i, we need to retrieve the corresponding APIs for each subtask. For a given subtask st_j, CAPIR uses Retriever module to retrieve API from the API docu- mentation D_o. Due to limited software resources related to a spe- cific low-resource library, fine-tuning a library-specific embedding model is not feasible. CAPIR utilizes the same off-the-shelf embed- ding model as Selector. The Retriever also has three steps: First, it calculates the embedding feature of the subtask description st_j and the description desc_q of API_q. Second, it calculates the cosine simi- larity based on the embeddings. Third, it returns the k_p most similar APIs, represented as A_j = < API_1, desc_1 >, ..., < API_{k_p}, desc_{k_p} >, as the API candidates for subtask st_j. The cosine similarity between a subtask description and an API description is computed using the following formula:\nsim(st_j, desc_q) = \\frac{Emb(st_j) \\cdot Emb(desc_q)}{||Emb(st_j)|| \\cdot ||Emb(desc_q)||}, (4)\nwhere Emb represents the embedding model, sim(st_j, desc_q) quanti- fies the cosine similarity between the embeddings of subtask st_j and API description desc_q."}, {"title": "3.3 API Reranking", "content": "CAPIR's LLM-based API reranking module first reranks the can- didate APIs within subtasks, then reranks across subtasks to form the final recommended results. There may be differences in the way subtasks and APIs are expressed, which can impact the API retrieval results. To address this, CAPIR utilizes a large language model as Reranker ws to rerank the retrieval results within subtasks. Meanwhile, the granularity of subtasks and APIs may not always be perfectly aligned, resulting in some subtasks corresponding to multiple APIs, while others may not require API implementation. To get the final compositional APIs for the task, CAPIR utilizes Rerankeras to rerank across subtasks to get top-k APIs."}, {"title": "3.3.1 Reranking within subtasks.", "content": "CAPIR reranks the candidate APIs within subtasks. By leveraging the comprehensive software knowledge and language comprehension capabilities of LLM, the module performs re-ranking of the APIs based on their relevance to the specific subtask. Prompt 3 shows the Rerankerws prompt. Formally, the Rerankerws process can be expressed as follows:\nP_j = LLM_{rw} (st_j, A_j), (5)\nwhere LLM_{rw} denotes the Reranker_{ws}, and P_j = {< API_1, desc'_1 >,..., < API'_{k_p}, desc'_{k_p} >} refers to the re-ordered API sequence. The refined API sequence P_j provides more suitable and relevant APIs for developers."}, {"title": "3.3.2 Reranking across subtasks.", "content": "Since the decomposed sub- tasks cannot guarantee a one-to-one correspondence with APIs, directly combining the retrieved APIs for each subtask might result in the omission of necessary APIs or the inclusion of redundant ones. CAPIR leverages cross-subtask API reranker Rerankeras to select APIs from each subtask's retrieval results to form the final API recommendation. Prompt 4 shows the Rerankeras prompt. For- mally, the Rerankeras process can be expressed as follows:\nI_i = LLM_{ra} (S_i, S_i, R_i), (6)\nwhere LLM_{ra} denotes the Reranker_{as} model, R_i = {P_1, ..., P_{m_i} } refers to the re-ordered API sequence corresponding to subtask sequence S_i = {st_1, ..., st_{m_i} }, and the result I_i = {< API''_1, desc''_1 >,..., < API''_{k'}, desc''_{k'} >} refers to the final recommended API se- quence."}, {"title": "4 EXPERIMENTAL SETUP", "content": "In this section, we will introduce our experimental setup, includ- ing the research questions, models, benchmarks, baselines, and evaluation metrics."}, {"title": "4.1 Research Questions", "content": "To evaluate the performance of CAPIR on API sequence recom- mendation and library-oriented code generation, we conducted experiments to address the following research questions:\nRQ1: How effective is CAPIR for Documentation-based API sequence recommendation?\nRQ2: How effective are the APIs recommended by CAPIR in library-oriented code generation?\nRQ3: How effective are the individual components of CAPIR in improving performance?\nRQ4: How effective is CAPIR with different large language mod- els?\nIn RQ1, we compare CAPIR with baselines on API sequence recommendation for both single-library and multi-libraries. In RQ2, we compare CAPIR with baselines on enhancing library-oriented code generation for both single-library and multi-libraries. In RQ3, we compare CAPIR with three ablation settings: (1) Without Ex- amples. We remove the examples in the Decomposer prompt, to assess whether they have positive impact on CAPIR. (2) Without Decomposer. We remove the Decomposer module and directly use the original task to retrieve APIs from Docs, while keeping the Reranker module, to assess whether the Decomposer module im- proves CAPIR's performance. (3) Without Reranker. We remove the Reranker module and directly return the top-k retrieve results, to assess whether the Reranker can improve the retrival perfor- mance. In RQ4, we evaluate the API recommendation performance of CAPIR with another LLM."}, {"title": "4.2 Models", "content": "For the Summarizer, Decomposer, and Reranker, we employ the gpt-3.5-turbo [2] model, which has demonstrated exceptional per- formance in various natural language processing tasks. The prompt for these modules is tailored to their specific tasks. To ensure the certainty of the outputs, we set the parameters of gpt-3.5-turbo as follows: temperature=0.0, top_p=1.0. For Decomposer, we set few- shot examples number k_e = 4, to balance the example diversity and the sequence length. For Reranker, we set candidate API number k_p = 20.\nOn the other hand, for the Selector and Retriever modules, we utilize the ada-embedding-002 [1] model as the embedding model. This model has the ability to generate high-quality embeddings for sentences, allowing us to perform effective similarity computations and retrieval."}, {"title": "4.3 Benchmark Construction", "content": "To evaluate API sequence recommendation and library-oriented code generation for coarse-grained developmental requirements, we construct two challenging benchmarks: (1) RAPID, a documentation- based API sequence recommendation benchmark. (2) LOCG, a library-oriented code generation benchmark."}, {"title": "4.3.1 RAPID.", "content": "The RAPID benchmark contains five datasets. Four single-library API recommendation datasets that correspond to four libraries: Torchdata-AR (AR refers to API Recommendation), Math- AR, Real-Matrix-AR and XMLStreamWriter-AR. One multi-libraries API recommendation task: Multi-Conala, contains seven libraries. The statistics for RAPID are shown in Table 1.\nTorchdata-AR. To evaluate CAPIR for low-resource libraries, we use Torchdata to construct Torchdata-AR datasets. Public infor- mation shows that gpt-3.5-turbo was trained on data collected from the internet before September 2021. Since Torchdata was released on Mar 11, 2022, it was not included in the training data of gpt- 3.5-turbo. Moreover, it has been released for more than one year, ensures there is sufficient client code in the open-source community to construct a test set. Construction details of Torchdata-AR will be provided in Section 4.3.3.\nMath-AR, RealMatrix-AR and XMLStreamWriter-AR. These three datasets are constructed based on DGAS [41], an API recom- mendation dataset. The original DGAS dataset includes training data and documentations corresponding to these libraries. In the datasets we constructed, we excluded the training data and only retained test samples that refer to multiple APIs.\nMulti-Conala. To validate the effectiveness of our approach in API sequence recommendation for multi-libraries, we conducted experiments using the well-established Conala dataset [45]. Conala contains a diverse and representative set of popular libraries, mak- ing it a valuable resource for conducting experiments in the field of API recommendation. Following the methodology outlined in a previous study [51], we obtained API documentation from Dev- Docs\u00b9, encompassing a total of 30,755 APIs extracted from different libraries. To evaluate the API sequence recommendation for coarse- grained requirements, we filtered out samples that only refer to one API. This filtering step resulted in a new test dataset of 232 samples and a training set of 625 samples. These samples ensure a diverse range of API recommendation scenarios, allowing us to comprehensively evaluate the performance of our approach on API recommendation for multi-libraries."}, {"title": "4.3.2 LOCG.", "content": "The LOCG benchmark contains two datasets: (1) Torchdata-Code, a single-library-oriented code generation dataset. (2) Multi-ODEX, a multi-library-oriented code generation dataset. The statistics of LOCG are shown in Table 1.\nTorchdata-Code. To evaluate the impact of recommended APIs on single-library-oriented code generation performance, we se- lected 50 samples from the Torchdata-AR to construct an excution- based code genration dataset. We extracted the original code snip- pets, manual transformed them into executable functions and wrote test cases for them.\nMulti-ODEX. To evaluate the impact of recommended APIs on multi-library-oriented code generation performance, we con- structed code generation dataset for multi-libraries based on ODEX [40] dataset. ODEX is a benchmark for evaluating natural language (NL)"}, {"title": "4.3.3 Torchdata-AR Construction Details.", "content": "We crawled reposi- tories on GitHub that import the Torchdata library to construct the Torchdata API recommendation dataset. By using the keywords \"from Torchdata\" and \"import Torchdata,\" we utilized the Github Search code's REST API and collected 1342 code files. We first re- moved duplicates and split the code files at the function granularity. Then, we filtered out code snippets that called fewer than 3 APIs. Since most of these code snippets lack descriptive comments, we need to summarize the functionality of the code snippets to use them as task descriptions during testing. To reduce manual annota- tion costs, we use GPT-4 [32] to generate comments for these code snippets. GPT-4 has achieved comparable to or even surpassing human performance in many tasks, making it competent for our annotation. For each code snippet, we extracted the Torchdata APIs that were used in the code and added their API descriptions into the prompt, enabling GPT-4 to generate higher quality code comments. To enhance the credibility and quality of the data generated by GPT-4, we engaged three graduate students with more than five years of programming experience to scrutinize the comments gen- erated by GPT-4. We first let them thoroughly read the Torchdata documentation to familiarize themselves with the functionalities and usage of the APIs. Then the code comments generated by GPT-4 were carefully examined, and were categorized into three types: \"appropriate,\" \"require improvement,\" and \"inappropriate.\"\n1) For samples that all three annotators deemed \"appropriate\", they were included in the final test set without any modifications. 2) If the comment is marked as \"require improvement\", the an- notators engaged in discussions to reach a consensus on how to improve it. If a consensus was reached, the sample was added to the test set after the new comment replaced the original one. On the other hand, if no agreement could be reached, the sample was excluded from the test set. 3) Comments that were found to be \"inappropriate\" by any of the annotators were immediately discarded from the dataset to ensure its high quality and reliability.\nFollowing this rigorous curation process, we finalized the Torch- data API recommendation dataset, comprising a total of 278 ex- amples. This curated test set ensures a robust evaluation of our approach on realistic development scenarios."}, {"title": "4.4 Baselines", "content": "We compare CAPIR with two baselines: (1) ADA-retrieve, which directly employs CAPIR's base embedding model for retrieval.(2) CLEAR [43], the state-of-the-art retrieval-based API recommenda- tion method.\n\u2022 ADA-retrieve [1]. To demonstrate the effectiveness of task de- composition for retrieval, we chose ADA-retrieve as a baseline. ADA-retrieve directly used OpenAI's off-the-shelf embedding model, ada-embedding-002, to retrieve the top-k APIs from the"}, {"title": "4.5 Metrics", "content": "In this section, we will separately introduce the evaluation met- rics for API recommendation and code generation. To evaluate the performance of our API recommendation approach, we adopt stan- dard evaluation metrics commonly used in existing studies [43]: Precision@k and Recall@k. where k = {3, 5, 10, 15}. To evaluate the impact of recommended APIs on library-oriented code gen- eration, we use the execution based metric pass@k to assess the accuracy of the generated code, where k = {1, 10, 100}.\nPrecision@kmeasures the proportion of correctly recommended APIs among the top-k recommended APIs. It assesses the accuracy of our approach in providing relevant and correct API recommen- dations to developers.\nPrecision@k = C({retrieved APIs}@k, {relevant APIs}), (7)\nwhere\nC(A, B) = \\frac{\\sum_{a \\in A} \\delta(a, B)}{|A|} (8)\n\\delta(a, B) = \\begin{cases} 1 & \\text{if } a \\in B \\\\ 0 & \\text{otherwise} \\end{cases} (9)\nRecall@k, on the other hand, measures the proportion of correctly recommended APIs out of all ground-truth APIs within the top-k recommendation results. It evaluates the ability of our approach to retrieve a sufficient number of relevant APIs, ensuring that no important API is overlooked.\nRecall@k = C({relevant APIs}, {retrieved APIs}@k). (10)\nBy calculating Precision@k and Recall@k for various values of k, we can comprehensively assess the effectiveness and coverage of our API recommendation approach.\nPass@k is an evaluation metric that has been widely used in previous work [11, 47, 48]. Each task in LOCG has several corre- sponding test cases. For a given task, we randomly sample k codes and calculate the proportion that can pass all test cases."}, {"title": "5 RESULTS AND ANALYSES", "content": "In this section, we will present the results for the four RQs, analyze qualitative examples, and evaluate the performance of CAPIR in practical development scenarios."}, {"title": "5.1 RQ1: Effectiveness of CAPIR on API Sequence Recommendation", "content": "Setup. To compare the performance of CAPIR with two baselines in API sequence recommendation, we conducted experiments on the RAPID benchmark. Due to the limited posts about low-resource library in StackOverflow and other question-answering communi- ties, we were unable to follow the exact method used in the CLEAR paper to select APIs from similar StackOverflow posts. Instead, we employed CLEAR to retrieve the closest k APIs from the API documentation as the results.\nResults. The results from Table 2 demonstrate that CAPIR out- performs the two baselines in compositional API recommendation. (1) CAPIR outperforms the two baselines on all four single library API sequence recommendation datasets. For Torchdata-AR dataset, comparing with ADA-retrieve, CAPIR improves the Recall@3, 5, 10, 15 by 73.9%, 83.1%, 58.6%, 38.2% and Precision@3, 5, 10, 15 by 74.1%, 84.6%, 58.1%, 37.3%; comparing with CLEAR, CAPIR im- proves the Recall@3, 5, 10, 15 by 131.8%, 131.0%, 86.3%, 52.5% and Precision@3, 5, 10, 15 by 137.5%, 139.4%, 91.8%, 55.9%. CAPIR also outperforms the two baselines on Math-AR, RealMatrix-AR and XMLStreamWriter-AR datasets. (2) CAPIR also outperforms the two baselines on multi-libraries API sequence recommendation. For Multi-Conala dataset, Comparing CAPIR with ADA-retrieve, CAPIR improves the Recall@3, 5, 10, 15 by 279.7%, 233.0%, 156.1%, 94.0% and Precision@3, 5, 10, 15 by 283.1%, 230.2%, 160.5%, 94.4%; Comparing CAPIR with CLEAR, CAPIR improves the Recall@3, 5, 10, 15 by 29.3%, 25.2%, 21.8%, 23.3% and Precision@3, 5, 10, 15 by 29.7%, 21.5%, 17.9%, 18.6%. CAPIR's performance on Multi-Conala demonstrates that it can recommend more appropriates APIs from a pool of multiple libraries. Under most experimental settings, CAPIR achieves higher performance, which indicates better API sequence recommendation performance. Morevover, the comparison between CAPIR and CLEAR further illustrates that previous methods per- form less effectively on unseen libraries.\nAnswer to RQ1: CAPIR outperforms ADA-retrieve and CLEAR on RAPID benchmark. The significant improvements indicate the effectiveness of CAPIR on documentation-based API sequence recommendation."}, {"title": "5.2 RQ2: Effectiveness of CAPIR on Library-Oriented Code Generation", "content": "Setup. To evaluate the impact of CAPIR on library-oriented code generation, we conducted experiments on Torchdata-Code and Multi-ODEX datasets. We used gpt-3.5-turbo as the base model for code generation. The prompt for code generation is shown in 1c, including the development task and the APIs recommended by CAPIR. To calculate pass@k with multiple samples, as done in previous studies, we set the temperature = 0.8 and top_p=0.95. The number of APIs included in the prompt is a crucial consideration. If there are too few APIs in the prompt, it may not cover the re- quired APIs for the given task, while having too many APIs could introduce excessive noise to the prompt. According to the results in Table 2, k=5 strikes the best trade-off between recall and precision. Therefore, we add 5 APIs for each task in the prompt.\nResults. The results of Table 3 show that adding CAPIR's rec- ommendation results into the prompt can improve code genera- tion performance. (1) On single-library-oriented code generation. Compared to using gpt-3.5-turbo to directly generate code, adding CAPIR's recommendation results improves pass@1, 10, 100 by 76.47%, 36.26%, 75.0%. Comparing with adding ADA-retrieve re- sults, adding CAPIR's recommendation results improves pass@1, 10, 100 by 130.77%, 77.14%, 133.33%. It's worth noting that on the Torchdata-Code dataset, adding ADA-retrieve results to the prompt decreased the code generation performance compared to directly generating code. This suggests that when the quality of API rec- ommendations is insufficient, it will introduce noise, thereby hav- ing a negative impact on library-oriented code generation. (1) On multi-library-oriented code generation. Compared to using gpt-3.5- turbo to directly generate code, adding CAPIR's recommendation results improves pass@1, 10, 100 by 28.0%, 25.14%, 32.06%. Com- pared to adding ADA-retrieve results, adding CAPIR's recommen- dation results improves pass@1, 10, 100 by 27.54%, 15.46%, 23.21%. CAPIR exhibits a more significant improvement on Torchdata-Code compared to Multi-ODEX, indicating that adding API information to the prompt is more effective for unseen-library-oriented code generation. Overall, the results suggest that the quality of API rec- ommendations plays a crucial role in enhancing the code generation effectiveness.\nAnswer to RQ2: Compared to ADA-retrieve, the API recom- mended by CAPIR can bring more improvements to library- oriented code generation."}, {"title": "5.3 RQ3: Effectiveness of Key Components", "content": "Setup. We conducted ablation experiments on Torchdata-AR dataset to assess"}]}