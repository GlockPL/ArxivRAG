{"title": "ProcessTBench: An LLM Plan Generation Dataset for Process Mining", "authors": ["Andrei Cosmin Redis", "Mohammadreza Fani Sani", "Bahram Zarrin", "Andrea Burattin"], "abstract": "Large Language Models (LLMs) have shown significant promise in plan generation. Yet, existing datasets often lack the complexity needed for advanced tool use scenarios such as handling paraphrased query statements, supporting multiple languages, and managing actions that can be done in parallel. These scenarios are crucial for evaluating the evolving capabilities of LLMs in real-world applications. Moreover, current datasets don't enable the study of LLMs from a process perspective, particularly in scenarios where understanding typical behaviors and challenges in executing the same process under different conditions or formulations is crucial. To address these gaps, we present the ProcessTBench dataset, an extension of the TaskBench dataset specifically designed to evaluate LLMs within a process mining framework.", "sections": [{"title": "1 Introduction", "content": "The rapid advancements in Large Language Models (LLMs) have generated significant interest in their potential across various domains, particularly in their tool use and plan generation capabilities. These capabilities are increasingly critical as LLMs are envisioned to provide natural language interfaces for complex process automation. Despite their promise, the emergent properties of LLMs remain not fully understood, making empirical studies essential for advancing LLM plan generation, as evidenced by the growing number of benchmarks aimed at testing various LLM use cases [1].\nDespite the promise of LLMs, plan generation remains in its early stages, often struggling with the reliability needed for complex tasks. This limitation highlights the importance of evaluating LLM behavior on sophisticated tasks to better equip them for executing advanced processes. Existing benchmarks, such as ToolBench [5], have made valuable progress in this area, but they frequently overlook the intricacies of LLM-generated plans, including sequence length, parallelism, and the handling of paraphrased queries. The absence of datasets featuring"}, {"title": "2 Generating ProcessTBench", "content": "The data generation pipeline 1 consists of the following components:\n* TaskBench Queries and Ground Truth Plans in Process Model Format. We selected the most challenging subset of the TaskBench dataset (TaskBench Multimedia), comprised of 565 queries and their respective ground truth plans as directed acyclic graphs.\n1. LLM Planner. Given a query and a set of available tools, it generates a sequence of tool invocations to resolve the query 2. It is slightly modified from planners like ReAct [6], in that with one inference, it generates all the tool invocations necessary to solve the query instead of only the next step.\n(a) Input: query text and available tools in text format. Only the tools required to solve the query were given.\n(b) Output: Plan that solves the query with the given tools 2"}, {"title": "3 Description of ProcessTBench", "content": "The ProcessTBench dataset builds upon TaskBench [3], focusing on task complexity, tool usage, and process characteristics. The ProcessTBench dataset includes 532 base queries drawn from the most challenging subset of TaskBench, each paraphrased 5 to 6 times, with an average of 4.08 solution plans per query. These plans involve action sequences utilizing a subset of 40 unique tools. Corresponding to the queries, ProcessTBench additionally includes the respective ground truth plans in Petri net format."}, {"title": "3.1 Queries", "content": "To validate the quality of the base queries selected from TaskBench, we present a balanced distribution of actions required to solve these queries, as shown in Figure 2a. This distribution suggests an even representation of task types, ensuring comprehensive coverage across various action categories.\nAdditionally, we evaluated the quality of the paraphrased queries by using an LLM plan generator to create plans for both the original TaskBench queries and their paraphrased counterparts. We then applied conformance checking, specifically alignment fitness (as described in Chapter 8 of [4]), to compare the generated plans. Figure 2b illustrates the alignment differences between the original and paraphrased queries, showing that the paraphrased queries generally maintain equivalent alignment quality. Specifically, the comparison revealed 1,965 instances of equivalence, 397 instances where the paraphrased queries performed worse, and 389 instances where they performed better. The mean alignment difference was 0.00 with a standard deviation of 0.11. A Wilcoxon signed-rank test yielded a p-value of 0.56, suggesting that there is no significant difference in the quality between the original and paraphrased queries."}, {"title": "3.2 Ground Truth and Generated Plans", "content": "Table 1 provides an overview of the process-related characteristics of both the ground truth and generated plans within ProcessTBench. Each query in ProcessTBench is associated with a ground truth plan in Petri net format and 5-6 LLM-generated plans derived using a custom prompt. The complexity of these ground truth Petri nets is quantified using Cardoso and Cyclomatic complexity metrics [2], enabling comparison with other process mining datasets. The degree of concurrency, defined as the ratio between the longest and shortest paths in a Petri net, measures the level of parallelism (a ratio of 1.00 indicates no parallelism, while values greater than 1 signify increasing levels of parallel behavior). Additionally, the mean number of plan variants reflects the diversity of alternative solutions generated for each query. One sample from the generated plan dataset is shown in Table 2."}, {"title": "4 Use-cases", "content": "In this section, we provide some of the potential applications of the ProcessT-Bench dataset.\nEvaluating Plan Generation by LLMs: The dataset offers a comprehensive platform to assess how efficiently and accurately LLMs can generate action plans for complex tasks. It enables the exploration of how these models interpret queries, utilize available tools, and sequence actions to solve problems.\nEvaluating Paraphrase Handling in Plan Generation by LLMs: ProcessTBench provides a unique opportunity to assess how well LLMs handle paraphrased or multi-language queries. The dataset features queries in multiple languages and paraphrased forms, thereby allowing the evaluation of LLMs' versatility and adaptability in different linguistic contexts."}, {"title": "5 Conclusion", "content": "ProcessTBench offers a platform for evaluating LLMs in complex plan generation scenarios. By incorporating multilingual query paraphrasing and generating multiple plan variants, this dataset allows for a more nuanced analysis of LLM behavior, including performance in multi-prompt situations, process discovery, and conformance checking. Future work will further focus on expanding the dataset with additional queries and languages and incorporating more sophisticated LLM framework. We alos plan to consider other plan generation techniques as a parameter in the dataset."}]}