{"title": "Symbolic Disentangled Representations for Images", "authors": ["Alexandr Korchemnyi", "Alexey K. Kovalev", "Aleksandr I. Panov"], "abstract": "The idea of disentangled representations is to reduce the data to a set of generative factors that produce it. Typically, such representations are vectors in latent space, where each coordinate corresponds to one of the generative factors. The object can then be modified by changing the value of a particular coordinate, but it is necessary to determine which coordinate corresponds to the desired generative factor a difficult task if the vector representation has a high dimension. In this article, we propose ArSyD (Architecture for Symbolic Disentanglement), which represents each generative factor as a vector of the same dimension as the resulting representation. In ArSyD, the object representation is obtained as a superposition of the generative factor vector representations. We call such a representation a symbolic disentangled representation. We use the principles of Hyperdimensional Computing (also known as Vector Symbolic Architectures), where symbols are represented as hypervectors, allowing vector operations on them. Disentanglement is achieved by construction, no additional assumptions about the underlying distributions are made during training, and the model is only trained to reconstruct images in a weakly supervised manner. We study ArSyD on the dSprites and CLEVR datasets and provide a comprehensive analysis of the learned symbolic disentangled representations. We also propose new disentanglement metrics that allow comparison of methods using latent representations of different dimensions. ArSyD allows to edit the object properties in a controlled and interpretable way, and the dimensionality of the object property representation coincides with the dimensionality of the object representation itself.", "sections": [{"title": "Introduction", "content": "Good data representation for machine learning algorithms is one of the key success factors for modern approaches. Initially, the construction of good representations consisted of feature engineering, i.e., the manual selection, creation, and generation of such features that allow the model to successfully solve the main problem. Although feature engineering is still used in some areas, current models rely on learning representations from data [1]. This approach has become ubiquitous in computer vision [2], natural language processing [3], processing of complex structures such as graphs [4, 5], and others. A good representation can mean a representation with different properties, e.g. proximity of representations for semantically related objects [3], identification of common features in objects [2], preservation of a complex structure with dimensionality reduction, and disentanglement of representations [6-8].\nIn this article, we consider disentangled representations. Following the work in [9], we define disentangled representations as those described by the following three properties: 1) disentanglement, i.e., the representation should factorize (disentangle) the underlying generative factors, so that one variable captures at most one factor, 2) completeness, the degree to which each underlying factor is captured by a single variable, 3) informativeness, i.e., the amount of information that a representation captures about the underlying generative factors (to be useful for tasks that require knowledge of the important data attributes, representations must fully capture information about the underlying generative factors). The disentangled representation can potentially improve generalization and explainability in many machine learning tasks: structured scene representation and scene generation [10, 11], reinforcement learning [12-14], planning [15], reasoning [16, 17], and object-centric visual tasks [18-22]. The main research in disentanglement focuses on obtaining representations expressed by a vector, where each coordinate of the vector captures a generative factor [6-8], and by changing this coordinate, we can change the property of the represented object.\nIn this work, we propose Architecture for Symbolic Disentanglement (ArSyD), which uses a disentangled representation in which an entire vector of the same dimension as the resulting representation of the object captures a generative factor. The object representation is then obtained as a superposition of the vectors responsible for each of the generative factors. We call such a representation a symbolic"}, {"title": "Disentangled Representation", "content": "Most of the methods for learning disentangled representations are based on either the Variational Autoencoder (VAE) [6, 8, 43-45] or on the Adversarial Generative Networks [46] (GAN) [7, 47, 48].\nIn these approaches, the disentanglement is achieved by imposing additional constraints on the loss: by introducing a \u1e9e parameter to balance the independence constraints with reconstruction accuracy in BetaVAE [6]; by adding a capacity control objective in [43]; by factorizing the representation distribution in FactorVAE [8]; by decomposing the evidence lower bound in B-TCVAE [44]; by minimizing the covariance between the latents in DIP-VAE [45]; by maximizing the mutual information in InfoGAN [7]; by introducing a contrastive regularizer in InfoGAN-CR [47]; by adding a mutual information loss to StyleGAN [49] in Info-StyleGAN [48]. Other approaches impose additional restrictions based on group theory on existing VAE models [50], or propagate inductive regulatory bias recursively over the compositional feature space [51], or provide a framework for learning disentangled representations and discovering the latent space [52].\nIn ArSyD, we do not impose additional specific restrictions on the loss, but use a structured representation of the object in the latent space using HDC principles and a special learning procedure. We represent each generative factor as a HV of the same dimension as the resulting representation. The object representation is then obtained as a superposition of the vectors responsible for the generative factors.\nTo evaluate the disentanglement of the representation, special metrics are used that assume a localist representation in the latent space, where each coordinate represents the generative factor. The BetaVae score [6] uses a simple classifier to predict the index of a generative factor that is held fixed while other factors are randomly sampled. This approach is not applicable to distributed representations because individual positions in such representations do not carry all the information about the generative factor. FactorVAE score [8] proposes a similar approach, but uses an index of a position with less variance. MIG [44] uses mutual information and also refers to the individual coordinates of the latent representation, making it impossible to use this metric for other representations. SAP score [45] uses classification or linear regression, which also does not work for distributed representations. In this article, we propose two new disentanglement metrics that allow the comparison of models regardless of whether they use a localist or a distributed representation in the latent space."}, {"title": "Methods", "content": "In this article, we apply HDC principles to obtain an object representation in the latent space and then map this representation to the raw data. We use a distributed representation and represent each generative factor as an HV with the same dimensionality as a final object representation. This means that the information is distributed across all components, no single HV component has a meaning, i.e., only the whole HV can be interpreted. This differs from the localist representation used by modern disentanglement models, where a single vector component potentially has a meaning. The nature of HVs can be different, such as binary [23], real [53\u201357], complex [53], or bipolar [58, 59]. Also, the exact implementation of the vector operations may vary for different vector spaces while maintaining the computational properties. For a detailed comparison of different VSA implementations, we recommend to refer to [60].\nTo represent a simple entity in HDC, the seed vector is sampled from the vector space and stored in an item memory. When an object has a complex structure, the resulting representation is obtained by performing vector operations on the seed vectors from the item memory [24]. We illustrate the vector operations defined in HDC with an example from the Holographic Reduced Representation (HRR) [54] that we use in this article. This implementation works with real-valued HVs sampled from the normal distribution $N(0,\\frac{1}{D})$ with mean 0 and variance $\\frac{1}{D}$, and uses cosine distance as the similarity measure, where D is the vector dimension.\nTwo main vector operations are bundling (addition) and binding (multiplication). The addition (+) is implemented as an element-wise sum with normalization: $R = \\frac{\\sum V_i}{\\lVert \\sum V_i \\rVert}$. The resulting vector R is similar to the summand vectors $V_i$, but quasi-orthogonal to other seed vectors. Semantically, bundling represents a set of vectors and, thus, a set of symbols. The multiplication (*) uses a circular convolution: $R = V_1 \\circledast V_2 = F^{-1}{F{V_1} \\odot F{V_2}}$, where $\\odot$ is an element-wise multiplication, F and $F^{-1}$ are the Fourier transform and the inverse Fourier transform, respectively. The multiplication maps the vectors V1 and V2 to another HV \u2013 R. The resulting vector R is dissimilar (quasi-orthogonal) to V1, V2 and other HVs of the vector space. Binding represents an attribute-value pair, i.e. the assignment of a value to a corresponding attribute.\nUsing these operations, we can represent any object O generated by N underlying generative factors as a set of attribute-value pairs (Figure 2a):\n$O = G_1 \\circledast V_{G_1} + G_2 \\circledast V_{G_2} + \\cdots + G_N \\circledast V_{G_N}$,\nwhere $G_i$ is a i-th generative factor, $V_{G_i}$ is a $j_m$-th value of a generative factor $G_i$. $j_m = 1...k_{G_i}$, where $k_{G_i}$ is the number of values of a generative factor $G_i$.\nThus, the target object O in Figure 1 can be represented as:\n$O = S \\circledast Cy + C \\circledast R + Si \\circledast L + M \\circledast Me +  C_x \\circledast X + C_y \\circledast Y,$\nwhere (S)hape, (C)olor, (Si)ze, (M)aterial, $(C_x)$oordx, $(C_y)$oordy and (Cy)linder, (R)ed, (L)arge, (Me)tal, X, Y are HVs for underlying generative factors and their corresponding values. The HVs for the generative factors $G_i$ and the values $V_{G_i}$ are sampled at the time of model initialization, then fixed and stored in the corresponding item memory (codebook). They do not change during the training and testing phases of the model. The number of"}, {"title": "End-to-end Model", "content": "For experiments with a single object in the scene, we use the architecture shown in Figure 2b and add a decoder that reconstructs the images of the target and donor objects. We use only the reconstruction losses, the mean squared error (MSE) between the original and reconstructed images, to train the model:\n$L_{Object} = MSE(O_t,\\widetilde{O_t}) + MSE(O_d,\\widetilde{O_d}),$\nwhere $O_t$ a target object, $\\widetilde{O_d}$ \u2013 a reconstructed target object, $O_d$ \u2013 a donor object, $\\widetilde{O_d}$ \u2013 a reconstructed donor object.\nThe overall end-to-end model for the case of multiple objects in the scene is shown in Figure 3. We use Slot Attention [39] to discover objects in the scene and represent it as a collection of individual objects. Then, for the target object, we reconstruct its image from a corresponding slot S and use this image in the feature exchange module. The Feature Exchange module is implemented in the same way as in Figure 1, except that that instead of reconstructing the obtained object representation with a changed value of the generative factor, we use a Multilayer Perceptron (MLP) to map it into the slot space \u2013 S'. Then, we replace the original slot of the object S with its modified version S' and reconstruct the scene image. The reconstructed scene contains an object with a modified value of the generative factor.\nDuring training, we only use reconstruction losses (MSE) to control the quality of the image restoration:\n$L_{Scene} = MSE(S, \\widehat{S}) + MSE(S', \\widehat{S'}) + MSE(O_d, \\widetilde{O_d}),$"}, {"title": "Evaluation Metrics", "content": "Popular disentanglement metrics such as BetaVAE score [6], DCI disentanglement [9], MIG [44], SAP score [45], and FactorVAE score [8] are based on the assumption that disentanglement is achieved by each individual vector coordinate capturing a generative factor in the data \u2013 localist representations. Since we use distributed representations in ArSyD, current metrics are not suitable for evaluating the disentanglement of the proposed model representations. When comparing models with different latent space representation structures (e.g., localist or distributed representations), the problem arises that a direct comparison using metrics based on the latent rep-"}, {"title": "Discussion", "content": "An important issue is the generalization of the ArSyD to real-world data. To do this effectively, two different situations must be considered: one where the set of generative factors is known, and another where it remains unknown. In scenarios where the generative factors for each object in the dataset are known, symbolized as $G^i = G(O_i)$ where $G^i$ is a disentangled representation of an object $O_i$, we can collect a set of training examples to initially teach the model to recognize these generative factors. If there are N generative factors and two objects O1 and O2 differ by k generative factors, then it is necessary to simultaneously exchange k corresponding generative factors in G\u00b9 and G2 to train the model. Due to the significant combinatorial increase in possible combinations, this approach does not require the creation of a large dataset with a large amount of initially labeled data.\nIn scenarios without using labeled data, it is possible to use pre-"}, {"title": "Conclusion", "content": "In this article, we propose ArSyD \u2013 Architecture for Symbolic Disentanglement \u2013 which learns symbolic disentangled representations using a structured representation of an object in latent space through a weakly supervised learning procedure. This representation is based"}]}