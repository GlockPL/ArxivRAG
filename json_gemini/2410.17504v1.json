{"title": "AN ONTOLOGY-ENABLED APPROACH FOR USER-CENTERED AND KNOWLEDGE-ENABLED EXPLANATIONS OF AI SYSTEMS", "authors": ["Shruthi Chari"], "abstract": "Explainability has been a well-studied problem in the Artificial Intelligence (AI) community through several AI ages, ranging from expert systems to the current deep learning era, to enable AI's safe and robust use. Through the ages, the unique nature of AI approaches and their applications have necessitated the explainability approaches to evolve as well. However, these multiple iterations of explaining AI decisions have all focused on helping humans better understand and analyze the results and workings of AI systems.\nIn this thesis, we seek to further the user-centered explainability sub-field in AI by addressing several challenges around explanations in the current AI era, which is characterized by the availability of many machine learning (ML) explainers and neuro-symbolic AI approaches. Previous research in user-centered explainability has mainly focused on what needs to be explained and less so on implementations for them. Additionally, there are challenges to supporting explanations in a manner that humans can easily interpret due to the lack of a unified framework for different explanation types and methods to support domain knowledge from authoritative literature. We address the three challenges or research questions around user-centered explainability: How can we formally represent explanations with support for interacting AI systems (AI methods in applied ecosystem), additional data sources, and along different dimensions? How useful and feasible are such explanations for clinical settings? Is it feasible to combine explanations from multiple data modalities and AI methods?\nFor the first research question, we design an Explanation Ontology (EO), a general-purpose semantic representation that can represent fifteen different literature-derived explanation types via their system-, interface- and user- related components. We demonstrate the utility of the EO in representing explanations across different use cases, supporting system designers to answer explanation-related questions via a set of competency questions, and categorizing explanations to be of supported explanation types within our ontology.\nFor the second research question, we focus on key explanation dimension, that is, contextual explanation and conduct a case study on supporting contextual explanations from an authoritative knowledge source, clinical practice guidelines (CPGs). Here, we design a clinical question-answering (QA) system to address CPG questions to provide contextual explanations to help clinicians interpret risk prediction scores and their post hoc explanations", "sections": [{"title": "INTRODUCTION", "content": "Artificial Intelligence (AI) has evolved over the years from having limited applications in orig- inally thought-about application domains such as the military to having more widespread use to being available to assist humans in both high-precision tasks such as healthcare, military and financial decisions to more everyday tasks such as helping in web search, weather alerts to navigation. Through these applications and improvements in computing technology and AI research, AI methods have evolved to support various applications and better computing infrastructure. Moreover, with the evolution of AI, different methods have emerged, such as rule-based expert systems to pattern-based machine learning (ML) and deep learning meth- ods. In today's AI ecosystem, we see a co-existence of both neural and symbolic approaches or neurosymbolic approaches. As humans, we tend to trust AI better if we can understand the reasoning of how the AI came to a decision or connect an AI decision to what we are familiar with [1], [2]. Clearly, explainability or explainable AI (XAI) has been one of the first conceived thrusts of what we know today as trustworthy AI [3], from early works such as Mycin [4] that were developed to explain an expert system in a medical setting to now the plethora of post-hoc explainability methods that provide reasoning for features that were found to be important by somewhat opaque ML models. In essence, explanations in AI have had to evolve in approaches and outputs with the advancements in AI methods. Additionally, explanations can be multi-dimensional in that, as humans, we reason through different paths before we trust a decision (Fig. 1.1). For example, when a clinician is deciding, \"which treatment option is suitable for a patient?\" they refer to multiple factors, including patient details such as their drug-related allergies, their current treatment, and lit- erature to understand what is recommended for the patient. In summary, explanations are often reactive to user questions [5], [6], [7], and often have multiple forms and types such as the \"What ifs\u201d or counterfactuals, \u201cWhat evidence\u201d or scientific and \u201cWhat data\u201d or data- based [8], [9], [10]. However, given the increase in complexity of AI and ML methods, they have become more opaque, and a lot of explainability research has focused on model explana- tions [11], [12] alone. Several researchers [2], [9], [8] have posited the need for conversational user-centered explainability, which is of multiple types and supported by various sources such as data, knowledge, and context. However, many publications in user-centered explainability"}, {"title": "Contributions", "content": "(Chapt. 3) We design an Explanation Ontology (EO), a general-purpose semantic representation that can represent fifteen different literature-derived explanation types via their system-, interface- and user- related components. We showcase the utility of the EO's model to represent explanation types across 7 different use cases ranging from domains of finance, food to healthcare. We design competency questions that our target end-user, a system developer, would ask when using the EO. Further, we have released two versions of the EO with added support for a wider range of commonly used explainer methods in EO V2.0. The EO is open-sourced and available at: https://tetherless-world.github.io/explan ation-ontology/index.\n(Chapt. 4) We design a clinical question-answering (QA) system to address questions from clinical practice guidelines (CPGs) to provide contextual explanations to help clini- cians interpret risk prediction scores and their post-hoc explanations in a comorbidity risk prediction setting. Here, we chose a setting of clinical and real-world importance, such as comorbidity risk prediction of a chronic disease, and refined the use case in consultation with a clinician. In addition, we identified dimensions of interest in the use case, along with which contextual explanations would be helpful for clinicians to interpret the scores and patient features better. From an implementation standpoint, we developed a QA framework to ex- tract and support contextual explanations from CPGs. We leverage large language models and their clinical variants for the QA and build knowledge augmentations to improve the semantic coherence of the answers to the questions. We conduct a quantitative evaluation to demonstrate the QA's efficacy and the feasibility of supporting contextual explanations from authoritative literature. We also engaged clinicians in expert panel sessions to understand if\n(Chapt. 5) We design a general-purpose metaexplainer framework capable of provid- ing multiple explanations to an end-user question (e.g., that of a clinician). Within the MetaExplainer, we first want to break down a user question into sub-questions addressed by explanation types supported in the EO. We then can invoke explainers registered to these types to generate individual explanations. Overall, through the MetaExplainer, we generate natural language explanations from their individual data, knowledge, and method output components. We evaluate the explanations at each stage by using metrics [24] that the XAI community has proposed."}, {"title": "Outline", "content": "In the rest of thesis, we will first explore the related work to each contribution (Chapter 2), and in individual chapters (Chapters 3, 4 and 5) discuss details of each contribution covering the motivation, methods and results of them. Finally, we discuss the challenges this thesis addresses and potential avenues for future work in the Discussion chapter (Chapt. 6)."}, {"title": "Explanation Ontology", "content": "Ontologies and Knowledge Graphs (KGs) capture an encoding of associations be- tween entities and relationships in domains, and hence, can be used to inform upstream tasks [25], [26], guide/constrain ML models [27], and structure content for the purpose of organization [28], [29]. User-centered explanations are composed of different components, such as outputs of Artificial Intelligence (AI) / Machine Learning (ML) methods and prior knowledge, and are also populated by content annotated by different domain ontologies and KGs (of which there are many). The former proposition of composing explanations from components has been attempted less frequently [30], [28], [20], [31] and at different degrees of content abstraction, hence providing open challenges to represent explanations semanti- cally. Additionally, there have been two multidisciplinary, comprehensive, and promising reviews [15], [32] highlighting the applications of KGs to explainable AI (XAI), either solely as the data store to populate explanations, or as aids to explain AI decisions from the knowledge captured by the KG encodings. Efforts to use ontologies and KGs to improve the explainability of AI models will become increasingly popular since several publications point out that single scores from ML models are hard for subject matter experts (SMEs) to interpret directly [21], [33], [34]. Here, we review semantic efforts to represent explanations and highlight how the EO is different in terms of its overall goal and representation."}, {"title": "Contextualizing Model Explanations via a Knowledge-augmented Question-answering (QA) Method", "content": "Our methods build on both expert feedback and past efforts to leverage clinical domain knowledge for generating explanations within AI assistants. Some notable and relevant past works include: MYCIN [4], where domain literature was encoded as rules and trace-based explanations, which addressed \u2018Why,' \u2018What,' and 'How,' were provided for the treatment of infectious diseases; the DESIREE project [40], where case, experience, and guideline-based knowledge was used to generate insights relevant to patient cases; and a mortality risk pre- diction effort [26] of cardiovascular patients, where a probabilistic model was utilized to combine insights from patient features, and domain knowledge, to ascertain patient confor- mance to the literature. However, these approaches are either not flexible nor scalable for the ingestion of new domain knowledge [4], [40], or are narrowly focused in their approach to explanations along limited dimensions [26]. We attempt to allow clinicians to probe the supporting evidence systematically and thoroughly while asking holistic questions about the supporting evidence(s) to understand their patients better. On the guideline QA front, there have been several efforts on representation formats for guidelines and more recent work on applying ML and large language model (LLM) [41] approaches on guidelines for upstream tasks other than QA [42], [43], [44]. Guideline rep- resentation efforts attempt to model guidelines as rules that can then be checked against"}, {"title": "MetaExplainer: A Method to Combine Multiple Explanations", "content": "While several model explainer [11], [12], [60] methods exist to provide post-hoc expla- nations of model decisions, or even counterfactuals and contrastive explanations, no single method can provide the composite view that users require [17]. Additionally, recent pa- pers point out that current model explainability [61], [21] does not align with the expected human-comprehensible explanations that domain experts expect when using AI aids in their practice: explanations required in real-world applications are multi-modal and conversational in nature [7], [17]. In an effort to produce explanations directly for user questions, Slack et al. [62] propose using utterances or cues to prompt LLMs to create filters that can be used to serve as inputs for model explainers and thus generate model explanations in-line with user questions. While this approach is promising, it mainly works for model explainers and not for other methods that borrow from domain knowledge. In our metaexplainer approach, we plan to develop methods that can provide users with multiple user-centered explanations supported within the EO and also produce these explanations from different modalities, including data, method outputs, and knowledge. Our closest related work is by Slack et al [62] who propose a TalkToModel or Megaexplainer framework where users can ask questions and are provided natural-language explanations of model explanations including contrastives or feature-importance and data explanations or details about data processing and analysis. However, there framework only supports a limited set of explanation types and is limited by the types of questions that can be addressed owing to the rule-based generation process of questions. Our MetaExplainer is more modular and is adaptable to more explanation methods (i.e., developers can add support for more explainer methods under the Delegate stage of the MetaExplainer) and we currently support five user-centered explanation types (Case based, Contrastive, Counterfactual, Data and Rationale) which is more than the four types of explanation types (Contrastive, Counterfactual, Data and System Performance) that Megaexplainer supports. We reused the design principles of Megaexplainer where possible.\nAdditionally, some researchers such as Krishna et al. [63] are also looking at when it is wise to produce explanations and when not, they propose a Robust Counterfactual Explanations under the Right to be Forgotten (ROCERF) framework. Further, Ghassemi et al. [21] also allude to the false hope of explanations in their article. While ROCERF is a start at analyzing whether explanations provide value, it is hard to apply the method to problems beyond the field of counterfactual explanations studied. In the MetaExplainer, we rank explanation outputs using well-studied explanation metrics [24]. Finally, McGuinness and Silva [64], [29] have early work on composing explanations from components in task- based environments. However, their work predates the explainer methods that we use today but is still relevant in the various modules such as dispatchers, constraint, and knowledge explainers contributing to explanations. We leverage their modular design while designing the MetaExplainer so that the different explanation composition stages are modular and easy to swap."}, {"title": "Overview", "content": "In recent years, with principles introduced in global policy around AI such as in Eu- rope's General Data Protection Regulation (GDPR) act [65] and the White House's National AI Research Resource (NAIRR) Task Force [66], there has been a growing focus on trust- worthy AI. This focus has reflected the need for transparency and trust around the vast amounts of data collected by parties in multiple domains and has brought to light potential concerns with the AI models increasingly used on such data to affect important decisions and policy. In the trustworthy AI age, several position statements [9], [2], [8], [7] focused on directions to move towards explainable, fair, and causal AI. These papers inspired com- putational efforts to improve trust in AI models. For example, to enable explainability in composition to provide confidence in model usage, IBM released the AIX-360 toolkit [39], [11], with multiple explainer methods capable of providing different types of model expla- nations, including local, global, and post-hoc explanations. At the same time, there have been user studies [8], [23], [67], [20] on the explanation types that users require and the questions that they want to be addressed, illustrating that user-centered explainability is question-driven, conversational and contextual. Inspired by these studies, we reviewed the social sciences and computer science litera- ture [19] regarding explanations and cataloged several user-centered explanation types that address different types of user questions and the associated informational needs. We con- ducted expert panel studies with clinicians [36] to understand which of these explanation types were most helpful in guideline-based care. We found that clinicians prefer a combi- nation of holistic, scientific, and question-driven explanations connected to broader medical"}, {"title": "Methods", "content": "In the EO, we capture the attributes that explanations build upon, such as a 'system recommendation' and \u2018knowledge,' and model their interactions with the \u2018question' (s) they address. We can broadly categorize explanation dependencies into the system, user, and interface attributes as seen in Fig. 3.1. These categorizations help us cover a broad space of attributes that explanations directly or indirectly interact with. User Attributes: User attributes are the concepts that are related to a 'user' who is consuming an explanation. These include the 'question' that the user is asking, 'user characteristic's that describe the user, and the user's \u2018situation'. Explanations that the user is consuming are modeled to address the user's \u2018question,' and may also take into account factors such as the user's 'situation' or their 'user characteristic' such as their education and location. System Attributes: System attributes encapsulate the concepts surrounding the AI"}, {"title": "Use Cases Represented Using the EO Model", "content": "To demonstrate the utility of the EO as a general-purpose ontology to represent expla- nations, we show how the EO's model can be used to compose explanations in five different use cases spanning food, healthcare, and finance domains (Tab. 3.9). All of these use cases"}, {"title": "Drug Recommendation", "content": "We demonstrate the use of EO in the design and operations of an AI system to sup- port treatment decisions in the care of patients with diabetes. We previously conducted a two-part user-centered design study that focused on determining which explanations types"}, {"title": "Food Recommendation", "content": "In the food recommendation use case, aimed at recommending foods that fit a person's preferences, dietary constraints, and health goals, we have previously published a customized version of the EO specifically for the food domain, the Food Explanation Ontology [77]. With the updates to the EO, we are now able to support the modeling of contextual and contrastive examples natively in the EO, whose capabilities were previously only in FEO as depicted in [77]. In the food use case, a knowledge base question-answering (QA) system [78] has been run and outputs answers to questions like \u201cWhat should I eat if I am on a keto diet?\u201d However, a standard QA system cannot directly address more complex questions that re- quire a reasoner to be run on the underlying knowledge graph to generate inferred content. For example, questions such as whether or not one can eat a particular recipe, like \"spiced"}, {"title": "Proactive Retention", "content": "In the proactive retention use case3, the objective is to learn the rules for employee retention that could signal to an employing organization whether or not employees are likely to have retention potential. Since these rules involve a deep understanding of the contributing attributes of the employee dataset, a domain expert is best adept at providing these rules. Potentially, a supervised ML method could be run to learn the rules for other unlabeled instances. The AIX-360 toolkit supports a TED Cartesian Explainer algorithm [79] that can learn from rules that are defined against a few cases and predict the rules for others. More specifically, in the proactive retention use case KG, the TED Cartesian Explainer method is defined as an instance of \u2018Providing rationale method' in the EO by virtue of the definition for this 'explanation method' subclass, and that the TED Explainer provides local explanations"}, {"title": "Health Survey Analysis", "content": "The health survey analysis use case4 utilizes the National Health and Nutritional Exam Survey (NHANES) dataset [80]. The objective in this use case entails two \u2018explanation tasks': (1) find the most representative patients for the income questionnaire, and (2) find the responses that are most indicative/representative of the income questionnaire. The Pro- todash method [81], an \u2018Exemplar explanation method,' that finds representative examples from datasets, is run for both these tasks. However, in the second task, an additional data interpretation or summarization method is run to evaluate the prototype patient cases of questionnaires for how well they correlate to responses in the income questionnaire. Hence, when we instantiate the outputs of these two tasks, we use different chains of representations to indicate the dependencies of the explanations of the two questions that are addressed by these tasks. More specifically, this chaining would mean that we define that the 'system rec- ommendation' of the 'summarization' method instance to be dependent on or use as input the 'system recommendations' of \u2018Protodash' instances."}, {"title": "Medical Expenditure", "content": "In the medical expenditure use case5, the objective is to learn the rules for the demo- graphic and socio-economic factors that impact the medical expenditure patterns of indi- viduals. Hence, from the description itself, we can infer that in this use case the rules are attempting to understand the patterns in the Medical Expenditure Panel Survey (MEPS) dataset or the general behavior of the prediction models being applied on the dataset, as opposed to attempting to understand why a particular decision was made. This use case involves the use of global explanation methods from the AIX-360 toolkit [39], [11], includ- ing the Boolean Rule Column Generator and Linear Rule Regression (LRR) methods. The 'explanation method' instances, in this case, produce explanations that are dependent on 'system recommendations,' which rely on the entire dataset itself. Therefore, system design- ers should link the \u2018system recommendations' to the dataset. We achieve this association in our MEPS KG by representing the 'dataset' instance as an input of the LRR and BRCG methods. Finally, as can be seen from Tab. 3.9, the reasoner can only infer data explanations (refer to Fig. 3.8) from the MEPS KG instances, as the explanations are dependent on the rules identified for patterns in the dataset. Hence, in a use case such as this, wherein the explanations are mainly dependent on the dataset and the patterns within them, if a system designer were to appropriately link the system recommendation that the explanation is based on to the entire dataset or a component of the same (i.e., a column, row or cell), a reasoner run on the use case KG can populate data explanations of the explanation representations. Such data explanations can be helpful to understand aspects of bias and coverage in the data, which can signal to system designers and users whether their dataset is serving its"}, {"title": "Credit Approval", "content": "In the credit approval use case6, there are several objectives depending on the 'user' persona, including to enable data scientists to familiarize themselves with the factors that impact the credit approval outcome, for loan officers to identify prototypical cases of credit approved owners, and for customers to understand what patterns in their profile contribute the most towards their credit approval. The analyses are conducted on the FICO HELOC dataset7, which contains \"anonymized information about Home Equity Line Of Credit (HE- LOC) applications made by real homeowners\" [82]. We run three explanation methods: (1) BRCG and LRR to provide data scientists with the rules for credit approval ratings, (2) Pro- todash to provide loan officers prototypical customer cases, and (3) Contrastive Explanation Method (CEM) to provide customers with explanations to what the minimally sufficient factors in achieving good credit ('fact') are and the factors which, if changed, would change their credit ('foil'). In the medical expenditure use case, we have already shown that a system designer dealing with outputs of rule-based methods, such as BRCG and LRR, can represent the explanations dependent on \u2018system recommendations' generated by the methods and, particularly in this use case, define the FICO HELOC dataset as input for these methods. In the case of representing the identified prototypical credit approval customers, system de- signers can seek inspiration from the health survey analysis case and similarly represent the customer cases as instances of \u2018system recommendations' and as inputs to an \u2018explanation task.' Finally, for the outputs of CEM (see Fig. 3.9), we represent the factors that need to be minimally present for credit approval as \u2018facts' in support of an explanation, and the factors which, if present, flip the decision as \u2018foils' of an explanation. Such a representation would align with our definition of restrictions against the contrastive explanation class. In addition, we create three different user instances for the data scientist, loan officer, and cus- tomer, respectively, and further associate the questions (see credit approval row of Tab. 3.9) that each of them asks via the properties supported in the EO (Fig. 3.1). When a reasoner"}, {"title": "Resource Contributions", "content": "We contribute the following publicly available artifacts: our expanded Explanation Ontology with the logical formalizations of the different explanation types and SPARQL queries to evaluate the competency questions, along with the applicable documentation, all available on our resource website. On our open-source Github repository, we also release our KG files (and the inferred versions too), for the five use cases (Sec. 3.3) described in this chapter. These resources, listed in Table 3.10, are useful for anyone interested in building explanation facilities into their systems. The ontology has been made available as an open-source artifact under the Apache 2.0 license [83] and we maintain all our artifacts on our Github repository. We also maintain a persistent URL for our ontology, hosted on the PURL service. All the relevant links are listed below in Tab. 3.10."}, {"title": "Evaluation", "content": "Our evaluation is inspired by ontology evaluation techniques proposed in Muhammad et al.'s comprehensive ontology evaluation techniques review paper [84]. They introduce an ontology evaluation taxonomy that combines evaluation techniques that each reveal differ- ent perspectives of the ontology, such as application-based, metric-based, user-based, and logic/rule-based evaluation techniques. These evaluation techniques in Muhammad et al.'s taxonomy are a collection of techniques proposed by four different well-cited papers, includ- ing Obrst et al. [85], Duque-Ramos et al., Tartir et al. [86] and Brank et al. [87]. From this taxonomy, we evaluate our ontology by addressing a representative range of competency questions that illustrate the task-based and application-based capabilities of the EO. We also"}, {"title": "Evolution-based Evaluation", "content": "We also evaluate the additions to the EO model since its first iteration described in Chari et al. [20] using the evolution-based evaluation method mentioned in Muhammad et al.'s taxonomy [84]. However, as analyzed by Muhammad et al. [84], it is hard to quantify the evolution-based evaluation technique of Tartir et al. [86] for knowledge gain provided by the updates to the ontology model. From a qualitative assessment, we find that the additions to the EO model helped us better represent capabilities including : \u2022 Capture more granular representations of \u2018AI methods' and their interactions with the explanation types, and support more ways to generate explanations. \u2022 Introduce characteristics at various strategic attributes that contribute to explanations (e.g., at the system, user, and object classes), which provide the flexibility to define characteristics at multiple levels and allow for better considering explanation types through the restriction of equivalent classes. For example, we could better represent restrictions against the 'contextual knowledge' class with the broader characteristic"}, {"title": "Task-based Evaluation", "content": "With the increasing demand to support explainability as a feature of user-facing ap- plications, thus improving uptake and usability of AI and ML-enabled applications [89], [8], [19], [37], it is crucial for system designers to understand how to support the kinds of expla- nations needed to address end user needs. An evolving landscape of the explanations, goals, and methods that support them, complicates the task, but querying the EO can help answer such questions in a standalone format. For the task-based abilities, we aim to showcase how the EO can provide \u201chuman ability to formulate queries using the query language provided by the ontology\" [85] and \"the degree of explanation capability offered by the system, the coverage of the ontology in terms of the degree of reuse across domains\" [85]. We detail some of the support that the EO can provide to help system designers understand the main entities interacting with explanations in Tab. 3.11. The support that we illustrate includes querying the \u2018AI Method' and \u2018AI Task' that generates the explanation, the example questions that different explanation types can address, and the more nuanced parts of explanation types, such as their components and when they can be generated. The table also shows a set of competency questions and answers that are retrieved from the EO. For answers in this table, we use, for better understanding, simpler descriptions than the results returned by the SPARQL query. The full set of results can be browsed through our resource websites.\nWe split the questions across two settings, including during system design (questions 1 - 3 in Tab. 3.11) when a system designer is planning for what explanation methods and types of support are needed, based on the user and business requirements. The other setting, during system analysis (questions 4 - 6 in Tab. 3.11 and Tab. 3.12), when they are trying to understand what explanation types can be supported given system outputs at"}, {"title": "Application-based Evaluation", "content": "When system designers represent use case specific content, they often need to commu- nicate these representations to other teams, including interface designers who support these use case KGs on UIs, or system developers who want to ensure that they correctly capture system outputs. Hence, in such scenarios, the system designers would need to provide results concerning the domain-specific content in their use case KGs, which could span questions like \"What entities are contained in the contrastive explanation?\u201d to more specific questions about particular objects in the use case, such as \"What is the outcome for employee 1?\u201d Through the EO, we enable the system designer to provide application-specific details about the explanations supported in their use cases, that improve the presentation of the \"output of the applications\" [87]. Some examples of questions that we can handle for the use cases we support from Sec. 3.3, are shown in Tab. 3.13. In these instances from Tab. 3.13, we query the use case KGs for domain-specific content by leveraging the properties of the EO model, as defined between the entities that contribute to explanation instances in these KGs. Some examples of domain-specific content that can be queried (see Tab. 3.13) include questions like, 'what are the facts contributing to a contrastive explanation (Q7)', 'what is"}, {"title": "Discussion", "content": "Here we discuss different aspects of the EO, including desired features and design choices, the relevance of results and limitations, and future outlook. How is EO considered to be general-purpose? We have described a general-purpose and mid-level ontology, the EO, that can be leveraged to represent user-centered explanations in domain applications. In the EO, we encode the attributes that contribute to explanations in a semantic representation as an ontology, and by doing so, the EO can be used to structure explanations based on linkages to the needs the explanations support and the method chains"}, {"title": "Conclusion", "content": "We have presented a significantly expanded explanation ontology that can serve as a resource for composing explanations from contributing components of the system, interface, and user- attributes. We have modeled the mid-level ontology to be used as a cross-domain"}, {"title": "Overview", "content": "In recent years, efforts to describe and formalize explanations [92], [20] have identi- fied various dimensions and types for it. Specifically, 'contextual explanations' [34], [20] hold great promise to satisfy clinical needs and can improve the adoption of AI methods among clinical workflows. Risk prediction is one of the most important tasks in clinical decision making, and an increasingly important in view of the move toward personalized medicine. [93], [94]. To interpret risk scores, clinicians often consult evidence from different levels of the scientific pyramid [95] to lookup associations that might impact the patient's treatment or future trajectory. For example, questions like those in Tab. 4.1, are often asked by clinicians when they are trying to understand or use AI model predictions in their practice. Additional contextual information, such as answers to these questions, can help clinicians interpret and trust predictions to take actions. However, current work in risk prediction has often narrowly focused on improving model's accuracy, ignoring the aforementioned needs. Interestingly, several researchers have posited contextual explanations [17], [96], [20], that go beyond post-hoc model explanations to frame the predictions in the context of the applied setting and decisions being made. However, the feasibility of extracting such contextual ex- planations and the added benefit in an end-to-end setting of clinical relevance has not been studied and forms the focus of this paper. Specifically, we consider how to derive and support contextual explanations from authoritative domain knowledge sources, not already consid- ered by prediction models, that clinicians would typically use to reason through decisions presented to them when dealing with recommendations from learning health systems."}, {"title": "Selected Contextual Entities of Interest", "content": "To support the goal of providing contextual, clinically relevant, user-centered expla- nations, in consultation with a medical expert on our team, we identified three entities of interest to provide contextual explanations about predicting the risk of CKD among T2DM patients. Fig. 4.1 shows an example of contextual explanation that can answer a clinician's question around patient management. It can be seen that such explanations are usually composed of multiple entities and from multiple sources. In general, we identified and sub- sequently focused on extracting the following contexts: \u2022 Contextualizing the patient by connecting their clinical history and indicators to treatments typically recommended for such patients, according to CPGs. \u2022 Contextualizing risk predictions for the patient in terms of the prediction's impact on decisions, based on general norms of practice concerning potential complications, as evident from guidelines and other domain knowledge, including medical ontologies. \u2022 Contextualizing details of algorithmic, post-hoc explanations, such as connecting features that were the most important to other information based on their potential medical signifi- cance, such as through connections to physiological pathways and CPGs. In Fig. 4.2, some examples of contexts that we support around the three entities of interest can be seen in the risk prediction setting. Also seen in the figure are the pathways in which responses providing context could borrow from different domain knowledge sources"}, {"title": "Data Sources", "content": "To conduct our real-world study, we focus on two specific sources of data as described below."}, {"title": "Patient Data", "content": "We conduct our analysis on and retrieve patient data from the claims sub-component of the Limited IBM MarketScan Explorys Claims-EMR Data Set (LCED) [100], covering both administrative claims and EHR data of over 5 million commercially insured patients between 2013 and 2017. Medical diagnoses are encoded using International Classification of Diseases (ICD) codes. We selected only those T2DM patients (with ICD9 codes 250.*0, 250.*2, 362.0, and ICD10 code E11) that satisfied the following criteria as our cohort. Only T2DM patients with the following criteria are included: \u2022 have had two or more visits with T2DM diagnosis, \u2022 were enrolled continuously for 12 months prior to T2DM diagnosis, \u2022 number of visits for T2DM is greater than those for other forms of diabetes such as T1D, and \u2022 age at the initial T2DM diagnosis is between 19-64 years. Among T2DM patients, we use the first diagnosis of chronic kidney disease (CKD) (ICD10 N18 or ICD9 585.*, 403.*) after the initial diagnosis of T2DM as the outcome to predict. At the time of the first T2DM diagnosis, we predict the risk that the patient develops CKD within 1 year using Clinical Classifications Software (CCS) codes, age group and sex as features for the predictive model."}, {"title": "Clinical Practice Guidelines", "content": "Clinical Practice Guidelines are position statements published by a board of experts in different disease areas [101]. These guidelines are updated often, latest summaries of up- dated evidence in the disease areas, and follow the highest standards of evidence appraisal (e.g., Grading of Recommendations, Assessment, Development and Evaluations (GRADE) evidence schemes 11). Further, the guidelines are written to be comprehensive sources cover- ing different aspects of treatment, management, and assessment of the disease and are often regarded as first-line lookup sources for clinicians and primary care physicians [102, 101]."}, {"title": "Methods", "content": "To study the problem of risk prediction of CKD among T2DM patients, we created an end-to-end AI enabled system to provide relevant contextual explanations from authoritative literature sources such as guidelines. Fig. 4.4 shows a conceptual overview of the components of this system. In general, to extract contextual explanations around our three identified entities of"}]}