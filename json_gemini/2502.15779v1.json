{"title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer", "authors": ["Euntae Choi", "Sumin Song", "Woosang Lim", "Sungjoo Yoo"], "abstract": "We propose Rotate, Clip, and Partition (RCP), a Quantization-Aware Training (QAT) approach that first realizes extreme compression of LLMs with W2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV-cache) configuration. RCP integrates recent rotation techniques with a novel non-uniform weight quantizer design, by quantitatively analyzing the impact of random rotation on 2-bit weight quantization. Our weight quantizer features Learnable Direct Partitioning (LDP), which introduces learnable parameters to directly learn non-uniform intervals jointly with LLM weights. We also present a specialized GPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP can compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and 5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging mobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and MetaMath-7B with no critical problems such as convergence failure and repetition. Code will be made available at blind_review.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have made significant advancements, but their growing size and resource demands create challenges for deployment across data centers and mobile devices. To address these constraints, extensive research efforts have focused on improving the efficiency of LLM serving through various strategies. Quantization, one of the various methods, has emerged as a particularly straightforward and effective method for reducing memory consumption and inference costs without severely compromising accuracy. By lowering the numerical precision of parameters and intermediate representations, quantization leverages hardware capabilities such as half-precision and integer tensor cores.\nPost-Training Quantization (PTQ) is a quantization technique that performs well up to levels like W4A4\u00b9, maintaining acceptable accuracy compared to original models. However, as the bit-width decreases, the representable information becomes insufficient, making it difficult to address challenges such as salient weights, outliers, and the quantization of activations and the KV-cache. To address these issues, rotation techniques have proven effective at W4A4KV4. Nevertheless, extending these methods to even lower bit widths, such as W2 or W3, remains insufficient, as the severe information bottleneck cannot be overcome with a uniform quantization and PTQ, ultimately resulting in a loss of generation quality.\nIn this work, we propose a Quantization-Aware Training (QAT) approach called Rotate, Clip, and Partition (RCP), which integrates rotation techniques with non-uniform quantization strategies to achieve extreme compression of LLMs, specifically realizing W2A4KV4 configurations. Our method jointly optimizes quantization parameters and model weights by systematically incorporating rotation and non-uniform group quantization. We"}, {"title": "Preliminaries", "content": "2.1 Random Rotation for LLM Quantization\nQuaRot proposed to apply random rotations while keeping the computational invariance suggested in SliceGPT. Random rotation suppresses activation outliers and helps quantization, successfully achieving W4A4KV4 with minimal performance loss.\nAs shown in Fig. 4, $R_1$ rotates all transformer decoder layers' input and output activations and its inverse ($R_1^T$) is usually fused into the neighboring model weights. $R_2$ and $R_4$ both require online rotation during inference as they target the intermediate activations in the MHA and FFN layers, respectively. $R_2$ is decomposed into two orthogonal matrices: one with head dimension size ($R_H$), applied during the V projection, and another with the number of heads ($R_{H'}$), applied to the self-attention activation. Its transpose ($R_2^T$) is applied as a whole to the out-projection in the self-attention layers. $R_3$ rotates query (Q) and key (K) vectors after RoPE, ensuring that the KV-cache can be compressed without affecting the self-attention output.\n2.2 Asymmetric Quantization and Clipping\nAsymmetric quantization uses min/max of data to match data's asymmetric distribution. This approach can offer higher quantization accuracy than symmetric quantization. The asymmetric quantization equation can be formulated as follows:\n$W_q = clamp(\\frac{[\\frac{W}{h}+z], 0, 2^N - 1),$\n$where \\  h=\\frac{max(W) - min(W)}{2^N - 1}$\n$z = - \\frac{min(W)}{h}$\n$\\tag{1}$\nClipping is an optimization technique that truncates outlier values to a predetermined range before quantization. It improves quantization precision within the compressed range, preserving performance while increasing compression ratios. Our proposed method, RCP is based on the learnable weight clipping (LWC) , an asymmetric quantization to obtain asymmetric clipping values that minimize quantization error"}, {"title": "Observation: Rotation Sharpens Weight Distribution", "content": "As shown in Fig. 1, simply applying rotation techniques before LLM quantization can't guarantee the performance in extremely low-bit scenarios like W2A4. Building on SmoothQuant's insights, outlier suppression methods improve quantization possibility in the activation space while accepting modest weight space compromises. But recent rotation based PTQ techniques predominantly emphasize the advantages in the activation space, not the disadvantages in the weight space. While rotation-based PTQ techniques work well for moderate quantization up to W4, at W2, the quantization possibility significantly drops.\nIn this section, we analyze quantization possibility after random Hadamard transformation in both weight and activation spaces in LLaMA-2 7B model. Kurtosis, which measures the fourth standardized moment of a distribution, serves as a key indicator for evaluating outlier frequency and concentration that intensify the difficulties in quantization. We selected the 15th key projection module for analysis because it showed one of the highest mean squared errors (MSE) when comparing the distributions before and after transformation among all modules. We analyze kurtosis of weight and input activation in group size of 128 following their channels to fully consider the effects of groupwise quantization configuration used in most quantization methods.\nOur analysis in Fig. 3b reveals that rotation effectively distributes activation values along their channel direction, similar to methods like SliceGPT, QuaRot and PrefixQuant . The original space shows infinite kurtosis due to extreme outliers, while the rotated space shows predominantly near-zero values. From the perspective of per-channel uniform quantization in the activation, the transformation reduces quantization error compared to the original space.\nConversely, our analysis in Fig. 3a shows contrasting circumstances. After going through essential processes such as LayerNorm Fusion and Random Hadamard Transformation, an initially platykurtic distribution with mean kurtosis near 1 becomes more leptokurtic with mean kurtosis near 3.75. These observations indicate that the weight distribution deviates further from the uniform distribution, leading to significant quantization errors when using W2 uniform quantizers, as most weights cluster in two center quantization bins. This finding motivates the development of non-uniform quantizers to optimize bin width distributions. This analysis also explains why FP3 variants outperform INT3 quantization in W3 regime, as NF3's approach better handles bell-shaped weight distributions, as demonstrated in AFPQ."}, {"title": "Methodology", "content": "We propose a non-linear weight quantization (Section 4.1), an efficient dequantization for non-linear weight partitions (Section 4.2), and a GPU inference kernel for extreme low-bit weights (Section 4.4)."}, {"title": "Differentiable Non-uniform INT2 Group Quantizer", "content": "4.1 Differentiable Non-uniform INT2 Group Quantizer\nRotation-aware Clipping Initialization We initialize clipping parameters in a rotation-aware manner as follows:\n$\\underset{\\beta,\\gamma}{argmin} \\ ||Q(W_R)X_R - W_RX_R||_2$\n$=\\tag{3}$\nwhere $W_R = R_{front}f(W, \\beta, \\gamma) R_{rear}$ is the weight matrix obtained by applying appropriate rotations after the clipping $f$ in Eqn. 2. For instance, $R_{front}$ and $R_{rear}$ are set to $R_1$ and $I$ for up and gate projections, respectively. Rotation configuration for the other types of weights can be found on the left side of Fig. 4.\nLearnable Direct Partitioning The key component of our proposed RCP is a weight quantizer module named learnable direct partitioning (LDP). In order to realize asymmetric weight quantization, we determine value range by LWC and the partitions within the value range by LDP. LDP starts by normalizing the weight W by a dynamic range $h = \\sigma(\\gamma)max(W) - \\sigma(\\beta)min(W)$ determined by the LWC. In LDP, two partitioning variables $s_1$ and $s_2$ are introduced per weight group\u00b2, which split the range h into three subsections.\n$p_1 = \\sigma(s_1), p_2 = (1 - p_1)\\sigma(s_2),$\n$p_3 = (1 - p_1)(1 - p_2)$\n$\\tag{4}$\nAs shown in Fig. 5 and Eqn. 4, $s_1$ defines the portion of the first partition $p_1$ in the whole dynamic range h. $s_2$ defines the portion of the second partition $p_2$ from the remaining range $(1 - p_1)h$. The third one is trivially calculated. This formulation guarantees that i) the dynamic range h is seamlessly filled out, ii) each portion is constrained between 0 and 100% via the sigmoid re-parametrization, and iii) no matter how the parameters are updated, the order of the partitions stays the same. The partitioning parameters are initialized as $s_1 = \\sigma^{-1}(1/3)$ and $s_2 = \\sigma^{-1}(1/2) = 0$ to evenly split the quantization range in the beginning. The AWQ-styled grid search can also be used to find the optimal partition widths; however, the computational burden will grow exponentially since we have to iterate over a four-dimensional loop (two for LWC and two for LDP).\nThe quantization process of our LDP can then be derived as follows:\n$W_q = \\frac{W}{h} = u(\\frac{W}{h} - t_1)+u(\\frac{W}{h}-t_2)+u(\\frac{W}{h}-t_3)$\n$\\tag{5}$\nwhere $u(x)$ is the step function and transition point $t_i$ (i.e., the right edge of each quantization bin) is set to the center of each partition, computed as $t_1 = p_1/2$ and $t_i = t_{i-1} + (p_{i-1} + p_i)/2$"}, {"title": "Dequantization for Non-Linear Weight Partitions", "content": "4.2 Dequantization for Non-Linear Weight Partitions\nUnlike the usual uniform quantization scheme, mapping the quantized weights back to real values is not trivial in NU quantizers as the design space of NU dequantization method is large and the inference pipeline is directly affected. In NU2U , the quantized weight $W_q$ is simply de-quantized to a uniform grid. Similarly, LLT trains a learnable lookup table for weight and activation to adjust the quantization bin widths but keeps the dequantization the same as other common uniform quantizers.\nThis design choice has an obvious advantage: Inference can be done on existing hardware without any modification. However, we propose non-uniform dequantization as described in Eqn. 6, based on our observation that uniform dequantization can lead to performance drop under extremely low bit configurations, especially on smaller models.\n$W_{deq} = \\sigma(\\beta)min(W)$\n$+$\\frac{W}{h}$u(\\frac{W}{h} - t_1)(w_1 - w_0)\n$+\\frac{W}{h}$u(\\frac{W}{h} - t_2)(w_2 - w_1)\n$+\\frac{W}{h}$u(\\frac{W}{h} - t_3)(w_3 - w_2)$$\\tag{6}$\nThe procedure is designed upon Eqn. 5 with several modifications. The value range is shifted and scaled from [0, 1] to [$\\sigma(\\beta)min(W), \\sigma(\\gamma)max(W)$]. The dequantization grid $w_i$ is defined in Eqn. 7; the first and last values are the minimum and maximum values of the normalized weight and the middle values are set to the center of the two consecutive transition points.\n$\\tag{7}$\nThe dequantization LUT for a quantization unit (a weight group of size 128 in this paper's experiments) can be pre-computed without any runtime overhead as follows:\n$W = \\{ \\hat{W_0}, \\hat{W_1}, \\hat{W_2}, \\hat{W_3} \\}$\nwhere $\\hat{W_i} = \\sigma(\\beta)min(W) + h \\cdot w_i$.\n$\\tag{8}$"}, {"title": "An NF3 Variant of LDP", "content": "4.3 An NF3 Variant of LDP\nWe apply not only 2-bit weight quantization but also 3-bit quantization using the asymmetric NF format of AFPQ where separate scale values are computed for the negative and positive weights ($s_{neg} = max(|W_{neg}|), s_{pos} = max(W_{pos}))$. Although shown effective, such NF3 quantizer can lead to suboptimal performance when the distribution is not zero-centered. Therefore, we make a further improvement by applying the proposed LDP to this situation.\nThe idea is to employ the same learnable clipping parameters ($\\beta, \\gamma$) to obtain the quantization range h and one partitioning parameter $\\delta_1$ to express the learnable center point as $c = \\sigma(\\beta)min(W) + h \\cdot \\sigma(\\delta_1)$. Then, the two scale values are updated as follows:\n$s_{neg} = |c - \\sigma(\\beta)min(W)|,$\n$s_{pos} = |\\sigma(\\gamma)max(W) - c|,$\\tag{9}$\nand the quantization process is derived as follows:\n$W_q = \\{\\frac{W-c}{s_{pos}}, if W > c\\frac{W-c}{s_{neg}}, otherwise.$\n$\\tag{10}$\nThe dequantization is done simply by multiplying the scales to $W_q$ and adding c."}, {"title": "W2A4 Look-up Table (LUT) Inference", "content": "4.4 W2A4 Look-up Table (LUT) Inference\nDesigning the inference pipeline of non-uniform W2A4-quantized models poses a big challenge. First, efficient INT tensor cores cannot be utilized since accumulating the multiplication results in INT quantized space makes it impossible to de-quantize the weights back to correct non-uniform real values in the LUT W. Second, both weights and activations must undergo online dequantization to support dynamic quantization, which adds a large amount of computation overhead.\nTherefore, we focus on designing GEMV kernel for LUT decoding predominantly bounded by memory bandwidth, which is ideal for featuring the advantage of extreme W2A4KV4 compression. We report our early exploration on GEMM design in Section A.5.\nKernel Arguments and Block Tiling We define the input channel dimension as C, the output channel dimension as H, and the number of groups per channel as N. The quantized activation tensor $X_q$ has a shape of 1 x C/2 and is INT8, with each element holding two INT4 activation elements. The"}, {"title": "Experiments", "content": "5.1 Experimental Settings\nModels and Tasks We evaluate RCP on LLaMA-1 7B, LLaMA-2 7B, LLaMA-3 (1B, 3B, 8B). Our evaluation of RCP was carried out on PIQA , HellaSwag, WinoGrande, ARC-c and MMLU . We use LLM-Humaneval-Benchmarks and GSM8K for reasoning tasks evaluation. We also report the perplexity score on WikiText2 for our evaluation.\nTraining Data For a fair comparison with our baseline, we use the instruction-tuning data from Alpaca and the training set of WikiText2 for general language tasks. For understanding and generating code, we use Evol-Instruct-Code\u00b3"}, {"title": "Results", "content": "5.2 Results\nLanguage Modeling Tasks The results are summarized in Table 1. From the perspective of general language tasks, our method demonstrates the ability to quantize activations and KV-cache under the W2 settings to 4-bit, which was previously unattainable using existing QAT methods. The application of rotation effectively addresses the outlier issues, a common bottleneck in quantization, enabling stable performance even in extremely low-bit quantization scenarios. Furthermore, the addition of LDP not only improves performance on general language tasks but also enhances the accuracy of zero/few shot tasks, which were not adequately addressed by rotation alone. In case of LLaMA-2 7B W2A4KV4, a performance degradation is observed when using rotation only compared to the baseline. However, by incorporating LDP, consistent performance improvements were achieved.\nReasoning Tasks The results of the reasoning tasks are summarized in Table 2. We evaluate reasoning capabilities in the domains of coding and mathematics.\nFor the coding domain-specific model, Wizard-Coder, BitDistiller failed to offer the functional quantized models in both W3 and W2 settings. In our method, applying rotation alone was not effective in W2 settings and recovered some output quality in W3 settings. By incorporating LDP, we achieved up to a threefold improvement in performance, with accuracy increasing from 6.09% to 23.20% under the W2A4KV4 configuration. As shown in Fig. 8 with the application of LDP, we were able to produce logically correct code outputs and eliminate repetition of meaningless code generation.\nFor the mathematical reasoning model, MetaMath , the baseline BitDistiller and ours without LDP failed to offer functional quantized models while ours with LDP could produce working quantized models. These results highlight the critical role of LDP in enabling proper task performance for reasoning models under extreme low-bit quantization. The output comparison for this task is summarized in Fig. 2."}, {"title": "Ablation Studies", "content": "5.3 Ablation Studies\nImpact of RCP Components As shown in Table 5, we conducted an ablation study to analyze the impact of removing each component of RCP on model performance. In 4-bit activation quantization, addressing the outliers in activations was crucial, and this was effectively resolved using rotation, which led to the largest performance gain compared to the baseline. This demonstrates that rotation is a viable solution when quantizing activations to low bit-width.\nHowever, we found that the narrow weight distribution caused by rotation hindered successful training of LWC. Specifically, when examining the training process with rotation applied during LWC training, the training loss curve exhibited instability. The combination of low bit-width quantization challenges and the difficulty in finding an optimal LWC required training stabilization, which was achieved by LDP. LDP reduced PPL from 10.59 to 8.31, demonstrating a clear advantage."}, {"title": "Conclusion", "content": "RCP enables weights to be quantized to extreme low-bit precision through learnable non-uniform quantization while harmonizing with rotation to optimize both activations and KV-cache to 4-bit. RCP has achieved the first W2A4KV4 configuration and implemented optimized kernels for inference, facilitating LLM serving even in resource-constrained environments."}, {"title": "Limitations", "content": "Although our proposed RCP first enables challenging W2A4KV4 quantization of commonly used LLM models, we report key limitations of our work.\nFirst, the online rotation operators (R2 through R4) inevitably introduce additional latency for training and evaluation. Custom CUDA kernels or FlashAttention3 can minimize such speed-down, however, it might not be a viable option for many edge application scenarios where no hardware support for fast Hadamard transform is available.\nSecond, RCP requires heavier hyperparameter tuning than BitDistiller since rotation tends to make the model weights more sensitive to the choice of learning rate. This can be prohibitive when a user is under a strict budget limit.\nIn future work, we could explore applying an optimized rotation matrix that achieves comparable performance to Cayley-optimized rotation matrices used in SpinQuant while maintaining similar computational costs to the Random Hadamard rotation matrices employed in QuaRot ."}, {"title": "Related Works", "content": "A.1 Related Works\nPTQ and QAT GPTQ introduced an accurate post-training quantization (PTQ) method based on approximate second-order information that enables weight-only quantization down to 3-4 bits through block-wise reconstruction.\nSmoothQuant proposed smoothing activation outliers by offline migrating quantization difficulty from activations to weights through equivalent transformation, enabling accurate 8-bit weight-activation quantization. AWQ built upon SmoothQuant's equivalent transformation concept but introduced activation-aware channel-wise scaling to protect salient weights during weight-only quantization. OmniQuant enhanced quantization by introducing learnable weight clipping and equivalent transformation parameters that are jointly optimized through block-wise reconstruction.\nLLM-QAT was the first to explore quantization-aware training (QAT) for LLMs using data-free knowledge distillation from the full-precision model to guide low-bit quantization. Bit-Distiller improved upon LLM-QAT by introducing a self-distillation framework with confidence-aware KL divergence to enable sub-4-bit quantization while maintaining efficiency. EfficientQAT made QAT more practical by introducing block-wise training of all parameters followed by end-to-end training of quantization parameters.\nRotation QuaRot introduced a rotation-based approach using Hadamard transformations to eliminate outliers in activations and KV-cache, enabling end-to-end 4-bit quantization including weights, activations and KV-cache. SpinQuant enhanced this rotation-based approach by learning optimal rotation matrices instead of using random ones.\nNon-uniform Quantization PACT introduced a learnable clipping parameter for activation quantization during training to help preserve model accuracy. SqueezeLLM took a different direction by focusing on identifying and extracting outlier values into a sparse format while quantizing the remaining dense values. NU2U proposed learning flexible non-uniform input thresholds while maintaining uniform output levels to balance quantization accuracy with hardware efficiency.\nServing Optimization Atom first introduced W4A4 quantization for LLM serving but faced performance challenges from dequantization overhead. QServe addressed the challenges by introducing W4A8KV4 quantization with progressive group quantization\nFLUTE focused on developing efficient GPU kernels for flexible lookup table-based quantization methods that can support arbitrary bit widths including 3-bit and 4-bit quantization."}, {"title": "Reasoning Task Example: HumanEval", "content": "A.2 Reasoning Task Example: HumanEval\nWe evaluate the capability of the WizardCoder 7B model to generate solutions for coding problems. The results are presented in Fig. 8. The orange box in Fig. 8 represent the model output after applying rotation and quantizing the weights to W2A4KV4 using a uniform asymmetric quantizer. Under uniform quantization, it is evident that the model fails to perform logical generation tasks even applying rotation; it merely produces the structural template of code without generating functionality correct code. In contrast, the green box shows the results when the weights are quantized to W2A4KV4 using LDP. Unlike the uniform quantizer, the LDP approach yields code that not only adheres faithfully to the given instructions and generates a functionality correct algorithm, but also provides detailed explanatory comments. While perplexity on standard language modeling tasks did not reveal significant differences between the two cases, these findings suggest that LDP plays a crucial role in enabling logical reasoning tasks under extreme low-bit quantization."}, {"title": "Implementation Details", "content": "A.3 Implementation Details\nAll model parameters are in BF16 format throughout training and evaluation since we observe overflow in the hidden activation of the last two FFNS on several models set to FP16.\nIn existing rotation-based PTQ methods , rotations are done in FP32 to avoid precision issues. However, this leads to computational overhead due to a large number of typecasting. When fusing rotations to model weights, they are temporarily promoted to FP32, multiplied by an appropriate rotation matrix, and then demoted back to their original precision. For online rotations ($R_2$, $R_3$, and $R_4$), all tensors are processed in BF16."}, {"title": "More Ablation Studies", "content": "A.4 More Ablation Studies\nFactorized Rotation In our algorithm, rotation serves as a pre-conditioning tool for reducing outliers in activation and KV-cache. All rotations except the matrices that should be applied online ($R_3$ and $R_4$) are fused into the corresponding model weight at the beginning of the QAT process. This means their orthogonality is not guaranteed during backpropagation steps with AdamW optimizer.\nWe investigate the impact of preserving the orthogonality of the rotations by modifying the LLaMA-2 model implementation to apply all rotation operators online while freezing the rotation matrices. Table 7 presents the results. Applying factorized rotation prevents the fusion of the rotation matrix into the weight tensor, resulting in an increase in the number of intermediate tensors (rotation matrix and intermediate activation), which significantly raises VRAM requirements. For instance, applying only R\u2081 needs to reduce the training batch size from 8 to 1. Under the condition of maintaining an equal total number of tokens processed by the model, we compared the performance of W2A16KV16 with only R\u2081 applied. The perplexity of BitDistiller with R\u2081 fused was 7.6, whereas applying QAT with factorized rotation resulted in a PPL of 12.5. This indicates that performing weight updates through QAT while preserving R1 orthogonality hinders QAT optimization. This is because the factorization constrains the weight updates to a restricted space defined by the factorized condition, requiring the backpropagation process to maintain within this space. This limitation reduces the flexibility of optimization, making it challenging to efficiently adjust the weights. Consequently, this leads to suboptimal training dynamics and ultimately results in degraded model performance. Furthermore, extending factorization to R2 and R4 would lead to an even greater increase in VRAM usage. In contrast, training fused weight effectively alters only the distribution and is analogous to standard LLM training, which is well-known to perform effectively. In summary, given that resource consumption increases while performance degrades, we have decided not to explicitly preserve orthogonality and instead allow the algorithm to handle this aspect.\nLayerwise vs. End-to-end QAT Recent work introduced layerwise QAT , which updates one layer at a time while freezing others, allowing training on a single GPU. We extended this approach by applying rotation but observed significant performance degradation. The main issue stemmed from fuse rotation matrices in the weights; layerwise updates disrupted orthogonality, preventing the activation space from restor-"}, {"title": "GEMM Kernel Design for Non-uniform W2A4 Quantization", "content": "A.5 GEMM Kernel Design for Non-uniform W2A4 Quantization\nIn our initial GEMM implementation, we attempted to leverage the asynchronous copy to perform dequantization and MMA operations while loading quantized weights and activations, which resulted in slower performance compared to half-precision PyTorch kernel (approx. 480\u00b5s versus 330\u00b5s on a single (4,096 \u00d7 4,096) linear layer with 2,048 tokens as input). We suggest two underlying reasons; 1) dequantization requires multiple iterations of shifting, masking, and casting to half-precision instruction, and these are typically expensive on the GPU, further deepening the compute-bound nature of the GEMM problem and 2) packing four quantized weights into a single UINT8 and two quantized activation elements into a single INT8 reduces the width of per-block global memory loads, thereby narrowing the chance for latency hiding. Therefore, we decided to leave the prefill acceleration as future work and instead focus on designing a GEMV kernel to accelerate decoding."}, {"title": "Details and More Results on GEMV", "content": "A.6 Details and More Results on GEMV\nOnline Dequantization and Vectorization Fig. 7 illustrates how the activations and weights are dequantized in our GEMV kernel. For activations, there are two INT4 elements ($X_{hi}$, $X_{low}$) in a packed INT8 $X_q$. For $X_{hi}$, $X_q$ is copied to an INT8 register, and the register is right-shifted by 4 bits with sign-filling. For $X_{low}$, $X_q$ is also copied to an INT8 register, which is left-shifted by 4 bits first to put the sign bit of $X_{low}$ to the MSB and then right-shifted by 4 bits with sign filling. This process is shown in Fig. 7a.\nFor weights, there are four UINT2 elements ($W_{q0}$, $W_{q1}$, $W_{q2}$, $W_{q3}$) in a packed UINT8 $W_q$. $W_q$ is copied to 4 UINT8 registers (for each UINT2 element) that are used as indices to look up the LUT W. For $W_{q0}$, the register is right-shifted by 6"}, {"title": "Information About Use of AI Assistants", "content": "A.7 Information About Use of AI Assistants\nAI assistance was strictly limited to linguistic perspectives, such as grammar and spell checking, and finding synonyms."}]}