{"title": "Enhancing Quantum Security over Federated Learning via Post-Quantum Cryptography", "authors": ["Pingzhi Li", "Tianlong Chen", "Junyu Liu"], "abstract": "Federated learning (FL) has become one of the standard approaches for deploying machine learning models on edge devices, where private training data are distributed across clients, and a shared model is learned by aggregating locally computed updates from each client. While this paradigm enhances communication efficiency by only requiring updates at the end of each training epoch, the transmitted model updates remain vulnerable to malicious tampering, posing risks to the integrity of the global model. Although current digital signature algorithms can protect these communicated model updates, they fail to ensure quantum security in the era of large-scale quan-tum computing. Fortunately, various post-quantum cryptography algorithms have been developed to address this vulnerability, especially the three NIST-standardized algorithms - Dilithium, FALCON, and SPHINCS+. In this work, we empirically investigate the impact of these three NIST-standardized PQC algorithms for digital signatures within the FL procedure, covering a wide range of models, tasks, and FL settings. Our results indicate that Dilithium stands out as the most efficient PQC algorithm for digital signature in federated learning. Additionally, we offer an in-depth discussion of the implications of our findings and potential directions for future research.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern mobile devices have access to vast amounts of data that can be leveraged to train machine learning mod-els, leading to significant improvements in the user experi-ence [7], [21]. For instance, language models can enhance speech recognition [11] and text entry [4] and function as intelligent assistants [10]. However, the data-driven nature of these models often requires continuous user data collection to refine and update the machine-learning models [9]. This chal-lenge becomes even more complex in large-scale distributed systems, such as smartphone networks, which involve millions of users. Federated learning addresses this issue, emerging as a promising machine learning paradigm, where many devices (i.e., clients) collaborate to train a machine learning model while keeping the data on the devices themselves [12], [21]. Various approaches have been developed to safeguard user privacy and ensure data security in the federated learning context. Among the various federated learning variants, one of the most widely used and foundational ones is FedAvg [12]. This approach distributes the training data across the clients and learns a shared model by aggregating locally computed up-dates for each client. During each epoch, every client computes an update to the current global model, which is maintained by a central server, and only this update is communicated at the end of the epoch. Although FedAvg is communication-efficient, the model updates transmitted from clients to the server can be vulnerable to malicious tampering, posing secu-"}, {"title": "II. THREATS TO FEDERATED LEARNING SECURITY", "content": "A. Communication in Federated Learning\na) Federated Learning Problem Definition: Consider a federated learning scenario involving a central server G and M clients $C_1...C_M$. Each client $C_i$ possesses a private training dataset $D_i = {X_i, Y_i}$, which remains accessible only to $C_i$. The goal of the server G is to utilize the data distributed across all clients to learn a global model that minimizes the average loss across the entire dataset $[D_1,..., D_M]$, i.e., $L = \\frac{1}{M}\\sum_{i=1}^{M} CrossEntropy(C_i(X_i), Y_i)$. The most common method to address this problem is FedAvg.\nb) FedAvg Communication: In each round $t$ of FedAvg, each client $C_i$ receives the current global parameters $\\theta_t$ and performs multiple training steps on its local dataset $D_i$, starting from $\\theta_t$. After completing these steps, the client sends the resulting model updates $\\Delta \\theta_i^t$ back to the server. The server then aggregates the updates from all clients into an average $\\Delta\\theta^t = \\frac{1}{M} \\sum_{i=1}^{M} \\Delta \\theta_i^t$ and uses this to update the global model: $\\theta_{t+1} = \\theta_t + \\Delta\\theta^t$. Thus, the only communication content in each round consists of the global model parameters, $\\theta_t$ and each client's model updates $\\Delta \\theta_i^t$.\nB. Poisoning Attacks and Federated Learning\nPoisoning attacks in machine learning refer to the deliberate alteration of training data with the intent of causing the learned model to fail in detecting subsequent attacks [17]. For instance, if the training dataset of a traffic sign recognition system is intentionally corrupted, it could lead autonomous vehicles that rely on the system to identify traffic signs [5] incorrectly. Generally, poisoning attacks can be classified into two categories: model failure poisoning attacks, which aim to render the model unusable, and targeted error poisoning attacks, which seek to induce the model to misclassify a specific label as another target label."}, {"title": "III. METHODOLOGY", "content": "In this section, we introduce the 3 NIST-standardized PQC algorithms for digital signature and how we employ them in federated learning to enhance quantum security.\nA. NIST-Standardized Post-Quantum Cryptography\nIn this part, we briefly introduce the 3 NIST-standardized PQC algorithms, i.e. Dilithium, Falcon, and SPHINCS+. We summarize the key characteristics of them as listed in Table I.\na) Dilithium: CRYSTALS-Dilithium [1] is a lattice-based digital signature scheme built using the Fiat-Shamir heuristic. Its security is based on the hardness of solving the Module Learning With Errors (MLWE) and Module Short Integer Solution (MSIS) problems. Dilithium offers a balanced performance in terms of key and signature size and efficiency in key generation, signing, and verification. It uses the ring $R_q := Z_q[X]/(X^{256} + 1)$, where $q$ is the prime number $2^{23} - 2^{13} + 1$. Dilithium has been implemented on various platforms, including ASIC, FPGA, and RISC-V, showing significant performance improvements over traditional implementations.\nb) Falcon: Falcon\u00b2 is another lattice-based digital sig-nature scheme that follows a \"hash and sign\" paradigm. Its security is based on the difficulty of solving the Short Integer Solution (SIS) problem on NTRU lattices. Falcon stands out for its compact signatures, smaller than Dilithium at comparable security levels, though its public key size is\nB. FL Quantum Security via PQC\nTo enhance the quantum security of federated learning (FL), we integrate PQC digital signature algorithms into the FL com-munication protocol. This integration aims to protect against potential quantum-enabled attacks on the model updates trans-mitted between clients and the central server. We implement this security enhancement using the three NIST-standardized PQC algorithms for digital signatures: Dilithium, Falcon, and SPHINCS+. As defined in Section II-A, let $\\theta_t$ represent the global model parameters at round $t$, and $\\Delta \\theta_i^t$ denote the model updates from client $i$. We modify the standard FL protocol as follows:\n\u2022 Key Generation: At the beginning of the FL process, each client $C_i$ and the central server $G$ generate their respective public-private key pairs using one of the PQC algorithms: $(pk_i, sk_i)$ for client $C_i$, $(pk_G, sk_G)$ for the central server $G$.\n\u2022 Model Distribution: When the server sends the global model $\\theta_t$ to client $C_i$, it signs the message using its private key: $\\sigma_G = sign(sk_G, \\theta_t)$. The client verifies the signature using the server's public key before accepting the model: $verify(pk_G, \\theta_t, \\sigma_G)$.\n\u2022 Update Submission: After computing local updates $\\Delta \\theta_i$, client $C_i$ signs the update before sending it to the server: $\\sigma_i = sign(sk_i, \\Delta \\theta_i)$. The signed update $(\\Delta \\theta_i, \\sigma_i)$ is then sent to the server.\n\u2022 Update Verification: Upon receiving the signed update, the server verifies the signature using the client's public key: $verify(pk_i, \\Delta \\theta_i, \\sigma_i)$. Only verified updates are included in the aggregation process.\n\u2022 Model Aggregation: The server aggregates the verified updates to compute the new global model: $\\theta_{t+1} = \\theta_t + (1/M) \\sum_{i=1}^{M} \\Delta \\theta_i$.\nWe provide the formal algorithm description in Algorithm 1. This protocol ensures that all communicated model parameters and updates are authenticated using quantum-resistant signa-tures, mitigating the risk of tampering by quantum-enabled adversaries. The choice of the PQC algorithm (Dilithium, Fal-con, or SPHINCS+) affects the performance characteristics of this secure FL system, which we evaluate in our experiments. By implementing this PQC-enhanced protocol, we provide a robust defense against potential quantum attacks on the FL communication process, ensuring the integrity and authenticity of model updates in a post-quantum environment."}, {"title": "IV. EXPERIMENTS", "content": "A. Implementation Details\na) Models and Datasets: Our evaluation encompasses both vision and natural language processing tasks, representing two of the most significant domains in deep learning. We employ three diverse settings to assess the impact of PQC\nalgorithms on federated learning across different scales and modalities:\n\u2022 MLP on MNIST: For a preliminary evaluation on a classic computer vision task, we use a Multi-Layer Per-ceptron (MLP) model on the MNIST dataset. MNIST consists of 70000 28\u00d728 grayscale images of handwritten digits, providing a straightforward yet informative bench-mark for image classification.\n\u2022 Pythia-70M on RTE: To examine performance on lan-guage tasks, we employ the Pythia-70M model, a 70 million parameter language model from the Pythia series, on the Recognizing Textual Entailment (RTE) dataset. RTE is a natural language inference task from the GLUE benchmark, challenging the model to determine the rela-tionship between pairs of sentences.\n\u2022 ResNet-18 on CIFAR-100: For a more complex vision task, we utilize ResNet-18, an 18-layer residual neural network, on the CIFAR-100 dataset. CIFAR-100 com-prises 60000 32 \u00d7 32 color images across 100 classes, presenting a more challenging image classification sce-nario than MNIST.\nb) Federated Learning Details: Our experiments are built upon the FedAvg algorithm, a fundamental approach in federated learning that aggregates local model updates from distributed clients. While our implementation focuses on FedAvg, our proposed PQC-enhanced protocol is adaptable to most federated learning algorithms where communication occurs between clients and the central server. To ensure consistency and facilitate fair comparison across different PQC algorithms, we maintain uniform hyper-parameters for each client within a given model and task setting. The data distribution across clients follows an independent and iden-tically distributed (i.i.d) splitting strategy, which allows us to focus on the impact of the PQC algorithms without the added complexity of non-i.i.d data challenges. We provide a comprehensive list of the hyper-parameters used in our experiments in Table II, including learning rates, batch sizes, and the number of local epochs for each setting. We use NVIDIA H100 GPU and PyTorch to conduct our experiments. All models are trained with the constant learning rates and AdamW optimizer.\nB. Main Results\nFigure 3 presents the training curves for three different model-dataset combinations: MLP on MNIST, Pythia-70M on\nRTE, and ResNet-18 on CIFAR-100. Each plot shows the loss over time for the three PQC algorithms: Dilithium, Falcon, and SPHINCS+. These graphs allow us to compare both the convergence speed and final performance across different PQC implementations in various federated learning scenarios.\na) Training Speed Comparison: Our results clearly demonstrate a consistent ranking in terms of training speed across all three scenarios: SPHINCS+ is the slowest, followed by Falcon, with Dilithium being the fastest. This observation aligns with the characteristics summarized in Table I. The impact of these speed differences for digital signature is less pronounced in the more complex models and datasets, partic-ularly evident in the Pythia-70M and ResNet-18 experiments, where the local model training time scales extend to hundreds and thousands of seconds, respectively, being much higher than that of digital signature.\nb) Consistency in Final Performance: These results indi-cate that the choice of the PQC algorithm does not necessarily affect the final training performance, as measured by the loss values. Across all three scenarios, we observe that the loss curves converge to exact values regardless of the PQC algo-rithm used. This outcome is natural, as the PQC algorithms are designed to protect the integrity of model updates during communication without altering their content.\nc) Trade-offs Between Security and Efficiency: These results highlight the inherent trade-offs between security guar-antees and computational efficiency in post-quantum federated learning. While SPHINCS+ offers strong security guarantees due to its hash-based nature, it consistently shows slower training times. Conversely, Dilithium balances strong security and computational efficiency, making it an attractive option for many federated learning scenarios. Falcon sits between these extremes, offering a middle ground in terms of both security and performance.\nC. Extended Study on Quantum Neural Network\nAs we progress towards the post-quantum era, it is increas-ingly likely that we will train and deploy quantum neural networks (QNNs) on pure quantum computers. This shift\nnecessitates an evaluation of PQC methods in the context of quantum neural network federated learning. To address this forward-looking scenario, we extended our study to include a simulated quantum neural network environment.\na) Experimental Setting: We implemented a federated learning system using a simulated quantum neural network. The simulation was conducted on classical GPUs, which, while not as efficient as a true quantum computer, allows us to approximate the behavior and challenges of quantum neural network training in a federated setting. We implement the training procedure with TorchQuantum\u00b3 [19] on NVIDIA H100 GPU.\nb) Results and Analysis: Figure 4 presents the training curves for the quantum neural network using the three PQC algorithms: Dilithium, Falcon, and SPHINCS+. The graph shows the loss over time for each algorithm during the federated learning process. Key observations are:\n\u2022 Minimal Speed Differences: Unlike our observations with classical MLP and Pythia-70M neural networks, the three PQC algorithms do not yield substantial differences"}, {"title": "V. DISCUSSION AND FUTURE WORK", "content": "a) Man-in-the-Middle Attack (MITM): While our pro-posed approach enhances the quantum security of federated learning using PQC digital signature algorithms, it is important to acknowledge a potential limitation in our current setting. Specifically, the system remains vulnerable to a sophisti-cated Man-in-the-Middle (MITM) attack during the initial key exchange phase. If an adversary successfully intercepts and replaces the public keys during distribution, they could effectively compromise the entire system. In this scenario, the attacker could replace legitimate public keys with their own, allowing them to intercept, decrypt, and potentially modify all subsequent communications. This would enable the attacker to inject malicious model updates, effectively poisoning the global model without detection by the central server or other clients. This vulnerability underscores the critical importance of secure key distribution and verification mechanisms in federated learning systems, even when employing quantum-resistant cryptographic algorithms. Future research should focus on developing robust protocols for secure key exchange and distribution in distributed learning environments, possi-bly leveraging additional cryptographic primitives or trusted hardware solutions to mitigate this risk.\nb) Evaluation of the Communication Time: A signif-icant limitation of our current study is the absence of a comprehensive evaluation of the communication costs asso-ciated with the three PQC signature algorithms in federated learning. Our experiments were conducted on a single GPU, which effectively eliminates real-world network communica-tion overhead. This simplification, while allowing us to focus on computational performance, overlooks a crucial aspect of federated learning systems: the impact of signature size on communication efficiency. The three PQC algorithms -Dilithium, Falcon, and SPHINCS+ - have notably different signature sizes, which could significantly affect the overall system performance in a distributed setting. For instance, SPHINCS+ is known for its larger signature size compared to Dilithium and Falcon, which could potentially lead to increased communication overhead in a real-world federated learning deployment. Future work should address this gap by implementing these PQC algorithms in a truly distributed environment, measuring not only the computational time but also the time required for secure communication of model updates. This evaluation would provide a more holistic view of the trade-offs between security, computational efficiency, and communication overhead in post-quantum federated learning systems. Such insights would be invaluable for practitioners in choosing the most suitable PQC algorithm for their specific federated learning applications, considering both the compu-tational resources and network conditions."}, {"title": "VI. RELATED WORKS", "content": "a) Security Protection for Federated Learning: The in-tegrity of federated learning systems faces significant chal-lenges, particularly in the form of model-stealing attacks. Participants in the FL process may surreptitiously embed malicious functionalities into the shared global model. For in-stance, an image classification model could be manipulated to misclassify specific images based on attacker-defined criteria, or a text prediction model might be coerced into generating predetermined completions for certain prompts. To counter these threats, researchers have developed various protective measures. One notable approach involves implementing homo-morphic encryption techniques [15]. This method safeguards user data by facilitating parameter exchanges within an en-crypted environment. However, it's worth noting that this se-curity enhancement comes at a cost: the necessity for encoding parameters prior to transmission and the subsequent exchange of public-private key pairs for decryption can significantly increase communication overhead.\nb) Post-Quantum Cryptography for KEM: Post-quantum key encapsulation mechanisms (KEM) are crucial components in cryptographic systems designed to withstand attacks from quantum computers. Several promising KEM candidates have emerged through rigorous evaluation processes. Lattice-based schemes, such as CRYSTALS-Kyber, which leverages the Module Learning with Errors problem, offer a balanced secu-rity profile, performance, and key size [2]. Code-based KEMs provide alternative approaches, with BIKE utilizing quasi-cyclic moderate-density parity-check [14] codes and Classic McEliece employing Goppa codes [8]. These post-quantum KEMs are undergoing extensive research and development, with implementation efforts focusing on various platforms, including FPGA, ASIC, and RISC-V architectures [13], [22]. Key areas of improvement include computational efficiency, power consumption reduction, and enhanced resistance to side-channel attacks [6]. As quantum computing technologies ad-vance, these post-quantum KEMs are expected to secure future communication systems against emerging quantum threats, en-suring long-term data protection in an evolving cryptographic landscape [20]."}]}