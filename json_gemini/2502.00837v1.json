{"title": "Explainability in Practice: A Survey of Explainable NLP Across Various Domains", "authors": ["Hadi Mohammadi", "Ayoub Bagheri", "Anastasia Giachanou", "Daniel L. Oberski"], "abstract": "Natural Language Processing (NLP) has become a cornerstone in many crit-\nical sectors, including healthcare, finance, customer relationship management, etc.\nThis is particularly true with the development and use of advanced models like\nGPT-40, Gemini, and BERT, which are now widely used for decision-making pro-\ncesses. However, the black-box nature of these advanced NLP models has created\nan urgent need for transparency and explainability. This review provides an explo-\nration of explainable NLP (XNLP) with a focus on its practical deployment\nand real-world applications, examining how these can be applied and what the\nchallenges are in domain-specific contexts. The paper underscores the importance\nof explainability in NLP and provides a comprehensive perspective on how XNLP\ncan be designed to meet the unique demands of various sectors, from healthcare's\nneed for clear insights to finance's focus on fraud detection and risk assessment.\nAdditionally, the review aims to bridge the knowledge gap in XNLP literature by\noffering a domain-specific exploration and discussing underrepresented areas such\nas real-world applicability, metric evaluation, and the role of human interaction in\nmodel evaluation. The paper concludes by suggesting future research directions\nthat could lead to a better understanding and broader application of XNLP.", "sections": [{"title": "1. Introduction", "content": "The advent of Natural Language Processing (NLP) and Large Language\nModels (LLMs) has mainly improved machine-human interaction by en-\nabling machines to better understand human language, making these sys-\ntems more accessible. Advanced models like OpenAI's GPT-40, Google's\nGemini, Meta's LLaMA 3.1, and BERT have improved the capabilities of\nmachines in understanding and processing human language. This has en-\nabled their applications across diverse domains such as healthcare, finance,\nand customer relationship management (CRM) [160]. These advancements\nhave reduced traditional processing times, enabling rapid content generation\nand complex question answers [147, 63]. For instance, a study by Oniani et al.\n[130] showed that LLMs could improve clinical decision support (CDS) by\nincorporating Clinical Practice Guidelines (CPGs). By using CPGs, LLMS\ncan provide more precise and contextually relevant treatment recommen-\ndations, thus enhancing patient care and assisting healthcare professionals\nin making more informed decisions [97]. These advanced LLMs techniques\nfor decision-making offer many advantages, including timely and accurate\nanalysis, user-friendly interfaces, and the ability to handle vast amounts of\ndata efficiently [208]. Advanced models have achieved high prediction per-\nformance in several tasks, including image classification [90, 79], NLP tasks\nsuch as machine translation [185], speech recognition [71], and game play-\ning [163]. However, it is not possible to understand their predictions due\nto the number of parameters. These models, known as \u201cblack boxes\" in\nmachine learning, are often developed directly from data by complex algo-\nrithms. Incorporating billions of parameters, these models are so opaque\nthat even their creators cannot fully understand how variables integrate to\ngenerate predictions, resulting in a reduction in transparency despite their\nadvanced capabilities [152]. Furthermore, those models have been trained on\nlarge amounts of data, which often contain biases\u00b9. The core issue is that\nthe lack of clarity in the decision-making process can not only hinder the\ndetection of biases embedded within the model but also potentially lead to\nfurther biases. For instance, biases can arise from various sources, such as\nhistorical imbalances and stereotypes present in training data [32], model\nassumptions that reinforce existing prejudices [26], and feedback loops that"}, {"title": "2. Modeling Techniques for XNLP", "content": "2.1. Explainability of Traditional NLP models\nTraditional NLP models, such as Bag of Words (BoW) and its variants\nlike term frequency (TF) and term frequency-inverse document frequency\n(TF-IDF), offer a foundational representation of textual data. The BoW,\nin particular, represents a text document as a collection of words without\nhighlighting their order or the context in which those words are used. When\npaired with transparent classifiers like logistic regression, this representa-\ntion emphasizes interpretability. Specifically, the logistic regression model's\ncoefficients allow us to discern each word's influence on the prediction out-\ncome. Metrics like semantic relationships highlight the importance of specific\nwords for a given prediction [68, 107]."}, {"title": "2.2. Explainability of Transformer-based models", "content": "Transformers, foundational to models like BERT [48], have transformed\nNLP by enabling parallel processing and handling long-range dependencies\neffectively, setting new performance benchmarks [86]. By using self-attention\nmechanisms, Transformers assign varying weights to input components, cap-\nturing relationships and dependencies even in distant parts of the input,\nwhich is crucial for tasks like machine translation and sentence summariza-\ntion [185]. BERT, a model using attention, can be pre-trained on extensive\ntext and fine-tuned for specific tasks, with attention mechanisms highlighting\nsalient input parts [185]. Advanced variants such as RoBERTa [106] and AL-\nBERT [94] have further improved performance through refined training and\nmodel configurations, excelling in tasks like sentiment analysis, question an-\nswering, and named entity recognition, often surpassing human performance\non benchmarks [114].\nHowever, despite the Transformer models' impressive performance and ca-\npabilities, they face obstacles. mainly, their complexity makes interpretabil-\nity and explainability more difficult to illuminate their decision-making pro-\ncesses and identify the linguistic features they rely on. One of the primary\nexplainability approaches for transformers has been the visualization of\nattention weights, which allows researchers and practitioners to see which"}, {"title": "3. Applications and Domains of XNLP", "content": "AI-based applications have recently been applied to various facets of hu-\nman life, including social networking, medicine, commerce, customer service,\nand finance. General NLP applications, such as machine translation, text\nsummarization, and sentiment analysis, are commonly used to automate rou-\ntine tasks and enhance user experiences [185, 43]. In these cases, the focus is\noften on performance rather than the need for explanations of how the mod-\nels reach their conclusions. However, in specific fields such as medicine, NLP\nmust provide explanations that are not only clear but also medically relevant.\nFor example, it is important to understand the predictions of systems trained\non electronic health records (EHRs), as these predictions can influence de-\ncisions about which patients are flagged for follow-up by a specialist or for\nadditional treatment, consequently impacting patient care. Fortunately, dif-\nferent from traditional AI, which can answer \u201cYes\u201d and \u201cNo\u201d type questions\nwithout explaining how they did so, XNLP can answer \u201cWh-questions\" such\nas \u201cWhy\u201d, \u201cWhen\u201d, and \u201cWhere\u201d. However, XNLP should produce expla-\nnations that humans can understand, ensuring that all beneficiaries can trust\nhow the models work. On one hand, confidence and trust in explanations\nof how answers were obtained are critical, and on the other hand, produced\nexplanations should be expressive enough for humans to comprehend.\nOn top of that, XNLP should ensure that AI/ML models make fair and\nequitable decisions. Like the EHRs, understanding how a patient is selected\nfor additional treatment shows how the system is fair [95]. An explanation\nof how an answer was obtained is critical for confidence and fairness in many\napplications. For example, advancements in AI technologies enable CRM"}, {"title": "3.1. Medicine", "content": "XNLP has emerged as a valuable tool in the medical domain, particu-\nlarly in managing the vast amounts of unstructured data contained in EHRs.\nEHR data comprises textual information like medical histories, physician's\nnotes, and medical narratives that are vital for various medical applications\nlike heart failure prediction [39]. The interpretability of predictions from\nEHR data is crucial due to the life-impacting decisions that may be based\non these predictions. Through XNLP, the logic behind diagnoses or treat-\nment recommendations can be elucidated, aiding medical practitioners in\nunderstanding the model's predictions, which in turn supports informed and\nreliable decision-making in patient care [98].\nFor instance, Choi et al. [39] used an interpretable predictive model to pre-\ndict heart failures from EHR data, assigning significance to various features\nlike medical codes to provide explainable risk factors [39]. Furthermore, Kang\net al. [87] and, Weng et al. [195]used rule-based NLP models to extract clini-\ncal evidence from randomized controlled trials, making the extraction process\ntransparent and traceable. Text mining and NLP techniques have been ap-\nplied to numerous health applications involving text de-identification tools,"}, {"title": "3.2. Finance", "content": "Recent research indicates that NLP applications in finance, such as ana-\nlyzing financial reports and news articles, can identify potential risk factors\nand provide timely warnings and decision support. For instance, an NLP-\nbased financial risk detection model can perform excellently in risk identifi-\ncation and prediction, offering effective tools for financial [190]. XNLP holds\npromise for risk assessment and fraud detection in the financial sector. It\ncan assist in detecting adversarial behaviors in fraud detection models, pro-\nviding companies and end users with more trust in the machine learning\ninferences [140]. Furthermore, user-centric XNLP methods have been devel-\noped to align fraud experts' tasks with explanation methods, enhancing the\noverall effectiveness of fraud detection systems [40]. So users, in turn, can\nunderstand the factors impacting risk assessment and take necessary actions\nto reduce risks.\nRisk assessment is a crucial facet of the financial sector, benefiting from\nXNLP. Financial institutions depend on comprehensive risk assessments to\nmake critical decisions like granting loans or determining insurance premi-\nums. Given the potential implications, these evaluations must be accurate,\ntrustworthy, and, importantly, understandable [58, 189]. XNLP can assist\nby providing clear explanations for generated risk scores or credit evalua-\ntions, enabling users to understand the rationale behind their assessments\nand take corrective actions to reduce risk and enhance their credit scores."}, {"title": "3.3. Systematic Reviews", "content": "Systematic reviews use research literature to inform evidence-based decision-\nmaking across various fields. Key aspects of the systematic review process,\nincluding identifying relevant studies, extracting data, and summarizing find-\nings, can be automated through specialized applications. For example, van de\nSchoot et al. [182] developed ASReview\u00b3, an open-source tool designed to\nsupport study selection in systematic reviews. XNLP techniques offer ad-\nditional benefits by helping researchers understand the rationale behind the\ninclusion or exclusion of studies. By incorporating explainability, such tools\nprovide insight into why certain studies are recommended, aiding in study\nselection decisions. NLP models further enhance the efficiency and reliability\nof systematic reviews by clarifying the criteria used for study inclusion and\nthe relevance of extracted information [150].\nThe growing volume of scientific publications has heightened the need\nfor automating systematic reviews. XNLP's application can accelerate the\nprocess and reduce researchers' workload by automating the identification"}, {"title": "3.4. Customer Relationship Management (CRM)", "content": "XNLP is vital for extracting and summarizing key data from identified\nstudies. Text summarization models can understand the content of a study\nand condense it into a summary encapsulating the major findings. By in-\ntroducing explainability, researchers can understand why certain information\nwas included or excluded from the summary, thereby promoting transparency"}, {"title": "3.5. Chatbots and Conversational AI", "content": "The prevalence of chatbots and conversational AI systems has increased\nin numerous applications, such as virtual assistants, customer support, and\ninformation retrieval. XNLP techniques can increase the effectiveness and\ntransparency of these systems by improving explanations for the responses\nor recommendations generated. By comprehending the logic behind the chat-\nbot's responses, users can place more faith in the system's suggestions and\nfeel more engaged in the conversation. In addition, context-aware recom-\nmendations that explain personalized recommendations can increase user\nsatisfaction and empower them to make informed decisions. COPPO and\nGUIDOTTI [42] describe a real-world application of a chatbot that justifies\nits responses, thereby fostering user trust and engagement.\nContext-aware recommendation systems can provide personalized sug-\ngestions by considering the context of the user's interaction. Incorporating\nXNLP into these systems can increase user satisfaction and confidence by\nclarifying why particular recommendations are made. For example, Zhang\net al. [206] proposed a context-aware recommendation model that employs\nXNLP to justify its recommendations. Their model incorporates the context\nof user-item interactions and provides clear explanations, which improves\nuser satisfaction.\nIn 2023, with the advent of GPT-4, Conversational AI has seen a leap\nforward. The larger context window and improved accuracy of GPT-4 enable\nmore coherent and contextually accurate interactions with users. OpenAI's\nlatest model, GPT-40 (\u201co\u201d for \u201comni\u201d), represents another substantial ad-\nvancement in the field. GPT-40 matches GPT-4 Turbo performance on text\nin English and code, while showing improvement on text in non-English lan-\nguages [173]. These advancements in context understanding and multimodal\nprocessing contribute to more delicate and accurate conversational AI in-\nteractions, potentially addressing some of the explainability challenges by\nproviding more comprehensive and context-aware responses [132, 133, 174].\nDespite advancements in AI chatbots and their ability to enhance user\ninteractions, recent studies highlight a critical need for explainable AI to\nfurther improve transparency and effectiveness in AI chatbots, consequently\nenhancing trust and user engagement [112, 156, 115]. While AI chatbots have"}, {"title": "3.6. Social and Behavioral Science", "content": "In the domain of social and behavioral science, the application of XNLP\nis an important tool for addressing complex issues like hate speech, sex-\nism, and misinformation on social media platforms. Notably, transformer\nmodels such as BERT have been vital in identifying offensive content, in-\ncluding racism and sexism. For instance, Mozafari et al. [124] used BERT to\nanalyze datasets annotated for such content, enhancing detection accuracy"}, {"title": "3.7. Human Resources (HR)", "content": "In the realm of Human Resources (HR), the application of XNLP has\nemerged as a pivotal tool for optimizing various HR functions, ranging from\ntalent acquisition to employee sentiment analysis. Recent studies have ex-\nplored the utilization of advanced NLP techniques to enhance the trans-\nparency and effectiveness of HR processes.\nOne of the applications of XNLP in HR is in the recruitment process.\nTransformer models, such as BERT and GPT, are extensively used to au-\ntomate the resume screening process. These models can parse resumes and\nrank candidates based on the job description, ensuring a better match be-\ntween the candidate's skills and the job requirements. For instance, a study\nby Patwardhan et al. [135] showed the application of BERT in automating\nthe initial stages of recruitment, reducing the time and bias associated with\nmanual screening processes. The study highlighted the use of explainable\nAI techniques to provide insights into the model's decision-making process,\nthereby enhancing trust in automated recruitment systems. Similarly, re-\ncent advancements have shown the effectiveness of transformer models in\nimproving recruitment efficiency and reducing biases through explainable\nmechanisms [65]. In line with this, Akkasi [4] demonstrated the use of a\ntransformer-based ensemble to extract both technical and non-technical skills\nfrom job descriptions, significantly enhancing the precision and transparency\nof the recruitment process.\nEmployee sentiment analysis is another area where XNLP is making sub-\nstantial contributions. By analyzing text data from employee feedback, sur-"}, {"title": "3.8. Other Applications", "content": "XNLP finds applications in various domains further than what was dis-\ncussed before. The explainability in NLP models helps in understanding the\nunderlying processes and decisions made by these models, which is crucial in\ntasks like language generation, text classification, machine translation, sum-\nmarization, visual question answering, and many others.\nIn the domain of language generation, Sheng et al. [158] used Transformer\nmodels like GPT-2 to visualize attention in order to detect biases in language\nmodels.\nOn the other hand, DeYoung et al. [49] explored rationale-based explana-\ntions for text classification tasks across multiple datasets, including BoolQ\nand e-SNLI, evaluating the explanations on metrics like fidelity, comprehen-\nsiveness, and sufficiency. Jacovi and Goldberg [78] and Ayyubi et al. [16]\nextended the explainability to machine translation, summarization, and vi-\nsual question answering. They employed transformer models and multimodal\ntransformer models, respectively, with the former focusing on faithfulness in\nNLP models through question answering and the latter on rationale genera-\ntion to improve answer accuracy and explanation quality."}, {"title": "4. Critical aspects of XNLP", "content": "According to the applications and studies that we covered, when exam-\nining the landscape of XNLP across various domains, some attractive com-\nparisons can be made in terms of explainability techniques and their appli-\ncations. In the medical field, where risks are high and accuracy is crucial,\nthe focus appears to be on clear, actionable insights using techniques such\nas feature importance, transparent rule extraction, and rationale-based ex-\nplanations. This contrasts with finance, where risk assessment and fraud\ndetection hinge on models like BERT combined with techniques like layer-\nwise relevance propagation and attention mechanisms. The attention mech-\nanism's popularity in finance might be attributed to its ability to highlight\ncritical features in vast financial transactions, making anomalous activities\nmore discernible.\nRegarding CRM and chatbots, there is a clear emphasis on enhancing user\ntrust, comprehension, and overall experience. While both domains prioritize\nuser interaction, CRM tends to focus on sentiment analysis and customer\nsupport, using models such as LSTM, BERT, and CNN. In contrast, conver-\nsational AI relies more on attention distributions to explain decisions, aiming\nto make chatbot responses as transparent and reliable as possible. This di-\nvergence possibly stems from CRM's need to interpret user sentiment from\nvaried expressions, while chatbots aim to offer coherent and contextually rel-\nevant responses. Across both, the overarching theme is building user trust\nthrough transparency"}, {"title": "4.1. Evaluation Metrics: Quantifying Understanding", "content": "What does it mean to understand a model? To evaluate the effectiveness\nof XNLP techniques, the level of comprehension provided by the explana-\ntions must be quantified using appropriate evaluation metrics. Quantitative\nmetrics evaluate the correspondence between the explanation and the under-\nlying model's reasoning. Qualitative metrics offer insight into user percep-\ntions, trust, and explanation satisfaction [55]. Quantitative metrics quantify\nthe numerical characteristics of the generated explanations. They include\nfidelity, coherence, and completeness measurements.\nThe fidelity metric evaluates how accurately the explanations correspond\nto the actual behavior of the NLP model. High-fidelity explanations accu-\nrately represent the model's decisions and generate the same exact predic-\ntions as the original model. LIME [147] is a method for assessing fidelity"}, {"title": "4.2. Trade-offs and Challenges: The Price of Explanation", "content": "Explainability in NLP models frequently involves trade-offs. XNLP re-\nquires a balance between the level of explainability and the model's per-\nformance. As Doshi-Velez and Kim [50] noted, increasing a model's trans-\nparency may reduce its accuracy, which may be due to the simplification of\nthe model or constraints placed on the modeling process to make it more\nunderstandable. For instance, simpler models like linear regressions are eas-\nier to interpret but often less accurate than complex models like deep neural\nnetworks, which capture complex patterns in data but are harder to explain.\nResearchers and practitioners face the challenge of achieving a suitable bal-\nance between explainability and performance, as overly complex models can"}, {"title": "4.3. Rationalization Techniques: Current Approaches and Challenges", "content": "In recent years, NLP models have made advancements, becoming increas-\ningly complex and capable of handling complex tasks. However, as these\nmodels grow in complexity, the need for transparency and explainability be-\ncomes more urgent. This has given rise to Rational NLP (RNLP), a subset"}, {"title": "4.4. Human Evaluation and User Studies: Human in the Loop", "content": "Human evaluation and user research are crucial for assessing the impact\nand effectiveness of XNLP techniques. Human-in-the-loop strategies incor-\nporate feedback to validate and improve NLP-generated explanations. For\nexample, S\u00f8gaard [166] highlights the value of iterative feedback to better\nalign explanations with human cognition and usability. Similarly, Valvoda\nand Cotterell [181] emphasize explainability in the legal domain, aiding pro-\nfessionals in understanding model decisions. Along this line, Mosca et al.\n[123] introduce IFAN, a framework for real-time human-NLP interaction to\ndebias and enhance performance.\nWith the growing prevalence of LLMs, the challenge of bias in data is\nsignificant. Recent studies on counterfactual explanations [188] and fairness-\naware machine learning [52] further address bias reduction, promoting fair-\nness and transparency in XNLP. Similarly, Alkhaled et al. [10] introduced\nBipol, a novel metric focused on detecting social biases in text data, improv-\ning the fairness and transparency of models by comprehensively assessing\nbias. These techniques not only mitigate biases but also enhance clarity and\nrefine models based on user preferences and needs."}, {"title": "4.5. Data and Code Availability: The Role of Open Science", "content": "One of the critical aspects of XNLP is the availability of data and code,\naligning well with the principles of Open Science. Open Science encapsulates\nvarious facets including open data, open-source software, open journal access,\nand reproducibility, which are essential for providing a transparent and col-\nlaborative research environment [156, 30, 47]. Tools like Ecco [6] contribute\nto this openness by offering an interactive framework for explaining trans-\nformer models, helping researchers explore and understand model behavior\nmore transparently. ChatGPT, for instance, operates under a commercial\nsubscription model where its paid version (ChatGPT Plus) provides access\nto GPT-4 with faster response times, while the free version only allows ac-\ncess to GPT-3.5. These commercial offerings, while beneficial for some users,\ncontrast with open-source models in terms of transparency and accessibility\nfor research purposes [131]. Additionally, open-source models and open data"}, {"title": "4.6. Future Directions and Research Opportunities in XNLP", "content": "XNLP can benefit from integrating XNLP and other ML techniques, such\nas reinforcement learning (RL). RL is a subfield of ML in which an agent\nlearns to make decisions by interacting with an environment and receiving\nrewards and penalties as feedback. RL has been applied in various NLP\napplications, including dialogue systems and machine translation [99]. NLP,\nparticularly transformers and advanced models like GPT, is increasingly us-\ning RL. This method not only enhances the performance of these models but\nalso holds the potential to make tThem more explainable RL, by its nature of\nlearning through trial and error and receiving feedback, can be instrumental\nin fine-tuning models to produce more accurate and contextually relevant\noutputs. Additionally, the incorporation of RL strategies can contribute to"}, {"title": "5. Conclusion", "content": "In this article, we have explored the landscape of XNLP with a particular\nemphasis on its practical deployment and real-world applications across var-\nious domains. The main aim was to address the critical gap in the literature\nby providing a detailed examination of how XNLP systems can be effectively\napplied across various domains to enhance user understanding, transparency,\nand trust in machine learning models. We began by discussing the advance-\nments in NLP technologies and the inherent challenges associated with the\nblack-box nature of complex models. The necessity for transparency in these\nmodels was highlighted, particularly in high-risk fields like healthcare and\nfinance, where decisions based on model predictions can have and lasting\nimpacts. Then, we investigated XNLP techniques for various modeling, in-\ncluding traditional models like Bag of Words and TF-IDF, embedding models\nsuch as Word2Vec and GloVe, and advanced transformer models like BERT.\nWe elaborated on the interpretability and explainability techniques relevant\nto these models, such as attention visualization, rationalization techniques,\nand gradient-based methods, which aim to demystify the decision-making\nprocesses of NLP systems.\nWe examined the application of XNLP across seven key domains, high-\nlighting its broad impact and versatility. In the field of medicine, XNLP\nfacilitates the analysis of electronic health records and medical literature,\nproviding clear, actionable insights that support clinical decision-making and\nimprove patient care. In finance, it enhances risk assessment and fraud detec-\ntion systems, making financial processes more transparent and reliable, with\na particular emphasis on explainability in credit scoring and firm valuation\nto ensure fair and accountable financial practices. For systematic reviews, it\nautomates the review process and offers clear explanations for study selection\nand data extraction, thereby improving efficiency and transparency and aid-\ning researchers in managing vast amounts of scientific literature. In CRM,\nit improves sentiment analysis and customer support automation, offering\ntransparent and context-aware responses that enhance customer engagement\nand satisfaction. It also plays a crucial role in chatbots and conversational\nAI, enabling these agents to provide clear, context-aware explanations for\ntheir responses, which in turn improves user trust and satisfaction. In so-\ncial and behavioral science, it aids in the detection of sexism, hate speech,"}, {"title": "Appendix A. Appendix", "content": "To start our research, we did a full literature search on Scopus, Google\nScholar, etc using related keywords. This gave us the first set of scholarly"}, {"title": "Appendix A.2. Terminology", "content": "The field of XNLP is marked by an array of specific terms, concepts,\nand methods that define its unique characteristics and scope. This section\nprovides essential definitions to ensure a comprehensive understanding of the\nsubject matter and facilitate a clear and coherent reading experience. These\nterms are central to the discussions and analyses presented in the subsequent\nsections of this review.\n\u2022 Natural Language Processing (NLP): A branch of artificial intelligence that\nfocuses on the interaction between computers and humans using natural\nlanguage, enabling computers to interpret, recognize, and generate human\nlanguage.\n\u2022 XNLP (eNLP): An area of NLP that emphasizes the understanding and\ninterpretability of machine learning models. It seeks to make models' logic,\ndecisions, and operations comprehensible to humans.\n\u2022 Rational NLP (RNLP): An extension of NLP that integrates reasoning ca-\npabilities into NLP systems, enabling them to provide logical justifications\nfor their outputs or decisions.\n\u2022 Embedding Models: Methods used to represent words, phrases, and sentences\nin a continuous vector space, which can capture semantic meanings.\n\u2022 Transformer Models: A type of neural network architecture that relies on\nself-attention mechanisms to process input data in parallel rather than se-\nquentially, allowing for more efficient training and handling of long-range\ndependencies in text.\n\u2022 Rationalization Techniques: Approaches used to provide explanations or\njustifications for the decisions made by NLP models.\n\u2022 Explainability-Performance Balance: A trade-off considering the complexity\nand accuracy of a model against its interpretability. More complex models\nmight perform better but are often less interpretable.\n\u2022 Human-in-the-Loop (HITL): An approach where human judgment is in-\nvolved in the training, tuning, or evaluation of machine learning models\nto ensure alignment with human values, ethics, and understanding."}, {"title": "Appendix A.3. Declaration of generative AI and AI-assisted technologies in the writing process", "content": "During the preparation of this work, the authors used ChatGPT 3.5 from\nOpenAI to check grammar and make other writing corrections to improve the\nreadability and language of the manuscript. After using this tool, the authors\nreviewed and edited the content as needed and took full responsibility for the\npublished article."}, {"title": "Appendix A.4. Funding sources", "content": "This research did not receive any specific grant from funding agencies in\nthe public, commercial, or not-for-profit sectors."}]}