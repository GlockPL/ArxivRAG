{"title": "Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search", "authors": ["Jayant Sachdev", "Sean D Rosario", "Abhijeet Phatak", "He Wen", "Swati Kirti", "Chittaranjan Tripathy"], "abstract": "Accurate query-product relevance labeling is indispensable to generate ground truth dataset for search ranking in e-commerce. Traditional approaches for annotating query-product pairs rely on human-based labeling services, which is expensive, time-consuming and prone to errors. In this work, we explore the application of Large Language Models (LLMs) to automate query-product relevance labeling for large-scale e-commerce search. We use several publicly available and proprietary LLMs for this task, and conducted experiments on two open-source datasets and an in-house e-commerce search dataset. Using prompt engineering techniques such as Chain-of-Thought (CoT) prompting, In-context Learning (ICL), and Retrieval Augmented Generation (RAG) with Maximum Marginal Relevance (MMR), we show that LLM's performance has the potential to approach human-level accuracy on this task in a fraction of the time and cost required by human-labelers, thereby suggesting that our approach is more efficient than the conventional methods. We have generated query-product relevance labels using LLMs at scale, and are using them for evaluating improvements to our search algorithms. Our work demonstrates the potential of LLMs to improve query-product relevance thus enhancing e-commerce search user experience. More importantly, this scalable alternative to human-annotation has significant implications for information retrieval domains including search and recommendation systems, where relevance scoring is crucial for optimizing the ranking of products and content to improve customer engagement and other conversion metrics.", "sections": [{"title": "1 INTRODUCTION", "content": "When a customer performs a search on an e-commerce platform like Walmart, it is essential that the search results show the exact products or relevant products they are looking for, saving the customer time and effort while providing a positive shopping experience. The product search experience on e-commerce platforms is powered by a collection of machine learning models and algorithms across the stages of pre-retrieval, retrieval, and post-retrieval ranking and filtering [20]. Many of these models rely on accurate query-product (Q-P) relevance label data for training and testing. Specifically, these labels indicate how relevant a given product is to a customer search query. For example, the product \"Ergonomic PU leather high back office chair with flip-up armrest\" is highly relevant to the search query \"leather chair\", while the product \"hard floor beveled edge chair mat\" is not relevant to the same query.\nWe have worked with an external data labeling service which employs human labelers. The labelers were given Q-P pairs with detailed instructions and examples to guide labelers. They were not given any information on customer engagement data for Q-P pairs. Typically, three to five independent assessments are gathered per Q-P pair. Despite the guidelines, human labelers do not always reach consensus on an annotation, hence labeled data is often noisy. Furthermore, external human labeling is expensive and time-consuming, and hence only Q-P pairs corresponding to a small fraction of all queries can be labeled. Despite these challenges, external human labelers are considered as the most reliable method of acquiring data annotations, and is widely used across e-commerce and internet industries. Previously, Q-P relevance labels were obtained from clickstream data and human annotation as shown in Figure 1(a). Higher engagement of a product given a search query would be considered a positive signal for relevance. However, cold-start products (such as newly introduced products and tail products that are sold infrequently) may lack sufficient impressions, click-through-rate (CTR), or add-to-cart (ATC) rate information due to their limited exposure to the customers. Furthermore, some products might get a lot of impressions and clicks even if they are not very relevant to the search query. For example, the product \"fresh lime\" gets a relatively high number of impressions and clicks for the query \"lemon\u201d, but it is not a very relevant product. Identifying the strong need for an automated framework for Q-P relevance labeling, in this work, we demonstrate that Large Language Models (LLMs) can be used successfully to obtain accurate relevance labels in a fully automated manner and at scale. A schematic of our approach is shown in Figure 1(b), which we will detail in the subsequent sections. We have experimented with several LLMs for this task. We have explored different e-commerce search datasets, as well as ablation studies on the effectiveness of prompt engineering techniques including Chain-of-Thought (CoT) prompting, In-context Learning (ICL), and Retrieval Augmented Generation (RAG) with Maximum Marginal Relevance (MMR). Figure 2 shows Retrieval-Augmented Generation (RAG) architecture, designed and used in our work, which contains a retrieval component that retrieves relevant labeled query-product pairs for a given query-product pair. These relevant query-product pairs, providing the contextual information, are then used to create prompts which are provided as input to the LLMs."}, {"title": "2 BACKGROUND", "content": "This work is an application of data-centric Artificial Intelligence (AI), which is systematic engineering of the data used to build an Al system, including but not limited to automated data labeling, collection, augmentation, and cleaning [29]. One approach to automated data labeling is called programmatic labeling which includes using human-defined labeling functions (LFs) that capture labeling logic to auto-generate large training sets [21]. Programmatic labeling significantly increases efficiency over manual labeling through automating the labeling process. However, labeling decisions still require human domain expertise on what heuristics should be applied [9]."}, {"title": "2.1 Large Language Models", "content": "Large Language Models can generate human-like text and demonstrate state-of-the-art logical reasoning ability across a broad range of reasoning tasks and benchmarks [7]. With advancements in the field, open-source models have also proven to be an effective alternative to GPT models in multiple reasoning tasks [12, 13, 18, 25]. Prior work has shown that LLMs can be used to label data at par with human labeling on a variety of NLP tasks, with up to 96% saving in costs [26]. LLMs are proven to be a viable substitute to crowd-sourced annotators and can match or surpass crowd-sourced annotators [10]. Recent work has also explored the idea of using LLMs to annotate shopping relevance [8, 24]."}, {"title": "2.2 Mathematical Formulation", "content": "Let (Q, P) denote the set of all possible Q-P pairs. We define the task of relevance labeling as a classification problem with the candidate classes set $\\mathcal{Y} = \\{Y_1, Y_2,..., Y_n\\}$. We want to find a map $\\Phi_h : (Q, P) \\rightarrow \\mathcal{R}^{\\mathcal{Y}}$ that is able to ideally score the relevance for the Q-P pairs against the candidate classes. We assume Q and P are infinite sets [1]. Given an input query $q \\in Q$ and product $p \\in P$, we want to maximize the probability\n$P(y_{pred} = Y_{true}|q, p)$,\nwhere\n$Y_{pred} = \\underset{Y_i\\in \\mathcal{Y}}{\\text{argmax}} \\Phi_h (q, p)$.\n$\\Phi_h$ can be expressed as a function of the choice of the LLM and associated sampling parameters ($\\mathcal{L}$) along with the baseline input prompt or context (P) to the LLM. With advanced prompting techniques, we modify the context to include additional instructions I and set of k few shot examples or demonstrations that are added to the input context provided to the LLM. This can be represented as $\\mathcal{P}_{I,k}$, and we can express\n$\\Phi_h = \\Psi(\\mathcal{L}, \\mathcal{P}_{I,k})$,\nwhere $\\Psi(.,)$ is a function of $\\mathcal{L}$ and $\\mathcal{P}_{I,k}$. In case of Chain-of-Thought (CoT) prompting, we include additional instructions to step by step reason through the generation process that is $\\mathcal{I}_{CoT}$. With crowd-sourced data annotation, we first need to provide the annotators with a thorough description of the labeling task, with meanings of each category and specific examples with explanations. Therefore, using the same approach, we guide LLMs to annotate data by providing the task description and some labeled examples. We observe that prompting the LLM to explain the reasoning behind the provided annotation can improve the labeling accuracy. We use RAG to retrieve most similar and hence most relevant demonstrations or examples that are represented as $\\mathcal{K}_{RAG}$. Since we use the retriever to provide additional context simply by concatenating the context with these examples. RAG exhibits significant potential in enhancing the quality of prompts by providing external knowledge as a preliminary step before generating the response, thereby ensuring contextual precision and a higher degree of detail. We use"}, {"title": "3 METHODS", "content": ""}, {"title": "3.1 Prompt Engineering", "content": "Prompt engineering is the systematic design and optimization of input prompts to guide the LLM responses, ensuring accuracy, relevance, and coherence in the generated output [4]."}, {"title": "3.1.1 In-Context Learning (Few-Shot Prompting)", "content": "In-context learning refers to a setting in which the model is given a few demonstrations of the task at inference time, but no weight and gradient updates are allowed [2, 6]. The major advantage of this technique is reduced reliance on task-specific data and does not require fine-tuning on domain specific datasets."}, {"title": "3.1.2 Chain-of-Thought (CoT) Prompting", "content": "CoT prompting can enhance reasoning in language models by decomposing problems into intermediate steps [27]. It provides an interpretable insight into the model's behavior, revealing how it might have reached a certain answer and helping identify where the reasoning may have gone wrong. Using CoT, we provided the reasoning as to why certain label was selected for each of the examples used in few-shot prompting. There are multiple ways to invoke a CoT response. In our experiments we use the words 'Let's think step by step' in the few-shot examples to invoke CoT responses [14]."}, {"title": "3.1.3 Retrieval Augmented Generation (RAG)", "content": "RAG technique utilizes input to identify relevant information for the task at hand, which is then used to offer additional context in the formation of input prompt to the LLMs. The RAG architecture contains two main components: a retriever that identifies relevant information (documents) in response to input query, and a generator that generates the ouput, taking into account the context of retrieved information, the initial query, and required task information. It has been noted that extensive pre-trained language models have the capacity to encapsulate factual knowledge within their parameters and can attain superior outcomes when they are fine-tuned for distinct NLP tasks. Despite this, their proficiency in accessing and manipulating knowledge precisely is still lacking, leading to less than satisfactory performance in tasks that are heavily reliant on knowledge[16]. As shown in Figure 2, given a query-product pair, using the RAG technique, we provide the relevant context information about the query-product pair to the LLMs, which proves to be beneficial in generating the correct relevance labels."}, {"title": "3.1.4 Maximum Marginal Relevance Retrieval (MMR)", "content": "Retrieving similar Q-P pairs as few shot examples with RAG often leads to a set of examples with a high degree of overlap. Hence the amount of new information that each example adds to the prompt is limited. In order to overcome this, we added a component of diversity into the few-shot examples retrieved, as described in the previous section. MMR ensures that examples retrieved are both relevant to the question Q-P pair, while being sufficiently diverse. We experiment with different values of $\\lambda_{MMR}$ to find the optimal balance between similarity and diversity."}, {"title": "3.2 Details of our Prompt", "content": "Our prompt consists of three parts. First, we have a context prompt which specifies the instruction I to the LLM including the classes relevance labels with explanations of each label and the expected output structure. Second, we do in-context learning by providing sample inputs (search query and product title) and the outputs (corresponding relevance labels). For CoT, we included more instructions in the instruction prompt I and provided annotated few-shot examples. Lastly, we provide the Q-P actual pairs we want labeled by the LLM (see Figure 2 for a schematic)."}, {"title": "3.3 Datasets", "content": "We perform experiments on two publicly available datasets and also on our proprietary dataset to validate the effectiveness of our methods.\nESCI Dataset: ESCI dataset [22] is an e-commerce dataset that has Q-P pairs that are human-annotated with corresponding relevance labels. We assume that the annotations provided are accurate, and serve as ground truth labels. There are 4 classes for relevance labels: Exact, Substitute, Complement, and Irrelevant. We sampled 5000 Q-P pairs from the test split of the ESCI dataset, with an even split across the four class labels to create a test set for experimentation and evaluation purposes.\nWANDS Dataset: WANDS [5] is another e-commerce dataset that contains Q-P pairs that are human-annotated with the corresponding relevance labels. We assume that the annotations provided are accurate, and serve as ground truth labels. There are 3 classes for relevance labels: Exact, Partial, and Irrelevant. Since there is no train/test split in the data, we sampled 5000 Q-P pairs evenly across the three class labels to create a test set for experimentation and evaluation purposes.\nWalmart Mexico Search Dataset: Our primary data source is Walmart Mexico search session data, which contains customer search queries and product impressions. This data is proprietary and acquired from Walmart's internal databases. We sampled 100 queries uniformly across search traffic segments. For each of the 100 sampled queries, we sample 10 products retrieved before re-ranking. Thus, we have 1000 distinct query product pairs in our ground truth data, mostly in Spanish. Domain experts have manually annotated relevance across the 1000 Q-P pairs. The ground truth data has 5 relevance classes: Excellent, Good, Okay, Bad, and Embarrassing. The data is heavily skewed towards Excellent because the products per query are sampled from search retrieval data, which is likely to be more relevant than not. Hence, for this dataset, weighted F1 score is a more reliable metrics than average F1 score."}, {"title": "3.4 Experimental Setup", "content": "Across all three datasets, we experimented with 8 Few-Shot examples and 16 Few-Shot examples for in-context learning. These examples could be randomly sampled from the training set (referred to as FS in the tables and figures), or retrieved based on embedding distance from the test question (referred to as FS_RAG in the tables and figures), and diversity could be introduced to the RAG examples (referred to as FS_RAG_MMR in tables and figures). For experiments with Walmart Mexico data, we sampled some Q-P pairs outside the test set from historical search relevance data to be used as Few-Shot (FS) examples. Explanations for few shot example were also manually annotated for CoT examples. Third-party human annotations serve as the baseline for experiments on this data. For the ESCI dataset, FS examples were randomly sampled from the train split of the ESCI dataset. For the prompt, we made use of descriptions of relevance labels as defined in the [5, 22] for the experiments on WANDS and ESCI respectively. We used an 80GB GPU for running inference of open source LLMs: LLM1 is an 8 billion parameter LLM [18], and LLM2 is a 70 billion parameter version of the same model. LLM3 is another open-source model with 7 billion parameters [12], and LLM4 [13] is a Mixture-of-Experts model based on 8 models of LLM3. We used VLLM [15] that uses PagedAttention to speed up the inference to make our approach more scalable and resource friendly. We used Activation-aware Weight Quantized (AWQ) models [17] that could not be fit in a single 80GB GPU like LLM2 and LLM4. LLM5 [23] is not open-sourced, so we used a paid API provided to perform the LLM calls. To implement RAG as shown in Figure 2, we used ChromaDB [11] as our vector store for retrieving similar query-product pairs from training set. We then use this retrieved query-product pairs along with our task prompt, and input query-product pair to create an input prompt for the LLM. We used scikit-learn [19] implementations for computing evaluation metrics that are described in the subsequent section in detail."}, {"title": "3.5 Evaluation Metrics", "content": "We compute the following metrics across LLM predictions on the aforementioned datasets.\n$\\text{Accuracy}$: Accuracy(y, $\\hat{y}$) = $\\frac{1}{n_{\\text{samples}}} \\sum_{i=1}^{n_{\\text{samples}}} 1(\\hat{y}_i = y_i)$, where $\\hat{y}_i$ is the predicted value of the i-th sample and $y_i$ is the corresponding true value, then the fraction of correct predictions over all $n_{\\text{samples}}$ gives us the accuracy score. 1(x) is the indicator function.\nAverage F1 Score: $F_1 = \\frac{2PR}{P+R}$, where P is Precision and R is Recall. This measures the prediction accuracy across all relevance classes without accounting for class imbalance. Higher Average $F_1$ score means better classification performance.\nWeighted F1 Score: $F_{1w} = \\sum_i^n w_iF_1(i)$ To compute the weighted F1 score the $F_1$ score for each class is calculated individually, and averaged using normalized support weights per label. This can result in a better measure than average $F_1$ score, as it accounts for class imbalance. Higher weighted $F_1$ score means better classification performance."}, {"title": "4 RESULTS AND DISCUSSION", "content": ""}, {"title": "4.1 Experiment Metrics", "content": "Across experiments on all three datasets, we observe LLM2 and LLM5 outperform all other models (Please refer to Fig 3, and for the numeric outputs in Table 3). Our results show that providing FS examples improves accuracy over zero-shot in most cases, and RAG FS offers further improvements in accuracy metrics in some instances (see Table 3). However, we observe that the best accuracy is obtained when RAG is used in combination with diversification by MMR, which we believe is due to how the contextual information exploited by the different LLMs. Further, for experiments with 8 and 16 few shot examples using RAG to select examples, we tried various values of $\\lambda_{MMR} \\in [0.75, 0.5, 0.25, 0.0]$. Setting $\\lambda_{MMR} = 1$ is the same as the FS_RAG experiments because retrieval is solely based on relevance thus inducing zero diversity. We observe that for different datasets, and different values of FS, the optimal value of $\\lambda_{MMR}$ up to the resolution of our grid (0.25), can be different. Overall, we observe that diversity helps in improving accuracy. We observe that increasing diversification i.e. lower values of MMR improves accuracy metrics, as shown in Fig 4. This is because the retrieved examples may have some degree of semantic overlap, hence diversification helps add novel information within the prompt. We also include sample prompts from our experiments with WANDS data in the Appendix A1 and A2. A sample set of Q-P pairs on the ESCI data is shown in Table 1 and a sample set of Q-P pairs on the WANDS data is shown in Table 2. For some predictions that do not match the ground truth, such as Example #2 in Table 1 and Example #4 in Table 2, the prediction labels are arguably more accurate than the ground truth labels. We can potentially use this approach to improve these datasets by identifying errors in the existing human-annotated ground truth labels. For Walmart Mexico data, LLM5 with 16 FS_RAG_MMR with $\\lambda_{MMR} = 0.75$ approaches human-level weighted F1 Score (see Table 3)."}, {"title": "4.2 Time and Cost Analysis", "content": "External human-annotations had a turnaround time of about 2-3 weeks. Labelers often take about a week to familiarize themselves with the task requirements, and then another 2 weeks to manually label the 1000 records (Q-P pairs) they are provided. Therefore, turnaround for third party human-annotations is estimated to be about 30 mins/record. In contrast, our experiments show that LLMs do the task in a fraction of the time. Our best performing configuration for LLM5 takes a little over 3 hours to complete, so the upper bound on LLMs for this annotation task is about 8.3 seconds/record. However, with open-source models most experiments ran in under 5 minutes on a GPU machine with an inference time of about 0.3 seconds/record. These models demonstrate a significantly efficient run time for large datasets such as those from Walmart's e-commerce ecosystem, thereby offering a viable alternative. Moreover, the open-source nature of these models confers an added advantage in terms of flexibility, allowing for more customized adjustments according to specific requirements through fine-tuning.\nThe costs of using LLMs are also significantly less than third-party human labelers. According to our estimates, labeling each Q-P pair is about 500 times cheaper with LLMs than human labelers, even when considering compute resources and API costs. LLMs have the potential to be even more cost-effective with techniques like batching of inputs/outputs, and conciseness of prompts. Open-source models may be a more suitable choice over proprietary LLMs in many situations since they can be fine-tuned on custom data to achieve better performance and mitigate any potential data privacy/leakage concerns."}, {"title": "5 CONCLUSION", "content": "Our work demonstrates the potential of LLMs in providing accurate and reliable query-product relevance data at scale. By leveraging LLMs with the state-of-the-art prompt engineering techniques, we are able to achieve accuracy comparable to human-labelled query-product pairs, while significantly reducing the time and cost required. Our experiments demonstrated the novel use of MMR in RAG for query-product relevance labeling has shown good improvements and strong promise for future work. We also showed how techniques like Chain-of-Thought and Few Shot prompting can help improve the accuracy of LLMs for this task. This scalable alternative to traditional human-annotation methods has far-reaching implications for information retrieval domains such as search and recommendation. One could consider using product descriptions and other attributes that are often available in the product catalogs and taxonomies, however, they are often noisy and would increase the number of input tokens to the LLMs which would have additional computational cost. Our productionized approach within Walmart Global Tech has already yielded a large number of query-product relevance labels, which are currently being used to evaluate improvements to various search algorithms. This not only improves the efficiency of our search systems but also enables us to provide better user experiences through more relevant search results. Furthermore, our method's broad applicability across various information retrieval domains makes it a valuable tool for optimizing products and content for greater user interactions and revenue generation."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 Sample prompt : WANDS Dataset - 8 FS with RAG", "content": "You are a search engine in an eCommerce website. For a given customer query and a product title, please annotate each product title in the list as one of these options: 'Exact', 'Partial', 'Irrelevant'\nExact this label represents the surfaced product fully\nmatches the search query.\nPartial this label represents the surfaced product that\ndoes not fully match the search query. It only\nmatches the target entity of the query, but does not\nsatisfy the modifiers for the query.\nIrrelevant this label indicates the product is not relevant to the query.\nThe response should be in a python dictionary format {\"rating\": label},\nwhere label which is either 'Exact', 'Partial', or 'Irrelevant '.\n#### Here are some examples:\nquery: wood coffee table set by storage, product title: coffee table\n{'rating': 'Partial '}\nquery: wood coffee table set by storage, product title: coffee table with storage\n{'rating': 'Partial '}\nquery: wood coffee table set by storage, product title: onshuntay coffee table\n{'rating': 'Partial '}\nquery: wood coffee table set by storage, product title: wooden coffee table\n{'rating': 'Partial '}\nquery: wood coffee table set by storage, product title: wood coffee table\n{'rating': 'Partial '}\nquery: wood coffee table set by storage, product title: ahern coffee table\n{'rating': 'Partial '}\nquery: wood coffee table set by storage, product title: fromm wood table\n{'rating': 'Partial '}\nquery: wood coffee table set by storage, product title: bahareh coffee table\n{'rating': 'Partial '}\nNow rate the relevance of this pair:\nquery: wood coffee table set by storage, product title: mikell 2 piece coffee table set\nIn the prompt above, there are some examples from RAG that have a high degree of overlap like the product titles \"wooden coffee ta-ble\" and \"wood coffee table\". This issue is overcome by MMR-based diversity, as seen in the next prompt"}, {"title": "A.2 Sample prompt : WANDS Dataset - 8 FS\nRAG MMR $\\lambda_{MMR}$ = 0", "content": "You are a search engine in an eCommerce website. For a given customer query and a product title, please annotate each product title in the list as one of these options: 'Exact', 'Partial', 'Irrelevant'\nExact this label represents the surfaced product fully\nmatches the search query.\nPartial this label represents the surfaced product that\ndoes not fully match the search query. It only\nmatches the target entity of the query, but does not\nsatisfy the modifiers for the query.\nIrrelevant this label indicates the product is not relevant to the query.\nThe response should be in a python dictionary format {\"rating\": label},\nwhere label which is either 'Exact', 'Partial', or 'Irrelevant '.\n#### Here are some examples:\nquery: wood coffee table set by storage, product title: coffee table\n{'rating': 'Partial '}\nquery: wood coffee table set by storage, product title: gabrielle end table storage\n{'rating': 'Partial'}\nquery: wood coffee table set by storage, product title: mylor solid wood coffee table\n{'rating': 'Partial '}\nquery: wood coffee table set by storage, product title: oday solid wood coffee table with lamp\n{'rating': 'Partial '}\nquery: wood coffee table set by storage, product title: radford coffee table with storage\n{'rating': 'Partial '}\nquery: wood coffee table set by storage, product title: berg solid coffee table with storage\n{'rating': 'Partial '}\nquery: wood coffee table set by storage, product title aule solid wood end table with storage\n{'rating': 'Partial '}\nquery: wood coffee table set by storage, product title hedda coffee table\n{'rating': 'Partial '}\nNow rate the relevance of this pair:\nquery: wood coffee table set by storage, product title: mikell 2 piece coffee table set"}, {"title": "A.3 Experimental results", "content": ""}]}