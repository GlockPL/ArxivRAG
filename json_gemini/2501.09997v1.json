{"title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models", "authors": ["Qiang Liu", "Xinlong Chen", "Yue Ding", "Shizhen Xu", "Shu Wu", "Liang Wang"], "abstract": "Hallucination has emerged as a significant barrier to the effective application of Large Language Models (LLMs). In this work, we introduce a novel Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. The AGSER method utilizes attention contributions to categorize the input query into attentive and non-attentive queries. Each query is then processed separately through the LLMs, allowing us to compute consistency scores between the generated responses and the original answer. The difference between the two consistency scores serves as a hallucination estimator. In addition to its efficacy in detecting hallucinations, AGSER notably reduces computational complexity, requiring only three passes through the LLM and utilizing two sets of tokens. We have conducted extensive experiments with four widely-used LLMs across three different hallucination benchmarks, demonstrating that our approach significantly outperforms existing methods in zero-shot hallucination detection.", "sections": [{"title": "1 Introduction", "content": "Recently, Large Language Models (LLMs) (Zhao et al., 2023) have demonstrated superior ability and achieved excellent results in various natural language processing tasks, such as summarization (Ravaut et al., 2024), machine translation (Zhang et al., 2023a), autonomous agents (Wang et al., 2024), information retrieval (Xu et al., 2024), and knowledge graph reasoning (Sun et al., 2024). Despite the convenience offered by LLMs, they may produce overly confident answers that deviate from factual reality (Manakul et al., 2023; Zhang et al., 2023b; He et al., 2024). This is usually called the Hallucination phenomenon, which makes LLMs very untrustworthy (Zhang et al., 2023c; Li et al., 2024). This strongly limits the application of LLMs, especially in medical, financial, legal, and other scenarios. Thus, it is urgent to investigate to conduct accurate and efficient hallucination detection in LLMs, and teach LLMs to say \"I don't know\" when they are not sure about the answers.\nThe most common hallucination detection method is based on answer consistency (Manakul et al., 2023; Zhang et al., 2023b; Chen et al., 2024), in which the answers to the same query are sampled multiple times. Though effective, such methods heavily increase computation cost through multiple LLM running. They also rely on randomness, and when the LLM is extremely confident in the wrong answer, the same answer may be constantly generated during resampling (Zhang et al., 2023b). Moreover, none of the existing consistency-based approaches guides LLMs to rethink the answer generation process like humans do, which may help us to obtain a better consistency evaluation. Recently, more hallucination detection approaches have been proposed from other perspectives, but they require tool usage (Cheng et al., 2024), or annotated hallucination datasets (Azaria and Mitchell, 2023; He et al., 2024; Chuang et al., 2024a).\nConsidering that attention contributions in LLMs reflect the key parts of the answer generation process and provide hints about hallucinations (Yuksekgonul et al., 2024), we propose an Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. Specifically, according to attention contributions of tokens, we split the input query for LLMs into attentive and non-attentive queries. As the attentive query contains the major information for LLMs to generate the answer, if we input the attentive query into LLMs, the generated answer should be very similar to the original answer for a non-hallucination sample. On the other hand, due to language differences between attentive and original queries, the randomness of generating the hallucination answer has been enlarged, and we have a greater chance of detecting hallucination based on the inconsistency of answers. This is similar as when a human is doing reading comprehension, if asked to rethink about the answer, he or she will re-examine the attentive parts of the article, and may provide a new answer. Meanwhile, for a non-hallucination sample, there is almost no important information in the non-attentive query, and thus when we input the non-attentive query into LLMs, the generated answer should be very random and totally different from the original answer. In Sec. 4, we provide some observations to verify the above analysis.\nAccordingly, in AGSER, we use attentive and non-attentive queries to guide LLMs to conduct self-reflection for hallucination detection. Specifically, we separately feed attentive and non-attentive queries into LLMs, and respectively calculate the consistency scores between the generated answers and the original answer, which are denoted as attentive and non-attentive consistency scores. Then, as smaller attentive consistency scores and larger non-attentive consistency scores indicate higher degrees of hallucination, we compute their difference as the hallucination estimator. This enables us to detect hallucinations in a zero-shot manner. Meanwhile, compared to conventional consistency-based approaches, AGSER reduces the computational complexity of resampling. It only requires three times of LLM running, and two times of token usage. We have conducted extensive experiments with four popular LLMs, and ASGER achieves state-of-the-art hallucination detection performances.\nThe main contributions of this work are summarized as follows:\n\u2022 According to attention contributions of tokens in LLMs, we define attentive and non-attentive queries. For a hallucination sample, the generated answer of the attentive query has a larger chance to be different from the original answer, and the generated answer of the non-attentive query has a larger chance to be similar to the original answer.\n\u2022 We propose a novel AGSER approach for zero-shot hallucination detection. AGSER uses attentive and non-attentive queries for constructing an effective hallucination estimator. It can also reduce the computational complexity of answer resampling.\n\u2022 We have conducted extensive experiments with four popular LLMs, which demonstrate the effectiveness of our proposed AGSER approach in hallucination detection."}, {"title": "2 Related Work", "content": "Hallucination has become the major obstacle in constructing trustworthy LLMs (Zhang et al., 2023c). LLMs may generate overly confident non-factual contents. This brings great demand for automatic hallucination detection in LLMs (Li et al., 2024), especially in a zero-shot manner.\nThe most common hallucination detection approach is based on the inconsistency of the generated contents. SelfCheckGPT (Manakul et al., 2023) stochastically generates multiple responses besides the original answer, and detects the hallucination via verifying whether the responses support the original answer. SAC\u00b3 (Zhang et al., 2023b) detects hallucinations through consistency analysis cross different LLMs or cross rephrased queries. It also points out that generated answers to the same query may be consistent but non-factual. Logic-CheckGPT (Wu et al., 2024) asks LLMs with questions with logical relationships for hallucination detection. INSIDE (Chen et al., 2024) attempts to calculate answer inconsistency in the sentence embedding space. InterrogateLLM (Yehuda et al., 2024) detects hallucinations via asking the reverse question, and verify whether the original question can be generated.\nMoreover, the inner states of LLMs can tell hallucinations to some extent (Azaria and Mitchell, 2023). We can use hidden states (He et al., 2024) or attention values (Chuang et al., 2024a) for training classifiers to detect hallucinations. However, such approaches require training datasets, and may have trouble generalizing among different LLMs and different data (Orgad et al., 2024). Meanwhile, some works propose to call tools for constructing hallucination detectors (Cheng et al., 2024; Yin et al., 2023). In addition, some works attempt to refine LLM parameters to enhance the factuality, via aligning with factuality analysis results (Zhang et al., 2024b), truthful space editing (Zhang et al., 2024a), over-trust penalty (Leng et al., 2024), and confidence calibration (Liu et al., 2024). Contrastive decoding (Li et al., 2023; Chuang et al., 2024b; Leng et al., 2024), which proposes to subtract logits of LLMs with less factuality, has also been used for improving the factuality.\nThere is research showing that, LLMs' attention to some constraint tokens (such as important entities) relates to the factuality of the generated responses (Yuksekgonul et al., 2024). Accordingly, attention contributions can reflect the answer generation process of LLMs, and guide LLMs to conduct self-reflection for accurate hallucination detection."}, {"title": "3 Preliminary", "content": "A query is denoted as a sequence of tokens $X = {x_1, x_2, ..., x_M}$, in which $x_i$ denotes the i-th token. We denote an LLM as $f (\\bullet)$, and the generated answer is $Y = f (X)$. Specifically, the answer is a sequence of tokens $Y = {Y_1, Y_2, ..., Y_N }$, in which $Y_j$ denotes the j-th token. Due to the hallucination phenomenon, Y may be factual or non-factual.\nThe self-attention layers are the core components in LLMs (Vaswani et al., 2017), and can reflect the key parts of the answer generation process of LLMs. We assume that the LLM has L self-attention layers and H heads. In the self-attention layers, there are four projection matrices $W_Q^h, W_K^h, W_V^h \\in \\mathbb{R}^{d \\times d_h}$ and $W_O^h \\in \\mathbb{R}^{d_h \\times d}$, which denote query, key, value and output projections respectively, for layer l and head h, and the dimensionality $d_h = d/H$. The attention values matrix for layer l and head h can be calculated as\n$A^{lh} = \\sigma(\\frac{(X^{l-1}W_Q^h)(X^{l-1}W_K^h)^T}{\\sqrt{d_h/H}}),$ (1)\nwhere \u03c3 denotes softmax function. And the attention contribution from token j to token i for layer l and head h can be calculated as\n$\\alpha_{i,j}^{l} = \\sum_{h=1}^H \\frac{\\alpha_{i,j}^{l, h}}{H}.$ (2)\nThen, to obtain a score for measuring the cotribution of the token i during the answer generation process of the LLM, we use the attention contribution from token i to the last token of the query as the token contribution score\n$s_i = \\alpha_{M,i}^l$ (3)"}, {"title": "4 Analysis", "content": "To verify that we can use attention to guide LLMs to conduct self-reflection and accurately detect hallucinations, we present the following analysis. We adopt the attention at the middle layer, i.e., layer L/2, for the token contribution calculation. The contribution score at the middle layer for token i is $s_i^{mid} = \\alpha_{M,i}^{L/2}$, and the contribution scores for the entire input query are $S^{mid} = {s_1^{mid}, ..., s_M^{mid}}$. Then, we can split the input query $X = {x_1, x_2, ..., x_M }$ into attentive and non-attentive queries\n$X^{att} = {x_i | s_i \\in topk (S)},$ (4)\n$X^{non\\_att} = {x_i | s_i \\notin topk (S)},$ (5)\nwhere $s_i = s_i^{mid}$, $S = S^{mid}$, and $topk (\\bullet)$ means selecting tokens with k highest contributions. Here, we select the top k = 2/3 tokens. Then, we can obtain the corresponding responses of the LLM as $Y^{att} = f (X^{att})$ and $Y^{non\\_att} = f (X^{non\\_att})$.\nTo measure the consistency between the attention-guided generated answers $Y^{att}$, $Y^{non\\_att}$ and the original answer Y, we adopt the Rouge-L (Lin, 2004) similarity estimation 1. Specifically, we have attentive consistency score and non-attentive consistency score as follows\n$r^{att} = Rouge (Y^{att}, Y),$ (6)\n$p^{non\\_att} = Rouge (Y^{non\\_att}, Y).$ (7)\nTo analyze the relationship between hallucinations in LLMs and attentive/non-attentive consistency scores, we conduct some pilot study on the Books dataset (Yehuda et al., 2024). We present the results with the Llama2-7b model (Touvron et al., 2023), which is a widely-used LLM. In Fig. 1, we illustrate four examples on feeding attentive and non-attentive queries into Llama2-7b. From the two non-hallucination samples we can observe that, the answers of the attentive queries stay consistent with the original answers, and the answers of the non-attentive queries are inconsistent with the original answers. Meanwhile, as shown in the"}, {"title": "5 Methodology", "content": "According to the above analysis and conclusion, in this section, we introduce the AGSER approach for zero-shot hallucination detection in LLMs. The whole procedure is illustrated in Alg. 1.\nIn addition to adopting attention at the middle layer of an LLM for token contribution calculation as in Sec. 4, we can define the following token contribution scores\n\u2022 The first layer value: $s^{first} = \\alpha_{M,i}^{1}$\n\u2022 The middle layer value: $s^{mid} = \\alpha_{M,i}^{L/2}$\n\u2022 The last layer value: $s^{last} = \\alpha_{M,i}^{L}$\n\u2022 The maximum value of all layers:\n$s^{max} = MAX(\\alpha_{M,i}^{l} | 0 < l < L)$.\n\u2022 The mean value of all layers:\n$s^{mean} = MEAN(\\alpha_{M,i}^{l} | 0 < l < L)$.\nThen, we can replace the token contribution score $s_i$ in Eqs. 4 and 5 with the above scores for calculating the corresponding attentive and non-attentive queries $X^{att}$ and $X^{non\\_att}$. And we can further obtain the attentive and non-attentive consistency scores $r^{att}$ and $r^{non\\_att}$ for estimating the degrees of hallucinations in LLMs as in Eqs. 6 and 7.\nAs smaller attentive consistency scores and larger non-attentive consistency scores indicate greater probabilities of hallucinations, we define the following score function as the final estimation of hallucinations in LLMS\n$r = \\lambda r^{att} - r^{non\\_att}$ (8)\nwhere \u03bb denotes a hyper-parameter for balancing the attentive and non-attentive consistency scores. To be noted, with smaller scores, an LLM is more likely to be with hallucinations, and generate non-factual contents."}, {"title": "6 Experiments", "content": "In this section, we conduct extensive experiments to evaluate the effectiveness of AGSER in zero-shot hallucination detection in LLMs."}, {"title": "6.1 Experimental Settings", "content": "Following (Yehuda et al., 2024), we conduct experiments on the Books, Movies and Global Country Information (GCI) datasets, which cover various domains. For the evaluation of hallucination detection results, the detection predictions are compared against the correctness of LLMs' answers. The correctness is determined as in (Yehuda et al., 2024) for samples from different datasets. We use the Area Under Curve (AUC) as the evaluation metric. We conduct comparison among the following zero-shot hallucination detection approaches:\n\u2022 SBERT: Following (Yehuda et al., 2024), we employ a pre-trained Sentence BERT model (Reimers and Gurevych, 2019) as a baseline, which embeds both query and answer into vectors. Then, we calculate the cosine similarity between them as the hallucination prediction.\n\u2022 SelfCheckGPT (Manakul et al., 2023): A detection approach that generates multiple responses and verifies whether they support the original answer.\n\u2022 INSIDE (Chen et al., 2024): An approach that calculates eigenvalues of multiple answers in the sentence embedding space as the hallucination prediction estimator.\n\u2022 InterrogateLLM (Yehuda et al., 2024): A state-of-the-art approach that detects hallucinations via feeding the reverse question into LLMs and verifies whether the original query could be generated.\n\u2022 AGSER (ours): An attention-guided self-reflection approach for zero-shot hallucination detection in LLMs.\nMoreover, we implement AGSER and other compared hallucination detection approaches with four popular and outstanding open-source LLMs: Llama2-7b, Llama2-13b (Touvron et al., 2023), Llama3-8b (Dubey et al., 2024), and Qwen2.5-14b (Qwen, 2024).\nFor InterrogateLLM, we adopt the best version reported in the original paper, i.e., an ensemble of GPT-3 (Brown et al., 2020), Llama2-7b and Llama2-13b. For SelfCheckGPT, INSIDE and InterrogateLLM, we perform resampling of answers for 5 times to calculate the consistency scores.\nIn our proposed AGSER approach, we set k = 2/3 and \u03bb = 1.0. And we adopt the mean value of all layers in an LLM, i.e., $s^{mean}$, for token contribution estimation. Meanwhile, the prompts used in our experiments are illustrated in App. B."}, {"title": "6.2 Performance Comparison", "content": "The zero-shot hallucination detection results with four popular LLMs are illustrated in Tab. 3. With different LLMs, similar comparison conclusions can be observed. Not surprisingly, SBERT performs poorly, for it has no special design for measuring hallucinations in LLMs. Detecting hallucinations in output space and embedding space respectively, SelfCheckGPT and INSIDE have similar detection results. With detection AUC about 80%, they show their effectiveness in hallucination detection. Meanwhile, via asking reverse questions, InterrogateLLM improves the detection results by large margins. It allows the LLMs to rethink the generated answers from a new perspective, rather than only conducting multiple response resampling. Moreover, obviously, compared to the above state-of-the-art approaches, our proposed AGSER approach achieves the best hallucination detection results. With Llama2-7b, AGSER improves SelfCheckGPT, INSIDE and InterrogateLLM by 16.1%, 13.2% and 3.6% in average, respectively. With Llama2-13b, AGSER improves SelfCheckGPT, INSIDE and InterrogateLLM by 10.4%, 7.5% and 2.8% in average, respectively. With Llama3-8b, AGSER improves SelfCheckGPT, INSIDE and InterrogateLLM by 16.4%, 13.7% and 0.9% in average, respectively. With Qwen2.5-14b, AGSER improves SelfCheckGPT, INSIDE and InterrogateLLM by 17.4%, 15.2% and 6.7% in average, respectively. AGSER can significantly improve the detection performance with different LLMs across different datasets. The only exception is evaluating with Llama3-8b on the GCI dataset, in which the detection AUC is nearly 1.0. These observations strongly demonstrate the superiority of using attention values to guide LLMs to conduct self-reflection for detecting hallucinations."}, {"title": "6.3 Ablation Study", "content": "To investigate the effects of components and options in our proposed AGSER approach, we perform extensive ablation studies, and report the corresponding results.\nFirstly, we investigate the effects of attentive and non-attentive queries, respectively. Hallucination detection results of AGSER with only attentive queries or non-attentive queries are shown and compared to the results of AGSER in Tab. 4. Obviously, attentive query plays the major role in the effectiveness of AGSER. And AGSER with only non-attentive queries achieves hallucination detection AUC of 0.575 in average, which indicates non-attentive queries are also necessary for hallucination detection. Specifically, without consideration of attentive queries, the detection AUC of AGSER decreases by 38.6%, 33.3%, 40.7% and 26.6% in average with the four LLMs respectively. Meanwhile, without consideration of non-attentive queries, the detection AUC of AGSER decreases by 0.9%, 0.4%, 0.6% and 1.4% in average with the four LLMs respectively. The above observations are reasonable, because only in a small portion of hallucination samples, the answers of non-attentive queries shall stay unchanged. It is not an extremely strong indicator, but still a necessary one for reflecting the reasoning and answer generating process in LLMs. In a word, both attentive and non-attentive queries are necessary and effective for detecting hallucinations in LLMs."}, {"title": "6.4 Hyper-parameter Study", "content": "To investigate the impact of hyper-parameters in AGSER on the hallucination detection results, we conduct some hyper-parameter studies. Firstly, we show the detection AUC with varying k values in"}, {"title": "6.5 Discussions", "content": "According to the above observations, AGSER significantly outperforms state-of-the-art approaches on zero-shot hallucination detection in LLMs. In addition, AGSER requires a lower computational complexity of resampling. The compared methods, i.e., SelfCheckGPT, INSIDE and InterrogateLLM, perform 5 times of LLM running. In contrast, AGSER only requires 3 times of LLM running (feeding original, attentive and non-attentive queries into LLMs), and 2 times of token usage (attentive and non-attentive queries together have the same tokens as the original one). In a word, AGSER has great advantages in both effectiveness and efficiency. Furthermore, some running example results of AGSER are presented in App. \u0421."}, {"title": "7 Conclusion", "content": "In summary, this work presents a systematic investigation of attention mechanisms in LLMs and proposes AGSER, a novel and computationally efficient approach for zero-shot hallucination detection. Through extensive experiments on three distinct factual knowledge recall tasks with four widely-used LLMs, AGSER demonstrates superior performance compared to existing hallucination detection methods.\nOur findings make several key contributions to the field: (1) we provide new insights into how attention patterns correlate with hallucination behaviors in LLMs; (2) we establish AGSER as a robust and resource-efficient framework for hallucination detection. We believe that this work represents a significant step toward more reliable and trustworthy large language models."}, {"title": "Limitations", "content": "While AGSER demonstrates promising results, we acknowledge several limitations of our approach.\nFirst, the method's reliance on attention allocation patterns during inference restricts its applicability to open-source LLMs, making it challenging to detect hallucinations in closed-source models accessed through APIs.\nFurthermore, while AGSER achieves a remarkable 50% or greater reduction in computational overhead compared to existing self-consistency methods, representing a significant breakthrough in efficiency, our approach still requires three inference passes with two token sets. The remaining computational requirements may still present challenges in specific scenarios, such as real-time applications or resource-constrained environments."}, {"title": "Ethical Considerations", "content": "While our work aims to detect hallucinations, it is crucial to note that LLMs may still produce unreliable, biased, or factually incorrect information. Therefore, we emphasize that the outputs from our experimental results should be interpreted primarily as indicators of hallucination detection effectiveness rather than as reliable sources of factual information."}, {"title": "A More Pilot Study Results", "content": "Following the analysis in Sec. 4, in this section, we present more pilot study results. We provide more results with Llama2-7b, Llama2-13b (Touvron et al., 2023), Llama3-8b (Dubey et al., 2024) and Qwen2.5-14b (Qwen, 2024) on the Books, Movies and GCI datasets (Yehuda et al., 2024). The corresponding results are shown in Tabs. 6-27. We can draw the same conclusion as in Sec. 4, i.e., smaller attentive consistency scores and larger non-attentive consistency scores indicate greater probabilities of hallucinations in LLMs."}, {"title": "B Prompts", "content": "In this section, we detail the prompts for generating answers in LLMs. The prompt template is shown in Fig. 4. And example prompts in the Books, Movies and GCI datasets are illustrated in Figs. 5-7 respectively."}, {"title": "C Example Results", "content": "In this section, we present some running example results of AGSER in Tabs. 28-35."}]}