{"title": "A deep learning-enabled smart garment for versatile sleep behaviour monitoring", "authors": ["Chenyu Tang", "Wentian Yi", "Muzi Xu", "Yuxuan Jin", "Zibo Zhang", "Xuhang Chen", "Caizhi Liao", "Peter Smielewski", "Luigi G. Occhipinti"], "abstract": "Continuous monitoring and accurate detection of complex sleep patterns associated to\ndifferent sleep-related conditions is essential, not only for enhancing sleep quality but also for\npreventing the risk of developing chronic illnesses associated to unhealthy sleep. Despite significant\nadvances in research, achieving versatile recognition of various unhealthy and sub-healthy sleep\npatterns with simple wearable devices at home remains a significant challenge. Here, we report a\nrobust and durable ultrasensitive strain sensor array printed on a smart garment, in its collar region.\nThis solution allows detecting subtle vibrations associated with multiple sleep patterns at the\nextrinsic laryngeal muscles. Equipped with a deep learning neural network, it can precisely identify\nsix sleep states\u2014nasal breathing, mouth breathing, snoring, bruxism, central sleep apnea (CSA),\nand obstructive sleep apnea (OSA)\u2014with an impressive accuracy of 98.6%, all without requiring\nspecific positioning. We further demonstrate its explainability and generalization capabilities in\npractical applications. Explainable artificial intelligence (XAI) visualizations reflect comprehensive\nsignal pattern analysis with low bias. Transfer learning tests show that the system can achieve high\naccuracy (overall accuracy of 95%) on new users with very few-shot learning (less than 15 samples\nper class). The scalable manufacturing process, robustness, high accuracy, and excellent\ngeneralization of the smart garment make it a promising tool for next-generation continuous sleep\nmonitoring.", "sections": [{"title": "I. Introduction", "content": "Sleep occupies about one-third of a person's daily life, and its quality is crucial to overall human\nwell-being. Statistics indicate that more than 60% of adults suffer from poor sleep quality, which\nresults in the loss of approximately 44 to 54 working days per year and contributes to an estimated\nannual global GDP reduction of between 0.64% and 1.31% [1, 2, 3]. Among the culprits are\nsub-healthy or high-risk sleep patterns such as mouth breathing, snoring, teeth grinding, and sleep\napnea, which significantly contribute to the degradation of sleep quality [4, 5, 6, 7]. These\nunhealthy sleep states have been extensively studied and identified as risk factors for various\nchronic diseases, including cardiovascular disease, diabetes, and emotional disorders [8, 9, 10]. \u03a4\u03bf\nimprove people's sleep quality and to provide early warnings for related chronic diseases,\nmonitoring and identifying these unhealthy sleep states have become a focal point in modern health\nmanagement. Traditional sleep monitoring methods, such as polysomnography (PSG), although\ncomprehensive and accurate, require specific facilities and involve complex, costly equipment,\nmaking them unsuitable for long-term or home use [11]. To address these issues, researchers and\nengineers have been exploring more portable, cost-effective, and user-friendly solutions.\nIn recent years, the rapid development of smart wearable devices has provided a promising\napproach to portable sleep health monitoring [12, 13, 14]. The most common monitoring platforms\nintegrate photoplethysmography (PPG) sensors and motion sensors within watches or wristbands, in\nboth academia and industry [15, 16, 17]. Although these platforms offer convenience as a\nscaled-down version of PSG, they inherently lack the capability to collect sufficient human\nphysiological data required for analyzing different sleep states [18]. Some innovative designs\nintegrate physical sensors such as humidity, mechanical, and acoustic sensors in mask-like setups\nplaced near the nose to monitor airflow and sounds during sleep [19, 20, 21, 22, 23]. While these\ndevices cover a broader range of physiological information, achieving comprehensive sleep pattern\nmonitoring with such setups requires integrating multiple sensors, either within the same area of the\nbody or across different body locations, which adds bulkiness and reduces system energy efficiency,\nhindering long-term continuous wear [19, 21, 22]. An alternative approach takes inspirations from\nthe modalities of PSG, integrating electrophysiological sensors like electroencephalograms (EEG),\nelectrooculogram (EOG), and electromyogram (EMG) into facial or ear areas using user-friendly\ntechnologies like electronic skin and miniaturized integration [24, 25, 26]. This method collects\nessential indicators related to sleep health. Despite the improvement in the level of comfort and\ninformation richness, the monitoring accuracy of non-invasive electrophysiological sensors is still\nhindered by inherent artefact noise, leaving room for improvement in accuracy [27, 28]. Thus, there\nis still a lack of versatile technology with both good comfort and high precision for continuous\nmonitoring and recognition of various sleep patterns.\nThe key to addressing this issue lies in finding a signal modality that not only encompasses the\ninformation necessary for analyzing various sleep states but can also be captured by wearable\ndevices in a manner that ensures a high signal-to-noise ratio and comfort. Here, we develop a\nversatile smart garment integrated with an ultrasensitive sensor array directly printed on the collar,\ncapable of monitoring and identifying multiple sleep patterns. Direct printing on garments allows\nfor the integration of multi-functional electronic elements directly onto the garment with scalability"}, {"title": "II. Results", "content": "and design flexibility. The developed smart garment is able to collect mixed-mode signals generated\nby vibrations from various sleep activities such as breathing, snoring, teeth grinding, and sleep\napnea, which are transmitted to the extrinsic laryngeal muscles from multiple anatomical locations\nincluding the velum, oropharynx, tongue, and epiglottis (Figure 1a). Utilizing a multi-channel\ngraphene textile strain sensor, screen printed at its collar based on principles of ordered cracks and\nselective sizing treatments, the smart garment leverages ultrahigh sensitivity (gauge factor >100),\nscalability ( \u00b1 20% conductivity fluctuation), and durability (stable over 10,000 cycles of stretching\ntests) to continuously monitor subtle vibrations of the extrinsic laryngeal muscles while ensuring\nuser comfort. Additionally, due to its multi-channel design, the smart garment can be easily used by\nwearers in a positioning-free manner in real-world scenarios. The designed deep learning model,\nSleepNet, uses the captured signals to accurately identify six sleep states ranging from healthy to\nsub-healthy to high-risk, including nasal breath, mouth breath, snoring, bruxism, central sleep apnea\n(CSA), and obstructive sleep apnea (OSA), achieving an accuracy of 98.6% along with decent\ninference speed (Figure 1b). Explainable artificial intelligence (XAI) visualizations confirm that the\nmodel comprehensively understands the sleep patterns, avoiding biases towards noisy regions, thus\ndemonstrating its robustness. Moreover, transfer learning tests show that after few-shot learning\n(with only 15 samples per class), the model can achieve up to 95% classification accuracy on new\nusers, showcasing the system's powerful generalization capabilities. In Table 1, we have\nsummarized the functions and features of our system compared with other wearable sleep\nmonitoring systems in the literature. We believe that this versatile smart garment will become a\npivotal tool in enhancing diagnostic precision, personal health tracking, and therapeutic\ninterventions for sleep-related disorders, as well as fostering advancements in telemedicine and\npersonalized medicine strategies."}, {"title": "Printed strain sensor array on garments", "content": "Figure 2a illustrates the multi-layered structure of the strain sensing array screen-printed on a\nhigh-neck top made of elastic knitted fabric. In the multi-layer screen printing process, printing\nquality is highly sensitive to parameters and can be easily affected by changes in printing conditions\nand ink properties. Here we introduce, for the first time, a starching layer consisting of sodium\ncarboxymethyl cellulose (CMC Na) and polyurethane acrylate (PUA) printed onto the interconnect\nand sensing areas to customize the rigidity and printability of the substrate. Both cellulose\nderivatives and acrylates are common starching agents in garment printing industry [29]. CMC Na\nis a water-soluble polymer derived from cellulose known for its film-forming ability, which\nprovides a smooth and uniform surface [30]. CMC Na also improves the adhesion of the printed\ngraphene ink to the substrate, reducing the likelihood of delamination under mechanical stress. The\nPUA provides high stiffness and excellent adhesion by forming a robust, cross-linked network upon\nUV exposure [31]. Introducing a rigid PUA layer in the textile strain sensor array modifies the\nrigidity of selected area on the textile, redistributing strain caused by body movements during sleep\n(Supplementary Figure 1). The isolated area remain inert to large-scale uniaxial stretching, ensuring\nthat local strain is accurately measured without interference, as shown in Supplementary Figure 3.\nThe UV-curable nature also prevents clogging of the screen during the printing process, extending\nthe lifespan of the printing screen and maintaining consistent printing quality. After starching"}, {"title": "Monitoring of sleep behaviour with the strain sensor array", "content": "We employed a six-channel strain sensor array integrated into the collars of textile garments,\npositioned around the participants' necks. This setup was designed to collect minute vibrational\nsignals from the extrinsic laryngeal muscles, which vary according to different sleep behaviours"}, {"title": "Sleep patterns recognition with a deep learning model", "content": "After simple preprocessing, which involves labeling (see detailed protocols in Supplementary Note\n2), segmentation (10s signals were segmented into one sample), and Z-score normalization, signals\nfrom the channel with the strongest response are fed into our specially designed deep learning\nmodel (SleepNet) for sleep pattern recognition (Figure 4a). The SleepNet is composed of three core\ncomponents: First, the learnable positional encoder adopts a residual bidirectional LSTM (BiLSTM)\nframework to understand the sequential nature of the input data. This approach surpasses traditional\nsinusoidal positional encoding by dynamically learning positional information, which is particularly\nadvantageous in sleep pattern analysis where the temporal relationship between events can signify\ndifferent breath cycles or disturbances [33]. Second, the multi-head self-attention module, derived\nfrom Transformer architecture, enables nuanced discrimination of significance across the data\nsequence. By effectively utilizing long-range dependencies, the model ensures a comprehensive\nunderstanding of the sequential data, reflecting the true complexity of sleep behaviours over time\n[34]. Lastly, the one-dimensional Residual Network (ResNet) layers function as a hierarchical\nfeature extractor. With their ability to skip connections, they prevent the vanishing gradient problem\nand enhance the flow of information, thus allowing the model to learn more complex patterns\neffectively. The 1D convolution within these layers ensures that the model is tailored to handle\ntime-series data, offering a nuanced analysis of temporal patterns [35].\nFigure 4b displays the confusion matrix for the model's classification of sleep patterns. The model\nachieved impressive classification results: the accuracy for each category was above 95%, with an\noverall accuracy of 98.6%. In the comparative experiments shown in Figure 4c, SleepNet surpassed\nthe performance of its individual component architectures, such as the Transformer and 1D ResNet,\nas well as other state-of-the-art models in terms of accuracy. Additionally, it's noteworthy that after\npruning the least important 50% of the nodes in the 1D-ResNet module of the model and re-training\nto obtain a pruned SleepNet, not only did the FLOPs (a critical indicator of model inference speed\nand energy efficiency) decrease by 30%, but there was also a slight increase in accuracy. This can\nbe attributed to the phenomenon known as \"pruning-induced efficiency\", where removing\nredundant or less important nodes can lead to a more streamlined and efficient network. The\nre-training phase helps the model to re-allocate its resources towards the most salient features,\npotentially improving generalization and thus accuracy [36]."}, {"title": "III. Discussion", "content": "Decoding human sleep patterns is important yet complex. Despite the encouraging development of\nwearable devices for sleep health monitoring in recent years, creating a system that combines\nversatility, comfort, and accuracy remains a significant challenge. In this work, we have designed a\nsmart garment integrated with a six-channel strain sensor array. This ultrasensitive strain sensor\narray, characterized by its excellent robustness and durability, can collect subtle vibrations from the\nextrinsic laryngeal muscles associated with various sleep patterns, and its multi-channel design\neliminates the need for positioning due to its spatial resolution. Despite utilizing only a single\nmodality of strain response signals, our smart garment, equipped with a customized deep learning\nneural network, can comprehensively analyze and recognize subtle vibrations originating from\nvarious physiological sites and transmitted to the extrinsic laryngeal muscles. It accurately classifies\nsix sleep patterns: nasal breath, mouth breath, snoring, bruxism, CSA, and OSA. Additionally, it\ncan efficiently and effectively adapt to new users, maintaining high accuracy in its classifications.\nWe believe our smart garment offers a promising solution for versatile sleep monitoring in wearable\ndevices, not only suitable for the consumer electronics market to provide ongoing sleep monitoring\nfor general users but also as a convenient, low-cost alternative for clinical sleep monitoring."}, {"title": "IV. Methods", "content": "TIMREX KS 25 Graphite (particle size of 25\u00b5m) was sourced from IMERYS. Stretchable\nconductive silver ink was obtained from Dycotec Materials Ltd. Ethyl cellulose and sodium\ncarboxymethyl cellulose were purchased from SIGMA-ALDRICH. Flexible UV Resin Clear was\nacquired from Photocentric Ltd. The textile substrate, composed of 95% Polyester and 5% spandex,\nwas procured from Jelly Fabrics Ltd."}, {"title": "Ink Formulation", "content": "The graphene ink for screen printing was prepared following a reported method. Briefly, 100g of\ngraphite powder and 2g of ethyl cellulose (EC) were mixed in 1L of isopropyl alcohol (IPA) and\nstirred at 3000 rpm for 30 minutes. The mixture was then added into a high-pressure homogenizer\n(PSI-40) at 2000 bar pressure for 50 cycles to obtain graphene dispersion. The graphene dispersion\nis centrifuged at 5000g for 30 min to remove unexfoliated graphite. To prepare the CMC Na\nstarching solution, CMC Na was dissolved in water at 20% wt. concentration."}, {"title": "Fabrication of Textile Strain Sensor Arrays", "content": "The textile substrate was washed with detergent, thoroughly dried, and then treated with UV-ozone\nfor 5 minutes to clean the surface. Screen printing was performed using a 165T polyester silk screen\non a semi-automatic printer (Kippax & Sons Ltd.) set with a squeegee angle of 45 degrees, a spacer\nof 2mm, a coating speed of 10mm/s, and a printing speed of 40mm/s. Printing pressure was"}]}