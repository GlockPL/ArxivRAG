{"title": "Generalizability of Graph Neural Networks for Decentralized Unlabeled Motion Planning", "authors": ["Shreyas Muthusamy", "Damian Owerko", "Charilaos I. Kanatsoulis", "Saurav Agarwal", "Alejandro Ribeiro"], "abstract": "Unlabeled motion planning involves assigning a set of robots to target locations while ensuring collision avoidance, aiming to minimize the total distance traveled. The problem forms an essential building block for multi-robot systems in applications such as exploration, surveillance, and transportation. We address this problem in a decentralized setting where each robot knows only the positions of its k-nearest robots and k-nearest targets. This scenario combines elements of combinatorial assignment and continuous-space motion planning, posing significant scalability challenges for traditional centralized approaches. To overcome these challenges, we propose a decentralized policy learned via a Graph Neural Network (GNN). The GNN enables robots to determine (1) what information to communicate to neighbors and (2) how to integrate received information with local observations for decision-making. We train the GNN using imitation learning with the centralized Hungarian algorithm as the expert policy, and further fine-tune it using reinforcement learning to avoid collisions and enhance performance. Extensive empirical evaluations demonstrate the scalability and effectiveness of our approach. The GNN policy trained on 100 robots generalizes to scenarios with up to 500 robots, outperforming state-of-the-art solutions by 8.6% on average and significantly surpassing greedy decentralized methods. This work lays the foundation for solving multi-robot coordination problems in settings where scalability is important.", "sections": [{"title": "I. INTRODUCTION", "content": "A fundamental challenge in large-scale multi-robot systems is the unlabeled motion planning problem, where a group of identical robots must be assigned to a set of target locations without predetermined pairings, ensuring collision-free paths while minimizing the total distance traveled [1]. This problem is inherently complex because it combines the difficulties of the assignment problem\u2014a combinatorial optimization challenge\u2014with those of motion planning in continuous geometric spaces. Efficiently solving unlabeled motion planning has significant implications for applications such as warehouse automation [2], swarm robotics [3], search and rescue missions [4], and exploration [5].\nA common formulation for assigning robots to target locations is the Linear Sum Assignment Problem (LSAP) [6], which seeks to minimize the total distance between robots and their assigned targets. The LSAP is well-studied, and algorithms based on the Hungarian method [6] can solve it in polynomial time [7]. However, these algorithms are centralized and have cubic complexity. Therefore, they do not scale well with large numbers of robots. Moreover, the LSAP does not account for inter-robot collisions, which are crucial in multi-robot systems. Approaches often rely on online replanners or potential field methods [8], which can prevent collisions but often disregard the global optimality of the solution.\nAnother approach minimizes the sum of squared distances for the assignment and then employs constant velocity trajectories so that all robots reach their respective targets simultaneously, as seen in the Concurrent Assignment and Planning of Trajectories (CAPT) algorithm [9]. This method ensures collision avoidance under mild restrictions on the initial configurations of the robots. However, the solution may not be optimal in terms of the total distance traveled; for example, Figure 1 illustrates a scenario where the CAPT solution is suboptimal. Additionally, since CAPT also relies on the Hungarian algorithm, it inherits scalability issues with large numbers of robots.\nTo mitigate the issues of a centralized system, we consider the decentralized setting, where the robots have only localized information and communicate with neighboring robots. Determining an optimal controller in a decentralized setting is difficult for many continuous-space tasks [10], leading to the development of approximate methods. Recently, learning-based methods, in particular graph neural networks (GNNs) have been used for multi-robot systems for applications such as multi-agent pathfinding [11], flocking [12], target tracking [13], and coverage control [14], [15]. A common theme of these approaches is that an expert clairvoyant algorithm is used to generate a dataset for imitation learning. Previous research has demonstrated that combining GNNs and reinforcement learning can effectively address problems that are unsolvable with imitation learning approaches alone [1], [16]. However, reinforcement learning is known to be a sample inefficient learning algorithm [17]. It is possible to lower the number of samples needed to train reinforcement learning, though, by referencing an optimal policy in the reward function [18]. Pre-training a model with imitation learning and fine-tuning with reinforcement learning can further improve sample efficiency [19]. To optimize the policy and obtain desired results, reward engineering has also been used to find a reward function to efficiently improve a policy [20].\nIn this paper, we present a learning-based approach that minimizes the total distance traveled while avoiding collisions. We focus on a decentralized setting, where robots have local information and limited communication capabilities. Under these constraints, we show that a GNN can imitate a centralized approach, outperforming decentralized baselines. Fine-tuning with reinforcement learning further improves GNN performance and provides nearly collision-free control. To our knowledge, this work is the first example of a continuous-space multi-agent system used for motion planning being trained using imitation learning and fine-tuned using reinforcement learning. Finally, we present empirical results that demonstrate the scalability of the proposed approach: a model trained on 100 robots generalizes to scenarios with up to 500 robots."}, {"title": "II. UNLABELED MOTION PLANNING", "content": "Consider N agents with radius R and a set of N goal positions. We study the problem of navigating the agents from their original positions to the goal positions. There is no prior assignment of agents to goals. Hence, this problem is known as unlabeled motion planning. To describe the problem mathematically, let the positions of the agents be\n$X(t) = [x_1(t) \\ x_2(t) \\ ... \\ x_N(t)]^T \\in \\mathbb{R}^{N\\times 2}$                                                                     (1)\nwhere $x_i(t) \\in \\mathbb{R}^2$ is the position of the $i^{th}$ at the time t. Similarly, let\n$G = [g_1 \\ g_2 \\ ... \\ g_N]^T \\in \\mathbb{R}^{N\\times 2}$                                                      (2)\nrepresent the goal positions.\nIn this article, we consider a first-order dynamical model of the agents' motion. The input to the dynamical system is each agent's velocity,\n$U(t) = [u_1(t) \\ u_2(t) \\ ... \\ u_n(t)]^T \\in \\mathbb{R}^{N\\times 2}$.                                                                (3)\nLike in Equation (1), the velocity of the $i^{th}$ agent is $u(t)$ at time t. The control inputs are constrained by the maximum velocity $U_{max} \\in \\mathbb{R}_+$.\n$||u_i(t)|| \\leq U_{max}$.                                                                                                                     (4)\nTherefore, we can describe the motion of the agents in discrete time as\n$X(t + 1) = X(t) + U(t)$.                                                                                                                        (5)\nIn Equation (5), we assume the time step to be one, without loss of generality. This simplifies notation, but we discretize at a finer interval of 0.1 seconds in our implementation.\nTo quantify the quality of a solution, we introduce coverage in Equation (6). It is defined as the proportion of goals that have an agent within a threshold distance Rc.\n$c(t) = c(X(t), G) = \\frac{1}{N} \\sum_{n=1}^{N} 1( \\min_j ||g_i - x_j||_2 < R_c)$.                                                           (6)\nWe can calculate coverage by finding the closest agent to each goal and checking if the distance is less than Re. To be practically viable, a control policy should produce trajectories that have few or no collisions. Define $p_i(t)$ as the number of collisions at a time t for the ith agent,\n$p_i(t) = \\sum_{j \\neq i} 1(||x_i(t) - x_j(t)||_2 < 2R)$                                                                                         (7)\nwhere R is the radius of the agents. An ideal controller would produce trajectories satisfying $p_i(t) = 0$, but we can practically allow $p_i(t) \\approx 0$.\nA potential solution to the unlabeled motion planning problem is a control policy \u03a0(X(t), G) such that\n$U(t) = \\Pi(X(t), G)$.                                                                                                                            (8)\nThe policy is feasible if it satisfies Inequality (4). We want to find a policy that converges to the highest coverage in the shortest amount of time. We define the discounted coverage:\n$C(\\Pi) = E_{X(0),G}[ \\sum_{t=0}^{T} \\gamma^t c(X(t), G)]$.                                                                                  (9)\nwhere $T \\in \\mathbb{Z}$ is the time horizon and $ \\gamma \\in \\mathbb{R}_+$ is a discount factor. To prioritize coverage in the short term, we assume $\\gamma < 1$. A lower discount factor prioritizes short-term performance. We are interested in finding a policy that maximizes the discounted coverage over a finite horizon T.\nA common way to solve the unlabeled motion planning problem is to first find a matching between agents and goals. Using the output of the linear sum assignment problem (LSAP) [6] with the distance between agent and goals as the cost yields one potential solution. Let $D(t) \\in \\mathbb{R}^{N\\times N}$ be a matrix with entries $D_{ij} = ||x_i(t)-g_j||_2$. Then, the solution to the related LSAP problem is a permutation matrix,\n$P_{LSAP}(t) = argmin_{P \\in P_N} \\mathbb{1}^T (P \\odot D(t)) \\mathbb{1}$                                                                     (10)\nwhere $P_N$ is the set of $N \\times N$ permutation matrices and $\\odot$ is the Hadamard product. Given this assignment, C(II) is maximized by each agent heading towards its assigned target at maximum velocity. The resultant trajectories never intersect unless the positions and goals of two agents are collinear [9].\nWhile the trajectories do not intersect, they are not guaranteed to be collision-free for agents with a non-zero radius [9]. To address this, [9] proposes the CAPT algorithm, which guarantees collision-free trajectories. The algorithm first uses LSAP with a cost matrix $D^2(t)$, the distance squared between the agents and goals.\n$P_{CAPT}(t) = argmin_{P \\in P_N} \\mathbb{1}^T (P \\odot D^2(t)) \\mathbb{1}$                                                                (11)\nFor the dynamics in Equation (5), CAPT produces a constant velocity trajectory for each agent, such that all agents reach their assigned goal at the same time. Each agent moves directly to their assigned target, but agents with a lower travel distance move at a proportionally lower speed.\nWe will refer to Equation (10) as the LSAP assignment and to Equation (11) as the CAPT assignment. Note that minimizing the sum of the distances squared, as CAPT does, leads to a different assignment than minimizing the sum of the distances. Figure 1 shows a representative example where the two solutions differ. In the LSAP policy, the first agent moves a distance of 1, and the second agent travels a distance of 5. In the CAPT solution, both agents will need to travel a distance of $\\sqrt{10}$. In our numerical experiments, we show that the LSAP policy outperforms the CAPT policy in terms of discounted coverage. However, the LSAP policy does not guarantee collision-free trajectories. Therefore, we use both imitation and reinforcement learning to find a policy that is close to the LSAP solution, but avoids collisions."}, {"title": "A. Decentralized Motion Planning", "content": "Both the CAPT and LSAP policies are centralized, meaning that they require global information about the positions of all the agents and all the obstacles. The LSAP problem also has cubic complexity in the number of robots [21]. A decentralized solution is well-motivated to address the issues with communication and complexity. We will say that a policy is decentralized whenever the global policy can be decomposed into,\n$\\Pi(t) = [\\pi_1(t) \\ \\pi_2(t) \\ ... \\ \\pi_N(t)]$                                                                                              (12)\nwhere $\\pi_i(t)$ is a local policy computed by each agent. A policy $\\pi_i(t)$ is local if each agent independently observes its surroundings and computes the policy through a series of information exchanges with nearby agents.\nWe assume that each agent has information about its current velocity, and can observe the relative position of the k nearest agents and the relative positions of the k closest goals. Let $o_i(t) \\in \\mathbb{R}^{2(1+2k)}$ be a vector concatenating these local observations. The number of observed agents and goals could vary, but this is a minor simplification.\nTo model decentralized communication, we consider a communication graph G = (V,E) with the ith agent represented by the ith node in the set, V = {1,..., N}. Let Ni be the set of k closest agents to the ith agent. Then, we say that whenever $j \\in N_i$ then there is an edge (j, i) \u2208 E from i to j. We allow each agent to iteratively aggregate information from its neighbors Ni.\nAs an example, consider the following decentralized policy, which aggregates information from a d hop neighborhood. We will call this the d-hop decentralized policy. First, each agent finds the relative positions of all agents and goals within its d-hop neighborhood in G. This requires d-1 information exchanges on G, since each agent already observes the relative positions of its neighboring agents without communication. Then, using this d-hop information, each agent locally solves the LSAP. The agent moves toward its locally assigned agent at maximum velocity. This family of policies has several weaknesses. First, the d-hop policy does not make guarantees for the number of collisions, similar to the centralized LSAP policy. Second, the amount of information that needs to be communicated can grow exponentially as we increase the number of hops. Finally, as we increase the number of hops, the local complexity is cubic with the number of agents in the d-hop neighborhood."}, {"title": "III. APPROACH", "content": "We propose to use a graph neural network (GNN) to parameterize the policy. We first use imitation learning to train the GNN to approximate the centralized LSAP policy. Such a GNN is unlikely to provide a collision-free trajectory. Therefore, we fine-tune the GNN using reinforcement learning with a reward function that balances maximizing coverage and minimizing the number of collisions."}, {"title": "A. Architecture", "content": "The GNN architecture is well-motivated. Graph convolution layers allow neighboring agents to communicate and collaborate, and as shown by [1], GNNs can leverage the locality of the motion planning problem. A GNN can also scale to larger versions of the same problem without retraining [22], and has the required expressive power [23], [24] to leverage the communication graph. Additionally, between each GNN layer, we incorporate local multi-layer perceptrons (MLPs) to increase the capacity of the architecture without increasing the amount of required communication.\nThe GNN operates over the communication graph G, which we defined in Section II-A. Let S(t) \u2208 {0,1}N\u00d7N be an adjacency matrix associated with G. Each edge (i, j) \u2208 E is represented by an entry of 1 in the ith row and jth column of S(t). Each layer in the GNN consists of a graph convolution, a pointwise nonlinearity, and a local MLP. The input to the lth layer of the GNN is a matrix $Z^{(l-1)} \u2208 \\mathbb{R}^{N \\times F}$. It is first processed by a graph convolution, which aggregates information from each agent's neighborhood:\n$\\hat{Z}^{(l)} = \\sigma [ \\sum_{k=0}^{K-1} S_k Z^{(l-1)} H_k^{(l-1)} ]$                                                                                              (13)\nIn Equation (13), $H_k^{(l-1)} \u2208 \\mathbb{R}^{F \\times F}$ are the parameters of the graph convolutional layer and $\\sigma(\\cdot)$ is a pointwise nonlinearity. The intermediate output, $\\hat{Z}^{(l)} \u2208 \\mathbb{R}^{N \\times F}$ is then processed by an MLP to obtain an output $Z^{(l)}$, which will be fed to the next layer.\n$Z^{(l)} = Z^{(l-1)} + \\sigma(\\sigma(\\hat{Z}^{(l)} W_1)...W_{L_{MLP}})$                                                                                             (14)\nEquation (14) describes an MLP with $L_{MLP}$ layers, F features and G hidden features. Hence, the weight matrices are $W_1 \u2208 \\mathbb{R}^{F \\times G}$ in the first layer, $W_i \u2208 \\mathbb{R}^{G \\times G}$ in the middle layers, and $W_{L_{MLP}} \u2208 \\mathbb{R}^{G \\times F}$ in the final layer. Notice, Equation (14) describes an MLP which locally processes information at each agent.\nThe inputs to the model are the local observations made by each agent, $o_i(t)$ as defined in Section II-A. Let\n$O(t) = [o_1(t) \\ o_2(t) \\ ... \\ o_N(t)]$                                                                                                                                      (15)\nbe a matrix in $\\mathbb{R}^{N \\times 2(1+2k)}$, representing the observations made at each agent. The output of the model is the control actions U(t) as defined in Equation (3). Therefore, denote the GNN model as so that\n$U(t) = \\Phi(O(t), S(t); \\mathcal{H})$                                                                                                                              (16)\nwhere H is a set of all the learnable parameters. We add MLPs at the input and output of the model to allow the number of features at the input and output to differ from F."}, {"title": "B. Imitation Learning", "content": "We use imitation learning (IL) [25] to find a decentralized GNN policy that approximates the centralized LSAP policy. This policy is described in Section II. We assume that we can readily sample tuples of the form (O, S, U*) from some abstract distribution D. Hence, imitation learning is an empirical risk minimization problem. The solution is a set of model parameters HIL,\n$\\mathcal{H}_{IL} = argmin_{\\mathcal{H}} E_{\\mathcal{D}} [||\\Phi(O, S; \\mathcal{H}) - U^*||_2]$.                                                                                                              (17)\nwhich minimizes the expected squared error between the output of the model and the LSAP policy. The natural way to generate samples for training is to start at some initial X(t), Y and simulate the dynamics of the simulation. As the control action, we can either use the LSAP policy, U*, or the GNN policy \u03a6. In our experiments, we found that a mix of both is effective.\nThe resulting model $(O, S; \\mathcal{H}_{IL})$ will be an approximation of the centralized policy. Since I is a GNN that contains local information, we may not be able to reproduce the centralized policy. Even if we could, the LSAP policy does not perform collision avoidance. Additionally, in the decentralized setting, there may be other behaviors, such as exploration, that might improve performance in terms of discounted coverage from Equation (9)."}, {"title": "C. Reinforcement Learning", "content": "We use reinforcement learning (RL) to fine-tune the policy learned through imitation learning. The goal is to find a policy that attains similar coverage, but produces collision-free trajectories. Hence, we define the reward function for the $i^{th}$ as,\n$r_i(t) = exp[ -\\alpha d_i - \\beta p_i(t)]$.                                                                                                                                          (18)\nwhere $d_i$ is the distance between the ith agent and its assigned goal $y_i$ based on the LSAP assignment $P_{LSAP}(t)$. The values $\\alpha, \\beta > 0$ are scaling coefficients. This reward function encourages agents to navigate to the optimal targets while penalizing collisions.\nDeep deterministic policy gradients (DDPG) [26] is an off-policy RL algorithm that can handle continuous action spaces. We use Twin Delayed DDPG (TD3) [27], which is a variation on DDPG that improves training stability. In DDPG and related algorithms, two models are trained simultaneously: an actor and a critic. The actor model $U = \\Phi(O(t), S(t); \\mathcal{H}_A)$ is the GNN that we pretrained with imitation learning. The critic $Q(t) = \\Psi(O(t), U(t), S(t); \\mathcal{H}_Q)$ is a similar GNN model that additionally takes the current action as an input. The models do not share parameters, and the critic will have different input and output dimensionalities in the read-in and read-out MLPs. The output of the critic, $Q \\in \\mathbb{R}^N$ estimates the expected future reward given the observations and actions.\nSince DPPG and TD3 are off-policy RL algorithms, we assume that we can readily sample tuples $(O(t), S(t), U(t), R(t), O(t + 1) \\sim \\mathcal{D}$ where $R(t) \\in \\mathbb{R}^N$ is a vector with elements $r_i(t)$ and O(t + 1) is the observation at the next time-step. We can do this by sampling trajectories. We use the output of the actor policy with additive Gaussian noise to sample the control actions."}, {"title": "IV. NUMERICAL EXPERIMENTS", "content": "In this section, we evaluate the performance of the imitation learned model and analyze the impact of fine-tuning with reinforcement. We compare the performance of both models against LSAP, CAPT, and n-hop policies. To evaluate the performance, we consider both the coverage and the number of collisions. In particular, we calculate the coverage with a discount factor of $\\gamma = 0.99$ and threshold distance $R_c = 0.2$.\nFor training and evaluation, we simulate the dynamical system as follows. We initialize the positions of the agents and goals uniformly within a w \u00d7 w region of interest. We sample the positions so that no two agents or goals are within 2R of each other with R = 0.05. We assume that each agent can communicate with the k = 3 nearest agents and that it can observe the same amount of nearest agents and obstacles. Given these initial conditions, we simulate the system for 20 seconds with T = 200 discrete time steps. The maximum velocity of each agent is $U_{max} = 0.5$ meters per second."}, {"title": "A. Imitation Learning", "content": "We use the GNN architecture described in III-A. The model is composed of L = 5 layers and K = 3 filter taps. Each layer contains a graph convolution with feature dimension F = 128 followed by an MLP with depth $L_{MLP}$ = 3 and G = 256 hidden features. To optimize the model parameters, we use AdamW [28] a learning rate of $5 \\cdot 10^{-4}$, weight decay of $10^{-8}$, and a batch size of 512. We train the GNN using imitation learning for 161 epochs. At the beginning of each epoch, we sample 100 trajectories that are stored in an experience replay buffer with a maximum size of 100,000. During training, we use w = 10 meters with N = 100 agents.\nFigure 2a shows the average coverage and number of collisions per agent at different training epochs. After 40 epochs, the coverage plateaus at 0.8. The number of collisions per agent decreases over time and continues to decrease, even after the coverage plateaus. This suggests that the LSAP policy already provides a suitable solution in terms of collision avoidance, even though, as evidenced by Table I, it is not collision-free.\nFigure 3 shows the average result of the policy over 50 random realizations, as well as its comparison to the centralized and decentralized policies. The figure shows that, in terms of coverage, the IL policy clearly outperformed the decentralized n-hop policies. The IL policy is dominated by the centralized LSAP policy on which it was trained."}, {"title": "B. Reinforcement Learning", "content": "After pre-training the GNN with IL, we use RL to fine-tune the parameters. The goals are to improve coverage and reduce collisions. Similarly to [19], for the first 100 epochs, we freeze the actor model and train only the critic. The initial learning rates are 0 and $10^{-4}$ for the actor and critic, respectively. Then, for the next 50 epochs, the learning rates are linearly interpolated to $10^{-5}$ and $5 \\cdot 10^{-5}$, respectively. Afterward, we continue training until we reach 500 epochs.\nThe effects of this schedule are clearly visible in Figure 2b. The scaling coefficients in Equation (18) for the policy where $\\alpha = 30$ and $\\beta = 0.1$. All other hyperparameters are the same as during imitation learning.\nAs shown by Table I, the RL policy achieves an average of 0.02 collisions per simulation. This is a drastic improvement from 45.20 for the IL policy and is almost collision-free. For comparison, the LSAP policy had 4.10 collisions on average. At the same time, the discounted coverage improved by 5.6% after fine-tuning to 0.76. This is an 8.6% improvement over CAPT (see Figure 3). The RL policy has the steepest initial improvement in coverage, even though its steady state performance is lower than the IL policy."}, {"title": "C. Generalizability", "content": "In general, one drawback of deep learning methods is the time it takes to train. Reusing the same model for environments with a different number of agents is desirable, particularly when we can do so without retraining [29], [30]. Our proposed GNN architecture makes no assumptions about the size of the graph. Thus, we consider out-of-distribution examples with a varying number of agents and their densities. During training, N = 100 agents and targets were placed in a w = 10 meter wide area. This implies an average density of $\\rho = N/w^2 = 1.0$ agents per meter squared. We test the policy on environments with various number of agents N \u2208 {20, 50, 100, 200, 500} and agent densities \u03c1 \u2208 {0.2, 0.5, 1.0, 2.0, 5.0}. For each combination, we perform 50 simulations.\nFigure 4 shows the effect of changing the number of agents as their density is kept constant at \u03c1 = 1.0. The coverage is nearly identical as we scale the number of agents. The scenarios with 20 agents perform better, likely because the GNN can process the entire graph. Conversely, Figure 5 shows the effect of changing the density \u03c1 while keeping the number of agents constant. Changing the density has a large impact on performance. This is partially explained by the fact that at lower densities, the average initial distance between agents and goals is higher. Specifically, since these observations contain larger values than what was seen in the training dataset, the model may not be able to generalize well to these inputs. Additionally, the lower coverage may stem from the fact that a larger distance between an agent and its corresponding goal simply means that more time is needed for the agent to reach the goal.\nThe impact of changing both N, \u03c1 is summarized by Figure 6, which demonstrates that the policy obtains similar discounted coverage values at the same agent density. We again note the positive correlation between density and coverage, an unintended but not necessarily harmful consequence of the GNN architecture and the nature of the problem. The impact on the number of collisions is summarized by Table II we normalize the number of collisions by 100/N. As we deviate from the trained scenario, the collision frequency increases, especially for densities lower than one. However, in many scenarios, the frequency of collisions remains close to zero. For comparison, recall that we previously observed 4.10 collisions per 100 agents for the LSAP policy. Overall, the GNN policy can generalize most scenarios in terms of both coverage and collision avoidance."}, {"title": "V. CONCLUSIONS", "content": "The paper addressed the multi-agent unlabeled motion planning problem with the following constraints. First, the agents only perceived nearby agents and goals. Second, the agents had limited communication capability. We proposed to use a GNN to parameterize the control policy. Model training consisted of two phases: (i) imitation learning produced a policy that approximated a centralized policy, and (ii) reinforcement learning fine-tuned the policy to improve coverage and reduce collision frequency. The final policy outperformed baseline decentralized algorithms. Extensive empirical results established that the GNN policy is scalable to larger teams of agents. The policy noticeably improves performance as agent density increases despite being trained with lower agent density. Future work could involve studying the problem in an obstacle-rich environment and performing real-world experiments."}]}