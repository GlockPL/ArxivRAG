{"title": "Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement Learning", "authors": ["Enrique Adrian Villarrubia-Martina", "Luis Rodriguez-Benitez", "David Mu\u00f1oz-Valero", "Giovanni Montana", "Luis Jimenez-Linares"], "abstract": "This paper addresses a critical challenge in the high-speed passenger railway industry: designing effective dynamic pricing strategies in the context of competing and cooperating operators. To address this, a multi-agent reinforcement learning (MARL) framework based on a non-zero-sum Markov game is proposed, incorporating random utility models to capture passenger decision making. Unlike prior studies in areas such as energy, airlines, and mobile networks, dynamic pricing for railway systems using deep reinforcement learning has received limited attention. A key contribution of this paper is a parametrisable and versatile reinforcement learning simulator designed to model a variety of railway network configurations and demand patterns while enabling realistic, microscopic modelling of user behaviour, called RailPricing-RL. This environment supports the proposed MARL framework, which models heterogeneous agents competing to maximise individual profits while fostering cooperative behaviour to synchronise connecting services. Experimental results validate the framework, demonstrating how user preferences affect MARL performance and how pricing policies influence passenger", "sections": [{"title": "1. Introduction", "content": "Dynamic pricing is a pivotal strategy for revenue optimisation and demand management across industries. By adjusting prices in real time, based on supply and demand, companies can enhance profitability and better align services with consumer needs. However, there are important challenges involved in implementing effective dynamic pricing strategies, particularly in environments with complex interactions among multiple stakeholders. These challenges include modelling market dynamics, predicting consumer behaviour, and optimising pricing decisions, under constraints such as competition, cooperation, and limited resources. Machine learning (ML) has shown great promise in addressing these challenges, offering data-driven approaches to learning and adapting pricing strategies over time.\nReinforcement learning (RL), in particular, has emerged as a powerful tool for developing dynamic pricing strategies. RL enables agents to learn optimal policies through interactions with the environment, which means it is well suited to applications requiring real-time decision making. RL-based dynamic pricing has been successfully applied in sectors such as electricity markets [1, 2, 3], airlines [4], and mobile networks [5]. These studies primarily focus on single-agent and multi-agent RL settings, where agents aim to maximise profits through individual or collaborative actions. Despite this progress, dynamic pricing for high-speed rail systems remains under-researched, even though the industry poses unique challenges and opportunities compared to other domains.\nHigh-speed railways are characterised by an intricate interplay of competition and cooperation between operators. Operators may cooperate to offer connecting journeys (e.g., A-C via A-B and B-C) or compete directly on shared routes (e.g., A-D). These interactions require sophisticated pricing strategies, as decisions in one market can ripple through others. Moreover, as networks grow larger and more interconnected, operators must balance profitability with seamless service integration, further complicating the optimisation of pricing strategies.\nIn order to apply RL effectively in this context, it is crucial to model passenger decision making accurately, as this directly affects the environment in which agents learn. Discrete Choice Modelling (DCM) is a widely used tool to model and predict travel choices. These models, grounded in the principles of classic Random Utility Models (RUMs), provide a framework for understanding how travellers make decisions from among a finite set of alternatives. DCM assigns utility values to travel options, assuming individuals choose the option with the highest utility. While DCM has been effectively applied to urban railway systems [6], these applications typically focus on short-distance, high-frequency travel. In contrast, intercity railways require models capable of handling a range of passenger behaviours, multi-operator networks, and longer travel distances. Despite their utility, existing simulators such as ROBIN [7] are limited in their ability to model dynamic pricing and connecting services, and they lack compatibility with RL frameworks like OpenAI Gymnasium [8]. These limitations lead to gaps in the ability to develop and evaluate advanced RL-based dynamic pricing strategies for high-speed railways.\nTo address these gaps, this paper introduces a multi-agent reinforcement learning (MARL) framework for dynamic pricing in high-speed railway networks. Central to this framework is a novel RL simulator called RailPricing-RL that extends ROBIN by enabling dynamic pricing, modelling multi-operator journeys, and supporting MARL algorithms. The simulator creates a mixed cooperative-competitive environment where heterogeneous agents dynamically adjust ticket prices in response to demand fluctuations. This framework allows for the study of pricing strategies to balance competition and cooperation, capturing the unique dynamics of the high-speed railway domain.\nThrough extensive experiments, the proposed framework is evaluated by testing advanced MARL algorithms, such as Multi-Actor Attention Critic (MAAC) [9] and Multi-Agent Deep Deterministic Policy Gradient (MAD-DPG) [10], in the context of dynamic pricing for high-speed rail networks. These experiments explore how agents adapt to mixed cooperative-competitive dynamics and study the effects of user preferences on agent performance, equity, and system-wide outcomes under varying demand scenarios. The results highlight the algorithms' ability to balance trade-offs between cooperation and competition, optimise pricing strategies, and align individual profitability with broader system efficiency. Furthermore, the findings of the study reveal the challenges posed by heterogeneous agent interactions and the significant role of user preferences in shaping overall system behaviour, offering key insights for the design of robust MARL-based solutions for real-world applications.\nThe remainder of this paper is organised as follows: Section 2 reviews related work, including dynamic pricing, Deep Reinforcement Learning (DRL) applications in railways, social dilemmas in MARL, and related RL environments. Section 3 formalises the mixed multi-agent task as a stochastic game. Section 4 details the proposed methodology, and Section 5 presents experimental results. Finally, Section 6 concludes the paper and discusses future lines of research."}, {"title": "2. Related work", "content": "To contextualise the contributions, this section reviews prior research in four key areas. First, dynamic pricing strategies (Section 2.1) are explored, as they provide the foundation for optimising prices in high-speed railway systems. Second, the use of DRL in railway systems (Section 2.2) is examined, highlighting its role in addressing operational challenges. Third, social dilemmas in multi-agent systems (Section 2.3) are discussed, as they capture the interplay between cooperative and competitive behaviours, central to the proposed framework. Finally, a review of related RL environments (Section 2.4) positions the contributions within the broader context of MARL research and simulation platforms."}, {"title": "2.1. Dynamic pricing", "content": "Dynamic pricing involves adjusting prices in real-time based on supply and demand to maximise profits and align market conditions. This concept, also known as pricing intelligence, has been widely explored in various fields through DRL methods.\nIn electricity markets, DRL has been used to optimise prices in energy consumption and transportation. For example, [1] developed algorithms enabling suppliers and customers to learn dynamic pricing and consumption scheduling without prior knowledge, reducing system costs. Similarly, for Electric Vehicle (EV) charging, [2] combined the Deep Deterministic Policy Gradient (DDPG) [11] algorithm with Prioritised Experience Replay (PER) [12] to enhance pricing strategies in deregulated systems. For Hydrogen Fuel Cell Vehicle (HFCEV) refuelling, [3] used DRL to coordinate refuelling schedules and determine prices, improving demand satisfaction and traffic flow in microgrids.\nDynamic pricing has also been applied in telecommunications. [5] presented a model for interactions between Mobile Virtual Network Operators (MVNOs) and users, framing the problem as a Stackelberg game and solving it using a Multi-Agent Deep Q-Network (MADQN). In smart grids, [13] proposed a distributed multi-agent optimisation approach for resolving supply-demand imbalances, strengthening privacy, autonomy, and efficiency in dynamic pricing. In air transport, dynamic pricing shares similarities with the rail sector. [4] used DRL to optimise pricing for patient customers, demonstrating that alternating between high and low prices during a sales period can improve revenues. These findings are particularly relevant for industries such as railways, where customer preferences and temporal dynamics significantly influence pricing strategies.\nIn contrast to these domains, there are further special challenges in dynamic pricing in high-speed railways. Operators must account for network interdependencies, requiring the synchronisation of multi-operator journeys,"}, {"title": "2.2. DRL in railway systems", "content": "The use of DRL in railway systems has shown significant potential for optimising operations and improving efficiency. Key applications include rail traffic optimisation [14], where DRL generates recommended trajectories for trains in real time, ensuring punctuality and energy efficiency. Train Timetable Rescheduling (TTR) [15] and the Vehicle Rescheduling Problem (VRSP) [16] use DRL to restore operations quickly after disruptions by adjusting train schedules or reassigning vehicles to routes.\nOther applications include predictive maintenance scheduling [17, 18], energy management, autonomous train control [19, 20], and supply chain optimisation for goods transportation. Simulation environments such as Flatland-RL [16] play a crucial role in enabling the safe, efficient development and testing of DRL algorithms. For example, Flatland-RL simplifies complex railway tasks like VRSP, while reducing computational costs, as demonstrated by the NeurIPS 2020 Benchmark [21]. For train interval control, [22] applied secure DRL to balance safety and traffic density by using vehicle-to-vehicle communication and Constrained Markov Decision Processes (CMDPs). Their approach improved safety by 30% while increasing system efficiency.\nHowever, most existing applications of DRL in railways focus on operational challenges, such as scheduling and maintenance, rather than market-driven strategies like dynamic pricing. Addressing dynamic pricing requires additional capabilities, such as modelling passenger behaviour, capturing interdependencies in multi-operator journeys, and balancing competition and cooperation between operators."}, {"title": "2.3. Social dilemmas in MARL", "content": "Social dilemmas in MARL arise when agents' decisions influence not only their own rewards but also those of others. These dilemmas are critical in scenarios where cooperation and competition coexist, requiring strategies that balance individual incentives with system-wide outcomes. In the context of MARL, addressing social dilemmas involves designing mechanisms and environments that encourage cooperation when beneficial while allowing agents to compete effectively.\n[23] introduces sequential social dilemmas, extending the concept of cooperation and competition from isolated actions to temporally extended policies rather than isolated actions, as seen in traditional matrix games like the prisoner's dilemma. The study demonstrates how environmental factors, such as resource abundance, shape agent behaviour and lead to conflicts over shared resources. Building on this, [24] explores inequity aversion as a mechanism for promoting fairness and cooperation, while [25] introduces causal influence rewards to enhance coordination in MARL settings. These studies highlight how carefully designed reward structures can incentivise cooperative behaviour even in competitive environments.\nMore recent research has focused on decentralised decision making and localised cooperation in MARL. For instance, [26] proposes a hierarchical framework where agents dynamically share rewards with neighbouring agents, fostering cooperation at a local level while pursuing broader objectives. Similarly, [27] considers mechanisms to sustain stable cooperation by adjusting agent strategies based on their experiences. These approaches underscore the importance of dynamic policies that adapt to both cooperative and competitive contexts.\nThe proposed MARL framework reflects many of these principles in its design. Social dilemmas emerge naturally in the environment, particularly in the trade-offs between cooperative behaviours, such as synchronising connecting services, and competitive pricing strategies in overlapping markets. Unlike traditional team-based MARL settings, the agents in this framework operate without fixed teams, and cooperation provides localised benefits specific to the participating agents, rather than system-wide rewards. This dynamic aligns with real-world scenarios, where agents optimise individual profits while navigating cooperative opportunities and competitive constraints.\nThe framework builds on the idea of mixed cooperation and competition as explored in [23]. It extends it, however, to the domain-specific context of high-speed railways. By enabling agents to dynamically interact based on localised incentives, the framework offers a unique perspective on how social dilemmas manifest in real-world multi-agent environments. Through the integration of random utility models and a flexible simulator, the framework provides a foundation for studying the interplay between individual incentives"}, {"title": "2.4. Related RL environments", "content": "In the literature, existing RL environments often model mixed settings where agents pursue individual goals while cooperating in specific contexts. For example, in the Clean Up task from the Melting Pot [28] evaluation suite, agents must balance harvesting berries for individual rewards with cleaning a river for the common good. Similarly, in the Predator and Prey task from the Multi Particle Environment [29, 10], predators must cooperate to catch faster prey, as rewards depend on capturing the prey but not on the number of participating predators.\nOther environments, such as Google Research Football [30], Melting Pot's Capture the Flag, and variations of Predator and Prey, focus on team-based settings where agents cooperate within fixed teams while competing against others. In contrast, the proposed environment features no fixed teams: agents can simultaneously compete and collaborate with the same opponents. Cooperation occurs in specific contexts, such as multi-operator journeys, and benefits only the participating agents rather than providing a shared system-wide advantage. This design captures the dynamics of real-world railway networks, where agents optimise individual objectives while navigating both cooperative and competitive market interactions."}, {"title": "3. Preliminaries", "content": "This study considers a mixed multi-agent task that can be modelled as a stochastic game G, which satisfies the Markov property. More specifically, G is a Markov Game [31], a multi-agent extension of Markov Decision Processes (MDPs). The game is defined by the tuple G = (S,U, P, r, Z, \u039f, \u03b7, \u03b3). Here, S represents the set of possible states, with $s \\in S$ denoting the current state of the environment. Then agents, denoted by $a \\in A = \\{1,...,n\\}$, select actions $u^a \\in U$ at each time step. These actions collectively form a joint action $u \\in U = U^n$, which induces a transition to a new state according to the state transition function $P(s'|s, u) : S \\times U \\times S \\rightarrow [0, 1]$.\nEach agent has an individual reward function, defined as $r^a(s, u) : S \\times U \\rightarrow R$, which depends on the global state and the joint action. This is in contrast to Decentralised Partially Observable Markov Decision Processes (Dec-POMDPs) [32], typically used in fully cooperative settings where all"}, {"title": "4. Methods", "content": "This section presents the methodology used to simulate and analyse the dynamic pricing problem in high-speed railway networks. The proposed framework incorporates a journey-based simulator to model passenger decision making, operator-defined services, and demand patterns across multiple markets (Section 4.1). Furthermore, Section 4.2 describes the key components of the environment, including the observation space, action space, and reward function used in the MARL framework."}, {"title": "4.1. Journey-based high-speed railway simulation", "content": "The simulation environment is based on the ROBIN simulator, which generates microscopic interactions between supply and demand for passengers in high-speed railway networks. It comprises three main components: a supply module, a demand module, and a simulation kernel integrating these elements (Figure 2).\nThe supply module defines the set of railway services provided by operators, including schedules, routes, and ticket prices. The demand module generates daily passenger demand by simulating user preferences, travel patterns, and market-level variations. The simulation kernel integrates these components, modelling interactions between passenger decisions and operator strategies, such as pricing adjustments and use of the services.\nUnlike the original service-oriented ROBIN simulator, the proposed framework adopts a journey-based model. This enables multi-operator journeys and dynamic interactions to be modelled, better reflecting real-world travel dynamics. By structuring the simulation environment in this way, the framework captures the complexities of interconnected railway networks and provides a flexible platform for evaluating dynamic pricing strategies."}, {"title": "Demand modelling.", "content": "Service demand for a given day t is represented as:\n$D_t = (\\{d_w\\}_{w\\in W}, \\{p_k^w\\}_{k\\in K})_p$,\nwhere W denotes the set of markets (origin-destination pairs), and K represents the set of passenger types. The term $\\{d_w\\}_{w\\in W}$ represents the potential demand for each market w, parameterised as random variables accounting for day-to-day variations. The term $\\{p_k^w\\}_{k\\in K}$ specifies the probability distribution over passenger types k in market w, indicating the likelihood of passengers belonging to each type.\nThe demand module takes daily samples from these distributions to generate randomised demand inputs, ensuring that variations in market conditions and passenger behaviour are realistically modelled."}, {"title": "Journey-based model.", "content": "A journey J represents a passenger's complete travel itinerary across origin-destination pairs. It is defined as $J = (S,\\omega,\\tau,t')$, where $S = \\{s_1, s_2, ..., s_n \\}$ is an ordered sequence of services where each $s_i \\in S$ for $i \\in \\{1,2, ..., n\\}$ is an individual service operated by a railway company,\n$w = (w_o, w_d)$ specifies the market with $w_o$ and $w_d$ denoting the stations of origin and destination, $\\tau = \\{t_{s_1}, t_{s_2}, ..., t_{s_n}\\}$ represents the departure times for the services in S, and t' is the travel date.\nA journey is valid if it satisfies specific conditions. Each consecutive pair of services $(s_i, s_{i+1})$ must have a transfer time $(t_{s_{i+1}} - t_{s_i})$ greater than or equal to the minimum transfer time $(d_{min})$. The journey must also align with service schedules and connect the desired origin and destination.\nMetrics for journey evaluation. The structure and efficiency of a journey are quantified using two key metrics. The first is the number of transfers, denoted by $N_{transfers}$, which measures the total number of service changes within the journey. For a journey comprising n services, this is calculated as:\n$N_{transfers} = n - 1$.\nThis metric provides insight into the complexity of the journey. Fewer transfers indicate a more straightforward and potentially more convenient"}, {"title": "Passenger utility modelling.", "content": "Passenger behaviour is represented using a utility function which captures the factors influencing their journey choices:\n$U_{jcnt'} = \\xi_{TSP,k} + \\delta_{ck} - f_k(\\Delta T_{jk}) - r_k(DT_{jk}) - g_k(P_{cwt'}) -h_k(TT_{jw}) - \\Tau_k(TR_{jw}) - l_k(NT_{jw}) + rand(\\varepsilon_k)$,\nwhere $U_{jcnt'}$ represents the utility of a specific journey j for passenger n of type k, given a seat c on day t'. Each term in the equation reflects a specific aspect of the passenger's decision making. For instance, $\\xi_{TSP,k}$ captures the passenger's perception of the train service provider, while $\\delta_{ck}$ refers to the perceived utility of the seat and $g_k(P_{cwt'})$ accounts for sensitivity to ticket price. Other components, such as $f_k(\\Delta T_{jk})$ and $r_k(DT_{jk})$, model preferences for arrival and departure times, respectively.\nThe utility function also includes factors such as the total travel time ($h_k(TT_{jw})$), transfer time $(\\Tau_k(TR_{jw}))$, and the number of transfers $(l_k(NT_{jw}))$. A random error term, $rand(\\varepsilon_k)$, accounts for unobserved variability in passenger preferences. In addition, for journeys involving multiple seats and railway companies, the average utility of the components $\\xi_{TSP,k} + \\delta_{ck}$ is calculated, and if at least one service within the journey has no available tickets, the utility of a journey is set to -\u221e.\nPassengers select the journey j* that maximises their utility, provided that utility is positive:\n$j^* = \\underset{j \\in J}{argmax} U_{jcnt'} > 0$."}, {"title": "Simulation transition dynamics.", "content": "The simulation models the interactions between passenger decisions and operator strategies, capturing the dynamics of demand generation, journey selection, and ticket purchasing. At the start of each simulated day, the supply and demand modules are initialised to define the available services and generate passenger demand $D_t$, based on market conditions and user patterns. Valid journeys I are then filtered to ensure they meet scheduling constraints and the minimum transfer time $d_{min}$.\nPassenger decision making is modelled using a utility-based approach. For each journey, the simulation evaluates service availability and seat quality, computing utility values at seat level that account for factors such as price, travel time, and passenger preferences. Passengers select the journey that maximises their utility, provided it is positive; otherwise, they opt not to travel. The system iterates this process, dynamically adjusting the state of the environment as passenger choices and operator strategies evolve.\nStrategic interactions. The proposed framework models a non-zero-sum game where passengers can opt out if no journey meets their utility threshold. This feature captures the realistic scenario in which passenger demand is elastic, influenced by operators' pricing strategies and service quality. By allowing for the emergence of new travellers or the loss of potential passengers based on these strategies, the framework adds complexity to operator interactions. This dynamic interplay reflects the real-world trade-offs between competitive pricing and service attractiveness, shaping the overall market dynamics."}, {"title": "4.2. Design of the dynamic pricing environment", "content": "This section outlines the implementation of the Markov game introduced earlier in the context of the MARL framework for dynamic pricing in high-speed railway networks. The environment is designed to simulate interactions between agents (operators) and the system, providing a platform to study their strategies under realistic conditions. The key components of the environment include the observation space (Section 4.2.1), which defines the information available to each agent, the action space (Section 4.2.2), which"}, {"title": "4.2.1. Observation space", "content": "The observation space defines the information accessible to agents in the MARL framework, capturing the state of the railway system in order to support dynamic pricing decisions. Each observation $O_s$ corresponds to a specific service $s \\in S$, where S denotes the set of all services in the supply module. Observations include both static and dynamic attributes.\nStatic attributes represent fixed properties of a service, such as the train service provider, the corridor (a set of stations within a region), the line (a sequence of stations in the corridor), the time slot, and the rolling stock. These are encoded as integer indices for computational efficiency. Dynamic attributes, on the other hand, capture real-time service information, including pricing and ticket sales. Pricing data specifies the departure and arrival stations and the seat prices for each seat type in an origin-destination pair. Ticket sales data reflects the number of seats sold for each seat type, providing a snapshot of service demand.\nIn the MARL context, agents observe only a subset of the full observation space, tailored to their role as operators. Public information, such as train service providers, prices, and time slots, is available to all agents. However, private data, such as ticket sales, is restricted to services operated by the corresponding agent. For services not operated by an agent, the tickets sold attribute is excluded. Formally, the observation space for an agent a is given by:\n$Z_a = \\begin{cases} O_s \\setminus \\{tickets_sold\\}, & \\text{if TSP}(s) \\neq a \\\\ O_s, & \\text{otherwise} \\end{cases}, \\forall s \\in S$\nwhere TSP(s) denotes the train service provider operating service s.\nThis design ensures that agents receive the information necessary for decision making while maintaining privacy for sensitive data. The initial observation space is fixed and configurable based on the demand hyperparameters, allowing flexibility for different simulation scenarios."}, {"title": "4.2.2. Action space", "content": "The action space defines the set of decisions available to agents for modifying ticket prices in their respective services. Each action represents a percentage change in the price of available seats for a specific service in the supply module. Rather than setting prices directly, agents adjust them relative to their current values, which aligns with real-world pricing practices.\nFormally, if the current price for a seat type c in a market w at time step t is $p_{cwt}$, and the action specifies a percentage change \u03b1, the updated price at time step t + 1 is:\n$p_{cwt+1} = p_{cwt} \\cdot (1 + \\frac{\\alpha}{\\beta})$, where $\\alpha \\in [-1,1]$ is a normalised value representing a percentage increase (\u03b1 > 0) or decrease (\u03b1 < 0), and \u03b2 is a scaling factor set to 25 by default. Prices are clipped to ensure they remain within a valid range [0, \u221e), preventing negative pricing.\nThe environment also supports a discrete action space for scenarios where algorithms require fixed action sets. In this case, each action corresponds to one of eleven discrete values, representing five levels of price increase, five levels of price decrease, and an option to leave the price unchanged. The relative modification of prices, rather than absolute adjustments, offers several benefits. It reflects common dynamic pricing practices, where prices are adjusted incrementally based on market conditions [33], and reduces the risk of destabilising the system with abrupt changes, which can negatively impact training convergence and performance."}, {"title": "4.2.3. Reward", "content": "At each time step t, measured in days, an agent a receives a reward $r_t^a$ which quantifies the profit generated from the tickets sold for its services. The reward is calculated as the difference in total profit between the current time step, denoted by $p_t^a$, and the previous time step, $p_{t-1}^a$:\n$r_t^a = \\sum_{s \\in S^a} p_{t}^s - p_{t-1}^s$, where $S^a$ is the set of services operated by agent a. This formulation assumes no associated costs for providing services, simplifying the reward function and focusing solely on revenue changes.\nThe reward structure encourages agents to maximise short-term profits by aligning their strategies to immediate revenue gains. However, this dense"}, {"title": "5. Experimental settings and results", "content": "This section evaluates the proposed MARL framework for dynamic pricing in high-speed railway networks. The scenarios used for the evaluation are described in Section 5.1, the algorithms considered are outlined in Section 5.2, and the details of implementation are given in Section 5.3. The simulation framework provides a flexible platform to explore a range of research questions, such as:\n\u2022 How do different MARL algorithms perform in terms of profitability, equity, and adaptability across varying user demand patterns?\n\u2022 To what extent do agent pricing policies influence passenger behaviour, including travel decisions, utility, and the total number of passengers?\n\u2022 How do cooperative and competitive dynamics shape agent strategies and system-wide outcomes? Can mechanisms such as attention or shared rewards enhance performance in mixed-agent environments?\nThe answers to these research questions are provided in Section 5.4, which presents the performance comparison and analysis of the algorithms in the proposed framework, and in Section 5.5, which explores additional insights from further studies. These experiments highlight the adaptability of the framework to diverse scenarios, uncovering trade-offs between profitability, equity, and cooperation. The insights thus gained contribute to broader MARL challenges, such as learning in non-stationary environments, and managing heterogeneous agent objectives."}, {"title": "5.1. Scenarios", "content": "The experiments use a fixed rail network topology as described in the introduction, where stations are nodes and connections are edges operated by different companies. The design incorporates both competitive and cooperative dynamics, with agents competing in shared markets and collaborating to provide connecting services. Two key markets, A-C and A-D, capture direct and connecting journeys. The agents are heterogeneous, as their action spaces differ and all services use a single seat type to focus the analysis on dynamic pricing strategies. Specifically, the characteristics of the scenarios are described as follows.\nBusiness. This scenario models a single user group with inelastic demand. Passengers are relatively insensitive to price changes, simulating a stable market dominated by business travellers. Each episode spans 5 days, with an average of 110 passengers across all markets.\nBusiness & Student. This scenario introduces two user groups with distinct price sensitivities. Business travellers are less price-sensitive and more likely to book tickets in advance, while students are highly price-sensitive and tend to buy tickets closer to their travel dates. Episodes last 7 days, with an average of 220 passengers across all markets, comprising 60% business and 40% student travellers.\nThese scenarios provide a structured framework for analysing agent behaviour, user preference, and system-wide outcomes. By comparing results across the scenarios, the experiments demonstrate the MARL framework's ability to model dynamic pricing challenges in both simple and complex market conditions."}, {"title": "5.2. Algorithms for evaluation", "content": "To evaluate the performance of the proposed MARL framework and its applicability to dynamic pricing, a diverse set of algorithms from both single-agent and multi-agent paradigms is tested. These algorithms are chosen to reflect a range of strategies and learning dynamics, allowing for a comprehensive analysis of agent behaviour, system outcomes, and pricing strategies in both cooperative and competitive scenarios.\nThe inclusion of single-agent RL serves as a benchmark to understand the performance of algorithms in a simplified monopolistic setting, where a single agent controls all services without interference from competitors. This allows the impact of competition and cooperation introduced in the multi-agent setting to be isolated. The selected single-agent RL algorithms are:\n\u2022 TD3 [35]: A deterministic actor-critic algorithm designed for continuous action spaces. TD3 is included for its robustness in stabilising learning through delayed policy updates and target networks, making it a strong choice for settings with high-dimensional action spaces.\n\u2022 SAC [36]: A stochastic policy gradient algorithm that balances reward maximisation with exploration by optimising an entropy-augmented objective. SAC is included for its ability to handle exploration-exploitation trade-offs effectively, which is critical in dynamic environments.\nIn the MARL setting, agents operate in a shared environment, interacting through both cooperative and competitive dynamics. The following MARL algorithms are included to explore different approaches to learning in mixed-agent environments:\n\u2022 IQL-SAC [37, 36]: A decentralised approach where each agent learns independently, treating others as part of the environment. Despite the inherent non-stationarity caused by changing policies of other agents, IQL-SAC provides a baseline for independent learning in MARL.\n\u2022 VDN-SAC [38, 36]: A cooperative algorithm that combines individual value functions into a shared global value. VDN-SAC is included to evaluate how shared rewards influence agent cooperation, despite their limited applicability in competitive real-world scenarios.\n\u2022 MAAC [9]: A multi-agent extension of SAC incorporating attention mechanisms. MAAC is selected for its ability to focus on relevant agent interactions, which is particularly useful in mixed cooperative-competitive settings.\n\u2022 MADDPG [10]: An extension of DDPG with centralised training and decentralised execution. MADDPG is included to evaluate its performance in environments with explicit cooperation and competition.\nFinally, random policies are included to provide a point of comparison against non-learning, purely stochastic decision-making strategies.\nThese algorithms are tested across the Business and Business & Student scenarios described in Section 5.1, enabling a systematic evaluation of their strengths and limitations in modelling dynamic pricing strategies and agent interactions in high-speed railway networks."}, {"title": "5.3. Implementation details", "content": "For a standardised comparison, the default hyperparameters of each algorithm were used, without additional tuning. All experiments were run on an Intel i7-13700KF CPU and an NVIDIA GeForce RTX 4080 16GB GPU.\nTo encourage initial exploration and diversify the state-action space, each algorithm was first trained with a random policy for 1,000 episodes. A replay buffer of one million steps was used to store experience for all algorithms, and each model was trained for 200,000 episodes. For algorithms such as TD3, SAC, IQL-SAC, and VDN-SAC, delayed policy updates were performed with a frequency of two for every Q-network update.\nTo ensure robust results, experiments were conducted with 16 parallel environments, each initialised with a unique random seed. Additionally, training and testing were conducted with separate sets of random seeds to avoid overfitting and ensure generalisability. Each experiment was repeated with three independent seeds, providing reliable statistical estimates for performance metrics.\nReward normalisation was applied during training to stabilise learning and improve convergence. The normalised reward $r_t$ at time step t was computed as:\n$\\hat{r_t} = \\frac{r_t - \\mu_t}{\\sqrt{\\sigma_t^2 + \\epsilon}}$, where $r_t$ is the raw reward at time t, $\\mu_t$ is the running mean of rewards up to time t, $\\sigma_t^2$ is the running variance of rewards up to time t, and $\\epsilon$ is a small constant for numerical stability."}, {"title": "5.4. Performance comparisons and analysis", "content": "This section analyses the performance of various algorithms under single-agent and multi-agent settings across both scenarios.\nIn the single-agent setting, agents optimise their policies independently, without considering the presence of competitors. Both TD3 and SAC outperform the random policy baseline in the Business and Business & Student scenarios, with TD3 achieving the highest performance in both cases. These results highlight the effectiveness of these algorithms in simpler, monopolistic environments where agents focus solely on maximising their own rewards."}, {"title": "5.5. Further studies", "content": "This section considers various aspects of agent behaviour and its impact on the environment, offering a deeper analysis of key dynamics within the proposed framework. The analysis in Section 5.5.1 assesses the equality of profit distribution between agents, identifying algorithms that achieve more balanced outcomes. This highlights the trade-offs between individual profitability and overall equity in multi-agent systems. Section 5.5.2 examines the relationship between agent pricing policies and passenger behaviour, focusing on their effects on utility and the percentage of passengers who choose to travel. These findings reveal the tension between maximising agent profits and ensuring passenger satisfaction.\nSection 5.5.3 explores the role of the attention mechanism in the MAAC algorithm, particularly in complex scenarios with heterogeneous interactions between agents and passengers. The analysis demonstrates how attention mechanisms can improve agent performance by prioritising relevant interactions. Finally, Section 5.5.4 studies the interplay between cooperation and competition between agents, showing how a strategic balance between these dynamics can improve system-wide outcomes."}, {"title": "5.5.1. How equally distributed are the profits among the agents?", "content": "The equality of profit distribution between agents serves as a key metric for assessing the fairness of policies learned by different algorithms. This is particularly relevant in scenarios where agents share access to a common-pool resource. Equality is quantified using the metric proposed in [39], defined as:\n$E = 1 - \\frac{\\sum_{i=1}^{N} \\sum_{j=1}^{N} |R^i - R^j|}{2N \\sum_{i=1}^{N} R^i}$, where E represents the equality metric, $R^i$ and $R^j$ are the profits of agents i and j at the end of the episode, and N is the total number of agents. Higher values of E indicate a more equitable distribution of profits.\nTable 3 presents the equality metrics for the Business and Business & Student scenarios. Of all the algorithms, VDN-SAC achieves the highest equality in both scenarios, due to its shared reward structure that inherently promotes cooperation between agents. However, this approach is less applicable in real-world settings, where such coordination is typically unfeasible due to legal constraints. In contrast, other algorithms, including MAAC and MADDPG, exhibit lower equality, particularly in the more complex Business & Student scenario, where heterogeneous user patterns make equitable profit distribution more challenging."}, {"title": "5.5.2. How does the policy of the agents affect passenger utility and travel decisions?"}]}