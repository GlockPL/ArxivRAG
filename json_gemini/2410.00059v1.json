{"title": "IDEA: An Inverse Domain Expert Adaptation Based Active DNN IP Protection Method", "authors": ["Chaohui Xu", "Qi Cui", "Jinxin Dong", "Weiyang He", "Chip-Hong Chang"], "abstract": "Illegitimate reproduction, distribution and derivation of Deep Neural Network (DNN) models can inflict economic loss, reputation damage and even privacy infringement. Passive DNN intellectual property (IP) protection methods such as watermarking and fingerprinting attempt to prove the ownership upon IP violation, but they are often too late to stop catastrophic damage of IP abuse and too feeble against strong adversaries. In this paper, we propose IDEA, an Inverse Domain Expert Adaptation based proactive DNN IP protection method featuring active authorization and source traceability. IDEA generalizes active authorization as an inverse problem of domain adaptation. The multi-adaptive optimization is solved by a mixture-of-experts model with one real and two fake experts. The real expert re-optimizes the source model to correctly classify test images with a unique model user key steganographically embedded. The fake experts are trained to output random prediction on test images without or with incorrect user key embedded by minimizing their mutual information (MI) with the real expert. The MoE model is knowledge distilled into a unified protected model to avoid leaking the expert model features by maximizing their MI with additional multi-layer attention and contrastive representation loss optimization. IDEA not only prevents unauthorized users without the valid key to access the functional model, but also enable the model owner to validate the deployed model and trace the source of IP infringement. We extensively evaluate IDEA on five datasets and four DNN models to demonstrate its effectiveness in authorization control, culprit tracing success rate, and robustness against various attacks.", "sections": [{"title": "I. INTRODUCTION", "content": "The past decade has witnessed proliferate applications of DNNs in almost every sector of business. Building a high-performance DNN requires significant data collection and labeling effort, computing resources, and expert knowledge. These painstakingly trained models are alluring targets of piracy and misappropriation. When a trained DNN is directly distributed to the end users, the internal structure and parameters of the deployed model can be easily duplicated by rival adversaries or dishonest consumers with full access to the model. Even if the trained model is deployed on the cloud to remote endpoint users for online inference through application program interface (API), recent studies [1]\u2013[5] demonstrated that a surrogate model can be trained to achieve comparable performance as the source model at a substantially reduced cost than designing the model from scratch.\nIn view of the rampant DNN intellectual property (IP) theft, various protection methods have been proposed in recent years. The approaches can be categorized into passive and active protection methods. Passive protection methods, such as watermarking [6]\u2013[12] and fingerprinting [13]\u2013[16], aim at detecting the ownership of the deployed DNN upon IP infringement, whereas active protection methods, such as [17]\u2013[21], aim at preventing IP violation proactively. The latter limits unauthorized users without a valid key from accessing the full utility of the deployed model by multi-task optimization. Constrained by multiple optimization objectives, these optimization techniques mainly alter the decision boundaries of the last and/or penultimate layers. As the feature distribution in the shallow layers remains almost identical for both authorized and unauthorized samples, they can be easily exploited for successful fine-tuning attacks.\nIn this paper, we propose IDEA an active DNN IP protection method based on Inverse Domain Expert Adaptation. It enables the owner to proactively lock the model's functionality from access by unauthorized users without a valid user key. Even if the model has been misappropriated, the owner can still claim the ownership and trace the culprit responsible for the model leakage. Our novel contributions are as follows.\nUnlike existing methods, IDEA treats the active authorization IP protection as an inverse problem of domain adaptation and solves it by training a mixture of experts (MoE) to minimize the mutual information (MI) between the authorized source and unauthorized target domains. A real expert is fine-tuned from the unprotected model to produce accurate prediction on authorized images. Authorized images can be produced by hiding a user key using a trained invertible Steganographic Generative Adversarial Network (SteganoGAN). The encoder of the SteganoGAN is distributed to legitimate users while the decoder is kept private by the model owner for user tracking. Two fake experts are trained to intentionally degrade the prediction accuracy on unencoded clean images and images encoded with invalid key, respectively. The three independently trained experts are inseparably fused into a coherent protected model by knowledge distillation across several selected hidden layers from shallow to depth. The stealthiness, effectiveness, fidelity and uniqueness of IDEA are evaluated by extensive experiments across five popular datasets and four network architectures. Its superiority over existing active protection methods and robustness against a wide range of attacks have also been validated.\nThe rest of the paper is organized as follows. Related works are reviewed in Section II. Our threat model and problem formulation, background knowledge on SteganoGAN and domain expected risk bound are presented in Section III. The proposed IDEA is elaborated in Section IV. Section V presents the experimental results and comparison, followed by the latent representations visualization and security analysis in Sections VI and VII, respectively. The paper is concluded in"}, {"title": "II. RELATED WORKS", "content": "DNN watermarking is a passive means of IP rights protection by concealing the copyright information into the target model. In the event of IP piracy, the embedded information can be extracted to prove the ownership. The first DNN watermarking method [6] embeds secret information into the model's weights by including an additional regularization loss during training and constraining the biases of the embedded hidden layers to follow a particular distribution. To enhance the watermark robustness against colluded modifications by malicious users, DeepMark [10] assigns each user a unique fingerprint using anti-collusion codebook and retrains the model with a fingerprint-specific regularization loss. To avoid accuracy degradation in black-box watermarking, RIGA [11] uses adversarial training to increase the covertness and robustness against watermark removal but white-box access is needed for watermark verification. To facilitate ownership verification by querying the model remotely via the API, adversarial examples [7] are used to tweak the decision boundaries for watermark embedding. For better transferability across various model architectures, the model is watermarked by backdooring [8]. The watermarking framework is generalized in [22] to support both black-box and white-box accesses by leveraging meaningful content, unrelated content, and meaningless noise for backdoor trigger generation.\nIn contrast, DNN fingerprinting methods extract unique intrinsic characteristics of the model for ownership proof without modifying the pretrained model. PoL [13] creates a unique fingerprint from specific details of the training process. IP-Guard [14] utilizes adversarial examples while ModelDiff [15] exploits the model's decision distance vectors to characterize and identify the decision boundary for unique fingerprint construction. Both methods allow remote black-box ownership verification through the APIs. More recently, a transfer learnt resilient fingerprint [16] is proposed by projecting selected weights from the front layers of the victim model to the random space generated by the owner's notary identity.\nActive protection methods control the model usage by allowing the full functionality of the model to be accessible by only authorized users. Our method falls into this category. HPNN [17] embeds the encoded model into a trustworthy hardware device. In [18], the model is trained on pixel-shuffled samples to attain optimal inference performance exclusively on test samples that have been shuffled with the same secret key. ChaoWs [20] rearranges the neuron positions to encrypt a trained model, which can only be decrypted with the secret key provided by the owner. M-LOCK [23] and SSAT [21] trains the model with correctly labeled trigger-embedded training samples and wrongly labeled clean training samples so that the trained model can only accurately classify images with the pre-defined backdoor trigger. DSN [24] pastes a binary pattern trigger on training images, and utilizes a gradient reversal layer [25] to train the protected model to behave differently on images with and without the trigger. NTL [26] restricts the model's generalization ability to a certain domain to realize active authorization.\nDeepIPR [19] is the first passport-based protection scheme for access control and ownership verification. This method replaces selected normalization layers of the model with passport layers to degrade its performance. Authorized users can restore the model's original performance by submitting a valid passport image along with the test images. PAN [27] jointly trains the target model with a passport-aware branch to avoid model structure modification; however, this method lacks the usage control in the distributed passport-free branch. TdN [28] enables irreversible ownership verification against ambiguity attacks with oracle passports. Recently, Steganographic passport [29] hides the user's identity images into the passport with an invertible stenographic network. This approach offers robust ownership proof without necessitating extra model retraining for each new user.\nDomain adaptation aims to improve the generalizability of a DNN model from the source domain to other related target domains with access to the target training data. Many existing domain adaptation approaches align domain distributions by minimizing the measured distance between domains, such as maximum mean discrepancy (MMD), correlation alignment (CORAL) [30], and contrastive domain discrepancy (CDD) [31]. Some approaches utilize neural networks, like autoencoders or adversarial-based networks [32], to diminish the domain gap by decreasing the discrepancy between feature representations in the hidden layers."}, {"title": "III. PRELIMINARIES", "content": "Under the active protection paradigm, the defender is assumed to have complete knowledge and control over all components and training pipelines of a model before it is distributed and deployed. As the adversary can be a legal buyer who illegally redeploys the purchased model for profit beyond the contracted term, the protected model's parameters and architecture are assumed to be transparent to the adversaries. During the verification stage, the defender can only query the suspected model with a few reserved test samples. The outputs can be analyzed to determine if it is a counterfeit copy, and trace the source of counterfeit if it is.\nWe formulate the active authorization as an inverse problem of domain adaptation. Our goal is to maintain a high inference accuracy of the protected model on the authorized (source) domain, where the input data are embedded with the correct key, while weakening its generalizability on unauthorized"}, {"title": "IV. THE PROPOSED METHOD", "content": "Fig. 1 illustrates the overall pipeline of our proposed method. It consists of three steps: key embedding, experts training, and model obfuscation.\nEach individual user is assigned a unique key kj \u2208 {0,1}c\u00d7r\u00d7r, which is a binary square matrix derived from the identity of a user, where c is the number of channels and r denotes the length of the square matrix. The entire key space K is 2c\u00d7r\u00d7r. Notably, r is much smaller than the height h and width w of the original image x \u2208 [0,1]3\u00d7h\u00d7w. To match the sizes of kj and x, we concatenate multiple non-overlapping blocks of kj to obtain the expanded key Exp(kj) \u2208 {0,1}c\u00d7h\u00d7w. The encoded image xj = E(x, Exp(kj)) is generated by simultaneously feeding xi and Exp(kj) into the encoder. During the\nLet Aj be the source domain, and B and Nj be two different target domains. An inverse problem of domain adaption can be solved by optimizing the domain expected risk bound in Sec. III-D to maintain the good performance of the authorized source domain and reduce the generalization of the unauthorized target domains. To fulfill Definition 1, the training objective of the protected model f\u03b8\u2081 (\u00b7) can be formulated as"}, {"title": "V. EXPERIMENTS", "content": "Table I specifies the five public image classification datasets, CIFAR-10 [42], CIFAR-100 [42], GTSRB [43], Caltech-101 [44], and ImageNet [45], with different resolutions and numbers of classes. To speed up the training without loss of generality, we perform the ImageNet evaluation on its subset of 10 arbitrarily selected classes named ImageNet-10. The real and fake experts share the same architecture across ResNet-18 (Res-18) [46], PreActResNet-18 (PreRes-18) [46], SENet [47] and MobileNet-V2 (Mob-V2) [48].\nWe set r according to the input size of the dataset. For ImageNet-10, r is set to 32, which gives a key space of 232\u00d732. The 32 \u00d7 32 key is replicated 49 times and then squarely concatenated to the same size of 224 \u00d7 224 as the benign images before they are input to the SteganoGAN encoder. For the other four datasets, we set r to 16 with a key\nFor each classification task, a dedicated SteganoGAN model is trained using 2500 samples randomly selected from the benign training dataset. The unprotected model f\u03b8(\u00b7) is trained from scratch for 200 epochs, and from which the real expert f\u03b8r(\u00b7) is fine-tuned for 40 epochs, both using the Stochastic Gradient Descent optimizer. Each fake expert is separately trained for 1000 iterations. During the knowledge distillation process, the student model is optimized from scratch for 50 epochs using the Adam optimizer. Ten protected models are generated from the same source model with different random keys for distribution to ten different users. Several common data augmentations, including random cropping, random horizontal flipping, random rotation and random erasing, are applied during training to alleviate overfitting.\nFig. 4 presents some examples of benign and authorized images for CIFAR-10, Caltech-101, GTSRB, and ImageNet-10 datasets. We also provide the magnified (\u00d710) residual images obtained by the absolute difference in pixel values between the benign and authorized images. The perturbations introduced by SteganoGAN encoding are visually imperceivable. In other words, the attacker can hardly figure out the embedded key by inspecting a few test images encoded with the same key. Furthermore, the perturbations are\nsample-specific. Even if the attacker can compute a residual image from a pair of benign and authorized image of a protected model, it is impossible to generate an authorized image by adding the residual image to a different test image. The means and standard deviations of three objective image quality assessments (IQA), Structural Similarity Index Measure (SSIM) [49], Peak Signal to Noise Ratio (PSNR) and Learned Perceptual Image Patch Similarity (LPIPS) [50] are presented in Table II. The outstanding IQA scores prove the stealthiness of the key-encoded images.\nThe inference performances of both unprotected and protected models on benign, authorized, and noise test images are presented in Table III. The results indicate that unprotected models consistently achieve high prediction accuracy across all three domains. All protected models fulfill the fidelity requirement of Eq. (4), with inference accuracy of authorized images comparable to the benign accuracy of the corresponding unprotected model. The accuracy drop is no more than 1.5%. Additionally, the protected models perform poorly on benign and noise images, with near-random guessing accuracies, making them useless to unauthorized users without the correct key.\nWe trained 10 protected models encoded with different keys and evaluated the prediction performance across all possible combinations of model-key pairs. The confusion matrices are presented in the first row of Fig. 5, where each row denotes a protected model and each column denotes a key. The diagonal and off-diagonal values represent test accuracies for matched and mismatched model-key pairs, respectively. All but the diagonal values of these confusion matrices are very low (<20%), indicating that user keys among protected models are non-exchangeable. Therefore, the protected model of one legitimate user cannot achieve accurate prediction on encoded images generated with another legitimate user's key.\nAn attacker may have stolen a protected model and the\nSteganoGAN encoder from a legitimate user. Without the legitimate user key, the complexity of brute force key guessing is O(2c), provided that the protected model produces random output to any input images generated by the stolen encoder with the wrong guessed key even if the guessed key contains only one error bit. To evaluate the tolerance of the protected model to bit errors in the key used for encoding the test images, we randomly flipped a few bits, ranging from 1 to 24, of the user keys. As shown in the second row of Fig. 5, for the four low-resolution datasets, all protected models suffer from severe performance degradation with just one error bit. Although the accuracies do not drop to the level of random prediction, they are unacceptably low. When the number of flipped bits has increased to 3, the prediction accuracies of all protected models have dropped to no more than 21%. On the other hand, for ImageNet-10, the accuracies of the models degraded slower with the number of flipped bits. The accuracies dropped by only around 4% with just one bit flip in the user key. However, all four models experienced a performance degradation of more than 40% when 13 key bits are flipped. Considering the image resolution (1024 bits), it is still computationally intractable to successfully brute force 13 correct key bits.\nFor each dataset, we select 1,000 benign test images and encode them with 10 different keys, k1,k2,\u2026, k10, to simulate an intercepted dataset Dint that contains 10000 images embedded with unknown keys. These images are fed into the private SteganoGAN decoder to extract their embedded keys. We set the threshold \u03b53 of Eq. (27) to 1, which is the most stringent threshold with at most one bit difference allowed. Table IV demonstrates that an extremely high TSR of over 97% is achievable across all datasets and keys, with an average TSR of above 99% for all five datasets. With such a high TSR, the owner can easily pinpoint the culprit with high confidence,\nTo evaluate the verification and tracing performance of black-box query, we create a query dataset Dq of 100 benign test images and encode them with 10 different keys to produce 10 encoded query datasets, Dk1q, Dk2q,..., Dk10q. We also train 10 benign models from scratch using the benign training dataset and mix them with the 10 protected models to create a suspected model set of 10 innocent models and 10 protected models pirated from different users. It is observed that innocent models consistently achieve high performance on all query datasets, regardless of the embedded keys. In contrast, pirated models perform optimally (with 70% to 90% accuracy depending on the dataset) only on a single query dataset embedded with the correct key, and poorly (with < 20% accuracy) otherwise.\nTable V shows that 100% TA is achieved across all datasets and networks, which confirms the excellent ownership verification and culprit tracing accuracy of IDEA.\nWe re-implemented four recent active protection methods, DeepIPR [19], SSAT [21], DSN [24], and pixel shuffling [18], on ResNet-18 across the five datasets for comparison. We also directly distilled a student model from the MoE model with only KL-divergence and without attention and contrastive representation loss terms. We call this model the \"simple-distilled model\u201d. Table VI presents the accuracies of the protected models on authorized and benign test images. The accuracy change for each protected model and dataset is also provided in bracket.\nThe performance of the pixel shuffling [18] method is dependent on the shuffling block size b, which trades model's accuracy on unauthorized images for model's accuracy on authorized images. For low-resolution datasets, b = 2 provides a good trade-off between effectiveness and fidelity. For ImageNet-10, it can only keep the prediction accuracy of the unauthorized images to below 50% by degrading the accuracy of authorized images by more than 5% with optimal b = 8.\nDeepIPR [19], SSAT [21], and DSN [24] can keep the accuracy drop to lower than 1.5% on authorized images and sufficiently degrade the performance on unauthorized images (<40%). However, these three methods have their own security vulnerability. The passport layers of DeepIPR can be easily localized since they require additional inputs. The attacker can replace these passport layers with custom normalization layers and re-train them to achieve high accuracy on benign images without passport inputs, as demonstrated in [51]. SSAT assigns ground truth labels to poisoned training samples and wrong labels to clean training samples. The ground-truth label y of clean input image is mapped to the wrong label y' = y + 1. This fixed bijective mapping can be easily reverse-engineered by observing the model's outputs to a small set of labeled test images. Any other deterministic bijective mapping is as vulnerable. The DSN method directly pastes a fixed binary pattern on the test images as secret key, which can be easily spotted by human eyes.\nIDEA achieves good effectiveness and fidelity on all datasets. Layer substitution attack is not able to break IDEA as we make no modification to the model architecture. Query-based reverse-engineering is also infeasible as the outputs of unauthorized images are randomized. Since the perturbations introduced by SteganoGAN encoder are imperceivable and sample-specific, it is impossible for the attacker to recognize and copy the secret key.\nAlthough the simple-distilled model can achieve similar classification accuracy as the protected model, their differences will be more closely scrutinized in Sec. VI."}, {"title": "VI. A CLOSER LOOK AT LATENT REPRESENTATIONS", "content": "In this section, the hidden layer outputs of the unprotected, MoE, simple-distilled and protected models are examined from three perspectives: MI estimation, attention map visualization, and representation visualization.\nFig. 6 visualizes the estimated MI between the latent representations collected from paired source (authorized) and target (unauthorized) test samples, with the first row showing the MI between authorized and benign images, and the second row showing the MI between authorized and noise images. Layers 1 to 4 are the selected layers for MI minimization and multi-layer attention and contrastive representation knowledge distillation, progressing from the shallow to deep layer.\nFor all selected layers, we observe that the estimated MI on the unprotected model is consistently high. This is expected, as this model has not been trained to recognize steganographic"}, {"title": "VII. SECURITY ANALYSIS", "content": "We further investigate the robustness of IDEA against both model transformation and reverse-engineering attacks. Model transformation attacks aim to remove the active authorization protection by directly modifying the pirated model through fine-tuning, model-pruning, and transfer-learning. Reverse-engineering attacks aim to reverse engineer the encoding process without modifying the pirated model.\nFollowing the configurations in [52], we apply the following four fine-tuning strategies, fine-tune all layers (FTAL), fine-tune last layer (FTLL), re-train all layers (RTAL), and retrain last layer (RALL) on the protected model.\nTable VII shows the accuracy of the fine-tuned models on the benign test dataset. The baseline performance of the unprotected models on the benign test dataset is shown in the last column. For fine-tuned protected models, the performance improves with increasing ratio of accessible samples. However, even with 30% training samples, the inference accuracy on benign data is still significantly lower than the baseline performance. In most cases, the model fine-tuned from the simple-distilled model has higher accuracy than from the protected model, especially for FTAL. This result agrees with our observation in Sec. VI that the shallow layers of the simple-distilled model can be exploited for fine-tuning attack. Therefore, the multi-layer attention and contrastive representation losses are required to prevent the distilled shallow layers from leaking the real expert functionality of the teacher model.\nWeight pruning (WP) and filter pruning (FP), are commonly used to reduce model size by remov-\nUnder Assumption 1, most reverse-engineering attacks result in low prediction accuracy close to random guessing. The best attack performance achieved on Caltech-101 with 5000 available benign images is merely 22.13%. As expected, the"}, {"title": "VIII. CONCLUSION", "content": "We have presented a novel active DNN IP protection method called IDEA. It steganographically encodes distributed model instances with user-specific keys. The authorized user of a protected model instance can unlock its inference performance by submitting test images embedded with a valid key assigned by the model owner. In the event of IP infringement, the owner can verify the authenticity of deployed models and trace the culprit by querying the suspected model with a small set of test samples or decoding the user submitted queries. Extensive experiments conducted across five image classification datasets and four DNN models validated the excellent inference performance on authorized inputs, and corroborated that IDEA-protected models cannot be unlocked by images encoded with incorrect keys even with only a few flipped bits or keys stolen from other legitimate users. This strong uniqueness property is important to assure that the tracking of any redistributed purchased models to its culprit is non-repudiable. The culprit can be traced by decoding the intercepted test images with almost 100% success rate. IDEA is also robust against model transformation and reverse-engineering attacks, and more stealthy, effective, and scalable with networks and dataset sizes compared with existing active protection methods."}]}