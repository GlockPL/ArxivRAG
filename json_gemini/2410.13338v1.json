{"title": "DIFFIMP: EFFICIENT DIFFUSION MODEL FOR PROB- ABILISTIC TIME SERIES IMPUTATION WITH BIDIREC- TIONAL MAMBA BACKBONE", "authors": ["Hongfan Gao", "Wangmeng Shen", "Xiangfei Qiu", "Ronghui Xu", "Jilin Hu", "Bin Yang"], "abstract": "Probabilistic time series imputation has been widely applied in real-world scenar- ios due to its ability to estimate uncertainty of imputation results. Meanwhile, denoising diffusion probabilistic models (DDPMs) have achieved great success in probabilistic time series imputation tasks with its power to model complex dis- tributions. However, current DDPM-based probabilistic time series imputation methodologies are confronted with two types of challenges: 1) The backbone modules of the denoising parts are not capable of achieving sequence modeling with low time complexity. 2) The architecture of denoising modules can not han- dle the inter-variable and bidirectional dependencies in the time series imputation problem effectively. To address the first challenge, we integrate the computational efficient state space model, namely Mamba, as the backbone denosing module for DDPMs. To tackle the second challenge, we carefully devise several SSM- based blocks for bidirectional modeling and inter-variable relation understanding. Experimental results demonstrate that our approach can achieve state-of-the-art time series imputation results on multiple datasets, different missing scenarios and missing ratios.", "sections": [{"title": "1 INTRODUCTION", "content": "The analysis of time series can model the intrinsic patterns within time-series data, thus providing robust support for decision-making in various fields, such as meteorology McGovern et al. (2011); Karevan & Suykens (2020), financial analysis Xiang et al. (2022); Owusu et al. (2023); Bai et al. (2020), healthcare Morid et al. (2023); Poyraz & Marttinen (2023) and power systems Tzelepi et al. (2023); Zhou et al. (2021). To enhance the reliability of analytical outcomes, it is critical to en- sure the integrity of time series. However, due to various reasons such as device failures, human errors, and privacy protection, time series data can easily be incomplete with missing observations at different timestamps.\nTime series imputation methods aim to estimate the values of missing points based on the observed points in incomplete time series, thereby restoring the integrity of the time series while preserv- ing its original statistical properties. According to the ability to provide uncertainty of estimations, time series imputation methods can be categorized into the following two perspectives: 1) Deter- ministic Cao et al. (2018); Cini et al. (2022); Du et al. (2023), and 2) Probabilistic Chen et al. (2023); Kim et al. (2023); Luo et al. (2018) imputation methods. Probabilistic time series im- putation is particularly important in dealing with complex and uncertain data environments, as it provides a quantification of uncertainty for the imputations. The key to probabilistic imputation lies in modeling the posterior distribution. Existing probabilistic time series imputation methods include Gaussian Process and Variational Autoencoder-based methods Fortuin et al. (2020), Normalization Flow-based methods Rasul et al. (2021), and Diffusion-based methods Tashiro et al. (2021). Among these, the Diffusion-based method has emerged as the optimal choice for probabilistic time series due to their accuracy in posterior modeling and adaptability to different scenarios and various types of time series data."}, {"title": "2 PRELIMINARIES", "content": null}, {"title": "2.1 STATE SPACE MODELS", "content": "State Space Models (SSMs) are an emerging approach to model sequential data, which is imple- mented by finding out state representations to model the relationship between input and output sequences. A SSM receives a one-dimensional sequence $X \\in R^L$ as the input and outputs a corre- sponding sequence $Y \\in R^M$. Under continuous settings, the SSMs are defined according to Eq.1:\n$\\begin{cases}\n\\dot{h}(t) = Ah(t) + Bx(t) \\\\\ny(t) = Ch(t) + Dx(t),\n\\end{cases}$ (1)\nwhere $x(t) \\in R^L$, $y(t) \\in R^M$, $h(t)$, and $\\dot{h}(t) \\in R^N$ stands for the input, output, hidden state, and derivative of hidden state at timestamp t, respectively; $A \\in R^{N \\times N}$, $B\\in R^{N \\times L}$, $C\\in R^{M \\times N}$ and $D \\in R^{M \\times L}$ are learnable model parameters.\nIn real-world applications, the input sequences are discrete samplings of continuous sequences. According to Gu et al. (2022), under discrete settings, by applying the zero-order hold technique to Eq.1, it can be reformulated as follows.\n$\\begin{cases}\nh_k = \\overline{A}h_{k-1} + \\overline{B}x_k \\\\\ny_k = Ch_k\n,\\end{cases}$ (2)\nwhere $\\overline{A} = exp(\\Delta A)$, $\\overline{B} = (\\Delta A)^{-1}(exp(\\Delta A) - I) \\cdot (\\Delta B)$ and $\\Delta$ is the learnable step size in discrete sampling. We can see from Eq.2 that the hidden state is updated according to the input x(t) and last hidden state h(t \u2212 1) while the output is generated by the hidden state h(t) and the input x(t) and in Gu et al. (2020), where it introduces High-order Polynomial Projection Operator (Hippo) to achieve longer sequence modeling.\nHowever, it is worth noticing that A, B, C, D in Eq.1 and Eq.2 are time-invariant parameters, i.e., they are data-independent parameters and do not change over time. Therefore the model is not ca- pable of assigning different weights at different positions in the input sequence while receiving new inputs. To address this issue, Gu & Dao (2023) proposed Mamba, in which the parameter matrices A, B, C, D are input-dependent, thus enhancing the performance of sequence modeling. To tackle the problem of non-parallelization, Gu & Dao (2023) also introduced selective scan mechanism for effective computing. For further performance and efficiency improvements, Dao & Gu (2024) point out that SSMs can be categorized as a variant of linear attention model. In this work, we follow the same architecture of parallel Mamba Blocks as Dao & Gu (2024) and a RMS-norm Zhang & Sen- nrich (2019) module is added after the parallel Mamba block. The details of the post-normalization Mamba Block (PNM Block) are illustrated in Fig.3a."}, {"title": "2.2 DIFFUSION MODELS", "content": "Let $x_t$ be a sequence of variables for $t = 1,2,\\dots,T$. The diffusion process consists of two pro- cesses: 1) The forward process without learnable parameters, which transforms the data distribu- tion into a standard Gaussian distribution by gradually adding noise to the data. 2) The reverse process with learnable parameters, which first samples from the standard Gaussian distribution and then progressively denoises the data to approximate the data distribution. The reverse process of diffusion models a parameterized distribution $p_\\theta$ defined with the following Markov chain to ap- proximate the real data distribution:\n$p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1}|x_t),$ (3)\nwhere $x_T \\sim \\mathcal{N}(0, I)$ denotes the latent variable sampled from standard Gaussian distribution and\n$p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_\\theta(x_t,t)I),$ (4)\nThe loss function of DDPM aims at minimizing the difference between the noise in the forward process $\\epsilon$ and the parameterized noise $\\epsilon_\\theta$ in the reverse process:\n$\\mathcal{L}_d = E_{x_0,\\epsilon}[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|],$ (5)\nwhere t stands for the diffusion time embedding and $x_t$ is calculated in the forward process. Please refer to Appendix 7.1 for more details about the diffusion models."}, {"title": "2.3 PROBLEM FORMULATION", "content": "Definition 1 (Time Series). A time series can be defined as a tuple, denoted as $X = (\\mathcal{X}, M,T)$, where $\\mathcal{X} \\in R^{K \\times L}$ is the observation matrix with K observations at a time, which are ordered along L time intervals chronologically; $M \\in R^{K \\times L}$ is an indicator matrix that indicates whether the observation at (i, j) in $\\mathcal{X}$ is missing or not: if the observation at position (i, j) is missing, i.e., $\\mathcal{X}_{i,j}$ = NA, then $M_{i,j}$ = 1, otherwise, $M_{i,j}$ = 0; $T \\in R^L$ is the time stamps of the time series.\nDefinition 2 (Probabilistic Time Series Imputation). Given an incomplete time series $X = (\\mathcal{X}, M,T)$, where $\\sum M < K \\cdot L$, the problem of probabilistic time series imputation is to learn an imputation function $\\mathcal{M}_\\theta$, such that\n$\\hat{\\mathcal{X}} = \\mathcal{M}_\\theta(\\mathcal{X}),$ (6)\nwhere $\\hat{\\mathcal{X}} \\in R^{K \\times L}$ is the imputed time series, where $\\hat{\\mathcal{X}}_{i,j} = \\hat{\\mu}_{i,j} \\pm \\hat{\\sigma}_{i,j}$ denotes the probabilistic output if $M_{i,j}$ = 1, otherwise $\\hat{\\mathcal{X}}_{i,j} = \\mathcal{X}_{i,j}$."}, {"title": "3 METHODOLOGY", "content": null}, {"title": "3.1 DIFFUSION MODELS FOR TIME SERIES IMPUTATION", "content": "When dealing with time series imputation using diffusion models, consider a time series $\\mathcal{X}$, our goal is to model the posterior $P(\\mathcal{X} | \\mathcal{X}^o, M,T)$. To make the modeled posterior more precisely, it is natural to introduce conditions to introduce the diffusion process. Considering the short range and long range inter-dependencies within time series, maximizing the observed values utilized in the diffusion process can effectively improve the performance of the imputation results. On the other hand, due to the fact that all the observed values are utilized as condition inputs in the diffusion process, we do not apply any extra process to the observed values to avoid the error accumulation caused by information propagation, the observed values $\\mathcal{X}^o$ are condition inputs for the diffusion process. Thus, the reverse process in Eq.3 is modified to a conditional form with time-series inputs:\n$p_\\theta(x_{0:T} | \\mathcal{X}^o, \\mathcal{X}^m) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1}|x_t, \\mathcal{X}^o),$ (7)\nwhere $x_T \\sim \\mathcal{N}(0, I)$, $\\mathcal{X}^m$ denotes the sequence of latent variables in the diffusion process and $t \\in \\{1,2,\\dots,T\\}$ is the diffusion time steps. Eq.4 is reformulated as:\n$p_\\theta(x_{t-1}|x_t, \\mathcal{X}^o) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t|\\mathcal{X}^o), \\sigma_\\theta(x_t,t|\\mathcal{X}^o)I),$ (8)\nthe parameterized mean turns to:\n$\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t) \\Big),$ (9)\nwhere\n$x_t = \\sqrt{\\alpha_t}x_0 + (1 - \\alpha_t)\\epsilon,$ (10)\nand $\\{\\beta_t \\in (0,1)\\}_{t=1}^T$ is a predefined variance scheduler and $\\alpha_t = \\prod_{i=1}^{t-1}(1 - \\beta_i)$, hence we get the conditional diffusion loss for time series imputation task:\n$L = E_{x_0,\\epsilon}[\\|\\epsilon - \\epsilon_\\theta(x_t, t|\\mathcal{X}^o)\\|^2] = E_{x_0,\\epsilon}[\\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\alpha_t}x_0 + (1 - \\alpha_t)\\epsilon, t|x^o)\\|^2],$ (11)\nwhere $\\epsilon \\sim \\mathcal{N}(0, I)$.\nIn the real world, the imputation problem encounters various complexities, such as different ra- tios of missing data, the positions of missing values within the sequence and the distribution of missing data. To simulate various complex missing situations in real-world scenarios, we adopt a self-supervised approach for training, i.e., applying a predefined mask to the complete dataset to construct corresponding dataset with missing data. We follow the same mask strategies in Alcaraz & Strodthoff (2023), including Random Missing (RM) which corresponds to the situation of uni- formly random missing values, Random Block Missing (RBM) which corresponds to the situation of continuous missing values (missing intervals) in different channels and Blackout Missing (BM) which contains missing intervals at the same timestamps among different channels."}, {"title": "3.2 MODEL ARCHITECTURE", "content": "The Overall Module Architecture Fig.1 illustrates the overall self-supervised framework and train- ing process of our model. We first mask part of the observed values according to the pattern of missing values, where the masked values serve as the imputation target $X_o$ during training. The remaining observed values form the conditional input $\\mathcal{X}^o$ for the noise prediction network $\\epsilon_\\theta$. We then combine $X_o$ with noise $\\epsilon$ sampled from a standard normal distribution to obtain the noisy input $X_t$. Both $\\mathcal{X}^o$, $X_t$, and the diffusion step t are fed into the noise prediction network $\\epsilon_\\theta$ to get the parameterized noise $\\epsilon$. The network minimizes the difference between $\\epsilon_\\theta$ and $\\epsilon$ according to Eq.11.\nAs shown in Fig.2, the forward process of $\\epsilon_\\theta$ are as follows: For each diffusion step, the input con- sists of the following parts: noisy input $X_t$, the condition input $\\mathcal{X}^o$ and the diffusion step $t$. To begin with, the inputs are embedded to the latent diffusion space. The embedding module of noisy inputs and condition inputs share a similar model structure, which consists of a linear projection module followed by an SMM block in Fig.3b. The SMM block is composed of stacks of Bidirectional At- tention Mamba (BAM) blocks and Channel Mamba Blocks (CMB), which is introduced in the next part. Due to the relatively limited information from t, the embedding module of t only consists of linear projection modules. After the embedding step, the embedded diffusion step is concatenated with the input embeddings. The concatenated embeddings are fed in to a SMM module. Then the output of the SMM module is concatenated with the condition embeddings. After feeding the final embeddings to another SMM module and final projection module, we can get the noise predictions $\\epsilon_\\theta(X_t, t)$. The training and sampling algorithm is detailed in Alg.1 and Alg.2.\nMamba Encoders for Bidirectional Modeling For probabilistic time series imputation tasks, the objective is to attain a more precise posterior estimation for the missing points contingent upon the observed points. Therefore, our proposed module should achieve two key objectives: Firstly, it should possess bidirectional analysis capability, which means that the model should be able to capture dependencies in both the forward and reverse temporal directions. Secondly, considering that the known points at different positions relative to the missing point have varying distances, the model should assign different weights to difference timestamps. To address these issues, we de- vise a bidirectional attention Mamba module (BAM). BAM takes the representations from previous layers as input, which are then fed into two distinct PNM modules (Fig.3a), enabling the model to capture bidirectional dependencies. More specifically, temporal attention is implemented by assign-"}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 EXPERIMENT SETTINGS", "content": "Datasets and Experimental Settings We conduct experiments on three real-world datasets to val- idate the effectiveness of our approach. These datasets span multiple domains, namely Electric- ity dataset Asuncion & Newman (2007), MuJoCo dataset Rubanova et al. (2019b) and ETTm1 dataset Zhou et al. (2021).\nAll experiments are conducted using PyTorch Paszke et al. (2019) in Python 3.9 and execute on an NVIDIA RTX3090 GPU. The training process is guided by Eq.11, employing the ADAM opti- mizer Kingma & Ba (2015) with a learning rate of 2 \u00d7 10\u22124. More details about the datasets and experimental settings can be found in the Appendix.\nEvaluation Metrics and Baselines To achieve an extensive evaluation of imputation performance, diverse metrics are utilized for evaluating deterministic imputation results, namely Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Square Error (RMSE). Due to reproducibility reasons of baselines, we compare with different baselines and report different metrics for different datasets. The datasets and corresponding baseline and metrics are listed in Table.2. We follow the same settings and dataset preprocessings as Alcaraz & Strodthoff (2023) and collect all the baseline results from the same paper."}, {"title": "4.2 TIME SERIES IMPUTATION", "content": "Deterministic Imputation Results Table.3 presents the experimental results on the MuJoCo dataset under RM missing scenario with high missing ratios of 70%, 80%, and 90%, respectively. On the MuJoCo dataset, DiffImp achieves SOTA performance under 80% and 90% missing ratio, delivering at least 50% performance improvement over previous SOTA methods. In the experiment of 70% missing ratio, our method achieves results very close to SOTA. The results on MuJoCo dataset indicate that our proposed DiffImp is the optimal method for high missing ratio imputation under the RM missing pattern."}, {"title": "4.3 TIME SERIES FORECASTING", "content": "As mentioned in 3.1, the probabilistic time series forecasting problem can be treated as a variant of the probabilistic time series imputation problem (as a special case of the missing manner BM). Therefore, we also conduct experiments to validate the effectiveness of our experiments on prob- abilistic time series forecasting task. Following the setup in previous works, we test five different forecasting horizons: 24, 48, 96, 288, and 672 time steps, with corresponding conditional lengths (i.e., the length of observed sequence) of 96, 48, 284, 288, and 384 time steps.\nTable.6 presents the experimental results on the ETTm1 dataset. Our method achieves state-of-the- art performance on prediction length of 24 and 96, outperforms other imputation-based algorithms at the prediction length of 672, and shows only a slight gap compared to the best imputation-based algorithms at the prediction length of 48 and 288."}, {"title": "4.4 VISUALIZATION RESULTS", "content": "Fig.4 shows the visualization results for channel 5 and channel 7 on the MuJoCo dataset with a 90% missing ratio. From the figure, we can see that almost all ground truth values for the points to be imputed fall within the 95% confidence interval, and most of the ground truth values are within the 50% confidence interval, which demonstrates the effectiveness of our method."}, {"title": "4.5 SAMPLING TIME ANALYSIS", "content": "Table presents a comparison of sampling time between our method and other backbone-based meth- ods across different datasets. We find that, in terms of sampling time, our method performs better"}, {"title": "4.6 ABLATION STUDIES", "content": "To validate the effectiveness of the proposed module, we conduct ablation experiments on the fol- lowing aspects: 1) the bidirectional modeling 2) the temporal attention mechanism 3) the inter- channel multivariate dependencies. All experiments are conducted on the MuJoCo dataset with the missing ratio 90%. During ablation experiments, we find out that our model converges much slower than other models in the ablation experiment, so we train till all models are converged (for same number of iterations, even if it has already been converged). The hyperparameters in the ablation studies are presented in the appendix.\nThe results are shown in Table.8. It can be observed that the module equipped with BAM and CMB block performs the best, significantly outperforming the results of removing any one of these components across all four metrics. The temporal attention module has the largest impact on the model, and its removal leads to a significant performance drop. Similarly, removing the CMB module also results in a notable degradation in performance. On the other hand, adjusting the BAM module to its unidirectional form also causes some degree of performance decrease. This fully demonstrates the effectiveness of our proposed blocks."}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In this paper, we propose DiffImp, a time series imputation model based on DDPM and Mamba backbone, which incorporates bidirectional information flow, temporal attention and inter-variable dependencies. DiffImp enables efficient time series modeling with linear complexity. Experimental results demonstrate that DiffImp achieves superior performance across multiple datasets, various missing patterns, and different missing ratios.\nFor future work, one possible direction is to further reduce the time complexity of the sampling process while already lowering the complexity of time series modeling, in order to enhance the model's inference efficiency. Another possible direction is to extend the application of diffusion models by applying DiffImp to other time series downstream tasks and time series representation learning tasks."}, {"title": "6 REPRODUCIBILITY", "content": "To ensure reproducibility and facilitate experimentation, datasets and code are available at:\nhttps://anonymous.4open.science/r/DiffImp-843F."}, {"title": "7 APPENDIX", "content": null}, {"title": "7.1 DETAILS OF DDPM", "content": "The denoising diffusion probabilistic model (DDPM) generates unknown data by modeling the dis- tribution of known training data with a parameterized distribution and sampling from the modeled distribution. Concretely, a typical DDPM model consists of two processes, namely the forward pro- cess and the reverse process. The forward process of the DDPM model is defined by a Markov chain, which adds noise sampled from standard gaussian noise to initial data distribution $q_0$ step by step until $q_0$ is transformed to standard gaussian distribution $q_T = \\mathcal{N}(0, I)$. In every single step, the amount of noise injected to the data distribution at current step is controlled by predefined varaince scheduler $\\{\\beta_T \\in (0,1)\\}_{t=1}^T$, which means the injected noise is not learnable. The forward process is defined as follows:\n$q(x_{1:T}|x_0) = \\prod_{t=1}^T q(x_t|x_{t-1}),$ (12)\nwhere $x_0, x_1, x_t$ stands for the latent variables in the Markov chain and\n$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{(1 - \\beta_t)}x_{t-1}, \\beta_tI),$ (13)\nBased on Eq.12 and Eq.13, $x_t$ can be represented with a closed form of:\n$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + (1 - \\bar{\\alpha}_t)\\epsilon,$ (14)\nwhere $\\bar{\\alpha}_t = \\prod_{i=1}^t(1 - \\beta_i)$ and $\\epsilon \\sim \\mathcal{N}(0, I)$.\nCorrespondingly, the reverse process simulates the denoising of a standard Gaussian distribution $p_T = \\mathcal{N}(0, I)$ to the target distribution $p_0$, the entire reverse process is formulated as the following Markov chain:\n$p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1}|x_t),$ (15)"}, {"title": "7.2 EXPERIMENT DETAILS", "content": null}, {"title": "7.2.1 DATASET DESCRIPTIONS", "content": "In this part, we give a brief introduction about the datasets in our experiments and the details of the datasets are presented in Table.9.\nMuJoCo: The MuJoCo dataset Rubanova et al. (2019b) collects a total of 10,000 simulations of the \"Hopper\" model from the DeepMind Control Suite and MuJoCo simulator. The position of the body in 2D space is uniformly sampled from the interval [0, 0.5]. The relative position of the limbs is sampled from the range [-2, 2], and initial velocities are sampled from the interval [-5, 5]. In all, there are 10000 sequences of 100 regularly sampled time points with a feature dimension of 14 and a random split of 80/20 is done for training and testing. We follow the same preprocessing as in Shan et al. (2023b) for fair comparison.\nElectricity: The Electricity dataset from the UCI repository Asuncion & Newman (2007) contains electricity usage data (in kWh) collected from 370 clients every 15 minutes. The dataset is collected and preprocessed as described in Du et al. (2023). Since the dataset does not contain missing values, values of the complete dataset are randomly dropped for the computation of targets according to the RM scenario and the data is already normalized. The first 10 months of data (2011/01 - 2011/10) are designated as the test set, the following 10 months of data (2011/11 - 2012/08) as the validation set, and the remaining data (2012/09 - 2014/12) as the training set. The training and test sets are directly utilized, while the validation set is excluded. The dataset comprises 817 samples, each with a length of 100 time steps and the aforementioned 370 features. Specifically, the 370 channels are split into 10 batches of 37 features each. Mini-batches of 43 samples, each containing 37 features and a respective length of 100, are then passed to the network to ensure that no data is dropped during training.\nETTm1: This dataset contains the amount of detail required for long-time series forecasting based on the Electricity Transformer Temperature (ETT). The data set contains information from a compi- lation of 2-year data from two distinct Chinese counties. In our experimeny, we work with ETTm1"}, {"title": "7.2.2 HYPERPARAMETERS", "content": null}, {"title": "7.3 EVALUATION METRIC DETAILS", "content": "In this part, we give details about the evaluation metrics in our experiments. As defined in Defi- nition.1, the original time series is denoted as $y \\in R^{K \\times L}$, the imputed time series is denoted as $\\hat{y} \\in R^{K \\times L}$, M is the indicator matrix.\nMean Absolute Error (MAE): MAE calculates the average $L_1$ distance between ground truth and the imputed values alongside the channel dimension, which is formulated as:\n$MAE(y, \\hat{y}) = \\frac{1}{KL} \\sum_{i=1}^K \\sum_{j=1}^L |(y - \\hat{y}) \\odot (1 - M)|_{i,j}$ (21)\nMean Square Error (MSE): MSE calculates the average $L_2$ between ground truth and the imputed values alongside the channel dimension, which is formulated as:\n$MSE(y, \\hat{y}) = \\frac{1}{k} \\sum_{i=1}^K \\sum_{j=1}^L((y - \\hat{y}) \\odot (1 - M))_{i,j}^2$ (22)\nRoot Mean Square Error (RMSE): RMSE is the square root of RMSE:\n$RMSE(y, \\hat{y}) = \\sqrt{MSE(y, \\hat{y})}$\n$= \\sqrt{\\frac{1}{k} \\sum_{i=1}^K \\sum_{j=1}^L((y - \\hat{y}) \\odot (1 - M))_{i,j}^2}$ (23)\nMean Relative Error (MRE): MRE estimates the relative difference between y and $\\hat{y}$:\n$MRE(y, \\hat{y}) = \\frac{1}{k} \\sum_{i=1}^K \\sum_{j=1}^L(1 - M)_{i,j} \\odot \\frac{|(y - \\hat{y})|_{i,j}}{y_{i,j}}$ (24)\nContinuous Ranked Probabilistic Score (CRPS): Given an estimated probability distribution function F modeled with an observation x, CRPS evaluates the compatibility and is defined as the integral of the quantile loss for all quantile levels:\n$CRPS(F^{-1}, z) = \\int_0^1 A_\\alpha(F^{-1}(a, z) da,$ (25)"}, {"title": "7.4 ALGORITHM DETAILS OF BAM BLOCK AND CMB BLOCK", "content": "Alg.3 and Alg.4 describes the details of forward process in BAM and CMB block."}, {"title": "7.5 VISUALIZATION RESULTS", "content": null}]}