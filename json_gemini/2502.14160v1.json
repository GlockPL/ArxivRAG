{"title": "EFFICIENT INVERSE MULTIAGENT LEARNING", "authors": ["Denizalp Goktas", "Amy Greenwald", "Sadie Zhao", "Alex Koppel", "Sumitra Ganesh"], "abstract": "In this paper, we study inverse game theory (resp. inverse multiagent learning) in which the goal is to find parameters of a game's payoff functions for which the expected (resp. sampled) behavior is an equilibrium. We formulate these problems as generative-adversarial (i.e., min-max) optimization problems, which we develop polynomial-time algorithms to solve, the former of which relies on an exact first-order oracle, and the latter, a stochastic one. We extend our approach to solve inverse multiagent simulacral learning in polynomial time and number of samples. In these problems, we seek a simulacrum, meaning parameters and an associated equilibrium that replicate the given observations in expectation. We find that our approach outperforms the widely-used ARIMA method in predicting prices in Spanish electricity markets based on time-series data.", "sections": [{"title": "1 INTRODUCTION", "content": "Game theory provides a mathematical framework, called games, which is used to predict the outcome of the interaction of preference-maximizing agents called players. Each player in a game chooses a strategy from its strategy space according to its preference relation, often represented by a payoff function over possible outcomes, implied by a strategy profile (i.e., a collection of strategies, one-per-player). The canonical outcome, or solution concept, prescribed by game theory is the Nash equilibrium (NE) (Nash, 1950): a strategy profile such that each player's strategy, fixing the equilibrium strategies of its opponents, is payoff-maximizing (or more generally, preference-maximizing).\nIn many applications of interest, such as contract design (Holmstr\u00f6m, 1979; Grossman & Hart, 1992) and counterfactual prediction (Peysakhovich et al., 2019), the payoff functions (or more generally, preference relations) of the players are not available, but the players' strategies are. In such cases, we are concerned with estimating payoff functions for which these observed strategies are an equilibrium. This estimation task serves to rationalize the players' strategies (i.e., we can interpret the observed strategies as solutions to preference-maximization problems). Estimation problems of this nature characterize inverse game theory (Waugh et al., 2013; Bestick et al., 2013).\nThe primary object of study of inverse game theory is the inverse game, which comprises a game with the payoff functions omitted, and an observed strategy profile. The canonical solution concept prescribed for an inverse game is the inverse Nash equilibrium, i.e., payoff functions for which the observed strategy profile corresponds to a Nash equilibrium. If the set of payoff functions in an inverse game is unrestricted, the set of inverse Nash equilibria can contain a wide variety of spurious solutions, e.g., in all inverse games, the payoff function that assigns zero payoffs to all outcomes is an inverse Nash equilibrium, because any strategy profile is a Nash equilibrium of a constant game: i.e., one whose payoffs are constant across strategies. To meaningfully restrict the class of payoff functions over which to search for an inverse Nash equilibrium, one common approach (Kuleshov &\nSchrijvers, 2015; Syrgkanis et al., 2017) is to assume that the inverse game includes in addition to all the aforementioned objects, a parameter-dependent payoff function for each player, in which case an inverse Nash equilibrium is simply defined as parameter values such that the observed strategy profile is a Nash equilibrium of the parameter-dependent payoff functions evaluated at those values."}, {"title": "2 PRELIMINARIES", "content": "Notation. All notation for variable types, e.g., vectors, should be clear from context; if any confusion arises, see Section 7.1. We denote by [n] the set of integers {1, ..., n}. Let X be any set and (X, F) any associated measurable space, where the o-algebra F unless otherwise noted is assumed to be the o-algebra of countable sets, i.e., F = {E \u2264 X | E is countable }. We write \u0394(X) = {\u03bc : (X, F) \u2192 [0, 1]} to denote the set of probability measures on (X, F). Additionally, we denote the orthogonal projection operator onto a set X by Ix(x) = arg miny\u2208x ||x - Y||2.\nMathematical Concepts. Consider any normed space (X, ||\u00b7||) where X C Rm and any function f: X \u2192 R. f is lf-Lipschitz-continuous w.r.t. norm (typically, Euclidean) ||\u00b7|| iff x1, x2 \u20ac A, || f(x1) - f(x2)|| \u2264 lf ||X1 - x2||. If the gradient of f is l\u2207 f-Lipschitz-continuous, we refer to f as l\u2207 f-Lipschitz-smooth. Furthermore, given \u03bc > 0, f is said to be \u00b5-gradient-dominated if\nminx'ex f(x') \u2265 f(x) + \u03bc\u00b7 min\u00e6'ex (x' x, \u2207 f(x)) (Bhandari & Russo, 2019).\nNormal-form Games. A (parametric) game G\u00ba = (n,m, d, X, \u0398, \u0398, u) comprises n \u2208 N+ players, each i \u2208 [n] of whom chooses a strategy x \u2208 X\u2081 from an strategy space X\u2081 \u2286 Rm simultaneously. We refer to any vector of per-player strategies x = (x1,...,xn) \u2208 X as a strategy profile, where X = Xi\u2208[n] Xi \u2286 Rnm denotes the space of all strategy profiles. After the players choose their strategies x \u2208 X, each receives a payoff u; (x;0) given by payoff function \u0e19\u2081 : X \u00d7 \u0472 \u2192 R parameterized by a vector 0 in a parameter space \u04e8 \u2286 Rd. We define the payoff profile function u(x;0) = (uz(x; 0))i\u2208[n]; the cumulative regret \u03c8 : X\u00d7X\u00d7\u04e8 \u2192 R across all players, between two strategy profiles x, y \u2208 X, given \u03b8 \u2208 \u0398, as \u03c8(x, y; 0) = \u2211i\u2208 [n] U\u017c (Yi, X\u2212\u00bf\u00a1 \u04e8) \u2013 u(x; 0); and the exploitability (or Nikaido-Isoda potential (Nikaido & Isoda, 1955)) (x; 0) = maxyex\u03c8(x, y; 0)."}, {"title": "3 INVERSE MULTIAGENT PLANNING", "content": "The goal of inverse multiagent planning is to invert an equilibrium: i.e., estimate a game's parameters, given observed behavior. In this section, we present our main idea, namely a zero-sum game (i.e., min-max optimization) characterization of inverse multiagent planning, where one player called\nTheorem 3.1. The set of inverse NE of G\u2212\u00b9 is the set of parameter profiles \u03b8 \u2208 \u04e8 that solve the optimization problem min\u0259\u2208\u04e9 6(x\u2020; 0), or equivalently, this min-max optimization problem:\nmin max f(0, y) = (x, y; 0) = \u2211 [Uz (Yi, x_z; 0) \u2013 uz(x+;0)]\n\u03b8\u0395\u0398\u03a5\u0395\u03a7\ni\u2208[n]\n(1)\nThis min-max optimization problem can be seen as a generalization of the dual of Waugh et al.'s (2013) maximum entropy likelihood maximization method for games with possibly continuous strategy spaces, taking Nash equilibrium rather than maximum entropy correlated equilibrium as the inverse equilibrium. In contrast to Waugh et al.'s dual, our min-max optimization problem characterizes the set of all inverse NE, and not only a subset of the inverse correlated equilibria, in particular those that maximize entropy. This formulation also generalizes Swamy et al.'s (2021) moment matching game from a single-agent to a multiagent setting."}, {"title": "4 INVERSE MULTIAGENT REINFORCEMENT LEARNING", "content": "In this section, we build on our zero-sum game (i.e., min-max optimization) characterization of inverse game theory to tackle inverse MARL in an analogous fashion. As it is unreasonable to assume exact oracle access to the players' (cumulative) payoffs in inverse MARL, we relax this assumption in favor of a stochastic oracle model. More specifically, we assume access to a differentiable game simulator (Suh et al., 2022), which simulates histories of play h ~ v\u2122 according to v\u2122, given any policy profile \u3160, and returns the rewards r and transition probabilities p,7 encountered along the way, together with their gradients.\nFormally, an inverse Markov game M\u22121 = (M\u00ba* \\ \u03b8\u2020, \u03c0\u2020) is an inverse game that comprises a Markov game form (i.e., a parametric Markov game sans its parameter) M\u00ba\u2020 \\ 0\u2020 together with an observed policy profile \u03c0\u2020, which we assume is a Nash equilibrium. Crucially, we do not observe the parameters of the payoff functions. Since a Markov game is a normal-form game with payoffs given by u(\u03c0;0) = EH\u223cv* [\u2211tor (S(t), A(t); 0)], the usual definitions of inverse NE and cumulative regret apply, and the following result, which characterizes the set of inverse NE as the minimizers of a stochastic min-max optimization problem, is a corollary of Theorem 3.1.\nCorollary 1. The set of inverse NE of M\u00af\u00b9 is characterized by solutions to the following problem:\nmin max f(0, \u03c0) = \u03a3  E  \u03a3r(St(t), At(t); 0) - \u03a3ra(St(t), At(t); 0) \n\u03b8\u0395\u0398 \u03a0\u0395\u03a1\ni\u2208[n] H~(\u3160\u3160) t=0\nH~\u3160\nt=0\n[(2)]\nAs is usual in reinforcement learning, we use policy gradient to solve the destabilizer's problem in Equation (2). To do so, we restrict the destabilizer's action space to a policy class P\u0142 parameterized by X CR\u00b9. Redefining f(0, x) = f(\u03b8, \u03c0\u2122), for \u03c0\u03b1 \u2208 PX, we aim to solve the stochastic min-max optimization problem min\u0259\u2208\u0259 max\u00e6\u2208x f(0,x). Solutions to this problem are a superset of the solutions to Equation (2), unless it so happens that all best responses can be represented by policies in\nPx, because restricting the expressivity of the policy class decreases the power of the destabilizer. As in Section 3, without any additional assumptions, f is in general non-convex, non-concave, and non- smooth. While we can ensure convexity and smoothness of 0 \u2192 f(0,x) under suitable assumptions on the game parameterization, namely by assuming the regret at each state is convex in 8, concavity in \u00e6 is not satisfied even by finite state and action Markov games. Under the following conditions, however, we can guarantee that f is Lipschitz-smooth, convex in 0, and gradient dominated in x."}, {"title": "5 SIMULACRAL LEARNING", "content": "In this section, we consider the more realistic setting in which we do not observe an equilibrium, but observe only sample histories {h(k)} = {(s(t,k), a(t,k))t}k ~ v\u03c0 associated with an unobserved equilibrium \u03c0\u00b9. The problem of interest then becomes one of not only inferring parameter values from observed behavior, but of additionally finding equilibrium policies that generate the observed behavior, a solution which we refer to as a first-order simulacrum. A first-order simulacrum can be seen as a generalization of an inverse equilibrium, as it not only comprises parameters that rationalize"}, {"title": "7 APPENDIX", "content": "DISCLAIMER\nThis paper was prepared for informational purposes in part by the Artificial Intelligence Research group of JP Morgan Chase & Co and its affiliates (\u201cJP Morgan\u201d), and is not a product of the Research Department of JP Morgan. JP Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful.\n7.1 ADDITIONAL PRELIMINARY DEFINITIONS\nNotation We use caligraphic uppercase letters to denote sets (e.g., X), bold uppercase letters to denote matrices (e.g., X), bold lowercase letters to denote vectors (e.g., p), lowercase letters to denote scalar quantities (e.g., x), and uppercase letters to denote random variables (e.g., X). We denote the ith row vector of a matrix (e.g., X) by the corresponding bold lowercase letter with subscript i (e.g., xi) and the jth entry of a vector (e.g., p or xi) by the corresponding Roman lowercase letter with subscript j (e.g., pj or xij). We denote functions by a letter determined by the value of the function: e.g., f if the mapping is scalar-valued, f if the mapping is vector-valued, and F if the mapping is set-valued. We denote the set of natural numbers by N and the set of real numbers by R. We denote the positive and strictly positive elements of a set by a + and ++ subscript, respectively, e.g., R+ and R++. For any set C, we denote its diameter maxc,c'\u2208c||c \u2013 c'|| by diam(C).\n7.2 OMMITED PROOFS\nTheorem 3.1. The set of inverse NE of G\u2212\u00b9 is the set of parameter profiles \u03b8 \u2208 \u0398 that solve the optimization problem mino\u2208\u04e9 6(x\u2020; 0), or equivalently, this min-max optimization problem:\nmin max f(0, y) = (x, y; 0) = \u2211 [ui (Yi, x_i; 0) \u2013 uz(x+;0)]\ni\u2208[n]\n(1)\nProof of Theorem 3.1. By the definition of f, for all parameter profiles \u03b8\u2208 \u0398,\nmax f(0, y) = max \u2211 [Uz (Yi, x\u2020 ; 0) \u2013 uz(x+;0)]\nYEX YEX\ni\u2208[n]\n= \u2211 max u (Yi, x_z; 0) \u2013 u\u017e (x\u2020 ; 0)\nY EXi\ni\u2208[n]\n\u2265 0\n\u22650"}, {"title": "7.4 MARKETS EXPERIMENTS", "content": "7.4.1 STATIC FISHER MARKETS\nA (one-shot) Fisher market consists of n buyers and m divisible goods with unit supply(Brainard et al., 2000). Each buyer i \u2208 [n] is endowed with a budget bi \u2208 B\u2081 \u2286 R+ and a utility function u\u2081 : R \u00d7 Ti \u2192 R, which is parameterized by a type ti \u2208 Ti that defines a preference relation over the consumption space Rm. An instance of a Fisher market is then a tuple M = (n, m, u, t, b), where u = (u\u2081, . . ., Un) is a vector-valued function of all utility functions and b = (b\u2081, ..., bn) \u2208 R is the vector of buyer budgets. When clear from context, we simply denote M by (t, b).\nGiven a Fisher market (t, b), an allocation X = (x1,...,xn) \u2208 Rxm is a map from goods to buyers, represented as a matrix, s.t. xij \u2265 0 denotes the amount of good j \u2208 [m] allocated to buyer i \u2208 [n]. Goods are assigned prices p = (p1,...,pm) \u2208 R. A tuple (X*, p*) is said to be a competitive equilibrium (CE) (Arrow & Debreu, 1954; Walras, 1896) if 1. buyers are utility maximizing, constrained by their budget, i.e., Vi \u2208 [n], x \u2208 arg maxx:x.p* <b\u2081 u\u2081(x, ti); and 2. the market clears, i.e., \u2200j \u2208 [m], p > 0 \u21d2 \u2211ie[n] xj = ej and p = 0 \u21d2 i\u2208 [n] xj \u2264 ej.\nThe set of CE of any Fisher market (t, b) with continuous, concave, and homogeneous\u00b9\u00b9 utility functions is equal to the set of Nash equilibria of the Eisenberg-Gale min-max game, a convex- concave min-max game between a seller who chooses prices p \u2208 R and buyers who collectively choose allocations X \u2208 Rnxm: the objective function of this game comprises two sums: the first is the logarithmic Nash social welfare of the buyers' utility, while the second is the profit of a fictional auctioneer who sells the goods in the market:\nmin max f(p, X; t, b) = \u2211 bi log (uz (x, t)) + \u2211 pj e  \u03a3 xij \nPER XERnXm i\u2208[n] j\u2208[m]\ni\u2208[n]\n(33)\nTherefore, for any Fisher market M = (n,m, u,t\u2020,b\u2020), we can construct a inverse game G\u22121 = (G0/0\u2020, x\u2020) where G\u00ba\u00b9 is the corresponding Eisenberg-Gale min-max game (Equation (33)) parameterized by the true types and budgets 0\u2020 = (t\u2020, b\u2020), and x\u2020 = (X*, p*) is not only a NE of the game Got but also a CE of market M. Our goal is to recover the true market parameters 0\u2020 = (t\u2020, b\u2020) given the observed \u00e6\u2020 = (X*, p*), by solving this inverse game problem using Theorem 3.1 and Algorithm 1."}, {"title": "7.4.2 COURNOT COMPETITION AND BERTRAND COMPETITION", "content": "A Cournot competition model C = (n, c, P) consists of n firms that produce a homogeneous product, and each firm i chooses a quantity level of production q\u2081 that maximizes its profits. All firms face a marginal cost c. That is, for a given firm i, the cost of producing qi unit of good is cqi. The price function P takes the total production of all firms Qtotal = \u2211i\u2208[n] qi as input and outputs the unit prices for the good. Thus, the profit function for firm i is fi (qi, q-i; c) = qi (P(\u2211i\u2208[n] qi) - c). Q* is a Nash equilibria of the Cournot game if and only if q \u2208 arg maxq;inR+ fi(qi, q*i; c) for all i \u2208 [n].\nA Bertrand competition model B = (n, c, D) is also a competition model that consists of n firms producing a homogeneous product, but this time, each firm i set prices p\u2081 to maximize its profits. All firms face a marginal cost c. That is, for a given firm i, the cost of producing qi unit of good is cqi. The demand function D takes the minimum price proposed by the firms pmin = mini\u2208 [n] Pi as input and outputs the demand for that good in the whole market. Firm i's individual demand function is a function of the price set by each firm:\nDi (Pi, P-i) =\n{\nD(Pmin) Pi = Pmin, Pj \u2265 Pmin\u2200j \u2260 i\nD(Pmin)\n0\nPi = Pmin, n =# of j \u2208 [n] with pj = Pmin\nPi \u2260 Pmin\n(34)\nThus, the profit function for the firm i is fi(pi, p\u2212i; c) = Di(Pi, P\u2212i)(Pi \u2013 c). p* is a Nash equilibria of the Bertrand game if and only if p\u2208 arg maxprin\u00ae fi(pi, p*i; c) for all i \u2208 [n]."}, {"title": "7.4.3 STOCHASTIC FISHER MARKETS", "content": "A (static) Fisher market (n,m,C, u, e, t, b), (e, t, b) when clear from context, consists of n \u2208 N++ buyers and m \u2208 N++ divisible goods Brainard et al. (2000). Each buyer i \u2208 [n] is represented by a tuple (Ci, Ui, ti, bi), (ti, bi) when clear from context, which consists of a budget bi \u2208 Bi \u2286 R+ of some num\u00e9raire good it is endowed with, a utility function u\u2081 : C\u2081 \u00d7 T\u00bf \u2192 R+, which is parameterized by a type ti \u2208 Ti s.t. uz( \u00b7 ; ti) defines a preference relation over the consumption space Ci \u2286 R.\nEach good is characterized by a supply ej \u2208 Ej \u2286 R+. We denote the collection of all utility functions u = (u\u2081,..., un), the collection of buyer types t = (t1, . . ., tn), the collection of buyer budgets b = (b1,..., bn) \u2208 R, the collection of all good supplies e = (e1,...,em) \u2208 R, the joint space of consumptions C = Xi\u2208[n] Ci, the joint space of types T = Xi\u2208[n] Ti, and the joint space of budgets B = Xi\u2208[n] Bi\nDefinition 1 (Stochastic Fisher Market Game). Given a stochastic Fisher market F = (n,m,l, S, u, p, \u03b3, \u03bc), we define the Stochastic Fisher Market Game MF = (n, S, A, B,r,p, \u03b3, \u03bc) where:\nS=OXE\u00d7B\u00d7T A=P B(s) = C \u00d7 A(s)\nr(w, e,r,t,b)i, P(w, e,r,b);X, \u03b2 =\n\uf8eb n \u2212 \u2211 xj \uf8f6\n\u03a3 (xi + \u03b2\u2081) log u\u017c(t\u017c, xi)\n( + \u03a3 \uf8ec+ + B =\u2211  ij \uf8f7 \uf8f8\u03b2 ( ,  \u22c5xi)i = (\u2211( ) i\ni\u2208[n]\u2208A\u2208 n + B\uf8ed \ni n bi bii n \uf8f8i \u2208\n(35)\n(36)\nExperimental setup We use Jax, and Haiku to traint he simulacrum policies and use a feedforward neural network with 4 layers with 200 nodes. We run our experiments with 5 random seed and report the best results."}, {"title": "7.5 GRADIENT ESTIMATORS", "content": "Notice that the deterministic policy gradient theorem tells us that to compute a the policy gradient we need to compute the gradient of state-action value function with respect to the actions and then multiply it by the gradient of the policy w.r.t. the policies. Since we have access to a first order oracle of the reward and transition we can then compute gradient of the cumulative regret with the following quantities:\nfx (0,y; H, h')\n\u03b1\u03a3\u2207r i(si,(0), ai,(0); \u03b8)\ni\u2208[n]\n+\u03a3\u2207p(si, (1) | si(0), a\u00b2,(0); \u03b8)\n00\n((1),(1);0) +\nt=2\n\u03a3 \u2207Ti(si,(t), ai,(t); \u03b8) + \u03a3 \u2207Pi(si, (t) | si(t\u22121), a\u00b2,(t\u22121) ); \u03b8\n\u03b1 \u03b1\nk=2\nsi,(t) ai, (t)ti\n\u03a0p(si, (k) | si, (k-1), a(k-1)\nk=2\n((1),(1);0) +\nt=2\nTi\n(37)\nfo(0,y;h, h') = \u2211 Vori(si,(t), ai,(t); 0)\ni\u2208[n] Lt\u03b8 t\u03b8\n(38)\nUnder Assumption 3, these estimators are unbiased estimates of the gradients of and \u2207 f, respectively. Assuming these estimators have bounded variance, we can now solve the min-max optimization problem min\u0259\u2208\u04e9 maxx\u2208x f(0, x) for an \u025b-inverse NE in O(1/\u025b) iterations via stochastic gradient descent (Algorithm 2).14 It thus remains to show that the variance of the gradient estimators are bounded: i.e., there exists \u03c3\u2208 [0,\u221e) s.t. for all (0,x) \u2208 \u0398\u00d7 \u03a7, ||En,n' [(fo, f\u00e6)(0,x;h,h')] \u2013 \u2207 f(0,x)|| \u2264 \u03c3. Since rewards, transitions, and policies, are twice continuously-differentiable, both the gradient estimates (fo, fx) and Vf also are, and\nwe have: ||Eh,n' [(fo, fx)(0,x;h, h')] \u2013 \u2207 f(0,x)|| \u2264 max0,x,h,n' || (fo, fx)(0,x;h,h')|| = || (fo, f\u00e6)||, where the max is well defined, since the objective is continuous and the maximization domains S, A, X, O are non-empty and compact. This means that, under Assumption 3, the variance of gradient estimator (fe, f\u00e6) is bounded by o\u00b2 = || (fo, f\u00e6) ||"}]}