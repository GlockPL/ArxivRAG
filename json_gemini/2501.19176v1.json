{"title": "Augmented Intelligence for Multimodal Virtual Biopsy\nin Breast Cancer Using Generative Artificial Intelligence", "authors": ["Aurora Rofena", "Claudia Lucia Piccolo", "Bruno Beomonte Zobel", "Paolo Soda", "Valerio Guarrasi"], "abstract": "Full-Field Digital Mammography (FFDM) is the primary imaging modality\nfor routine breast cancer screening; however, its effectiveness is limited in pa-\ntients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced\nSpectral Mammography (CESM), a second-level imaging technique, offers\nenhanced accuracy in tumor detection. Nonetheless, its application is re-\nstricted due to higher radiation exposure, the use of contrast agents, and\nlimited accessibility. As a result, CESM is typically reserved for select cases,\nleaving many patients to rely solely on FFDM despite the superior diagnostic\nperformance of CESM. While biopsy remains the gold standard for definitive\ndiagnosis, it is an invasive procedure that can cause discomfort for patients.\nWe introduce a multimodal, multi-view deep learning approach for virtual\nbiopsy, integrating FFDM and CESM modalities in craniocaudal and medi-\nolateral oblique views to classify lesions as malignant or benign. To address\nthe challenge of missing CESM data, we leverage generative artificial intel-\nligence to impute CESM images from FFDM scans. Experimental results\ndemonstrate that incorporating the CESM modality is crucial to enhance\nthe performance of virtual biopsy. When real CESM data is missing, syn-\nthetic CESM images proved effective, outperforming the use of FFDM alone,\nparticularly in multimodal configurations that combine FFDM and CESM\nmodalities. The proposed approach has the potential to improve diagnostic\nworkflows, providing clinicians with augmented intelligence tools to improve\ndiagnostic accuracy and patient care. Additionally, as a contribution to the\nresearch community, we publicly release the dataset used in our experiments,\nfacilitating further advancements in this field.", "sections": [{"title": "1. Introduction", "content": "Breast cancer has become a significant global health challenge, ranking\nas the most commonly diagnosed cancer worldwide by the end of 2020 [3].\nThat year, it affected 2.3 million women and caused nearly 685,000 deaths.\nProjections estimate a more than 40% increase in annual cases by 2040,\nreaching approximately 3 million, while related deaths are expected to rise\nby over 50%, surpassing 1 million per year [3].\nCurrently, Full-Field Digital Mammography (FFDM) is the primary imag-\ning modality used in routine breast cancer screening. FFDM, commonly\nreferred to as standard mammography, uses low-energy X-rays to produce\nbreast images typically from craniocaudal (CC) and mediolateral oblique\n(MLO) projections. Despite FFDM being the standard mammography method,\nit has limitations, particularly in women with dense breast tissue, where\noverlapping structures can obscure lesions and contrast between normal and\nabnormal tissues. Diagnostic accuracy can also be reduced in cases involving\nfibrocystic disease and post-treatment follow-ups after breast-conserving or\nadjuvant therapies [7].\nTo overcome these limitations, Contrast Enhanced Spectral Mammog-\nraphy (CESM) has emerged as a valuable second-level imaging technique.\nCESM uses an iodinated contrast agent to enhance the visualization of vas-\ncular structures associated with tumors, significantly improving the accuracy\nof tumor detection. The technique involves dual-energy imaging during a\nsingle breast compression. After administering the contrast agent, CC and\nMLO projections are captured, acquiring both low-energy (LE) and high-\nenergy (HE) images. The LE image is equivalent to a standard mammo-\ngram produced by FFDM, as validated by several studies [10, 21, 11], while\nthe HE image, taken with a higher radiation dose, is not used directly for\ndiagnostic purposes. These LE and HE images are combined using a dual-\nenergy weighted logarithmic subtraction technique, resulting in a recombined\nor dual-energy subtracted (DES) image that highlights areas of contrast up-\ntake, improving lesion visibility. For diagnostic evaluation, radiologists rely\non both DES and LE images to ensure comprehensive assessment.\nCESM is particularly valuable for patients with dense breast tissue or\nthose at higher risk of cancer when FFDM alone is insufficient. [24]. It offers\ndiagnostic accuracy comparable to contrast-enhanced Magnetic Resonance\nImaging (MRI) but with advantages such as reduced cost, shorter imaging\ntimes, and wider accessibility [18]. Despite its benefits, CESM presents some\ndrawbacks. Patients are exposed to slightly higher radiation doses compared\nto FFDM, and there is a risk of allergic reactions to the contrast agent, which,\nalthough generally mild, can occasionally result in more severe complications\nsuch as contrast-induced nephropathy, shortness of breath, or facial swelling\n[24]. Given these limitations and the role of CESM as a second-line imaging\nmethod, CESM is generally reserved for specific cases, leaving many patients\nreliant solely on FFDM despite the superior diagnostic capabilities of CESM.\nUltimately, confirming whether a lesion is malignant or benign often requires\na biopsy, the gold standard for definitive diagnosis. While highly reliable,\nbiopsy is invasive, potentially uncomfortable, and time-consuming, with in-\nherent risks and delays in treatment initiation.\nIn this context, Artificial Intelligence (AI) offers promising solutions to\novercome existing limitations. Specifically, we propose a multimodal, multi-\nview deep learning approach that integrates FFDM and CESM modalities\nin both CC and MLO views to enable a virtual biopsy for breast cancer,\nclassifying lesions as malignant or benign. Our approach addresses the issue\nof missing CESM data by employing generative AI techniques to synthesize\nCESM images from existing FFDM scans, ensuring continuity in diagnostic\nworkflows. When both FFDM and CESM modalities are available, we pro-\ncess them through state-of-the-art classifiers to perform the virtual biopsy.\nIn cases where the CESM modality is unavailable, the synthesized CESM\ndata enables the classification process to proceed, leveraging a multimodal\nframework enriched with generative insights. Specifically, our contributions\nare:\n\u2022 Introduced a novel multimodal, multi-view deep learning approach for\nvirtual biopsy, integrating FFDM and CESM modalities in CC and\nMLO views, and pioneering the use of generative AI to impute CESM"}, {"title": "2. Related Works", "content": "In recent years, the application of AI, particularly deep learning, has\nrevolutionized medical imaging by significantly improving diagnostic accu-\nracy and enabling the development of automated systems for lesion detec-\ntion and classification across various imaging modalities. Convolutional Neu-\nral Networks (CNNs) are increasingly prominent in medical image analysis\n[34]. CNN-based models, such as ResNet [15] and VGG [29], have been\nwidely adopted for the virtual biopsy classification task in a variety of med-\nical imaging domains, such as Computed Tomography (CT) [36, 4, 35],\nMRI [33, 37, 23], FFDM [6, 22, 28], and CESM [1] due to their ability to\nlearn intricate spatial patterns in image data.\nMultimodal learning, which integrates information from multiple imaging\nmodalities, is gaining widespread adoption in medical imaging due to its\npotential to significantly improve diagnostic accuracy and performance [16].\nFor instance, combining MRI and PET enables the simultaneous analysis\nof both structural and functional information, proving highly effective in\nclassifying Alzheimer's disease (AD) [30]. Similarly, integrating FDG-PET\nwith CT allows for the concurrent evaluation of metabolic and anatomical\ndata, offering significant advantages in lung cancer classification [8]. In breast\ncancer, multimodal learning models integrating mammogram and ultrasound\nimages have shown improved accuracy in the virtual biopsy task compared\nto unimodal approaches [5].\nA significant challenge in multimodal medical imaging is the issue of miss-\ning modalities. In real-world clinical settings, the unavailability of certain\nimaging modalities is common, often due to cost constraints, patient-specific\nfactors, or technical limitations [32]. Despite its widespread occurrence, this\nissue has been examined in relatively few studies. Managing incomplete"}, {"title": "3. Methods", "content": "We propose a multimodal, multi-view deep learning pipeline for perform-\ning breast cancer virtual biopsy, classifying lesions as malignant or benign.\nThis framework integrates FFDM and CESM as input modalities, utilizing\nboth CC and MLO views for each modality. If the CESM modality is un-\navailable, we employ a generative AI model to synthesize CESM images from\nthe corresponding FFDM scans. This discussion focuses on the two key components of our approach:\nthe virtual biopsy task, which involves classifying lesions as malignant or\nbenign, and the generative process for synthesizing CESM images to address\nmissing modalities."}, {"title": "3.1. Virtual Biopsy", "content": "The virtual biopsy task is formulated as a binary classification problem,\nwhere the objective is to determine whether a tumor lesion is malignant or\nbenign. Let $M = \\{F,C\\}$ denote the set of imaging modalities, where $F$\ncorresponds to FFDM and $C$ corresponds to CESM. Let $V = \\{CC, MLO\\}$\ndenote the set of views. For each modality $m\\in M$ and view $v \\in V$, we\ndefine the corresponding image as $X_{m,v}$. Specifically:\n$p_{m,v} = f_{m,v} (X_{m,v}), with 0 \u2264 p_{m,v} \u2264 1, \u2200m \u2208 M, \u2200v \u2208 V. (1)$\nWe aggregate the probabilities from the CC and MLO views within each\nmodality using a late fusion function \u03c6\u03bd, which combines the outputs from"}, {"title": "3.2. Generative AI for Missing CESM Data", "content": "When the CESM modality is unavailable, we employ view-specific gen-\nerative AI models, denoted as $G_v$ for each view $v \u2208 V$, to synthesize CESM\nimages $X_{C,v}$ from FFDM images $X_{F,v}$:\n$G_v (X_{F,v}) = X_{C,v}, \u2200v \u2208 V. (4)$\nSubsequently, for virtual biopsy, the synthetic CESM images $X_{C,v}$ are pro-\ncessed by their corresponding $f_{C,v}$, in the same way as for the real CESM\nimages $X_{C,v}$, ensuring consistency in the analysis pipeline.\nTo summarize, Algorithm 1 presents the pseudocode of the proposed mul-\ntimodal, multi-view deep learning approach for virtual biopsy for a single\npatient."}, {"title": "4. Materials", "content": "In this study, we expanded the publicly available CESM@UCBM dataset\n[27] to create a comprehensive collection of CESM exams, and we make the\nextended version publicly available after anonymizing sensitive data. The\nresulting dataset comprises CESM exams from 204 patients, ranging in age\nfrom 31 to 90 years, with an average age of 56.7 years and a standard de-\nviation of 11.2 years. All exams were conducted at the Fondazione Poli-\nclinico Universitario Campus Bio-Medico in Rome using the GE Healthcare\nSenographe Pristina system, with a total of 2278 images. Among these, 1998\nand DES images belonging to the dataset, which differ in the ACR category.\nBreasts of ACR categories a and c are shown in the CC view, while breasts\nof ACR categories b and d are shown in the MLO view.\n presents the distribution of images utilized in our experimental\nstudy. For the implementation of the virtual biopsy process outlined in sub-\nsection 3.1, we excluded all late acquisitions from the dataset to maintain\nconsistency in imaging conditions. This reduced the number of FFDM and\nCESM images from 1139 each to 799.\nFurthermore, the dataset was refined by focusing solely on images con-\ntaining either malignant or benign lesions, thereby reducing the number to\n230 FFDM and 230 CESM images. This subset comprised 83 malignant and"}, {"title": "4.1. Image Pre-processing", "content": "We pre-processed all images to ensure data consistency and uniformity.\nSpecifically, we squared the images by padding them with the average back-\nground value, followed by contrast stretching to enhance brightness and im-\nprove contrast. Next, we normalized the pixel values to the range [0, 1] and\nresized the images to 256 \u00d7 256, balancing computational efficiency with"}, {"title": "5. Experimental Setup", "content": "In this section, we outline the experimental setup of our study, detail-\ning the training and evaluation processes for the classifiers and generative\nmodel used, as well as describing the experiments conducted to assess their\nrobustness to missing modalities."}, {"title": "5.1. Classifiers", "content": "In this work, to perform the virtual biopsy task, we evaluate three well-\nestablished CNN architectures: ResNet18 [15], ResNet50 [15], and VGG16 [29].\nThese networks have demonstrated strong performance in classification tasks\nacross various domains, including breast cancer virtual biopsy [17], making\nthem suitable candidates for our multimodal, multi-view approach.\nAll experiments were conducted using a stratified 5-fold cross-validation\nto ensure that each fold contained a representative distribution of the tar-\nget labels. We split the dataset into training, validation, and test sets in\nproportions of 80%, 10%, and 10%, respectively. To avoid data leakage, we"}, {"title": "5.1.1. Training", "content": "We trained each CNN for up to 300 epochs, employing an early stopping\ncriterion based on validation loss, with a patience of 50 epochs, following an\ninitial 50-epoch warm-up phase. We used the Cross-Entropy loss function\nand the Adam with an initial learning rate of $10^{-3}$, $\u03b2 = 0.9$, and momentum\nof 1. If the validation loss did not improve for 10 consecutive epochs, we\nreduced the learning rate by a factor of 0.1. A weight decay of $10^{-5}$ was\napplied to enhance generalization and prevent overfitting. We did not further\ninvestigate any other hyperparameter configuration since their tuning is out\nof the scope of this manuscript. Nevertheless, the \u201cNo Free Lunch\" Theorem\nfor optimization asserts that no single set of hyperparameters can universally\noptimize model performance across all datasets [2]."}, {"title": "5.1.2. Evaluation", "content": "In the inference phase, we leverage the trained classifiers $f_{m,v}$ to conduct\nthe virtual biopsy on the corresponding test set images $X_{m,v}$, classifying le-\nsions as either malignant or benign. Each classifier $f_{m,v}$ outputs a malignancy\nprobability $p_{m,v}$, specific for the modality m and view v. Then, based on a\nmax membership rule, it predicts the lesion class $C_{m,v}$ as malignant or be-\nnign. To evaluate the performance of the classifiers, we compute the following\nmetrics based on the predicted classes and the ground truth:\n$G-mean = \\sqrt{Recall \u00d7 Specificity} (5)$"}, {"title": "5.1.3. Fusion Strategy", "content": "As discussed in subsection 3.1, we use a late fusion strategy to achieve a\nmultimodal, multi-view classification for each lesion. This process involves\ntwo stages of integration. Initially, the fusion function \u03c6\u03bd aggregates the\nprobabilities of malignancy across views within each modality, obtaining the\nunimodal, multi-view probabilities of malignancy. Then, the fusion function\n\u03a6M integrates the unimodal, multi-view probabilities of malignancy, obtain-\ning an overall multimodal, multi-view probability of malignancy. For both\nfusion functions, we utilize a weighted average strategy, where probabilities\nare averaged using weights given by the MCC values computed from the val-\nidation set. This approach is motivated by the ability of MCC to address\nclass imbalance effectively, providing a robust measure of model performance,\neven in datasets with disproportionate class distributions [40]. Thus, for each\nmodality m\u2208 M, we compute the unimodal, multi-view probability of ma-\nlignancy pm as follows:\n$P_m = \\frac{\\Sigma_{v\\in \\{CC,MLO\\}} (P_{m,v} \\cdot MCC_{m,v})}{\\Sigma_{v\\in \\{CC,MLO\\}} MCC_{m,v}} (7)$\nwhere pm, v represents the probability of malignancy for lesions in images from\nmodality m and view v, while MCCm,v represents the MCC value computed\non the validation set for images of modality m and view v, based on the\npredicted class Cm,v and the ground truth. For each modality, we achieve an\nunimodal, multi-view classification by predicting the class cm based on the\npm value. Subsequently, we aggregate the unimodal, multi-view probabilities,\nusing the same weighted strategy to compute the multimodal, multi-view\nprobability of malignancy p for each lesion, as follows:\n$P = \\frac{\\Sigma_{m\\in \\{F,C\\}} P_m \\cdot MCC_m}{\\Sigma_{m\\in \\{F,C\\}} MCC_m} (8)$\nwhere MCCm represents the MCC value computed on the validation set for\nimages of modality m, based on the predicted class cm and the ground truth.\nWe achieve a multimodal, multi-view classification by predicting the class c\nbased on the p value."}, {"title": "5.2. Generative model", "content": "We utilize the CycleGAN [39] as the generative model due to its demon-\nstrated effectiveness in generating CESM images from FFDM images [27].\nThe experiments were conducted using a 5-fold cross-validation strategy.\nThe dataset was divided into training, validation, and test sets in proportions\nof 80%, 10%, and 10%, respectively, ensuring consistency with the splits used\nfor the classification task in the virtual biopsy. To prevent overfitting, we\napplied random data augmentation to the training set, including vertical and\nhorizontal shift (up to \u00b110% of the original dimension), zoom (up to \u00b110%),\nand rotation (up to \u00b115\u00b0)."}, {"title": "5.2.1. Training", "content": "Given the set of imaging views V = {CC, MLO}, we trained the Cy-\ncleGAN to generate CESM images XC, from FFDM images XF,v, where\nv\u2208 V. The training process involves two parallel pathways: one for generat-\ning CESM images in the CC view (Xc,cc) from FFDM images in the same\nview (XF,CC), and another for generating CESM images in the MLO view\n(XC,MLO) from FFDM images in the same view (XF,MLO). Our objective is\nto train the CycleGAN to synthesize CESM images, XC,cc and XC,MLO, that\nclosely resemble the corresponding real CESM images, Xc,cc and XC,MLO.\nThis can be formalized as follows:\n$G_v(X_{F,v}) = X_{C,v} \u2248 X_{C,v} \u2200v \u2208 V (9)$\nAfter a pre-training on the public dataset described in [20], we fine-tuned\nthe CycleGAN separately for the CC and MLO views using the dataset intro-\nduced in section 4, which had been preprocessed as outlined in subsection 4.1."}, {"title": "5.2.2. Evaluation", "content": "Both training processes ran for up to 200 epochs, employing an early stopping\ncriterion based on validation loss, with patience of 50 epochs, following an\ninitial 50-epoch warm-up phase. Although CycleGAN can typically operate\nwith unpaired datasets, our study used paired data to compute losses by di-\nrectly comparing the synthesized CESM images XC, with the corresponding\nreal CESM images XC,\u03c5. To ensure effective and coherent image-to-image\ntranslation, we employ three complementary loss functions:\nWe used Mean Squared Error for the adversarial loss, and L1 loss for both\ncycle consistency and identity mapping losses, with weighting factor of \u03bb\u2081 =\n10 and \u03bb2 = 5 respectively. We optimized the generator and discriminator\nnetworks using the Adam optimizer, with a learning rate of $10^{-5}$, weight\ndecay of $10^{-5}$, a beta of 0.5, and momentum set to 1. We did not further\ninvestigate any other hyperparameter configuration since their tuning is out\nof the scope of this manuscript. Nevertheless, the \u201cNo Free Lunch\u201d Theorem\nfor optimization asserts that no single set of hyperparameters can universally\noptimize model performance across all datasets.\nThe quality of image generation is quantitatively assessed on the test set,\ncalculating three metrics between the synthetic CESM images XC, and the\ntarget CESM images XC,v. For simplicity, we will refer to synthetic CESM\nimages as \u0177 and real CESM images as y throughout this section. The selected\nmetrics are Mean Squared Error (MSE), Peak-Signal-to-Noise Ratio (PSNR),\nand Structural Similarity Index (SSIM).\nThe MSE quantifies the mean squared difference between the pixel values\nof the target image y and the reconstructed image \u0177, thus is formulated as:\n$MSE(y, \u0177) = \\frac{1}{kh} \\sum_{i=1}^{k} \\sum_{j=1}^{h} (y_{ij} - \u0177_{ij})^2 (10)$\nwhere k and h are the number of rows and columns in the images, respectively,\nand $y_{ij}$ and $\u0177_{ij}$ represent the pixels elements at the i-th row and j-th column\nof y and \u0177, respectively. It varies in the range [0,\u221e], with lower values\nindicating higher quality of the reconstructed image.\nThe PSNR is defined as the ratio of the maximum possible power of a\nsignal to the power of the noise that affects the signal. In this context, the\nsignal represents the target image, while the noise corresponds to the error\nintroduced during its reconstruction. The PSNR is expressed in decibels\n(dB), with a PSNR value of 30 dB considered as excellent quality, 27 dB as\ngood quality, 24 dB as poor quality, and 21 dB as bad quality [26]. It is\ncommonly expressed as a function of the MSE as follows:\n$PSNR(y, \u0177) = 10.log_{10} (\\frac{max\u00b2(y)}{MSE(y,\u0177)}) (11)$\nThe SSIM [31] measures the similarity between two images by comparing\ntheir luminance, contrast, and structure. It is defined as follows:\n$SSIM(y, \u0177) = \\frac{(2\u00b5_y \u00b5_\u0177 + B_1)(2\u03c3_{y\u0177} + B_2)}{(\u00b5\u00b2_y + \u00b5\u00b2_\u0177 + B_1)(\u03c3\u00b2_y + \u03c3\u00b2_\u0177 + B_2)} (12)$\nwhere \u03bcy and \u03bc\u0177 represent the means of the images y and \u0177 respectively, while\n\u03c3y and \u03c3\u0177 represent their standard errors. B\u2081 and B2 are small constants\nused for stabilization. The SSIM varies in the range [0, 1], with higher values\nsignifying greater similarity between the synthetic and target images [31]."}, {"title": "5.3. Missing Modality Robustness", "content": "In clinical practice, it is common for certain patients to undergo only\nFFDM without the additional CESM examination. To mimic this real-world\nlimitation and evaluate the robustness of the virtual biopsy under missing\ndata conditions, we simulate scenarios with varying proportions of missing\nCESM images among patients. In these scenarios, we repeat the inference\nprocess using imputed CESM data Xc, generated with CycleGAN to replace\nthe missing CESM inputs Xcv for the respective classifier fc,v. We assess\nthe effect of different proportions of real and synthetic CESM images on\nvirtual biopsy performance, by testing datasets containing n% synthetic and\n(100-n)% real CESM images in the CC and MLO views, with n ranging\nfrom 10 to 100 in increments of 10. For each n value, we randomly select\npatients requiring synthetic CESM images, repeating the sampling process\n10 times to ensure statistical robustness. For each sampling at a given n\nvalue, we perform the virtual biopsy process. We evaluate the classification\nperformance using AUC, G-mean, and MCC and obtain a comprehensive\nperformance for each value of n by averaging these metrics across the 10\nsampling.\nTo evaluate the impact of AI-generated CESM images Xc,v, we compare\nthe virtual biopsy performance obtained with real FFDM images XF,v and\nvarying proportions of synthetic CESM images Xc,v against two benchmarks:\n(1) performance with both real FFDM images XF, and real CESM images\nXc,v, and (2) performance with only real FFDM images XF,v. The perfor-\nmance with real FFDM and real CESM images establishes the upper bound,\nrepresenting the ideal scenario where all imaging modalities are available.\nIn contrast, the performance with only real FFDM images defines the lower\nbound, reflecting the worst-case scenario where CESM is not performed, and\nvirtual biopsy relies solely on FFDM. Since our approach generates CESM\nimages when unavailable and integrates them into the virtual biopsy work-\nflow, the FFDM-only scenario is particularly relevant as the baseline for\nevaluating the effectiveness of our method."}, {"title": "6. Results and Discussion", "content": "In this section, we first present and discuss the results achieved from the\ngeneration of synthetic CESM images, and then we move to the results of\nthe virtual biopsy task."}, {"title": "6.1. Generative AI for Missing CESM Data", "content": "Here we present the evaluation of CycleGAN's capability to generate\nCESM images from FFDM inputs for both CC and MLO views."}, {"title": "6.2. Virtual Biopsy", "content": "Let us now turn the attention to the results of our multimodal, multi-view\nclassification pipeline designed to perform the virtual biopsy. The\nformer figure displays for each classifier the mean values and standard errors\nof the classification metrics across the following experimental settings:\n\u2022 F: virtual biopsy performed using only FFDM modality, combining CC\nand MLO views;\n\u2022 C: virtual biopsy performed using only CESM modality, combining CC\nand MLO views;\n\u2022 \u0108: virtual biopsy performed using only synthetic CESM modality gen-\nerated by CycleGAN, combining CC and MLO views;\n\u2022 F+C: virtual biopsy combining the results of F and C;\n\u2022 F+\u0108: virtual biopsy combining the results of F and \u0108.\n investigates the robustness of the classification framework by\nvarying the percentage of synthetic CESM images used. It evaluates the per-\nformance of the F, C*, and F+C* experimental settings across the different\nclassifiers. Specifically:\n\u2022 As for , the F setting represents virtual biopsy using only\nFFDM modality, combining CC and MLO views, and is inherently\nunaffected by the proportion of synthetic CESM images;\n\u2022 The C* setting corresponds to experiments where the CESM modality\nis used, with the proportion of synthetic CESM images varying from\n0% to 100% in increments of 10%. A 0% proportion corresponds to the\nC setting (real CESM only), while a 100% proportion corresponds to\nthe \u0108 setting (synthetic CESM only).\n\u2022 The F+C* setting involves combining the FFDM and CESM modali-\nties, where the CESM modality includes a varying percentage of syn-\nthetic data. At 0% synthetic CESM, this setting corresponds to F+C,\nand at 100% synthetic CESM, it corresponds to F+C.\nFrom both figures, it is evident that for both ResNet models and VGG16,\nthe experimental settings C and F+C consistently achieve higher values of\nAUC, G-mean, and MCC compared to the F setting. This finding highlights\nthe critical role of the CESM modality in enhancing the virtual biopsy pro-\ncess, aligning with the study's objective of exploring the use of synthetic\nCESM images as a viable alternative when real CESM images are unavail-\nable. However, as shown in , the performance of both the C* and\nF+C* settings declines as the proportion of synthetic CESM data increases.\nDespite this decrease, the results demonstrate that even synthetic CESM im-\nages positively contribute to virtual biopsy when compared to the F setting,\nwhich uses only the FFDM modality. Specifically, for ResNet18, both C*\nand F+C* consistently outperform the F setting, including the \u0108 and F+C\nscenarios, where 100% of synthetic CESM images are involved. These re-\nsults support the utility of synthetic CESM data in improving virtual biopsy\nwhen the real CESM modality is unavailable. Similarly, for ResNet50, the\nC* and F+C* settings outperform F, with the F+C* setting providing a\nslight improvement over the C* setting, further highlighting the benefits of\nthe multimodal approach. On the other hand, for VGG16, the C* setting\ndemonstrates a performance decline relative to F when the proportion of\nsynthetic CESM data exceeds 70%. However, the F+C* setting mitigates\nthis decline, delivering improved performance compared to both the C* and\nF settings, even in the F+\u0108 scenario. This reinforces the advantage of the\nmultimodal approach, confirming that integrating synthetic CESM images is\npreferable to relying solely on FFDM. The numerical values of metrics for\nthe F, C, \u0108, F+C, and F+\u0108 experimental settings, are provided in in the appendix. These values are expressed on a [0,100] scale for AUC and\nG-mean, and on a [-100, 100] scale for MCC, across all classifiers. As a fur-\nther analysis, shows the AUC, G-mean, and MCC metrics calculated\nfor test set images grouped according to breast density, as defined by the\nACR categories b, c, or d. We compare the performance of the virtual biopsy\nacross three experimental settings: F, F+C, and F+\u0108. For clarity, results for\nthe C and \u0108 settings are excluded, as previous analyses have demonstrated\nthe advantages of the multimodal approach. This investigation aims to assess\nwhether the F+\u0108 configuration, which integrates synthetic CESM images,\nconsistently outperforms the F setting, reliant solely on FFDM images, par-\nticularly for dense breasts under the c and d categories, where tumor masses\nare often obscured by surrounding tissue. Together with the performance of\nthe three models, we report their average performance in the \u201cAVG\u201d column.\nWe use this average performance as the basis for evaluation since our focus is\nnot on identifying the top-performing individual classifier for virtual biopsy\nbut rather on demonstrating the efficacy of the multimodal approach, also\nwhen synthetic CESM images are utilized. As expected, the F+C setting con-\nsistently achieves the best performance across all breast density categories.\nThus, the subsequent discussion focuses on the comparison between the F\nand F+\u0108 settings. Notably, for category a, where all test images belong to\nthe same class, AUC cannot be computed, and while MCC could technically\nbe calculated, it would always yield a value of 0, rendering it uninformative.\nConsequently, G-mean is the only meaningful metric in this scenario, and\nthe results clearly show that the F+\u0108 configuration surpasses the F setting.\nSimilarly, for the more challenging categories, represented by ACR c and d,\nall metrics (AUC, G-mean, and MCC) exhibit superior performance under\nthe F+C setting, confirming the added value of synthetic CESM images for\nthe virtual biopsy. However, this trend is not observed for ACR category b,\nwhere the F+C setting shows a slight decline across all metrics compared to\nthe F setting. Despite this exception, the consistent gains observed for dense\nbreast categories highlight the effectiveness of incorporating synthetic CESM\nin enhancing virtual biopsy when real CESM are unavailable."}, {"title": "7. Conclusion", "content": "We investigated the application of generative AI and multimodal deep\nlearning to enable breast cancer virtual biopsy, addressing the challenge of\nmissing CESM data in diagnostic workflows. Our proposed approach inte-\ngrates FFDM and CESM modalities in both CC and MLO views, utilizing\nCycleGAN to synthesize CESM images when real CESM data are unavail-\nable. Rather than centering on identifying the top-performing classifier, our\nstudy focused on demonstrating the efficacy of a multimodal framework ca-\npable of incorporating synthetic CESM images when required. Our findings\nreveal that real CESM images significantly improve virtual biopsy perfor-\nmance compared to relying on FFDM alone. Furthermore, when real CESM\ndata is missing, synthetic CESM images generated by CycleGAN proved ef-"}]}