{"title": "Lifelong Learning of Large Language Model based Agents: A Roadmap", "authors": ["Junhao Zheng", "Chengming Shi", "Xidi Cai", "Qiuke Li", "Duzhen Zhang", "Chenxing Li", "Dong Yu", "Qianli Ma"], "abstract": "Lifelong learning, also known as continual or incremental learning, is a crucial component for advancing Artificial General Intelligence (AGI) by enabling systems to continuously adapt in dynamic environments. While large language models (LLMs) have demonstrated impressive capabilities in natural language processing, existing LLM agents are typically designed for static systems and lack the ability to adapt over time in response to new challenges. This survey is the first to systematically summarize the potential techniques for incorporating lifelong learning into LLM-based agents. We categorize the core components of these agents into three modules: the perception module for multimodal input integration, the memory module for storing and retrieving evolving knowledge, and the action module for grounded interactions with the dynamic environment. We highlight how these pillars collectively enable continuous adaptation, mitigate catastrophic forgetting, and improve long-term performance. This survey provides a roadmap for researchers and practitioners working to develop lifelong learning capabilities in LLM agents, offering insights into emerging trends, evaluation metrics, and application scenarios. Relevant literature and resources are available at https://github.com/qianlima-lab/awesome-lifelong-llm-agent.", "sections": [{"title": "1 INTRODUCTION", "content": "IFELONG learning [1], [2], also known as continual or incremental learning [3], [4], has become a key focus in the development of intelligent systems. As shown in Figure 1, lifelong learning has attracted increasing research attention in recent years. It plays a crucial role in allowing these systems to continuously adapt and improve over time. As noted by Legg et al. [5], human intelligence is fundamentally about fast adaptation to a wide range of environments, highlighting the need for AI systems to exhibit this same level of adaptability. Lifelong learning refers to a system's ability to acquire, integrate, and retain knowledge while avoiding the forgetting of previously learned information. This ability is particularly important for systems that operate in dynamic, complex environments, where new tasks and challenges frequently arise. In contrast to traditional machine learning models, which are typically trained on fixed datasets and optimized for specific tasks, lifelong learning systems are designed to evolve. They accumulate new knowledge and continuously refine their capabilities as they encounter new situations.\nDespite its potential, there remains a significant gap between advancements in AI and the practical application of lifelong learning. While humans can naturally integrate new knowledge while retaining the old, current AI systems face two main challenges in lifelong learning: catastrophic forgetting [6] and loss of plasticity [7], [8]. These challenges form the stability-plasticity dilemma [9]. On one hand, catastrophic forgetting occurs when systems forget previously learned information as they learn new tasks, which is particularly problematic when the environment changes. On the other hand, the loss of plasticity refers to the system's inability to adapt to new tasks or environments. These two issues represent opposing ends of the learning spectrum: static"}, {"title": "1.1 Motivation for Building Lifelong Learning LLM Agents", "content": "The recent advancements in large language models (LLMs) [11], [12] have significantly transformed the field of natural language processing. Models like GPT-4 [12] are designed to process and generate human-like text by learning from vast amounts of textual data. They excel in tasks such as text generation, machine translation, and question answering due to their ability to understand complex language patterns. However, traditional LLMs [11], [12] are static after training, meaning they cannot adapt to new tasks or environments once deployed. Their knowledge remains fixed, and they struggle to integrate new information without retraining, limiting their applicability in dynamic real-world scenarios.\nIn contrast, LLM agents represent a more advanced form of artificial intelligence. Unlike standard LLMs, which process input text and generate output based on prior training, LLM agents [13], [14] are autonomous entities capable of interacting with their environment. These agents can perceive multimodal data (e.g., text, images, sensory data), store this information in memory, and take actions to influence or respond to their surroundings [15]\u2013[17]. Designed to continuously adapt to new contexts, LLM agents learn from their interactions and experiences, improving their decision-making capabilities over time.\nThe motivation for incorporating lifelong learning into LLM agents arises from the need to develop intelligent systems that can not only adapt to new tasks but also retain and apply prior knowledge across a wide range of dynamic environments, aligning with Legg et al.'s [5] view of intelligence as fast adaptation to a wide range of environments. Currently, existing LLM agents are typically developed as static systems, limiting their ability to evolve in response to new challenges. Moreover, most lifelong learning research on LLMs [1], [4] focuses on handling ever-changing data distributions without interacting with an environment. For example, continual fine-tuning of LLMs to adapt to instructions from specific domains [1]. However, these approaches still treat LLMs as static black-box systems and do not address the practical need for LLMs to learn interactively within real-world environments. Figure 2 compares the traditional lifelong learning paradigm with the novel paradigm of LLM agents that interact with dynamic environments, as discussed in this survey.\nIn real-world applications, LLM agents are expected to adapt to diverse environments such as gaming, web browsing, shopping, household tasks, and operating systems without the need to design separate agents for each new context. By incorporating lifelong learning capabilities, these agents can overcome such limitations. They continuously learn and store knowledge from multiple modali-"}, {"title": "1.2 Scope of the Survey", "content": "This survey provides a comprehensive overview of the key concepts, techniques, and challenges involved in developing lifelong learning systems for LLM-based agents. As the first survey to systematically summarize potential techniques for lifelong learning in LLM agents, it addresses the following research questions (RQs):\nRQ1: What are the core concepts, development processes, and fundamental architectures of LLM agents designed for lifelong learning? (Section 3)\nRQ2: How can LLM agents continuously perceive and process single-modal and multi-modal data to adapt to new environments and tasks? (Sections 4, 5)\nRQ3: What strategies can mitigate catastrophic forgetting and enable the retention of previously learned knowledge? (Sections 6, 7, 8, 9)\nRQ4: How can LLM agents perform various actions, such as grounding, retrieval, and reasoning, in dynamic environments? (Sections 10, 11, 12)\nRQ5: What evaluation metrics and benchmarks are used to assess the performance of lifelong learning in LLM agents? (Section 13)\nRQ6: What are the real-world applications and use cases of lifelong learning LLM agents, and how do they benefit from continuous adaptation? (Section 14)\nRQ7: What are the key challenges, limitations, and open problems in the development of lifelong learning for LLM-based agents? (Section 15)\nBy addressing these research questions, this survey serves as a step-by-step guide for understanding the design, challenges, and applications of lifelong learning in LLM agents. It reviews state-of-the-art techniques and highlights emerging trends and future directions for research in this rapidly evolving field."}, {"title": "1.3 Contribution of the Survey", "content": "To the best of our knowledge, this is the first survey that systematically reviews the latest advancements at the intersection of lifelong learning and LLM agents. The main contributions of this survey are as follows:\n\u25a0 Foundational Overview: Provides a thorough overview of the foundational concepts and architectures essential for implementing lifelong learning in LLM agents.\n\u25a0 In-Depth Component Analysis: Examines key components, including perception, memory, and action modules, that enable adaptive behavior in LLM agents.\n\u25a0 Comprehensive Discussion: Discusses real-world applications, evaluation metrics, benchmarks, as well as key challenges and future research directions in the domain of lifelong learning for LLM agents."}, {"title": "1.4 Survey Structure", "content": "This survey is organized as follows. Section 2 reviews related surveys and literature on LLM agents and lifelong learning. Section 3 introduces the foundational concepts, development processes, and overall architecture of LLM agents designed for lifelong learning. Sections 4 and 5 discuss the design of lifelong learning LLM agents from a perceptual perspective, focusing on single-modal and multi-modal approaches, respectively. Sections 6, 7, 8, and 9 examine the design of LLM agents from a memory perspective, covering working memory, episodic memory, semantic memory, and parametric memory. Sections 10, 11, and 12 explore the design of LLM agents from the action perspective, including grounding actions, retrieval actions, and reasoning actions. Section 13 presents evaluation metrics and benchmarks for assessing lifelong learning in LLM agents. Section 14 delves into real-world applications and use cases of lifelong learning LLM agents. Section 15 offers practical insights and outlines future research directions. Finally, Section 16 concludes the survey."}, {"title": "2 RELATED WORK", "content": "In recent years, the rapid growth of lifelong learning has received a great deal of academic attention. In order to summarize the research in this area, researchers have written a large number of surveys. In this subsection, we will briefly review these surveys and summarize their main contributions in order to better highlight our research, as illustrated in Table 1.\nSome surveys involve presenting research advances in lifelong learning in the field of natural language processing. For instance, Zheng et al. [1] systematically summarize lifelong learning methods for LLMs for the first time by considering them from the perspective of 12 scenarios, which are classified into two categories: internal and external knowledge. In the section on internal knowledge, this survey presents scenarios about continual finetuning in natural"}, {"title": "2.1 Surveys on Lifelong Learning", "content": "language processing. Ke and Liu [26] first introduce the settings and learning modes of continual learning and summarize the natural language processing problems in continual learning. Then, they propose that existing continual learning techniques are mainly used to solve the two challenges of catastrophic forgetting and knowledge transfer. On this basis, this survey analyzes approaches for catastrophic forgetting prevention and knowledge transfer. Besides, Biesialska et al. [28] classify continual learning methods into three main categories: rehearsal, regularization, and architectural as well as a few hybrid categories. This survey also explores the application of continual learning to several tasks in natural language processing, including question answering, sentiment analysis, and text classification.\nOther surveys present progress in lifelong learning from a variety of perspectives. For instance, Wu et al. [24] describe the different training stages of LLMs, including continual pre-training, continual instruction tuning, and continual alignment. In addition, this survey presents benchmarks and evaluations on continual learning. Shi et al. [25] present the general picture of continually learning LLMs from vertical continuity and horizontal continuity. In addition, this survey introduces the evaluation protocols and datasets for continually learning large language models. Starting from the motivation of solving the learning problem, Zhou et al. [27] group pre-trained models-based continual learning studies into three categories, i.e., prompt-based methods, representation-based methods, and model mixture-based methods. In addition, this survey analyzes the experimental results of ten algorithms of the above three categories of methods on seven benchmark datasets. Wang et al. [4] classify continual learning methods into five main categories: regularization-based approach, replay-based approach, optimization-based approach, representation-based approach and architecture-based approach, where each category is analyzed in detail for its typical implementations and empirical properties. From the biological perspective of lifelong learning, Parisi et al. [29] analyze lifelong learning and catastrophic forgetting in neural networks. In addition,"}, {"title": "2.2 Surveys on Large Language Model-based Agent", "content": "With the boom of LLM-based Agents, a wide variety of surveys have emerged that systematically summarize the research in this area. In this subsection, we briefly review these surveys, presenting their main contributions, as illustrated in Table 1.\nFrom the perspective of agent architecture design, Wang et al. [13] propose a unified framework including four parts, namely profiling module, memory module, planning module and action module for the first time. In addition, this survey summarizes the applications, evaluations and challenges of LLM-based autonomous agent. Similarly, Xi et al. [14] introduce the three main components of the agent from the perspective of construction: brain, perception and action. In addition, this survey also investigates the practical applications of agents in different scenarios and the personal and social behaviors of agents.\nAdditionally, different focus and categorization produce a diverse understanding of the field. Some other papers review specific aspects of LLM-based agents. For example, Li et al. [30] present the background and technological advances of intelligent personal assistants and introduce the concept of personal LLM agents. This survey delves into the issues of fundamental capabilities and efficiency of personal LLM agents. Finally, it analyzes the challenges regarding security and privacy. Cheng et al. [31] consider the LLM-based agent system framework and analyze the single agent system and multi-agent system in detail. Similarly, this survey discusses the performance evaluation and prospect applications of LLM-based agent. Li et al. [32] propose a unified framework of the general multi-agent system, which includes five modules, namely profile, perception, self-action, mutual interaction and evolution. In addition, this survey also analyzes the application and some key open issues of multi-agent systems. Besides, Zhang et al. [33]define the concept of LLM-brained GUI agents and analyze the architecture and progress of LLM-brained GUI agents.\nThese surveys analyze important progress in building LLM-based agents, covering everything from personal LLM agents to multi-agent systems. However, they fall short in their connection to lifelong learning. Lifelong learning emphasizes the ability of agents to continuously learn and adapt in changing environments, whereas these surveys mainly focus on current technology implementations and application scenarios. In contrast to them, our survey provides an in-depth summary of how to equip LLM-based agents with the ability to learn and evolve in the long term,"}, {"title": "3 BUILDING LIFELONG LEARNING LLM AGENTS", "content": "We begin by presenting the formal definition of lifelong learning in LLM agents, followed by an overview of its historical development, and conclude with a detailed description of the overall architecture for lifelong learning in LLM agents."}, {"title": "3.1 Formal Definition of Lifelong Learning for LLM-based Agents", "content": "Definition 3.1 (Environment of LLM Agents). We model the environment of an LLM-based agent as a goal-conditional partially observable Markov decision process (POMDP). Formally, a POMDP is defined as an 8-tuple:\n$\\mathcal{E} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{G},\\mathcal{T}, \\mathcal{R}, \\Omega, \\mathcal{O}, \\gamma)$,\nwhere:\n\u2022 $\\mathcal{S}$ is a set of states. Each $s \\in \\mathcal{S}$ can include multimodal information such as textual descriptions, images, or structured data (e.g., a product page on an e-commerce website containing text, images, and product specifications).\n\u2022 $\\mathcal{A}$ is a set of actions. Each $a \\in \\mathcal{A}$ represents an instruction or command that the agent can issue, often expressed in natural language (e.g., \"add this item to the cart\").\n\u2022 $\\mathcal{G}$ is a set of possible goals. Each $g \\in \\mathcal{G}$ specifies a particular objective (e.g., \"purchase a laptop\").\n\u2022 $\\mathcal{T}(s' | s,a)$ is the state transition probability function. For each state-action pair $(s,a)$, $\\mathcal{T}$ defines a probability distribution over next states $s'\\in \\mathcal{S}$. For example, if the agent clicks on a product link, $\\mathcal{T}$ models the probability of transitioning to the product's detail page.\n\u2022 $\\mathcal{R}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{G} \\rightarrow \\mathbb{R}$ is the goal-conditional reward function. For each triplet $(s,a,g)$, $\\mathcal{R}$ may return a numeric value or textual feedback (e.g., \u201cGood job!\") indicating how well the action $a$ taken in state $s$ advances the objective $g$. This allows the environment to interactively provide user feedback aligned with the chosen goal.\n\u2022 $\\Omega$ is a set of observations. Each $o' \\in \\Omega$ can be textual, visual, or a combination thereof. Observations represent the agent's partial view of the underlying state (e.g., the content visible on a webpage).\n\u2022 $\\mathcal{O}(o' | s', a)$ is the observation probability function. Given that the environment transitioned to state $s'$, and action $a$ was taken, $\\mathcal{O}$ defines the probability of receiving observation $o'$. For instance, upon navigating to a product page, $\\mathcal{O}$ might model the probability of observing a particular product image and description.\n\u2022 $\\gamma \\in [0,1)$ is the discount factor, which balances immediate versus long-term rewards. In the context of LLM agents, the discount factor is used only when the reward is numeric.\nThis framework captures the complexities of real-world scenarios. For instance, a virtual shopping assistant (the agent) interacts with a complex, multimodal website (the environment) to achieve a goal such as completing a purchase. The assistant receives partial observations (e.g., product listings), takes actions (e.g., clicking a link), and obtains feedback (numeric or textual) reflecting progress toward the goal.\""}, {"title": "3.2 Background and History of Lifelong Learning for Al Systems", "content": "Lifelong learning, also referred to as continual or incremental learning, is grounded in the idea that intelligent systems should continually acquire, refine, and retain knowledge over extended periods\u2014much like humans. Unlike traditional machine learning approaches that assume access to a fixed, stationary dataset, lifelong learning frameworks confront the reality that data and tasks evolve over time, and that models must adapt without forgetting previously mastered skills. A illustration of the development of lifelong learning is provided in Figure 5.\nHuman and Neuroscience Perspectives: The principles of lifelong learning draw inspiration from human cognitive development. Humans do not train on a fixed dataset; instead, we accumulate knowledge from diverse and changing experiences [34]. Memory consolidation in the human brain, involving complex interactions between the hippocampus and neocortex, ensures that new learning does not completely overwrite old memories. Studies of synaptic plasticity and learning in neuroscience help inform algorithms that attempt to preserve and integrate previously"}, {"title": "3.2.1 Foundation Concepts (1980s\u2013Present)", "content": "The origins of lifelong learning research can be traced back to early adaptive control and incremental learning studies in the 1980s and 1990s. Researchers recognized that many real-world environments are non-stationary: the distribution of data changes over time, and models must adapt accordingly. During this period, foundational concepts such as catastrophic forgetting and the stability-plasticity dilemma were first identified. Catastrophic forgetting describes the tendency of neural networks to lose previously acquired knowledge when trained on new data [6], [38], while the stability-plasticity dilemma [34], [39] addresses the need to balance retaining old knowledge (stability) with acquiring new information (plasticity).\nThroughout the 1990s and into the 2000s, catastrophic forgetting became a central research focus. Early attempts to mitigate forgetting relied on replaying previously encountered examples, adjusting network architectures to accommodate new tasks, or using heuristics to preserve existing representations [9]. As neural networks became more widely adopted across domains, more sophisticated methods for lifelong learning were developed. Researchers introduced structured approaches, such as dynamically expanding architectures that allocate new network capacity for novel tasks, dual-memory frameworks inspired by the interplay between short-term and long-term memory in the brain, and early forms of regularization and parameter isolation techniques to reduce interference with older knowledge [29], [40]."}, {"title": "3.2.2 Deep Lifelong Learning (2010\u2013Present)", "content": "The rise of deep learning in the early 2010s significantly expanded the capabilities of neural networks. However, these initial breakthroughs often assumed a stationary training dataset. Incorporating incremental learning into deep networks required new strategies. Techniques like Elastic Weight Consolidation (EWC) [41] and Learning without Forgetting (LwF) [42] became popular, using careful parameter adjustments to preserve previously learned representations. These methods [2] leveraged deep learning's powerful representational ability while addressing the core issue of catastrophic forgetting, pushing lifelong learning closer to practical real-world applicability.\nDuring this period, various approaches were categorized based on their methodologies: Rehearsal-Based Methods: Involve replaying or retraining on a subset of previously seen data to retain past knowledge [43]. Regularization-Based Methods: Apply constraints or penalties to the loss function to prevent significant changes to important parameters [41]. Architecture-Based Methods: Dynamically modify the network architecture to accommodate new tasks without interfering with existing ones [44]. Representation-Based Methods: Focus on learning robust and transferable representations that facilitate knowledge retention and transfer [45]. Prompt-Based Methods: Utilize prompts or auxiliary inputs to guide the model in retaining and utilizing past knowledge [46]."}, {"title": "3.2.3 LLM Lifelong Learning (2020-Present)", "content": "With the advent of large-scale pre-training, especially in language models, the landscape of Al has been reshaped. LLMs like GPT-3 [11] introduced context-aware word representations, enabling models to perform a wide range of Natural Language Processing (NLP) tasks with high efficiency. Lifelong learning in LLMs initially focused on conventional"}, {"title": "3.2.4 LLM-based Agents Lifelong Learning (2023\u2013Present)", "content": "Starting around 2023, the focus of lifelong learning has expanded from conventional NLP tasks to more realistic and complex applications embodied by LLM-based agents. Unlike LLMs, which primarily handle tasks such as text generation and classification, LLM-based agents are designed to interact with dynamic environments and perform intricate tasks like online shopping, household management, operating system operations, and more. These agents require advanced lifelong learning capabilities to manage multimodal inputs, execute sequential decision-making processes, and maintain coherent performance across diverse and evolving tasks.\nKey advancements in this period include: Dynamic Task Adaptation: Developing models that can seamlessly switch between varied tasks without compromising performance on previously learned ones [21]\u2013[23]. Multimodal Integration: Enhancing agents' abilities to process and integrate information from multiple modalities (e.g., text, images, and sensor data) to perform complex real-world tasks [18]\u2013[20]. Memory and Knowledge Management: Implementing sophisticated memory systems that allow agents to retain and efficiently retrieve past experiences, facilitating better decision-making and knowledge transfer [55]\u2013[57]. Reinforcement Learning Integration: Combining lifelong learning with reinforcement learning techniques to enable agents to learn optimal policies in interactive environments continuously [15], [33], [58]. External Knowledge Integration: Enabling agents to utilize external tools and databases to enhance their capabilities, as well as integrating retrieval mechanisms to access relevant information on-the-fly [48], [59]. Broader Real-world Applications: Developing chatbots and interactive agents that can sustain long-term dialogues, adapt to user preferences, and perform tasks within web or gaming environments [60]\u2013[62]."}, {"title": "3.2.5 Summary", "content": "In summary, the evolution of lifelong learning in Al systems is a story of increasing sophistication-moving from foundational concepts aimed at mitigating forgetting to advanced, principled methods informed by cognitive science and neuroscience. The advent of LLMs has further propelled the field, initially enhancing conventional NLP tasks and now expanding into the realm of LLM-based agents that tackle complex, real-world applications. As LLMs and other large-scale models become integral to real-world applications, lifelong learning stands at the forefront, driving innovation toward systems that improve continuously and gracefully over their operational lifetimes."}, {"title": "3.3 Overall Architecture", "content": "The architecture of a lifelong learning LLM-based agent is designed to continually adapt, integrate, and optimize its behavior across a range of tasks and environments. In this subsection, we identify three essential modules\u2014Perception, Memory, and Action\u2014that enable lifelong learning. This division follows the framework introduced in prior work [14], with one notable difference: rather than retaining the \u201cBrain\u201d module, we adopt a \u201cMemory\u201d module as proposed in [14], offering clearer functionality and improved modularity.\nEach module interacts with the others to ensure that the agent can process new information, retain valuable knowledge, and select contextually appropriate actions. The rationale for these three modules stems from the agent's need to (i) perceive and interpret evolving data, (ii) store and manage knowledge from past experiences, and (iii) perform tasks that adapt to changing circumstances.\nThese modules form a dynamic feedback loop: the Perception module delivers new information to the Memory module, where it is stored and processed. The Memory module then guides the Action module, influencing the environment and informing future perception. Through this continuous cycle, agents progressively refine their knowledge and improve their adaptability, ultimately enhancing their performance in complex, dynamic environments.\nIn what follows, we describe each module in detail, examining how its design contributes to the agent's lifelong learning capabilities. Figure 6 provides an illustration of this overall architecture, while Figure 7 summarizes the organization of the subsequent sections."}, {"title": "3.3.1 Perception Module", "content": "The perception module of a lifelong learning LLM-based agent is responsible for acquiring and integrating informa-"}, {"title": "3.3.2 Memory Module", "content": "The memory module in a lifelong learning LLM agent allows the agent to store, retain, and recall information\u2014essential for both learning from past experiences and improving decision-making. Memory is the foundation for an agent's ability to develop coherent long-term behavior, make informed decisions, and interact meaningfully with other agents or humans. The memory module thus supports"}, {"title": "3.3.3 Action Module", "content": "The action module enables the agent to interact with its environment, make decisions, and execute behaviors that shape the course of its learning. In a lifelong learning framework, actions are crucial for closing the feedback loop: actions influence the environment, and the environment provides feedback that is used to refine future actions.\nWe categorize actions into three main types: Grounding Actions, Retrieval Actions, and Reasoning Actions: Grounding Actions: These are the primary means by which the agent interacts with the environment. Grounding ac- tions involve physically or digitally affecting the environment, whether through manipulating objects, generating text, or triggering specific behaviors. The effects of these ac- tions may persist, influencing future behavior and feedback"}, {"title": "4 PERCEPTION DESIGN: SINGLE-MODAL PERCEPTION", "content": "The single-modal perception of LLM-based agents primarily involves the reception of textual information. Throughout the process of lifelong learning, the sources of textual information that agents encounter may originate from various structures and environments. In natural text environments, current LLM-based systems have demonstrated the fundamental capability to communicate with humans through text input and output [12], [63]. Building on this foundation, the agent needs to acquire text information from non-natural text environments to better simulate information perception in the real world. Therefore, in this section, we present methods for single-modality perception in environments such as web and game environments."}, {"title": "4.1 Web Pages and Charts Environment", "content": "Certain methods have been developed to extract structured text that adheres to standardized formats [21], [64]\u2013[67], thereby transforming complex information into a format accessible to LLM-based agents. The mainstream approaches can be broadly classified into two categories: HTML manipulation and screenshot. For instance, Synapse [65] and AgentOccam [64] simplify HTML elements from web pages and selectively incorporate them into prompts, while WebAgent [66] summarizes HTML documents and breaks instructions down into multiple sub-instructions, proposing an enhanced planning strategy. Additionally, to effectively harness the information provided by images, several studies [68]\u2013[72] have converted screenshots into textual formats for alignment with LLMs."}, {"title": "4.2 Game Environment", "content": "LLM-based agents can perceive their surroundings through textual mediums [73]\u2013[75], recognizing elements such as characters, time, location, events, and emotions [76], and can perform corresponding actions based on the feedback from these gaming elements using textual instructions. For example, JARVIS-1 [73] improves its understanding of the environment through self-reflection and self-explanation, incorporating previous plans into its prompts. VillagerAgent [74] utilizes a dedicated state manager module to filter task-relevant environmental information from the global context. By interfacing with graphical user interfaces (GUIs), LLM-based agents can enhance their ability to extract textual information from visual data [69], [72], [77], [78], thus facilitating a better understanding of the graphics and elements within user interfaces.\nIn conclusion, a human-like LLM-based agent should possess strong text perception and adaptability across a variety of complex environments. As related research continues to grow, exploring how agents perceive text input in broader and more diverse environments holds significant promise for future advancements."}, {"title": "5 PERCEPTION DESIGN: MULTIMODAL PERCEPTION", "content": "The real world comprises diverse data modalities, making unimodal perception methods insufficient to address its complexity. With the explosive growth of image, text, and video content on online platforms, developing LLM-based agents capable of continuously perceiving multimodal information has become crucial. These agents must effectively integrate information from different modalities while maintaining the accumulation and adaptation of prior knowledge. This enables them to better emulate the lifelong learning process of humans in multimodal environments, thereby enhancing their overall perceptual and cognitive capabilities. In this section, we categorize the lifelong learning methods for the agent's perception of multimodal information into new knowledge perception and old knowledge perception. A comparison and summarization of the related methods in provided in Table 2."}, {"title": "5.1 New Knowledge Perception", "content": "In lifelong learning scenarios involving the perception of various modalities, agents must focus on the interaction between different modalities and the perception and processing of new modalities to better handle the rapidly evolving forms of information in the real world. Many studies explore how an agent can maintain stability on tasks involving existing modalities while improving its capability to handle new tasks when encountering those with new modalities. We categorize new knowledge perception into the following two scenarios: Modality-Complete Learning and Modality-Incomplete Learning."}, {"title": "5.1.1 Modality-Complete Learning", "content": "Modality-Complete Learning assumes that all data has the same modality during both the training and inference stages. In this scenario, the agent's lifelong learning for multimodal perception focuses on how to perceive data from multiple modalities and achieve cross-modal knowledge transfer in new tasks.\nSome studies [79]\u2013[82] have explored Modality-Agnostic Models, which can perceive any modality as input. Perceiver [79] utilizes an iterative attention mechanism to dynamically encode any modality of data and generate a unified high-dimensional representation. VATT [80] adopts a shared transformer architecture and learns cross-modal correlations effectively from raw video, audio, and text through multimodal self-supervised learning tasks. Omnivore [81] introduces a multimodal feature aggregation mechanism, using shared convolutional layers and an adaptive feature fusion strategy to achieve a unified representation of various visual modalities such as images, videos, and depth maps. ModaVerse [82] leverages LLMs as the core to build an efficient modality transformer framework, enabling fast information fusion across different modalities and improving the processing efficiency of multimodal tasks.\nOn the other hand, Cross-Modal Knowledge Transfer refers to the effective application of knowledge acquired in one modality to improve performance on tasks involving another modality [83], [84], [87], [89], [90], [112]. Some methods achieve knowledge transfer between different modalities by feeding connected unimodal features into linear layers [87], [89], [90]. In visual-language tasks, some studies [83], [84], [112] focus on transferring visual knowledge into language models. For example, Vokenization [83] introduces"}, {"title": "5.1.2 Modality-Incomplete Learning", "content": "Modality-Incomplete Learning involves the challenge of how the agent can dynamically adapt to effectively learn and infer when encountering incomplete or missing modality information during lifelong learning (See Figure 8). By utilizing the MoE [114] module, PathWeave [92] introduces a novel Adapter in Adapter (AnA) framework, enabling the seamless integration of single-modal and cross-modal adapters, allowing for incremental learning of new modal knowledge. Similarly, DDAS [93] can automatically route inputs within and outside the training data distribution to the MoE adapter and the original CLIP model, respectively, to achieve efficient dynamic modality adaptation.\nSome studies [91], [94]\u2013[96] have utilized available modality information to predict the representation of the"}, {"title": "5.2 Old Knowledge Perception", "content": "In the process of lifelong learning, the agent not only needs to leverage existing modality experiences to complete tasks involving new modalities, but also must maintain the stability of old knowledge after receiving new information. In this section, we present a classification of representative"}, {"title": "5.2.1 Regularization-based Approach", "content": "The regularization-based Approach aims to mitigate the phenomenon of catastrophic forgetting by introducing regularization terms that limit the changes in model parameters during the learning of new tasks. Based on the method of constraint application, the Regularization-based Approach can be subdivided into two directions: weight Regularization and function Regularization.\nWeight Regularization directly applies penalty terms to the model's weights, restricting their changes when learning new tasks. For example, TIR [100] and Model Tailor [101] utilize existing importance measurement methods like EWC [41] to calculate the importance of parameters for old tasks, thereby applying penalties.\nFunction Regularization focuses on constraining the intermediate or final outputs of the model, ensuring that the model retains the output features of old tasks while learning new tasks. Function Regularization often uses knowledge distillation strategies, where the previously learned model serves as the teacher, and the model learning the new task"}, {"title": "5.2.2 Replay-based Approach", "content": "The Replay-based Approach is a method that alleviates catastrophic forgetting by preserving and reusing previously learned experiences. In multimodal continual perception learning, depending on the specific content of the replay, the method can be divided into Experience Replay and Generative Replay.\nDue to storage space limitations, Experience Replay focuses on how to store more representative old training samples in a limited memory space. Some studies [104], [105] randomly select multimodal samples for replay. TAM-CL [106] and KDR [107] combine experience replay with"}, {"title": "6 MEMORY DESIGN: WORKING MEMORY", "content": "Working Memory is primarily manifested as the short-term memory of the agent, encompassing elements such as prompts, workspace memory, and context provided by users. Agents utilize these prompts or contextual inputs as their working memory to generate responses or to proceed with planning and actions. Working memory serves as the operational memory of the agent, facilitating real-time interactions and decision-making processes. In short, working memory is the active workspace of the agent, where immediate information is processed and manipulated to produce responses or to guide subsequent actions. We discuss working memory from five main perspectives: prompt compression, long context comprehension, role playing, self correction, and prompt optimization, as illustrated in Figure 9."}, {"title": "6.1 Prompt Compression", "content": "In terms of working memory, agents can include more contextual content within the limited length of prompts by compressing prompt words that users input. This process not only increases the efficiency of information processing, but also allows agents to effectively avoid catastrophic forgetting of historical information when integrating old prompts into new ones. In this way, agents can retain memories of previous experiences while continuously learning new information, achieving the goal of lifelong learning. The"}]}