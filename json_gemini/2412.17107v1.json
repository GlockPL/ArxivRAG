{"title": "Grams: Gradient Descent with Adaptive Momentum Scaling", "authors": ["Yang Cao", "Xiaoyu Li", "Zhao Song"], "abstract": "We introduce Gradient Descent with Adaptive Momentum Scaling (Grams), a novel op-\ntimization algorithm that decouples the direction and magnitude of parameter updates in deep\nlearning. Unlike traditional optimizers that directly integrate momentum into updates, Grams\nseparates the update direction, derived from current gradients, from momentum, which is used\nsolely for adaptive magnitude scaling. This approach enables Grams to achieve improved loss\ndescent compared to state-of-the-art cautious and momentum-based optimizers. We estab-\nlish a global convergence guarantee for Grams and validate its effectiveness through extensive\nempirical evaluations. The results demonstrate Grams' superior performance, including faster\nconvergence and better generalization, compared to widely-used optimizers such as Adam, Lion,\nand their cautious variants. Our results highlight Grams' potential as a transformative approach\nfor efficient optimization in large-scale machine learning.", "sections": [{"title": "1 Introduction", "content": "Optimization plays a pivotal role in modern machine learning, serving as the cornerstone for training\nand fine-tuning models across diverse applications. Over the past decade, the introduction of\nadaptive optimizers like Adam [KB14] and its variant AdamW [LH17] has significantly shaped\nthe landscape of optimization. These algorithms have become the de facto choices for a variety of\ntasks, ranging from pre-training Large Language Models (LLMs) [TLI+23] to fine-tuning models for\ntext-to-image diffusion [RBL+22]. Despite the advent of new methods, AdamW has maintained its\ndominance, particularly in large-scale training regimes, thanks to its robust convergence properties\nand general applicability.\nThe era of LLMs has ushered in unprecedented scaling of model sizes, demanding billions or\neven trillions of parameters [AAA+23]. This scaling places an immense burden on computational\nresources, intensifying the need for efficient optimization strategies. A faster optimizer directly\ntranslates to the ability to process more training tokens within a fixed time budget, leading to the\ndevelopment of more capable models [KMH+20]. This necessity has rekindled interest in identi-\nfying optimizers that can surpass AdamW in terms of speed, memory efficiency, and convergence\nguarantees.\nRecent innovations, such as SHAMPOO [GKS18], Schedule Free [DYM+24], Lion [CLH+24],\nSOAP [VMZ+24], and ADOPT [THM+24], have pushed the boundaries of optimization by intro-\nducing novel update rules, momentum mechanisms, and regularization techniques. These methods\npromise substantial improvements in training efficiency and model performance, particularly in\nspecialized scenarios. The cautious [LCLL24] mechanism addresses optimization challenges by\nadaptively masking the momentum term ut to align with the gradient gt, preventing conflicts that\nhinder training. This approach extends to Adam and Lion, resulting in variants like Cautious\nAdam (C-Adam) and Cautious Lion (C-Lion).\nIn this paper, we propose Gradient Descent with Adaptive Momentum Scaling (Grams), a novel\noptimization algorithm designed to address the limitations of existing methods. Unlike traditional\noptimizers that directly couple momentum with gradient updates, Grams decouples the direction\nand magnitude of parameter updates. This approach allows the update direction to be derived\nsolely from current gradients, while momentum is utilized to scale the update magnitude. Such\ndecoupling enhances stability and robustness, particularly in dynamic optimization landscapes.\nOur contributions are summarized as follows:\n\u2022 We introduce the Grams optimizer, which empirically outperforms existing methods such as"}, {"title": "2 Related Work", "content": "Adam Variants and Memory-Efficient Optimization Adam and its numerous variants have\nbeen pivotal in addressing optimization challenges across diverse applications [KB14, LOG+19].\nAmong these, AdamW [LOG+19] introduced a crucial modification by decoupling weight decay\nfrom gradient updates, restoring the original intent of weight regularization. NAdam [Doz16] inte-\ngrated Nesterov momentum, and AdaBelief [ZTD+20] refined the second moment estimation for im-\nproved generalization. Adan [XZL+24] extended these advancements with an additional momentum"}, {"title": "3 Preliminaries", "content": "In this section, we outline foundational concepts and notations that will be referenced through-\nout the paper. In Section 3.1, we define some useful notations. In Section 3.2 provides essential\ndefinitions in optimization, which are critical for understanding the theoretical guarantees of our\nproposed method. Section 3.3 highlights useful mathematical properties that facilitate the devel-\nopment and analysis of the Grams algorithm. In Section 3.5, 3.6 and 3.7, we review key optimizers,"}, {"title": "3.1 Notations", "content": "For two vectors u,v \u2208 Rd, we use \u27e8u, v\u27e9 to denote the standard inner product in the Euclidean\nspace. We use ||u||2 to denote the l2-norm of u and use ||u||\u221e to denote the l\u221e-norm of u. For\na matrix A, we use ||A||F to denote the Frobenius norm of A. For a twice differentiable function\nf : Rd \u2192 R, we use \u2207 f(x) and \u22072 f(x) to denote the gradient and Hessian of f, respectively. Given\na vector x \u2208 Rd, we use 1x\u22650 \u2208 Rd to denote the vector where each entry indicates whether the\ncorresponding entry of x is non-negative, i.e., for each i \u2208 [d], (1x>0)i = 1 if xi \u2265 0, and (1x>0)i = 0\notherwise."}, {"title": "3.2 Backgrounds on Optimization", "content": "We define the L-smoothness of functions as below.\nDefinition 3.1 (L-smooth). We say that a function f : Rd \u2192 R is L-smooth if ||\u2207f(x1) \u2212\n\u25bdf(x2)||2 \u2264 L||x1 \u2212 x2||2 for all x1,x2 \u2208 Rd.\nWe state a common fact of L-smooth functions as follow.\nFact 3.2. If a function f : Rd \u2192 R is L-smooth, then we have\nf(x2) \u2264 f(x1) + \u27e8\u2207f(x1), x2 \u2212 x1\u27e9 + \\frac{L}{2} ||x2 \u2212 x1||2,\nf(x2) \u2265 f(x1) + \u27e8\u2207f(x1), x2 \u2212 x1\u27e9 \u2212 \\frac{L}{2} ||x2 \u2212 x1||2.\nWe also define PL-condition as below.\nDefinition 3.3 (PL-condition). A function f : Rd \u2192 R satisfies the \u00b5-Polyak-\u0141ojasiewicz (PL)\ncondition with constant \u03bc > 0 if the following inequality holds for all x \u2208 Rd:\n||\u2207f(x)||\u00b2 \u2265 2\u03bc(f(x) \u2013 f\u2217),\nwhere f\u2217 is the minimum value of the function f, i.e., f\u2217 = infz\u2208Rd f(x)."}, {"title": "3.3 Useful Facts", "content": "We introduce some useful facts, which plays crucial roles in the later proofs.\nFact 3.4. Given vectors a, b, c \u2208 Rd, we have\n\u27e8a, b \u25e6 c\u27e9 = \u27e8a \u25e6 b, c\u27e9."}, {"title": "3.5 Adam Optimizer", "content": "Adam (Adaptive Moment Estimation) [KB14] is a widely-used optimizer that combines the benefits\nof RMSprop [HSS12] and momentum by maintaining both first and second moment estimates of\nthe gradients. The algorithm adapts the learning rates for each parameter using these estimates.\nDefinition 3.11 (Adam). The parameter update rule for Adam is given by:\nmt := \u03b21mt\u22121 + (1 \u2212 \u03b21)gt\nvt := \u03b22vt\u22121 + (1 \u2212 \u03b22)g\u0142\n\\hat{m_t} := \\frac{m_t}{1 - \u03b2_1}\n\\hat{v_t} := \\frac{v_t}{1 - \u03b2_2}\n\nut := \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t} + \\epsilon}}\nwt+1 := wt \u2212 \u03b7tut,\nwhere wt is the weight at time step t, mt and vt are the first and second moment estimates re-\nspectively, gt = \u2207wLt(wt\u22121) is the current gradient, \u03b21 and \u03b22 are decay rates for the moment\nestimates, e is a small constant for numerical stability, and \u03b7t is the learning rate at step t."}, {"title": "3.6 Lion Optimizer", "content": "Evolved Sign Momentum (Lion) [CLH+24] is an efficient optimizer that leverages momentum and\nsign-based updates. Lion's key innovation lies in its update rule, which combines both current and\nmomentum gradients through sign operations.\nDefinition 3.12 (Lion Parameter Update). The parameter update rule for Lion is given by:\nut := sign(\u03b21mt\u22121 + (1 \u2212 \u03b21)gt)"}, {"title": "3.7 Cautious Optimizers", "content": "Cautious mechanism [LCLL24] addresses a key challenge in optimization dynamics: when the\nmomentum term ut moves in a different direction from the current gradient gt, it can potentially\nimped training progress. To mitigate this issue, Cautious mechanism introduces an adaptive\nmasking mechanism that modifies the momentum term based on its alignment with the gradient\ndirection. Cautious mechanism could apply to Adam and Lion, which form Cautious Adam (C-\nAdam) and Cautious Lion (C-Lion).\nDefinition 3.13 (Cautious Mechanism Parameter Update). The general parameter update rule for\nCautious mechanism is given by:\n\\hat{u_t} := u_t \\circ 1_{u_t \\circ g_t \\geq 0}\nwt+1 := wt \u2212 \u03b7tut,\nwhere wt is the weight at time step t, \u25e6 denotes Hadamard product. For C-Adam, ut is from\nDefinition 3.11; For C-Lion, ut is from Definition 3.12. gt is the current gradient.\nThe cautious mechanism in Definition 3.13 modifies the parameter updates to ensure they align\nwith the gradient direction, thereby reducing the risk of adverse updates that could impede con-\nvergence. To analyze the impact of this mechanism, we introduce Definition 3.14, which quantifies\nthe change in the loss function after an update."}, {"title": "3.8 Hamiltonian Descent", "content": "Hamiltonian descent provides a theoretical framework for analyzing momentum-based optimization\nalgorithms by introducing an augmented objective function, the Hamiltonian. This framework\nallows us to study optimization dynamics through the lens of continuous-time differential equations,\nlinking the monotonic descent of the Hamiltonian function to the stability and convergence of the\noptimization process. Below, we formalize this concept based on the formulation presented in\nSection 2.1 of [LCLL24].\nDefinition 3.17 (Section 2.1 from [LCLL24]). Momentum-based algorithms can be typically viewed\nas monotonic descending algorithms on an augmented loss H(W, S), which satisfies mins H(W, S) =\nL(W), so that minimizing L(W) is equivalent to minimizing H(W,S). A typical choice is\nH(w, s) = L(w) + K(s),\nwhere K(\u00b7) is any lower bounded function. The continuous-time form of most momentum-based\nalgorithms can be written into a Hamiltonian descent form:\n\\frac{d}{dt} Wt = \u2013 \u2207K(st) \u2013 \u03a6t(\u2207L(wt))\n\\frac{d}{dt} st = \u2207L(wt) \u2013 \u03a8t(\u2207K(st))\nwhere H(W, S) is a Hamiltonian (or Lyapunov) function that satisfies\nmin H(W, S) = L(W),\u2200W,\nS\nso that minimizing L(W) reduces to minimizing H(W, S); and \u03a6(\u00b7), \u03a8(\u00b7) are two monotonic map-\npings satisfying\n\u27e8x, \u03a6(x)\u27e9 \u2265 0,\n\u27e8x, \u03a8(x)\u27e9 \u2265 0,\n\u2200x \u2208 X.\nWith \u03a6(X) = \u03a8(X) = 0, the system in (5) reduces to the standard Hamiltonian system that keeps\nH(Wt, St) = const along the trajectory. When adding the descending components with \u03a6 and \u03a8,\nthe system then keeps H(W, S) monotonically decreasing:\n\\frac{d}{dt} H(Wt, St) = \u2206H(Wt, st) \u2264 0,\nwhere\n\u2206H(wt, st) := \u2212\u27e8x, \u03a6(x)\u27e9 \u2212 \u27e8x, \u03a8(x)\u27e9.\nOn the other hand, L(w), which is the true objective, is not necessarily decreasing monotonically.\n\\frac{d}{dt} L(wt) = -\u2206L(Wt, st),\nwhere\n\u2206L(wt, st) := \u27e8\u2207L(wt), \u2207K(st)\u27e9 + \u27e8\u2207L(wt), \u03a6t(\u2207L(wt))\u27e9."}, {"title": "4 Gradient Descent with Adaptive Momentum Scaling", "content": "We propose Gradient Descent with Adaptive Momentum Scaling (Grams). Grams decouples the\ndirection and magnitude of the update by using the direction from gradients while scaling it with\nthe norm of momentum. This section formalizes the Grams update rule, introduces its key compo-\nnents, and provides theoretical guarantees in both loss descent and Hamiltonian dynamics for its\nperformance."}, {"title": "4.1 Definitions", "content": "We define the parameter updating rule of Grams formally as below.\nDefinition 4.1 (Grams Parameter Update). The parameter update rule for Grams is:\nmt := \u03b21mt\u22121 + (1 \u2212 \u03b21)gt\nvt := \u03b22vt\u22121 + (1 \u2212 \u03b22)g\u0142\n\\hat{m_t} := \\frac{m_t}{1 - \u03b2_1}\n\\hat{v_t} := \\frac{v_t}{1 - \u03b2_2}\n\nut := \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t} + \\epsilon}}\n\\hat{u_t} := sign(gt) \\circ |ut|\nwt := wt\u22121 \u2212 \u03b7t\\hat{u_t},\nwhere wt is the weight at time step t, gt = \u2207wLt(wt\u22121) is the current gradient, | \u00b7 | is element-wise\nabsolute value, \u25e6 denotes Hadamard product, and sign(\u00b7) is defined in Definition 3.8."}, {"title": "4.2 Loss Descent", "content": "In this subsection, we analyze the loss descent properties of the Grams algorithm. Understanding\nhow the loss function decreases over optimization steps provides insights into the efficiency and\nstability of the method. Below, we formalize the relationship between the step size, gradients,\nand the resulting decrease in the loss value, leveraging the L-smoothness property of the objective\nfunction.\nLemma 4.2. Suppose that L : Rd \u2192 R is L-smooth. Let  \u2206LGrams\nwt+1\n wt  be defined in Definition 3.14,\n wGrams\nwt+1  is updated from wt using Eq. (8). Then we have the following:\n\u2022 Part 1. It holds that\n\u2206LGrams\nwt+1\n wt  <-\u03b7t\u27e8|gt|, |ut|\u27e9 +Ln2||ut||2.\n\u2022 Part 2. It holds that\n\u2206LGrams\nwt+1\n wt  \u2265-\u03b7t\u27e8|gt|, |ut|\u27e9.\n\u2022 Part 3. If nt \u2264 Lut2\u27e8gt,ut\u27e9, then we have \u2206LGrams\nwt+1\n wt  \u2264 0."}, {"title": "4.3 Hamiltonian Dynamics", "content": "In this subsection, we present the Grams Hamiltonian dynamics, which builds upon the augmented\nHamiltonian framework to analyze optimization algorithms. By leveraging this framework, we show\nthat the Grams optimizer achieves a monotonic descent of the Hamiltonian and the loss function,\nwith a descent speed that is provably equal to or faster than C-Adam. This highlights Grams'\nefficiency and robustness in dynamic optimization landscapes. The formal definition is provided\nbelow.\nDefinition 4.5 (Grams Hamiltonian Dynamics). We could modify Hamiltonian dynamics,\n\\frac{d}{dt} Wt := sign(\u2207L(wt) \u25e6 |\u2207K(st)| \u2013 \u03a6t(\u2207L(wt))\n\\frac{d}{dt} st := \u2207L(wt) \u2013 \u03a8t(\u2207K(st)),\nwhere | \u00b7 | denotes element-wise absolute value, \u25e6 is the Hadamard product, and \u03a6t, \u03a8t are scaling\nfunctions.\nThe convergence properties of Grams within the Hamiltonian dynamics framework are formal-\nized in the theorem below."}, {"title": "4.4 Global Convergence of Grams", "content": "In this subsection, we establish the global convergence properties of the Grams optimizer. By\nanalyzing the update rules and assumptions on the optimization landscape, we demonstrate that\nGrams converges to a stationary point of the objective function. This analysis underscores the\noptimizer's robustness and effectiveness in a wide range of optimization scenarios."}, {"title": "4.4.1 Assumptions", "content": "To ensure theoretical rigor, we base our analysis on the following standard assumptions commonly\nused in optimization theory. These assumptions define the properties of the loss function and the\noptimization setting, enabling precise derivations of convergence guarantees.\nAssumption 4.10 (Lower bound of loss). The Loss function L : Rd \u2192 R is differentiable and closed\nwithin its open domain dom(L) \u2286 Rd and is bounded from below, i.e., L\u2217 := infw L(w) > \u2212\u221e.\nAssumption 4.11 (Bounded gradient). The Loss function L : Rd \u2192 R satisfies \u2207L(w) \u2264 G for\nall w \u2208 dom(L).\nAssumption 4.12 (L-smooth). The Loss function L : Rd \u2192 R is L-smooth for some L > 0.\nAssumption 4.13 (\u03bc-PL-condition). The Loss function L : Rd \u2192 R satisfies \u00b5-PL-condition for\nsome \u03bc > 0."}, {"title": "4.4.2 Convergence", "content": "In this subsection, we provide a detailed analysis of the convergence properties of the Grams\noptimizer. We begin by revisiting the convergence guarantee of the widely-used Adam optimizer as\nestablished in [LRJ23]. Using this as a foundation, we extend the analysis to Grams, highlighting\nits enhanced convergence behavior under the same assumptions.\nLemma 4.14 (Convergence of Adam, Section 5.3 in [LRJ23]). Suppose that Assumptions 4.10, 4.11,\nand 4.12 hold. Given initial weight w1 with initial optimality gap \u22061 := L(w1) \u2212 L\u2217 < \u221e, choose\nan large enough G such that G > max{e,3\u221aL\u22061}, a small enough fixed step size \u03b7 > 0, and\n\u03b2 = \u0398(\u03b7G1/2). Consider that the weight wt is updated by Adam (Algorithm 1) for each t \u2208 [T].\nThen we have\n1T\u2211Tt=1||\u2207L(Wt)||2\u22648G\u22061\u03b7T\nThe result in Lemma 4.14 establishes a baseline for the convergence of Adam under standard\nassumptions. Building on this, we extend the analysis to Grams by leveraging its unique update\nmechanism, which decouples the direction and magnitude of updates. The following theorem\ndemonstrates that Grams achieves global convergence, meaning that it is guaranteed to reach\nthe optimal objective value from any initial point with finite initial optimality gap."}, {"title": "5 Empirical Experiments", "content": "We conducted comprehensive experiments across both pre-training and fine-tuning stages to eval-\nuate the performance of our proposed Grams optimizer. Comparisons were made against several\nbaseline optimizers, including Adam [KB14], Lion [CLH+24], C-Adam, C-Lion [LCLL24], and, in\nsome experiments, RMSprop [HSS12, Rud16].\nFor Lion and C-Lion, we followed the recommendation from [CLH+24], setting their learning\n1\nrates to 10 \u00d7 Adam learning rate. Additional details and hyperparameters of our experiments can\nbe found in Section A."}, {"title": "5.1 Pre-Training", "content": "We trained from scratch on the Llama 60M model [DJP+24] using the first 2,048,000 rows of\ndata from English subset of the C4 dataset [RSR+20] to assess Grams' optimization capability\nfor Transformer-based [VSP+17] natural language generation (NLG) tasks. Due to the limited\ncomputing resources, we trained 1,000 steps using constant with warm-up scheduler, in order to\nstimulate the beginning part of regular pre-training.We used the first 10,000 rows of validation data\nfrom the English section of the C4 dataset for evaluation."}, {"title": "5.2 Fine-Tuning", "content": "We performed full fine-tuning (FT) experiments on the Llama 3.2 1B model [DJP+24] using the\nMetaMathQA dataset [YJS+23]. To evaluate the model, we measured accuracy on the GSM-8K\ndataset [CKB+21]."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we introduced Gradient Descent with Adaptive Momentum Scaling (Grams), a novel\noptimization algorithm designed to decouple the direction and magnitude of parameter updates.\nBy leveraging this decoupling, Grams demonstrated superior performance in both theoretical con-\nvergence guarantees and empirical evaluations, outperforming state-of-the-art optimizers such as\nAdam [LH17], Lion [CLH+24], and their cautious variants [LCLL24]. The results across various\ntasks highlight Grams' potential as a transformative approach for efficient optimization in large-\nscale machine learning.\nGrams achieved faster convergence and better generalization in our experiments. These prop-\nerties make it particularly well-suited for modern applications such as large-scale pre-training and\nfine-tuning of deep learning models, where efficiency and stability are critical.\nBuilding on the promising results of Grams, future work will focus on integrating ideas from\nrecent advancements such as ADOPT [THM+24] and Schedule Free [DYM+24] optimization meth-\nods. Incorporating the ADOPT and schedule-free learning rate adjustment strategies might improve\nGrams' robustness and performance across diverse tasks and architectures. By blending these com-\nplementary innovations with the core principles of Grams, we aim to develop an even more versatile\nand efficient optimization framework for large-scale machine learning challenges."}, {"title": "A Experiments Details", "content": "For the Lion and C-Lion optimizers, we set the learning rate to 10\u00d7 Adam learning rate, as rec-\nommended in [CLH+24]."}, {"title": "A.1 Pre-Training", "content": "For the pre-training experiments with Llama 3.2 60M [DJP+24], we used the first 2,048,000 rows\nof training data from the English section of the C4 dataset [RSR+20]. We used the first 10,000\nrows of validation data from the English section of the C4 dataset for evaluation."}, {"title": "A.2 Fine-Tuning", "content": "For fine-tuning experiments of the Llama 3.2 1B model, Table 7 provides the detailed hyperparam-\neters.\nFor PEFT of the Llama 3.2 3B model, Table 7 provides the detailed hyperparameters."}]}