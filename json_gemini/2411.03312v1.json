{"title": "INFERENCE OPTIMAL VLMS NEED ONLY ONE VISUAL TOKEN BUT LARGER MODELS", "authors": ["Kevin Y. Li", "Sachin Goyal", "Jo\u00e3o D. Semedo", "J. Zico Kolter"], "abstract": "Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., 5 \u201310\u00d7), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take some initial steps towards building approaches tailored for high token compression settings.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in Large Language Models (LLMs) have enabled Vision Language Models (VLMs) to perceive, reason, and respond through both text and image inputs (Liu et al., 2023; Alayrac et al., 2022; Dai et al., 2023). Many VLMs are built on top of pretrained vision encoders, like CLIP, and pass the patch-based tokens from the visual encoder into the pretrained LLM backbone at a one-to-one ratio for visual context. This results in the LLM processing hundreds of tokens per image, overshadowing those from the user prompt and accounting for most of inference time compute. Consequently, deploying VLMs in real-world applications, particularly on consumer-side edge devices such as monitoring systems, driving assistants, etc., is often limited by the significant inference cost and resulting latency.\nTo reduce the inference cost of VLMs, many recent works have focused on decreasing the number of visual tokens, via a small learnable module, prior to passing image tokens into the LLM while minimizing performance degradation (Li et al., 2024c; Shang et al., 2024). For example, (Li et al., 2024c) learn a cross attention module over the CLIP output tokens to compress the number of tokens. Alternatively, inference FLOPs, proportional to the number of parameters and number of tokens processed, can be reduced by using a smaller LLM. Since both the LLM size and number of visual input tokens directly affect the VLM\u2019s performance, it becomes unclear what the optimal trade-off between the two is. For example, one could process all visual input tokens using a 4B LLM or use an 8B LLM on a reduced set of half the original visual tokens, as both result in similar inference costs currently, the ideal choice is unknown."}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 ESTIMATING INFERENCE COST FOR VLMS", "content": "The language model in VLMs processes the visual input tokens along with the user text query tokens. As language models become larger, the FLOPs (Floating Point Operations) required to process each input token scales accordingly. We follow the standard practice for estimating the inference time FLOPs as (Kaplan et al., 2020; Sardana et al., 2024; Snell et al., 2024):\n$\\text{FLOPS}_{inf} = O(N \\times T)$,\nwhere N denotes the parameter count of LLM and T denotes the total inference time tokens. We ignore the inference cost stemming from the visual encoder, as we use the CLIP-L vision encoder (Radford et al., 2021) with the same input image resolution across all experiments. In addition, many current open-source VLMs utilize the same encoder, incurring a fixed cost across models.\nWe highlight that the inference cost of VLMs scales proportionally with both the parameters and the number of input tokens processed by the LLM.\nIn the context of VLMs, the total inference tokens, T, can be further decomposed as T = Q+V+G, where Q represents the text input tokens, i.e., the question/prompt, V represents the number of visual tokens from the vision encoder (after token compression), and G accounts for the generated tokens. In many real world applications, such as driving assistance systems, the text input remains constant (e.g., \u201cAlert the driver if the scene ahead has a hazard\u201d). In these scenarios, the text input can be cached, effectively making Q = 0 by bypassing self-attention projections and feed-forward calculations. However, in other interactive applications, Q may vary based on dynamic input. We will study the behavior of the downstream error with $\\text{FLOPS}_{inf}$ under both Q = 0 and varying Q regimes. Finally, the generation tokens again are quite small for most inference tasks (single word answers). However, the analysis with increasing Q transfers to increasing Q + G as well."}, {"title": "2.2 \u03a4\u039f\u039a\u0395N COMPRESSION IN VLMS", "content": "As discussed in the previous section, inference FLOPs for VLMs increase proportionally with the number of visual input tokens (e.g., 576 per image with CLIP-ViT-L visual encoder). The number of visual tokens often dominates the total number of tokens processed by the language model, espe-cially in applications where the text input can be cached or is comparatively shorter. Thus, there has been a growing interest in developing approaches to compress the visual information into a fewer number of tokens via a small learnable module. For example, TokenPacker (Li et al., 2024c) learns a small cross-attention module over the image tokens to compress them before passing to the LLM.\nMore formally, let the visual encoder be defined as a function f(I) = X, where X \u2208 Rnxd represents a sequence of n vision embedding tokens produced by the encoder from the input image I. Token compression then learns a vision projector go (X) = Y that maps these embeddings X to Y \u2208 Rm\u00d7d, a compressed sequence of m < n tokens to be processed by the language model (n = m for standard VLMs without any token compression). We refer the reader to Section 5.1 for a detailed discussion on some of the recent token compression algorithms.\nNote that token compression doesn't refer to using a smaller visual encoder or using smaller image resolutions as inputs to the encoder. These approaches usually either do not decrease the visual token count much (beyond around 224) or lead to a large drops in performance (Li et al., 2024a)."}, {"title": "3 TOKENS VS PARAMETERS: INFERENCE TIME SCALING LAWS FOR VLMS", "content": "The deployment of vision language models in real-world applications comes with significant chal-lenges, particularly surrounding inference latency and frames per second (FPS). For instance, in real-time systems, such as automotive driver assistance or hazard monitoring, maintaining high FPS and quick response times is crucial for safe and effective deployment. Consequently, reducing inference FLOPs while minimizing downstream performance degradation is of critical, practical importance, especially on consumer-grade edge devices, which are often severely compute constrained.\nThis has led to a growing interest in visual token compression for VLMs (\u00a7 2.2). Alternatively, one could also use a smaller LLM to reduce inference cost. However, both of the above factors directly"}, {"title": "3.1 SCALING LAW FORMULATION", "content": "Recall that the performance of a VLM is primarily governed by the parameter count of the language model and the number of visual tokens processed by the LLM, assuming a fixed visual encoder. Accordingly, we model the scaling behavior of VLM performance as:\n$Y(N,T) = \\frac{A}{N^{\\alpha}} + \\frac{B}{T^{\\beta}} + D,$\nwhere N denotes the LLM parameters, T denotes the input visual tokens, {A, B, D, \u03b1, \u03b2} are learn-able parameters, and Y (N, T) is a measure of model quality. Although traditional scaling laws have been studied in the context of pretraining loss Kaplan et al. (2020), practitioners often use the direct downstream performance to assess model quality (Gadre et al., 2024; Goyal et al., 2024b; Liu et al., 2022). Thus, we use average downstream error on a suite of nine commonly used visual reasoning benchmarks (\u00a7 3.2) as a measure of model quality Y (N,T).\nBelow, we summarize the role of each of these learnable parameter in the scaling law (Eq. 2).\nLLM Quality Parameter (a): This parameter dictates how the downstream error changes with the complexity of the LLM, i.e., its parameter count. A larger a indicates a better language model, such as Llama3-7B outperforming Llama2-7B, which often stems from better pretraining.\nVisual Token Quality Parameter (\u03b2): \u03b2 captures the quality of the visual input tokens fed into the LLM, reflecting the quality of the compression technique. A more effective token compression algorithm would yield a larger \u1e9e, allowing for more reductions in number of T visual tokens than less effective methods while maintaining the same downstream performance.\nConstants (A, B, D): A and B are normalizing constants and D refers to irreducible loss, which cannot be reduced even with the largest N-sized language model or all T visual tokens (capped at 576 for our choice of vision encoder)."}, {"title": "3.2 EXPERIMENTAL SETUP", "content": "VLM Training and Evaluation: We use the LLaVA-Next framework (Liu et al., 2024b) to train VLMs with the Qwen-1.5 family of language models as the backbone. Specifically, we utilize the Qwen-{0.5, 1.8, 4, 7, 14}B-chat models (Bai et al., 2023). The pretraining and finetuning dataset and hyperparameters follow Liu et al. (2024a), except we double the effective batch size for finetuning. We use CLIP ViT-L/14 (Radford et al., 2021) as the vision encoder for all experiments, and compress the original 576 tokens to {144, 64, 36, 16, 4, 1} tokens using TokenPacker (Li et al., 2024c).\nTo estimate the downstream error Y(N,T), we evaluate on 9 commonly used benchmarks for visual reasoning and understanding: MME (Fu et al., 2024), GQA (Hudson & Manning, 2019), AI2D (Kembhavi et al., 2016), MMBench (Liu et al., 2024c), MMMU (Yue et al., 2023), Sci-enceQA (Lu et al., 2022), MathVista (Lu et al., 2024), POPE (Li et al., 2023c), and ChartQA (Masry et al., 2022). We compute Y (N, T) by averaging the errors of the normalized evaluation metric. For MME, the Cognition and Perception scores were combined and the F1 scores were used for POPE.\nFitting Scaling Laws: We fit the proposed scaling law (Eq. 2) on {Y(N,T), N, T} pairs, with N\u2208 {0.5B, 1.8B, 4B, 7B} and T \u2208 {1, 4, 16, 36, 64, 144, 576} (described in the experiment setup above). We use grid-search, for its stability (Goyal et al., 2024b), to estimate the scaling parameters \u03b1, \u03b2, \u0391, \u0392, and D. The final scaling law is evaluated on a N = 14B VLM model at various number of visual tokens in T. Further details about the grid-search fit can be found in Appendix A.2."}, {"title": "3.3 RESULTS: ESTIMATED SCALING CURVES", "content": "Recall from Section 2.1 that $\\text{FLOPS}_{inf} = O(N(Q+V))$, where Q represents the input text tokens, and V is the visual input tokens. We first visualize our scaling laws under 2 settings \u2014 (a) cached text input (Fig.1a): The input text tokens (Q) are fixed and can be cached, leading to $\\text{FLOPS}_{inf}$ ~ O(NV), and (b) non-cached text input (Fig.1b): The input text tokens are approximated as 50, i.e., $\\text{FLOPS}_{inf} = O(N(50 + V))$ (we consider more granular variation of Q in \u00a7 3.3.2).\nFigure 1 visualizes the fitted scaling curve, illustrating the variation in the average downstream error as inference FLOPs are varied (under both the cached and non-cached text input setting). We vary the inference FLOPs on the x-axis by increasing the number of visual input tokens processed by the LLM (the scatter size), while the color scale indicates the varying number of language model parameters. We make some key observations below.\nLog-Linear Relation between Error and Number of Visual Input Tokens: Consider the change in performance for the 7B model as the number of visual input tokens varies (maroon curves in Fig. 1.) Recent works on visual token compression (Li et al., 2024c; Shang et al., 2024) claim little to no performance degradation with token compression. For example, they report similar per-formance to the base model's 576 tokens even when visual token count is reduced to 36 or 144 on certain tasks. However, our scaling curves in Figure la reveal a different trend, showing a log-linear decrease in visual reasoning performance as the number of visual input tokens is reduced. We be-lieve this discrepancy arises because of the limited downstream evaluation benchmarks considered in the previous works which may not fully capture the VLM's overall capabilities.\nError Varies 5\u00d7 Faster with LLM Parameters than with Tokens: Recall from the scaling law (Eq. 2) that a represents the LLM quality parameter and \u1e9e represents the visual token quality parameter, both denoting the rate at which they influence the downstream error respectively. From Figure 1a, we observe that a = 0.077 is more than five times larger than \u03b2 = 0.015, signifying that VLM error increases significantly faster when reducing the LLM parameters compared to reducing the number of visual tokens. Therefore, when minimizing inference FLOPs, it is more effective to prioritize reducing visual tokens (V) first, as the impact on performance is less pronounced than reducing the LLM parameters (N). This finding is reflected in Figure 4 where we observe that, under fixed inference compute, using a larger LLM with fewer visual tokens (7B LM w/ 36 tokens) provides better performance than using a smaller LLM with more visual input tokens (1.8B LM w/ 144 tokens) for visual reasoning tasks."}, {"title": "3.3.1 COMPUTE-OPTIMAL INFERENCE REQUIRES A SINGLE VISUAL TOKEN", "content": "Observe the pareto optimal curve (black dotted curve) in Figure la. For cached query, at any given inference compute, the optimal behavior, i.e., lowest downstream error, occurs when using the largest possible LLM while reducing the number of visual input tokens to one. Thus, for sce-narios where the text input can be cached (Q = 0), such as monitoring systems with static text"}, {"title": "3.3.2 VARIATION IN OPTIMAL TOKENS WITH TEXT QUERY LENGTH", "content": "The shift in performance trends and the ideal visual token count from Q = 0 \u2192 50 raises the question; how does the input text length impact the optimal selection of LLM size and number of visual tokens? To explore the variations in trends, we consider the effect of text input length on the optimal inference behavior in Figure 3a. First, when the text input length, Q, is small (purple curves), it is always better to use the larger model (solid curve) with less visual tokens compared to the smaller model (dashed curve) with more visual tokens. However, consider an edge case where the text input length is extremely high (e.g., 100 for the green curves). We observe that there is a sharp increase in error as inference FLOPs are reduced. This is because visual tokens need to be reduced significantly for any effective change in inference FLOPs, as the fixed cost from text tokens is quite high. At a certain point (marked by the red dot in Figure 3a), it becomes more advantageous to use the 4B model with a higher number of visual tokens rather than the 7B model with fewer tokens (contrary to the case for lower Q). Thus, the optimal number of visual input tokens rises with an increase in Q. This case demonstrates the need for careful balancing of visual token"}, {"title": "3.3.3 SCALING LAWS FOR OCR TASKS", "content": "Until now, we have focused on scaling behavior for visual reasoning and understanding tasks, high-lighting the key finding that using a single visual token with the maximum possible LLM parameters is the inference-optimal configuration. However, is the same valid for all tasks? VLMs have recently been applied to document reading and OCR-style tasks where a single visual token may be insuffi-cient due to the high density of information. Unlike visual reasoning tasks, these tasks lack visual structure in the image and intuitively need more tokens to record the (generally textual) details in the image. We verify the same by fitting our scaling laws (Eq. 2) on DocVQA (Mathew et al., 2021) and TextVQA (Singh et al., 2019) benchmarks, where the tasks require mainly OCR capabilities.\nFigure 3b presents the fitted scaling law for OCR tasks. Notably, there are no significant gains in average downstream performance from increasing LLM parameters; instead, the number of visual tokens predominantly dictates the performance. This observation is reflected in the scaling law parameters, where the LLM-quality parameter a = 0.029 is nearly twice as smaller than the token quality parameter \u03b2 = 0.048. This trend is in stark contrast to the scaling parameters observed for visual reasoning tasks where the LLM-quality parameter (a) was more than five times larger than"}, {"title": "4 QUERY-BASED TOKEN COMPRESSION", "content": "The Need for Token Compression in Extreme Regimes: While prior work has primarily focused on moderately compressing the tokens (e.g., reducing 576 tokens to 144) while trying to match the performance of the base model (no token compression), our findings (\u00a7 3.3.1) suggest the need for a paradigm shift. Rather than aiming for moderate token compression, new approaches should be tailored for extreme token reduction down to 1, 4, or 16 tokens with minimal possible degradation, as our scaling laws demonstrate that compute-optimal behavior is within this range.\nOur work takes initial steps in this direction by introducing a query-based token compression strat-egy designed for such high-compression regimes. In cases where tokens are reduced to as few as 1, token compression based on the user's input query becomes critical for retaining relevant informa-tion and minimizing performance reductions. In the following section, we build on existing algo-rithms (Li et al., 2024c), to incorporate query-based token compression. Figure 5 summarizes our query-based convolutional cross-attention (QueCC, pronounced \"quick\") compression technique.\nUser Query Information Injection: To make our projector query-dependent, we add the text embedding of the user's most recent query to the image embeddings from the vision encoder. We do this by taking the last hidden states prior to the LM head of the user input from the language model as the representation of the user's overall query. The hidden state is converted into the text embedding via a linear projection and added to the image visual token embeddings. These fused tokens are later used as the query component for cross-attention. The text embedding can easily be cached for applications where the query is static or is part of a predetermined set. Even if the query varies, the text-embedding can be precalculated prior to processing the image and KV values cached and reused when processing the visual and text tokens together during generation.\nToken Downsampling with Cross-Attention and Learnable Convolutions: To compress the number of visual tokens passed into the LLM, we utilize a region-based, cross-attention mechanism that downsamples the vision encoder tokens, X, into a more information-dense form. The mecha-nism hinges on viewing X as a \u221an \u00d7 \u221an grid due to the patchification of the image by the vision encoder. Li et al. (2024c;d) passes the \u201c2D\u201d version of X through a downsampling function that compresses the input by a s\u00b2 factor where each resulting token corresponds to a s \u00d7 s region in the original input. After this, cross-attention is applied independently between each downsampled token and the corresponding tokens in its s \u00d7 s region. We improve on the bilinear interpolation-based downsampling techniques (Li et al., 2024c; Wang et al., 2024b) by using a learnable, depth-wise 2D convolution filter of kernel size and stride s, providing better expressivity."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "We use a training setup similar to LLaVa-1.5 (Liu et al., 2024a) and use Vicuna-1.5 7B as the LLM. Based on the optimality of high token compression underscored by our scaling laws (\u00a7 3.3), we focus on visual token budgets of {1, 4, 16, 36, 64}, corresponding to compression rates of 88.9% to 99.8%. We benchmark our method on a diverse, comprehensive set of visual reasoning/understanding and OCR/text-understanding tasks: GQA (Hudson & Manning, 2019), MMBench (MMB) (Liu et al., 2024c), MME (Fu et al., 2024), POPE (Li et al., 2023c), ScienceQA (SQA) (Lu et al., 2022), TextVQA (Singh et al., 2019) VizWiz (Gurari et al., 2018), and VQAv2 (Goyal et al., 2017)."}, {"title": "4.2 QUERY-BASED CONVOLUTIONAL CROSS-ATTENTION (QUECC) RESULTS", "content": "Table 1 presents the results of our QueCC algorithm in comparison to previous methods, includ-ing TokenPacker (Li et al., 2024c), LLaVa-PruMerge (Shang et al., 2024), Matryoshka Multimodal Models (Cai et al., 2024), and Matryoshka Query Transformer (Hu et al., 2024), in low token regimes. We find that our method performs better than alternatives at the highest compression levels in multiple different data sets, leading to a 12% and 19% improvement in the gap between the origi-nal LLaVA-1.5 model and the next-best method on MME and MMB for the one-visual-token level. The trend continues at the four-token level, where the gap between QueCC and the next-best algo-rithm was reduced by 26% and 21% on MME and MMB. Our model exhibits strong performance on GQA, MME, SQA, and VQAv2 across compression rates, signaling the prospects of using the user's query to identify and compress key visual tokens."}, {"title": "5 RELATED WORK", "content": ""}, {"title": "5.1 TOKEN REDUCTION IN VISION-LANGUAGE MODELS (VLMS)", "content": "VLMs are composed of three key components: (a) a visual encoder that encodes the input images, (b) a large language model (LLM) that processes the visual tokens from the encoder along with the user text query, and (c) a projector that maps the visual tokens to the input embedding space of the LLM. Section A.1 contains additional details exploring various projector designs. Often, the number of visual tokens (576 tokens per image for CLIP-ViT-L, for instance) significantly exceeds the number of text tokens, leading to high inference costs. This disproportionate scaling of visual tokens also"}, {"title": "5.2 SCALING LAWS AND SCALING INFERENCE COMPUTE", "content": "Understanding how the performance of modern deep networks improves as key design factors, such as the number of parameters or training tokens, are scaled has become a focal point of research, particularly as these models continue to grow in size and complexity. Scaling laws offer crucial guidance for optimizing the architecture of such models. Notably, Kaplan et al. (2020); Hernandez et al. (2021); Hoffmann et al. (2022) do a thorough investigation into training compute-optimal language models, highlighting the need to scale pretraining tokens and parameters at the same rate. Cherti et al. (2023); Gadre et al. (2023) perform a similar study on scaling laws for CLIP (Radford et al., 2021), corroborating that performance improvements arise from increasing both parameter counts and pretraining image-caption pairs.\nClosest to our work, Li et al. (2024a) investigate what factors improve the performance of LLaVA (Liu et al., 2023). They observe performance gains with increasing language model size, visual encoder size, and input resolution. They investigate each of these factors when scaled in-dependently. In contrast, in this work we focus on understanding the optimal trade-off between language model size and the number of visual input tokens, given a fixed inference budget to fit in. Note that in our work, visual input token count is varied (decreased) using token compression algorithms (\u00a7 5.1) and not by varying the input image resolution or using a different CLIP model.\nWhile scaling the pretraining of LLMs has led to emergent capabilities, there has recently been a growing interest in improving their reasoning capabilities by scaling inference time compute. Brown et al. (2024) show impressive performance boosts if the language model is allowed mul-tiple attempts on a problem. In fact, Snell et al. (2024) show that scaling test time compute by parallel multiple generations at inference gives performance comparable to a 14\u00d7 larger model on math tasks. Goyal et al. (2024a) show performance gains by appending special tokens at the end of input to scale test time compute. In contrast, we characterize the optimal trade-off between tokens and parameters, for getting the best performance at a given fixed test time (inference) compute."}, {"title": "6 DISCUSSION AND CONCLUSION", "content": "In our work, we demonstrate that the optimal trade-off for VLMs inference is to use very few visual input tokens along with the largest possible LLM that fits within the budget. This result has quite important consequences. Existing works aim towards moderate reduction in token count (e.g., from 576 to 144), while trying to match the performance of the base model (no token reduction). However, our results show that the community needs to focus towards extreme token reduction (e.g., down to 1, 4 or 16 tokens), as the inference optimal regime requires very few visual input tokens. Note that although extreme token reduction can lead to a drop in performance compared to the base model, it is still better than using more tokens with a smaller LLM. The performance with very few visual tokens is poised to only improve further as we develop token reduction algorithms tailored for extreme reduction. Our work takes an initial step in this direction by proposing input query-based token reduction, as it is better to prioritize visual tokens with information relevant to the text input query, under such an extreme token compression. While our findings are focused on visual token compression at the projector level prior to passing into the LLM, we leave the compute-optimal scaling properties of adaptive token processing algorithms that operate within the LLM component for subsequent work. We hope that these critical insights from our paper will guide future research towards developing better token reduction techniques and thus inference optimal VLMs."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 ADDITIONAL RELATED WORKS", "content": ""}, {"title": "A.1.1 VISION PROJECTOR DESIGN", "content": "To bridge the gap between the separate image and text modalities presented by the vision encoder and language model respectively, vision projectors map the image tokens from the vision encoder into the language space. Many design choices for the projector exist. Numerous VLMs utilize query-based projectors, which combine the embeddings of visual tokens with that of query tokens via cross-attention or similar mechanisms, like the Q-Former projector introduced BLIP-2 (Li et al., 2023a) and used in following work (Dai et al., 2023; Zhu et al., 2023). Other VLMs use simple linear projectors or MLPs to connect the encoder and LLM (Liu et al., 2023; 2024a; Su et al., 2023). While most architectures use the projectors to create new tokens to feed into the LLM alongside text, some architectures like Flamingo (Alayrac et al., 2022) or CogVLM (Wang et al., 2024a) directly interweave the visual information into the language model. In our work, we will be focusing on projectors that fall in the former category."}, {"title": "A.1.2 ADDITIONAL APPROACHES FOR EFFICIENT VLMS", "content": "Apart from reducing the number of visual input tokens to the language model, people have explored various other techniques, including a mix of quantization (Liu et al., 2024a) and smaller encoders or language models (Yao et al., 2024; Chu et al., 2023; Zhou et al., 2024) for improving inference.\nVLMs utilized in video processing often combine decreases in vision encoder output size with token compression techniques to prevent excessive latency and memory constraints. Visual tokens are often merged temporally across frames (Xu et al., 2024; Shen et al., 2024) as well as spatially for individual frames (Xu et al., 2024). Vision encoders, such as Q-Former (Li et al., 2023a), are preferred over more traditional CLIP models due to their ability to extract a smaller fixed number of tokens per image (Weng et al., 2024; Li et al., 2024b). Although compression techniques used for video processing often can reduce token counts by large margins, they are rarely evaluated on image datasets, and when they are, compress visual tokens very little or not at all (Li et al., 2023b)."}, {"title": "A.2 GRID SEARCH DETAILS", "content": "While there are many choices of optimizer for fitting the scaling laws like curve-fitting in SciPy, gradient descent based solvers, etc. We observed that these are not stable and give varying so-lutions. We converged to using grid-search to fit the scaling laws, similar to the recent works like Goyal et al. (2024b). The grid-search range for each of the parameters were as follows: \u03b1, \u03b2\u2208 {0, 0.1}, A, B, D \u2208 {0, 1}."}, {"title": "A.3 ADDITIONAL RESULTS", "content": ""}]}