{"title": "AnyBimanual: Transferring Unimanual Policy for General Bimanual Manipulation", "authors": ["Guanxing Lu", "Tengbo Yu", "Haoyuan Deng", "Season Si Chen", "Yansong Tang", "Ziwei Wang"], "abstract": "Performing general language-conditioned bimanual manipulation tasks is of great importance for many applications ranging from household service to industrial assembly. However, collecting bimanual manipulation data is expensive due to the high-dimensional action space, which poses challenges for conventional methods to handle general bimanual manipulation tasks. In contrast, unimanual policy has recently demonstrated impressive generalizability across a wide range of tasks because of scaled model parameters and training data, which can provide sharable manipulation knowledge for bimanual systems. To this end, we propose a plug-and-play method named AnyBimanual, which transfers pre-trained unimanual policy to general bimanual manipulation policy with few bimanual demonstrations. Specifically, we first introduce a skill manager to dynamically schedule the skill representations discovered from pre-trained unimanual policy for bimanual manipulation tasks, which linearly combines skill primitives with task-oriented compensation to represent the bimanual manipulation instruction. To mitigate the observation discrepancy between unimanual and bimanual systems, we present a visual aligner to generate soft masks for visual embedding of the workspace, which aims to align visual input of unimanual policy model for each arm with those during pre-training stage. AnyBimanual shows superiority on 12 simulated tasks from RLBench2 with a sizable 12.67% improvement in success rate over previous methods. Experiments on 9 real-world tasks further verify its practicality with an average success rate of 84.62%.", "sections": [{"title": "1. Introduction", "content": "Bimanual systems play an important role in robotic manipulation due to the high capacity of completing diverse tasks in household service [72], robotic surgery [33], and component assembly in factories [9]. Compared to unimanual systems, bimanual systems expand the workspace and are able to handle more complex manipulation tasks by stabilizing the target with one arm and interacting with that using another arm [30, 45]. Even for tasks that unimanual policies can handle, bimanual systems are often more efficient because multiple action steps can be simultaneously accomplished [31]. Since modern robotic applications require the robot to interact with different tasks and objects, it is desirable to design a generalizable policy model for bimanual manipulation.\nTo enhance the generalization ability of the manipulation agent, prior works present to leverage the high-level reasoning and semantic understanding capabilities of foundation models like Large Language Models (LLMs) and Vision Language Models (VLMs) to breakdown tasks into executable sub-tasks that can be solved by external low-level controllers [26, 27, 34, 37, 44], which thus struggles with contact-rich tasks that require complex and precise low-level motion. To generalize to contact-rich tasks, recent methods [1, 41, 51] tend to learn robotic foundation models directly from large-scale teleoperation data [17, 40, 55], which has shown impressive generalizability across various unimanual tasks. However, bimanual demonstrations are extremely expensive to acquire in real-world, which often require specialized teleoperation systems with additional sensors and fine-grained calibration with high human labor cost [12, 18, 22, 56, 62, 73]. To address this challenge, recent methods aim to simplify the learning budget by exploiting human inductive bias such as parameterized atomic movements that detail the position and rotation [5, 15] or assigning stabilizing and acting roles for each arm [24, 45, 54], thereby reducing the need for extensive expert data. Nevertheless, shareable atomic movements and fixed cooperation patterns struggle to generalize across different bimanual manipulation tasks, which limits the deployment scenario of these classes of methods.\nIn this paper, we propose a plug-and-play module named AnyBimanual that transfers any pre-trained unimanual policy to bimanual manipulation policy with limited demonstrations. Since unimanual policy model [39, 50] has demonstrated impressive generalization ability across tasks due to the large model sizes and numerous training demonstrations, we realize high generalizability across diverse language-conditioned bimanual manipulation tasks by mining and transferring the commonsense knowledge in pre-trained unimanual policies. More specifically, we first introduce a skill manager that dynamically schedules discovered skill representations. Skill representations are formed by skill primitives that store shareable manipulation knowledge across embodiments, with task-oriented importance weights and compensation. To enhance the transferability of the pre-trained unimanual policy in bimanual manipulation tasks, the observation discrepancy between unimanual and bimanual systems should be minimized. We propose a voxel aligner to generate spatial soft masks to highlight relevant visual clues for different arms, whose goal is to align the visual input of the unimanual policy model for each arm with those during the pretraining stage. We evaluate AnyBimanual on a comprehensive task suite composed of 12 simulated tasks from RLBench2 [31] and 9 real-world tasks, where our method surpasses the previous state-of-the-art method by a large margin. The contributions are summarized as follows:\n\u2022 We propose a model-agnostic plug-and-play framework named AnyBimanual that transfers an arbitrary pre-trained unimanual policy to generalizable bimanual manipulation policy with limited bimanual demonstrations.\n\u2022 We introduce a skill manager to dynamically schedule skill representations for unimanual policy transferring and a visual aligner to mitigate the observation discrepancy between unimanual and bimanual systems for transferability enhancement.\n\u2022 We conduct extensive experiments of 12 tasks from RL-Bench2 and 9 tasks from the real world. The results demonstrate that our method achieves a higher success rate than the state-of-the-art methods."}, {"title": "2. Related Work", "content": "Generalizable Bimanual Manipulation. Bimanual manipulation agents [6, 11, 22, 24, 27, 31, 38, 45, 68, 71] are able to handle a large variety of tasks by predicting a trajectory of bimanual operation, which is of great significance in complex applications from household service [72], robotic surgery [33], to component assembly in factories [9]. To achieve multi-task learning for generalizable Bimanual manipulation, earlier studies attempted to leverage the emerged general understanding and reasoning capacities of pre-trained foundation models like LLMs [53] and VLMs [10], where the foundation model was prompted to generate a high-level plan for low-level executors. However, the performance of directly leveraging foundation models in a training-free manner is bottlenecked by the predefined low-level executor, which struggles to generalize to more contact-rich tasks like straightening a rope. To overcome this challenge, robotic foundation models [7, 8, 17, 41, 46, 51] that pre-trained on large-scale real-world demonstrations were proposed under the unimanual setting, which has shown high generalizability across everyday manipulation tasks. However, bimanual tasks demand precise coordination of two high degree-of-freedom arms, making the teleoperation of demonstrations for training generalizable policies also costly. Although some recent approaches [13, 16, 18, 22, 65, 73] have developed more specialized teleoperation systems to reduce these costs, scaling up demonstrations for high generalization ability remains a challenge. To address the limited availability of demonstrations, alternative methods [30, 45, 60] proposed to simplify the learning of bimanual policies by decoupling the bimanual system into a stabilizing arm and an acting arm. Nevertheless, these methods often rely on predefined roles for each arm, which precludes their applicability to tasks requiring more flexible collaboration patterns. In contrast to these approaches, our work presents a novel method that transfers generalizable unimanual policies to bimanual tasks, which eliminates the necessity for explicit inductive bias like role specification.\nSkill-based Methods. Skill learning [70] is the process where intelligent agents acquire new abilities that are transferable across different tasks, which is of great significance for cross-task generalization. Thus, skill learning is being attractive in enhancing the generalizability of different models, such as game agents [57], robotic manipulation [43], and autonomous driving [20]. The initial attempt to utilize skill learning was orchestrating a set of predefined skill [48], which hindered their scalability to unseen tasks. To overcome this limitation, [15, 43] proposed to learn shareable skill primitives from data. For example, skill diffuser [43] introduced a hierarchical planning framework that integrates learnable skill embedding into conditional trajectory generation, which realized accurate execution of diverse compositional tasks. In the field of bimanual manipulation, skill learning was dominated by handcrafted primitives. For example, [2-4, 14, 19, 21, 23, 29, 32, 64, 66] propose to utilize parameterized atomic movements to shrink the high dimensionality of the bimanual action space, which has shown impressive performance on templated bimanual manipulation tasks. While the predefined atomic movements did boost the success rate on specific tasks, they are often difficult for even human users to specify, which restricts the deployment scenarios of these methods. In this paper, we propose leveraging learnable skill primitives to represent the learned commonsense of the pre-trained unimanual policy, so that the knowledge can be transferred across different levels of tasks."}, {"title": "3. Approach", "content": "In this section, we first introduce required preliminaries (Section 3.1), and then we present a pipeline overview (Section 3.2). Then, we introduce a skill manager (Section 3.3) that schedules skills to each arm to form general bimanual manipulation policies. We build a visual aligner (Section 3.4) to mitigate the observation discrepancy between bimanual and unimanual systems for better generalization. Finally, we outline the training objectives (Section 3.5)."}, {"title": "3.1. Problem Formulation", "content": "The task of policy learning for bimanual manipulation can be defined as follows. To complete a wide range of manipulation tasks specified in natural language, the bimanual agent is required to interactively predict the actions of both end-effectors based on the visual observation and robot states, where the motion is acquired by a low-level planner (e.g., RRT-Connect). The observation $o_t$ at the tth time step includes the voxel $v_t$ converted from RGB and depth images [31, 50] and the robot proprioception $p_t$. The action $a_t$ for each end-effector at the tth time step contains the location $\\alpha_{trans}$, orientation $\\alpha_{rot}$, gripper open state $d_{open}$ and usage of collision avoidance in the motion planner $a_{col}$. For the training data, human demonstrators produce a limited set of M offline expert trajectories $D = \\{(o_1, a^{left}_1, a^{right}_1), ..., (o_M, a^{left}_M, a^{right}_M)\\}$ for each task instruction $l$, where $a^{arm}$ demonstrates the actions for left and right grippers. Existing methods directly learn the policy model from expert demonstrations, which have shown effectiveness in single-task settings. However, due to the high cost of data collection in bimanual systems, the scarcity of expert demonstrations limits the generalizability of these methods across tasks. To address this, we present to transfer pre-trained generalizable unimanual policy for general bimanual manipulation."}, {"title": "3.2. Overall Pipeline", "content": "Overall pipeline of our AnyBimanual method is shown in Figure 2. For the language branch, we employed a pre-trained text encoder [49] to parse the bimanual instruction to language embeddings with high-level semantics, where the skill manager schedules the skill primitives with composition and compensation to enhance the language embeddings that instruct relevant subtasks for different arms. Therefore, the pre-trained unimanual policy model can be prompted to generate feasible manipulation policy for each arm with high generalization ability across tasks with the sharable manipulation knowledge. For the visual branch, we voxelize the RGB-D input to the voxel space as observation, and a 3D sparse voxel encoder is utilized to tokenize the voxel observation for informative volumetric representation. The visual aligner generates a soft spatial mask to align the visual representation of unimanual policy model with its representation during pretraining, so that the observation discrepancy between unimanual and bimanual systems can be minimized for policy transferability enhancement. We employ two pre-trained unimanual models to predict the left and right robot actions based on the text embeddings and visual representations, where the pre-trained unimanual policy can be multi-modal transformer-based policy [7, 8, 17, 41, 50] or diffusion-based policy [39, 51]."}, {"title": "3.3. Scheduling Unimanual Skill Primitives", "content": "In order to transfer unimanual manipulation policy to bimanual settings without generalizability drops, we propose a skill manager to decompose the action policies from unimanual foundation models into skill primitives and integrate primitives for bimanual systems. However, the given offline expert demonstrations $D$ do not contain any explicit intermediate skill primitives or sub-task boundaries, but only low-level end-effector poses and high-level natural language instruction are provided. Therefore, we design an automatic skill discovery method in an unsupervised manner to learn skill representations and their schema from offline bimanual manipulation datasets during training. In the test phase, the skill manager predicts different weighted combinations of primitive skills to orchestrate each arm given high-level language instruction, which enables effective transfer of pre-trained unimanual policy to diverse bimanual manipulation tasks.\nSkill Manager. We start with a discrete primitive skill set $Z = \\{Z_1, Z_2, ..., Z_K\\}$, where K is a hyper-parameter that indicates the number of skill primitives. To realize end-to-end skill discovery and scheduling, each potential skill is an implicit embedding $z_k \\in R^D$, which can be initialized with the corresponding language template tokens of the pre-trained unimanual policy to mitigate the domain gap. By combining the primitives from the skill set, the language embedding for the unimanual policy model can be represented as a linear combination of these primitives. Hence, the reconstructed language embedding as skill representation can be expressed as:\n$\\begin{aligned}f_{t}^{left} & = \\sum_{k=1}^{K} \\omega_{k t}^{left} Z_{k}+E_{t}^{left} \\\\f_{t}^{right} & = \\sum_{k=1}^{K} \\omega_{k t}^{right} Z_{k}+E_{t}^{right} \\end{aligned}$                                                         (1)\nwhere $f_t^{arm}$ is the decomposed unimanual language embedding for one arm of the bimanual system, and $\\omega_{kt}^{arm} \\in R^K$ denotes the importance weight for linear combination for both values of $arm \\in A$. We also introduce a task-oriented compensation $E_t^{arm} \\in R^D$ to introduce the embodiment-specific knowledge for the policy transfer. The upscript arm of the variables can be selected from left or right to indicate the embodiment in the bimanual systems. Considering a bimanual task Handover, it can be explicitly solved by scheduling two unimanual primitive skills, i.e., the left arm Place the block to the right gripper while the right arm Pick it from the left gripper.\nThough every language embedding that passed through the pre-trained single policy can be represented as a linear combination of the skill set, the combination weights that specify the importance of each skill are dynamic across the task completion process. We propose to parametrize a multi-model transformer named skill manager to dynamically predict the combination weight for each arm at each time step. Therefore, our skill manager $f_\\theta$ can be formulated as $(f_{t}^{left}, E_t^{left}, \\omega^{right}_{kt}, E_t^{right}) = f_\\theta(v_t,l,p_t)$, which takes the overall bimanual visual and language embeddings, proprioception as input, and assigns the reconstructed unimanual language embedding for each arm as output to schedule the skill primitive of each arm dynamically. $\\theta$ represents the learnable parameters. Finally, the combined skill primitives are concatenated with the initial bimanual language embedding to enhance the global context, which is then forwarded to the corresponding unimanual policy.\nLearning Generalizable Skill Representations. To update the skill library, we expect the discorverd skill representations are informative to encode fundamental robot motions that are sharable across a variety of tasks, thereby enhancing the generalizability of our framework. To realize this, the learning objective of the skill manager is designed as a sparse representation problem [61]:\n$L_{skill} = ||\\hat{\\omega}^{left}||_1 + ||\\hat{\\omega}^{right}||_1 + \\lambda_e(||e^{left}||_{2,1} + ||e^{right}||_{2,1})$ (2)\nwhere $|| \\cdot ||_1$, and $|| \\cdot ||_{2,1}$ denote the $l_1$ and $l_{2,1}$ norm, respectively. $\\lambda_e$ is a hyper-parameter that balances the denoising term. By minimizing this sparse regularization term, the skill manager is encouraged to reconstruct the skill representation by selecting fewer skill primitives, which is further surrogated by minimizing a differentiable $l_1$ regularization [52]. Therefore, the skill subspaces are required to be orthogonal and disjoint with each other to reconstruct the language embedding with sparse combination and compensation, which implicitly enforces each skill representation to capture an independent fundamental motion."}, {"title": "3.4. Aligning Unimanual Visual Representations", "content": "Visual Aligner. Despite the skill manager enabling generalization in the language modality, the distributional shift from the unimanual to bimanual workspace in terms of the visual context still may harm the model performance. To mitigate the observation discrepancy, we present a visual aligner $q_\\phi$ that predict two spatial soft masks at each step t to edit the voxel space so that the decomposed subspace of unimanual policy model for each arm aligns with those during pretraining stage: $(\\hat{l}_t^{left}, \\hat{l}_t^{right}) = q_\\phi(v_t, l, p_t)$. The decomposed observation represents the locality of the workspace, which is then augmented by the bimanual observation to form the final visual embedding:\n$v^{left} = (\\hat{l}_t^{left} \\odot v_t) \\oplus v_t, v^{right} = (\\hat{l}_t^{right} \\odot v_t) \\oplus v_t,$ (3)\nwhere $\\odot$ is the element-wise multiplication, and $\\oplus$ refers to the concatenation operator. As a result, the augmented visual representations for each arm contains both embodiment-specific information and the global context, which is then passed through the two unimanual policy models to decode the optimal bimanual action.\nLearning Aligned Visual Representations. Our goal is to mitigate the visual domain gap between the unimanual and bimanual setting, so that the pre-trained commonsense knowledge in unimanual policy can be transferred with high adaptation ability. Since we can not access the unimanual pretraining data in common usages, we instead impose a mutually exclusive prior to the visual aligner. This prior is regularized by optimizing a Jensen-Shannon (JS) divergence regularization term:\n$L_{voxel} = -D_{KL}(\\hat{l}_t^{left} || \\hat{l}_t^{right}) / 2 - D_{KL}(\\hat{l}_t^{right} || \\hat{l}_t^{left}) / 2$   (4)\nwhere $D_{KL}$ means the Kullback-Leibler (KL) divergence operator. To provide further explanation, bimanual manipulation tasks often involve asynchronous collaboration that requires the left and right arm to attend to different areas of the whole workspace to act as different roles, such as stabilizing and acting. As a result, the mutually exclusive division of the entire bimanual workspace will naturally separate one arm and its target from the other, which highly resembles the unimanual configuration. Hence by maximizing the divergence between the two soft masks, the voxel input of the bimanual manipulation agent can be disentangled into unimanual visual representations that align with those in the pretraining phase effectively."}, {"title": "3.5. Learning Objectives", "content": "The decomposed skill and volumetric representation are used to pass through the two pre-trained unimanual policies to predict the optimal actions of the two end-effectors. We assume access to a pre-trained unimanual policy $p$, which is fundamentally a multi-model multi-task neural network that takes visual and language embedding as inputs and outputs end-effector actions. Our AnyBimanual is a model-agnostic plug-and-play method, which indicates that the architecture of pre-trained unimanual policy $p$ is flexible in different architectures such as multi-modal transformer-based policies [41, 50] and diffusion policies [39, 51]. To supervise the model with the provided expert demonstrations for behavior cloning, we leverage the cross-entropy loss to learn accurate action prediction for each arm:\n$L_{BC} = \\sum_{arm \\in A} \\frac{1}{n_{arm}} \\sum_{i=1}^{n_{arm}} CE(p_{arm}^{trans}, p_{arm}^{rot}, p_{arm}^{open}, p_{arm}^{col})$              (5)\nwhere $p_{arm}^{trans}, p_{arm}^{rot}, p_{arm}^{open}, p_{arm}^{col}$ represents the distribution of the ground-truth actions for translation, rotation, gripper openness, and collision avoidance for the corresponding robot arm, respectively. The behavior cloning loss is then combined with the two regularization terms described above to learn informative skill manager and visual aligner. To sum up, the training objective of AnyBimanual is:\n$L_{total} = L_{BC} + \\lambda_{skill} L_{skill} + \\lambda_{voxel} L_{voxel},$                                                   (6)\nwhere $\\lambda_{skill}$ and $\\lambda_{voxel}$ refer to hyper-parameters that balance the importance of the regularizations."}, {"title": "4. Experiments", "content": "In this section, we first introduce the experimental setups (Section 4.1). Then, we transfer various unimanual methods"}]}