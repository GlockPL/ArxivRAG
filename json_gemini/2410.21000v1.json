{"title": "Efficient Bilinear Attention-based Fusion for Medical Visual Question Answering", "authors": ["Zhilin Zhang", "Jie Wang", "Ruiqi Zhu", "Xiaoliang Gong"], "abstract": "Medical Visual Question Answering (MedVQA) has gained increasing attention at the intersection of computer vision and natural language processing. Its capability to interpret radiological images and deliver precise answers to clinical inquiries positions MedVQA as a valuable tool for supporting diagnostic decision-making for physicians and alleviating the workload on radiologists. While recent approaches focus on using unified pre-trained large models for multi-modal fusion like cross-modal Transformers, research on more efficient fusion methods remains relatively scarce within this discipline. In this paper, we introduce a novel fusion model that integrates Orthogonality loss, Multi-head attention and Bilinear Attention Network (OMniBAN) to achieve high computational efficiency and strong performance without the need for pre-training. We conduct comprehensive experiments and clarify aspects of how to enhance bilinear attention fusion to achieve performance comparable to that of large models. Experimental results show that OmniBAN outperforms traditional models on key MedVQA benchmarks while maintaining a lower computational cost, which indicates its potential for efficient clinical application in radiology and pathology image question answering.", "sections": [{"title": "I. INTRODUCTION", "content": "Medical Visual Question Answering (MedVQA) is an emerging field within multi-modal artificial intelligence that adapts the principles of general Visual Question Answering (VQA) to meet the specific demands of the medical domain. The primary goal of MedVQA is to support healthcare professionals by automatically generating accurate answers to clinical questions based on medical images, thereby assisting in clinical decision-making and relieving workload. This task involves the fusion of computer vision and natural language processing techniques to analyze visual data alongside natural language questions, enabling contextually relevant and clinically accurate responses.\nMedVQA presents unique challenges due to the specialized nature of medical images and the technical terminology prevalent in clinical inquiries. Effectively addressing these complexities requires models capable of capturing fine-grained details and sophisticated relationships between visual and textual data. While traditional VQA models such as VGGNet [1], ResNet [2], GRU [3], and LSTM [4] have been adapted for MedVQA tasks, they often fall short in the medical context due to limited labeled data and the need for robust generalization across diverse clinical scenarios.\nDespite the recent success of multimodal fusion techniques in enhancing MedVQA performance, there is a significant gap in research focusing on computationally efficient fusion methods. Transformer-based models, especially cross-modal Transformers, have demonstrated strong fusion capabilities and have been widely adopted in this domain. However, these large unified models come with substantial computational demands, making them less suitable for real-time clinical applications where computational efficiency is essential. This motivates the need for exploring alternative fusion techniques that maintain high performance while reducing computational complexity, particularly for applications constrained by processing power.\nIn this paper, we introduce a novel model, OmniBANet, that integrates Orthogonality loss, Multi-head attention, and a Bilinear Attention Network (OMniBAN). Our model is specifically designed to address the challenges of MedVQA by achieving a high level of computational efficiency and robust performance without requiring pre-training, making it suitable for clinical scenarios with resource constraints. Through comprehensive experiments, we demonstrate that OmniBANet achieves competitive results on key MedVQA benchmarks while operating at a lower computational cost than Transformer-based models. This balance of efficiency and accuracy indicates OmniBANet's potential for effective application in medical image question answering across radiology and pathology domains."}, {"title": "II. RELATED WORK", "content": "A. Medical Visual Question Answering\nMedical Visual Question Answering (MedVQA) is an emerging research area within multimodal artificial intelligence that applies the general principles of Visual Question Answering (VQA) to the medical domain. This field combines computer vision and natural language processing techniques to analyze and understand medical images in conjunction with natural language questions, with the goal of generating accurate answers that can assist in clinical decision-making. Initial research efforts in MedVQA adopt VQA models that have proven effective in general VQA, and adapt them for medical applications."}, {"title": "III. METHOD", "content": "A. Problem Formulation\nMedical Visual Question Answering is regarded as a classification task, and the objective is to identify the most probable answer a from a predefined set of possible answers A = {a1,a2, a3, ..., an}. This prediction can be expressed as:\na = \\arg \\max_{\\alpha\\E A} P(a | V_i, q_i)  (1)\nwhere P(a | Vi, qi) denotes the probability of a given answer a being correct given the image vi and the question qi, and \u00e2 is the predicted answer that maximizes this probability.\nB. Multi-modal Feature Extraction\nImage Encoder. MedVQA requires highly specialized image encoders capable of capturing the intricate details specific to medical images, which often differ significantly from general image data. Medical imaging tasks demand a high level of precision, as even subtle visual cues can hold crucial clinical significance."}, {"title": "C. Orthogonal Multi-head Bilinear Attention Network", "content": "Our proposed Orthogonal Multi-head Bilinear Attention Network (OMniBAN) integrates a single-layer multi-head self-attention mechanism with bilinear attention networks to efficiently fuse visual and textual features for Medical Visual Question Answering. This design enables the model to capture complex intra-modal and cross-modal interactions effectively while maintaining computational efficiency. By leveraging orthogonal multi-head attention, OMniBAN enhances feature diversity and maximizes information extraction across modalities without requiring extensive pre-training.\n1) Intra-modal Feature Attention: In OMniBAN, we employ a single layer of multi-head self-attention to act as intra-modal attention to refine image and question features independently before cross-modal fusion. Given image features Vi \u2208 R^{N_v \\times d_v} and question features qi \u2208 R^{N_q \\times d_q}, where N = 1 for image features (since CLIP outputs global image-level features) and Nq denotes the sequence length for question features, we apply multi-head self-attention separately to each modality.\n\u2022 Linear Transformations for Queries, Keys, and Values: For each modality's input x (either vi or qi), we generate queries Q, keys K, and values V through linear transformations:\nQ=xW^Q, K=xW^K, V = xW^V (4)\n\u2022 Scaled Dot-Product Attention: We compute attention scores by taking the dot product of Q and K scaled by \u221adk, and applying a softmax to emphasize relevant information:\nAttention(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V (5)\n\u2022 Multi-Head Attention Output: The outputs from multiple attention heads are concatenated and linearly transformed to form the final refined features:\nx = \\text{Concat}(\\text{head}_1,..., \\text{head}_H)W^O (6)\n2) Cross-modal Bilinear Attention: After intra-modal refinement, OMniBAN applies a bilinear attention mechanism to fuse the refined visual and textual features. This mechanism captures interactions between modalities by evaluating attention distributions across all pairs of input channels, enabling comprehensive cross-modal feature integration.\nTo compute the bilinear attention map A, we use learnable projection matrices Wv \u2208 R^{d_v \\times d_m} and Wq \u2208 R^{d_q \\times d_m} for the refined image and question features, respectively, where dm is the dimension of the shared attention space. The attention map A is calculated as:\nA = \\text{softmax}((\\mathbf{W}_v\\tilde{v}_i) \\odot (\\mathbf{W}_q\\tilde{q}_i)) (7)\nwhere \u2299 denotes the Hadamard (element-wise) product, allowing for fine-grained alignment between corresponding elements in the image and question features.\nThe bilinear attention features for each attention head h are then computed by summing over all interactions between image and question features, weighted by the attention map:\nf_h = \\sum_{j=1}^{N_v} \\sum_{k=1}^{N_q} A_{jk} \\left(W_{v,h} \\tilde{v}_j\\right)^T W_{q,h} \\tilde{q}_k (8)\nIn this formulation, each attention glimpse h learns specialized cross-modal relationships, allowing the model to capture diverse interaction patterns between visual and textual features.\n3) Orthogonality Loss: To ensure diverse information captured by the model, we introduce an Orthogonality Loss [19] to encourage each attention glimpse to focus on unique aspects of the input. This loss reduces redundancy across glimpses, which is particularly beneficial for the complex nature of Medical Visual Question Answering (MedVQA), where both image and text data contain rich, detailed information. Additionally, it helps address with over-fitting, which is common in the field of MedVQA.\nIn this work, Orthogonality Loss is applied to the attention distributions obtained from the bilinear attention mechanism, as shown in Algorithm 1. For each pair of attention distributions (glimpses), the inner product of their normalized vectors is computed and squared, with these values summed to form the final orthogonality loss. This approach promotes orthogonality between attention glimpses, reducing overlap and increasing diversity in the attended features across different heads.\n4) Classifier and Prediction: The joint representation output is then fed into a classifier to predict the most probable answer. A simple feed-forward neural network is used as the classifier, comprising two fully connected layers with an intermediate activation function. This prediction can be represented as:\na = \\text{Classify}(\\mathbf{h}_{joint}) (9)\nThe main loss function for this task is Binary Cross-Entropy with Logits Loss, a common choice for multi-label classification. The total loss used during training combines the main classification loss with the Orthogonality Loss, which encourages both accurate predictions and diversified attention features.\nL = L_{class} + \\alpha \\cdot L_{ortho} (10)\nwhere \u03b1 is the threshold for Orthogonality Loss.\n5) Theoretical Analysis of Computational Complexity: In terms of computational complexity, both the OMniBAN model and pure cross-modal Transformer-based models compute attention across all pairs of elements in both modalities, but they differ in how they handle these interactions and in the subsequent processing steps.\nIn a typical cross-modal Transformer-based model, the attention mechanism has a complexity of O(Nv \u00d7 Nq \u00d7 d) per layer, where Nv and Nq are the number of visual and textual elements, respectively, and d is the feature dimension. This complexity arises because the Transformer computes attention weights for every possible pair of elements in the two modalities, followed by a weighted sum that produces a new set of feature representations.\nIn contrast, the Bilinear Attention Network used in OMniBAN also calculates interactions across all pairs, but it does so in bilinear space by factorizing the interaction into lower-dimensional spaces. Specifically, BAN employs learnable projection matrices Wv and Wq, which project the image and text features into a joint space where the interactions are computed. While this operation still involves all pairs of elements, the subsequent bilinear pooling and the use of lower-dimensional projections (i.e., dm instead of d) make the computation more efficient in practice. The complexity of this bilinear interaction is O(Nv \u00d7 Nq \u00d7 dm), where dm is typically smaller than d, particularly in cases where the dimensionality reduction is significant.\nAdditionally, in Transformer-based models, after the attention computation, the features are passed through multiple layers of feed-forward networks, which further add to the computational burden. In contrast, OMniBAN uses only a single layer of multi-head self-attention and leverages bilinear attention's direct aggregation of multimodal features. It can achieve performance comparable to Transformer-based models without the need for extensive post-attention processing or pre-training.\nTherefore, while both models compute interactions across all pairs, OMniBAN achieves a lower overall complexity by reducing the dimensionality of the interaction space and simplifying the subsequent processing steps."}, {"title": "IV. EXPERIMENTS", "content": "A. Dataset and Metric\nWe use VQA-RAD dataset in our experiments, which contains 3,515 QA pairs based on 315 radiology images. The questions in this dataset are categorized into two types: Closed and Open. Closed questions are those with a limited set of possible answers, most commonly yes/no, while Open questions are more varied and do not restrict the type of response. The dataset is split into a training set with 3,064 question-answer pairs and a test set with 451 pairs.\nFor the evaluation metric, we primarily use accuracy, as MedVQA can be viewed as a multi-class classification task. Accuracy measures the proportion of correctly predicted answers out of the total number of questions, providing a straightforward and widely accepted indicator of model performance in this context.\nB. Experimental Setup\nWe conduct the experiments on a single NVIDIA RTX 3090 (24GB) GPU. The learning rate is set to 0.0005, with a batch size of 32. We train the model for 40 epochs, and the best-performing model on the validation set is saved as the representative model. The Adamax optimizer is used to optimize the model parameters. To minimize the impact of random variation, the OMniBAN model is trained ten times with ten different random seeds, and the average performance and standard deviation is reported in Table II.\nAt the model level, based on the architecture described in III, we integrate the TCR module [20], which includes a pre-trained question classifier used to distinguish between open and closed questions. By training separate models for each question type, we aim to address the unique characteristics and linguistic structures associated with open and closed questions, enhancing model specialization and accuracy. Table I indicates models that utilize the TCR module with the label \"QCR\".\nC. Results and Analysis\nThe results of our experiments on the VQA-RAD test set are presented in Table I, comparing different methods across open, closed, and overall accuracy categories. The baseline models MEVF+SAN [5] and MEVF+BAN [5] achieve overall accuracies of 60.8% and 62.7%, respectively. This incremental improvement suggests that BAN has advantages over SAN in capturing cross-modal interactions, likely due to BAN's bilinear attention structure, which allows for more intricate alignment between image and text features.\nThe MMQ [21] model further increases overall accuracy to 67.0%, while the QCR [20] model brings a notable improvement, reaching 71.6% overall accuracy. This leap demonstrates the value of integrating question-type classification to specialize the model for handling distinct question types in MedVQA tasks. The PubMedCLIP+BAN (QCR) model attains an accuracy of 72.1%, showcasing how combining BAN with domain-specific encoders like PubMedCLIP provides gains over general-purpose encoders by better capturing the specific semantics of medical images and text.\nThe integration of OMniBAN with PubMedCLIP and BiomedCLIP produces competitive results in both closed and overall accuracy. For the PubMedCLIP+OMniBAN model, closed question accuracy reaches 80.6%, marking a slight improvement over other PubMedCLIP configurations. The BiomedCLIP+OMniBAN combination achieves 80.9% accuracy on closed questions, slightly outperforming the original Transformer-based BiomedCLIP model on this metric. This performance on closed questions suggests that OMniBAN effectively captures the structured relationships typically associated with closed questions. The overall accuracy of BiomedCLIP+OMniBAN reaches 75.1%, closely matching the Transformer-based BiomedCLIP model at 75.2%, indicating that OMniBAN provides a viable, computationally efficient alternative to Transformer architectures in cross-modal fusion without sacrificing performance.\nA noteworthy trend emerges when observing open and closed question performance with OMniBAN integration. In both PubMedCLIP+OMniBAN and BiomedCLIP+OMniBAN models, closed question accuracy sees a slight increase, whereas open question accuracy experiences a minor decline. This result may be attributed to the structured nature of closed questions, which tend to follow specific patterns or have defined answer sets that OMniBAN can capture effectively through its bilinear attention mechanism. Conversely, open questions are more diverse and less constrained in language, potentially benefiting more from the broader contextual capacity of Transformer models. This highlights a nuanced distinction between BAN and Transformer models in handling different question types within MedVQA, suggesting that while BAN is highly effective for structured information, Transformers may still have advantages in dealing with more flexible, open-ended inquiries.\nIn practical terms, these results underscore the potential of OMniBAN as an alternative to Transformer-based fusion in MedVQA, particularly for applications where computational efficiency is critical. OMniBAN's performance, closely matching that of Transformer-based BiomedCLIP without the need for extensive pre-training, suggests that BAN-based architectures can provide similar accuracy with reduced computational demands. This is significant for real-world applications, as it enables deployment in resource-limited environments, such as portable devices or edge computing platforms in healthcare settings. Additionally, the integration of domain-specific encoders like BiomedCLIP and PubMedCLIP further enhances BAN's effectiveness, underlining the importance of specialized feature extraction for domains like medical imaging and question answering.\nD. Ablation Study\nTable II presents an ablation study on the VQA-RAD test set, illustrating how key components of the OMniBAN model contribute to its performance improvements. Starting with the baseline configuration, MEVF+LSTM+BAN [5] achieves 43.9% accuracy on open questions and 75.1% on closed questions, yielding an overall accuracy of 62.7%. This model configuration, which does not include BiomedCLIP or BioBERT, serves as a benchmark for assessing the impact of our modifications.\nReplacing the basic image and text encoders with BiomedCLIP and BioBERT leads to a significant performance gain, particularly on open questions (54.3% accuracy), with overall accuracy rising to 68.2%. This highlights the advantage of using domain-specific encoders trained on large-scale biomedical data, allowing the model to capture more relevant medical features from both images and questions. This improvement confirms the value of BiomedCLIP and BioBERT in enhancing baseline performance without additional fusion mechanisms.\nIntroducing multi-head self-attention further improves model accuracy, raising the overall score to 73.4%. The addition of multi-head self-attention refines intra-modal relationships within each modality, enriching the feature representations before cross-modal fusion. The multi-head structure allows the model to capture diverse perspectives within each input modality, particularly enhancing the model's ability to process open-ended questions.\nFinally, with Orthogonality Loss added, the model achieves an overall accuracy of 75.1%, with noticeable gains in both open and closed question performance. Orthogonality Loss encourages diversity among the glimpses, reducing redundancy and ensuring that each glimpse focuses on unique aspects of the input features. This component proves particularly effective in a multi-modal setting, as it enables the model to leverage complementary information across heads, capturing a broader range of relevant details.\nThis demonstrates that each component\u2014domain-specific encoders, multi-head self-attention, and Orthogonality Loss\u2014plays a distinct role in improving BAN model. The combination of these components enables the model to achieve levels of accuracy comparable to Transformer-based models, without the need for extensive pre-training. Together, these results highlight the effectiveness of OMniBAN's architecture in capturing both intra-modal and cross-modal relationships essential for the MedVQA task."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this paper, we proposed the Orthogonal Multi-head Bilinear Attention Network (OMniBAN) as an efficient fusion approach for Medical Visual Question Answering. Our model integrates a single-layer multi-head self-attention mechanism with bilinear attention networks to efficiently fuse visual and textual features, and add Orthogonality Loss to reduce redundancy among attention glimpses. Compared to traditional BAN and Transformer-based fusion methods, OMniBAN achieves a balanced performance in terms of accuracy and computational efficiency. Specifically, experimental results show that OMniBAN, when combined with BiomedCLIP, slightly outperforms the original Transformer-based BiomedCLIP model in closed-type questions without the need for extensive pre-training. This highlights the ability of OMniBAN to model complex cross-modal interactions effectively and offers a computationally efficient alternative to Transformer-based approaches while maintaining competitive performance.\nFor future work, there are several directions worth exploring. One potential avenue is to modify the internal attention computation structure of BAN to further enhance its ability to capture cross-modal interactions. Additionally, incorporating more advanced attention mechanisms or integrating external medical knowledge could further improve the model's performance on MedVQA tasks."}]}