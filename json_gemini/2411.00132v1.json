{"title": "Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales", "authors": ["Tang Li", "Mengmeng Ma", "Xi Peng"], "abstract": "Large pretrained foundation models demonstrate exceptional performance and, in some high-stakes applications, even surpass human experts. However, most of these models are currently evaluated primarily on prediction accuracy, overlooking the validity of the rationales behind their accurate predictions. For the safe deployment of foundation models, there is a pressing need to ensure double-correct predictions, i.e., correct prediction backed by correct rationales. To achieve this, we propose a two-phase scheme: First, we curate a new dataset that offers structured rationales for visual recognition tasks. Second, we propose a rationale-informed optimization method to guide the model in disentangling and localizing visual evidence for each rationale, without requiring manual annotations. Extensive experiments and ablation studies demonstrate that our model outperforms state-of-the-art models by up to 10.1% in prediction accuracy across a wide range of tasks. Furthermore, our method significantly improves the model's rationale correctness, improving localization by 7.5% and disentanglement by 36.5%. Our dataset, source code, and pretrained weights: https://github.com/deep-real/DCP", "sections": [{"title": "1 Introduction", "content": "Large foundation models, such as CLIP [1] and GPT-4V [2], exhibit exceptional performance or even surpass human experts in some high-stakes applications, such as medical diagnosis [3] and autonomous driving [4, 5]. However, most of these models are currently evaluated primarily on prediction accuracy, overlooking a critical aspect for ensuring safety, i.e., the validity of the reasons behind their accurate predictions. Understanding the rationales - the \"how\" and \"why\" behind model predictions - is crucial for developing safe predictions. To build trust in real-world deployment, a natural question arises: Can models make double-correct predictions, i.e., correct predictions backed by correct rationales?\nCorrect rationales generally align with how humans would reason about the same decision and are based on valid visual evidence [6, 7, 8]. There are existing attempts to provide rationales for machine learning models' predictions. They either explicitly force the models to make decisions based on human-understandable concepts by introducing bottleneck layers [9, 10], or implicitly inject commonsense knowledge into models by contrastive learning between similar yet distinct textual concepts [11, 12]. However, none of them ensures double-correct predictions. Observations from our previous research [13] and recent studies in the field [14, 15] reveal that these models might provide incorrect rationales, as they fail to base the rationales on valid visual evidence.\nTo this end, we develop double-correct predictions by focusing on two foundational aspects:\ni) \"What\" are the correct rationales? Structured rationale acquisition. Existing vision datasets typically provide ground truth labels of predictions, whereas missing the rationales behind these decisions [16, 17]. To fill this gap, we curate a new dataset that offers over 4,000 unique textual rationales designed for predicting the 1,000 categories in ImageNet [18], structured in a tree format. This design differs from existing knowledge graphs [19, 20, 21], which either provide irrelevant knowledge for the vision task or are too coarse-grained, providing insufficient information. Our rationale dataset is tailored to capture the detailed reasoning processes for visual recognition.\nii) \"Where\" are the correct rationales? Rationale-informed optimization. The other challenge in developing double-correct predictions is the absence of pixel-wise annotations for rationales' visual evidence. Although some datasets provide segmentation masks of object parts [17, 22], they lack sufficient rationale coverage and are limited to small-scale use cases [23]. To address this issue, we propose a rationale-informed optimization method to guide the model in disentangling and localizing the visual evidence of rationales, without requiring manual annotations. Our method can be integrated into the existing model training process without architectural changes and extra parameters.\nWe evaluate the proposed method on a wide range of benchmark datasets and tasks. For prediction correctness, our model outperforms state-of-the-art models in zero-shot, linear probe, and fine-tuning settings by 2.6%, 2.0%, and 10.1%. For rationale correctness, the empirical results exhibit that our model significantly improves ground truth rationale localization and rationale disentanglability by 7.5% and 36.5%. Furthermore, the extensive qualitative results and ablation studies demonstrate the effectiveness of the proposed method.\nOur contribution includes: 1) We curate a new structured rationale dataset. 2) A faithful explanation method tailored for explaining CLIP-ViT predictions. 3) A principled optimization method that seamlessly integrates structured rationale information to develop double-correct predictions. 4) Empirical results in a wide range of benchmark datasets and tasks including image classification and retrieval demonstrate the superior prediction and rationale correctness of our model."}, {"title": "2 Problem Formulation", "content": "In this section, we first formally define rationales, then provide the mathematical formulation of the double-correct prediction problem.\nDefinition 1 (Rationales) Given a category y, rationales are a set of K underlying abstract notions {r_k}^K_{k=1} and relations that capture the reasoning process leading to the recognition of y.\nIn the real world, rationales can be represented through textual descriptions [24, 25]. For example, when recognizing a specific breed of dog in an image, the rationales could be a set of concepts such as the shape of the ears, the color of the fur, and the size of the dog. Mathematically, given a textual rationale r, we assume the existence of a ground truth labeling function V(x, r) that can provide the pixel-wise annotations of visual evidence corresponding to r on an input x.\nDefinition 2 (Double-Correct Predictions) A correct prediction is double-correct when it is backed by correct rationales that are based on valid visual evidence.\nDenote (x, y) ~ P(X, Y) as a data point sampled from the training distribution P(X, Y), g(\u00b7) as an explanation method that attributes the prediction of text r to a group of pixels in input x depending on model f, l(\u00b7) as the task-specific loss function, and F as a function class that is model-agnostic for the prediction task. To ensure the model f makes double-correct prediction, we propose to solve the following constrained optimization problem:\n$\\min_{f \\in F} R(f) := E_{(x,y) \\sim P(X,Y)}[l(f(x), y)] \\quad s.t. \\quad g(x, r; f) = V(x,r), \\forall r \\in \\{r_k\\}_{k=1}^K \\text{  (1)}$"}, {"title": "3 Double-Correct Predictions", "content": "To bridge the gaps, in Sec. 3.1 we present how to acquire rationales {r_k}_{k=1}^K, in Sec. 3.2 we propose a new explanation method g(\u00b7), and in Sec. 3.3 we develop double-correct predictions without V(\u00b7)."}, {"title": "3.1 Structured Rationale Dataset", "content": "In this section, we curate a new rationale dataset to offer {r_k}_{k=1}^K in Eq. 1. According to Def. 1, rationales are structured human knowledge. Therefore, ontologies that encapsulate complex, interconnected information while maintaining semantic relationships between entities [28, 29], present a proper tool to represent rationales. The benefits are bi-directional: i) in the human-to-machine direction, it offers a standardized, machine-readable format; ii) in the machine-to-human direction, ontology structure mirroring how humans organize and retrieve information to explain the model's decision-making process.\nAcquire structured rationales: Different from existing works that are limited to small-scale manual annotation, we generate our rationale dataset in a scalable manner. Specifically, we utilize Large Language Models (LLMs) like GPT-4 [2] to extract the structured rationales. Existing studies prove that GPT-4 has expert-level expertise in commonsense [30] and domain knowledge [31]. However, we find that directly querying LLMs would yield inconsistent tree structures that can hardly be used by machine learning models. To address this issue, we provide a series of exemplary structured rationales before the query, employing in-context learning [32] to extract standardized rationales in a . JSON format. See Appendix A for our full prompt and rationale examples.\nRationale dataset statistics: Our dataset covers all 1,000 categories in the ImageNet [18]. For each category, we generate an ontology tree with a maximum height of two. As illustrated in Fig. 2, the root node is the category, the children of the root are the attributes, and the leaves are the sub-attributes. The edges represent the relationships between nodes. Combining attributes and sub-attributes, our dataset contains over 4,000 unique rationales. Our rationale ontology trees capture the reasoning processes leading to the recognition of the corresponding root categories.\nCan we trust the rationales extracted from GPT-4? Although there are plenty of works showing GPT-4's remarkable capabilities [30, 31], it still could suffer from hallucinations [33, 34]. However, evaluations on the generation quality are largely missing from existing works that generate data from LLMs [9, 35, 36]. To fill this gap and ensure the quality of our rationale data, we conduct comprehensive human and machine evaluations. As detailed in Sec. 4.1, on a 5-point Likert scale across three metrics, 964 out of 1,000 categories are scored as having high-quality rationales (\u22654.0).\nIn contrast to existing Knowledge Graphs [19, 20, 21] that either offer knowledge unrelated to the visual prediction task, or are too coarse-grained that provide insufficient information, our structured rationales are tailored for visual recognition tasks in a fine-grained attribute level. Furthermore, our dataset can expand to accommodate new rationales, providing flexibility to dealing with evolving datasets where more data becomes available. For example, our rationale ontologies can be seamlessly integrated following the ImageNet [18] category ontology derived from WordNet [20]."}, {"title": "3.2 Faithful Explanation Method", "content": "In this section, we develop a new explanation method to implement g(\u00b7) in Eq. 1. To incorporate both image and text inputs, we instantiate the model f using the CLIP-ViT architectures [37] because of their proven capability [1, 38]. Existing methods for explaining the ViT model either directly use the attention maps as explanations [39], or weigh them using gradients [40, 41]. However, these methods might be unfaithful to the ViT predictions. This is because the computation of each ViT prediction involves queries, keys, and values, whereas the attention maps only capture the inner products of queries and keys, ignoring information in values that also affect predictions [42, 40]. Therefore, explanations based on attention maps might not fully reflect the reasons behind ViT predictions.\nDecompose ViT outputs: Recent works [43, 44] prove that, for ViT models, the image embeddings can be decomposed into the contributions of each token within each attention head. Let \\& and 0 parameterize the image- and text-encoder of the CLIP-ViT model, P is the projection matrix, L, M, N are the numbers of layers, heads, and image tokens, a_{l,i,m} is the output of the m-th attention head in layer l for the i-th image token, then the embedding of image I can be decomposed as:\n$e_I = f_{\\phi}(I) = PVIT(I) = \\sum_{l=1}^{L} \\sum_{i=1}^{N} \\sum_{m=1}^{M} Pa_{l,i,m}\\text{  (2)}$\nBy contracting along layers and heads, [44] calculates the contribution of the i-th image token to the final image embedding using $\\sum_{l=1}^{L}\\sum_{m=1}^{M}Pa_{l,i,m}$.\nFaithful explanations weighted by mean-ablation results: As indicated by our mean-ablation results in Fig. 3, the final layers contribute the most to the predictions, whereas the earlier layers have minimal impact. Thus, noise from early layers could obscure key information by a naive summation across all layers as in [44]. To address this issue, we weigh each layer's contribution based on its importance, measured by the corresponding performance drop in the mean-ablation study. Denote the performance drop of layer l as \u0394_l, we calculate the contribution of the i-th image token by:\n$e_i = \\sum_{l=1}^{L} w^l (\\sum_{i=1}^{N} \\sum_{m=1}^{M}Pa_{l,i,m}), \\text{ where } w^l = \\frac{\\Delta_l}{\\sum_{l'=1}^{L} \\Delta_{l'}}\\text{  (3)}$\nNote that e_i is projected onto the image-text embedding space by P. Thus, we can use g(I,r) = {(e_i, f_{\\theta}(r))}_{i \\in I} to calculate the explanations of rationale r on an image I, i.e., visual evidence.\nOur method significantly improves the explanation accuracy, as shown in Tab. 1. In contrast to attention-based explanations [39], our method fully utilizes the information from queries, keys, and values that are used for ViT predictions. Compared to gradient-weighted attention maps [40, 41], our method cuts down the computational complexity from O(n\u00b2) to O(n) over n image tokens."}, {"title": "3.3 Rationale-informed Optimization", "content": "In this section, we develop double-correct predictions by disentangling and localizing rationales without pixel-wise human annotations V(\u00b7) in Eq. 1.\nDisentanglement via reconstruction: Drawing insights from our previous research [47, 13], we propose to contrast between explanation heatmaps of rationales to guide the model training in a self-supervised manner. Specifically, we enforce the following two constraints: i) the image embeddings for different rationales within the same category are disentangled, and ii) the aggregated image embedding of all rationales within the same category aligns with the text embedding of the category.\nMathematically, the backbone objective is to learn a mapping function f \u2208 F such that for each image-text pair (I,T) ~ P(I, T), the embeddings f_\\phi(I) and f_\\theta(T) are aligned in a shared space if they are a correct match, where T is a text description of category y. Let l(\u00b7) be the InfoNCE loss [48]. h(g(I,r)) = \\sum_i e_i \u00b7 1(g(I,r); > \\tau) extracts the image embedding of a given rationale. D(, ) is a distance metric such as L2 distance. \u03c4, \u20ac, and 8 are thresholding hyperparameters. For all r,r' \u2208 {r_k}_{k=1}^K, we propose to develop double-correct predictions by optimizing:\n$\\min_{f \\in F} R(f) := E_{(I,T) \\sim P(I,T)}[l(f_{\\phi}(I), f_{\\theta}(T))] \\qquad \\text{Correct Predictions} \\\\s.t. \\quad D(h(g(I,r)), h(g(I, r'))) \\geq \\epsilon, \\quad D(\\sum h(g(I,r)), f_{\\theta}(y)) \\leq \\delta.\\text{ (4)} \\qquad \\text{Correct Rationales} \\\\ \\text{Disentanglement} \\qquad \\text{Reconstruction}$\nIntuitively, the reconstruction term prevents the disentanglement from collapsing into trivial solutions, thereby ensuring localization. Solving Eq. 4 often leads to a non-convex problem, wherein methods such as stochastic gradient descent (SGD) cannot guarantee constraint satisfaction [49, 50]. To address this issue, we leverage Karush\u2013Kuhn\u2013Tucker (KKT) conditions [51, 52] and introduce Lagrange multipliers \u03bb and \u03b3 to convert the constrained problem into its unconstrained counterpart:\n$\\min_{f \\in F} \\{R(f) := E_{(I,T) \\sim P(I,T)} [l(f(I,T))] \\qquad \\text{  (5)} \\\\+ \\lambda D(h(g(I,r)),h(g(I,r'))) + \\gamma D(\\sum h(g(I,r)), f_{\\theta}(y))\\}.$\nOur method has the following merits: i) In contrast to existing works that rely on expensive pixel-wise annotations to localize objects [53, 54], the proposed rationale-informed optimization achieves a more fine-grained, attribute-level localization without manual annotations. ii) Our method can be integrated into vision-language model training without architectural changes and extra parameters."}, {"title": "4 Experiments", "content": "In this section, we first evaluate the quality of our curated rationale dataset in Sec. 4.1. To best validate double-correct predictions, we then conduct a series of experiments to compare the proposed method with existing methods in Secs 4.2 - 4.7. The experimental results prove that our model achieves superior prediction and rationale correctness on a wide range of benchmark datasets and tasks."}, {"title": "4.1 Evaluation of Rationale Quality", "content": "Metrics: We focus on three essential aspects of the rationale quality. (1) Factual Consistency: whether the rationales are consistent with facts. (2) Comprehensiveness: whether the rationales provide sufficient information necessary to predict the category. (3) Visual Disentanglement: whether the rationales are visually disentanglable or non-overlap. We rate them on a 5-point Likert scale scoring system, where higher scores indicate better performance. For example, in Factual Consistency, score 5 means 100% of the generated rationales are consistent with facts, score 4 means 75%, score 3 means 50%, score 2 means 25%, and score 1 means completely wrong.\nEvaluators: (1) Human Evaluators: We recruited four human evaluators, who are mostly graduate students. They are asked to conduct assessments based on commonsense knowledge and perform Internet searches for validation. On average, it takes them around one minute per sample. (2) Machine Evaluators: The latest GPT-4o and GPT-4v models (date accessed: Aug. 6th, 2024). For each evaluation, we perform three independent runs and calculate the average scores. Note that expanding human evaluations to the entire dataset is not scalable. To this end, we first prove the reliability of machine evaluations, then use it to automatically evaluation the entire dataset.\nHuman evaluations: We sample three independent groups of data from our rationale dataset, each consisting of 50 categories and their corresponding rationales. Specifically, categories were randomly selected from their superclasses: Animals (20), Objects & Artifacts (15), Natural Scenes (5), Plants (5), and Human Activities (5). This ensures that not only each superclass is represented but also that our results are robust [55]. As shown in Tab. 2, The dataset consistently achieves scores of 4.61 or higher on the average of evaluators for each metric, indicating that over 90.3% of the rationales for each category are highly factual, comprehensive, and visually disentanglable.\nMachine evaluations: Note that the scores of all three metrics are almost identical between machines and humans. The Pearson Correlation coefficient of 0.82 reveals the strong positive correlation"}, {"title": "4.2 Benchmark Datasets and Implementation Details", "content": "Backbone model: Due to the computational cost of training large vision-language models (VLMs) from scratch, we focus on fine-tuning experiments. Specifically, we fine-tune the ViT-B/32 variant of CLIP on the ImageNet [18] dataset combined with our curated rationale dataset. To maintain simple and interpretable rationales, the ontology graph for each category is limited to a maximum depth of two, allowing for the extraction of five to six independent concepts on average.\nBaseline models: We compare our model with state-of-the-art VLMs that use ViT-B/32 as their vision encoders, including large-scale pretrained models (CLIP [1], DeCLIP [56]), knowledge-augmented model (NegCLIP [11]), and fine-grained alignment models (FILIP [57], PyramidCLIP [53]). For fair comparisons, we also compare our model with ImageNet [18] fine-tuned models using the same CLIP initialization and augmented text descriptions as our model, including full model fine-tuning (-ft) and vision-encoder-only fine-tuning (-ft-vision).\nEvaluation datasets: We validate the prediction correctness of the models on image classification and image-text retrieval tasks. For image classification (zero-shot, linear probe), experiments are carried out on nine benchmark datasets, including CUB [17], Caltech101 [58], OxfordPets [59], Food101 [60], SUN397 [61], StanfordCars [62], DTD [63], CIFAR-10 [64], and CIFAR-100 [64]. For retrieval, we conduct experiments on Flickr30K [65] and MSCOCO [66]. To evaluate the correctness of rationales, we evaluate the models' rationale localizability on CUB-Part [67] and PartImageNet [68] that provide ground truth segmentation masks of object parts, e.g., \"head\" and \"body\". Furthermore, we evaluate the rationale disentanglability on the aforementioned nine benchmark datasets. \nImplementation details: We follow the same architecture design as CLIP [1] for ViT-B/32. The input resolution of image encoder is 224\u00d7224 and the maximum context length of text encoder is 77. We train our model using an AdamW [69] optimizer and the cosine learning rate scheduler with a linear warmup. Specifically, the learning rate linearly increases from 0 to the peak value within 10% of the total steps, and then decreases with a cosine anneal strategy. Our learning rate is set to 5e-7 and train the model for eight epochs."}, {"title": "4.3 Evaluation Metrics", "content": "Prediction correctness: We use standard category prediction accuracy to evaluate the prediction correctness for zero-shot, linear probe, and fine-tuned settings.\nRationale correctness: We define two new metrics to measure rationale correctness.\ni) Rationale localizability. We evaluate the correctness of rationales using ground truth segmentation masks of object parts [67, 68]. Following the standard evaluation protocol [70], we threshold the rationale explanation heatmaps to segmentation masks and calculate a mean Intersection over Union"}, {"title": "4.4 Evaluation on Prediction Correctness", "content": "Zero-shot image classification: We compare our model against other state-of-the-art and fine-tuned VLMs on zero-shot image classification tasks. The results are shown in Tab. 3. On the average of nine datasets, our model outperforms the second-best result by 2.6%. The results indicate the strong transferability of our model to other vision datasets.\nLinear probe: Following the common practice [1, 71], we conduct linear probe experiments on the nine image classification datasets. As shown in Tab. 3, our model outperforms the second-best result by 2.0%. These results demonstrate the superior vision representations learned by our model.\nFair comparison with fine-tuned models: As shown in Tab. 3, our model outperforms the best fine-tuned model by 10.1% and 5.3% on zero-shot and linear probe results. This suggests that the proposed Rationale-informed Optimization is essential in improving the model's performance."}, {"title": "4.5 Evaluation on Rationale Correctness", "content": "Rationale localizability: We compare our model with state-of-the-art and fine-tuned VLMs. As shown in Tab. 4, our model significantly improves the localization accuracy of rationales by 7.5% and 6.0% on CUB-Part [67] and PartImageNet [68]. This suggests that even without using explicit region annotations, our method significantly enhances the model's localizability of rationales.\nRationale disentanglability: We compare the rationale disentanglement performance of our model with state-of-the-art and fine-tuned models. As shown in Tab. 5, on the average of nine image classification datasets, our model outperforms the second-best result by 36.5%. This significant improvement reveals that our model can distinguish between different rationales.\nFair comparison with fine-tuned models: To evaluate whether our model's performance gain can be obtained by solely introducing information from our rationale dataset, we conduct fair comparison"}, {"title": "4.6 Ablation Study", "content": "Ablation on rationale disentanglement: The \"w/o disen.\" refers to a variant of our method without rationale disentanglement constraint. As shown in Tab. 7, the rationale localizability decreased by 10.4%, indicating the model might not learn to distinguish between rationales without constraints.\nAblation on reconstruction: The \"w/o recon.\" refers to a variant of our method without reconstruction constraint. As shown in Tab. 7, the rationale localizability and prediction accuracy drastically decreased by 13.3% and 30.2%. This reveals that recklessly optimizing the disentanglement between rationale can easily fall into trivial solutions.\nGeneralize to different rationale sets: According to DCLIP [9], using the text embeddings of concepts as a bottleneck layer to force the CLIP model [1] to predict based on them can improve prediction accuracy and interpretability. Specifically, the final prediction will be made by the average embedding similarity between the image and all concepts, namely \u0177 = argmaxy \\frac{1}{K} \\sum_{i=1}^{K} f_{\\phi}(I), f_{\\theta}(c_i)."}, {"title": "4.7 Evaluation on Retrieval Tasks", "content": "Zero-shot image-text retrieval: We evaluate our model on zero-shot image-text retrieval tasks. As shown in Tab. 6, the improved rationale correctness also benefits retrieval tasks.\nRationale-based text-to-image retrieval: To better evaluate the rationale correctness of our model, we conduct a novel retrieval task: rationale-based text-to-image retrieval. The model should retrieve the image with a specified rationale presented. As shown in Fig. 5, in contrast to the CLIP model [1] that entangles rationales with specific categories, our model precisely understands the semantic meaning of rationales independent to categories."}, {"title": "5 Related Works", "content": "Vision model explainability. A widely adopted branch of explainability methods post hoc generates heatmaps to identify the image regions most crucial to the model's predictions, e.g., GradCAM [41], LIME [7], and SHAP [73]. Although useful for revealing the correlations between inputs and outputs, such explanations might be ambiguous, and fail to correspond to high-level concepts that humans easily understand [8]. Methods like TCAV [74] curate attribute datasets to explain vision models using concepts familiar to humans. However, such methods can fail when the models do not learn these concepts [75]. Another branch of methods attempts to design specific architecture to intrinsically interpret model predictions, e.g., CBM [76] and ProtoPNet [77]. However, they cannot guarantee the model learns the semantic meanings of the concepts correctly [14] and yield compromised prediction"}, {"title": "6 Limitation", "content": "While our study advances the double-correctness of predictions, it is not without limitations. First, the absence of explicit ground truth for rationale localization in large-scale datasets remains a significant challenge. We mitigated this by leveraging a self-supervised rationale disentanglement and localization method, but this approach depends heavily on the quality of the structured rationale ontologies. Second, our methods, though effective, are computationally intensive, which may limit their applicability in resource-constrained scenarios."}, {"title": "7 Conclusion", "content": "We introduce a new concept of double-correct predictions aimed at training vision-language foundation models to make accurate predictions backed by correct rationales, thereby enhancing their safety for real-world deployment. To support this, we establish a solid foundation for the development of double-correct predictions. Specifically, we develop a unique dataset with structured rationales that clearly outline the reasoning processes necessary for visual recognition tasks. Furthermore, we propose a principled rationale-informed optimization method tailored for double-correct prediction. Our comprehensive empirical evaluations demonstrate that our method significantly enhances the double correctness of vision-language model predictions."}, {"title": "Appendix", "content": "A Full Prompts\nOur Prompt to Obtain Structured Rationales\nAmerican Robin = {\n\"nodes\": [\n{\"id\": \"American Robin\", \"label\": \"American Robin\"},\n{\"id\": \"Breast\", \"label\": \"Breast\"},\n{\"id\": \"Tail\", \"label\": \"Tail\"},\n{\"id\": \"Beak\", \"label\": \"Beak\"},\n{\"id\": \"Eyes\", \"label\": \"Eyes\"},\n{\"id\": \"Red\", \"label\": \"Red\"},\n{\"id\": \"Gray\", \"label\": \"Gray\"},\n{\"id\": \"Yellow\", \"label\": \"Yellow\"},\n{\"id\": \"Round\", \"label\": \"Round\"},\n{\"id\": \"Long\", \"label\": \"Long\"}\n],\n\"edges\": [\n{\"source\": \"American Robin\", \"target\": \"Breast\", \"relation\": \"has\"},\n{\"source\": \"American Robin\", \"target\": \"Tail\", \"relation\": \"has\"},\n{\"source\": \"American Robin\", \"target\": \"Beak\", \"relation\": \"has\"},\n{\"source\": \"American Robin\", \"target\": \"Eyes\", \"relation\": \"has\"},\n{\"source\": \"Breast\", \"target\": \"Red\", \"relation\": \"is\"},\n{\"source\": \"Tail\", \"target\": \"Gray\", \"relation\": \"is\"},\n{\"source\": \"Beak\", \"target\": \"Yellow\", \"relation\": \"is\"},\n{\"source\": \"Eyes\", \"target\": \"Round\", \"relation\": \"are\"},\n{\"source\": \"Tail\", \"target\": \"Long\", \"relation\": \"is\"}\n]\n}\nAirliner = {\n\"nodes\": [\n{\"id\": \"Airliner\", \"label\": \"Airliner\"},\n{\"id\": \"Wings\", \"label\": \"Wings\"},\n{\"id\": \"Tail\", \"label\": \"Tail\"},\n{\"id\": \"Fuselage\", \"label\": \"Fuselage\"},\n{\"id\": \"Engines\", \"label\": \"Engines\"},\n{\"id\": \"Windows\", \"label\": \"Windows\"},\n{\"id\": \"Logo\", \"label\": \"Logo\"},\n{\"id\": \"Large\", \"label\": \"Large\"},\n{\"id\": \"Horizontal stabilizer\", \"label\": \"Horizontal stabilizer\"},\n{\"id\": \"Cylindrical\", \"label\": \"Cylindrical\"},\n{\"id\": \"Under wings\", \"label\": \"Under wings\"},\n{\"id\": \"Rowed\", \"label\": \"Rowed\"},\n{\"id\": \"Tail fin\", \"label\": \"Tail fin\"}\n],\n\"edges\": [\n{\"source\": \"Airliner\", \"target\": \"Wings\", \"relation\": \"has\"},\n{\"source\": \"Airliner\", \"target\": \"Tail\", \"relation\": \"has\"},\n{\"source\": \"Airliner\", \"target\": \"Fuselage\", \"relation\": \"has\"},\n{\"source\": \"Airliner\", \"target\": \"Engines\", \"relation\": \"has\"},\n{\"source\": \"Airliner\", \"target\": \"Windows\", \"relation\": \"has\"},\n{\"source\": \"Airliner\", \"target\": \"Logo\", \"relation\": \"has\"},\n{\"source\": \"Wings\", \"target\": \"Large\", \"relation\": \"are\"},\n{\"source\": \"Tail\", \"target\": \"Horiz. stabilizer\", \"relation\": \"has\"},\n{\"source\": \"Fuselage\", \"target\": \"Cylindrical\", \"relation\": \"is\"},\n{\"source\": \"Engines\", \"target\": \"Under wings\", \"relation\": \"are\"},\n{\"source\": \"Windows\", \"target\": \"Rowed\", \"relation\": \"are\"},\n{\"source\": \"Tail\", \"target\": \"Tail fin\", \"relation\": \"has\"}\n]\n}\nWhat are useful visual concepts for distinguishing a {category_name}\nin a photo? These features should be visually distinctable and have\nlimited overlap with each other. These features should include\nattributes and their relations. For each item, you should be concise\nand precise, and use no more than five words. No ambiguous answers.\nShow your answer using a tree structure in JSON format strictly\nfollowing the examples shown above. Only contains two depths of\nnodes (depth 1: attributes, depth 2: subattributes). No connections\nbetween node with the same depth. Do not contain a node without an\nedge connected to it. No other explanations, only provide the graph."}]}