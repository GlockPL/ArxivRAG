{"title": "Increasing the Robustness of Model Predictions\nto Missing Sensors in Earth Observation", "authors": ["Francisco Mena", "Diego Arenas", "Andreas Dengel"], "abstract": "Multi-sensor ML models for EO aim to enhance prediction\naccuracy by integrating data from various sources. However, the pres-\nence of missing data poses a significant challenge, particularly in non-\npersistent sensors that can be affected by external factors. Existing liter-\nature has explored strategies like temporal dropout and sensor-invariant\nmodels to address the generalization to missing data issues. Inspired\nby these works, we study two novel methods tailored for multi-sensor\nscenarios, namely Input Sensor Dropout (ISensD) and Ensemble Sen-\nsor Invariant (ESensI). Through experimentation on three multi-sensor\ntemporal EO datasets, we demonstrate that these methods effectively\nincrease the robustness of model predictions to missing sensors. Particu-\nlarly, we focus on how the predictive performance of models drops when\nsensors are missing at different levels. We observe that ensemble multi-\nsensor models are the most robust to the lack of sensors. In addition, the\nsensor dropout component in ISensD shows promising robustness results.", "sections": [{"title": "1 Introduction", "content": "Multi-sensor Machine Learning (ML) models for Earth Observation (EO) use\ndata from diverse sources like satellites and ground-based sensors, providing a\ncomprehensive view of the Earth. Recently, the literature has focused on using\nthese multi-sensor models to improve accuracy with respect to single-sensor mod-\nels [10]. However, when including more data sources, there is a greater chance\nthat one of the sources will face technical issues and miss some data.\nMissing data is inherent in the EO domain, hindering accurate predictions\nand introducing biases. Moreover, ML models, even adaptive models such as\ntransformers, are not naturally robust to missing data [8]. In Satellite Image\nTime Series (SITS), cloudy conditions hinder the availability of optical images,\nnegatively affecting model predictions [12]. The drop in performance has also\nbeen observed when all the features of a specific sensor are unavailable in land-\nuse classification [6] and vegetation applications [9].\nThe design of ML models aligned to the missing data scenario has been\nvaguely explored in the literature. Garnot et al. [12] present a Temporal Dropout"}, {"title": "2 Handling missing data in multi-sensor learning", "content": "Multi-sensor learning considers the case when multiple set of features, coming\nfrom different sensors, are used as input data in predictive ML models. The main\nobjective is to corroborate the information on individual sensor observations and\nimprove the predictive performance [10]. The literature in EO evidenced that\nusing a multi-sensor perspective is crucial to enrich the input data and improve\nmodel performance [6,12]. However, an appropriate modeling is necessary to get\nthe most advantage of the multi-sensor data [8,10].\nWhen multi-sensor ML models are deployed in real environments, the as-\nsumption that sensor data is persistently available cannot be taken. During in-\nference, the occurrence of missing sensors leads to a scenario for which the model\nis unprepared. Since re-training can be impractical due to limited resources, some\ntechniques can help to handle in a better way the missing sensor data scenario.\nIn the following, we describe such techniques commonly used in the literature.\nImpute. As ML models operate with fixed-size matrices, if sensor features are\nmissing, we need to fill in some numerical value to yield a prediction. This has\nbeen studied with EO data when all features of a specific sensor are missing. For\ninstance, when optical or radar images are missing in land-cover classification\n[6], and when optical or radar time series are missing in different vegetation\napplications [9]. The single hyperparameter of this technique is the imputation\nvalue. For instance, Srivastava et al. [14] used random values, Hong et al. [6]\nused zero, and Mena et al. [9] the average of each feature in the training data.\nExemplar. Another option to filling in missing sensor features is to use existing\nvalues observed during training. Then, the missing features can be replaced with\na similar sample via a training-set lookup. For instance, Srivastava et al. [15]\nperforms the search with the sensors available in a shared space. The shared space\nis obtained with a linear Canonical Correlation Analysis (CCA) projection from\nfeatures learned by sensor-dedicated encoders. Aksoy et al. [1] uses a k-nearest\nneighbor algorithm to search for a candidate value to replace the missing features.\nThe hyperparameters of this technique are how to define the shared space, which\nsimilarity function to use, and which sensors to use in the search."}, {"title": "Reconstruction", "content": "A more sophisticated way to fill in the missing sensors is to learn\na function that predicts all the missing sensor features. Such techniques include\ncross-modal auto-encoders and translation models [14]. The latter method has\nbeen applied to EO data by translating a radar to optical image with cycleGAN\n[3], or using multiple optical sensors at different times to reconstruct the optical\nfeatures at a specific time [13]. Here, the practitioner has to define the entire\nmodel that reconstruct, as well as its optimization."}, {"title": "Ignore", "content": "A multi-sensor ML model can be designed in a way to explicitly ignore\nthe missing sensors. For instance, in the dynamic ensemble framework [7] the\nmodels used in the aggregation are dynamically selected for each sample, com-\npleting ignoring the predictions of other models. Recently, this technique was\napplied to EO data where a sensor-dedicated model was trained on each sensor\n[9]. In this case, the predictions from the missing sensors are simply omitted\nin the aggregation. Furthermore, it has been applied to average intermediate\nfeatures in the model by ignoring sensor-dedicated features [9].\nWhile prior methods solely address the handling of missing sensor data, our\nwork goes beyond this issue by introducing methods aimed at enhancing the\nmodel's robustness under missing sensor data."}, {"title": "3 Methods", "content": "Two multi-sensor ML models are proposed to tackle the missing sensor problem."}, {"title": "3.1 Input Sensor Dropout (ISensD)", "content": "Inspired by recent works that use dropout as data augmentation [12,18], we\nintroduce a Sensor Dropout (SensD) technique at the input-level. The idea is to\nrandomly mask out all input features of some sensors during training. Consider\nthe set of sensor features (input data) for a sample i as $X^{(i)} = \\{X^{(i)}_s\\}_{s \\in S}$, with S\nthe set of all available sensors. Then, the masked input features can be expressed\nby $\\tilde{X}^{(i)} = \\{d^{(i)}_s \\cdot X^{(i)}_s\\}_{s \\in S}$, with $d^{(i)}_s \\sim Bernoulli(r)$ and $r \\in [0, 1]$ the SensD ratio\nthat controls how much the sensor features will be masked out. In addition,\nwe introduce a special case of the previous technique that does not rely on\nthe hyperparameter r. The alternative masking technique lists all the possible\ncombinations of missing sensors ($2^{|S|} - 1$) and then selects one randomly. We\napply these techniques in a batch-wise manner, with a different random masking\n$d^{(i)}$ at each batch. Finally,\nthe masked features are given as input to a multi-sensor model implementing\ninput-level fusion [10], i.e. $\\hat{y}^{(i)} = M_{\\theta}(concat(\\tilde{X}^{(i)}))$, with $M_{\\theta}$ the model function\nparameterized by $\\theta$."}, {"title": "3.2 Ensemble Sensor Invariant (ESensI)", "content": "Inspired by recent works that use sensor invariant models in the context of EO [4],\nwe propose a model with sensor invariant layers. The idea is to use an ensemble"}, {"title": "4 Experiments", "content": "4.1 Datasets\nCropHarvest (CH) We use the cropharvest dataset for multi-sensor crop recog-\nnition [17], in a multi-crop version [9]. This involves a classification task in which\nthe crop-type (between 10 crop-type groups including non-crop) at a given lo-\ncation during a particular season is predicted. The dataset has 29,642 sam-\nples around the globe between 2016 and 2022. The temporal sensors are multi-\nspectral optical (from Sentinel-2), radar (from Sentinel-1), and weather (from\nERA5). These were re-sampled monthly during the crop growing year. An ad-\nditional static sensor is the topographic information. All sensor features were\nspatially interpolated to a pixel resolution of 10 m.\nLive Fuel Moisture Content (LFMC) We use a dataset for multi-sensor moisture\ncontent estimation [11]. This involves a regression task in which the vegetation\nwater (moisture) per dry biomass (in percentage) in a given location at a specific\ntime is predicted. There are 2,578 samples from the western US collected between\n2015 and 2019. The temporal sensors are multi-spectral optical (from Landsat-8)\nand radar (from Sentinel-1). These features were re-sampled monthly during a\nfour-month window prior to the moisture measurement. Additional static sensors\nare the topographic information, soil properties, canopy height, and land-cover\nclass. All features were interpolated to a pixel resolution of 250 m.\nParticulate Matter 2.5 (PM25) We use a dataset for multi-sensor PM2.5 esti-\nmation [2]. This involves a regression task in which the concentration of PM2.5"}, {"title": "4.2 Baselines", "content": "We consider four baselines in the EO literature for missing sensors.\nInput: This corresponds to a standard input-level fusion model commonly\nused in the literature. For the case of missing sensors, the missing input\nfeatures are replaced by the mean of the features in the training set [9].\nITempD: This corresponds to the Input method in addition to using the\nTempD technique. For the case of missing sensors, the missing input features\nare replaced by the same masking value used in the TempD [12].\nFeature: This corresponds to a feature level fusion model that concatenates\nthe features extracted by sensor-dedicated encoders. For the case of miss-\ning sensors, the missing features are replaced with the exemplar technique\n(similarity-based search) proposed in [15].\nEnsemble: This corresponds to an ensemble-based aggregation model that\naverages the predictions of sensor-dedicated models. For the case of missing\nsensors, the missing predictions are just ignored during the aggregation [9].\nFor the input-level fusion methods, we align the features of the temporal and\nstatic sensors by repeating the static features across the time series, as commonly\ndone in EO [17,12]."}, {"title": "4.3 Experimental setting", "content": "We assess the methods with a 10-fold cross-validation by simulating missing\nsensors in the validation fold. The assessment consists of having fewer sensors\nat validation time than during training to make the prediction. We experiment\nwith different degrees of missingness controlled by a percentage of samples that\nare affected by missing sensors. We consider that sensors with temporal features\nmight be missing, leaving the static sensors as permanently available informa-\ntion. For evaluating model robustness, we use the Performance Robustness Score\n(PRS) proposed in [5]. This metric is based on the predictive error with missing\nsensors relative to the predictive error when using all sensors:\n$PRS(y, \\hat{y}_{miss}, \\hat{y}_{full}) = min \\bigg(1, exp \\bigg(1 - \\frac{RMSE(y, \\hat{y}_{miss})}{RMSE(y, \\hat{y}_{full})} \\bigg)\\bigg)$ (3)\nFor the predictive performance, we use the F1 Macro (F1) macro in classification,\nand the Coefficient of Determination (R2) in regression tasks. All metrics are\nupper bounded by 1, and a higher value means a better result."}, {"title": "4.4 Results", "content": "We only report the results of the best dropout ratio in the SensD, which is 20%\nin CH data, 80% in the LFMC data, and 80% in the PM25 data. For the ESensI\nwe include the sensor encoding vector in CH data, in LFMC we add the sensor\nencoding with normalized vectors, and in PM25 the vector is not included. All\nthe results in the CH data are in Sec. 4.5, while the rest is in the appendix.\nIn Fig 3, 4, and 5 we display the PRS of the compared methods in the CH,\nLFMC, and PM25 data respectively. Overall, we observe that Ensemble and\nESensI methods are the most robust when increasing the level of missing sensors\nin the datasets. In addition, Input and ITempD methods have a linear decrease in\nthe robustness with different slopes, depending on the dataset and which sensor\nis missing. For instance, in the CH and LFMC data, Fig 3 and 4, the decrease\nis higher when the optical sensor is missing. Besides, in the PM25 data, these\nmethods have an exponential decrease in the robustness.\nThe PRS results highlight that, by increasing the percentage of missing sen-\nsor data, our proposals show a stronger robustness (PRS decreases by a smaller\namount) in all cases. The ISensD robustness is significantly increased regarding"}, {"title": "4.5 Ablation study", "content": "We assess different method configurations in the CH data. Fig. 6 displays the\npredictive performance when different values of the SensD ratio are used in the\nISensD method. There is no clear pattern in the results when this parameter\nincreases or decreases. However, the F1 scores with different SensD ratios oscil-\nlate around the \"no ratio\" version, showing its capacity as a non-parameterizable\nalternative. For the ESensI method, in Table 2 we show the predictive perfor-\nmance effect of different model components. In this case, it reflects that simply\nsharing weights is not enough, but rather the other components must be added."}, {"title": "5 Conclusion", "content": "We introduced two methods for multi-sensor modeling with missing sensors,\nISensD and ESensI. The validation in three time series EO datasets showed that\nour methods can significantly increase the model robustness to missing sensors.\nHowever, there is a limitation regarding predictive performance when all sensors\nare available. Future work will focus on detecting the sources of robustness, as it\nis unclear if it comes from: (i) the model's adaptability, (ii) model's disregard for\nthat sensor information, or (iii) the irrelevance of the sensor data for prediction."}, {"title": "A Experimental setting", "content": "Regarding the implementation details and preprocessing, we apply a z-score\nnormalization to the input data and encode the categorical and ordinal features\n(like land-cover and canopy height) with a one-hot-vector. We use a 1D CNN\nfor the temporal features, an MLP encoder for the static features and as a\nprediction head. We use two layers on all architectures with 128 units and a\n20% of dropout. We use the ADAM optimizer with a batch-size of 128, early\nstopping, and standard loss functions (cross-entropy in classification and mean\nsquared error in regression).\nIn Table 3 we display the number of features per sensor in each dataset."}, {"title": "B Predictive performance results", "content": "As a complement to the PRS results shown in the main content of the manuscript,\nwe include predictive performance scores, the F1 in classification and R2 in re-\ngression. Fig. 7, 8, 9 show the results for the CH, LFMC, and PM25 datasets\nrespectively. In classification, the ensemble-based methods are the best in the"}, {"title": "C Extended ablation results", "content": "In Fig 10 and 11 the predictive performance (R2) is shown at different levels of\nthe SensD ratio value in the LFMC and PM25 datasets.\nIn Table 4 and 5 the predictive performance (R2) is shown for different con-\nfigurations of the ESensI model in the LFMC and PM25 datasets."}]}