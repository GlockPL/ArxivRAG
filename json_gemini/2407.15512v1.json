{"title": "Increasing the Robustness of Model Predictions to Missing Sensors in Earth Observation", "authors": ["Francisco Mena", "Diego Arenas", "Andreas Dengel"], "abstract": "Multi-sensor ML models for EO aim to enhance prediction accuracy by integrating data from various sources. However, the presence of missing data poses a significant challenge, particularly in non-persistent sensors that can be affected by external factors. Existing literature has explored strategies like temporal dropout and sensor-invariant models to address the generalization to missing data issues. Inspired by these works, we study two novel methods tailored for multi-sensor scenarios, namely Input Sensor Dropout (ISensD) and Ensemble Sensor Invariant (ESensI). Through experimentation on three multi-sensor temporal EO datasets, we demonstrate that these methods effectively increase the robustness of model predictions to missing sensors. Particularly, we focus on how the predictive performance of models drops when sensors are missing at different levels. We observe that ensemble multi-sensor models are the most robust to the lack of sensors. In addition, the sensor dropout component in ISensD shows promising robustness results.", "sections": [{"title": "1 Introduction", "content": "Multi-sensor Machine Learning (ML) models for Earth Observation (EO) use data from diverse sources like satellites and ground-based sensors, providing a comprehensive view of the Earth. Recently, the literature has focused on using these multi-sensor models to improve accuracy with respect to single-sensor models [10]. However, when including more data sources, there is a greater chance that one of the sources will face technical issues and miss some data.\nMissing data is inherent in the EO domain, hindering accurate predictions and introducing biases. Moreover, ML models, even adaptive models such as transformers, are not naturally robust to missing data [8]. In Satellite Image Time Series (SITS), cloudy conditions hinder the availability of optical images, negatively affecting model predictions [12]. The drop in performance has also been observed when all the features of a specific sensor are unavailable in land-use classification [6] and vegetation applications [9].\nThe design of ML models aligned to the missing data scenario has been vaguely explored in the literature. Garnot et al. [12] present a Temporal Dropout"}, {"title": "2 Handling missing data in multi-sensor learning", "content": "Multi-sensor learning considers the case when multiple set of features, coming from different sensors, are used as input data in predictive ML models. The main objective is to corroborate the information on individual sensor observations and improve the predictive performance [10]. The literature in EO evidenced that using a multi-sensor perspective is crucial to enrich the input data and improve model performance [6,12]. However, an appropriate modeling is necessary to get the most advantage of the multi-sensor data [8,10].\nWhen multi-sensor ML models are deployed in real environments, the assumption that sensor data is persistently available cannot be taken. During inference, the occurrence of missing sensors leads to a scenario for which the model is unprepared. Since re-training can be impractical due to limited resources, some techniques can help to handle in a better way the missing sensor data scenario. In the following, we describe such techniques commonly used in the literature.\nImpute. As ML models operate with fixed-size matrices, if sensor features are missing, we need to fill in some numerical value to yield a prediction. This has been studied with EO data when all features of a specific sensor are missing. For instance, when optical or radar images are missing in land-cover classification [6], and when optical or radar time series are missing in different vegetation applications [9]. The single hyperparameter of this technique is the imputation value. For instance, Srivastava et al. [14] used random values, Hong et al. [6] used zero, and Mena et al. [9] the average of each feature in the training data.\nExemplar. Another option to filling in missing sensor features is to use existing values observed during training. Then, the missing features can be replaced with a similar sample via a training-set lookup. For instance, Srivastava et al. [15] performs the search with the sensors available in a shared space. The shared space is obtained with a linear Canonical Correlation Analysis (CCA) projection from features learned by sensor-dedicated encoders. Aksoy et al. [1] uses a k-nearest neighbor algorithm to search for a candidate value to replace the missing features. The hyperparameters of this technique are how to define the shared space, which similarity function to use, and which sensors to use in the search."}, {"title": "3 Methods", "content": "Two multi-sensor ML models are proposed to tackle the missing sensor problem."}, {"title": "3.1 Input Sensor Dropout (ISensD)", "content": "Inspired by recent works that use dropout as data augmentation [12,18], we introduce a Sensor Dropout (SensD) technique at the input-level. The idea is to randomly mask out all input features of some sensors during training. Consider the set of sensor features (input data) for a sample i as $X^{(i)} = \\{X_s^{(i)}\\}_{s \\in S}$, with S the set of all available sensors. Then, the masked input features can be expressed by$\\tilde{X}^{(i)} = \\{d_s^{(i)} \\cdot X_s^{(i)}\\}_{s \\in S}$, with $d_s^{(i)} \\sim Bernoulli(r)$ and $r \\in [0, 1]$ the SensD ratio that controls how much the sensor features will be masked out. In addition, we introduce a special case of the previous technique that does not rely on the hyperparameter r. The alternative masking technique lists all the possible combinations of missing sensors $(2^{|S|} - 1)$ and then selects one randomly. We apply these techniques in a batch-wise manner, with a different random masking $d_s^{(i)}$ at each batch. Finally, the masked features are given as input to a multi-sensor model implementing input-level fusion [10], i.e. $\\hat{y}^{(i)} = M_{\\theta}(concat(\\tilde{X}^{(i)}))$, with $M_{\\theta}$ the model function parameterized by $\\theta$."}, {"title": "3.2 Ensemble Sensor Invariant (ESensI)", "content": "Inspired by recent works that use sensor invariant models in the context of EO [4], we propose a model with sensor invariant layers. The idea is to use an ensemble"}, {"title": "4 Experiments", "content": "4.1 Datasets\nCropHarvest (CH) We use the cropharvest dataset for multi-sensor crop recognition [17], in a multi-crop version [9]. This involves a classification task in which the crop-type (between 10 crop-type groups including non-crop) at a given location during a particular season is predicted. The dataset has 29,642 samples around the globe between 2016 and 2022. The temporal sensors are multi-spectral optical (from Sentinel-2), radar (from Sentinel-1), and weather (from ERA5). These were re-sampled monthly during the crop growing year. An additional static sensor is the topographic information. All sensor features were spatially interpolated to a pixel resolution of 10 m.\nLive Fuel Moisture Content (LFMC) We use a dataset for multi-sensor moisture content estimation [11]. This involves a regression task in which the vegetation water (moisture) per dry biomass (in percentage) in a given location at a specific time is predicted. There are 2,578 samples from the western US collected between 2015 and 2019. The temporal sensors are multi-spectral optical (from Landsat-8) and radar (from Sentinel-1). These features were re-sampled monthly during a four-month window prior to the moisture measurement. Additional static sensors are the topographic information, soil properties, canopy height, and land-cover class. All features were interpolated to a pixel resolution of 250 m.\nParticulate Matter 2.5 (PM25) We use a dataset for multi-sensor PM2.5 esti-mation [2]. This involves a regression task in which the concentration of PM2.5"}, {"title": "4.2 Baselines", "content": "We consider four baselines in the EO literature for missing sensors.\nInput: This corresponds to a standard input-level fusion model commonly used in the literature. For the case of missing sensors, the missing input features are replaced by the mean of the features in the training set [9].\nITempD: This corresponds to the Input method in addition to using the TempD technique. For the case of missing sensors, the missing input features are replaced by the same masking value used in the TempD [12].\nFeature: This corresponds to a feature level fusion model that concatenates the features extracted by sensor-dedicated encoders. For the case of missing sensors, the missing features are replaced with the exemplar technique (similarity-based search) proposed in [15].\nEnsemble: This corresponds to an ensemble-based aggregation model that averages the predictions of sensor-dedicated models. For the case of missing sensors, the missing predictions are just ignored during the aggregation [9].\nFor the input-level fusion methods, we align the features of the temporal and static sensors by repeating the static features across the time series, as commonly done in EO [17,12]."}, {"title": "4.3 Experimental setting", "content": "We assess the methods with a 10-fold cross-validation by simulating missing sensors in the validation fold. The assessment consists of having fewer sensors at validation time than during training to make the prediction. We experiment with different degrees of missingness controlled by a percentage of samples that are affected by missing sensors. We consider that sensors with temporal features might be missing, leaving the static sensors as permanently available information. For evaluating model robustness, we use the Performance Robustness Score (PRS) proposed in [5]. This metric is based on the predictive error with missing sensors relative to the predictive error when using all sensors:\n$PRS(y, \\hat{y}_{miss}, \\hat{y}_{full}) = min \\Biggl (1, exp \\Biggl(1 - \\frac{RMSE(y, \\hat{y}_{miss})}{RMSE(y, \\hat{y}_{full})} \\Biggr) \\Biggr)$  (3)\nFor the predictive performance, we use the F1 Macro (F1) macro in classification, and the Coefficient of Determination (R2) in regression tasks. All metrics are upper bounded by 1, and a higher value means a better result."}, {"title": "4.4 Results", "content": "We only report the results of the best dropout ratio in the SensD, which is 20% in CH data, 80% in the LFMC data, and 80% in the PM25 data. For the ESensI we include the sensor encoding vector in CH data, in LFMC we add the sensor encoding with normalized vectors, and in PM25 the vector is not included. All the results in the CH data are in Sec. 4.5, while the rest is in the appendix.\nIn Fig 3, 4, and 5 we display the PRS of the compared methods in the CH, LFMC, and PM25 data respectively. Overall, we observe that Ensemble and ESensI methods are the most robust when increasing the level of missing sensors in the datasets. In addition, Input and ITempD methods have a linear decrease in the robustness with different slopes, depending on the dataset and which sensor is missing. For instance, in the CH and LFMC data, Fig 3 and 4, the decrease is higher when the optical sensor is missing. Besides, in the PM25 data, these methods have an exponential decrease in the robustness.\nThe PRS results highlight that, by increasing the percentage of missing sensor data, our proposals show a stronger robustness (PRS decreases by a smaller amount) in all cases. The ISensD robustness is significantly increased regarding"}, {"title": "4.5 Ablation study", "content": "We assess different method configurations in the CH data. Fig. 6 displays the predictive performance when different values of the SensD ratio are used in the ISensD method. There is no clear pattern in the results when this parameter increases or decreases. However, the F1 scores with different SensD ratios oscillate around the \"no ratio\" version, showing its capacity as a non-parameterizable alternative. For the ESensI method, in Table 2 we show the predictive performance effect of different model components. In this case, it reflects that simply sharing weights is not enough, but rather the other components must be added."}, {"title": "5 Conclusion", "content": "We introduced two methods for multi-sensor modeling with missing sensors, ISensD and ESensI. The validation in three time series EO datasets showed that our methods can significantly increase the model robustness to missing sensors. However, there is a limitation regarding predictive performance when all sensors are available. Future work will focus on detecting the sources of robustness, as it is unclear if it comes from: (i) the model's adaptability, (ii) model's disregard for that sensor information, or (iii) the irrelevance of the sensor data for prediction."}, {"title": "A Experimental setting", "content": "Regarding the implementation details and preprocessing, we apply a z-score normalization to the input data and encode the categorical and ordinal features (like land-cover and canopy height) with a one-hot-vector. We use a 1D CNN for the temporal features, an MLP encoder for the static features and as a prediction head. We use two layers on all architectures with 128 units and a 20% of dropout. We use the ADAM optimizer with a batch-size of 128, early stopping, and standard loss functions (cross-entropy in classification and mean squared error in regression).\nIn Table 3 we display the number of features per sensor in each dataset."}, {"title": "B Predictive performance results", "content": "As a complement to the PRS results shown in the main content of the manuscript, we include predictive performance scores, the F1 in classification and R2 in regression. Fig. 7, 8, 9 show the results for the CH, LFMC, and PM25 datasets respectively. In classification, the ensemble-based methods are the best in the"}]}