{"title": "Automatic Labelling with Open-source LLMs using Dynamic Label Schema Integration", "authors": ["Thomas Walshe", "Sae Young Moon", "Chunyang Xiao", "Yawwani Gunawardana", "Fran Silavong"], "abstract": "Acquiring labelled training data remains a costly task in real world machine learning projects to meet quantity and quality requirements. Recently Large Language Models (LLMs), notably GPT-4, have shown great promises in labelling data with high accuracy. However, privacy and cost concerns prevent the ubiquitous use of GPT-4. In this work, we explore effectively leveraging open-source models for automatic labelling. We identify integrating label schema as a promising technology but found that naively using the label description for classification leads to poor performance on high cardinality tasks. To address this, we propose Retrieval Augmented Classification (RAC) for which LLM performs inferences for one label at a time using corresponding label schema; we start with the most related label and iterates until a label is chosen by the LLM. We show that our method, which dynamically integrates label description, leads to performance improvements in labelling tasks. We further show that by focusing only on the most promising labels, RAC can trade off between label quality and coverage - a property we leverage to automatically label our internal datasets.", "sections": [{"title": "1 Introduction", "content": "Real-life applications must go beyond model architecture choices and consider all stages of the ML development lifecycle [7]. In the case of supervised learning, central to this process is a requirement to possess or create a high quality labelled dataset that reflects the goals of the business, often facilitated through an iterative annotation process [25, 34]. Despite recent popularity of platforms for crowdsourced labelling, such as Mechanical Turk (MTurk) [30], and active studies in this domain, annotation remain costly in terms of both time and money [37].\nRecently, Large Language Models (LLMs) [26, 32, 6, 33] have emerged as generalist models, and have demonstrated promising performance in zero-shot [21, 14] and few-shot [4, 28] tasks. In the context of data annotation, there has been significant interest in leveraging these powerful generalist models as cost-effective data labelers [11, 9], augmenters [22], and generators [16, 36]. However, many of these works have focused on the use of externally-hosted LLMs that are made accessible through public-facing APIs and billing that operates on a per-token basis. Certain scenarios, particularly those in which data privacy is paramount, necessitate that models are hosted in-house. In this work, we study whether open-source LLMs, hosted internally, can generate high quality labels for classification tasks. Furthermore, we conduct labelling in a zero-shot scenario to minimise the amount of human effort required for annotation, and use 7B models to account for resource provision constraints.\nWe investigate label quality by benchmarking on classification tasks of various levels of difficulty: simple ones usually involve a few well separated categories (low cardinality) and complex ones, such as our internal dataset, involve more nuanced categories (high cardinality, see Figure 1 for illustration). In Section 2.3, we conduct preliminary experiments that demonstrate the benefit to labelling accuracy that comes with providing more detailed label schema information in the prompt. However, we find that classification performance degrades with high cardinality datasets if all label schema information is provided at once, probably because LLMs can't leverage all information precisely for inference [19]. To mitigate this issue, we cast the problem of multi-class classification as a series of binary ones [1], and further propose Retrieval Augmented Classification (RAC) as a solution."}, {"title": "2 Preliminary Studies", "content": "2.1 Leveraging label description for\nclassification\nIn alignment with existing works [9], we evaluate the effect of leveraging label description to improve zero-shot classification with LLM. A label schema thus contains both the label name (L) and description (D):\n\u2022 Label Name (L) - Approximately five words that provide a concise name for a given category.\n\u2022 Label Description (D) - Approximately 50 words that describe in greater detail the semantics of a given category. For intent classification this could include descriptions of user actions or objectives [29].\nFor internal experiments, we receive well-defined label names and descriptions in our case manually created by Subject Matter Experts (SMEs), allowing for a good understanding of each category. However, for public datasets, similar annotation materials are not commonly provided. To investigate into the effect of label description, we use Mistral-7B to generate the label description by giving in prompt a small quantity of training examples. We use prompts based on those of Shah et al. [29] with exact prompts shown in Figure 3 in the Appendix.1"}, {"title": "2.2 Experiment settings", "content": "We choose two open-sourced LLMs, namely Mistral-7B [13] and Llama-7B [33], to evaluate whether their performance is sufficient for industry-grade data labelling. To identify optimal prompting approaches for labelling, we vary the label schema component included in the prompt and examine the effect of Chain-of-Thought prompting (COT) [38]. The specific prompts are outlined in the Appendix, non-COT prompt in Figure 4 and COT prompt in Figure 5.\nWe use AGNews [39], DBpedia [17], Amazon intents [3, 8], and Banking77 [5] for our preliminary study. To compare across experiment configurations, we construct a per-class binary classification problem for each dataset where for each class, a balanced set of positives and negatives are sampled from the withheld test splits. For the Banking77 and Amazon datasets, we sample the maximum possible per class, for AGNews and DBpedia we sample 512 positives and 512 negatives for each class. For efficient offline batch inference during the experiments, vLLM is used as a backend [15]. We report macro-averaged accuracy."}, {"title": "2.3 LLM-based binary classification results", "content": "From our preliminary results, we observe that Mistral 7B significantly outperforms Llama 7B on this binary classification task (results are shown in Table 7 in the Appendix), thus we use Mistral 7B as our LLM for the rest of our experiments in the paper.2\nResults are shown in Table 1. Firstly, we observe that including label description always improves labelling performance for all settings we tested (8 in total with 4 datasets and both COT and non-"}, {"title": "3 Retrieval Augmented Classification", "content": "Given the potential of label description to improve performance for automatic labelling, we first conduct experiments where we incorporate all label names and descriptions into an LLM as a single prompt, letting LLM act as multi-class classifier. We find a significant degradation in performance in the challenging, high cardinality setting. For example, on Banking77, a consumer banking customer intent classification dataset with 77 categories, we observe a 4.0 F1 with Mistral-7B (macro-averaged). We hypothesise that, despite the supported large context length, the LLM is unable to handle all the label and their information effectively in the prompt, which is consistent with what literature reports as 'lost in the middle' [19].\nTo mitigate the long prompt issue, we leverage the well established idea to convert the multi-class classification problem into a series of binary ones [1], allowing the LLM to focus on label schema information for a single category at a time. The optimal configuration for conducting zero-shot binary classification using LLM was chosen through our preliminary studies in Section 2. Furthermore, drawing inspiration from existing approaches using Retrieval Augmented Classification (RAC) [20, 12], we propose to incorporate a retrieval component so that binary classification is done iteratively starting from the most promising labels. The process is illustrated in Figure 1:\n1. Search index construction - Prior to label generation (i.e. inference step), the label description corresponding to each category is embedded and a search index created and stored, enabling a nearest neighbour search to the most semantically related category given a query. At inference time, a ranked retrieved list is produced as input to the LLM, incorporating label name and description as part of the prompts.\n2. Iterative classification - For each category in the ranked list, and including the sample to be annotated, a zero-shot prompt is formed using the setup described in Section 2 for LLM to perform binary classification. If the result returns True (i.e., the LLM believes the sample belongs to a given category), the sample is assigned the label of the current category and no further classification is performed (early stopping). The next category is considered if LLM returns False.\nDespite the algorithm being simple and intuitive, it has several interesting properties that we highlight below:\nFurther usage of label description In our preliminary study, Section 2.3, we show that label description helps LLMs to perform better in its binary decision to accept the label or not. The label description in RAC is also used in the retrieval component, which we believe should enhance the retriever performance. Retriever performance enhancement not only improves the label accuracy by feeding most relevant labels first, but also improves the system latency as the system can accept a label earlier. We show the retriever performance in this section while leaving the latency discussion later in our ablations studies.\nWe benchmark the retrieval performance across each dataset with an off-the-shelf sentence transformer (all-mpnet-base-v2) without considering finetuning [10, 35] and quantify the performance by measuring the Mean Reciprocal Rank (MRR) of categories against the queries. A higher MRR@1 indicates that the correct category is more likely to be ranked first.3 The results are shown in Table 2. Retriever performance is found to be maximised when each category is represented by the written labels and descriptions, showing that label description indeed improves the retrieval performance.\nTruncated RAC One approach to use RAC is to iterate through the labels one by one and stop only"}, {"title": "4 Results on public datasets", "content": "4.1 Datasets\nWe evaluate our proposed solution on four public datasets for text classification, ranging in difficulty from low to high cardinality: AGNews (4 categories) [39], DBpedia (14 categories) [17], Amazon intents (60 categories) [3, 8], and Banking77 (77 categories) [5]."}, {"title": "4.2 Full and Truncated RAC", "content": "The results in Table 3 show the results of full and Truncated RAC across each of the public datasets. We observe that 1) Integrating label description consistently improves labelling quality. Across all datasets and RAC settings (i.e. Full and Truncated) (in total 8 settings), we observe improvement when we integrate label description into RAC. The improvement gap ranges from small (around 2% for AGNews Full setting) to large (around 20% for DBpedia Full setting). The coverage for the Truncated setting overall improves for 3 datasets among 4, which is consistent with our Table 2 that label description improves the coverage at the retrieval stage. 2). Truncated RAC effectively trades off between label quality and coverage. By comparing Full (L+D) and Truncated (L+D), Truncated settings improve F1 scores about 20 absolute points on both AGNews and Amazon, while covering around 70% of the cases.\nWe analyse the errors made by Truncated RAC. First, we observe that some categories are overly"}, {"title": "4.3 Analysis", "content": "4.3.1 Latency investigation\nWith the Banking77 dataset, we show in Table 4 our latency study for different RAC configurations. The baseline 'All info in prompt' performs poorly despite its fast latency, and the random retriever takes significant amount of time to accept a label. Full RAC improves over random retriever by ranking the label candidates. Truncated RAC further improves F1 and matches almost \u2018All info in prompt' inference time; as a trade-off, it does not cover all instances (88.7% shown in Table 3).\n4.3.2 Label Distillation\nWe evaluate the usefulness of our annotations by training on generated labels (i.e. label distillation) and compare the performance with a model of the same architecture (all-mpnet-base-v2 with 100M parameters) but trained on the full training dataset. In both settings, only the classification head is trained. The label distillation experiments use labels generated by Truncated RAC (L+D), which cover a portion of the original training datasets with various level of noise."}, {"title": "5 Results on internal datasets", "content": "We now present our studies on our internal consumer banking dataset. Our dataset shares similar characteristics with the Banking77 dataset and contains 61 categories with detailed category descriptions. The business has relatively abundant data in this case, thus mainly requires an auto labelling tool with high label quality as long as the tool is able to annotate a significant percentage of cases. Based on our studies on public datasets, we use our best settings which runs RAC at maximally 5 steps with detailed label description. We cover 70.7% of the cases with 61.5% Micro-F1. However, the generated label quality does not meet the business requirements. To improve the label quality, we 1) further leverage the truncated tradeoffs by selecting only the most promising label as input to the LLM, together with the corresponding label description 2) apply self consistency [23] where we run n inferences and only accept the answer in which all inferences agree with one another. The detailed results are shown in Table 5 where the settings can trade-off between 60 F1 score to 80 F1 score; further improvement on label quality needs more powerful open-source LLMs, switching to few-shot cases with some manual efforts or more innovation upon the current label generation process.4"}, {"title": "6 Related Work", "content": "It has been extensively shown that LLMs is a promising technique to automatically label data, reducing significantly annotation costs. Wang et al. [37] are arguably one of the first along this line, with GPT-3 as their LLM. With ChatGPT and GPT-4 [24], the LLM labeling techniques give more accurate labels. For example, Gilardi et al. [11] demonstrate the zero-shot abilities of ChatGPT (gpt-3.5-turbo), and find superior accuracy versus MTurk workers on 4 out of 5 datasets relating to political science; auto labelling benchmarks emerge enabling studying and comparing in a systematic way.5 We follow these works but also take into account real-life constraints including privacy concerns and computing resource provision concerns, leading us to consider \u201csmall\u201d LLMs for these tasks.\nTo improve the labelling quality for our considered LLMs, we mainly follow [9] to integrate label schema information into LLMs. While we show it improves performance on binary classification in preliminary results, we also show that it is non trivial to extend to a challenging multi-class classification setting. We propose RAC, which casts multi-class classification into a series of binary classification ones. Adding a retrieval step certainly"}, {"title": "7 Conclusion", "content": "In this work, we study label generation performance under privacy and resource provisioning. We propose Retrieval Augmented Classification (RAC) to integrate label information into LLM based label generation. RAC reformulates challenging multiclass classification into a sequential of binary classifications starting from the most promising label to the least promising ones. We show that the framework allows integrating label schema information, which improves the generated label quality. Furthermore, by focusing solely on promising labels, RAC allows efficiently trade-offs between generated label quality and data coverage. When abstaining, the existing techniques generate labels of high quality for simple datasets that can be distilled efficiently even compared to fine-tuning cases, although there is still a performance gap for more challenging datasets such as our internal dataset."}]}