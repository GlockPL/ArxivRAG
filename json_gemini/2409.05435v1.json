{"title": "Semifactual Explanations for Reinforcement Learning", "authors": ["Jasmina Gajcin", "Jovan Jeromela", "Ivana Dusparic"], "abstract": "Reinforcement Learning (RL) is a learning paradigm in which the agent learns from its environment through trial and error. Deep reinforcement learning (DRL) algorithms represent the agent's policies using neural networks, making their decisions difficult to interpret. Explaining the behaviour of DRL agents is necessary to advance user trust, increase engagement, and facilitate integration with real-life tasks. Semifactual explanations aim to explain an outcome by providing \"even if\" scenarios, such as \"even if the car were moving twice as slowly, it would still have to swerve to avoid crashing\". Semifactuals help users understand the effects of different factors on the outcome and support the optimisation of resources. While extensively studied in psychology and even utilised in supervised learning, semifactuals have not been used to explain the decisions of RL systems. In this work, we develop a first approach to generating semifactual explanations for RL agents. We start by defining five properties of desirable semifactual explanations in RL and then introducing SGRL-Rewind and SGRL-Advance, the first algorithms for generating semifactual explanations in RL. We evaluate the algorithms in two standard RL environments and find that they generate semifactuals that are easier to reach, represent the agent's policy better, and are more diverse compared to baselines. Lastly, we conduct and analyse a user study to assess the participant's perception of semifactual explanations of the agent's actions.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement learning (RL) is a machine learning paradigm in which the agent learns a policy from environment interactions through a trial-and-error process [29]. In deep reinforcement learning (DRL), the RL learning process is powered by deep neural networks, and DRL algorithms have found applications in numerous domains, including robotics, healthcare, and navigation [2, 6, 14]. However, neural networks are black-box algorithms, meaning their decisions are difficult to interpret. This lack of transparency is a serious concern, as it can affect the user's trust and RL deployment, especially in high-risk domains [18, 27]. Prior works have also indicated that allowing the user to interpret AI systems increases user acceptance of AI systems [21] and enables effective collaboration between humans and AI agents [17]. Furthermore, there are often legal requirements for the users of AI systems to be presented with an explanation [7, 12]. This has led to the proliferation of the field of eXplainable Artificial Intelligence (XAI). Challenges of XAI include devising algorithms to generate explanations for black-box models as well as evaluating different types of explanations [18, 28].\nCounterfactual explanations have been established as a powerful explanation technique in X\u0391\u0399 [13, 33]. To explain an outcome, counterfactuals present the user with an alternative scenario where the outcome would have been different (e.g. \"If only your loan application was for a lower amount, it would have been accepted\"). Counterfactual explanations are considered to be user-friendly, as they are actionable, contrasting and inherent to human reasoning [16, 24, 26].\nSemifactual explanations are similar to counterfactuals in that they also aim to explain an outcome by exploring alternative worlds. However, unlike counterfactuals, which explore worlds where the outcome has changed, semifactuals present those alternatives where the outcome remains the same [22]. For example, when explaining why a loan application was rejected, a semifactual might state that \"Even if your salary was twice as high, the application would still have been rejected\". While counterfactual explanations can be used to understand how a negative outcome can be avoided, semifactuals could be utilised to optimise resources necessary for a positive outcome. For example, an AI system utilising semifactuals could explain to the farmer that even if they used less fertiliser, the yield would have remained the same [4]. McCloy and Byrne [22] reported that semifactuals engage psychological processes different from those triggered by counterfactuals. First, semifactuals focus on confirming the outcome even in the alternative scenario. They make the outcome seem more correct and immutable, making them more convincing. Second, semifactuals seem to elicit less negative emo- tions in users when it comes to understanding negative outcomes such as loan rejection. While counterfactuals might potentially be perceived as blaming the users for the outcome, semifactuals re-instate the outcome and reconfirm that nothing could have been done to prevent it [22]. Research into semifactuals as an XAI tech- nique is fairly new, and while there are methods for generating semifactuals in supervised learning tasks [3, 4, 19, 20], they have not yet been explored for RL. Additionally, while their benefits have been speculated, only a fraction of works conducted user studies to explore how users perceive semifactuals and how they affect the interaction between humans and RL-based agents [4].\nIn this work, we explore how semifactuals can be utilised to explain the decisions of RL agents. First, we propose a set of desired properties that informative semifactual explanations should meet. These properties are based on the sequential and stochastic nature of RL algorithms. Next, we provide two algorithms\u00b9 for searching for semifactuals in RL: SGRL-Rewind\u00b2 that searches for changes in the past that would have maintained an outcome; and SGRL- Advance that looks into the future to explore how future actions might maintain an outcome. We evaluated our algorithms in two RL tasks against S-GEN, a state-of-the-art approach for generating semifactuals for supervised learning [19]. Our findings show that using our algorithms generates semifactual explanations that are more true to the model (have better fidelity) and are more diverse. The analysis of our subsequent user study indicates that semifac- tuals may positively affect user understanding of RL-led decisions, although further research remains needed to better investigate the effects on task performance. Our contributions are as follows:\n\u2022 We propose a set of five semifactual properties that determine the most informative semifactual.\n\u2022 We introduce SGRL-Advance and SGRL-Rewind, two algo- rithms for semifactual search which optimise the semifactual properties.\n\u2022 We evaluate our approach against a state-of-the-art baseline in two RL environments and perform a user study to assess the effect of semifactual explanations on user understanding of RL agents."}, {"title": "2 RELATED WORK", "content": "Counterfactuals have been explored in RL both to explain decisions and to correct behaviour. Olson et al. [26] published the first approach to generating counterfactuals in RL based on generative modelling. Similarly, Huber et al. [16] proposed GANterfactual-RL, a model-agnostic generative approach based on the StarGAN archi- tecture for generating counterfactuals. Gajcin and Dusparic [10] introduced RACCER, an approach which relies on RL-specific coun- terfactual properties and searches for the counterfactual instance within the agent's execution tree. Additionally, counterfactual ex- planations providing sequences of actions that can be used to avoid a negative outcome have been explored [9, 30, 31].\nSemifactual explanations have only recently been introduced as an XAI technique [5]. Approaches to generating semifactuals in supervised learning can be divided into counterfactual-guided methods, which require a counterfactual to be generated to guide the search for the semifactual, and counterfactual-free methods, which typically rely on some distance metric to choose the furthest instance of the same class as the query [5]. In counterfactual-guided methods, semifactuals are often searched for between the original instance and the decision boundary. Kenny and Keane [20] intro- duced PIECE, an approach to generating plausible counterfactuals in image-based tasks by identifying and resetting them to their ex- pected values for the counterfactual class. A counterfactual-guided semifactual generation can follow the same process, except that it terminates before the decision boundary is reached [20]. On the other hand, counterfactual-free methods often rely on a metric to choose a semifactual that is the farthest away from the original instance while retaining the same outcome or classification. For example, Nugent et al. [25] proposed generating a neighbourhood of instances around the original instance and using logistic regres- sion to classify them. The instance from the neighbourhood that belongs to the same class as the original instance but has the lowest probability of such a classification is then chosen as the semifactual explanation. Thus, the approach reaffirms the classification deci- sion by showcasing similar, yet more extreme, examples within the same class. Aryal and Keane [4] propose the MDN (Most Distant Neighbour) approach, which searches for the semifactual as an instance that differs most significantly from the original in a chosen key feature. Notably, Kenny and Huang [19] introduce S-GEN, a method for generating semifactuals that rely on causality. To gener- ate semifactuals, authors start by creating a structural causal model (SCM) around the query instance. Neighbouring instances repre- sent nodes, while actions that can transform one state into another are edges. Achieving the semifactual means taking action in the SCM. S-GEN then searches for semifactuals by optimising three metrics - gain, robustness, and diversity. The gain is determined by the distance to the states, taking into account a specified user preference. Robustness is measured in relation to instances in the neighbourhood of the semifactual in the same class. The diversity encourages the generation of semifactuals with diverse features to enable personalisation.\nWhile explored in supervised learning, to the best of our knowl- edge, there are no works on generating semifactual explanations for RL. In this work, we explore how semifactuals can be defined for RL tasks, and propose the first algorithm for generating semifactuals."}, {"title": "3 SEMIFACTUAL GENERATION FOR RL", "content": "In this section, we introduce SGRL-Advance and SGRL-Rewind, two algorithms for generating semifactual explanations in RL. First, Section 3.1 covers the preliminary considerations. Section 3.2 defines the five properties that we put forward as optimisation criteria for the selection of the most informative RL semifactual explana- tions. Then, in subsection 3.3, we describe the two algorithms that generate semifactual explanations based on these properties."}, {"title": "3.1 Preliminaries", "content": ""}, {"title": "3.1.1 Outcomes in Semifactual Explanations.", "content": "The goal of semifac- tuals is to explain the outcome of an RL policy. We use a broad definition of outcome in this work, referring to any function of the state."}, {"title": "Definition 3.1 (Outcome).", "content": "Let F be the function whose properties semifactual explanations aim to illustrate. Then, the outcome of a state s is the outcome of the function F in that state s:\n$O(s) := F(s)$\n(1)\nIn RL, there is a wide array of outcomes of interest. For example, semifactuals can be used to explain an agent's action choice, high or low rewards, or an agent's choice to follow a specific objective or goal. Our approach can be used to explain any such outcome that can be defined as a function of the agent's state. The outcomes can be manually defined based on the user's interests or automatically inferred from the policy, such as achieving a low or high reward in the state."}, {"title": "3.1.2 Stochastic Configurations.", "content": "In a typical RL task, the agent's action policy is shaped by the rewards the agent receives after per- forming actions in the environment. Both the function determining the rewards as well as the agent's policy may be stochastic [29]. Therefore, generating semifactual explanations requires reasoning about the stochastic processes in the environment, which can affect the agent's decisions. To reason formally about stochastic processes, we define stochastic context P to be the set of all stochastic processes $P_i$ that the agent cannot control.\n$P := \\bigcup_{i \\in I} p_i$\n(2)\nWe then define a stochastic configuration $P_T$ as the stochastic context along a particular trajectory T.\n$P_T := \\bigcup_{i \\in I} P_i$\n(3)\nTo illustrate these definitions, let us reconsider the example of the farmer wondering how the watering of the plants affected the yield. Factors such as daily precipitation or atmospheric temperature are among factors that cannot be influenced by the farmer and are thus elements of the stochastic context, P. An example of a trajectory T would be the past days leading to the day on which the outcome (the harvest yield) is determined. The amounts of rain and measured temperatures during these days would then be in the stochastic configuration PT. Determining the actual values in PT allows looking into the past states in T and identifying factors that can be changed, thus enabling the generation of informative semifactuals. Such an approach is consistent with the previously proposed definition of stochastic configurations for counterfactuals [9]. In this work too, we consider that the stochastic configuration PT can be known in full for the trajectory of interest T."}, {"title": "3.1.3 Temporality of Semifactual Explanations.", "content": "Unlike supervised learning, RL deals with sequential execution. For this reason, semi- factual statements can be phrased in two ways: either by looking into the past or by looking into the future. For example, consider a farmer wondering about their yield projection. A semifactual on how the past affects outcome might explain that even if they had applied more fertiliser in the past weeks, the yield would have stayed the same. Alternatively, a semifactual focusing on a future scenario might state that even if they applied more fertiliser in the upcoming days, the yield would not have changed. We thus define two types of semifactual explanations: backward semifactuals that explore alternative scenarios in the past that retain the same out- come and forward semifactuals that look into how future scenarios might have retained the outcome, as illustrated in Figure 1."}, {"title": "Definition 3.2.", "content": "Let $(s_{n-k}, a_{n-k}), ..., (s_{n\u22121}, a_{n\u22121})$ be an arbitrary number of immediate prior state-action pairs that led the agent to the factual state $s_n$ and the outcome $O(s_n)$ to be explained.\nThe backward method of searching for semifactuals generates alternative state-action pairs that would also result in the same outcome as that of the factual state: $(s'_{n-k} a'_{n-k}),..., (s'_{n-1} a'_{n-1})$. All such s' states are backward semifactuals.\nThe forward method of searching for semifactuals begins at the factual state $s_n$ and then generates alternative future scenarios that retain the outcome of the factual state: $(s_{n+1}, a_{n+1}),..., (s_{n+k}, a_{n+k})$. All alternative future states $s'_{n+k}$ are forward semifactuals.\nForward and backward semifactuals are conflated in supervised learning due to a one-step prediction process that does not assume a temporal connection between instances. In RL, however, we have to consider both ways to explain the event. McCloy and Byrne [22] explored a similar distinction in psychology research in counterfac- tual explanations and has found that different cognitive processes are engaged when using previous and future causes to explain an outcome. The role of temporality in counterfactual explanations in RL was also discussed previously [11]."}, {"title": "3.1.4 Actionability of Semifactual Explanations.", "content": "Semifactual expla- nations require changing the original state. Changes to a RL state can occur from two sources \u2013 agent's actions or stochastic processes in the environment. While the first results in actionable changes, the second is non-actionable. For example, while an RL agent can choose to increase the amount of fertiliser in a farming task, the agent cannot control if it rains. Thus, semifactual explanations might differ from the factual state in the amount of fertiliser added, in the weather \"observed\", or in both the weather and fertiliser. Both actionable and non-actionable semifactuals have a role in ex- plaining outcomes in RL. Actionable semifactuals can help the users understand how actions affect outcomes, while non-actionable semi- factuals could explain how stochastic processes in the environment affect the outcome. In this work, we allow both types of changes and see them as equal when looking for semifactuals. However, further research is needed into how these different types of semifactuals affect user understanding of RL policies."}, {"title": "3.2 Desired Semifactual Properties for RL", "content": "Finding a semifactual explanation for the given state s necessitates finding an alternative state s' so that the outcome is the same in both states. Since there might be infinitely many such s' states, we ought to define properties of desirable semifactual states that lead to an informative explanation."}, {"title": "3.2.1 Validity.", "content": "Validity denotes the criterion that the outcome in the factual state s and the semifactual state s' is the same. Formally:\n$V(s, s') := \\begin{cases}\n1 & \\text{if } O(s) = O(s') \\\\\n0 & \\text{otherwise}\n\\end{cases}$\n(4)\nThe validity of semifactuals is analogous to the validity of coun- terfactuals [11, 32], for which the outcome must be different from (rather than equal to) the outcome of the factual state. In this work, as we consider action choices as outcomes, we only consider a semifactual state s' valid if the same action is predicted in s' as in s by the agent's policy \u03c0."}, {"title": "3.2.2 Temporal Distance.", "content": "Semifactuals should take place in the same temporal context of the agent's execution as the state being explained. Intuitively, telling a farmer that more watering of the fields 10 years ago would not have increased the yield is not as informative as indicating that more watering last month would not have changed the outcome. We define temporal distance as the distance in terms of RL actions between the original and the semifactual state. For a state s being explained, a semifactual s', and a sequence of actions A between the two states, temporal distance is defined as:\n$TD(s, s', A) := len(A)$\n(5)\nThe action sequence A can go backward or forward from the factual state, depending on whether we are considering backward or forward semifactuals. This is formalised in Definition 3.2 and illustrated in Figure 1."}, {"title": "3.2.3 Stochastic Uncertainty.", "content": "To be informative, a semifactual ex- planation must offer a stronger argument than the state in question. Coming back to our loan example from the beginning of the paper, saying \"even if your income was twice as high, your application would still be rejected\" is more telling than stating that the loan would have been rejected if the applicant's income was lower than it actually is. For this reason, desired semifactual explanations should prioritise paths with a higher possibility of an outcome change, as they lead the search to the part of the state space where the outcome is more likely to change. We call this property stochastic uncertainty, in line with a similar property for counterfactuals [9]. The property is defined for the factual state s and the sequence of actions A' leading to the semifactual state. Formally, we have:\n$SU(s, A') := P(\\pi(A'(s_{n+k}|w)) = \\pi(s) \\forall w \\in W)$\n(6)\nwhere W is the set of all possible stochastic configurations and \u03c0(s) is the action that the black-box policy chooses in state s."}, {"title": "3.2.4 Fidelity.", "content": "In order to ensure the semifactuals are represen- tative of the policy they are to clarify, we rely on the fidelity, as defined for counterfactuals by Gajcin and Dusparic [10]. Fidelity measures the probability that the sequence of actions A (leading to some semifactual state) would be chosen by the black-box policy from the given (factual) state s.\n$F(s, A) := 1 - ||\\text{softmax}(Q(s, A))[a]||\\\\\n= 1 - \\frac{\\exp (Q(s, a))}{\\sum_{a' \\in A} \\exp ((s, a'))}$\n(7)\nQ(s, a) denotes the policy-dependent Q-value, that is, the valua- tion of taking the action a from state s. While in this case, we use the softmax function over all possible actions from state s, other methods of normalising the likelihood of taking action a is possible. If we consider the example of a farmer, an explanation that would indicate that the yield would have been the same even if he applied additional fertiliser a day before the harvest would have low fidelity, as doing so is an unreasonable practice and would have a corre- sponding low Q-value. Thus, fidelity is a mechanism of hindering semifactuals that are not in line with the policy to be explained."}, {"title": "3.2.5 Exceptionality.", "content": "As proposed by Aryal and Keane [4], desired semifactuals are supposed to be surprising or even counterintuitive. For example, knowing that the yield would have been the same even if the field suffered a drought might reassure the farmer in their decisions and reinforce their trust in the system. Thus, infor- mative semifactuals ought to be exceptional, i.e. we want the chosen explanation to prioritise paths where the unexpected happens. We formalise this property as a measure of the likelihood of the path of k - 1 state-action pairs leading from the factual state s = s1 to the semifactual state s' = sk:\n$E(s_1, s_k, A) := \\sum_{i \\in {1,...,k-1}} P(s_{i+1} | s_i, a_i)$\n(8)\nFor computational reasons we use the likelihood (the sum of probabilities) rather than cumulative probability. Intuitively, exc\u0435\u0440- tionality prioritises paths where unexpected things happen."}, {"title": "3.3 Proposed NSGA-II-Based Algorithms for Generating Semifactual Explanations", "content": "To find a semifactual that optimises the five desired properties, we define two search algorithms, SGRL-Rewind and SGRL-Advance. Both algorithms are model-agnostic and counterfactual-free, i.e. they are applicable to all RL models and do not require searching for counterfactuals first. SGRL-Rewind and SGRL-Advance search for the semifactual state by exploring the neighbourhood around the original instance. While SGRL-Advance searches for semifactual states that can be reached from the original state in k actions, SGRL- Rewind searches for semifactuals that arise from the agent choosing different actions in the previous k actions. Parameter k denotes the maximum distance between the original and semifactual state.\nSGRL-Rewind and SGRL-Advance utilise an evolutionary algo- rithm NSGA-II (Nondominated Sorting Genetic Algorithm II) [8] to generate and evaluate action sequences leading to semifactual states. An overview of the NSGA-II, as used by SGRL-Advance and SGRL- Rewind, is shown in Algorithm 1. Through G generations, NSGA-II modifies the initial population of solutions Po while maintaining a collection of the most promising solutions. For each generation g, a child population Qg is obtained from the parent population Pg through selection, recombination, and mutation (Line 3). Pg and Qg are joined into Rg = Pg + Qg (Line 4) and used to obtain the new parent population $P_{9+1}$. Firstly, Rg is split into fronts $F_0,..., F_t$ using nondominated sorting where F\u2081 contains the best solutions according to the optimisation objectives (Line 5). $P_{9+1}$ is populated using solutions from the best fronts until the size N is reached (Lines 7-9). The last front $F_t$ that can be included is sorted accord- ing to the crowding distance (Lines 10-13) and the best solutions are included into $P_{9+1}$ until the size N is reached (Lines 14 \u2013 19). Using crowding distance promotes solutions from different parts of the solution space and ensures diversity. For both SGRL-Rewind and SGRL-Advance, we use NSGA-II with four properties - tempo- ral distance, stochastic uncertainty, fidelity, and exceptionally - as optimisation objectives and the validity property as a constraint.\nSGRL-Advance explores semifactuals that can be reached by the agent's future actions from the original state. To that end, SGRL- Advance uses NSGA-II to generate and evaluate different action sequences that lead to potential semifactual states. For each action sequence $A_{GP}$, $p \\in [0, N]$, from generation $g \\in [0, G]$ considered by the NSGA-II algorithm, we execute $A_{GP}$ starting in state sn and collect all states reached by this rollout as a set SF of potential semifactual states. We store only those states that satisfy validity. Each state in SF is then evaluated according to the four semifactual properties - fidelity, temporal distance, stochastic uncertainty and exceptionalism. Additionally, the action sequence AP is assigned the average value for each property evaluated for SF($A_{GP}$). This value is the fitness function value for $A_{GP}$ and is used to further guide the selection process of NSGA-II. After G generations, a Pareto front of all potential semifactual states is selected from SF, and these are presented as the results. Formally, to explain state sn, SGRL-Advance aims to find a state s' that optimises the following:\n$argmin\\limits_{s' = A(s_n)}[TD(s_n, s', A), 1 \u2013 SU (s_n, A), F(s_n, A), 1 - E(s_n, s', A) | \\text{ and } V (s_n, s') = 1]$\n(9)\nConversely, SGRL-Rewind uses the backward method of search- ing for semifactuals. Given a sequence of k previous state-action pairs $[(s_{n-k}, a_{n-k}), (s_{n-k+1}, a_{n-k+1}), . . ., (s_{n\u22121}, a_{n-1})]$ leading to the factual state sn, SGRL-Rewind explores semifactual states that could be reached if the agent had chosen an alternative sequence of actions starting in $s_{n-k}$. Formally, NSGA-Rewind optimises the following when explaining state sn:\n$argmin\\limits_{s'=A(s_{n-k})}[TD(s_n, s', A), 1 \u2013 SU (s_n, A), F(s_n, A), 1 - E(s_n, s', A) | \\text{ and } V (s_n, s') = 1]$\n(10)"}, {"title": "4 EXPERIMENTAL EVALUATION", "content": "In this section, we outline our SGRL-Rewind and SGRL-Advance evaluation approach. The evaluation tasks are introduced in Section 4.1, and the baselines are described in Section 4.2."}, {"title": "4.1 Evaluation Environments", "content": "We evaluate SGRL-Rewind and SGRL-Advance in two RL tasks."}, {"title": "4.1.1 Stochastic Gridworld.", "content": "In the Stochastic Gridworld task, the agent navigates a 5x5 grid world. At each step, the agent chooses between 6 actions \u2013 UP, DOWN, LEFT, RIGHT (to make one step in that direction), CHOP (to chop down an obstacle if located next to it), and SHOOT (to kill the dragon). Agent's goal is to shoot the dragon by performing a SHOOT action. This action is only effective if both the agent and dragon are in the same column or the same row and there are no obstacles between them. The obstacles in the environment include trees and walls, which the agent can tear down or navigate around to reach the dragon. Tearing down a tree is less expensive than tearing down a wall. Trees can also spontaneously regrow, and walls can be rebuilt by processes outside of the agent's control, making the environment stochastic."}, {"title": "4.1.2 Frozen Lake.", "content": "In the Frozen Lake task, the agent aims to reach the goal state of a 5x5 grid world. At each step, the agent can choose between moving in any of the four directions or choosing to exit the game. Exiting the game when the agent has reached the goal square is met with a positive reward; otherwise, it does not have any effect on the agent's position. Some squares in the grid world are frozen, so there is a chance of the agent moving in an unintended direction from them."}, {"title": "4.2 Baselines", "content": "SGRL-Advance and SGRL-Rewind are the first approaches for gen- erating semifactuals in RL. For this reason, we compare them to methods developed for supervised learning. As a baseline, we use S-GEN [19], an approach to generating semifactuals in supervised learning. S-GEN generates semifactuals by optimising a loss func- tion consisting of three components: gain, robustness, and diversity. We use the non-causal version of S-GEN, as the causal version re- quires a manually crafted causal model of the environment. S-GEN can create diverse explanations and requires a diversity parameter, which indicates how many explanations will be generated. We use three baselines: S-GEN1, S-GEN3, and S-GEN5, which generate 1, 3, and 5 diverse semifactuals, respectively [19].\nWe start by training a black-box RL policy \u03c0 in both tasks, which we want to explain. We use a DQN [23] to train in both envi- ronments. However, our approach is model-agnostic and can be used to explain any RL policy. The parameters for training \u03c0 in both environments are given in Table 1. In each environment, we then execute \u03c0 and collect factual states to explain. For each pos- sible action a' in the environment, we collect 100 states in which action a' was not chosen. Our goal is then to generate semifactual states answering the question of why the agent did not choose a' in the collected states. In total, we collect 100 states for each action in both environments, resulting in 600 factual states in the Stochastic Gridworld and 500 in the Frozen Lake environment. For each factual state, we then generate semifactual states according to our approaches SGRL-Advance and SGRL-Rewind. Additionally, we create semifactuals using baseline approaches S-GEN1, S-GEN3 and S-GEN5."}, {"title": "5 RESULTS", "content": "In this section, we present the evaluation results. Firstly, we eval- uate SGRL-Advance and SGRL-Rewind against the baselines on semifactual properties (Section 5.1). In Section 5.2 we evaluate fea- ture gain and in Section 5.3 diversity of generated semifactuals. Section 5.4 details the conditions and results of our user study."}, {"title": "5.1 Semifactual Properties Evaluation", "content": "Our first goal is to evaluate how semifactuals perform on the five RL-specific properties defined in Section 3.2 - validity, temporal distance, stochastic uncertainty, fidelity, and exceptionalism. Ad- ditionally, we record the percentage of factual states for which a semifactual was successfully generated. As some properties need a sequence of actions leading to the semifactual, they cannot be straightforwardly evaluated for the S-GEN approach, which can only generate a semifactual state without the sequence of actions leading to it. For this reason, for each semifactual generated by S-GEN, we search for a sequence of actions that could lead to the semifactual using an evolutionary algorithm and use this sequence of actions to evaluate the semifactual properties.\nThe results are shown in Table 3. In both environments, SGRL- Advance and SGRL-Rewind find the semifactual states for more factual states than the baselines. The average validity is always 1, as this property was set as a constraint, and all candidate states not satisfying it were rejected. SGRL-Rewind performs best on the tem- poral distance metric in both environments, generating semifactual states that are closest in the number of actions to the original state. On the other hand, SGRL-Advance performs best on the metrics of fidelity and exceptionalism in both environments. Thus, SGRL- Advance-generated semifactuals are most representative of the policy being explained and they showcase most unexpected events in which the agent does not change its action choice. Stochastic uncertainty is the only property where the baseline outperforms SGRL-Advance and SGRL-Rewind algorithms. Specifically, SGEN1 and SGEN3 yielded better results for stochastic uncertainty in the Stochastic Gridworld task. In the Frozen Lake task, however, SGRL-Advance achieved the best results. Given that the SGRL algorithms outperformed S-GEN in all but one case across the two environ- ments indicates that the resulting semifactuals are easier to reach, represent the underlying policy better, and are more illustrative of even how unexpected scenarios still maintain the outcome. As the first RL-specific semifactual generation approaches, we think that these results are promising, and we encourage future work in the area to use the SGRL algorithms as their baseline."}, {"title": "5.2 Feature Gain", "content": "Semifactual explanations can be used to enhance decision-making and reduce user effort without changing the outcome. For example, a semifactual can advise the user that even if they used less water, the yield would have remained the same, thus saving resources. For this reason, our experiments measured gain. Intuitively, it measures the difference between the factual and semifactual instances; larger differences can indicate higher resource savings. Formally, the gain was defined as feature distance metric by Kenny and Huang [19]:\n$G(x, x') = |x - x' |_2$\n(11)\nAs shown in Table 4, in the Stochastic Gridworld, SGRL-Rewind achieves the biggest gain. In the Frozen Lake, however, the biggest difference in features is reported by the S-GEN1 algorithm. This result is not surprising, given that gain is one of the optimisation goals for S-GEN but not for SGRL algorithms. Unlike in supervised learning, features alone may not indicate the gain in RL. Namely, states in RL can have different features and still be very close to each other in execution. For this reason, we do not use this property as an optimisation goal for either SGRL-Advance or SGRL-Rewind."}, {"title": "5.3 Diversity", "content": "Diversity has been recognised as an important characteristic of semifactual explanations", "19": ".", "as": "n$D(x, X') = \\frac{1}{|X'|} \\sum_{x' \\in X'} |x - x' |_2$\n(12)\nThe results are shown in Table 4. Similar as in the case of gain, our SGRL algorithms do not optimise for diversity, but the S-GEN algorithms do. Still, in both environments, our approaches gener- ate more diverse sets of explanations. In the Stochastic Gridworld, SGRL-Rewind performs best according to the diversity metric, while in the Frozen Lake task, SGRL-Advance achieves the highest diver- sity. The high diversity of explanations generated by SGRL-Advance and SGRL"}]}