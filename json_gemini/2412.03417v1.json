{"title": "LEARNING SEMANTIC ASSOCIATION RULES FROM INTERNET OF THINGS DATA", "authors": ["Erkan Karabulut", "Paul Groth", "Victoria Degeler"], "abstract": "Association Rule Mining (ARM) is the task of discovering commonalities in data in the form of logical\nimplications. ARM is used in the Internet of Things (IoT) for different tasks including monitoring and\ndecision-making. However, existing methods give limited consideration to IoT-specific requirements\nsuch as heterogeneity and volume. Furthermore, they do not utilize important static domain-specific\ndescription data about IoT systems, which is increasingly represented as knowledge graphs. In this\npaper, we propose a novel ARM pipeline for IoT data that utilizes both dynamic sensor data and\nstatic IoT system metadata. Furthermore, we propose an Autoencoder-based Neurosymbolic ARM\nmethod (Aerial) as part of the pipeline to address the high volume of IoT data and reduce the total\nnumber of rules that are resource-intensive to process. Aerial learns a neural representation of a\ngiven data and extracts association rules from this representation by exploiting the reconstruction\n(decoding) mechanism of an autoencoder. Extensive evaluations on 3 IoT datasets from 2 domains\nshow that ARM on both static and dynamic IoT data results in more generically applicable rules\nwhile Aerial can learn a more concise set of high-quality association rules than the state-of-the-art\nwith full coverage over the datasets.", "sections": [{"title": "1 Introduction", "content": "Association Rule Mining (ARM) is a common data mining task that aims to discover associations between features of\na given dataset in the form of logical implications [Agrawal et al., 1994]. In Internet of Things (IoT) systems, ARM\nmethods are utilized for various tasks including monitoring, decision-making, and optimization, for example, of a\nsystem's resources [Sunhare et al., 2022]. Some IoT application domains in which ARM has been successfully utilized\ninclude agriculture [Fan et al., 2021], smart buildings [Degeler et al., 2014] and energy [Dolores et al., 2023]. However,\nmost applications of ARM in IoT give limited considerations to characteristics of IoT data such as heterogeneity\nand volume [Ma et al., 2013] as they are mere adaptations of rule mining methods not specifically tailored to IoT\nrequirements.\nIoT systems can produce or use data from diverse sources which can be categorized as static and dynamic. Static\ndata refers to data that is not subject to frequent changes such as system models while dynamic data is subject to\nfrequent changes, for instance, sensor data. The static part of IoT systems is increasingly represented as knowledge\ngraphs [Rhayem et al., 2020, Karabulut et al., 2024], large databases of structured semantic information [Hogan\net al., 2021]. ARM algorithms are often run on the dynamic part of IoT data, not utilizing the valuable information\nin knowledge graphs. In addition, ARM algorithms can generate a high number of rules as the input dimension\nincreases [Kaushik et al., 2023, Telikani et al., 2020], which is time-consuming to process and maintain. Generating\na high number of rules can be the case for large-scale IoT environments, as each sensor is treated as a different data\ndimension.\nTo address these two issues, this paper presents two new contributions. The first contribution is a novel ARM pipeline\nfor IoT data that combines knowledge graphs and sensor data to learn association rules with semantic properties,\nsemantic association rules (Section Problem Statement), that represent IoT data as a whole (Section Pipeline). We\nhypothesize that semantic association rules are more generically applicable than association rules based on sensor data\nonly, requiring fewer rules to have full data coverage. As an example, an association rule based on sensor data only\nlooks as follows: 'if sensorl measures a value in range R, then sensor2 must measure a value in range R2'. This rule"}, {"title": "2 Related Work", "content": "This section introduces the related work and background concepts."}, {"title": "2.1 Association Rule Mining", "content": "ARM is the problem of learning commonalities in data in the form of logical implications, e.g., $X \\rightarrow Y$, which is\nread as 'if X then Y'. Initial ARM algorithms such as Apriori [Agrawal et al., 1994] and HMine [Pei et al., 2001]\nfocused on mining rules from categorical datasets. The initial methods needed pre-discretization for numerical data,\nstruggled with scaling on big high-dimensional data, and produced a high number of rules that are costly to post-process.\nFP-Growth [Han et al., 2000], a widely used ARM algorithm, has many variations to tackle some of the aforementioned\nissues. ARM with item constraints [Srikant et al., 1997] is an ARM variation that focuses on mining rules for the\nitems of interest rather than all, which reduces the number of rules and execution time [Baralis et al., 2012]. Guided\nFP-Growth [Shabtay et al., 2021] is an FP-Growth variation for ARM with item constraints. Other variations include\nParallel FP-Growth [Li et al., 2008] and FP-Growth on GPU [Jiang and Meng, 2017] for better execution times.\nRecently, a few DL-based ARM algorithms have been proposed. Patel et al. [Patel et al., 2022] proposed to use\nAutoencoders [Chen and Guo, 2023] to learn frequent patterns in a grocery dataset, however, no source code or pseudo-\ncode was given. Berteloot et al. [Berteloot et al., 2023] also utilized Autoencoders (ARM-AE) to learn association rules\ndirectly from categorical tabular datasets. However, ARM-AE has fundamental issues while extracting association rules\nfrom an Autoencoder, which we elaborate on in Section Setting 2: Aerial vs state-of-the-art."}, {"title": "2.2 Association Rule Mining in Internet of Things", "content": "In IoT, both exhaustive ARM, such as Apriori and FP-Growth, and the optimization-based NARM methods are used for\nvarious tasks. Shang et al. [2021] utilized the Apriori algorithm for big data mining in IoT in the enterprise finance\ndomain for financial risk detection. Sarker and Kayes [2020] utilized an exhaustive ARM approach with item constraints"}, {"title": "3 Problem Definition", "content": "This research problem relates to learning association rules from sensor data in IoT systems with semantic properties\nfrom a knowledge graph describing the system and its components.\nGiven a sensor dataset T with sensors mapped to nodes in knowledge graph G with binding B, produce a set of association\nrules with clauses based on T and G. Association rules are formal logical formulas in the form of implications, e.g.\nX \u2192 Y, where X \u2192 Y is a horn clause with |Y| = 1 referring to a single literal and |X| > 1 referring to a set of\nliterals. X is referred to as the antecedent, and Y is the consequent. A horn clause is defined as a disjunction of literals\nwith at most one positive literal. Note that p \u2192 q\u0245r can be re-written as p \u2192 q and p\u2192 r, hence |Y = 1.\nNote that the T is converted to a set of transactions before the learning process, e.g., by grouping sensor data based on\ntime frames. G is in the form of a directed property graph which contains semantic information of the items in T, e.g.,"}, {"title": "3.1 Input", "content": "This section presents input notation. To help readers understand easier, Table 1 lists symbols used in the notation,\nhigh-level explanations, and examples from the water network domain.\nKnowledge graph. The knowledge graph described in this section is a property graph with an ontology or data schema\nas the underlying structure [Tama\u0161auskait\u0117 and Groth, 2023]. We adapt the definition for a property graph, given in the\nnext paragraph, from [Hogan et al., 2021].\nProperty Graph. Let Con be a countably infinite set of constants. A property graph is a tuple $G =\n(V, E, L, P, U, e, l,p)$, where $V \\subseteq Con$ is a set of node IDs, $E \\subseteq Con$ is a set of edge IDs, $L \\subseteq Con$ is a set\nof labels, $P C Con$ is a set of properties, $U \\subseteq Con$ is a set of values, $e : E \\rightarrow V \u00d7 V$ maps an edge ID to a pair of\nnode IDs, $I : VUE \\rightarrow 2^L$ maps a node or edge ID to a set of labels, and $p : VUE \\rightarrow 2^{P\u00d7U}$ maps a node or edge ID\nto a set of property-value pairs.\nOntology/Data Schema. Let $O = (C, R, A, r, a)$ be an ontology or data schema, where $C C Con$ is a set of classes,\n$RC Con$ is a set of relations, $A C Con$ be a set of properties, $r : R \\rightarrow C \u00d7 C$ maps a relation to a pair of classes, and\n$a:CUR \\rightarrow 2^P$ maps a class or a relation to a set of properties.\nTo express that G has O as its underlying structure, we define; i) L C CUR, meaning that the labels in G can only be\none of the classes or relations defined in O, ii) PC A, meaning that the properties of V and E in G, can only be one of\nthe properties in A.\nSensor data. We define sensor data generically as a tuple $T = (M, S, F, s)$, where M C (RU Con) is either real\nnumbers representing numerical sensor measurements or constants representing categorical sensor values (states, e.g.,\na door is open or closed), S \u2286 Con is a set of sensor IDs, F is an ordered numerical sequence of timestamps and\n$s: (S, F) \u2192 M$ maps every sensor ID and timestamp to a value. Note, further in this approach, the order of timestamps\nis considered only to aggregate sensor measurements into transactions (of time frames) to enable generalizable rule\nlearning, since the task is not to learn temporal rules.\nBinding. It is a tuple B = (V, S, b), where V is the set of node IDs from G, and S is the set of sensors IDs from T,\n$b: S\u2192 V$ maps each sensor ID to a node in G, and $b(S) \u2286 V$ meaning that there is a node ID for each sensor ID, and\nthere can be node IDs for more e.g., instances of classes in C."}, {"title": "3.2 Output", "content": "The output is a set of rules of the form described below.\nLet I be a set of items. We define the following forms for an item, which are basic comparison operations: Vi' \u2208\nI(((i' = (p'#z')) v (i' = (m'#z')) v (i' = (v{ = l')) v (i' = (e{ = l')) \u2228 (i' = (v' \u2192 v\" = e'))), with p' \u2208 P,\nm' \u2208 \u039c, \u03bd', \u03bd\" \u2208 V, e' \u2208 E, l', v\u00ed, e{ \u2208 L where v\u00ed refers to a label mapped to a node with the ID v', and ef refers to a\nlabel mapped to an edge with the ID e'. z' refers to a value that is either categorical or numerical, # refers to one of the\ncomparison operations with a truth value defined below:\n#categorical(p, 9) ::= (p = g)|(p \u2260 g)|(p \u2208 {g})|(p \u2209 {g}) #numerical(p, g) ::= (p = g)|(p \u2260 g)|(p > g)|(p <\ng)|(p \u2264 g)|(p \u2265 g)\nX \u2192 Y is an association rule where (X, Y \u2286 I) ^ (|Y| = 1). This means that items of the rule can only consist of\nproperties of classes or relations defined in the ontology, and the consequent can only have 1 item. Examples and"}, {"title": "4 Semantic Association Rules from IoT Data", "content": "This section introduces our proposed ARM pipeline for IoT data and an Autoencoder-based Neurosymbolic ARM\napproach (Aerial) as part of the pipeline. The goal is to learn a concise set of high-quality semantic association rules\nfrom sensor data and knowledge graphs with full coverage over the data."}, {"title": "4.1 Pipeline", "content": "Figure 1 depicts the proposed full pipeline of operations. First, sensor data is aggregated into time frames (e.g., average\nmeasurements per minute), hence, forming transactions. Each row in the Sensor Data depiction in Figure 1 refers to a\ntransaction, representing the state of the IoT system at a certain moment in time. Second, binding B is utilized to enrich\nsensor data with semantics from the knowledge graph. Let j be the number of sensors in S, i be the number of semantic\nproperty values in U mapped to each s1..j, z be the number of classes per input feature for simplicity, and n be the\nnumber of transactions. In practice, i and z usually are different per 81..j, and property values p \u2208 U can be different\nper transaction if G changes over time. Property values from neighbors of node v can also be in the transaction set\ndepending on the application.\nThird, in the vectorize step, semantically enriched sensor data is then one-hot encoded and fed into an under-complete\ndenoising Autoencoder [Vincent et al., 2008]. The Autoencoder creates a neural representation of the input data. Our\nAutoencoder architecture is described in Autoencoder Architecture section and the training process is described in\nTraining and Execution section. Input transactions to the Autoencoder look as follows:\n[{1, ..., 1, P111,..., P111, ..., P11, 1},\n{mn,..., mn, P11,\n}\n..., mPn},\nz\nmnz\n\nThe final step is to extract association rules from a trained Autoencoder which is described in Section Rule Extraction\nfrom Autoencoders. Note that some parts of the architecture are kept flexible as they may vary depending on the\ndownstream task that the proposed approach is applied to, such as the type of discretization, sensor data aggregation,\nencoding, etc."}, {"title": "4.2 Autoencoder Architecture", "content": "We employ an under-complete denoising Autoencoder [Vincent et al., 2008] which creates a lower dimensional\nrepresentation of the noisy variant of its input (encoder) and then reconstructs the noise-free input from the dimensionally\nreduced version (decoder). In this way, the model learns a neural representation of the input data and becomes more\nrobust to noise. Our under-complete denoising autoencoder has 3 layers for encoding and decoding units. During\ntraining, $tanh(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}$ is preferred in the hidden layers and $softmax(zi) = \\frac{e^{zi}}{\\sum_{j=1}^{n} e^{zj}}$ preferred at the output\nlayer, as activation functions. The softmax function is applied per category of features so that probabilities per class\nvalues are obtained for each category. As the lost function, aggregated binary cross-entropy loss, $BCE\\_Loss =\\newline \\frac{1}{n} \\sum_{i=1}^{n} - (yi \\cdot log(pi) + (1 - yi) \\cdot log(1 - pi))$, is applied to each feature to calculate the loss between Autoencoder\nreconstruction and the initial noise-free input. The training process is described in Section Training and Execution."}, {"title": "4.3 Rule Extraction from Autoencoders", "content": "The last step of our pipeline is to extract association rules from a trained Autoencoder using Algorithm 1. Aerial is\na Neurosymbolic approach to rule mining as it combines a neural network (an Autoencoder) and an algorithm that\ncan extract associations in the form of logical rules from a neural representation of input data created by training the\nAutoencoder. Note that any other ARM algorithm can be used within the pipeline after the semantic enrichment.\nIntuition: Aerial exploits the reconstruction loss of a trained Autoencoder to learn associations. If reconstruction for an\ninput vector with marked features is more successful than a similarity threshold then we say that the marked features\nimply the successfully reconstructed features. Marking features is done by assigning 1 (100%) probability to a certain\nclass value for a feature, 0 to the other classes for the same feature, and assigning equal probabilities to the rest of the\nfeatures in an input vector.\nAlgorithm: The rule extraction algorithm is given in Algorithm 1. The parameters are the set of input vectors (input), a\ntrained Autoencoder (ae), a similarity threshold (sim_threshold), and a maximum number of antecedents (antecedents)\nthat the rules will contain. Based on the antecedents, in line 3, the algorithm creates combinations of features to\nbe tested (test_ftrs), for instance, to test whether values of features f\u2081 and f2 are associated with other features, a\ntuple of (f1, f2) is created. Lines 4-13 go through each feature tuple (ftr_list) in the test features and first create an\ninitial test vector with all equal probabilities per feature (line 5). Line 6 marks feature values in the ftr_list with a\nprobability of 1, and returns a list of test vectors (test_vectors). Lines 7-13 perform a forward run per test vector and; \ni) check whether output probabilities for the marked features are higher than the given threshold (lines 9-10), ii) find\nfeatures (other than marked features) that have higher probability than the given threshold, which are added to the rule\nlist as consequences together with the marked features which are the antecedents (lines 11-13). The algorithm's time\ncomplexity is O(($f\\choose a$)), where f represents the number of features and a denotes the maximum number of antecedents\n(see the section below for details)."}, {"title": "4.3.1 Time Complexity Analysis of Aerial.", "content": "This section provides a time complexity analysis of our Aerial approach, Algorithm 1, in big O notation. We analyze\neach line in the algorithm and aggregate the results at the end.\nLine 2 initializes the rules array, therefore it is O(1).\nLine 3 is a combination operation over the input features, input.features, taken antecedent at a time. Let's assume f is\nthe total number of features, and a is the maximum number of antecedents parameter, then the complexity is O(($f\\choose a$)).\nLine 4 iterates over the test_ftrs. Therefore, the operations inside the loop are repeated ($(\\frac{f}{a})$) times.\nLine 5 initiates a vector with equal probabilities per feature class values. It is linear over the feature count, O(f).\nLine 6 creates a set of vectors in which class values of the features in ftr_list are marked with 1. In the worst-case\nscenario, this step is linear over features when the ftr_list is equal to all of the features in the input dataset, hence,\nO(f).\nAssuming that line 6 generated m vectors, line 7 iterates m times over the generated vectors.\nLine 8 performs a forward pass with the given test vector. Since each forward pass performs a softmax operation\nover the class values of features, this operation is linear over the number of features, O(f), assuming that softmax is\nperformed in O(1).\nLines 9 and 10 perform a comparison operation to check whether probabilities inside the out_probs array that\ncorresponds to the marked features are higher than a threshold or not. Assuming the worst-case scenario, this operation\nis repeated for each feature in the input data, O(f).\nAggregation of the results:\n1. The outer loop runs $(f\\choose a)$ times.\n2. For each iteration of the outer loop, lines 5 and 6 create an initial vector with equal probabilities and mark\nsome of the features in O(f) time.\n3. The middle loop (line 7) runs over the m test vectors. A forward pass and the probability check in lines 8-10\nare performed in O(f) time.\n4. The inner-most loop (line 11) runs in O(f) time.\nTherefore, the complexity is O(($f\\choose a$)) \u00d7 O(f) \u00d7 O(f \u00d7 m) \u00d7 O(f). Assuming that m is linear over the number of\nfeatures f, and $(f\\choose a)$ being the most expensive operation, the time complexity of Algorithm 1 is O(($f\\choose a$))."}, {"title": "5 Evaluation", "content": "Two different experimental settings are used to evaluate the two main contributions of this paper; i) evaluation of\nutilizing semantics with sensor data for ARM in comparison to ARM on sensor data only, and ii) evaluation of the\nproposed Aerial approach in comparison to state-of-the-art ARM algorithms.\nThis section first describes common elements across both settings such as datasets, and then describes setting-specific\npoints including baselines. Additional experiments that are not directly relevant to the two settings are given in Appendix\nAdditional Experiments."}, {"title": "5.1 Setup", "content": "This section describes the common elements for both of the evaluation settings.\nDatasets. 3 open-source IoT datasets from two different domains, water networks and energy, are used for all the\nexperiments. A knowledge graph is created per dataset by mapping metadata about each component to domain-specific\ndata structures. LeakDB [Vrachimis et al., 2018] is an artificially generated realistic dataset in water distribution\nnetworks. It contains sensor data from 96 sensors of various types, and semantic information such as the formation\nof the network, sensor placement, and properties of components. L-Town [Vrachimis et al., 2020] is another dataset\nin the water distribution networks domain with the same characteristics. It has 118 sensors. LBNL Fault Detection\nand Diagnostics Dataset [Granderson et al., 2022] contains sensor data from 29 sensors and semantics for Heating,\nVentilation, and Air Conditioning (HVAC) systems. As semantic properties, it only includes a type property.\nTraining and Execution. The Aerial Autoencoder is trained for each dataset. The training parameters found via\ngrid search are as follows: learning rate is set to 5e-3, the models are trained for 5 epochs, Adam [Kingma and Ba,\n2014] optimizer is used for gradient optimization with a weight decay of 2e-8, and the noise factor for the denoising\nAutoencoder is 0.5. All experiments are repeated 20 times over 20 randomly selected sensors for each dataset, and the\naverage results are presented unless otherwise specified. The random selection is done by picking a random sensor\nnode on the knowledge graph, and traversing through the first, second, etc. neighbors until reaching 20 sensor nodes.\nEqual-frequency discretization [Foorthuis, 2020] with 10 intervals is used for numerical features for the methods that\nrequire pre-discretization (Table 3).\nEvaluation Metrics. The most common way of evaluating ARM algorithms is to measure the quality of the rules from\ndifferent aspects as there is no single criterion that fits all cases. In the evaluation, we used the standard metrics in\nARM literature which are support, confidence, data coverage, number of rules, and execution time [Kaushik et al., 2023,\nTelikani et al., 2020]. In addition, we selected Zhang's metric [Yan et al., 2009] to evaluate the association strength of\nthe rules, commonly used in many open-source libraries including MLxtend [Raschka, 2018] and NiaARM [Stupan and\nFister, 2022]. The definitions are given below:\n\u2022 Support: Percentage of transactions with a certain item or rule, among all transactions (D): $support(X \u2192\\nY) = \\frac{|X\\cup Y|}{|D|}$\n\u2022 Confidence: Conditional probability of a rule, e.g., given the transactions with the antecedent X in, the\nprobability of having the consequent Y in the same transaction set: $confidence(X \u2192 Y) = \\frac{|X\\cup Y|}{|X|}$\n\u2022 Rule Coverage: Percentage of transactions that contains antecedent(s) of a rule: $coverage(X \u2192 Y) =$\n$support(X)$.\n\u2022 Data Coverage: It refers to the percentage of transactions to which the learned rules are applicable.\n\u2022 Zhang's Metric: This metric also considers the case in which the consequent appears alone in the trans-\naction set, besides their co-occurrence, and therefore measures dissociation as well. A score of > 0\nindicates an association, 0 indicates independence and < 0 indicates dissociation: $zm(X \u2192 Y) =\n\\frac{confidence(X\\rightarrow Y)-confidence(X'\\rightarrow C)}{max(confidence(X\\rightarrow Y),confidence(X'\\rightarrow Y))}$ in which X' refers to the absent of X in the transaction set."}, {"title": "5.2 Experimental Settings", "content": "This section describes the two core experimental settings together with baselines in each setting. Please refer to Table 3\nfor baseline methods described in the settings below."}, {"title": "5.2.1 Setting 1: Semantics vs without Semantics.", "content": "To show that semantics can enable learning more generically applicable rules, two different ARM algorithms, our\nAerial approach and a popular exhaustive method FP-Growth [Han et al., 2000], are run with and without semantically\nenriched sensor data. Two algorithms are used to show that including semantics is beneficial regardless of the ARM\nmethod applied. The results are compared based on the number of rules, average rule support, confidence and coverage,\nand execution time. FP-Growth is implemented using MLxtend [Raschka, 2018]."}, {"title": "5.2.2 Setting 2: Aerial vs state-of-the-art.", "content": "The goal is to evaluate the proposed Aerial method for IoT data, and the experiments are run on sensor data with\nsemantics. The only existing semantic ARM approach Naive SemRL [Karabulut et al., 2023] is chosen as a baseline and\nexecuted with the exhaustive FP-Growth (as in the original paper) and HMine algorithms. In addition, the optimization-\nbased NARM method TS-NARM [Fister Jr et al., 2023] with standard confidence metric as optimization goal is run\nwith 5 algorithms (as in the original paper), Differential Evolution (DE) [Storn and Price, 1997], Particle Swarm\nOptimization (PSO) [Kennedy and Eberhart, 1995], Genetic Algorithm (GA) [Goldberg, 2013], jDE [Brest et al.,\n2006], and LSHADE [Viktorin et al., 2016]). TS-NARM is implemented using NiaPy [Vrban\u010di\u010d et al., 2018] and\nNiaARM [Stupan and Fister, 2022], and FP-Growth and HMine are implemented using Mlxtend [Raschka, 2018]. All\nrule quality criteria described earlier are used in the comparison."}, {"title": "5.2.3 Challenges in comparison.", "content": "The distinct nature of different types of algorithms makes comparability a challenge. The exhaustive algorithms can\nfind all rules with a given support and confidence threshold. The execution time of the 5 optimization-based approaches\n(TS-NARM) is directly controlled by the pre-set maximum evaluation parameter. And running them longer leads\nto better results up to a certain point (Section Aerial vs state-of-the-art). The quality of the rules learned by the\nDL-based ARM approaches depends on the given similarity threshold parameter (or likeness for ARM-AE). Given\nthese differences, we made our best effort to compare algorithms fairly and showed the trade-offs under different\nconditions. Table 4 lists the parameters of each algorithm for both of the settings, unless otherwise specified. For\nTS-NARM, the population size is set to 200 which represents an initial set of solutions, and the maximum evaluation is\nset to 50,000 which represents the number of fitness function evaluations before convergence. The parameters of the 5"}, {"title": "5.3 Experimental Results", "content": "This section presents the experimental results for both settings."}, {"title": "5.3.1 Setting 1: Semantics vs without Semantics.", "content": "Experiment 1.1: Rule Quality. Table 5 shows the results for running Aerial and FP-Growth with (w-s) and without\n(wo-s) semantic properties. Average support and rule coverage for both algorithms on all datasets increased significantly\nupon including semantics. The rule count is increased for FP-Growth with semantics, while it decreased with our Aerial\napproach. The confidence values did not change significantly.\nThe results indicate that association rules learned from sensor data and semantics are more generically applicable than\nrules learned from sensor data only, as the support and rule coverage values are significantly higher. Furthermore, this\nexperiment is repeated with varying numbers of sensors, and the results (Experiment 4 in Appendices) show that a\nhigher number of sensors results in more generically applicable rules. The comparison of rule count and confidence for\ndifferent approaches will be investigated in Experimental Setting 2."}, {"title": "5.3.2 Setting 2: Aerial vs state-of-the-art", "content": "Experiment 2.1: Execution Time and Number of Rules Analysis. This experiment investigates how execution time\nand the number of rules change for the proposed Aerial approach and baselines depending on their relevant parameters.\nThe exhaustive methods' execution time and num-\nber of rules they mine are strictly dependent on the\npreset minimum support threshold and the number\nof antecedents. Figure 4 shows how the number\nof rules and execution time change based on an-\ntecedents (for 1, 2, 3, and 4 antecedents) and mini-\nmum support thresholds (for 0.05, 0.1, 0.2 and 0.3).\nThe results show that the execution time increases\nas the support threshold decreases and the number\nof rules increases above 10 million for LeakDB\nwhile it reaches 1-2 million for LBNL and L-Town\ndatasets which are highly costly to post-process.\nSimilarly, as the number of antecedents increases\nthe number of rules reaches the levels of millions,\nwhile the execution time reaches minutes. The ex-\necution did not terminate for the LeakDB dataset\nwhen using 4 antecedents after 30 minutes.\nExecution time, number of rules as well as the qual-\nity of the rules mined by the optimization-based\nmethods (TS-NARM) strictly depend on the num-\nber of evaluations. Table 7 shows the effect of the\nmaximum evaluations parameter on the execution\ntime, number of rules, and confidence of the rules\nfor the LeakDB dataset (the results are consistent\nacross datasets, see Experiment 5 in Appendices).\nThe results show that longer executions lead to a\nhigher number of rules with higher confidence for\nall 5 algorithms. 50,000 is chosen as the maximum\nevaluation for the rule quality experiment (Exper-\niment 2.2) as this is also the case in the original\npaper.\nLastly, the rule extraction time of the proposed Aerial approach is affected by the number of antecedent parameters, as\nit increases the number of test vectors used in the algorithm. Figure 5 shows the effect of increasing the number of\nantecedents on the number of rules and execution time. The number of learned rules is 10-100 times lower than the\nexhaustive methods. Exhaustive methods run slower on datasets with low support rules, LeakDB (see Tables 5 and 8),"}, {"title": "5.4 Discussion", "content": "This section discusses and summarizes the experimental findings.\nSemantics for generalizability. The results in Experimental Setting 1 showed that learning association rules from\nboth static and dynamic data in IoT systems results in rules that have higher support and data coverage and, therefore,\nare more generically applicable than rules learned from sensor data only. The experiments also showed that including\nsemantics is beneficial regardless of the ARM approach as the results were similar for both exhaustive FP-Growth and\nour proposed Aerial approach.\nNeurosymbolic methods can help learning a concise set of high-quality rules. As semantic enrichment of sensor\ndata increases data dimension, current ARM methods result in a higher number of rules which is already identified as a\nresearch problem in the ARM literature. As an alternative, our proposed Neurosymbolic Aerial approach can learn a\nconcise number of rules with full data coverage, high confidence, and association strength, which is demonstrated in\nExperimental Setting 2. We believe that there is a potential in the direction of neurosymbolic rule learning, and Aerial\nis a strong initial step.\nExecution time. Semantic enrichment increases execution time by 2-3 times for Aerial and 3-12 times for exhaustive\nmethods, as shown in Experiment 2.1. However, semantic association rules have higher support and rule coverage, and\na substantially smaller number of them can have full data coverage, therefore we argue that the increment is acceptable.\nThe exhaustive methods perform poorly on low-support (LeakDB) datasets with a low minimum support threshold\nand also perform poorly with a high number of antecedents as demonstrated in Experiment 2.1. This experiment also\nshowed that Aerial runs faster than the exhaustive methods on low-support datasets and Aerial's execution time does\nnot depend on the datasets' support characteristics. Note that the Aerial can be parallelized and run on GPU (similar to\nthe exhaustive methods). The optimization-based methods' execution time is directly controlled by the preset maximum\nevaluation parameter. Longer executions are required to obtain higher-quality rules and this also results in a high number\nof rules, which are costly to process and maintain. Aerial is faster than the optimization-based methods for learning\nrules with low-to-medium-size antecedents (1 to 4). Note that the number of antecedents for the optimization-based\nmethods can not be controlled.\nVariations of Aerial. Many existing ideas in ARM literature can be integrated into our Aerial approach. For instance, in\nARM with item constraints, rules of interest are described using a taxonomy or an ontology and then ARM algorithms\nfocus on those rules only which speeds up the execution and leads to a smaller number of rules [Srikant et al., 1997,\nBaralis et al., 2012"}]}