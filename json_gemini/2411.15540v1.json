{"title": "Optical-Flow Guided Prompt Optimization for Coherent Video Generation", "authors": ["Hyelin Nam", "Jaemin Kim", "Dohun Lee", "Jong Chul Ye"], "abstract": "While text-to-video diffusion models have made significant strides, many still face challenges in generating videos with temporal consistency. Within diffusion frameworks, guidance techniques have proven effective in enhancing output quality during inference; however, applying these methods to video diffusion models introduces additional complexity of handling computations across entire sequences. To address this, we propose a novel framework called MotionPrompt that guides the video generation process via optical flow. Specifically, we train a discriminator to distinguish optical flow between random pairs of frames from real videos and generated ones. Given that prompts can influence the entire video, we optimize learnable token embeddings during reverse sampling steps by using gradients from a trained discriminator applied to random frame pairs. This approach allows our method to generate visually coherent video sequences that closely reflect natural motion dynamics, without compromising the fidelity of the generated content. We demonstrate the", "sections": [{"title": "1. Introduction", "content": "Recently, diffusion models have become the de-facto standard for image generation. In particular, text-to-image (T2I) diffusion models [7, 27] have gained significant attention due to their ability to enable users control over the generation process via text prompts.\nBuilding on these advancements, research has now progressed toward text-to-video (T2V) diffusion models [3, 13, 34] that generate videos which are both visually engaging and contextually coherent. Despite these advances, achieving temporally consistent videos remains a challenge in many T2V models. Although recent work has attempted to address this, many proposed methods rely on fine-tuning [10] or introduce unnecessary interventions that can potentially adversely impact [4, 19].\nIn diffusion frameworks, guidance is a commonly used technique to obtain samples aligned with specific objectives during inference. For example, in diffusion inverse solvers (DIS) [5, 18, 31], the guidance is usually given in the form of the likelihood function from the data consistency terms. However, a key technical issue in diffusion model guidance is that intermediate samples are corrupted by the additive Gaussian noise from the forward diffusion processes, making the computation of the likelihood term computationally prohibitive [5]. To address this issue, diffusion posterior sampling (DPS) [5] approximates the likelihood function around the posterior mean, computed using Tweedie's formula [9]. Another way to applying guidance is fine-tuning of the underlying diffusion models. For example, in DiffusionCLIP [17], the diffusion model for reverse sampling is fine-tuned using a directional CLIP guidance to generate images that align with a target text prompt.\nHowever, applying these well-established techniques to video diffusion models (VDM) poses significant technical challenges. Unlike image generation, VDMs require the modeling of dependencies of across frames. For instance, if DPS is applied to VDMs, backpropagation must occur across all frames, making the process not only computationally expensive but also prone to instability. Applying fine-tuning of a VDM is even more challenging due to the model's large size. As a result, there remains a lack of a reliable, computationally efficient guidance mechanism that ensures temporal coherence across frames-a crucial factor in generating realistic videos.\nRecently, semantic-preserving prompt optimization method has been proposed for generating minority images [33]. This approach integrates learnable tokens into a given prompt P and updates their embeddings on-the-fly during inference based on specific optimization criteria.\nWhile the original motivation of this work was different from reducing computational burden, we found that this on-the-fly prompt optimization concept aligns well with VDM guidance as a novel and computationally efficient guidance method, as the text prompt can influence all frames simultaneously.\nSpecifically, our new method, MotionPrompt, is designed to enhance temporal consistency in video generation while preserving the semantics through inference-time prompt optimization. By leveraging the global influence of the text prompt, MotionPrompt reduces the computational demands of entire sequence guidance, enabling indirect control over the latent video representation through gradients computed from only a subset of frames. Specifically, we append a placeholder string S to the prompt P similar to [33], which serves as a marker for the learnable tokens. This approach preserves desired attributes across the video sequence and maintain the semantic meaning of the original prompt.\nTo further enhance temporal coherence in generated videos while preserving semantic integrity, our method incorporates a discriminator that uses optical flow to evaluate temporal consistency and guide prompt optimization. Specifically, we first train a discriminator to distinguish optical flow between random pairs of frames from real and generated videos. During sampling, a subset of generated frames is evaluated by the discriminator to assess the realism of their relative motion, as measured by optical flow. This process enables MotionPrompt to refine generated frames, achieving more natural and realistic motion patterns. In summary, our contributions are as follows:\n\u2022 We propose MotionPrompt-a novel video guidance method that uses on-the-fly semantic prompt optimization to enhance temporal consistency and motion coherence in generated videos, without requiring diffusion models retraining or gradient calculations for every frame.\n\u2022 By utilizing an optical flow-based discriminator to guide prompt optimization, we enforce temporal consistency in generated videos, enabling smoother, more realistic motion by aligning flows with real-world patterns while minimizing impact on samples already close to real videos."}, {"title": "2. Related Works", "content": "Video Latent Diffusion Models. Video latent diffusion models [14] have gained attraction for their ability to efficiently generate videos by operating within a compressed latent space, thereby reducing the computational cost and memory demands associated with high-resolution video generation. Building on this approach, VideoCrafter [2, 3], AnimatedDiff [13] and Lavie [34] extended latent diffusion to handle text-to-video generation to produce contextually relevant and controllable video outputs. While these advancements have significantly improved video generation, ensuring temporal consistency remains challenging. FreeInit [37] alleviates this"}, {"title": "3. Main Contribution: MotionPrompt", "content": ""}, {"title": "3.1. Conditional Video Diffusion", "content": "Video latent diffusion models encode N-frame clean\nvideo, {x()}1 \u2208 RN\u00d7C\u00d7H\u00d7W, to {z}1 using an\nencoder &, where C, H and W represent channel, height\nand width of the video, respectively. Unless otherwise\n(i)\nnoted, we simply denote zo as {z}1 for convenience\nand zo ~ po(z).\nThe diffusion model aims to estimate the noise in the\nnoised latent zt from the forward diffusion process [7]\nq(zt zo) = N(zt; \u221a1 \u2013 \u0101t zo, \u0101t\u2160), (1)\nwhere at is the noise scheduling coefficient at timestep t.\nIn the text-to-video diffusion model, the text condition is\nprovided as an additional input. Given a prompt P, the text\nembedding c is obtained through the text encoder Etext, i.e.,\nc = Etext(P). Then, the training objective is to minimize\nEzo,e,t,c [||\u20ac - Eo (Zt, t, c) ||2], (2)\nwhere \u20ac ~ N(0, I), and ee(zt, t, c) denotes the diffusion\nmodel parameterized by @ with the text condition c and the\nnoisy latent zt at t. Once the diffusion model is trained,\nreverse diffusion sampling is performed. For example, in\nDDIM [30], the reverse diffusion follows:\n1\nZt-1 = \u221aat-12t + \u221a\u221a1 \u2013 At\u22121\u20ac0(zt, t, c)\n(3)\n2t =\n(zt - \u221a1 \u2013 ateo (zt, t, c)),\nVat\n(4)\nwhere 2t denotes the denoised sample at t, is obtained from\nTweedie's formula [9]. To enhance the impact of the text\ncondition, we applied classifier-free guidance (CFG) [15] to\nall e predictions. The modified prediction is given by:\n\u20ac(Zt, t, c) = Eo(Zt, t, \u00d8) + w [\u20aco (Zt, t, c) \u2013 Eo (Zt, t, \u00d8)],\n(5)\nwhere w denotes the guidance scale and \u00d8 represents the null\ntext prompt. Thus, unless specified otherwise, e\u0473(\u00b7) denotes\nterms with CFG applied.\nSo far, we have discussed text conditioned diffusion sampling. Moving beyond this, to navigate the sampling process\nin a way that minimizes a general loss function l(z), it is\nessential to find the solution on the correct clean manifold:\nmin l(z)\nZEM\n(6)\nwhere M represents the clean data manifold sampled from\nthe unconditional distribution po(z). In DPS [5], this is\nachieved by enforcing the updated estimate from the noisy\nsample xt \u2208 Mt to be constrained to stay on the same noisy\nmanifold Mt.\nZt-1 = \u221a\u0101t-1 (Zt - Yt\u2207z\u2081l(zt)) + \u221a1 \u2013 \u0101t\u22121\u20ac0(zt, t, c),\n(7)\nwhere Yt > 0 denotes the step size."}, {"title": "3.2. Prompt Optimization for Video Guidance", "content": "While direct guidance of the latent representation using\n(7) has proven effective in image generation, calculating\nVz\u2081l(2t) in the video domain poses significant challenges.\nSpecifically, calculating the gradient of all frames is compu-\ntationally expensive. Providing guidance for only selected\nframes may reduce memory usage, but this can disrupt frame-to-frame consistency, resulting in inconsistencies in appearance, motion, and coherence throughout the video.\nTo address this, we employ the prompt optimization\nmethod and extend it to capitalize the text prompt's influ-\nence across the entire video. This approach enables indirect\ncontrol of the latent video representation by using gradients\nderived from only a subset of frames, rather than necessitat-\ning gradients for every frame. Specifically, instead of using\n(6) for the latent, we introduce an inference-time optimiza-\ntion problem with respect to the text embedding c:\n\u0109t = arg min l(zt, c)\n(8)\nC\nOne of the most important advantages of this approach is\nthat it enables the use of a simple reverse diffusion process:\nZt-1 = \u221a\u0101t-1zt(\u0109t) + \u221a1 \u2013 \u0101t\u22121\u20ac0(zt,t,\u0109t) (9)\n2t(\u0109t) = (zt - \u221a1 \u2013 at\u2208o(zt, t, \u0109t)) /\u221aat\n(10)\nFurthermore, to preserve the semantic meaning of the\noriginal prompt, rather than optimizing the entire text em-\nbedding c, we follow the approach introduced in Um and Ye\n[33], attaching learnable token embeddings to the end of the\nprompt and optimizing only these embeddings. Specifically,\nwe first add new text tokens S = {Si}=1 to the tokenizer vo-\ncabulary and initialize their embeddings with words that can\nhelp improve video quality, such as \"authentic\" and \"real\".\nWe then append these learnable tokens to the end of the given\ntext prompt (e.g., \"White fox on the rock.\" \u2192 \"White fox\non the rock S\u2081 ... Sn.\"). We denote this modified prompt"}, {"title": "3.3. Defining Loss Function from Optical Flow", "content": "In this section, we introduce the objective function l(z)\nto ensure the temporal coherence in the generated video. The\ntotal objective function is formulated as follows:\nltotal (zt, T) := 11ldisc (zt, c(T)) + d2ltv (zt, c(T))\n+ 13||T - To||2,\nwhere A1, A2 and 3 are regularization parameters. The 12\nloss term in ltotal is to ensure that the optimized token embedding is not far from the original token embedding space. The\nother two loss terms ldisc and lry will be explained in detail\nsoon. See Algorithms for the pseudo-code of the generation\nprocesses with our prompt optimization.\nOptical flow discriminator loss. The main idea of Mo-\ntionPrompt is to utilize the realistic optical flow to guide\nthe diffusion model to generate temporally coherent video.\nUnfortunately, the primary challenge of utilizing optical flow\nin generation tasks lies in the inherent lack of paired supervised optical flow data. To address this, we aim to guide\nthe sampling process toward aligning the optical flow of the\ngenerated video with that of real videos."}, {"title": "Algorithm 1 MotionPrompt", "content": "Require: \u20ac0, P, T, Etext, D, TimeCond(t)T\n1: zT ~ N(0, I)\n2: fort T to 1 do\n3: if TimeCond(t) then\n4: c\u2190 OptEmb(zt, eo, P+ S, Etext)\n5: else\n6: c\u2190 Etext (P)\n7: end if\n8: et \u2190 CFG from (5)\n9: zt \u2190 Reverse sampling from (3)\n10: end for\n11: return D(zo)"}, {"title": "Algorithm 2 Prompt Optimization", "content": "1: function OPTEMB(Zt, \u20ac0, P + S, Etext)\n2: fork= 1 to K do\n3: c(T) \u2190 Etext(P + S)\n4: \u20act(c(T)) \u2190 CFG from (5)\n5: 2t(c(T)) \u2190 Tweedie's formula from (4)\n6: Select & Decode frames: (c(T)), x(c(T))\n7: f \u2190 OF(x+(c(T)), x(c(T)))\n8: ltotal \u2190 A1ldisc(f) + 12lTv (f)\n+13||T - To||2\n9: T\u2190T-\u0ed7\u2207TLtotal\n10: end for\n11: \u0109 Etext(P+S)\n12: return \u0109\n13: end function"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Implementation Details", "content": "Baselines. To evaluate our method across different\nframeworks, we test it with open-source text-to-video\ndiffusion models, including Lavie [34], AnimateDiff [13],\nand Videocrafter2 [3]. For AnimateDiff, we use the\nRealisticVision pre-trained model\u00b9 . We generate videos\nusing a DDIM sampler with 50 steps and 800 prompts from\nVBench [16], ensuring both the baselines and our method\nuse the same seed.\nDiscriminator training. For discriminator training, we\nsample videos from each model using the same set of 800\nprompts. Leveraging our prompt optimization, the discrimi-\nnator operates on single images rather than full videos. We\nfine-tune a pre-trained Vision Transformer (ViT) [8] as an im-\nage encoder, adding a projection layer to adapt two-channel\noptical flow for ViT and using a 3-layer MLP as the clas-\nsifier. This setup achieved rapid convergence in under 20\nepochs. Optical flow was extracted with RAFT [32] from\nreal videos selected from the DAVIS [26] and WebVid [1], as\nwell as from generated videos. Finally, we train and deploy\na separate discriminator for each video model. All training"}, {"title": "4.2. Results", "content": "Qualitative comparisons. We provide visual comparisons\nof our method against three baselines in Fig. 3. The baselines\nstruggle with maintaining temporal consistency, often failing\nto preserve the appearance or quantity of objects, and some-\ntimes resulting in objects that suddenly appear or disappear.\nIn contrast, the proposed framework effectively suppresses\nappearance changes and sudden shifts in video generation.\nAdditionally, our method maintains a consistent color tone\nacross all frames and accurately captures the scene attributes\nand details intended by the original prompts.\nQuantitative comparisons. For quantitative comparison,\nwe evaluate five key metrics from VBench [16]-subject\nconsistency, background consistency, temporal flickering,\nmotion smoothness, and dynamic degree-to assess improve-\nments in consistency and motion. We also measure overall\nconsistency to confirm that prompt optimization maintains\nfidelity to the text prompt. Table 1 shows that our method\nimproves object consistency, reduces temporal flickering,\nand enhances motion smoothness with minimal impact on\ntext alignment. We are aware that there may exist a trade-off\nbetween consistency and motion dynamics in the proposed\nmethod. However, visualization results demonstrate that our\nmethod balances dynamics and coherence effectively. See\nsupplementary materials for more details.\nUser study. Additionally, we conduct a user study to evaluate our method's effectiveness in overall quality, temporal quality, and text alignment. We select three videos from each of the three models, totaling nine videos, and 20 participants rated them on a scale of 1 to 5. As shown in Table 2, our method achieved higher ratings in all aspects, demonstrating its ability to produce visually appealing, temporally"}, {"title": "5. Additional Experiments", "content": "We performed additional analysis to support the effectiveness of our method and to analyze the design components of the proposed approach, focusing on the AnimateDiff [34]."}, {"title": "5.1. Token Variations", "content": "To demonstrate that the improvement in temporal consistency is not simply due to the addition of prompt, we measure the cosine similarity between the initial token embedding T and the optimized embedding at each timestep t. As shown in Fig. 4, the cosine similarity decreases with sampling step and the rate of change gradually converging. Additionally, we compare the average cosine similarity using the top 50 videos and lowest 50 videos based on the subject consistency scores. We observe that videos with higher consistency exhibited less token variation, indicating that our method performs less optimization on videos which the discriminator judges to be closer to real video, thereby preserving their original characteristics."}, {"title": "5.2. Ablation Study", "content": "Analysis of hyperparameters. In Table 3, we examine how key hyperparameters affect our framework's performance, focusing on the optimal number of iterations per optimization step and the most effective stage within the 50-step sampling process. For iteration count, a single iteration was insufficient, while 7 iterations smoothed motion but reduced dynamic degree and overall consistency, as well as increasing computation time. An effective balance was achieved with 3 iterations. Similarly, starting optimization early improved motion smoothness but reduced consistency, while delaying it too late weakened its impact. Based on these findings, we established an optimal range for applying optimization. Except for the hyperparameters being compared, all other settings use the optimal hyperparameters listed in the supplementary materials."}, {"title": "5.3. Extensions: Image-to-Video Diffusion Model", "content": "To further verify our framework's capabilities, we extended it to an image-to-video (I2V) diffusion model, DynamiCrafter [38], which also uses text prompts as input. Although the vanilla model produced relatively consistent videos due to the reference image, issues arose with differences in appearance details and artifacts around objects. When combined with our method during sampling, these issues were significantly mitigated (Fig. 6)."}, {"title": "6. Conclusion", "content": "In this work, we introduced MotionPrompt, addressing the fundamental challenge in text-to-video models: generating temporally consistent and natural motion. Specifically, we leveraged optical flow, specifically using a discriminator trained to distinguish between optical flow from real videos and that from generated (i.e., fake) videos. By incorporating text optimization, our approach effectively addressed inefficiencies in guiding video models. Qualitative and quantitative experiments demonstrated the effectiveness of our proposed method.\nLimitations. While our approach improves baseline model results, it requires slightly more generation time. However, optimization is applied to only 10 to 15 steps, keeping costs low relative to performance gains. Additionally, since our objective function is not grounded in physics, improvements may not always produce physically plausible outcomes. We anticipate that as foundational models capable of assessing physical plausibility using representations extracted from video-such as optical flow or point correspondence-advance, this limitation can be mitigated."}, {"title": "A. Implementation and Evaluation Details", "content": ""}, {"title": "A.1. Training Details of the Discriminator", "content": "The discriminator was trained to distinguish between real and generated optical flow representations, aiming to enhance temporal consistency in video generation. For training, we use a batch size of 32 and the SGD optimizer with a learning rate of 0.0005 and a momentum of 0.9. The training process spans approximately 20 epochs, with the model parameters from the epoch achieving the best validation loss being selected."}, {"title": "A.2. Hyperparameters for Evaluation", "content": "Table 4 lists the hyperparameters used for our quantitative evaluation. The '# of frames' denotes the number of video frames decoded into pixels and used for optical flow calculation. Specifically, if the value is 6, this indicates that 3 sets of adjacent frames were sampled, resulting in 3 optical flows being used to compute the corresponding loss."}, {"title": "A.3. User study", "content": "For the user study, we utilize videos presented in the paper and on the project webpage. Participants are asked to evaluate the videos based on the following questions: (1) Overall Quality: Does the generated video exhibit good overall quality? (2) Motion Smoothness: Are the motions and transitions in the generated video smooth? (3) Text Alignment: Does the generated video align well with the given textual conditions?"}, {"title": "B. Additional Analysis", "content": "In this section, we present additional analyses to evaluate the generalization capability and performance of our approach across various scenarios. The experiments primarily focus on AnimateDiff [13]."}, {"title": "B.1. Ablation on Token Optimization", "content": "Here, we conduct an ablation study to investigate the impact of various configurations related to token optimization on the overall performance (Table 5). Except for the factors being examined, all other settings were kept consistent with the values in Table 4.\nThe number of tokens First, we examine whether increasing the number of tokens enhances the optimization effect. Specifically, we added three additional tokens initialized as \"authentic\", \"real\" and \"clear\". Although this increased the factors that could potentially improve temporal quality, leading to an improvement in some temporal quality metrics, it also resulted in a decline in dynamic degree and overall consistency. Consequently, we determined that the benefits were not significant enough and chose to use a single token as the default setting.\nThe placement of tokens In addition, we investigate the effect of the learnable token's placement. Instead of appending the learnable token to the end of the given prompt, we placed it at the front of the prompt to evaluate its impact. Similar to the observations in Um and Ye [33], we also find that appending the token to the end of the given prompt is more effective. This aligns with the general practice of structuring sentences where content-related information is stated first, followed by descriptive elements like adjectives.\nRobustness to Initialization Words In main paper, we demonstrate that the cosine similarity with the initialization token starts sufficiently low and gradually decreases over time, indicating that the improvement is not merely a result of adding tokens. To further support this, we initialize the token with a seemingly unrelated word, 'the', and assess its impact on video generation quality. While the performance improvement was smaller compared to our final configuration, it still outperformed the baseline. This further reinforces the effectiveness and robustness of our method."}, {"title": "B.2. Exploring Discriminator Generalization", "content": "We primarily use a discriminator trained on paired data. In this setup, the model generating fake data for discriminator training matches the model used during inference. To explore the impact of the discriminator's training approach and evaluate its generalization capability, we conduct inference using a discriminator trained on a different dataset, referred to as cross-dataset inference.\nTable 6 presents the results of this evaluation. Surprisingly, we observe a general improvement in performance when using a discriminator trained on different data (i.e., videos generated by Lavie [34] and VideoCrafter2 [3]). We conjecture that this is because the baseline performance of Lavie and VideoCrafter2 is higher compared to AnimateDiff, making it more challenging for the discriminator to differentiate these videos from real ones. This likely resulted in stricter training, which may have contributed to the improved performance observed during inference. These findings suggest the potential for further performance enhancements through improved discriminator training strategies."}, {"title": "B.3. Synergies with the Existing Method", "content": "We demonstrate how our approach can be combined with orthogonal methods to achieve enhanced performance. For instance, while FreeInit [37] focuses on initializing noise, our method emphasizes guidance through prompt optimization. These complementary mechanisms allow the two approaches to work synergistically. In this section, we present the results obtained by using both methods together, highlighting their combined potential.\nWhile FreeInit significantly improves temporal quality, it does so at the expense of overall video quality. Specifically,"}, {"title": "C. Additional Results", "content": "In this section, we provide additional result images to further demonstrate the performance and effectiveness of our approach across different models. First, we present additional results for DynamiCrafter [38], an image-to-video model that takes prompts as input (Fig. 8). Furthermore, we provide results for AnimateDiff [13], Lavie [34], and VideoCrafter2 [3].\nWe observe that our method enhances temporal quality without significantly altering the generated content across these three models. Notably, in Lavie [34], the last frame often deviates significantly from the preceding frames (see rows 3, 6, and 9 in Fig. 10). Our approach effectively mitigates this issue to a large extent."}]}