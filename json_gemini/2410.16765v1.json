{"title": "Survival Models: Proper Scoring Rule and Stochastic Optimization with Competing Risks", "authors": ["Julie Alberge", "Vincent Maladi\u00e8re", "Olivier Grisel", "Judith Ab\u00e9cassis", "Ga\u00ebl Varoquaux"], "abstract": "When dealing with right-censored data, where some outcomes are missing due to a limited observation period, survival analysis-known as time-to-event analysis focuses on predicting the time until an event of interest occurs. Multiple classes of outcomes lead to a classification variant: predicting the most likely event, a less explored area known as competing risks. Classic competing risks models couple architecture and loss, limiting scalability.\nTo address these issues, we design a strictly proper censoring-adjusted separable scoring rule, allowing optimization on a subset of the data as each observation is evaluated independently. The loss estimates outcome probabilities and enables stochastic optimization for competing risks, which we use for efficient gradient boosting trees. SURVIVALBOOST not only outperforms 12 state-of-the-art models across several metrics on 4 real-life datasets, both in competing risks and survival settings, but also provides great calibration, the ability to predict across any time horizon, and computation times faster than existing methods.", "sections": [{"title": "INTRODUCTION", "content": "We all die at some point. Some applications call for predicting not if but when an event of interest is likely to occur. In such a setting of time-to-event regression, samples often have unobserved outcomes, e.g. individuals that have not been followed long enough for the event of interest to occur. Limiting the analysis to fully observed samples creates a censoring bias. To address this, survival analysis models use dedicated corrections for censorship. These have long been central to health applications. Nowadays, survival analysis is also used in diverse fields, such as predictive maintenance, or user-engagement studies. Survival analysis has led to many dedicated models, such as the Kaplan and Meier estimator or the Cox proportional hazard model.\nCompeting risks analysis generalizes survival analysis to multiple events, determining which will happen first. For instance, if a breast-cancer patient dies from a different cause, it is impossible to determine when they would have succumbed to cancer, regardless of the duration of the observation period. The caregiver may also want to adapt the treatment if it is predicted that the patient will die of a competing event, such as a heart attack, sooner than from cancer. As the risks of the various events are seldom independent-for example, cancer and cardiovascular disease share inflammation or age risk factors-competing risks cannot be solved by running a survival model for each event. The estimated risk of an event of interest will be biased if the competing risks are not included. Hence, adequate models for these risks are critical for decision-making.\nSurvival models have traditionally been developed with ad hoc adjustments for censoring. The most common approach is to design a likelihood using the probability of censoring per unit time-i.e. the time-derivative of the risk-which either comes with strong parametric assumptions or ad hoc corrections. Given that the risk, which is the probability of the outcome at a specific time, is crucial for various applications, it is preferable to use proper scoring rules, that directly control probabilities, as"}, {"title": "RELATED WORK", "content": "Survival settings Various survival models have been developed, ranging from approaches like the Kaplan and Meier estimator, which estimates the general survival curve for an entire population, to models that account for covariates. The Cox Proportional Hazards Model, a linear model of the hazards, which represents the instantaneous probability of an event, i.e., the logarithmic derivative of outcome probabilities over time. More complex models have been adapted to the survival setting: Support Vector Machines, survival games and Neural networks with DeepSurv or PCHazard. While these models do not control risks, more recent neural networks employ appropriate loss functions: DQS SumoNet which requires differentiable models.\nCompeting risks Competing risks, involving multiple possible outcomes, require new methods that can naturally adapt to the simpler survival analysis setting. Derived from the Kaplan and Meier estimator, the Nelson-Aalen estimator is an unbiased marginal model for competing risks.\nThe linear Fine and Gray estimator, inspired by the Cox estimator in survival analysis, is the most popular model in clinical research. Recently, machine learning models have been adapted to competing risks settings, including tree-based approaches such as the Random Survival Forests boosting approaches and neural networks approaches such as DeepHit and Gaussian mixtures approaches . Tranformer-based approaches with SurvTRACE using a loss corrected to predict rare competing events, independently forecasts all events but do not ensure that probabilities sum to one.\nFor a comprehensive review of competing risks models, refer to."}, {"title": "Evaluation for such models", "content": "Prediction evaluation in survival or competing risks settings requires adapted metrics to account for right-censored data points such as the C-index, which is an adaptation of the Area Under the ROC Curve (AUC) used in classification tasks. However, the C-index only evaluates the ranking of samples, i.e. which samples are likely to experience the event of interest first. It is also dependent on the censoring distribution, which can introduce bias in the evaluation . In fact, the score may be inflated for distributions that differ from the oracle-censoring distribution . Alternative methods have been proposed, such as the time-dependent C-index, Cr, which is the same metric but computed at a specific time horizon \u03b6. The C-index ranking metric has also been extended to competing risks , but, as in the survival setting, it only evaluates relative risks between pairs of individuals and does not assess the absolute risk for a given individual. Other time-dependent adaptations of the ROC curve have been developed, though these also measure discriminative power rather than the actual risks or probabilities . Yet, controlling risk is crucial for decision making . Proper scoring rules offer an alternative to overcome the limitations of existing metrics, as they capture more aspects of the problem. Additionally, they can be used for both the training and evaluating probabilistic predictive models."}, {"title": "Proper Scoring Rules (PSR)", "content": "Scoring rules are cost functions of observations and a candidate probability distribution. When proper, they target the oracle probability distribution (Definition 3.2). Crucially, they give machine-learning losses that recover probabilities of outcomes. For classification, where discrete events are observed rather than probabilities, the Brier score and the log loss give proper scoring rules, with relative merits.\nGraf et al. adapt the Brier score to survival analysis, with a strong assumption of independence of the covariates in the censoring distribution. Yet, this assumption is often violated , leading to bias. show that the likelihood of the survival function yields a proper scoring rule, but requires both the density function and the survival function, which is a time-wise derivative of outcome probabilities (Definition 3.2). For quantile regression, adapt the Pinball loss to a proper scoring rule for survival analysis, but requiring an oracle parameter. introduce a double optimization problem, where the stationary point corresponds to the oracle distributions.\nFor competing risks, extend the Brier score to a proper scoring rule. However, the Brier score does not capture the uncertainty as effectively as the log loss ."}, {"title": "PROBLEM FORMULATION", "content": "Notations We write oracle quantities as a* and estimates as \u00e2, vectors in bold, a, random variables in upper case, A, observations in lower cases a, and distributions in calligraphic style A."}, {"title": "Problem Setting", "content": "We consider $K \\in \\mathbb{N}^*$ competing events. For $k \\in [1, K]$, we denote $T_k \\in \\mathbb{R}^+$ the event time of the event k, depending on the covariates $X \\thicksim \\mathcal{X}$. We also denote $T^* \\in \\mathbb{R}^+$, the first event of interest that occurs, $T^* = \\min_{k \\in [1,K]}(T_k)$. We observe $(X, T, \\Delta) \\thicksim \\mathcal{D}$, with $T = \\min(T^*, C)$ where $C \\in \\mathbb{R}$ is the censoring time, which can depend on X, and $\\Delta \\in [0, K]$, $\\Delta = \\arg \\min_{k \\in [0,K]}(T_k)$, where 0 denotes a censored observation.\nHowever, we are primarily interested in the distribution of the uncensored data, $(X, T^*, \\Delta) \\thicksim \\mathcal{D}^*$, particularly the joint distribution of $T^*, \\Delta | X = x$.\nGiven a data set of n individuals, we denote each individual i by its associated covariates $x_i$. The outcome is represented by $(t_i, \\delta_i)$, where $t_i$ is the observed time, and $\\delta_i \\in [0, K]$ is the event indicator. $\\delta_i k$ indicates that the event of interest k was observed at time $t_i$, while $\\delta_i = 0$ indicates that the observation was censored at time $t_i$. This paper aims to predict an unbiased estimate of all cause-specific Cumulative Incidence functions (CIFs) at any time horizon (Definition 3.1)."}, {"title": "Definition 3.1 (Quantities of interest).", "content": "Survival Function to any event:\n$S^*(x) = P(T^* > \\zeta | X = x)$\nCIF (Cumulative Incidence Function):\n$F^*_{\\zeta}(x) = P(T^* < \\zeta | X = x) = 1 - S^*_{\\zeta}(x)$\nCIF of the kth event:\n$F_{k,\\zeta}(x) = P(T^* \\leq \\zeta \\cap \\Delta = k | X = x)$\nCensoring Function:\n$G^*_{\\zeta}(x) = P(C > \\zeta | X = x)$"}, {"title": "Assumption 3.1 (Non-informative censoring).", "content": "We make the classic assumption in survival analysis that censoring is non-informative with respect to covariates:\n$T^* \\amalg C | X$"}, {"title": "Assumption 3.1", "content": "is essential for most theoretical results in survival analysis . It shows that single-event survival analysis becomes invalid in the presence of competing risks: if some observations are censored due to other events that share unobserved risk factors with the event of interest, this assumption is violated."}, {"title": "CIF Scoring Rule", "content": "Proper Scoring Rule A scoring rule $l$ evaluates a distribution $P$ on an observation $Y$, producing a corresponding score $l(P, Y)$. The higher the score, the better the model fits the observation. For a proper scoring rule, the score reflects the model's ability to predict the oracle distribution [for more on scoring rules, see Gneiting and Raftery, 2007, Ovcharov, 2018, Merkle and Steyvers, 2013].\nDefinition 3.2 (Proper Scoring Rule). A scoring rule $l$ is considered proper if\n$\\forall P, Q$, distributions $E_{Y\\thicksim Q}[l(P,Y)] < E_{Y\\thicksim Q}[l(Q, Y)]$.\nIf the equality holds if and only if $P = Q$, in which case the scoring rule is strictly proper.\nProper scoring rule for the Global CIF We denote $L_{\\zeta}$ a scoring rule for the global CIF at time $\\zeta$.\nDefinition 3.3 (PSR for competing risks settings). In competing risks settings, where censoring is present, a scoring rule $L_{\\zeta}$ for the CIF at time $\\zeta$ for an observation $(X, T, \\Delta)$ is proper if and only if:"}, {"title": null, "content": "$\\forall \\zeta, (X, T, \\Delta) \\thicksim \\mathcal{D}, \\forall (\\hat{F}_1, ..., \\hat{F}_K, \\hat{S}),$\n$[E_{T^*,\\zeta,\\Delta|X=x}[L_{\\zeta}( (\\hat{F}_1(\\zeta|x), ..., \\hat{F}_K(\\zeta|x), \\hat{S}(\\zeta|x)), (T, \\Delta))]$\nEstimated distributions\n$<$\n$[E_{T^*,\\zeta,\\Delta|X=x}[L_{\\zeta}( (F^*_1(\\zeta|x), ..., F^*_K(\\zeta|x), S^*(\\zeta|x)), (T, \\Delta))]$\nOracle distributions \n(1)\nWhen equality is achieved only for the oracle distributions, the scoring rule is strictly proper."}, {"title": "A STRICTLY PROPER SCORING RULE FOR COMPETING RISKS", "content": "We prove that the negative log-likelihood, re-weighted by the censoring distribution (IPCW: Inverse Probabilities of Censoring Weights), is strictly proper.\nDefinition 4.1 (Competitive Weights Negative LogLoss). We introduce the multiclass negative log-likelihood, re-weighted with the censoring distribution. The different classes represent the loss for all the cumulative incidence functions and the survival function.\n$\\forall \\zeta, (x, t, \\delta) \\thicksim \\mathcal{D},$\n$L_{\\zeta}((\\hat{F}_1(\\zeta|x), ..., \\hat{F}_K(\\zeta|x), \\hat{S}(\\zeta|x)), (t, \\delta)) \\overset{\\text{def}}{=} \\frac{1}{n} \\sum_{i=1}^{n} \\Big( \\sum_{k=1}^{K} 1_{t_i \\leq \\zeta, \\delta_i=k} \\log \\frac{\\hat{F}_k(\\zeta|x)}{\\mathbb{G}^*(t_i|x)} + 1_{t_i \\leq \\zeta} \\log(\\frac{\\hat{S}(\\zeta|x)}{\\mathbb{G}^*(t_i|x)}) \\Big)$\nProbability of remaining\nProbability of remaining (2)\ncensor-free at $t_i$\ncensor-free at $\\zeta$\n(1- probability of censoring)\nEq.2 is a standard log-loss (also known as cross-entropy), reweighted by appropriate sample weights -the inverse probabilities, or IPCW. Therefore, it can easily be added to most multiclass estimators.\nLemma 4.1. Accounting for the time horizon $\\zeta$, the expectation of the above scoring rule can be written as:\n$\\forall \\zeta, (X, T, \\Delta) \\thicksim \\mathcal{D},$\n$E_{T^*,C,\\Delta|X=x}[L_{\\zeta}((\\hat{F}_1(\\zeta|x), ..., \\hat{F}_K(\\zeta|x), \\hat{S}(\\zeta|x)), (T, \\Delta))]$\n$=\\sum_{k=1}^{K} \\log (f_k(\\zeta|x)) F^*_k(\\zeta|x) + \\log (\\hat{S}(\\zeta|x)) S^* ((\\zeta|x)$\nProof sketch. The weights allow us to transition from the observed distribution T to the uncensored distribution T*, which is crucial for demonstrating properness. The full proof can be found in Appendix B."}, {"title": null, "content": "Theorem 1 (Properness of the scoring rule). Under the assumption that the weights are appropriately chosen, $L_{\\zeta} : \\mathbb{R}^{K+1} \\times \\mathcal{D} \\rightarrow \\mathbb{R}$ is a strictly proper scoring rule for the global CIF on a fixed time horizon $\\zeta \\in \\mathbb{R}^+$.\nProof sketch. Using the previous result, the properties of the negative log-likelihood, and Definition 3.3, we conclude that the loss is strictly proper. Full proof in Appendix B."}, {"title": "SURVIVALBOOST: GRADIENT BOOSTING COMPETING RISKS", "content": "While eq.2 can be used as a loss in any multiclass machine learning algorithm, we choose Gradient Boosting Trees due to their strong performance on tabular data and their compatibility with stochastic optimization. Gradient boosting methods approximate complex functions by combining weak learners (or base learners). At each iteration m, the algorithm focuses on the residuals of the loss function and builds a base learner $h_m$ that minimizes these residuals. For gradient boosting trees, the estimator typically takes the form $H_m(x) = H_{m-1}(x) + \\nu h_m(x)$ where $\\nu$ represents a chosen learning rate. For more on gradient boosting, refer to Friedman [1999].\nMost survival or competing risk loss functions cannot be used with tree-based models, as they require time derivates and thus smoothness. To address this, we introduce an algorithm called SURVIVALBOOST, which predicts all CIFs for each competing event as well as the global survival function. By predicting these jointly, we ensure that the stability of the probabilities is maintained, as the outputs of the classification models naturally sum to one. This ensures that $P(T^* < \\zeta | X = x) + P(T^* > \\zeta | X = x) = 1$, meaning the model's outputs are consistent and sum to one:\n$\\sum_{k=1}^K P(T^* < \\Delta^* = k | X = x) + P(T^* > \\zeta; | X = x) = 1$\nkth CIF Survival Probability\nUsing the loss in eq.3, we can directly predict the CIF instead of predicting the hazard function (the derivative of the CIF), as is often done for example, in DeepHit or SurvTRACE . This approach allows us to drop the constant-hazard assumption present in .\nOur algorithm utilizes two classifiers (here, gradient-boosted trees), one for censoring, trained on binary censored/non-censored labels (i.e., for time $\\zeta$, $P(C > \\zeta | X = x)$), and one for multiple events. Both the censoring and event models are adjusted using IPCW weights. To compute these IPCW weights, we iterate"}, {"title": "COMPETING RISKS EXPERIMENTS", "content": "The evaluation is mainly performed on two metrics\u00b9.\nEvaluating the predicted probability We extend the method proposed by Graf et al. and Schoop et al. [2011]. The formula and a formal proof of the properness of the loss can be found in Appendix C. To avoid potential circularity with the loss function that we optimized, we apply this evaluation metric to the Brier Score rather than the log-loss. To evaluate the model across all time points, we sum the Brier Score over time, resulting in the Integrated Brier Score (IBS).\nPrediction accuracy in time In many applications, such as predictive maintenance or medicine, it is crucial to determine the first event a subject is likely to encounter. We use a validation metric to check, for each sample, whether the observed event is predicted as the most likely at given times, selected as before using quantiles. For example, for an individual who encounters event 2 at time t, the probability of surviving until t should be the highest compared to the proba-"}, {"title": "Definition 6.1 (Prediction accuracy at time ().", "content": "For a fixed time horizon $\\zeta$, and denoting survival to any event as index 0, define $\\hat{y} = arg\\underset{k \\in [0,K]}{max} F_k (X = x)$, the most probable event at $\\zeta$, and $y_\\zeta = 1_{t<\\zeta d}$. We remove censored individuals, and $n_{nc}$ represents the number of uncensored individuals at $\\zeta$.\n$Acc(\\zeta) = \\frac{1}{n_{nc}}\\sum_{i=1}^n 1_{\\hat{y}_i=y_i, 1_{\\delta_i=0,t_i\\le\\zeta}}$\n(4)"}, {"title": "Experimental Settings", "content": "Synthetic Dataset We design a synthetic dataset with linear relations between features and targets, as well as dependencies between the censoring distribution and the features (Appendix S4). To create the synthetic dataset, for each sample, we draw $2*n_{events}$ parameters from a normal distribution. We then generate the event durations from a Weibull distribution based on those parameters. The observation is determined by the minimum duration and its associated event. The censoring event is computed using the same method.\nSEER Dataset This dataset tracks 470,000 breast cancer patients for up to ten years, with mortality due to various diseases as the outcomes. The censoring rate is approximately 63%, and Figure S3 shows the distribution of events. Unlike  (DeepHit) and (SurvTRACE), which focus on the two most prevalent events and censor the others (undermining the competing risk framework), we consider three competing events, aggregating the remaining events into a third class. We also remove some features following Wang and Sun [2022].\nBaselines We compare our approach with 7 other competing risks models from simpler models with Aalen et al. 's global estimator and the Fine and Gray linear model to more complex methods.\nWe benchmark against tree-based approach - Random Survival Forests (RSF) , often criticized for its memory limitations. In our comparison, we also include several neural network-based models. This includes DeepHit which is trained with a ranking loss that combines the C-index with a negative log-likelihood, Deep Survival Machines (DSM) which employ a graphical method for feature encoding and DeSurv solves Ordinal Differential Equations for continuous time predictions. Finally, we include a transformer-based model, SurvTRACE [Wang and Sun,"}, {"title": "Results: Competing Risks", "content": "Synthetic dataset Figure 2 illustrates the trade-off between statistical performance and training time for each model. Using the synthetic dataset, we are able to compute an oracle IBS. SURVIVALBOOST performs best in terms of IBS and is the fastest to train.\nResults on SEER Dataset On the real-life dataset, we keep 30% of the data for testing the models."}, {"title": "USAGE IN SURVIVAL ANALYSIS", "content": "As our model can also handle survival analysis, we conducted experiments on three real-life survival datasets.\nMETABRIC The Molecular Taxonomy of Breast Cancer International Consortium dataset contains gene expression data with approximately 2,000 data points.\nSUPPORT Study to Understand Prognoses Preferences Outcomes and Risks of Treatment dataset includes survival times for hospital patients, with more than 8,000 data points.\nKKBOX The Churn Prediction Challenge 2017 hosted on Kaggle, which features administrative censoring and 2.5M data points. We trained the models over 100k, 1M, and 2M data points to assess scalability (see Appendix, Fig. S1)."}, {"title": "Results: Survival Analysis", "content": "Figure 5 shows the trade-off between training time and performance in terms of IBS, where SURVIVALBOOST excels, being the top model in statistical performance and one of the fastest on the datasets with enough data (SUPPORT and KKBOX) while being one of the best models for smaller datasets (METABRIC). Appendix G.2 provides a similar figure for the Scen-log-simple metric, where SURVIVALBOOST achieves an excellent trade-off rivaled only by SumoNet, which has comparable performance on the Scen-log-simple loss. Varying the sample size from 100k to 2M on the KKBOX dataset confirms that SURVIVAL BOOST and DQS are faster (taking less than 1 minute on 100k data points), while Han et al., SumoNet, and RSF are slower for larger sample size. They exhibit super-linear time complexity, making them impractical for large datasets; for more than 100k data points they exceed memory limitations (See Appendix I.1).\nTable 1 report evaluation metrics, including $SC_{en-log-simple}$ which is not what SURVIVALBOOST directly optimizes. Across datasets, SURVIVAL BOOST achieves the best results in terms of IBS and is tied with SumoNet for Scen-log-simple (also for C-index, Appendix H.1). It is worth noting that SumoNet uses Scen-log-simple as its training loss. However, this metric is not guaranteed to be a proper scoring rule, meaning it does not necessarily ensure accurate recovery of the true risks. For KKBOX, we only show the results for 1M data points.\nBeyond proper scores, we investigate calibration, MAE, MSE, and the AUC adapted for survival analysis (Appendix S5, S7, S8). We assess the calibration using four tests, including distribution calibration ($D_c$) and One-time calibration ($ONE_C$). Kaplan-Meier, SURVIVALBOOST, and RSF are the most calibrated models (Appendix S9)."}, {"title": "DISCUSSION AND CONCLUSION", "content": "Code reproducibility and data The code will be made available on GitHub as a library.\nCombination of tree-based architecture and loss function makes the difference Our work shares similarities with the equations in Han et al., which also uses IPCW [introduced by Robins et al., 1994], though for survival and not competing risks. Their learning strategy targets an equilibrium, showing that it recovers the oracle distribution in survival analysis settings. Meanwhile, our optimization uses a loss on all classes to compute the censoring distribution, while the other part optimizes only for the survival distribution. This last part departs from the schema in Han et al. [2021]. Despite similarities, the two approaches behave markedly different our empirical study.\nBuilding upon trees-based model is probably important to this difference and to the success of SURVIVAL BOOST. Yet, comparing to GBS and RSF show that trees in themselves do not suffice. Our loss is crucial for scalability (as it is separable) and to facilitate fitting trees, as it avoids the need for time derivatives. It avoids issues that plague many competing risks methods. The excellent empirical results, superior performance with less computational resources, come from combining the loss function with the tree-based approach results in a very stable algorithm. This double gain is especially valuable as health datasets continue to grow in size.\nAcknowledgments JA, JA, and GV acknowledge funding from the ERC grand INTERCEPT-T2D.\nLimitations and further work Further work should consider removing the assumption of non-informative censoring 3.1. This assumption is very common in the literature, though some recent work has relaxed it in survival settings"}, {"title": "Conclusion", "content": "For competing risks, which generalizes survival analysis to classify the type of outcome, we first propose and prove a strictly proper scoring rule. This reweighted log loss can easily be used in machine learning models: it is separable by observation, making it suitable for stochastic solvers, it does not require time derivatives (unlike most survival models) and it can be applied to non-differentiable models. We integrate it into gradient-boosting trees, resulting in an algorithm called SURVIVALBOOST. By using time as a feature and incorporating a feedback loop to better estimate censoring probabilities, SURVIVALBOOST outperforms state-of-the-art methods on both synthetic and real-life datasets, for both competing risks (classification on time-censored data) and standard survival analysis (time-to-event regression with right censoring). It also trains faster on large datasets. As a loss function, it allows survival analysis or competing risks modeling to be easily extended to a wide range of models- from scalable linear models to deep learning architectures, including fine-tuning foundation models- replacing clinical standards like Fine and Gray that do not scale."}]}