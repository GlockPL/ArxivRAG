{"title": "Scaling Law Hypothesis for Multimodal Model", "authors": ["Qingyun Sun", "Zhen Guo"], "abstract": "We propose a scaling law hypothesis for multimodal models processing text, audio, images, and video within a shared token and embedding space. Our framework predicts model performance based on modality-specific compression and tokenization efficiency, extending established scaling laws from text-based decoder models to mixed-modality systems. We explore whether leveraging more training data in multiple modalities can reduce the size of the multimodal model, enabling efficient deployment on resource-constrained devices.", "sections": [{"title": "Introduction", "content": "Scaling laws in large language models (LLMs) have unveiled fundamental relationships between model performance, size, and the volume of training data [1, 2, 3, 4, 5, 6]. These laws serve as a guide for resource allocation in LLM development, helping to balance model size and data volume to optimize performance. The initial scaling laws proposed by OpenAI [1] suggested that larger models are more sample-efficient, leading to the creation of massive models like GPT-3. However, subsequent research from DeepMind, notably the Chinchilla study [2], revealed that many large models were undertrained. Their findings indicated that smaller models trained on more data could outperform larger models when the compute budget is held constant.\nDespite these insights, recent trends challenge the Chinchilla-optimal law. For instance, models like Llama 3 and 3.1 have been trained on significantly more tokens (up to 10 times more than Chinchilla's recommendations), yet still demonstrate outstanding performance [7]. This discrepancy has prompted researchers to reconsider the optimal allocation of compute resources in autoregressive pre-training [8].\nRecent work suggests a unified scaling law, where model performance is driven primarily by total compute, regardless of how it is distributed between model size and dataset size [8]. This approach introduces bits per character (BPC) as a performance metric that reflects the model's compression efficiency. BPC has been shown to correlate linearly with model performance across various modalities (Figure 1).\nThis perspective reveals a linear relationship between BPC and the logarithm of compute used, which can be formalized as:\nBPC x log(N) + log(P)\nwhere N is the number of training tokens, and P is the number of model parameters (Figure 2).\nThis unified scaling law suggests that smaller models trained on larger datasets may be prioritized for inference efficiency, especially in settings where resource constraints in inference are significant."}, {"title": "Modality-Specific Compression and Tokenization Efficiency", "content": "The compression and tokenization efficiency of each modality is a key determinant of overall model performance. For each modality i, the relationship between the raw input size T\u2081, the compression factor Ci, and the number of tokens generated Ni can be expressed as:\nlog Ti = log Ci + log Ni,\nwhere Ci represents the efficiency with which continuous data is compressed into tokens, and Ni is the number of tokens produced after compression.\nIn text-based models, tokenization efficiency is relatively stable due to the consistency of algorithms like Byte Pair Encoding (BPE). This consistency allows scaling laws to focus primarily on the number of text tokens Ntext and model size P as performance predictors. However, in multimodal models, tokenization efficiency varies significantly between modalities due to differences in the complexity and redundancy of raw input data.\nFor example, visual data such as images tends to have a high level of redundancy, allowing for significant compression. Nevertheless, tokenizing image data through CNNs or vision transformers results in a larger number of tokens compared to text, primarily due to the higher dimensionality of images. Video data, which combines both spatial and temporal dimensions, is even more complex and generates a greater number of tokens, further increasing the compute required.\nThis variability in compression efficiency across modalities underscores the need for a scaling law that accounts for the specific characteristics of each data type, ensuring that model performance is accurately predicted for mixed-modality systems."}, {"title": "Predicting Multimodal Model Performance", "content": "For text-only models, performance typically scales according to:\nperformance x log(Ntext) + log(P),\nwhere Ntext is the number of text tokens, and P is the number of model parameters [8].\nWhen this scaling law is extended to multimodal models, it must account for the differing compression efficiencies across modalities. The performance of multimodal models is determined by the total amount of raw data represented in the shared token space, adjusted by the compression efficiency of each modality. The performance can be predicted by the following equation:\nmultimodal performance \u221d log (\u2211) + log P,\nwhere T\u2081 represents the raw data size for each modality, C\u2081 is the compression efficiency for that modality, and P is the number of model parameters.\nThis equation highlights that while the total raw data processed by the model contributes to overall performance, the efficiency of tokenization for each modality (i.e., how effectively each modality compresses its raw data into tokens) plays a significant role in the compute required. A modality with lower compression efficiency, such as video, will require substantially more compute to reach the same performance level as text, which generally has higher compression efficiency."}, {"title": "Conclusion", "content": "We propose a scaling law hypothesis that extends established text-based scaling laws to multimodal models, emphasizing the crucial role of modality-specific compression and tokenization efficiency. The performance of multimodal models depends not only on the total amount of raw data and model size but also on how efficiently each modality compresses its data into tokens. This relationship directly affects the computational resources required to train the model.\nOur hypothesis explores the potential to leverage larger amounts of training data across multiple modalities to reduce model size without sacrificing performance. This could enable more efficient multimodal models, especially in resource-constrained environments like mobile devices or edge computing. By optimizing the trade-off between data volume and model size, we aim to make multimodal models more suitable for on-device deployment.\nFuture work should focus on refining the quantification of compression factors for each modality, allowing for more accurate performance predictions and guiding the development of optimized multimodal architectures for a wide range of tasks and data types."}, {"title": "Limitations", "content": "The proposed scaling law assumes that the multimodal model is trained from scratch. It may not directly apply to models that utilize cross-modal connectors, such as LLaVA [16, 17] and VILA [18, 19], which align pre-trained vision and language models. These approaches leverage pre-trained components, which could affect the scaling dynamics and performance predictions outlined in this work."}]}