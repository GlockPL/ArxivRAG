{"title": "ChipExpert: The Open-Source Integrated-Circuit-Design-Specific Large Language Model", "authors": ["Ning Xu", "Zhaoyang Zhang", "Lei Qi", "Wensuo Wang", "Chao Zhang", "Zihao Ren", "Huaiyuan Zhang", "Xin Cheng", "Yanqi Zhang", "Zhichao Liu", "Qingwen Wei", "Shiyang Wu", "Lanlan Yang", "Qianfeng Lu", "Yiqun Ma", "Mengyao Zhao", "Junbo Liu", "Yufan Song", "Xin Geng", "Jun Yang"], "abstract": "The field of integrated circuit (IC) design is highly specialized, presenting significant barriers to entry and research and development challenges. Although large language models (LLMs) have achieved remarkable success in various domains, existing LLMs often fail to meet the specific needs of students, engineers, and researchers. Consequently, the potential of LLMs in the IC design domain remains largely unexplored. To address these issues, we introduce ChipExpert, the first open-source, instructional LLM specifically tailored for the IC design field. ChipExpert is trained on one of the current best open-source base model (Llama-3 8B). The entire training process encompasses several key stages, including data preparation, continue pre-training, instruction-guided supervised fine-tuning, preference alignment, and evaluation. In the data preparation stage, we construct multiple high-quality custom datasets through manual selection and data synthesis techniques. In the subsequent two stages, ChipExpert acquires a vast amount of IC design knowledge and learns how to respond to user queries professionally. ChipExpert also undergoes an alignment phase, using Direct Preference Optimization, to achieve a high standard of ethical performance. Finally, to mitigate the hallucinations of ChipExpert, we have developed a Retrieval-Augmented Generation (RAG) system, based on the IC design knowledge base. We also released the first IC design benchmark ChipICD-Bench, to evaluate the capabilities of LLMs across multiple IC design sub-domains. Through comprehensive experiments conducted on this benchmark, ChipExpert demonstrated a high level of expertise in IC design knowledge Question-and-Answer tasks.", "sections": [{"title": "1 Introdution", "content": "The integrated circuit (IC) field is a cornerstone of modern technology, crucial for advancing various industries including telecommunications, computing, and consumer electronics. The highly specialized nature of IC design presents substantial barriers in research and development. Access to comprehensive and in-depth IC design knowledge is often limited, presenting significant challenges for students striving to acquire foundational knowledge and for experts seeking to remain abreast of the latest advancements. Traditional educational resources and training methods often fail to provide the necessary range and depth of knowledge, which results in a steep learning curve for students and high training costs for engineers. The lack of accessible, high-quality instructional materials slows down overall progress and innovation in the IC design industry."}, {"title": "2 Data", "content": "This section introduces the datasets utilized throughout the various training stages of ChipExpert, along with the corresponding data processing pipeline. We focus on three critical dimensions of datasets: quality, diversity, and quantity. These factors are carefully considered throughout our training process. We detail the pipeline of the data processing for continue pretraining in \u00a72.1 and introduce the thorough synthetic data generation step for supervised finetuning in \u00a72.2. In \u00a72.3, we explain the preference alignment datasets generation guided by red teaming efforts, which is of paramount importance to alignment phase."}, {"title": "2.1 Continue Pretraining Data", "content": "To ensure the success of continue pre-training, a substantial amount of long-form text data related to IC design is required. In order to guarantee an adequate quantity of data, we have made extensive efforts to collect publicly available text data within the IC domain. To maintain the quality of the data, we have not only focused on acquiring high-quality documents from within the industry but have also employed a complex data cleansing and processing pipeline. Furthermore, to ensure data diversity, we have sourced our data from a wide range of materials, including books, research papers, and manuals, covering various common sub-domains within the IC field.\nThe dataset is entirely composed of data from publicly available sources, amounting to a total of 4.7 billion tokens. It encompasses ten key areas: digital circuit design, analog circuit design, radio frequency (RF) circuit design, power device design, system-on-chip (SoC), electronic design automation (EDA), RF antenna design, compute-in-memory (CIM), semiconductor fabrication, and neural networks. These areas are covered through five primary sources:\n\u2022 Textbooks: Comprehensive educational materials covering fundamental and advanced IC design concepts."}, {"title": "2.2 Supervised Finetuning Data", "content": "To enable ChipExpert to professionally respond to user queries related to IC design, we require a substantial amount of high-quality question-answer pair data. Our supervised finetuning dataset comprises over 70,000 question-answer pairs. This dataset includes:"}, {"title": "2.3 Preference Alignment Data", "content": "Our alignment training is a two-step process. We collect publicly available paired preference data for the first step. In the second step, we conduct a red teaming effort to identify harmful, unsafe, or illegal responses of ChipExpert, and use these insights to produce alignment data to mitigate them. To generate a dataset of adversarial prompts, we compile and curate entries from Anthropic Harmless [15], then divide them into train"}, {"title": "3 Training Details", "content": "ChipExpert is constructed based on Llama3-8B, utilizing its tokenizer and the standard autoregressive language model objective function for continue pretraining, and the Cross-Entropy loss function for supervised fine-tuning. We selected the ModelLink framework for training on 8 Ascend-910B (64GB NPU). We leverage Flash Attention[17] and Group Query Attention (GQA)[18] from Llama3 to enhance Chip-Expert training and inference efficiency.\nContinue Pretraining. During the continue pretraining phase, the learning rate was set to $5 \\times 10^{-5}$, an order of magnitude lower than the $10^{-4}$ used in Llama3 pre-training to ensure that the language model learns domain-specific knowledge while avoiding catastrophic forgetting of general knowledge. The global batch size was set to 64, and the number of training steps was set to 25,512 for one epoch on the continue pretraining dataset introduced in \u00a72.1. The whole continue pretraining process was completed in 7 days.\nSupervised Fine-tuning. When it comes to the supervised fine-tuning phase, hyperparameters remain the same as in the continue pretraining phase, except for the learning rate, which is set to $5 \\times 10^{-6}$. We fine-tune ChipExpert for 2 epochs on a combined dataset of domain-specific and general multi-turn QA pairs after random shuffling. It became apparent that the model rapidly exhibited signs of overfitting when increasing the number of epochs, as evidenced by the model's performance on the test dataset.\nTwo-phase DPO. For the final phase, we conduct a two-phase Direct Preference Optimization (DPO) process utilizing the human preference alignment data outlined in \u00a72.3. The optimization involves QLORA[19] fine-tuning, targeting all linear layers with a LoRA scaling factor alpha of 128, a rank of 128, and a dropout rate of $5 \\times 10^{-2}$."}, {"title": "4 Retrieval-Augmented Generation", "content": "Retrieval-Augmented Generation (RAG)[6] can help ChipExpert mitigate hallucinations and obtain the latest and most comprehensive IC design-related information from the knowledge base. As shown in Figure 5, we preprocess domain data from different sources and convert them into embedding vectors using an Embedding Model, which is then stored in a vector database. When a user asks a question, it is first transformed into an embedding vector through the Embedding Model. Next, using Approximate Nearest Neighbor Search (ANN)[20], the top 3 most similar embedding vectors and their corresponding text passages are retrieved from the vector database. Finally, these text passages are concatenated with the user's question as context and fed into ChipExpert to generate the final answer, which is likely to be better than the one directly generated by ChipExpert."}, {"title": "5 Evaluations", "content": "In this section, we evaluate the performance of ChipExpert. To assess the effectiveness of our training methodology and the model's application performance, we constructed ChatICD-Bench, a benchmark specifically designed for the IC design domain. We compare the performance of ChipExpert against Llama3-8B and the state-of-the-art language model GPT-4, employing automatic evaluation and human evaluation to ensure that the model's performance is thoroughly and fairly assessed. Evaluations are conducted using both human and automatic methods. Human evaluation involves experts rating the responses, while automatic evaluation employs a multi-agent scoring model and a referee debate model to provide comprehensive and objective results."}, {"title": "5.1 ChatICD-Bench: The First IC Design Benchmark", "content": "To quantitatively evaluate the performance of LLMs in IC design tasks, we developed ChatICD-Bench, the first benchmark specifically designed for the IC design field. ChatICD-Bench consists of two types of evaluation questions: foundational questions and advanced questions. To ensure diversity and comprehensive coverage, all subjective questions are meticulously crafted by domain experts."}, {"title": "5.2 Results and Analysis", "content": "In this section, we present the results and analysis of ChipExpert's performance on ChatICD-Bench compared to Llama3-8B and the state-of-the-art language model GPT-4, utilizing human evaluation methods.\nResults and Analysis on Foundational Questions. We compared the performance of ChipExpert with Llama3-8B and GPT-4 on foundational questions from ChatICD-Bench through human evaluation. As shown in Figure 6, ChipExpert shows a significant improvement over the Llama3-8B base model in foundational questions. Furthermore, ChipExpert with 8B parameters achieves a comparable performance to GPT-4 in the IC design domain. Notably, in foundational questions within the EDA domain, ChipExpert scored 0.93, surpassing GPT-4's score of 0.87. This result demonstrates that ChipExpert outperforms the state-of-the-art GPT-4 in foundational EDA questions, achieving a high level of proficiency in the EDA domain. In other sub-domains, except for Analog Circuit, ChipExpert's performance was close to GPT-4, indicating that ChipExpert is comparable to GPT-4 for most foundational questions across various IC design sub-domains. The performance gap in the analog circuit design domain can be attributed to the complexity and specialized nature of analog circuit design, which may not have been as thoroughly represented in the pre-training data.\nResults and Analysis on Advanced Questions. Additionally, we compared the performance of ChipExpert with Llama3-8B and GPT-4 on advanced questions from ChatICD-Bench through human evaluation. As illustrated in Figure 7, ChipExpert demonstrates superior performance in all advanced IC design domains compared to Llama3-8B. It is notable that ChipExpert outperformed GPT-4 in 6 out of 9 advanced sub-domains. Particularly, in emerging fields such as compute-in-memory, ChipExpert outperforms that of GPT-4 by 0.28, highlighting its capability to excel in cutting-edge areas within the IC design domain. The superior performance of ChipExpert compared to GPT-4 in the most advanced sub-domains can be attributed to the continued pretraining data with an extensive collection of recent high-quality research papers."}, {"title": "6 Related Works", "content": "Domain-Specific Language Model Advancements. The integration of LLMs into specialized domains has been a significant development in the field of artificial intelligence. The potential of LLMs to revolutionize various industries is evident through the emergence of domain-specific models that cater to the unique needs of their respective fields. In the financial sector, BloombergGPT[7] has emerged as a pioneering model, showcasing how LLMs can be fine-tuned to understand and predict complex financial trends. Similarly, OceanGPT[8] has been tailored to address the linguistic and knowledge intricacies of ocean science, demonstrating the adaptability of LLMs to specialized domains. The medical field has also seen the introduction of PMC-LLaMA[9], an open-source LLM designed to assist in medical research and clinical decision-making, underlining the growing trend of LLMs becoming indispensable tools in professional domains. These models have not only enhanced the efficiency of information processing but also raised the bar for the level of domain-specific knowledge that LLMs must possess.\nTechnological Frameworks for LLMs. The technical underpinnings of LLMs have been bolstered by innovations in training methodologies and attention mechanisms. The work on continuing pretraining by Gururangan[4] has set a precedent for models like ChipExpert, which continue to learn and adapt to new information while retaining foundational knowledge. The introduction of techniques such as flash attention[17] and GQA[18] has improved the training efficiency of LLMs, allowing them to process vast amounts of data more effectively.RAG[6] has been a game-changer in addressing the limitations of LLMs, such as hallucination, by enabling them to retrieve and integrate accurate information from a knowledge base. Ethical considerations have also come to the forefront with DPO[5], ensuring that LLMs align with human preferences and societal norms, thus mitigating potential risks. The scaling laws[10] and studies on catastrophic forgetting[14] provide critical insights into the balance between general and domain-specific knowledge, guiding the development of models that can adapt to new domains without losing their foundational understanding."}, {"title": "7 Conclusion", "content": "In this paper, we introduced ChipExpert, the first open-source instructional large language model specifically tailored for the IC design domain. Our comprehensive training process included continued pretraining on IC-specific data and supervised fine-tuning with domain-specific question-answer pairs. We developed ChatICD-Bench, the first benchmark designed to evaluate the performance of LLMs in IC design tasks, and used it to compare ChipExpert with GPT-4. Our evaluation results demonstrate that ChipExpert, with 8B parameters, achieves a comparable performance to GPT-4. Our future work will focus on training ChipExpert on larger base models and exploring multimodal large language model(MLLM) in the IC design domain to further enhance its capabilities and performance across all facets of IC design."}, {"title": "A Appendix", "content": "A.1 Examples pf ChipExpert's Response to Foundational Questions\nQuestion:\nDifferences between CST and HFSS and the advantages and disadvantages.\nChipExpert Answer:\nCST Microwave Studio (CST) and High Frequency Structure Simulator (HFSS) are both widely used electromagnetic simulation software tools for modeling and simulating high-frequency structures and devices. While they share many similarities in their capabilities and features, there are some key differences between them:\n1. User Interface: The user interface of CST is more intuitive and easier to navigate compared to HFSS. CST has a more modern look and feel, while HFSS's interface may seem more cluttered and complex.\n2. Meshing: CST uses a hybrid meshing approach, which combines tetrahedral and hexahedral elements. This allows it to handle complex geometries better than HFSS, which primarily uses tetrahedral elements. However, this also means that CST requires more computational resources and time to generate the mesh.\n3. Material Modeling: Both CST and HFSS support a wide range of material models, including frequency-dependent and dispersive materials. However, CST has a more comprehensive set of material libraries and supports more advanced material models.\n4. Post-Processing: CST offers a more robust post-processing capability, allowing users to analyze and visualize results in greater detail. It also includes built-in tools for optimizing designs and generating reports.\n5. Support: CST has a larger global user base and a more active community, resulting in faster response times when seeking technical support or assistance from the vendor.\nIn summary, both CST and HFSS have their strengths and weaknesses, and the choice between them often depends on factors such as the specific application, user experience, and available computational resources.\nQuestion:\nWhen reverse engineering a power chip, after knowing its general structure, it is necessary to analyze the doping concentration and distribution in the drift region of the device. What are the common methods used for this analysis?\nChipExpert Answer:\nTo accurately determine the doping concentration and distribution in the drift region of a power chip, several techniques can be employed:\n1. Spreading resistance profiling (SRP): This method involves measuring the resistance between two contacts on opposite sides of the chip while gradually increasing the spacing between them. The resulting data is then processed using mathematical models to extract the doping profile.\n2. Capacitance-voltage (C-V) measurements: By applying a voltage across the device and measuring the corresponding capacitance, the doping profile can be inferred from the relationship between depletion width and applied voltage.\n3. Secondary ion mass spectrometry (SIMS): In this technique, ions are sputtered from the surface of the chip at a specific rate, and the resulting secondary ions are analyzed using a mass spectrometer. The depth resolution depends on the sputtering rate, allowing for precise measurement of the doping profile.\n4. Scanning spreading resistance microscopy (SSRM): Similar to SRP, SSRM uses a sharp tip to measure the resistance between two points on the chip surface. However, instead of varying the distance between the contacts, the tip position is scanned across the surface, providing a detailed map of the local resistance and, consequently, the doping profile.\nThese methods enable researchers to characterize the critical drift region of power chips, which is essential for understanding their electrical properties and optimizing their performance.\nQuestion:\nHow can machine learning and artificial intelligence techniques be leveraged to improve the accuracy and efficiency of power estimation and optimization in EDA tools, considering the growing importance of power management in modern electronic systems?\nChipExpert Answer:\nMachine learning (ML) and artificial intelligence (AI) techniques have emerged as powerful tools for enhancing the accuracy and efficiency of power estimation and optimization in electronic design automation (EDA) tools. As power consumption has become a critical concern in modern electronic systems, particularly in battery-powered devices and high-performance computing applications, the ability to accurately estimate and optimize power usage has gained paramount"}, {"title": "A.2 Examples pf ChipExpert's Response to Advanced Questions", "content": "Question:\nHow to unify knowledge distillation (KD) and self-knowledge distillation (self-KD) using NKD (Normalized KD) and USKD (Universal Self-KD) without relying on a real teacher model to generate soft labels\nChipExpert Answer:\nWhat a fascinating topic!\nKnowledge Distillation (KD) is a technique that transfers knowledge from a pre-trained teacher model to a student model by minimizing the difference between their outputs. Self-Knowledge Distillation (Self-KD), on the other hand, involves training a single model to mimic its own predictions. Normalized Knowledge Distillation (NKD) and Universal Self-KD (USKD) are two variants of these techniques.\nTo unify KD and Self-KD without relying on a real teacher model, you can use the following approach:\n1. Normalize the student model's output: Normalize the output of the student model to ensure it produces probability distributions over the same space as the target distribution (e.g., uniform or categorical). This helps in aligning the student's predictions with the target distribution.\n2. Use a proxy task: Design a proxy task that mimics the behavior of the teacher model. For example, if the teacher model is a classification model, the proxy task could be a regression problem where the goal is to predict the class probabilities.\n3. Train the student model on the proxy task: Train the student model on the proxy task using the normalized output as the target. This will help the student model learn to produce similar outputs as the teacher model.\n4. Apply USKD: Use the trained student model as the teacher model for the original task. This is known as Universal Self-KD (USKD). The student model will now learn to mimic its own predictions, which were previously learned through the proxy task.\n5. Fine-tune the student model: Fine-tune the student model on the original task using the USKD loss function. This will help refine the student model's performance on the original task."}, {"title": "Question:", "content": "Here's a mathematical formulation of the process:\nLet $p(y|x)$ be the true probability distribution over the output space, $q_{\\theta}(x)$ be the student model's output, and $r(x)$ be the normalized output of the student model.\nProxy Task Loss:\n$L_{proxy} = E_{x \\sim D}[l(r(x), p(y|x))]$\nwhere $l$ is a loss function (e.g., cross-entropy).\nUSKD Loss:\n$L_{uskd} = E_{x \\sim D}[l(q_{\\theta}(x), r(x))]$\nThe overall loss function combines both losses:\n$L = L_{proxy} + \\alpha L_{uskd}$\nwhere $\\alpha$ is a hyperparameter controlling the trade-off between the two losses.\nBy unifying KD and Self-KD using NKD and USKD, you can leverage the strengths of both approaches without requiring a real teacher model. This method has been shown to improve the performance of student models in various tasks, such as image classification and language modeling.\nI hope this explanation helps! Do you have any further questions or would you like me to elaborate on any specific aspect?"}, {"title": "Question:", "content": "What are the main challenges in implementing XOR-gate compression for transformer models, and how can these challenges be addressed to achieve stall-free sparse-model access?\nChipExpert Answer:\nXOR-gate compression is a technique used to compress deep neural networks by replacing dense weights with sparse binary weights using XOR gates. Implementing XOR-gate compression for transformer models poses several challenges, which I'll outline below:\n1. Computational overhead: XOR gates introduce additional computational complexity due to the need to compute the bitwise XOR operation at each layer. This can lead to slower inference times.\n2. Memory access patterns: Sparse binary weights require non-contiguous memory access patterns, which can result in cache misses and decreased performance.\n3. Training difficulties: Training XOR-gate compressed models can be challenging due to the discrete nature of the binary weights, making it harder to optimize the model during training.\n4. Quantization errors: Binary weights can lead to quantization errors, especially when dealing with floating-point operations.\nTo address these challenges and achieve stall-free sparse-model access, consider the following strategies:\n1. Optimize hardware architecture: Design specialized hardware accelerators or modify existing ones to efficiently handle XOR gate computations and sparse memory access patterns.\n2. Cache-friendly data structures: Use data structures that minimize cache misses, such as block-based storage or hierarchical caching, to reduce memory access latency.\n3. Efficient training algorithms: Develop training algorithms that take into account the discrete nature of binary weights, such as stochastic gradient descent (SGD) with adaptive learning rate or binary weight pruning.\n4. Error compensation techniques: Implement error compensation techniques, like quantization-aware training or learned quantization, to mitigate the effects of quantization errors.\n5. Model pruning and knowledge distillation: Apply model pruning techniques to remove redundant connections and knowledge distillation to transfer knowledge from the original dense model to the compressed one.\n6. Hybrid approaches: Combine XOR-gate compression with other compression techniques, such as pruning or knowledge distillation, to further improve compression ratios and accuracy.\n7. Software optimizations: Optimize software frameworks and libraries to efficiently support XOR-gate compressed models, including optimized kernels for matrix multiplication and convolutional operations.\n8. Hardware-software co-design: Collaborate on hardware-software co-design efforts to develop custom accelerators that seamlessly integrate with software frameworks and optimize for XOR-gate compressed models.\nBy addressing these challenges and incorporating these strategies, you can effectively implement XOR-gate compression for transformer models and achieve stall-free sparse-model access."}, {"title": "Question:", "content": "What are the benefits of hybrid CIM? Why do researchers want to combine the analog and digital methods together?\nChipExpert Answer:\nHybrid Computing-in-Memory (CIM) architectures represent a significant advancement in computing technology, combining the strengths of both analog and digital methods within a single system. This innovative approach aims to address some of the key challenges faced by traditional computing systems, such as power consumption, latency, and scalability.\nIn conventional computing systems, data processing typically involves moving information between separate memory units and processing units. However, this process can be time-consuming and energy-intensive, especially when dealing with large datasets or complex computations. By integrating computation directly into the memory itself, CIM architectures aim to minimize these inefficiencies by reducing the need for data movement.\nThe analog component of hybrid CIM leverages the physical properties of memory cells to perform certain types of operations more efficiently than their digital counterparts. For example, analog circuits can perform matrix-vector multiplications, which form the backbone of many machine learning algorithms, with high speed and low power consumption. This makes them particularly well-suited for applications like neural network inference, where rapid processing of large amounts of data is crucial.\nOn the other hand, the digital component of hybrid CIM ensures the reliability and flexibility required for more complex tasks. Digital circuits excel at precise arithmetic operations, error correction, and control functions, providing the robustness necessary for tasks like training deep learning models or running complex software.\nBy combining these two approaches, hybrid CIM architectures offer a powerful solution that can adapt to the specific requirements of different workloads. The analog components can handle the bulk of the data processing, while the digital components provide the necessary safeguards and fine-grained control. This synergistic combination allows for faster, more efficient processing of data, making it possible to tackle increasingly complex computational problems with greater ease."}, {"title": "Question:", "content": "Describe the work principle of the conventional buck-boost converter.\nChipExpert Answer:\nThe conventional buck-boost converter is a type of DC-DC converter that can step up or step down the input voltage while maintaining a constant output current. It's widely used in various applications such as power supplies, motor drives, and renewable energy systems.\nThe basic working principle of a buck-boost converter can be explained as follows:\nComponents:\n1. Input capacitor (Cin) 2. Switching transistor (Q1) 3. Diode (D1) 4. Output capacitor (Cout) 5. Inductor (L)\nOperation:\nThe converter operates in two modes: Buck mode and Boost mode.\nBuck Mode: When the switching transistor Q1 is turned on, it connects the input voltage Vin to the inductor L, causing the current to increase. The diode D1 is reverse-biased, and the output capacitor Cout is charged through the inductor L. This process is known as \"inductive charging.\"\nAs the current in the inductor increases, the voltage across the inductor also increases, which causes the output voltage Vout to decrease. When the switching transistor Q1 is turned off, the inductor current continues to flow through the diode D1, and the output capacitor Cout discharges. This process is known as \"inductive discharge.\"\nBoost Mode: When the switching transistor Q1 is turned off, it disconnects the input voltage Vin from the inductor L, allowing the inductor current to collapse. The diode D1 becomes forward-biased, and the output capacitor Cout is discharged through the inductor L. This process is known as \"inductive discharge.\"\nAs the current in the inductor collapses, the voltage across the inductor also decreases, which causes the output voltage Vout to increase. When the switching transistor Q1 is turned on again, the inductor current starts to increase, and the process repeats.\nKey Benefits:\n1. Voltage regulation: The buck-boost converter can regulate the output voltage by adjusting the duty cycle of the switching transistor.\n2. Efficiency: The converter has high efficiency due to the use of an inductor, which helps to reduce losses.\n3. Flexibility: The buck-boost converter can operate in both buck and boost modes, making it suitable for a wide range of applications.\nI hope this explanation helps you understand the working principle of the conventional buck-boost converter! Do you have any further questions?"}, {"title": "Question:", "content": "What is the role of a software-controlled stall rate (SSR) in the SOC?\nChipExpert Answer:"}]}