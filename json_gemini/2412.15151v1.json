{"title": "Language Models as Continuous Self-Evolving Data Engineers", "authors": ["Peidong Wang", "Ming Wang", "Zhiming Ma", "Xiaocui Yang", "Shi Feng", "Daling Wang", "Yifei Zhang"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert-labeled data, setting an upper limit on the performance of LLMs. To address this issue, we propose a novel paradigm that enables LLMs to train itself by autonomously generating, cleaning, reviewing, and annotating data with preference information, named LANCE. Our approach demonstrates that LLMS can serve as continuous self-evolving data engineers, significantly reducing the time and cost of the post-training data construction process. Through iterative fine-tuning on different variants of the Qwen2, we validate the effectiveness of LANCE across various tasks, showing that it can continuously improve model performance and maintain high-quality data generation. Across eight benchmark dimensions, LANCE resulted in an average score enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This training paradigm with autonomous data construction not only reduces the reliance on human experts or external models but also ensures that the data aligns with human values and preferences, paving the way for the development of future superintelligent systems that can exceed human capabilities.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have exhibited extraordinary proficiency in tackling diverse tasks, spanning natural language understanding, logical reasoning, code generation, mathematical reasoning, etc (Dubey et al., 2024; Achiam et al., 2023; Liu et al., 2024). These advancements are largely attributed to instruction tuning (Wei et al., 2021) and Reinforcement Learning from Human Feedback(RLHF) (Stiennon et al., 2020), which have significantly improved the performance of LLMs using quality data. High-quality data not only enhances the precision and reliability of the models but also ensures that the outputs are more aligned with human values and preferences (Wang et al., 2023b).\nHowever, with the rapid development of LLMs, the acquisition of high-quality data becomes more difficult (Penedo et al., 2023). On the one hand, as the volume of LLM training data increases, previous high-quality data are likely to have been used (Villalobos et al., 2024), making their reuse ineffective for further performance improvement. There is a constant need for new high-quality data to enhance LLMs' intelligence. On the other hand, relying on human experts to get high-quality data is time-consuming and costly, limiting efficiencies and making it challenging to keep up with the rapid demands of LLMs (Villalobos et al., 2024). Additionally, for future superintelligence that exceeds the ability level of human experts, human supervisory signals may be of limited help, and the quality of human-generated data may be one of the bottlenecks in the emergence of superintelligence (Burns et al., 2023). Some research (Lee et al., 2024; Dai et al., 2023; Chen et al., 2024a) suggests leveraging synthetic data generated by teacher LLMs to train student LLMs, yet this approach hinges on supervised signals from external models. Conversely, other studies (Zelikman et al., 2022; Wang et al., 2023a; Gulcehre et al., 2023) advocate for the use of LLM construction data to iteratively fine-tune the model itself. However, these methods do not comprehensively address the entire data construction lifecycle for the post-training of LLMs.\nTo address these challenges, we propose a novel training paradigm that enables LLMs to autonomously generate, clean, review, and annotate data with preference information, thereby using this data to train itself. Through the full cycle of LLM's processing of the training data, we show that LLMs are thus decent data engineers. First, the existing seed dataset is reviewed. For the lower-quality data, the model generates instructions and responses that explore the same themes or forms to construct instruction data, compensating for the distributional deficiencies of the seed data. For the higher-quality data, the model generates new responses that appear correct but are expressively flawed and contain misleading information, constructing preference data to enhance model response quality and reduce hallucinations. For the newly generated data, the model reviews the new data after calling the tool for initial cleaning to ensure that the new instruction data are of high quality and that the preference pairs in the preference data are correct. Finally, this data is used to train the model itself, improving its performance. This process can be repeated multiple times to continuously enhance the model's performance. LANCE enables the autonomous construction of the data required for the full cycle of model post-training, requiring no human involvement or reliance on external models, and significantly reducing the time and cost required in each data process compared to traditional methods.\nTo evaluate our method, we conducted iterative fine-tuning on different models and tested their performance in various areas such as scientific reasoning, commonsense reasoning, knowledge understanding, complex tasks, mathematics, and generative diversity. We found that even with iterative processing on a small dataset, the average performance of the model across various tasks continues to rise, and individual evaluation metrics either remain stable or show improvement. In contrast, continuous Supervised Fine-Tuning (SFT) on the same dataset either leads to no performance improvement or even degradation. This demonstrates that our method can consistently provide high-quality data, which is crucial for the continuous enhancement of model intelligence.\nLANCE represents a valuable exploration towards the path of superintelligence by enabling LLMs to autonomously construct high-quality data. By enabling LLMs to autonomously generate, clean, review, and annotate data with preference information, our approach ensures that the data is not only of high quality but also produced quickly and at a lower cost, thereby significantly reducing the need for human or external models intervention. Ultimately, our approach not only enhances the performance of LLMs but also sets the stage for the development of systems that can exceed human capabilities, making significant strides towards the realization of superintelligence.\nIn summary, our contributions are: (1) We propose a new training paradigm that enables LLMs to autonomously generate, refine, evaluate, and annotate data, which is then used to further train themselves. This method substantially cuts down on the time and expense associated with the post-training data preparation phase. (2) We demonstrate that Language Models as Continuous Self-Evolving Data Engineers (LANCE), capable of autonomously managing the entire post-training data construction process. This capability not only enhances the efficiency of data generation but also ensures high-quality data production. (3) We validate the effectiveness of LANCE across a variety of tasks, showing that it can continuously improve the performance of the model and maintain high-quality data generation."}, {"title": "2 Related Work", "content": "Post-Training After the pre-training phase, a Large Language Model (LLM) undergoes a post-training stage where it is fine-tuned to follow instructions, align with human preferences, and enhance specific capabilities (e.g., coding and reasoning) (Dubey et al., 2024). Common post-training methods include supervised fine-tuning (Wei et al., 2021) and preference learning (Stiennon et al., 2020; Rafailov et al., 2024). However, the success of these methods heavily relies on the availability and quality of human-annotated data."}, {"title": "3 Methodology", "content": "Figure 2 illustrates an overview of our approach, which begins with a small seed dataset. The model then generates instruction data and preference data through a data pipeline. These data are used to optimize the model via Negative Log-Likelihood (NLL) and Maximum Likelihood Estimation (MLE). The resulting model is then employed as the starting point for the next iteration. By repeating this cycle, the model's overall performance is continuously improved."}, {"title": "3.1 Preliminaries", "content": "Supervised Fine-Tuning (SFT) Supervised Fine-Tuning, also known as instruction tuning, is typically applied to a pre-trained LLM to enhance its ability to understand and follow instructions, improving its performance on specific tasks. For a given model $M_t$, the training dataset $D_S = \\{(x_i, Y_i)\\}_{i=1}^N$ consists of instruction-response pairs (x, y). During SFT, the LLM is trained to minimize the NLL loss:\n$L_{SFT} = -E_{(x,y)\\sim D_S} [\\sum_{t=1}^{T}log(M_t(Y_t | Y_{<t}, x)]  \\qquad (1)$\nFrom Equation 1, when the distribution of $M_t$ coincides with the conditional probability distribution p(y|x) of the responses in $D^S$, LSFT attains its minimum value.\nReinforcement Learning from Human Feedback (RLHF) As the name suggests, Reinforcement Learning from Human Feedback is where the model learns human preferences from human feedback through reinforcement learning, enabling the model to generate responses that align more closely with human expectations. This approach not only helps the model generate more desirable outputs across a wide range of tasks but also plays a crucial role in building safe, high-performance, and controllable AI systems (Ouyang et al., 2022).\nDirect Preference Optimization (DPO) (Rafailov et al., 2024) eliminates the need for a reward model and instead uses human preferences directly to optimize the model, making it a stable and computationally lightweight RLHF algorithm. Given the SFT model $M_S$ and the model under optimization $M_t$, the preference dataset $D_P = \\{(x, y^w, y^l)\\}_{i=1}^N$ is used for training. Here, $y^w$ and $y^l$ denote the preferred and dispreferred responses, respectively. The training objective is to minimize the MLE loss:\n$L_D = -E_{(x,y^w,y^l)\\sim D_P} [log \\sigma(\\Delta r(x, y^w, y^l))]  \\qquad (2)$\nIn this context, $\\Delta r(x, y^w, y^l)$ represents the difference in reward between the preferred response $y^w$ and the dispreferred response $y^l$, defined as:\n$\\Delta r(x,y) = f(x, y^w) - f(x, y^l)  \\qquad (3)$\nMore specifically, the rewards $f(x, y^w)$ and $r(x, y^l)$ correspond to the values assigned by the model under optimization, $M_t$, and the reference model, $M_S$, for the preferred and dispreferred responses, respectively. These rewards are computed as:\n$f(x, y) = \\beta log \\frac{M_t(y|x)}{M_S(y|x)}  \\qquad (4)$\nwhere $\\beta$ is a parameter controlling the deviation from the base reference policy, namely the initial SFT model $M_S$."}, {"title": "3.2 Initialization", "content": "Seed Data Our seed data consists of two components:\n1.  A small labeled dataset, which serves as the initial foundation for the model to sample new data.\n2.  A review dataset, where the inputs are an instruction and a response integrated into a prompt template, and the outputs are the rationale for assigning a score and the final score. This dataset is designed to facilitate the model's proficiency in scoring data, ensuring the seamless progression of the subsequent iteration process.\nThe data from these two components are randomly mixed together to form the seed data.\nLarge Language Model In the iterative loop, we use an LLM that has undergone SFT on the seed data as our initial model $M_0$. This SFT process equips the model with a foundational level of instruction-following and review capabilities. The model, starting from $M_0$, continues to improve through subsequent iterations, evolving into $M_1, M_t,..., M_N$, where t denotes the current iteration, with the process beginning at t = 0 and concluding at t = N."}, {"title": "3.3 Post-training data construction", "content": "Review Seed data The distribution of data points in an instruction tuning dataset is often uneven, with some topics or tasks having lower-quality instruction data than others. When such data is used for training, it may result in a language model with potential deficiencies in certain capabilities. Zheng et al. (2023) demonstrated the potential of LLM as a judge, showing that a well-trained LLM can achieve high agreement with human evaluations. Based on these insights, we utilize the language model $M_t$ to review the seed data, assessing its quality across different tasks and topics. This enables us to identify gaps in the original dataset and target the generation of new data to address these deficiencies.\nSpecifically, we design a template for additive evaluation prompts, scoring each data point on a scale of 0 to 10 across various criteria, including clarity, usefulness, challenge, safety, professionalism, and guidance. For each seed data point, the instruction and response are incorporated into the prompt template and input into the model $M_t$. $M_t$ then generates a review, providing a detailed rationale for the score along with the final score. This review serves as a key differentiator for how the data is referenced and used to generate new data in later stages of the process.\nReward-Based Generation For each seed data point, we compare the reward value generated in the previous step to a given threshold V. If the reward value is below V, it indicates that the dataset is poorly distributed across topics or tasks related to this data, potentially lacking high-quality examples. To mitigate the impact of data distribution deficiencies on model performance, we employ the few-shot learning (Brown, 2020) technique to prompt the model to generate new instructions based on the original data. These new instructions are then fed into $M_t$ to generate new responses, exploring the same topics as the original instructions but with higher quality. The resulting data is used for instruction tuning after further processing.\nIf the reward value meets or exceeds V, it suggests that this data is effective at instructing the model to accomplish that corresponding task. To enhance this, we utilize the few-shot learning technique to prompt the model to generate new, intentionally flawed responses containing misleading information. These generated responses form preference pairs with the original ones, which can then be used for preference learning in subsequent steps.\nData Filtering and Training Data Construction In order to construct a high-quality training dataset, the data generated in the previous step must first be filtered. We begin with length filtering to remove data that is too short or too long. Next, we use"}, {"title": "3.4 Self post-training full-cycle", "content": "The end-to-end algorithm for LANCE is presented in Algorithm 2. The process iteratively optimizes the language model over multiple rounds. Initially, the algorithm uses the initial seed data Seed, and the language model M to perform SFT, resulting in the initial model $M_0$. In each iteration t (from 0 to N-1), it calls Algorithm 1 to generate two datasets $D^P$ and $D^S$, which are used for the preference dataset"}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nModels We used Qwen2-7B and Qwen2-7B-Instruct (Yang et al., 2024) as the backbone models to assess the effectiveness of our training paradigm across different model sizes and alignment phases.\nDatasets We sampled 3,184 data points from UltraChat (Ding et al., 2023), which formed the first part of the seed dataset. Additionally, we sampled 5,632 data points from OpenAssistant Conversations (K\u00f6pf et al., 2024) and used the Llama3-70B (Dubey et al., 2024) model to generate reward rationales and corresponding scores. Only those reward rationales and scores that were consistent with the human-labeled scores were retained. These, together with the original instructions and responses, made up the second part of the seed dataset.\nBenchmarks In our experiment, we employed a diverse set of benchmarks to evaluate the model's performance across various capabilities, demon-"}, {"title": "4.2 LANCE Effectively Improves Benchmark Performance", "content": "Table 1 presents the results of LANCE and other iterative self-evolution methods across multiple benchmarks on Qwen2, showing the performance at their optimal iteration rounds. The \"Average\" column displays the mean scores of the models across these benchmarks. Figure 3 illustrates the average performance of these methods across each iteration round. Notably, our approach proves effective not only in post-training pre-trained models but also in further post-training fully trained models.\nFor the average scores, LANCE showed an improvement of 2.97 on Qwen2-7B over the initial model (SFT), and 2.70 on Qwen2-7B-Instruct. The most notable improvement was in mathematical abilities. On GSM8K, Qwen2-7B improved by 19.18, while Qwen2-7B-Instruct improved by 3.72. On MATH, Qwen2-7B improved by 5.48, and Qwen2-7B-Instruct by 12.34. For MMLU, a benchmark covering a wide range of domains and significant challenges, our method also yielded improvements: Qwen2-7B improved by 0.53, and Qwen2-7B-Instruct by 1.48. Other abilities either improved or remained largely unchanged.\nRegarding iterative self-evolution, it is clear that our method enables continuous iterative improvements, both for pre-trained models and fully post-trained models, as indicated by the trend in the line graph. In contrast, SPIN showed improvements only in the first round, with performance declining in subsequent rounds. I-SHEEP showed improvement in the first iteration on Qwen2-7B-Instruct, but performance degraded in later iterations, while on Qwen2-7B, improvements were slow and unstable. Limited by computational resources, we conducted only four rounds of iterative experiments. However, based on the previous iterations, we speculate that additional iteration steps would yield better results."}, {"title": "4.3 Ablation Studies", "content": "In this subsection, we have investigated the impact of various stages on the effectiveness of LANCE. Furthermore, we compared our method with the base model across multiple epochs of SFT on the seed dataset, thereby confirming that the performance improvement of our method is not due to overfitting on the seed dataset. By conducting SFT on equivalent amounts of data and comparing it with our method, we demonstrated that our approach does not rely solely on expanding the dataset to enhance performance. Instead, it generates more targeted and higher-quality data to improve the model's capabilities.\nProcess Ablation Table 2 illustrates the impact of omitting SFT-related or DPO-related components on the model's average performance. When the DPO-related components are removed, the model's performance can still improve iteratively in the first three iterations, but at a slower rate compared to the complete pipeline, and performance starts to decline after the fourth iteration. When the SFT-related components are removed, the performance improvement becomes highly unstable: after a performance gain in iter1, performance deteriorates in iter2 and iter3, and then improves again in iter4. These results highlight the necessity of a complete data generation and training pipeline for achieving fast and stable iterative self-evolution.\nComparison with SFT Figures 5 and 6 respectively present the comparison between our iterative self-evolution approach and multi-round SFT, as well as SFT with equivalent training data. Multi-round SFT refers to the initial model undergoing multiple epochs of SFT on the seed dataset, while equivalent data refers to sampling an equal amount of data from UltraChat and performing SFT on the initial model. The comparison with multi-round SFT demonstrates that our method does not merely involve crude repetition of the seed dataset, the performance improvement is not solely due to repeated learning of the seed data but rather the generation of new, valuable data. The comparison with SFT using equivalent training data clearly indicate that, under the same training data volume, models trained with LANCE consistently outperform those trained with SFT. This significant performance gap underscores the superior quality of the synthetic data generated by LANCE, which not only matches but exceeds the quality of commonly used high-quality instruction-tuning datasets. Moreover, the results highlight a crucial aspect of LANCE's effectiveness: its ability to achieve superior performance with minimal external supervision. Unlike traditional methods that heavily rely on extensive external supervision, LANCE leverages a small seed dataset as the initial external signal to generate a"}, {"title": "5 Conclusion", "content": "We introduce a novel training paradigm that empowers LLMs to autonomously generate, clean, review, and annotate data with preference information, significantly reducing the time and cost of post-training data construction. Our method, validated across various tasks, demonstrates continuous improvements in model performance, outperforming traditional supervised fine-tuning and other self-evolution methods. By ensuring that the generated data aligns with human values and preferences while greatly reducing the resource requirements for high-quality data creation, we take a step toward addressing a key bottleneck in the emergence of superintelligence, laying the groundwork for future systems that can exceed human capabilities."}, {"title": "Appendix", "content": "A Data Distribution Visualization\nWe sampled 1000 data points each from the seed dataset, the synthetic SFT dataset, and the DPO dataset. Using the stella_en_400M_v5 model(Kusupati et al., 2024), we extracted embeddings for each data point. Subsequently, we applied t-SNE dimensionality reduction to these embeddings and plotted the resulting feature vectors in Figure 7. The visualization reveals that the synthetic data generated by LANCE not only encompasses the distribution range of the original seed data but also explores new regions. This indicates that LANCE can produce data that aligns with a broader distribution, effectively expanding the original data distribution. Notably, the SFT dataset exhibits the most extensive distribution range, attributed to the generation of new instructions during its construction.\nB Evaluation Setting\nFigure 3 provides an overview of the evaluation details for all benchmarks included in this study.\nThe 'num shots' column indicates the number of examples provided when using few-shot prompts, 'version' denotes the version of the evaluation configuration file utilized, and 'eval tools' specifies the evaluation tools employed for assessing each benchmark.\nC Small-Scale Validation\nTo explore the boundaries of our method, we conducted experiments on models of varying scales. Generally, larger models tend to produce higher quality data and exhibit stronger filtering capabilities, facilitating easier self-evolution. Consequently, we performed preliminary exploration experiments using models of smaller scales compared to those in the main text. Specifically, we conducted four iterations of experiments on Qwen 2.5-1.5B (Team, 2024), with the results presented in Table 4. It was observed that the model's performance began to decline starting from the fourth iteration. This decline may be attributed to the weaker capabilities of the base model in generating, filtering, and annotating data, resulting in the inability to produce high-quality data and leading to the collapse of iterative training. However, on the whole, even on smaller-scale models, our method can still enhance the performance of the models to some extent. This demonstrates the versatility of our approach.\nD Iteration details\nFigure 5 illustrates the detailed scores of LANCE and baseline methods across each benchmark during the iterative process. The average performance changes between iterations reveal that LANCE consistently delivers iterative improvements when applied to both the pre-trained and fully fine-tuned models (indicated by red values for average changes). In contrast, SPIN only shows improvement in the first iteration and subsequently leads to a decline in model performance on both models. I-SHEEP demonstrates a similar pattern, with only the first iteration showing improvement on Qwen2-7B-Instruct, followed by performance degradation in subsequent iterations, while on Qwen2-7B, it only offers slow and unstable enhancements. These observations highlight the robustness of LANCE.\nE Case Study\nFigure 8 illustrates the comparison between the SFT data generated from reference seed data and"}]}