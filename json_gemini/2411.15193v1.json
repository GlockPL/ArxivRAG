{"title": "Gradient-Weighted Feature Back-Projection: A Fast Alternative to Feature Distillation in 3D Gaussian Splatting", "authors": ["Joji Joseph", "Bharadwaj Amrutur", "Shalabh Bhatnagar"], "abstract": "We introduce a training-free method for feature field rendering in Gaussian splatting. Our approach back-projects 2D features into pre-trained 3D Gaussians, using a weighted sum based on each Gaussian's influence in the final rendering. While most training-based feature field rendering methods excel at 2D segmentation but perform poorly at 3D segmentation without post-processing, our method achieves high-quality results in both 2D and 3D segmentation. Experimental results demonstrate that our approach is fast, scalable, and offers performance comparable to training-based methods. Project page: https://jojijoseph.github.io/3dgs-backprojection/", "sections": [{"title": "1. Introduction", "content": "3D Gaussian Splatting (3DGS) [3] is a novel-view synthesis technique that uses 3D Gaussians as rendering primitives, each defined by its mean position and variance. Additionally, these Gaussians carry payloads such as opacity and anisotropic color, enabling photorealistic rendering.\nGenerating intermediate feature maps directly, rather than passing RGB renderings to a foundation model, can be advantageous for tasks like segmentation. However, training Gaussian splatting models to render feature maps directly is computationally intensive, especially given the high dimensionality of feature space.\nMoreover, segmentation queries are more effective when performed on rendered features than on features assigned to each Gaussian, as only the surface Gaussians typically align with the feature map. Since rendered features result in a weighted sum, Gaussians deeper within the scene may not accurately correspond to features in the feature map, complicating accurate 3D segmentation.\nGiven these drawbacks in training time and segmentation accuracy, alternative methods are essential. One approach is to aggregate 2D features into Gaussians that influence the regions where the 2D features are projected. In other words, the Gaussian features are computed as a weighted sum of the 2D features, with weights proportional to the influence of each Gaussian on the corresponding region in the 2D feature map.\nOur main contributions are as follows:\n\u2022 We introduce a training-free approach that projects 2D features onto 3D Gaussians, providing a fast and scalable alternative to traditional feature field distillation.\n\u2022 We demonstrate that our method achieves comparable or superior performance to feature field distillation methods that rely on extensive training, particularly in producing clean 3D segmentations.\n\u2022 Our method effectively supports both rendered feature space and 3D feature space, enabling seamless querying for downstream applications such as 3D object manipulation and real-time scene understanding."}, {"title": "2. Related Works", "content": "Neural Radiance Fields (NeRF) [12] has emerged as a leading approach for synthesizing novel views from sparse image inputs. By leveraging a neural network, NeRF captures a volumetric representation of a scene, modeling the color and density at every 3D location to generate highly realistic views from arbitrary perspectives. However, the implicit nature of NeRF's scene representation poses challenges for tasks like object modification or rearrangement, as such operations typically demand retraining the entire network.\nIn contrast, 3D Gaussian Splatting [8] provides an explicit representation of 3D scenes, using 3D Gaussians as the primary rendering primitives. Attributes like color, opacity, and orientation characterize these Gaussians. This explicit structure allows for direct manipulation of Gaussians and their associated parameters, enabling efficient object rearrangement and editing. Such flexibility makes 3D Gaussian Splatting well-suited for interactive applications, including object editing, augmented reality, digital twins, and robotics.\nAn intuitive progression from radiance field rendering is feature field rendering, which incorporates additional feature embeddings to enrich the representation. Recent studies, such as [15, 17, 20], have advanced this concept for tasks like segmentation and semantic querying.\nFeature-3DGS [20] focuses on training high-dimensional feature embeddings, while LangSplat [15] prioritizes compressed, low-dimensional features. Both methods demonstrate strong performance in 2D segmentation of rendered outputs but face significant challenges with 3D object segmentation. Feature-3DGS attempts 3D segmentation by matching language embeddings with Gaussian feature embeddings. However, this approach often falls short because the features of individual Gaussians do not directly map to the final rendered feature, which results from the weighted sum of contributions from multiple Gaussians (see Equation 2). This inherent mismatch hinders reliable 3D segmentation. In contrast, our method overcomes this limitation by directly leveraging gradient information, enabling more accurate and effective 3D segmentation.\nAnother class of methods tackles object segmentation from visual prompts such as points and masks[1, 6, 7, 16]. In SAGD [6], a binary voting system is employed, while methods in [16] and [7] use similar influence-based voting approaches. FlashSplat [16] formulates segmentation as an optimization problem, whereas [7] leverages inference-time gradient backpropagation. Our approach also utilizes inference-time backpropagation for feature back-projection, enabling robust feature representation across 3D space."}, {"title": "3. Method", "content": "In this section, we present the feature back-projection equation and describe four direct use cases 3D segmentation, affordance transfer, and identity encoding\u2014that do not require post-processing other than similarity search. Notably, we perform these use cases directly in 3D space rather than in 2D image space, as is common in similar works."}, {"title": "3.1. Feature Back-Projection", "content": "Consider the color $C$ of a pixel at $(x, y)$ in a 3DGS rendering,\n$C(x,y) = \\sum_{n<N} C_n\\alpha_n(x,y) \\prod_{m<n} (1 - \\alpha_m(x,y)) \\tag{1}$\n$= \\sum_{n<N} C_n\\alpha_n(x, y)T_n(x, y) \\tag{2}$\nWhere $N$ is the total number of Gaussians, each indexed by its sorted position, $C_n$ is the color associated with the nth Gaussian, $\\alpha_n(x, y)$ is the opacity of the nth Gaussian at $(x, y)$ adjusted with exponential falloff, and $T_n = \\prod_{m<n} (1 - \\alpha_m(x,y))$ is the transmittance of nth Gaussian at $(x, y).\nFrom this equation, it's clear that the rendered color is a weighted sum of colors of individual Gaussians. Moreover, the weight is opacity multiplied by transmittance.\nTaking the derivative with respect to color of kth Gaussian $C_k$,\n$\\frac{\\partial C(x, y)}{\\partial C_k} = \\alpha_k(x, y)T_k(x, y) \\tag{3}$\nThis gradient is equivalent to the weight of each Gaussian in a pixel rendering. This insight enables us to leverage inference-time gradient backpropagation to compute feature back-projections based on the Gaussian's influence efficiently.\nGiven this gradient-based weighting, we can define the feature back-projection equation as follows:\n$f_k = \\frac{\\sum_{(x,y,n)} F_{2D}(x, y, n)\\alpha_k(x, y, n)T_k(x, y, n)}{\\sum_{(x,y,n)} \\alpha_k(x, y, n)T_k(x, y, n)} \\tag{4}$\nWe call this equation the expected feature back-projection equation or simply the back-projection equation. Where $f_k$ is the feature of kth Gaussian. $F_{2D}(x, y, n)$ is the feature at $(x, y)$ in the nth viewpoint. This formulation allows for efficient aggregation of features across viewpoints, weighted by opacity and transmittance, resulting in an accurate feature back-projection that reflects the Gaussian's contribution to the final rendered scene.\nWhen we remove the denominator from this equation, it becomes accumulated back-projection.\n$f_k = \\sum_{(x,y,n)} F_{2D}(x, y, n)\\alpha_k(x, y, n)T_k(x, y, n) \\tag{5}$\nIf we replace the feature $F_{2D}(x, y, n)$ with a function indicating binary function that indicates the presence of a mask this simply becomes the vote using masked gradients as described in [7].\nIf we do Euclidian normalization on $f_k$ after calculation, both equations 4, 5 become equivalent."}, {"title": "3.2. 3D Segmentation", "content": "Once the feature back-projection is complete, we obtain a set of feature vectors of dimension D, ${f_k} \\in \\mathbb{R}^D$ in the projected feature space. Let $q \\in \\mathbb{R}^D$ represent the embedding extracted from the language query (or a query from another modality). The scalar value $sim(f_k, q)$, where $sim$ denotes a similarity function (typically cosine similarity), measures the relevance of each Gaussian $g_k$ to the query.\nTo perform segmentation, we apply a threshold on $sim(f_k, q)$, allowing us to isolate Gaussians that correspond closely to the specified query. Formally, the set of segmented Gaussians $G$ can be defined as:\n$G = \\{g_k | sim(f_k, q) > \\theta\\}, \\tag{6}$\nwhere $\\theta$ is a user-defined threshold.\nSegmentation can also be performed in 2D. The equation 7 represents the 2D mask by querying over rendered 2D features.\n$M(x,y) = \\begin{cases} 1, & \\text{if } sim(f_k, q) > \\theta \\\\ 0, & \\text{otherwise} \\end{cases} \\tag{7}$\nHere, $M(x, y)$ is a binary mask that identifies regions in 2D space where the similarity to the query exceeds the threshold."}, {"title": "3.3. Affordance Transfer", "content": "Affordance transfer[5] is the process of transferring annotated regions from source images to a target object. Although the source and target objects may differ in appearance, they belong to the same category, enabling the transfer of meaningful information across instances. This technique is a particular case of few-shot segmentation, commonly applied in robotics, where annotations of irregularly shaped regions are transferred to a target object. This transfer allows a robot to manipulate the target object using knowledge from the annotated regions.\nOur approach follows a straightforward pipeline. First, we transfer DINOv2[4, 14] features to the target scenes via feature back-projection. Then, we identify the closest matching source annotation for each Gaussian in the target scene by performing a k-nearest neighbors (kNN) classification on the feature space. Here, the KNN classification finds source annotations most similar to the Gaussian features, ensuring that the transferred affordances correspond closely to the target object's structure."}, {"title": "3.4. Identity Encoding", "content": "When annotations are available for different objects within a scene, we can encode each object with a unique vector and back-project these vectors into the 3D scene. Even if annotations are missing for some views, this method can generalize across the entire 3D scene given sufficient annotated views.\nWe implement two types of identity encoding, each with unique strengths:\nOrthogonal Encoding: In this approach, we assign procedurally generated orthogonal vectors to represent each object uniquely, where each vector direction is associated with a distinct object. The orthogonality of these vectors ensures that each object has a distinct, non-overlapping identity in the feature space, maximizing separation and simplifying the segmentation task. This back-projection operation can be viewed as an extension of segmentation with masked gradients [7], enabling the simultaneous encoding of multiple classes within a single scene. While effective, orthogonal encoding becomes challenging when representing a large number of objects, as the dimensionality of the feature space limits orthogonal vectors.\nContrastive Encoding: For cases where we need to encode many objects, we use non-orthogonal vectors made sufficiently distinct through contrastive learning. Contrastive learning maximizes the distance between feature vectors of different objects by pulling representations of different objects apart and bringing representations of the same object closer together. This method allows us to encode more objects than possible with strictly orthogonal vectors while still achieving clear separation between object identities in feature space. Although the separation is not as strict as with orthogonal encoding, contrastive learning provides a scalable and effective alternative, maintaining discriminative power even with far more objects than the embedding dimension.\nThe loss function for training the Identity Encoder-Decoder model is defined as:\n$\\mathcal{L} = \\mathcal{L}_{classification} + \\mathcal{L}_{orthogonality}, \\tag{8}$\n$\\mathcal{L}_{classification} = CrossEntropy(\\hat{y}, y), \\tag{9}$\n$\\mathcal{L}_{orthogonality} = ||E E^T - I||_F. \\tag{10}$\nWhere $\\hat{y}$ is the model's predicted class probability and $y$ is the true class label. $E$ is an embedding matrix of size $N \\times D$, where $N$ is the number of classes and $D$ is the embedding dimension. $I$ is the identity matrix and $|| ||_F$ denotes the Frobenius norm."}, {"title": "4. Experiments and Results", "content": "4.1. Setup\nWe use gsplat[19] as our rasterizer. All experiments are performed on NIVIDA A6000 GPU.\nWe use LSeg[10], DINOv2[4, 14] features for our experiments. We also show we can encode objects using procedurally generated features as in orthogonal encoding and latent features generated by contrastive encoding.\nThere are cases where the numerator in the back-projection equation(4) is zero since there is no influence of Gaussian in the screen. In those cases, we can prune out the Gaussians with zero numerators before proceeding with inference."}, {"title": "4.2. 3D Segmentation", "content": "In this section, we use LSeg[10] features for our segmentation. We compare it qualitatively with Feature 3DGS. We use a custom implementation of Feature 3DGS based on gsplat [19] to prevent excessive RAM usage. We train Feature 3DGS versions for 7000 iterations. The training process takes around 20-30 minutes on our system, while the same number of iterations of Vannila 3DGS, which doesn't support feature field training, takes only 2 minutes 30 seconds. That is 10x faster compared to using Feature 3DGS.\nAfter back-projection, we do segmentation as follows, Let $Q = \\{q\\}$ be the set of prompts. Let $q_o$ be the category to segment, $q_i, i \\neq 0$ are negative prompts. We use the last prompt as 'other'.\nThe 3D mask is found by taking cosine similarity with Gaussian with both positive and negative prompts. Then, take the Gaussians, where the category prompt gives the highest cosine similarity as the 3D mask. Additionally, we can use a threshold over the cosine similarity with the category prompt."}, {"title": "4.3. Affordance Transfer", "content": "We transfer DINOv2 features to each scene of the [13] dataset. Then use source images and annotations from [7] and transfer it directly to the Gaussians of target scene. We label the method from [7] as 2D-2D-3D transfer because source annotations are transferred to 2D target frames before applying to 3D and our method as 2D-3D for contrasting."}, {"title": "4.4. Identity Encoding", "content": "Orthogonal Encoding: For simplicity we use one-hot encoding for identity encoding. That is one element is 1 and rest of them are 0. The qualitative results are shown in figure 4. The scene is taken from 3D-OVS dataset[11].\nContrastive Encoding: We use LERF-Mask dataset introduced for the identity encoding method Gaussian Grouping[18] for quantitative evaluation. Here we use 16 dimension embeddings for each group. Total number of groups are around 200.\nWe first train a classifier to predict the group over the embeddings. We make sure to use a contrastive loss to make embeddings far apart from each other. Then back-project this embeddings to each Gaussian. We use the classifier to predict the groups each rendered pixel belongs.\nWe follow the evaluation protocol from [18] to evaluate our method."}, {"title": "5. Discussion and Conclusion", "content": "We have introduced a novel, training-free, efficient, scalable alternative to feature field distillation in Gaussian splatting. Our method achieves fast, clean segmentation by directly querying the features associated with each Gaussian.\nOur approach aggregates features from all available training views in a single pass, mitigating any inconsistencies in individual 2D feature maps through an averaging effect. Nevertheless, minor imperfections may still propagate, though they have minimal impact on overall performance.\nSince our method does not update Gaussian parameters, it does not benefit from the regularization effect inherent in traditional feature field methods. However, we found no significant drawbacks when compared to feature field methods-in fact, our approach is considerably faster for generating feature embeddings for Gaussians and produces superior qualitative results."}]}