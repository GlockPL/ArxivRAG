{"title": "The Impact of Quantization and Pruning on Deep Reinforcement Learning\nModels", "authors": ["Heng Lu", "Mehdi Alemi", "Reza Rawassizadeh"], "abstract": "Deep reinforcement learning (DRL) has achieved remarkable success across various domains, such as video games, robotics,\nand, recently, large language models. However, the computational costs and memory requirements of DRL models often limit\ntheir deployment in resource-constrained environments. The challenge underscores the urgent need to explore neural network\ncompression methods to make RDL models more practical and broadly applicable. Our study investigates the impact of\ntwo prominent compression methods, quantization and pruning on DRL models. We examine how these techniques influence\nfour performance factors: average return, memory, inference time, and battery utilization across various DRL algorithms and\nenvironments. Despite the decrease in model size, we identify that these compression techniques generally do not improve\nthe energy efficiency of DRL models, but the model size decreases. We provide insights into the trade-offs between model\ncompression and DRL performance, offering guidelines for deploying efficient DRL models in resource-constrained settings.", "sections": [{"title": "Introduction and Background", "content": "Reinforcement learning has been applied in many fields, including robotics, video games, and recently Reinforcement Learning\nwith Human Feedback (RLHF) [1, 2, 3, 4, 5], has become common in large language models. RLHF methods mitigate biases\ninherent in language models themselves [4, 6, 7]. Reinforcement learning models that address real-world problems predominantly\nutilize continuous models based on neural network architecture, known as Deep Reinforcement Learning (DRL).\nDRL methods typically involve a world model, agents interacting with the world, and a reward function that evaluates the\neffectiveness of actions based on the agent's policy towards predefined objectives [8]. Depending on whether the algorithm learns\nspecific world model, DRL algorithms are categorized into model-based DRL algorithms and model-free DRL algorithms [9].\nModel-free DRL methods generally fall into three main categories: deep Q-learning methods [10, 11, 12], policy gradient methods\n13, 14, 15], and actor-critic methods [16, 17, 18, 19]. Unlike model-based algorithms, model-free approaches circumvent model\nbias and offer greater generalizability, which contributes to their popularity in RLHF applications [4, 20].\n\u2611 Neural networks, which are the backbone of DRL methods, are associated with high computational costs and, therefore,\nresource intensive. Recently, there has been a significant increase in the energy and water consumption of artificial intelligence\n(AI) data centers 1234. This trend has led to several research studies [21, 22, 23] investigating the resource costs of recent\nadvances in AI. Reducing the energy utilization of DRL has become a crucial need.\nAdditionally, many systems that benefit from reinforcement learning operate on battery-powered devices, such as extended\nreality devices and mobile robots. As the size of these devices decreases, their computational capabilities also diminish [24].\nOne common approach to reducing the computational costs of neural network models is compressing them via pruning and\nquantization [25, 26]. Network compression methods have been widely applied in computer vision [27, 28] and large language\nmodels [29, 30, 31] to improve inference times and reduce memory requirement with minimal compromise to accuracy. Com-\npression enables advanced DRL models to be deployed in robots with low latency and high energy efficiency under constrained\nresources. Despite its promise, neural network compression in DRL models has received less research attention [32, 33] compared\nto other fields, such as computer vision and natural language processing."}, {"title": "2 Methods", "content": "We have applied two types of neural network compression techniques -pruning and quantization- across five prominent\nDRL models: TRPO[14], PPO[15], DDPG[17], TD3[18], and SAC[19]. This section outlines our quantization methods, followed\nby our pruning methods."}, {"title": "2.1 Quantization", "content": "We applied linear quantization across all models, where the relationship between the original input $r$ and its quantized version $q$\nis defined as $r = S(q+Z)$. Here, $Z$ represents the zero point in the quantization space, and the scaling factor $S$ maps floating-point\nnumbers to the quantization space. For Post-Training Dynamic Quantization (PTDQ) and Post-Training Static Quantization\n(PTSQ), we computed $S$ and $Z$ for activations exclusively. In PTSQ, first, baseline models go through a calibration process\nto compute these quantization parameters and then the models make inferences based on the fixed quantization parameters.\nIn PTDQ, the quantization parameters are computed dynamically. In Quantization-Aware Training (QAT), baseline models\nare pseudo-quantized during training, meaning computations are conducted in floating-point precision but rounded to integer\nvalues to simulate quantization. Subsequently, the original models are converted into quantized versions, and the quantization\nparameters are stabilized."}, {"title": "2.2 Pruning", "content": "Neural network pruning typically involves removing neurons within layers, and dependencies can exist where pruning in one\nlayer affects subsequent related layers. The DepGraph approach we employed [50], addresses these dependencies by grouping\nlayers based on their inter-dependencies rather than manually resolving dependencies."}, {"title": null, "content": "Conceptually, one might consider constructing a grouping matrix $G \\in R^{L\\times L}$, where $G_{ij} = 1$ signifies a dependency between\nlayer $i$ and layer $j$. However, due to the complexity arising from non-local relations, $G$ can not be easily constructed. Thus,\ndependency graph $D$ is proposed, which only contains the local dependency between adjacent layers and from which the grouping\nmatrix can be reduced. These dependencies are categorized into two types: inter-layer dependencies, where the output of one\nlayer $i$ connects to the input of another layer $j$, and intra-layer dependencies, such as within BatchNorm layers, where inputs\nand outputs share the same pruning scheme.\nAfter constructing the dependency graph and determining grouping parameters based on this graph, we utilized a norm-\nbased importance score. However, directly summing importance scores across different layers can lead to meaningless results\nand potential divergence. Therefore, for a parameter $w$ in group $g$ with $K$ prune-able dimensions, a regularization term $R(g,k)$\nis used in sparse training to select the optimal input variables, $R(g,k) = \\Sigma_{k=1}^K \\gamma_k I_{g,k}$, where $I_{g,k} = \\sum_{w \\in g} ||w[k]||^2$ is the\nimportance for dimension $k$ in $L_2$ pruning and $\\gamma_k = 20(I_{max} - I_{g,k})/(I_{max} - I_{min})$."}, {"title": "3 Experiments", "content": "Our experiments are structured into two main components: quantization and pruning of DRL algorithms. We evaluated the\nperformance of TRPO[14], PPO[15], DDPG[17], TD3[18], and SAC[19] across five Gymnasium[51] (formerly OpenAI Gym[52])\nMujoco environments including: HalfCheetah, HumanoidStandup, Ant, Humanoid, and Hopper. These models are trained using\nGymnasium (formerly OpenAI Gym) environments.\nTo ensure consistency among our reported results, each experiment has been repeated at least 10 times in the same configu-\nration.\nQuantization and Pruning Libraries: The implementations of quantization and pruning in neural network libraries are\nnot as mature as other functionalities. For instance, in pyTorch pruning does not remove neurons but merely masks them. To\nensure the reliability of our experiments, we evaluated various quantization and pruning libraries and selected those that offer\nthe highest accuracy and resource efficiency.\nTherefore, to implement pruning, we explored PyTorch and Torch-pruning. In our experiment, Torch-pruning, integrated\nwith DepGraph [50], performed exceptionally well, and thus, we utilized it for pruning purposes. Regarding quantization, we\nexperimented with Pytroch, TensorFlow, and ONNX Runtime7. Ultimately, we chose PyTorch for QAT, and ONNX Runtime\nfor PTDQ and PTSQ.\nHardware Settings: Our hardware infrastructure included two NVidia RTX 4090 GPUs with 24GB of VRAM, 256GB\nof RAM, and an Intel Core i9 CPU running at 3.30 GHz. The operating system was Ubuntu 20.04 LTS, and we used CUDA\nVersion 12.0 for GPU operations."}, {"title": "3.1 Experimental Settings", "content": null}, {"title": "3.2 Quantization", "content": "To implement quantization, we experimented with three approaches: PTDQ, PTSQ and QAT. Quantization-aware training\n(QAT) involved initially training quantized models with an equivalent dataset size as the baseline models, followed by exporting\nthem into ONNX runtime for comparative analysis."}, {"title": "3.2.1 Average Return", "content": "The impact of quantization on average return is reported in Table 1. The table underscores the variability of quantization out-\ncomes across different environments and DRL models. For instance, QAT demonstrates its highest efficacy in HumanoidStandup\nenvironments, resulting in improved average returns across models except for PPO. The SAC algorithm generally benefits more\nfrom QAT, except in the Hopper environment, where its effectiveness is limited. Overall, PTDQ exhibits superior performance,\nwhile PTSQ consistently shows the lowest results. The observed performance discrepancies may stem from distribution shifts\nbetween data used for optimal path calculations and that utilized during the calibration phase, which are challenging to rectify\ndue to the stochastic nature of the environment."}, {"title": "3.2.2 Resource Utilization", "content": "To assess the impact of quantization on resource utilization, we conducted measurements and comparisons of memory usage,\ninference time, and energy consumption between baseline models and their quantized counterparts. Figure 1 illustrates the\ndifferences observed in inference time and energy usage between baseline and quantized models."}, {"title": "3.3 Pruning", "content": "To implement pruning, we utilized the torch-pruning package & for all our experiments. Each baseline model underwent $L_1$\nand $L_2$ pruning, with various pruning percentages ranging from 5% to 70%. In particular, experimented pruning percentages\nare as follows: {5%,10%,15%,20%,25%,30%,35%,40%,45%,50%,55%,60%,65%,70%}.\n\nThe optimal pruning method for each baseline model was determined based on earning at least 90% average return of the\ncorresponding baseline model while achieving the highest possible pruning percentage. The results of pruning experiments are\npresented in Table 2 and summarized comprehensively in Table 3. In Figure 2 and Figure 3, we present the effect of $L_1$ and $L_2$\npruning on the inference speed, energy usage, and memory usage. In these figures, we scaled the data according to the baseline."}, {"title": "4 Discussions and Findings", "content": "In this work, we studied two pruning approaches and three quantization approaches on five platforms (HalfCheetah-v4,\nHumanoidStandup-v4, Ant-v4, Hopper-v4, and Humanoid-v4) used for experimenting reinforcement learning methods and five\ncommon DRL methods (TRPO, PPO, DDPG, TD3). \u03a4\u03bf our knowledge, this is the largest study performed on compressing\nDRL methods, and we listed our findings in this section. These findings could be used as a guideline for further studies that try\nto compress DRL methods.\nPruning and quantization do not improve the energy efficiency and memory usage of DRL models. While pruning and\nquantization reduce model size (see Table 3), they do not necessarily enhance the energy efficiency of DRL models due to the\nmaintained or increased average return. Energy consumption tends to decrease only when there is a significant drop in average\nreturn, prompting the agent to terminate early and requiring less computation.\nDespite reducing model size, quantization does not improve memory usage, and pruning yields only a negligible 1% decrease\nin memory usage. Results in Figure 1 present no changes in memory utilization in any platforms while applying quantization.\nEven PTDQ and PTSQ cause more memory utilization than the baseline method. This might be due to the overhead of the\nquantization library, and the way it is implemented is not optimized.\n$L_2$ pruning is favored over $L_1$ pruning for most of DRL models. Table 2-3 illustrates that the optimal pruning method\nvaries based on the DRL algorithm and environmental complexity. Most environments, except for those trained with SAC on\nHalfCheetah, allow for substantial pruning without a notable decline in average return, while PPO models exhibit lower pruning\nthresholds. In instances where $L_1$ pruning outperforms $L_2$, the average return values remain closely aligned. Generally, a 10%\nreduction in DRL model size through $L_2$ pruning is beneficial, although exceptions include PPO models applied to HalfCheetah\nenvironments.\nPTDQ emerges as the superior quantization method for DRL algorithms, whereas PTSQ is not recommended. As shown in\nTable 1, 40% of our quantized models benefit from PTDQ, 36% from QAT, and only 24% from PTSQ. Our findings reveal that\npost-training dynamic quantization statistically outperforms the other methods, while post-training static quantization performs\nthe worst, likely due to distribution shifts between the calibration data and the randomness that existed in RL environments.\nThe Lottery ticket hypothesis [53] does not hold for DRL models. The Lottery Ticket Hypothesis (LTH) in the context\nof neural networks suggests that within a large, randomly initialized network, there exists a smaller sub-network, typically\naround 10-20% of the original size, that, when trained in isolation, can achieve performance comparable to the original large\nnetwork. This idea has significant implications for model quantization and pruning, two techniques used to reduce the size and\ncomputational requirements of neural networks. However, based on the results demonstrated in Table 2 demonstrate significant\nperformance drops in most models after 50% pruning, contradicting the hypothesis's assertion that original network performance\ncan persist even when pruned to less than 10%-20% of its original size. In particular, around 40% of the models don't survive\nafter more than 5% pruning, but 80% of the models don't survive after 50%.\nOur work has two limitations. First, by focusing on classical Mujoco environments with continuous action spaces, our\nwork excludes discrete action spaces, which are common in video games or some decision-making scenarios. However, in these\nsituations, task-specific methods might be employed for a satisfying performance, which adds additional complexities, and we\nmight explore it in our future work. Moreover, we are limited to six simulated environments, this leaves an important aspect of\nreal-world applicability unexplored. An ideal scenario is to experiment with this compression approach on a robot or drone in a\nreal-world task and measure the differences in their performance."}, {"title": "5 Conclusion", "content": "In this paper, we examined the effect of quantization methods and pruning methods on deep reinforcement learning algorithms.\nWhile the effect depended on the specific DRL algorithm used and the environment in which the agent is trained, the results\nshared some common patterns. Quantization converts the floating point with 32 bits into an integer with 8 bits model and\neffectively shrinks the model size while maintaining acceptable performance. We found that PTDQ models generally had the\nbest average return, while PTSQ models might suffer from distribution shifts and had poorer results. DepGraph pruned baseline\nmodels by constructing a dependency graph, and we experimented with $L_1$ and $L_2$ pruning. Experiments outlined that $L_2$\npruning was preferred for DRL algorithms on continuous action spaces, and in general, models benefited from 10% $L_2$ pruning\nwith some exceptions. However, while pruning actually removed some neurons, it did not always result in inference speedup or\nenergy saving."}]}