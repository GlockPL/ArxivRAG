{"title": "The Impact of Quantization and Pruning on Deep Reinforcement Learning Models", "authors": ["Heng Lu", "Mehdi Alemi", "Reza Rawassizadeh"], "abstract": "Deep reinforcement learning (DRL) has achieved remarkable success across various domains, such as video games, robotics, and, recently, large language models. However, the computational costs and memory requirements of DRL models often limit their deployment in resource-constrained environments. The challenge underscores the urgent need to explore neural network compression methods to make RDL models more practical and broadly applicable. Our study investigates the impact of two prominent compression methods, quantization and pruning on DRL models. We examine how these techniques influence four performance factors: average return, memory, inference time, and battery utilization across various DRL algorithms and environments. Despite the decrease in model size, we identify that these compression techniques generally do not improve the energy efficiency of DRL models, but the model size decreases. We provide insights into the trade-offs between model compression and DRL performance, offering guidelines for deploying efficient DRL models in resource-constrained settings.", "sections": [{"title": "Introduction and Background", "content": "Reinforcement learning has been applied in many fields, including robotics, video games, and recently Reinforcement Learning with Human Feedback (RLHF) [1, 2, 3, 4, 5], has become common in large language models. RLHF methods mitigate biases inherent in language models themselves [4, 6, 7]. Reinforcement learning models that address real-world problems predominantly utilize continuous models based on neural network architecture, known as Deep Reinforcement Learning (DRL).\nDRL methods typically involve a world model, agents interacting with the world, and a reward function that evaluates the effectiveness of actions based on the agent's policy towards predefined objectives [8]. Depending on whether the algorithm learns a specific world model, DRL algorithms are categorized into model-based DRL algorithms and model-free DRL algorithms [9]. Model-free DRL methods generally fall into three main categories: deep Q-learning methods [10, 11, 12], policy gradient methods 13, 14, 15], and actor-critic methods [16, 17, 18, 19]. Unlike model-based algorithms, model-free approaches circumvent model bias and offer greater generalizability, which contributes to their popularity in RLHF applications [4, 20].\n Neural networks, which are the backbone of DRL methods, are associated with high computational costs and, therefore, resource intensive. Recently, there has been a significant increase in the energy and water consumption of artificial intelligence (AI) data centers 1234. This trend has led to several research studies [21, 22, 23] investigating the resource costs of recent advances in AI. Reducing the energy utilization of DRL has become a crucial need.\nAdditionally, many systems that benefit from reinforcement learning operate on battery-powered devices, such as extended reality devices and mobile robots. As the size of these devices decreases, their computational capabilities also diminish [24]. One common approach to reducing the computational costs of neural network models is compressing them via pruning and quantization [25, 26]. Network compression methods have been widely applied in computer vision [27, 28] and large language models [29, 30, 31] to improve inference times and reduce memory requirement with minimal compromise to accuracy. Compression enables advanced DRL models to be deployed in robots with low latency and high energy efficiency under constrained resources. Despite its promise, neural network compression in DRL models has received less research attention [32, 33] compared to other fields, such as computer vision and natural language processing.\nSeveral general approaches have been proposed for neural network compression method quantization and pruning [34, 35]. As for quantization, DRL researchers might be more familiar with vector quantization, which aims to discretize continuous space into a discrete vector set to reduce dimensions [36, 37, 38, 39]. However, in this work, we specifically apply neural network quantization, which focuses on converting float32 format weights and biases into smaller-scale numbers such as int8 or 4-bits rather than vector quantization. Pruning methods, on the other hand, involve removing neurons deemed least important based on criteria such as weight or activation value [40, 41, 42, 43, 44].\nIn addition to conventional compression approaches [35, 25, 26], there are promising DRL-specific compression approaches [32, 36, 40, 43]. AquaDem [39] discretizes the action space and learns a discrete set of actions for each state s using behavior cloning from expert demonstrations [45]. The adaptive state aggregation algorithm [36] adaptively discretizes the state space based on Bregman divergence, enabling distinct partitions of the state space. Another group of methods focuses on scalar quantization that reduces the numeric precision of values [46, 40, 33, 32]. NNC-DRL[46] accelerates the DRL training process by speeding up prediction based on GA3C[47] and employs policy distillation to compress the behavior policy network prediction with minimal performance degradation. FIXAR [33] proposes quantization-aware training in fixed points to reduce model size without significant accuracy loss. ActorQ [32] introduces an int8 quantized policy block for rollouts within a traditional distributed RL training loop. PoPS[41] accelerates model speed by initially training a sparse network from a large-scale teacher network through iterative policy pruning, then compacting it into a dense network with minimal performance loss. UVNQ[40] integrates sparse variational dropout [48] with quantization, adjusting the dropout rate to enhance quantization awareness. Dynamic Structured Pruning[43] enhances DRL training by applying a neuron-importance group sparse regularizer and dynamically pruning insignificant neurons based on a threshold. Double Sparse Deep Reinforcement Learning [44] uses multilayer sparse-coding structural network with a nonconvex log regularizer to enforce sparsity while maintaining performance.\nIn this work, we apply common neural network compression methods, including common quantization and pruning, to five popular deep reinforcement learning models (TRPO[14], PPO[15], DDPG[17], TD3[18], and SAC[19]). We then measure the performance of these algorithms post-compression using metrics such as average return, inference time, and energy usage. In particular, we apply L\u2081 and L2 pruning techniques to these models. For quantization, we utilize int8 quantization and apply (i) post-training dynamic quantization, (ii) post-training static quantization, and (iii) quantization aware training across the listed models.\nTo our knowledge, this study represents the first comprehensive evaluation of the effects of pruning and quantization across a range of deep reinforcement learning models. Our experiments and findings offer valuable insights to researchers and developers, assisting them in making informed decisions when choosing between quantization or pruning methods for DRL models. Another prevalent approach for compressing neural network is knowledge distillation [49], but due to its model or application-specific nature (e.g., image classification), we did not include knowledge distillation in our experimental setup."}, {"title": "2 Methods", "content": "We have applied two types of neural network compression techniques -pruning and quantization- across five prominent DRL models: TRPO[14], PPO[15], DDPG[17], TD3[18], and SAC[19]. This section outlines our quantization methods, followed by our pruning methods."}, {"title": "2.1 Quantization", "content": "We applied linear quantization across all models, where the relationship between the original input r and its quantized version q is defined as \\(r = S(q+Z)\\). Here, Z represents the zero point in the quantization space, and the scaling factor S maps floating-point numbers to the quantization space. For Post-Training Dynamic Quantization (PTDQ) and Post-Training Static Quantization (PTSQ), we computed S and Z for activations exclusively. In PTSQ, first, baseline models go through a calibration process to compute these quantization parameters and then the models make inferences based on the fixed quantization parameters. In PTDQ, the quantization parameters are computed dynamically. In Quantization-Aware Training (QAT), baseline models are pseudo-quantized during training, meaning computations are conducted in floating-point precision but rounded to integer values to simulate quantization. Subsequently, the original models are converted into quantized versions, and the quantization parameters are stabilized."}, {"title": "2.2 Pruning", "content": "Neural network pruning typically involves removing neurons within layers, and dependencies can exist where pruning in one layer affects subsequent related layers. The DepGraph approach we employed [50], addresses these dependencies by grouping layers based on their inter-dependencies rather than manually resolving dependencies.\nConceptually, one might consider constructing a grouping matrix \\(G \\in R^{L \\times L}\\), where \\(G_{ij} = 1\\) signifies a dependency between layer i and layer j. However, due to the complexity arising from non-local relations, G can not be easily constructed. Thus, dependency graph D is proposed, which only contains the local dependency between adjacent layers and from which the grouping matrix can be reduced. These dependencies are categorized into two types: inter-layer dependencies, where the output of one layer i connects to the input of another layer j, and intra-layer dependencies, such as within BatchNorm layers, where inputs and outputs share the same pruning scheme.\nAfter constructing the dependency graph and determining grouping parameters based on this graph, we utilized a norm-based importance score. However, directly summing importance scores across different layers can lead to meaningless results and potential divergence. Therefore, for a parameter w in group g with K prune-able dimensions, a regularization term R(g,k) is used in sparse training to select the optimal input variables, \\(R(g,k) = \\Sigma^{K}_{k=1} \\gamma_k I_{g,k}\\), where \\(I_{g,k} = \\sum_{w \\in g} ||w[k]||_2\\) is the importance for dimension k in L2 pruning and \\(\\gamma_k = \\frac{2\\sigma(I_{max} - I_{g,k})}{(I_{max}-I_{min})}\\)."}, {"title": "3 Experiments", "content": "Our experiments are structured into two main components: quantization and pruning of DRL algorithms. We evaluated the performance of TRPO[14], PPO[15], DDPG[17], TD3[18], and SAC[19] across five Gymnasium[51] (formerly OpenAI Gym[52]) Mujoco environments including: HalfCheetah, HumanoidStandup, Ant, Humanoid, and Hopper. These models are trained using Gymnasium (formerly OpenAI Gym) environments.\nTo ensure consistency among our reported results, each experiment has been repeated at least 10 times in the same configuration.\nQuantization and Pruning Libraries: The implementations of quantization and pruning in neural network libraries are not as mature as other functionalities. For instance, in pyTorch pruning does not remove neurons but merely masks them. To ensure the reliability of our experiments, we evaluated various quantization and pruning libraries and selected those that offer the highest accuracy and resource efficiency.\nTherefore, to implement pruning, we explored PyTorch and Torch-pruning. In our experiment, Torch-pruning, integrated with DepGraph [50], performed exceptionally well, and thus, we utilized it for pruning purposes. Regarding quantization, we experimented with Pytroch, TensorFlow, and ONNX Runtime7. Ultimately, we chose PyTorch for QAT, and ONNX Runtime for PTDQ and PTSQ.\nHardware Settings: Our hardware infrastructure included two NVidia RTX 4090 GPUs with 24GB of VRAM, 256GB of RAM, and an Intel Core i9 CPU running at 3.30 GHz. The operating system was Ubuntu 20.04 LTS, and we used CUDA Version 12.0 for GPU operations."}, {"title": "3.2 Quantization", "content": "To implement quantization, we experimented with three approaches: PTDQ, PTSQ and QAT. Quantization-aware training (QAT) involved initially training quantized models with an equivalent dataset size as the baseline models, followed by exporting them into ONNX runtime for comparative analysis."}, {"title": "3.2.1 Average Return", "content": "The impact of quantization on average return is reported in Table 1. The table underscores the variability of quantization out- comes across different environments and DRL models. For instance, QAT demonstrates its highest efficacy in HumanoidStandup environments, resulting in improved average returns across models except for PPO. The SAC algorithm generally benefits more from QAT, except in the Hopper environment, where its effectiveness is limited. Overall, PTDQ exhibits superior performance, while PTSQ consistently shows the lowest results. The observed performance discrepancies may stem from distribution shifts between data used for optimal path calculations and that utilized during the calibration phase, which are challenging to rectify due to the stochastic nature of the environment."}, {"title": "3.2.2 Resource Utilization", "content": "To assess the impact of quantization on resource utilization, we conducted measurements and comparisons of memory usage, inference time, and energy consumption between baseline models and their quantized counterparts. Figure 1 illustrates the differences observed in inference time and energy usage between baseline and quantized models."}, {"title": "3.3 Pruning", "content": "To implement pruning, we utilized the torch-pruning package & for all our experiments. Each baseline model underwent L\u2081 and L2 pruning, with various pruning percentages ranging from 5% to 70%. In particular, experimented pruning percentages are as follows: {5%,10%,15%,20%,25%,30%,35%,40%,45%,50%,55%,60%,65%,70%}."}, {"title": "4 Discussions and Findings", "content": "In this work, we studied two pruning approaches and three quantization approaches on five platforms (HalfCheetah-v4, HumanoidStandup-v4, Ant-v4, Hopper-v4, and Humanoid-v4) used for experimenting reinforcement learning methods and five common DRL methods (TRPO, PPO, DDPG, TD3). \u03a4\u03bf our knowledge, this is the largest study performed on compressing DRL methods, and we listed our findings in this section. These findings could be used as a guideline for further studies that try to compress DRL methods.\nPruning and quantization do not improve the energy efficiency and memory usage of DRL models. While pruning and quantization reduce model size (see Table 3), they do not necessarily enhance the energy efficiency of DRL models due to the maintained or increased average return. Energy consumption tends to decrease only when there is a significant drop in average return, prompting the agent to terminate early and requiring less computation.\nDespite reducing model size, quantization does not improve memory usage, and pruning yields only a negligible 1% decrease in memory usage. Results in Figure 1 present no changes in memory utilization in any platforms while applying quantization. Even PTDQ and PTSQ cause more memory utilization than the baseline method. This might be due to the overhead of the quantization library, and the way it is implemented is not optimized.\nL2 pruning is favored over L\u2081 pruning for most of DRL models. Table 2-3 illustrates that the optimal pruning method varies based on the DRL algorithm and environmental complexity. Most environments, except for those trained with SAC on HalfCheetah, allow for substantial pruning without a notable decline in average return, while PPO models exhibit lower pruning thresholds. In instances where L\u2081 pruning outperforms L2, the average return values remain closely aligned. Generally, a 10% reduction in DRL model size through L2 pruning is beneficial, although exceptions include PPO models applied to HalfCheetah environments.\nPTDQ emerges as the superior quantization method for DRL algorithms, whereas PTSQ is not recommended. As shown in Table 1, 40% of our quantized models benefit from PTDQ, 36% from QAT, and only 24% from PTSQ. Our findings reveal that post-training dynamic quantization statistically outperforms the other methods, while post-training static quantization performs the worst, likely due to distribution shifts between the calibration data and the randomness that existed in RL environments.\nThe Lottery ticket hypothesis [53] does not hold for DRL models. The Lottery Ticket Hypothesis (LTH) in the context of neural networks suggests that within a large, randomly initialized network, there exists a smaller sub-network, typically around 10-20% of the original size, that, when trained in isolation, can achieve performance comparable to the original large network. This idea has significant implications for model quantization and pruning, two techniques used to reduce the size and computational requirements of neural networks. However, based on the results demonstrated in Table 2 demonstrate significant performance drops in most models after 50% pruning, contradicting the hypothesis's assertion that original network performance can persist even when pruned to less than 10%-20% of its original size. In particular, around 40% of the models don't survive after more than 5% pruning, but 80% of the models don't survive after 50%.\nOur work has two limitations. First, by focusing on classical Mujoco environments with continuous action spaces, our work excludes discrete action spaces, which are common in video games or some decision-making scenarios. However, in these situations, task-specific methods might be employed for a satisfying performance, which adds additional complexities, and we might explore it in our future work. Moreover, we are limited to six simulated environments, this leaves an important aspect of real-world applicability unexplored. An ideal scenario is to experiment with this compression approach on a robot or drone in a real-world task and measure the differences in their performance."}, {"title": "5 Conclusion", "content": "In this paper, we examined the effect of quantization methods and pruning methods on deep reinforcement learning algorithms. While the effect depended on the specific DRL algorithm used and the environment in which the agent is trained, the results shared some common patterns. Quantization converts the floating point with 32 bits into an integer with 8 bits model and effectively shrinks the model size while maintaining acceptable performance. We found that PTDQ models generally had the best average return, while PTSQ models might suffer from distribution shifts and had poorer results. DepGraph pruned baseline models by constructing a dependency graph, and we experimented with L\u2081 and L2 pruning. Experiments outlined that L2 pruning was preferred for DRL algorithms on continuous action spaces, and in general, models benefited from 10% L2 pruning with some exceptions. However, while pruning actually removed some neurons, it did not always result in inference speedup or energy saving."}]}