{"title": "FROM SPARSE TO DENSE: TODDLER-INSPIRED REWARD\nTRANSITION IN GOAL-ORIENTED REINFORCEMENT LEARNING", "authors": ["Junseok Park", "Hyeonseo Yang", "Min Whoo Lee", "Won-Seok Choi", "Minsu Lee", "Byoung-Tak Zhang"], "abstract": "Reinforcement learning (RL) agents often face challenges in balancing exploration and exploitation,\nparticularly in environments where sparse or dense rewards bias learning. Biological systems, such\nas human toddlers, naturally navigate this balance by transitioning from free exploration with sparse\nrewards to goal-directed behavior guided by increasingly dense rewards. Inspired by this natural\nprogression, we investigate the Toddler-Inspired Reward Transition in goal-oriented RL tasks. Our\nstudy focuses on transitioning from sparse to potential-based dense (S2D) rewards while preserving\noptimal strategies. Through experiments on dynamic robotic arm manipulation and egocentric 3D\nnavigation tasks, we demonstrate that effective S2D reward transitions significantly enhance learning\nperformance and sample efficiency. Additionally, using a Cross-Density Visualizer, we show that S2D\ntransitions smooth the policy loss landscape, resulting in wider minima that improve generalization\nin RL models. In addition, we reinterpret Tolman's maze experiments, underscoring the critical role\nof early free exploratory learning in the context of S2D rewards.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) is a branch of machine learning where agents make decisions to maximize environmental\nrewards, balancing between exploration \u2013 trying new actions \u2013 and exploitation, using known actions to optimize rewards.\nAdjusting the density of the reward function-between sparse and dense-plays a crucial role in achieving an effective\nbalance, as it directly shapes the agent's exploration and decision-making process [21, 23]. However, excessively\nsparse or dense rewards can bias this balance, hindering effective learning, especially in complex environments with\nhigh-dimensional inputs such as egocentric raw image observations from 3D real-world-like settings [36, 51, 59].\nTherefore, achieving this balance necessitates a deeper understanding of the interplay between sparse and dense reward\nstructures. Sparse rewards, typically provided only upon achieving specific goals, encourage extensive environmental\nexploration but can significantly slow down learning [2, 32]. Conversely, dense rewards offer frequent feedback,\naccelerating learning but may cause agents to prioritize short-term gains over long-term strategies [34]. Given these\ntrade-offs, relying solely on one type of reward structure may fail to capture the complexities required for effective RL\nlearning.\nTo address this challenge, we draw inspiration from toddlers, who naturally leverage both sparse and dense rewards\nduring their developmental learning processes. Initially, as depicted in Figure 1-(a), toddlers act as innate explorers,"}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Exploration-Exploitation in Deep Reinforcement Learning", "content": "Balancing exploration and exploitation is a key challenge in deep RL [33]. Exploration allows agents to discover new\nstrategies, while exploitation maximizes rewards from known behaviors. Striking this balance is particularly challenging\nin sparse-reward settings, where feedback is rare and tied to specific goals, offering little guidance for effective learning.\nTo address this, additional rewards are introduced through two complementary methods. Extrinsic rewards, aligned\nwith task objectives, provide feedback for intermediate milestones, guiding agents toward their goals. Intrinsic rewards,\ndriven by curiosity or novelty, promote exploration of new states using techniques like next-state prediction [3, 4, 48].\nThese mechanisms work together to help agents overcome the limitations of sparse rewards by encouraging exploration\nwhile maintaining goal-oriented behavior. Within this framework, we propose a reward strategy inspired by human\ndevelopment. Similar to toddlers, who initially explore freely in sparse-reward environments before transitioning\nto goal-directed behaviors supported by denser feedback, we investigate how this paradigm can enhance RL agents'\nadaptability, exploration efficiency, and overall performance across varying reward structures."}, {"title": "2.2 Toddler-Inspired Learning", "content": "The developmental stages of toddlers have provided a novel perspective for advancing deep learning. Researchers\nstudied the natural exploratory behaviors and unique learning mechanisms of toddlers and discovered ways to refine\nboth supervised and reinforcement learning approaches. For example, classifiers trained on datasets reflecting a toddler's\nperspective of objects have been shown to outperform those based on adult perspectives [5], demonstrating the benefits\nof exploration-centered learning. Similarly, critical learning periods in toddlers correspond to similar phases in RL\n[9, 46] and deep neural networks [1]. These toddler-inspired methodologies highlight significant parallels between\nbiological growth and AI model development, underscoring the value of biological insights in advancing AI."}, {"title": "2.3 Curriculum Learning", "content": "Curriculum Learning (CL), inspired by educational curriculums, has been shown to improve training speed [20], learning\nefficiency, and safety [57] in machine learning. The progression of CL from easy to more challenging tasks is effective\nin enhancing generalization and convergence rates [6, 58] in both supervised and reinforcement learning [12, 18, 41].\nWhile numerous studies focus on easy-to-hard tasks [10, 11, 25], other studies [37, 60] suggest a general-to-specific\napproach. In such an approach, agents first gather diverse experiences and then exploiting them. Following this idea,\nwe incorporate the toddler-inspired S2D reward transition into RL, applying it to goal-directed reward transitions."}, {"title": "2.4 Potential-Based Reward Shaping (PBRS)", "content": "In RL, the objective is to maximize cumulative rewards. However, designing optimal reward functions often poses\nsignificant challenges, frequently involving intensive reward engineering. Reward Shaping (RS) is a well-established\nmethod used to accelerate training by offering supplementary feedback [54]. When reward structures are variable,\npotential-based reward shaping ensures that optimal strategies remain stable by integrating rewards based on potential\nfunctions [43]. Traditionally, these shaped rewards are applied consistently throughout the training process. In contrast,\nour study introduces the concept of Toddler-Inspired Reward Transition, examining the impact of dynamically adjusting\nreward density over time."}, {"title": "3 Preliminaries", "content": null}, {"title": "3.1 Reinforcement Learning", "content": "Reinforcement learning (RL) is a field of machine learning particularly suited for solving sequential decision-making\nproblems. The core principle of RL is to maximize an agent's expected reward through trial and error, analogous to\nhow humans acquire skills to complete tasks. RL problems are commonly modeled using a Markov Decision Process\n(MDP), defined as \u3008S, A, P, R, \u03b3), which consists of the following components: a set of states S, a set of actions A, a\nstate transition probability matrix P : S \u00d7 A \u2192 S, and a reward function R : S \u00d7 A \u2192 R. The discount factor y is\nused to limit the influence of rewards from distant future states in a trajectory.\nAt each time step t, the agent selects an action at \u2208 A based on a policy \u03c0(\u00b7|st), which specifies a probability over\nactions given the current state st \u2208 S. The MDP updates its state to st+1 ~ P(\u00b7|st, at), and the agent receives a reward\nR(st, at) during the transition. The goal of an RL algorithm is to determine an optimal policy \u03c0* \u2208 \u03a0* \u2286 II, where\nII is the set of all possible policies, and II* represents the subset of policies that maximizes the expected cumulative\nreward R = \u0395 [\u03a3\u03c4=0y+R (st, at)]."}, {"title": "3.2 Potential-Based Reward Shaping", "content": "To improve an agent's performance, selecting an appropriate curriculum is crucial. In this study, we argue that adjusting\nthe proportions of provided rewards is instrumental in achieving robust and generalized performance. Formally, we\ndefine supp(R) \u2286 S as the support set of the reward function R. In other words, supp(R) comprises the states that\nyield non-zero rewards for certain actions:\nsupp(R) = {s \u2208 S | \u2203a \u2208 A s.t. R(s,a) \u2260 0}.\nThe sparsity of a reward function is quantified by the ratio of the cardinalities of supp(R) and S. For two reward\nfunctions RD and Rs defined on S, we say that R\u0189 is denser than Rs if the condition |supp(Rs)| < |supp(RD)|\nis satisfied. In the context of curriculum learning, we assume that the support set of a dense reward function RD\nencompasses that of a sparse reward function Rs: supp(Rs) \u2286 supp(RD).\nFor reward transition, mechanisms that systematically move from sparse to dense rewards while maintaining learning\nstability are essential. Potential-based reward shaping (PBRS) provides a practical approach by densifying the reward\nsignal with an additional potential-based reward Fi, all while preserving the optimal policy. In PBRS, the potential-based\nreward Fi for the i-th MDP is defined as follows:"}, {"title": "3.3 Multi-stage RL with Potential-based Reward Function", "content": "Curriculum learning [20, 57] is a multi-stage approach for training models robustly by progressively adjusting the difficulty of\ntasks over time. In RL, curriculum learning is defined as a series of MDPs {M}1 where each MDP M\u2081 = (S, A, P, Ri, Y)\nis characterized by a unique reward function Ri, representing different task difficulties [6, 58]. By setting the stage transitions\nT = (T1, T2, TN-1), the MDP transitions from one to another.\nDefinition 1 (Curriculum) Let a series of MDPs be {M}=1 with M\u2081 = (S, A,P,Ri, y), and its state transitions be T =\n(T1, T2,\uff65\uff65\uff65, Tv\u22121) \u2208 NN\u22121. A curriculum C is defined as a tuple C = ({M\u2081}=1,T) where the M1(t;T) is chosen to train the\nagent at training step t \u2208 N. The stage indicator I(t;T) is defined as:\nVi, Vt \u2208 [Ti-1, Ti), I(t; T) := i,\nwhere To := 0 and TN: 8."}, {"title": "3.4 Wide Minima Phenomenon and Loss Landscape", "content": "In deep neural networks, the loss landscape refers to the multi-dimensional space where each point's altitude represents the loss\nfor specific parameters [35]. The objective is to find minima in this landscape. Wide minima have broad gradients, facilitating\nsmooth convergence to global minima via gradient descent, which enhances robustness and generalization to data distribution\nperturbations [27]. In contrast, sharp minima possess steep gradients that are sensitive to such perturbations, often leading to\noverfitting and poor generalization [15]. Empirical studies have shown that models located in wide minima tend to perform better\nand generalize more effectively than those situated in sharp minima [24, 28]. This principle also applies to RL, where the distribution\nof the experiences of an agent can vary slightly at each time step. Our empirical results confirm that policies positioned in wide\nminima improve generalization and robustness in these fluctuating environments."}, {"title": "3.5 Tolman's Maze Experiment", "content": "The classic maze experiment conducted by Edward C. Tolman provides a foundational basis for understanding the Sparse-to-Dense\n(S2D) reward transition strategy [56]. Tolman's study revealed how rats navigated mazes under varying reward conditions, yielding\nvaluable insights into the role of free exploration and reward timing. Specifically, three groups of rats were tested:\n1. No Reward Group: Rats freely explored the maze without receiving any rewards. (analogous to only sparse)\n2. Consistent Reward Group: Rats received rewards consistently upon reaching the goal. (analogous to only dense)\n3. Delayed Reward Group: Rats began in a reward-free phase but later transitioned to consistent rewards. (analogous to\nSparse-to-Dense, S2D)\nNotably, the Delayed Reward Group outperformed others once rewards were introduced, suggesting that the period of free exploration\nallowed the rats to form internal representations, or cognitive maps, of their environment. These cognitive maps facilitated efficient\nnavigation when the rewards became available. Inspired by these cognitive and developmental phenomena, our study explores\nwhether free exploration under sparse rewards in the S2D framework can similarly cultivate foundational experiences in AI agents,\nthereby enhancing their ability to construct cognitive maps and ultimately improving learning efficiency and policy robustness in RL."}, {"title": "4 Method", "content": "To implement our experiments, we design a reward transition frameworks inspired by toddler behavior. We investigate how this\ntransition affects agent learning, focusing on its impact on the policy loss landscape and the emergence of wide minima. Inspired by\nTolman's experiments, we further examine the role of free exploration under sparse rewards within S2D frameworks by analyzing the\ninternal representations formed."}, {"title": "4.1 Toddler-Inspired Sparse to Dense Reward Curriculum", "content": "We first design the Sparse to Dense (S2D) reward transition to infuse the exploration-to-exploitation strategy into curriculum learning.\nA curriculum & becomes an S2D-curriculum if the reward functions {R}=1 of their respective MDPs {M}=1 gradually become\ndenser while preserving optimal policies.\nDefinition 2 (Toddler-inspired S2D-curriculum) A curriculum C = ({M}=1,T) with its corresponding MDPs Mi =\n(S, A, P, Ri, y) is an S2D-curriculum if the following conditions are satisfied:\nsupp(R1) supp(R2) \u2286\u2286 supp(RN)\n, \u05db\u05db\u05d7\u05db\u05d7\nII is a set of optimal policies within the MDP Mi. Equation 2 indicates that the sequence of reward functions should increase in\ndensity. Equation 3 constrains the optimality on the policies such that the optimal policies of Mi+1 are also optimal in Mi.\nFrom Equations 2 and 3, the reward functions must become denser while preserving the same set of optimal policies. To achieve this,\nwe use the potential-based reward shaping (PBRS) approach [22, 42], which allows adjusting the reward density without altering the\noptimal policy.\nFor the experiments, we assume that the agent can only get a reward if it reaches the goal g\u2208 G within a certain radius in the\nsparse reward setting (M1): F\u2081(s) = 0. On the other hand, in the dense reward setting (M2, M3), the agent gets an additional\npotential-based dense reward Fi>2 with the potential function (), shown in Equation 4:"}, {"title": "4.2 Visualizing Policy Loss Landscapes", "content": "This study examines the impact of the S2D transition on the policy loss landscape. Following the method outlined in [35], we plot\npolicy loss landscapes by varying parameters \u0113 = 0 + ax + \u03b2y, where 0 denotes the current parameters and a and \u1e9e are normalized\ncoordinates. The axes, represented by vectors x and y, introduce specific perturbations in the parameter space. These vectors are\nnormalized to have unit length and are orthogonalized for clarity and consistency in scaling. The z-axis represents the average policy\nloss over a batch of transitions from the replay buffer. It is important to note that the relative position of one landscape over another\nis not significant since each landscape corresponds to distinct network parameters with different loss ranges due to varying stages of\nlearning.\nGiven the lack of effective visualization techniques for policy loss landscapes during reward transitions in previous research, we\nhave created the Cross-Density Visualizer. This tool provides a 3D view of the shift of policy loss landscapes from exclusively\nsparse or dense rewards to mixed-reward settings. Our approach involves two distinct sets of transitions: Sparse-to-Dense (S2D) and\nSparse-to-Sparse (Only Sparse) in one, and Dense-to-Sparse (D2S) and Dense-to-Dense (Only Dense) in the other. As illustrated in\nFigure 5 and further elaborated in Appendices B and C, our visualizations reveal smoothing effects, especially prominent in the S2D\nmodel."}, {"title": "4.3 Exploring Minima Sharpness After Reward Transitions", "content": "Our findings of smoothing effects prompted us to hypothesize that the S2D transition helps escape local minima and enhances\ngeneralization in wider minima. Wide minima in neural networks are indicative of robust and adaptable models [24, 28]. By\ninvestigating minima after this transition, we aim to enhance performance and gain a better understanding of agent adaptability in\nvarious situations. To evaluate the extent to which the policy remains in wide minima, we measure the end-of-training convergence\nof the neural network of S2D to wide minima using the sharpness metric defined in Equation 5 and compare it with those of other\ntransitions. This follows the approach proposed in [13], which outlines a specific form of sharpness measure as described in [28]."}, {"title": "4.4 Analyzing Policy Behavior in Tolman's Maze Experiments", "content": "We also examine the impact of the S2D transition on agents' internal representations, inspired by Tolman's maze experiments [56].\nWe hypothesize that early free exploration under sparse rewards fosters robust initial parameters through diverse experiences, enabling\nefficient policy learning under dense rewards. Using (1) RNN feature convergence, (2) policy visualization and (3) Visualization of\nTrajectory, we highlight another dimension of the S2D approach's advantages."}, {"title": "4.4.1 Measuring the Mean Distance Between RNN Features", "content": "In our partially observable 3D egocentric playroom maze, the agent uses a recurrent neural network (RNN) to maintain hidden states.\nTo measure how quickly these internal representations converge, we:"}, {"title": "4.4.2 Action Frequency Analysis", "content": "Figure 9-(b) presents a temporal analysis of discrete action frequencies, such as move forward, turn left, and turn right, over\ncheckpoints saved at regular intervals during training. The sparse-to-dense reward transition occurred at 3 million steps, and the\nstatistical distribution of actions sampled from the policy \u03c0(\u03b1 | s) was evaluated over three rollout episodes. For each step t within\nan episode, actions at were drawn according to \u03c0(\u03b1 | st), where st denotes the observed state at time t. The aggregated frequency\nof an action a, denoted as fa, is computed as:"}, {"title": "4.4.3 Trajectory Visualization", "content": "The agent's spatial trajectories, defined by its global coordinates, are plotted on a 2D overhead map to visualize navigation patterns.\nFigure 7 highlights the most frequently traversed paths, identified via visual inspection. These paths represent common strategies\nemployed by the agents to reach the goal, often corresponding to optimal or near-optimal navigation routes. Frequent trajectories are\nextracted using frequency-based methods, providing insights into the agent's pathfinding efficiency. As shown in Figure 8, compared\nto an only dense\u2014which tends to be biased toward stronger rewards and thus exhibits more \u201cangular\u201d exploration\u2014S2D explores\nmore freely across multiple directions. This broader range of experiences leads to richer, more foundational learning."}, {"title": "5 Experiment Design", "content": "In this section, we explore in-depth the dynamics of the S2D reward transition compared to multiple reward-driven methods. We\ndiscuss its substantial effects across multiple challenging environments, as illustrated in Appendix A. We specifically explored the\nimplications of applying the reward transition to RL by addressing four critical questions:"}, {"title": "5.1 Reward Setting Details", "content": null}, {"title": "5.1.1 Reward-driven baselines for comparison.", "content": "As outlined in Figure 2, we examine four primary reward settings: (1) Only Sparse, where rewards are given only upon reaching the\ngoal, encouraging broad exploration; (2) Only Dense, which uses potential-based reward shaping (PBRS) [43] to provide additional"}, {"title": "5.1.2 Hyperparameter Analysis of Reward Transition Timing.", "content": "Furthermore, we analyzed hyperparameters for the timing of reward transitions through ablation studies (see Table 2). Inspired by\nearly developmental interactions [49, 53], we compare three transition points, t \u2208 {1N, 2N, 3N}, where N is roughly 1/12 of the\nentire training period. The specific value of N, adjusted for each environment's episode length, is detailed in Appendix A. These\ntransition points are labeled as C1, C2, and C3, respectively, for S2D and D2S reward transitions."}, {"title": "5.2 Environment Details", "content": "To evaluate the impact of reward dynamics, we tested under various conditions, including state-based and visual observations, as\nwell as both discrete and continuous action spaces, detailed in Appendix A-Table A.2. We examined different reward configurations,\nincluding the S2D reward transition, across several goal-directed tasks in established benchmark environments. Figure 3-(c) depicts\nexamples such as LunarLander [7], CartPole, and UR5 [55]. Appendix A provides a comprehensive description of the challenging\ndynamics introduced for UR5 and CartPole, with randomized placements for agents, goals, and obstacles, labeled as the 'reacher'\nversion. All agents had full access to state information and were assessed using the Soft Actor-Critic (SAC) [19] algorithm.\nAdditionally, we adjusted the reward structure for both sparse and dense settings, with additional details in Appendix A."}, {"title": "5.2.1 Enhanced Generalization Environment", "content": "To deepen the evaluation of generalization capabilities, we designed a challenging egocentric navigation scenario within the ViZDoom\nenvironment [26], as illustrated in Figure 3-(a). In the Seen environment (Appendix Figure A.12-(a), objects were randomly placed,\nand walls featured one of three textures. The Unseen environment (Appendix Figure A.12-(b)) required the agent to adapt to three\nnew wall textures, distinct from those in the Seen scenario. The A3C [40] algorithm was employed to assess performance in this\ncontext."}, {"title": "5.2.2 Tolman's Maze Environments", "content": "To emulate the learning behavior in Tolman's maze, we created two 3D egocentric navigation scenarios using the Minecraft toolkit\n(see Appendix A). The first scenario, illustrated in Figure 3b-Upper, is a cross maze where agents spawn in a designated blue zone at\nthe maze's center and must move outward along different corridors to reach three goals-two used in training and a newly introduced"}, {"title": "6 Results", "content": null}, {"title": "6.1 Performance Results", "content": null}, {"title": "6.1.1 Sample Efficiency and Success Rate", "content": "We conducted experiments in diverse environments with static points of view. The results are presented in Figure 4-(1-3) and\nTable 2. These environments vary in the agents' performance under sparse reward; LunarLander and CartPole-Reacher exhibited\npoor performance with default sparse rewards. In these scenarios, the S2D approach consistently outperformed all other baselines\nand showed superior sample efficiency. Even in the more challenging UR5-Reacher, which requires more precise control and\nhas a higher-dimensional action space, S2D still led the performance. Unlike intrinsic motivation-based algorithms that often\nprioritize exploration state over goal achievement, S2D outperformed other methods. Furthermore, we conducted experiments in\nViZDoom-Seen and Unseen, Minecraft Cross, and Minecraft playroom maze, which are environments with an egocentric viewpoint.\nSimilar to the results mentioned above, S2D exhibited superior performance across all cases, as demonstrated in Figure 4-(4-5),\nFigure 6 and Figure 7. Notably, D2S outcomes were consistently lower than those of S2D in all environments, highlighting the\neffectiveness of the S2D transition as a training curriculum."}, {"title": "6.1.2 Enhanced Generalization Performance", "content": "The S2D reward transition consistently outperformed other agents in various dynamic environments that require strong generalization,\nsuch as those with varying goal locations or agent spawn positions, as shown in Figure 3a to 3c. We specifically designed more\nchallenging environments that introduce visual changes not seen during training, illustrated in Figure 3a and 3b.\nIn the ViZDoom-Unseen environment, where agents face significant visual changes due to the addition of three new wall textures\n(Figure 3a), the S2D transition demonstrates superior generalization and sample efficiency compared to other baselines, as shown\nin Figure 4-(4),(5). Similarly, in the Minecraft Cross maze, where a newly occurring goal location appears during evaluation\n(Figure 3b-Upper), the S2D transition still displayed superior results, as shown in Figure 6."}, {"title": "6.2 Impact on 3D Policy Loss Landscape", "content": "Our visualizations, presented in Figure 5 and detailed in Appendix B, emphasize significant smoothing effects, especially with the\nS2D transition. In Figure 5, the upper row shows dense-to-dense (Only Dense) and D2S transitions, while the lower row displays\nS2D and sparse-to-sparse (Only Sparse) transitions. Significant smoothing effects were primarily observed during the S2D transition,\naiding in overcoming local minima and promoting wider minima, thereby enhancing generalization. These effects became evident\nafter the transition at T = 50 and T = 2000 in LunarLander, and at T = 3500 in Cartpole-Reacher. Detailed 3D visualizations are\nprovided in Appendix B.\nWhile our primary experiments focused on Soft Actor-Critic (SAC)[19], we also evaluated other algorithms, such as Proximal Policy\nOptimization (PPO)[52] and Deep Q-Network (DQN)[38], as detailed in Appendix C, and observed similar smoothing effects during\nthe S2D reward transitions. Moreover, to further illustrate these smoothing effects, we experimented with these other algorithms in a\ngridworld environment that reveals changes in the policy loss landscape more intuitively."}, {"title": "6.3 Results of Wide Minima", "content": "Using sharpness metrics, we analyzed the convergence behavior at the end of training for networks guided by S2D reward transitions\nand compared them to baseline models. Lower sharpness values, which correspond to wider minima, were found to be associated\nwith improved generalization. As evident from Table 2, only agents following the S2D reward transition converged to these wider\nminima, indicating superior performance in various complex environments."}, {"title": "6.4 Results of Tolman's Maze Experiment", "content": null}, {"title": "6.4.1 Cross Maze", "content": "We measured episode length, a performance metric used in Tolman's maze experiment. Figure 6a shows that agents using the S2D\nreward transition achieve consistently shorter episode lengths across Goal Points 0, 1, and 2 during training and evaluation compared\nto other reward structures. Consequently, the plot of S2D extended furthest along the horizontal axis, indicating that the agent\ncompleted more episodes within the same number of global steps.\nTo get a better understanding of learning trends, we measured the number of episodes completed during training and evaluation as\na function of global steps in Figure 6b. All S2D agents demonstrated a steeper increase in completed episodes, even in the more\nchallenging scenario of Goal Point 2. This indicates that S2D agents display higher sample efficiency and success rates across all\nscenarios, demonstrating superior performance and generalization to unseen goal positions."}, {"title": "6.4.2 playroom maze", "content": "Figure 7(a),(b)-(1),(2) show that S2D agents achieve significantly shorter episode lengths during training, indicating improved sample\nefficiency and enhanced performance compared to other reward strategies. This suggests that the S2D reward transition mechanism\neffectively guides agents to reach goals faster by balancing exploration and exploitation more efficiently and accelerating learning."}, {"title": "6.4.3 Visualization of Trajectory", "content": "In the playroom maze (Figure8-(a)), agent trajectories under different reward settings reveal significant differences in exploration\nbehavior. The top row showcases agents' extensive exploratory paths. S2D and Only Sparse agents exhibit diverse, exploratory\ntrajectories, providing opportunities to robustly learn about the environment and objects from various angles. This exploration\nsuggests that these agents can learn more about their environment, similar to how toddlers learn through extensive exploration. In\ncontrast, Only Dense agents show more direct and angular trajectories, indicating limited exploration and a focus on reaching the\ngoal quickly. This pattern suggests that dense reward agents focus on quickly reaching the goal, which may limit their ability to learn\nabout the environment comprehensively. The bottom row illustrates the most frequent shortest trajectories. S2D agents show the\nmost efficient paths to the goal, effectively balancing the exploration-exploitation trade-off. In the Cross Maze (Figure8-(b)), similar\npatterns are observed. Agents using the S2D reward transition demonstrate better shortest trajectories."}, {"title": "6.4.4 Mean Distance Between RNN Features", "content": "To evaluate the impact of reward transitions on RL agents' internal representations, we analyzed the convergence of RNN feature\nrepresentations in the playroom maze. Figure 9-(a) depicts the mean Euclidean distance between hidden state vectors for agents\ntrained with S2D, Only Dense, and Only Sparse reward settings. Agents trained using the S2D framework exhibited a significant\nreduction in feature distance following the reward transition, indicating faster convergence of internal representations. This suggests\nthat the sparse reward phase serves as a foundational learning stage, fostering robust initial parameter configurations through\nextensive exploration and facilitating the discovery of diverse state-action mappings. These robust initial parameters enable stable\nand generalizable optimization during subsequent dense reward learning. In contrast, agents trained exclusively with dense rewards\nexhibited slower and less consistent convergence compared to those using the S2D approach. This disparity is likely due to limited\nexploration, which prematurely reinforces suboptimal behaviors. Agents trained solely with sparse rewards demonstrated the slowest\nconvergence overall, as the scarcity of reward signals impeded the development of meaningful representations. For D2S agents, most\nconvergence occurred during the dense reward phase. The sparse reward phase had minimal impact post-transition, as initial dense\nreward optimization induced a primary dense reward bias, thereby limiting adaptability to sparse rewards in later stages."}, {"title": "6.4.5 Action Frequency Analysis", "content": "Figure 9-(b) illustrates the behavior distributions of agents trained under various reward baseline models. Each colored line-blue\n(straight), orange (left), and gray (right)\u2014represents the proportion of behaviors observed at specific checkpoints. During the sparse\nreward phase, both the S2D and Only Sparse models exhibited significant instability in policy behaviors. Upon transitioning to\ndense rewards, the S2D and Only Dense models displayed markedly divergent outcomes. The Only Dense model continued to\nshow instability even after apparent convergence, suggesting that its policy may have settled into a suboptimal local minimum. This\npersistent instability indicates a vulnerability to environmental changes, thereby limiting the model's generalizability. In contrast, the\nS2D approach maintained consistent stability across all five trials, implying that its policy occupies a broader and more optimal"}, {"title": "7 Discussion", "content": "Throughout this study, we focus on the key challenge of balancing exploration and exploitation in goal-oriented RL, particularly\nwith reward shaping. This challenge is heightened in scenarios involving high-dimensional raw input, such as egocentric real-world\nenvironments. To address this, we explore the significant advantages of incorporating S2D reward transitions, ranging from simple\ngridworld environments to complex 3D egocentric-view settings, inspired by toddler learning patterns."}, {"title": "7.1 Performance Improvement", "content": "Our results consistently show that S2D outperforms other reward-shaping strategies across both discrete and continuous action spaces.\nIn more generalizable environments like ViZDoom and mazes, S2D agents still converged faster, achieved optimal performance,\nand exhibited lower variance compared to reward baselines. Moreover, we observed that agents equipped with intrinsic motivation\nalgorithms excel at discovering diverse states but mainly struggle to focus on specific goals, a critical requirement in goal-oriented\nRL. In contrast, the S2D transition mechanism effectively balances exploration with exploitation, thereby facilitating stronger goal\nattainment. Ablation studies reveal that the most beneficial point for transitioning from sparse to dense rewards typically lies around\nthe first quarter of the early training schedule, although the precise timing depends on task complexity. For instance, UR5-Reacher\nrequires an extended free exploration phase before transitioning, aligning with early critical learning periods observed in infant\ndevelopment."}, {"title": "7.2 Impact on 3D Policy Loss Landscape", "content": "One of the most striking findings of this study is the impact of S2D transitions on policy loss landscapes. Using our Cross-Density\nVisualizer (Figure 5), we observed significant smoothing effects during S2D transitions, particularly in environments requiring\ngeneralization. These effects reduce the sharp peaks and valleys typically associated with dense reward settings, thereby facilitating\nconvergence to wider minima. While our primary experiments utilize SAC, we extended our analysis to include other algorithms,\nsuch as PPO [52] and DQN [38], to ensure a broader evaluation. Notably, this smoothing effect predominantly appears with the S2D\ntransition, as further confirmed in additional gridworld experiments detailed in Appendix C."}, {"title": "7.3 Link Between Wide Minima and Toddler-Inspired Reward Transition", "content": "Wide minima, by virtue of their broad and flat characteristics, tend to produce solutions that generalize well to previously unseen\nenvironments. The sharpness metrics in Table 2 support this claim, showing that S2D agents consistently achieve lower sharpness\nvalues indicative of wider minima. Indeed, only the S2D reward transition allowed agents to converge to the broadest minima in\nLunarLander and CartPole-Reacher, where even the Only Sparse approach demonstrated some success. A notable exception arises in\nUR5-Reacher, where the Only Sparse setting exhibits unexpectedly low sharpness but simultaneously yields near-zero performance.\nThis outcome is likely due to limited or absent gradient updates, causing gradient stagnation and high variance-factors that can"}]}