{"title": "Performance Review on LLM for solving leetcode problems", "authors": ["Lun Wang", "Chuanqi Shi", "Shaoshuai Du", "Yiyi Tao", "Yixian shen", "Hang Zheng", "Xinyu Qiu"], "abstract": "This paper presents a comprehensive performance evaluation of Large Language Models (LLMs) in solving programming challenges from Leetcode, a widely used platform for algorithm practice and technical interviews. We began by crawling the Leetcode website to collect a diverse set of problems encompassing various difficulty levels and topics. Using this dataset, we generated solutions with multiple LLMs, including GPT-4 and GPT-3.5-turbo (ChatGPT-turbo). The generated solutions were systematically evaluated for correctness and efficiency. We employed the pass@k metric to assess the success rates within a given number of attempts and analyzed the runtime performance of the solutions. Our results highlight the strengths and limitations of current LLMs [10] in code generation and problem-solving tasks, providing insights into their potential applications and areas for improvement in automated programming assistance.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) like ChatGPT(OpenAI, 2023) have revolutionized artificial intelligence, demonstrating remarkable capabilities in text and image generation. In software development, specialized code-focused LLMs\u2014such as CodeGen(Nijkamp etal., 2022), StarCoder(Li etal., 2023), WizardCoder(Luo etal., 2023), CodeT5(Wang etal., 2021), and Incoder(Fried etal., 2022)\u2014assist developers by automating tasks like code generation, documentation, and unit testing. Additionally, LLMs have been integrated into Integrated Development Environments (IDEs) as code assistants [3], including GitHub Copilot,\u00b9 Amazon CodeWhisperer,2 and Tabnine.3 These tools aim to accelerate development by providing real-time code suggestions and automating routine coding tasks\nThe integration of LLMs into software development offers significant benefits. Developers can save time, focus on higher-level design decisions, and potentially reduce time to market. LLMs help generate boilerplate code, suggest improvements, and assist with complex problem-solving, enhancing productivity and fostering innovation by leveraging the vast knowledge embedded within these models [4]. Despite their widespread adoption, research is increasingly focused on understanding the limitations and evaluating the performance of LLMs. Studies have highlighted security vulnerabilities in AI-generated code(Pearce etal., 2022; Sandoval etal., 2023; Perry 2023) and the prevalence of bugs(Jesse, 2023), emphasizing the need for thorough code review and testing. Other research explores how developers interact with LLMs [12] and integrate them into their workflows(Vaithilingam., 2022; Barke., 2023), examining the dynamics between human creativity and AI assistance [13]. However, evaluating the runtime performance of LLM-generated code has received less attention. While correctness is crucial, the efficiency of code-how fast it runs and how optimally it uses resources is a significant concern in software engineering [6]. Performance optimization is essential when resources are limited, scalability is needed, or energy consumption is a concern(Verdecchia., 2017; Acar etal., 2016). In areas like high-frequency trading, real-time data processing, or large-scale web applications, even minor execution time improvements can have substantial impacts. [8]\nTo address this gap, our research evaluates the performance of code generated by LLMs on algorithmic challenges typical of programming contests and technical interviews. We conduct a comprehensive performance review using problems from Leetcode, a widely used platform offering a vast repository of algorithmic problems across various difficulty levels and topics. [1] Our key contributions are: 1. Performance Analysis of LLM-Generated Code: We analyze the performance of code generated by 18 LLMs on 204 Leetcode problems, investigating performance differences across models using a novel method for measuring and comparing runtime efficiency [11]. 2. Comparison with Human-Written Code: We compare the performance of LLM-generated code with human-written code, providing insights into the current capabilities of LLMs in producing efficient code and highlighting areas where they may lag behind human expertise. 3. Evaluation of Leetcode as a Dataset: We assess the usability of Leetcode as a public repository of algorithmic problems for research purposes,"}, {"title": "II. EXPERIMENT", "content": "This section outlines the experimental framework employed to evaluate the performance of Large Language Models (LLMs) in solving algorithmic problems from Leetcode. The experiment is structured into three primary phases: data collection, code generation, and solution evaluation."}, {"title": "A. Data Collection", "content": "To establish a comprehensive dataset for our evaluation, we crawled the Leetcode website and collected a total of 2,014 problems. These problems span various difficulty levels-Easy, Medium, and Hard\u2014and encompass a wide range of topics including data structures, algorithms, mathematics, and system design. During data collection, we focused on extracting the essential components necessary for code generation:\nProblem Statements: The detailed descriptions of each problem, including the objective and any specific requirements.\nFunction Signatures: The provided code frameworks or templates, specifying input and output formats.\nCode Comments: Any comments included in the code templates that provide additional guidance or constraints.\nWe standardized the problem data by removing any extraneous information such as solution discussions, hints, or previously submitted solutions. This preprocessing ensured that the input to the LLMs was consistent and contained only the information that a typical developer would have when attempting to solve the problem independently."}, {"title": "B. Code Generation", "content": "The code generation phase involved utilizing two categories of LLMs to generate solutions for the collected Leetcode problems: OpenAI Models and GitHub Copilot Model. For each problem, we generated solutions using these models under varying levels of randomness and creativity, controlled by the temperature parameter in the models' settings. The temperature parameter influences the diversity of the output, with higher values producing more varied and creative responses. We utilized a Python framework to automate the code generation process [16]. This framework automatically sent requests to the OpenAI API, providing the standardized problem input (problem statement, code comments, and code framework) as prompts. The solutions returned by the LLM were then parsed and formatted into the required Leetcode submission format in Python code.\nTemperature Settings: We used five different temperature values: 0.2, 0.4, 0.6, 0.8, and 1.0.\nSolution Generation: At each temperature setting, we generated 10 distinct solutions per model for each problem.\nOpenAI Models: We interfaced with the OpenAI API, providing the standardized problem input (problem statement, code comments, and code framework) as prompts. We set the temperature parameter accordingly and generated multiple solutions by invoking the model repeatedly.\nGitHub Copilot: We integrated Copilot into a compatible code editor (e.g., Visual Studio Code) and input the problem's code framework. Copilot's suggestions were captured for each temperature setting by configuring its randomness settings if available or by inducing variability through prompt modifications.\nBy generating multiple solutions across different temperatures, we aimed to observe the impact of the temperature parameter on the correctness and efficiency of the generated code. This process also allowed us to assess the models' ability to produce diverse solutions and their propensity to generate optimal or suboptimal code under varying conditions [5]."}, {"title": "C. Solution Evaluation", "content": "The evaluation phase involved assessing the correctness and performance of the generated solutions by submitting them to Leetcode's online judge system. The Leetcode platform provides an automated environment that compiles and executes submitted code against a predefined set of test cases.\nFor each submitted solution, we collected the following metrics\nNumber of Unit Tests Passed: The total number of test cases successfully passed by the solution.\nOverall status: indicating whether the solution met all the problem requirements.\nRuntime: The execution time of the solution, measured by Leetcode's evaluation system.\nMemory Usage: The amount of memory consumed during execution.\nThe evaluation process was conducted systematically:\n1) Automated Submission: Solutions were programmatically submitted to Leetcode using their API or through automated scripting to ensure consistency and efficiency. [2]\n2) Data Recording: All evaluation results were recorded in a structured format for subsequent analysis. This included capturing the raw output from Leetcode and parsing relevant information."}, {"title": "D. Data Analysis", "content": "In this section, we present the methods and metrics used to analyze the functional correctness and performance of the code generated by the Large Language Models (LLMs). Our analysis aims to assess not only whether the models can produce correct solutions but also how efficiently these solutions run compared to human-written code [9].\n1) D.1 Functional Correctness: Functional correctness measures the extent to which the code generated by an LLM adheres to the specified problem requirements, effectively conforming to the \"program contract\" defined by the input prompt. To evaluate this aspect, we employed the pass@k metric, which calculates the probability that at least one of the k generated samples passes all the test cases for a given problem [7].\nWe computed the pass@ k metrics for k = 1 and k = 10, utilizing the unbiased estimator proposed by Chen et al. (2021). This estimator accounts for the likelihood of obtaining a correct solution among multiple attempts and is defined as:\n$pass@k = E[1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}}]$,\nwhere:\nn is the total number of generated samples,\nc is the number of correct samples (i.e., samples that pass all test cases),\nE denotes the expected value.\nThis formula provides an unbiased estimate of the pass@ k metric by considering all possible combinations of correct and incorrect samples without replacement.\nFollowing the methodology suggested by Chen et al. (2021), we calculated the pass@ k for each temperature setting when evaluating an LLM's functional correctness. The temperature parameter influences the randomness and diversity of the generated solutions. By evaluating across different temperatures, we aimed to identify the optimal setting for each model. The best pass@k value observed across all temperatures was then considered the final pass@ k metric for that LLM.\n2) D.2 Code Performance: To assess the performance of the code generated by the LLMs, we considered three key metrics:\n1) Memory Usage:\nWe recorded the memory consumption reported by Leetcode's evaluation system for each submitted solution. Memory usage is a critical factor in code performance, especially for problems with large input sizes or when operating under memory constraints.\n2) Runtime Performance:\nWe measured the execution time of the generated solutions using pytest-benchmark, a Python benchmarking tool. For each solution, we conducted multiple runs to obtain a reliable estimate of its runtime performance. The median runtime was computed to mitigate the impact of outliers and variability in execution times.\n3) Leetcode Runtime Percentile Rank:\nUpon submission, Leetcode provides a percentile ranking that indicates how a solution's runtime compares to other users' submissions for the same problem. This rank is a value between 0 and 100, representing the percentage of submissions that the current solution outperforms. For example, a rank of 90 implies that the solution is faster than 90% of all other submitted solutions. This metric allowed us to benchmark the LLM-generated code against human-written code at a global scale."}, {"title": "III. RESULTS", "content": "As show in TABLE 1, which presents the performance of various Al models in pass-k metrics, likely representing different tasks or evaluation benchmarks. Below is an analysis of the data:\nOur dataset analysis encompasses approximately 2,100 LeetCode problems, meticulously selected to provide a comprehensive evaluation of Large Language Models (LLMs) across a diverse range of algorithmic challenges. These problems are systematically categorized into three difficulty levels: Easy, Medium, and Hard, adhering to a distribution ratio of approximately 11:50:10, respectively. Furthermore, all solutions generated by the LLMs were implemented in Python, a language renowned for its readability and widespread use in coding competitions and technical interviews. Additionally, each problem was approached using LLMs configured with five different temperature settings-0.2, 0.4, 0.6, 0.8, and 1.0. The temperature parameter controls the creativity and variability of the generated solutions, allowing us to examine how different levels of randomness impact the correctness and efficiency of the code produced. All the experiment code and dataset is published at:"}, {"title": "B. LLMs solution compared with Humans", "content": "To facilitate a robust comparison between LLM-generated solutions and human-written code, we selected the ol-mini model tested on LeetCode for this analysis. The results of this comparison are depicted in Figure 3. Utilizing LeetCode's runtime percentile rankings\u2014which assume that the majority of historical submissions originate from human programmers-we assessed the execution speed of the LLM-generated solutions relative to human-written counterparts. Our findings reveal that the solutions produced by the selected LLM achieve a mean runtime percentile rank of 63%, indicating that they are faster than 63% of all previous submissions."}, {"title": "C. Performance Overview", "content": "Top Performers:\nCanonical Solutions is the highest-performing model with near-perfect scores (97.94 and 98.04). This suggests it is tailored or highly optimized for the specific tasks.\nGTP-4-omni, GPT-4, and GPT-4-turbo follow but with significantly lower scores, indicating a strong performance but a noticeable gap compared to Canonical Solutions.\nMid-Tier Performers:\nModels such as Copilot, CodeLlama-13B-Instruct, and WizardCoder-Python-7B show moderate performance (scores in the range of ~4-19). This reflects some utility but highlights significant room for improvement compared to the top-tier models.\nLower Performers:\nModels like SantaCoder, InCoder-6B, and CodeT5-Large-NTP-PY perform poorly with scores often below 5. These results suggest limited capability in handling the evaluated tasks effectively."}]}